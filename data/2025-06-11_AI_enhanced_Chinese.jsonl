{"id": "2506.08519", "title": "Graph signal aware decomposition of dynamic networks via latent graphs", "authors": ["Bishwadeep Das", "Andrei Buciulea", "Antonio G. Marques", "Elvin Isufi"], "summary": "Dynamics on and of networks refer to changes in topology and node-associated\nsignals, respectively and are pervasive in many socio-technological systems,\nincluding social, biological, and infrastructure networks. Due to practical\nconstraints, privacy concerns, or malfunctions, we often observe only a\nfraction of the topological evolution and associated signal, which not only\nhinders downstream tasks but also restricts our analysis of network evolution.\nSuch aspects could be mitigated by moving our attention at the underlying\nlatent driving factors of the network evolution, which can be naturally\nuncovered via low-rank tensor decomposition. Tensor-based methods provide a\npowerful means of uncovering the underlying factors of network evolution\nthrough low-rank decompositions. However, the extracted embeddings typically\nlack a relational structure and are obtained independently from the node\nsignals. This disconnect reduces the interpretability of the embeddings and\noverlooks the coupling between topology and signals. To address these\nlimitations, we propose a novel two-way decomposition to represent a dynamic\ngraph topology, where the structural evolution is captured by a linear\ncombination of latent graph adjacency matrices reflecting the overall joint\nevolution of both the topology and the signal. Using spatio-temporal data, we\nestimate the latent adjacency matrices and their temporal scaling signatures\nvia alternating minimization, and prove that our approach converges to a\nstationary point. Numerical results show that the proposed method recovers\nindividually and collectively expressive latent graphs, outperforming both\nstandard tensor-based decompositions and signal-based topology identification\nmethods in reconstructing the missing network especially when observations are\nlimited.", "comment": "13 Pages, 9 Figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.08519v1", "AI": {"title_translation": "图信号感知动态网络潜在图分解", "tldr": "本文提出了一种新的双向分解方法，通过潜在图来表示动态图拓扑，以解决现有张量分解方法在解释性和拓扑与信号耦合方面的局限性，尤其在数据有限时能有效恢复缺失网络。", "motivation": "现有张量分解方法提取的嵌入通常缺乏关系结构，且独立于节点信号，降低了解释性并忽略了拓扑与信号之间的耦合。此外，实际限制、隐私问题或故障常导致我们只能观察到拓扑演化和相关信号的一小部分，这阻碍了下游任务并限制了对网络演化的分析。", "method": "提出了一种新颖的双向分解方法来表示动态图拓扑，其中结构演化由潜在图邻接矩阵的线性组合捕获，反映了拓扑和信号的整体联合演化。使用时空数据，通过交替最小化估计潜在邻接矩阵及其时间尺度特征，并证明该方法收敛到驻点。", "result": "数值结果表明，所提出的方法能够恢复单独和集体表达性强的潜在图，在重建缺失网络方面优于标准的基于张量的分解方法和基于信号的拓扑识别方法，尤其是在观测数据有限的情况下。", "conclusion": "本文提出的图信号感知动态网络分解方法通过引入潜在图的概念，有效解决了传统张量分解在处理动态网络时解释性不足和拓扑与信号耦合缺失的问题，并在数据稀疏条件下展现出优越的缺失网络重建能力。", "translation": "动态网络上的动态和网络本身的动态分别指拓扑和节点相关信号的变化，这些变化普遍存在于许多社会技术系统中，包括社交、生物和基础设施网络。由于实际限制、隐私问题或故障，我们通常只能观察到拓扑演化和相关信号的一小部分，这不仅阻碍了下游任务，也限制了我们对网络演化的分析。通过关注网络演化中潜在的驱动因素，这些驱动因素可以通过低秩张量分解自然地揭示出来，可以缓解这些问题。基于张量的方法通过低秩分解提供了一种强大的手段来揭示网络演化的潜在因素。然而，提取的嵌入通常缺乏关系结构，并且独立于节点信号。这种脱节降低了嵌入的可解释性，并忽视了拓扑和信号之间的耦合。为了解决这些局限性，我们提出了一种新颖的双向分解方法来表示动态图拓扑，其中结构演化由潜在图邻接矩阵的线性组合捕获，反映了拓扑和信号的整体联合演化。使用时空数据，我们通过交替最小化估计潜在邻接矩阵及其时间尺度特征，并证明我们的方法收敛到驻点。数值结果表明，所提出的方法能够恢复单独和集体表达性强的潜在图，在重建缺失网络方面优于标准的基于张量的分解方法和基于信号的拓扑识别方法，尤其是在观测数据有限的情况下。", "summary": "本文提出了一种新颖的图信号感知双向分解方法，用于动态网络拓扑表示，旨在解决现有张量分解方法在处理动态网络时解释性差和忽略拓扑与信号耦合的问题。该方法通过潜在图的线性组合来捕获网络结构演化，并使用交替最小化进行参数估计。实验结果表明，与传统方法相比，该方法能更有效地恢复具有表达性的潜在图，尤其在观测数据有限时，在缺失网络重建方面表现出显著优势。", "keywords": "动态网络分解, 潜在图, 图信号, 张量分解, 缺失网络重建", "comments": "这篇论文的创新点在于提出了图信号感知的双向分解方法，有效解决了传统张量分解在动态网络分析中忽视拓扑与信号耦合的局限性。通过引入潜在图的概念，提高了模型的可解释性，并且在数据稀疏的情况下展现出优越的性能，这对于实际应用中数据不完整的情况具有重要意义。"}}
{"id": "2506.08188", "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors", "authors": ["Wenlong Meng", "Shuguo Fan", "Chengkun Wei", "Min Chen", "Yuwei Li", "Yuanchao Zhang", "Zhikun Zhang", "Wenzhi Chen"], "summary": "In this paper, we introduce GradEscape, the first gradient-based evader\ndesigned to attack AI-generated text (AIGT) detectors. GradEscape overcomes the\nundifferentiable computation problem, caused by the discrete nature of text, by\nintroducing a novel approach to construct weighted embeddings for the detector\ninput. It then updates the evader model parameters using feedback from victim\ndetectors, achieving high attack success with minimal text modification. To\naddress the issue of tokenizer mismatch between the evader and the detector, we\nintroduce a warm-started evader method, enabling GradEscape to adapt to\ndetectors across any language model architecture. Moreover, we employ novel\ntokenizer inference and model extraction techniques, facilitating effective\nevasion even in query-only access.\n  We evaluate GradEscape on four datasets and three widely-used language\nmodels, benchmarking it against four state-of-the-art AIGT evaders.\nExperimental results demonstrate that GradEscape outperforms existing evaders\nin various scenarios, including with an 11B paraphrase model, while utilizing\nonly 139M parameters. We have successfully applied GradEscape to two real-world\ncommercial AIGT detectors. Our analysis reveals that the primary vulnerability\nstems from disparity in text expression styles within the training data. We\nalso propose a potential defense strategy to mitigate the threat of AIGT\nevaders. We open-source our GradEscape for developing more robust AIGT\ndetectors.", "comment": "Accepted to USENIX Security'25", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08188v1", "AI": {"title_translation": "GradEscape: 一种基于梯度的对抗AI生成文本检测器的规避器", "tldr": "GradEscape是一种新型的基于梯度的规避器，能够有效攻击AI生成文本检测器，即使在查询受限的情况下也能成功，并且优于现有方法。", "motivation": "这篇论文的动机是开发一种能够攻击AI生成文本（AIGT）检测器的规避器，以克服文本离散性导致的不可微分计算问题，并解决规避器与检测器之间的分词器不匹配问题，揭示AIGT检测器的漏洞。", "method": "GradEscape通过引入一种新颖的方法来构建检测器输入的加权嵌入，克服了文本离散性导致的不可微分计算问题。它利用受害者检测器的反馈更新规避器模型参数。为了解决分词器不匹配问题，引入了热启动规避器方法。此外，还采用了新颖的分词器推断和模型提取技术，以实现查询受限下的有效规避。", "result": "GradEscape在四个数据集和三种广泛使用的语言模型上进行了评估，并与四种最先进的AIGT规避器进行了基准测试。实验结果表明，GradEscape在各种场景下（包括使用11B复述模型）均优于现有规避器，同时仅使用1.39亿参数。它成功应用于两个真实世界的商业AIGT检测器。分析揭示主要漏洞源于训练数据中文本表达风格的差异。", "conclusion": "GradEscape是第一个基于梯度的AIGT检测器规避器，它成功克服了文本离散性、分词器不匹配等挑战，并揭示了AIGT检测器的主要漏洞在于训练数据中文本表达风格的差异。论文还提出了潜在的防御策略，并开源了GradEscape以促进更鲁棒的AIGT检测器开发。", "translation": "在本文中，我们介绍了GradEscape，这是第一个基于梯度的规避器，旨在攻击AI生成文本（AIGT）检测器。GradEscape通过引入一种新颖的方法来构建检测器输入的加权嵌入，克服了文本离散性导致的不可微分计算问题。然后，它利用受害者检测器的反馈更新规避器模型参数，以最小的文本修改实现高攻击成功率。为了解决规避器和检测器之间分词器不匹配的问题，我们引入了一种热启动规避器方法，使GradEscape能够适应任何语言模型架构的检测器。此外，我们采用了新颖的分词器推断和模型提取技术，即使在仅查询访问的情况下也能实现有效规避。\n我们在四个数据集和三个广泛使用的语言模型上评估了GradEscape，并将其与四种最先进的AIGT规避器进行了基准测试。实验结果表明，GradEscape在各种场景下（包括使用11B复述模型）均优于现有规避器，同时仅使用1.39亿参数。我们已成功将GradEscape应用于两个真实世界的商业AIGT检测器。我们的分析表明，主要漏洞源于训练数据中文本表达风格的差异。我们还提出了一种潜在的防御策略，以减轻AIGT规避器的威胁。我们开源了GradEscape，以开发更鲁棒的AIGT检测器。", "summary": "本文介绍了GradEscape，这是一种新型的基于梯度的规避器，用于攻击AI生成文本（AIGT）检测器。它通过构建加权嵌入克服了文本离散性问题，并利用检测器反馈更新模型。为解决分词器不匹配，GradEscape引入了热启动方法，并采用分词器推断和模型提取技术实现查询受限下的规避。实验证明，GradEscape在多个数据集和语言模型上均优于现有规避器，成功应用于商业检测器，并揭示AIGT检测器的主要漏洞在于训练数据中表达风格的差异。论文还提出了一种防御策略并开源了代码。", "keywords": "AI生成文本检测器, 规避器, 梯度攻击, 文本对抗, GradEscape", "comments": "GradEscape的创新之处在于其首次提出了基于梯度的AIGT检测器规避方法，并成功克服了文本离散性带来的不可微分计算挑战。其热启动、分词器推断和模型提取技术使其在现实世界的复杂场景（如分词器不匹配和查询受限）下依然高效。该研究不仅展示了当前AIGT检测器的脆弱性，更通过开源代码和提出防御策略，为未来开发更鲁棒的检测器指明了方向，具有重要的实践和理论意义。"}}
{"id": "2506.08192", "title": "Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms", "authors": ["Jared Claypoole", "Steven Cheung", "Ashish Gehani", "Vinod Yegneswaran", "Ahmad Ridley"], "summary": "We analyze two open source deep reinforcement learning agents submitted to\nthe CAGE Challenge 2 cyber defense challenge, where each competitor submitted\nan agent to defend a simulated network against each of several provided\nrules-based attack agents. We demonstrate that one can gain interpretability of\nagent successes and failures by simplifying the complex state and action spaces\nand by tracking important events, shedding light on the fine-grained behavior\nof both the defense and attack agents in each experimental scenario. By\nanalyzing important events within an evaluation episode, we identify patterns\nin infiltration and clearing events that tell us how well the attacker and\ndefender played their respective roles; for example, defenders were generally\nable to clear infiltrations within one or two timesteps of a host being\nexploited. By examining transitions in the environment's state caused by the\nvarious possible actions, we determine which actions tended to be effective and\nwhich did not, showing that certain important actions are between 40% and 99%\nineffective. We examine how decoy services affect exploit success, concluding\nfor instance that decoys block up to 94% of exploits that would directly grant\nprivileged access to a host. Finally, we discuss the realism of the challenge\nand ways that the CAGE Challenge 4 has addressed some of our concerns.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08192v1", "AI": {"title_translation": "基于强化学习的网络对抗模拟平台中智能体行为的解释", "tldr": "本文分析了CAGE Challenge 2中的强化学习网络防御智能体，通过简化状态空间和事件追踪，解释了智能体的成功与失败，并评估了特定行为的有效性及诱饵服务的影响。", "motivation": "旨在通过解释智能体在网络对抗模拟平台中的行为，理解其成功与失败的原因，并评估不同策略的有效性，以提升网络防御智能体的性能。", "method": "分析CAGE Challenge 2中提交的两个开源深度强化学习智能体；通过简化复杂的स्टेट和动作空间来提高可解释性；追踪重要事件，揭示防御和攻击智能体的细粒度行为；分析评估回合中的重要事件，识别渗透和清除事件的模式；检查环境状态因各种可能动作引起的转换，以确定有效和无效的动作；研究诱饵服务对漏洞利用成功的影响。", "result": "通过简化状态和动作空间并追踪重要事件，可以解释智能体的成功和失败；防御者通常能在主机被利用后一到两个时间步内清除渗透；某些重要动作的无效性在40%到99%之间；诱饵服务可以阻止高达94%的直接授予主机特权访问的漏洞利用；讨论了CAGE Challenge 4如何解决了一些挑战的现实性问题。", "conclusion": "通过对强化学习网络防御智能体行为的深入分析，可以有效地解释其表现，识别有效和无效的策略，并指出提高挑战现实性的方向。", "translation": "我们分析了提交给CAGE Challenge 2网络防御挑战赛的两个开源深度强化学习智能体，在该挑战赛中，每个参赛者提交一个智能体来防御模拟网络，对抗几个提供的基于规则的攻击智能体。我们证明，通过简化复杂的स्टेट和动作空间以及追踪重要事件，可以获得智能体成功和失败的可解释性，从而揭示每个实验场景中防御和攻击智能体的细粒度行为。通过分析评估回合中的重要事件，我们识别了渗透和清除事件的模式，这些模式告诉我们攻击者和防御者各自角色的表现如何；例如，防御者通常能够在主机被利用后的一到两个时间步内清除渗透。通过检查环境状态因各种可能动作引起的转换，我们确定了哪些动作有效，哪些无效，结果显示某些重要动作的无效性在40%到99%之间。我们研究了诱饵服务如何影响漏洞利用的成功，例如，得出结论诱饵可以阻止高达94%的直接授予主机特权访问的漏洞利用。最后，我们讨论了挑战的现实性以及CAGE Challenge 4如何解决了一些我们关注的问题。", "summary": "本文分析了CAGE Challenge 2中的两个开源深度强化学习网络防御智能体。研究通过简化状态和动作空间并追踪关键事件，提高了智能体行为的可解释性，从而揭示了防御和攻击智能体的细粒度表现。研究识别了渗透和清除事件的模式，评估了特定动作的有效性（发现某些动作无效性高达99%），并量化了诱饵服务在阻止漏洞利用方面的显著效果（最高阻止94%的特权访问利用）。最后，文章讨论了挑战的现实性问题，并提及CAGE Challenge 4的改进。", "keywords": "强化学习, 网络安全, 智能体行为解释, 网络对抗模拟, CAGE挑战赛", "comments": "这篇论文通过深入分析具体的强化学习智能体在网络对抗模拟中的行为，为理解复杂RL系统的决策过程提供了宝贵的视角。其创新点在于通过简化复杂环境和追踪关键事件来提高可解释性，并量化了不同防御策略（如诱饵服务）的实际效果。这对于未来设计更有效、更可信赖的网络防御智能体具有重要指导意义。"}}
{"id": "2506.08218", "title": "gh0stEdit: Exploiting Layer-Based Access Vulnerability Within Docker Container Images", "authors": ["Alan Mills", "Jonathan White", "Phil Legg"], "summary": "Containerisation is a popular deployment process for application-level\nvirtualisation using a layer-based approach. Docker is a leading provider of\ncontainerisation, and through the Docker Hub, users can supply Docker images\nfor sharing and re-purposing popular software application containers. Using a\ncombination of in-built inspection commands, publicly displayed image layer\ncontent, and static image scanning, Docker images are designed to ensure end\nusers can clearly assess the content of the image before running them. In this\npaper we present \\textbf{\\textit{gh0stEdit}}, a vulnerability that\nfundamentally undermines the integrity of Docker images and subverts the\nassumed trust and transparency they utilise. The use of gh0stEdit allows an\nattacker to maliciously edit Docker images, in a way that is not shown within\nthe image history, hierarchy or commands. This attack can also be carried out\nagainst signed images (Docker Content Trust) without invalidating the image\nsignature. We present two use case studies for this vulnerability, and showcase\nhow gh0stEdit is able to poison an image in a way that is not picked up through\nstatic or dynamic scanning tools. Our attack case studies highlight the issues\nin the current approach to Docker image security and trust, and expose an\nattack method which could potentially be exploited in the wild without being\ndetected. To the best of our knowledge we are the first to provide detailed\ndiscussion on the exploit of this vulnerability.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08218v1", "AI": {"title_translation": "gh0stEdit：利用Docker容器镜像中的基于层访问漏洞", "tldr": "gh0stEdit是一种新的漏洞，允许攻击者在不被检测的情况下恶意修改Docker镜像，甚至包括已签名的镜像，从而破坏Docker镜像的完整性和信任。", "motivation": "Docker作为领先的容器化提供商，其镜像被广泛共享和重用。然而，现有的检查命令、公开的镜像层内容和静态镜像扫描不足以确保最终用户能够清晰评估镜像内容，存在信任和透明度假设被颠覆的风险。", "method": "研究人员提出了gh0stEdit漏洞，该漏洞利用Docker镜像中基于层的访问漏洞，允许攻击者恶意编辑Docker镜像，且不体现在镜像历史、层次结构或命令中。该攻击甚至可以针对已签名的镜像（Docker内容信任）进行，而不会使镜像签名失效。研究人员通过两个用例研究展示了gh0stEdit如何投毒镜像，且无法被静态或动态扫描工具检测。", "result": "gh0stEdit能够恶意编辑Docker镜像，这种修改不会在镜像历史、层次结构或命令中显示。即使是使用Docker Content Trust签名的镜像也能被攻击，且签名不会失效。攻击案例表明，现有的Docker镜像安全和信任方法存在问题，这种攻击方法有可能在实际环境中被利用而 undetected。", "conclusion": "gh0stEdit漏洞从根本上破坏了Docker镜像的完整性，颠覆了对其信任和透明度的假设。它揭示了当前Docker镜像安全和信任方法中的严重问题，并提出了一种可能在实际环境中被利用且不被检测到的攻击方法。这是首次详细讨论此类漏洞的利用。", "translation": "容器化是一种流行的部署过程，用于使用基于层的方法进行应用程序级虚拟化。Docker是容器化的领先提供商，通过Docker Hub，用户可以提供Docker镜像，用于共享和重用流行的软件应用程序容器。通过结合内置检查命令、公开显示的镜像层内容和静态镜像扫描，Docker镜像旨在确保最终用户在运行它们之前能够清楚地评估镜像内容。在本文中，我们提出了gh0stEdit，这是一个从根本上破坏Docker镜像完整性并颠覆其所利用的假定信任和透明度的漏洞。使用gh0stEdit允许攻击者恶意编辑Docker镜像，这种方式不会在镜像历史、层次结构或命令中显示。这种攻击也可以针对已签名的镜像（Docker内容信任）进行，而不会使镜像签名失效。我们展示了该漏洞的两个用例研究，并展示了gh0stEdit如何以一种无法通过静态或动态扫描工具检测到的方式来污染镜像。我们的攻击案例研究突出了当前Docker镜像安全和信任方法中的问题，并揭示了一种可能在实际环境中被利用而 undetected 的攻击方法。据我们所知，我们是第一个提供关于此漏洞利用的详细讨论。", "summary": "本研究揭示了Docker镜像中一个名为gh0stEdit的严重漏洞。该漏洞允许攻击者在不被Docker历史、层次结构或命令记录的情况下恶意修改Docker镜像，甚至可以针对已签名的镜像进行攻击而不使其签名失效。通过两个用例研究，研究人员展示了gh0stEdit如何规避静态和动态扫描工具的检测，从而破坏了Docker镜像的完整性和信任。这项工作强调了当前Docker镜像安全机制的不足，并警告了可能在实际环境中未被检测到的新型攻击。", "keywords": "Docker, 容器安全, 漏洞利用, gh0stEdit, 镜像完整性", "comments": "gh0stEdit漏洞的发现对Docker镜像的供应链安全构成了重大威胁。其创新之处在于揭示了在不影响可见历史或签名的情况下修改镜像内容的能力，这颠覆了业界对Docker镜像透明度和信任的基本假设。这项工作的重要性在于它揭示了一个可能在实际环境中被恶意利用的潜在高风险漏洞，并强调了需要重新评估和加强Docker镜像的安全审计和验证机制。该研究的局限性可能在于没有提供具体的缓解措施或补丁建议，但其主要贡献在于首次识别并详细讨论了这一关键漏洞。"}}
{"id": "2506.08252", "title": "PoSyn: Secure Power Side-Channel Aware Synthesis", "authors": ["Amisha Srivastava", "Samit S. Miftah", "Hyunmin Kim", "Debjit Pal", "Kanad Basu"], "summary": "Power Side-Channel (PSC) attacks exploit power consumption patterns to\nextract sensitive information, posing risks to cryptographic operations crucial\nfor secure systems. Traditional countermeasures, such as masking, face\nchallenges including complex integration during synthesis, substantial area\noverhead, and susceptibility to optimization removal during logic synthesis. To\naddress these issues, we introduce PoSyn, a novel logic synthesis framework\ndesigned to enhance cryptographic hardware resistance against PSC attacks. Our\nmethod centers on optimal bipartite mapping of vulnerable RTL components to\nstandard cells from the technology library, aiming to minimize PSC leakage. By\nutilizing a cost function integrating critical characteristics from both the\nRTL design and the standard cell library, we strategically modify mapping\ncriteria during RTL-to-netlist conversion without altering design\nfunctionality. Furthermore, we theoretically establish that PoSyn minimizes\nmutual information leakage, strengthening its security against PSC\nvulnerabilities. We evaluate PoSyn across various cryptographic hardware\nimplementations, including AES, RSA, PRESENT, and post-quantum cryptographic\nalgorithms such as Saber and CRYSTALS-Kyber, at technology nodes of 65nm, 45nm,\nand 15nm. Experimental results demonstrate a substantial reduction in success\nrates for Differential Power Analysis (DPA) and Correlation Power Analysis\n(CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further\nconfirms that synthesized netlists exhibit negligible leakage. Additionally,\ncompared to conventional countermeasures like masking and shuffling, PoSyn\nsignificantly lowers attack success rates, achieving reductions of up to 72%,\nwhile simultaneously enhancing area efficiency by as much as 3.79 times.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08252v1", "AI": {"title_translation": "PoSyn: 安全的功耗侧信道感知综合", "tldr": "PoSyn是一种新的逻辑综合框架，通过优化RTL组件到标准单元的映射，显著降低了密码硬件对功耗侧信道攻击的脆弱性，同时提高了面积效率。", "motivation": "功耗侧信道攻击（PSC）利用功耗模式提取敏感信息，对安全系统中的密码操作构成风险。传统的对策（如掩码）面临综合集成复杂、面积开销大以及逻辑综合过程中易受优化移除等挑战。", "method": "本文引入了PoSyn，一个新颖的逻辑综合框架，旨在增强密码硬件对PSC攻击的抵抗力。该方法核心在于将易受攻击的RTL组件最优二分图映射到技术库中的标准单元，以最小化PSC泄漏。通过利用一个集成RTL设计和标准单元库关键特性的成本函数，在RTL到网表转换过程中策略性地修改映射标准，而不改变设计功能。此外，理论上建立了PoSyn最小化互信息泄漏，从而增强了其安全性。", "result": "在AES、RSA、PRESENT以及Saber和CRYSTALS-Kyber等后量子密码算法的各种密码硬件实现上，在65nm、45nm和15nm技术节点进行了PoSyn评估。实验结果表明，差分功耗分析（DPA）和相关功耗分析（CPA）攻击的成功率显著降低，分别达到3%和6%。TVLA分析证实综合后的网表泄漏可忽略不计。与掩码和混淆等传统对策相比，PoSyn显著降低了攻击成功率，降幅高达72%，同时面积效率提升高达3.79倍。", "conclusion": "PoSyn通过其创新的逻辑综合框架，有效解决了功耗侧信道攻击的挑战，显著增强了密码硬件的安全性，同时提高了面积效率。", "translation": "功耗侧信道（PSC）攻击利用功耗模式提取敏感信息，对安全系统中至关重要的密码操作构成风险。传统的对策，如掩码，面临着综合集成复杂、面积开销大以及逻辑综合过程中易受优化移除等挑战。为了解决这些问题，我们引入了PoSyn，一个新颖的逻辑综合框架，旨在增强密码硬件对PSC攻击的抵抗力。我们的方法核心在于将易受攻击的RTL组件最优二分图映射到技术库中的标准单元，旨在最小化PSC泄漏。通过利用一个集成RTL设计和标准单元库关键特性的成本函数，我们在RTL到网表转换过程中策略性地修改映射标准，而不改变设计功能。此外，我们理论上建立了PoSyn最小化互信息泄漏，从而增强了其对抗PSC漏洞的安全性。我们在65nm、45nm和15nm技术节点上，对包括AES、RSA、PRESENT以及Saber和CRYSTALS-Kyber等后量子密码算法在内的各种密码硬件实现进行了PoSyn评估。实验结果表明，差分功耗分析（DPA）和相关功耗分析（CPA）攻击的成功率显著降低，分别达到3%和6%的低点。TVLA分析进一步证实，综合后的网表表现出可忽略的泄漏。此外，与掩码和混淆等传统对策相比，PoSyn显著降低了攻击成功率，降幅高达72%，同时面积效率提升高达3.79倍。", "summary": "本文提出了PoSyn，一个创新的逻辑综合框架，旨在增强密码硬件对功耗侧信道攻击的抵抗力。PoSyn通过优化RTL组件到标准单元的二分图映射来最小化功耗泄漏，并理论证明了其能最小化互信息泄漏。实验结果表明，PoSyn在多种密码算法上显著降低了DPA和CPA攻击的成功率（分别低至3%和6%），并与传统对策相比，攻击成功率降低高达72%，同时面积效率提升高达3.79倍。", "keywords": "功耗侧信道攻击, 逻辑综合, 密码硬件, PoSyn, 二分图映射", "comments": "这篇论文提出了一种新颖的、在逻辑综合阶段解决功耗侧信道攻击的方法，这与传统的在更高层级（如RTL或算法层面）进行对策的方法有所不同。其创新点在于将抗侧信道能力融入到设计流程的早期，通过优化映射策略来降低泄漏。实验结果显示出显著的性能提升，尤其是在降低攻击成功率和提高面积效率方面。该方法有望为安全芯片的设计提供更高效的解决方案。"}}
{"id": "2506.08055", "title": "A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing", "authors": ["Sabbir M. Saleh", "Nazim Madhavji", "John Steinbacher"], "summary": "As cloud environments become widespread, cybersecurity has emerged as a top\npriority across areas such as networks, communication, data privacy, response\ntimes, and availability. Various sectors, including industries, healthcare, and\ngovernment, have recently faced cyberattacks targeting their computing systems.\nEnsuring secure app deployment in cloud environments requires substantial\neffort. With the growing interest in cloud security, conducting a systematic\nliterature review (SLR) is critical to identifying research gaps. Continuous\nSoftware Engineering, which includes continuous integration (CI), delivery\n(CDE), and deployment (CD), is essential for software development and\ndeployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,\nand challenges related to the security of CI/CD in the cloud. We addressed key\naspects of cloud security and CI/CD and reported on tools such as Harbor,\nSonarQube, and GitHub Actions. Challenges such as image manipulation,\nunauthorised access, and weak authentication were highlighted. The review also\nuncovered research gaps in how tools and practices address these security\nissues in CI/CD pipelines, revealing a need for further study to improve\ncloud-based security solutions.", "comment": "11 pages, 3 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08055v1", "AI": {"title_translation": "关于安全云计算中持续集成和部署 (CI/CD) 的系统文献综述", "tldr": "本文对安全云计算中的持续集成和部署 (CI/CD) 进行了系统文献综述，总结了工具、方法和挑战，并揭示了研究空白。", "motivation": "随着云环境的普及，网络安全已成为首要任务，各行业面临网络攻击。在云环境中确保应用程序安全部署需要大量努力。鉴于对云安全日益增长的兴趣，进行系统文献综述对于识别研究空白至关重要。", "method": "作者进行了一项系统文献综述 (SLR)，审查了66篇论文，总结了与云中 CI/CD 安全相关的工具、方法和挑战。他们探讨了云安全和 CI/CD 的关键方面，并报告了具体的工具。", "result": "该综述总结了与云中 CI/CD 安全相关的工具（如 Harbor、SonarQube 和 GitHub Actions）、方法和挑战（如图像篡改、未经授权访问和弱认证）。此外，它还揭示了工具和实践如何解决 CI/CD 流水线中这些安全问题的研究空白。", "conclusion": "需要进一步研究以改进基于云的安全解决方案，特别是在工具和实践如何解决 CI/CD 流水线中的安全问题方面。", "translation": "随着云环境的普及，网络安全已成为网络、通信、数据隐私、响应时间和可用性等领域的首要任务。各行各业，包括工业、医疗保健和政府，最近都面临针对其计算系统的网络攻击。确保云环境中应用程序的安全部署需要付出巨大努力。随着对云安全兴趣的增长，进行系统文献综述 (SLR) 对于识别研究空白至关重要。持续软件工程，包括持续集成 (CI)、持续交付 (CDE) 和持续部署 (CD)，对于软件开发和部署至关重要。在我们的系统文献综述中，我们审查了66篇论文，总结了与云中 CI/CD 安全相关的工具、方法和挑战。我们探讨了云安全和 CI/CD 的关键方面，并报告了 Harbor、SonarQube 和 GitHub Actions 等工具。图像篡改、未经授权访问和弱认证等挑战被重点强调。该综述还揭示了工具和实践如何解决 CI/CD 流水线中这些安全问题的研究空白，表明需要进一步研究以改进基于云的安全解决方案。", "summary": "本文对安全云计算中的持续集成和部署 (CI/CD) 进行了系统文献综述。鉴于云环境中网络安全的重要性日益增加和面临的挑战，作者审查了66篇论文，总结了与 CI/CD 安全相关的工具、方法和挑战。综述强调了如 Harbor 和 SonarQube 等工具，并指出了图像篡改和未经授权访问等挑战。研究发现当前工具和实践在解决 CI/CD 流水线安全问题方面存在研究空白，强调了对改进云安全解决方案的持续需求。", "keywords": "持续集成, 持续部署, 云计算, 网络安全, 系统文献综述", "comments": "该论文的重要性在于其对云环境中 CI/CD 安全这一关键且不断发展的领域进行了系统性回顾。通过识别现有工具、方法、挑战和明确的研究空白，它为未来的研究和开发提供了宝贵的路线图，有助于增强云原生应用部署的安全性。"}}
{"id": "2506.08039", "title": "AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production", "authors": ["Ray Wai Man Kong"], "summary": "Efficiency, speed, and precision are essential in modern manufacturing. AI\nMaglev Conveyor system, combining magnetic levitation (maglev) technology with\nartificial intelligence (AI), revolutionizes automated production processes.\nThis system reduces maintenance costs and downtime by eliminating friction,\nenhancing operational efficiency. It transports goods swiftly with minimal\nenergy consumption, optimizing resource use and supporting sustainability. AI\nintegration enables real-time monitoring and adaptive control, allowing\nbusinesses to respond to production demand fluctuations and streamline supply\nchain operations.\n  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse\nproduct types and sizes for flexible manufacturing without extensive\nreconfiguration. AI algorithms optimize routing, reduce cycle times, and\nimprove throughput, creating an agile production line adaptable to market\nchanges.\n  This applied research paper introduces the Maglev Conveyor system, featuring\nan electromagnetic controller and multiple movers to enhance automation. It\noffers cost savings as an alternative to setups using six-axis robots or linear\nmotors, with precise adjustments for robotic arm loading. Operating at high\nspeeds minimizes treatment time for delicate components while maintaining\nprecision. Its adaptable design accommodates various materials, facilitating\nintegration of processing stations alongside electronic product assembly.\nPositioned between linear-axis and robotic systems in cost, the Maglev Conveyor\nis ideal for flat parts requiring minimal travel, transforming production\nefficiency across industries. It explores its technical advantages,\nflexibility, cost reductions, and overall benefits.", "comment": "12 pages, 9 Figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08039v1", "AI": {"title_translation": "自动化装配生产中的AI磁悬浮输送机", "tldr": "该论文介绍了一种结合AI和磁悬浮技术的输送系统，旨在提高自动化生产的效率、速度和精度，同时降低成本和能耗，实现灵活、适应性强的生产线。", "motivation": "现代制造业对效率、速度和精度有极高要求，传统生产系统存在维护成本高、停机时间长、能耗大、适应性差等问题。", "method": "本文介绍了一种结合磁悬浮技术（Maglev）和人工智能（AI）的AI磁悬浮输送系统。该系统通过电磁控制器和多个移动器实现无摩擦运输，AI算法支持实时监控、自适应控制、路径优化、缩短周期时间及提高吞吐量。", "result": "该系统通过消除摩擦，降低维护成本和停机时间，提高运行效率；以最小能耗快速运输货物，优化资源利用；AI实现实时监控和自适应控制，提高对生产需求波动的响应能力和供应链运作效率。它提供平稳、安静的运行，适应多种产品类型和尺寸，实现灵活制造。与六轴机器人或直线电机设置相比，可节省成本，并为机械臂装载提供精确调整。高速运行可缩短敏感部件的处理时间并保持精度。其适应性设计可兼容各种材料，便于集成加工站，尤其适用于需要短距离移动的扁平部件。", "conclusion": "AI磁悬浮输送系统在成本上介于直线轴和机器人系统之间，特别适用于扁平部件的短距离运输，能够显著提升各行业的生产效率。它通过技术优势、灵活性和成本降低，彻底改变自动化生产流程。", "translation": "效率、速度和精度在现代制造业中至关重要。AI磁悬浮输送系统将磁悬浮（maglev）技术与人工智能（AI）相结合，彻底改变了自动化生产流程。该系统通过消除摩擦来降低维护成本和停机时间，从而提高运营效率。它以最小的能耗快速运输货物，优化资源利用并支持可持续发展。AI的集成实现了实时监控和自适应控制，使企业能够响应生产需求波动并简化供应链运营。AI磁悬浮输送机提供平稳、安静的运行，可适应各种产品类型和尺寸，实现灵活制造而无需大量重新配置。AI算法优化路径，缩短周期时间，提高吞吐量，从而创建了一条适应市场变化的敏捷生产线。这篇应用研究论文介绍了磁悬浮输送系统，该系统具有电磁控制器和多个移动器，以增强自动化。它作为六轴机器人或直线电机设置的替代方案，可节省成本，并为机械臂装载提供精确调整。高速运行可最大限度地缩短敏感部件的处理时间，同时保持精度。其适应性设计可适应各种材料，有助于将加工站与电子产品组装集成。磁悬浮输送机在成本上介于直线轴和机器人系统之间，是需要短距离移动的扁平部件的理想选择，可改变各行业的生产效率。本文探讨了其技术优势、灵活性、成本降低和整体效益。", "summary": "本文介绍了一种结合AI和磁悬浮技术的创新输送系统，旨在提升自动化生产的效率、速度和精度。该系统通过无摩擦运行降低维护成本和能耗，并利用AI实现实时监控、自适应控制和路径优化，从而提高生产线的灵活性、吞吐量和对市场变化的响应能力。它作为一种经济高效的替代方案，适用于多种产品类型和材料，尤其适合扁平部件的短距离运输，有望彻底改变工业生产效率。", "keywords": "磁悬浮输送机, 人工智能, 自动化生产, 柔性制造, 供应链优化", "comments": "这篇论文提出了一种结合AI和磁悬浮技术的创新性输送系统，其主要亮点在于通过消除摩擦显著降低了维护成本和能耗，并利用AI实现了高度的自动化、灵活性和适应性。该系统在成本和性能上提供了介于传统直线轴和机器人系统之间的新选择，对于提升现代制造业的效率和可持续性具有重要意义。其对精细部件的高速精确处理能力以及对多种材料的适应性，展现了其在未来智能工厂中的巨大潜力。"}}
{"id": "2506.08048", "title": "Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts", "authors": ["Zheng Han", "Jun Zhou", "Jialun Pei", "Jing Qin", "Yingfang Fan", "Qi Dou"], "summary": "In augmented reality (AR)-guided surgical navigation, preoperative organ\nmodels are superimposed onto the patient's intraoperative anatomy to visualize\ncritical structures such as vessels and tumors. Accurate deformation modeling\nis essential to maintain the reliability of AR overlays by ensuring alignment\nbetween preoperative models and the dynamically changing anatomy. Although the\nfinite element method (FEM) offers physically plausible modeling, its high\ncomputational cost limits intraoperative applicability. Moreover, existing\nalgorithms often fail to handle large anatomical changes, such as those induced\nby pneumoperitoneum or ligament dissection, leading to inaccurate anatomical\ncorrespondences and compromised AR guidance. To address these challenges, we\npropose a data-driven biomechanics algorithm that preserves FEM-level accuracy\nwhile improving computational efficiency. In addition, we introduce a novel\nhuman-in-the-loop mechanism into the deformation modeling process. This enables\nsurgeons to interactively provide prompts to correct anatomical misalignments,\nthereby incorporating clinical expertise and allowing the model to adapt\ndynamically to complex surgical scenarios. Experiments on a publicly available\ndataset demonstrate that our algorithm achieves a mean target registration\nerror of 3.42 mm. Incorporating surgeon prompts through the interactive\nframework further reduces the error to 2.78 mm, surpassing state-of-the-art\nmethods in volumetric accuracy. These results highlight the ability of our\nframework to deliver efficient and accurate deformation modeling while\nenhancing surgeon-algorithm collaboration, paving the way for safer and more\nreliable computer-assisted surgeries.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08048v1", "AI": {"title_translation": "迈向可靠的AR引导手术导航：基于数据驱动生物力学和提示的交互式形变建模", "tldr": "该研究提出了一种结合数据驱动生物力学和交互式医生提示的形变建模算法，以提高AR引导手术导航的可靠性和准确性。", "motivation": "在AR引导手术导航中，术前器官模型与术中解剖结构对齐的准确形变建模至关重要。然而，现有方法如有限元法(FEM)计算成本高昂，且难以处理气腹或韧带解剖引起的大幅度解剖变化，导致AR指导不准确。", "method": "提出了一种数据驱动的生物力学算法，在保持有限元法(FEM)级别精度的同时提高了计算效率。此外，引入了一种新颖的人机协作机制，允许外科医生通过交互式提示纠正解剖结构错位，从而将临床专业知识融入模型。", "result": "在公开数据集上的实验表明，该算法的平均目标配准误差为3.42毫米。通过交互式框架整合外科医生提示后，误差进一步降低至2.78毫米，在体积精度上超越了现有最先进的方法。", "conclusion": "该框架能够提供高效且准确的形变建模，同时增强外科医生与算法的协作，为更安全、更可靠的计算机辅助手术铺平了道路。", "translation": "在增强现实（AR）引导的手术导航中，术前器官模型叠加到患者的术中解剖结构上，以可视化血管和肿瘤等关键结构。准确的形变建模对于通过确保术前模型与动态变化的解剖结构之间的一致性来维持AR叠加的可靠性至关重要。尽管有限元法（FEM）提供了物理上合理的建模，但其高计算成本限制了术中适用性。此外，现有算法通常无法处理大的解剖变化，例如由气腹或韧带解剖引起的，导致解剖对应关系不准确和AR指导受损。为了解决这些这些挑战，我们提出了一种数据驱动的生物力学算法，该算法在保持FEM级别精度的同时提高了计算效率。此外，我们在形变建模过程中引入了一种新颖的人机协作机制。这使得外科医生能够交互式地提供提示来纠正解剖错位，从而融入临床专业知识并允许模型动态适应复杂的手术场景。在公开数据集上的实验表明，我们的算法实现了3.42毫米的平均目标配准误差。通过交互式框架整合外科医生提示进一步将误差降低到2.78毫米，在体积精度上超越了现有最先进的方法。这些结果凸显了我们框架提供高效准确形变建模的能力，同时增强了外科医生与算法的协作，为更安全、更可靠的计算机辅助手术铺平了道路。", "summary": "本文针对AR引导手术导航中形变建模的挑战，提出了一种结合数据驱动生物力学和人机协作机制的新框架。该框架通过数据驱动算法实现FEM级别的精度和高计算效率，并通过外科医生交互式提示纠正解剖错位，整合临床经验。实验证明，该方法能有效降低配准误差，在体积精度上优于现有技术，有望提升计算机辅助手术的可靠性与安全性。", "keywords": "AR引导手术导航, 形变建模, 数据驱动生物力学, 交互式提示, 计算机辅助手术", "comments": "本文的创新点在于结合了数据驱动的生物力学模型与新颖的人机协作（外科医生提示）机制，有效解决了AR引导手术导航中形变建模的计算效率与精度难题，尤其是在处理大幅度解剖变化方面。其重要性在于通过提升术中AR叠加的可靠性，有望显著提高计算机辅助手术的安全性与准确性。"}}
{"id": "2506.08263", "title": "Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming", "authors": ["Pouya Agheli", "Tugce Kobal", "François Durand", "Matthew Andrews"], "summary": "We investigate the multiuser scheduling problem in multiple-input\nmultiple-output (MIMO) systems using orthogonal frequency division multiplexing\n(OFDM) and hybrid beamforming in which a base station (BS) communicates with\nmultiple users over millimeter wave (mmWave) channels in the downlink. Improved\nscheduling is critical for enhancing spectral efficiency and the long-term\nperformance of the system from the perspective of proportional fairness (PF)\nmetric in hybrid beamforming systems due to its limited multiplexing gain. Our\nobjective is to maximize PF by properly designing the analog and digital\nprecoders within the hybrid beamforming and selecting the users subject to the\nnumber of radio frequency (RF) chains. Leveraging the characteristics of mmWave\nchannels, we apply a two-timescale protocol. On a long timescale, we assign an\nanalog beam to each user. Scheduling the users and designing the digital\nprecoder are done accordingly on a short timescale. To conduct scheduling, we\npropose combinatorial solutions, such as greedy and sorting algorithms,\nfollowed by a machine learning (ML) approach. Our numerical results highlight\nthe trade-off between the performance and complexity of the proposed\napproaches. Consequently, we show that the choice of approach depends on the\nspecific criteria within a given scenario.", "comment": "To appear in the proceedings of the European Conference on Networks\n  and Communications (EuCNC) & 6G Summit, 2025", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.08263v1", "AI": {"title_translation": "混合波束成形MIMO-OFDM系统中基于学习的多用户调度", "tldr": "该论文研究了混合波束成形MIMO-OFDM系统中的多用户调度问题，旨在通过设计模拟和数字预编码器以及选择用户来最大化比例公平性，并提出了组合式（如贪婪和排序算法）和机器学习方法，结果表明性能和复杂性之间存在权衡。", "motivation": "由于混合波束成形系统的多路复用增益有限，改进调度对于从比例公平性（PF）指标角度提高频谱效率和系统长期性能至关重要。目标是在无线射频（RF）链数量限制下，通过适当设计混合波束成形中的模拟和数字预编码器并选择用户来最大化PF。", "method": "论文采用双时间尺度协议：在长时间尺度上为每个用户分配模拟波束，在短时间尺度上相应地进行用户调度和数字预编码器设计。为了进行调度，提出了组合式解决方案（如贪婪和排序算法），随后采用机器学习（ML）方法。", "result": "数值结果突出了所提出方法在性能和复杂性之间的权衡。", "conclusion": "研究表明，方法的选择取决于给定场景中的具体标准。", "translation": "我们研究了使用正交频分复用（OFDM）和混合波束成形的多输入多输出（MIMO）系统中的多用户调度问题，其中基站（BS）在下行链路中通过毫米波（mmWave）信道与多个用户通信。由于混合波束成形系统的多路复用增益有限，从比例公平性（PF）指标的角度来看，改进调度对于提高频谱效率和系统长期性能至关重要。我们的目标是在无线射频（RF）链数量限制下，通过适当设计混合波束成形中的模拟和数字预编码器并选择用户来最大化PF。利用毫米波信道的特性，我们采用了双时间尺度协议。在长时间尺度上，我们为每个用户分配一个模拟波束。用户调度和数字预编码器设计相应地在短时间尺度上完成。为了进行调度，我们提出了组合式解决方案，如贪婪和排序算法，随后采用机器学习（ML）方法。我们的数值结果突出了所提出方法在性能和复杂性之间的权衡。因此，我们表明方法的选择取决于给定场景中的具体标准。", "summary": "该论文研究了混合波束成形MIMO-OFDM系统中毫米波下行链路的多用户调度问题，旨在通过设计模拟和数字预编码器并选择用户来最大化比例公平性。研究提出了一种双时间尺度协议，并在短时间尺度上采用组合式（贪婪、排序）和机器学习方法进行调度。数值结果揭示了所提方法在性能和复杂性之间的权衡，表明最佳方法取决于具体场景。", "keywords": "多用户调度, 混合波束成形, MIMO-OFDM, 毫米波, 机器学习", "comments": "该论文的创新之处在于将机器学习和组合方法应用于混合波束成形毫米波系统中有限多路复用增益下的多用户调度，以优化比例公平性。提出的双时间尺度协议也是一个亮点。其局限性可能在于数值结果揭示的性能与复杂性之间的权衡，这意味着没有一种单一的解决方案适用于所有场景。"}}
{"id": "2506.08026", "title": "TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load", "authors": ["Xibai Wang"], "summary": "This paper proposes TIP-Search, a time-predictable inference scheduling\nframework for real-time market prediction under uncertain workloads. Motivated\nby the strict latency demands in high-frequency financial systems, TIP-Search\ndynamically selects a deep learning model from a heterogeneous pool, aiming to\nmaximize predictive accuracy while satisfying per-task deadline constraints.\nOur approach profiles latency and generalization performance offline, then\nperforms online task-aware selection without relying on explicit input domain\nlabels. We evaluate TIP-Search on three real-world limit order book datasets\n(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms\nstatic baselines with up to 8.5% improvement in accuracy and 100% deadline\nsatisfaction. Our results highlight the effectiveness of TIP-Search in robust\nlow-latency financial inference under uncertainty.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08026v1", "AI": {"title_translation": "TIP-Search：不确定负载下市场预测的时间可预测推理调度", "tldr": "TIP-Search 是一种针对不确定负载下市场预测的时间可预测推理调度框架，通过动态选择深度学习模型，在满足截止日期的同时最大化预测准确性。", "motivation": "高频金融系统对延迟有严格要求，需要在不确定工作负载下进行实时市场预测。", "method": "TIP-Search 离线分析模型延迟和泛化性能，然后在线进行任务感知选择，动态地从异构模型池中选择深度学习模型，以在满足每个任务截止日期的同时最大化预测准确性。", "result": "在三个真实世界限价订单簿数据集（FI-2010、Binance BTC/USDT、LOBSTER AAPL）上，TIP-Search 优于静态基线，准确性提高了 8.5%，截止日期满足率达到 100%。", "conclusion": "TIP-Search 在不确定性下实现了鲁棒的低延迟金融推理，证明了其有效性。", "translation": "本文提出了 TIP-Search，一个用于不确定工作负载下实时市场预测的时间可预测推理调度框架。受高频金融系统严格延迟需求的驱动，TIP-Search 动态地从异构模型池中选择一个深度学习模型，旨在最大化预测准确性，同时满足每任务截止日期约束。我们的方法离线分析延迟和泛化性能，然后在线执行任务感知选择，而不依赖于显式输入域标签。我们在三个真实世界的限价订单簿数据集（FI-2010、Binance BTC/USDT、LOBSTER AAPL）上评估了 TIP-Search，结果表明它优于静态基线，准确性提高了 8.5%，截止日期满足率达到 100%。我们的结果强调了 TIP-Search 在不确定性下进行鲁棒低延迟金融推理的有效性。", "summary": "本文介绍了 TIP-Search，一个为不确定负载下的实时市场预测设计的时间可预测推理调度框架。该框架通过动态选择合适的深度学习模型，以在高频金融环境中满足严格的延迟要求并最大化预测准确性。实验结果表明，TIP-Search 在准确性和截止日期满足率方面显著优于传统方法。", "keywords": "时间可预测性, 推理调度, 市场预测, 深度学习, 金融系统", "comments": "TIP-Search 的创新之处在于其动态模型选择机制，能够在不确定负载下保证时间可预测性，这对于高频金融系统至关重要。该方法通过结合离线分析和在线任务感知选择，有效解决了实时预测中的准确性和延迟权衡问题，具有重要的实际应用价值。"}}
{"id": "2506.08018", "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache", "authors": ["Fei Li", "Song Liu", "Weiguo Wu", "Shiqiang Nie", "Jinyu Wang"], "summary": "The high memory demands of the Key-Value (KV) Cache during the inference of\nLarge Language Models (LLMs) severely restrict their deployment in\nresource-constrained platforms. Quantization can effectively alleviate the\nmemory pressure caused by KV Cache. However, existing methods either rely on\nstatic one-size-fits-all precision allocation or fail to dynamically prioritize\ncritical KV in long-context tasks, forcing memory-accuracy-throughput\ntradeoffs. In this work, we propose a novel mixed-precision quantization method\nfor KV Cache named KVmix. KVmix leverages gradient-based importance analysis to\nevaluate how individual Key and Value projection matrices affect the model\nloss, enabling layer-specific bit-width allocation for mix-precision\nquantization. It dynamically prioritizes higher precision for important layers\nwhile aggressively quantizing less influential ones, achieving a tunable\nbalance between accuracy and efficiency. KVmix also introduces a dynamic\nlong-context optimization strategy that adaptively keeps full-precision KV\npairs for recent pivotal tokens and compresses older ones, achieving\nhigh-quality sequence generation with low memory usage. Additionally, KVmix\nprovides efficient low-bit quantization and CUDA kernels to optimize\ncomputational overhead. On LLMs such as Llama and Mistral, KVmix achieves\nnear-lossless inference performance with extremely low quantization\nconfiguration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x\nmemory compression and a 5.3x speedup in inference throughput.", "comment": "14 pages, 8 figures, 4 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08018v1", "AI": {"title_translation": "KVmix：面向KV缓存的基于梯度的层重要性感知混合精度量化", "tldr": "KVmix是一种新的混合精度量化方法，通过梯度分析和动态长上下文优化，显著压缩KV缓存内存并提高LLM推理速度，同时保持近乎无损的性能。", "motivation": "大语言模型（LLMs）推理过程中，KV缓存的高内存需求严重限制了其在资源受限平台上的部署。现有量化方法要么采用静态的统一精度分配，要么未能动态优先处理长上下文任务中的关键KV，导致内存-精度-吞吐量之间的权衡问题。", "method": "KVmix是一种新颖的KV缓存混合精度量化方法。它利用基于梯度的重要性分析来评估单个Key和Value投影矩阵对模型损失的影响，从而实现层特定的位宽分配。KVmix动态地为重要层分配更高精度，同时积极量化影响较小的层，以平衡精度和效率。它还引入了一种动态长上下文优化策略，自适应地为最近的关键token保留全精度KV对并压缩旧的KV对。此外，KVmix提供高效的低位量化和CUDA内核来优化计算开销。", "result": "在Llama和Mistral等LLM上，KVmix在极低的量化配置（Key 2.19bit，Value 2.38bit）下实现了近乎无损的推理性能，同时实现了4.9倍的内存压缩和5.3倍的推理吞吐量加速。", "conclusion": "KVmix通过其创新的梯度重要性分析和动态长上下文优化策略，成功解决了LLM KV缓存高内存需求的问题，实现了显著的内存压缩和推理加速，同时保持了高精度，使其成为资源受限环境下部署LLM的有效解决方案。", "translation": "大语言模型（LLM）推理过程中Key-Value（KV）缓存的高内存需求严重限制了其在资源受限平台上的部署。量化可以有效缓解KV缓存造成的内存压力。然而，现有方法要么依赖静态的“一刀切”精度分配，要么未能动态优先处理长上下文任务中的关键KV，从而强制进行内存-精度-吞吐量的权衡。在这项工作中，我们提出了一种新颖的KV缓存混合精度量化方法，名为KVmix。KVmix利用基于梯度的重要性分析来评估单个Key和Value投影矩阵如何影响模型损失，从而实现层特定的位宽分配，用于混合精度量化。它动态地为重要层优先分配更高精度，同时积极量化影响较小的层，从而在精度和效率之间实现可调的平衡。KVmix还引入了一种动态长上下文优化策略，自适应地为最近的关键token保留全精度KV对并压缩旧的KV对，以低内存使用实现高质量的序列生成。此外，KVmix提供了高效的低位量化和CUDA内核来优化计算开销。在Llama和Mistral等LLM上，KVmix在极低的量化配置（Key 2.19bit，Value 2.38bit）下实现了近乎无损的推理性能，同时实现了惊人的4.9倍内存压缩和5.3倍推理吞吐量加速。", "summary": "KVmix是一种创新的混合精度量化方法，旨在解决大语言模型（LLM）KV缓存的高内存占用问题。该方法通过梯度分析评估Key和Value投影矩阵对模型损失的影响，从而实现层级位宽分配，并动态地为重要层分配更高精度。此外，KVmix引入了动态长上下文优化策略，以保留近期关键token的全精度KV对并压缩旧的。实验表明，KVmix在LLM上实现了显著的内存压缩（4.9倍）和推理速度提升（5.3倍），同时保持了近乎无损的性能。", "keywords": "KV缓存量化, 混合精度, 大语言模型, 梯度分析, 内存优化", "comments": "KVmix的创新点在于其结合了梯度重要性分析和动态长上下文优化策略，实现了KV缓存的精细化混合精度量化。这种方法不仅解决了LLM在资源受限设备上的部署挑战，还通过动态策略确保了长上下文任务的性能。其显著的内存压缩和推理加速效果，同时保持近乎无损的精度，使其在实际应用中具有重要价值。"}}
{"id": "2506.08041", "title": "The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students", "authors": ["Siddharth Siddharth", "Brainerd Prince", "Amol Harsh", "Shreyas Ramachandran"], "summary": "This work presents a novel course titled The World of AI designed for\nfirst-year undergraduate engineering students with little to no prior exposure\nto AI. The central problem addressed by this course is that engineering\nstudents often lack foundational knowledge of AI and its broader societal\nimplications at the outset of their academic journeys. We believe the way to\naddress this gap is to design and deliver an interdisciplinary course that can\na) be accessed by first-year undergraduate engineering students across any\ndomain, b) enable them to understand the basic workings of AI systems sans\nmathematics, and c) make them appreciate AI's far-reaching implications on our\nlives. The course was divided into three modules co-delivered by faculty from\nboth engineering and humanities. The planetary module explored AI's dual role\nas both a catalyst for sustainability and a contributor to environmental\nchallenges. The societal impact module focused on AI biases and concerns around\nprivacy and fairness. Lastly, the workplace module highlighted AI-driven job\ndisplacement, emphasizing the importance of adaptation. The novelty of this\ncourse lies in its interdisciplinary curriculum design and pedagogical\napproach, which combines technical instruction with societal discourse. Results\nrevealed that students' comprehension of AI challenges improved across diverse\nmetrics like (a) increased awareness of AI's environmental impact, and (b)\nefficient corrective solutions for AI fairness. Furthermore, it also indicated\nthe evolution in students' perception of AI's transformative impact on our\nlives.", "comment": "Accepted for publication at AIED 2025 in the late-breaking work track", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.08041v1", "AI": {"title_translation": "人工智能世界：面向一年级工科学生的人工智能素养新方法", "tldr": "该研究介绍了一门名为“人工智能世界”的新课程，旨在提高一年级工科学生的人工智能素养，通过跨学科教学帮助他们理解人工智能的基本原理及其对社会的影响。", "motivation": "工程系学生在学业初期普遍缺乏人工智能的基础知识及其更广泛的社会影响认知。", "method": "设计并开设了一门名为“人工智能世界”的跨学科课程，面向一年级工科学生。课程分为三个模块，由工程学和人文学科的教师共同授课：行星模块（人工智能与可持续性/环境挑战）、社会影响模块（人工智能偏见、隐私和公平性）、工作场所模块（人工智能驱动的就业替代和适应）。课程的创新之处在于其跨学科课程设计和教学方法，将技术指导与社会讨论相结合。", "result": "学生对人工智能挑战的理解有所提高，具体表现在：a) 对人工智能环境影响的认识增强；b) 针对人工智能公平性问题的纠正解决方案效率提高。此外，学生对人工智能对生活变革性影响的看法也发生了演变。", "conclusion": "该课程成功提高了学生对人工智能挑战的理解和对人工智能社会影响的认知，证明了跨学科教学方法在培养人工智能素养方面的有效性。", "translation": "这项工作提出了一门名为“人工智能世界”的新课程，专为几乎没有接触过人工智能的一年级本科工程学生设计。本课程旨在解决工程学生在学业开始时常常缺乏人工智能基础知识及其更广泛社会影响的核心问题。我们认为解决这一差距的方法是设计并提供一门跨学科课程，该课程需满足：a) 任何领域的一年级本科工程学生均可学习；b) 使他们无需数学即可理解人工智能系统的基本工作原理；c) 使他们认识到人工智能对我们生活产生的深远影响。课程分为三个模块，由工程学和人文学科的教师共同授课。行星模块探讨了人工智能作为可持续发展催化剂和环境挑战贡献者的双重角色。社会影响模块侧重于人工智能偏见以及对隐私和公平性的担忧。最后，工作场所模块强调了人工智能驱动的就业替代，强调适应的重要性。本课程的新颖之处在于其跨学科课程设计和教学方法，将技术指导与社会讨论相结合。结果显示，学生对人工智能挑战的理解在各项指标上均有所提高，例如：a) 对人工智能环境影响的认识增强；b) 针对人工智能公平性的高效纠正解决方案。此外，结果还表明学生对人工智能对我们生活变革性影响的看法有所演变。", "summary": "该论文介绍了一门名为“人工智能世界”的创新课程，旨在提升一年级工科学生的人工智能素养。课程通过跨学科教学，涵盖人工智能的基本原理、社会影响（如环境、偏见、隐私、公平）和职业适应。研究结果表明，该课程显著提高了学生对人工智能挑战的理解和对其社会影响的认知。", "keywords": "人工智能素养, 工程教育, 跨学科课程, 社会影响, 课程设计", "comments": "这篇论文提出了一种创新且及时的方法来解决工程教育中的一个关键差距：缺乏人工智能素养。其跨学科方法，将技术知识与社会影响讨论相结合，是其主要创新点。该课程的重要性在于它为未来工程师提供了全面理解人工智能及其复杂性的基础，这对于他们在日益由AI驱动的世界中取得成功至关重要。这种方法可以作为其他 STEM 领域类似课程的典范。"}}
{"id": "2506.08053", "title": "Power Domain Sparse Dimensional Constellation Multiple Access (PD-SDCMA) for Enabled Flexible PONs", "authors": ["Yuhao Lian", "Xiao Han", "Xinmao Deng"], "summary": "With the commercial deployment of 5G and the in-depth research of 6G, the\ndemand for high-speed data services in the next-generation fiber optic access\nsystems is growing increasingly. Passive optical networks (PONs) have become a\nresearch hotspot due to their characteristics of low loss, high bandwidth, and\nlow cost. However, the traditional orthogonal multiple access (OMA-PON) has\ndifficulty meeting the requirements of the next-generation PON for high\nspectral efficiency and flexibility. In this paper, a novel transmission\ntechnology, namely power-domain sparse dimension constellation multiple access\n(PD-SDCMA), is proposed for the first time. Through the signal space dimension\nselection strategy (S2D-strategy) in the high-dimensional signal space, the\nlow-dimensional constellation is sparsely superimposed into the\nhigh-dimensional space, thereby reducing multi-user interference and enhancing\nthe system capacity. PD-SDCMA supports higher-order modulation formats and more\naccess groups, and is also compatible with the existing orthogonal frequency\ndivision multiplexing (OFDM) architecture. The simulation results show that in\na 25 km single-mode fiber system, compared with PD-NOMA and 3D-NOMA, PD-SDCMA\ncan support more users and significantly reduce BER. This technology provides\nan efficient and low-cost solution for the evolution of Flexible PONs.", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.16271 by\n  other authors", "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.08053v1", "AI": {"title_translation": "功率域稀疏维度星座多址接入 (PD-SDCMA) 实现灵活无源光网络", "tldr": "本文提出了一种新的传输技术PD-SDCMA，通过在高维信号空间中采用信号空间维度选择策略，显著提升了无源光网络（PON）的频谱效率和系统容量，并有效降低误码率，为灵活PONs的发展提供了高效解决方案。", "motivation": "随着5G的商业部署和6G的深入研究，下一代光纤接入系统对高速数据服务的需求日益增长。然而，传统的正交多址接入（OMA-PON）难以满足下一代PON在高频谱效率和灵活性方面的要求。", "method": "本文首次提出了一种名为功率域稀疏维度星座多址接入（PD-SDCMA）的新型传输技术。该技术通过在高维信号空间中采用信号空间维度选择策略（S2D-strategy），将低维星座稀疏叠加到高维空间，从而减少多用户干扰并增强系统容量。PD-SDCMA支持更高阶的调制格式和更多的接入组，并兼容现有的正交频分复用（OFDM）架构。", "result": "仿真结果表明，在25公里单模光纤系统中，与功率域非正交多址接入（PD-NOMA）和三维非正交多址接入（3D-NOMA）相比，PD-SDCMA能够支持更多用户并显著降低误码率。", "conclusion": "PD-SDCMA为灵活无源光网络（Flexible PONs）的演进提供了一种高效且低成本的解决方案。", "translation": "随着5G的商用部署和6G的深入研究，下一代光纤接入系统对高速数据服务需求日益增长。无源光网络（PON）因其低损耗、高带宽和低成本等特点成为研究热点。然而，传统的正交多址接入（OMA-PON）难以满足下一代PON对高频谱效率和灵活性的要求。本文首次提出了一种新型传输技术，即功率域稀疏维度星座多址接入（PD-SDCMA）。通过高维信号空间中的信号空间维度选择策略（S2D-strategy），将低维星座稀疏叠加到高维空间中，从而减少多用户干扰并增强系统容量。PD-SDCMA支持更高阶的调制格式和更多的接入组，并且与现有的正交频分复用（OFDM）架构兼容。仿真结果表明，在25公里单模光纤系统中，与PD-NOMA和3D-NOMA相比，PD-SDCMA能够支持更多用户并显著降低误码率。这项技术为灵活PON的演进提供了一种高效且低成本的解决方案。", "summary": "本文提出了一种名为功率域稀疏维度星座多址接入（PD-SDCMA）的新型传输技术，旨在解决传统无源光网络（PON）在满足下一代高速数据服务需求方面的挑战。PD-SDCMA通过在高维信号空间中采用信号空间维度选择策略，有效地减少了多用户干扰并提升了系统容量。仿真结果表明，该技术在支持更多用户和降低误码率方面优于现有技术，为灵活PON的发展提供了高效且低成本的解决方案。", "keywords": "功率域稀疏维度星座多址接入, PD-SDCMA, 无源光网络, PON, 灵活PON, 信号空间维度选择策略", "comments": "PD-SDCMA的创新性在于将低维星座稀疏叠加到高维空间，结合信号空间维度选择策略，有效解决了多用户干扰问题并提升了系统容量。这为下一代PONs的高速、高效率发展提供了新的思路，尤其在兼容现有OFDM架构方面显示出其潜在的实用价值和重要性。"}}
{"id": "2506.08507", "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning", "authors": ["Kuo Yang", "Xingjie Yang", "Linhui Yu", "Qing Xu", "Yan Fang", "Xu Wang", "Zhengyang Zhou", "Yang Wang"], "summary": "Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently\nemerged as a powerful paradigm for tackling complex real-world tasks. However,\nexisting Mas construction methods typically rely on manually crafted\ninteraction mechanisms or heuristic rules, introducing human biases and\nconstraining the autonomous ability. Even with recent advances in adaptive Mas\nconstruction, existing systems largely remain within the paradigm of\nsemi-autonomous patterns. In this work, we propose MasHost, a Reinforcement\nLearning (RL)-based framework for autonomous and query-adaptive Mas design. By\nformulating Mas construction as a graph search problem, our proposed MasHost\njointly samples agent roles and their interactions through a unified\nprobabilistic sampling mechanism. Beyond the accuracy and efficiency objectives\npursued in prior works, we introduce component rationality as an additional and\nnovel design principle in Mas. To achieve this multi-objective optimization, we\npropose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy\nthat collaboratively integrates group-relative advantages and action-wise\nrewards. To our knowledge, our proposed MasHost is the first RL-driven\nframework for autonomous Mas graph construction. Extensive experiments on six\nbenchmarks demonstrate that MasHost consistently outperforms most competitive\nbaselines, validating its effectiveness, efficiency, and structure rationality.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.08507v1", "AI": {"title_translation": "MasHost 构建一切：强化学习导向的自主多智能体系统", "tldr": "MasHost是一个基于强化学习的框架，用于自主设计多智能体系统，通过联合采样角色和交互来优化准确性、效率和组件理性，并在基准测试中表现优异。", "motivation": "现有的大语言模型驱动的多智能体系统构建方法依赖手动或启发式规则，引入人类偏见，限制了自主性，且多为半自主模式。", "method": "本文提出了MasHost，一个基于强化学习（RL）的框架，用于自主和查询自适应的多智能体系统设计。它将Mas构建公式化为图搜索问题，通过统一的概率采样机制联合采样智能体角色及其交互。除了准确性和效率，MasHost引入了组件理性作为新的设计原则。为实现多目标优化，提出了分层相对策略优化（HRPO），一种协同整合群体相对优势和动作奖励的RL策略。", "result": "在六个基准测试中，MasHost始终优于大多数有竞争力的基线，验证了其有效性、效率和结构理性。", "conclusion": "MasHost是首个由强化学习驱动的自主多智能体图构建框架，能够有效、高效且理性地设计多智能体系统，解决了现有方法的局限性。", "translation": "大型语言模型（LLM）驱动的多智能体系统（Mas）最近已成为解决复杂现实世界任务的强大范式。然而，现有的Mas构建方法通常依赖手动制作的交互机制或启发式规则，引入了人类偏见并限制了自主能力。即使最近在自适应Mas构建方面取得了进展，现有系统仍 largely 停留在半自主模式的范式内。在这项工作中，我们提出了MasHost，一个基于强化学习（RL）的框架，用于自主和查询自适应的Mas设计。通过将Mas构建公式化为图搜索问题，我们提出的MasHost通过统一的概率采样机制联合采样智能体角色及其交互。除了先前工作中追求的准确性和效率目标外，我们引入了组件理性作为Mas中一个额外且新颖的设计原则。为了实现这种多目标优化，我们提出了分层相对策略优化（HRPO），一种新颖的RL策略，它协同整合了群体相对优势和动作奖励。据我们所知，我们提出的MasHost是第一个由RL驱动的自主Mas图构建框架。在六个基准测试中进行的广泛实验表明，MasHost始终优于大多数有竞争力的基线，验证了其有效性、效率和结构理性。", "summary": "本文提出了MasHost，一个基于强化学习的框架，旨在解决现有LLM驱动多智能体系统构建中手动依赖和自主性受限的问题。MasHost将多智能体系统构建视为图搜索问题，通过统一概率采样机制联合设计智能体角色与交互，并首次引入组件理性作为多目标优化原则。为实现此目标，该框架提出分层相对策略优化（HRPO）策略。实验证明，MasHost在多个基准测试中表现优异，验证了其在自主性、效率和结构理性方面的优势。", "keywords": "多智能体系统, 强化学习, 自主设计, 图搜索, 组件理性", "comments": "MasHost的创新之处在于它是首个将强化学习应用于自主多智能体系统图构建的框架，并且引入了“组件理性”这一新颖的设计原则，超越了传统的准确性和效率目标。这对于提升多智能体系统的真正自主性和适应性具有重要意义，减少了对人工干预的依赖。"}}
{"id": "2506.08200", "title": "AffectMachine-Pop: A controllable expert system for real-time pop music generation", "authors": ["Kat R. Agres", "Adyasha Dash", "Phoebe Chua", "Stefan K. Ehrlich"], "summary": "Music is a powerful medium for influencing listeners' emotional states, and\nthis capacity has driven a surge of research interest in AI-based affective\nmusic generation in recent years. Many existing systems, however, are a black\nbox which are not directly controllable, thus making these systems less\nflexible and adaptive to users. We present \\textit{AffectMachine-Pop}, an\nexpert system capable of generating retro-pop music according to arousal and\nvalence values, which can either be pre-determined or based on a listener's\nreal-time emotion states. To validate the efficacy of the system, we conducted\na listening study demonstrating that AffectMachine-Pop is capable of generating\naffective music at target levels of arousal and valence. The system is tailored\nfor use either as a tool for generating interactive affective music based on\nuser input, or for incorporation into biofeedback or neurofeedback systems to\nassist users with emotion self-regulation.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08200v1", "AI": {"title_translation": "AffectMachine-Pop：一个用于实时流行音乐生成的、可控的专家系统", "tldr": "AffectMachine-Pop是一个可控的专家系统，能够根据唤醒度和效价生成复古流行音乐，并已通过听力研究验证其有效性，可用于互动音乐生成或情绪自我调节。", "motivation": "音乐在影响听众情绪状态方面具有强大作用，近年来AI情感音乐生成研究激增。然而，现有许多系统是不可直接控制的黑箱，导致其灵活性和用户适应性较差。本研究旨在开发一个可控的系统来解决这一问题。", "method": "本研究提出了AffectMachine-Pop，一个专家系统，能够根据预设或听众实时情绪状态的唤醒度（arousal）和效价（valence）值生成复古流行音乐。", "result": "通过一项听力研究，验证了AffectMachine-Pop系统能够生成符合目标唤醒度和效价水平的情感音乐。", "conclusion": "AffectMachine-Pop系统可以作为基于用户输入的交互式情感音乐生成工具，也可以整合到生物反馈或神经反馈系统中，辅助用户进行情绪自我调节。", "translation": "音乐是影响听众情绪状态的强大媒介，这种能力近年来推动了AI情感音乐生成研究的激增。然而，许多现有系统是不可直接控制的黑箱，这使得这些系统对用户来说不够灵活和适应。我们提出了AffectMachine-Pop，一个专家系统，能够根据唤醒度和效价生成复古流行音乐，这些值可以是预先确定的，也可以基于听众的实时情绪状态。为了验证系统的有效性，我们进行了一项听力研究，结果表明AffectMachine-Pop能够生成目标唤醒度和效价水平的情感音乐。该系统既可以作为根据用户输入生成交互式情感音乐的工具，也可以整合到生物反馈或神经反馈系统中，辅助用户进行情绪自我调节。", "summary": "本文介绍了一个名为AffectMachine-Pop的专家系统，旨在解决现有AI情感音乐生成系统缺乏直接控制性的问题。该系统能够根据预设或实时情绪的唤醒度和效价生成复古流行音乐。通过听力研究验证，该系统能够有效地生成符合目标情感参数的音乐。AffectMachine-Pop可应用于交互式情感音乐生成工具，或集成到生物反馈/神经反馈系统中以辅助情绪自我调节。", "keywords": "情感音乐生成, 可控专家系统, 流行音乐, 唤醒度与效价, 情绪自我调节", "comments": "该论文提出了一种可控的AI情感音乐生成系统，解决了现有系统“黑箱”和不可控的局限性，具有创新性。其能够根据情绪参数实时生成音乐，并在听力研究中得到验证，展示了其实用性和潜在应用价值，特别是在情绪调节领域的应用前景广阔。"}}
{"id": "2506.08132", "title": "Congestion-Aware Path Selection for Load Balancing in AI Clusters", "authors": ["Erfan Nosrati", "Majid Ghaderi"], "summary": "Fast training of large machine learning models requires distributed training\non AI clusters consisting of thousands of GPUs. The efficiency of distributed\ntraining crucially depends on the efficiency of the network interconnecting\nGPUs in the cluster. These networks are commonly built using RDMA following a\nClos-like datacenter topology. To efficiently utilize the network bandwidth,\nload balancing is employed to distribute traffic across multiple redundant\npaths. While there exists numerous techniques for load-balancing in traditional\ndatacenters, these are often either optimized for TCP traffic or require\nspecialized network hardware, thus limiting their utility in AI clusters.\n  This paper presents the design and evaluation of Hopper, a new load-balancing\ntechnique optimized for RDMA traffic in AI clusters. Operating entirely at the\nhost level, Hopper requires no specialized hardware or modifications to network\nswitches. It continuously monitors the current path for congestion and\ndynamically switches traffic to a less congested path when congestion is\ndetected. Furthermore, it incorporates a lightweight mechanism to identify\nalternative paths and carefully controls the timing of path switching to\nprevent excessive out-of-order packets.\n  We evaluated Hopper using ns-3 simulations and a testbed implementation. Our\nevaluations show that Hopper reduces the average and 99-percentile tail flow\ncompletion time by up to 20% and 14%, respectively, compared to\nstate-of-the-art host-based load balancing techniques.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.08132v1", "AI": {"title_translation": "AI集群中用于负载均衡的拥塞感知路径选择", "tldr": "Hopper是一种新的主机级负载均衡技术，专为AI集群中的RDMA流量优化，通过持续监控拥塞并动态切换路径，无需专用硬件，可显著减少流完成时间。", "motivation": "大型机器学习模型需要分布式训练，其效率高度依赖于网络互连效率。现有数据中心负载均衡技术通常针对TCP流量优化或需要专用硬件，不适用于AI集群中的RDMA流量。", "method": "本文提出了Hopper，一种针对AI集群中RDMA流量优化的新型主机级负载均衡技术。Hopper无需专用硬件或交换机修改，它持续监控当前路径的拥塞情况，并在检测到拥塞时动态将流量切换到较不拥塞的路径。此外，它包含一个轻量级机制来识别替代路径，并仔细控制路径切换的时机以防止过多的乱序数据包。", "result": "通过ns-3仿真和测试床实现评估，Hopper将平均流完成时间减少了高达20%，将99%尾部流完成时间减少了高达14%，优于现有最先进的主机级负载均衡技术。", "conclusion": "Hopper是一种高效的、无需专用硬件的AI集群负载均衡解决方案，能够显著提升分布式训练的网络效率。", "translation": "大型机器学习模型的快速训练需要数千个GPU组成的AI集群进行分布式训练。分布式训练的效率关键取决于集群中GPU互连网络的效率。这些网络通常使用RDMA技术，遵循类似Clos的数据中心拓扑结构。为了有效利用网络带宽，采用负载均衡技术将流量分配到多个冗余路径。虽然传统数据中心存在大量负载均衡技术，但它们通常要么针对TCP流量优化，要么需要专用网络硬件，从而限制了它们在AI集群中的实用性。\n本文提出并评估了Hopper的设计，这是一种针对AI集群中RDMA流量优化的新型负载均衡技术。Hopper完全在主机级别运行，无需专用硬件或对网络交换机进行修改。它持续监控当前路径的拥塞情况，并在检测到拥塞时动态地将流量切换到较不拥塞的路径。此外，它包含一个轻量级机制来识别替代路径，并仔细控制路径切换的时机以防止过多的乱序数据包。\n我们使用ns-3仿真和测试床实现对Hopper进行了评估。我们的评估结果表明，与现有最先进的主机级负载均衡技术相比，Hopper将平均流完成时间和99%尾部流完成时间分别减少了高达20%和14%。", "summary": "本文提出Hopper，一种专为AI集群中RDMA流量设计的主机级负载均衡技术。Hopper通过持续监控路径拥塞并动态切换流量来优化网络带宽利用率，无需专用硬件。评估结果表明，Hopper在流完成时间方面优于现有技术，可将平均流完成时间缩短20%，99%尾部流完成时间缩短14%。", "keywords": "AI集群, 负载均衡, RDMA, 拥塞控制, 路径选择", "comments": "Hopper的创新之处在于其纯主机级的实现，避免了对专用硬件或网络交换机修改的需求，使其在AI集群中具有高度的实用性。其拥塞感知和动态路径切换机制，结合对乱序数据包的控制，有效提升了RDMA流量的负载均衡性能，对于大规模分布式AI训练的效率提升至关重要。"}}
{"id": "2506.08320", "title": "How Good LLM-Generated Password Policies Are?", "authors": ["Vivek Vaidya", "Aditya Patwardhan", "Ashish Kundu"], "summary": "Generative AI technologies, particularly Large Language Models (LLMs), are\nrapidly being adopted across industry, academia, and government sectors, owing\nto their remarkable capabilities in natural language processing. However,\ndespite their strengths, the inconsistency and unpredictability of LLM outputs\npresent substantial challenges, especially in security-critical domains such as\naccess control. One critical issue that emerges prominently is the consistency\nof LLM-generated responses, which is paramount for ensuring secure and reliable\noperations.\n  In this paper, we study the application of LLMs within the context of\nCybersecurity Access Control Systems. Specifically, we investigate the\nconsistency and accuracy of LLM-generated password policies, translating\nnatural language prompts into executable pwquality.conf configuration files.\nOur experimental methodology adopts two distinct approaches: firstly, we\nutilize pre-trained LLMs to generate configuration files purely from natural\nlanguage prompts without additional guidance. Secondly, we provide these models\nwith official pwquality.conf documentation to serve as an informative baseline.\nWe systematically assess the soundness, accuracy, and consistency of these\nAI-generated configurations. Our findings underscore significant challenges in\nthe current generation of LLMs and contribute valuable insights into refining\nthe deployment of LLMs in Access Control Systems.", "comment": "11 pages, 2 Tables, 9 figures, 3 Algorithms", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08320v1", "AI": {"title_translation": "LLM 生成的密码策略有多好？", "tldr": "本文研究了大型语言模型（LLM）在网络安全访问控制系统中生成密码策略的质量，特别是其一致性和准确性。研究发现当前 LLM 在生成可执行配置文件方面存在显著挑战。", "motivation": "尽管大型语言模型（LLMs）在自然语言处理方面能力显著，但其输出的不一致性和不可预测性在安全关键领域（如访问控制）带来了重大挑战，尤其是在确保安全可靠操作所需的响应一致性方面。", "method": "研究采用了两种实验方法：首先，使用预训练的 LLM 纯粹根据自然语言提示生成配置文件，不提供额外指导；其次，向模型提供官方 pwquality.conf 文档作为信息基线。系统评估了这些 AI 生成配置的健全性、准确性和一致性。", "result": "研究结果强调了当前 LLM 在生成密码策略方面存在的显著挑战，并为优化 LLM 在访问控制系统中的部署提供了宝贵见解。", "conclusion": "本文的结论是，目前 LLM 在生成安全关键的配置方面仍面临显著挑战，需要进一步的工作来提高其一致性和准确性，以更好地部署在访问控制系统中。", "translation": "生成式人工智能技术，特别是大型语言模型（LLMs），因其卓越的自然语言处理能力，正迅速被工业界、学术界和政府部门采用。然而，尽管它们具有优势，LLM 输出的不一致性和不可预测性带来了巨大的挑战，尤其是在访问控制等安全关键领域。其中一个突出且关键的问题是 LLM 生成响应的一致性，这对于确保安全可靠的操作至关重要。\n本文研究了 LLM 在网络安全访问控制系统中的应用。具体来说，我们调查了 LLM 生成密码策略的一致性和准确性，将自然语言提示转换为可执行的 pwquality.conf 配置文件。我们的实验方法采用了两种截然不同的方法：首先，我们利用预训练的 LLM 纯粹根据自然语言提示生成配置文件，无需额外指导。其次，我们为这些模型提供了官方 pwquality.conf 文档作为信息基线。我们系统地评估了这些 AI 生成配置的健全性、准确性和一致性。我们的研究结果强调了当前 LLM 的显著挑战，并为优化 LLM 在访问控制系统中的部署提供了宝贵见解。", "summary": "本文研究了大型语言模型（LLM）在网络安全访问控制系统中生成密码策略（具体为 pwquality.conf 配置文件）的质量。考虑到 LLM 输出在安全领域的挑战，作者通过两种实验方法评估了 LLM 生成配置的健全性、准确性和一致性：一是仅基于自然语言提示，二是结合官方文档。研究发现当前 LLM 在此应用中存在显著挑战，并为 LLM 在访问控制系统中的未来部署提供了重要见解。", "keywords": "大型语言模型, 密码策略, 访问控制, 网络安全, 配置生成", "comments": "本文探讨了 LLM 在安全关键领域（如访问控制）的应用，并指出了其在生成一致且准确的配置方面的局限性。这项研究的重要性在于揭示了 LLM 在实际安全应用中的潜在风险和改进方向，对于推动 LLM 在企业级安全系统中的可靠部署具有指导意义。"}}
{"id": "2506.08153", "title": "A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems", "authors": ["Renato Cordeiro Ferreira"], "summary": "How can the complexity of ML-enabled systems be managed effectively? The goal\nof this research is to investigate how complexity affects ML-Enabled Systems\n(MLES). To address this question, this research aims to introduce a\nmetrics-based architectural model to characterize the complexity of MLES. The\ngoal is to support architectural decisions, providing a guideline for the\ninception and growth of these systems. This paper showcases the first step for\ncreating the metrics-based architectural model: an extension of a reference\narchitecture that can describe MLES to collect their metrics.", "comment": "4 pages, 3 figures (2 diagrams, 1 table), to be published in CAIN\n  2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08153v1", "AI": {"title_translation": "衡量机器学习系统复杂度的面向指标的架构模型", "tldr": "本研究旨在引入一个基于指标的架构模型来表征机器学习系统的复杂性，以支持架构决策。", "motivation": "有效管理机器学习系统（MLES）的复杂性，并调查复杂性如何影响这些系统。", "method": "引入一个面向指标的架构模型来表征机器学习系统的复杂性，具体通过扩展一个能够描述MLES并收集其指标的参考架构来实现。", "result": "提出了创建面向指标架构模型的第一步：一个能够描述机器学习系统并收集其指标的参考架构的扩展。", "conclusion": "该研究旨在支持架构决策，为机器学习系统的启动和发展提供指导。", "translation": "如何有效管理机器学习系统的复杂性？本研究的目标是调查复杂性如何影响机器学习系统 (MLES)。为了解决这个问题，本研究旨在引入一个基于指标的架构模型来表征 MLES 的复杂性。目标是支持架构决策，为这些系统的启动和发展提供指导。本文展示了创建基于指标的架构模型的第一步：一个可以描述 MLES 以收集其指标的参考架构的扩展。", "summary": "本文提出了一种面向指标的架构模型，旨在表征机器学习系统（MLES）的复杂性，以支持架构决策并指导系统的发展。研究展示了创建此模型的第一步，即扩展一个参考架构以描述MLES并收集相关指标。", "keywords": "机器学习系统, 复杂性, 架构模型, 指标, 参考架构", "comments": "这篇论文的创新点在于提出了一个面向指标的架构模型来量化和管理机器学习系统的复杂性，这对于提高MLES的可管理性和可扩展性具有重要意义。其方法从扩展现有参考架构入手，具有一定的实用性和可操作性。"}}
{"id": "2506.08045", "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence,\nintegrating perception, decision-making, memory, and collaborative planning to\noperate adaptively in complex, real-world environments. Driven by recent\nadvances in Agentic AI, these systems surpass traditional UAVs by exhibiting\ngoal-driven behavior, contextual reasoning, and interactive autonomy. We\nprovide a comprehensive foundation for understanding the architectural\ncomponents and enabling technologies that distinguish Agentic UAVs from\ntraditional autonomous UAVs. Furthermore, a detailed comparative analysis\nhighlights advancements in autonomy with AI agents, learning, and mission\nflexibility. This study explores seven high-impact application domains\nprecision agriculture, construction & mining, disaster response, environmental\nmonitoring, infrastructure inspection, logistics, security, and wildlife\nconservation, illustrating the broad societal value of agentic aerial\nintelligence. Furthermore, we identify key challenges in technical constraints,\nregulatory limitations, and data-model reliability, and we present emerging\nsolutions across hardware innovation, learning architectures, and human-AI\ninteraction. Finally, a future roadmap is proposed, outlining pathways toward\nself-evolving aerial ecosystems, system-level collaboration, and sustainable,\nequitable deployments. This survey establishes a foundational framework for the\nfuture development, deployment, and governance of agentic aerial systems\n(Agentic UAVs) across diverse societal and industrial domains.", "comment": "40 pages, 6 Figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08045v1", "AI": {"title_translation": "无人机邂逅智能体AI：自主航空智能和智能体无人机的多领域综述", "tldr": "智能体无人机结合了感知、决策、记忆和协作规划，在复杂环境中自适应运行，超越了传统无人机。本文综述了智能体无人机的架构、技术、应用领域、挑战和未来发展路线图。", "motivation": "本文旨在介绍智能体无人机，这些无人机通过整合感知、决策、记忆和协作规划，在复杂、真实环境中自适应运行，超越了传统无人机。其研究动机源于智能体AI的最新进展，旨在提供理解智能体无人机与传统自主无人机区别的架构组件和使能技术的全面基础。", "method": "本研究提供了一个理解智能体无人机架构组件和使能技术的全面基础。它进行了一项详细的比较分析，突出了AI智能体、学习和任务灵活性在自主性方面的进步。研究探索了七个高影响力应用领域，并识别了技术限制、监管限制和数据模型可靠性方面的关键挑战，同时提出了硬件创新、学习架构和人机交互方面的新兴解决方案。最后，提出了一份未来路线图。", "result": "本研究突出了AI智能体、学习和任务灵活性在自主性方面的进步。它阐明了智能体航空智能在精细农业、建筑与采矿、灾害响应、环境监测、基础设施检查、物流、安全和野生动物保护等七个高影响力应用领域的广泛社会价值。研究还识别了技术限制、监管限制和数据模型可靠性方面的关键挑战，并提出了硬件创新、学习架构和人机交互方面的新兴解决方案。", "conclusion": "本综述为智能体航空系统（智能体无人机）在不同社会和工业领域的未来开发、部署和治理建立了基础框架，并概述了通往自我演化航空生态系统、系统级协作以及可持续、公平部署的路径。", "translation": "智能体无人机代表了自主航空智能的新前沿，整合了感知、决策、记忆和协作规划，以在复杂、真实环境中自适应运行。在智能体AI最新进展的推动下，这些系统通过展现目标驱动行为、上下文推理和交互式自主性超越了传统无人机。我们为理解区分智能体无人机与传统自主无人机的架构组件和使能技术提供了全面的基础。此外，详细的比较分析突出了AI智能体、学习和任务灵活性在自主性方面的进步。本研究探讨了七个高影响力应用领域——精细农业、建筑与采矿、灾害响应、环境监测、基础设施检查、物流、安全和野生动物保护，阐明了智能体航空智能的广泛社会价值。此外，我们识别了技术限制、监管限制和数据模型可靠性方面的关键挑战，并提出了硬件创新、学习架构和人机交互方面的新兴解决方案。最后，提出了一份未来路线图，概述了通往自我演化航空生态系统、系统级协作以及可持续、公平部署的路径。本综述为智能体航空系统（智能体无人机）在不同社会和工业领域的未来开发、部署和治理建立了基础框架。", "summary": "本文对智能体无人机进行了多领域综述，这些无人机通过整合感知、决策、记忆和协作规划，在复杂环境中实现自适应操作。文章详细介绍了智能体无人机的架构组件和使能技术，并与传统无人机进行了比较。研究探讨了智能体无人机在七个关键应用领域的社会价值，识别了技术、监管和数据可靠性方面的挑战，并提出了相应的解决方案。最后，论文展望了智能体航空系统的未来发展方向，包括自我演化生态系统和系统级协作。", "keywords": "智能体无人机, 自主航空智能, 智能体AI, 多领域综述, 未来路线图", "comments": "这是一篇全面的综述性论文，不仅定义了智能体无人机及其能力，还广泛概述了它们的应用、面临的挑战和未来的发展路线图。其优势在于多领域的方法和前瞻性的视角，为该领域奠定了基础框架。"}}
{"id": "2506.08052", "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "authors": ["Yongkang Li", "Kaixin Xiong", "Xiangyu Guo", "Fang Li", "Sixu Yan", "Gangwei Xu", "Lijun Zhou", "Long Chen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "summary": "Although end-to-end autonomous driving has made remarkable progress, its\nperformance degrades significantly in rare and long-tail scenarios. Recent\napproaches attempt to address this challenge by leveraging the rich world\nknowledge of Vision-Language Models (VLMs), but these methods suffer from\nseveral limitations: (1) a significant domain gap between the pre-training data\nof VLMs and real-world driving data, (2) a dimensionality mismatch between the\ndiscrete language space and the continuous action space, and (3) imitation\nlearning tends to capture the average behavior present in the dataset, which\nmay be suboptimal even dangerous. In this paper, we propose ReCogDrive, an\nautonomous driving system that integrates VLMs with diffusion planner, which\nadopts a three-stage paradigm for training. In the first stage, we use a\nlarge-scale driving question-answering datasets to train the VLMs, mitigating\nthe domain discrepancy between generic content and real-world driving\nscenarios. In the second stage, we employ a diffusion-based planner to perform\nimitation learning, mapping representations from the latent language space to\ncontinuous driving actions. Finally, we fine-tune the diffusion planner using\nreinforcement learning with NAVSIM non-reactive simulator, enabling the model\nto generate safer, more human-like driving trajectories. We evaluate our\napproach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6\nand setting a new state-of-the-art that surpasses the previous vision-only SOTA\nby 5.6 PDMS.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08052v1", "AI": {"title_translation": "ReCogDrive：一种用于端到端自动驾驶的强化认知框架", "tldr": "ReCogDrive提出了一种结合视觉-语言模型（VLMs）和扩散规划器的自动驾驶系统，通过三阶段训练范式（VLM预训练、基于扩散的模仿学习和强化学习微调）解决了现有方法在长尾场景中的性能下降、领域差距和维度不匹配问题，并在NAVSIM基准测试中取得了最先进的成果。", "motivation": "尽管端到端自动驾驶取得了显著进展，但在罕见和长尾场景中性能显著下降。现有利用视觉-语言模型（VLMs）的方法也存在局限性：1) VLM预训练数据与真实驾驶数据之间存在显著领域差距；2) 离散语言空间与连续动作空间之间存在维度不匹配；3) 模仿学习倾向于捕捉数据集中平均行为，可能次优甚至危险。", "method": "本文提出了ReCogDrive，一个将视觉-语言模型（VLMs）与扩散规划器集成的自动驾驶系统，采用三阶段训练范式：1) 使用大规模驾驶问答数据集训练VLM，以减轻通用内容与真实驾驶场景之间的领域差异；2) 采用基于扩散的规划器进行模仿学习，将潜在语言空间的表示映射到连续驾驶动作；3) 使用NAVSIM非反应式模拟器通过强化学习对扩散规划器进行微调，使模型能够生成更安全、更类人的驾驶轨迹。", "result": "在面向规划的NAVSIM基准测试中，ReCogDrive取得了89.6的PDMS（规划驱动度量分数），创造了新的最先进水平，超越了之前仅基于视觉的最先进技术5.6 PDMS。", "conclusion": "ReCogDrive通过结合VLMs和扩散规划器，并采用三阶段训练范式，有效解决了端到端自动驾驶在长尾场景中的挑战，并显著提升了规划性能，达到了最先进水平。", "translation": "尽管端到端自动驾驶取得了显著进展，但其在罕见和长尾场景中的性能显著下降。最近的方法试图通过利用视觉-语言模型（VLMs）丰富的世界知识来解决这一挑战，但这些方法存在以下局限性：(1) VLM预训练数据与真实驾驶数据之间存在显著的领域差距，(2) 离散语言空间与连续动作空间之间存在维度不匹配，以及(3) 模仿学习倾向于捕捉数据集中存在的平均行为，这可能是次优甚至危险的。在本文中，我们提出了ReCogDrive，一个将VLMs与扩散规划器集成的自动驾驶系统，该系统采用三阶段训练范式。在第一阶段，我们使用大规模驾驶问答数据集来训练VLMs，以减轻通用内容与真实驾驶场景之间的领域差异。在第二阶段，我们采用基于扩散的规划器进行模仿学习，将潜在语言空间的表示映射到连续驾驶动作。最后，我们使用NAVSIM非反应式模拟器通过强化学习对扩散规划器进行微调，使模型能够生成更安全、更类人的驾驶轨迹。我们在面向规划的NAVSIM基准测试中评估了我们的方法，取得了89.6的PDMS，并创造了新的最先进水平，超越了之前仅基于视觉的最先进技术5.6 PDMS。", "summary": "ReCogDrive提出了一种新颖的端到端自动驾驶框架，旨在克服现有方法在处理罕见和长尾驾驶场景时的局限性。该框架将视觉-语言模型（VLMs）与扩散规划器相结合，并通过一个三阶段训练过程进行优化。首先，利用大规模驾驶问答数据集对VLM进行预训练，以缩小领域差距；其次，使用扩散规划器进行模仿学习，将语言表示映射到连续驾驶动作；最后，通过强化学习对扩散规划器进行微调，以生成更安全、更类人的驾驶轨迹。ReCogDrive在NAVSIM基准测试中表现出色，实现了89.6的PDMS，超越了现有最先进技术，证明了其在复杂驾驶场景中的有效性和优越性。", "keywords": "自动驾驶, 视觉-语言模型, 扩散模型, 强化学习, 长尾场景", "comments": "ReCogDrive的创新之处在于其将VLMs与扩散规划器相结合，并通过三阶段训练范式（包括VLM预训练、模仿学习和强化学习微调）来解决端到端自动驾驶在长尾场景中的关键挑战。这种方法有效地弥补了领域差距、解决了维度不匹配问题，并提升了轨迹的安全性与类人性。在NAVSIM基准测试中取得的显著SOTA结果表明了该方法的有效性和潜力，为未来的自动驾驶研究提供了新的方向。"}}
{"id": "2506.08651", "title": "Reed-Muller Codes for Quantum Pauli and Multiple Access Channels", "authors": ["Dina Abdelhadi", "Colin Sandon", "Emmanuel Abbe", "Ruediger Urbanke"], "summary": "Reed-Muller (RM) codes have undergone significant analytical advancements\nover the past decade, particularly for binary memoryless symmetric (BMS)\nchannels. We extend the scope of RM codes development and analysis to\nmultiple-access channels (MACs) and quantum Pauli channels, leveraging a\nunified approach. Specifically, we first derive the achievable rate region for\nRM codes on so-called Q-MACs, a class of MACs with additive correlated noise.\nThis is achieved via a generalization of the bending and boosting arguments\ndefined in arXiv:2304.02509. We then put forward a connection between the rate\nregion of these QMACs and quantum RM codes designed for Pauli noise channels.\nThis connection highlights a universality property of quantum RM codes,\ndemonstrating their rate-optimal performance across a range of channel\nparameters, rather than for a single Pauli channel.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.08651v1", "AI": {"title_translation": "用于量子泡利和多址信道的Reed-Muller码", "tldr": "本文将Reed-Muller (RM) 码的分析扩展到多址信道 (MACs) 和量子泡利信道，通过推导Q-MACs的可达速率区域并揭示量子RM码的普适性，证明其在不同信道参数下的速率最优性能。", "motivation": "Reed-Muller (RM) 码在二元无记忆对称 (BMS) 信道方面取得了显著进展，但其在多址信道 (MACs) 和量子泡利信道上的发展和分析尚不完善，因此需要将RM码的范围扩展到这些新领域。", "method": "本文采用统一的方法，首先通过推广 arXiv:2304.02509 中定义的弯曲和提升论证，推导了Q-MACs（一类具有附加相关噪声的MACs）的可达速率区域。随后，将这些Q-MACs的速率区域与为泡利噪声信道设计的量子RM码的速率区域联系起来。", "result": "研究结果是推导了Q-MACs的可达速率区域，并揭示了量子RM码的普适性，证明了它们在不同信道参数下（而不仅仅是单一泡利信道）的速率最优性能。", "conclusion": "量子Reed-Muller码具有普适性，在多种信道参数下表现出速率最优性能，这表明了其在量子通信和多址通信中的广泛适用性。", "translation": "Reed-Muller (RM) 码在过去十年中取得了显著的分析进展，特别是在二元无记忆对称 (BMS) 信道方面。我们利用统一的方法，将RM码的开发和分析范围扩展到多址信道 (MACs) 和量子泡利信道。具体来说，我们首先推导了所谓的Q-MACs（一类具有附加相关噪声的MACs）上RM码的可达速率区域。这是通过推广 arXiv:2304.02509 中定义的弯曲和提升论证来实现的。然后，我们将这些QMACs的速率区域与为泡利噪声信道设计的量子RM码的速率区域联系起来。这种联系突出了量子RM码的普适性，证明了它们在各种信道参数下（而不仅仅是单一泡利信道）的速率最优性能。", "summary": "本文将Reed-Muller (RM) 码的研究扩展到多址信道 (MACs) 和量子泡利信道。研究人员采用统一方法，首先推导了Q-MACs的可达速率区域，该区域利用了弯曲和提升论证的推广。进一步，研究建立了Q-MACs速率区域与量子RM码在泡利噪声信道之间的新联系，揭示了量子RM码的普适性，证明其在不同信道参数下均能达到最优速率性能。", "keywords": "Reed-Muller码, 量子泡利信道, 多址信道, 可达速率, 普适性", "comments": "该研究的创新之处在于将Reed-Muller码的应用范围从传统的二元无记忆对称信道扩展到更复杂的量子泡利信道和多址信道。其通过统一方法推导可达速率区域并揭示量子RM码的普适性，为未来量子通信和多址系统中的编码设计提供了重要理论基础。特别强调的“普适性”是一个重要的亮点，表明了这些码的强大适应性。"}}
{"id": "2506.08098", "title": "Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph", "authors": ["Akash Vishwakarma", "Hojin Lee", "Mohith Suresh", "Priyam Shankar Sharma", "Rahul Vishwakarma", "Sparsh Gupta", "Yuvraj Anupam Chauhan"], "summary": "The emergence of capable large language model (LLM) based agents necessitates\nmemory architectures that transcend mere data storage, enabling continuous\nlearning, nuanced reasoning, and dynamic adaptation. Current memory systems\noften grapple with fundamental limitations in structural flexibility, temporal\nawareness, and the ability to synthesize higher-level insights from raw\ninteraction data. This paper introduces Cognitive Weave, a novel memory\nframework centered around a multi-layered spatio-temporal resonance graph\n(STRG). This graph manages information as semantically rich insight particles\n(IPs), which are dynamically enriched with resonance keys, signifiers, and\nsituational imprints via a dedicated semantic oracle interface (SOI). These IPs\nare interconnected through typed relational strands, forming an evolving\nknowledge tapestry. A key component of Cognitive Weave is the cognitive\nrefinement process, an autonomous mechanism that includes the synthesis of\ninsight aggregates (IAs) condensed, higher-level knowledge structures derived\nfrom identified clusters of related IPs. We present comprehensive experimental\nresults demonstrating Cognitive Weave's marked enhancement over existing\napproaches in long-horizon planning tasks, evolving question-answering\nscenarios, and multi-session dialogue coherence. The system achieves a notable\n34% average improvement in task completion rates and a 42% reduction in mean\nquery latency when compared to state-of-the-art baselines. Furthermore, this\npaper explores the ethical considerations inherent in such advanced memory\nsystems, discusses the implications for long-term memory in LLMs, and outlines\npromising future research trajectories.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08098v1", "AI": {"title_translation": "认知织网：用时空共振图谱合成抽象知识", "tldr": "本文提出了一种名为Cognitive Weave的新型记忆框架，通过多层时空共振图谱管理和合成抽象知识，显著提升了大型语言模型（LLM）代理在长期规划、问答和多会话对话中的性能。", "motivation": "现有的LLM记忆系统在结构灵活性、时间感知和从原始交互数据中合成高层洞察力方面存在局限性，无法满足LLM代理对持续学习、细致推理和动态适应的需求。", "method": "本文引入了Cognitive Weave，一个以多层时空共振图谱（STRG）为核心的新型记忆框架。该框架将信息作为语义丰富的洞察粒子（IPs）进行管理，这些粒子通过语义预言机接口（SOI）动态地富集共振键、指示符和情境印记。IPs通过类型化关系链相互连接，形成一个不断演进的知识织网。Cognitive Weave的一个关键组件是认知精炼过程，这是一个自主机制，包括从已识别的相关IPs集群中合成洞察聚合体（IAs），即凝练的高层知识结构。", "result": "实验结果表明，Cognitive Weave在长期规划任务、演进的问答场景和多会话对话连贯性方面显著优于现有方法。系统在任务完成率上平均提升了34%，平均查询延迟降低了42%。", "conclusion": "Cognitive Weave提供了一种先进的记忆架构，克服了现有LLM记忆系统的局限性，并通过其新颖的图谱结构和认知精炼过程，显著提高了LLM代理的性能，为未来LLM的长期记忆研究奠定了基础。", "translation": "大型语言模型（LLM）代理的出现，要求记忆架构超越单纯的数据存储，以实现持续学习、细致推理和动态适应。当前的记忆系统在结构灵活性、时间感知以及从原始交互数据中合成更高层次洞察力方面往往面临根本性限制。本文介绍了Cognitive Weave，一个以多层时空共振图谱（STRG）为核心的新型记忆框架。该图谱将信息作为语义丰富的洞察粒子（IPs）进行管理，这些粒子通过专门的语义预言机接口（SOI）动态地富集共振键、指示符和情境印记。这些IPs通过类型化的关系链相互连接，形成一个不断演进的知识织网。Cognitive Weave的一个关键组成部分是认知精炼过程，这是一个自主机制，包括从已识别的相关IPs集群中合成洞察聚合体（IAs），即凝练的更高层次知识结构。我们提供了全面的实验结果，证明Cognitive Weave在长期规划任务、演进的问答场景和多会话对话连贯性方面显著优于现有方法。与最先进的基线相比，该系统在任务完成率上平均提升了34%，平均查询延迟降低了42%。此外，本文探讨了此类高级记忆系统固有的伦理考量，讨论了对LLM长期记忆的影响，并概述了有前景的未来研究方向。", "summary": "本文提出了Cognitive Weave，一种基于多层时空共振图谱（STRG）的新型记忆框架，旨在解决现有大型语言模型（LLM）记忆系统在结构灵活性、时间感知和高层知识合成方面的不足。Cognitive Weave通过将信息组织为动态丰富的洞察粒子（IPs）并进行认知精炼，能够自主合成高层知识结构。实验证明，该系统在长期规划、问答和多会话对话中表现出色，任务完成率平均提升34%，查询延迟降低42%，为LLM的长期记忆和智能代理的持续学习提供了有效方案。", "keywords": "认知织网, 时空共振图谱, 大型语言模型, 记忆系统, 知识合成", "comments": "Cognitive Weave的创新之处在于其独特的多层时空共振图谱设计和自主认知精炼机制，这使其能够从原始数据中提取并合成高层次的抽象知识，超越了传统记忆系统的限制。这对于需要持续学习和复杂推理的LLM代理至关重要，为构建更智能、更适应性强的AI系统提供了新的范式。其显著的性能提升也证明了该框架的有效性。"}}
{"id": "2506.08019", "title": "Gridding Forced Displacement using Semi-Supervised Learning", "authors": ["Andrew Wells", "Geraldine Henningsen", "Brice Bolane Tchinde Kengne"], "summary": "We present a semi-supervised approach that disaggregates refugee statistics\nfrom administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan\nAfrican countries. By integrating UNHCR's ProGres registration data with\nsatellite-derived building footprints from Google Open Buildings and location\ncoordinates from OpenStreetMap Populated Places, our label spreading algorithm\ncreates spatially explicit refugee statistics at high granularity.This\nmethodology achieves 92.9% average accuracy in placing over 10 million refugee\nobservations into appropriate grid cells, enabling the identification of\nlocalized displacement patterns previously obscured in broader regional and\nnational statistics. The resulting high-resolution dataset provides a\nfoundation for a deeper understanding of displacement drivers.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08019v1", "AI": {"title_translation": "使用半监督学习对强制流离失所进行网格化", "tldr": "该研究提出了一种半监督方法，将难民统计数据从行政边界分解到撒哈拉以南非洲25个国家的0.5度网格单元，实现了高精度的数据定位，揭示了以前被掩盖的局部流离失所模式。", "motivation": "传统难民统计数据通常基于行政边界，导致局部流离失所模式被掩盖。本研究旨在通过高粒度、空间明确的难民统计数据，克服这一限制，从而更深入地理解流离失所的驱动因素。", "method": "该研究提出了一种半监督方法。它结合了联合国难民署（UNHCR）的ProGres注册数据、Google Open Buildings的卫星衍生建筑足迹以及OpenStreetMap人口密集地点的坐标。利用标签传播算法，将难民统计数据从行政边界分解到0.5度网格单元。", "result": "该方法在将超过1000万难民观测数据放置到适当的网格单元中时，实现了92.9%的平均准确率。这使得能够识别以前在更广泛的区域和国家统计数据中被掩盖的局部流离失所模式。", "conclusion": "所生成的高分辨率数据集为更深入地理解流离失所的驱动因素提供了基础，揭示了以前未知的局部流离失所模式。", "translation": "我们提出了一种半监督方法，将来自行政边界的难民统计数据分解到撒哈拉以南非洲25个国家的0.5度网格单元。通过将联合国难民署的ProGres注册数据与来自Google Open Buildings的卫星衍生建筑足迹以及来自OpenStreetMap人口密集地点的地理坐标相结合，我们的标签传播算法以高粒度创建了空间明确的难民统计数据。该方法在将超过1000万难民观测数据放置到适当的网格单元中时，实现了92.9%的平均准确率，从而能够识别以前在更广泛的区域和国家统计数据中被掩盖的局部流离失所模式。由此产生的高分辨率数据集为更深入地理解流离失所的驱动因素提供了基础。", "summary": "该论文介绍了一种半监督学习方法，用于将撒哈拉以南非洲25个国家的难民统计数据从行政边界细化到0.5度网格单元。通过整合联合国难民署注册数据、卫星建筑足迹和地理坐标，该方法利用标签传播算法生成了高粒度的空间明确难民数据。该方法在难民定位方面达到了92.9%的准确率，成功揭示了以前被掩盖的局部流离失所模式，并为理解流离失所驱动因素提供了高分辨率数据集。", "keywords": "半监督学习, 强制流离失所, 难民统计, 地理空间数据, 网格化", "comments": "这项研究的创新之处在于利用半监督学习和多源地理空间数据（UNHCR数据、Google Open Buildings、OpenStreetMap）来解决难民统计数据粒度不足的问题。其重要性在于能够揭示更精细的局部流离失所模式，这对于人道主义援助和政策制定具有重要意义。所创建的高分辨率数据集为后续研究流离失所驱动因素提供了宝贵的基础。"}}
{"id": "2506.08047", "title": "Evaluation of Machine Learning Models in Student Academic Performance Prediction", "authors": ["A. G. R. Sandeepa", "Sanka Mohottala"], "summary": "This research investigates the use of machine learning methods to forecast\nstudents' academic performance in a school setting. Students' data with\nbehavioral, academic, and demographic details were used in implementations with\nstandard classical machine learning models including multi-layer perceptron\nclassifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across\nall implementations. Under 10-fold cross validation, MLPC obtained 79.58%\naverage accuracy for test set while for train set, it was 99.65%. MLP's better\nperformance over other machine learning models strongly suggest the potential\nuse of neural networks as data-efficient models. Feature selection approach\nplayed a crucial role in improving the performance and multiple evaluation\napproaches were used in order to compare with existing literature. Explainable\nmachine learning methods were utilized to demystify the black box models and to\nvalidate the feature selection approach.", "comment": "Paper Accepted for IEEE ICARC Conference (2025). 6 pages, 5 figures", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.08047v1", "AI": {"title_translation": "学生学业表现预测中机器学习模型的评估", "tldr": "本研究评估了机器学习模型，特别是多层感知器分类器（MLPC），在预测学生学业表现方面的效用。MLPC在测试集上取得了86.46%的最高准确率，并强调了神经网络作为数据高效模型的潜力。", "motivation": "本研究旨在探讨利用机器学习方法预测学校环境中学生学业表现的可能性和效果。", "method": "研究使用了包含学生行为、学业和人口统计细节的数据，并应用了包括多层感知器分类器（MLPC）在内的标准经典机器学习模型。采用了10折交叉验证、特征选择方法，并利用可解释机器学习方法来验证特征选择和解释模型。", "result": "多层感知器分类器（MLPC）在所有实现中，测试集获得了86.46%的最高准确率。在10折交叉验证下，MLPC在测试集上获得了79.58%的平均准确率，在训练集上为99.65%。MLP优于其他机器学习模型，表明神经网络作为数据高效模型的潜在用途。", "conclusion": "多层感知器（MLP）在学生学业表现预测中表现出色，其优于其他机器学习模型的性能强烈表明神经网络具有作为数据高效模型的潜力。", "translation": "这项研究调查了在学校环境中利用机器学习方法预测学生学业表现。在实施中使用了包含学生行为、学业和人口统计细节的数据，并结合了标准经典机器学习模型，包括多层感知器分类器（MLPC）。MLPC在所有实现中，测试集获得了86.46%的最高准确率。在10折交叉验证下，MLPC在测试集上获得了79.58%的平均准确率，而在训练集上则达到了99.65%。MLP优于其他机器学习模型的表现强烈表明神经网络作为数据高效模型的潜在用途。特征选择方法在提高性能方面发挥了关键作用，并且采用了多种评估方法以与现有文献进行比较。可解释机器学习方法被用于揭示黑箱模型并验证特征选择方法。", "summary": "本研究评估了机器学习模型在预测学生学业表现方面的应用。利用包含行为、学业和人口统计信息的学生数据，研究人员使用了多层感知器分类器（MLPC）等经典模型。MLPC在测试集上取得了86.46%的最高准确率，并在10折交叉验证下获得了79.58%的平均准确率。研究强调了特征选择的重要性，并利用可解释机器学习方法验证了模型。结果表明神经网络在学生学业预测中作为数据高效模型的潜力。", "keywords": "机器学习, 学生学业预测, 多层感知器, 特征选择, 可解释机器学习", "comments": "该研究通过使用可解释机器学习方法来揭示黑箱模型并验证特征选择方法，增加了模型的透明度，这一点值得称赞。尽管MLPC在测试集上表现出良好的准确性，但训练集和测试集之间（99.65% vs 79.58%）的显著准确率差异可能暗示存在一定程度的过拟合，这可能是未来研究需要解决的局限性。该研究强调了神经网络作为数据高效模型的潜力，并为教育领域应用ML提供了参考。"}}
{"id": "2506.08702", "title": "Educators' Perceptions of Large Language Models as Tutors: Comparing Human and AI Tutors in a Blind Text-only Setting", "authors": ["Sankalan Pal Chowdhury", "Terry Jingchen Zhang", "Donya Rooein", "Dirk Hovy", "Tanja Käser", "Mrinmaya Sachan"], "summary": "The rapid development of Large Language Models (LLMs) opens up the\npossibility of using them as personal tutors. This has led to the development\nof several intelligent tutoring systems and learning assistants that use LLMs\nas back-ends with various degrees of engineering. In this study, we seek to\ncompare human tutors with LLM tutors in terms of engagement, empathy,\nscaffolding, and conciseness. We ask human tutors to annotate and compare the\nperformance of an LLM tutor with that of a human tutor in teaching grade-school\nmath word problems on these qualities. We find that annotators with teaching\nexperience perceive LLMs as showing higher performance than human tutors in all\n4 metrics. The biggest advantage is in empathy, where 80% of our annotators\nprefer the LLM tutor more often than the human tutors. Our study paints a\npositive picture of LLMs as tutors and indicates that these models can be used\nto reduce the load on human teachers in the future.", "comment": "Accepted to BEA@ACL 2025", "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.08702v1", "AI": {"title_translation": "教育工作者对大型语言模型作为导师的看法：在盲文文本环境中比较人类和人工智能导师", "tldr": "本研究比较了大型语言模型（LLM）导师与人类导师在教学中的表现，发现有教学经验的教育工作者认为LLM在参与度、同理心、支架式教学和简洁性方面表现更优，尤其在同理心方面优势显著。", "motivation": "大型语言模型的快速发展使其有望成为个人导师，并催生了多个基于LLM的智能辅导系统。本研究旨在比较LLM导师与人类导师在关键教学质量方面的表现。", "method": "研究要求人类导师在盲文文本环境中，评估并比较大型语言模型导师和人类导师在小学数学应用题教学中，在参与度、同理心、支架式教学和简洁性这四个方面的表现。", "result": "有教学经验的注释者认为，大型语言模型在所有四项指标上的表现均优于人类导师。其中，在同理心方面优势最大，80%的注释者更倾向于大型语言模型导师。", "conclusion": "研究结果描绘了大型语言模型作为导师的积极前景，表明这些模型未来有望减轻人类教师的负担。", "translation": "大型语言模型（LLM）的快速发展开启了将其用作私人导师的可能性。这促使了多个以LLM为后端、工程化程度各异的智能辅导系统和学习助手的开发。在本研究中，我们旨在比较人类导师与LLM导师在参与度、同理心、支架式教学和简洁性方面的表现。我们要求人类导师在盲文文本环境中，注释并比较LLM导师与人类导师在小学数学应用题教学中这些质量的表现。我们发现，有教学经验的注释者认为LLM在所有四项指标上的表现均高于人类导师。最大的优势在于同理心，80%的注释者更常选择LLM导师而非人类导师。我们的研究描绘了LLM作为导师的积极前景，并表明这些模型未来可用于减轻人类教师的负担。", "summary": "本研究旨在比较大型语言模型（LLM）导师与人类导师在教学质量方面的表现，包括参与度、同理心、支架式教学和简洁性。通过让有教学经验的教育工作者在盲文文本环境中评估小学数学应用题教学，结果显示LLM导师在所有四个方面均被认为表现更优，尤其在同理心方面优势显著（80%的注释者更偏好LLM）。这表明LLM作为导师具有积极潜力，未来有望减轻人类教师的负担。", "keywords": "大型语言模型, 人工智能导师, 教育工作者感知, 教学辅助, 同理心", "comments": "这项研究为大型语言模型在教育领域的应用提供了积极的证据，尤其是在情感交互（如同理心）方面的表现超出了预期。其创新之处在于采用盲文文本设置进行比较，减少了人类偏见。然而，研究仅限于文本环境和特定学科（小学数学应用题），未来需要更广泛的研究来验证其在不同学科、年龄段和多模态环境中的有效性与局限性。"}}
{"id": "2506.08038", "title": "Joint Routing and Control Optimization in VANET", "authors": ["Chen Huang", "Dingxuan Wang", "Ronghui Hou"], "summary": "In this paper, we introduce DynaRoute, an adaptive joint optimization\nframework for dynamic vehicular networks that simultaneously addresses platoon\ncontrol and data transmission through trajectory-aware routing and\nsafety-constrained vehicle coordination. DynaRoute guarantees continuous\nvehicle movement via platoon safety control with optimizing transmission paths\nthrough real-time trajectory prediction and ensuring reliable data. Our\nsolution achieves three key objectives: (1) maintaining platoon stability\nthrough accurate data transmission, (2) enabling adaptive routing based on\nvehicle movement patterns, and (3) enhancing overall intelligent transportation\nsystem performance. DynaRoute equires predefined traffic models and adapts to\ndynamic network conditions using local vehicle state information. We present\ncomprehensive simulation results demonstrating that DynaRoute maintains control\nand transmission performance in multiple complex scenarios while significantly\nimproving throughput and reliability compared to traditional approaches.", "comment": "11 pages; 10 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08038v1", "AI": {"title_translation": "VANET中的联合路由与控制优化", "tldr": "本文提出了DynaRoute，一个用于动态车辆网络的自适应联合优化框架，通过轨迹感知路由和安全约束车辆协调，同时解决了车队控制和数据传输问题，并显著提升了吞吐量和可靠性。", "motivation": "在动态车辆网络（VANET）中，同时解决车队控制和数据传输的挑战，以提高智能交通系统性能。", "method": "本文提出了DynaRoute，一个自适应联合优化框架，通过轨迹感知路由和安全约束车辆协调，同时处理车队控制和数据传输。它通过实时轨迹预测优化传输路径，并通过车队安全控制保证车辆连续移动和可靠数据传输。", "result": "全面的模拟结果表明，DynaRoute在多个复杂场景下保持了控制和传输性能，与传统方法相比，显著提高了吞吐量和可靠性。", "conclusion": "DynaRoute成功地在动态车辆网络中实现了车队控制和数据传输的联合优化，有效提升了VANET的性能和可靠性。", "translation": "本文介绍了DynaRoute，一个用于动态车辆网络的自适应联合优化框架，它通过轨迹感知路由和安全约束协调，同时解决车队控制和数据传输问题。DynaRoute通过车队安全控制保证车辆的连续移动，并通过实时轨迹预测优化传输路径，确保数据传输的可靠性。我们的解决方案实现了三个关键目标：(1) 通过精确数据传输维持车队稳定性，(2) 基于车辆移动模式实现自适应路由，以及 (3) 提升整体智能交通系统性能。DynaRoute需要预定义的交通模型，并利用本地车辆状态信息适应动态网络条件。我们提供了全面的仿真结果，证明DynaRoute在多个复杂场景中保持了控制和传输性能，同时与传统方法相比显著提高了吞吐量和可靠性。", "summary": "本文提出了DynaRoute，一个针对动态车辆网络的自适应联合优化框架，旨在同时解决车队控制和数据传输问题。该框架通过轨迹感知路由和安全约束车辆协调，确保车队安全控制下的连续移动和可靠数据传输。DynaRoute通过实时轨迹预测优化传输路径，并能够适应动态网络条件。仿真结果表明，DynaRoute在复杂场景中显著提升了VANET的吞吐量和可靠性，并维持了控制和传输性能。", "keywords": "VANET, 联合优化, 路由, 车队控制, 轨迹感知", "comments": "本文的创新点在于提出了一个联合优化框架DynaRoute，同时考虑了VANET中的车队控制和数据路由，这在现有研究中通常是分开处理的。通过轨迹感知和安全约束，DynaRoute能够更有效地适应动态车辆环境，对于提升智能交通系统的整体性能具有重要意义。"}}
{"id": "2506.08294", "title": "Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling", "authors": ["Ruanqianqian Huang", "Ayana Monroe", "Peli de Halleux", "Sorin Lerner", "Nikolaj Bjørner"], "summary": "Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from\nbudgeting for grocery shopping to verifying software behavior. Logic modeling\nhelps solve CSPs programmatically using SMT solvers. Despite its importance in\nmany Computer Science disciplines, resources for teaching and learning logic\nmodeling are scarce and scattered, and challenges remain in designing\neducational environments for logic modeling that are accessible and meet the\nneeds of teachers and students. This paper explores how to design such an\nenvironment and probes the impact of the design on the learning experience.\nFrom a need-finding interview study and a design iteration with teachers of\nlogic modeling, we curated 10 design guidelines spanning three main\nrequirements: providing easy access, supporting various educational modalities,\nand allowing extensions for customized pedagogical needs. We implemented nine\nguidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a\nlogic modeling learning workshop with more than 100 students, we gathered\npositive feedback on its support for learning and identified opportunities for\nfuture improvements.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08294v1", "AI": {"title_translation": "Z3Guide: 一个可扩展、以学生为中心且可扩展的逻辑建模教育环境", "tldr": "Z3Guide是一个可扩展、以学生为中心且可扩展的逻辑建模教育环境，旨在解决现有教学资源稀缺和分散的问题。通过访谈和设计迭代，研究者制定了10项设计准则，并实现了Z3Guide。在超过100名学生的学习研讨会中，Z3Guide获得了积极反馈。", "motivation": "尽管逻辑建模在计算机科学中很重要，但其教学资源稀缺且分散，设计易于访问并满足师生需求的教育环境仍面临挑战。", "method": "研究者通过需求访谈研究和与逻辑建模教师的设计迭代，整理出10项设计准则。他们将其中9项准则实现在开源的浏览器工具Z3Guide中，并在一个有100多名学生的逻辑建模学习研讨会中使用了该工具。", "result": "Z3Guide在学习支持方面获得了积极反馈，并发现了未来改进的机会。", "conclusion": "本文设计并实现了Z3Guide，一个基于需求分析的逻辑建模教育环境，并通过实践证明了其对学生学习的积极支持，为未来的改进提供了方向。", "translation": "约束满足问题（CSPs）无处不在，从杂货购物预算到验证软件行为。逻辑建模有助于使用SMT求解器以编程方式解决CSPs。尽管逻辑建模在许多计算机科学学科中很重要，但其教学和学习资源稀缺且分散，并且在设计易于访问并满足教师和学生需求的逻辑建模教育环境方面仍然存在挑战。本文探讨了如何设计这样的环境，并探究了设计对学习体验的影响。通过一项需求发现访谈研究和与逻辑建模教师的设计迭代，我们整理了10项设计准则，涵盖了三个主要要求：提供便捷访问、支持各种教育模式以及允许根据定制教学需求进行扩展。我们在Z3Guide（一个开源的浏览器工具）中实现了其中九项准则。在与100多名学生进行的逻辑建模学习研讨会中使用Z3Guide，我们收集了关于其学习支持的积极反馈，并确定了未来改进的机会。", "summary": "本研究旨在解决逻辑建模教育资源稀缺和环境设计挑战的问题。通过访谈和迭代设计，研究者提出了10项设计准则，并基于此开发了开源的浏览器工具Z3Guide。该工具在百余名学生的学习研讨会中得到了积极反馈，证明其在支持学生学习方面的有效性，并为未来改进提供了方向。", "keywords": "逻辑建模, 教育环境, Z3Guide, 约束满足问题, 学生中心", "comments": "Z3Guide创新性地将学生需求置于设计中心，通过系统化的设计准则和开源工具实现，有效解决了逻辑建模教学中的痛点。其重要性在于提供了一个可扩展且易于访问的解决方案，有望显著提升逻辑建模的学习体验。该研究的局限性可能在于其评估主要基于学生反馈，未来可进行更严格的学习效果量化评估。"}}
{"id": "2506.08386", "title": "5G Aero: A Prototyping Platform for Evaluating Aerial 5G Communications", "authors": ["Matteo Bordin", "Madhukara S. Holla", "Sakthivel Velumani", "Salvatore D'Oro", "Tommaso Melodia"], "summary": "The application of small-factor, 5G-enabled Unmanned Aerial Vehicles (UAVs)\nhas recently gained significant interest in various aerial and Industry 4.0\napplications. However, ensuring reliable, high-throughput, and low-latency 5G\ncommunication in aerial applications remains a critical and underexplored\nproblem. This paper presents the 5th generation (5G) Aero, a compact UAV\noptimized for 5G connectivity, aimed at fulfilling stringent 3rd Generation\nPartnership Project (3GPP) requirements. We conduct a set of experiments in an\nindoor environment, evaluating the UAV's ability to establish high-throughput,\nlow-latency communications in both Line-of-Sight (LoS) and Non-Line-of-Sight\n(NLoS) conditions. Our findings demonstrate that the 5G Aero meets the required\n3GPP standards for Command and Control (C2) packets latency in both LoS and\nNLoS, and video latency in LoS communications and it maintains acceptable\nlatency levels for video transmission in NLoS conditions. Additionally, we show\nthat the 5G module installed on the UAV introduces a negligible 1% decrease in\nflight time, showing that 5G technologies can be integrated into commercial\noff-the-shelf UAVs with minimal impact on battery lifetime. This paper\ncontributes to the literature by demonstrating the practical capabilities of\ncurrent 5G networks to support advanced UAV operations in telecommunications,\noffering insights into potential enhancements and optimizations for UAV\nperformance in 5G networks", "comment": "IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications (PIMRC) 2025 - Track 3: Mobile and Wireless Networks", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.08386v1", "AI": {"title_translation": "5G Aero: 一个评估空中5G通信的原型平台", "tldr": "5G Aero是一个为5G连接优化的紧凑型无人机，旨在评估其在视距和非视距条件下的高吞吐量、低延迟通信能力。实验证明5G Aero符合3GPP标准，且5G模块对飞行时间影响可忽略不计。", "motivation": "在空中应用中，确保可靠、高吞吐量、低延迟的5G通信仍然是一个关键且未被充分探索的问题，这促使了对5G无人机通信能力的研究。", "method": "本文提出了5G Aero，一个为5G连接优化的紧凑型无人机。研究人员在室内环境中进行了一系列实验，评估了该无人机在视距（LoS）和非视距（NLoS）条件下建立高吞吐量、低延迟通信的能力。", "result": "研究结果表明，5G Aero在LoS和NLoS条件下均符合3GPP对命令与控制（C2）数据包延迟的要求，并在LoS条件下符合视频延迟要求，在NLoS条件下视频传输保持可接受的延迟水平。此外，无人机上安装的5G模块对飞行时间的影响可忽略不计（仅减少1%）。", "conclusion": "本文通过实验证明了当前5G网络支持先进无人机操作的实际能力，并为5G网络中无人机性能的潜在增强和优化提供了见解。", "translation": "小型、支持5G的无人机（UAV）最近在各种空中和工业4.0应用中引起了极大的兴趣。然而，确保空中应用中可靠、高吞吐量和低延迟的5G通信仍然是一个关键且未被充分探索的问题。本文介绍了第五代（5G）Aero，一种为5G连接优化的紧凑型无人机，旨在满足严格的第三代合作伙伴计划（3GPP）要求。我们在室内环境中进行了一系列实验，评估了无人机在视距（LoS）和非视距（NLoS）条件下建立高吞吐量、低延迟通信的能力。我们的研究结果表明，5G Aero在LoS和NLoS条件下均符合命令与控制（C2）数据包延迟所需的3GPP标准，并在LoS通信中符合视频延迟标准，同时在NLoS条件下视频传输保持可接受的延迟水平。此外，我们还发现安装在无人机上的5G模块对飞行时间的影响微乎其微，仅减少1%，这表明5G技术可以集成到商用现成无人机中，对电池寿命的影响最小。本文通过展示当前5G网络支持先进无人机电信操作的实际能力，为文献做出了贡献，并为5G网络中无人机性能的潜在增强和优化提供了见解。", "summary": "本文介绍了5G Aero，一个为5G连接优化的紧凑型无人机原型平台，旨在解决空中5G通信中可靠性、高吞吐量和低延迟的挑战。通过室内实验，验证了5G Aero在视距和非视距条件下满足3GPP对命令与控制以及视频传输的延迟要求，并且5G模块对无人机飞行时间影响甚微。这表明了当前5G网络支持先进无人机操作的潜力。", "keywords": "5G Aero, 无人机, 5G通信, 原型平台, 延迟", "comments": "该论文通过构建并测试一个实际的5G无人机原型，填补了空中5G通信领域实践验证的空白。其创新之处在于展示了5G技术与商用无人机的有效集成，并量化了关键性能指标，如延迟和对飞行时间的影响。这对于推动无人机在工业4.0和电信领域的应用具有重要意义，尤其是在强调低延迟和高可靠性的场景下。该研究的局限性可能在于其室内实验环境，未来的工作可以扩展到更复杂的室外环境验证。"}}
{"id": "2506.08461", "title": "ABC-FHE : A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption", "authors": ["Sungwoong Yune", "Hyojeong Lee", "Adiwena Putra", "Hyunjun Cho", "Cuong Duong Manh", "Jaeho Jeon", "Joo-Young Kim"], "summary": "As the demand for privacy-preserving computation continues to grow, fully\nhomomorphic encryption (FHE)-which enables continuous computation on encrypted\ndata-has become a critical solution. However, its adoption is hindered by\nsignificant computational overhead, requiring 10000-fold more computation\ncompared to plaintext processing. Recent advancements in FHE accelerators have\nsuccessfully improved server-side performance, but client-side computations\nremain a bottleneck, particularly under bootstrappable parameter\nconfigurations, which involve combinations of encoding, encrypt, decoding, and\ndecrypt for large-sized parameters. To address this challenge, we propose\nABC-FHE, an area- and power-efficient FHE accelerator that supports\nbootstrappable parameters on the client side. ABC-FHE employs a streaming\narchitecture to maximize performance density, minimize area usage, and reduce\noff-chip memory access. Key innovations include a reconfigurable Fourier engine\ncapable of switching between NTT and FFT modes. Additionally, an on-chip\npseudo-random number generator and a unified on-the-fly twiddle factor\ngenerator significantly reduce memory demands, while optimized task scheduling\nenhances the CKKS client-side processing, achieving reduced latency. Overall,\nABC-FHE occupies a die area of 28.638 mm2 and consumes 5.654 W of power in 28\nnm technology. It delivers significant performance improvements, achieving a\n1112x speed-up in encoding and encryption execution time compared to a CPU, and\n214x over the state-of-the-art client-side accelerator. For decoding and\ndecryption, it achieves a 963x speed-up over the CPU and 82x over the\nstate-of-the-art accelerator.", "comment": "7 pages, 6 figures, DAC 2025: 62st IEEE/ACM Design Automation\n  Conference. (DAC'25)", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.08461v1", "AI": {"title_translation": "ABC-FHE：一种资源高效的加速器，为客户端全同态加密实现可引导参数", "tldr": "ABC-FHE是一个资源高效的加速器，显著提升了客户端全同态加密（FHE）中可引导参数配置下的编码、加密、解码和解密速度，解决了现有FHE计算开销大的问题。", "motivation": "随着隐私保护计算需求的增长，全同态加密（FHE）成为关键解决方案，但其面临巨大的计算开销（比明文处理高10000倍）。尽管服务器端FHE性能有所改善，但客户端计算，特别是在涉及大尺寸参数的编码、加密、解码和解密的可引导参数配置下，仍然是主要的性能瓶颈。", "method": "本文提出了ABC-FHE，一种面积和功耗高效的FHE加速器，专门支持客户端可引导参数。该加速器采用流式架构以最大化性能密度、最小化面积使用并减少片外内存访问。其关键创新包括：一个可在NTT和FFT模式间切换的可重构傅里叶引擎；一个片上伪随机数生成器和统一的即时旋转因子生成器，显著减少了内存需求；以及优化的任务调度，增强了CKKS客户端处理并降低了延迟。", "result": "ABC-FHE在28nm技术下占用28.638 mm2的芯片面积，功耗5.654 W。在编码和加密执行时间方面，它比CPU快1112倍，比最先进的客户端加速器快214倍。在解码和解密方面，它比CPU快963倍，比最先进的加速器快82倍。", "conclusion": "ABC-FHE成功地提供了一个资源高效的解决方案，显著提升了客户端全同态加密在可引导参数配置下的计算性能，有效解决了现有FHE应用的计算开销瓶颈。", "translation": "随着隐私保护计算需求的持续增长，全同态加密（FHE）——一种能够对加密数据进行连续计算的技术——已成为一个关键解决方案。然而，其应用受到显著计算开销的阻碍，与明文处理相比，需要多出10000倍的计算量。FHE加速器在服务器端性能方面取得了成功进展，但客户端计算仍然是瓶颈，特别是在可引导参数配置下，这涉及大尺寸参数的编码、加密、解码和解密组合。为应对这一挑战，我们提出了ABC-FHE，一种面积和功耗高效的FHE加速器，支持客户端可引导参数。ABC-FHE采用流式架构以最大化性能密度、最小化面积使用并减少片外内存访问。主要创新包括：一个可在NTT和FFT模式之间切换的可重构傅里叶引擎。此外，片上伪随机数生成器和统一的即时旋转因子生成器显著减少了内存需求，而优化的任务调度增强了CKKS客户端处理，实现了更低的延迟。总的来说，ABC-FHE在28nm技术下占用了28.638 mm2的芯片面积，功耗为5.654 W。它提供了显著的性能改进，与CPU相比，编码和加密执行时间实现了1112倍的加速，比最先进的客户端加速器快214倍。对于解码和解密，它比CPU快963倍，比最先进的加速器快82倍。", "summary": "本文提出了ABC-FHE，一种面积和功耗高效的硬件加速器，旨在解决客户端全同态加密（FHE）在可引导参数配置下的计算瓶颈。通过采用流式架构、可重构傅里叶引擎、片上伪随机数生成器和优化的任务调度，ABC-FHE显著减少了内存需求和延迟，并实现了比CPU和现有加速器在编码、加密、解码和解密方面更高的性能提升。", "keywords": "全同态加密, 硬件加速器, 客户端计算, 可引导参数, 资源高效", "comments": "ABC-FHE的创新之处在于其针对FHE客户端计算瓶颈的专门优化，特别是对可引导参数的支持。流式架构和多种硬件优化（如可重构傅里叶引擎、片上PRNG和即时旋转因子生成器）有效提升了性能密度并降低了资源消耗。这对于推动FHE在实际隐私保护应用中的普及具有重要意义。"}}
{"id": "2506.08563", "title": "KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks", "authors": ["Siyuan Yang", "Cheng Song", "Zhilu Lai", "Wenjia Wang"], "summary": "Differential equations are involved in modeling many engineering problems.\nMany efforts have been devoted to solving differential equations. Due to the\nflexibility of neural networks, Physics Informed Neural Networks (PINNs) have\nrecently been proposed to solve complex differential equations and have\ndemonstrated superior performance in many applications. While the L2 loss\nfunction is usually a default choice in PINNs, it has been shown that the\ncorresponding numerical solution is incorrect and unstable for some complex\nequations. In this work, we propose a new PINNs framework named Kernel Packet\naccelerated PINNs (KP-PINNs), which gives a new expression of the loss function\nusing the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel\nPacket (KP) method to accelerate the computation. Theoretical results show that\nKP-PINNs can be stable across various differential equations. Numerical\nexperiments illustrate that KP-PINNs can solve differential equations\neffectively and efficiently. This framework provides a promising direction for\nimproving the stability and accuracy of PINNs-based solvers in scientific\ncomputing.", "comment": "Accepted to IJCAI 2025", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.08563v1", "AI": {"title_translation": "KP-PINNs：核包加速物理信息神经网络", "tldr": "提出KP-PINNs框架，通过新的损失函数表达和核包方法，提高PINNs在求解微分方程时的稳定性、效率和准确性。", "motivation": "现有的物理信息神经网络（PINNs）框架通常使用L2损失函数，但在解决一些复杂微分方程时，其数值解可能不正确且不稳定。", "method": "本文提出了一个名为核包加速物理信息神经网络（KP-PINNs）的新框架。该框架使用再生核希尔伯特空间（RKHS）范数给出了损失函数的新表达式，并利用核包（KP）方法来加速计算。", "result": "理论结果表明KP-PINNs在各种微分方程中都能保持稳定。数值实验表明KP-PINNs可以有效且高效地求解微分方程。", "conclusion": "KP-PINNs框架为提高基于PINNs的科学计算求解器的稳定性、准确性和效率提供了一个有前景的方向。", "translation": "微分方程涉及许多工程问题的建模。\n人们为求解微分方程付出了许多努力。由于神经网络的灵活性，物理信息神经网络（PINNs）最近被提出来解决复杂的微分方程，并在许多应用中表现出卓越的性能。虽然L2损失函数通常是PINNs的默认选择，但已表明其在某些复杂方程中对应的数值解是不正确且不稳定的。在这项工作中，我们提出了一个名为核包加速物理信息神经网络（KP-PINNs）的新PINNs框架，该框架使用再生核希尔伯特空间（RKHS）范数给出了损失函数的新表达式，并使用核包（KP）方法来加速计算。理论结果表明，KP-PINNs在各种微分方程中都能稳定。数值实验表明，KP-PINNs可以有效且高效地求解微分方程。该框架为提高基于PINNs的科学计算求解器的稳定性、准确性和效率提供了一个有前景的方向。", "summary": "本文提出了KP-PINNs，一个改进的物理信息神经网络框架。它通过使用再生核希尔伯特空间范数重新定义损失函数，并利用核包方法加速计算，解决了传统PINNs在复杂微分方程中L2损失函数导致的数值解不稳定和不准确的问题。理论和实验结果均表明KP-PINNs在求解微分方程时具有更高的稳定性和效率。", "keywords": "物理信息神经网络, 核包, 微分方程, 再生核希尔伯特空间, 稳定性", "comments": "该论文的创新点在于提出了KP-PINNs框架，通过引入RKHS范数改进了PINNs的损失函数，从而解决了传统L2损失函数在复杂微分方程中表现出的不稳定性问题。同时，结合核包方法加速计算，提升了PINNs的实用性。这为科学计算中微分方程的求解提供了一个更稳定、高效且准确的工具。"}}
{"id": "2506.08183", "title": "A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation", "authors": ["Isha Puri", "David Cox"], "summary": "Research in neuroscience and vision science relies heavily on careful\nmeasurements of animal subject's gaze direction. Rodents are the most widely\nstudied animal subjects for such research because of their economic advantage\nand hardiness. Recently, video based eye trackers that use image processing\ntechniques have become a popular option for gaze tracking because they are easy\nto use and are completely noninvasive. Although significant progress has been\nmade in improving the accuracy and robustness of eye tracking algorithms,\nunfortunately, almost all of the techniques have focused on human eyes, which\ndoes not account for the unique characteristics of the rodent eye images, e.g.,\nvariability in eye parameters, abundance of surrounding hair, and their small\nsize. To overcome these unique challenges, this work presents a flexible,\nrobust, and highly accurate model for pupil and corneal reflection\nidentification in rodent gaze determination that can be incrementally trained\nto account for variability in eye parameters encountered in the field. To the\nbest of our knowledge, this is the first paper that demonstrates a highly\naccurate and practical biomedical image segmentation based convolutional neural\nnetwork architecture for pupil and corneal reflection identification in eye\nimages. This new method, in conjunction with our automated infrared videobased\neye recording system, offers the state of the art technology in eye tracking\nfor neuroscience and vision science research for rodents.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08183v1", "AI": {"title_translation": "一种基于卷积神经网络的生物医学图像分割系统，用于啮齿动物眼球运动的精确跟踪和视频记录", "tldr": "该研究开发了一种基于卷积神经网络的系统，用于精确跟踪和记录啮齿动物的眼球运动，解决了现有技术在啮齿动物眼部图像方面的局限性。", "motivation": "神经科学和视觉科学研究严重依赖对动物凝视方向的精确测量。尽管现有视频眼球追踪技术在人类眼部追踪方面取得了显著进展，但几乎所有技术都未能考虑到啮齿动物眼部图像的独特特征（如眼部参数的可变性、周围毛发丰富、尺寸小），导致准确性和鲁棒性不足。", "method": "本研究提出了一种灵活、鲁棒且高度准确的模型，用于啮齿动物凝视确定中的瞳孔和角膜反射识别。该模型基于生物医学图像分割的卷积神经网络架构，并可增量训练以适应现场遇到的眼部参数变异性。该方法与自动化红外视频眼部记录系统相结合。", "result": "该系统实现了对啮齿动物瞳孔和角膜反射识别的高度准确和实用性。它是首个展示用于眼部图像中瞳孔和角膜反射识别的高度准确和实用的生物医学图像分割卷积神经网络架构的论文。", "conclusion": "该新方法结合自动化红外视频眼部记录系统，为啮齿动物的神经科学和视觉科学研究提供了最先进的眼球追踪技术。", "translation": "神经科学和视觉科学研究严重依赖对动物受试者凝视方向的仔细测量。啮齿动物因其经济优势和顽强性，是此类研究中被研究最广泛的动物受试者。最近，使用图像处理技术的基于视频的眼球追踪器因其易用性和完全非侵入性而成为凝视追踪的流行选择。尽管在提高眼球追踪算法的准确性和鲁棒性方面取得了显著进展，但不幸的是，几乎所有的技术都集中在人眼上，这没有考虑到啮齿动物眼部图像的独特特征，例如眼部参数的可变性、周围毛发的丰富性以及它们的小尺寸。为了克服这些独特的挑战，这项工作提出了一种灵活、鲁棒且高度准确的模型，用于啮齿动物凝视确定中的瞳孔和角膜反射识别，该模型可以增量训练以适应现场遇到的眼部参数变异性。据我们所知，这是第一篇展示用于眼部图像中瞳孔和角膜反射识别的高度准确和实用的基于生物医学图像分割的卷积神经网络架构的论文。这种新方法与我们自动化的红外视频眼部记录系统相结合，为啮齿动物的神经科学和视觉科学研究提供了最先进的眼球追踪技术。", "summary": "该论文介绍了一种用于精确跟踪和视频记录啮齿动物眼球运动的系统。该系统利用基于生物医学图像分割的卷积神经网络来识别瞳孔和角膜反射，旨在克服现有眼球追踪技术在处理啮齿动物眼部图像（如眼部参数变异性、毛发干扰和小尺寸）时遇到的挑战。该方法结合自动化红外视频眼部记录系统，为啮齿动物的神经科学和视觉科学研究提供了先进的眼球追踪解决方案。", "keywords": "啮齿动物眼球运动, 卷积神经网络, 生物医学图像分割, 眼球追踪, 追踪", "comments": "该研究的创新之处在于它是首次将基于生物医学图像分割的卷积神经网络架构应用于啮齿动物眼球图像的瞳孔和角膜反射识别，从而解决了现有技术在处理啮齿动物独特眼部特征时的不足。其提出的增量训练能力也增加了模型的灵活性和实用性。这项工作为神经科学和视觉科学研究提供了更准确、鲁棒的啮齿动物眼球追踪工具，具有重要的应用价值。"}}
{"id": "2506.08330", "title": "Distortion Search, A Web Search Privacy Heuristic", "authors": ["Kato Mivule", "Kenneth Hopkinson"], "summary": "Search engines have vast technical capabilities to retain Internet search\nlogs for each user and thus present major privacy vulnerabilities to both\nindividuals and organizations in revealing user intent. Additionally, many of\nthe web search privacy enhancing tools available today require that the user\ntrusts a third party, which make confidentiality of user intent even more\nchallenging. The user is left at the mercy of the third party without the\ncontrol over his or her own privacy. In this article, we suggest a user-centric\nheuristic, Distortion Search, a web search query privacy methodology that works\nby the formation of obfuscated search queries via the permutation of query\nkeyword categories, and by strategically applying k-anonymised web navigational\nclicks on URLs and Ads to generate a distorted user profile and thus providing\nspecific user intent and query confidentiality. We provide empirical results\nvia the evaluation of distorted web search queries in terms of retrieved search\nresults and the resulting web ads from search engines. Preliminary experimental\nresults indicate that web search query and specific user intent privacy might\nbe achievable from the user side without the involvement of the search engine\nor other third parties.", "comment": "11 pages, 11 figures, Future Technologies Conference (FTC) 2017", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08330v1", "AI": {"title_translation": "失真搜索，一种网页搜索隐私启发式方法", "tldr": "本文提出了一种名为“失真搜索”的用户中心启发式方法，通过混淆查询和战略性地应用k-匿名化点击来保护网页搜索隐私，避免了对第三方或搜索引擎的依赖。", "motivation": "搜索引擎保留用户搜索日志的能力带来了严重的隐私漏洞，可能泄露用户意图。现有的大多数隐私增强工具需要用户信任第三方，这进一步增加了用户意图保密的挑战，使用户无法掌控自己的隐私。", "method": "本文提出了一种用户中心的启发式方法，名为“失真搜索”，这是一种网页搜索查询隐私方法。它通过以下方式工作：1. 通过查询关键词类别的排列组合形成混淆的搜索查询。2. 战略性地对URL和广告应用k-匿名化的网页导航点击，以生成一个失真的用户配置文件，从而提供特定的用户意图和查询保密性。", "result": "通过评估失真的网页搜索查询在检索到的搜索结果和搜索引擎产生的网络广告方面的表现，提供了实证结果。初步实验结果表明，网页搜索查询和特定用户意图的隐私可能可以在用户端实现，而无需搜索引擎或其他第三方的参与。", "conclusion": "初步实验结果表明，在没有搜索引擎或其他第三方参与的情况下，用户端可以实现网页搜索查询和特定用户意图的隐私。", "translation": "搜索引擎拥有强大的技术能力来保留每个用户的互联网搜索日志，因此在揭示用户意图方面对个人和组织都构成了重大的隐私漏洞。此外，目前许多可用的网页搜索隐私增强工具要求用户信任第三方，这使得用户意图的保密性更具挑战性。用户完全受制于第三方，无法控制自己的隐私。在本文中，我们提出了一种用户中心的启发式方法，即“失真搜索”，这是一种网页搜索查询隐私方法，它通过查询关键词类别的排列组合来形成混淆的搜索查询，并通过战略性地对URL和广告应用k-匿名化的网页导航点击来生成一个失真的用户配置文件，从而提供特定的用户意图和查询保密性。我们通过评估失真的网页搜索查询在检索到的搜索结果和搜索引擎产生的网络广告方面的表现，提供了实证结果。初步实验结果表明，网页搜索查询和特定用户意图的隐私可能可以在用户端实现，而无需搜索引擎或其他第三方的参与。", "summary": "本文介绍了一种名为“失真搜索”的用户中心网页搜索隐私保护方法，旨在解决搜索引擎记录用户搜索日志带来的隐私泄露问题以及现有第三方隐私工具的信任风险。该方法通过混淆搜索查询（排列关键词类别）和战略性应用k-匿名化网页点击（URL和广告）来生成失真的用户配置文件，从而保护用户意图和查询的机密性。初步实验结果表明，这种方法有望在用户端实现搜索隐私，且无需依赖搜索引擎或第三方。", "keywords": "网页搜索隐私, 失真搜索, 用户意图, k-匿名化, 隐私启发式", "comments": "这篇论文的创新点在于提出了一种完全由用户控制的搜索隐私保护机制，避免了对第三方信任的需求，这在当前隐私日益受关注的环境下具有重要意义。通过混淆查询和k-匿名化点击来生成失真用户画像，是一种新颖的思路。该方法旨在解决用户意图泄露的核心隐私问题。"}}
{"id": "2506.08171", "title": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models", "authors": ["Daniel Koh", "Yannic Noller", "Corina S. Pasareanu", "Adrians Skapars", "Youcheng Sun"], "summary": "Large language models (LLMs) have been successfully applied to a variety of\ncoding tasks, including code generation, completion, and repair. However, more\ncomplex symbolic reasoning tasks remain largely unexplored by LLMs. This paper\ninvestigates the capacity of LLMs to reason about worst-case executions in\nprograms through symbolic constraints analysis, aiming to connect LLMs and\nsymbolic reasoning approaches. Specifically, we define and address the problem\nof worst-case symbolic constraints analysis as a measure to assess the\ncomprehension of LLMs. We evaluate the performance of existing LLMs on this\nnovel task and further improve their capabilities through symbolic\nreasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)\nconstraint solving and supported by a specially designed dataset of symbolic\nconstraints. Experimental results show that our solver-aligned model,\nWARP-1.0-3B, consistently surpasses size-matched and even much larger\nbaselines, demonstrating that a 3B LLM can recover the very constraints that\npin down an algorithm's worst-case behaviour through reinforcement learning\nmethods. These findings suggest that LLMs are capable of engaging in deeper\nsymbolic reasoning, supporting a closer integration between neural\nnetwork-based learning and formal methods for rigorous program analysis.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08171v1", "AI": {"title_translation": "最坏情况符号约束分析与大型语言模型的泛化", "tldr": "本文研究大型语言模型（LLM）在程序最坏情况符号约束分析中的能力，通过SMT引导的微调和强化学习，证明一个3B大小的LLM能有效识别算法最坏情况行为的约束，并超越现有基线模型。", "motivation": "尽管大型语言模型（LLM）已成功应用于代码生成等任务，但它们在更复杂的符号推理任务上的能力尚未得到充分探索。本文旨在连接LLM与符号推理方法，通过研究LLM对程序最坏情况执行的符号约束分析能力来评估其理解力。", "method": "定义并提出了“最坏情况符号约束分析”问题作为评估LLM理解力的指标。评估了现有LLM在此任务上的性能。通过基于SMT（可满足性模理论）约束求解的符号推理引导式微调，并利用专门设计的符号约束数据集，提升了LLM的能力。模型通过强化学习方法恢复约束。", "result": "实验结果表明，本文提出的与求解器对齐的模型WARP-1.0-3B，持续超越了同等规模甚至更大的基线模型。这证明了一个3B大小的LLM能够通过强化学习方法恢复确定算法最坏情况行为的关键约束。", "conclusion": "这些发现表明，大型语言模型有能力进行更深层次的符号推理，支持将基于神经网络的学习与形式化方法更紧密地结合，以进行严谨的程序分析。", "translation": "大型语言模型（LLM）已成功应用于各种编码任务，包括代码生成、补全和修复。然而，更复杂的符号推理任务在LLM中仍未得到充分探索。本文通过符号约束分析，研究LLM对程序中最坏情况执行进行推理的能力，旨在连接LLM和符号推理方法。具体来说，我们定义并解决了最坏情况符号约束分析问题，以此作为衡量LLM理解力的标准。我们评估了现有LLM在这项新任务上的表现，并通过基于SMT（可满足性模理论）约束求解的符号推理引导式微调，并辅以专门设计的符号约束数据集，进一步提升了它们的能力。实验结果表明，我们与求解器对齐的模型WARP-1.0-3B持续超越了同等规模甚至更大的基线模型，证明了一个3B大小的LLM能够通过强化学习方法恢复确定算法最坏情况行为的关键约束。这些发现表明，LLM有能力进行更深层次的符号推理，支持将基于神经网络的学习与形式化方法更紧密地结合，以进行严谨的程序分析。", "summary": "本文探讨了大型语言模型（LLM）在程序最坏情况符号约束分析中的能力，旨在弥补LLM在复杂符号推理方面的不足。研究定义了此任务作为评估LLM理解力的指标，并提出了一种基于SMT求解和专门数据集的符号推理引导式微调方法。实验证明，一个3B大小的LLM（WARP-1.0-3B）通过强化学习能够有效识别算法最坏情况行为的约束，其性能优于现有模型，表明LLM具备进行深度符号推理的潜力，可促进神经网络与形式化方法在程序分析中的结合。", "keywords": "大型语言模型, 符号约束分析, 最坏情况执行, 强化学习, SMT", "comments": "这项研究创新性地将大型语言模型应用于传统上由形式化方法处理的复杂符号推理任务，特别是程序的最坏情况分析。通过引入“最坏情况符号约束分析”这一新问题，并采用SMT引导的微调和强化学习，成功地展示了小型LLM在识别关键算法约束方面的强大能力。这不仅扩展了LLM的应用边界，也为程序分析领域提供了一种结合神经和符号方法的新途径，对于提高软件的可靠性和性能分析具有重要意义。"}}
{"id": "2506.08061", "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments.", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08061v1", "AI": {"title_translation": "结构化和非结构化果园中基于移动激光雷达的自适应单棵树冠体积估算", "tldr": "开发了一种使用移动LiDAR的实时系统，用于在不同果园结构中自适应地估算单棵树冠体积，并实现了高精度分割。", "motivation": "先前的方法依赖于静态扫描或假设统一的果园结构，无法适应变化的田地几何形状。", "method": "该方法通过集成LiDAR-惯性里程计、自适应分割和几何重建的管道来适应不同的田地几何形状。采用结合DBSCAN和谱聚类的混合聚类策略进行鲁棒的单棵树分割。", "result": "系统在开心果园（规则间距）和杏仁园（密集重叠树冠）中进行了评估。在开心果园中实现了93%的分割成功率，在杏仁园中实现了80%的成功率，与无人机获得的树冠体积估算结果高度一致。", "conclusion": "这项工作推进了可扩展、非侵入性的树木监测，适用于结构多样化的果园环境。", "translation": "我们提出了一种实时系统，用于使用在常规机器人导航期间收集的移动激光雷达数据估算单棵树冠体积。与依赖静态扫描或假设统一果园结构的先前方法不同，我们的方法通过集成激光雷达惯性里程计、自适应分割和几何重建的管道来适应变化的田地几何形状。我们在两个商业果园评估了该系统：一个具有规则间距的开心果园和一个具有密集重叠树冠的杏仁园。结合DBSCAN和谱聚类的混合聚类策略实现了鲁棒的单棵树分割，在开心果园中取得了93%的成功率，在杏仁园中取得了80%的成功率，并与无人机衍生的树冠体积估算结果高度一致。这项工作推进了可扩展的、非侵入性的树木监测，适用于结构多样化的果园环境。", "summary": "本文介绍了一种利用移动激光雷达数据进行实时单棵树冠体积估算的系统。该系统通过集成LiDAR-惯性里程计、自适应分割和几何重建，克服了传统方法对静态扫描或统一果园结构的依赖，能够适应不同田地几何形状。在开心果园和杏仁园的评估显示，其采用的混合聚类策略实现了高精度的树木分割（93%和80%），且与无人机数据一致，为多样化果园环境提供了可扩展的非侵入性树木监测方案。", "keywords": "移动LiDAR, 树冠体积估算, 自适应分割, 果园监测, 机器人导航", "comments": "这项工作的创新在于其自适应性，能够处理结构多样化的果园环境，并通过结合LiDAR-惯性里程计和混合聚类策略实现实时、非侵入性的高精度树冠体积估算，对于农业自动化和精准管理具有重要意义。"}}
{"id": "2506.08071", "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems", "authors": ["Aniket Rege", "Zinnia Nie", "Mahesh Ramesh", "Unmesh Raskar", "Zhuoran Yu", "Aditya Kusupati", "Yong Jae Lee", "Ramya Korlakai Vinayak"], "summary": "Popular text-to-image (T2I) systems are trained on web-scraped data, which is\nheavily Amero and Euro-centric, underrepresenting the cultures of the Global\nSouth. To analyze these biases, we introduce CuRe, a novel and scalable\nbenchmarking and scoring suite for cultural representativeness that leverages\nthe marginal utility of attribute specification to T2I systems as a proxy for\nhuman judgments. Our CuRe benchmark dataset has a novel categorical hierarchy\nbuilt from the crowdsourced Wikimedia knowledge graph, with 300 cultural\nartifacts across 32 cultural subcategories grouped into six broad cultural axes\n(food, art, fashion, architecture, celebrations, and people). Our dataset's\ncategorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing\ntheir response to increasing the informativeness of text conditioning, enabling\nfine-grained cultural comparisons. We empirically observe much stronger\ncorrelations of our class of scorers to human judgments of perceptual\nsimilarity, image-text alignment, and cultural diversity across image encoders\n(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,\nGemini 2.0 Flash) and state-of-the-art text-to-image systems, including three\nvariants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,\nand DALL-E 3. The code and dataset is open-sourced and available at\nhttps://aniketrege.github.io/cure/.", "comment": "41 pages, 22 figures, 17 tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08071v1", "AI": {"title_translation": "CuRe: 文本到图像系统长尾中的文化鸿沟", "tldr": "文本到图像(T2I)系统存在文化偏见，本文引入CuRe基准测试套件，用以量化评估并发现其与人类判断高度相关。", "motivation": "流行的文本到图像（T2I）系统在网络抓取数据上进行训练，这些数据严重偏向欧美中心，未能充分代表全球南方文化。为了分析和解决这些文化偏见，本研究旨在引入一个新颖且可扩展的基准测试和评分套件。", "method": "本文引入了CuRe，一个新颖且可扩展的文化代表性基准测试和评分套件。CuRe利用属性规范对T2I系统的边际效用作为人类判断的代理。其基准数据集具有从众包Wikimedia知识图谱构建的新颖分类层次结构，包含32个文化子类别中的300个文化物品，分为六个广泛的文化轴（食物、艺术、时尚、建筑、庆典和人物）。该数据集的分类层次结构使CuRe评分器能够通过分析T2I系统对文本条件信息量增加的响应来评估它们，从而实现细粒度的文化比较。", "result": "经验观察到CuRe的评分器与图像编码器（SigLIP 2, AIMV2和DINOv2）、视觉-语言模型（OpenCLIP, SigLIP 2, Gemini 2.0 Flash）以及最先进的文本到图像系统（包括Stable Diffusion的三个变体（1.5, XL, 3.5 Large）, FLUX.1 [dev], Ideogram 2.0和DALL-E 3）在感知相似性、图像-文本对齐和文化多样性方面的人类判断具有更强的相关性。代码和数据集已开源。", "conclusion": "CuRe提供了一个有效且可扩展的工具来量化和分析文本到图像系统中存在的文化偏见，并且其评估结果与人类判断高度一致，有望促进开发更具文化包容性的AI系统。", "translation": "流行的文本到图像（T2I）系统在网络抓取数据上进行训练，这些数据严重偏向欧美中心，未能充分代表全球南方文化。为了分析这些偏见，我们引入了CuRe，一个新颖且可扩展的文化代表性基准测试和评分套件，它利用属性规范对T2I系统的边际效用作为人类判断的代理。我们的CuRe基准数据集具有从众包Wikimedia知识图谱构建的新颖分类层次结构，包含32个文化子类别中的300个文化物品，分为六个广泛的文化轴（食物、艺术、时尚、建筑、庆典和人物）。我们数据集的分类层次结构使CuRe评分器能够通过分析T2I系统对文本条件信息量增加的响应来评估它们，从而实现细粒度的文化比较。我们经验观察到，我们这类的评分器与图像编码器（SigLIP 2、AIMV2和DINOv2）、视觉-语言模型（OpenCLIP、SigLIP 2、Gemini 2.0 Flash）以及最先进的文本到图像系统（包括Stable Diffusion的三个变体（1.5、XL、3.5 Large）、FLUX.1 [dev]、Ideogram 2.0和DALL-E 3）在感知相似性、图像-文本对齐和文化多样性方面的人类判断具有更强的相关性。代码和数据集已开源，可在 https://aniketrege.github.io/cure/ 获取。", "summary": "本文介绍了CuRe，一个用于评估文本到图像（T2I）系统中文化偏见的新型基准测试和评分套件。CuRe通过分析T2I系统对增加文本信息量的响应来量化文化代表性，并构建了一个包含全球南方文化元素的分类数据集。实验结果表明，CuRe的评分与人类对感知相似性、图像-文本对齐和文化多样性的判断高度相关，证实了其在识别和量化T2I系统文化鸿沟方面的有效性。CuRe的代码和数据集已开源，为未来研究提供了工具。", "keywords": "文本到图像系统, 文化偏见, 基准测试, 文化代表性, 长尾效应", "comments": "这篇论文通过引入CuRe基准测试，创新性地解决了文本到图像系统中存在的文化偏见问题。其方法利用了属性规范的边际效用，并构建了一个层次化的、包含全球南方文化元素的众包数据集，这对于量化和理解AI模型的文化代表性具有重要意义。CuRe与人类判断的高度相关性以及其开源性，使其成为未来研究和开发更具文化包容性T2I系统的重要工具。"}}
{"id": "2506.09017", "title": "Linear exact repair schemes for free MDS and Reed-Solomon codes over Galois rings", "authors": ["Daniel P. Bossaller", "Hiram H. López"], "summary": "Codes over rings, especially over Galois rings, have been extensively studied\nfor nearly three decades due to their similarity to linear codes over finite\nfields. A distributed storage system uses a linear code to encode a large file\nacross several nodes. If one of the nodes fails, a linear exact repair scheme\nefficiently recovers the failed node by accessing and downloading data from the\nrest of the servers of the storage system. In this article, we develop a linear\nrepair scheme for free maximum distance separable codes, which coincide with\nfree maximum distance with respect to the rank codes over Galois rings. In\nparticular, we give a linear repair scheme for full-length Reed-Solomon codes\nover a Galois ring.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09017v1", "AI": {"title_translation": "伽罗瓦环上自由MDS码和Reed-Solomon码的线性精确修复方案", "tldr": "本文为伽罗瓦环上的自由MDS码和Reed-Solomon码开发了一种线性精确修复方案，用于分布式存储系统中的失效节点恢复。", "motivation": "分布式存储系统中，当存储节点失效时，需要高效地恢复数据。线性精确修复方案可以实现这一点。伽罗瓦环上的码因其与有限域上线性码的相似性，已被广泛研究，为开发此类方案提供了理论基础。", "method": "本文开发了一种针对伽罗瓦环上自由最大距离可分离码（与伽罗瓦环上关于秩的自由最大距离码一致）的线性修复方案。特别地，他们为伽罗瓦环上的全长Reed-Solomon码提供了一种线性修复方案。", "result": "成功开发了伽罗瓦环上自由MDS码和全长Reed-Solomon码的线性精确修复方案。", "conclusion": "本文为伽罗瓦环上的自由MDS码和Reed-Solomon码提供了一种线性精确修复方案，能够高效地恢复分布式存储系统中失效节点的数据，解决了分布式存储系统中的数据恢复问题。", "translation": "环上的码，特别是伽罗瓦环上的码，由于其与有限域上线性码的相似性，在近三十年来得到了广泛研究。分布式存储系统使用线性码将大文件编码到多个节点上。如果其中一个节点失效，线性精确修复方案可以通过访问和下载存储系统中其余服务器的数据来高效地恢复失效节点。在本文中，我们为自由最大距离可分离码（其与伽罗瓦环上关于秩的自由最大距离码一致）开发了一种线性修复方案。特别是，我们给出了一种伽罗瓦环上全长Reed-Solomon码的线性修复方案。", "summary": "本文针对分布式存储系统中节点失效的数据恢复问题，提出了一种基于伽罗瓦环上自由MDS码和Reed-Solomon码的线性精确修复方案。该方案能够通过访问存储系统中其余服务器的数据来高效地恢复失效节点，扩展了伽罗瓦环上编码理论在实际应用中的潜力，为分布式存储系统的可靠性提供了新的解决方案。", "keywords": "伽罗瓦环, 线性精确修复, MDS码, Reed-Solomon码, 分布式存储系统", "comments": "本文的创新点在于将线性精确修复方案应用于伽罗瓦环上的自由MDS码和Reed-Solomon码，这对于分布式存储系统中的数据可靠性和效率具有重要意义。伽罗瓦环上的码的研究历史悠久，本文将其与现代分布式存储需求相结合，具有较强的实用价值，有望提升未来分布式存储系统的性能。"}}
{"id": "2506.08119", "title": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents", "authors": ["Subhrangshu Nandi", "Arghya Datta", "Nikhil Vichare", "Indranil Bhattacharya", "Huzefa Raja", "Jing Xu", "Shayan Ray", "Giuseppe Carenini", "Abhi Srivastava", "Aaron Chan", "Man Ho Woo", "Amar Kandola", "Brandon Theresa", "Francesco Carbone"], "summary": "Large Language Models (LLMs) demonstrate impressive general-purpose reasoning\nand problem-solving abilities. However, they struggle with executing complex,\nlong-horizon workflows that demand strict adherence to Standard Operating\nProcedures (SOPs), a critical requirement for real-world industrial automation.\nDespite this need, there is a lack of public benchmarks that reflect the\ncomplexity, structure, and domain-specific nuances of SOPs. To address this, we\npresent three main contributions. First, we introduce a synthetic data\ngeneration framework to create realistic, industry-grade SOPs that rigorously\ntest the planning, reasoning, and tool-use capabilities of LLM-based agents.\nSecond, using this framework, we develop SOP-Bench, a benchmark of over 1,800\ntasks across 10 industrial domains, each with APIs, tool interfaces, and\nhuman-validated test cases. Third, we evaluate two prominent agent\narchitectures: Function-Calling and ReAct Agents, on SOP-Bench, observing\naverage success rates of only 27% and 48%, respectively. Remarkably, when the\ntool registry is much larger than necessary, agents invoke incorrect tools\nnearly 100% of the time. These findings underscore a substantial gap between\ncurrent agentic capabilities of LLMs and the demands of automating real-world\nSOPs. Performance varies significantly by task and domain, highlighting the\nneed for domain-specific benchmarking and architectural choices before\ndeployment. SOP-Bench is publicly available at\nhttp://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the\nprompts underpinning the data generation framework to support new\ndomain-specific SOP benchmarks. We invite the community to extend SOP-Bench\nwith SOPs from their industrial domains.", "comment": "Under review", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08119v1", "AI": {"title_translation": "SOP-Bench：用于评估LLM代理的复杂工业SOP", "tldr": "SOP-Bench是一个新的基准测试，用于评估LLM代理在复杂工业SOP执行方面的能力，发现当前LLM代理在SOP执行方面与实际需求存在显著差距。", "motivation": "尽管大型语言模型（LLM）展现出强大的通用推理和问题解决能力，但在执行需要严格遵守标准操作程序（SOP）的复杂、长周期工作流程时，它们表现不佳。当前缺乏反映SOP复杂性、结构和领域特定细微差别的公共基准测试。", "method": "1. 引入了一个合成数据生成框架，用于创建真实的工业级SOP，以严格测试LLM代理的规划、推理和工具使用能力。2. 利用该框架开发了SOP-Bench，这是一个包含10个工业领域、超过1,800个任务的基准测试，每个任务都包含API、工具接口和人工验证的测试用例。3. 评估了两种主要的代理架构：函数调用（Function-Calling）和ReAct代理，在SOP-Bench上的表现。", "result": "函数调用代理的平均成功率仅为27%，ReAct代理为48%。当工具注册表远大于必要时，代理调用错误工具的概率几乎达到100%。性能因任务和领域而异。", "conclusion": "当前LLM的代理能力与自动化真实世界SOP的需求之间存在巨大差距。在部署之前，需要进行特定领域的基准测试和架构选择。", "translation": "大型语言模型（LLM）展现出令人印象深刻的通用推理和问题解决能力。然而，它们在执行需要严格遵守标准操作程序（SOP）的复杂、长周期工作流程时表现不佳，而这正是现实世界工业自动化的一项关键要求。尽管存在这种需求，但目前缺乏反映SOP复杂性、结构和领域特定细微差差别的公共基准测试。为了解决这个问题，我们提出了三项主要贡献。首先，我们引入了一个合成数据生成框架，用于创建真实的工业级SOP，以严格测试LLM代理的规划、推理和工具使用能力。其次，利用该框架，我们开发了SOP-Bench，这是一个包含10个工业领域、超过1,800个任务的基准测试，每个任务都包含API、工具接口和人工验证的测试用例。第三，我们评估了两种主要的代理架构：函数调用（Function-Calling）和ReAct代理，在SOP-Bench上的表现，观察到平均成功率分别仅为27%和48%。值得注意的是，当工具注册表远大于必要时，代理调用错误工具的概率几乎达到100%。这些发现凸显了当前LLM代理能力与自动化真实世界SOP的需求之间存在巨大差距。性能因任务和领域而异，这强调了在部署之前进行特定领域基准测试和架构选择的必要性。SOP-Bench可在http://sop-bench.s3-website-us-west-2.amazonaws.com/公开获取。我们还发布了支撑数据生成框架的提示，以支持新的领域特定SOP基准测试。我们邀请社区用其工业领域的SOP来扩展SOP-Bench。", "summary": "本研究介绍了SOP-Bench，一个用于评估大型语言模型（LLM）代理在复杂工业标准操作程序（SOP）执行能力的基准测试。针对LLM在遵循复杂SOP方面存在的挑战和公共基准的缺乏，研究构建了一个合成数据生成框架来创建真实的工业SOP，并基于此开发了包含10个工业领域、1800多个任务的SOP-Bench。评估结果显示，当前LLM代理（如Function-Calling和ReAct）在SOP-Bench上的成功率分别为27%和48%，表明它们在执行复杂SOP方面存在显著不足，尤其是在工具选择方面。研究强调了LLM代理能力与实际工业自动化需求之间的巨大差距，并呼吁进行更多特定领域的基准测试和架构优化。", "keywords": "SOP-Bench, LLM代理, 工业SOP, 基准测试, 工具使用", "comments": "SOP-Bench的创新之处在于其提供了首个公开的、专注于工业SOP的复杂基准测试，填补了LLM代理在现实世界工业自动化场景中评估的空白。它通过合成数据生成框架，确保了SOP的真实性和复杂性，并提供了丰富的工具接口和人工验证的测试用例。该研究揭示了当前LLM代理在处理复杂SOP和工具选择方面的显著局限性，为未来的研究指明了方向。其公开可用性将极大促进该领域的发展。"}}
{"id": "2506.08020", "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation", "authors": ["Zi-Ying Chen", "Chuan-Xian Ren", "Hong Yan"], "summary": "Partial domain adaptation (PDA) problem requires aligning cross-domain\nsamples while distinguishing the outlier classes for accurate knowledge\ntransfer. The widely used weighting framework tries to address the outlier\nclasses by introducing the reweighed source domain with a similar label\ndistribution to the target domain. However, the empirical modeling of weights\ncan only characterize the sample-wise relations, which leads to insufficient\nexploration of cluster structures, and the weights could be sensitive to the\ninaccurate prediction and cause confusion on the outlier classes. To tackle\nthese issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model\nto simultaneously characterize the sample-wise and class-wise relations in a\nunified transport framework. Specifically, a cooperation mechanism between\nsample-level and class-level transport is introduced, where the sample-level\ntransport provides essential structure information for the class-level\nknowledge transfer, while the class-level transport supplies discriminative\ninformation for the outlier identification. The bi-level transport plan\nprovides guidance for the alignment process. By incorporating the label-aware\ntransport cost, the local transport structure is ensured and a fast computation\nformulation is derived to improve the efficiency. Extensive experiments on\nbenchmark datasets validate the competitiveness of BUOT.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08020v1", "AI": {"title_translation": "用于部分域适应的双层不平衡最优传输", "tldr": "本文提出了一种名为BUOT的双层不平衡最优传输模型，用于解决部分域适应（PDA）问题中现有加权框架的局限性，通过同时表征样本级和类别级关系，并引入合作机制，有效识别异常类并提高对齐效率。", "motivation": "现有的部分域适应（PDA）加权框架在处理异常类时，仅能表征样本级关系，未能充分探索聚类结构，且权重对不准确的预测敏感，可能导致对异常类的混淆。", "method": "本文提出了双层不平衡最优传输（BUOT）模型，通过引入样本级和类别级传输的合作机制，在统一的传输框架中同时表征样本级和类别级关系。样本级传输提供结构信息，类别级传输提供判别信息以识别异常类。通过结合标签感知传输成本，确保局部传输结构并推导出快速计算公式。", "result": "在基准数据集上的大量实验验证了BUOT的竞争力。", "conclusion": "Not mentioned in abstract", "translation": "部分域适应（PDA）问题需要对齐跨域样本，同时区分异常类以实现准确的知识迁移。广泛使用的加权框架试图通过引入与目标域具有相似标签分布的重新加权源域来解决异常类问题。然而，权重的经验建模只能表征样本间的关系，导致对聚类结构的探索不足，并且权重可能对不准确的预测敏感，从而导致异常类的混淆。为了解决这些问题，我们提出了一种双层不平衡最优传输（BUOT）模型，以在统一的传输框架中同时表征样本级和类别级关系。具体来说，引入了样本级和类别级传输之间的合作机制，其中样本级传输为类别级知识迁移提供必要的结构信息，而类别级传输为异常识别提供判别信息。双层传输计划为对齐过程提供指导。通过结合标签感知传输成本，确保了局部传输结构，并推导出了快速计算公式以提高效率。在基准数据集上的大量实验验证了BUOT的竞争力。", "summary": "针对部分域适应（PDA）中现有加权框架的局限性，即无法充分探索聚类结构且对异常类识别敏感的问题，本文提出了一种新颖的双层不平衡最优传输（BUOT）模型。该模型在一个统一的框架中同时表征样本级和类别级关系，通过样本级和类别级传输的协同作用，提供结构信息和判别信息，以有效地识别异常类并指导域对齐。此外，通过引入标签感知传输成本和快速计算公式，提高了模型的效率和局部传输结构的保证。实验结果表明BUOT具有竞争力。", "keywords": "部分域适应, 不平衡最优传输, 双层学习, 异常类识别, 知识迁移", "comments": "该论文的创新点在于提出了双层不平衡最优传输（BUOT）模型，它超越了传统的样本级加权方法，通过引入样本级和类别级传输的合作机制，更全面地捕捉了数据结构和类间关系。这种双层方法有望更准确地识别异常类并进行知识迁移。此外，结合标签感知传输成本和快速计算公式，提升了方法的效率和实用性。"}}
{"id": "2506.08117", "title": "We Are AI: Taking Control of Technology", "authors": ["Julia Stoyanovich", "Armanda Lewis", "Eric Corbett", "Lucius E. J. Bynum", "Lucas Rosenblatt", "Falaah Arif Khan"], "summary": "Responsible AI (RAI) is the science and practice of ensuring the design,\ndevelopment, use, and oversight of AI are socially sustainable--benefiting\ndiverse stakeholders while controlling the risks. Achieving this goal requires\nactive engagement and participation from the broader public. This paper\nintroduces \"We are AI: Taking Control of Technology,\" a public education course\nthat brings the topics of AI and RAI to the general audience in a peer-learning\nsetting.\n  We outline the goals behind the course's development, discuss the multi-year\niterative process that shaped its creation, and summarize its content. We also\ndiscuss two offerings of We are AI to an active and engaged group of librarians\nand professional staff at New York University, highlighting successes and areas\nfor improvement. The course materials, including a multilingual comic book\nseries by the same name, are publicly available and can be used independently.\nBy sharing our experience in creating and teaching We are AI, we aim to\nintroduce these resources to the community of AI educators, researchers, and\npractitioners, supporting their public education efforts.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.08117v1", "AI": {"title_translation": "我们是AI：掌控技术", "tldr": "论文介绍了“我们是AI：掌控技术”这一公共教育课程，旨在向普通大众普及AI和负责任AI的知识，并分享了课程的开发经验和可公开获取的资源，以支持AI公共教育。", "motivation": "确保AI的设计、开发、使用和监督具有社会可持续性，使不同利益相关者受益并控制风险，这需要公众的积极参与。本课程旨在将AI和负责任AI的话题带给普通大众。", "method": "本文介绍了“我们是AI：掌控技术”这一公共教育课程，旨在将AI和负责任AI的话题带给普通大众。文章概述了课程开发的背后目标，讨论了塑造其创建的多年迭代过程，并总结了其内容。此外，文章还讨论了该课程在纽约大学面向图书馆员和专业员工的两次授课经验，强调了成功之处和需要改进的领域。课程材料（包括多语言漫画书系列）可公开获取。", "result": "“我们是AI”课程成功地在纽约大学面向一群积极参与的图书馆员和专业员工进行了两次授课，并从中识别了成功之处和需要改进的领域。课程材料，包括同名的多语言漫画书系列，已公开可用。", "conclusion": "通过分享创建和教授“我们是AI”课程的经验，旨在向AI教育者、研究人员和从业者社区介绍这些资源，以支持他们的公共教育工作，促进负责任AI的普及。", "translation": "负责任的AI（RAI）是确保AI的设计、开发、使用和监督具有社会可持续性——使不同的利益相关者受益同时控制风险——的科学和实践。实现这一目标需要更广泛公众的积极参与。本文介绍了“我们是AI：掌控技术”，这是一门公共教育课程，它以同伴学习的形式将AI和RAI的话题带给普通大众。\n我们概述了课程开发背后的目标，讨论了塑造其创建的多年迭代过程，并总结了其内容。我们还讨论了“我们是AI”在纽约大学面向一群积极参与的图书馆员和专业员工的两次授课，强调了成功之处和需要改进的领域。课程材料，包括同名的多语言漫画书系列，是公开可用的，可以独立使用。通过分享我们创建和教授“我们是AI”的经验，我们旨在向AI教育者、研究人员和从业者社区介绍这些资源，支持他们的公共教育工作。", "summary": "这篇论文介绍了一个名为“我们是AI：掌控技术”的公共教育课程，旨在向普通大众普及AI和负责任AI的知识，以促进AI的社会可持续发展和风险控制。论文详细阐述了课程的开发目标、迭代过程、内容总结，并分享了在纽约大学对图书馆员和专业员工进行两次授课的经验，包括成功点和改进空间。所有课程材料，包括多语言漫画书，均已公开，旨在支持AI教育者和研究人员的公共教育努力。", "keywords": "负责任AI, 公共教育, AI素养, 课程开发, 社会可持续性", "comments": "这篇论文的创新之处在于它将负责任AI的复杂概念通过一个公共教育课程和多语言漫画书的形式，以可访问和引人入胜的方式呈现给非专业大众。这对于提升公众对AI的理解和参与，实现AI的社会可持续发展至关重要。其开放共享的课程材料也极具价值，有助于推广负责任AI教育。"}}
{"id": "2506.06353", "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy", "authors": ["Naseem Babu", "Jimson Mathew", "A. P. Vinod"], "summary": "The growing convergence between Large Language Models (LLMs) and\nelectroencephalography (EEG) research is enabling new directions in neural\ndecoding, brain-computer interfaces (BCIs), and affective computing. This\nsurvey offers a systematic review and structured taxonomy of recent\nadvancements that utilize LLMs for EEG-based analysis and applications. We\norganize the literature into four domains: (1) LLM-inspired foundation models\nfor EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal\ngeneration including image and 3D object synthesis, and (4) clinical\napplications and dataset management tools. The survey highlights how\ntransformer-based architectures adapted through fine-tuning, few-shot, and\nzero-shot learning have enabled EEG-based models to perform complex tasks such\nas natural language generation, semantic interpretation, and diagnostic\nassistance. By offering a structured overview of modeling strategies, system\ndesigns, and application areas, this work serves as a foundational resource for\nfuture work to bridge natural language processing and neural signal analysis\nthrough language models.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.06353v1", "AI": {"title_translation": "大型语言模型在脑电图中的应用：一项综合调查与分类", "tldr": "本调查系统回顾并分类了大型语言模型在脑电图分析与应用中的最新进展，涵盖了表示学习、语言解码、跨模态生成及临床应用。", "motivation": "大型语言模型（LLM）与脑电图（EEG）研究之间日益增长的融合正在神经解码、脑机接口（BCI）和情感计算领域开辟新方向，因此需要对该领域的最新进展进行系统回顾和分类。", "method": "本调查通过提供一个系统回顾和结构化分类来组织文献，将其分为四个主要领域：(1) LLM启发的EEG表示学习基础模型，(2) EEG到语言解码，(3) 包括图像和3D对象合成在内的跨模态生成，以及(4) 临床应用和数据集管理工具。", "result": "调查强调了通过微调、少样本和零样本学习调整的基于Transformer的架构，如何使基于EEG的模型能够执行自然语言生成、语义解释和诊断辅助等复杂任务。", "conclusion": "这项工作通过提供建模策略、系统设计和应用领域的结构化概述，为未来通过语言模型连接自然语言处理和神经信号分析的工作提供了基础资源。", "translation": "大型语言模型（LLM）与脑电图（EEG）研究之间日益增长的融合正在神经解码、脑机接口（BCI）和情感计算领域开辟新方向。本调查对利用LLM进行EEG分析和应用的最新进展提供了一个系统回顾和结构化分类。我们将文献组织成四个领域：(1) LLM启发的EEG表示学习基础模型，(2) EEG到语言解码，(3) 包括图像和3D对象合成在内的跨模态生成，以及(4) 临床应用和数据集管理工具。调查强调了通过微调、少样本和零样本学习调整的基于Transformer的架构如何使基于EEG的模型能够执行自然语言生成、语义解释和诊断辅助等复杂任务。通过提供建模策略、系统设计和应用领域的结构化概述，这项工作为未来通过语言模型连接自然语言处理和神经信号分析的工作提供了基础资源。", "summary": "本调查系统地回顾并分类了大型语言模型（LLM）与脑电图（EEG）研究融合的最新进展。它将文献组织成LLM启发的EEG表示学习、EEG到语言解码、跨模态生成和临床应用四个核心领域，并强调了基于Transformer的架构如何通过微调、少样本和零样本学习赋能EEG模型执行复杂任务。这项工作旨在为未来连接自然语言处理和神经信号分析的研究提供基础资源。", "keywords": "大型语言模型, 脑电图, 神经解码, 脑机接口, 综述", "comments": "该论文的创新之处在于它系统地梳理并分类了大型语言模型与脑电图交叉领域的研究进展，为新兴的神经解码、脑机接口和情感计算方向提供了全面的视角。它强调了Transformer架构在EEG任务中的潜力，为未来的跨学科研究奠定了基础。"}}
{"id": "2506.08430", "title": "CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models", "authors": ["Ziqi. Liu", "Ziyang. Zhou", "Mingxuan. Hu"], "summary": "Large language model (LLM) have become mainstream methods in the field of\nsarcasm detection. However, existing LLM methods face challenges in irony\ndetection, including: 1. single-perspective limitations, 2. insufficient\ncomprehensive understanding, and 3. lack of interpretability. This paper\nintroduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven\nmulti-agent system designed to overcome these issues. CAF-I employs specialized\nagents for Context, Semantics, and Rhetoric, which perform multidimensional\nanalysis and engage in interactive collaborative optimization. A Decision Agent\nthen consolidates these perspectives, with a Refinement Evaluator Agent\nproviding conditional feedback for optimization. Experiments on benchmark\ndatasets establish CAF-I's state-of-the-art zero-shot performance. Achieving\nSOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of\n76.31, a 4.98 absolute improvement over the strongest prior baseline. This\nsuccess is attained by its effective simulation of human-like multi-perspective\nanalysis, enhancing detection accuracy and interpretability.", "comment": "ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08430v1", "AI": {"title_translation": "CAF-I：一种用于增强大型语言模型反讽检测的协作式多智能体框架", "tldr": "CAF-I是一个基于LLM的多智能体框架，通过模拟人类多视角分析，显著提升了反讽检测的零样本性能，并达到了SOTA。", "motivation": "现有LLM在反讽检测中面临单视角限制、综合理解不足和缺乏可解释性等挑战。", "method": "本文提出了反讽协作智能体框架（CAF-I），一个由LLM驱动的多智能体系统。CAF-I采用专门的上下文、语义和修辞智能体进行多维度分析和交互式协作优化，一个决策智能体整合这些视角，并由一个细化评估智能体提供条件反馈进行优化。", "result": "CAF-I在基准数据集上实现了最先进的零样本性能，在绝大多数指标上达到SOTA，平均Macro-F1为76.31，比之前最强的基线绝对提升4.98。", "conclusion": "CAF-I通过有效模拟人类般的多视角分析，提高了检测准确性和可解释性，从而取得了成功。", "translation": "大型语言模型（LLM）已成为讽刺检测领域的主流方法。然而，现有LLM方法在反讽检测中面临挑战，包括：1. 单视角限制，2. 综合理解不足，以及3. 缺乏可解释性。本文引入了反讽协作智能体框架（CAF-I），这是一个由LLM驱动的多智能体系统，旨在克服这些问题。CAF-I采用专门的上下文、语义和修辞智能体，它们执行多维度分析并进行交互式协作优化。然后，决策智能体整合这些视角，细化评估智能体提供条件反馈以进行优化。在基准数据集上的实验证明了CAF-I最先进的零样本性能。CAF-I在绝大多数指标上达到SOTA，平均Macro-F1为76.31，比之前最强的基线绝对提升4.98。这一成功是通过其有效模拟人类般的多视角分析实现的，从而提高了检测准确性和可解释性。", "summary": "本文提出了CAF-I，一个基于大型语言模型的多智能体框架，旨在解决现有LLM在反讽检测中面临的单视角、理解不足和可解释性差的问题。CAF-I利用上下文、语义和修辞等专业智能体进行多维度协作分析，并通过决策和评估智能体进行优化。实验证明CAF-I在零样本反讽检测上达到了SOTA性能，显著提升了准确性和可解释性。", "keywords": "反讽检测, 大型语言模型, 多智能体系统, 零样本学习, 协作框架", "comments": "CAF-I的创新之处在于其独特的多智能体协作框架，它模拟了人类多视角分析的过程，有效解决了LLM在反讽检测中面临的固有挑战。其在零样本设置下取得SOTA性能，表明了该方法在实际应用中的巨大潜力，特别是在处理复杂语言现象如反讽时。"}}
{"id": "2506.08303", "title": "EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation", "authors": ["Thomas M. Kwok", "Hilary HY Cheng", "Wai Tuck Chow"], "summary": "In this work, we introduce HJ-Pal, a lightweight wearable haptic device that\nleverages EMG-driven honeycomb jamming to render muscle activation as\nkinesthetic feedback, enabling remote palpation for small muscle assessment in\ntelerehabilitation.", "comment": "Accepted by the Workshop on Human-Robot Contact and Manipulation\n  (HRCM 2025) at RSS Conference 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08303v1", "AI": {"title_translation": "EMG驱动的刚度调节触诊用于远程康复", "tldr": "本文介绍了一种名为HJ-Pal的轻量级可穿戴触觉设备，该设备利用EMG驱动的蜂窝式阻塞技术，旨在实现远程康复中对小肌肉的远程触诊评估。", "motivation": "该研究的动机是为了实现在远程康复中对小肌肉进行远程触诊评估。", "method": "该研究引入了HJ-Pal，一种轻量级可穿戴触觉设备，它利用EMG驱动的蜂窝式阻塞技术将肌肉激活转化为动觉反馈。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在这项工作中，我们引入了HJ-Pal，这是一种轻量级可穿戴触觉设备，它利用EMG驱动的蜂窝式阻塞技术将肌肉激活呈现为动觉反馈，从而实现在远程康复中对小肌肉进行远程触诊评估。", "summary": "本文介绍了一种名为HJ-Pal的轻量级可穿戴触觉设备。该设备利用EMG驱动的蜂窝式阻塞技术，将肌肉激活转化为动觉反馈，从而实现远程康复中对小肌肉的远程触诊评估。", "keywords": "EMG, 触觉设备, 远程康复, 触诊, 肌肉评估", "comments": "该论文的创新之处在于利用EMG驱动的蜂窝式阻塞技术为远程康复提供触觉反馈，这对于远程肌肉评估具有重要意义。"}}
{"id": "2506.08408", "title": "Aerial Shepherds: Enabling Hierarchical Localization in Heterogeneous MAV Swarms", "authors": ["Haoyang Wang", "Jingao Xu", "Chenyu Zhao", "Yuhan Cheng", "Xuecheng Chen", "Chaopeng Hong", "Xiao-Ping Zhang", "Yunhao Liu", "Xinlei Chen"], "summary": "A heterogeneous micro aerial vehicles (MAV) swarm consists of\nresource-intensive but expensive advanced MAVs (AMAVs) and resource-limited but\ncost-effective basic MAVs (BMAVs), offering opportunities in diverse fields.\nAccurate and real-time localization is crucial for MAV swarms, but current\npractices lack a low-cost, high-precision, and real-time solution, especially\nfor lightweight BMAVs. We find an opportunity to accomplish the task by\ntransforming AMAVs into mobile localization infrastructures for BMAVs. However,\ntranslating this insight into a practical system is challenging due to issues\nin estimating locations with diverse and unknown localization errors of BMAVs,\nand allocating resources of AMAVs considering interconnected influential\nfactors. This work introduces TransformLoc, a new framework that transforms\nAMAVs into mobile localization infrastructures, specifically designed for\nlow-cost and resource-constrained BMAVs. We design an error-aware joint\nlocation estimation model to perform intermittent joint estimation for BMAVs\nand introduce a similarity-instructed adaptive grouping-scheduling strategy to\nallocate resources of AMAVs dynamically. TransformLoc achieves a collaborative,\nadaptive, and cost-effective localization system suitable for large-scale\nheterogeneous MAV swarms. We implement and validate TransformLoc on industrial\ndrones. Results show it outperforms all baselines by up to 68\\% in localization\nperformance, improving navigation success rates by 60\\%. Extensive robustness\nand ablation experiments further highlight the superiority of its design.", "comment": "18 pages", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.08408v1", "AI": {"title_translation": "空中牧羊人：实现异构MAV蜂群的分层定位", "tldr": "该研究提出TransformLoc框架，将高级MAV（AMAV）转化为低成本基本MAV（BMAV）的移动定位基础设施，显著提高定位性能和导航成功率。", "motivation": "当前MAV蜂群的定位方案缺乏低成本、高精度和实时性，特别是对于轻量级BMAV。现有实践难以解决BMAV定位误差多样且未知以及AMAV资源分配的问题。", "method": "本研究引入TransformLoc框架，将AMAVs转换为移动定位基础设施。设计了一个误差感知联合位置估计模型，对BMAVs进行间歇性联合估计。引入了相似性指导的自适应分组调度策略，动态分配AMAV的资源。", "result": "TransformLoc在定位性能上优于所有基线高达68%，导航成功率提高了60%。广泛的鲁棒性和消融实验进一步突出了其设计的优越性。", "conclusion": "TransformLoc实现了一个协作、自适应且经济高效的定位系统，适用于大规模异构MAV蜂群，并通过实验验证了其在工业无人机上的卓越性能。", "translation": "一个异构微型飞行器（MAV）蜂群由资源密集但昂贵的高级MAV（AMAV）和资源有限但成本效益高的基本MAV（BMAV）组成，在不同领域提供了机会。准确和实时的定位对于MAV蜂群至关重要，但目前的实践缺乏低成本、高精度和实时的解决方案，特别是对于轻量级BMAV。我们发现了一个通过将AMAVs转化为BMAVs的移动定位基础设施来完成任务的机会。然而，由于BMAVs定位误差多样且未知以及考虑相互关联影响因素的AMAVs资源分配问题，将这一见解转化为实际系统具有挑战性。这项工作引入了TransformLoc，一个新框架，它将AMAVs转化为移动定位基础设施，专门为低成本和资源受限的BMAVs设计。我们设计了一个误差感知联合位置估计模型，对BMAVs进行间歇性联合估计，并引入了相似性指导的自适应分组调度策略来动态分配AMAVs的资源。TransformLoc实现了一个协作、自适应且经济高效的定位系统，适用于大规模异构MAV蜂群。我们在工业无人机上实施并验证了TransformLoc。结果显示，它在定位性能上优于所有基线高达68%，导航成功率提高了60%。广泛的鲁棒性和消融实验进一步突出了其设计的优越性。", "summary": "该论文提出了TransformLoc框架，旨在解决异构MAV蜂群中，特别是资源受限的基本MAV（BMAV）的低成本、高精度实时定位难题。通过将高级MAV（AMAV）转化为移动定位基础设施，TransformLoc利用误差感知联合估计模型和自适应资源分配策略，显著提升了BMAV的定位性能和导航成功率，经验证其性能远超现有基线。", "keywords": "异构MAV蜂群, 分层定位, 移动基础设施, 误差感知估计, 资源分配", "comments": "该论文通过创新性地将资源丰富的AMAVs转化为移动定位基础设施，为异构MAV蜂群的低成本、高精度定位提供了切实可行的解决方案，具有重要的工程实践意义。"}}
{"id": "2506.08496", "title": "CoQMoE: Co-Designed Quantization and Computation Orchestration for Mixture-of-Experts Vision Transformer on FPGA", "authors": ["Jiale Dong", "Hao Wu", "Zihao Wang", "Wenqi Lou", "Zhendong Zheng", "Lei Gong", "Chao Wang", "Xuehai Zhou"], "summary": "Vision Transformers (ViTs) exhibit superior performance in computer vision\ntasks but face deployment challenges on resource-constrained devices due to\nhigh computational/memory demands. While Mixture-of-Experts Vision Transformers\n(MoE-ViTs) mitigate this through a scalable architecture with sub-linear\ncomputational growth, their hardware implementation on FPGAs remains\nconstrained by resource limitations. This paper proposes a novel accelerator\nfor efficiently implementing quantized MoE models on FPGAs through two key\ninnovations: (1) A dual-stage quantization scheme combining\nprecision-preserving complex quantizers with hardware-friendly simplified\nquantizers via scale reparameterization, with only 0.28 $\\%$ accuracy loss\ncompared to full precision; (2) A resource-aware accelerator architecture\nfeaturing latency-optimized streaming attention kernels and reusable linear\noperators, effectively balancing performance and resource consumption.\nExperimental results demonstrate that our accelerator achieves nearly 155\nframes per second, a 5.35$\\times$ improvement in throughput, and over $80\\%$\nenergy reduction compared to state-of-the-art (SOTA) FPGA MoE accelerators,\nwhile maintaining $<1\\%$ accuracy loss across vision benchmarks. Our\nimplementation is available at https://github.com/DJ000011/CoQMoE.", "comment": "Accepted by Euro-Par 2025 (oral)", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.08496v1", "AI": {"title_translation": "CoQMoE：用于FPGA上专家混合视觉Transformer的协同设计量化与计算编排", "tldr": "本文提出CoQMoE，一种用于FPGA上MoE-ViT的高效加速器，通过双阶段量化和资源感知架构，显著提升了吞吐量并降低了能耗，同时保持高精度。", "motivation": "Vision Transformers (ViTs) 在计算机视觉任务中性能卓越，但计算和内存需求高，难以在资源受限设备上部署。专家混合视觉Transformer (MoE-ViTs) 虽通过可扩展架构缓解了计算增长，但其在FPGA上的硬件实现仍受资源限制。", "method": "本文提出了一种新颖的加速器，用于在FPGA上高效实现量化MoE模型。其包含两项关键创新：1) 双阶段量化方案，结合精度保持的复杂量化器与硬件友好的简化量化器，通过尺度重参数化实现，仅有0.28%的精度损失。2) 资源感知加速器架构，具有延迟优化的流式注意力核和可重用线性运算符，有效平衡性能和资源消耗。", "result": "实验结果表明，该加速器实现了近155帧每秒的速度，吞吐量提高了5.35倍，与SOTA FPGA MoE加速器相比能耗降低了80%以上，同时在视觉基准测试中保持了小于1%的精度损失。", "conclusion": "CoQMoE通过协同设计量化和计算编排，成功在FPGA上高效部署了专家混合视觉Transformer，显著提升了性能和能效。", "translation": "视觉Transformer (ViTs) 在计算机视觉任务中表现出卓越性能，但由于高计算/内存需求，在资源受限设备上部署面临挑战。虽然专家混合视觉Transformer (MoE-ViTs) 通过可扩展架构和亚线性计算增长缓解了这一问题，但它们在FPGA上的硬件实现仍受资源限制。本文提出了一种新颖的加速器，通过两项关键创新，在FPGA上高效实现量化MoE模型：(1) 一种双阶段量化方案，通过尺度重参数化将精度保持的复杂量化器与硬件友好的简化量化器相结合，与全精度相比仅有0.28%的精度损失；(2) 一种资源感知加速器架构，具有延迟优化的流式注意力核和可重用线性运算符，有效平衡了性能和资源消耗。实验结果表明，与最先进 (SOTA) 的FPGA MoE加速器相比，我们的加速器实现了近155帧每秒的速度，吞吐量提高了5.35倍，能耗降低了80%以上，同时在视觉基准测试中保持了小于1%的精度损失。我们的实现代码可在https://github.com/DJ000011/CoQMoE 获取。", "summary": "本文提出CoQMoE，一种针对FPGA上MoE-ViT的高效加速器，旨在解决MoE-ViT在资源受限设备上的部署挑战。CoQMoE通过创新的双阶段量化方案（精度损失仅0.28%）和资源感知的加速器架构（包含延迟优化流式注意力核和可重用线性运算符）实现性能与资源消耗的平衡。实验证明，CoQMoE在吞吐量、能效和精度方面均显著优于现有SOTA FPGA MoE加速器。", "keywords": "专家混合视觉Transformer, FPGA加速器, 量化, 计算编排, 视觉Transformer", "comments": "该论文的创新点在于其协同设计方法，将量化策略与硬件架构紧密结合。双阶段量化方案在保证精度的同时兼顾了硬件友好性，而资源感知架构则有效利用了FPGA资源。这对于ViTs和MoE-ViTs在边缘设备上的实际部署具有重要意义，克服了计算和内存瓶颈。"}}
{"id": "2506.08987", "title": "Rapid cardiac activation prediction for cardiac resynchronization therapy planning using geometric deep learning", "authors": ["Ehsan Naghavi", "Haifeng Wang", "Vahid Ziaei Rad", "Julius Guccione", "Ghassan Kassab", "Vishnu Boddeti", "Seungik Baek", "Lik-Chuan Lee"], "summary": "Cardiac resynchronization therapy (CRT) is a common intervention for patients\nwith dyssynchronous heart failure, yet approximately one-third of recipients\nfail to respond due to suboptimal lead placement. Identifying optimal pacing\nsites remains challenging, largely due to patient-specific anatomical\nvariability and the limitations of current individualized planning strategies.\nIn a step towards constructing an in-silico approach to help address this\nissue, we develop two geometric deep learning (DL) models, based on graph\nneural network (GNN) and geometry-informed neural operator (GINO), to predict\ncardiac activation time map in real-time for CRT planning and optimization.\nBoth models are trained on a large synthetic dataset generated from\nfinite-element (FE) simulations over a wide range of left ventricular (LV)\ngeometries, pacing site configurations, and tissue conductivities. The GINO\nmodel significantly outperforms the GNN model, with lower prediction errors\n(1.14% vs 3.14%) and superior robustness to noise and various mesh\ndiscretization. Using the GINO model, we also develop a workflow for optimizing\nthe pacing site in CRT from given activation time map and LV geometry. Compared\nto randomly selecting a pacing site, the CRT optimization workflow produces a\nlarger reduction in maximum activation time (20% vs. 8%). In conjunction with\nan interactive web-based graphical user interface (GUI) available at\nhttps://dcsim.egr.msu.edu/, the GINO model shows promising potential as a\nclinical decision-support tool for personalized pre-procedural CRT\noptimization.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.08987v1", "AI": {"title_translation": "几何深度学习在心脏再同步治疗规划中快速心脏激活预测的应用", "tldr": "本文开发了几何深度学习模型（GINO和GNN）来实时预测心脏激活时间图，以优化心脏再同步治疗（CRT）的起搏位点，其中GINO模型表现更优，并有望作为临床决策支持工具。", "motivation": "心脏再同步治疗（CRT）约有三分之一的患者因导线放置不佳而无响应，识别最佳起搏位点仍然具有挑战性，这主要是由于患者特定的解剖变异性和当前个体化规划策略的局限性。", "method": "开发了两个基于图神经网络（GNN）和几何感知神经算子（GINO）的几何深度学习模型，用于实时预测心脏激活时间图。模型在一个大型合成数据集上进行训练，该数据集通过有限元（FE）模拟生成，涵盖了广泛的左心室（LV）几何形状、起搏位点配置和组织电导率。还开发了一个利用GINO模型优化CRT起搏位点的工作流程。", "result": "GINO模型显著优于GNN模型，预测误差更低（1.14% vs 3.14%），并且对噪声和各种网格离散化具有卓越的鲁棒性。与随机选择起搏位点相比，CRT优化工作流程在最大激活时间方面产生了更大的减少（20% vs 8%）。", "conclusion": "GINO模型结合交互式网络图形用户界面，作为个性化术前CRT优化的临床决策支持工具，显示出有前景的潜力。", "translation": "心脏再同步治疗（CRT）是治疗心力衰竭失同步患者的常见干预措施，然而约有三分之一的患者因导线放置不佳而未能响应。识别最佳起搏位点仍然具有挑战性，这在很大程度上是由于患者特定的解剖变异性以及当前个体化规划策略的局限性。为了构建一种体外（in-silico）方法来帮助解决这个问题，我们开发了两个几何深度学习（DL）模型，基于图神经网络（GNN）和几何感知神经算子（GINO），以实时预测心脏激活时间图，用于CRT规划和优化。这两个模型都在一个大型合成数据集上进行训练，该数据集通过有限元（FE）模拟生成，涵盖了广泛的左心室（LV）几何形状、起搏位点配置和组织电导率。GINO模型显著优于GNN模型，预测误差更低（1.14% vs 3.14%），并且对噪声和各种网格离散化具有卓越的鲁棒性。使用GINO模型，我们还开发了一个从给定激活时间图和左心室几何形状优化CRT起搏位点的工作流程。与随机选择起搏位点相比，CRT优化工作流程在最大激活时间方面产生了更大的减少（20% vs 8%）。结合可在https://dcsim.egr.msu.edu/访问的交互式基于网络的图形用户界面（GUI），GINO模型作为个性化术前CRT优化的临床决策支持工具，显示出有前景的潜力。", "summary": "本文提出了一种使用几何深度学习模型（GNN和GINO）来实时预测心脏激活时间图的方法，以改进心脏再同步治疗（CRT）的规划和优化。通过在大量有限元模拟生成的合成数据上训练，研究发现GINO模型在预测精度和鲁棒性方面均优于GNN模型。此外，基于GINO模型开发的CRT优化工作流程能够显著减少最大激活时间。该模型与交互式GUI结合，有望成为个性化CRT优化的临床决策支持工具。", "keywords": "心脏再同步治疗, 几何深度学习, 激活时间预测, 图神经网络, 神经算子", "comments": "这项研究通过引入几何深度学习，特别是GINO模型，为心脏再同步治疗（CRT）的个性化规划提供了创新的解决方案。通过在大量合成数据上训练并展示出优越的预测性能和鲁棒性，该方法有望克服现有CRT规划中识别最佳起搏位点所面临的挑战。结合交互式GUI的开发，该技术在转化为临床决策支持工具方面具有重要意义，有助于提高CRT的响应率并改善患者预后。"}}
{"id": "2506.08280", "title": "Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing", "authors": ["Daniel H. Pak", "Shubh Thaker", "Kyle Baylous", "Xiaoran Zhang", "Danny Bluestein", "James S. Duncan"], "summary": "High-quality volumetric meshing from medical images is a key bottleneck for\nphysics-based simulations in personalized medicine. For volumetric meshing of\ncomplex medical structures, recent studies have often utilized deep learning\n(DL)-based template deformation approaches to enable fast test-time generation\nwith high spatial accuracy. However, these approaches still exhibit\nlimitations, such as limited flexibility at high-curvature areas and\nunrealistic inter-part distances. In this study, we introduce a simple yet\neffective snap-and-tune strategy that sequentially applies DL and test-time\noptimization, which combines fast initial shape fitting with more detailed\nsample-specific mesh corrections. Our method provides significant improvements\nin both spatial accuracy and mesh quality, while being fully automated and\nrequiring no additional training labels. Finally, we demonstrate the\nversatility and usefulness of our newly generated meshes via solid mechanics\nsimulations in two different software platforms. Our code is available at\nhttps://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08280v1", "AI": {"title_translation": "即时调整：结合深度学习和测试时优化实现高保真心血管体积网格划分", "tldr": "一种新的“即时调整”方法结合深度学习和测试时优化，用于创建高质量心血管体积网格，提高了精度和网格质量，并实现了自动化。", "motivation": "从医学图像生成高质量体积网格是个性化医疗中基于物理的模拟的关键瓶颈。现有的基于深度学习的模板变形方法在高曲率区域的灵活性有限以及部件间距离不切实际等问题。", "method": "本研究引入了一种简单而有效的“即时调整”（snap-and-tune）策略，该策略顺序应用深度学习进行快速初始形状拟合，并应用测试时优化进行更详细的样本特定网格校正。该方法完全自动化，无需额外的训练标签。", "result": "该方法在空间精度和网格质量方面都提供了显著改进。新生成的网格具有多功能性和实用性，通过在两个不同软件平台上的固体力学模拟得到了验证。", "conclusion": "“即时调整”策略是一种简单而有效的方法，它结合了深度学习和测试时优化，解决了以往方法的局限性，为心血管模拟提供了高质量、高精度和多功能的网格。", "translation": "从医学图像生成高质量体积网格是个性化医疗中基于物理的模拟的关键瓶颈。对于复杂医疗结构的体积网格划分，最近的研究经常利用基于深度学习（DL）的模板变形方法，以实现快速的测试时生成和高空间精度。然而，这些方法仍然存在局限性，例如在高曲率区域的灵活性有限和不切实际的部件间距离。在本研究中，我们引入了一种简单而有效的即时调整（snap-and-tune）策略，该策略顺序应用深度学习和测试时优化，将快速初始形状拟合与更详细的样本特定网格校正相结合。我们的方法在空间精度和网格质量方面都提供了显著改进，同时完全自动化且无需额外的训练标签。最后，我们通过在两个不同软件平台上的固体力学模拟，证明了我们新生成的网格的多功能性和实用性。我们的代码可在https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh 获取。", "summary": "本文提出了一种新颖的“即时调整”（snap-and-tune）策略，用于从医学图像生成高保真心血管体积网格，解决了现有基于深度学习方法的局限性。该方法顺序结合深度学习进行快速初始形状拟合，并结合测试时优化进行详细的网格校正。这种完全自动化的方法显著提高了空间精度和网格质量，并通过在固体力学模拟中的应用证明了其价值，使其成为个性化医疗中基于物理模拟的重要工具。", "keywords": "深度学习, 体积网格划分, 心血管, 测试时优化, 医学图像", "comments": "“即时调整”策略的创新之处在于它结合了深度学习的速度和测试时优化的精度，有效解决了医学图像处理中常见的权衡问题。其完全自动化且不依赖额外训练标签的特性，对于实际应用，特别是在数据标注具有挑战性的个性化医疗领域，具有显著优势。在不同模拟平台上的多功能性展示进一步突出了其潜在影响。"}}
{"id": "2506.08528", "title": "PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production", "authors": ["Yu Guan", "Zhiyu Yin", "Haoyu Chen", "Sheng Cheng", "Chaojie Yang", "Tianyin Xu", "Yang Zhang", "Hanyu Zhao", "Yong Li", "Dennis Cai", "Ennan Zhai"], "summary": "Troubleshooting performance problems of large model training (LMT) is\nimmensely challenging, due to unprecedented scales of modern GPU clusters, the\ncomplexity of software-hardware interactions, and the data intensity of the\ntraining process. Existing troubleshooting approaches designed for traditional\ndistributed systems or datacenter networks fall short and can hardly apply to\nreal-world training systems. In this paper, we present PerfTracker, the first\nonline troubleshooting system utilizing fine-grained profiling, to diagnose\nperformance issues of large-scale model training in production. PerfTracker can\ndiagnose performance issues rooted in both hardware (e.g., GPUs and their\ninterconnects) and software (e.g., Python functions and GPU operations). It\nscales to LMT on modern GPU clusters. PerfTracker effectively summarizes\nruntime behavior patterns of fine-grained LMT functions via online profiling,\nand leverages differential observability to localize the root cause with\nminimal production impact. PerfTracker has been deployed as a production\nservice for large-scale GPU clusters of O(10, 000) GPUs (product homepage\nhttps://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).\nIt has been used to diagnose a variety of difficult performance issues.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.08528v1", "AI": {"title_translation": "PerfTracker：生产环境中大规模模型训练的在线性能故障排除", "tldr": "PerfTracker是一个在线故障排除系统，利用细粒度分析来诊断大规模模型训练中的性能问题，已在数万个GPU集群上部署并有效。", "motivation": "由于现代GPU集群的空前规模、软硬件交互的复杂性以及训练过程的数据密集性，排除大规模模型训练（LMT）的性能问题极具挑战。现有针对传统分布式系统或数据中心网络的方法不足以应用于实际训练系统。", "method": "本文提出了PerfTracker，首个利用细粒度分析的在线故障排除系统，用于诊断生产环境中的大规模模型训练性能问题。PerfTracker通过在线分析有效总结细粒度LMT函数的运行时行为模式，并利用差异可观察性以最小的生产影响定位根本原因，能够诊断硬件和软件层面的性能问题。", "result": "PerfTracker已作为生产服务部署在O(10,000)个GPU的大规模GPU集群中，并已用于诊断各种困难的性能问题。", "conclusion": "PerfTracker成功解决了大规模模型训练在生产环境中复杂的性能故障排除挑战，通过在线细粒度分析和差异可观察性提供了有效的解决方案。", "translation": "大规模模型训练（LMT）的性能问题排查极具挑战性，原因在于现代GPU集群的空前规模、软硬件交互的复杂性以及训练过程的数据密集性。现有针对传统分布式系统或数据中心网络的故障排除方法已不足以应对，难以应用于实际的训练系统。在本文中，我们提出了PerfTracker，这是第一个利用细粒度分析的在线故障排除系统，用于诊断生产环境中的大规模模型训练性能问题。PerfTracker能够诊断源于硬件（例如GPU及其互连）和软件（例如Python函数和GPU操作）的性能问题。它可扩展到现代GPU集群上的LMT。PerfTracker通过在线分析有效地总结了细粒度LMT函数的运行时行为模式，并利用差异可观察性以最小的生产影响定位根本原因。PerfTracker已作为生产服务部署在O(10,000)个GPU的大规模GPU集群中（产品主页https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool）。它已被用于诊断各种困难的性能问题。", "summary": "PerfTracker是一个为大规模模型训练（LMT）设计的在线性能故障排除系统，旨在解决现有方法在现代GPU集群和复杂软硬件交互下的不足。它通过细粒度在线分析和差异可观察性，有效诊断并定位LMT中的硬件和软件性能问题。该系统已在数万个GPU的生产环境中部署，并成功解决了多种性能难题。", "keywords": "大规模模型训练, 性能故障排除, 在线分析, GPU集群, PerfTracker", "comments": "PerfTracker的创新之处在于它是首个将细粒度在线分析应用于大规模模型训练性能故障排除的系统，并且已在阿里云的大规模GPU集群中得到实际部署和验证，解决了生产环境中的真实痛点，具有重要的实用价值和影响力。"}}
{"id": "2506.08239", "title": "Low-Cost Wideband Tilted Beam Antenna for Millimeter-wave Vehicle Applications", "authors": ["Jiawang Li"], "summary": "To facilitate vehicle coverage for millimeter-wave applications, this\ncommunication presents a low-cost, wideband tilted-beam antenna. A novel design\nis proposed in which a slot antenna is both directly excited and\nelectromagnetically coupled to a monopole array. This slot-monopole\nconfiguration is inherently robust against substrate losses, enabling low-cost\nfabrication while maintaining high realized gain and compact size. Furthermore,\nthe slot-fed structure effectively excites multiple resonant modes within the\nmonopole array, resulting in a significantly enhanced bandwidth. Experimental\nresults demonstrate that the antenna achieves a -10-dB impedance bandwidth of\nover 76.5% (20-44.78 GHz) and a peak realized gain of 6.1 dBi.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.08239v1", "AI": {"title_translation": "毫米波车载应用的低成本宽带倾斜波束天线", "tldr": "提出了一种新型低成本宽带倾斜波束天线，用于毫米波车载应用，具有高增益和宽带宽（20-44.78 GHz）。", "motivation": "旨在为毫米波车载应用提供车辆覆盖。", "method": "提出了一种新颖的设计，其中缝隙天线既直接激励又电磁耦合到单极子阵列。这种缝隙-单极子配置对衬底损耗具有固有的鲁棒性，从而实现低成本制造，同时保持高实现增益和紧凑尺寸。缝隙馈电结构有效激发单极子阵列内的多个谐振模式，显著增强了带宽。", "result": "天线实现了超过76.5%（20-44.78 GHz）的-10dB阻抗带宽，峰值实现增益为6.1 dBi。", "conclusion": "该天线成功地为毫米波车载应用提供了一种低成本、宽带、高增益和紧凑的解决方案。", "translation": "为了促进毫米波应用的车辆覆盖，本文提出了一种低成本、宽带倾斜波束天线。提出了一种新颖的设计，其中缝隙天线既直接激励又电磁耦合到单极子阵列。这种缝隙-单极子配置对衬底损耗具有固有的鲁棒性，从而实现低成本制造，同时保持高实现增益和紧凑尺寸。此外，缝隙馈电结构有效激发单极子阵列内的多个谐振模式，从而显著增强了带宽。实验结果表明，该天线实现了超过76.5%（20-44.78 GHz）的-10dB阻抗带宽，峰值实现增益为6.1 dBi。", "summary": "本文介绍了一种用于毫米波车载应用的低成本宽带倾斜波束天线。该天线采用新颖的缝隙天线直接激励并电磁耦合到单极子阵列的设计。这种结构对衬底损耗具有鲁棒性，有助于低成本制造、高增益和紧凑尺寸。通过激发多谐振模式，该天线实现了显著的带宽增强。实验结果验证了其在20-44.78 GHz频率范围内超过76.5%的带宽和6.1 dBi的峰值增益。", "keywords": "毫米波天线, 宽带, 倾斜波束, 低成本, 车载应用", "comments": "这篇论文提出了一种创新的缝隙-单极子天线设计，其亮点在于通过结构优化实现了低成本制造、宽带宽和高增益的结合，同时保持了紧凑尺寸。特别是在毫米波车载通信领域，这种集成性能对于实际部署具有重要意义。其对衬底损耗的鲁棒性是其创新点之一。"}}
{"id": "2506.08633", "title": "Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs", "authors": ["Šimon Sedláček", "Bolaji Yusuf", "Ján Švec", "Pradyoth Hegde", "Santosh Kesiraju", "Oldřich Plchot", "Jan Černocký"], "summary": "In this work, we approach spoken Dialogue State Tracking (DST) by bridging\nthe representation spaces of speech encoders and LLMs via a small connector\nmodule, with a focus on fully open-sourced and open-data components\n(WavLM-large, OLMo). We focus on ablating different aspects of such systems\nincluding full/LoRA adapter fine-tuning, the effect of agent turns in the\ndialogue history, as well as fuzzy matching-based output post-processing, which\ngreatly improves performance of our systems on named entities in the dialogue\nslot values. We conduct our experiments on the SpokenWOZ dataset, and\nadditionally utilize the Speech-Aware MultiWOZ dataset to augment our training\ndata. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned\nmodels achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our\nsystem with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%\nJGA on SpokenWOZ test.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.08633v1", "AI": {"title_translation": "通过对齐语音编码器和大型语言模型来处理对话状态跟踪", "tldr": "本文通过一个小型连接器模块，将语音编码器与大型语言模型对齐，以处理口语对话状态跟踪，并在SpokenWOZ数据集上实现了最先进的性能。", "motivation": "本文旨在通过连接语音编码器和大型语言模型的表示空间，以完全开源和开放数据组件为重点，改进口语对话状态跟踪（DST）。", "method": "本文通过一个小型连接器模块来桥接语音编码器（如WavLM-large）和大型语言模型（如OLMo）的表示空间。研究人员对系统不同方面进行了消融实验，包括完全/LoRA适配器微调、对话历史中代理轮次的影响，以及基于模糊匹配的输出后处理。实验在SpokenWOZ数据集上进行，并利用Speech-Aware MultiWOZ数据集进行数据增强。", "result": "最佳的WavLM + 连接器 + OLMo-1B对齐模型在SpokenWOZ测试集上实现了34.66%的JGA，达到了最先进水平。使用Gemma-2-9B-instruct的系统进一步将该结果提升至42.17%的JGA，再次刷新了SpokenWOZ测试集的最佳性能。", "conclusion": "通过连接器模块对齐语音编码器和大型语言模型的方法，结合特定的微调和后处理技术，对于口语对话状态跟踪非常有效，并取得了新的最先进结果。", "translation": "本文旨在通过一个小型连接器模块，桥接语音编码器和大型语言模型（LLM）的表示空间，从而解决口语对话状态跟踪（DST）问题，并专注于完全开源和开放数据组件（如WavLM-large, OLMo）。我们重点对这类系统的不同方面进行消融实验，包括完全/LoRA适配器微调、对话历史中代理轮次的影响，以及基于模糊匹配的输出后处理，后者极大地提高了我们系统在对话槽值中命名实体上的性能。我们在SpokenWOZ数据集上进行了实验，并额外利用Speech-Aware MultiWOZ数据集来扩充训练数据。最终，我们性能最佳的WavLM + 连接器 + OLMo-1B对齐模型在SpokenWOZ测试集上取得了最先进的性能（34.66% JGA），而我们使用Gemma-2-9B-instruct的系统进一步超越了这一结果，在SpokenWOZ测试集上达到了42.17% JGA。", "summary": "本文提出了一种新颖的口语对话状态跟踪（DST）方法，通过一个小型连接器模块对齐语音编码器（如WavLM-large）和大型语言模型（LLM，如OLMo-1B，Gemma-2-9B-instruct）。作者研究了包括微调策略、代理轮次的作用以及用于输出后处理的模糊匹配等多个方面，并专注于开源组件。在SpokenWOZ数据集（并使用Speech-Aware MultiWOZ进行数据增强）上的实验表明，他们性能最佳的模型取得了最先进的性能，其中使用Gemma-2-9B-instruct的系统达到了42.17%的JGA。", "keywords": "对话状态跟踪, 语音编码器, 大型语言模型, WavLM, SpokenWOZ, 模糊匹配", "comments": "该论文的创新之处在于通过一个简单的连接器模块，有效地桥接了语音编码器和大型语言模型，从而解决了口语对话状态跟踪问题，这是对话系统中多模态理解的重要一步。对开源组件的关注和详细的消融研究增加了其实用价值和可复现性。取得新的最先进结果证明了其对齐方法和后处理技术（特别是针对命名实体识别）的有效性。"}}
{"id": "2506.08336", "title": "Your Agent Can Defend Itself against Backdoor Attacks", "authors": ["Li Changjiang", "Liang Jiacheng", "Cao Bochuan", "Chen Jinghui", "Wang Ting"], "summary": "Despite their growing adoption across domains, large language model\n(LLM)-powered agents face significant security risks from backdoor attacks\nduring training and fine-tuning. These compromised agents can subsequently be\nmanipulated to execute malicious operations when presented with specific\ntriggers in their inputs or environments. To address this pressing risk, we\npresent ReAgent, a novel defense against a range of backdoor attacks on\nLLM-based agents. Intuitively, backdoor attacks often result in inconsistencies\namong the user's instruction, the agent's planning, and its execution. Drawing\non this insight, ReAgent employs a two-level approach to detect potential\nbackdoors. At the execution level, ReAgent verifies consistency between the\nagent's thoughts and actions; at the planning level, ReAgent leverages the\nagent's capability to reconstruct the instruction based on its thought\ntrajectory, checking for consistency between the reconstructed instruction and\nthe user's instruction. Extensive evaluation demonstrates ReAgent's\neffectiveness against various backdoor attacks across tasks. For instance,\nReAgent reduces the attack success rate by up to 90\\% in database operation\ntasks, outperforming existing defenses by large margins. This work reveals the\npotential of utilizing compromised agents themselves to mitigate backdoor\nrisks.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08336v1", "AI": {"title_translation": "你的智能体可以抵御后门攻击", "tldr": "大型语言模型（LLM）驱动的智能体面临后门攻击的重大安全风险。本文提出了ReAgent，一种新颖的防御机制，通过在执行和规划层面检查智能体的思考与行为、以及指令与重建指令之间的一致性来检测后门。实验证明ReAgent能有效降低攻击成功率，例如在数据库操作任务中降低高达90%。", "motivation": "大型语言模型（LLM）驱动的智能体在训练和微调过程中面临后门攻击带来的重大安全风险。这些被破坏的智能体在接收到特定输入或环境中的触发器时，可能会被操纵执行恶意操作。", "method": "本文提出了ReAgent，一种针对LLM智能体后门攻击的新型防御机制。ReAgent采用双层方法来检测潜在的后门：在执行层面，ReAgent验证智能体思维和行动之间的一致性；在规划层面，ReAgent利用智能体根据其思维轨迹重建指令的能力，检查重建指令与用户指令之间的一致性。", "result": "广泛的评估表明ReAgent在各种任务中对多种后门攻击有效。例如，在数据库操作任务中，ReAgent将攻击成功率降低了高达90%，大幅优于现有防御措施。", "conclusion": "这项工作揭示了利用受损智能体本身来减轻后门风险的潜力。", "translation": "尽管大型语言模型（LLM）驱动的智能体在各个领域日益普及，但它们在训练和微调过程中面临后门攻击带来的重大安全风险。这些被破坏的智能体在接收到特定输入或环境中的触发器时，可能会被操纵执行恶意操作。为了解决这一紧迫风险，我们提出了ReAgent，一种针对LLM智能体上各种后门攻击的新型防御机制。直观地说，后门攻击通常导致用户指令、智能体规划和执行之间出现不一致。基于这一洞察，ReAgent采用双层方法来检测潜在的后门。在执行层面，ReAgent验证智能体思维和行动之间的一致性；在规划层面，ReAgent利用智能体根据其思维轨迹重建指令的能力，检查重建指令与用户指令之间的一致性。广泛的评估表明ReAgent在各种任务中对多种后门攻击有效。例如，在数据库操作任务中，ReAgent将攻击成功率降低了高达90%，大幅优于现有防御措施。这项工作揭示了利用受损智能体本身来减轻后门风险的潜力。", "summary": "本文提出了一种名为ReAgent的新型防御机制，旨在解决大型语言模型（LLM）驱动智能体面临的后门攻击风险。ReAgent基于后门攻击导致指令、规划和执行不一致的洞察，采用双层方法检测后门：在执行层面检查思维与行动的一致性，在规划层面检查重建指令与用户指令的一致性。实验证明，ReAgent能有效降低各种后门攻击的成功率，在数据库操作任务中甚至能将攻击成功率降低高达90%，显著优于现有防御手段，并展示了利用智能体自身能力抵御后门风险的潜力。", "keywords": "LLM智能体, 后门攻击, ReAgent, 防御, 一致性检测", "comments": "这项工作非常创新，它利用了被攻击智能体本身的“自我防御”能力，通过检测内部一致性来识别并抵御后门攻击。其双层检测机制设计巧妙，且在性能上显著优于现有防御措施，为LLM智能体的安全防护提供了新的思路和有效方案。"}}
{"id": "2506.08173", "title": "Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles", "authors": ["Nguyen Phu Vinh", "Anh Chung Hoang", "Chris Ngo", "Truong-Son Hy"], "summary": "Large Language Models (LLMs) have shown strong capabilities in code\ngeneration and comprehension, yet their application to complex software\nengineering tasks often suffers from low precision and limited\ninterpretability. We present Repeton, a fully open-source framework that\nleverages LLMs for precise and automated code manipulation in real-world Git\nrepositories. Rather than generating holistic fixes, Repeton operates through a\nstructured patch-and-test pipeline: it iteratively diagnoses issues, proposes\ncode changes, and validates each patch through automated testing. This stepwise\nprocess is guided by lightweight heuristics and development tools, avoiding\nreliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite\nbenchmark, our method shows good performance compared to RAG-based methods in\nboth patch validity and interpretability. By decomposing software engineering\ntasks into modular, verifiable stages, Repeton provides a practical path toward\nscalable and transparent autonomous debugging.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08173v1", "AI": {"title_translation": "Repeton：ReAct引导的补丁测试循环结构化错误修复", "tldr": "Repeton是一个开源框架，利用LLM通过结构化的补丁测试循环实现精确自动的代码修复，在SWE-bench Lite上表现优于RAG方法，并提高了可解释性。", "motivation": "大型语言模型（LLMs）在代码生成和理解方面表现出强大能力，但其在复杂软件工程任务中的应用往往精度低且可解释性有限。", "method": "Repeton是一个完全开源的框架，它不生成整体修复，而是通过结构化的补丁测试管道运行：迭代诊断问题，提出代码更改，并通过自动化测试验证每个补丁。这个逐步过程由轻量级启发式方法和开发工具指导，避免依赖基于嵌入的检索系统。", "result": "在SWE-bench Lite基准测试中，Repeton的方法在补丁有效性和可解释性方面与基于RAG的方法相比表现出良好性能。", "conclusion": "通过将软件工程任务分解为模块化、可验证的阶段，Repeton为可扩展和透明的自主调试提供了一条实用路径。", "translation": "大型语言模型（LLMs）在代码生成和理解方面表现出强大能力，但其在复杂软件工程任务中的应用往往精度低且可解释性有限。我们提出了Repeton，一个完全开源的框架，它利用LLMs在实际Git仓库中进行精确和自动化的代码操作。Repeton不生成整体修复，而是通过结构化的补丁测试管道运行：它迭代诊断问题，提出代码更改，并通过自动化测试验证每个补丁。这个逐步过程由轻量级启发式方法和开发工具指导，避免依赖基于嵌入的检索系统。在SWE-bench Lite基准测试中，我们的方法在补丁有效性和可解释性方面与基于RAG的方法相比表现出良好性能。通过将软件工程任务分解为模块化、可验证的阶段，Repeton为可扩展和透明的自主调试提供了一条实用路径。", "summary": "Repeton是一个开源框架，旨在解决LLM在复杂软件工程任务中精度低和可解释性有限的问题。它采用结构化的补丁测试循环，迭代诊断、提出代码更改并验证补丁，避免依赖嵌入式检索系统，而是由启发式方法和开发工具指导。在SWE-bench Lite基准测试中，Repeton在补丁有效性和可解释性方面优于RAG方法，为实现可扩展和透明的自主调试提供了可行方案。", "keywords": "Repeton, 错误修复, LLMs, 补丁测试, 自主调试", "comments": "Repeton的创新之处在于其结构化的“补丁测试循环”方法，它将复杂的错误修复任务分解为可管理、可验证的步骤。这种迭代式、由启发式指导的过程，避免了对嵌入式检索系统的依赖，从而可能提高了修复的精度和可解释性。这对于推动自主调试技术的发展具有重要意义，尤其是在需要高可靠性和透明度的软件工程领域。"}}
{"id": "2506.08149", "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08149v1", "AI": {"title_translation": "自动驾驶中基于自我中心的通信世界模型学习", "tldr": "针对自动驾驶等复杂环境中的多智能体强化学习（MARL）问题，本文提出了一个名为CALL的通信世界模型，通过学习低维潜在表示并进行轻量级信息共享，解决了部分可观测性和非平稳性问题，并提升了预测准确性和规划性能。", "motivation": "解决多智能体强化学习（MARL）在复杂高维环境（如自动驾驶）中面临的部分可观测性和非平稳性问题，以及传统信息共享方法存在的通信开销过大和可扩展性差的挑战。", "method": "提出了一种名为“CALL”（Communicative World Model）的MARL框架。该框架包含两个主要部分：1) 每个智能体首先学习其世界模型，将其自身状态和意图编码为低维潜在表示，以减少内存占用，并通过轻量级通信与其他相关智能体共享；2) 每个智能体进行以自我为中心的学习，同时利用轻量级信息共享来丰富其世界模型，并利用其泛化能力提高预测准确性，从而改善规划。", "result": "通过信息共享，预测准确性得到提升，并缩小了性能差距。在CARLA平台上的挑战性局部轨迹规划任务中进行了大量实验，证明了使用CALL带来了性能提升。", "conclusion": "CALL框架通过轻量级信息共享和以自我为中心的学习，有效解决了MARL在复杂环境中的挑战，显著提升了自动驾驶任务中的预测准确性和规划性能。", "translation": "我们研究了复杂高维环境（如自动驾驶）中任务的多智能体强化学习（MARL）。众所周知，MARL 存在“部分可观测性”和“非平稳性”问题。为了应对这些挑战，通常采用信息共享，然而在实践中，这面临着主要的障碍，包括巨大的通信开销和可扩展性问题。通过利用世界模型及其潜在表示中体现的生成式 AI，我们为 MARL 开发了“CALL”（通信世界模型），其中 1) 每个智能体首先学习其世界模型，将其状态和意图编码为内存占用较小的低维潜在表示，可以通过轻量级通信与感兴趣的其他智能体共享；2) 每个智能体在利用轻量级信息共享来丰富其世界模型的同时，进行以自我为中心的学习，然后利用其泛化能力来改进预测，从而实现更好的规划。我们描述了信息共享在预测准确性方面的增益及其对性能差距的影响。在 CARLA 平台上的挑战性局部轨迹规划任务中进行了大量实验，以证明使用 CALL 的性能增益。", "summary": "本文针对自动驾驶等复杂环境中的多智能体强化学习（MARL）挑战，特别是部分可观测性、非平稳性以及传统信息共享的通信开销问题，提出了一种名为“CALL”（Communicative World Model）的框架。CALL使每个智能体学习其世界模型，将状态和意图编码为低维潜在表示，并通过轻量级通信进行共享。智能体通过以自我为中心的学习并利用共享信息来丰富世界模型，从而提高预测准确性并优化规划。实验证明，CALL在CARLA平台上的局部轨迹规划任务中展现了显著的性能提升。", "keywords": "多智能体强化学习, 世界模型, 通信, 自动驾驶, 潜在表示", "comments": "CALL的创新之处在于将生成式AI（世界模型）与轻量级通信结合，解决了多智能体强化学习中的核心挑战，即部分可观测性和通信效率问题。通过学习低维潜在表示并进行ego-centric学习，CALL有效地平衡了信息共享的必要性与实际应用中的通信开销和可扩展性限制，这对于自动驾驶等需要实时高效协作的领域具有重要意义。"}}
{"id": "2506.08137", "title": "IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation", "authors": ["Oishee Bintey Hoque", "Abhijin Adiga", "Aniruddha Adiga", "Siddharth Chaudhary", "Madhav V. Marathe", "S. S. Ravi", "Kirti Rajagopalan", "Amanda Wilson", "Samarth Swarup"], "summary": "Accurate canal network mapping is essential for water management, including\nirrigation planning and infrastructure maintenance. State-of-the-art semantic\nsegmentation models for infrastructure mapping, such as roads, rely on large,\nwell-annotated remote sensing datasets. However, incomplete or inadequate\nground truth can hinder these learning approaches. Many infrastructure networks\nhave graph-level properties such as reachability to a source (like canals) or\nconnectivity (roads) that can be leveraged to improve these existing ground\ntruth. This paper develops a novel iterative framework IGraSS, combining a\nsemantic segmentation module-incorporating RGB and additional modalities (NDWI,\nDEM)-with a graph-based ground-truth refinement module. The segmentation module\nprocesses satellite imagery patches, while the refinement module operates on\nthe entire data viewing the infrastructure network as a graph. Experiments show\nthat IGraSS reduces unreachable canal segments from around 18% to 3%, and\ntraining with refined ground truth significantly improves canal identification.\nIGraSS serves as a robust framework for both refining noisy ground truth and\nmapping canal networks from remote sensing imagery. We also demonstrate the\neffectiveness and generalizability of IGraSS using road networks as an example,\napplying a different graph-theoretic constraint to complete road networks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08137v1", "AI": {"title_translation": "IGraSS：通过迭代图约束语义分割从卫星图像中学习识别基础设施网络", "tldr": "IGraSS是一个迭代框架，结合语义分割和图基真值细化，用于从卫星图像中准确识别运河和道路等基础设施网络，显著减少了运河段的不可达性。", "motivation": "准确的运河网络测绘对于水资源管理至关重要，而现有语义分割模型依赖于大量标注数据，但数据集不完整或不充分的真值会阻碍学习。许多基础设施网络具有图级别属性（如可达性、连通性），可以利用这些属性改进现有真值。", "method": "本文开发了一个名为IGraSS的迭代框架，它结合了两个模块：一个语义分割模块（结合RGB和NDWI、DEM等额外模态处理卫星图像块）和一个图基真值细化模块（在整个数据上将基础设施网络视为图进行操作）。", "result": "实验表明，IGraSS将不可达运河段的比例从约18%降低到3%。使用细化后的真值进行训练显著提高了运河识别的准确性。该框架还被证明对道路网络有效且具有通用性。", "conclusion": "IGraSS是一个强大的框架，既能细化有噪声的真值，又能从遥感图像中测绘运河网络。它通过结合语义分割和图约束真值细化，有效解决了遥感图像中基础设施网络识别的挑战。", "translation": "准确的运河网络测绘对于灌溉规划和基础设施维护等水资源管理至关重要。用于基础设施测绘（如道路）的最新语义分割模型依赖于大型、标注良好的遥感数据集。然而，不完整或不充分的真值可能会阻碍这些学习方法。许多基础设施网络具有图级别属性，例如到源头（如运河）的可达性或连通性（道路），可以利用这些属性来改进现有真值。本文开发了一个新颖的迭代框架IGraSS，它结合了一个语义分割模块（包含RGB和额外的模态，如NDWI、DEM）和一个基于图的真值细化模块。分割模块处理卫星图像补丁，而细化模块则在整个数据上运行，将基础设施网络视为图。实验表明，IGraSS将不可达运河段从约18%减少到3%，并且使用细化后的真值进行训练显著改善了运河识别。IGraSS作为一个强大的框架，既能细化有噪声的真值，又能从遥感图像中测绘运河网络。我们还以道路网络为例，应用不同的图论约束来完成道路网络，从而证明了IGraSS的有效性和通用性。", "summary": "IGraSS是一种新颖的迭代框架，旨在通过结合语义分割和图约束真值细化，从卫星图像中准确识别运河和道路等基础设施网络。该方法解决了现有遥感数据集中真值不完整的问题，通过利用基础设施网络的图级别属性来改进标注。实验证明，IGraSS显著提高了运河识别的准确性，并减少了不可达运河段，同时展示了其在道路网络上的通用性。", "keywords": "基础设施网络, 语义分割, 图约束, 真值细化, 卫星图像", "comments": "IGraSS的创新之处在于其迭代框架，将图像级的语义分割与网络级的图约束真值细化相结合，有效解决了遥感数据中真值不足的问题。这种方法利用了基础设施网络的内在结构属性，提高了识别精度和鲁棒性，对于水资源管理和城市规划等领域具有重要意义。"}}
{"id": "2506.08662", "title": "Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization", "authors": ["Florian Borzechowski", "Michael Schäfer", "Heiko Schwarz", "Jonathan Pfaff", "Detlev Marpe", "Thomas Wiegand"], "summary": "The continuous improvements on image compression with variational\nautoencoders have lead to learned codecs competitive with conventional\napproaches in terms of rate-distortion efficiency. Nonetheless, taking the\nquantization into account during the training process remains a problem, since\nit produces zero derivatives almost everywhere and needs to be replaced with a\ndifferentiable approximation which allows end-to-end optimization. Though there\nare different methods for approximating the quantization, none of them model\nthe quantization noise correctly and thus, result in suboptimal networks.\nHence, we propose an additional finetuning training step: After conventional\nend-to-end training, parts of the network are retrained on quantized latents\nobtained at the inference stage. For entropy-constraint quantizers like\nTrellis-Coded Quantization, the impact of the quantizer is particularly\ndifficult to approximate by rounding or adding noise as the quantized latents\nare interdependently chosen through a trellis search based on both the entropy\nmodel and a distortion measure. We show that retraining on correctly quantized\ndata consistently yields additional coding gain for both uniform scalar and\nespecially for entropy-constraint quantization, without increasing inference\ncomplexity. For the Kodak test set, we obtain average savings between 1% and\n2%, and for the TecNick test set up to 2.2% in terms of Bj{\\o}ntegaard-Delta\nbitrate.", "comment": "Accepted at ICIP2024, the IEEE International Conference on Image\n  Processing", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08662v1", "AI": {"title_translation": "优化标量和熵约束量化上的学习图像压缩", "tldr": "本文提出了一种新的微调训练步骤，通过在推理阶段对量化后的潜在变量进行再训练，从而在不增加推理复杂性的情况下，显著提升了学习图像压缩的编码效率，尤其是在熵约束量化方面。", "motivation": "在学习图像压缩中，将量化纳入训练过程仍然是一个难题，因为量化会产生几乎处处为零的导数，需要用可微分近似来替代。然而，现有的近似方法都未能正确模拟量化噪声，导致网络性能不佳。", "method": "本文提出了一种额外的微调训练步骤：在传统的端到端训练之后，网络的部分内容在推理阶段获得的量化潜在变量上进行再训练。对于像格栅编码量化这样的熵约束量化器，该方法特别有效。", "result": "在Kodak测试集上，平均节省了1%到2%的Bj{\\o}ntegaard-Delta比特率，在TecNick测试集上，最高节省了2.2%。再训练在正确量化数据上始终能为均匀标量和熵约束量化带来额外的编码增益，且不增加推理复杂性。", "conclusion": "通过在推理阶段对量化后的潜在变量进行再训练的微调步骤，能够有效解决现有学习图像压缩中量化噪声建模不准确的问题，从而在不增加推理复杂性的前提下，显著提升了编码效率。", "translation": "变分自编码器在图像压缩方面的持续改进，使得学习型编解码器在码率-失真效率方面与传统方法具有竞争力。然而，在训练过程中考虑量化仍然是一个问题，因为它几乎在所有地方都产生零导数，需要用可微分近似来替代，以实现端到端优化。尽管存在不同的量化近似方法，但它们都未能正确模拟量化噪声，从而导致次优网络。因此，我们提出一个额外的微调训练步骤：在传统的端到端训练之后，网络的部分内容在推理阶段获得的量化潜在变量上进行再训练。对于像格栅编码量化这样的熵约束量化器，量化器的影响尤其难以通过舍入或添加噪声来近似，因为量化后的潜在变量是通过基于熵模型和失真度量的格栅搜索相互依赖地选择的。我们表明，在正确量化数据上进行再训练，对于均匀标量和特别是熵约束量化，始终能带来额外的编码增益，且不增加推理复杂性。对于Kodak测试集，我们在Bj{\\o}ntegaard-Delta比特率方面获得了1%到2%的平均节省，对于TecNick测试集，最高可达2.2%。", "summary": "该论文提出了一种新的训练方法，旨在优化学习图像压缩中的量化问题。针对现有可微分近似未能正确模拟量化噪声导致网络性能不佳的问题，研究人员引入了一个额外的微调训练步骤。在传统的端到端训练之后，网络的某些部分会在推理阶段获得的正确量化潜在变量上进行再训练。实验结果表明，这种方法对于均匀标量和熵约束量化都能持续带来编码增益，且不增加推理复杂性，在多个测试集上实现了显著的比特率节省。", "keywords": "图像压缩, 量化, 微调, 变分自编码器, 码率-失真", "comments": "本文的创新点在于提出了一个简单而有效的微调步骤，解决了学习图像压缩中量化噪声建模不准确的关键问题。通过在推理阶段利用实际的量化潜在变量进行再训练，避免了对量化过程进行复杂且不准确的近似，从而显著提升了压缩性能。这种方法在不增加推理成本的情况下实现了编码增益，具有很高的实用价值。"}}
{"id": "2506.08134", "title": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning", "authors": ["Qiyao Wei", "Samuel Holt", "Jing Yang", "Markus Wulfmeier", "Mihaela van der Schaar"], "summary": "Peer review, the bedrock of scientific advancement in machine learning (ML),\nis strained by a crisis of scale. Exponential growth in manuscript submissions\nto premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite\ncapacity of qualified reviewers, leading to concerns about review quality,\nconsistency, and reviewer fatigue. This position paper argues that AI-assisted\npeer review must become an urgent research and infrastructure priority. We\nadvocate for a comprehensive AI-augmented ecosystem, leveraging Large Language\nModels (LLMs) not as replacements for human judgment, but as sophisticated\ncollaborators for authors, reviewers, and Area Chairs (ACs). We propose\nspecific roles for AI in enhancing factual verification, guiding reviewer\nperformance, assisting authors in quality improvement, and supporting ACs in\ndecision-making. Crucially, we contend that the development of such systems\nhinges on access to more granular, structured, and ethically-sourced peer\nreview process data. We outline a research agenda, including illustrative\nexperiments, to develop and validate these AI assistants, and discuss\nsignificant technical and ethical challenges. We call upon the ML community to\nproactively build this AI-assisted future, ensuring the continued integrity and\nscalability of scientific validation, while maintaining high standards of peer\nreview.", "comment": "18 pages, 3 figures. Position paper", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08134v1", "AI": {"title_translation": "AI的必然性：在机器学习中扩展高质量的同行评审", "tldr": "机器学习领域的同行评审正面临规模危机，本论文主张将AI辅助同行评审作为紧急优先事项，利用大型语言模型（LLMs）作为人类的协作工具，以提升评审质量和可扩展性，并强调需更多数据和解决伦理挑战。", "motivation": "机器学习（ML）领域顶级会议（如NeurIPS、ICML、ICLR）的稿件提交量呈指数级增长，超出了合格审稿人的有限能力，导致审稿质量、一致性下降以及审稿人疲劳。", "method": "本论文倡导建立一个全面的AI增强生态系统，利用大型语言模型（LLMs）作为作者、审稿人和领域主席（ACs）的复杂协作者，而非替代品。具体提出了AI在增强事实核查、指导审稿人表现、协助作者提高质量以及支持ACs决策方面的作用。强调这些系统的开发依赖于获取更细粒度、结构化和合乎道德的同行评审过程数据，并概述了一个研究议程和说明性实验。", "result": "本论文概述了一个研究议程，包括说明性实验，旨在开发和验证这些AI助手，并讨论了重大的技术和伦理挑战。期望通过AI辅助确保科学验证的持续完整性和可扩展性，同时保持同行评审的高标准。", "conclusion": "机器学习社区应积极构建AI辅助的未来，以确保科学验证的持续完整性和可扩展性，同时保持同行评审的高标准。", "translation": "同行评审是机器学习（ML）科学进步的基石，正受到规模危机的严峻考验。NeurIPS、ICML和ICLR等顶级ML会议的稿件提交量呈指数级增长，超出了合格审稿人有限的能力，导致对审稿质量、一致性和审稿人疲劳的担忧。这份立场文件认为，AI辅助的同行评审必须成为一项紧迫的研究和基础设施优先事项。我们倡导一个全面的AI增强生态系统，利用大型语言模型（LLMs）不是作为人类判断的替代品，而是作为作者、审稿人和领域主席（ACs）的复杂协作者。我们提出了AI在增强事实核查、指导审稿人表现、协助作者提高质量以及支持ACs决策方面的具体作用。至关重要的是，我们认为这些系统的开发取决于获取更细粒度、结构化和合乎道德的同行评审过程数据。我们概述了一个研究议程，包括说明性实验，以开发和验证这些AI助手，并讨论了重大的技术和伦理挑战。我们呼吁ML社区积极构建这个AI辅助的未来，确保科学验证的持续完整性和可扩展性，同时保持高标准的同行评审。", "summary": "本立场文件指出，机器学习领域日益增长的稿件量给同行评审带来了规模危机。为解决这一问题，论文倡导将AI辅助同行评审作为优先事项，提出构建一个利用大型语言模型（LLMs）作为人类协作工具的AI增强生态系统。具体而言，AI可辅助事实核查、审稿人指导、作者质量提升及AC决策。论文强调此类系统开发需依赖结构化、合乎道德的评审数据，并提出了研究议程及相关技术伦理挑战，呼吁ML社区共同推动AI辅助评审以维护科学验证的完整性和可扩展性。", "keywords": "同行评审, 机器学习, AI辅助, 大型语言模型, 规模危机", "comments": "这篇立场文件明确指出了机器学习领域同行评审面临的实际挑战，并提出了一种前瞻性的解决方案。其创新之处在于将LLMs定位为人类的“复杂协作者”而非“替代品”，这在强调AI赋能的同时，也保留了人类判断的核心地位。论文不仅提出了愿景，还指出了实现这一愿景的关键瓶颈（数据获取）和挑战（技术与伦理），具有重要的指导意义。"}}
{"id": "2506.08021", "title": "FlowBERT: Prompt-tuned BERT for variable flow field prediction", "authors": ["Weihao Zou", "Weibing Feng", "Pin Wu"], "summary": "This study proposes a universal flow field prediction framework based on\nknowledge transfer\n  from large language model (LLM), addressing the high computational costs of\ntraditional\n  computational fluid dynamics (CFD) methods and the limited cross-condition\ntransfer capability\n  of existing deep learning models. The framework innovatively integrates\nProper Orthogonal\n  Decomposition (POD) dimensionality reduction with fine-tuning strategies for\npretrained LLM,\n  where POD facilitates compressed representation of flow field features while\nthe fine-tuned model\n  learns to encode system dynamics in state space. To enhance the model's\nadaptability to flow field\n  data, we specifically designed fluid dynamics-oriented text templates that\nimprove predictive\n  performance through enriched contextual semantic information. Experimental\nresults demonstrate\n  that our framework outperforms conventional Transformer models in few-shot\nlearning scenarios while\n  exhibiting exceptional generalization across various inflow conditions and\nairfoil geometries.\n  Ablation studies reveal the contributions of key components in the FlowBERT\narchitecture. Compared\n  to traditional Navier-Stokes equation solvers requiring hours of computation,\nour approach reduces\n  prediction time to seconds while maintaining over 90% accuracy. The developed\nknowledge transfer\n  paradigm establishes a new direction for rapid fluid dynamics prediction,\nwith potential\n  applications extending to aerodynamic optimization, flow control, and other\nengineering domains.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08021v1", "AI": {"title_translation": "FlowBERT: 用于可变流场预测的Prompt-tuned BERT", "tldr": "FlowBERT是一种基于LLM知识迁移的通用流场预测框架，它结合了POD降维和Prompt-tuned BERT，解决了传统CFD计算成本高和现有深度学习模型泛化能力有限的问题。该框架在少样本学习中表现出色，具有卓越的泛化能力，并将预测时间从数小时缩短到数秒，同时保持90%以上的准确率。", "motivation": "传统的计算流体动力学（CFD）方法计算成本高昂，而现有的深度学习模型在跨条件迁移能力方面有限。", "method": "该框架创新性地将本征正交分解（POD）降维与预训练大型语言模型（LLM， specifically BERT）的微调策略相结合。POD用于流场特征的压缩表示，而微调模型则学习在状态空间中编码系统动力学。此外，设计了面向流体动力学的文本模板，通过丰富的上下文语义信息提高预测性能。", "result": "实验结果表明，该框架在少样本学习场景中优于传统的Transformer模型，并且在各种流入条件和翼型几何形状下表现出卓越的泛化能力。与需要数小时计算的传统Navier-Stokes方程求解器相比，该方法将预测时间缩短到数秒，同时保持90%以上的准确率。消融研究揭示了FlowBERT架构中关键组件的贡献。", "conclusion": "所开发的知识迁移范式为快速流体动力学预测开辟了新方向，潜在应用可扩展到气动优化、流量控制和其他工程领域。", "translation": "本研究提出了一种基于大型语言模型（LLM）知识迁移的通用流场预测框架，旨在解决传统计算流体动力学（CFD）方法计算成本高昂以及现有深度学习模型跨条件迁移能力有限的问题。该框架创新性地将本征正交分解（POD）降维与预训练LLM的微调策略相结合，其中POD有助于流场特征的压缩表示，而微调模型则学习在状态空间中编码系统动力学。为了增强模型对流场数据的适应性，我们专门设计了面向流体动力学的文本模板，通过丰富的上下文语义信息提高了预测性能。实验结果表明，我们的框架在少样本学习场景中优于传统的Transformer模型，同时在各种流入条件和翼型几何形状下表现出卓越的泛化能力。消融研究揭示了FlowBERT架构中关键组件的贡献。与需要数小时计算的传统Navier-Stokes方程求解器相比，我们的方法将预测时间缩短到数秒，同时保持90%以上的准确率。所开发的知识迁移范式为快速流体动力学预测开辟了新方向，潜在应用可扩展到气动优化、流量控制和其他工程领域。", "summary": "本研究提出了一种名为FlowBERT的通用流场预测框架，旨在通过利用大型语言模型（LLM）的知识迁移来解决传统计算流体动力学（CFD）的高计算成本和现有深度学习模型泛化能力不足的问题。该框架创新性地将本征正交分解（POD）与预训练BERT模型的微调策略相结合，并通过专门设计的流体动力学文本模板增强模型适应性。实验证明，FlowBERT在少样本学习和跨条件泛化方面表现出色，与传统方法相比，显著缩短了预测时间并保持了高精度，为流体动力学快速预测开辟了新方向。", "keywords": "流场预测, 知识迁移, 大型语言模型, 本征正交分解, 流体动力学", "comments": "这篇论文通过将大型语言模型（LLM）的知识迁移应用于流体动力学预测，提出了一种创新方法，具体是使用Prompt-tuned BERT与POD相结合。其核心创新在于巧妙地将通常用于文本的LLM通过数据表示和提示工程适配到流场等科学数据上。在保持高准确率的同时显著减少计算时间是其一大优势，对于实际工程应用具有重要意义。此外，其在不同条件下表现出的卓越泛化能力也解决了该领域现有深度学习模型的一个常见局限性。"}}
{"id": "2506.08258", "title": "Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era", "authors": ["Lorenzo Arboit", "Dennis N. Schneider", "Toby Collins", "Daniel A. Hashimoto", "Silvana Perretta", "Bernard Dallemagne", "Jacques Marescaux", "EAES Working Group", "Nicolas Padoy", "Pietro Mascagni"], "summary": "Artificial Intelligence (AI) is transforming medicine, with generative AI\nmodels like ChatGPT reshaping perceptions of its potential. This study examines\nsurgeons' awareness, expectations, and involvement with AI in surgery through\ncomparative surveys conducted in 2021 and 2024. Two cross-sectional surveys\nwere distributed globally in 2021 and 2024, the first before an IRCAD webinar\nand the second during the annual EAES meeting. The surveys assessed\ndemographics, AI awareness, expectations, involvement, and ethics (2024 only).\nThe surveys collected a total of 671 responses from 98 countries, 522 in 2021\nand 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in\n2024, while course attendance increased from 12.9% to 23%. Despite this,\nfamiliarity with foundational AI concepts remained limited. Expectations for\nAI's role shifted in 2024, with hospital management gaining relevance. Ethical\nconcerns gained prominence, with 87.2% of 2024 participants emphasizing\naccountability and transparency. Infrastructure limitations remained the\nprimary obstacle to implementation. Interdisciplinary collaboration and\nstructured training were identified as critical for successful AI adoption.\nOptimism about AI's transformative potential remained high, with 79.9% of\nrespondents believing AI would positively impact surgery and 96.6% willing to\nintegrate AI into their clinical practice. Surgeons' perceptions of AI are\nevolving, driven by the rise of generative AI and advancements in surgical data\nscience. While enthusiasm for integration is strong, knowledge gaps and\ninfrastructural challenges persist. Addressing these through education, ethical\nframeworks, and infrastructure development is essential.", "comment": "11 pages, 3 figures", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.08258v1", "AI": {"title_translation": "外科医生对人工智能的认知、期望和参与度：GPT时代前后的调查", "tldr": "本研究通过比较2021年和2024年的调查，发现GPT时代后外科医生对AI的认知和期望有所提升，但仍存在知识和基础设施障碍，需通过教育和伦理框架解决。", "motivation": "鉴于人工智能（AI）正在改变医学，特别是生成式AI模型（如ChatGPT）重塑了人们对其潜力的看法，本研究旨在通过比较2021年和2024年的调查，考察外科医生对AI在手术中的认知、期望和参与度。", "method": "研究通过在2021年（IRCAD网络研讨会前）和2024年（EAES年会期间）在全球范围内分发两次横断面调查问卷进行。调查评估了人口统计学信息、AI认知、期望、参与度和伦理（2024年独有）。总共收集了来自98个国家的671份回复（2021年522份，2024年149份）。", "result": "2021年至2024年，AI课程认知度从14.5%上升到44.6%，课程参与度从12.9%增至23%。然而，对基础AI概念的熟悉度仍然有限。2024年，对AI角色的期望有所转变，医院管理相关性增加。伦理问题日益突出，87.2%的2024年参与者强调问责制和透明度。基础设施限制仍是主要障碍。受访者对AI的变革潜力保持高度乐观，79.9%认为AI将对外科手术产生积极影响，96.6%愿意将AI整合到临床实践中。", "conclusion": "外科医生对AI的看法正在演变，受生成式AI和外科数据科学进步的推动，整合AI的热情高涨，但知识差距和基础设施挑战依然存在。通过教育、伦理框架和基础设施发展解决这些问题至关重要。", "translation": "人工智能（AI）正在改变医学，生成式AI模型如ChatGPT正在重塑人们对其潜力的看法。本研究通过在2021年和2024年进行的比较调查，考察了外科医生对AI在外科手术中的认知、期望和参与度。两次横断面调查分别于2021年和2024年在全球范围内分发，第一次是在IRCAD网络研讨会之前，第二次是在EAES年会期间。调查评估了人口统计学、AI认知、期望、参与度和伦理（仅限2024年）。调查共收集了来自98个国家的671份回复，其中2021年522份，2024年149份。对AI课程的认知度从2021年的14.5%上升到2024年的44.6%，而课程参与度从12.9%增加到23%。尽管如此，对基础AI概念的熟悉度仍然有限。2024年，对AI角色的期望发生了变化，医院管理的重要性增加。伦理问题日益突出，87.2%的2024年参与者强调问责制和透明度。基础设施限制仍然是实施的主要障碍。跨学科合作和结构化培训被认为是成功采用AI的关键。对AI变革潜力的乐观情绪仍然很高，79.9%的受访者认为AI将对外科手术产生积极影响，96.6%的人愿意将AI整合到他们的临床实践中。外科医生对AI的看法正在演变，受到生成式AI和外科数据科学进步的推动。虽然整合AI的热情很高，但知识差距和基础设施挑战依然存在。通过教育、伦理框架和基础设施发展来解决这些问题至关重要。", "summary": "本研究通过比较2021年和2024年对全球外科医生的调查，探讨了GPT时代前后他们对人工智能的认知、期望和参与度。结果显示，外科医生对AI课程的认知和参与度显著提高，对AI在临床实践中的整合持高度乐观态度。然而，对基础AI概念的熟悉度有限，伦理关注度增加，且基础设施限制仍是主要障碍。研究强调，通过教育、伦理框架和基础设施建设来弥合知识和实践差距，是成功推广AI的关键。", "keywords": "人工智能, 外科医生, 调查, GPT, 认知, 期望", "comments": "这项研究的创新之处在于其比较了GPT时代前后外科医生对AI态度的变化，突出了生成式AI对医学领域认知的影响。其重要性在于揭示了AI在外科领域推广面临的实际挑战（知识差距、基础设施）和机遇（高整合意愿），为未来的教育和政策制定提供了宝贵数据。"}}
{"id": "2506.08350", "title": "Complex-Valued Holographic Radiance Fields", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Akşit"], "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations.", "comment": "28 pages, 21 figures", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.08350v1", "AI": {"title_translation": "复数值全息辐射场", "tldr": "该论文提出了一种利用复数值高斯基元改进3D高斯splatting的新方法，用于直接优化3D全息场景表示，从而在保持图像质量的同时显著提高了渲染速度。", "motivation": "在3D表示中建模光的完整属性（包括振幅和相位）对于推进物理真实感渲染，特别是在全息显示中至关重要。", "method": "提出了一种不依赖于基于强度的中间体来优化3D场景的新型表示。通过复数值高斯基元重新构建3D高斯splatting，扩展了对光波渲染的支持。利用RGBD多视图图像，该方法直接优化复数值高斯作为3D全息场景表示，从而消除了计算成本高昂的全息图重新优化。", "result": "与最先进的方法相比，该方法实现了30倍至10,000倍的速度提升，同时保持了同等的图像质量。", "conclusion": "该方法代表着朝着几何对齐、物理真实感全息场景表示迈出了第一步。", "translation": "在3D表示中建模光的完整属性（包括振幅和相位）对于推进物理真实感渲染，特别是在全息显示中至关重要。为了支持这些特性，我们提出了一种不依赖于基于强度的中间体来优化3D场景的新型表示。我们通过复数值高斯基元重新构建3D高斯splatting，扩展了对光波渲染的支持。通过利用RGBD多视图图像，我们的方法直接优化复数值高斯作为3D全息场景表示。这消除了计算成本高昂的全息图重新优化。与最先进的方法相比，我们的方法实现了30倍至10,000倍的速度提升，同时保持了同等的图像质量，代表着朝着几何对齐、物理真实感全息场景表示迈出了第一步。", "summary": "该论文提出了一种名为复数值全息辐射场的新型3D场景表示方法，通过将3D高斯splatting与复数值高斯基元结合，直接优化3D全息场景，无需依赖基于强度的中间体。该方法利用RGBD多视图图像，显著提高了渲染速度（30x-10,000x），同时保持了图像质量，为物理真实感全息显示提供了高效解决方案。", "keywords": "全息渲染, 复数值高斯, 3D场景表示, 辐射场, 高斯splatting", "comments": "这项工作通过引入复数值高斯基元和直接优化策略，解决了传统全息渲染中计算成本高昂的问题，实现了显著的速度提升。其创新性在于将光的完整属性（振幅和相位）直接集成到3D场景表示中，为物理真实感全息显示奠定了基础。这对于未来全息技术的发展具有重要意义。"}}
{"id": "2506.08807", "title": "Confidence Boosts Trust-Based Resilience in Cooperative Multi-Robot Systems", "authors": ["Luca Ballotta", "Áron Vékássy", "Stephanie Gil", "Michal Yemini"], "summary": "Wireless communication-based multi-robot systems open the door to\ncyberattacks that can disrupt safety and performance of collaborative robots.\nThe physical channel supporting inter-robot communication offers an attractive\nopportunity to decouple the detection of malicious robots from task-relevant\ndata exchange between legitimate robots. Yet, trustworthiness indications\ncoming from physical channels are uncertain and must be handled with this in\nmind. In this paper, we propose a resilient protocol for multi-robot operation\nwherein a parameter {\\lambda}t accounts for how confident a robot is about the\nlegitimacy of nearby robots that the physical channel indicates. Analytical\nresults prove that our protocol achieves resilient coordination with\narbitrarily many malicious robots under mild assumptions. Tuning {\\lambda}t\nallows a designer to trade between near-optimal inter-robot coordination and\nquick task execution; see Fig. 1. This is a fundamental performance tradeoff\nand must be carefully evaluated based on the task at hand. The effectiveness of\nour approach is numerically verified with experiments involving platoons of\nautonomous cars where some vehicles are maliciously spoofed.", "comment": "This work has been submitted to IEEE for possible publication", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.08807v1", "AI": {"title_translation": "置信度提升合作多机器人系统中基于信任的弹性", "tldr": "本文提出了一种多机器人弹性协议，通过物理信道指示的置信度参数来应对网络攻击，实现了在存在恶意机器人情况下的弹性协调，并揭示了协调与任务执行速度之间的权衡。", "motivation": "无线通信多机器人系统易受网络攻击，这些攻击会破坏协作机器人的安全性和性能。在多机器人操作中，需要一种能够将恶意机器人检测与合法机器人之间任务相关数据交换解耦的方法，同时处理物理信道信任指示的不确定性。", "method": "本文提出了一种多机器人弹性协议。该协议引入了一个参数{\\lambda}t，用于衡量机器人对物理信道指示的附近机器人合法性的置信度。通过调整{\\lambda}t，可以在近乎最优的机器人间协调和快速任务执行之间进行权衡。", "result": "分析结果证明，在温和假设下，所提出的协议在存在任意数量的恶意机器人时也能实现弹性协调。通过涉及自动驾驶汽车车队（其中一些车辆被恶意欺骗）的实验，数值验证了该方法的有效性。", "conclusion": "该协议实现了多机器人系统的弹性协调，但设计者需要根据具体任务仔细权衡近乎最优的机器人间协调和快速任务执行之间的关系。", "translation": "基于无线通信的多机器人系统为网络攻击打开了大门，这些攻击可能会扰乱协作机器人的安全性和性能。支持机器人间通信的物理信道提供了一个有吸引力的机会，可以将恶意机器人的检测与合法机器人之间任务相关的数据交换解耦。然而，来自物理信道的信任指示是不确定的，必须考虑到这一点来处理。在本文中，我们提出了一种多机器人操作的弹性协议，其中参数{\\lambda}t表示机器人对物理信道指示的附近机器人合法性的置信度。分析结果证明，在温和假设下，我们的协议在存在任意数量的恶意机器人时也能实现弹性协调。调整{\\lambda}t允许设计者在近乎最优的机器人间协调和快速任务执行之间进行权衡；参见图1。这是一个基本的性能权衡，必须根据手头的任务仔细评估。我们的方法通过涉及自动驾驶汽车车队（其中一些车辆被恶意欺骗）的实验进行了数值验证。", "summary": "本文提出了一种针对多机器人系统的弹性协议，旨在应对无线通信带来的网络攻击威胁。该协议利用物理信道来解耦恶意机器人检测与合法数据交换，并通过一个置信度参数{\\lambda}t来处理物理信道指示的不确定性。分析结果表明，该协议在存在任意数量恶意机器人的情况下仍能实现弹性协调，并通过自动驾驶汽车车队的实验进行了验证。研究还揭示了机器人间协调性能与任务执行速度之间的基本权衡。", "keywords": "多机器人系统, 弹性协议, 网络攻击, 物理信道, 置信度", "comments": "本文的创新之处在于利用物理信道来辅助恶意机器人检测，并将其与任务相关数据交换解耦，这在提高系统安全性的同时，也避免了对正常通信的干扰。引入的{\\lambda}t参数允许系统在安全性与性能之间进行灵活权衡，这对于实际应用具有重要指导意义。此外，对不确定性信任指示的处理也增强了协议的鲁棒性。"}}
{"id": "2506.08443", "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "summary": "While current AI illustration tools can generate high-quality images from\ntext prompts, they rarely reveal the step-by-step procedure that human artists\nfollow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based\nimage generation with a large-language-model tutor. At each stage, novices\nreceive real-time feedback on anatomy, perspective, and composition, revise any\nstep non-linearly, and branch alternative versions. By exposing intermediate\noutputs and embedding pedagogical dialogue, SakugaFlow turns a black-box\ngenerator into a scaffolded learning environment that supports both creative\nexploration and skills acquisition.", "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI\n  (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08443v1", "AI": {"title_translation": "SakugaFlow：一个模拟人类绘画过程并为初学者绘画技能提供互动辅导的分阶段插画框架", "tldr": "SakugaFlow是一个四阶段AI绘画工具，结合扩散模型和LLM导师，模拟人类绘画过程，为初学者提供实时反馈和互动学习。", "motivation": "现有的AI插画工具虽然能生成高质量图像，但未能揭示人类艺术家遵循的逐步绘画过程。", "method": "提出SakugaFlow，一个四阶段的流程，将基于扩散的图像生成与大型语言模型导师相结合。在每个阶段，初学者获得关于解剖、透视和构图的实时反馈，可以非线性地修改任何步骤，并分支生成替代版本。", "result": "SakugaFlow通过暴露中间输出并嵌入教学对话，将一个黑盒生成器转变为一个支架式学习环境。它支持创造性探索和技能习得。", "conclusion": "SakugaFlow成功地将AI图像生成器转化为一个支持初学者学习和技能获取的互动式、分阶段的教学环境。", "translation": "尽管当前的AI插画工具可以从文本提示生成高质量图像，但它们很少揭示人类艺术家所遵循的循序渐进的过程。我们提出了SakugaFlow，一个四阶段的流水线，它将基于扩散的图像生成与大型语言模型导师配对。在每个阶段，初学者都会收到关于解剖学、透视和构图的实时反馈，可以非线性地修改任何步骤，并分支生成替代版本。通过暴露中间输出并嵌入教学对话，SakugaFlow将一个黑盒生成器转变为一个支架式学习环境，支持创造性探索和技能习得。", "summary": "SakugaFlow是一个创新的AI插画框架，旨在弥补现有AI工具缺乏逐步指导的不足。它采用四阶段流水线，结合扩散模型和大型语言模型导师，模拟人类绘画过程。该系统为初学者提供实时反馈，支持非线性修改和版本分支，从而将传统的黑盒生成器转变为一个促进创造性探索和绘画技能习得的支架式学习环境。", "keywords": "AI插画, 绘画教学, 扩散模型, 大型语言模型, 支架式学习", "comments": "这篇论文的创新点在于它不仅仅是一个图像生成工具，更是一个教学平台。它将AI的生成能力与交互式教学相结合，解决了AI工具在艺术教育中“黑盒”操作的局限性，为初学者提供了宝贵的学习机会，具有重要的实践意义。"}}
{"id": "2506.09039", "title": "Deep Reinforcement Learning-Based RAN Slicing with Efficient Inter-Slice Isolation in Tactical Wireless Networks", "authors": ["Abderrahime Filali", "Diala Naboulsi", "Georges Kaddoum"], "summary": "The next generation of tactical networks (TNs) is poised to further leverage\nthe key enablers of 5G and beyond 5G (B5G) technology, such as radio access\nnetwork (RAN) slicing and the open RAN (O-RAN) paradigm, to unlock multiple\narchitectural options and opportunities for a wide range of innovative\napplications. RAN slicing and the O-RAN paradigm are considered game changers\nin TNs, where the former makes it possible to tailor user services to users\nrequirements, and the latter brings openness and intelligence to the management\nof the RAN. In TNs, bandwidth scarcity requires a dynamic bandwidth slicing\nstrategy. Although this type of strategy ensures efficient bandwidth\nutilization, it compromises RAN slicing isolation in terms of quality of\nservice (QoS) performance. To deal with this challenge, we propose a deep\nreinforcement learning (DRL)-based RAN slicing mechanism that achieves a\ntrade-off between efficient RAN bandwidth sharing and appropriate inter- and\nintra-slice isolation. The proposed mechanism performs bandwidth allocation in\ntwo stages. In the first stage, the bandwidth is allocated to the RAN slices.\nIn the second stage, each slice partitions its bandwidth among its associated\nusers. In both stages, the slicing operation is constrained by several\nconsiderations related to improving the QoS of slices and users that in turn\nfoster inter- and intra-slice isolation. The proposed RAN slicing mechanism is\nbased on DRL algorithms to perform the bandwidth sharing operation in each\nstage. We propose to deploy the mechanism in an O-RAN architecture and describe\nthe O-RAN functional blocks and the main DRL model lifecycle management phases\ninvolved. We also develop three different implementations of the proposed\nmechanism, each based on a different DRL algorithm, and evaluate their\nperformance against multiple baselines across various parameters.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09039v1", "AI": {"title_translation": "战术无线网络中基于深度强化学习的RAN切片与高效切片间隔离", "tldr": "本文提出了一种基于深度强化学习（DRL）的RAN切片机制，旨在战术网络中平衡高效带宽共享与切片隔离，以解决带宽稀缺对服务质量（QoS）隔离的影响。", "motivation": "下一代战术网络（TNs）将利用RAN切片和O-RAN等5G/B5G技术，但带宽稀缺导致动态带宽切片策略损害了RAN切片隔离的服务质量（QoS）性能。因此，挑战在于如何在高效带宽利用的同时，保持RAN切片隔离。", "method": "本文提出了一种基于深度强化学习（DRL）的RAN切片机制，旨在平衡高效的RAN带宽共享与适当的切片间和切片内隔离。该机制分两个阶段进行带宽分配：首先将带宽分配给RAN切片，然后每个切片将其带宽分配给关联用户。这两个阶段的切片操作均受限于改善切片和用户QoS的考量，从而促进切片间和切片内隔离。该机制基于DRL算法执行带宽共享操作，并建议部署在O-RAN架构中。论文还开发了三种基于不同DRL算法的实现。", "result": "论文开发了所提出机制的三种不同实现，每种都基于不同的DRL算法，并评估了它们在各种参数下相对于多个基线的性能。具体性能数据未在摘要中提及。", "conclusion": "本文提出了一种基于深度强化学习（DRL）的RAN切片机制，该机制成功地在战术无线网络中解决了高效RAN带宽共享与切片间及切片内隔离之间的权衡问题。", "translation": "下一代战术网络（TNs）将进一步利用5G和超越5G（B5G）技术的关键使能器，例如无线接入网络（RAN）切片和开放RAN（O-RAN）范式，以解锁多种架构选项和广泛创新应用的机会。RAN切片和O-RAN范式被认为是战术网络中的颠覆性技术，前者使得根据用户需求定制用户服务成为可能，后者则为RAN的管理带来了开放性和智能化。在战术网络中，带宽稀缺需要动态带宽切片策略。尽管这类策略确保了高效的带宽利用，但它在服务质量（QoS）性能方面损害了RAN切片隔离。为了应对这一挑战，我们提出了一种基于深度强化学习（DRL）的RAN切片机制，该机制在高效的RAN带宽共享和适当的切片间及切片内隔离之间实现了权衡。所提出的机制分两个阶段执行带宽分配。在第一阶段，带宽分配给RAN切片。在第二阶段，每个切片将其带宽分配给其关联用户。在这两个阶段中，切片操作都受到若干考虑因素的限制，这些因素与改善切片和用户的QoS有关，从而促进切片间和切片内隔离。所提出的RAN切片机制基于DRL算法，以在每个阶段执行带宽共享操作。我们建议将该机制部署在O-RAN架构中，并描述了所涉及的O-RAN功能块和主要的DRL模型生命周期管理阶段。我们还开发了所提出机制的三种不同实现，每种都基于不同的DRL算法，并评估了它们在各种参数下相对于多个基线的性能。", "summary": "本论文提出了一种基于深度强化学习（DRL）的RAN切片机制，以应对战术无线网络中带宽稀缺导致的RAN切片隔离挑战。该机制分两阶段进行带宽分配，旨在平衡高效的RAN带宽共享与切片间和切片内隔离，以提升服务质量。研究将该机制部署在O-RAN架构中，并开发了三种基于不同DRL算法的实现，通过与多个基线比较来评估其性能。", "keywords": "深度强化学习, RAN切片, 战术网络, 切片隔离, O-RAN", "comments": "该论文的创新点在于将深度强化学习应用于战术无线网络中的RAN切片管理，以智能地解决带宽共享与服务质量隔离之间的复杂权衡问题。结合O-RAN架构的部署考虑，为未来战术网络资源管理提供了新的思路和潜在解决方案。然而，摘要中未提供具体的性能评估结果，这限制了对其有效性的初步判断。"}}
{"id": "2506.08785", "title": "POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration", "authors": ["Mukul Lokhande", "Santosh Kumar Vishvakarma"], "summary": "The increasing complexity of AI models requires flexible hardware capable of\nsupporting diverse precision formats, particularly for energy-constrained edge\nplatforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC\nengine that performs efficient multiply-accumulate operations using a unified\ndata-path for 4/8/16-bit fixed-point, floating point, and posit formats. The\narchitecture incorporates a layer adaptive precision strategy to align\ncomputational accuracy with workload sensitivity, optimizing both performance\nand energy usage. PARV-CE integrates quantization-aware execution with a\nreconfigurable SIMD pipeline, enabling high-throughput processing with minimal\noverhead through hardware-software co-design. The results demonstrate up to 2x\nimprovement in PDP and 3x reduction in resource usage compared to SoTA designs,\nwhile retaining accuracy within 1.8% FP32 baseline. The architecture supports\nboth on-device training and inference across a range of workloads, including\nDNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE\nincorporated POLARON as a scalable and energy-efficient solution for\nprecision-adaptive AI acceleration at edge.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.08785v1", "AI": {"title_translation": "POLARON：精度感知片上学习与自适应运行时可配置AI加速", "tldr": "POLARON提出PARV-CE，一种多精度MAC引擎，通过硬件-软件协同设计实现高效的片上AI加速，显著提升性能并降低资源消耗。", "motivation": "随着AI模型日益复杂，能源受限的边缘平台需要灵活的硬件来支持多种精度格式，以优化性能和能耗。", "method": "本文提出了PARV-CE，一个SIMD使能的多精度MAC引擎，使用统一数据路径支持4/8/16位定点、浮点和posit格式。它采用层自适应精度策略，结合量化感知执行和可重构SIMD管道，通过硬件-软件协同设计实现高吞吐量处理。", "result": "相比SoTA设计，PDP提升高达2倍，资源使用减少3倍，同时精度保持在FP32基线1.8%以内。该架构支持设备上的训练和推理，适用于DNNs, RNNs, RL和Transformer模型。", "conclusion": "经验分析表明，结合POLARON的PARV-CE是边缘设备上精度自适应AI加速的可扩展且节能的解决方案。", "translation": "标题：POLARON：精度感知片上学习与自适应运行时可配置AI加速\n摘要：AI模型日益增长的复杂性要求灵活的硬件，能够支持多种精度格式，特别是对于能源受限的边缘平台。这项工作提出了PARV-CE，一个支持SIMD的多精度MAC引擎，它使用统一数据路径对4/8/16位定点、浮点和posit格式执行高效的乘积累加操作。该架构结合了层自适应精度策略，以使计算精度与工作负载敏感度保持一致，从而优化性能和能耗。PARV-CE将量化感知执行与可重构SIMD流水线集成，通过硬件-软件协同设计以最小开销实现高吞吐量处理。结果表明，与最先进的设计相比，PDP提高了2倍，资源使用减少了3倍，同时精度保持在FP32基线1.8%以内。该架构支持一系列工作负载的片上训练和推理，包括DNN、RNN、RL和Transformer模型。经验分析证实，集成了POLARON的PARV-CE是边缘设备上精度自适应AI加速的可扩展且节能的解决方案。", "summary": "POLARON提出了一种名为PARV-CE的AI加速器，专为边缘设备设计，以解决AI模型复杂性对多精度支持的需求。PARV-CE是一个SIMD使能的多精度MAC引擎，通过统一数据路径支持多种数值格式，并采用层自适应精度策略。它通过硬件-软件协同设计实现了量化感知执行和可重构SIMD管道，从而显著提高了性能和能效，同时保持了高精度。该解决方案适用于多种AI模型，为边缘AI提供了可扩展且节能的加速。", "keywords": "边缘AI, 硬件加速, 多精度, 片上学习, MAC引擎", "comments": "这篇论文的创新点在于提出了PARV-CE，一个能够支持多种精度格式并具有层自适应精度策略的MAC引擎，这对于能耗敏感的边缘AI设备至关重要。通过硬件-软件协同设计，它在提高性能和降低资源消耗方面取得了显著进展，同时保持了高精度，这对于实际部署具有重要意义。其对片上训练和推理的支持也增加了其通用性。"}}
{"id": "2506.08023", "title": "Aligning Proteins and Language: A Foundation Model for Protein Retrieval", "authors": ["Qifeng Wu", "Zhengzhe Liu", "Han Zhu", "Yizhou Zhao", "Daisuke Kihara", "Min Xu"], "summary": "This paper aims to retrieve proteins with similar structures and semantics\nfrom large-scale protein dataset, facilitating the functional interpretation of\nprotein structures derived by structural determination methods like\ncryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of\nvision-language models (VLMs), we propose a CLIP-style framework for aligning\n3D protein structures with functional annotations using contrastive learning.\nFor model training, we propose a large-scale dataset of approximately 200,000\nprotein-caption pairs with rich functional descriptors. We evaluate our model\nin both in-domain and more challenging cross-database retrieval on Protein Data\nBank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In\nboth cases, our approach demonstrates promising zero-shot retrieval\nperformance, highlighting the potential of multimodal foundation models for\nstructure-function understanding in protein biology.", "comment": "4 pages for body, 3 pages for appendix, 11 figures. Accepted to CVPR\n  2025 Workshop on Multimodal Foundation Models for Biomedicine: Challenges and\n  Opportunities(MMFM-BIOMED)", "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.08023v1", "AI": {"title_translation": "对齐蛋白质和语言：一个用于蛋白质检索的基础模型", "tldr": "本文提出一个CLIP风格的框架，通过对比学习对齐3D蛋白质结构和功能注释，以实现大规模蛋白质检索，并在域内和跨数据库检索中展现出良好的零样本性能。", "motivation": "旨在从大规模蛋白质数据集中检索具有相似结构和语义的蛋白质，从而促进通过冷冻电镜等结构测定方法获得的蛋白质结构的功能解释。受到视觉-语言模型（VLMs）最新进展的启发。", "method": "提出一个CLIP风格的框架，使用对比学习对齐3D蛋白质结构与功能注释。为模型训练构建了一个包含约200,000个蛋白质-描述对的大规模数据集。", "result": "在蛋白质数据库（PDB）和电子显微镜数据库（EMDB）数据集的域内和更具挑战性的跨数据库检索中，该方法均展现出有前景的零样本检索性能。", "conclusion": "多模态基础模型在蛋白质生物学中理解结构-功能方面具有潜力。", "translation": "本文旨在从大规模蛋白质数据集中检索具有相似结构和语义的蛋白质，从而促进通过冷冻电镜（cryo-EM）等结构测定方法获得的蛋白质结构的功能解释。受视觉-语言模型（VLMs）最新进展的启发，我们提出了一个CLIP风格的框架，利用对比学习对齐3D蛋白质结构和功能注释。为了模型训练，我们提出了一个包含约200,000个蛋白质-描述对的大规模数据集，其中包含丰富的功能描述符。我们在蛋白质数据库（PDB）和电子显微镜数据库（EMDB）数据集上分别评估了我们的模型在域内和更具挑战性的跨数据库检索中的性能。在这两种情况下，我们的方法都展示了有前景的零样本检索性能，突出了多模态基础模型在蛋白质生物学中理解结构-功能方面的潜力。", "summary": "本文提出一个受视觉-语言模型启发的CLIP风格框架，通过对比学习对齐3D蛋白质结构和功能注释，以实现大规模蛋白质检索。为训练构建了20万个蛋白质-描述对数据集。在PDB和EMDB数据集上进行域内和跨数据库检索评估，结果显示出良好的零样本性能，表明多模态基础模型在蛋白质结构-功能理解上的潜力。", "keywords": "蛋白质检索, 结构-功能理解, 对比学习, 基础模型, CLIP", "comments": "该论文的创新点在于将视觉-语言模型的成功范式（CLIP）应用于蛋白质领域，通过对齐蛋白质结构和功能注释，解决了大规模蛋白质数据检索和功能解释的挑战。构建大规模蛋白质-描述对数据集是其重要贡献，零样本检索性能的展示也突出了其泛化能力和作为基础模型的潜力，对蛋白质结构生物学和功能基因组学具有重要意义。"}}
{"id": "2506.08520", "title": "Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models", "authors": ["Srinivasan Kidambi", "Pravin Nair"], "summary": "Multi-head self-attention (MHSA) has become a core component in modern\ncomputer vision models. However, its quadratic complexity with respect to input\nlength poses a significant computational bottleneck in real-time and resource\nconstrained environments. We propose PnP-Nystra, a Nystr\\\"om based linear\napproximation of self-attention, developed as a plug-and-play (PnP) module that\ncan be integrated into the pre-trained image and video restoration models\nwithout retraining. As a drop-in replacement for MHSA, PnP-Nystra enables\nefficient acceleration in various window-based transformer architectures,\nincluding SwinIR, Uformer, and RVRT. Our experiments across diverse image and\nvideo restoration tasks, including denoising, deblurring, and super-resolution,\ndemonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU\nand a 2-5x speed-up on CPU inference. Despite these significant gains, the\nmethod incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To\nthe best of our knowledge, we are the first to demonstrate a linear attention\nfunctioning as a training-free substitute for MHSA in restoration models.", "comment": "6 pages, 1 pseudo-code, 3 figure panels, 2 plot panels, 7 tables, 24\n  references", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08520v1", "AI": {"title_translation": "预训练图像和视频恢复模型的可插拔线性注意力机制", "tldr": "提出PnP-Nystra，一种基于Nyström的线性注意力模块，可以作为预训练图像和视频恢复模型中多头自注意力（MHSA）的免训练替代品，显著提升推理速度，同时保持可接受的性能损失。", "motivation": "多头自注意力（MHSA）在现代计算机视觉模型中是核心组件，但其二次复杂度在实时和资源受限环境中造成显著的计算瓶颈。", "method": "本文提出了PnP-Nystra，一种基于Nyström的自注意力线性近似，被开发为可插拔（PnP）模块，无需再训练即可集成到预训练的图像和视频恢复模型中。它作为MHSA的直接替代品，能有效加速SwinIR、Uformer和RVRT等基于窗口的Transformer架构。", "result": "在包括去噪、去模糊和超分辨率在内的多种图像和视频恢复任务中，PnP-Nystra在NVIDIA RTX 4090 GPU上实现了2-4倍的加速，在CPU推理上实现了2-5倍的加速。尽管有显著的速度提升，该方法在所有评估任务中最大PSNR下降仅为1.5 dB。", "conclusion": "据作者所知，这是首次证明线性注意力可以在恢复模型中作为MHSA的免训练替代品。", "translation": "多头自注意力（MHSA）已成为现代计算机视觉模型中的核心组件。然而，其相对于输入长度的二次复杂度在实时和资源受限环境中构成了显著的计算瓶颈。我们提出了PnP-Nystra，一种基于Nyström的自注意力线性近似，开发为可插拔（PnP）模块，无需再训练即可集成到预训练的图像和视频恢复模型中。作为MHSA的直接替代品，PnP-Nystra在各种基于窗口的Transformer架构中实现了高效加速，包括SwinIR、Uformer和RVRT。我们在包括去噪、去模糊和超分辨率在内的各种图像和视频恢复任务中的实验表明，PnP-Nystra在NVIDIA RTX 4090 GPU上实现了2-4倍的加速，在CPU推理上实现了2-5倍的加速。尽管有这些显著的增益，该方法在所有评估任务中最大PSNR下降仅为1.5 dB。据我们所知，我们是第一个证明线性注意力可以在恢复模型中作为MHSA的免训练替代品。", "summary": "该论文提出PnP-Nystra，一种基于Nyström的可插拔线性注意力模块，旨在解决多头自注意力（MHSA）在图像和视频恢复模型中造成的计算瓶颈。PnP-Nystra可作为MHSA的免训练替代品，集成到SwinIR、Uformer、RVRT等预训练模型中。实验结果表明，该方法在GPU和CPU上分别实现了2-4倍和2-5倍的推理加速，同时PSNR性能损失最大仅为1.5 dB，验证了其在不牺牲过多性能的情况下显著提升效率的能力。", "keywords": "线性注意力, 即插即用, 图像恢复, 视频恢复, 自注意力", "comments": "本文的创新点在于提出了PnP-Nystra，一个免训练的线性注意力模块，首次实现了线性注意力在图像和视频恢复模型中作为MHSA的直接替代。这对于在资源受限环境下部署高性能视觉模型具有重要意义，因为它在显著加速的同时，仅带来可接受的性能下降。"}}
{"id": "2506.08597", "title": "Towards Provenance-Aware Earth Observation Workflows: the openEO Case Study", "authors": ["H. Omidi", "L. Sacco", "V. Hutter", "G. Irsiegler", "M. Claus", "M. Schobben", "A. Jacob", "M. Schramm", "S. Fiore"], "summary": "Capturing the history of operations and activities during a computational\nworkflow is significantly important for Earth Observation (EO). The data\nprovenance helps to collect the metadata that records the lineage of data\nproducts, providing information about how data are generated, transferred,\nmanipulated, by whom all these operations are performed and through which\nprocesses, parameters, and datasets. This paper presents an approach to improve\nthose aspects, by integrating the data provenance library yProv4WFs within\nopenEO, a platform to let users connect to Earth Observation cloud back-ends in\na simple and unified way. In addition, it is demonstrated how the integration\nof data provenance concepts across EO processing chains enables researchers and\nstakeholders to better understand the flow, the dependencies, and the\ntransformations involved in analytical workflows.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.08597v1", "AI": {"title_translation": "迈向溯源感知的地球观测工作流：openEO案例研究", "tldr": "本文介绍了将数据溯源库yProv4WFs集成到openEO平台的方法，以改善地球观测工作流中的操作历史记录，从而帮助研究人员和利益相关者更好地理解分析工作流中的数据流、依赖关系和转换。", "motivation": "在地球观测（EO）的计算工作流中，捕获操作和活动的完整历史记录至关重要。数据溯源有助于收集记录数据产品沿袭的元数据，提供数据如何生成、传输、操作以及由谁执行、通过哪些过程、参数和数据集的信息。", "method": "本文提出了一种通过将数据溯源库yProv4WFs集成到openEO（一个让用户以简单统一的方式连接地球观测云后端的平台）中来改进这些方面的方法。", "result": "研究结果表明，在EO处理链中集成数据溯源概念，能够帮助研究人员和利益相关者更好地理解分析工作流中涉及的数据流、依赖关系和转换。", "conclusion": "通过将数据溯源库yProv4WFs集成到openEO平台中，可以显著改善地球观测工作流中的操作历史记录捕获，从而增强对数据流、依赖关系和转换的理解。", "translation": "在地球观测（EO）的计算工作流中，捕获操作和活动的完整历史记录至关重要。数据溯源有助于收集记录数据产品沿袭的元数据，提供数据如何生成、传输、操作，以及由谁执行、通过哪些过程、参数和数据集的信息。本文提出了一种通过将数据溯源库yProv4WFs集成到openEO（一个让用户以简单统一的方式连接地球观测云后端的平台）中来改进这些方面的方法。此外，本文还展示了数据溯源概念在EO处理链中的集成如何使研究人员和利益相关者更好地理解分析工作流中涉及的数据流、依赖关系和转换。", "summary": "本文旨在解决地球观测（EO）计算工作流中捕获操作历史记录的重要性问题。通过将数据溯源库yProv4WFs集成到openEO平台（一个用于连接EO云后端的统一平台），本研究提出了一种改进数据沿袭记录的方法。文中展示了这种集成如何使研究人员和利益相关者更好地理解EO处理链中分析工作流的数据流、依赖关系和转换。", "keywords": "地球观测, 数据溯源, 工作流, openEO, 数据沿袭", "comments": "本文的创新点在于将数据溯源库yProv4WFs集成到openEO平台，为地球观测工作流提供了更全面的历史记录追踪能力。这对于确保数据产品的可信度、可重复性和透明度至关重要，尤其是在复杂的科学数据处理领域。这种集成有助于提升数据治理和科学研究的效率，但抽象中未提及具体实现细节或性能评估。"}}
{"id": "2506.08357", "title": "MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion", "authors": ["Franck Meyer", "Kyunghoon Hur", "Edward Choi"], "summary": "Despite the remarkable progress of deep-learning methods generating a target\nvital sign waveform from a source vital sign waveform, most existing models are\ndesigned exclusively for a specific source-to-target pair. This requires\ndistinct model architectures, optimization procedures, and pre-processing\npipelines, resulting in multiple models that hinder usability in clinical\nsettings. To address this limitation, we propose the Multi-Directional\nVital-Sign Converter (MD-ViSCo), a unified framework capable of generating any\ntarget waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or\narterial blood pressure (ABP) from any single input waveform with a single\nmodel. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin\nTransformer that leverages Adaptive Instance Normalization (AdaIN) to capture\ndistinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct\nmulti-directional waveform generation on two publicly available datasets. Our\nframework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average\nacross all waveform types, lowering Mean absolute error (MAE) by 8.8% and\nimproving Pearson correlation (PC) by 4.9% over two datasets. In addition, the\ngenerated ABP waveforms satisfy the Association for the Advancement of Medical\nInstrumentation (AAMI) criterion and achieve Grade B on the British\nHypertension Society (BHS) standard, outperforming all baselines. By\neliminating the need for developing a distinct model for each task, we believe\nthat this work offers a unified framework that can deal with any kind of vital\nsign waveforms with a single model in healthcare monitoring.", "comment": "Main paper (16 pages, 5 figures). Paper submitted for review. Code\n  available at https://github.com/fr-meyer/MD-ViSCo", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08357v1", "AI": {"title_translation": "MD-ViSCo：一种多方向生命体征波形转换的统一模型", "tldr": "MD-ViSCo是一个统一的深度学习模型，能够将任何生命体征波形（如ECG、PPG、ABP）转换为任何其他波形，解决了现有模型需要为每个转换对单独设计的问题，并在多个数据集上表现优于现有SOTA方法。", "motivation": "尽管深度学习在生命体征波形转换方面取得了显著进展，但大多数现有模型都专门针对特定的源-目标对设计，导致需要多个独立的模型架构、优化过程和预处理流程，这阻碍了其在临床环境中的可用性。", "method": "提出了多方向生命体征转换器（MD-ViSCo），这是一个统一的框架。MD-ViSCo采用了一个浅层的1维U-Net，并集成了Swin Transformer，利用自适应实例归一化（AdaIN）来捕获不同的波形风格。", "result": "MD-ViSCo在两个公开数据集上进行了多方向波形生成，平均表现优于最先进的基线模型（NabNet和PPG2ABP）。平均绝对误差（MAE）降低了8.8%，皮尔逊相关系数（PC）提高了4.9%。生成的ABP波形满足AAMI标准，并在BHS标准上达到B级，优于所有基线。", "conclusion": "MD-ViSCo通过消除为每个任务开发独立模型的需要，提供了一个统一的框架，能够使用单个模型处理医疗监测中任何类型的生命体征波形。", "translation": "尽管深度学习方法在从源生命体征波形生成目标生命体征波形方面取得了显著进展，但大多数现有模型都专门针对特定的源-目标对设计。这需要不同的模型架构、优化过程和预处理流程，导致模型数量众多，阻碍了其在临床环境中的可用性。为了解决这一限制，我们提出了多方向生命体征转换器（MD-ViSCo），这是一个统一的框架，能够使用单个模型从任何单一输入波形生成任何目标波形，如心电图（ECG）、光体积描记图（PPG）或动脉血压（ABP）。MD-ViSCo采用了一个浅层的1维U-Net，并集成了Swin Transformer，利用自适应实例归一化（AdaIN）来捕获不同的波形风格。为了评估MD-ViSCo的有效性，我们在两个公开数据集上进行了多方向波形生成。我们的框架在所有波形类型上平均超越了最先进的基线模型（NabNet和PPG2ABP），在两个数据集上平均绝对误差（MAE）降低了8.8%，皮尔逊相关系数（PC）提高了4.9%。此外，生成的ABP波形满足医疗仪器促进协会（AAMI）标准，并在英国高血压学会（BHS）标准上达到B级，优于所有基线。通过消除为每个任务开发独立模型的需要，我们相信这项工作提供了一个统一的框架，可以使用单个模型处理医疗监测中任何类型的生命体征波形。", "summary": "本研究提出MD-ViSCo，一个统一的深度学习模型，旨在解决现有生命体征波形转换模型仅限于特定源-目标对的问题。MD-ViSCo采用1D U-Net与Swin Transformer结合，并利用AdaIN实现多方向波形转换。实验结果表明，MD-ViSCo在两个公开数据集上优于现有SOTA模型，显著降低了MAE并提高了PC，且生成的ABP波形满足医疗标准，展示了其在临床应用中的潜力。", "keywords": "生命体征波形转换, 统一模型, 深度学习, MD-ViSCo, 医疗监测", "comments": "MD-ViSCo的创新之处在于其提出了一个统一的模型框架，能够处理多种生命体征波形之间的转换，这显著简化了临床应用中的模型部署和管理。其结合U-Net和Swin Transformer并利用AdaIN捕捉波形风格的方法是其性能优越的关键。这项工作的重要性在于它解决了现有模型的碎片化问题，为医疗监测提供了一个更实用、高效的解决方案。"}}
{"id": "2506.08028", "title": "Sensor Fusion for Track Geometry Monitoring: Integrating On-Board Data and Degradation Models via Kalman Filtering", "authors": ["Huy Truong-Ba", "Jacky Chin", "Michael E. Cholette", "Pietro Borghesani"], "summary": "Track geometry monitoring is essential for maintaining the safety and\nefficiency of railway operations. While Track Recording Cars (TRCs) provide\naccurate measurements of track geometry indicators, their limited availability\nand high operational costs restrict frequent monitoring across large rail\nnetworks. Recent advancements in on-board sensor systems installed on\nin-service trains offer a cost-effective alternative by enabling\nhigh-frequency, albeit less accurate, data collection. This study proposes a\nmethod to enhance the reliability of track geometry predictions by integrating\nlow-accuracy sensor signals with degradation models through a Kalman filter\nframework. An experimental campaign using a low-cost sensor system mounted on a\nTRC evaluates the proposed approach. The results demonstrate that incorporating\nfrequent sensor data significantly reduces prediction uncertainty, even when\nthe data is noisy. The study also investigates how the frequency of data\nrecording influences the size of the credible prediction interval, providing\nguidance on the optimal deployment of on-board sensors for effective track\nmonitoring and maintenance planning.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08028v1", "AI": {"title_translation": "轨道几何监测的传感器融合：通过卡尔曼滤波集成车载数据和退化模型", "tldr": "本研究提出了一种通过卡尔曼滤波将低精度车载传感器数据与轨道几何退化模型相结合的方法，以提高预测可靠性，并证明频繁传感器数据能显著降低预测不确定性。", "motivation": "轨道几何监测对于铁路运营的安全性和效率至关重要。传统的轨道检测车(TRCs)虽然提供精确测量，但其有限的可用性和高运营成本限制了大规模铁路网络的频繁监测。车载传感器系统提供了高频、低成本的替代方案，但数据精度较低，因此需要一种方法来提高预测的可靠性。", "method": "本研究提出了一种通过卡尔曼滤波框架将低精度传感器信号与退化模型相结合的方法，以提高轨道几何预测的可靠性。该方法通过将低成本传感器系统安装在轨道检测车上进行实验评估。", "result": "结果表明，即使数据存在噪声，整合频繁的传感器数据也能显著降低预测不确定性。研究还探讨了数据记录频率如何影响可信预测区间的范围。", "conclusion": "通过卡尔曼滤波整合车载低精度传感器数据和退化模型，可以有效提高轨道几何预测的可靠性，并降低不确定性。研究结果为车载传感器的最佳部署提供了指导，以实现有效的轨道监测和维护规划。", "translation": "轨道几何监测对于维持铁路运营的安全性和效率至关重要。虽然轨道检测车(TRCs)能提供精确的轨道几何指标测量，但其有限的可用性和高运营成本限制了其在大型铁路网络中的频繁监测。车载传感器系统安装在在役列车上，提供了经济高效的替代方案，能够实现高频但精度较低的数据采集。本研究提出了一种通过卡尔曼滤波框架，将低精度传感器信号与退化模型相结合，以提高轨道几何预测可靠性的方法。一项使用安装在TRC上的低成本传感器系统进行的实验评估了所提出的方法。结果表明，即使数据存在噪声，整合频繁的传感器数据也能显著降低预测不确定性。本研究还探讨了数据记录频率如何影响可信预测区间的范围，为车载传感器的最佳部署提供了指导，以实现有效的轨道监测和维护规划。", "summary": "本研究旨在解决传统轨道检测车(TRCs)在轨道几何监测方面成本高昂且频率受限的问题。论文提出了一种创新的传感器融合方法，通过卡尔曼滤波框架将车载低精度传感器数据与轨道退化模型相结合，从而提高轨道几何预测的可靠性。实验结果验证了该方法的有效性，表明频繁的传感器数据（即使有噪声）也能显著减少预测不确定性。此外，研究还分析了数据记录频率对预测区间的影响，为车载传感器的优化部署提供了实用指导，以支持高效的轨道监测和维护规划。", "keywords": "传感器融合, 轨道几何监测, 卡尔曼滤波, 车载传感器, 退化模型", "comments": "该论文提出了一种实用的解决方案，通过融合低成本、高频的车载传感器数据与退化模型，克服了传统轨道检测车监测的局限性。其创新之处在于利用卡尔曼滤波有效处理低精度但频繁的数据，从而在保证成本效益的同时提高了预测的可靠性。这对于铁路维护规划具有重要意义，有助于实现更主动、更高效的轨道管理。"}}
{"id": "2506.08110", "title": "Fair Diversity Maximization with Few Representatives", "authors": ["Florian Adriaens", "Nikolaj Tatti"], "summary": "Diversity maximization problem is a well-studied problem where the goal is to\nfind $k$ diverse items. Fair diversity maximization aims to select a diverse\nsubset of $k$ items from a large dataset, while requiring that each group of\nitems be well represented in the output. More formally, given a set of items\nwith labels, our goal is to find $k$ items that maximize the minimum pairwise\ndistance in the set, while maintaining that each label is represented within\nsome budget. In many cases, one is only interested in selecting a handful (say\na constant) number of items from each group. In such scenario we show that a\nrandomized algorithm based on padded decompositions improves the\nstate-of-the-art approximation ratio to $\\sqrt{\\log(m)}/(3m)$, where $m$ is the\nnumber of labels. The algorithms work in several stages: ($i$) a preprocessing\npruning which ensures that points with the same label are far away from each\nother, ($ii$) a decomposition phase, where points are randomly placed in\nclusters such that there is a feasible solution with maximum one point per\ncluster and that any feasible solution will be diverse, $(iii)$ assignment\nphase, where clusters are assigned to labels, and a representative point with\nthe corresponding label is selected from each cluster. We experimentally verify\nthe effectiveness of our algorithm on large datasets.", "comment": "To appear in KDD 2025", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.08110v1", "AI": {"title_translation": "少量代表的公平多样性最大化", "tldr": "本文提出了一种基于填充分解的随机算法，用于解决每个组仅需少量代表的公平多样性最大化问题，并将近似比率提高到$\\\\sqrt{\\\\log(m)}/(3m)$。", "motivation": "多样性最大化是一个被广泛研究的问题，目标是找到k个多样化的项目。公平多样性最大化旨在从大型数据集中选择一个多样化的k个项目子集，同时要求每个项目组在输出中得到很好的代表。更具体地说，给定一组带有标签的项目，目标是找到k个项目，使集合中的最小成对距离最大化，同时保持每个标签都在预算内得到代表。在许多情况下，人们只对从每个组中选择少量（例如常数）项目感兴趣。", "method": "本文提出了一种基于填充分解的随机算法。该算法分几个阶段进行：(i) 预处理剪枝，确保具有相同标签的点彼此相距很远；(ii) 分解阶段，将点随机放置在簇中，使得每个簇最多有一个点的可行解，并且任何可行解都将是多样化的；(iii) 分配阶段，将簇分配给标签，并从每个簇中选择具有相应标签的代表点。", "result": "该算法将最先进的近似比率提高到$\\\\sqrt{\\\\log(m)}/(3m)$，其中m是标签的数量。通过实验验证了该算法在大型数据集上的有效性。", "conclusion": "本文提出的基于填充分解的随机算法有效解决了少量代表的公平多样性最大化问题，在近似比率上取得了显著改进，并在大型数据集上验证了其有效性。", "translation": "多样性最大化问题是一个被充分研究的问题，目标是找到k个多样化的项目。公平多样性最大化旨在从大型数据集中选择一个多样化的k个项目子集，同时要求每个项目组在输出中得到很好的代表。更具体地说，给定一组带有标签的项目，我们的目标是找到k个项目，使集合中的最小成对距离最大化，同时保持每个标签都在预算内得到代表。在许多情况下，人们只对从每个组中选择少量（例如常数）项目感兴趣。在这种情况下，我们表明基于填充分解的随机算法将最先进的近似比率提高到$\\\\sqrt{\\\\log(m)}/(3m)$，其中m是标签的数量。该算法分几个阶段进行：(i) 预处理剪枝，确保具有相同标签的点彼此相距很远；(ii) 分解阶段，将点随机放置在簇中，使得每个簇最多有一个点的可行解，并且任何可行解都将是多样化的；(iii) 分配阶段，将簇分配给标签，并从每个簇中选择具有相应标签的代表点。我们通过实验验证了该算法在大型数据集上的有效性。", "summary": "本文研究了公平多样性最大化问题，即在保证每个组得到充分代表（特别是当每个组仅需少量代表时）的前提下，从大型数据集中选择一个多样化的k个项目子集。文中提出了一种基于填充分解的随机算法，该算法包括预处理剪枝、分解和分配三个阶段。该方法将近似比率提高到$\\\\sqrt{\\\\log(m)}/(3m)$，并通过实验证明了其在大型数据集上的有效性。", "keywords": "公平多样性最大化, 随机算法, 近似比率, 填充分解, 组代表性", "comments": "本文的创新之处在于，针对公平多样性最大化问题中每个组仅需少量代表的特定场景，提出了一种新的随机算法。该算法显著改善了近似比率，这对于需要平衡代表性的实际应用具有重要意义。"}}
{"id": "2506.08043", "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.08043v1", "AI": {"title_translation": "神经增强型开尔文小波：多抓手实时软组织变形", "tldr": "提出一种结合开尔文小波和神经网络的物理信息模拟器，实现实时、准确的软组织变形模拟，适用于外科手术机器人和医疗训练。", "motivation": "外科机器人和医疗训练中，快速准确的软组织变形模拟至关重要。", "method": "引入了一种新颖的物理信息神经网络模拟器，将基于开尔文小波的先验知识整合到神经网络中，首次利用开尔文小波进行数据驱动软组织建模中的残差学习和正则化。通过结合线性和非线性软组织响应的大规模有限元法(FEM)模拟来改进神经网络预测。", "result": "该方法提高了不同神经网络架构的预测准确性和物理一致性，同时保持低延迟以实现实时性能。通过执行高保真模拟标准腹腔镜组织抓取工具使用的精确手术操作，证明了其有效性。", "conclusion": "开尔文小波增强学习是外科应用中实时、物理感知软组织模拟的强大而高效的策略。", "translation": "软组织变形的快速准确模拟是外科机器人和医疗训练的关键因素。在本文中，我们引入了一种新颖的物理信息神经网络模拟器，以逼真和实时的方式近似软组织变形。我们的框架将基于开尔文小波的先验知识整合到神经网络模拟器中，使其成为第一个利用开尔文小波进行数据驱动软组织建模中的残差学习和正则化的方法。通过结合线性和非线性软组织响应的大规模有限元法（FEM）模拟，我们的方法改进了不同架构的神经网络预测，在保持低延迟以实现实时性能的同时，提高了准确性和物理一致性。我们通过执行高保真模拟标准腹腔镜组织抓取工具使用的精确手术操作，证明了我们方法的有效性。这些结果确立了开尔文小波增强学习作为外科应用中实时、物理感知软组织模拟的强大而高效的策略。", "summary": "本文提出了一种新颖的物理信息神经网络模拟器，名为“神经增强型开尔文小波”，用于实时、准确地模拟软组织变形。该框架首次将开尔文小波作为先验知识引入神经网络，用于残差学习和正则化。通过整合大规模有限元法(FEM)模拟数据，该方法显著提高了神经网络预测的准确性和物理一致性，同时保持了实时性能，并在模拟外科手术操作中展现出高保真度。", "keywords": "软组织变形, 开尔文小波, 神经网络, 实时模拟, 外科机器人", "comments": "这篇论文的创新点在于首次将开尔文小波（Kelvinlet）引入到数据驱动的软组织建模中，并将其用于神经网络的残差学习和正则化，从而有效地结合了物理先验知识和数据驱动模型的优势。这种结合不仅提高了模拟的准确性和物理真实性，还实现了实时性能，对于外科机器人和医疗训练领域具有重要的实际应用价值。"}}
{"id": "2506.08445", "title": "GPS Spoofing Attacks on AI-based Navigation Systems with Obstacle Avoidance in UAV", "authors": ["Ji Hyuk Jung", "Mi Yeon Hong", "Ji Won Yoon"], "summary": "Recently, approaches using Deep Reinforcement Learning (DRL) have been\nproposed to solve UAV navigation systems in complex and unknown environments.\nHowever, despite extensive research and attention, systematic studies on\nvarious security aspects have not yet been conducted. Therefore, in this paper,\nwe conduct research on security vulnerabilities in DRL-based navigation\nsystems, particularly focusing on GPS spoofing attacks against the system. Many\nrecent basic DRL-based navigation systems fundamentally share an efficient\nstructure. This paper presents an attack model that operates through GPS\nspoofing attacks briefly modeling the range of spoofing attack against EKF\nsensor fusion of PX4 autopilot, and combine this with the DRL-based system to\ndesign attack scenarios that are closer to reality. Finally, this paper\nexperimentally demonstrated that attacks are possible both in the basic DRL\nsystem and in attack models combining the DRL system with PX4 autopilot system.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08445v1", "AI": {"title_translation": "无人机中基于AI的避障导航系统上的GPS欺骗攻击", "tldr": "本文研究了针对基于深度强化学习的无人机导航系统的GPS欺骗攻击，并实验证明了攻击的可能性。", "motivation": "尽管基于深度强化学习（DRL）的无人机导航系统在复杂和未知环境中取得了进展，但其安全方面尚未得到系统性研究。因此，本文旨在探究DRL导航系统中的安全漏洞，特别是GPS欺骗攻击。", "method": "本文提出了一个通过GPS欺骗攻击操作的攻击模型，简要模拟了针对PX4自动驾驶仪EKF传感器融合的欺骗攻击范围，并将其与基于DRL的系统结合，设计出更接近现实的攻击场景。", "result": "实验证明，攻击在基本的DRL系统以及结合了DRL系统与PX4自动驾驶仪系统的攻击模型中都是可行的。", "conclusion": "针对基于深度强化学习的无人机导航系统，GPS欺骗攻击是可行的，这揭示了此类系统在安全方面的脆弱性。", "translation": "最近，已经提出了使用深度强化学习（DRL）的方法来解决复杂和未知环境中的无人机导航系统问题。然而，尽管进行了广泛的研究和关注，但尚未对各种安全方面进行系统性研究。因此，在本文中，我们对基于DRL的导航系统中的安全漏洞进行了研究，特别关注针对该系统的GPS欺骗攻击。许多最近基本的基于DRL的导航系统从根本上共享一个高效的结构。本文提出了一个通过GPS欺骗攻击操作的攻击模型，简要模拟了针对PX4自动驾驶仪EKF传感器融合的欺骗攻击范围，并将其与基于DRL的系统结合，设计出更接近现实的攻击场景。最后，本文通过实验证明，在基本的DRL系统以及将DRL系统与PX4自动驾驶仪系统结合的攻击模型中，攻击都是可能的。", "summary": "本文研究了基于深度强化学习（DRL）的无人机导航系统的安全漏洞，特别关注GPS欺骗攻击。文章提出了一个攻击模型，结合了GPS欺骗对PX4自动驾驶仪EKF传感器融合的影响与DRL系统，设计了现实的攻击场景。实验结果表明，无论是基本的DRL系统还是结合了PX4自动驾驶仪的DRL系统，都容易受到此类攻击。", "keywords": "GPS欺骗攻击, 深度强化学习, 无人机导航, 安全漏洞, PX4自动驾驶仪", "comments": "本文填补了DRL-based导航系统安全研究的空白，首次系统性地探讨了GPS欺骗攻击对其影响，具有重要的理论和实践意义。"}}
{"id": "2506.08179", "title": "MBTModelGenerator: A software tool for reverse engineering of Model-based Testing (MBT) models from clickstream data of web applications", "authors": ["Sasidhar Matta", "Vahid Garousi"], "summary": "Automated testing has become a standard practice in software engineering, yet\nthe creation of test models and suites remains labor-intensive. To reduce this\neffort, we developed an open-source tool that automatically generates\nModel-Based Testing (MBT) models from clickstream data collected during user\ninteraction with web applications. The tool captures UI events, transforms them\ninto state-transition models, and exports the result in a format compatible\nwith the GraphWalker MBT tool. This enables immediate test execution without\nthe need for manual model creation. The approach lowers the barrier to MBT\nadoption by leveraging actual usage behavior and reducing the reliance on\nupfront modeling. This technical report documents the system requirements,\ndesign decisions, implementation details, testing process, and empirical\nevaluation of the tool, which is publicly available as open-source.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08179v1", "AI": {"title_translation": "MBTModelGenerator：一种从Web应用程序点击流数据逆向工程基于模型测试（MBT）模型的软件工具", "tldr": "MBTModelGenerator工具通过分析Web应用点击流数据，自动生成MBT模型，从而简化测试模型创建并降低MBT采用门槛。", "motivation": "软件工程中自动化测试已是标准实践，但测试模型和测试套件的创建仍然是劳动密集型工作，需要减少此项工作量。", "method": "开发了一个开源工具MBTModelGenerator，它从用户与Web应用程序交互期间收集的点击流数据中自动生成基于模型测试（MBT）模型。该工具捕获UI事件，将其转换为状态-转换模型，并以与GraphWalker MBT工具兼容的格式导出结果。", "result": "该工具能够立即执行测试，无需手动创建模型。它利用实际使用行为并减少对前期建模的依赖，从而降低了MBT的采用门槛。", "conclusion": "MBTModelGenerator工具通过自动化MBT模型生成，显著降低了MBT的采用门槛，并提高了测试效率。", "translation": "自动化测试已成为软件工程中的标准实践，但测试模型和测试套件的创建仍然是劳动密集型工作。为了减少这项工作量，我们开发了一个开源工具，可以根据用户与Web应用程序交互期间收集的点击流数据自动生成基于模型测试（MBT）模型。该工具捕获UI事件，将其转换为状态-转换模型，并以与GraphWalker MBT工具兼容的格式导出结果。这使得无需手动创建模型即可立即执行测试。该方法通过利用实际使用行为并减少对前期建模的依赖，降低了MBT的采用门槛。本技术报告记录了该工具的系统要求、设计决策、实现细节、测试过程和实证评估，该工具已作为开源项目公开发布。", "summary": "MBTModelGenerator是一个开源工具，旨在通过逆向工程Web应用程序的点击流数据来自动化基于模型测试（MBT）模型的生成。该工具将UI事件转换为状态-转换模型，并输出与GraphWalker兼容的格式，从而消除了手动模型创建的需求，降低了MBT的实施难度，并提高了测试效率。", "keywords": "基于模型测试, 逆向工程, 点击流数据, 自动化测试, Web应用程序", "comments": "该工具通过自动化MBT模型生成，显著降低了基于模型测试的准入门槛，使得测试人员可以更便捷地利用用户实际行为数据进行测试，具有较高的实用价值和创新性。"}}
{"id": "2506.08291", "title": "TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation", "authors": ["Won Kyung Do", "Matthew Strong", "Aiden Swann", "Boshu Lei", "Monroe Kennedy III"], "summary": "Advanced dexterous manipulation involving multiple simultaneous contacts\nacross different surfaces, like pinching coins from ground or manipulating\nintertwined objects, remains challenging for robotic systems. Such tasks exceed\nthe capabilities of vision and proprioception alone, requiring high-resolution\ntactile sensing with calibrated physical metrics. Raw optical tactile sensor\nimages, while information-rich, lack interpretability and cross-sensor\ntransferability, limiting their real-world utility. TensorTouch addresses this\nchallenge by integrating finite element analysis with deep learning to extract\ncomprehensive contact information from optical tactile sensors, including\nstress tensors, deformation fields, and force distributions at pixel-level\nresolution. The TensorTouch framework achieves sub-millimeter position accuracy\nand precise force estimation while supporting large sensor deformations crucial\nfor manipulating soft objects. Experimental validation demonstrates 90% success\nin selectively grasping one of two strings based on detected motion, enabling\nnew contact-rich manipulation capabilities previously inaccessible to robotic\nsystems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08291v1", "AI": {"title_translation": "TensorTouch：用于灵巧操作的高分辨率应力张量和变形的触觉传感器校准", "tldr": "TensorTouch 结合有限元分析和深度学习，从光学触觉传感器中提取详细的接触信息（应力、变形、力），以实现更灵巧的机器人操作。", "motivation": "机器人系统在涉及多点同时接触的复杂灵巧操作（如从地面捏硬币或操作缠绕物体）方面面临挑战，这需要高分辨率的触觉传感和校准的物理指标，而原始光学触觉图像缺乏可解释性和跨传感器可迁移性。", "method": "TensorTouch 通过将有限元分析与深度学习相结合，从光学触觉传感器中提取像素级分辨率的综合接触信息，包括应力张量、变形场和力分布。", "result": "TensorTouch 框架实现了亚毫米级的定位精度和精确的力估计，同时支持对软物体操作至关重要的大传感器变形。实验验证表明，通过检测到的运动选择性抓取两根线中的一根成功率达到90%。", "conclusion": "TensorTouch 显著提高了机器人系统的灵巧操作能力，使其能够执行以前无法实现的接触密集型任务。", "translation": "涉及跨不同表面的多个同时接触的先进灵巧操作，例如从地面捏硬币或操作缠绕的物体，对机器人系统来说仍然具有挑战性。此类任务超出了仅凭视觉和本体感觉的能力，需要具有校准物理指标的高分辨率触觉传感。原始光学触觉传感器图像虽然信息丰富，但缺乏可解释性和跨传感器可迁移性，限制了它们的实际应用。TensorTouch 通过将有限元分析与深度学习相结合，从光学触觉传感器中提取全面的接触信息，包括像素级分辨率的应力张量、变形场和力分布，从而解决了这一挑战。TensorTouch 框架实现了亚毫米级的定位精度和精确的力估计，同时支持对软物体操作至关重要的大传感器变形。实验验证表明，通过检测到的运动选择性抓取两根线中的一根成功率达到90%，从而实现了机器人系统以前无法实现的新的接触密集型操作能力。", "summary": "TensorTouch 通过整合有限元分析与深度学习，使光学触觉传感器能够提供高分辨率的接触信息，包括应力张量、变形和力分布。该框架在定位和力估计方面实现了高精度，支持大传感器变形，并显著增强了机器人执行复杂任务（如抓取软物体或缠绕物体）的灵巧操作能力。", "keywords": "触觉传感器, 应力张量, 灵巧操作, 有限元分析, 深度学习", "comments": "这项研究通过将有限元分析与深度学习相结合，创新性地从光学触觉传感器中提取高分辨率的应力张量、变形和力分布，从而极大地提高了机器人系统的灵巧操作能力。它解决了现有触觉传感器数据缺乏可解释性和可迁移性的问题，为机器人执行复杂的接触密集型任务（如操作软物体或缠绕物体）开辟了新的可能性。"}}
{"id": "2506.08163", "title": "Spectral Domain Neural Reconstruction for Passband FMCW Radars", "authors": ["Harshvardhan Takawale", "Nirupam Roy"], "summary": "We present SpINRv2, a neural framework for high-fidelity volumetric\nreconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.\nExtending our prior work (SpINR), this version introduces enhancements that\nallow accurate learning under high start frequencies-where phase aliasing and\nsub-bin ambiguity become prominent. Our core contribution is a fully\ndifferentiable frequency-domain forward model that captures the complex radar\nresponse using closed-form synthesis, paired with an implicit neural\nrepresentation (INR) for continuous volumetric scene modeling. Unlike\ntime-domain baselines, SpINRv2 directly supervises the complex frequency\nspectrum, preserving spectral fidelity while drastically reducing computational\noverhead. Additionally, we introduce sparsity and smoothness regularization to\ndisambiguate sub-bin ambiguities that arise at fine range resolutions.\nExperimental results show that SpINRv2 significantly outperforms both classical\nand learning-based baselines, especially under high-frequency regimes,\nestablishing a new benchmark for neural radar-based 3D imaging.", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.23313", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08163v1", "AI": {"title_translation": "用于通带FMCW雷达的谱域神经重建", "tldr": "SpINRv2是一个新的神经网络框架，用于使用FMCW雷达进行高保真体积重建，通过直接在频域监督并引入正则化，解决了高起始频率下的相位混叠和子箱模糊问题，性能优于现有方法。", "motivation": "FMCW雷达在高起始频率下进行体积重建时，会面临相位混叠和子箱模糊等问题，影响重建的准确性。现有方法可能无法很好地解决这些问题，且计算开销较大。", "method": "该论文提出了SpINRv2，一个神经框架。其核心贡献是一个完全可微分的频域正向模型，该模型使用闭合形式合成捕获复杂的雷达响应，并与隐式神经表示（INR）结合用于连续体积场景建模。SpINRv2直接监督复杂的频率谱，而不是时域基线。此外，引入了稀疏性和平滑性正则化来消除在精细距离分辨率下出现的子箱模糊。", "result": "实验结果表明，SpINRv2显著优于经典和基于学习的基线方法，尤其是在高频条件下，为基于神经雷达的3D成像建立了新的基准。", "conclusion": "SpINRv2通过其创新的频域正向模型和正则化策略，成功解决了高频FMCW雷达体积重建中的挑战，实现了卓越的性能，并为该领域设定了新标准。", "translation": "我们提出了SpINRv2，一个用于使用调频连续波（FMCW）雷达进行高保真体积重建的神经框架。作为我们之前工作（SpINR）的扩展，此版本引入了增强功能，允许在高起始频率下进行精确学习——在高起始频率下，相位混叠和子箱模糊变得突出。我们的核心贡献是一个完全可微分的频域正向模型，该模型使用闭合形式合成捕获复杂的雷达响应，并与用于连续体积场景建模的隐式神经表示（INR）配对。与时域基线不同，SpINRv2直接监督复数频率谱，在保持频谱保真度的同时大幅降低了计算开销。此外，我们引入了稀疏性和平滑性正则化，以消除在精细距离分辨率下出现的子箱模糊。实验结果表明，SpINRv2显著优于经典和基于学习的基线，尤其是在高频条件下，为基于神经雷达的3D成像建立了新的基准。", "summary": "SpINRv2是一个用于FMCW雷达高保真体积重建的神经框架。它通过引入一个可微分的频域正向模型和隐式神经表示，解决了高起始频率下的相位混叠和子箱模糊问题。该方法直接在频域进行监督，减少了计算开销，并通过正则化进一步提高精度。实验证明，SpINRv2在高频条件下优于现有方法，为雷达3D成像设定了新标准。", "keywords": "FMCW雷达, 神经重建, 频域, 隐式神经表示, 3D成像", "comments": "该论文的核心创新在于其完全可微分的频域正向模型，这使得在高频FMCW雷达数据下进行准确的神经重建成为可能，同时显著降低了计算成本。引入的稀疏性和平滑性正则化也有效地解决了子箱模糊问题。这项工作为基于雷达的3D成像领域带来了重要的进展，尤其是在处理具有挑战性的高频数据方面。"}}
{"id": "2506.08761", "title": "Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification", "authors": ["Matthias Beckmann", "Robert Beinert", "Jonas Bresch"], "summary": "The Radon cumulative distribution transform (R-CDT), is an easy-to-compute\nfeature extractor that facilitates image classification tasks especially in the\nsmall data regime. It is closely related to the sliced Wasserstein distance and\nprovably guaranties the linear separability of image classes that emerge from\ntranslations or scalings. In many real-world applications, like the recognition\nof watermarks in filigranology, however, the data is subject to general affine\ntransformations originating from the measurement process. To overcome this\nissue, we recently introduced the so-called max-normalized R-CDT that only\nrequires elementary operations and guaranties the separability under arbitrary\naffine transformations. The aim of this paper is to continue our study of the\nmax-normalized R-CDT especially with respect to its robustness against\nnon-affine image deformations. Our sensitivity analysis shows that its\nseparability properties are stable provided the Wasserstein-infinity distance\nbetween the samples can be controlled. Since the Wasserstein-infinity distance\nonly allows small local image deformations, we moreover introduce a\nmean-normalized version of the R-CDT. In this case, robustness relates to the\nWasserstein-2 distance and also covers image deformations caused by impulsive\nnoise for instance. Our theoretical results are supported by numerical\nexperiments showing the effectiveness of our novel feature extractors as well\nas their robustness against local non-affine deformations and impulsive noise.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08761v1", "AI": {"title_translation": "归一化Radon累积分布变换在基于最优传输的图像分类中实现不变性和鲁棒性", "tldr": "本文引入并研究了归一化Radon累积分布变换（max-normalized R-CDT和mean-normalized R-CDT），以提高基于最优传输的图像分类在仿射变换和非仿射变形下的不变性和鲁棒性，并通过理论和实验证明了其有效性。", "motivation": "现有的Radon累积分布变换（R-CDT）在图像分类中对平移和缩放具有不变性，但在实际应用中数据常受更普遍的仿射变换影响。此外，还需要解决非仿射图像变形（如局部变形和脉冲噪声）带来的鲁棒性问题。", "method": "引入了max-normalized R-CDT来解决仿射变换下的可分性问题。进一步引入了mean-normalized R-CDT来增强对非仿射图像变形（如脉冲噪声）的鲁棒性。通过敏感性分析研究其可分性属性，并使用数值实验验证。", "result": "敏感性分析表明，max-normalized R-CDT在Wasserstein-infinity距离可控时，其可分性属性是稳定的。mean-normalized R-CDT的鲁棒性与Wasserstein-2距离相关，并能覆盖由脉冲噪声引起的图像变形。数值实验支持了新型特征提取器的有效性及其对局部非仿射变形和脉冲噪声的鲁棒性。", "conclusion": "新型归一化Radon累积分布变换特征提取器在图像分类中表现出良好的有效性，并且对局部非仿射变形和脉冲噪声具有鲁棒性。", "translation": "Radon累积分布变换（R-CDT）是一种易于计算的特征提取器，尤其在小数据量情况下有助于图像分类任务。它与切片Wasserstein距离密切相关，并被证明能保证由平移或缩放引起的图像类别线性可分。然而，在许多实际应用中，例如水印识别，数据会受到测量过程引起的普遍仿射变换的影响。为了克服这个问题，我们最近引入了所谓的max-normalized R-CDT，它只需要基本操作，并保证在任意仿射变换下的可分性。本文旨在继续研究max-normalized R-CDT，特别是其对非仿射图像变形的鲁棒性。我们的敏感性分析表明，只要样本间的Wasserstein-infinity距离可以控制，其可分性属性就保持稳定。由于Wasserstein-infinity距离只允许小的局部图像变形，我们还引入了R-CDT的mean-normalized版本。在这种情况下，鲁棒性与Wasserstein-2距离相关，并且也涵盖了例如由脉冲噪声引起的图像变形。我们的理论结果得到了数值实验的支持，这些实验显示了我们新型特征提取器的有效性以及它们对局部非仿射变形和脉冲噪声的鲁棒性。", "summary": "本文研究了归一化Radon累积分布变换（R-CDT）在图像分类中的应用，以提高其在各种变换下的不变性和鲁棒性。针对仿射变换，引入了max-normalized R-CDT，并通过理论证明其可分性。为应对非仿射变形和脉冲噪声，进一步提出了mean-normalized R-CDT，其鲁棒性与Wasserstein-2距离相关。理论分析和数值实验均证实了这些新型特征提取器在处理复杂图像变形时的有效性和鲁棒性。", "keywords": "Radon累积分布变换, 图像分类, 仿射变换, 鲁棒性, 最优传输", "comments": "本文通过引入max-normalized R-CDT和mean-normalized R-CDT，有效解决了传统R-CDT在更广泛的图像变换（包括仿射变换和非仿射变形）下不变性和鲁棒性不足的问题。其创新在于结合最优传输理论，提出易于计算且性能优越的特征提取器，尤其在小数据量和复杂噪声环境下具有重要应用价值。"}}
{"id": "2506.08150", "title": "Compiling Metric Temporal Answer Set Programming", "authors": ["Arvid Becker", "Pedro Cabalar", "Martin Diéguez", "Javier Romero", "Susana Hahn", "Torsten Schaub"], "summary": "We develop a computational approach to Metric Answer Set Programming (ASP) to\nallow for expressing quantitative temporal constrains, like durations and\ndeadlines. A central challenge is to maintain scalability when dealing with\nfine-grained timing constraints, which can significantly exacerbate ASP's\ngrounding bottleneck. To address this issue, we leverage extensions of ASP with\ndifference constraints, a simplified form of linear constraints, to handle\ntime-related aspects externally. Our approach effectively decouples metric ASP\nfrom the granularity of time, resulting in a solution that is unaffected by\ntime precision.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08150v1", "AI": {"title_translation": "编译度量时间答案集编程", "tldr": "该论文提出了一种度量答案集编程（ASP）的计算方法，通过利用差分约束来处理定量时间约束，有效地将度量ASP与时间粒度解耦，从而提高了可扩展性。", "motivation": "为了在度量答案集编程（ASP）中表达定量时间约束（如持续时间和截止日期），同时解决细粒度时间约束导致ASP接地瓶颈加剧所带来的可扩展性挑战。", "method": "该方法利用带有差分约束（一种简化的线性约束形式）的ASP扩展，在外部处理时间相关方面。这种方法有效地将度量ASP与时间粒度解耦。", "result": "该解决方案不受时间精度影响，并且在处理细粒度时间约束时能有效保持可扩展性。", "conclusion": "该方法有效地将度量答案集编程（ASP）与时间粒度解耦，从而得到一个不受时间精度影响的解决方案，并解决了细粒度时间约束带来的可扩展性问题。", "translation": "我们开发了一种度量答案集编程（ASP）的计算方法，以允许表达定量时间约束，如持续时间和截止日期。一个核心挑战是在处理细粒度时间约束时保持可扩展性，这会显著加剧ASP的接地瓶颈。为了解决这个问题，我们利用带有差分约束的ASP扩展，这是一种简化形式的线性约束，以在外部处理时间相关方面。我们的方法有效地将度量ASP与时间粒度解耦，从而得到一个不受时间精度影响的解决方案。", "summary": "本文介绍了一种用于度量答案集编程（ASP）的计算方法，旨在整合持续时间和截止日期等定量时间约束。它通过利用带有差分约束的ASP扩展来解决细粒度时间约束导致ASP接地瓶颈加剧的可扩展性问题。这种外部处理时间方面的方法有效地将度量ASP与时间粒度分离，从而使解决方案对时间精度变化具有鲁棒性。", "keywords": "度量答案集编程, 时间约束, 可扩展性, 差分约束, 时间精度", "comments": "该论文解决了度量答案集编程在处理精确时间时面临的关键可扩展性问题，这是时间推理中的常见挑战。其创新之处在于使用差分约束来外部化时间处理，使解决方案独立于时间粒度。这种方法有望显著提高度量ASP在需要细粒度时间推理的实际问题中的实用性。"}}
{"id": "2506.08022", "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08022v1", "AI": {"title_translation": "大型多模态模型通过对抗性负样本挖掘进行模态平衡偏好优化", "tldr": "本文提出了模态平衡偏好优化（MBPO）框架，通过对抗性负样本挖掘和在线数据结合GRPO来解决大型多模态模型（LMMs）的模态不平衡问题，从而提升性能并减少幻觉。", "motivation": "现有LMMs在推理时存在严重的模态不平衡，即语言先验偏差超过视觉输入，这限制了其泛化能力并导致幻觉。当前的偏好优化方法未能有效抑制LLM骨干的内部偏见，且过度依赖离线数据，缺乏适应动态分布变化的能力。此外，GRPO在LMM对齐中的应用仍未被充分探索。", "method": "本文提出了一种新颖的偏好学习框架——模态平衡偏好优化（MBPO），以解决LMMs中的模态不平衡问题。MBPO通过对抗性扰动输入图像生成“困难负样本”（即被LLM偏见误导的拒绝响应），以此构建更有效的离线偏好数据集。同时，MBPO利用封闭式任务易于验证的特性生成带有验证奖励的在线响应。最后，采用GRPO结合离线-在线混合数据来训练模型。", "result": "广泛的实验表明，MBPO可以增强LMM在挑战性视觉-语言任务上的性能，并有效减少幻觉。", "conclusion": "MBPO框架成功解决了大型多模态模型中的模态不平衡问题，显著提升了模型在视觉-语言任务上的表现并有效减少了幻觉。", "translation": "大型多模态模型（LMMs）的任务适应和对齐已通过指令微调得到显著推进，并由最近的偏好优化进一步加强。然而，大多数LMMs在推理时仍遭受严重的模态不平衡，即语言先验偏差超过视觉输入，这限制了它们对下游任务的泛化能力并导致幻觉。但是，现有的LMM偏好优化方法在整理训练数据时，并未侧重于抑制其大型语言模型（LLM）骨干的内部偏差。此外，它们严重依赖离线数据，缺乏探索适应训练过程中动态分布变化的各种响应的能力。同时，组相对策略优化（GRPO）作为一种利用在线生成数据和验证奖励来提高推理能力的新方法，在LMM对齐中仍未得到充分探索。在本文中，我们提出了一种新颖的偏好学习框架——模态平衡偏好优化（MBPO），以解决LMMs中的模态不平衡问题。MBPO通过对抗性扰动输入图像生成困难负样本，即由于视觉信息使用有限而被LLM偏差误导的拒绝响应，从而构建更有效的离线偏好数据集。此外，MBPO利用封闭式任务易于验证的特性生成带有验证奖励的在线响应。然后，采用GRPO结合离线-在线混合数据来训练模型。广泛的实验表明，MBPO可以提高LMM在挑战性视觉-语言任务上的性能，并有效减少幻觉。", "summary": "本文提出了一种名为模态平衡偏好优化（MBPO）的新型框架，旨在解决大型多模态模型（LMMs）中存在的语言偏见过重导致视觉信息利用不足的模态不平衡问题。MBPO通过对抗性负样本挖掘构建高质量的离线偏好数据集，并结合在线生成的验证奖励数据，利用GRPO训练LMM。实验结果表明，MBPO显著提升了LMM在视觉-语言任务上的表现并有效减少了幻觉。", "keywords": "大型多模态模型, 偏好优化, 模态不平衡, 对抗性负样本挖掘, 幻觉减少", "comments": "这篇论文的创新点在于提出了一个结合对抗性负样本挖掘和在线数据生成的新颖偏好优化框架MBPO，以专门解决LMMs中的模态不平衡问题。通过生成“困难负样本”来明确抑制LLM的语言偏见，并结合GRPO利用在线验证数据，这为提升LMM的泛化能力和减少幻觉提供了一种有效途径，对于多模态模型的鲁棒性研究具有重要意义。"}}
{"id": "2506.08846", "title": "Addressing Pitfalls in Auditing Practices of Automatic Speech Recognition Technologies: A Case Study of People with Aphasia", "authors": ["Katelyn Xiaoying Mei", "Anna Seo Gyeong Choi", "Hilke Schellmann", "Mona Sloane", "Allison Koenecke"], "summary": "Automatic Speech Recognition (ASR) has transformed daily tasks from video\ntranscription to workplace hiring. ASR systems' growing use warrants robust and\nstandardized auditing approaches to ensure automated transcriptions of high and\nequitable quality. This is especially critical for people with speech and\nlanguage disorders (such as aphasia) who may disproportionately depend on ASR\nsystems to navigate everyday life. In this work, we identify three pitfalls in\nexisting standard ASR auditing procedures, and demonstrate how addressing them\nimpacts audit results via a case study of six popular ASR systems' performance\nfor aphasia speakers. First, audits often adhere to a single method of text\nstandardization during data pre-processing, which (a) masks variability in ASR\nperformance from applying different standardization methods, and (b) may not be\nconsistent with how users - especially those from marginalized speech\ncommunities - would want their transcriptions to be standardized. Second,\naudits often display high-level demographic findings without further\nconsidering performance disparities among (a) more nuanced demographic\nsubgroups, and (b) relevant covariates capturing acoustic information from the\ninput audio. Third, audits often rely on a single gold-standard metric -- the\nWord Error Rate -- which does not fully capture the extent of errors arising\nfrom generative AI models, such as transcription hallucinations. We propose a\nmore holistic auditing framework that accounts for these three pitfalls, and\nexemplify its results in our case study, finding consistently worse ASR\nperformance for aphasia speakers relative to a control group. We call on\npractitioners to implement these robust ASR auditing practices that remain\nflexible to the rapidly changing ASR landscape.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.08846v1", "AI": {"title_translation": "解决自动语音识别技术审计实践中的陷阱：以失语症患者为例", "tldr": "本文识别了当前ASR审计实践中的三个主要陷阱，并提出了一个更全面的审计框架，通过对失语症患者的案例研究，发现ASR对失语症患者的表现一致性差于对照组，呼吁采用更稳健的审计方法。", "motivation": "自动语音识别（ASR）系统日益普及，需要稳健和标准化的审计方法来确保转录的高质量和公平性，尤其对于依赖ASR的失语症患者等言语和语言障碍人群。现有审计程序存在缺陷，未能充分反映ASR性能的真实情况和公平性。", "method": "作者识别了现有标准ASR审计程序中的三个主要陷阱：1) 数据预处理中单一文本标准化方法的问题（掩盖性能变异性，不符合用户需求）；2) 仅显示高层级人口统计学结果，未考虑细微亚组和声学协变量的性能差异；3) 依赖单一金标准指标（词错误率），未能捕捉生成式AI模型产生的错误（如幻觉）。他们提出了一个更全面的审计框架来解决这些陷阱，并通过对六种流行ASR系统在失语症患者上的表现进行案例研究来验证。", "result": "案例研究发现，与对照组相比，ASR系统对失语症患者的表现始终更差。", "conclusion": "现有ASR审计实践存在缺陷，需要更全面和稳健的审计框架。研究呼吁从业者实施这些稳健且灵活的ASR审计实践，以适应快速变化的ASR领域。", "translation": "自动语音识别（ASR）已经改变了从视频转录到职场招聘的日常任务。ASR系统日益增长的使用需要稳健和标准化的审计方法，以确保自动化转录的高质量和公平性。这对于可能过度依赖ASR系统来应对日常生活的言语和语言障碍人群（如失语症患者）来说尤其关键。在这项工作中，我们识别了现有标准ASR审计程序中的三个陷阱，并通过对六种流行ASR系统在失语症患者上的表现进行的案例研究，展示了解决这些陷阱如何影响审计结果。首先，审计通常在数据预处理过程中遵循单一的文本标准化方法，这（a）掩盖了应用不同标准化方法时ASR性能的变异性，并且（b）可能与用户——尤其是来自边缘言语社区的用户——希望他们的转录被标准化的方式不一致。其次，审计通常显示高层级的人口统计学发现，而没有进一步考虑（a）更细微的人口统计学亚组之间以及（b）捕获输入音频声学信息的相关协变量之间的性能差异。第三，审计通常依赖单一的金标准指标——词错误率，这未能完全捕捉生成式AI模型产生的错误程度，例如转录幻觉。我们提出了一个更全面的审计框架来解决这三个陷阱，并在我们的案例研究中例证了其结果，发现与对照组相比，ASR对失语症患者的表现始终更差。我们呼吁从业者实施这些稳健的ASR审计实践，这些实践应保持灵活以适应快速变化的ASR领域。", "summary": "本文旨在解决自动语音识别（ASR）技术审计实践中的固有缺陷，尤其关注失语症患者群体。研究识别并详细阐述了当前ASR审计中的三个主要陷阱：单一文本标准化方法导致的性能评估偏差、对细微人口统计学差异和声学协变量的忽视，以及过度依赖单一错误率指标而未能捕捉生成式AI模型的复杂错误。为克服这些问题，作者提出了一个更全面的审计框架。通过对六种主流ASR系统在失语症患者中的表现进行案例研究，验证了新框架的有效性，并发现ASR对失语症患者的表现普遍劣于对照组。研究强调了采纳更稳健、灵活的ASR审计实践的重要性，以确保技术的公平性和高质量。", "keywords": "自动语音识别审计, 失语症, 性能评估, 文本标准化, 词错误率", "comments": "这篇论文很有创新性，它不仅指出了当前ASR审计实践中存在的具体且重要的缺陷，而且通过失语症患者这一特殊群体的案例研究，突出了ASR技术在边缘群体中可能存在的不公平性问题。提出的三点审计陷阱和相应的更全面的审计框架，为未来ASR系统的评估和改进提供了明确的方向。特别是对文本标准化方法多样性、细致的人口统计学分析以及多维度错误度量的关注，对于提升ASR系统的公平性和鲁棒性至关重要。其局限性可能在于案例研究的规模和具体ASR系统的选择，但其提出的框架具有普遍指导意义。"}}
{"id": "2506.09046", "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation", "authors": ["Xiaowen Ma", "Chenyang Lin", "Yao Zhang", "Volker Tresp", "Yunpu Ma"], "summary": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09046v1", "AI": {"title_translation": "代理神经网络：通过文本反向传播实现自进化的多智能体系统", "tldr": "提出Agentic Neural Network (ANN) 框架，将多智能体协作视为分层神经网络，通过前向和反向传播优化，实现智能体自进化，在多个基准测试中超越现有方法，提供可扩展的数据驱动多智能体系统。", "motivation": "当前的利用多个大型语言模型（LLMs）解决复杂高维任务的方法通常依赖于静态、手动设计的多智能体配置，这限制了其有效性。本研究旨在克服这些限制。", "method": "本文提出了代理神经网络（ANN）框架，将多智能体协作概念化为分层神经网络架构。在该设计中，每个智能体作为一个节点，每个层形成一个专注于特定子任务的合作“团队”。ANN遵循两阶段优化策略：1. 前向阶段：受神经网络前向传播启发，任务被动态分解为子任务，并逐层构建具有合适聚合方法的合作智能体团队。2. 反向阶段：模仿反向传播，通过迭代反馈改进全局和局部协作，使智能体能够自进化其角色、提示和协调。这种神经符号方法使ANN能够在训练后创建新的或专门的智能体团队。", "result": "ANN在准确性和适应性方面取得了显著提升。在四个基准数据集上，ANN在相同配置下超越了领先的多智能体基线，显示出持续的性能改进。", "conclusion": "ANN为多智能体系统提供了一个可扩展、数据驱动的框架，结合了大型语言模型的协作能力与神经网络原理的效率和灵活性。", "translation": "利用多个大型语言模型（LLMs）已被证明在解决复杂、高维任务方面是有效的，但目前的方法通常依赖于静态、手动设计的多智能体配置。为了克服这些限制，我们提出了代理神经网络（ANN），一个将多智能体协作概念化为分层神经网络架构的框架。在这种设计中，每个智能体作为一个节点，每个层形成一个专注于特定子任务的合作“团队”。代理神经网络遵循两阶段优化策略：(1) 前向阶段——受神经网络前向传播启发，任务被动态分解为子任务，并逐层构建具有合适聚合方法的合作智能体团队。(2) 反向阶段——模仿反向传播，我们通过迭代反馈改进全局和局部协作，使智能体能够自进化其角色、提示和协调。这种神经符号方法使ANN能够在训练后创建新的或专门的智能体团队，从而在准确性和适应性方面实现显著提升。在四个基准数据集上，ANN在相同配置下超越了领先的多智能体基线，显示出持续的性能改进。我们的发现表明，ANN为多智能体系统提供了一个可扩展、数据驱动的框架，结合了大型语言模型的协作能力与神经网络原理的效率和灵活性。我们计划开源整个框架。", "summary": "本文提出了代理神经网络（ANN），一个将多智能体协作视为分层神经网络架构的框架。它通过两阶段优化策略——前向阶段动态分解任务并构建智能体团队，以及反向阶段通过迭代反馈实现智能体的角色、提示和协调的自进化——来克服当前静态多智能体配置的局限性。这种神经符号方法使ANN能够创建新的或专门的智能体团队，并在四个基准数据集上超越了现有的多智能体基线，证明了其在准确性和适应性方面的显著提升，为多智能体系统提供了一个可扩展、数据驱动的解决方案。", "keywords": "代理神经网络, 多智能体系统, 大型语言模型, 反向传播, 自进化", "comments": "该论文的创新之处在于将多智能体协作系统与神经网络的概念相结合，特别是引入了“文本反向传播”机制，使得智能体能够自进化其角色和协作方式，从而克服了传统手动配置的局限性。这为构建更灵活、自适应的多智能体系统提供了一个新颖且有前景的框架。"}}
{"id": "2506.08467", "title": "Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education", "authors": ["Prakash Shukla", "Suchismita Naik", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parson"], "summary": "The growing integration of AI tools in student design projects presents an\nunresolved challenge in HCI education: how should AI-generated content be cited\nand documented? Traditional citation frameworks -- grounded in credibility,\nretrievability, and authorship -- struggle to accommodate the dynamic and\nephemeral nature of AI outputs. In this paper, we examine how undergraduate\nstudents in a UX design course approached AI usage and citation when given the\nfreedom to integrate generative tools into their design process. Through\nqualitative analysis of 35 team projects and reflections from 175 students, we\nidentify varied citation practices ranging from formal attribution to indirect\nor absent acknowledgment. These inconsistencies reveal gaps in existing\nframeworks and raise questions about authorship, assessment, and pedagogical\ntransparency. We argue for rethinking AI citation as a reflective and\npedagogical practice; one that supports metacognitive engagement by prompting\nstudents to critically evaluate how and why they used AI throughout the design\nprocess. We propose alternative strategies -- such as AI contribution\nstatements and process-aware citation models that better align with the\niterative and reflective nature of design education. This work invites\neducators to reconsider how citation practices can support meaningful\nstudent--AI collaboration.", "comment": "8 pages, EduCHI 2025: 7th Annual Symposium on HCI Education,\n  Bloomington, IN, USA, July 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08467v1", "AI": {"title_translation": "人机交互设计教育中学生与AI协作中AI来源引用的再思考", "tldr": "探讨人机交互设计教育中AI内容引用挑战，提出将AI引用作为反思性教学实践的新策略。", "motivation": "现有引用框架难以适应AI生成内容的动态性和短暂性，导致在人机交互设计教育中学生与AI协作时，AI内容的引用和文档化面临未解决的挑战。", "method": "通过对35个团队项目进行定性分析，并收集175名学生的反馈，考察了本科生在UX设计课程中如何使用AI并进行引用。", "result": "发现了学生在AI使用和引用方面存在多样化的实践，从正式归属到间接或缺失的确认；这些不一致性揭示了现有框架的不足，并引发了关于作者身份、评估和教学透明度的问题。", "conclusion": "论文主张将AI引用重新定义为一种反思性和教学实践，以支持学生批判性地评估AI的使用；并提出了AI贡献声明和过程感知引用模型等替代策略，以更好地适应设计教育的迭代和反思性质。", "translation": "随着AI工具在学生设计项目中日益深入的整合，人机交互（HCI）教育面临着一个尚未解决的挑战：AI生成的内容应如何被引用和记录？传统的引用框架——以可信度、可检索性和作者身份为基础——难以适应AI输出的动态和短暂性。在本文中，我们考察了UX设计课程中的本科生在被赋予自由将生成式工具整合到其设计过程中时，如何处理AI的使用和引用。通过对35个团队项目和175名学生反思的定性分析，我们识别出了从正式归属到间接或缺失确认的各种引用实践。这些不一致性揭示了现有框架的不足，并引发了关于作者身份、评估和教学透明度的问题。我们主张将AI引用重新思考为一种反思性和教学实践；一种通过促使学生批判性地评估他们在整个设计过程中如何以及为何使用AI来支持元认知参与的实践。我们提出了替代策略——例如AI贡献声明和过程感知引用模型，这些策略能更好地与设计教育的迭代和反思性质相符。这项工作邀请教育工作者重新思考引用实践如何能够支持有意义的学生-AI协作。", "summary": "本文探讨了人机交互设计教育中AI生成内容引用的挑战，指出传统引用框架难以适应AI输出的动态性。研究通过对本科生UX设计项目进行定性分析，发现学生在AI引用方面存在不一致性，揭示了现有框架的不足。文章提出将AI引用视为一种反思性教学实践，并建议采用AI贡献声明和过程感知引用模型等新策略，以促进学生与AI的有效协作和批判性思维。", "keywords": "AI引用, 人机交互教育, 学生-AI协作, 设计教育, 反思性实践", "comments": "这篇论文探讨了AI时代下教育领域一个非常及时且重要的问题：如何规范AI工具的使用和引用。其创新之处在于不仅仅关注技术层面的引用规范，更将其提升到教学和元认知层面，强调引用作为一种反思性实践的价值。这对于培养学生批判性思维和负责任地使用AI具有重要意义。"}}
{"id": "2506.08842", "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN Accelerator with Algorithm and Hardware Co-Design", "authors": ["Kainan Wang", "Chengyi Yang", "Chengting Yu", "Yee Sin Ang", "Bo Wang", "Aili Wang"], "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.08842v1", "AI": {"title_translation": "STI-SNN：一种0.14 GOPS/W/PE的单时间步推理FPGA SNN加速器，采用算法与硬件协同设计", "tldr": "STI-SNN是一种基于FPGA的SNN加速器，通过算法与硬件协同设计实现单时间步推理、高能效和低延迟，解决了SNN硬件处理的挑战。", "motivation": "脑启发脉冲神经网络（SNNs）因其事件驱动和高能效特性而备受关注。然而，脉冲的时间依赖性和不规则性给硬件并行处理和数据重用带来了显著挑战，导致现有加速器在处理延迟和能效方面表现不佳。为了克服这些挑战，本文引入了STI-SNN加速器，旨在满足资源受限应用对高能效、灵活性和低延迟的需求。", "method": "STI-SNN加速器通过算法与硬件协同设计实现。在算法层面，引入了基于时间高效训练（TET）损失函数的时间剪枝方法，以缓解时间步减少期间的脉冲消失，保持推理精度，并扩展TET的应用。在硬件设计中，分析数据访问模式并采用输出驻留（OS）数据流，消除存储膜电位和内存访问操作的需要。此外，基于OS数据流，提出了一种压缩和排序的脉冲表示方法，并缓存到行缓冲器中，以降低内存访问成本并提高重用效率。STI-SNN还通过调整处理单元（PEs）的计算模式和参数化计算阵列，支持不同卷积方法（如深度可分离卷积DSCs），增强硬件灵活性。最后，STI-SNN支持层间和层内并行处理。", "result": "该加速器实现了0.14 GOPS/W/PE的能效，并能进行单时间步推理。", "conclusion": "STI-SNN通过算法与硬件协同设计，有效解决了SNN硬件处理中的挑战，实现了高能效、低延迟和高灵活性，适用于资源受限的应用。", "translation": "脑启发脉冲神经网络（SNNs）因其事件驱动特性和高能效而备受关注。然而，脉冲的时间依赖性和不规则性给硬件并行处理和数据重用带来了显著挑战，导致一些现有加速器在处理延迟和能效方面表现不佳。为了克服这些挑战，我们引入了STI-SNN加速器，旨在满足资源受限应用对高能效、灵活性和低延迟的需求。该加速器通过算法与硬件协同设计。首先，STI-SNN可以进行单时间步推理。在算法层面，我们引入了一种基于时间高效训练（TET）损失函数的时间剪枝方法。该方法缓解了时间步减少期间的脉冲消失，保持了推理精度，并扩展了TET的应用。在硬件设计中，我们分析了数据访问模式并采用了输出驻留（OS）数据流，无需存储膜电位和进行内存操作。此外，基于OS数据流，我们提出了一种压缩和排序的脉冲表示方法，然后缓存到行缓冲器中，以降低内存访问成本并提高重用效率。其次，STI-SNN支持不同的卷积方法。通过调整处理单元（PEs）的计算模式和参数化计算阵列，STI-SNN可以适应基于深度可分离卷积（DSCs）的轻量级模型，进一步增强了硬件灵活性。最后，STI-SNN还支持层间和层内并行处理。对于层间并行性，我们...", "summary": "本文介绍了一种名为STI-SNN的FPGA基SNN加速器，旨在解决脉冲神经网络在硬件并行处理和数据重用方面的挑战。该加速器通过算法与硬件协同设计实现单时间步推理、高能效和低延迟。在算法层面，引入了基于TET损失函数的时间剪枝方法以保持精度；在硬件层面，采用输出驻留数据流、压缩和排序的脉冲表示以及行缓冲缓存，以优化内存访问。此外，STI-SNN支持多种卷积方法和层间层内并行处理，提升了硬件灵活性。其性能达到了0.14 GOPS/W/PE。", "keywords": "脉冲神经网络, FPGA加速器, 算法硬件协同设计, 单时间步推理, 能效", "comments": "该论文通过算法与硬件协同设计的方法，创新性地解决了SNN在硬件加速中面临的时间依赖性和数据不规则性挑战。特别是其单时间步推理能力和对多种卷积的支持，以及高效的数据流设计，使其在资源受限应用中具有重要意义。0.14 GOPS/W/PE的能效指标也显示了其潜在的实际应用价值。"}}
{"id": "2506.08205", "title": "A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts", "authors": ["Shadab Anwar Shaikh", "Kranthi Balusu", "Ayoub Soulami"], "summary": "Residual stresses, which remain within a component after processing, can\ndeteriorate performance. Accurately determining their full-field distributions\nis essential for optimizing the structural integrity and longevity. However,\nthe experimental effort required for full-field characterization is\nimpractical. Given these challenges, this work proposes a machine learning (ML)\nbased Residual Stress Generator (RSG) to infer full-field stresses from limited\nmeasurements. An extensive dataset was initially constructed by performing\nnumerous process simulations with a diverse parameter set. A ML model based on\nU-Net architecture was then trained to learn the underlying structure through\nsystematic hyperparameter tuning. Then, the model's ability to generate\nsimulated stresses was evaluated, and it was ultimately tested on actual\ncharacterization data to validate its effectiveness. The model's prediction of\nsimulated stresses shows that it achieved excellent predictive accuracy and\nexhibited a significant degree of generalization, indicating that it\nsuccessfully learnt the latent structure of residual stress distribution. The\nRSG's performance in predicting experimentally characterized data highlights\nthe feasibility of the proposed approach in providing a comprehensive\nunderstanding of residual stress distributions from limited measurements,\nthereby significantly reducing experimental efforts.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08205v1", "AI": {"title_translation": "摩擦搅拌加工零件中利用稀疏表征数据生成残余应力分布的机器学习方法", "tldr": "本文提出了一种基于U-Net架构的机器学习模型（RSG），能够从有限测量数据中推断摩擦搅拌加工零件的全场残余应力分布，从而显著减少实验工作量，并表现出高精度和泛化能力。", "motivation": "残余应力会影响零件性能，准确确定其全场分布对于优化结构完整性和寿命至关重要。然而，全场表征所需的实验工作量巨大且不切实际。", "method": "本研究提出了一种基于机器学习（ML）的残余应力生成器（RSG），采用U-Net架构。首先，通过大量过程模拟构建了广泛的数据集。然后，通过系统的超参数调整训练了ML模型以学习底层结构。最后，评估了模型生成模拟应力的能力，并在实际表征数据上进行了测试以验证其有效性。", "result": "模型在模拟应力预测上表现出卓越的预测精度和显著的泛化能力，表明它成功学习了残余应力分布的潜在结构。RSG在预测实验表征数据方面的表现突出，强调了所提出方法从有限测量中提供残余应力分布全面理解的可行性。", "conclusion": "所提出的基于机器学习的残余应力生成器（RSG）方法能够从有限测量中提供残余应力分布的全面理解，从而显著减少实验工作量。", "translation": "残余应力在加工后保留在部件内部，会降低性能。准确确定其全场分布对于优化结构完整性和寿命至关重要。然而，全场表征所需的实验工作量是不切实际的。鉴于这些挑战，本工作提出了一种基于机器学习（ML）的残余应力生成器（RSG），用于从有限测量中推断全场应力。首先通过执行大量具有多样参数集的过程模拟构建了一个广泛的数据集。然后训练了一个基于U-Net架构的ML模型，通过系统的超参数调整来学习底层结构。随后，评估了模型生成模拟应力的能力，并最终在实际表征数据上进行了测试以验证其有效性。模型对模拟应力的预测表明它获得了卓越的预测精度并表现出显著的泛化能力，表明它成功学习了残余应力分布的潜在结构。RSG在预测实验表征数据方面的性能突出了所提出方法从有限测量中提供残余应力分布全面理解的可行性，从而显著减少了实验工作量。", "summary": "本文提出了一种基于机器学习（ML）的残余应力生成器（RSG），该生成器利用U-Net架构，旨在从摩擦搅拌加工零件的稀疏表征数据中推断全场残余应力分布。鉴于全场实验表征的实际困难，RSG通过对大量过程模拟数据集进行训练。评估结果显示，该模型在模拟数据上具有出色的预测精度和泛化能力，并通过实际实验数据验证了其有效性，证明了其在显著减少全面残余应力分析所需实验工作量方面的巨大潜力。", "keywords": "残余应力, 机器学习, U-Net, 摩擦搅拌加工, 稀疏数据", "comments": "该论文通过利用机器学习克服传统实验方法的局限性，解决了材料科学中的一个实际挑战。使用U-Net架构从稀疏数据推断全场分布在残余应力领域具有创新性。其显著减少实验工作量的潜力对优化零件设计和制造过程，以及提高结构完整性和寿命具有重要意义。"}}
{"id": "2506.08534", "title": "DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View", "authors": ["Donglian Li", "Hui Guo", "Minglang Chen", "Huizhen Chen", "Jialing Chen", "Bocheng Liang", "Pengchen Liang", "Ying Tan"], "summary": "Accurate segmentation of anatomical structures in the apical four-chamber\n(A4C) view of fetal echocardiography is essential for early diagnosis and\nprenatal evaluation of congenital heart disease (CHD). However, precise\nsegmentation remains challenging due to ultrasound artifacts, speckle noise,\nanatomical variability, and boundary ambiguity across different gestational\nstages. To reduce the workload of sonographers and enhance segmentation\naccuracy, we propose DCD, an advanced deep learning-based model for automatic\nsegmentation of key anatomical structures in the fetal A4C view. Our model\nincorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,\nenabling superior multi-scale feature extraction, and a Convolutional Block\nAttention Module (CBAM) to enhance adaptive feature representation. By\neffectively capturing both local and global contextual information, DCD\nachieves precise and robust segmentation, contributing to improved prenatal\ncardiac assessment.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08534v1", "AI": {"title_translation": "DCD：一种用于胎儿超声心动图四腔心视图的语义分割模型", "tldr": "提出DCD模型，结合Dense ASPP和CBAM，用于自动精确分割胎儿超声四腔心视图中的关键解剖结构，以辅助先天性心脏病诊断。", "motivation": "胎儿超声心动图心尖四腔心（A4C）视图中解剖结构的准确分割对于先天性心脏病（CHD）的早期诊断和产前评估至关重要。然而，由于超声伪影、散斑噪声、解剖变异性以及不同孕期的边界模糊性，精确分割仍然具有挑战性。为了减少超声医师的工作量并提高分割精度，本文提出了DCD模型。", "method": "本文提出了DCD，一个先进的基于深度学习的模型，用于自动分割胎儿A4C视图中的关键解剖结构。该模型结合了密集空洞空间金字塔池化（Dense ASPP）模块，以实现优越的多尺度特征提取，并引入卷积块注意力模块（CBAM）以增强自适应特征表示。", "result": "DCD模型通过有效捕获局部和全局上下文信息，实现了精确和鲁棒的分割。", "conclusion": "DCD模型有助于改善产前心脏评估。", "translation": "胎儿超声心动图心尖四腔心（A4C）视图中解剖结构的准确分割对于先天性心脏病（CHD）的早期诊断和产前评估至关重要。然而，由于超声伪影、散斑噪声、解剖变异性以及不同孕期的边界模糊性，精确分割仍然具有挑战性。为了减少超声医师的工作量并提高分割精度，我们提出了DCD，一种先进的基于深度学习的模型，用于胎儿A4C视图中关键解剖结构的自动分割。我们的模型结合了密集空洞空间金字塔池化（Dense ASPP）模块，能够实现卓越的多尺度特征提取，以及卷积块注意力模块（CBAM）以增强自适应特征表示。通过有效捕获局部和全局上下文信息，DCD实现了精确和鲁棒的分割，有助于改善产前心脏评估。", "summary": "本文提出了DCD模型，一个基于深度学习的语义分割模型，用于自动精确分割胎儿超声心动图四腔心视图中的关键解剖结构。该模型通过结合Dense ASPP模块进行多尺度特征提取和CBAM增强自适应特征表示，有效应对了超声图像中存在的挑战，如伪影和噪声。DCD旨在提高分割精度并减轻超声医师的工作负担，从而改善先天性心脏病的产前评估。", "keywords": "胎儿超声, 语义分割, 四腔心视图, 深度学习, 先天性心脏病", "comments": "该研究通过引入Dense ASPP和CBAM模块，有效提升了深度学习模型在复杂胎儿超声图像中进行精确语义分割的能力，这对于先天性心脏病的早期诊断具有重要意义。模型的创新性在于其对多尺度特征和自适应特征表示的强化，有望显著提高诊断效率和准确性。"}}
{"id": "2506.08636", "title": "Blockchain and Edge Computing Nexus: A Large-scale Systematic Literature Review", "authors": ["Zeinab Nezami", "Zhuolun Li", "Chuhao Qin", "Fatemeh Banaie", "Rabiya Khalid", "Evangelos Pournaras"], "summary": "Blockchain and edge computing are two instrumental paradigms of decentralized\ncomputation, driving key advancements in Smart Cities applications such as\nsupply chain, energy and mobility. Despite their unprecedented impact on\nsociety, they remain significantly fragmented as technologies and research\nareas, while they share fundamental principles of distributed systems and\ndomains of applicability. This paper introduces a novel and large-scale\nsystematic literature review on the nexus of blockchain and edge computing with\nthe aim to unravel a new understanding of how the interfacing of the two\ncomputing paradigms can boost innovation to provide solutions to timely but\nalso long-standing research challenges. By collecting almost 6000 papers from 3\ndatabases and putting under scrutiny almost 1000 papers, we build a novel\ntaxonomy and classification consisting of 22 features with 287 attributes that\nwe study using quantitative and machine learning methods. They cover a broad\nspectrum of technological, design, epistemological and sustainability aspects.\nResults reveal 4 distinguishing patterns of interplay between blockchain and\nedge computing with key determinants the public (permissionless) vs. private\n(permissioned) design, technology and proof of concepts. They also demonstrate\nthe prevalence of blockchain-assisted edge computing for improving privacy and\nsecurity, in particular for mobile computing applications.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.08636v1", "AI": {"title_translation": "区块链与边缘计算的结合：一项大规模系统文献综述", "tldr": "本文对区块链和边缘计算的结合进行了大规模系统文献综述，揭示了两种技术融合如何促进创新，并提出了新的分类法和四种主要交互模式。", "motivation": "尽管区块链和边缘计算对智能城市应用具有巨大影响，但它们在技术和研究领域仍高度分散。本文旨在通过研究它们的结合，理解如何促进创新，解决研究挑战。", "method": "收集了来自3个数据库的近6000篇论文，并仔细审查了近1000篇论文。构建了一个包含22个特征和287个属性的新颖分类法。使用定量和机器学习方法进行研究。", "result": "结果揭示了区块链和边缘计算之间4种独特的相互作用模式，关键决定因素是公共（无许可）与私有（有许可）设计、技术和概念验证。还表明区块链辅助边缘计算在提高隐私和安全性方面的普遍性，特别是对于移动计算应用。", "conclusion": "区块链与边缘计算的结合能够显著促进创新，解决智能城市应用中的隐私和安全等挑战，尤其是在移动计算领域。", "translation": "区块链和边缘计算是去中心化计算的两个重要范式，推动了智能城市应用（如供应链、能源和移动）的关键进步。尽管它们对社会产生了前所未有的影响，但作为技术和研究领域，它们仍然高度分散，尽管它们共享分布式系统的基本原则和适用领域。本文对区块链和边缘计算的结合进行了一项新颖且大规模的系统文献综述，旨在揭示对两种计算范式如何接口能够促进创新，为及时但也是长期存在的研究挑战提供解决方案的新理解。通过从3个数据库收集近6000篇论文并仔细审查近1000篇论文，我们构建了一个包含22个特征和287个属性的新颖分类法，并使用定量和机器学习方法对其进行研究。它们涵盖了技术、设计、认识论和可持续性方面的广泛范围。结果揭示了区块链和边缘计算之间4种独特的相互作用模式，关键决定因素是公共（无许可）与私有（有许可）设计、技术和概念验证。它们还表明区块链辅助边缘计算在提高隐私和安全性方面的普遍性，特别是对于移动计算应用。", "summary": "本文对区块链和边缘计算的结合进行了大规模系统文献综述，旨在理解这两种去中心化计算范式如何协同作用以解决智能城市应用中的挑战。通过分析近千篇论文，研究构建了一个包含22个特征和287个属性的新颖分类法，并利用定量和机器学习方法揭示了四种主要的交互模式。研究结果强调了区块链辅助边缘计算在提升移动应用隐私和安全方面的显著作用。", "keywords": "区块链, 边缘计算, 系统文献综述, 智能城市, 隐私安全", "comments": "本文通过大规模系统文献综述，系统地梳理了区块链与边缘计算的结合点，填补了该领域研究碎片化的空白。其提出的新颖分类法和揭示的四种交互模式，为后续研究提供了宝贵的理论框架和方向，尤其是在隐私和安全方面。"}}
{"id": "2506.08620", "title": "Extended Spherical Geometry Algorithm for Spaceborne SAR Processing in Stripmap and TOPS Imaging Modes", "authors": ["Xinhua Mao", "Manyi Tao", "Jixia Fan"], "summary": "The Spherical Geometry Algorithm (SGA) demonstrates superior capability in\nachieving efficient and precise spaceborne SAR image formation processing, even\nunder challenging imaging conditions including non-linear radar trajectories\nand spherical Earth surface geometry. Nevertheless, the original SGA is\nspecifically developed for spotlight SAR data processing and can't directly\napplied to processing spaceborne SAR data in other modes. In this paper, we\nfirst analyze the limitations of the SGA algorithm when applied to stripmap or\nTOPS mode SAR processing, and then propose an improved SGA algorithm which can\nprocess both stripmap and TOPS SAR data. Compared with the original algorithm,\nthe new algorithm has two main differences. Firstly, in order to avoid\nundersampling during azimuth resampling in both modes, an instantaneous Doppler\ncentroid removal process was added before azimuth interpolation processing by\nexploiting the endomorphism property of resampling operation. Secondly, the\nspectral analysis method used for the final step of azimuth compression in the\noriginal SGA has been replaced with a new matched filtering processing, which\ncan avoid image aliasing in azimuth direction and improve computational\nefficiency. Measured real data processing results are presented to demonstrate\nthe validity of the proposed algorithms.", "comment": "11 pages, 11 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.08620v1", "AI": {"title_translation": "星载SAR条带和TOPS成像模式扩展球面几何算法", "tldr": "本文提出了一种改进的球面几何算法（SGA），能够有效处理星载SAR条带和TOPS模式数据，解决了原始SGA的局限性并提高了处理效率和图像质量。", "motivation": "原始的球面几何算法（SGA）虽然在聚束式SAR数据处理方面表现出色，但在处理星载SAR条带和TOPS模式数据时存在局限性，无法直接应用。因此，需要开发一种能够处理这些模式的改进算法。", "method": "本文首先分析了SGA算法在条带或TOPS模式SAR处理中的局限性。然后，提出了一种改进的SGA算法，主要有两点改进：1. 在方位向插值处理前增加了瞬时多普勒中心去除过程，以避免方位向重采样中的欠采样。2. 用新的匹配滤波处理取代了原始SGA中方位向压缩的频谱分析方法，以避免图像混叠并提高计算效率。", "result": "实测数据处理结果证明了所提出算法的有效性。", "conclusion": "所提出的扩展球面几何算法（SGA）能够有效处理星载SAR条带和TOPS成像模式数据，克服了原始算法的局限性，并提高了处理效率和图像质量。", "translation": "球面几何算法（SGA）在实现高效、精确的星载SAR图像形成处理方面表现出卓越的能力，即使在非线性雷达轨迹和球面地球表面几何等挑战性成像条件下也是如此。然而，原始SGA是专门为聚束式SAR数据处理而开发的，不能直接应用于处理其他模式的星载SAR数据。在本文中，我们首先分析了SGA算法在应用于条带或TOPS模式SAR处理时的局限性，然后提出了一种可以处理条带和TOPS SAR数据的改进SGA算法。与原始算法相比，新算法有两个主要区别。首先，为了避免在这两种模式的方位向重采样过程中出现欠采样，通过利用重采样操作的自同态特性，在方位向插值处理之前添加了瞬时多普勒中心去除过程。其次，原始SGA中用于方位向压缩最后一步的频谱分析方法被一种新的匹配滤波处理所取代，这可以避免方位向图像混叠并提高计算效率。文中展示了实测数据处理结果，以证明所提出算法的有效性。", "summary": "球面几何算法（SGA）在聚束式SAR处理中表现出色，但无法直接应用于条带和TOPS模式。本文提出了一种扩展SGA，通过引入瞬时多普勒中心去除以避免欠采样，并用匹配滤波取代频谱分析以提高效率和防止混叠，从而解决了这些限制。实测数据处理验证了新算法在条带和TOPS SAR处理中的有效性。", "keywords": "球面几何算法, SAR处理, 条带模式, TOPS模式, 方位向压缩", "comments": "该论文解决了现有高效算法在应用范围上的实际限制，通过具体的改进措施扩展了其对条带和TOPS模式SAR数据的处理能力。提出的两项修改都旨在提高处理的精度（避免欠采样和混叠）和计算效率，这在SAR图像处理中至关重要。使用真实数据进行验证进一步增强了研究的可信度。"}}
{"id": "2506.08457", "title": "A Review on Score-based Generative Models for Audio Applications", "authors": ["Ge Zhu", "Yutong Wen", "Zhiyao Duan"], "summary": "Diffusion models have emerged as powerful deep generative techniques,\nproducing high-quality and diverse samples in applications in various domains\nincluding audio. These models have many different design choices suitable for\ndifferent applications, however, existing reviews lack in-depth discussions of\nthese design choices. The audio diffusion model literature also lacks\nprincipled guidance for the implementation of these design choices and their\ncomparisons for different applications. This survey provides a comprehensive\nreview of diffusion model design with an emphasis on design principles for\nquality improvement and conditioning for audio applications. We adopt the score\nmodeling perspective as a unifying framework that accommodates various\ninterpretations, including recent approaches like flow matching. We\nsystematically examine the training and sampling procedures of diffusion\nmodels, and audio applications through different conditioning mechanisms. To\naddress the lack of audio diffusion model codebases and to promote reproducible\nresearch and rapid prototyping, we introduce an open-source codebase at\nhttps://github.com/gzhu06/AudioDiffuser that implements our reviewed framework\nfor various audio applications. We demonstrate its capabilities through three\ncase studies: audio generation, speech enhancement, and text-to-speech\nsynthesis, with benchmark evaluations on standard datasets.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08457v1", "AI": {"title_translation": "基于分数的生成模型在音频应用中的综述", "tldr": "本文综述了扩散模型在音频应用中的设计选择和原理，并引入了一个开源代码库来促进可复现研究。", "motivation": "现有综述缺乏对扩散模型设计选择的深入讨论，且音频扩散模型文献中缺少针对不同应用的设计选择实现和比较的指导原则。", "method": "本文采用分数建模视角作为统一框架，系统地审查了扩散模型的训练和采样过程，并通过不同的条件机制审查了音频应用。此外，还引入了一个开源代码库。", "result": "本文提供了一个全面的扩散模型设计综述，强调了音频应用中质量改进和条件化的设计原则。通过开源代码库，展示了音频生成、语音增强和文本到语音合成的案例研究，并在标准数据集上进行了基准评估。", "conclusion": "本文通过提供扩散模型设计选择的深入综述和实用的指导原则，填补了现有文献的空白，并引入了一个开源代码库以促进音频扩散模型领域的可复现研究和快速原型开发。", "translation": "扩散模型已成为强大的深度生成技术，在包括音频在内的各个领域应用中生成高质量和多样化的样本。这些模型有许多适合不同应用的不同设计选择，然而，现有综述缺乏对这些设计选择的深入讨论。音频扩散模型文献也缺乏针对这些设计选择的实现及其在不同应用中比较的指导原则。本调查提供了扩散模型设计的全面综述，重点关注音频应用中质量改进和条件化的设计原则。我们采用分数建模视角作为一个统一框架，容纳各种解释，包括最近的流匹配方法。我们系统地审查了扩散模型的训练和采样过程，以及通过不同条件机制的音频应用。为了解决音频扩散模型代码库的缺乏，并促进可复现研究和快速原型开发，我们引入了一个开源代码库https://github.com/gzhu06/AudioDiffuser，它实现了我们审查的用于各种音频应用的框架。我们通过三个案例研究展示了其能力：音频生成、语音增强和文本到语音合成，并在标准数据集上进行了基准评估。", "summary": "这篇综述论文深入探讨了扩散模型在音频应用中的设计选择和原理，旨在弥补现有文献中对这些设计细节讨论的不足。文章采用分数建模作为统一框架，系统分析了训练、采样过程及不同条件机制下的音频应用。为促进研究和开发，论文还发布了一个开源代码库，并通过音频生成、语音增强和文本到语音合成的案例研究展示了其有效性。", "keywords": "扩散模型, 音频应用, 分数生成模型, 综述, 开源代码库代码", "comments": "这篇综述的重要性在于它不仅填补了现有文献中对扩散模型设计选择缺乏深入讨论的空白，还提供了一个统一的视角和实用的指导原则，这对于音频扩散模型领域的研究人员和开发者非常有价值。同时，开源代码库的发布极大地促进了可复现研究和快速原型开发，降低了进入该领域的门槛，具有显著的实践意义。"}}
{"id": "2506.08029", "title": "Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning", "authors": ["Jiayu Li", "Masood Mortazavi", "Ning Yan", "Yihong Ma", "Reza Zafarani"], "summary": "The goal of inverse design in distributed circuits is to generate\nnear-optimal designs that meet a desirable transfer function specification.\nExisting design exploration methods use some combination of strategies\ninvolving artificial grids, differentiable evaluation procedures, and specific\ntemplate topologies. However, real-world design practices often require\nnon-differentiable evaluation procedures, varying topologies, and\nnear-continuous placement spaces. In this paper, we propose DCIDA, a design\nexploration framework that learns a near-optimal design sampling policy for a\ntarget transfer function. DCIDA decides all design factors in a compound\nsingle-step action by sampling from a set of jointly-trained conditional\ndistributions generated by the policy. Utilizing an injective interdependent\n``map\", DCIDA transforms raw sampled design ``actions\" into uniquely equivalent\nphysical representations, enabling the framework to learn the conditional\ndependencies among joint ``raw'' design decisions. Our experiments demonstrate\nDCIDA's Transformer-based policy network achieves significant reductions in\ndesign error compared to state-of-the-art approaches, with significantly better\nfit in cases involving more complex transfer functions.", "comment": "A briefer version of this paper was accepted as a Work-in-Progress\n  (WIP) at the Design Automation Conference (DAC) 2024", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08029v1", "AI": {"title_translation": "分布式电路中的单步强化学习逆向设计", "tldr": "DCIDA使用单步强化学习进行分布式电路逆向设计，显著降低了设计误差，尤其适用于复杂传递函数。", "motivation": "现有分布式电路逆向设计方法在处理不可微分评估程序、变化的拓扑和准连续放置空间时存在局限性，无法满足实际设计需求。", "method": "本文提出DCIDA框架，该框架学习一个近乎最优的设计采样策略。DCIDA通过从联合训练的条件分布中采样，在单一复合的单步动作中决定所有设计因素。它利用一个内射的相互依赖的“映射”将原始采样的设计“动作”转换为唯一的物理表示，从而学习联合“原始”设计决策之间的条件依赖性。其策略网络基于Transformer。", "result": "DCIDA的Transformer策略网络与现有最先进方法相比，显著降低了设计误差，并且在涉及更复杂传递函数的情况下具有显著更好的拟合度。", "conclusion": "DCIDA框架能够有效地进行分布式电路的逆向设计，特别是在处理复杂传递函数时表现出优异的性能和准确性，解决了现有方法在实际应用中的不足。", "translation": "分布式电路逆向设计的目标是生成满足期望传递函数规格的近乎最优设计。现有的设计探索方法结合了人工网格、可微分评估程序和特定模板拓扑等策略。然而，实际设计实践通常需要不可微分的评估程序、变化的拓扑和准连续的放置空间。在本文中，我们提出了DCIDA，一个设计探索框架，它学习针对目标传递函数的近乎最优设计采样策略。DCIDA通过从策略生成的联合训练的条件分布中采样，在复合单步动作中决定所有设计因素。DCIDA利用一个内射的相互依赖的“映射”，将原始采样的设计“动作”转换为唯一等效的物理表示，使该框架能够学习联合“原始”设计决策之间的条件依赖性。我们的实验表明，DCIDA基于Transformer的策略网络与现有最先进方法相比，显著降低了设计误差，并且在涉及更复杂传递函数的情况下具有显著更好的拟合度。", "summary": "本文提出DCIDA，一个用于分布式电路逆向设计的框架，旨在生成满足特定传递函数的近乎最优设计。针对现有方法在非可微分评估和复杂拓扑中的局限性，DCIDA采用基于Transformer的单步强化学习策略，通过采样联合训练的条件分布来决定设计因素，并通过一个内射映射将抽象动作转换为物理表示。实验证明DCIDA在降低设计误差和处理复杂传递函数方面优于现有技术。", "keywords": "逆向设计, 分布式电路, 强化学习, Transformer, 单步学习", "comments": "本文的创新点在于将单步强化学习应用于分布式电路的逆向设计，并通过独特的映射机制处理非连续和非可微分的设计空间，解决了现有方法在实际应用中的局限性。其Transformer策略网络的应用也提升了性能，为复杂电路的自动化设计提供了新的思路。"}}
{"id": "2506.08225", "title": "Testing Suffixient Sets", "authors": ["Davide Cenzato", "Francisco Olivares", "Nicola Prezza"], "summary": "Suffixient sets are a novel prefix array (PA) compression technique based on\nsubsampling PA (rather than compressing the entire array like previous\ntechniques used to do): by storing very few entries of PA (in fact, a\ncompressed number of entries), one can prove that pattern matching via binary\nsearch is still possible provided that random access is available on the text.\nIn this paper, we tackle the problems of determining whether a given subset of\ntext positions is (1) a suffixient set or (2) a suffixient set of minimum\ncardinality. We provide linear-time algorithms solving these problems.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.08225v1", "AI": {"title_translation": "测试充分集", "tldr": "本文提出了线性时间算法来判断给定文本位置子集是否是充分集，以及是否是最小基数的充分集。充分集是一种基于前缀数组（PA）子采样的创新压缩技术。", "motivation": "本文旨在解决确定给定文本位置子集是否为充分集以及是否为最小基数充分集的问题。", "method": "本文提供了解决这些问题的线性时间算法。", "result": "本文提出了用于判断充分集和最小基数充分集的线性时间算法。", "conclusion": "本文成功开发了用于确定充分集及其最小基数的线性时间算法，为前缀数组压缩技术提供了有效的验证和优化方法。", "translation": "充分集是一种新颖的前缀数组（PA）压缩技术，它基于PA的子采样（而不是像以前的技术那样压缩整个数组）：通过存储PA中非常少的条目（实际上是压缩数量的条目），可以证明只要文本上随机访问可用，通过二分查找进行模式匹配仍然是可能的。在本文中，我们解决了确定给定文本位置子集是否为（1）充分集或（2）最小基数充分集的问题。我们提供了解决这些问题的线性时间算法。", "summary": "本文介绍了一种新颖的前缀数组（PA）压缩技术，称为充分集，它通过子采样PA来实现高效的模式匹配。研究重点是开发线性时间算法来解决两个关键问题：一是确定给定的文本位置子集是否为充分集，二是确定其是否为最小基数的充分集。这些算法为验证和优化这种新型压缩技术提供了有效工具。", "keywords": "充分集, 前缀数组压缩, 线性时间算法, 模式匹配, 子采样", "comments": "本文的创新之处在于提出了线性时间算法来验证和寻找最小基数的充分集。充分集作为一种新颖的前缀数组压缩技术，通过子采样而非完整压缩，为模式匹配提供了新的思路。所提出的算法对于充分集的实际应用和理论研究具有重要意义，尤其是在需要高效文本处理和模式匹配的场景中。"}}
{"id": "2506.08064", "title": "A Real-time 3D Desktop Display", "authors": ["Livio Tenze", "Enrique Canessa"], "summary": "A new extended version of the altiro3D C++ Library -- initially developed to\nget glass-free holographic displays starting from 2D images -- is here\nintroduced aiming to deal with 3D video streams from either 2D webcam images or\nflat video files. These streams are processed in real-time to synthesize\nlight-fields (in Native format) and feed realistic 3D experiences. The core\nfunction needed to recreate multiviews consists on the use of MiDaS\nConvolutional Neural Network (CNN), which allows to extract a depth map from a\nsingle 2D image. Artificial Intelligence (AI) computing techniques are applied\nto improve the overall performance of the extended altiro3D Library. Thus,\naltiro3D can now treat standard images, video streams or screen portions of a\nDesktop where other apps may be also running (like web browsers, video chats,\netc) and render them into 3D. To achieve the latter, a screen region need to be\nselected in order to feed the output directly into a light-field 3D device such\nas Looking Glass (LG) Portrait. In order to simplify the acquisition of a\nDesktop screen area by the user, a multi-platform Graphical User Interface has\nbeen also implemented. Sources available at:\nhttps://github.com/canessae/altiro3D/releases/tag/2.0.0", "comment": "10 pages, 5 figures", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.08064v1", "AI": {"title_translation": "实时3D桌面显示", "tldr": "本文介绍了一个扩展的C++库 (altiro3D)，它利用MiDaS卷积神经网络和AI技术，将来自2D摄像头图像、平面视频文件或桌面屏幕的2D图像/视频流实时转换为3D光场，并馈送给如Looking Glass等3D显示设备，以提供逼真的3D体验。", "motivation": "最初的altiro3D C++库旨在从2D图像生成无眼镜全息显示。本文介绍的扩展版本旨在处理来自2D网络摄像头图像或平面视频文件的3D视频流，并能实时处理桌面屏幕上的内容，将其渲染为3D，以提供逼真的3D体验。", "method": "该方法引入了altiro3D C++库的扩展版本。它实时处理来自2D网络摄像头图像或平面视频文件的视频流，以合成光场（原生格式）。核心功能是使用MiDaS卷积神经网络（CNN）从单个2D图像中提取深度图。应用人工智能（AI）计算技术来提高库的整体性能。扩展后的altiro3D库现在可以处理标准图像、视频流或桌面屏幕的选定部分，并将其渲染为3D。为了实现这一点，需要选择一个屏幕区域以将输出直接馈送到如Looking Glass (LG) Portrait等光场3D设备。此外，还实现了一个多平台图形用户界面（GUI）以简化用户对桌面屏幕区域的获取。", "result": "扩展后的altiro3D库现在能够处理标准图像、视频流或桌面屏幕的选定部分（包括其他正在运行的应用程序），并将其实时渲染为3D，从而为光场3D设备提供逼真的3D体验。", "conclusion": "扩展后的altiro3D库，通过整合MiDaS CNN和AI计算技术，实现了对2D图像、视频流和桌面屏幕内容的实时3D渲染，并能输出到光场3D设备，同时通过多平台GUI简化了用户操作，从而提供了逼真的3D体验。", "translation": "altiro3D C++库的一个新的扩展版本——最初是为了从2D图像获得无眼镜全息显示而开发的——在此被引入，旨在处理来自2D网络摄像头图像或平面视频文件的3D视频流。这些流被实时处理以合成光场（原生格式）并提供逼真的3D体验。重现多视角所需的核心功能是使用MiDaS卷积神经网络（CNN），它允许从单个2D图像中提取深度图。应用人工智能（AI）计算技术来提高扩展后的altiro3D库的整体性能。因此，altiro3D现在可以处理标准图像、视频流或桌面的屏幕部分，其中可能还运行着其他应用程序（如网络浏览器、视频聊天等），并将其渲染成3D。为了实现后者，需要选择一个屏幕区域，以便将输出直接馈送到光场3D设备，例如Looking Glass (LG) Portrait。为了简化用户获取桌面屏幕区域，还实现了一个多平台图形用户界面。源代码可在以下地址获取：https://github.com/canessae/altiro3D/releases/tag/2.0.0", "summary": "本文介绍了一个扩展的altiro3D C++库，它能够将来自2D网络摄像头、平面视频文件或桌面屏幕的2D图像和视频流实时转换为3D光场。该库利用MiDaS卷积神经网络从单个2D图像中提取深度图，并通过AI计算技术提升性能。它支持将桌面上的标准图像、视频流或屏幕区域渲染成3D，并直接输出到如Looking Glass Portrait等光场3D设备，同时提供了一个多平台图形用户界面以简化操作，从而提供逼真的3D体验。", "keywords": "实时3D, 光场, MiDaS CNN, altiro3D, 桌面显示", "comments": "该论文的创新之处在于将一个原本用于无眼镜全息显示的库扩展到能够实时处理更广泛的2D输入源，包括桌面屏幕内容，并将其转换为3D光场。MiDaS CNN在单幅图像深度估计中的应用是关键技术，而AI的整合则进一步提高了性能。特别是将桌面屏幕内容实时渲染成3D的能力，为远程协作、娱乐和桌面应用带来了新的可能性，具有重要的实用价值。"}}
{"id": "2506.08138", "title": "A Practical Guide to Tuning Spiking Neuronal Dynamics", "authors": ["William Gebhardt", "Alexander G. Ororbia", "Nathan McDonald", "Clare Thiem", "Jack Lombardi"], "summary": "In this work, we examine fundamental elements of spiking neural networks\n(SNNs) as well as how to tune them. Concretely, we focus on two different\nfoundational neuronal units utilized in SNNs -- the leaky integrate-and-fire\n(LIF) and the resonate-and-fire (RAF) neuron. We explore key equations and how\nhyperparameter values affect behavior. Beyond hyperparameters, we discuss other\nimportant design elements of SNNs -- the choice of input encoding and the setup\nfor excitatory-inhibitory populations -- and how these impact LIF and RAF\ndynamics.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.08138v1", "AI": {"title_translation": "尖峰神经元动力学调优实用指南", "tldr": "本文探讨了尖峰神经网络（SNNs）的基本要素及其调优方法，重点关注LIF和RAF神经元，并讨论了超参数、输入编码和兴奋-抑制群体设置对神经元动力学的影响。", "motivation": "本文旨在深入探讨尖峰神经网络（SNNs）的基本组成部分及其调优方法，以提供实用指导。", "method": "本文聚焦于两种SNN中常用的神经元单元：漏积分-发射（LIF）和共振-发射（RAF）神经元。研究内容包括关键方程、超参数值如何影响神经元行为，以及输入编码选择和兴奋-抑制群体设置等设计元素对LIF和RAF动力学的影响。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在这项工作中，我们研究了尖峰神经网络（SNNs）的基本元素以及如何对其进行调优。具体来说，我们重点关注SNNs中使用的两种不同的基础神经元单元——漏积分-发射（LIF）和共振-发射（RAF）神经元。我们探索了关键方程以及超参数值如何影响行为。除了超参数，我们还讨论了SNNs的其他重要设计元素——输入编码的选择和兴奋-抑制群体的设置——以及这些如何影响LIF和RAF的动力学。", "summary": "本文提供了一份关于尖峰神经网络（SNNs）调优的实用指南，详细分析了漏积分-发射（LIF）和共振-发射（RAF）两种核心神经元模型。研究内容涵盖了关键方程、超参数对神经元行为的影响，以及输入编码和兴奋-抑制群体设置等设计元素如何塑造LIF和RAF神经元的动力学。", "keywords": "尖峰神经网络, 神经元动力学, LIF神经元, RAF神经元, 超参数调优", "comments": "该论文作为一份实用指南，对于理解和调优尖峰神经网络具有重要价值。它深入探讨了两种核心神经元模型（LIF和RAF），并考虑了超参数、输入编码和网络结构等多个维度的影响，这对于实际应用和研究SNNs的动力学特性非常有帮助。"}}
{"id": "2506.08074", "title": "Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval", "authors": ["Abdellah Ghassel", "Ian Robinson", "Gabriel Tanase", "Hal Cooper", "Bryan Thompson", "Zhen Han", "Vassilis N. Ioannidis", "Soji Adeshina", "Huzefa Rangwala"], "summary": "Retrieval-Augmented Generation (RAG) grounds large language models in\nexternal evidence, yet it still falters when answers must be pieced together\nacross semantically distant documents. We close this gap with the Hierarchical\nLexical Graph (HLG), a three-tier index that (i) traces every atomic\nproposition to its source, (ii) clusters propositions into latent topics, and\n(iii) links entities and relations to expose cross-document paths. On top of\nHLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,\nwhich performs fine-grained entity-aware beam search over propositions for\nhigh-precision factoid questions, and TopicGraphRAG, which selects coarse\ntopics before expanding along entity links to supply broad yet relevant context\nfor exploratory queries. Additionally, existing benchmarks lack the complexity\nrequired to rigorously evaluate multi-hop summarization systems, often focusing\non single-document queries or limited datasets. To address this, we introduce a\nsynthetic dataset generation pipeline that curates realistic, multi-document\nquestion-answer pairs, enabling robust evaluation of multi-hop retrieval\nsystems. Extensive experiments across five datasets demonstrate that our\nmethods outperform naive chunk-based RAG achieving an average relative\nimprovement of 23.1% in retrieval recall and correctness. Open-source Python\nlibrary is available at https://github.com/awslabs/graphrag-toolkit.", "comment": "KDD '25", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08074v1", "AI": {"title_translation": "用于增强多跳检索的层次化词汇图", "tldr": "本文提出了层次化词汇图（HLG）及其配套检索器，以提升RAG在语义距离较远的文档中进行多跳检索的能力，并引入了新的数据集生成管道以进行更鲁棒的评估。", "motivation": "检索增强生成（RAG）在需要从语义上相距遥远的文档中拼凑答案时表现不佳。此外，现有基准缺乏评估多跳摘要系统所需的复杂性。", "method": "提出了层次化词汇图（HLG），一个三层索引，用于追溯原子命题、聚类命题为潜在主题、并链接实体和关系。基于HLG，构建了两个互补的检索器：StatementGraphRAG（用于高精度事实性问题）和TopicGraphRAG（用于探索性查询）。此外，引入了一个合成数据集生成管道，用于生成真实的多文档问答对，以进行多跳检索系统的鲁棒评估。", "result": "在五个数据集上的广泛实验表明，所提出的方法优于简单的基于分块的RAG，在检索召回率和正确性方面平均相对提高了23.1%。", "conclusion": "层次化词汇图（HLG）及其配套的检索器显著增强了RAG系统的多跳检索能力，并且通过引入新的合成数据集生成管道，为多跳检索系统的评估提供了更复杂和鲁棒的基准。", "translation": "检索增强生成（RAG）将大型语言模型建立在外部证据的基础上，但在答案需要从语义上相距遥远的不同文档中拼凑而成时，它仍然会遇到困难。我们通过层次化词汇图（HLG）弥补了这一空白，HLG是一个三层索引，它（i）将每个原子命题追溯到其来源，（ii）将命题聚类成潜在主题，以及（iii）链接实体和关系以揭示跨文档路径。在HLG之上，我们构建了两个互补的、即插即用的检索器：StatementGraphRAG，它对命题执行细粒度的实体感知束搜索，用于高精度事实性问题；以及TopicGraphRAG，它在沿实体链接扩展之前选择粗粒度主题，为探索性查询提供广泛而相关的上下文。此外，现有基准缺乏严格评估多跳摘要系统所需的复杂性，通常侧重于单文档查询或有限数据集。为了解决这个问题，我们引入了一个合成数据集生成管道，该管道策划了真实的、多文档的问答对，从而能够对多跳检索系统进行鲁棒评估。在五个数据集上进行的广泛实验表明，我们的方法优于简单的基于分块的RAG，在检索召回率和正确性方面平均相对提高了23.1%。开源Python库可在https://github.com/awslabs/graphrag-toolkit获得。", "summary": "本文介绍了层次化词汇图（HLG），一个旨在提升检索增强生成（RAG）处理跨语义距离文档多跳查询能力的三层索引。HLG能够追溯原子命题、聚类主题并链接实体关系。基于HLG，作者开发了StatementGraphRAG和TopicGraphRAG两个互补的检索器，分别适用于高精度事实性问题和探索性查询。为解决现有基准的不足，本文还提出了一个合成数据集生成管道，用于创建复杂的多文档问答对。在五个数据集上进行的实验证明，与简单的基于分块的RAG相比，本文方法在检索召回率和正确性方面平均相对提高了23.1%。", "keywords": "多跳检索, 层次化词汇图, 检索增强生成, RAG, 数据集生成", "comments": "HLG的层次化结构和双检索器（StatementGraphRAG和TopicGraphRAG）的方法为解决RAG在多跳检索中的挑战提供了创新方案。引入新的合成数据集生成管道对于多跳检索系统的鲁棒评估至关重要。实验中23.1%的显著性能提升凸显了其重要的实际应用价值和效果。"}}
{"id": "2506.08482", "title": "One Patch to Rule Them All: Transforming Static Patches into Dynamic Attacks in the Physical World", "authors": ["Xingshuo Han", "Chen Ling", "Shiyi Yao", "Haozhao Wang", "Hangcheng Liu", "Yutong Wu", "Shengmin Xu", "Changhai Ou", "Xinyi Huang", "Tianwei Zhang"], "summary": "Numerous methods have been proposed to generate physical adversarial patches\n(PAPs) against real-world machine learning systems. However, each existing PAP\ntypically supports only a single, fixed attack goal, and switching to a\ndifferent objective requires re-generating and re-deploying a new PAP. This\nrigidity limits their practicality in dynamic environments like autonomous\ndriving, where traffic conditions and attack goals can change rapidly. For\nexample, if no obstacles are present around the target vehicle, the attack may\nfail to cause meaningful consequences.\n  To overcome this limitation, we propose SwitchPatch, a novel PAP that is\nstatic yet enables dynamic and controllable attack outcomes based on real-time\nscenarios. Attackers can alter pre-defined conditions, e.g., by projecting\ndifferent natural-color lights onto SwitchPatch to seamlessly switch between\nattack goals. Unlike prior work, SwitchPatch does not require re-generation or\nre-deployment for different objectives, significantly reducing cost and\ncomplexity. Furthermore, SwitchPatch remains benign when the enabling\nconditions are absent, enhancing its stealth.\n  We evaluate SwitchPatch on two key tasks: traffic sign recognition\n(classification and detection) and depth estimation. First, we conduct\ntheoretical analysis and empirical studies to demonstrate the feasibility of\nSwitchPatch and explore how many goals it can support using techniques like\ncolor light projection and occlusion. Second, we perform simulation-based\nexperiments and ablation studies to verify its effectiveness and\ntransferability. Third, we conduct outdoor tests using a Unmanned Ground\nVehicle (UGV) to confirm its robustness in the physical world. Overall,\nSwitchPatch introduces a flexible and practical adversarial strategy that can\nbe adapted to diverse tasks and real-world conditions.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08482v1", "AI": {"title_translation": "一补丁统领一切：将静态补丁转化为物理世界中的动态攻击", "tldr": "SwitchPatch是一种新型的物理对抗补丁，它本身是静态的，但能根据实时场景通过改变外部条件（如投影不同颜色的光）实现动态可控的攻击目标切换，无需重新生成或部署。", "motivation": "现有的物理对抗补丁（PAPs）通常只支持单一固定的攻击目标，在动态环境中（如自动驾驶）实用性受限，因为攻击目标和环境条件可能快速变化，而切换目标需要重新生成和部署新的PAPs。", "method": "提出SwitchPatch，这是一种静态的物理对抗补丁，通过改变预定义条件（如投影不同颜色的自然光或遮挡）来动态切换攻击目标。该方法无需重新生成或重新部署。通过理论分析、实证研究、基于仿真的实验、消融研究以及使用无人地面车辆（UGV）进行户外测试，在交通标志识别（分类和检测）和深度估计任务上进行了评估。", "result": "SwitchPatch被证明是可行的，能够支持多个攻击目标，并且在启用条件不存在时保持良性，增强了隐蔽性。实验验证了其有效性、可迁移性以及在物理世界中的鲁棒性。", "conclusion": "SwitchPatch引入了一种灵活且实用的对抗策略，能够适应各种任务和真实世界条件。", "translation": "已提出许多方法来生成针对现实世界机器学习系统的物理对抗补丁（PAPs）。然而，每个现有的PAP通常只支持一个单一、固定的攻击目标，切换到不同目标需要重新生成和重新部署新的PAP。这种僵硬性限制了它们在动态环境（如自动驾驶）中的实用性，在这些环境中，交通状况和攻击目标可以迅速变化。例如，如果目标车辆周围没有障碍物，攻击可能无法造成有意义的后果。\n为了克服这一限制，我们提出了SwitchPatch，这是一种新颖的PAP，它本身是静态的，但能根据实时场景实现动态和可控的攻击结果。攻击者可以改变预定义的条件，例如，通过将不同自然色光投射到SwitchPatch上，从而无缝切换攻击目标。与以往的工作不同，SwitchPatch不需要为不同目标重新生成或重新部署，显著降低了成本和复杂性。此外，当启用条件不存在时，SwitchPatch保持良性，增强了其隐蔽性。\n我们在两个关键任务上评估了SwitchPatch：交通标志识别（分类和检测）和深度估计。首先，我们进行了理论分析和实证研究，以证明SwitchPatch的可行性，并探索它可以使用颜色光投影和遮挡等技术支持多少个目标。其次，我们进行了基于仿真的实验和消融研究，以验证其有效性和可迁移性。第三，我们使用无人地面车辆（UGV）进行了户外测试，以确认其在物理世界中的鲁棒性。总的来说，SwitchPatch引入了一种灵活实用的对抗策略，可以适应各种任务和真实世界条件。", "summary": "该研究提出SwitchPatch，一种新型的物理对抗补丁（PAP），旨在解决现有PAPs在动态环境中仅支持单一固定攻击目标的局限性。SwitchPatch本身是静态的，但能通过改变外部条件（如投影不同颜色的光）实现动态可控的攻击目标切换，无需重新生成或重新部署，显著降低了成本和复杂性。当启用条件不存在时，SwitchPatch保持良性，提高了隐蔽性。研究在交通标志识别和深度估计任务上对SwitchPatch进行了评估，通过理论分析、仿真实验和户外测试，验证了其可行性、有效性、可迁移性和在物理世界中的鲁棒性，展示了其作为一种灵活实用对抗策略的潜力。", "keywords": "对抗补丁, 动态攻击, 物理世界, 机器学习安全, SwitchPatch", "comments": "这项工作极具创新性，解决了传统物理对抗补丁在动态环境中实用性差的问题。通过引入“静态补丁动态攻击”的概念，并利用外部条件（如光照）实现攻击目标切换，极大地提高了对抗攻击的灵活性和隐蔽性，且无需重新生成或部署，这对于实际应用场景（如自动驾驶安全）具有重要意义。其“良性”状态也增强了实用性。"}}
{"id": "2506.08311", "title": "Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study", "authors": ["Ira Ceka", "Saurabh Pujar", "Shyam Ramji", "Luca Buratti", "Gail Kaiser", "Baishakhi Ray"], "summary": "With the advent of large language models (LLMs), software engineering agents\n(SWE agents) have emerged as a powerful paradigm for automating a range of\nsoftware tasks -- from code generation and repair to test case synthesis. These\nagents operate autonomously by interpreting user input and responding to\nenvironmental feedback. While various agent architectures have demonstrated\nstrong empirical performance, the internal decision-making worfklows that drive\ntheir behavior remain poorly understood. Deeper insight into these workflows\nhold promise for improving both agent reliability and efficiency. In this work,\nwe present the first systematic study of SWE agent behavior through the lens of\nexecution traces. Our contributions are as follows: (1) we propose the first\ntaxonomy of decision-making pathways across five representative agents; (2)\nusing this taxonomy, we identify three core components essential to agent\nsuccess -- bug localization, patch generation, and reproduction test generation\n-- and study each in depth; (3) we study the impact of test generation on\nsuccessful patch production; and analyze strategies that can lead to successful\ntest generation; (4) we further conduct the first large-scale code clone\nanalysis comparing agent-generated and developer-written patches and provide a\nqualitative study revealing structural and stylistic differences in patch\ncontent. Together, these findings offer novel insights into agent design and\nopen avenues for building agents that are both more effective and more aligned\nwith human development practices.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08311v1", "AI": {"title_translation": "通过可追溯性视角理解软件工程代理：一项实证研究", "tldr": "该研究通过执行轨迹深入分析了软件工程代理的内部决策过程，旨在提高其可靠性和效率，并为未来代理设计提供新见解。", "motivation": "尽管各种软件工程代理（SWE agents）架构已表现出强大的经验性能，但驱动其行为的内部决策工作流程仍知之甚少。深入了解这些工作流程有望提高代理的可靠性和效率。", "method": "本研究首次系统地通过执行轨迹来研究软件工程代理的行为。具体方法包括：1) 提出了五种代表性代理的决策路径分类法；2) 利用该分类法，识别并深入研究了代理成功的三个核心组成部分：错误定位、补丁生成和复现测试生成；3) 研究了测试生成对成功补丁生产的影响，并分析了成功测试生成的策略；4) 首次进行了大规模代码克隆分析，比较了代理生成和开发者编写的补丁，并提供了定性研究揭示补丁内容在结构和风格上的差异。", "result": "本研究提出了首个涵盖五种代表性代理的决策路径分类法；识别出代理成功所需的三项核心组件：错误定位、补丁生成和复现测试生成，并对其进行了深入研究；分析了测试生成对补丁成功生产的影响及有效测试生成的策略；首次大规模比较了代理生成和开发者编写的补丁，揭示了两者在结构和风格上的差异。", "conclusion": "这些发现为代理设计提供了新颖的见解，并为构建更有效且与人类开发实践更一致的代理开辟了道路。", "translation": "随着大型语言模型（LLMs）的出现，软件工程代理（SWE agents）已成为自动化一系列软件任务——从代码生成和修复到测试用例合成——的强大范式。这些代理通过解释用户输入和响应环境反馈自主运行。虽然各种代理架构已表现出强大的经验性能，但驱动其行为的内部决策工作流程仍知之甚少。深入了解这些工作流程有望提高代理的可靠性和效率。在这项工作中，我们首次通过执行轨迹的视角对软件工程代理行为进行了系统研究。我们的贡献如下：(1) 我们提出了五种代表性代理的决策路径的首个分类法；(2) 利用该分类法，我们识别了代理成功的三个核心组成部分——错误定位、补丁生成和复现测试生成——并深入研究了每个部分；(3) 我们研究了测试生成对成功补丁生产的影响；并分析了可以导致成功测试生成的策略；(4) 我们进一步进行了首次大规模代码克隆分析，比较了代理生成和开发者编写的补丁，并提供了一项定性研究，揭示了补丁内容在结构和风格上的差异。总而言之，这些发现为代理设计提供了新颖的见解，并为构建更有效且与人类开发实践更一致的代理开辟了道路。", "summary": "本研究首次系统地分析了软件工程代理（SWE agents）的内部决策工作流程，以解决其行为理解不足的问题。通过对执行轨迹的深入研究，论文提出了代理决策路径的分类法，识别了错误定位、补丁生成和复现测试生成等核心成功要素。研究还探讨了测试生成对补丁生产的影响，并首次对代理生成与人工编写的补丁进行了大规模代码克隆分析，揭示了两者在结构和风格上的差异。这些发现为未来设计更高效、更符合人类开发实践的软件工程代理提供了关键见解。", "keywords": "软件工程代理, 大型语言模型, 可追溯性, 实证研究, 补丁生成", "comments": "这项研究的创新之处在于它是首次系统地通过执行轨迹来深入理解软件工程代理的内部决策机制。其重要性在于，通过揭示代理行为的深层逻辑，为设计更可靠、更高效且与人类开发实践更协调的软件工程代理提供了宝贵的经验证据和设计方向。"}}
{"id": "2506.08296", "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation", "authors": ["Hongjun Wu", "Heng Zhang", "Pengsong Zhang", "Jin Wang", "Cong Wang"], "summary": "Recent advances in multimodal vision-language-action (VLA) models have\nrevolutionized traditional robot learning, enabling systems to interpret\nvision, language, and action in unified frameworks for complex task planning.\nHowever, mastering complex manipulation tasks remains an open challenge,\nconstrained by limitations in persistent contextual memory, multi-agent\ncoordination under uncertainty, and dynamic long-horizon planning across\nvariable sequences. To address this challenge, we propose \\textbf{HiBerNAC}, a\n\\textbf{Hi}erarchical \\textbf{B}rain-\\textbf{e}mulated \\textbf{r}obotic\n\\textbf{N}eural \\textbf{A}gent \\textbf{C}ollective, inspired by breakthroughs\nin neuroscience, particularly in neural circuit mechanisms and hierarchical\ndecision-making. Our framework combines: (1) multimodal VLA planning and\nreasoning with (2) neuro-inspired reflection and multi-agent mechanisms,\nspecifically designed for complex robotic manipulation tasks. By leveraging\nneuro-inspired functional modules with decentralized multi-agent collaboration,\nour approach enables robust and enhanced real-time execution of complex\nmanipulation tasks. In addition, the agentic system exhibits scalable\ncollective intelligence via dynamic agent specialization, adapting its\ncoordination strategy to variable task horizons and complexity. Through\nextensive experiments on complex manipulation tasks compared with\nstate-of-the-art VLA models, we demonstrate that \\textbf{HiBerNAC} reduces\naverage long-horizon task completion time by 23\\%, and achieves non-zero\nsuccess rates (12\\textendash 31\\%) on multi-path tasks where prior\nstate-of-the-art VLA models consistently fail. These results provide indicative\nevidence for bridging biological cognition and robotic learning mechanisms.", "comment": "31 pages,5 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08296v1", "AI": {"title_translation": "HiBerNAC: 用于解耦复杂操作的分层脑模拟机器人神经智能体集合", "tldr": "HiBerNAC是一个受神经科学启发的分层脑模拟机器人神经智能体集合，旨在通过结合多模态VLA规划、神经启发式反射和多智能体机制，解决复杂操作任务中的记忆、协调和规划挑战。它在复杂操作任务中显著提高了任务完成时间和成功率。", "motivation": "尽管多模态视觉-语言-动作（VLA）模型在机器人学习方面取得了进展，但掌握复杂操作任务仍然是一个挑战，主要受限于持久上下文记忆、不确定性下的多智能体协调以及可变序列中的动态长周期规划。", "method": "本文提出了HiBerNAC，一个分层脑模拟机器人神经智能体集合，其灵感来源于神经科学（特别是神经回路机制和分层决策）。该框架结合了：(1) 多模态VLA规划和推理，以及 (2) 神经启发式反射和多智能体机制，专为复杂机器人操作任务设计。它利用神经启发式功能模块和去中心化多智能体协作，实现了复杂操作任务的鲁棒和增强的实时执行。该智能体系统通过动态智能体专业化展现出可扩展的集体智能，使其协调策略适应可变的任务周期和复杂性。", "result": "与最先进的VLA模型相比，HiBerNAC在复杂操作任务中将平均长周期任务完成时间缩短了23%，并在先前的最先进VLA模型持续失败的多路径任务上实现了非零成功率（12%-31%）。", "conclusion": "这些结果为弥合生物认知和机器人学习机制之间的鸿沟提供了指示性证据。", "translation": "多模态视觉-语言-动作（VLA）模型的最新进展彻底改变了传统的机器人学习，使系统能够在统一框架中解释视觉、语言和动作，从而实现复杂的任务规划。然而，掌握复杂操作任务仍然是一个开放的挑战，这受限于持久上下文记忆、不确定性下的多智能体协调以及可变序列中的动态长周期规划。为了解决这一挑战，我们提出了HiBerNAC，一个分层脑模拟机器人神经智能体集合，其灵感来源于神经科学的突破，特别是在神经回路机制和分层决策方面。我们的框架结合了：(1) 多模态VLA规划和推理，以及 (2) 神经启发式反射和多智能体机制，专为复杂机器人操作任务设计。通过利用神经启发式功能模块和去中心化多智能体协作，我们的方法能够鲁棒且增强地实时执行复杂操作任务。此外，该智能体系统通过动态智能体专业化展现出可扩展的集体智能，使其协调策略适应可变的任务周期和复杂性。通过与最先进的VLA模型相比，在复杂操作任务上进行的大量实验表明，HiBerNAC将平均长周期任务完成时间缩短了23%，并在先前的最先进VLA模型持续失败的多路径任务上实现了非零成功率（12%-31%）。这些结果为弥合生物认知和机器人学习机制之间的鸿沟提供了指示性证据。", "summary": "该论文提出了HiBerNAC，一个受神经科学启发的、分层的脑模拟机器人神经智能体集合，旨在解决复杂机器人操作任务中持久上下文记忆、多智能体协调和动态长周期规划的挑战。HiBerNAC结合了多模态VLA规划、神经启发式反射和多智能体机制，通过去中心化协作和动态智能体专业化实现鲁棒的实时执行和可扩展的集体智能。实验表明，HiBerNAC显著缩短了任务完成时间，并在现有VLA模型失败的多路径任务上取得了成功，为连接生物认知和机器人学习提供了新的方向。", "keywords": "机器人操作, 神经智能体, 多模态VLA, 脑模拟, 分层决策", "comments": "HiBerNAC的创新之处在于其将神经科学的原理（如神经回路和分层决策）融入到机器人学习中，以解决复杂操作任务的固有挑战。通过结合多模态VLA和神经启发式机制，它提供了一个新颖的框架来增强机器人的记忆、协调和规划能力。其在多路径任务上的非零成功率尤其突出，表明了超越现有SOTA模型的潜力，为未来类脑机器人智能体的开发奠定了基础。"}}
{"id": "2506.08185", "title": "Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework", "authors": ["Huixin Zhan", "Jason H. Moore"], "summary": "Surgeons exhibit distinct operating styles due to differences in training,\nexperience, and motor behavior - yet current AI systems often ignore this\npersonalization signal. We propose a novel approach to model fine-grained,\nsurgeon-specific fingerprinting in robotic surgery using a discrete diffusion\nframework integrated with a vision-language-action (VLA) pipeline. Our method\nformulates gesture prediction as a structured sequence denoising task,\nconditioned on multimodal inputs including endoscopic video, surgical intent\nlanguage, and a privacy-aware embedding of surgeon identity and skill.\nPersonalized surgeon fingerprinting is encoded through natural language prompts\nusing third-party language models, allowing the model to retain individual\nbehavioral style without exposing explicit identity. We evaluate our method on\nthe JIGSAWS dataset and demonstrate that it accurately reconstructs gesture\nsequences while learning meaningful motion fingerprints unique to each surgeon.\nTo quantify the privacy implications of personalization, we perform membership\ninference attacks and find that more expressive embeddings improve task\nperformance but simultaneously increase susceptibility to identity leakage.\nThese findings demonstrate that while personalized embeddings improve\nperformance, they also increase vulnerability to identity leakage, revealing\nthe importance of balancing personalization with privacy risk in surgical\nmodeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08185v1", "AI": {"title_translation": "基于视觉-语言-动作框架的离散扩散模型外科医生风格指纹识别与隐私风险量化", "tldr": "该研究提出了一种利用离散扩散模型和视觉-语言-动作框架，对机器人手术中外科医生的操作风格进行精细化指纹识别的方法，并通过成员推断攻击量化了个性化带来的隐私风险，发现性能提升与身份泄露风险并存。", "motivation": "当前AI系统在机器人手术中忽略了外科医生独特的个性化操作风格，而这些风格因训练、经验和运动行为差异而存在。", "method": "该研究提出了一种新颖的方法，通过集成离散扩散框架与视觉-语言-动作（VLA）管道，对机器人手术中的外科医生特定指纹进行建模。它将手势预测表述为结构化序列去噪任务，以包括内窥镜视频、手术意图语言以及外科医生身份和技能的隐私感知嵌入在内的多模态输入为条件。个性化的外科医生指纹通过自然语言提示使用第三方语言模型编码，从而在不暴露明确身份的情况下保留个体行为风格。", "result": "该方法在JIGSAWS数据集上进行了评估，结果表明它能准确重建手势序列，同时学习到每个外科医生独特的有意义的运动指纹。为了量化个性化的隐私影响，研究进行了成员推断攻击，发现更具表达力的嵌入虽然提高了任务性能，但同时增加了身份泄露的敏感性。", "conclusion": "个性化嵌入虽然能提高性能，但也会增加身份泄露的脆弱性，这揭示了在手术建模中平衡个性化与隐私风险的重要性。", "translation": "外科医生由于训练、经验和运动行为的差异，表现出独特的操作系统风格——然而，当前的AI系统往往忽略了这种个性化信号。我们提出了一种新颖的方法，利用集成视觉-语言-动作（VLA）管道的离散扩散框架，对机器人手术中精细化的、外科医生特有的指纹进行建模。我们的方法将手势预测表述为结构化序列去噪任务，以包括内窥镜视频、手术意图语言以及外科医生身份和技能的隐私感知嵌入在内的多模态输入为条件。个性化的外科医生指纹通过使用第三方语言模型的自然语言提示进行编码，允许模型在不暴露明确身份的情况下保留个体行为风格。我们在JIGSAWS数据集上评估了我们的方法，并证明它能准确重建手势序列，同时学习到每个外科医生独特的有意义的运动指纹。为了量化个性化的隐私影响，我们执行了成员推断攻击，发现更具表达力的嵌入提高了任务性能，但同时增加了身份泄露的敏感性。这些发现表明，虽然个性化嵌入提高了性能，但它们也增加了身份泄露的脆弱性，揭示了在手术建模中平衡个性化与隐私风险的重要性。代码可在以下网址获取：https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting。", "summary": "本文提出了一种基于离散扩散模型和视觉-语言-动作（VLA）框架的新方法，用于在机器人手术中识别外科医生的独特操作风格。该方法将手势预测视为多模态输入（包括视频、语言和隐私感知身份嵌入）条件下的序列去噪任务，并通过自然语言提示编码外科医生指纹以保护隐私。在JIGSAWS数据集上的评估表明，该方法能准确重建手势并学习独特的运动指纹。研究进一步通过成员推断攻击量化了隐私风险，发现性能提升与身份泄露风险之间存在权衡，强调了在手术建模中平衡个性化和隐私的重要性。", "keywords": "外科医生风格指纹识别, 离散扩散模型, 隐私风险量化, 机器人手术, 视觉-语言-动作框架", "comments": "该研究的创新之处在于将离散扩散模型引入外科医生风格指纹识别，并结合视觉-语言-动作框架处理多模态数据。其重要性在于，它不仅提出了个性化建模的有效方法，还首次量化了这种个性化带来的隐私泄露风险，为未来外科手术AI系统在性能与隐私之间做出权衡提供了重要见解。"}}
{"id": "2506.08884", "title": "InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis", "authors": ["Shiqin Tang", "Shujian Yu"], "summary": "Extracting meaningful latent representations from high-dimensional sequential\ndata is a crucial challenge in machine learning, with applications spanning\nnatural science and engineering. We introduce InfoDPCCA, a dynamic\nprobabilistic Canonical Correlation Analysis (CCA) framework designed to model\ntwo interdependent sequences of observations. InfoDPCCA leverages a novel\ninformation-theoretic objective to extract a shared latent representation that\ncaptures the mutual structure between the data streams and balances\nrepresentation compression and predictive sufficiency while also learning\nseparate latent components that encode information specific to each sequence.\nUnlike prior dynamic CCA models, such as DPCCA, our approach explicitly\nenforces the shared latent space to encode only the mutual information between\nthe sequences, improving interpretability and robustness. We further introduce\na two-step training scheme to bridge the gap between information-theoretic\nrepresentation learning and generative modeling, along with a residual\nconnection mechanism to enhance training stability. Through experiments on\nsynthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool\nfor representation learning. Code of InfoDPCCA is available at\nhttps://github.com/marcusstang/InfoDPCCA.", "comment": "accepted by UAI-25, code is available at\n  \\url{https://github.com/marcusstang/InfoDPCCA}", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08884v1", "AI": {"title_translation": "InfoDPCCA：信息论动态概率典型相关分析", "tldr": "InfoDPCCA是一种新的动态概率典型相关分析框架，通过信息论目标从高维序列数据中提取共享和特定潜在表示，提高了可解释性和鲁棒性。", "motivation": "从高维序列数据中提取有意义的潜在表示是机器学习中的一个关键挑战，其应用涵盖自然科学和工程领域。", "method": "本文引入了InfoDPCCA，一个动态概率典型相关分析（CCA）框架，旨在建模两个相互依赖的观测序列。InfoDPCCA利用新颖的信息论目标来提取共享的潜在表示，该表示捕获数据流之间的相互结构，并在表示压缩和预测充分性之间取得平衡，同时学习编码每个序列特定信息的单独潜在组件。与之前的动态CCA模型不同，InfoDPCCA明确强制共享潜在空间只编码序列之间的互信息，从而提高了可解释性和鲁棒性。此外，还引入了两步训练方案以弥合信息论表示学习和生成建模之间的差距，以及残差连接机制以增强训练稳定性。", "result": "在合成数据和医学fMRI数据上的实验表明，InfoDPCCA作为一种表示学习工具表现出色。", "conclusion": "InfoDPCCA是一个用于从相互依赖的序列中提取共享和特定潜在表示的有效框架，通过其信息论目标和明确的互信息编码，提高了可解释性和鲁棒性。", "translation": "从高维序列数据中提取有意义的潜在表示是机器学习中的一个关键挑战，其应用涵盖自然科学和工程领域。我们引入了InfoDPCCA，一个动态概率典型相关分析（CCA）框架，旨在建模两个相互依赖的观测序列。InfoDPCCA利用一个新颖的信息论目标来提取共享的潜在表示，该表示捕获数据流之间的相互结构，并在表示压缩和预测充分性之间取得平衡，同时还学习编码每个序列特定信息的单独潜在组件。与之前的动态CCA模型（如DPCCA）不同，我们的方法明确强制共享潜在空间只编码序列之间的互信息，从而提高了可解释性和鲁棒性。我们进一步引入了两步训练方案，以弥合信息论表示学习和生成建模之间的差距，以及残差连接机制以增强训练稳定性。通过在合成数据和医学fMRI数据上的实验，我们证明了InfoDPCCA作为一种表示学习工具表现出色。InfoDPCCA的代码可在https://github.com/marcusstang/InfoDPCCA 获取。", "summary": "本文提出了InfoDPCCA，一个针对高维序列数据的动态概率典型相关分析（CCA）框架。该模型利用信息论目标，旨在从两个相互依赖的序列中提取共享的潜在表示，同时学习每个序列特有的潜在组件。InfoDPCCA通过明确确保共享空间只编码互信息，显著提高了模型的解释性和鲁棒性，并引入了两步训练和残差连接机制以增强性能。实验证明其在表示学习方面的优越性。", "keywords": "InfoDPCCA, 典型相关分析, 潜在表示, 序列数据, 信息论", "comments": "InfoDPCCA的创新点在于其信息论目标和明确强制共享潜在空间只编码互信息，这大大提高了模型的可解释性和鲁棒性，解决了现有动态CCA模型的局限性。结合两步训练和残差连接，使其成为处理高维序列数据表示学习的强大工具。"}}
{"id": "2506.08306", "title": "AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data", "authors": ["Tuan Truong", "Rithwik Sudharsan", "Yibo Yang", "Peter Xiangyuan Ma", "Ruihan Yang", "Stephan Mandt", "Joshua S. Bloom"], "summary": "The site conditions that make astronomical observatories in space and on the\nground so desirable -- cold and dark -- demand a physical remoteness that leads\nto limited data transmission capabilities. Such transmission limitations\ndirectly bottleneck the amount of data acquired and in an era of costly modern\nobservatories, any improvements in lossless data compression has the potential\nscale to billions of dollars worth of additional science that can be\naccomplished on the same instrument. Traditional lossless methods for\ncompressing astrophysical data are manually designed. Neural data compression,\non the other hand, holds the promise of learning compression algorithms\nend-to-end from data and outperforming classical techniques by leveraging the\nunique spatial, temporal, and wavelength structures of astronomical images.\nThis paper introduces AstroCompress: a neural compression challenge for\nastrophysics data, featuring four new datasets (and one legacy dataset) with\n16-bit unsigned integer imaging data in various modes: space-based,\nground-based, multi-wavelength, and time-series imaging. We provide code to\neasily access the data and benchmark seven lossless compression methods (three\nneural and four non-neural, including all practical state-of-the-art\nalgorithms). Our results on lossless compression indicate that lossless neural\ncompression techniques can enhance data collection at observatories, and\nprovide guidance on the adoption of neural compression in scientific\napplications. Though the scope of this paper is restricted to lossless\ncompression, we also comment on the potential exploration of lossy compression\nmethods in future studies.", "comment": "ICLR 2025 conference paper. See reviews at\n  https://openreview.net/forum?id=kQCHCkNk7s", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08306v1", "AI": {"title_translation": "AstroCompress：天文数据多功能压缩的基准数据集", "tldr": "AstroCompress是一个用于天文数据压缩的基准数据集，旨在通过评估神经压缩方法来提高数据传输效率。", "motivation": "天文观测站面临数据传输限制，这直接瓶颈了数据采集量。传统的无损压缩方法是手动设计的，而神经数据压缩有望通过学习算法来超越经典技术，从而为昂贵的现代观测站节省成本并实现更多科学成果。", "method": "本文介绍了AstroCompress，一个用于天体物理数据神经压缩挑战的基准数据集，包含四个新数据集和一个遗留数据集，涵盖空间、地面、多波长和时间序列成像等16位无符号整数成像数据。作者提供了代码，并对七种无损压缩方法（三种神经和四种非神经，包括所有实用的最先进算法）进行了基准测试。", "result": "无损压缩的结果表明，无损神经压缩技术可以提高天文观测站的数据采集能力。", "conclusion": "研究结果为科学应用中神经压缩的采用提供了指导。", "translation": "使空间和地面天文台如此理想的场地条件——寒冷和黑暗——要求物理上的偏远，导致数据传输能力有限。这种传输限制直接瓶颈了数据采集量，在昂贵的现代天文台时代，无损数据压缩的任何改进都有可能为相同的仪器带来价值数十亿美元的额外科学成果。传统的无损天体物理数据压缩方法是手动设计的。另一方面，神经数据压缩有望通过从数据中端到端学习压缩算法，并利用天文图像独特的空间、时间和波长结构来超越经典技术。本文介绍了AstroCompress：一个用于天体物理数据神经压缩挑战的基准数据集，包含四个新数据集（和一个遗留数据集），其中包含各种模式的16位无符号整数成像数据：天基、地基、多波长和时间序列成像。我们提供了易于访问数据和对七种无损压缩方法（三种神经和四种非神经，包括所有实用的最先进算法）进行基准测试的代码。我们在无损压缩上的结果表明，无损神经压缩技术可以增强观测站的数据收集，并为科学应用中神经压缩的采用提供指导。尽管本文的范围仅限于无损压缩，但我们也评论了未来研究中探索有损压缩方法的潜力。", "summary": "本文介绍了AstroCompress，一个专为天体物理数据神经压缩设计的基准数据集。该数据集包含多模式的16位天文图像数据，旨在解决天文观测中数据传输受限的问题。研究者对包括神经和非神经在内的七种无损压缩方法进行了基准测试，结果显示无损神经压缩能有效提升观测站的数据采集效率，并为神经压缩在科学领域的应用提供了实践指导。文章还提及了未来探索有损压缩的可能性。", "keywords": "天文数据压缩, 基准数据集, 神经压缩, 无损压缩, AstroCompress", "comments": "AstroCompress的引入对于推动天文数据压缩领域的发展具有重要意义。它不仅提供了一个标准化的数据集，便于研究人员比较和评估不同的压缩算法，特别是新兴的神经压缩技术，而且通过基准测试证明了神经压缩在提高数据传输效率方面的潜力。这对于解决天文领域日益增长的数据量与有限传输能力之间的矛盾至关重要，有望带来显著的科学和经济效益。其创新性在于提供了一个多功能且易于访问的平台，加速了该领域的研究进展。"}}
{"id": "2506.08027", "title": "Recipes for Pre-training LLMs with MXFP8", "authors": ["Asit Mishra", "Dusan Stosic", "Simon Layton"], "summary": "Precision scaling - using fewer bits to represent model parameters and\nrelated tensors during pre-training - has emerged as a compelling technique for\nimproving GPU efficiency without sacrificing accuracy. Microscaling (MX)\nformats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling\nthis precision scaling aspect. These formats combine narrow floating-point data\ntypes with per-block scaling factors, offering a fine-grained approach to\nquantizing tensors.\n  Although MX-formats offer the promise of improved numeric stability compared\nto other reduced-precision representations, in practice they must be used\ncarefully in order to successfully converge an LLM on a multi-trillion token\ndataset. In this paper, we show that the rounding mode suggested in OCP\nspecification can lead to divergence when pre-training an LLM. We show an\nimproved rounding mode, which uses round-to-infinity to compute scaling\nfactors, enables successful pre-training in MXFP8 for an 8B model on 15T\ntokens.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08027v1", "AI": {"title_translation": "使用MXFP8预训练LLM的秘诀", "tldr": "本文提出了一种改进的舍入模式，用于MXFP8预训练LLM，解决了OCP规范中舍入模式导致的散度问题，成功地在15万亿token上预训练了一个8B模型。", "motivation": "在预训练大型语言模型（LLM）时，精度缩放技术（如MXFP8）在提高GPU效率方面具有潜力，但其数值稳定性，特别是舍入模式，可能导致模型发散，尤其是在大规模数据集上。", "method": "本文首先指出OCP规范中建议的舍入模式可能导致LLM预训练发散。然后，提出并展示了一种改进的舍入模式，该模式采用“向无穷大舍入”的方法来计算缩放因子。", "result": "改进的舍入模式（使用向无穷大舍入计算缩放因子）使得在MXFP8中成功预训练一个8B模型（在15万亿token数据集上）成为可能。", "conclusion": "通过采用改进的“向无穷大舍入”模式来计算缩放因子，MXFP8能够克服在预训练大型语言模型时遇到的收敛性挑战，从而实现高效且成功的精度缩放。", "translation": "精度缩放——在预训练期间使用更少的比特来表示模型参数和相关张量——已成为一种在不牺牲准确性的前提下提高GPU效率的引人注目的技术。NVIDIA最新Blackwell GPU中的微缩放（MX）格式代表了实现这种精度缩放方面的一大飞跃。这些格式将窄浮点数据类型与每块缩放因子相结合，提供了一种细粒度的张量量化方法。尽管MX格式有望比其他低精度表示提供更好的数值稳定性，但在实践中，为了成功地在万亿级token数据集上收敛LLM，它们必须谨慎使用。在本文中，我们表明OCP规范中建议的舍入模式在预训练LLM时可能导致发散。我们展示了一种改进的舍入模式，该模式使用向无穷大舍入来计算缩放因子，从而使得在MXFP8中成功预训练一个8B模型（在15万亿token上）成为可能。", "summary": "本文探讨了在使用NVIDIA Blackwell GPU的MXFP8格式预训练大型语言模型时遇到的精度缩放挑战。研究发现，OCP规范中建议的舍入模式可能导致模型发散。为解决此问题，作者提出了一种改进的舍入模式，即使用“向无穷大舍入”来计算缩放因子。实验证明，这种新模式成功地在15万亿token数据集上预训练了一个8B模型，从而实现了MXFP8在LLM训练中的有效应用。", "keywords": "MXFP8, 精度缩放, LLM预训练, 舍入模式, Blackwell GPU", "comments": "这篇论文通过提出一种创新的舍入模式，解决了MXFP8在大型语言模型预训练中遇到的实际收敛性问题，这对于利用最新GPU硬件进行高效AI训练具有重要意义。它强调了在低精度训练中数值稳定性细节的重要性。"}}
{"id": "2506.08962", "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis", "authors": ["Liangliang Chen", "Huiru Xie", "Jacqueline Rohde", "Ying Zhang"], "summary": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled\nsmart tutor designed to provide homework assessment and feedback for students\nin an undergraduate circuit analysis course. We detail the tutor's design\nphilosophy and core components, including open-ended question answering and\nhomework feedback generation. The prompts are carefully crafted to optimize\nresponses across different problems. The smart tutor was deployed on the\nMicrosoft Azure platform and is currently in use in an undergraduate circuit\nanalysis course at the School of Electrical and Computer Engineering in a\nlarge, public, research-intensive institution in the Southeastern United\nStates. Beyond offering personalized instruction and feedback, the tutor\ncollects student interaction data, which is summarized and shared with the\ncourse instructor. To evaluate its effectiveness, we collected student\nfeedback, with 90.9% of responses indicating satisfaction with the tutor.\nAdditionally, we analyze a subset of collected data on preliminary circuit\nanalysis topics to assess tutor usage frequency for each problem and identify\nfrequently asked questions. These insights help instructors gain real-time\nawareness of student difficulties, enabling more targeted classroom\ninstruction. In future work, we will release a full analysis once the complete\ndataset is available after the Spring 2025 semester. We also explore the\npotential applications of this smart tutor across a broader range of\nengineering disciplines by developing improved prompts, diagram-recognition\nmethods, and database management strategies, which remain ongoing areas of\nresearch.", "comment": "Accepted to 2025 Frontiers in Education (FIE) Conference", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.08962v1", "AI": {"title_translation": "WIP: 大型语言模型增强型智能导师用于本科电路分析", "tldr": "本文介绍并部署了一个基于大型语言模型的智能导师，为本科电路分析学生提供作业评估和反馈，初步评估显示学生满意度高，并能为教师提供学生学习困难的实时洞察。", "motivation": "为本科电路分析课程的学生提供个性化的作业评估和反馈，并帮助教师实时了解学生的学习困难，从而优化课堂教学。", "method": "该研究设计并开发了一个AI驱动的智能导师，其核心组件包括开放式问题回答和作业反馈生成。通过精心设计提示以优化响应，并将该导师部署在Microsoft Azure平台，目前已在一门本科电路分析课程中实际使用。系统还收集学生交互数据并与教师共享，并通过学生反馈（满意度调查）和对部分数据的分析（使用频率和常见问题）来评估其有效性。", "result": "90.9%的学生对该智能导师表示满意。系统成功收集了学生交互数据，并为教师提供了学生困难的实时感知，有助于实现更有针对性的课堂教学。通过分析部分数据，评估了导师的使用频率并识别了常见问题。", "conclusion": "该基于大型语言模型的智能导师在本科电路分析课程中表现出高学生满意度，并能有效帮助教师了解学生学习情况，具有提升教学效果的潜力，并有望推广到其他工程学科。", "translation": "这篇研究到实践的进行中工作（WIP）论文介绍了一个AI驱动的智能导师，旨在为本科电路分析课程的学生提供作业评估和反馈。我们详细阐述了导师的设计理念和核心组件，包括开放式问题回答和作业反馈生成。提示经过精心设计，以优化不同问题的响应。该智能导师已部署在Microsoft Azure平台，目前正在美国东南部一所大型公共研究型机构的电气与计算机工程学院的本科电路分析课程中使用。除了提供个性化指导和反馈外，该导师还收集学生交互数据，这些数据经过总结并分享给课程教师。为了评估其有效性，我们收集了学生反馈，其中90.9%的反馈表示对导师满意。此外，我们分析了初步电路分析主题的部分收集数据，以评估每个问题的导师使用频率并识别常见问题。这些见解有助于教师实时了解学生的困难，从而实现更有针对性的课堂教学。在未来的工作中，一旦2025年春季学期之后完整的数据库可用，我们将发布全面分析。我们还将通过开发改进的提示、图表识别方法和数据库管理策略，探索该智能导师在更广泛工程学科中的潜在应用，这些仍是正在进行的研究领域。", "summary": "本文介绍了一个基于大型语言模型（LLM）的智能导师，旨在为本科电路分析学生提供作业评估和个性化反馈。该导师已部署在Microsoft Azure平台，并在实际课程中应用。研究表明，学生满意度高达90.9%，且系统能为教师提供学生学习困难的实时洞察，从而优化教学。未来的工作将包括对完整数据集的全面分析，并探索其在其他工程领域的应用潜力。", "keywords": "智能导师, 大型语言模型, 电路分析, 作业评估, 学生反馈", "comments": "这项工作展示了大型语言模型在教育领域，特别是智能辅导系统中的实际应用潜力。通过提供个性化反馈和实时数据洞察，该系统有望显著提升教学效率和学生学习体验。其创新性在于结合了LLM的强大生成能力与教育场景的特定需求，并已在真实环境中进行部署和初步验证。未来的研究重点将是更全面的数据分析和跨学科推广，这对于评估其长期效果和泛化能力至关重要。"}}
{"id": "2506.08653", "title": "Parallel FFTW on RISC-V: A Comparative Study including OpenMP, MPI, and HPX", "authors": ["Alexander Strack", "Christopher Taylor", "Dirk Pflüger"], "summary": "Rapid advancements in RISC-V hardware development shift the focus from\nlow-level optimizations to higher-level parallelization. Recent RISC-V\nprocessors, such as the SOPHON SG2042, have 64 cores. RISC-V processors with\ncore counts comparable to the SG2042, make efficient parallelization as crucial\nfor RISC-V as the more established processors such as x86-64. In this work, we\nevaluate the parallel scaling of the widely used FFTW library on RISC-V for MPI\nand OpenMP. We compare it to a 64-core AMD EPYC 7742 CPU side by side for\ndifferent types of FFTW planning. Additionally, we investigate the effect of\nmemory optimization on RISC-V in HPX-FFT, a parallel FFT library based on the\nasynchronous many-task runtime HPX using an FFTW backend. We generally observe\na performance delta between the x86-64 and RISC-V chips of factor eight for\ndouble-precision 2D FFT. Effective memory optimizations in HPX-FFT on x86-64 do\nnot translate to the RISC-V chip. FFTW with MPI shows good scaling up to 64\ncores on x86-64 and RISC-V regardless of planning. In contrast, FFTW with\nOpenMP requires measured planning on both architectures to achieve good scaling\nup to 64 cores. The results of our study mark an early step on the journey to\nlarge-scale parallel applications running on RISC-V.", "comment": "12 pages, 9 figures, Fifth International workshop on RISC-V for HPC\n  co-located with ISC 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.08653v1", "AI": {"title_translation": "RISC-V上的并行FFTW：一项包含OpenMP、MPI和HPX的比较研究", "tldr": "本文比较了在RISC-V处理器上使用OpenMP、MPI和HPX并行FFTW库的性能，并与x86-64进行了对比，发现RISC-V在双精度2D FFT上性能差距较大，但MPI和OpenMP在特定条件下可实现良好扩展性，标志着RISC-V大规模并行应用发展的重要一步。", "motivation": "RISC-V硬件的快速发展使得并行化变得至关重要，尤其是在核心数量与x86-64相当的处理器上，因此需要评估FFTW等常用库在RISC-V上的并行扩展性。", "method": "本研究评估了在RISC-V上使用MPI和OpenMP的FFTW库的并行扩展性，并与64核AMD EPYC 7742 CPU进行了并排比较，针对不同类型的FFTW规划。此外，还研究了基于HPX的并行FFT库HPX-FFT在RISC-V上内存优化的效果。", "result": "在双精度2D FFT中，x86-64和RISC-V芯片之间普遍存在8倍的性能差异。在x86-64上有效的HPX-FFT内存优化在RISC-V芯片上未能体现。无论规划如何，FFTW与MPI在x86-64和RISC-V上都能很好地扩展到64核。相反，FFTW与OpenMP在两种架构上都需要经过测量的规划才能实现良好的64核扩展性。", "conclusion": "本研究的结果标志着RISC-V上运行大规模并行应用程序的早期一步。", "translation": "RISC-V硬件开发的快速进展将焦点从低级优化转向更高级的并行化。SOPHON SG2042等近期RISC-V处理器拥有64个核心。核心数量与SG2042相当的RISC-V处理器，使得高效并行化对于RISC-V而言，与x86-64等更成熟的处理器一样至关重要。在这项工作中，我们评估了RISC-V上广泛使用的FFTW库在MPI和OpenMP方面的并行扩展性。我们将其与64核AMD EPYC 7742 CPU并排比较，针对不同类型的FFTW规划。此外，我们还研究了基于异步多任务运行时HPX并使用FFTW后端构建的并行FFT库HPX-FFT中内存优化对RISC-V的影响。我们普遍观察到，在双精度2D FFT中，x86-64和RISC-V芯片之间存在8倍的性能差异。在x86-64上有效的HPX-FFT内存优化未能移植到RISC-V芯片上。无论规划如何，FFTW与MPI在x86-64和RISC-V上都能很好地扩展到64核。相比之下，FFTW与OpenMP在两种架构上都需要经过测量的规划才能实现良好的64核扩展性。我们研究的结果标志着在RISC-V上运行大规模并行应用程序的早期一步。", "summary": "本研究评估了在RISC-V处理器上并行FFTW库的性能，并将其与x86-64架构进行了比较，重点关注OpenMP、MPI和HPX并行范式。结果显示，在双精度2D FFT方面，RISC-V与x86-64存在显著的性能差距（约8倍）。尽管HPX-FFT在x86-64上的内存优化在RISC-V上表现不佳，但FFTW结合MPI在两种架构上均能良好扩展至64核。FFTW与OpenMP则需要精细的规划才能实现类似扩展。这项工作为RISC-V上开发大规模并行应用奠定了基础。", "keywords": "RISC-V, FFTW, 并行计算, OpenMP, MPI, HPX", "comments": "本文对RISC-V平台上的并行计算性能进行了早期且重要的探索。通过比较FFTW在不同并行编程模型（OpenMP, MPI, HPX）下的表现，并与成熟的x86-64架构进行对比，揭示了RISC-V在高性能计算领域面临的挑战和潜力。特别是指出了现有优化策略在RISC-V上可能不直接适用的问题，为未来的硬件和软件协同优化提供了方向。这项研究的重要性在于其为RISC-V生态系统在高性能计算领域的发展提供了宝贵的数据和见解。"}}
{"id": "2506.08517", "title": "Guidelines for Gaze-based Neural Preliminary Diagnosis", "authors": ["Mayar Elfares", "Salma Younis", "Pascal Reisert", "Ralf Küsters", "Tobias Renner", "Andreas Bulling"], "summary": "Neural disorders refer to any condition affecting the nervous system and that\ninfluence how individuals perceive and interact with the world. Traditional\nneural diagnoses rely on cumbersome, time-consuming, or subjective methods,\nsuch as clinical interviews, behavioural observations, or medical imaging. Eye\ntracking is an attractive alternative because analysing eye movements, such as\nfixations and saccades, can provide more objective insights into brain function\nand cognitive processing by capturing non-verbal and unconscious responses.\nDespite its potential, existing gaze-based studies presented seemingly\ncontradictory findings. They are dispersed across diverse fields, requiring\nfurther research to standardise protocols and expand their application,\nparticularly as a preliminary indicator of neural processes for differential\ndiagnosis. Therefore, this paper outlines the main agreed-upon findings and\nprovides a systematisation of knowledge and key guidelines towards advancing\ngaze-based neural preliminary diagnosis.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08517v1", "AI": {"title_translation": "基于眼动追踪的神经系统初步诊断指南", "tldr": "本论文概述了关于基于眼动追踪的神经系统初步诊断的主要共识发现，并提供了知识系统化和关键指南，以推进该领域的发展。", "motivation": "传统的神经系统诊断方法繁琐、耗时或主观。尽管眼动追踪作为一种替代方案具有潜力，但现有研究结果存在矛盾且分散，缺乏标准化的协议和应用扩展，尤其是在作为鉴别诊断的初步指标方面。", "method": "本论文概述了主要共识发现，并提供了知识系统化和关键指南。", "result": "本论文为推进基于眼动追踪的神经系统初步诊断提供了知识系统化和关键指南。", "conclusion": "本论文旨在通过提供共识发现、知识系统化和关键指南，推进基于眼动追踪的神经系统初步诊断领域的发展。", "translation": "神经系统疾病是指影响神经系统的任何疾病，并影响个体感知和与世界互动的方式。传统的神经系统诊断依赖于繁琐、耗时或主观的方法，例如临床访谈、行为观察或医学影像。眼动追踪是一种有吸引力的替代方案，因为分析眼球运动，如注视和扫视，可以通过捕捉非语言和无意识的反应，为大脑功能和认知处理提供更客观的见解。尽管其潜力巨大，但现有的基于眼动追踪的研究结果似乎相互矛盾。它们分散在不同的领域，需要进一步研究以标准化协议并扩展其应用，特别是作为鉴别诊断中神经过程的初步指标。因此，本文概述了主要的共识发现，并提供了知识系统化和关键指南，以推进基于眼动追踪的神经系统初步诊断。", "summary": "本论文旨在解决传统神经系统诊断方法的局限性以及现有基于眼动追踪研究的矛盾和分散性。文章概述了眼动追踪在提供客观大脑功能和认知处理见解方面的潜力，并通过总结主要共识发现、系统化知识和提供关键指南，以标准化协议并扩展基于眼动追踪的神经系统初步诊断的应用。", "keywords": "眼动追踪, 神经系统诊断, 初步诊断, 神经系统疾病, 指南", "comments": "该论文的重要性在于其对现有分散的眼动追踪研究进行整合和标准化，为基于眼动追踪的神经系统初步诊断提供了急需的指南和知识系统化。这对于推动该领域的研究和临床应用具有重要意义。"}}
{"id": "2506.08655", "title": "When Simple Model Just Works: Is Network Traffic Classification in Crisis?", "authors": ["Kamil Jerabek", "Jan Luxemburk", "Richard Plny", "Josef Koumar", "Jaroslav Pesek", "Karel Hynek"], "summary": "Machine learning has been applied to network traffic classification (TC) for\nover two decades. While early efforts used shallow models, the latter 2010s saw\na shift toward complex neural networks, often reporting near-perfect accuracy.\nHowever, it was recently revealed that a simple k-NN baseline using packet\nsequences metadata (sizes, times, and directions) can be on par or even\noutperform more complex methods. In this paper, we investigate this phenomenon\nfurther and evaluate this baseline across 12 datasets and 15 TC tasks, and\ninvestigate why it performs so well. Our analysis shows that most datasets\ncontain over 50% redundant samples (identical packet sequences), which\nfrequently appear in both training and test sets due to common splitting\npractices. This redundancy can lead to overestimated model performance and\nreduce the theoretical maximum accuracy when identical flows have conflicting\nlabels. Given its distinct characteristics, we further argue that standard\nmachine learning practices adapted from domains like NLP or computer vision may\nbe ill-suited for TC. Finally, we propose new directions for task formulation\nand evaluation to address these challenges and help realign the field.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08655v1", "AI": {"title_translation": "当简单模型有效时：网络流量分类是否处于危机之中？", "tldr": "本文研究发现，简单的k-NN模型在网络流量分类中表现出色，但这是因为现有数据集存在大量冗余样本，导致模型性能被高估。作者认为现有机器学习实践不适用于流量分类，并提出了新的方向。", "motivation": "尽管复杂的神经网络在网络流量分类中报告了近乎完美的准确率，但最近发现简单的k-NN基线模型在性能上可以与之匹敌甚至超越。本文旨在深入调查这一现象，并探究其原因。", "method": "本文在12个数据集和15个流量分类任务上评估了一个简单的k-NN基线模型，并分析了其表现优异的原因。研究方法包括对数据集冗余样本的分析。", "result": "研究表明，大多数数据集包含超过50%的冗余样本（相同的包序列），这些样本由于常见的数据划分实践而频繁出现在训练集和测试集中。这种冗余会导致模型性能被高估，并且当相同流量具有冲突标签时，会降低理论上的最大准确率。", "conclusion": "本文认为，从NLP或计算机视觉等领域借鉴的标准机器学习实践可能不适用于网络流量分类。因此，作者提出了新的任务制定和评估方向，以应对这些挑战并重新调整该领域。", "translation": "机器学习已应用于网络流量分类（TC）二十多年。早期工作使用浅层模型，而2010年代后期则转向复杂的神经网络，通常报告接近完美的准确率。然而，最近发现一个使用数据包序列元数据（大小、时间、方向）的简单k-NN基线模型可以与更复杂的方法相媲美甚至超越。在本文中，我们进一步调查了这一现象，并在12个数据集和15个TC任务上评估了该基线模型，并研究了它表现如此出色的原因。我们的分析表明，大多数数据集包含超过50%的冗余样本（相同的包序列），由于常见的数据划分实践，这些样本频繁出现在训练集和测试集中。这种冗余可能导致模型性能被高估，并且当相同流量具有冲突标签时，会降低理论上的最大准确率。鉴于其独特的特性，我们进一步认为，从NLP或计算机视觉等领域借鉴的标准机器学习实践可能不适用于TC。最后，我们提出了任务制定和评估的新方向，以应对这些挑战并帮助重新调整该领域。", "summary": "本文深入探讨了在网络流量分类中，简单k-NN模型为何能与复杂神经网络媲美甚至超越的现象。研究发现，现有数据集普遍存在大量冗余样本（超过50%），这些冗余样本在训练集和测试集中重复出现，导致模型性能被高估，并限制了理论最大准确率。文章指出，从其他领域借鉴的标准机器学习实践可能不适用于流量分类，并提出了新的任务制定和评估方法以应对这些挑战。", "keywords": "网络流量分类, k-NN, 数据冗余, 机器学习评估, 简单模型", "comments": "这篇论文对网络流量分类领域提出了一个重要的警示，挑战了当前普遍采用的机器学习实践和对模型性能的认知。它揭示了数据质量和评估方法中的深层问题，强调了在特定领域（如流量分类）中，简单模型可能因数据缺陷而表现出“虚假”的优越性。其创新之处在于对数据冗余的深入分析及其对模型评估的影响，为该领域未来的研究指明了方向，即需要更严格的数据处理和更适合领域特性的评估方法，而不是盲目追求复杂模型。"}}
{"id": "2506.08475", "title": "Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems", "authors": ["Xiaolong He", "Yeonjong Shin", "Anthony Gruber", "Sohyeon Jung", "Kookjin Lee", "Youngsoo Choi"], "summary": "We propose an efficient thermodynamics-informed latent space dynamics\nidentification (tLaSDI) framework for the reduced-order modeling of parametric\nnonlinear dynamical systems. This framework integrates autoencoders for\ndimensionality reduction with newly developed parametric GENERIC\nformalism-informed neural networks (pGFINNs), which enable efficient learning\nof parametric latent dynamics while preserving key thermodynamic principles\nsuch as free energy conservation and entropy generation across the parameter\nspace. To further enhance model performance, a physics-informed active learning\nstrategy is incorporated, leveraging a greedy, residual-based error indicator\nto adaptively sample informative training data, outperforming uniform sampling\nat equivalent computational cost. Numerical experiments on the Burgers'\nequation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed\nmethod achieves up to 3,528x speed-up with 1-3% relative errors, and\nsignificant reduction in training (50-90%) and inference (57-61%) cost.\nMoreover, the learned latent space dynamics reveal the underlying thermodynamic\nbehavior of the system, offering valuable insights into the physical-space\ndynamics.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08475v1", "AI": {"title_translation": "参数系统热力学一致潜在动力学识别", "tldr": "本文提出了一种名为tLaSDI的热力学信息潜在空间动力学识别框架，用于参数非线性动力学系统的降阶建模。该框架结合了自编码器、参数化GENERIC形式信息神经网络以及物理信息主动学习策略，实现了高效、热力学一致且计算成本显著降低的降阶建模，并能揭示系统的潜在热力学行为。", "motivation": "针对参数非线性动力学系统，需要一种高效且能保持关键热力学原理的降阶建模方法。", "method": "本文提出了热力学信息潜在空间动力学识别（tLaSDI）框架。该框架集成了：1. 自编码器用于降维。2. 新开发的参数化GENERIC形式信息神经网络（pGFINNs），用于在参数空间中学习参数化潜在动力学，同时保持自由能守恒和熵生成等热力学原理。3. 引入了物理信息主动学习策略，利用基于残差的贪婪误差指示器自适应地采样信息训练数据，以提高模型性能。", "result": "1. 在Burgers方程和1D/1V Vlasov-Poisson方程上的数值实验表明，所提出的方法实现了高达3,528倍的加速，相对误差为1-3%。2. 训练成本显著降低（50-90%）。3. 推理成本显著降低（57-61%）。4. 学习到的潜在空间动力学揭示了系统的潜在热力学行为，为物理空间动力学提供了有价值的见解。", "conclusion": "所提出的tLaSDI框架能够高效、准确地对参数非线性动力学系统进行降阶建模，同时保持热力学一致性。通过结合深度学习和物理信息主动学习，该方法显著提高了计算效率并降低了成本，同时能深入理解系统的热力学行为。", "translation": "我们提出了一种高效的热力学信息潜在空间动力学识别（tLaSDI）框架，用于参数非线性动力学系统的降阶建模。该框架将自编码器用于降维，并结合了新开发的参数化GENERIC形式信息神经网络（pGFINNs），这使得在整个参数空间中能够高效学习参数化潜在动力学，同时保持关键的热力学原理，如自由能守恒和熵生成。为了进一步提高模型性能，我们引入了一种物理信息主动学习策略，利用基于残差的贪婪误差指示器自适应地采样信息训练数据，在相同的计算成本下优于均匀采样。在Burgers方程和1D/1V Vlasov-Poisson方程上的数值实验表明，所提出的方法实现了高达3,528倍的加速，相对误差为1-3%，并且显著降低了训练（50-90%）和推理（57-61%）成本。此外，学习到的潜在空间动力学揭示了系统的潜在热力学行为，为物理空间动力学提供了有价值的见解。", "summary": "本文提出了一种名为tLaSDI的热力学信息潜在空间动力学识别框架，旨在对参数非线性动力学系统进行高效的降阶建模。该框架结合了自编码器进行降维和基于GENERIC形式的神经网络（pGFINNs）来学习热力学一致的参数化潜在动力学。通过引入物理信息主动学习策略，tLaSDI能够自适应地选择训练数据，进一步提升了模型性能。数值实验证明，该方法在保持高精度的同时，显著提高了计算速度并降低了训练和推理成本，同时还能揭示系统的深层热力学行为。", "keywords": "降阶建模, 潜在动力学, 热力学一致性, 神经网络, 主动学习", "comments": "这篇论文的创新点在于将深度学习（自编码器和神经网络）与热力学原理（GENERIC形式）相结合，实现了一种热力学一致的降阶建模方法，这对于复杂物理系统的仿真和分析具有重要意义。引入物理信息主动学习策略进一步提升了模型的效率和鲁棒性。其在加速和成本降低方面的表现令人印象深刻。"}}
{"id": "2506.08623", "title": "Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification", "authors": ["Rinat Prochii", "Elizaveta Dakhova", "Pavel Birulin", "Maxim Sharaev"], "summary": "Accurate classification of second-trimester fetal ultrasound images remains\nchallenging due to low image quality, high intra-class variability, and\nsignificant class imbalance. In this work, we introduce a simple yet powerful,\nbiologically inspired deep learning ensemble framework that-unlike prior\nstudies focused on only a handful of anatomical targets-simultaneously\ndistinguishes 16 fetal structures. Drawing on the hierarchical, modular\norganization of biological vision systems, our model stacks two complementary\nbranches (a \"shallow\" path for coarse, low-resolution cues and a \"detailed\"\npath for fine, high-resolution features), concatenating their outputs for final\nprediction. To our knowledge, no existing method has addressed such a large\nnumber of classes with a comparably lightweight architecture. We trained and\nevaluated on 5,298 routinely acquired clinical images (annotated by three\nexperts and reconciled via Dawid-Skene), reflecting real-world noise and\nvariability rather than a \"cleaned\" dataset. Despite this complexity, our\nensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies\n90% of organs with accuracy > 0.75 and 75% of organs with accuracy >\n0.85-performance competitive with more elaborate models applied to far fewer\ncategories. These results demonstrate that biologically inspired modular\nstacking can yield robust, scalable fetal anatomy recognition in challenging\nclinical settings.", "comment": "16 pages, 2 figures, 3 tables", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08623v1", "AI": {"title_translation": "生物启发式深度学习方法用于胎儿超声图像分类", "tldr": "开发了一种生物启发式深度学习集成框架，可同时识别16种胎儿结构，在真实临床数据上表现出色。", "motivation": "由于图像质量低、类内变异性高和类别严重不平衡，第二孕期胎儿超声图像的准确分类仍然具有挑战性。现有方法通常只关注少数解剖目标。", "method": "引入一个简单而强大的生物启发式深度学习集成框架，该框架借鉴生物视觉系统的分层模块化组织，堆叠两个互补分支（一个用于粗略、低分辨率线索的“浅层”路径和一个用于精细、高分辨率特征的“详细”路径），并连接它们的输出进行最终预测。该模型是轻量级的，处理16个胎儿结构，并在5,298张真实临床图像上训练和评估（使用EfficientNet-B0 + EfficientNet-B6与LDAM-Focal损失）。", "result": "该集成模型能以 > 0.75 的准确率识别90%的器官，以 > 0.85 的准确率识别75%的器官，性能与处理更少类别的更复杂模型相当。", "conclusion": "生物启发式模块化堆叠可以在具有挑战性的临床环境中实现鲁棒、可扩展的胎儿解剖识别。", "translation": "由于图像质量低、类内变异性高和类别严重不平衡，第二孕期胎儿超声图像的准确分类仍然具有挑战性。在这项工作中，我们引入了一种简单而强大的、受生物学启发的深度学习集成框架，与以往只关注少数解剖目标的研究不同，该框架同时区分16种胎儿结构。借鉴生物视觉系统的分层、模块化组织，我们的模型堆叠了两个互补的分支（一个用于粗略、低分辨率线索的“浅层”路径和一个用于精细、高分辨率特征的“详细”路径），连接它们的输出进行最终预测。据我们所知，目前没有现有方法以类似轻量级的架构处理如此大量的类别。我们在5,298张常规获取的临床图像上进行了训练和评估（由三位专家标注并通过Dawid-Skene协调），这些图像反映了真实世界的噪声和变异性，而非“清理过”的数据集。尽管存在这种复杂性，我们的集成模型（EfficientNet-B0 + EfficientNet-B6，使用LDAM-Focal损失）能够以 > 0.75 的准确率识别90%的器官，并以 > 0.85 的准确率识别75%的器官——其性能与应用于更少类别的更复杂模型具有竞争力。这些结果表明，受生物学启发的模块化堆叠可以在具有挑战性的临床环境中实现鲁棒、可扩展的胎儿解剖识别。", "summary": "本文提出了一种受生物学启发的深度学习集成框架，用于解决第二孕期胎儿超声图像分类中图像质量差、变异性大和类别不平衡的挑战。该框架包含浅层和详细两个路径，可同时识别16种胎儿结构，并在5,298张真实临床图像上进行训练和评估。实验结果表明，该轻量级模型在复杂临床数据上表现出高准确率，证明了生物启发式模块化堆叠在胎儿解剖识别中的鲁棒性和可扩展性。", "keywords": "胎儿超声, 深度学习, 图像分类, 生物启发, 集成学习", "comments": "该研究的创新点在于其生物启发式的模块化设计，能够同时处理多达16种胎儿结构，且模型轻量化，这在多类别胎儿超声图像分类中是一个显著的进步。此外，使用大规模真实世界的、未“清理”的临床数据集进行训练和评估，增强了模型的实际应用价值和鲁棒性。这对于临床辅助诊断具有重要意义。"}}
{"id": "2506.08739", "title": "Timing advance and Doppler shift estimation in LEO satellite networks: A recursive Bayesian study", "authors": ["Ashutosh Balakrishnan", "Pierre Popineau", "Philippe Martins"], "summary": "Low earth orbit (LEO) satellite based non-terrestrial networks are a key\ntheme of the upcoming 6G networks. These space networks are proposed to be used\nfor high-mobility use-cases like airplanes and vehicles. The initial access\nprocess between a base station (BS) and a user equipment (UE) involves timing\nadvance (TA) value computation at the BS, requiring precise BS location\ninformation at the UE. It becomes more challenging in LEO satellite networks\ndue to the fast moving LEO satellites and large pathloss, in addition to the\nmobile UE. This paper aims to compute the TA and Doppler shift experienced at\nthe UE by modeling the joint system dynamics in a LEO satellite-mobile UE\nnetwork through an extended Kalman filter (EKF) based recursive Bayesian\nframework. The framework accurately models the joint system dynamics by\nconsidering the LEO satellite acceleration. It constructs the Jacobian to\nlinearize the inherent non-linearities present in the motion. Probabilistic\ninsights regarding the state-update and propagation are also provided. The\nanalytical framework factors in the limited satellite visibility at the UE and\nthe satellite-UE geometry w.r.t. the earth center. The proposed framework is\nalso useful when the satellite and UE clocks are not in sync, with the\ncorresponding clock drift a function of the measured time difference of\narrivals. Our results showcase the efficacy and robustness of the proposed EKF\nframework to estimate the TA and Doppler shift, even at very high UE speeds.\nThe work is expected to be extremely useful in realizing LEO satellite based\nnon-terrestrial networks.", "comment": "The manuscript is currently under review. It consists of 6 pages, 7\n  figures including subfigures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.08739v1", "AI": {"title_translation": "LEO卫星网络中的定时提前量和多普勒频移估计：一项递归贝叶斯研究", "tldr": "LEO卫星网络中，由于卫星和用户的高速移动，精确估计定时提前量（TA）和多普勒频移极具挑战。本文提出了一种基于扩展卡尔曼滤波器（EKF）的递归贝叶斯框架，通过建模联合系统动力学，即使在极高用户速度下也能有效且鲁棒地估计TA和多普勒频移。", "motivation": "即将到来的6G网络中，基于低地球轨道（LEO）卫星的非地面网络是关键主题，旨在支持飞机和车辆等高移动性用例。然而，在LEO卫星网络中，由于快速移动的LEO卫星和用户设备（UE）以及大的路径损耗，初始接入过程中基站（BS）计算定时提前量（TA）和多普勒频移变得极具挑战。", "method": "本文提出了一种基于扩展卡尔曼滤波器（EKF）的递归贝叶斯框架，通过建模LEO卫星-移动用户设备网络中的联合系统动力学来计算用户设备所经历的定时提前量和多普勒频移。该框架考虑了LEO卫星加速度，构建雅可比矩阵以线性化固有的非线性，并提供了状态更新和传播的概率洞察。它还考虑了用户设备有限的卫星可见性、卫星-用户设备相对于地心的几何关系，并且在卫星和用户设备时钟不同步时也适用。", "result": "研究结果表明，所提出的EKF框架在估计定时提前量和多普勒频移方面具有高效性和鲁棒性，即使在非常高的用户设备速度下也表现出色。", "conclusion": "这项工作预计将对实现基于LEO卫星的非地面网络极其有用。", "translation": "低地球轨道（LEO）卫星非地面网络是即将到来的6G网络的一个关键主题。这些空间网络被提议用于飞机和车辆等高移动性用例。基站（BS）和用户设备（UE）之间的初始接入过程涉及在BS处计算定时提前量（TA），这要求UE具有精确的BS位置信息。由于快速移动的LEO卫星和大的路径损耗，加上移动的UE，这在LEO卫星网络中变得更具挑战性。本文旨在通过扩展卡尔曼滤波器（EKF）的递归贝叶斯框架，建模LEO卫星-移动UE网络中的联合系统动力学，以计算UE所经历的TA和多普勒频移。该框架通过考虑LEO卫星加速度，准确建模了联合系统动力学。它构建了雅可比矩阵以线性化运动中固有的非线性。还提供了关于状态更新和传播的概率洞察。该分析框架考虑了UE处有限的卫星可见性以及卫星-UE相对于地球中心的几何关系。所提出的框架在卫星和UE时钟不同步时也很有用，相应的时钟漂移是测量到的到达时间差的函数。我们的结果展示了所提出的EKF框架在估计TA和多普勒频移方面的有效性和鲁棒性，即使在非常高的UE速度下也是如此。这项工作预计将对实现基于LEO卫星的非地面网络极其有用。", "summary": "针对LEO卫星网络中，由于卫星和用户高速移动导致的定时提前量和多普勒频移精确估计难题，本文提出了一种基于扩展卡尔曼滤波器（EKF）的递归贝叶斯框架。该框架通过建模联合系统动力学，考虑LEO卫星加速度，利用雅可比矩阵处理非线性，并兼顾有限卫星可见性、卫星-UE几何关系及非同步时钟等实际因素。实验结果证明，即使在极高的用户速度下，该框架也能有效且鲁棒地估计定时提前量和多普勒频移，对未来实现基于LEO卫星的非地面网络具有重要实践意义。", "keywords": "LEO卫星网络, 定时提前量, 多普勒频移, 扩展卡尔曼滤波器, 递归贝叶斯", "comments": "本文的创新点在于提出了一个专门针对LEO卫星网络环境优化的EKF-based递归贝叶斯框架，能够有效处理高速移动带来的复杂动力学（如卫星加速度和非线性）。其鲁棒性，尤其是在高UE速度下的表现，是其重要优势，为未来6G非地面网络中的高精度定位和同步提供了有价值的解决方案。"}}
{"id": "2506.08471", "title": "Passive acoustic non-line-of-sight localization without a relay surface", "authors": ["Tal I. Sommer", "Ori Katz"], "summary": "The detection and localization of a source hidden outside the Line-of-Sight\n(LOS) traditionally rely on the acquisition of indirect signals, such as those\nreflected from visible relay surfaces such as floors or walls. These reflected\nsignals are then utilized to reconstruct the obscured scene. In this study, we\npresent an approach that utilize signals diffracted from an edge of an obstacle\nto achieve three-dimensional (3D) localization of an acoustic point source\nsituated outside the LOS. We address two scenarios - a doorway and a convex\ncorner - and propose a localization method for each of them. For the first\nscenario, we utilize the two edges of the door as virtual detector arrays. For\nthe second scenario, we exploit the spectral signature of a knife-edge\ndiffraction, inspired by the human perception of sound location by the\nhead-related transfer function (HRTF). In both methods, knife-edge diffraction\nis utilized to extend the capabilities of non-line-of-sight (NLOS) acoustic\nsensing, enabling localization in environments where conventional relay-surface\nbased approaches may be limited.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08471v1", "AI": {"title_translation": "无中继面被动声学非视距定位", "tldr": "该研究提出了一种无需中继面，利用障碍物边缘衍射信号实现三维声源非视距定位的方法，适用于门口和凸角两种场景。", "motivation": "传统的非视距检测和定位依赖于从可见中继面（如地板或墙壁）反射的间接信号。然而，这种基于中继面的方法在某些环境中可能受到限制。", "method": "本研究提出一种利用障碍物边缘衍射信号实现声学点源三维定位的方法。针对两种场景：门口和凸角，分别提出了定位方法。对于门口场景，利用门的两个边缘作为虚拟探测器阵列。对于凸角场景，受人耳对声音定位的头相关传输函数（HRTF）启发，利用刀口衍射的光谱特征。两种方法都利用刀口衍射来扩展非视距声学传感的能力。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "传统上，视线外（LOS）隐藏声源的检测和定位依赖于获取间接信号，例如从可见中继面（如地板或墙壁）反射的信号。然后利用这些反射信号来重建被遮挡的场景。在本研究中，我们提出了一种利用障碍物边缘衍射信号来实现位于视线外的声学点源三维（3D）定位的方法。我们探讨了两种场景——门口和凸角——并为每种场景提出了一种定位方法。对于第一种场景，我们利用门的两个边缘作为虚拟探测器阵列。对于第二种场景，我们受人耳通过头相关传输函数（HRTF）感知声音位置的启发，利用刀口衍射的光谱特征。在这两种方法中，都利用刀口衍射来扩展非视距（NLOS）声学传感的能力，从而能够在传统基于中继面的方法可能受限的环境中实现定位。", "summary": "本研究提出了一种创新性的被动声学非视距定位方法，无需依赖传统的中继面反射信号。该方法利用声波在障碍物边缘（如门口和凸角）产生的衍射信号，实现了三维声源的定位。研究针对不同场景设计了特定的定位策略，扩展了非视距声学传感在复杂环境中的应用能力。", "keywords": "被动声学, 非视距定位, 衍射, 刀口衍射, 三维定位", "comments": "该论文的创新点在于提出了不依赖传统反射中继面，而是利用障碍物边缘衍射信号进行非视距声源定位的方法。这为在传统方法受限的环境中进行声学传感提供了新的思路和能力，具有重要的理论和实际意义。特别是将人耳对声音定位的启发应用于刀口衍射的利用，体现了跨学科的融合。"}}
{"id": "2506.08032", "title": "Tram Positioning with Map-Enabled GNSS Data Reconciliation", "authors": ["Jakub Kašpar", "Vít Fanta", "Vladimír Havlena"], "summary": "This paper presents an approach to tackle the problem of tram localization\nthrough utilizing a custom processing of Global Navigation Satellite System\n(GNSS) observables and the track map. The method is motivated by suboptimal\nperformance in dense urban environments where the direct line of sight to GNSS\nsatellites is often obscured which leads to multipath propagation of GNSS\nsignals. The presented concept is based upon the iterated extended Kalman\nfilter (IEKF) and has linear complexity (with respect to the number of GNSS\nmeasurements) as opposed to some other techniques mitigating the multipath\nsignal propagation. The technique is demonstrated both on a simulated example\nand real data. The root-mean-squared errors from the simulated ground truth\npositions show that the presented solution is able to improve performance\ncompared to a baseline localization approach. Similar result is achieved for\nthe experiment with real data, while treating orthogonal projections onto the\ntram track as the true position, which is unavailable in the realistic\nscenario. This proof-of-concept shows results which may be further improved\nwith implementation of a bank-of-models method or $\\chi^2$-based rejection of\noutlying GNSS pseudorange measurements.", "comment": "Submitted to European Control Conference 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08032v1", "AI": {"title_translation": "有地图辅助的GNSS数据协调式有轨电车定位", "tldr": "本文提出了一种利用GNSS数据和轨道地图，通过迭代扩展卡尔曼滤波器解决城市环境中电车定位多径效应问题的方法，并在模拟和真实数据上显示出比基线方法更好的性能。", "motivation": "在密集的城市环境中，GNSS卫星的直射视线经常被遮挡，导致GNSS信号的多径传播，从而使得有轨电车定位性能不佳。", "method": "本文提出了一种利用全球导航卫星系统（GNSS）可观测数据和轨道地图进行定制处理的有轨电车定位方法。该方法基于迭代扩展卡尔曼滤波器（IEKF），并且相对于其他缓解多径信号传播的技术，具有线性复杂度。", "result": "模拟地面真实位置的均方根误差表明，所提出的解决方案能够提高定位性能，优于基线定位方法。在真实数据实验中也取得了类似的结果，其中将有轨电车轨道上的正交投影视为真实位置。", "conclusion": "该概念验证表明，利用GNSS数据和轨道地图的IEKF方法能够有效改善有轨电车在城市环境中的定位精度，未来可通过实现模型库方法或基于$\\chi^2$的异常GNSS伪距测量值剔除进一步改进。", "translation": "本文提出了一种通过利用全球导航卫星系统（GNSS）可观测数据和轨道地图的定制处理来解决有轨电车定位问题的方法。该方法旨在解决在密集城市环境中，GNSS卫星的直射视线经常被遮挡，导致GNSS信号多径传播，从而导致性能不佳的问题。所提出的概念基于迭代扩展卡尔曼滤波器（IEKF），并且与一些其他缓解多径信号传播的技术相比，具有线性复杂度（相对于GNSS测量值的数量）。该技术在模拟示例和真实数据上均进行了演示。模拟地面真实位置的均方根误差表明，所提出的解决方案能够提高性能，优于基线定位方法。在真实数据实验中也取得了类似的结果，其中将有轨电车轨道上的正交投影视为真实位置，这在实际场景中是不可用的。这项概念验证显示的结果可以通过实现模型库方法或基于$\\chi^2$的异常GNSS伪距测量值剔除进一步改进。", "summary": "本文提出了一种新颖的有轨电车定位方法，通过结合定制的GNSS数据处理和轨道地图来解决城市多径效应导致的定位精度问题。该方法采用迭代扩展卡尔曼滤波器（IEKF），并具有线性计算复杂度。在模拟和真实数据测试中，该方法均展现出优于传统基线定位方案的性能提升，为城市环境下有轨电车的高精度定位提供了可行的解决方案。", "keywords": "有轨电车定位, GNSS, 多径效应, 迭代扩展卡尔曼滤波器, 轨道地图", "comments": "本文创新性地将轨道地图信息与GNSS数据处理结合，并采用IEKF来解决城市环境中GNSS信号多径传播导致的定位精度问题，具有较好的实用价值。其线性复杂度使其在实时应用中具有优势。然而，真实数据实验中对“真实位置”的处理方式（正交投影）是一个局限性，未来改进方向也明确指出，显示了进一步提升性能的潜力。"}}
{"id": "2506.08261", "title": "Towards universally optimal sorting algorithms", "authors": ["Sandeep Sen"], "summary": "We formalize a new paradigm for optimality of algorithms, that generalizes\nworst-case optimality based only on input-size to problem-dependent parameters\nincluding implicit ones. We re-visit some existing sorting algorithms from this\nperspective, and also present a novel measure of sortedness that leads to an\noptimal algorithm based on partition sort. This paradigm of measuring\nefficiency of algorithms looks promising for further interesting applications\nbeyond the existing ones.", "comment": "8 pages", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.08261v1", "AI": {"title_translation": "迈向普遍最优排序算法", "tldr": "提出一种新的算法最优性范式，超越传统基于输入大小的衡量标准，引入问题依赖参数，并设计出一种基于分区排序的最优算法。", "motivation": "现有算法最优性衡量标准仅基于输入大小，无法捕捉问题依赖参数，因此需要一种更通用的最优性范式。", "method": "形式化了一种新的算法最优性范式，该范式将最坏情况最优性从仅基于输入大小推广到包括隐式参数在内的、依赖于问题的参数。同时，提出了一种新的排序程度度量方法，并基于此设计了一种最优的分区排序算法。", "result": "提出了一种新的排序程度度量，并基于此度量开发了一种最优的分区排序算法。", "conclusion": "新的算法效率衡量范式在现有应用之外，对于未来的更多有趣应用具有广阔前景。", "translation": "我们形式化了一种新的算法最优性范式，该范式将仅基于输入大小的最坏情况最优性推广到包括隐式参数在内的、依赖于问题的参数。我们从这个角度重新审视了一些现有排序算法，并提出了一种新的排序程度度量方法，该方法导出了一个基于分区排序的最优算法。这种衡量算法效率的范式对于现有应用之外的更多有趣应用看起来很有前景。", "summary": "这篇论文提出了一种新的算法最优性范式，旨在将传统的、仅基于输入大小的最坏情况最优性扩展到包含问题依赖参数。作者从这一新视角重新审视了现有排序算法，并引入了一种新颖的排序程度度量，进而设计出一种基于分区排序的最优算法。研究表明，这种新的算法效率衡量范式在更广泛的应用中具有潜力。", "keywords": "算法最优性, 排序算法, 分区排序, 问题依赖参数, 排序程度度量", "comments": "这篇论文的创新点在于提出了一个更普适的算法最优性衡量框架，超越了传统上仅关注输入大小的限制，引入了问题依赖参数。这种方法可能为设计更高效、更适应特定问题的算法提供新的思路。其重要性在于为算法设计和分析提供了新的理论基础。"}}
{"id": "2506.08161", "title": "GATE: Geometry-Aware Trained Encoding", "authors": ["Jakub Bokšanský", "Daniel Meister", "Carsten Benthin"], "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.08161v1", "AI": {"title_translation": "几何感知训练编码 (GATE)", "tldr": "提出了一种名为 GATE 的新型几何感知编码方法，它将特征向量存储在三角网格表面，以解决传统哈希编码的局限性，并提升神经网络训练和自适应细节控制。", "motivation": "神经网络算法中输入参数的编码是其基本组成部分之一，对于神经网络的效率和近似质量至关重要。然而，现有的哈希编码方案存在诸多局限性，如哈希冲突、分辨率与场景大小选择困难以及发散的内存访问。", "method": "本文提出了一种名为 GATE (Geometry-Aware Trained Encoding) 的新型几何感知编码方法。该方法将特征向量直接存储在三角网格的表面上，并通过网格颜色将特征向量密度与几何密度解耦。", "result": "GATE 编码成功避免了传统哈希编码方案的局限性，包括哈希冲突、分辨率与场景大小选择的困扰以及发散的内存访问问题。此外，它允许对神经网络训练进行更精细的控制，并支持自适应的细节级别。该方法特别适用于神经渲染相关的算法，例如神经辐射缓存。", "conclusion": "GATE 是一种新颖且有效的几何感知编码方案，通过利用三角网格结构来存储特征向量，成功克服了现有哈希编码方法的不足，并为神经网络训练提供了更优的控制和自适应细节能力，尤其适用于神经渲染等应用。", "translation": "神经网络算法中输入参数的编码是其基本组成部分之一。其目标是将输入数据映射到更高维空间，通常由经过训练的特征向量支持。这种映射对于神经网络的效率和近似质量至关重要。我们提出了一种新颖的几何感知编码方法，称为 GATE，它将特征向量存储在三角网格的表面上。我们的编码适用于神经渲染相关的算法，例如神经辐射缓存。它还避免了以前基于哈希的编码方案的局限性，例如哈希冲突、分辨率与场景大小的选择，以及发散的内存访问。我们的方法使用网格颜色将特征向量密度与几何密度解耦，同时允许对神经网络训练进行更精细的控制和自适应细节级别。", "summary": "本文介绍了一种名为 GATE (Geometry-Aware Trained Encoding) 的新型几何感知编码方法，旨在优化神经网络中输入参数的编码过程。GATE 的核心创新在于将特征向量直接存储在三角网格的表面上，这有效地解决了传统哈希编码方案所面临的哈希冲突、内存访问效率低下以及分辨率与场景大小选择困难等问题。该方法通过网格颜色实现了特征向量密度与几何密度的解耦，从而提供了对神经网络训练更精细的控制，并支持自适应的细节级别。GATE 特别适用于神经渲染等需要高效率和高精度的应用。", "keywords": "几何感知编码, 神经网络, 三角网格, 特征向量, 神经渲染", "comments": "GATE 的创新之处在于将几何结构（三角网格）直接融入到特征编码中，这与传统的基于哈希或空间划分的编码方式有显著区别。这种方法有望在神经渲染等需要精确几何对应关系的领域提供更稳定和高效的解决方案，尤其是在处理复杂场景和实现细节自适应方面。其通过网格颜色解耦特征向量密度与几何密度，也为精细控制和自适应 LOD 提供了新的思路。"}}
{"id": "2506.08484", "title": "Efficient Fireworks Algorithm Equipped with an Explosion Mechanism based on Student's T-distribution", "authors": ["Cen Shipeng", "Tan Ying"], "summary": "Many real-world problems can be transformed into optimization problems, which\ncan be classified into convex and non-convex. Although convex problems are\nalmost completely studied in theory, many related algorithms to many non-convex\nproblems do not work well and we need more optimization techniques. As a swarm\nintelligence optimization algorithm, the Fireworks Algorithm(FWA) has been\nwidely studied and applied to many real-world scenarios, even including large\nlanguage model fine-tuning. But the current fireworks algorithm still has a\nnumber of problems. Firstly, as a heuristic algorithm, its performance on\nconvex problems cannot match the SOTA results, and can even be said to be\nunsatisfactory; secondly, the sampling methods (explosion) of most FWA variants\nare still uniform sampling, which is actually inefficient in high dimensional\ncases. This work of ours proposes a new student's t-distribution based\nFWA(TFWA) with a solid theoretical foundation, which fully utilizes the\nadvantage that student's t-distribution can adjust the parameters (degrees of\nfreedom) and thus adjust the exploitation capability. We have fully\nexperimented on mainstream benchmarks CEC2013 and CEC2017, which proves that\nTFWA not only becomes the strongest variant of the fireworks algorithm, but\nalso achieves results comparable to SOTA on the test set, and its performance\nis far superior to that of the SOTA algorithm in some scenarios with a large\nnumber of extreme points.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.08484v1", "AI": {"title_translation": "基于学生t分布爆炸机制的高效烟花算法", "tldr": "本文提出了一种基于学生t分布的烟花算法（TFWA），通过调整参数来优化其在非凸问题上的性能，并在主流基准测试中取得了与SOTA算法媲美的结果。", "motivation": "许多非凸优化问题的现有算法表现不佳，需要更多的优化技术。当前的烟花算法（FWA）在凸问题上的表现不尽如人意，且其采样方法（爆炸）在处理高维情况时效率低下。", "method": "提出了一种新的基于学生t分布的烟花算法（TFWA）。该方法充分利用学生t分布能够调整参数（自由度）的优势，从而调整探索能力，并具有坚实的理论基础。", "result": "TFWA不仅成为烟花算法中最强的变体，而且在主流基准测试CEC2013和CEC2017上取得了与SOTA算法媲美的结果，在某些具有大量极值点的情况下，其性能远优于SOTA算法。", "conclusion": "所提出的TFWA显著提升了烟花算法的性能，是一种具有竞争力的优化技术，特别适用于复杂的非凸问题。", "translation": "许多现实世界的问题可以转化为优化问题，这些问题可以分为凸问题和非凸问题。尽管凸问题在理论上几乎得到了完全研究，但许多与非凸问题相关的算法效果不佳，我们需要更多的优化技术。作为一种群智能优化算法，烟花算法（FWA）已被广泛研究并应用于许多现实场景，甚至包括大型语言模型微调。但当前的烟花算法仍然存在一些问题。首先，作为一种启发式算法，其在凸问题上的性能无法与SOTA结果匹配，甚至可以说是不尽如人意；其次，大多数FWA变体的采样方法（爆炸）仍然是均匀采样，这在处理高维情况时实际上效率低下。我们的这项工作提出了一种新的基于学生t分布的FWA（TFWA），具有坚实的理论基础，它充分利用了学生t分布可以调整参数（自由度）从而调整探索能力的优势。我们已经在主流基准CEC2013和CEC2017上进行了充分实验，这证明TFWA不仅成为烟花算法中最强的变体，而且在测试集上取得了与SOTA媲美的结果，在某些具有大量极值点的情况下，其性能远优于SOTA算法。", "summary": "本文提出了一种基于学生t分布的烟花算法（TFWA），旨在解决现有烟花算法在凸问题上性能不佳以及在高维空间中采样效率低下的问题。TFWA利用学生t分布可调整参数的特性来增强探索能力。在CEC2013和CEC2017主流基准测试上的实验表明，TFWA不仅是烟花算法中最强的变体，而且达到了与最先进算法相当的性能，在某些场景下甚至表现更优。", "keywords": "烟花算法, 学生t分布, 优化, 群体智能, 元启发式算法", "comments": "本文的创新点在于将学生t分布引入到烟花算法的爆炸机制中，这为调整算法的探索能力提供了坚实的理论基础，并有效提高了算法在高维空间中的效率。这解决了启发式算法的一个已知局限性，并为解决复杂优化问题带来了新的希望。"}}
{"id": "2506.08196", "title": "No Stupid Questions: An Analysis of Question Query Generation for Citation Recommendation", "authors": ["Brian D. Zimmerman", "Julien Aubert-Béduchaud", "Florian Boudin", "Akiko Aizawa", "Olga Vechtomova"], "summary": "Existing techniques for citation recommendation are constrained by their\nadherence to article contents and metadata. We leverage GPT-4o-mini's latent\nexpertise as an inquisitive assistant by instructing it to ask questions which,\nwhen answered, could expose new insights about an excerpt from a scientific\narticle. We evaluate the utility of these questions as retrieval queries,\nmeasuring their effectiveness in retrieving and ranking masked target\ndocuments. In some cases, generated questions ended up being better queries\nthan extractive keyword queries generated by the same model. We additionally\npropose MMR-RBO, a variation of Maximal Marginal Relevance (MMR) using\nRank-Biased Overlap (RBO) to identify which questions will perform\ncompetitively with the keyword baseline. As all question queries yield unique\nresult sets, we contend that there are no stupid questions.", "comment": "6 pages, 5 figures, 2 tables", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08196v1", "AI": {"title_translation": "没有愚蠢的问题：引文推荐中问题查询生成分析", "tldr": "该研究利用GPT-4o-mini生成问题查询以改进引文推荐，发现这些问题有时比关键词查询更有效，并提出了MMR-RBO来识别高质量的问题查询。", "motivation": "现有的引文推荐技术受限于其对文章内容和元数据的依赖。", "method": "利用GPT-4o-mini作为探究性助手，生成关于科学文章摘录的问题。评估这些问题作为检索查询的效用，测量其在检索和排名被遮蔽的目标文档方面的有效性。提出MMR-RBO（一种结合排名偏差重叠RBO的最大边际相关性MMR变体）来识别与关键词基线具有竞争力的问句。", "result": "生成的问句有时比同一模型生成的抽取式关键词查询效果更好。MMR-RBO能够识别出与关键词基线表现出竞争力的问句。所有问题查询都产生独特的结果集。", "conclusion": "由于所有问题查询都产生独特的结果集，因此没有愚蠢的问题，它们在引文推荐中具有独特的效用。", "translation": "现有的引文推荐技术受限于其对文章内容和元数据的依赖。我们利用GPT-4o-mini作为探究性助手的潜在专长，指示它提出问题，当这些问题得到回答时，可以揭示科学文章摘录的新见解。我们评估这些问题作为检索查询的效用，衡量它们在检索和排名被遮蔽的目标文档方面的有效性。在某些情况下，生成的问题最终比同一模型生成的抽取式关键词查询效果更好。我们还提出了MMR-RBO，这是一种使用基于排名重叠（RBO）的最大边际相关性（MMR）的变体，用于识别哪些问题将与关键词基线表现出竞争力。由于所有问题查询都产生独特的结果集，我们认为没有愚蠢的问题。", "summary": "本文探索了一种利用GPT-4o-mini生成问题查询的新型引文推荐方法，旨在克服现有内容和元数据方法的局限性。研究评估了这些生成问题作为检索查询的有效性，发现它们有时优于传统的关键词查询。此外，论文引入了MMR-RBO来识别高性能的问题查询，并得出结论，基于问题的查询为引文推荐提供了独特且有价值的见解。", "keywords": "引文推荐, 问题生成, GPT-4o-mini, 检索查询, MMR-RBO", "comments": "该论文通过使用大型语言模型（GPT-4o-mini）生成疑问式查询进行引文推荐，超越了传统的关键词或基于内容的方法，具有创新性。这种方法有望通过揭示潜在联系显著增强相关引文的发现。MMR-RBO的引入也是评估和选择有效问题查询的重要贡献。"}}
{"id": "2506.08767", "title": "Telescoping Algorithms for $Σ^*$-Extensions via Complete Reductions", "authors": ["Shaoshi Chen", "Yiman Gao", "Hui Huang", "Carsten Schneider"], "summary": "A complete reduction on a difference field is a linear operator that enables\none to decompose an element of the field as the sum of a summable part and a\nremainder such that the given element is summable if and only if the remainder\nis equal to zero. In this paper, we present a complete reduction in a tower of\n$\\Sigma^*$-extensions that turns to a new efficient framework for the\nparameterized telescoping problem. Special instances of such\n$\\Sigma^*$-extensions cover iterative sums such as the harmonic numbers and\ngeneralized versions that arise, e.g., in combinatorics, computer science or\nparticle physics. Moreover, we illustrate how these new ideas can be used to\nreduce the depth of the given sum and provide structural theorems that connect\ncomplete reductions to Karr's Fundamental Theorem of symbolic summation.", "comment": null, "cate": "cs.SC", "url": "http://arxiv.org/abs/2506.08767v1", "AI": {"title_translation": "通过完全归约在$\\Sigma^*$-扩展中的伸缩算法", "tldr": "本文提出了一种在$\\\\Sigma^*$-扩展塔中的完全归约，为参数化伸缩问题提供了一个新的高效框架，并将其与Karr的符号求和基本定理联系起来。", "motivation": "旨在为参数化伸缩问题提供一个新的高效框架。", "method": "提出了一种在$\\\\Sigma^*$-扩展塔中的完全归约。", "result": "这种完全归约成为了参数化伸缩问题的一个新的高效框架；此外，它还可以用于减少给定和的深度，并提供了将完全归约与Karr符号求和基本定理联系起来的结构定理。", "conclusion": "通过在$\\\\Sigma^*$-扩展塔中引入完全归约，为参数化伸缩问题提供了一个高效的新框架，并建立了其与Karr符号求和基本定理的深层联系。", "translation": "差分域上的完全归约是一种线性算子，它能将域中的一个元素分解为可求和部分和余数的和，使得给定元素可求和当且仅当余数等于零。在本文中，我们提出了一种在$\\Sigma^*$-扩展塔中的完全归约，这为参数化伸缩问题提供了一个新的高效框架。这种$\\Sigma^*$-扩展的特殊实例涵盖了迭代和，例如调和数和广义版本，这些在组合学、计算机科学或粒子物理学中都有出现。此外，我们阐述了如何利用这些新思想来减少给定和的深度，并提供了将完全归约与Karr符号求和基本定理联系起来的结构定理。", "summary": "本文介绍了一种在$\\Sigma^*$-扩展塔中的完全归约，这是一种线性算子，能够将差分域元素分解为可求和部分和余数。此方法为参数化伸缩问题提供了一个高效的新框架，其应用涵盖了调和数等迭代和。研究还表明，该方法可用于减少求和深度，并建立了完全归约与Karr符号求和基本定理之间的结构联系。", "keywords": "完全归约, $\\Sigma^*$-扩展, 参数化伸缩, 符号求和, Karr定理", "comments": "这项工作通过引入在$\\Sigma^*$-扩展塔中的完全归约，为符号求和领域带来了创新。它不仅提供了一个解决参数化伸缩问题的高效新框架，还通过建立与Karr基本定理的联系，加深了对该领域理论基础的理解。其在组合学、计算机科学和粒子物理学中的潜在应用凸显了其实用价值。"}}
{"id": "2506.08251", "title": "On Finite Element Methods for Heterogeneous Elliptic Problems", "authors": ["Abimael F. D. Loula", "Maicon R. Correa", "João N. C. Guerreiro", "Elson M. Toledo"], "summary": "Dealing with variational formulations of second order elliptic problems with\ndiscontinuous coefficients, we recall a single field minimization problem of an\nextended functional presented by Bevilacqua et al (1974), which we associate\nwith the basic idea supporting discontinuous Galerkin finite element methods.\nWe review residual based stabilized mixed methods applied to Darcy flow in\nhomogeneous porous media and extend them to heterogeneous media with an\ninterface of discontinuity. For smooth interfaces, the proposed formulations\npreserve the continuity of the flux and exactly imposes the constraint between\nthe tangent components of Darcy velocity on the interface. Convergence studies\nfor a heterogeneous and anisotropic porous medium confirm the same rates of\nconvergence predicted for homogeneous problem with smooth solutions.", "comment": "arXiv admin note: text overlap with arXiv:2505.13924", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08251v1", "AI": {"title_translation": "异构椭圆问题有限元方法研究", "tldr": "本文将基于残差的稳定混合有限元方法扩展到具有不连续界面的异构介质达西流问题，提出的公式能保持通量连续性并精确施加切向速度约束，且收敛率与齐次问题相同。", "motivation": "处理具有不连续系数的二阶椭圆问题的变分公式，并将其应用于异构介质中的达西流问题。", "method": "回顾了Bevilacqua等人（1974）提出的扩展泛函的单场最小化问题，并将其与不连续伽辽金有限元方法的基本思想联系起来。回顾了应用于均质多孔介质达西流的基于残差的稳定混合方法，并将其扩展到具有不连续界面的异构介质。", "result": "对于光滑界面，提出的公式保持了通量的连续性，并精确施加了界面上达西速度切向分量之间的约束。对异构和各向异性多孔介质的收敛性研究证实了与齐次问题平滑解相同的收敛率。", "conclusion": "本文提出的有限元方法能够有效处理具有不连续界面的异构椭圆问题，在保持物理量连续性的同时，获得了与齐次问题相当的收敛性能。", "translation": "处理具有不连续系数的二阶椭圆问题的变分公式时，我们回顾了Bevilacqua等人（1974）提出的一个扩展泛函的单场最小化问题，我们将其与支持不连续伽辽金有限元方法的基本思想联系起来。我们回顾了应用于均质多孔介质达西流的基于残差的稳定混合方法，并将其扩展到具有不连续界面的异构介质。对于光滑界面，所提出的公式保持了通量的连续性，并精确施加了界面上达西速度切向分量之间的约束。对异构和各向异性多孔介质的收敛性研究证实了与齐次问题平滑解相同的收敛率。", "summary": "本文研究了具有不连续系数的二阶椭圆问题的变分公式，并回顾了与不连续伽辽金方法相关的单场最小化问题。在此基础上，将基于残差的稳定混合有限元方法从均质介质达西流扩展到具有不连续界面的异构介质。研究表明，对于光滑界面，所提出的公式能够保持通量连续性并精确施加切向速度约束，且在异构和各向异性多孔介质中的收敛率与均质问题相同。", "keywords": "有限元方法, 异构椭圆问题, 不连续系数, 达西流, 稳定混合方法", "comments": "本文的创新点在于将现有的稳定混合有限元方法成功地扩展到了更复杂的异构介质问题，特别是在处理界面不连续性方面。其方法能够精确保持物理量的连续性，这对于数值模拟的准确性至关重要。收敛性研究结果表明该方法在处理异构问题时仍能保持良好的性能，具有重要的理论和实际应用价值。"}}
{"id": "2506.08346", "title": "SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models", "authors": ["Wenhan Yao", "Fen Xiao", "Xiarun Chen", "Jia Liu", "YongQiang He", "Weiping Wen"], "summary": "Deep speech classification tasks, including keyword spotting and speaker\nverification, are vital in speech-based human-computer interaction. Recently,\nthe security of these technologies has been revealed to be susceptible to\nbackdoor attacks. Specifically, attackers use noisy disruption triggers and\nspeech element triggers to produce poisoned speech samples that train models to\nbecome vulnerable. However, these methods typically create only a limited\nnumber of backdoors due to the inherent constraints of the trigger function. In\nthis paper, we propose that speech backdoor attacks can strategically focus on\nspeech elements such as timbre and emotion, leveraging the Speech Large\nLanguage Model (SLLM) to generate diverse triggers. Increasing the number of\ntriggers may disproportionately elevate the poisoning rate, resulting in higher\nattack costs and a lower success rate per trigger. We introduce the Multiple\nGradient Descent Algorithm (MGDA) as a mitigation strategy to address this\nchallenge. The proposed attack is called the Speech Prompt Backdoor Attack\n(SPBA). Building on this foundation, we conducted attack experiments on two\nspeech classification tasks, demonstrating that SPBA shows significant trigger\neffectiveness and achieves exceptional performance in attack metrics.", "comment": "Accepted by IJCNN 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08346v1", "AI": {"title_translation": "SPBA：利用语音大语言模型对语音分类模型进行后门攻击", "tldr": "本文提出SPBA，利用语音大语言模型（SLLM）生成多样化触发器，并通过多梯度下降算法（MGDA）优化，成功对语音分类模型进行后门攻击，并取得了显著效果。", "motivation": "现有的深度语音分类任务后门攻击方法，由于触发器功能的固有限制，通常只能创建有限数量的后门，这导致攻击成本高且每个触发器的成功率较低。", "method": "本文提出语音提示后门攻击（SPBA），利用语音大语言模型（SLLM）生成多样化的语音元素触发器（如音色和情感）。为了应对触发器数量增加可能导致投毒率不成比例上升的问题，引入了多梯度下降算法（MGDA）作为缓解策略。", "result": "在两项语音分类任务上进行的攻击实验表明，SPBA展示了显著的触发器有效性，并在攻击指标上取得了卓越的性能。", "conclusion": "SPBA利用语音大语言模型生成多样化触发器并结合多梯度下降算法，能够有效且高性能地对深度语音分类模型实施后门攻击。", "translation": "深度语音分类任务，包括关键词识别和说话人验证，在基于语音的人机交互中至关重要。最近，这些技术的安全性被发现容易受到后门攻击。具体来说，攻击者利用嘈杂干扰触发器和语音元素触发器来产生中毒语音样本，从而训练模型变得脆弱。然而，这些方法通常由于触发器功能的固有限制，只能创建有限数量的后门。在本文中，我们提出语音后门攻击可以战略性地关注语音元素，如音色和情感，利用语音大语言模型（SLLM）生成多样化的触发器。增加触发器数量可能会不成比例地提高投毒率，导致更高的攻击成本和每个触发器的成功率降低。我们引入多梯度下降算法（MGDA）作为缓解策略来解决这一挑战。所提出的攻击被称为语音提示后门攻击（SPBA）。在此基础上，我们对两项语音分类任务进行了攻击实验，证明SPBA显示出显著的触发器有效性，并在攻击指标上取得了卓越的性能。", "summary": "本文提出了一种名为SPBA（语音提示后门攻击）的新型语音后门攻击方法。该方法利用语音大语言模型（SLLM）生成多样化的语音元素触发器，以克服现有方法触发器数量有限的局限性。为解决触发器数量增加可能导致投毒率不成比例上升的问题，引入了多梯度下降算法（MGDA）。实验结果表明，SPBA在语音分类任务中表现出显著的触发器有效性和卓越的攻击性能。", "keywords": "语音后门攻击, 语音大语言模型, 语音分类, 多梯度下降算法, SPBA", "comments": "这篇论文的创新点在于首次将语音大语言模型（SLLM）应用于生成多样化的后门攻击触发器，突破了传统方法在触发器生成上的限制。同时，引入多梯度下降算法（MGDA）来优化攻击过程，有效管理了触发器数量与攻击成本及成功率之间的平衡。这项研究揭示了语音分类模型在SLLM驱动下的新型后门攻击威胁，对语音AI的安全防护具有重要警示意义。"}}
{"id": "2506.08602", "title": "WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks", "authors": ["Tingzhi Li", "Xuefeng Liu"], "summary": "Graph Neural Networks (GNNs) are increasingly deployed in graph-related\napplications, making ownership verification critical to protect their\nintellectual property against model theft. Fingerprinting and black-box\nwatermarking are two main methods. However, the former relies on determining\nmodel similarity, which is computationally expensive and prone to ownership\ncollisions after model post-processing such as model pruning or fine-tuning.\nThe latter embeds backdoors, exposing watermarked models to the risk of\nbackdoor attacks. Moreover, both methods enable ownership verification but do\nnot convey additional information. As a result, each distributed model requires\na unique trigger graph, and all trigger graphs must be used to query the\nsuspect model during verification. Multiple queries increase the financial cost\nand the risk of detection.\n  To address these challenges, this paper proposes WGLE, a novel black-box\nwatermarking paradigm for GNNs that enables embedding the multi-bit string as\nthe ownership information without using backdoors. WGLE builds on a key insight\nwe term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the\ndifference between the feature distance and the prediction distance of two\nconnected nodes. By predefining positive or negative LDDE values for multiple\nselected edges, WGLE embeds the watermark encoding the intended information\nwithout introducing incorrect mappings that compromise the primary task. WGLE\nis evaluated on six public datasets and six mainstream GNN architectures along\nwith state-of-the-art methods. The results show that WGLE achieves 100%\nownership verification accuracy, an average fidelity degradation of 0.85%,\ncomparable robustness against potential attacks, and low embedding overhead.\nThe code is available in the repository.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08602v1", "AI": {"title_translation": "WGLE：针对图神经网络的无后门多比特黑盒水印技术", "tldr": "WGLE是一种新的针对GNN的黑盒水印范式，它无需后门即可嵌入多比特字符串作为所有权信息，并实现了高验证准确性、低性能下降和强鲁棒性。", "motivation": "现有的GNN所有权验证方法（指纹识别和黑盒水印）存在问题：指纹识别计算成本高，易受后处理影响；黑盒水印嵌入后门，有攻击风险；两者都无法传递额外信息，导致验证时需要多个触发图，增加成本和检测风险。", "method": "本文提出了WGLE，一种新颖的针对GNN的黑盒水印范式，旨在解决现有水印技术的挑战。其核心思想是“边上的层级距离差异”（LDDE），它量化了两个连接节点的特征距离和预测距离之间的差异。通过为多个选定边预定义正或负的LDDE值，WGLE在不引入损害主要任务的错误映射的情况下嵌入编码了预期信息的水印。", "result": "WGLE在六个公共数据集和六个主流GNN架构上进行了评估，结果显示：实现了100%的所有权验证准确率，平均0.85%的保真度下降，与潜在攻击相当的鲁棒性，以及低嵌入开销。", "conclusion": "WGLE成功地解决了现有GNN水印技术的挑战，提供了一种安全、高效且能够嵌入多比特信息的所有权验证方法，无需引入后门。", "translation": "图神经网络（GNNs）越来越多地部署在图相关应用中，因此所有权验证对于保护其知识产权免受模型盗窃至关重要。指纹识别和黑盒水印是两种主要方法。然而，前者依赖于确定模型相似性，这计算成本高昂，并且在模型后处理（如模型剪枝或微调）后容易发生所有权冲突。后者嵌入后门，使带有水印的模型面临后门攻击的风险。此外，这两种方法都只实现所有权验证，但不能传达额外信息。因此，每个分布式模型都需要一个唯一的触发图，并且在验证期间必须使用所有触发图来查询可疑模型。多次查询增加了财务成本和被检测的风险。\n为了解决这些挑战，本文提出了WGLE，一种新颖的针对GNN的黑盒水印范式，它无需使用后门即可嵌入多比特字符串作为所有权信息。WGLE建立在一个关键的洞察力之上，我们称之为“边上的层级距离差异”（LDDE），它量化了两个连接节点的特征距离和预测距离之间的差异。通过为多个选定边预定义正或负的LDDE值，WGLE在不引入损害主要任务的错误映射的情况下嵌入编码了预期信息的水印。WGLE在六个公共数据集和六个主流GNN架构以及最先进的方法上进行了评估。结果表明，WGLE实现了100%的所有权验证准确率，平均0.85%的保真度下降，与潜在攻击相当的鲁棒性，以及低嵌入开销。代码已在存储库中提供。", "summary": "本文提出了WGLE，一种创新的针对图神经网络（GNNs）的无后门多比特黑盒水印技术，旨在解决现有方法中计算成本高、后门风险和信息传递不足的问题。WGLE利用“边上的层级距离差异”（LDDE）来嵌入水印，实现了100%的所有权验证准确率、极低的性能下降和强大的抗攻击鲁棒性，为GNN模型的所有权保护提供了一种高效且安全的新范式。", "keywords": "图神经网络, 水印, 黑盒水印, 所有权验证, 无后门, 多比特", "comments": "WGLE的创新之处在于其无后门的多比特水印嵌入能力，通过LDDE机制巧妙地将所有权信息编码到GNN中，同时保持了模型性能和鲁棒性。这解决了现有GNN水印技术的主要痛点，对于保护GNN的知识产权具有重要意义。"}}
{"id": "2506.08561", "title": "Detecting State Manipulation Vulnerabilities in Smart Contracts Using LLM and Static Analysis", "authors": ["Hao Wu", "Haijun Wang", "Shangwang Li", "Yin Wu", "Ming Fan", "Yitao Zhao", "Ting Liu"], "summary": "An increasing number of DeFi protocols are gaining popularity, facilitating\ntransactions among multiple anonymous users. State Manipulation is one of the\nnotorious attacks in DeFi smart contracts, with price variable being the most\ncommonly exploited state variable-attackers manipulate token prices to gain\nillicit profits. In this paper, we propose PriceSleuth, a novel method that\nleverages the Large Language Model (LLM) and static analysis to detect Price\nManipulation (PM) attacks proactively. PriceSleuth firstly identifies core\nlogic function related to price calculation in DeFi contracts. Then it guides\nLLM to locate the price calculation code statements. Secondly, PriceSleuth\nperforms backward dependency analysis of price variables, instructing LLM in\ndetecting potential price manipulation. Finally, PriceSleuth utilizes\npropagation analysis of price variables to assist LLM in detecting whether\nthese variables are maliciously exploited. We presented preliminary\nexperimental results to substantiate the effectiveness of PriceSleuth . And we\noutline future research directions for PriceSleuth.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08561v1", "AI": {"title_translation": "使用LLM和静态分析检测智能合约中的状态操纵漏洞", "tldr": "PriceSleuth结合LLM和静态分析，主动检测DeFi智能合约中的价格操纵攻击。", "motivation": "DeFi协议中状态操纵（特别是价格操纵）是一种臭名昭著的攻击，攻击者通过操纵代币价格获取非法利润。", "method": "PriceSleuth首先识别DeFi合约中与价格计算相关的核心逻辑函数，并指导LLM定位价格计算代码语句。其次，它对价格变量执行反向依赖分析，指导LLM检测潜在的价格操纵。最后，它利用价格变量的传播分析，协助LLM检测这些变量是否被恶意利用。", "result": "初步实验结果证实了PriceSleuth的有效性。", "conclusion": "PriceSleuth被证实可以有效检测DeFi智能合约中的价格操纵漏洞。", "translation": "越来越多的DeFi协议受到欢迎，它们促进了多个匿名用户之间的交易。状态操纵是DeFi智能合约中臭名昭著的攻击之一，其中价格变量是最常被利用的状态变量——攻击者操纵代币价格以获取非法利润。在本文中，我们提出了PriceSleuth，这是一种利用大型语言模型（LLM）和静态分析来主动检测价格操纵（PM）攻击的新方法。PriceSleuth首先识别DeFi合约中与价格计算相关的核心逻辑函数。然后，它指导LLM定位价格计算代码语句。其次，PriceSleuth对价格变量执行反向依赖分析，指导LLM检测潜在的价格操纵。最后，PriceSleuth利用价格变量的传播分析，协助LLM检测这些变量是否被恶意利用。我们展示了初步的实验结果，以证实PriceSleuth的有效性。并且我们概述了PriceSleuth未来的研究方向。", "summary": "PriceSleuth是一种结合大型语言模型（LLM）和静态分析的新方法，旨在主动检测DeFi智能合约中的价格操纵漏洞。该方法通过识别价格计算函数、进行价格变量的反向依赖分析以及传播分析，指导LLM定位并检测潜在的恶意价格变量利用。初步实验结果证实了其有效性。", "keywords": "智能合约, 状态操纵, 价格操纵, 大型语言模型, 静态分析", "comments": "该论文创新性地结合了LLM和传统静态分析方法来解决DeFi智能合约中的状态操纵漏洞，特别是价格操纵问题。这种混合方法有望提高漏洞检测的准确性和效率，特别是在处理复杂代码逻辑时。其重要性在于能够帮助保护DeFi协议免受财务损失。"}}
{"id": "2506.08344", "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "authors": ["Neşet Ünver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Taşkın Padır"], "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism.", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08344v1", "AI": {"title_translation": "Re4MPC：基于深度强化学习的多模型运动规划的反应式非线性MPC", "tldr": "Re4MPC提出了一种新的多模型运动规划方法，通过深度强化学习学习反应式决策，以提高计算效率和成功率。", "motivation": "传统机器人运动规划方法对于多自由度机器人（如移动机械臂）在实际应用中计算成本过高。", "method": "本文提出了一种名为Re4MPC的多模型运动规划流程。Re4MPC通过根据任务复杂性和机器人状态，反应式地选择NMPC问题的模型、成本和约束来生成轨迹。这种反应式决策的策略是通过深度强化学习（DRL）框架学习的。文中引入了一个数学公式来将NMPC整合到DRL框架中。", "result": "实验结果表明，Re4MPC比没有学习机制的NMPC基线在计算上更高效，并且在达到末端执行器目标方面取得了更高的成功率。", "conclusion": "Re4MPC通过结合反应式多模型非线性模型预测控制和深度强化学习，显著提高了多自由度机器人运动规划的计算效率和任务成功率。", "translation": "Re4MPC：基于深度强化学习的多模型运动规划的反应式非线性MPC\n\n传统的多自由度机器人（例如移动机械臂）运动规划方法在实际应用中往往计算成本过高。本文提出了一种新颖的多模型运动规划流程，命名为Re4MPC，它使用非线性模型预测控制（NMPC）来计算轨迹。Re4MPC通过根据任务的复杂性和机器人的状态，反应式地选择NMPC问题的模型、成本和约束，从而以计算高效的方式生成轨迹。这种反应式决策的策略是通过深度强化学习（DRL）框架学习的。我们引入了一个数学公式来将NMPC整合到这个DRL框架中。为了验证我们的方法和设计选择，我们在涉及移动机械臂的基于物理的模拟中评估了DRL训练和测试结果。实验结果表明，与没有我们学习机制的NMPC基线相比，Re4MPC在计算上更高效，并且在达到末端执行器目标方面取得了更高的成功率。", "summary": "Re4MPC提出了一种针对多自由度机器人的新型多模型运动规划方法，其核心在于结合非线性模型预测控制（NMPC）与深度强化学习（DRL）。该方法通过DRL学习一种反应式策略，根据任务和机器人状态动态选择NMPC的模型、成本和约束，从而高效地生成轨迹。实验验证了Re4MPC在计算效率和目标达成成功率上均优于传统NMPC基线。", "keywords": "运动规划, 非线性模型预测控制, 深度强化学习, 多模型, 机器人", "comments": "本文提出了一种将深度强化学习与非线性模型预测控制相结合的创新方法，解决了多自由度机器人运动规划的计算效率问题。其核心思想在于通过学习实现对NMPC参数的反应式选择，从而在保证性能的同时显著降低计算负担。这对于实时机器人控制具有重要意义。"}}
{"id": "2506.08189", "title": "Open World Scene Graph Generation using Vision Language Models", "authors": ["Amartya Dutta", "Kazi Sajeed Mehrab", "Medha Sawhney", "Abhilash Neog", "Mridul Khurana", "Sepideh Fatemi", "Aanish Pradhan", "M. Maruf", "Ismini Lourentzou", "Arka Daw", "Anuj Karpatne"], "summary": "Scene-Graph Generation (SGG) seeks to recognize objects in an image and\ndistill their salient pairwise relationships. Most methods depend on\ndataset-specific supervision to learn the variety of interactions, restricting\ntheir usefulness in open-world settings, involving novel objects and/or\nrelations. Even methods that leverage large Vision Language Models (VLMs)\ntypically require benchmark-specific fine-tuning. We introduce Open-World SGG,\na training-free, efficient, model-agnostic framework that taps directly into\nthe pretrained knowledge of VLMs to produce scene graphs with zero additional\nlearning. Casting SGG as a zero-shot structured-reasoning problem, our method\ncombines multimodal prompting, embedding alignment, and a lightweight\npair-refinement strategy, enabling inference over unseen object vocabularies\nand relation sets. To assess this setting, we formalize an Open-World\nevaluation protocol that measures performance when no SGG-specific data have\nbeen observed either in terms of objects and relations. Experiments on Visual\nGenome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate\nthe capacity of pretrained VLMs to perform relational understanding without\ntask-level training.", "comment": "Accepted in CVPR 2025 Workshop (CVinW)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08189v1", "AI": {"title_translation": "使用视觉语言模型进行开放世界场景图生成", "tldr": "提出了一种无需训练的开放世界场景图生成框架，利用预训练的视觉语言模型实现零样本关系理解。", "motivation": "现有场景图生成方法依赖数据集特定监督，限制了其在涉及新对象和/或关系（开放世界设置）中的有效性，即使是利用大型视觉语言模型的方法也通常需要基准特定的微调。", "method": "引入了Open-World SGG，一个无需训练、高效、模型无关的框架，它直接利用VLMs的预训练知识来生成场景图，无需额外学习。该方法将SGG视为零样本结构化推理问题，结合了多模态提示、嵌入对齐和轻量级对偶细化策略。", "result": "在Visual Genome、Open Images V6和Panoptic Scene Graph (PSG) 数据集上的实验表明，预训练的VLMs无需任务级训练即可执行关系理解。", "conclusion": "预训练的视觉语言模型有能力在不进行特定任务训练的情况下执行关系理解，这对于开放世界场景图生成至关重要。", "translation": "场景图生成（SGG）旨在识别图像中的对象并提取其显著的成对关系。大多数方法依赖于数据集特定的监督来学习各种交互，这限制了它们在涉及新对象和/或关系（开放世界设置）中的实用性。即使是利用大型视觉语言模型（VLMs）的方法通常也需要基准特定的微调。我们引入了开放世界SGG，这是一个无需训练、高效、模型无关的框架，它直接利用VLMs的预训练知识来生成场景图，无需额外学习。将SGG视为一个零样本结构化推理问题，我们的方法结合了多模态提示、嵌入对齐和轻量级对偶细化策略，从而能够在未见过的对象词汇和关系集上进行推理。为了评估这种设置，我们制定了一个开放世界评估协议，用于衡量在未观察到任何SGG特定数据（无论是对象还是关系）时的性能。在Visual Genome、Open Images V6和Panoptic Scene Graph（PSG）数据集上的实验表明，预训练的VLMs无需任务级训练即可执行关系理解。", "summary": "本文提出了一个名为Open-World SGG的创新框架，旨在解决传统场景图生成方法在开放世界场景中表现受限的问题。该框架无需训练，通过直接利用预训练视觉语言模型的知识，结合多模态提示、嵌入对齐和轻量级对偶细化策略，实现了零样本的场景图生成能力，即使面对未见过的对象和关系也能有效推理。实验证明了预训练VLMs在无需特定任务训练的情况下进行关系理解的强大潜力。", "keywords": "场景图生成, 视觉语言模型, 开放世界, 零样本学习, 关系理解", "comments": "该论文的创新之处在于提出了一个完全无需训练的开放世界场景图生成框架，这显著降低了对特定数据集监督的依赖。通过直接利用预训练视觉语言模型的能力，它为处理新颖对象和关系提供了一种通用且高效的解决方案，有望推动场景图生成在更广泛实际应用中的普及。"}}
{"id": "2506.08944", "title": "Semantic Communication for Cooperative Multi-Tasking over Rate-Limited Wireless Channels with Implicit Optimal Prior", "authors": ["Ahmad Halimi Razlighi", "Carsten Bockelmann", "Armin Dekorsy"], "summary": "In this work, we expand the cooperative multi-task semantic communication\nframework (CMT-SemCom) introduced in [1], which divides the semantic encoder on\nthe transmitter side into a common unit (CU) and multiple specific units (SUs),\nto a more applicable design. Our proposed system model addresses real-world\nconstraints by introducing a general design that operates over rate-limited\nwireless channels. Further, we aim to tackle the rate-limit constraint,\nrepresented through the Kullback-Leibler (KL) divergence, by employing the\ndensity ratio trick alongside the implicit optimal prior method (IoPm). By\napplying the IoPm to our multi-task processing framework, we propose a hybrid\nlearning approach that combines deep neural networks with kernelized-parametric\nmachine learning methods, enabling a robust solution for the CMT-SemCom. Our\nframework is grounded in information-theoretic principles and employs\nvariational approximations to bridge theoretical foundations with practical\nimplementations. Simulation results demonstrate the proposed system's\neffectiveness in rate-constrained multi-task SemCom scenarios, highlighting its\npotential for enabling intelligence in next-generation wireless networks.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.08944v1", "AI": {"title_translation": "具有隐式最优先验的速率受限无线信道上协作多任务的语义通信", "tldr": "本文提出了一种新的CMT-SemCom系统模型，通过结合密度比技巧和隐式最优先验方法，解决了速率受限无线信道上的多任务语义通信问题，并采用混合学习方法提高了鲁棒性。", "motivation": "扩展现有的协作多任务语义通信（CMT-SemCom）框架，以解决真实世界中速率受限无线信道上的多任务语义通信问题，并克服速率限制。", "method": "提出了一种新的系统模型，通过引入密度比技巧和隐式最优先验方法（IoPm）来处理速率限制（由Kullback-Leibler散度表示）。该方法将IoPm应用于多任务处理框架，并采用结合深度神经网络和核化参数机器学习方法的混合学习方法。", "result": "仿真结果表明，所提出的系统在速率受限的多任务语义通信场景中是有效的。", "conclusion": "所提出的系统模型和混合学习方法能够有效解决速率受限无线信道上的协作多任务语义通信问题，并有望为下一代无线网络赋能智能。", "translation": "在这项工作中，我们将[1]中引入的协作多任务语义通信框架（CMT-SemCom）扩展为一种更适用的设计，该框架将发送端的语义编码器分为一个通用单元（CU）和多个特定单元（SUs）。我们提出的系统模型通过引入一种在速率受限无线信道上运行的通用设计来解决实际约束。此外，我们旨在通过采用密度比技巧结合隐式最优先验方法（IoPm）来解决由Kullback-Leibler（KL）散度表示的速率限制约束。通过将IoPm应用于我们的多任务处理框架，我们提出了一种结合深度神经网络和核化参数机器学习方法的混合学习方法，为CMT-SemCom提供了一个鲁棒的解决方案。我们的框架基于信息论原理，并采用变分近似来连接理论基础与实际实现。仿真结果表明，所提出的系统在速率受限的多任务语义通信场景中是有效的，突出了其在下一代无线网络中实现智能的潜力。", "summary": "本文扩展了协作多任务语义通信（CMT-SemCom）框架，提出了一种新的系统模型，用于在速率受限的无线信道上进行多任务处理。通过结合密度比技巧和隐式最优先验方法，该系统有效应对了速率限制，并采用深度神经网络与核化参数机器学习相结合的混合学习方法，增强了解决方案的鲁棒性。仿真结果验证了其在速率受限场景下的有效性，展现了其在未来无线网络中的应用潜力。", "keywords": "语义通信, 协作多任务, 速率受限信道, 隐式最优先验, 混合学习", "comments": "这项工作通过引入隐式最优先验方法和混合学习策略，有效解决了速率受限无线信道上的语义通信挑战，是现有CMT-SemCom框架的重要扩展。其基于信息论原理并结合深度学习和传统机器学习的方法，为实现下一代无线网络中的智能通信提供了新的思路。"}}
{"id": "2506.08321", "title": "LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs", "authors": ["Manooshree Patel", "Rayna Bhattacharyya", "Thomas Lu", "Arnav Mehta", "Niels Voss", "Narges Norouzi", "Gireeja Ranade"], "summary": "We present LeanTutor, a Large Language Model (LLM)-based tutoring system for\nmath proofs. LeanTutor interacts with the student in natural language, formally\nverifies student-written math proofs in Lean, generates correct next steps, and\nprovides the appropriate instructional guidance. LeanTutor is composed of three\nmodules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and\n(iii) a natural language feedback generator. The first module faithfully\nautoformalizes student proofs into Lean and verifies proof accuracy via\nsuccessful code compilation. If the proof has an error, the incorrect step is\nidentified. The next-step generator module outputs a valid next Lean tactic for\nincorrect proofs via LLM-based candidate generation and proof search. The\nfeedback generator module leverages Lean data to produce a\npedagogically-motivated natural language hint for the student user. To evaluate\nour system, we introduce PeanoBench, a human-written dataset derived from the\nNatural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each\nnatural language proof step is paired with the corresponding logically\nequivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of\ntactics in correct proofs and accurately identifies the incorrect step in 30%\nof incorrect proofs. In generating natural language hints for erroneous proofs,\nLeanTutor outperforms a simple baseline on accuracy and relevance metrics.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08321v1", "AI": {"title_translation": "LeanTutor：一个用于数学证明的经过形式验证的AI导师", "tldr": "LeanTutor是一个基于LLM的数学证明辅导系统，它能形式化验证学生证明，生成下一步骤，并提供自然语言反馈。", "motivation": "旨在创建一个能与学生自然语言交互、形式化验证数学证明、生成正确下一步骤并提供教学指导的AI辅导系统。", "method": "LeanTutor包含三个模块：自动形式化/证明检查器、下一步生成器和自然语言反馈生成器。第一个模块将学生证明形式化为Lean代码并验证准确性；第二个模块通过LLM生成并搜索有效Lean策略；第三个模块利用Lean数据生成教学提示。通过引入PeanoBench数据集进行评估。", "result": "自动形式化器在正确证明中正确形式化了57%的策略，在不正确证明中准确识别了30%的错误步骤。LeanTutor在生成错误证明的自然语言提示方面优于简单基线。", "conclusion": "LeanTutor系统在数学证明辅导方面展现了潜力，尤其是在形式化验证、下一步生成和提供教学反馈方面取得了一定成效。", "translation": "我们提出了LeanTutor，一个基于大型语言模型（LLM）的数学证明辅导系统。LeanTutor以自然语言与学生交互，在Lean中形式化验证学生编写的数学证明，生成正确的下一步，并提供适当的教学指导。LeanTutor由三个模块组成：(i) 自动形式化器/证明检查器，(ii) 下一步生成器，以及(iii) 自然语言反馈生成器。第一个模块将学生证明忠实地自动形式化为Lean代码，并通过成功的代码编译验证证明的准确性。如果证明有错误，则会识别出错误的步骤。下一步生成器模块通过基于LLM的候选生成和证明搜索，为不正确的证明输出有效的下一个Lean策略。反馈生成器模块利用Lean数据为学生用户生成具有教学动机的自然语言提示。为了评估我们的系统，我们引入了PeanoBench，这是一个源自自然数游戏的、由人类编写的数据集，包含371个皮亚诺算术证明，其中每个自然语言证明步骤都与Lean中对应的逻辑等价策略配对。自动形式化器在正确证明中正确形式化了57%的策略，并准确识别了30%不正确证明中的错误步骤。在为错误证明生成自然语言提示方面，LeanTutor在准确性和相关性指标上优于一个简单的基线。", "summary": "LeanTutor是一个基于LLM的AI数学证明辅导系统。它通过自动形式化、证明检查、下一步骤生成和自然语言反馈模块，帮助学生进行数学证明学习。系统能将学生证明形式化为Lean代码进行验证，识别错误，并提供基于LLM的正确下一步骤和教学提示。通过在PeanoBench数据集上的评估，LeanTutor在部分功能上表现出有效性，并在生成反馈方面优于基线。", "keywords": "数学证明, AI辅导, 大型语言模型, 形式验证, Lean", "comments": "LeanTutor的创新之处在于将LLM与形式验证相结合，为数学证明学习提供交互式辅导。其模块化设计清晰，涵盖了从证明检查到反馈生成的核心辅导功能。虽然在自动形式化和错误识别方面仍有提升空间（如57%和30%的准确率），但其在生成自然语言提示方面的表现以及引入PeanoBench数据集进行评估，都为未来研究提供了有价值的参考。"}}
{"id": "2506.08051", "title": "ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity", "authors": ["Mahmuda Sultana Mimi", "Md Monzurul Islam", "Anannya Ghosh Tusti", "Shriyank Somvanshi", "Subasish Das"], "summary": "Understanding the spatial and temporal dynamics of automated vehicle (AV)\ncrash severity is critical for advancing urban mobility safety and\ninfrastructure planning. In this work, we introduce ST-GraphNet, a\nspatio-temporal graph neural network framework designed to model and predict AV\ncrash severity by using both fine-grained and region-aggregated spatial graphs.\nUsing a balanced dataset of 2,352 real-world AV-related crash reports from\nTexas (2024), including geospatial coordinates, crash timestamps, SAE\nautomation levels, and narrative descriptions, we construct two complementary\ngraph representations: (1) a fine-grained graph with individual crash events as\nnodes, where edges are defined via spatio-temporal proximity; and (2) a\ncoarse-grained graph where crashes are aggregated into Hexagonal Hierarchical\nSpatial Indexing (H3)-based spatial cells, connected through hexagonal\nadjacency. Each node in the graph is enriched with multimodal data, including\nsemantic, spatial, and temporal attributes, including textual embeddings from\ncrash narratives using a pretrained Sentence-BERT model. We evaluate various\ngraph neural network (GNN) architectures, such as Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN\n(DSTGCN), to classify crash severity and predict high-risk regions. Our\nproposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3\ngraph, achieves a test accuracy of 97.74\\%, substantially outperforming the\nbest fine-grained model (64.7\\% test accuracy). These findings highlight the\neffectiveness of spatial aggregation, dynamic message passing, and multi-modal\nfeature integration in capturing the complex spatio-temporal patterns\nunderlying AV crash severity.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08051v1", "AI": {"title_translation": "ST-GraphNet：一种用于理解和预测自动驾驶汽车碰撞严重程度的时空图神经网络", "tldr": "ST-GraphNet是一种时空图神经网络，通过结合细粒度和区域聚合的空间图来建模和预测自动驾驶汽车碰撞严重程度，在粗粒度H3图上取得了97.74%的测试准确率。", "motivation": "理解自动驾驶汽车（AV）碰撞严重程度的时空动态对于提升城市交通安全和基础设施规划至关重要。", "method": "本研究引入了ST-GraphNet，一个时空图神经网络框架，通过使用细粒度和区域聚合的空间图来建模和预测AV碰撞严重程度。研究人员使用包含2,352份德克萨斯州（2024年）真实AV相关碰撞报告的平衡数据集，构建了两种互补的图表示：1) 以单个碰撞事件为节点的细粒度图，边通过时空邻近性定义；2) 碰撞聚合到基于H3（Hexagonal Hierarchical Spatial Indexing）的空间单元的粗粒度图，通过六边形邻接连接。每个节点都通过多模态数据丰富，包括语义、空间和时间属性，以及使用预训练Sentence-BERT模型从碰撞叙述中提取的文本嵌入。研究评估了多种GNN架构，如GCN、GAT和DSTGCN，用于分类碰撞严重程度和预测高风险区域。", "result": "所提出的ST-GraphNet在粗粒度H3图上使用DSTGCN骨干，实现了97.74%的测试准确率，显著优于最佳细粒度模型（64.7%的测试准确率）。", "conclusion": "这些发现强调了空间聚合、动态消息传递和多模态特征集成在捕捉AV碰撞严重程度背后复杂时空模式方面的有效性。", "translation": "理解自动驾驶汽车（AV）碰撞严重程度的时空动态对于提升城市交通安全和基础设施规划至关重要。在这项工作中，我们引入了ST-GraphNet，一个时空图神经网络框架，旨在通过使用细粒度和区域聚合的空间图来建模和预测AV碰撞严重程度。我们使用一个包含2,352份来自德克萨斯州（2024年）的真实AV相关碰撞报告的平衡数据集，其中包括地理空间坐标、碰撞时间戳、SAE自动化级别和叙述性描述，构建了两种互补的图表示：（1）以单个碰撞事件为节点的细粒度图，其中边通过时空邻近性定义；（2）将碰撞聚合到基于六边形分层空间索引（H3）的空间单元的粗粒度图，通过六边形邻接连接。图中的每个节点都通过多模态数据丰富，包括语义、空间和时间属性，以及使用预训练的Sentence-BERT模型从碰撞叙述中提取的文本嵌入。我们评估了各种图神经网络（GNN）架构，例如图卷积网络（GCN）、图注意力网络（GAT）和动态时空GCN（DSTGCN），以分类碰撞严重程度并预测高风险区域。我们提出的ST-GraphNet在粗粒度H3图上利用DSTGCN骨干，实现了97.74%的测试准确率，大大优于最佳细粒度模型（64.7%的测试准确率）。这些发现突出了空间聚合、动态消息传递和多模态特征集成在捕捉AV碰撞严重程度背后复杂时空模式方面的有效性。", "summary": "本研究提出了ST-GraphNet，一个时空图神经网络框架，用于理解和预测自动驾驶汽车（AV）碰撞的严重程度。该模型利用了包含地理空间、时间、SAE自动化级别和文本描述的真实世界AV碰撞数据，构建了细粒度（基于个体事件）和粗粒度（基于H3空间聚合单元）两种图表示。ST-GraphNet在粗粒度H3图上采用DSTGCN骨干，并整合了多模态节点特征。实验结果显示，ST-GraphNet在粗粒度图上达到了97.74%的测试准确率，显著优于细粒度模型，证明了空间聚合、动态消息传递和多模态特征集成在捕捉复杂时空碰撞模式上的有效性。", "keywords": "时空图神经网络, 自动驾驶汽车, 碰撞严重程度, 空间聚合, 多模态数据", "comments": "ST-GraphNet的创新之处在于其双重图表示（细粒度和粗粒度）以及对多模态特征（包括文本嵌入）的有效集成。尤其值得注意的是，粗粒度H3图在预测AV碰撞严重程度方面表现出卓越的性能，这表明在某些时空预测任务中，适当的空间聚合能够显著提升模型效果和泛化能力。这项工作对于推动城市交通安全和智能交通系统（ITS）的发展具有重要意义。"}}
{"id": "2506.08749", "title": "Superposed Parameterised Quantum Circuits", "authors": ["Viktoria Patapovich", "Mo Kordzanganeh", "Alexey Melnikov"], "summary": "Quantum machine learning has shown promise for high-dimensional data\nanalysis, yet many existing approaches rely on linear unitary operations and\nshared trainable parameters across outputs. These constraints limit\nexpressivity and scalability relative to the multi-layered, non-linear\narchitectures of classical deep networks. We introduce superposed parameterised\nquantum circuits to overcome these limitations. By combining flip-flop quantum\nrandom-access memory with repeat-until-success protocols, a superposed\nparameterised quantum circuit embeds an exponential number of parameterised\nsub-models in a single circuit and induces polynomial activation functions\nthrough amplitude transformations and post-selection. We provide an analytic\ndescription of the architecture, showing how multiple parameter sets are\ntrained in parallel while non-linear amplitude transformations broaden\nrepresentational power beyond conventional quantum kernels. Numerical\nexperiments underscore these advantages: on a 1D step-function regression a\ntwo-qubit superposed parameterised quantum circuit cuts the mean-squared error\nby three orders of magnitude versus a parameter-matched variational baseline;\non a 2D star-shaped two-dimensional classification task, introducing a\nquadratic activation lifts accuracy to 81.4% and reduces run-to-run variance\nthree-fold. These results position superposed parameterised quantum circuits as\na hardware-efficient route toward deeper, more versatile parameterised quantum\ncircuits capable of learning complex decision boundaries.", "comment": "20 pages, 6 figures, 3 tables", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.08749v1", "AI": {"title_translation": "叠加参数化量子电路", "tldr": "本文引入了叠加参数化量子电路，通过结合翻转跳变量子随机存取存储器和重复直到成功协议，在一个单一电路中嵌入指数数量的参数化子模型，并通过振幅变换和后选择引入多项式激活函数，从而克服了现有量子机器学习在表达能力和可扩展性方面的限制。数值实验表明，该方法在回归和分类任务上显著优于基线。", "motivation": "现有的量子机器学习方法依赖于线性酉运算和跨输出共享的可训练参数，这限制了其表达能力和相对于经典深度网络的非线性多层架构的可扩展性。", "method": "本文引入了叠加参数化量子电路（superposed parameterised quantum circuits）。该方法通过结合翻转跳变量子随机存取存储器（flip-flop quantum random-access memory）和重复直到成功协议（repeat-until-success protocols），在一个单一电路中嵌入指数数量的参数化子模型。通过振幅变换和后选择，该电路能够诱导多项式激活函数。文章提供了该架构的分析描述，展示了如何并行训练多个参数集，并且非线性振幅变换拓宽了表示能力。", "result": "在1D阶梯函数回归任务中，一个双量子比特的叠加参数化量子电路将均方误差比参数匹配的变分基线降低了三个数量级。在2D星形二维分类任务中，引入二次激活函数将准确率提高到81.4%，并将运行间方差降低了三倍。", "conclusion": "叠加参数化量子电路为实现更深、更通用的参数化量子电路提供了一条硬件高效的途径，使其能够学习复杂的决策边界。", "translation": "量子机器学习在高维数据分析方面显示出前景，然而许多现有方法依赖于线性酉运算和跨输出共享的可训练参数。这些约束限制了其相对于经典深度网络的多层、非线性架构的表达能力和可扩展性。我们引入了叠加参数化量子电路来克服这些限制。通过结合翻转跳变量子随机存取存储器和重复直到成功协议，一个叠加参数化量子电路在一个单一电路中嵌入了指数数量的参数化子模型，并通过振幅变换和后选择诱导多项式激活函数。我们提供了该架构的分析描述，展示了如何并行训练多个参数集，同时非线性振幅变换拓宽了超越传统量子核的表示能力。数值实验强调了这些优势：在一个1D阶梯函数回归任务中，一个双量子比特的叠加参数化量子电路将均方误差比参数匹配的变分基线降低了三个数量级；在一个2D星形二维分类任务中，引入二次激活函数将准确率提高到81.4%，并将运行间方差降低了三倍。这些结果将叠加参数化量子电路定位为实现更深、更通用、能够学习复杂决策边界的参数化量子电路的硬件高效途径。", "summary": "本文提出了一种名为“叠加参数化量子电路”的新型量子机器学习架构，旨在解决现有量子模型在表达能力和可扩展性上的局限性。该电路通过结合翻转跳变量子随机存取存储器和重复直到成功协议，在一个单一电路中嵌入指数数量的参数化子模型，并引入多项式激活函数以增强非线性表示能力。数值实验证明，与传统方法相比，该电路在回归和分类任务中显著提升了性能，并降低了误差和方差，为构建更深、更强大的量子机器学习模型提供了硬件高效的途径。", "keywords": "量子机器学习, 参数化量子电路, 激活函数, 量子随机存取存储器, 深度学习", "comments": "本文提出了一种创新的量子电路架构，通过在单一电路中嵌入指数级子模型并引入非线性激活函数，显著提升了量子机器学习的表达能力和可扩展性，有望推动量子深度学习的发展。其硬件高效的特性也使其在实际应用中具有重要潜力。"}}
{"id": "2506.08549", "title": "Exploring the Convergence of HCI and Evolving Technologies in Information Systems", "authors": ["Rajan Das Gupta", "Ashikur Rahman", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "summary": "Modern technology driven information systems are part of our daily lives.\nHowever, this deep integration poses new challenges to the human computer\ninteraction (HCI) professionals. With the rapid growth of mobile and cloud\ncomputing and the Internet of Things (IoT), the demand for HCI specialists to\ndesign user-friendly and adaptable interfaces has never been more pressing.\nEspecially for diverse user groups such as children, the elderly and people\nwith disabilities who need interfaces tailored to their needs regardless of\ntime and location. This study reviewed 50 recent papers on HCI interface design\nfor modern information systems. The goal is to see how well these methods\naddress the demands of current technology. The findings show that most HCI\ndesign methods are still based on old desktop models and do not support mobile\nusers and location-based services well. Most existing interface design\nguidelines do not align with the flexibility and dynamism of emerging\ntechnologies. The goal of this study is to improve interface design by\ncombining agile methodologies with human-centered design principles. Future\nstudies should also incorporate both qualitative and quantitative approaches,\nparticularly in the context of cloud-based technologies and organizational\ninformation systems. This approach aims to bridge the gap between current\ninterface design practices and the changing technological landscape.", "comment": "Accepted in CITIC 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08549v1", "AI": {"title_translation": "探索人机交互与信息系统中新兴技术的融合", "tldr": "现有HCI设计方法未能适应移动、云和物联网等新兴技术，本研究旨在通过结合敏捷和以人为本的设计原则来改进界面设计。", "motivation": "现代信息系统与日常生活的深度融合，以及移动、云计算和物联网的快速发展，对人机交互（HCI）专业人员设计用户友好和适应性强的界面提出了新挑战，特别是对于儿童、老年人和残疾人等不同用户群体。", "method": "本研究回顾了50篇关于现代信息系统HCI界面设计的最新论文，以评估现有方法对当前技术需求的适应程度。", "result": "研究结果表明，大多数HCI设计方法仍基于旧的桌面模型，对移动用户和基于位置的服务支持不足。现有的大多数界面设计指南与新兴技术的灵活性和动态性不符。", "conclusion": "为了弥合当前界面设计实践与不断变化的技术格局之间的差距，本研究的目标是通过结合敏捷方法和以人为本的设计原则来改进界面设计。未来的研究应结合定性和定量方法，特别是在基于云的技术和组织信息系统背景下。", "translation": "现代技术驱动的信息系统已成为我们日常生活的一部分。然而，这种深度融合给人机交互（HCI）专业人员带来了新的挑战。随着移动和云计算以及物联网（IoT）的快速发展，对HCI专家设计用户友好且适应性强的界面的需求从未如此紧迫。特别是对于儿童、老年人和残疾人等不同用户群体，他们需要根据自身需求量身定制、不受时间和地点限制的界面。本研究回顾了50篇关于现代信息系统HCI界面设计的最新论文。目的是了解这些方法如何很好地满足当前技术的需求。研究结果表明，大多数HCI设计方法仍然基于旧的桌面模型，并且对移动用户和基于位置的服务支持不佳。大多数现有的界面设计指南与新兴技术的灵活性和动态性不符。本研究的目标是通过将敏捷方法与以人为本的设计原则相结合来改进界面设计。未来的研究还应结合定性和定量方法，特别是在基于云的技术和组织信息系统背景下。这种方法旨在弥合当前界面设计实践与不断变化的技术格局之间的差距。", "summary": "本研究探讨了人机交互（HCI）设计方法与新兴技术（如移动、云计算和物联网）之间的差距。通过回顾50篇论文，研究发现现有HCI设计方法和指南未能充分适应现代技术和多样化用户群体的需求。为解决此问题，本研究提出结合敏捷方法和以人为本的设计原则来改进界面设计，并建议未来研究在云技术和组织信息系统背景下采用定性和定量方法。", "keywords": "人机交互, 新兴技术, 信息系统, 界面设计, 敏捷方法", "comments": "这篇论文强调了HCI领域面临的紧迫挑战，即现有设计范式未能跟上快速发展的技术步伐。其创新点在于提出将敏捷方法与以人为本的设计相结合，为弥合这一差距提供了明确方向。然而，该研究主要是文献回顾，并未提出具体的实现框架或案例研究，这可能是其局限性。其重要性在于指出了HCI研究和实践的未来发展方向。"}}
{"id": "2506.08604", "title": "Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation", "authors": ["Giacomo Baldan", "Qiang Liu", "Alberto Guardone", "Nils Thuerey"], "summary": "Generative machine learning methods, such as diffusion models and flow\nmatching, have shown great potential in modeling complex system behaviors and\nbuilding efficient surrogate models. However, these methods typically learn the\nunderlying physics implicitly from data. We propose Physics-Based Flow Matching\n(PBFM), a novel generative framework that explicitly embeds physical\nconstraints, both PDE residuals and algebraic relations, into the flow matching\nobjective. We also introduce temporal unrolling at training time that improves\nthe accuracy of the final, noise-free sample prediction. Our method jointly\nminimizes the flow matching loss and the physics-based residual loss without\nrequiring hyperparameter tuning of their relative weights. Additionally, we\nanalyze the role of the minimum noise level, $\\sigma_{\\min}$, in the context of\nphysical constraints and evaluate a stochastic sampling strategy that helps to\nreduce physical residuals. Through extensive benchmarks on three representative\nPDE problems, we show that our approach yields up to an $8\\times$ more accurate\nphysical residuals compared to FM, while clearly outperforming existing\nalgorithms in terms of distributional accuracy. PBFM thus provides a principled\nand efficient framework for surrogate modeling, uncertainty quantification, and\naccelerated simulation in physics and engineering applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08604v1", "AI": {"title_translation": "流匹配与偏微分方程的结合：一个物理约束生成统一框架", "tldr": "提出PBFM，一种将物理约束显式嵌入流匹配的生成框架，在物理残差和分布精度上显著优于现有方法。", "motivation": "现有的生成机器学习方法（如扩散模型和流匹配）通常从数据中隐式学习底层物理，未能显式地嵌入物理约束。", "method": "提出物理驱动流匹配（PBFM），通过将PDE残差和代数关系等物理约束显式嵌入流匹配目标中。该方法在训练时引入时间展开以提高样本预测精度，并联合最小化流匹配损失和基于物理的残差损失，无需调整超参数权重。同时，分析了最小噪声水平的作用，并评估了随机采样策略以减少物理残差。", "result": "在三个代表性PDE问题上的广泛基准测试表明，PBFM在物理残差方面比FM准确度提高高达8倍，并且在分布精度方面明显优于现有算法。", "conclusion": "PBFM为物理和工程应用中的代理建模、不确定性量化和加速模拟提供了一个有原则且高效的框架。", "translation": "生成式机器学习方法，如扩散模型和流匹配，在建模复杂系统行为和构建高效代理模型方面展现出巨大潜力。然而，这些方法通常从数据中隐式学习底层物理。我们提出了基于物理的流匹配（PBFM），这是一种新颖的生成框架，它将物理约束（包括偏微分方程残差和代数关系）显式地嵌入到流匹配目标中。我们还在训练时引入了时间展开，这提高了最终无噪声样本预测的准确性。我们的方法联合最小化流匹配损失和基于物理的残差损失，而无需对其相对权重进行超参数调优。此外，我们分析了最小噪声水平 $\\sigma_{\\min}$ 在物理约束背景下的作用，并评估了一种有助于减少物理残差的随机采样策略。通过在三个代表性偏微分方程问题上的广泛基准测试，我们表明我们的方法在物理残差方面比流匹配（FM）准确度提高高达8倍，同时在分布精度方面明显优于现有算法。因此，PBFM为物理和工程应用中的代理建模、不确定性量化和加速模拟提供了一个有原则且高效的框架。", "summary": "本文提出了一种名为物理驱动流匹配（PBFM）的新型生成框架，旨在解决现有生成模型在隐式学习物理规律的局限性。PBFM通过将物理约束（如PDE残差和代数关系）显式地嵌入到流匹配目标中，并引入时间展开和联合损失最小化策略，显著提升了模型在物理残差和分布精度上的表现。实验证明，PBFM在物理残差方面比传统流匹配模型准确度高出8倍，为物理与工程领域的代理建模、不确定性量化和加速模拟提供了高效且有原则的解决方案。", "keywords": "流匹配, 偏微分方程, 物理约束, 生成模型, 代理建模", "comments": "这篇论文的创新点在于将物理约束显式地融入到流匹配模型中，解决了现有生成模型在物理一致性方面的不足。通过引入时间展开和智能的损失函数设计，该方法不仅提升了生成样本的物理准确性，也保持了良好的分布精度，这对于物理和工程领域的精确模拟和不确定性量化具有重要意义。其无需超参数调优的特性也增加了方法的实用性。"}}
{"id": "2506.08677", "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "authors": ["Milica Škipina", "Nikola Jovišić", "Nicola Dall'Asen", "Vanja Švenda", "Anil Osman Tur", "Slobodan Ilić", "Elisa Ricci", "Dubravko Ćulibrk"], "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final patch-based\nmodel, significantly aiding the noise removal process. This thoughtful design\nenables MAMBO to generate highly realistic mammograms of up to 3840x3840\npixels. Importantly, this approach can be used to enhance the training of\nclassification models and extended to anomaly detection. Experiments, both\nnumerical and radiologist validation, assess MAMBO's capabilities in image\ngeneration, super-resolution, and anomaly detection, highlighting its potential\nto enhance mammography analysis for more accurate diagnoses and earlier lesion\ndetection.", "comment": "21 pages, 14 figures, 7 tables", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08677v1", "AI": {"title_translation": "MAMBO：乳腺X线图像的高分辨率生成方法", "tldr": "MAMBO是一种新颖的基于补丁的扩散模型，能够生成高分辨率的乳腺X线图像，以解决AI训练数据稀缺的问题，并可用于增强分类模型训练和异常检测。", "motivation": "乳腺X线摄影是乳腺癌检测和诊断的金标准，AI辅助软件可以显著增强该过程。然而，训练AI系统需要大量多样化的数据集，由于隐私和伦理限制，这些数据往往难以获取。现有的扩散模型研究很少关注乳腺X线图像，并且未能成功生成捕获细微病变特征所需的高分辨率输出。", "method": "本文引入了MAMmography ensemBle mOdel (MAMBO)，这是一种新颖的基于补丁的扩散方法，旨在生成全分辨率乳腺X线图像。MAMBO整合了独立的扩散模型，以捕获局部和全局（图像级）上下文信息。然后将这些上下文信息输入到最终的基于补丁的模型中，这显著有助于去噪过程。", "result": "MAMBO能够生成高达3840x3840像素的高度逼真的乳腺X线图像。该方法可用于增强分类模型的训练，并可扩展到异常检测。数值实验和放射科医生验证均评估了MAMBO在图像生成、超分辨率和异常检测方面的能力。", "conclusion": "MAMBO通过生成高分辨率、逼真的乳腺X线图像，解决了AI训练数据稀缺的问题，并有望增强乳腺X线分析，从而实现更准确的诊断和更早的病变检测。", "translation": "乳腺X线摄影是检测和诊断乳腺癌的金标准。人工智能（AI）辅助软件可以显著增强这一过程，帮助放射科医生识别异常。然而，训练AI系统需要大量多样化的数据集，由于隐私和伦理限制，这些数据往往难以获取。为了解决这个问题，本文引入了MAMmography ensemBle mOdel (MAMBO)，这是一种新颖的基于补丁的扩散方法，旨在生成全分辨率乳腺X线图像。扩散模型在逼真图像生成方面取得了突破性成果，但很少有研究关注乳腺X线图像，并且没有成功生成捕获小病灶细微特征所需的高分辨率输出。为了实现这一点，MAMBO整合了独立的扩散模型，以捕获局部和全局（图像级）上下文信息。然后将这些上下文信息输入到最终的基于补丁的模型中，这显著有助于去噪过程。这种周到的设计使MAMBO能够生成高达3840x3840像素的高度逼真的乳腺X线图像。重要的是，这种方法可以用于增强分类模型的训练，并可扩展到异常检测。通过数值和放射科医生验证的实验评估了MAMBO在图像生成、超分辨率和异常检测方面的能力，突出了其在增强乳腺X线分析以实现更准确诊断和更早病变检测方面的潜力。", "summary": "本文提出MAMBO（乳腺X线图像集成模型），一种新颖的基于补丁的扩散方法，用于生成高分辨率乳腺X线图像。针对AI训练数据获取困难及现有模型无法生成高分辨率乳腺X线图像的问题，MAMBO通过整合独立扩散模型捕获局部和全局上下文，并将其输入到最终的基于补丁的模型中，从而实现高达3840x3840像素的图像生成。实验证明，MAMBO在图像生成、超分辨率和异常检测方面表现出色，有望提升乳腺X线分析的准确性。", "keywords": "乳腺X线图像生成, 扩散模型, 高分辨率图像, 数据增强, 异常检测", "comments": "MAMBO的创新之处在于其结合局部和全局上下文的独特扩散模型设计，成功解决了医学图像生成中高分辨率和细粒度特征捕获的挑战。这对于弥补医疗数据稀缺性、增强AI诊断模型训练具有重要意义。其生成高分辨率图像的能力，特别是针对小病灶的细微特征，是其重要的突破。"}}
{"id": "2506.08661", "title": "Synchronization in Anonymous Networks Under Continuous Dynamics", "authors": ["Rida Bazzi", "Anya Chaturvedi", "Andréa W. Richa", "Peter Vargas"], "summary": "We present the $\\kappa$-Synchronizer that works in non-synchronous dynamic\nnetworks under minimal assumptions. Our model allows continuous topological\nchanges without any guarantee of eventual global or partial stabilization and\nassumes that nodes are anonymous. This deterministic synchronizer is the first\nto enable nodes to simulate a dynamic network synchronous algorithm for\nexecutions in a semi-synchronous dynamic environment under a weakly-fair node\nactivation scheduler, despite the absence of a global clock, node ids,\npersistent connectivity or any assumptions about the edge dynamics (in both the\nsynchronous and semi-synchronous environments). In summary, we make the\nfollowing contributions: (1) we extend the definition of synchronizers to\nnetworks with continuous arbitrary edge dynamics; (2) we present the first\nsynchronizer from the semi-synchronous to the synchronous model in a network\nwith continuous arbitrary edge dynamics; and (3) we present non-trivial\napplications of the proposed synchronizer to existing algorithms. We assume an\nextension of the Pull communication model by adding a single 1-bit multi-writer\natomic register at each edge-port of a node, since we show that the standard\nPull model is not sufficient to allow for non-trivial synchronization in our\nscenario. The $\\kappa$-Synchronizer operates with memory overhead at the nodes\nthat is linear on the maximum node degree and logarithmic on the runtime of the\nunderlying synchronous algorithm being simulated.", "comment": "18 pages, 1 figure, 2 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.08661v1", "AI": {"title_translation": "匿名网络在连续动态下的同步", "tldr": "本文提出了一个名为 $\\kappa$-Synchronizer 的确定性同步器，它能在节点匿名、拓扑结构持续变化的非同步动态网络中，以最小假设实现同步算法的模拟。", "motivation": "在缺乏全局时钟、节点ID、持久连接或边缘动态假设的情况下，在具有弱公平节点激活调度器的半同步动态环境中，使节点能够模拟动态网络同步算法。", "method": "本文提出了 $\\kappa$-Synchronizer，它是一种确定性同步器。它扩展了Pull通信模型，在每个节点的边端口添加一个1比特的多写入原子寄存器。该同步器在节点上的内存开销与最大节点度呈线性关系，并与所模拟的底层同步算法的运行时间呈对数关系。", "result": "1. 将同步器定义扩展到具有连续任意边缘动态的网络；2. 提出了第一个在具有连续任意边缘动态的网络中，从半同步到同步模型的同步器；3. 展示了所提出的同步器在现有算法上的非平凡应用。", "conclusion": "$\\\\kappa$-Synchronizer 是第一个在匿名、持续动态变化的非同步网络中，以最小假设实现同步算法模拟的确定性同步器，并扩展了同步器定义和通信模型。", "translation": "我们提出了 $\\kappa$-Synchronizer，它在最小假设下的非同步动态网络中工作。我们的模型允许持续的拓扑变化，不保证最终的全局或局部稳定，并假设节点是匿名的。尽管没有全局时钟、节点ID、持久连接或关于边缘动态的任何假设（在同步和半同步环境中），这个确定性同步器是第一个使节点能够在弱公平节点激活调度器下的半同步动态环境中模拟动态网络同步算法的。总而言之，我们做出了以下贡献：(1) 我们将同步器的定义扩展到具有连续任意边缘动态的网络；(2) 我们提出了第一个在具有连续任意边缘动态的网络中，从半同步到同步模型的同步器；(3) 我们展示了所提出的同步器在现有算法上的非平凡应用。我们假设通过在节点的每个边缘端口添加一个1比特的多写入原子寄存器来扩展Pull通信模型，因为我们表明在我们的场景中，标准的Pull模型不足以实现非平凡的同步。$\\kappa$-Synchronizer 在节点上的内存开销与最大节点度呈线性关系，并与所模拟的底层同步算法的运行时间呈对数关系。", "summary": "本文提出并详细介绍了 $\\kappa$-Synchronizer，这是一个在节点匿名且拓扑结构持续变化的非同步动态网络中运行的确定性同步器。该同步器在最小假设下，首次实现了在缺乏全局时钟、节点ID和持久连接等条件下的半同步动态环境中，节点对同步算法的模拟。其主要贡献包括将同步器定义扩展到连续任意边缘动态网络，提出了第一个从半同步到同步模型的同步器，并展示了其在现有算法中的应用。该同步器通过扩展Pull通信模型实现，内存开销与最大节点度呈线性关系，与模拟算法运行时间呈对数关系。", "keywords": "同步器, 匿名网络, 动态网络, 连续动态, $\\kappa$-Synchronizer", "comments": "这项工作具有重要意义，因为它首次在极端动态和匿名网络条件下实现了同步，这对于分布式系统中的鲁棒性设计至关重要。其创新点在于引入了 $\\kappa$-Synchronizer 和对Pull通信模型的扩展，以克服标准模型的限制。这项研究为未来在高度动态且资源受限环境下的分布式算法设计奠定了基础。"}}
{"id": "2506.08540", "title": "Higher-Order Network Representation of J. S. Bach's Solo Violin Sonatas and Partitas: Topological and Geometrical Explorations", "authors": ["Dima Mrad", "Sara Najem"], "summary": "Music is inherently complex, with structures and interactions that unfold\nacross multiple layers. Complex networks have emerged as powerful structures\nfor the quantitative analysis of Western classical music, revealing significant\nfeatures of its harmonic and structural organization. Although notable works\nhave used these approaches to study music, dyadic representations of\ninteractions fall short in conveying the underlying complexity and depth. In\nrecent years, the limitations of traditional graph representations have been\nquestioned and challenged in the context of interactions that could be\nhigher-dimensional. Effective musical analysis requires models that capture\nhigher-order interactions and a framework that simultaneously captures\ntransitions between them. Subsequently, in this paper, we present a topological\nframework for analyzing J. S. Bach's Solo Violin Sonatas and Partitas that uses\nhigher-order networks where single notes are vertices, two-note chords are\nedges, three-notes are triangles, etc. We subsequently account for the flow of\nmusic, by modeling transitions between successive notes. We identify\ngenre-specific patterns in the works' geometric and topological properties. In\nparticular, we find signatures in the trends of the evolution of the Euler\ncharacteristic and curvature, as well as examining adherence to the\nGauss-Bonnet theorem across different movement types. The distinctions are\nrevealed between slow movements, Fugues, and Baroque dance movements through\ntheir simplicial complex representation.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08540v1", "AI": {"title_translation": "J. S. 巴赫无伴奏小提琴奏鸣曲与帕蒂塔的高阶网络表示：拓扑与几何探索", "tldr": "本文提出了一种使用高阶网络分析巴赫无伴奏小提琴作品的拓扑框架，揭示了不同乐章类型的几何和拓扑特征。", "motivation": "传统复杂网络在音乐分析中，二元交互表示无法充分传达音乐的潜在复杂性和深度，且未能捕捉高阶交互和其间的转换。", "method": "本文提出了一种拓扑框架，使用高阶网络分析J. S. 巴赫的无伴奏小提琴奏鸣曲与帕蒂塔。其中，单音符是顶点，双音符和弦是边，三音符是三角形等。通过建模连续音符之间的转换来解释音乐的流动。识别作品几何和拓扑属性中的特定流派模式，并通过欧拉示性数和曲率的演变趋势、以及高斯-博内定理的遵循情况进行分析。", "result": "发现慢乐章、赋格曲和巴洛克舞曲乐章通过其单纯复形表示揭示了区别。在欧拉示性数和曲率的演变趋势中发现了特征，并检验了不同乐章类型对高斯-博内定理的遵循情况。", "conclusion": "通过高阶网络和拓扑框架，可以有效地揭示J. S. 巴赫无伴奏小提琴作品中不同乐章类型的内在结构和特征。", "translation": "音乐本质上是复杂的，其结构和相互作用在多个层面展开。复杂网络已成为定量分析西方古典音乐的强大结构，揭示了其和声和结构组织的重要特征。尽管已有一些著名的作品使用这些方法研究音乐，但二元交互表示未能充分传达其潜在的复杂性和深度。近年来，传统图表示的局限性在可能更高维度的交互背景下受到了质疑和挑战。有效的音乐分析需要能够捕捉高阶交互的模型，以及同时捕捉它们之间转换的框架。因此，在本文中，我们提出了一种分析J. S. 巴赫无伴奏小提琴奏鸣曲与帕蒂塔的拓扑框架，该框架使用高阶网络，其中单音符是顶点，双音符和弦是边，三音符是三角形等。随后，我们通过建模连续音符之间的转换来解释音乐的流动。我们识别了作品几何和拓扑属性中的特定流派模式。特别是，我们发现欧拉示性数和曲率演变趋势中的特征，并检验了不同乐章类型对高斯-博内定理的遵循情况。通过其单纯复形表示，揭示了慢乐章、赋格曲和巴洛克舞曲乐章之间的区别。", "summary": "本文提出了一种创新的拓扑框架，利用高阶网络对J. S. 巴赫的无伴奏小提琴奏鸣曲与帕蒂塔进行分析。该框架将单音符视为顶点，和弦视为高阶结构，并建模音符间的转换，以克服传统二元网络表示的局限性。研究通过分析作品的几何和拓扑属性（如欧拉示性数和曲率），成功识别了不同乐章类型（慢乐章、赋格曲、巴洛克舞曲）的独特模式和区别，为西方古典音乐的复杂性分析提供了新的视角。", "keywords": "高阶网络, 音乐分析, 拓扑学, J. S. 巴赫, 几何属性", "comments": "这篇论文通过引入高阶网络和拓扑学概念，为音乐分析提供了一个新颖且强大的工具，超越了传统二元网络表示的局限性。它不仅揭示了巴赫作品中深层次的结构和模式，还为理解音乐的复杂性和动态性开辟了新的途径。将数学拓扑概念（如欧拉示性数和高斯-博内定理）应用于音乐，展示了跨学科研究的巨大潜力。"}}
{"id": "2506.08033", "title": "Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases", "authors": ["Axel TahmasebiMoradi", "Vincent Ren", "Benjamin Le-Creurer", "Chetra Mang"], "summary": "Aiming to reduce the computational cost of numerical simulations, a\nconvolutional neural network (CNN) and a multi-layer perceptron (MLP) are\nintroduced to build a surrogate model to approximate radiative heat transfer\nsolutions in a 2-D walled domain with participative gases. The originality of\nthis work lays in the adaptation of the inputs of the problem (gas and wall\nproperties) in order to fit with the CNN architecture, more commonly used for\nimage processing. Two precision datasets have been created with the classical\nsolver, ICARUS2D, that uses the discrete transfer radiation method with the\nstatistical narrow bands model. The performance of the CNN architecture is\ncompared to a more classical MLP architecture in terms of speed and accuracy.\nThanks to Optuna, all results are obtained using the optimized hyper parameters\nnetworks. The results show a significant speedup with industrially acceptable\nrelative errors compared to the classical solver for both architectures.\nAdditionally, the CNN outperforms the MLP in terms of precision and is more\nrobust and stable to changes in hyper-parameters. A performance analysis on the\ndataset size of the samples have also been carried out to gain a deeper\nunderstanding of the model behavior.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08033v1", "AI": {"title_translation": "二维炉膛内光谱参与气体辐射传热中CNN和MLP的可行性研究", "tldr": "研究了CNN和MLP作为代理模型在二维炉膛辐射传热模拟中替代传统求解器的可行性，结果表明它们能显著加速计算并保持可接受的精度，其中CNN表现优于MLP。", "motivation": "旨在降低数值模拟的计算成本，特别是二维炉膛内辐射传热问题的计算成本。", "method": "引入卷积神经网络（CNN）和多层感知机（MLP）构建代理模型，以近似二维炉膛内参与气体的辐射传热解。为了适应CNN架构，对问题输入（气体和壁面特性）进行了调整。使用经典求解器ICARUS2D（采用离散传输辐射方法和统计窄带模型）创建了两个精度数据集。通过Optuna优化超参数，比较了CNN和MLP在速度和精度方面的性能。还对数据集样本量进行了性能分析。", "result": "两种架构（CNN和MLP）都显示出显著的加速，且与传统求解器相比，相对误差在工业上可接受。CNN在精度方面优于MLP，并且对超参数的变化更鲁棒和稳定。对数据集样本量的分析也深入了解了模型行为。", "conclusion": "CNN和MLP作为代理模型在二维炉膛辐射传热模拟中是可行的，能够有效降低计算成本。CNN在性能上优于MLP，是更优的选择。", "translation": "旨在降低数值模拟的计算成本，本文引入了卷积神经网络（CNN）和多层感知机（MLP）来构建代理模型，以近似二维壁面域中参与气体的辐射传热解决方案。这项工作的原创性在于调整了问题输入（气体和壁面特性）以适应CNN架构，该架构更常用于图像处理。使用经典求解器ICARUS2D（该求解器采用离散传输辐射方法与统计窄带模型）创建了两个精度数据集。在速度和精度方面，将CNN架构的性能与更经典的MLP架构进行了比较。借助Optuna，所有结果均通过优化超参数网络获得。结果表明，与经典求解器相比，两种架构都显著提高了速度，且相对误差在工业上可接受。此外，CNN在精度方面优于MLP，并且对超参数的变化更鲁棒和稳定。为了更深入地了解模型行为，还对样本数据集大小进行了性能分析。", "summary": "本文旨在通过构建代理模型来降低二维炉膛内光谱参与气体辐射传热数值模拟的计算成本。研究人员引入了卷积神经网络（CNN）和多层感知机（MLP），并调整了输入以适应CNN架构。通过经典求解器ICARUS2D生成数据集，并比较了CNN和MLP在速度和精度上的表现。结果显示，两种模型都能显著加速计算，并保持可接受的误差，其中CNN在精度、鲁棒性和稳定性方面优于MLP。研究还分析了数据集大小对模型性能的影响。", "keywords": "辐射传热, 卷积神经网络, 多层感知机, 代理模型, 计算流体力学", "comments": "这项工作创新性地将通常用于图像处理的CNN架构应用于辐射传热模拟，通过调整输入数据使其适应CNN，体现了跨领域应用机器学习的潜力。其重要性在于为计算流体力学（CFD）等计算密集型领域提供了一种有效的加速方法，有望显著降低工业模拟的计算资源需求。CNN优于MLP的结果也为后续研究提供了方向。"}}
{"id": "2506.08405", "title": "Optimal Graph Reconstruction by Counting Connected Components in Induced Subgraphs", "authors": ["Hadley Black", "Arya Mazumdar", "Barna Saha", "Yinzhan Xu"], "summary": "The graph reconstruction problem has been extensively studied under various\nquery models. In this paper, we propose a new query model regarding the number\nof connected components, which is one of the most basic and fundamental graph\nparameters. Formally, we consider the problem of reconstructing an $n$-node\n$m$-edge graph with oracle queries of the following form: provided with a\nsubset of vertices, the oracle returns the number of connected components in\nthe induced subgraph. We show $\\Theta(\\frac{m \\log n}{\\log m})$ queries in\nexpectation are both sufficient and necessary to adaptively reconstruct the\ngraph. In contrast, we show that $\\Omega(n^2)$ non-adaptive queries are\nrequired, even when $m = O(n)$. We also provide an $O(m\\log n + n\\log^2 n)$\nquery algorithm using only two rounds of adaptivity.", "comment": "To appear in COLT 2025", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.08405v1", "AI": {"title_translation": "诱导子图中连通分量计数的图最优重建", "tldr": "本文提出了一种基于诱导子图中连通分量计数的新型图重建查询模型，并给出了自适应查询的最优复杂度和非自适应查询的下限。", "motivation": "旨在提出一种新的图重建查询模型，该模型关注连通分量数量这一基本图参数。", "method": "该论文提出了一种预言机查询模型，其中给定一个顶点子集，预言机返回诱导子图中的连通分量数量。它研究了自适应和非自适应查询策略，并开发了一种仅使用两轮自适应的算法。", "result": "结果表明，对于自适应图重建，期望情况下$\\Theta(\\frac{m \\log n}{\\log m})$次查询是充分且必要的。相比之下，即使对于稀疏图（$m=O(n)$），也需要$\\Omega(n^2)$次非自适应查询。此外，还提供了一种使用两轮自适应的$O(m\\log n + n\\log^2 n)$次查询算法。", "conclusion": "该论文在新查询模型下确立了自适应图重建的最优查询复杂度，并突出了非自适应方法的低效率。", "translation": "图重建问题在各种查询模型下得到了广泛研究。在本文中，我们提出了一种关于连通分量数量的新查询模型，连通分量是最基本和最基础的图参数之一。形式上，我们考虑使用以下形式的预言机查询来重建一个$n$节点$m$边图的问题：提供一个顶点子集，预言机返回诱导子图中的连通分量数量。我们证明，期望情况下$\\Theta(\\frac{m \\log n}{\\log m})$次查询对于自适应地重建图是充分且必要的。相比之下，我们表明即使当$m = O(n)$时，也需要$\\Omega(n^2)$次非自适应查询。我们还提供了一种仅使用两轮自适应的$O(m\\log n + n\\log^2 n)$次查询算法。", "summary": "本文介绍了一种用于图重建的新型查询模型，其中查询涉及计算诱导子图中的连通分量。研究确定了$\\Theta(\\frac{m \\log n}{\\log m})$次查询对于自适应重建既充分又必要。研究还表明，即使对于稀疏图，非自适应查询的下限也显著更高，达到$\\Omega(n^2)$，并提出了一种使用两轮自适应的高效算法。", "keywords": "图重建, 连通分量, 查询模型, 自适应算法, 查询复杂度", "comments": "该论文的创新之处在于提出了一种基于连通分量的、用于图重建的全新且基础的查询模型。它通过建立自适应查询的下限和上限，突出了自适应策略的效率，并提供了一种实用的算法，从而提供了全面的分析。自适应和非自适应查询复杂度之间的对比是一个关键的见解。"}}
{"id": "2506.08237", "title": "Solving partial differential equations in participating media", "authors": ["Bailey Miller", "Rohan Sawhney", "Keenan Crane", "Ioannis Gkioulekas"], "summary": "We consider the problem of solving partial differential equations (PDEs) in\ndomains with complex microparticle geometry that is impractical, or\nintractable, to model explicitly. Drawing inspiration from volume rendering, we\npropose tackling this problem by treating the domain as a participating medium\nthat models microparticle geometry stochastically, through aggregate\nstatistical properties (e.g., particle density). We first introduce the problem\nsetting of PDE simulation in participating media. We then specialize to\nexponential media and describe the properties that make them an attractive\nmodel of microparticle geometry for PDE simulation problems. We use these\nproperties to develop two new algorithms, volumetric walk on spheres and\nvolumetric walk on stars, that generalize previous Monte Carlo algorithms to\nenable efficient and discretization-free simulation of linear elliptic PDEs\n(e.g., Laplace) in participating media. We demonstrate experimentally that our\nalgorithms can solve Laplace boundary value problems with complex microparticle\ngeometry more accurately and more efficiently than previous approaches, such as\nensemble averaging and homogenization.", "comment": "SIGGRAPH 2025. Project page\n  https://imaging.cs.cmu.edu/volumetric_walk_on_spheres", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.08237v1", "AI": {"title_translation": "在参与介质中求解偏微分方程", "tldr": "该论文引入了新的蒙特卡洛算法（体积球体漫步和体积星体漫步），通过将复杂微粒几何形状的域建模为参与介质，从而高效求解偏微分方程，并优于先前的方法。", "motivation": "在具有复杂微粒几何形状的域中求解偏微分方程时，对微粒几何形状进行显式建模是不切实际或难以处理的。", "method": "受体渲染的启发，提出将域视为参与介质，通过聚合统计特性（如粒子密度）随机建模微粒几何形状。论文专门研究指数介质，并利用其特性开发了两种新的蒙特卡洛算法：体积球体漫步和体积星体漫步。这些算法推广了先前的蒙特卡洛算法，以实现参与介质中线性椭圆偏微分方程（如拉普拉斯方程）的高效且无离散化的模拟。", "result": "实验证明，所提出的算法在解决具有复杂微粒几何形状的拉普拉斯边值问题时，比以往的方法（如系综平均和均匀化）更准确、更高效。", "conclusion": "所提出的体积漫步算法为在具有复杂、随机建模微粒几何形状的域中求解偏微分方程提供了一种更优越的方法。", "translation": "我们考虑在具有复杂微粒几何形状的域中求解偏微分方程 (PDE) 的问题，这种几何形状显式建模是不切实际或难以处理的。借鉴体渲染的灵感，我们提出通过将域视为参与介质来解决此问题，该介质通过聚合统计特性（例如，粒子密度）随机建模微粒几何形状。我们首先介绍了参与介质中 PDE 模拟的问题设置。然后，我们专门研究指数介质，并描述了使其成为 PDE 模拟问题中微粒几何形状的有吸引力模型的特性。我们利用这些特性开发了两种新算法：体积球体漫步 (volumetric walk on spheres) 和体积星体漫步 (volumetric walk on stars)，它们推广了先前的蒙特卡洛算法，以实现在参与介质中线性椭圆 PDE（例如，拉普拉斯方程）的高效且无离散化的模拟。我们通过实验证明，我们的算法可以比以前的方法（例如，系综平均和均匀化）更准确、更有效地解决具有复杂微粒几何形状的拉普拉斯边值问题。", "summary": "本论文解决了在具有复杂且难以显式建模的微粒几何形状的域中求解偏微分方程 (PDE) 的挑战。受体渲染的启发，它提出将此类域视为参与介质，利用统计特性对微粒进行随机建模。作者引入了两种新颖的蒙特卡洛算法——“体积球体漫步”和“体积星体漫步”，它们能够高效且无离散化地模拟此类介质中的线性椭圆 PDE（如拉普拉斯方程）。实验结果表明，这些算法在处理具有复杂微粒结构的拉普拉斯边值问题时，在准确性和效率方面均优于传统方法。", "keywords": "偏微分方程, 参与介质, 蒙特卡洛算法, 微粒几何, 拉普拉斯方程", "comments": "该论文通过借鉴体渲染和随机建模的概念，为复杂几何形状中的偏微分方程模拟提供了一种创新方法。开发无离散化的蒙特卡洛算法是解决此类问题（尤其是在无法进行显式建模的情况下）提高效率和准确性的重大进展。"}}
{"id": "2506.08283", "title": "Serendipitous Recommendation with Multimodal LLM", "authors": ["Haoting Wang", "Jianling Wang", "Hao Li", "Fangjun Yi", "Mengyu Fu", "Youwei Zhang", "Yifan Liu", "Liang Liu", "Minmin Chen", "Ed H. Chi", "Lichan Hong", "Haokai Lu"], "summary": "Conventional recommendation systems succeed in identifying relevant content\nbut often fail to provide users with surprising or novel items. Multimodal\nLarge Language Models (MLLMs) possess the world knowledge and multimodal\nunderstanding needed for serendipity, but their integration into\nbillion-item-scale platforms presents significant challenges. In this paper, we\npropose a novel hierarchical framework where fine-tuned MLLMs provide\nhigh-level guidance to conventional recommendation models, steering them\ntowards more serendipitous suggestions. This approach leverages MLLM strengths\nin understanding multimodal content and user interests while retaining the\nefficiency of traditional models for item-level recommendation. This mitigates\nthe complexity of applying MLLMs directly to vast action spaces. We also\ndemonstrate a chain-of-thought strategy enabling MLLMs to discover novel user\ninterests by first understanding video content and then identifying relevant\nyet unexplored interest clusters. Through live experiments within a commercial\nshort-form video platform serving billions of users, we show that our\nMLLM-powered approach significantly improves both recommendation serendipity\nand user satisfaction.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08283v1", "AI": {"title_translation": "多模态大型语言模型下的意外发现推荐", "tldr": "本文提出了一种结合多模态大型语言模型（MLLM）与传统推荐系统的分层框架，以解决传统推荐系统缺乏意外发现（serendipity）的问题，并在真实商业平台中验证了其在提升推荐意外发现性和用户满意度方面的显著效果。", "motivation": "传统推荐系统在识别相关内容方面表现出色，但在提供令用户惊喜或新颖的项目方面常常不足。多模态大型语言模型（MLLM）拥有世界知识和多模态理解能力，具备实现意外发现推荐的潜力，但将其集成到十亿级别规模的平台面临巨大挑战。", "method": "本文提出了一种新颖的分层框架，其中经过微调的MLLM为传统推荐模型提供高层指导，引导它们生成更具意外发现性的推荐。这种方法利用了MLLM在理解多模态内容和用户兴趣方面的优势，同时保留了传统模型在项目级别推荐的效率，从而减轻了直接将MLLM应用于巨大行动空间的复杂性。此外，本文还展示了一种思维链策略，使MLLM能够通过首先理解视频内容，然后识别相关但未被探索的兴趣簇来发现新的用户兴趣。", "result": "通过在服务数十亿用户的商业短视频平台进行的实时实验表明，我们基于MLLM的方法显著提高了推荐的意外发现性和用户满意度。", "conclusion": "本文提出的结合多模态大型语言模型（MLLM）的分层框架和思维链策略，成功解决了传统推荐系统在提供意外发现方面的问题，并在实际应用中显著提升了用户体验。", "translation": "传统推荐系统在识别相关内容方面表现出色，但常常无法为用户提供惊喜或新颖的项目。多模态大型语言模型（MLLM）拥有实现意外发现所需的世界知识和多模态理解能力，但将其集成到十亿级别规模的平台面临巨大挑战。在本文中，我们提出了一种新颖的分层框架，其中经过微调的MLLM为传统推荐模型提供高层指导，引导它们生成更具意外发现性的建议。这种方法利用了MLLM在理解多模态内容和用户兴趣方面的优势，同时保留了传统模型在项目级别推荐的效率。这减轻了将MLLM直接应用于巨大行动空间的复杂性。我们还展示了一种思维链策略，使MLLM能够通过首先理解视频内容，然后识别相关但未被探索的兴趣簇来发现新的用户兴趣。通过在服务数十亿用户的商业短视频平台进行的实时实验，我们表明我们基于MLLM的方法显著提高了推荐的意外发现性和用户满意度。", "summary": "本文提出了一种创新的分层框架，将微调后的多模态大型语言模型（MLLM）与传统推荐模型相结合，旨在提升推荐系统的意外发现性。该框架通过MLLM提供高层指导，利用其多模态理解能力和世界知识，同时保持传统模型的效率。此外，研究还引入了一种思维链策略，使MLLM能够通过分析内容发现新的用户兴趣。在商业短视频平台上的实时实验证明，该MLLM驱动的方法显著改善了推荐的意外发现性和用户满意度。", "keywords": "意外发现推荐, 多模态大型语言模型, 分层框架, 思维链, 推荐系统", "comments": "这篇论文的创新点在于提出了一个巧妙的分层框架，有效地将MLLM的强大理解能力与传统推荐系统的效率相结合，解决了MLLM在超大规模推荐平台直接应用的复杂性问题。通过提供高层指导而非直接进行项目推荐，该方法成功地利用了MLLM的优势，同时规避了其计算成本和规模限制。思维链策略的引入也进一步增强了MLLM发现新颖兴趣的能力。这项工作对于未来将大型预训练模型应用于实际工业级推荐系统具有重要的参考价值和启发性。"}}
{"id": "2506.08824", "title": "Support bound for differential elimination in polynomial dynamical systems", "authors": ["Yulia Mukhina", "Gleb Pogudin"], "summary": "We study an important special case of the differential elimination problem:\ngiven a polynomial dynamical system $\\mathbf{x}' = \\mathbf{g}(\\mathbf{x})$ and\na polynomial observation function $y = f(\\mathbf{x})$, find the minimal\ndifferential equation satisfied by $y$. In our previous work, for the case $y =\nx_1$, we established a bound on the support of such a differential equation and\nshown that it can be turned into an algorithm via the evaluation-interpolation\napproach. The main contribution of the present paper is a generalization of the\naforementioned result in two directions: to allow any polynomial function $y =\nf(\\mathbf{x})$, not just a single coordinate, and to allow $\\mathbf{g}$ and $f$\ndepend on unknown symbolic parameters. We conduct computation experiments to\nevaluate the accuracy of our new bound and show that the approach allows to\nperform elimination for some cases out of reach for the state of the art\nsoftware.", "comment": null, "cate": "cs.SC", "url": "http://arxiv.org/abs/2506.08824v1", "AI": {"title_translation": "多项式动力系统中微分消除的支持界", "tldr": "本文将多项式动力系统中寻找观测函数最小微分方程的支持界推广到任意多项式观测函数和含有未知符号参数的情况，并展示其优于现有软件。", "motivation": "本文旨在解决微分消除问题中的一个重要特例：给定多项式动力系统和多项式观测函数，找到y满足的最小微分方程。之前的研究仅限于y=x1的情况，且不包含未知符号参数。本研究的动机是推广该结果，使其适用于任意多项式观测函数f(x)以及g和f依赖于未知符号参数的情况。", "method": "本文通过推广之前工作中的支持界概念，允许任意多项式函数y=f(x)作为观测函数，并且允许g和f依赖于未知符号参数。通过计算实验评估了新界的准确性，并结合了求值-插值方法。", "result": "研究结果表明，推广后的方法能够对现有最先进软件无法处理的一些情况进行消除。计算实验评估了新界的准确性。", "conclusion": "本文成功地将多项式动力系统中微分消除的支持界推广到更普遍的情况，包括任意多项式观测函数和含有未知符号参数的系统，这使得该方法能够解决当前最先进软件无法处理的问题。", "translation": "我们研究了微分消除问题的一个重要特例：给定一个多项式动力系统 $\\mathbf{x}' = \\mathbf{g}(\\mathbf{x})$ 和一个多项式观测函数 $y = f(\\mathbf{x})$，找到 $y$ 满足的最小微分方程。在我们之前的工作中，对于 $y = x_1$ 的情况，我们建立了这样一个微分方程的支持界，并表明它可以通过求值-插值方法转化为算法。本文的主要贡献是将上述结果在两个方向上进行了推广：允许任何多项式函数 $y = f(\\mathbf{x})$，而不仅仅是单个坐标；以及允许 $\\mathbf{g}$ 和 $f$ 依赖于未知的符号参数。我们进行了计算实验，以评估我们新界的准确性，并表明该方法允许对一些现有最先进软件无法触及的情况执行消除。", "summary": "本文研究了多项式动力系统中的微分消除问题，旨在找到观测函数y满足的最小微分方程。作者在前人工作的基础上，将微分方程的支持界推广到任意多项式观测函数以及系统和观测函数包含未知符号参数的情况。通过计算实验验证了新界的准确性，并展示该方法能够处理现有最先进软件无法解决的一些消除问题。", "keywords": "微分消除, 多项式动力系统, 支持界, 符号参数, 观测函数", "comments": "本文的创新之处在于将微分消除中的支持界理论推广到更广泛的场景，即允许任意多项式观测函数和包含未知符号参数的系统。其重要性体现在能够解决现有最先进软件无法处理的复杂问题，从而推动了多项式动力系统分析领域的发展。这为实际应用中更复杂的系统建模和分析提供了新的工具。"}}
{"id": "2506.08284", "title": "A structure preserving H-curl algebraic multigrid method for the eddy current equations", "authors": ["Raymond Tuminaro", "Christian Glusa"], "summary": "A new algebraic multigrid method (AMG) is presented for solving the linear\nsystems associated with the eddy current approximation to the Maxwell\nequations. This AMG method extends an idea proposed by Reitzinger and Schoberl.\nThe main feature of the Reitzinger and Schoberl algorithm (RSAMG) is that it\nmaintains null-space properties of the Curl-Curl operator throughout all levels\nof the AMG hierarchy. It does this by enforcing a commuting relationship\ninvolving grid transfers and the discrete gradient operator. This null-space\npreservation property is critical to the algorithm's success, however enforcing\nthis commuting relationship is non-trivial except in the special case where one\nleverages a piece-wise constant nodal interpolation operator. For this reason,\nmesh independent convergence rates are generally not observed for RSAMG due to\nits reliance on sub-optimal piece-wise constant interpolation. We present a new\nAMG algorithm that enforces the same commuting relationship. The main advance\nis that the new structure preserving H-curl algorithm (SpHcurlAMG) does not\nrely on piece-wise constant interpolation and can leverage fairly general and\nmore sophisticated nodal interpolation operators. The key idea is to employ\nenergy minimization AMG (EAMG) to construct edge interpolation grid transfers\nand to enforce the commuting relationship by embedding it as constraints within\nan EAMG procedure. While it might appear that solving such a constrained energy\nminimization is costly, we illustrate how this is not the case in our context.\nNumerical results are then given demonstrating mesh independent convergence\nover a range of test problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08284v1", "AI": {"title_translation": "一种结构保持H-curl代数多重网格法求解涡流方程", "tldr": "本文提出了一种新的代数多重网格法（SpHcurlAMG），用于求解涡流方程。该方法在保留RSAMG的零空间特性的同时，通过利用更复杂的插值技术克服了RSAMG对次优分段常数插值的依赖，从而实现了网格无关的收敛。", "motivation": "现有的RSAMG方法虽然能保持零空间特性，但依赖于次优的分段常数插值，这通常导致无法实现网格无关的收敛速度。", "method": "新的SpHcurlAMG方法通过采用能量最小化AMG（EAMG）来构建边缘插值网格传输，并通过将交换关系作为约束嵌入EAMG过程中来强制执行该关系，从而不依赖于分段常数插值。", "result": "数值结果表明，该方法在各种测试问题上都实现了网格无关的收敛。", "conclusion": "新的SpHcurlAMG方法通过在不依赖次优插值的情况下保持零空间特性，成功地实现了涡流方程的网格无关收敛。", "translation": "提出了一种新的代数多重网格法（AMG）用于求解与麦克斯韦方程组的涡流近似相关的线性系统。该AMG方法扩展了Reitzinger和Schoberl提出的一个思想。Reitzinger和Schoberl算法（RSAMG）的主要特点是它在AMG层次结构的所有级别中都保持Curl-Curl算子的零空间特性。它通过强制执行涉及网格传输和离散梯度算子的交换关系来实现这一点。这种零空间保持特性对于算法的成功至关重要，然而，除了在利用分段常数节点插值算子的特殊情况下，强制执行这种交换关系并非易事。因此，由于RSAMG依赖于次优的分段常数插值，通常观察不到网格无关的收敛速度。我们提出了一种新的AMG算法，它强制执行相同的交换关系。主要的进步是，新的结构保持H-curl算法（SpHcurlAMG）不依赖于分段常数插值，并且可以利用相当通用和更复杂的节点插值算子。其关键思想是采用能量最小化AMG（EAMG）来构建边缘插值网格传输，并通过将其作为约束嵌入EAMG过程中来强制执行交换关系。虽然解决这种受约束的能量最小化可能看起来代价高昂，但我们说明了在我们这种情况下并非如此。随后给出了数值结果，证明了在各种测试问题上实现了网格无关的收敛。", "summary": "本文提出了一种名为SpHcurlAMG的新型代数多重网格法，用于求解涡流方程。该方法改进了现有RSAMG的不足，在保留Curl-Curl算子关键零空间特性的同时，避免了对次优分段常数插值的依赖。SpHcurlAMG通过利用能量最小化AMG（EAMG）构建插值传输并将交换关系约束嵌入其中来实现这一目标。数值结果证实了其网格无关的收敛性。", "keywords": "代数多重网格, 涡流方程, H-curl, 零空间保持, 网格无关收敛", "comments": "该方法的创新之处在于克服了RSAMG对次优插值的依赖，同时保持了至关重要的零空间保持特性。通过集成EAMG并嵌入约束，新方法实现了网格无关的收敛，这对于涡流方程求解的效率和鲁棒性是一个显著的进步。"}}
{"id": "2506.08348", "title": "Pureformer-VC: Non-parallel Voice Conversion with Pure Stylized Transformer Blocks and Triplet Discriminative Training", "authors": ["Wenhan Yao", "Fen Xiao", "Xiarun Chen", "Jia Liu", "YongQiang He", "Weiping Wen"], "summary": "As a foundational technology for intelligent human-computer interaction,\nvoice conversion (VC) seeks to transform speech from any source timbre into any\ntarget timbre. Traditional voice conversion methods based on Generative\nAdversarial Networks (GANs) encounter significant challenges in precisely\nencoding diverse speech elements and effectively synthesising these elements\ninto natural-sounding converted speech. To overcome these limitations, we\nintroduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer\nblocks to build a disentangled encoder and employs Zipformer blocks to create a\nstyle transfer decoder. We adopt a variational decoupled training approach to\nisolate speech components using a Variational Autoencoder (VAE), complemented\nby triplet discriminative training to enhance the speaker's discriminative\ncapabilities. Furthermore, we incorporate the Attention Style Transfer\nMechanism (ASTM) with Zipformer's shared weights to improve the style transfer\nperformance in the decoder. We conducted experiments on two multi-speaker\ndatasets. The experimental results demonstrate that the proposed model achieves\ncomparable subjective evaluation scores while significantly enhancing objective\nmetrics compared to existing approaches in many-to-many and many-to-one VC\nscenarios.", "comment": "Accepted by IJCNN 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08348v1", "AI": {"title_translation": "Pureformer-VC：基于纯风格化Transformer块和三元组判别训练的非并行语音转换", "tldr": "Pureformer-VC通过新颖的Transformer块和三元组判别训练改进了非并行语音转换，实现了更好的客观性能。", "motivation": "传统的基于生成对抗网络（GAN）的语音转换方法在精确编码多样化语音元素和有效合成这些元素以产生自然听感的转换语音方面面临重大挑战。", "method": "本文提出了Pureformer-VC，一个编码器-解码器框架。它利用Conformer块构建解耦编码器，并采用Zipformer块创建风格迁移解码器。该方法采用变分解耦训练结合变分自编码器（VAE）分离语音组件，并辅以三元组判别训练以增强说话人的判别能力。此外，还结合了注意力风格迁移机制（ASTM）与Zipformer的共享权重，以提高解码器中的风格迁移性能。", "result": "在两个多说话人数据集上的实验结果表明，与现有方法相比，在多对多和多对一的VC场景中，所提出的模型在主观评估得分上取得了可比性，同时显著提升了客观指标。", "conclusion": "Pureformer-VC通过改进客观指标和保持主观质量，有效解决了传统基于GAN的非并行语音转换方法的局限性。", "translation": "作为智能人机交互的基础技术，语音转换（VC）旨在将任意源音色的语音转换为任意目标音色。传统的基于生成对抗网络（GAN）的语音转换方法在精确编码多样化语音元素和有效合成这些元素以产生自然听感的转换语音方面遇到了重大挑战。为了克服这些限制，我们引入了Pureformer-VC，一个编码器-解码器框架，它利用Conformer块构建一个解耦编码器，并采用Zipformer块创建风格迁移解码器。我们采用变分解耦训练方法，使用变分自编码器（VAE）分离语音组件，并辅以三元组判别训练以增强说话人的判别能力。此外，我们结合了注意力风格迁移机制（ASTM）与Zipformer的共享权重，以提高解码器中的风格迁移性能。我们在两个多说话人数据集上进行了实验。实验结果表明，与现有方法相比，在多对多和多对一的VC场景中，所提出的模型在主观评估得分上取得了可比性，同时显著提升了客观指标。", "summary": "本文介绍了Pureformer-VC，一种新颖的非并行语音转换框架，旨在克服传统基于GAN方法在语音元素编码和自然合成方面的局限性。Pureformer-VC采用带有Conformer块的解耦编码器和带有Zipformer块的风格迁移解码器。它结合了变分解耦训练、三元组判别训练和注意力风格迁移机制，以改善语音组件分离、说话人判别和风格迁移性能。实验结果表明，Pureformer-VC在各种语音转换场景中实现了可比的主观质量和显著更好的客观指标。", "keywords": "语音转换, 非并行VC, Transformer, 三元组判别训练, Pureformer-VC", "comments": "该论文通过结合先进的Transformer块（Conformer、Zipformer）和复杂的训练技术（变分解耦训练、三元组判别训练、ASTM），解决了语音转换中的一个重要挑战。这种方法旨在改善语音组件的解耦和风格迁移，这对于产生自然听感的语音转换至关重要。在保持主观质量的同时，侧重于客观指标的提升是一个亮点。"}}
{"id": "2506.08693", "title": "On the Ethics of Using LLMs for Offensive Security", "authors": ["Andreas Happe", "Jürgen Cito"], "summary": "Large Language Models (LLMs) have rapidly evolved over the past few years and\nare currently evaluated for their efficacy within the domain of offensive\ncyber-security. While initial forays showcase the potential of LLMs to enhance\nsecurity research, they also raise critical ethical concerns regarding the\ndual-use of offensive security tooling.\n  This paper analyzes a set of papers that leverage LLMs for offensive\nsecurity, focusing on how ethical considerations are expressed and justified in\ntheir work. The goal is to assess the culture of AI in offensive security\nresearch regarding ethics communication, highlighting trends, best practices,\nand gaps in current discourse.\n  We provide insights into how the academic community navigates the fine line\nbetween innovation and ethical responsibility. Particularly, our results show\nthat 13 of 15 reviewed prototypes (86.6\\%) mentioned ethical considerations and\nare thus aware of the potential dual-use of their research. Main motivation\ngiven for the research was allowing broader access to penetration-testing as\nwell as preparing defenders for AI-guided attackers.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08693v1", "AI": {"title_translation": "关于将大型语言模型用于攻击性安全的伦理", "tldr": "本文分析了将LLMs用于攻击性安全研究的伦理考量，发现大多数原型都提到了伦理问题，主要动机是促进渗透测试和防御AI攻击。", "motivation": "大型语言模型（LLMs）在攻击性网络安全领域显示出潜力，但也引发了双重用途的伦理担忧。本文的动机是分析现有研究中伦理考量的表达和理由，以评估攻击性安全研究中AI伦理沟通的文化，并识别趋势、最佳实践和不足。主要研究动机是促进更广泛的渗透测试和为防御者准备AI引导的攻击。", "method": "本文分析了一组利用LLMs进行攻击性安全的论文，重点关注其中伦理考量如何表达和论证。", "result": "在审查的15个原型中，有13个（86.6%）提到了伦理考量，表明研究人员意识到了研究的潜在双重用途。研究给出的主要动机是允许更广泛地进行渗透测试以及为防御者准备AI引导的攻击者。", "conclusion": "学术界在创新和伦理责任之间取得了平衡，大多数相关研究都意识到了其工作的潜在双重用途，并给出了促进安全实践的动机。", "translation": "大型语言模型（LLMs）在过去几年中迅速发展，目前正在攻击性网络安全领域评估其有效性。虽然初步尝试展示了LLMs增强安全研究的潜力，但它们也引发了关于攻击性安全工具双重用途的关键伦理问题。\n本文分析了一组利用LLMs进行攻击性安全的论文，重点关注其工作中伦理考量如何表达和论证。目标是评估攻击性安全研究中AI伦理沟通的文化，突出当前讨论中的趋势、最佳实践和不足。\n我们提供了关于学术界如何在创新和伦理责任之间取得平衡的见解。特别是，我们的结果显示，在审查的15个原型中，有13个（86.6%）提到了伦理考量，因此意识到了其研究的潜在双重用途。研究给出的主要动机是允许更广泛地进行渗透测试以及为防御者准备AI引导的攻击者。", "summary": "本文探讨了在攻击性网络安全中使用大型语言模型（LLMs）的伦理问题。通过分析相关论文，研究旨在评估该领域AI伦理沟通的现状，识别趋势和差距。研究发现，绝大多数被审查的原型都提到了伦理考量，并主要以促进渗透测试和增强防御能力作为研究动机，这表明学术界在创新与伦理责任之间寻求平衡。", "keywords": "大型语言模型, 攻击性安全, 伦理, 双重用途, 网络安全", "comments": "这篇论文探讨了一个非常及时和重要的主题，即LLMs在攻击性安全领域的伦理影响。其创新之处在于通过分析现有文献来评估该领域的伦理沟通文化。论文的发现表明，研究人员对双重用途问题有较高的认识，这对于促进负责任的AI发展至关重要。其重要性在于它为未来在敏感领域使用AI提供了伦理指南和讨论基础。"}}
{"id": "2506.08581", "title": "Evaluating the Performance and Efficiency of Sentence-BERT for Code Comment Classification", "authors": ["Fabian C. Peña", "Steffen Herbold"], "summary": "This work evaluates Sentence-BERT for a multi-label code comment\nclassification task seeking to maximize the classification performance while\ncontrolling efficiency constraints during inference. Using a dataset of 13,216\nlabeled comment sentences, Sentence-BERT models are fine-tuned and combined\nwith different classification heads to recognize comment types. While larger\nmodels outperform smaller ones in terms of F1, the latter offer outstanding\nefficiency, both in runtime and GFLOPS. As result, a balance between a\nreasonable F1 improvement (+0.0346) and a minimal efficiency degradation (+1.4x\nin runtime and +2.1x in GFLOPS) is reached.", "comment": "4th Intl. Workshop on NL-based Software Engineering (NLBSE)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08581v1", "AI": {"title_translation": "评估Sentence-BERT在代码注释分类中的性能和效率", "tldr": "本文评估了Sentence-BERT在多标签代码注释分类任务中的性能和效率，旨在实现分类性能最大化，同时控制推理时的效率限制。", "motivation": "该研究旨在最大化代码注释分类的性能，同时控制推理时的效率限制。", "method": "使用包含13,216个标记注释句子的大型数据集，对Sentence-BERT模型进行微调，并结合不同的分类头来识别注释类型。", "result": "较大模型在F1分数上优于较小模型，但较小模型在运行时间和GFLOPS方面表现出卓越的效率。最终，在合理的F1改进（+0.0346）和最小的效率下降（运行时间+1.4倍，GFLOPS +2.1倍）之间取得了平衡。", "conclusion": "该研究发现，在代码注释分类任务中，可以在Sentence-BERT模型的性能和效率之间找到一个平衡点。", "translation": "这项工作评估了Sentence-BERT在多标签代码注释分类任务中的应用，旨在最大化分类性能，同时控制推理时的效率限制。使用包含13,216个标记注释句子的大型数据集，对Sentence-BERT模型进行微调，并结合不同的分类头来识别注释类型。虽然较大模型在F1分数上优于较小模型，但较小模型在运行时间和GFLOPS方面表现出卓越的效率。最终，在合理的F1改进（+0.0346）和最小的效率下降（运行时间+1.4倍，GFLOPS +2.1倍）之间取得了平衡。", "summary": "本研究评估了Sentence-BERT在多标签代码注释分类任务中的表现，重点关注性能与效率的平衡。研究人员使用13,216个标记注释句子数据集，对Sentence-BERT模型进行微调，并结合不同的分类头。结果表明，虽然大型模型在F1分数上表现更佳，但小型模型在效率上更突出。最终，研究找到了一个在F1分数小幅提升与效率轻微下降之间的最佳平衡点。", "keywords": "Sentence-BERT, 代码注释分类, 性能, 效率, 多标签", "comments": "本文的创新点在于系统地评估了Sentence-BERT在代码注释分类任务中的性能与效率权衡，并量化了不同模型大小对F1分数、运行时间和GFLOPS的影响。这对于实际部署时需要在性能和资源消耗之间做出选择的场景具有重要指导意义。"}}
{"id": "2506.08416", "title": "Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots", "authors": ["Bolin Li", "Linwei Sun", "Xuecong Huang", "Yuzhi Jiang", "Lijun Zhu"], "summary": "This paper presents a periodic bipedal gait learning method using reward\ncomposition, integrated with a real-time gait planner for humanoid robots.\nFirst, we introduce a novel gait planner that incorporates dynamics to design\nthe desired joint trajectory. In the gait design process, the 3D robot model is\ndecoupled into two 2D models, which are then approximated as hybrid inverted\npendulums (H-LIP) for trajectory planning. The gait planner operates in\nparallel in real time within the robot's learning environment. Second, based on\nthis gait planner, we design three effective reward functions within a\nreinforcement learning framework, forming a reward composition to achieve\nperiodic bipedal gait. This reward composition reduces the robot's learning\ntime and enhances locomotion performance. Finally, a gait design example and\nperformance comparison are presented to demonstrate the effectiveness of the\nproposed method.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08416v1", "AI": {"title_translation": "机器人周期性双足步态学习：基于新型步态规划器和奖励组合", "tldr": "本文提出了一种结合新型步态规划器和奖励组合的周期性双足步态学习方法，能有效减少机器人学习时间并提高运动性能。", "motivation": "旨在通过结合实时步态规划器和奖励组合，实现人形机器人的周期性双足步态学习，以减少学习时间并提高运动性能。", "method": "首先，引入了一种考虑动力学、将3D机器人模型解耦为两个2D混合倒立摆（H-LIP）模型进行轨迹规划的新型实时步态规划器。其次，在此步态规划器的基础上，在强化学习框架内设计了三种有效的奖励函数，形成奖励组合，以实现周期性双足步态。", "result": "所提出的方法能够减少机器人的学习时间并提高运动性能。通过步态设计示例和性能比较证明了该方法的有效性。", "conclusion": "结合新型步态规划器和奖励组合的周期性双足步态学习方法是有效的，能够实现人形机器人的稳定步态并提升学习效率和运动表现。", "translation": "本文提出了一种使用奖励组合的周期性双足步态学习方法，该方法与人形机器人的实时步态规划器相结合。首先，我们介绍了一种结合动力学设计期望关节轨迹的新型步态规划器。在步态设计过程中，3D机器人模型被解耦为两个2D模型，然后近似为混合倒立摆（H-LIP）进行轨迹规划。步态规划器在机器人的学习环境中实时并行运行。其次，基于该步态规划器，我们在强化学习框架内设计了三个有效的奖励函数，形成奖励组合，以实现周期性双足步态。这种奖励组合减少了机器人的学习时间并增强了运动性能。最后，通过步态设计示例和性能比较来证明所提出方法的有效性。", "summary": "本文提出一种新颖的周期性双足步态学习方法，该方法将一个考虑动力学且能实时运行的新型步态规划器与强化学习中的奖励组合相结合。通过将3D机器人模型解耦为2D H-LIP模型进行轨迹规划，并设计了三种奖励函数。实验结果表明，该方法有效减少了机器人的学习时间并提升了其运动性能。", "keywords": "周期性双足步态学习, 奖励组合, 步态规划器, 人形机器人, 强化学习", "comments": "该论文的创新点在于结合了新型的实时步态规划器和奖励组合来优化周期性双足步态学习。将复杂的3D模型解耦为简化的2D H-LIP模型进行轨迹规划，提高了计算效率。奖励组合的设计有助于加速学习过程并提升机器人运动性能，这对于人形机器人的实际应用具有重要意义。"}}
{"id": "2506.08191", "title": "Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes", "authors": ["Antoni Nowinowski", "Krzysztof Krawiec"], "summary": "This study builds on the architecture of the Disentangler of Visual Priors\n(DVP), a type of autoencoder that learns to interpret scenes by decomposing the\nperceived objects into independent visual aspects of shape, size, orientation,\nand color appearance. These aspects are expressed as latent parameters which\ncontrol a differentiable renderer that performs image reconstruction, so that\nthe model can be trained end-to-end with gradient using reconstruction loss. In\nthis study, we extend the original DVP so that it can handle multiple objects\nin a scene. We also exploit the interpretability of its latent by using the\ndecoder to sample additional training examples and devising alternative\ntraining modes that rely on loss functions defined not only in the image space,\nbut also in the latent space. This significantly facilitates training, which is\notherwise challenging due to the presence of extensive plateaus in the\nimage-space reconstruction loss. To examine the performance of this approach,\nwe propose a new benchmark featuring multiple 2D objects, which subsumes the\npreviously proposed Multi-dSprites dataset while being more parameterizable. We\ncompare the DVP extended in these ways with two baselines (MONet and LIVE) and\ndemonstrate its superiority in terms of reconstruction quality and capacity to\ndecompose overlapping objects. We also analyze the gradients induced by the\nconsidered loss functions, explain how they impact the efficacy of training,\nand discuss the limitations of differentiable rendering in autoencoders and the\nways in which they can be addressed.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08191v1", "AI": {"title_translation": "复杂场景组合解释的可微分对象模型生成学习", "tldr": "本研究扩展了DVP自编码器，使其能够处理多对象场景，并通过利用潜在空间进行训练和定义新的损失函数来显著改善训练过程，同时提出了一个新的基准数据集，并证明了其在重建质量和分解重叠对象方面的优越性。", "motivation": "本研究旨在扩展现有的视觉先验解缠器（DVP）架构，使其能够处理复杂场景中的多个对象。同时，通过利用潜在空间来生成额外训练样本并定义新的损失函数，以解决图像空间重建损失中存在的广泛平台期问题，从而显著促进模型的训练。", "method": "本研究扩展了原始的视觉先验解缠器（DVP）架构，使其能够处理场景中的多个对象。方法包括利用解码器从潜在空间采样额外的训练样本，并设计替代训练模式，这些模式不仅依赖于图像空间中的损失函数，还依赖于潜在空间中的损失函数。此外，为了评估性能，研究提出了一个新的多2D对象基准数据集，并与MONet和LIVE两个基线模型进行了比较。", "result": "扩展后的DVP在重建质量和分解重叠对象的能力方面，表现出优于MONet和LIVE两个基线模型的优越性。研究还分析了不同损失函数引起的梯度，并解释了它们如何影响训练效率。", "conclusion": "本研究成功地扩展了DVP自编码器以处理多对象场景，并通过在潜在空间中引入损失函数显著改善了训练效率和模型性能。结果表明，该方法在复杂场景的组合解释方面具有优越性，同时也讨论了可微分渲染在自编码器中的局限性及其潜在的解决方案。", "translation": "本研究基于视觉先验解缠器（DVP）的架构，这是一种自编码器，通过将感知到的对象分解为形状、大小、方向和颜色外观等独立的视觉方面来学习解释场景。这些方面被表达为潜在参数，这些参数控制一个执行图像重建的可微分渲染器，从而使模型可以通过梯度和重建损失进行端到端训练。在本研究中，我们扩展了原始DVP，使其能够处理场景中的多个对象。我们还通过使用解码器采样额外的训练样本，并设计不仅在图像空间中，而且在潜在空间中定义损失函数的替代训练模式，来利用其潜在空间的解释性。这显著促进了训练，否则由于图像空间重建损失中存在广泛的平台期而变得具有挑战性。为了检验这种方法的性能，我们提出了一个新的多2D对象基准，它包含了先前提出的Multi-dSprites数据集，同时更具参数化能力。我们将以这些方式扩展的DVP与两个基线（MONet和LIVE）进行了比较，并证明了其在重建质量和分解重叠对象能力方面的优越性。我们还分析了所考虑损失函数引起的梯度，解释了它们如何影响训练效率，并讨论了自编码器中可微分渲染的局限性以及可以解决这些局限性的方法。", "summary": "本研究在视觉先验解缠器（DVP）架构的基础上进行了扩展，使其能够处理包含多个对象的复杂场景。通过利用DVP潜在空间的解释性，研究人员使用解码器采样额外训练样本，并设计了结合图像空间和潜在空间损失函数的训练模式，从而显著克服了图像空间重建损失中存在的训练挑战。为了评估该方法，研究提出了一个新的多2D对象基准数据集，并与现有基线模型进行了比较。结果表明，扩展后的DVP在重建质量和分解重叠对象方面表现出优越性。此外，研究还分析了损失函数对训练效率的影响，并讨论了可微分渲染在自编码器中的局限性。", "keywords": "生成学习, 可微分对象模型, 组合解释, DVP, 潜在空间损失", "comments": "这项研究的创新之处在于其对DVP架构的扩展，使其能够有效地处理多对象场景，并通过在潜在空间中引入损失函数来解决训练难题。这种方法利用了模型固有的可解释性，通过生成合成数据和在更高层次的特征空间进行优化，极大地提高了训练效率和模型性能。提出新的基准数据集也对未来的研究提供了有价值的工具。"}}
{"id": "2506.08332", "title": "ORFS-agent: Tool-Using Agents for Chip Design Optimization", "authors": ["Amur Ghose", "Andrew B. Kahng", "Sayak Kundu", "Zhiang Wang"], "summary": "Machine learning has been widely used to optimize complex engineering\nworkflows across numerous domains. In the context of integrated circuit design,\nmodern flows (e.g., going from a register-transfer level netlist to physical\nlayouts) involve extensive configuration via thousands of parameters, and small\nchanges to these parameters can have large downstream impacts on desired\noutcomes - namely design performance, power, and area. Recent advances in Large\nLanguage Models (LLMs) offer new opportunities for learning and reasoning\nwithin such high-dimensional optimization tasks. In this work, we introduce\nORFS-agent, an LLM-based iterative optimization agent that automates parameter\ntuning in an open-source hardware design flow. ORFS-agent adaptively explores\nparameter configurations, demonstrating clear improvements over standard\nBayesian optimization approaches in terms of resource efficiency and final\ndesign metrics. Our empirical evaluations on two different technology nodes and\na range of circuit benchmarks indicate that ORFS-agent can improve both routed\nwirelength and effective clock period by over 13%, all while using 40% fewer\noptimization iterations. Moreover, by following natural language objectives to\ntrade off certain metrics for others, ORFS-agent demonstrates a flexible and\ninterpretable framework for multi-objective optimization. Crucially, RFS-agent\nis modular and model-agnostic, and can be plugged in to any frontier LLM\nwithout any further fine-tuning.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08332v1", "AI": {"title_translation": "ORFS-agent：用于芯片设计优化的工具使用型智能体", "tldr": "ORFS-agent是一个基于LLM的迭代优化智能体，用于自动化芯片设计中的参数调整，在资源效率和设计指标方面优于传统的贝叶斯优化方法。", "motivation": "集成电路设计流程涉及数千个参数的配置，参数的微小变化会对设计性能、功耗和面积产生重大影响。大型语言模型（LLMs）为这种高维优化任务提供了新的学习和推理机会。", "method": "本文引入了ORFS-agent，一个基于LLM的迭代优化智能体，它自动化了开源硬件设计流程中的参数调整。ORFS-agent自适应地探索参数配置。", "result": "ORFS-agent在资源效率和最终设计指标方面明显优于标准贝叶斯优化方法。在两种不同的技术节点和一系列电路基准测试中，ORFS-agent可以将布线长度和有效时钟周期改善超过13%，同时减少40%的优化迭代次数。它还通过遵循自然语言目标实现了灵活且可解释的多目标优化。", "conclusion": "ORFS-agent是一个模块化且模型无关的智能体，可以无需进一步微调地插入任何前沿LLM，并有效提高芯片设计优化效率和性能。", "translation": "机器学习已广泛应用于优化众多领域的复杂工程工作流程。在集成电路设计中，现代流程（例如，从寄存器传输级网表到物理布局）涉及通过数千个参数进行广泛配置，这些参数的微小变化可能对所需结果（即设计性能、功耗和面积）产生巨大的下游影响。大型语言模型（LLMs）的最新进展为这种高维优化任务中的学习和推理提供了新的机会。在这项工作中，我们引入了ORFS-agent，一个基于LLM的迭代优化智能体，它自动化了开源硬件设计流程中的参数调整。ORFS-agent自适应地探索参数配置，在资源效率和最终设计指标方面，表现出比标准贝叶斯优化方法更明显的改进。我们对两种不同技术节点和一系列电路基准测试的实证评估表明，ORFS-agent可以将布线长度和有效时钟周期改善超过13%，同时减少40%的优化迭代次数。此外，通过遵循自然语言目标来权衡某些指标以实现其他目标，ORFS-agent展示了一个灵活且可解释的多目标优化框架。至关重要的是，RFS-agent是模块化且模型无关的，可以无需任何进一步微调地插入任何前沿LLM。", "summary": "ORFS-agent是一个基于大型语言模型（LLM）的迭代优化智能体，旨在自动化集成电路设计中的参数调整。它通过自适应探索参数配置，在资源效率和最终设计指标方面显著优于传统的贝叶斯优化方法。实验结果表明，ORFS-agent能将布线长度和有效时钟周期改善超过13%，并减少40%的优化迭代次数。此外，它支持通过自然语言进行灵活的多目标优化，并且具有模块化和模型无关的特点，无需微调即可与现有LLM集成。", "keywords": "芯片设计优化, LLM智能体, 参数调优, 多目标优化, 集成电路", "comments": "本文提出了一种新颖的基于LLM的智能体用于芯片设计优化，创新性地将LLM的推理能力应用于高维工程参数调优。其显著的性能提升和资源效率优势，以及支持自然语言多目标优化的灵活性，展示了LLM在复杂工程领域的巨大潜力。该方法的模块化和模型无关性也大大提高了其实用性和可推广性。"}}
{"id": "2506.08054", "title": "STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation", "authors": ["Yiming Wang", "Hao Peng", "Senzhang Wang", "Haohua Du", "Chunyang Liu", "Jia Wu", "Guanlin Wu"], "summary": "Traffic data imputation is fundamentally important to support various\napplications in intelligent transportation systems such as traffic flow\nprediction. However, existing time-to-space sequential methods often fail to\neffectively extract features in block-wise missing data scenarios. Meanwhile,\nthe static graph structure for spatial feature propagation significantly\nconstrains the models flexibility in handling the distribution shift issue for\nthe nonstationary traffic data. To address these issues, this paper proposes a\nSpatioTemporal Attention Mixture of experts network named STAMImputer for\ntraffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)\nframework to capture latent spatio-temporal features and their influence\nweights, effectively imputing block missing. A novel Low-rank guided Sampling\nGraph ATtention (LrSGAT) mechanism is designed to dynamically balance the local\nand global correlations across road networks. The sampled attention vectors are\nutilized to generate dynamic graphs that capture real-time spatial\ncorrelations. Extensive experiments are conducted on four traffic datasets for\nevaluation. The result shows STAMImputer achieves significantly performance\nimprovement compared with existing SOTA approaches. Our codes are available at\nhttps://github.com/RingBDStack/STAMImupter.", "comment": "10 pages, 5 figures, 3 tables. Extended version of paper accepted at\n  IJCAI 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08054v1", "AI": {"title_translation": "STAMImputer: 时空注意力MoE交通数据插补", "tldr": "STAMImputer利用时空注意力MoE框架和动态图机制，显著提升了交通数据插补在块缺失和非平稳数据场景下的性能。", "motivation": "现有方法在块缺失数据场景下特征提取能力不足；静态图结构限制了模型处理非平稳交通数据分布漂移的灵活性。", "method": "本文提出STAMImputer，一个时空注意力混合专家网络。引入混合专家(MoE)框架捕获潜在时空特征和影响权重，有效处理块缺失。设计低秩引导采样图注意力(LrSGAT)机制，动态平衡路网的局部和全局关联，并利用采样注意力向量生成动态图捕获实时空间关联。", "result": "在四个交通数据集上进行了广泛实验，结果显示STAMImputer比现有SOTA方法显著提高了性能。", "conclusion": "STAMImputer通过其创新的时空注意力MoE框架和动态图机制，有效解决了交通数据插补中块缺失和非平稳数据的挑战，并取得了显著的性能提升。", "translation": "交通数据插补对于支持智能交通系统中的各种应用（如交通流量预测）至关重要。然而，现有的时空序列方法往往无法有效提取块缺失数据场景下的特征。同时，用于空间特征传播的静态图结构显著限制了模型在处理非平稳交通数据分布漂移问题时的灵活性。为了解决这些问题，本文提出了一种名为STAMImputer的时空注意力混合专家网络，用于交通数据插补。具体而言，我们引入了一个混合专家（MoE）框架来捕获潜在的时空特征及其影响权重，从而有效地插补块缺失数据。设计了一种新颖的低秩引导采样图注意力（LrSGAT）机制，以动态平衡路网中的局部和全局关联。利用采样的注意力向量生成动态图，以捕获实时空间关联。在四个交通数据集上进行了广泛的实验评估。结果表明，与现有SOTA方法相比，STAMImputer取得了显著的性能提升。我们的代码可在https://github.com/RingBDStack/STAMImupter 获取。", "summary": "本文提出了STAMImputer，一种时空注意力混合专家网络，用于解决交通数据插补中块缺失和非平稳数据分布漂移的问题。通过引入混合专家框架捕获时空特征和权重，以及设计低秩引导采样图注意力机制动态生成实时空间关联图，STAMImputer在多个交通数据集上表现出优于现有SOTA方法的性能。", "keywords": "交通数据插补, 时空注意力, 混合专家, 动态图, 块缺失", "comments": "该论文的创新点在于结合了混合专家（MoE）框架来处理交通数据的复杂时空特征和块缺失问题，并引入了动态图生成机制（LrSGAT）以适应非平稳交通数据的分布漂移，增强了模型的灵活性和鲁棒性。这对于智能交通系统的实际应用具有重要意义。"}}
{"id": "2506.08759", "title": "Qymera: Simulating Quantum Circuits using RDBMS", "authors": ["Tim Littau", "Rihan Hai"], "summary": "Quantum circuit simulation is crucial for quantum computing such as\nvalidating quantum algorithms. We present Qymera, a system that repurposes\nrelational database management systems (RDBMSs) for simulation by translating\ncircuits into SQL queries, allowing quantum operations to run natively within\nan RDBMS. Qymera supports a wide range of quantum circuits, offering a\ngraphical circuit builder and code-based interfaces to input circuits. With a\nbenchmarking framework, Qymera facilitates comparison of RDBMS-based simulation\nagainst state-of-the-art simulation methods. Our demonstration showcases\nQymera's end-to-end SQL-based execution, seamless integration with classical\nworkflows, and its utility for development, benchmarking, and education in\nquantum computing and data management.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.08759v1", "AI": {"title_translation": "Qymera：使用关系型数据库管理系统模拟量子电路", "tldr": "Qymera是一个利用关系型数据库管理系统（RDBMS）通过将量子电路转换为SQL查询来模拟量子电路的系统，支持广泛的电路类型，并可用于开发、基准测试和教育。", "motivation": "量子电路模拟对于量子计算至关重要，例如验证量子算法。", "method": "Qymera通过将量子电路翻译成SQL查询，使量子操作能够在RDBMS中原生运行，从而利用RDBMS进行模拟。它还提供图形电路构建器和基于代码的接口来输入电路，并包含一个基准测试框架。", "result": "演示展示了Qymera的端到端SQL执行、与经典工作流的无缝集成，以及其在量子计算和数据管理领域的开发、基准测试和教育中的实用性。", "conclusion": "Qymera提供了一种新颖的、基于RDBMS的量子电路模拟方法，具有实际应用价值，可以促进量子计算和数据管理领域的发展。", "translation": "量子电路模拟对于量子计算至关重要，例如验证量子算法。我们提出了Qymera，一个通过将电路翻译成SQL查询，利用关系型数据库管理系统（RDBMS）进行模拟的系统，允许量子操作在RDBMS中原生运行。Qymera支持广泛的量子电路，提供图形电路构建器和基于代码的接口来输入电路。通过一个基准测试框架，Qymera有助于将基于RDBMS的模拟与最先进的模拟方法进行比较。我们的演示展示了Qymera的端到端基于SQL的执行、与经典工作流的无缝集成，以及其在量子计算和数据管理领域的开发、基准测试和教育中的实用性。", "summary": "Qymera是一个创新系统，它通过将量子电路转换为SQL查询，利用现有的关系型数据库管理系统（RDBMS）进行量子电路模拟。该系统支持多种量子电路输入方式，并包含一个基准测试框架，以便与现有模拟方法进行比较。Qymera的演示突出了其端到端SQL执行能力、与传统工作流程的良好兼容性，以及其在量子计算开发、性能评估和教育方面的潜在价值。", "keywords": "量子电路模拟, 关系型数据库管理系统, SQL, Qymera, 量子计算", "comments": "Qymera的创新之处在于其将量子电路模拟与成熟的RDBMS技术相结合，这可能为大规模量子模拟提供新的途径，并利用RDBMS的现有优化和管理能力。这种方法也可能降低量子模拟的门槛，使其更容易集成到现有数据管理实践中。"}}
{"id": "2506.08634", "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback", "authors": ["Alvaro Becerra", "Daniel Andres", "Pablo Villegas", "Roberto Daza", "Ruth Cobos"], "summary": "In this article, we present a novel multimodal feedback framework called\nMOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal\nLearning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),\nand Collaborative assessments for generating personalized feedback on student\nlearning activities. This framework consists of four key steps. First, peers\nand professors' assessments are conducted through standardized rubrics (that\ninclude both quantitative and qualitative evaluations). Second, multimodal data\nare collected during learning activities, including video recordings, audio\ncapture, gaze tracking, physiological signals (heart rate, motion data), and\nbehavioral interactions. Third, personalized feedback is generated using AI,\nsynthesizing human-based evaluations and data-based multimodal insights such as\nposture, speech patterns, stress levels, and cognitive load, among others.\nFinally, students review their own performance through video recordings and\nengage in self-assessment and feedback visualization, comparing their own\nevaluations with peers and professors' assessments, class averages, and\nAI-generated recommendations. By combining human-based and data-based\nevaluation techniques, this framework enables more accurate, personalized and\nactionable feedback. We tested MOSAIC-F in the context of improving oral\npresentation skills.", "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain\n  2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08634v1", "AI": {"title_translation": "MOSAIC-F：一个通过个性化反馈提升学生口头表达能力的框架", "tldr": "MOSAIC-F是一个结合多模态学习分析、传感器、AI和协作评估的框架，旨在为学生口头表达提供个性化反馈，通过整合人工和数据评估来提高反馈的准确性和可操作性。", "motivation": "提升学生的口头表达能力，并提供更准确、个性化和可操作的反馈。", "method": "MOSAIC-F框架包括四个步骤：1. 师生通过标准化评估量规进行评估；2. 收集多模态数据（视频、音频、眼动、生理信号、行为互动）；3. AI结合人工评估和多模态数据生成个性化反馈（姿态、语音模式、压力水平、认知负荷等）；4. 学生通过视频、自我评估和反馈可视化回顾表现，并与他人评估和AI建议进行比较。", "result": "通过结合人工和数据评估技术，该框架能够提供更准确、个性化和可操作的反馈。MOSAIC-F已在提升口头表达能力方面进行了测试。", "conclusion": "MOSAIC-F是一个有效的多模态反馈框架，通过整合多源数据和AI技术，能够为学生提供增强的、个性化的口头表达技能反馈。", "translation": "在本文中，我们提出了一个新颖的多模态反馈框架，名为MOSAIC-F，它是“一个数据驱动的框架，整合多模态学习分析（MMLA）、观察、传感器、人工智能（AI）和协作评估，用于生成学生学习活动的个性化反馈”的首字母缩写。该框架包括四个关键步骤。首先，通过标准化评估量规（包括定量和定性评估）进行同伴和教授的评估。其次，在学习活动期间收集多模态数据，包括视频记录、音频捕获、眼动追踪、生理信号（心率、运动数据）和行为互动。第三，使用AI生成个性化反馈，综合基于人类的评估和基于数据的多模态洞察，例如姿态、语音模式、压力水平和认知负荷等。最后，学生通过视频记录回顾自己的表现，并进行自我评估和反馈可视化，将自己的评估与同伴和教授的评估、班级平均分以及AI生成的建议进行比较。通过结合基于人类和基于数据的评估技术，该框架能够实现更准确、个性化和可操作的反馈。我们在提升口头表达能力的背景下测试了MOSAIC-F。", "summary": "MOSAIC-F是一个创新的多模态反馈框架，旨在通过整合多模态学习分析、观察、传感器、AI和协作评估来提升学生的口头表达能力。该框架分四步：人工评估、多模态数据收集、AI生成个性化反馈，以及学生自我评估和反馈可视化。它通过结合人工与数据驱动的评估方法，提供了更准确、个性化且可操作的反馈。", "keywords": "多模态反馈, 个性化反馈, 口头表达, 人工智能, 学习分析", "comments": "MOSAIC-F的创新之处在于其多模态数据整合和AI驱动的个性化反馈机制，这超越了传统的单一评估方式。它结合了人类的专业判断和客观的生理/行为数据，有望显著提高反馈的深度和实用性。该框架在提升口头表达能力方面具有重要应用潜力，未来可扩展至其他技能培养。"}}
{"id": "2506.08762", "title": "EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements", "authors": ["Issa Sugiura", "Takashi Ishida", "Taro Makino", "Chieko Tazuke", "Takanori Nakagawa", "Kosuke Nakago", "David Ha"], "summary": "Financial analysis presents complex challenges that could leverage large\nlanguage model (LLM) capabilities. However, the scarcity of challenging\nfinancial datasets, particularly for Japanese financial data, impedes academic\ninnovation in financial analytics. As LLMs advance, this lack of accessible\nresearch resources increasingly hinders their development and evaluation in\nthis specialized domain. To address this gap, we introduce EDINET-Bench, an\nopen-source Japanese financial benchmark designed to evaluate the performance\nof LLMs on challenging financial tasks including accounting fraud detection,\nearnings forecasting, and industry prediction. EDINET-Bench is constructed by\ndownloading annual reports from the past 10 years from Japan's Electronic\nDisclosure for Investors' NETwork (EDINET) and automatically assigning labels\ncorresponding to each evaluation task. Our experiments reveal that even\nstate-of-the-art LLMs struggle, performing only slightly better than logistic\nregression in binary classification for fraud detection and earnings\nforecasting. These results highlight significant challenges in applying LLMs to\nreal-world financial applications and underscore the need for domain-specific\nadaptation. Our dataset, benchmark construction code, and evaluation code is\npublicly available to facilitate future research in finance with LLMs.", "comment": null, "cate": "q-fin.ST", "url": "http://arxiv.org/abs/2506.08762v1", "AI": {"title_translation": "EDINET-Bench：使用日本财务报表评估LLM在复杂金融任务上的表现", "tldr": "引入EDINET-Bench，一个用于评估LLM在复杂日本金融任务（如欺诈检测、盈利预测）上的基准数据集，发现当前LLM表现不佳。", "motivation": "金融分析面临复杂挑战，大型语言模型（LLM）有潜力。然而，具有挑战性的金融数据集稀缺，特别是对于日本金融数据，阻碍了LLM在该领域的开发和评估。", "method": "引入EDINET-Bench，一个开源的日本金融基准数据集，旨在评估LLM在欺诈检测、盈利预测和行业预测等挑战性金融任务上的性能。该基准通过从EDINET下载过去10年的年度报告并自动分配标签来构建。", "result": "实验表明，即使是最先进的LLM也表现不佳，在欺诈检测和盈利预测的二元分类中仅略优于逻辑回归。", "conclusion": "结果强调了将LLM应用于现实世界金融应用的重大挑战，并突出了领域特定适应的必要性。", "translation": "金融分析提出了复杂的挑战，可以利用大型语言模型（LLM）的能力。然而，具有挑战性的金融数据集稀缺，特别是对于日本金融数据，阻碍了金融分析领域的学术创新。随着LLM的发展，这种可访问研究资源的缺乏日益阻碍了它们在这一专业领域中的开发和评估。为了弥补这一差距，我们引入了EDINET-Bench，一个开源的日本金融基准，旨在评估LLM在包括会计欺诈检测、盈利预测和行业预测在内的挑战性金融任务上的性能。EDINET-Bench通过从日本投资者电子披露网络（EDINET）下载过去10年的年度报告并自动为每个评估任务分配相应标签来构建。我们的实验表明，即使是最先进的LLM也表现不佳，在欺诈检测和盈利预测的二元分类中仅略优于逻辑回归。这些结果突显了将LLM应用于现实世界金融应用的重大挑战，并强调了领域特定适应的必要性。我们的数据集、基准构建代码和评估代码均已公开，以促进未来LLM在金融领域的研究。", "summary": "该论文介绍了EDINET-Bench，一个开源的日本金融基准数据集，旨在评估大型语言模型（LLM）在复杂金融任务（如欺诈检测、盈利预测和行业预测）上的性能。该基准通过收集EDINET过去10年的年度报告并自动标注构建。实验结果显示，尽管LLM在这些任务上表现出潜力，但即使是最先进的模型也仅略优于传统方法，表明LLM在真实金融应用中仍面临重大挑战，需要进行领域特定适应。该数据集及其相关代码已公开。", "keywords": "LLM, 金融分析, 日本金融数据, EDINET-Bench, 基准数据集", "comments": "EDINET-Bench的创新之处在于提供了一个针对日本金融数据且具有挑战性的开源基准，填补了该领域研究资源的空白。其重要性在于揭示了当前LLM在复杂金融任务上的局限性，特别是对于非英文、领域专业性强的场景，强调了未来研究应关注领域特定适应性。这对于推动LLM在金融领域的实际应用具有指导意义。"}}
{"id": "2506.08716", "title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment", "authors": ["Maximilian Tschuchnig", "Lukas Lamminger", "Philipp Steininger", "Michael Gadermayr"], "summary": "Cone-Beam Computed Tomography (CBCT) is widely used for real-time\nintraoperative imaging due to its low radiation dose and high acquisition\nspeed. However, despite its high resolution, CBCT suffers from significant\nartifacts and thereby lower visual quality, compared to conventional Computed\nTomography (CT). A recent approach to mitigate these artifacts is synthetic CT\n(sCT) generation, translating CBCT volumes into the CT domain. In this work, we\nenhance sCT generation through multimodal learning, integrating intraoperative\nCBCT with preoperative CT. Beyond validation on two real-world datasets, we use\na versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT\nquality affect sCT quality. The results demonstrate that multimodal sCT\nconsistently outperform unimodal baselines, with the most significant gains\nobserved in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate\nthat these findings are highly reproducible in real-world clinical datasets.", "comment": "Data is open source. Code will be provided on acceptance. Paper\n  currently under review", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08716v1", "AI": {"title_translation": "通过多模态融合增强CBCT的合成CT：一项关于CBCT质量和对齐影响的研究", "tldr": "本研究通过多模态学习（结合术中CBCT和术前CT）增强了合成CT（sCT）的生成，并分析了CBCT-CT对齐和CBCT质量对sCT质量的影响，结果显示多模态sCT表现优于单模态基线，尤其是在对齐良好、质量较低的CBCT-CT病例中。", "motivation": "锥形束计算机断层扫描（CBCT）因其低辐射剂量和高采集速度而被广泛用于实时术中成像。然而，尽管CBCT分辨率高，但与传统CT相比，它存在显著的伪影，导致视觉质量较低。合成CT（sCT）生成是一种将CBCT体积转换为CT域以减轻这些伪影的新方法。", "method": "本研究通过多模态学习，将术中CBCT与术前CT相结合，以增强sCT生成。除了在两个真实世界数据集上进行验证外，研究还使用了一个多功能合成数据集，以分析CBCT-CT对齐和CBCT质量如何影响sCT质量。", "result": "结果表明，多模态sCT始终优于单模态基线，其中在对齐良好、质量较低的CBCT-CT病例中观察到最显著的增益。", "conclusion": "本研究的发现高度可复现于真实世界临床数据集。", "translation": "锥形束计算机断层扫描（CBCT）因其低辐射剂量和高采集速度而被广泛用于实时术中成像。然而，尽管CBCT分辨率高，但与传统计算机断层扫描（CT）相比，它存在显著的伪影，从而导致视觉质量较低。最近一种减轻这些伪影的方法是合成CT（sCT）生成，即将CBCT体积转换为CT域。在这项工作中，我们通过多模态学习，将术中CBCT与术前CT相结合，从而增强sCT生成。除了在两个真实世界数据集上进行验证外，我们还使用了一个多功能合成数据集，以分析CBCT-CT对齐和CBCT质量如何影响sCT质量。结果表明，多模态sCT始终优于单模态基线，其中在对齐良好、质量较低的CBCT-CT病例中观察到最显著的增益。最后，我们证明这些发现在真实世界临床数据集中具有高度可复现性。", "summary": "本研究旨在通过多模态学习，将术中CBCT与术前CT融合，以提高合成CT（sCT）的生成质量，从而解决CBCT固有的伪影问题。研究不仅在真实数据集上验证了该方法，还利用合成数据集深入分析了CBCT-CT对齐和CBCT质量对sCT性能的影响。结果显示，与单模态方法相比，多模态sCT表现出卓越的性能，尤其是在对齐良好且CBCT质量较低的情况下提升显著。这些发现已在真实临床数据集中得到验证，证明了其在临床应用中的潜力。", "keywords": "合成CT, CBCT, 多模态融合, 图像质量, 对齐", "comments": "该研究通过引入多模态学习，有效地解决了CBCT图像伪影导致的视觉质量问题，提高了合成CT的准确性。其创新点在于不仅提出了多模态融合方法，还深入分析了CBCT质量和对齐对合成CT性能的影响，为实际临床应用提供了宝贵的指导。特别是发现对齐良好、质量较低的CBCT在多模态融合中获益最大，这对于优化图像采集流程具有重要意义。"}}
{"id": "2506.08715", "title": "Balancing Fixed Number of Nodes Among Multiple Fixed Clusters", "authors": ["Paritosh Ranjan", "Surajit Majumder", "Prodip Roy", "Bhuban Padhan"], "summary": "Cloud infrastructure users often allocate a fixed number of nodes to\nindividual container clusters (e.g., Kubernetes, OpenShift), resulting in\nunderutilization of computing resources due to asynchronous and variable\nworkload peaks across clusters. This research proposes a novel system and\nmethod for dynamic rebalancing of a fixed total number of nodes among multiple\nfixed clusters based on real-time resource utilization thresholds. By\nintroducing a Node Balancing Cluster Group (NBCG), clusters are grouped and\nallowed to dynamically share nodes through a controlled reallocation mechanism,\nmanaged by a Node Balancing Cluster Balancer and a Resizing Rule Engine. The\nsystem identifies overutilized and underutilized clusters using threshold\nparameters, and reassigns nodes without incurring additional provisioning\ncosts. If reallocation causes a violation of utilization thresholds, the system\nreverses the operation to maintain cluster stability. The proposed architecture\nnot only optimizes resource utilization and operational cost but also\nintroduces a strategic advantage for cloud service providers like IBM Cloud.\nUnlike existing solutions, this approach enables intra-account node sharing\nacross clusters with strict adherence to user-defined constraints and ensures\nconsistent cluster state management. This invention has the potential to\nsignificantly reduce computing resource waste and position IBM Cloud services\nas more efficient and competitive.", "comment": "9 pages, 2 figures", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.08715v1", "AI": {"title_translation": "多固定集群间固定节点数均衡", "tldr": "该研究提出了一种新的系统和方法，用于根据实时资源利用率阈值在多个固定集群之间动态重新平衡固定总数的节点，从而优化资源利用率并降低运营成本。", "motivation": "云基础设施用户通常为单个容器集群分配固定数量的节点，由于集群间异步和可变的负载峰值，导致计算资源利用率低下。", "method": "该研究提出了一种基于实时资源利用率阈值在多个固定集群之间动态重新平衡固定总数节点的新系统和方法。通过引入节点平衡集群组（NBCG），集群被分组并通过受控的重新分配机制动态共享节点，该机制由节点平衡集群均衡器和调整规则引擎管理。系统使用阈值参数识别过载和未充分利用的集群，并在不产生额外供应成本的情况下重新分配节点。如果重新分配导致利用率阈值违规，系统会撤销操作以保持集群稳定性。", "result": "该架构不仅优化了资源利用率和运营成本，还为IBM Cloud等云服务提供商带来了战略优势。与现有解决方案不同，该方法能够在严格遵守用户定义约束的情况下实现跨集群的账户内节点共享，并确保一致的集群状态管理。", "conclusion": "这项发明有可能显著减少计算资源浪费，并使IBM Cloud服务更高效和更具竞争力。", "translation": "云基础设施用户通常为单个容器集群（例如Kubernetes、OpenShift）分配固定数量的节点，由于集群间异步且可变的负载峰值，导致计算资源利用率低下。本研究提出了一种新颖的系统和方法，用于根据实时资源利用率阈值在多个固定集群之间动态重新平衡固定总数的节点。通过引入节点平衡集群组（NBCG），集群被分组并通过受控的重新分配机制动态共享节点，该机制由节点平衡集群均衡器和调整规则引擎管理。该系统使用阈值参数识别过载和未充分利用的集群，并在不产生额外供应成本的情况下重新分配节点。如果重新分配导致利用率阈值违规，系统会撤销操作以保持集群稳定性。所提出的架构不仅优化了资源利用率和运营成本，还为IBM Cloud等云服务提供商带来了战略优势。与现有解决方案不同，该方法能够在严格遵守用户定义约束的情况下实现跨集群的账户内节点共享，并确保一致的集群状态管理。这项发明有可能显著减少计算资源浪费，并使IBM Cloud服务更高效和更具竞争力。", "summary": "本研究提出了一种在多个固定容器集群之间动态重新平衡固定总数节点的新系统和方法，以解决由于异步工作负载峰值导致的资源利用率低下问题。该系统通过引入节点平衡集群组（NBCG）和相关管理机制，根据实时资源利用率阈值智能地重新分配节点，同时避免额外成本并确保集群稳定性。这种方法优化了资源利用率和运营成本，并为云服务提供商提供了竞争优势，通过实现账户内节点共享和一致的集群状态管理，有望大幅减少计算资源浪费。", "keywords": "节点平衡, 资源利用率, 容器集群, 动态再平衡, 云基础设施", "comments": "该论文提出了一种创新的方法来解决云环境中资源利用率低下的常见问题。其核心创新在于引入了节点平衡集群组（NBCG）以及智能的重新分配机制，能够在不增加额外成本的前提下动态调整节点分配，并确保集群稳定性。这种方法在现有解决方案中独树一帜，因为它实现了账户内节点共享并严格遵守用户定义约束，这对于大型云服务提供商（如IBM Cloud）来说具有重要的战略意义和竞争优势。"}}
{"id": "2506.08930", "title": "Hardware Limitations and Optimization Approach in 1-Bit RIS Design at 28 GHz", "authors": ["Hossein Rezaei", "Mehmet Emin Arslan", "George Yammine", "Niels Neumann", "Norman Franchi"], "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a transformative\ntechnology for electromagnetic (EM) wave manipulation, offering unprecedented\ncontrol over wave reflections compared to traditional metallic reflectors. By\nutilizing an array of tunable elements, RIS can steer and shape electromagnetic\nwaves to enhance signal quality in wireless communication and radar systems.\nHowever, practical implementations face significant challenges due to hardware\nlimitations and phase quantization errors. In this work, a 1-bit RIS prototype\noperating at 28 GHz is developed to experimentally evaluate the impact of\nhardware constraints on RIS performance. Unlike conventional studies that model\nRIS as an ideal phase-shift matrix, this study accounts for physical parameters\nthat influence the actual reflection pattern. In particular, the presence of\nspecular reflection due to hardware limitations is investigated. Additionally,\nthe effects of phase quantization errors, which stem from the discrete nature\nof RIS elements, are analyzed, and a genetic algorithm (GA)-based optimization\nis introduced to mitigate these errors. The proposed optimization strategy\neffectively reduces gain degradation at the desired angle caused by 1-bit\nquantization, enhancing the overall performance of RIS. The effectiveness of\nthe approach is validated through measurements, underscoring the importance of\nadvanced phase control techniques in improving the functionality of RIS.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.08930v1", "AI": {"title_translation": "28 GHz 1比特RIS设计中的硬件限制与优化方法", "tldr": "本文开发了一个28 GHz的1比特RIS原型，实验评估硬件限制和相位量化误差的影响，并引入基于遗传算法的优化来缓解这些误差，有效提升了RIS性能。", "motivation": "尽管可重构智能表面（RIS）在电磁波操控方面具有变革性潜力，但实际应用面临硬件限制和相位量化误差的重大挑战。传统研究常将RIS建模为理想相位移矩阵，忽略了物理参数对实际反射模式的影响。", "method": "开发了一个工作在28 GHz的1比特RIS原型，用于实验评估硬件约束（如镜面反射）和相位量化误差的影响。与传统研究不同，本研究考虑了影响实际反射模式的物理参数。此外，引入了一种基于遗传算法（GA）的优化方法来缓解相位量化误差。", "result": "研究发现硬件限制会导致镜面反射。提出的遗传算法优化策略有效降低了1比特量化引起的期望角度增益下降，提升了RIS的整体性能。该方法的有效性通过测量得到了验证。", "conclusion": "高级相位控制技术对于改善RIS的功能至关重要。", "translation": "可重构智能表面（RIS）已成为电磁（EM）波操控的一项变革性技术，与传统金属反射器相比，它能以前所未有的方式控制波反射。通过利用可调谐元件阵列，RIS可以引导和塑形电磁波，以提高无线通信和雷达系统中的信号质量。然而，由于硬件限制和相位量化误差，实际实现面临重大挑战。在这项工作中，开发了一个工作在28 GHz的1比特RIS原型，以实验评估硬件约束对RIS性能的影响。与将RIS建模为理想相位移矩阵的传统研究不同，本研究考虑了影响实际反射模式的物理参数。特别是，研究了由于硬件限制导致的镜面反射的存在。此外，分析了源于RIS元件离散性质的相位量化误差的影响，并引入了一种基于遗传算法（GA）的优化方法来缓解这些误差。所提出的优化策略有效地降低了1比特量化引起的期望角度增益下降，从而提高了RIS的整体性能。该方法的有效性通过测量得到了验证，强调了先进相位控制技术在改善RIS功能方面的重要性。", "summary": "本文开发并实验评估了一个28 GHz的1比特可重构智能表面（RIS）原型，以研究硬件限制（如镜面反射）和相位量化误差对RIS性能的影响。与理想模型不同，该研究考虑了影响实际反射模式的物理参数。为缓解1比特量化引起的增益下降，提出了一种基于遗传算法的优化策略，并通过测量验证了其有效性，强调了先进相位控制对提升RIS功能的重要性。", "keywords": "可重构智能表面, 硬件限制, 相位量化, 遗传算法, 28 GHz RIS", "comments": "本文的创新点在于实验性地评估了实际硬件限制（如镜面反射）和相位量化误差对RIS性能的影响，并提出了一种基于遗传算法的优化方法来弥补1比特量化带来的性能损失。这对于RIS的实际部署和性能优化具有重要指导意义，超越了传统理想模型的假设。"}}
{"id": "2506.08570", "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation", "authors": ["Or Tal", "Felix Kreuk", "Yossi Adi"], "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08570v1", "AI": {"title_translation": "自回归 vs. 流匹配：文本到音乐生成建模范式的比较研究", "tldr": "本研究系统比较了文本到音乐生成中两种主流建模范式：自回归和条件流匹配，以揭示它们在受控条件下的优缺点。", "motivation": "文本到音乐生成领域进展迅速，但现有模型在训练数据、建模范式和架构选择上差异很大，导致难以公平评估和找出影响性能的关键设计。本研究旨在隔离并分析建模范式的影响。", "method": "通过使用相同的数据集、训练配置和相似的骨干架构，从头开始训练所有模型，进行受控比较。评估维度包括生成质量、推理配置鲁棒性、可扩展性、文本和时间对齐条件依从性以及音频修复编辑能力。", "result": "研究揭示了自回归和条件流匹配两种范式各自的独特优势和局限性。", "conclusion": "该比较研究提供了可操作的见解，可以为未来文本到音乐生成系统的架构和训练决策提供信息。", "translation": "文本到音乐生成领域的最新进展使模型能够合成高质量的音乐片段、完整的作品，甚至响应细粒度的控制信号，例如和弦进行。最先进（SOTA）系统在许多维度上存在显著差异，例如训练数据集、建模范式和架构选择。这种多样性使得公平评估模型和确定哪些设计选择对性能影响最大变得复杂。虽然数据和架构等因素很重要，但在这项研究中，我们专门关注建模范式。我们进行了一项系统的实证分析，以隔离其影响，提供有关相关权衡和新兴行为的见解，这些见解可以指导未来的文本到音乐生成系统。具体来说，我们比较了两种可以说最常见的建模范式：自回归解码和条件流匹配。我们通过使用相同的数据集、训练配置和相似的骨干架构从头开始训练所有模型来进行受控比较。性能评估涵盖多个方面，包括生成质量、对推理配置的鲁棒性、可扩展性、对文本和时间对齐条件的遵循程度以及以音频修复形式出现的编辑能力。这项比较研究揭示了每种范式的独特优势和局限性，提供了可操作的见解，可以为文本到音乐生成不断发展的领域中的未来架构和训练决策提供信息。音频采样示例可在以下网址获取：https://huggingface.co/spaces/ortal1602/ARvsFM", "summary": "本文对文本到音乐生成领域中两种主流建模范式——自回归解码和条件流匹配进行了系统性的实证比较研究。鉴于现有系统在多方面差异导致评估困难，本研究通过严格控制数据集、训练配置和架构，旨在隔离建模范式对性能的影响。研究评估了生成质量、鲁棒性、可扩展性、条件依从性及编辑能力等多个维度，揭示了两种范式的各自优势和局限性，为未来文本到音乐系统的设计提供了宝贵指导。", "keywords": "文本到音乐生成, 自回归, 流匹配, 建模范式, 比较研究", "comments": "这篇论文的创新之处在于其严格的受控实验设计，专门隔离并比较了两种主流建模范式对文本到音乐生成性能的影响，这对于理解不同设计选择的实际效果至关重要。其重要性在于为该领域未来的架构和训练决策提供了具体且可操作的见解，有助于推动文本到音乐生成技术的发展。"}}
{"id": "2506.08034", "title": "Exploring Noncommutative Polynomial Equation Methods for Discrete-Time Quaternionic Control", "authors": ["Michael Sebek"], "summary": "We present new polynomial-based methods for discrete-time quaternionic\nsystems, highlighting how noncommutative multiplication modifies classical\ncontrol approaches. Defining quaternionic polynomials via a backward-shift\noperator, we examine left and right fraction representations of transfer\nfunctions, showing that right zeros correspond to similarity classes of\nquaternionic matrix right eigenvalues. We then propose a feedback design\nprocedure that generalizes pole placement to quaternions - a first approach\nusing a genuine quaternionic polynomial equation.", "comment": "5 pages, 1 figure, to be presented at the 33rd Mediterranean\n  Conference on Control and Automation, Tangier, Morocco, June 10-13, 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08034v1", "AI": {"title_translation": "探索离散时间四元数控制中的非交换多项式方程方法", "tldr": "本文提出了离散时间四元数系统的新多项式方法，通过反向移位算子定义四元数多项式，并展示了如何将经典的极点配置推广到四元数领域，这是首次使用真正的四元数多项式方程。", "motivation": "探索非交换乘法如何修改经典控制方法，并将极点配置推广到四元数领域，首次使用真正的四元数多项式方程。", "method": "提出了新的基于多项式的方法，通过反向移位算子定义四元数多项式，检查传递函数的左右分数表示，并提出了一个反馈设计程序。", "result": "证明了右零点对应于四元数矩阵右特征值的相似类，并提出了一个将极点配置推广到四元数的反馈设计程序，这是首次使用真正的四元数多项式方程的方法。", "conclusion": "成功地提出了一种使用真正的四元数多项式方程的反馈设计程序，将经典的极点配置方法推广到了四元数系统。", "translation": "我们为离散时间四元数系统提出了新的基于多项式的方法，强调非交换乘法如何修改经典控制方法。通过反向移位算子定义四元数多项式，我们检查了传递函数的左右分数表示，表明右零点对应于四元数矩阵右特征值的相似类。然后，我们提出了一种反馈设计程序，将极点配置推广到四元数——这是首次使用真正的四元数多项式方程的方法。", "summary": "本文针对离散时间四元数系统，提出了一种新的基于多项式的方法。研究通过反向移位算子定义四元数多项式，并分析了传递函数的左右分数表示，发现右零点与四元数矩阵右特征值的相似类相关。在此基础上，文章首次提出了一种利用真正的四元数多项式方程，将经典极点配置方法推广到四元数领域的反馈设计过程。", "keywords": "离散时间系统, 四元数控制, 多项式方程, 极点配置, 非交换乘法", "comments": "这篇论文的创新点在于首次提出了使用真正的四元数多项式方程来实现极点配置，为离散时间四元数系统的控制提供了一种新颖且重要的方法，克服了非交换乘法带来的挑战。"}}
{"id": "2506.09004", "title": "Improving Online Bin Covering with Little Advice", "authors": ["Andrej Brodnik", "Bengt J. Nilsson", "Gordana Vujović"], "summary": "The online bin covering problem is: given an input sequence of items find a\nplacement of the items in the maximum number of bins such that the sum of the\nitems' sizes in each bin is at least~1. Boyar~{\\em et~al}.\\@~\\cite{boyar2021}\npresent a strategy that with $O(\\log \\log n)$ bits of advice, where $n$ is the\nlength of the input sequence, achieves a competitive ratio of\n$8/15\\approx0.5333\\ldots$. We show that with a strengthened analysis and some\nminor improvements, the same strategy achieves the significantly improved\ncompetitive ratio of~$135/242\\approx0.5578\\ldots$, still using $O(\\log \\log n)$\nbits of advice.", "comment": "13 pages. This is an extended version of an article published in\n  \"35th International Workshop on Combinatorial Algorithms. IWOCA 2024.\"\n  Lecture Notes in Computer Science, vol 14764. Springer Verlag", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.09004v1", "AI": {"title_translation": "在线箱子覆盖问题的小建议改进", "tldr": "通过对现有策略进行强化分析和微小改进，将在线箱子覆盖问题的竞争比从约0.5333提高到约0.5578，且仍使用$O(\\log \\log n)$的建议位。", "motivation": "旨在改进在线箱子覆盖问题中现有策略的竞争比。", "method": "对Boyar等人提出的策略进行了强化分析和一些微小改进。", "result": "在使用$O(\\log \\log n)$建议位的情况下，将竞争比从$8/15 \\approx 0.5333\\ldots$提高到$135/242 \\approx 0.5578\\ldots$。", "conclusion": "通过强化分析和改进，可以在相同建议位下显著提高在线箱子覆盖问题的竞争比。", "translation": "在线箱子覆盖问题是：给定一个物品的输入序列，找到将物品放置在最大数量的箱子中的方法，使得每个箱子中物品大小的总和至少为1。Boyar等人提出了一种策略，使用$O(\\log \\log n)$位的建议（其中$n$是输入序列的长度），实现了$8/15 \\approx 0.5333\\ldots$的竞争比。我们表明，通过强化分析和一些微小改进，相同的策略可以实现显著提高的竞争比$135/242 \\approx 0.5578\\ldots$，并且仍然使用$O(\\log \\log n)$位的建议。", "summary": "本文研究在线箱子覆盖问题，目标是将物品放置在尽可能多的箱子中，使每个箱子的物品总和至少为1。在Boyar等人现有策略的基础上，通过强化分析和微小改进，作者成功地在保持相同$O(\\log \\log n)$建议位的前提下，将该策略的竞争比从约0.5333显著提高到约0.5578。", "keywords": "在线箱子覆盖, 竞争比, 算法改进, 建议位, 强化分析", "comments": "本文的创新点在于在不增加额外信息量（建议位）的情况下，通过对现有算法的深入分析和优化，显著提升了其性能，这对于在线算法的设计和分析具有重要意义。"}}
{"id": "2506.08334", "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos", "authors": ["Weikun Peng", "Jun Lv", "Cewu Lu", "Manolis Savva"], "summary": "Articulated objects are prevalent in daily life. Understanding their\nkinematic structure and reconstructing them have numerous applications in\nembodied AI and robotics. However, current methods require carefully captured\ndata for training or inference, preventing practical, scalable, and\ngeneralizable reconstruction of articulated objects. We focus on reconstruction\nof an articulated object from a casually captured RGBD video shot with a\nhand-held camera. A casually captured video of an interaction with an\narticulated object is easy to acquire at scale using smartphones. However, this\nsetting is quite challenging, as the object and camera move simultaneously and\nthere are significant occlusions as the person interacts with the object. To\ntackle these challenges, we introduce a coarse-to-fine framework that infers\njoint parameters and segments movable parts of the object from a dynamic RGBD\nvideo. To evaluate our method under this new setting, we build a 20$\\times$\nlarger synthetic dataset of 784 videos containing 284 objects across 11\ncategories. We compare our approach with existing methods that also take video\nas input. Experiments show that our method can reconstruct synthetic and real\narticulated objects across different categories from dynamic RGBD videos,\noutperforming existing methods significantly.", "comment": "Project website can be found at\n  https://3dlg-hcvc.github.io/video2articulation/", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.08334v1", "AI": {"title_translation": "从随意捕获的RGBD视频中重建通用铰接物体", "tldr": "本文提出了一种从随意捕获的RGBD视频中重建铰接物体的粗到精框架，其性能优于现有方法。", "motivation": "当前铰接物体重建方法需要精心捕获的数据进行训练或推理，这阻碍了铰接物体实用、可扩展和通用化的重建。本文旨在解决从手持相机随意捕获的RGBD视频中重建铰接物体的挑战，在这种情况下，物体和相机同时移动，并且在人与物体交互时存在明显的遮挡。", "method": "本文引入了一个粗到精的框架，用于从动态RGBD视频中推断关节参数并分割物体的可移动部分。为了在这种新环境下评估该方法，研究人员构建了一个20倍大的合成数据集，包含11个类别的284个物体共784个视频。", "result": "实验表明，该方法可以从动态RGBD视频中重建不同类别的合成和真实铰接物体，并且显著优于现有方法。", "conclusion": "该方法成功地从随意捕获的动态RGBD视频中重建了铰接物体，有效解决了现有方法的局限性，并显著超越了现有方法，为具身AI和机器人技术提供了更实用和可扩展的解决方案。", "translation": "铰接物体在日常生活中普遍存在。理解它们的运动学结构并对其进行重建在具身AI和机器人技术中具有众多应用。然而，当前方法需要精心捕获的数据进行训练或推理，这阻碍了铰接物体实用、可扩展和通用化的重建。我们专注于从手持相机随意捕获的RGBD视频中重建铰接物体。使用智能手机可以大规模地轻松获取与铰接物体交互的随意捕获视频。然而，这种设置非常具有挑战性，因为物体和相机同时移动，并且在人与物体交互时存在明显的遮挡。为了应对这些挑战，我们引入了一个粗到精的框架，该框架从动态RGBD视频中推断关节参数并分割物体的可移动部分。为了在这种新环境下评估我们的方法，我们构建了一个20倍大的合成数据集，包含11个类别的284个物体共784个视频。我们将我们的方法与也以视频作为输入的现有方法进行了比较。实验表明，我们的方法可以从动态RGBD视频中重建不同类别的合成和真实铰接物体，并且显著优于现有方法。", "summary": "本文提出了一种新颖的粗到精框架，用于从手持相机随意捕获的RGBD视频中重建铰接物体。针对当前方法对精心捕获数据的高要求以及随意捕获视频中物体与相机同步移动和严重遮挡的挑战，该框架能够推断关节参数并分割物体可移动部分。为验证其有效性，研究人员构建了一个大型合成数据集。实验结果表明，该方法在重建合成和真实铰接物体方面显著优于现有方法，展现出在具身AI和机器人领域的应用潜力。", "keywords": "铰接物体重建, RGBD视频, 随意捕获, 粗到精框架, 运动学结构", "comments": "该论文的创新之处在于其提出的粗到精框架，能够有效应对随意捕获RGBD视频中存在的复杂挑战，如同步运动和严重遮挡。这对于实现铰接物体的实用、可扩展和通用化重建具有重要意义，尤其是在智能手机普及和具身AI及机器人应用日益增长的背景下。构建大规模合成数据集也为该领域的研究提供了宝贵的资源。"}}
{"id": "2506.08783", "title": "syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum", "authors": ["Lukas Kammerer", "Deaglan J. Bartlett", "Gabriel Kronberger", "Harry Desmond", "Pedro G. Ferreira"], "summary": "Baryonic physics has a considerable impact on the distribution of matter in\nour Universe on scales probed by current and future cosmological surveys,\nacting as a key systematic in such analyses. We seek simple symbolic\nparametrisations for the impact of baryonic physics on the matter power\nspectrum for a range of physically motivated models, as a function of\nwavenumber, redshift, cosmology, and parameters controlling the baryonic\nfeedback. We use symbolic regression to construct analytic approximations for\nthe ratio of the matter power spectrum in the presence of baryons to that\nwithout such effects. We obtain separate functions of each of four distinct\nsub-grid prescriptions of baryonic physics from the CAMELS suite of\nhydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as\nwell as for a baryonification algorithm. We also provide functions which\ndescribe the uncertainty on these predictions, due to both the stochastic\nnature of baryonic physics and the errors on our fits. The error on our\napproximations to the hydrodynamical simulations is comparable to the sample\nvariance estimated through varying initial conditions, and our baryonification\nexpression has a root mean squared error of better than one percent, although\nthis increases on small scales. These errors are comparable to those of\nprevious numerical emulators for these models. Our expressions are enforced to\nhave the physically correct behaviour on large scales and at high redshift. Due\nto their analytic form, we are able to directly interpret the impact of varying\ncosmology and feedback parameters, and we can identify parameters which have\nlittle to no effect. Each function is based on a different implementation of\nbaryonic physics, and can therefore be used to discriminate between these\nmodels when applied to real data. We provide publicly available code for all\nsymbolic approximations found.", "comment": "14 pages, 6 figures. Submitted to A&A", "cate": "astro-ph.CO", "url": "http://arxiv.org/abs/2506.08783v1", "AI": {"title_translation": "syren-baryon: 重子对物质功率谱影响的解析模拟器", "tldr": "该研究使用符号回归为重子对物质功率谱的影响构建了分析近似，可用于区分不同的重子物理模型。", "motivation": "重子物理对宇宙中物质分布有显著影响，是当前和未来宇宙学调查中的一个关键系统误差，需要简单的符号参数化来描述其影响。", "method": "本研究使用符号回归为重子存在与否时物质功率谱的比率构建解析近似。它针对CAMELS流体动力学模拟（Astrid, IllustrisTNG, SIMBA, Swift-EAGLE）中的四种不同子网格重子物理方案以及一种重子化算法分别获取了函数，并提供了描述预测不确定性的函数。", "result": "对流体动力学模拟的近似误差与通过改变初始条件估计的样本方差相当；重子化表达式的均方根误差优于1%，但在小尺度上会增加。这些误差与之前数值模拟器的误差相当。所得到的表达式在大尺度和高红移处具有物理上正确的行为，其解析形式允许直接解释宇宙学和反馈参数变化的影响，并识别影响不大的参数。", "conclusion": "构建的解析函数基于不同的重子物理实现，可用于在应用于真实数据时区分这些模型。所有符号近似的代码均已公开可用。", "translation": "重子物理对我们宇宙中物质的分布在当前和未来宇宙学调查探测的尺度上具有相当大的影响，是此类分析中的一个关键系统误差。我们寻求对重子物理对物质功率谱影响的简单符号参数化，作为波数、红移、宇宙学和控制重子反馈参数的函数，适用于一系列具有物理动机的模型。我们使用符号回归构建了在存在重子效应时物质功率谱与无重子效应时物质功率谱比率的解析近似。我们从CAMELS流体动力学模拟套件（Astrid、IllustrisTNG、SIMBA和Swift-EAGLE）中获得了四种不同子网格重子物理方案以及一种重子化算法的单独函数。我们还提供了描述这些预测不确定性的函数，这既是由于重子物理的随机性，也是由于我们拟合的误差。我们对流体动力学模拟的近似误差与通过改变初始条件估计的样本方差相当，我们的重子化表达式的均方根误差优于百分之一，尽管在小尺度上会增加。这些误差与这些模型之前数值模拟器的误差相当。我们的表达式在大尺度和高红移处被强制具有物理上正确的行为。由于它们的解析形式，我们能够直接解释宇宙学和反馈参数变化的影响，并且我们可以识别影响很小或没有影响的参数。每个函数都基于重子物理的不同实现，因此在应用于真实数据时可以用来区分这些模型。我们提供了所有找到的符号近似的公开可用代码。", "summary": "本研究利用符号回归为重子对物质功率谱的影响构建了分析近似，以解决其在宇宙学调查中的关键系统误差问题。研究针对多种流体动力学模拟和重子化算法提供了独立的函数，并量化了预测的不确定性。这些解析表达式的误差与现有数值模拟器相当，且在大尺度和高红移处表现出正确的物理行为，能够帮助解释宇宙学和反馈参数的影响，并区分不同的重子物理模型。", "keywords": "重子物理, 物质功率谱, 解析模拟器, 符号回归, 宇宙学", "comments": "这项工作通过提供重子效应的解析模拟器，显著简化了宇宙学分析中对重子反馈建模的复杂性。其解析形式允许更直接的物理解释和参数敏感性分析，这对于从真实数据中区分不同的重子物理模型至关重要，是理解宇宙大尺度结构形成的关键一步。"}}
{"id": "2506.08314", "title": "Rule-Assisted Attribute Embedding", "authors": ["Sibo Zhao", "Michael Bewong", "Selasi Kwashie", "Junwei Hu", "Zaiwen Feng"], "summary": "Recommendation systems often overlook the rich attribute information embedded\nin property graphs, limiting their effectiveness. Existing graph convolutional\nnetwork (GCN) models either ignore attributes or rely on simplistic <user,\nitem, attribute> triples, failing to capture deeper semantic structures. We\npropose RAE (Rule- Assisted Approach for Attribute Embedding), a novel method\nthat improves recommendations by mining semantic rules from property graphs to\nguide attribute embedding. RAE performs rule-based random walks to generate\nenriched attribute representations, which are integrated into GCNs. Experiments\non real-world datasets (BlogCatalog, Flickr) show that RAE outperforms\nstate-of-the-art baselines by 10.6% on average in Recall@20 and NDCG@20. RAE\nalso demonstrates greater robustness to sparse data and missing attributes,\nhighlighting the value of leveraging structured attribute information in\nrecommendation tasks.", "comment": "ECML-PKDD2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08314v1", "AI": {"title_translation": "规则辅助属性嵌入", "tldr": "RAE通过从属性图中挖掘语义规则来指导属性嵌入，从而改进推荐系统，并在稀疏数据下表现出更好的鲁棒性。", "motivation": "推荐系统通常忽视属性图中嵌入的丰富属性信息，限制了它们的有效性。现有图卷积网络（GCN）模型要么忽略属性，要么依赖简单的<用户、项目、属性>三元组，未能捕获更深层的语义结构。", "method": "本文提出RAE（Rule-Assisted Approach for Attribute Embedding）方法，通过从属性图中挖掘语义规则来指导属性嵌入。RAE执行基于规则的随机游走以生成丰富的属性表示，并将其集成到GCN中。", "result": "在真实世界数据集（BlogCatalog, Flickr）上的实验表明，RAE在Recall@20和NDCG@20方面平均优于现有最佳基线10.6%。RAE还对稀疏数据和缺失属性表现出更强的鲁棒性。", "conclusion": "利用结构化属性信息在推荐任务中具有重要价值，RAE通过规则辅助的属性嵌入显著提升了推荐性能并增强了对数据稀疏性的鲁棒性。", "translation": "推荐系统经常忽视属性图中嵌入的丰富属性信息，限制了它们的有效性。现有的图卷积网络（GCN）模型要么忽略属性，要么依赖于简单的<用户、项目、属性>三元组，未能捕获更深层的语义结构。我们提出了RAE（Rule-Assisted Approach for Attribute Embedding），一种通过从属性图中挖掘语义规则来指导属性嵌入，从而改进推荐的新方法。RAE执行基于规则的随机游走以生成丰富的属性表示，这些表示被集成到GCN中。在真实世界数据集（BlogCatalog、Flickr）上的实验表明，RAE在Recall@20和NDCG@20方面平均优于最先进的基线10.6%。RAE还表现出对稀疏数据和缺失属性更强的鲁棒性，突出了在推荐任务中利用结构化属性信息的价值。", "summary": "本文提出了一种名为RAE（Rule-Assisted Approach for Attribute Embedding）的新方法，旨在通过从属性图中挖掘语义规则来指导属性嵌入，从而改进推荐系统。RAE通过规则辅助的随机游走生成增强的属性表示，并将其整合到图卷积网络（GCN）中。实验证明，RAE在推荐性能上优于现有方法，并对数据稀疏性表现出更强的鲁棒性，强调了结构化属性信息在推荐任务中的重要性。", "keywords": "属性嵌入, 推荐系统, 图卷积网络, 语义规则, 随机游走", "comments": "本文的创新点在于利用语义规则来指导属性嵌入，这比传统方法更能捕获深层语义结构，尤其在处理稀疏数据方面表现出色，对提升推荐系统性能具有重要意义。"}}
{"id": "2506.08308", "title": "An efficient Fourier spectral algorithm for the Bogoliubov-de Gennes excitation eigenvalue problem", "authors": ["Yu Li", "Zhixuan Li", "Manting Xie", "Yong Zhang"], "summary": "In this paper, we propose an efficient Fourier spectral algorithm for an\neigenvalue problem, that is, the Bogoliubov-de Gennes (BdG) equation arsing\nfrom spin-1 Bose-Einstein condensates (BEC) to describe the\nelementary/collective excitations around the mean-field ground state. The BdG\nequation is essentially a constrained eigenvalue/eigenfunction system. Firstly,\nwe investigate its analytical properties, including exact eigenpairs,\ngeneralized nullspace, and bi-orthogonality of eigenspaces. Secondly, by\ncombining the standard Fourier spectral method for spatial discretization and a\nstable Gram-Schmidt bi-orthogonal algorithm, we develop a subspace iterative\nsolver for such a large-scale dense eigenvalue problem, and it proves to be\nnumerically stable, efficient, and accurate. Our solver is matrix-free and the\noperator-function evaluation is accelerated by discrete Fast Fourier Transform\n(FFT) with almost optimal efficiency. Therefore, it is memory-friendly and\nefficient for large-scale problems. Furthermore, we give a rigorous and\ndetailed numerical analysis on the stability and spectral convergence. Finally,\nwe present extensive numerical results to illustrate the spectral accuracy and\nefficiency, and investigate the excitation spectrum and Bogoliubov amplitudes\naround the ground state in 1-3 spatial dimensions.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08308v1", "AI": {"title_translation": "Bogoliubov-de Gennes激发特征值问题的傅里叶谱算法", "tldr": "本文提出了一种高效的傅里叶谱算法，用于解决描述自旋-1玻色-爱因斯坦凝聚体中激发态的Bogoliubov-de Gennes (BdG)方程，该算法具有数值稳定性、高效性和准确性，并且适用于大规模问题。", "motivation": "解决描述自旋-1玻色-爱因斯坦凝聚体（BEC）中基态周围基本/集体激发的Bogoliubov-de Gennes（BdG）方程的特征值问题。", "method": "首先，研究了BdG方程的解析性质，包括精确特征对、广义零空间和特征空间的双正交性。其次，结合标准傅里叶谱方法进行空间离散化和稳定的Gram-Schmidt双正交算法，开发了一种子空间迭代求解器。该求解器是无矩阵的，并通过离散快速傅里叶变换（FFT）加速了算子函数评估，并进行了严格的数值分析。", "result": "开发的求解器被证明是数值稳定、高效和准确的。它无矩阵且内存友好，通过FFT加速，对于大规模问题具有近乎最优的效率。数值结果证明了其谱精度和效率，并成功研究了1-3空间维度中基态周围的激发谱和Bogoliubov振幅。", "conclusion": "本文提出了一种高效的傅里叶谱算法，用于解决Bogoliubov-de Gennes激发特征值问题，该算法在数值稳定性、效率和准确性方面表现出色，并适用于大规模计算。", "translation": "本文提出了一种高效的傅里叶谱算法，用于解决一个特征值问题，即描述自旋-1玻色-爱因斯坦凝聚体（BEC）中平均场基态周围基本/集体激发的Bogoliubov-de Gennes（BdG）方程。BdG方程本质上是一个受约束的特征值/特征函数系统。首先，我们研究了它的解析性质，包括精确特征对、广义零空间和特征空间的双正交性。其次，通过结合用于空间离散化的标准傅里叶谱方法和稳定的Gram-Schmidt双正交算法，我们开发了一种用于解决此类大规模密集特征值问题的子空间迭代求解器，并且它被证明是数值稳定、高效和准确的。我们的求解器是无矩阵的，并且通过离散快速傅里叶变换（FFT）加速了算子函数评估，具有近乎最优的效率。因此，它对内存友好，并且对于大规模问题是高效的。此外，我们对稳定性和谱收敛性进行了严格而详细的数值分析。最后，我们展示了大量的数值结果，以说明谱精度和效率，并研究了1-3空间维度中基态周围的激发谱和Bogoliubov振幅。", "summary": "本文提出了一种高效的傅里叶谱算法，用于解决自旋-1玻色-爱因斯坦凝聚体中描述激发态的Bogoliubov-de Gennes (BdG) 特征值问题。该方法首先分析了BdG方程的解析性质，然后结合傅里叶谱方法和稳定的Gram-Schmidt双正交算法，开发了一种无矩阵、内存友好的子空间迭代求解器。通过离散快速傅里叶变换（FFT）加速，该算法在数值上稳定、高效、准确，并能有效处理大规模问题。广泛的数值结果验证了其谱精度和效率，并成功应用于1-3维空间中激发谱和Bogoliubov振幅的研究。", "keywords": "傅里叶谱算法, Bogoliubov-de Gennes方程, 特征值问题, 玻色-爱因斯坦凝聚体, 子空间迭代求解器", "comments": "该论文提出了一种创新的、无矩阵且内存友好的傅里叶谱算法来解决复杂的Bogoliubov-de Gennes方程，这对于处理大规模问题具有重要意义。通过结合FFT加速，该方法在效率和准确性方面取得了显著提升，为研究自旋-1玻色-爱因斯坦凝聚体的激发特性提供了强大的工具。"}}
{"id": "2506.08781", "title": "Lightweight and High-Throughput Secure Logging for Internet of Things and Cold Cloud Continuum", "authors": ["Saif E. Nouma", "Attila A. Yavuz"], "summary": "The growing deployment of resource-limited Internet of Things (IoT) devices\nand their expanding attack surfaces demand efficient and scalable security\nmechanisms. System logs are vital for the trust and auditability of IoT, and\noffloading their maintenance to a Cold Storage-as-a-Service (Cold-STaaS)\nenhances cost-effectiveness and reliability. However, existing cryptographic\nlogging solutions either burden low-end IoT devices with heavy computation or\ncreate verification delays and storage inefficiencies at Cold-STaaS. There is a\npressing need for cryptographic primitives that balance security, performance,\nand scalability across IoT-Cold-STaaS continuum.\n  In this work, we present Parallel Optimal Signatures for Secure Logging\n(POSLO), a novel digital signature framework that, to our knowledge, is the\nfirst to offer constant-size signatures and public keys, near-optimal signing\nefficiency, and tunable fine-to-coarse-grained verification for log auditing.\nPOSLO achieves these properties through efficient randomness management,\nflexible aggregation, and multiple algorithmic instantiations. It also\nintroduces a GPU-accelerated batch verification framework that exploits\nhomomorphic signature aggregation to deliver ultra-fast performance. For\nexample, POSLO can verify 231 log entries per second on a mid-range consumer\nGPU (NVIDIA GTX 3060) while being significantly more compact than\nstate-of-the-art. POSLO also preserves signer-side efficiency, offering\nsubstantial battery savings for IoT devices, and is well-suited for the\nIoT-Cold-STaaS ecosystem.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08781v1", "AI": {"title_translation": "物联网和冷云连续体的轻量级高吞吐量安全日志记录", "tldr": "POSLO是一种新型数字签名框架，为物联网-冷存储提供了轻量级、高吞吐量的安全日志记录，具有恒定大小的签名和公钥，以及利用GPU加速的超快验证。", "motivation": "资源受限的物联网设备日益普及，其攻击面不断扩大，需要高效可扩展的安全机制。系统日志对于物联网的信任和可审计性至关重要。将日志维护卸载到冷存储即服务(Cold-STaaS)可提高成本效益和可靠性。然而，现有加密日志解决方案要么给低端物联网设备带来沉重计算负担，要么在Cold-STaaS端造成验证延迟和存储效率低下。因此，迫切需要一种在物联网-冷存储连续体中平衡安全性、性能和可扩展性的加密原语。", "method": "本文提出了并行最优签名安全日志（POSLO），这是一种新颖的数字签名框架，首次提供恒定大小的签名和公钥、接近最优的签名效率以及用于日志审计的可调细粒度到粗粒度验证。POSLO通过高效的随机性管理、灵活的聚合和多种算法实例化来实现这些特性。它还引入了一个GPU加速的批处理验证框架，利用同态签名聚合提供超高速性能。", "result": "POSLO可以在中端消费级GPU（NVIDIA GTX 3060）上每秒验证231条日志条目，同时比现有技术更紧凑。POSLO还保持了签名方效率，为物联网设备节省了大量电池，并且非常适合物联网-冷存储生态系统。", "conclusion": "POSLO是一种为物联网和冷存储环境设计的、高效且可扩展的数字签名框架，它解决了现有安全日志记录方案的性能和效率问题，为物联网设备提供了重要的电池节省和高速的日志验证能力。", "translation": "资源受限的物联网（IoT）设备日益普及及其不断扩大的攻击面，要求高效且可扩展的安全机制。系统日志对于物联网的信任和可审计性至关重要，将其维护卸载到冷存储即服务（Cold-STaaS）可以提高成本效益和可靠性。然而，现有加密日志解决方案要么给低端物联网设备带来沉重计算负担，要么在Cold-STaaS端造成验证延迟和存储效率低下。因此，迫切需要一种在物联网-冷存储连续体中平衡安全性、性能和可扩展性的加密原语。\n在这项工作中，我们提出了并行最优签名安全日志（POSLO），这是一种新颖的数字签名框架，据我们所知，它是第一个提供恒定大小签名和公钥、接近最优的签名效率以及用于日志审计的可调细粒度到粗粒度验证的框架。POSLO通过高效的随机性管理、灵活的聚合和多种算法实例化来实现这些特性。它还引入了一个GPU加速的批处理验证框架，利用同态签名聚合提供超高速性能。例如，POSLO可以在中端消费级GPU（NVIDIA GTX 3060）上每秒验证231条日志条目，同时比现有技术显著更紧凑。POSLO还保持了签名方效率，为物联网设备节省了大量电池，并且非常适合物联网-冷存储生态系统。", "summary": "本文提出了一种名为并行最优签名安全日志（POSLO）的新型数字签名框架，旨在解决物联网设备和冷存储服务（Cold-STaaS）之间安全日志记录的挑战。现有解决方案存在计算负担重或验证效率低的问题。POSLO通过提供恒定大小的签名和公钥、接近最优的签名效率以及利用GPU加速的同态签名聚合实现超快批处理验证，从而平衡了安全性、性能和可扩展性。实验表明，POSLO在NVIDIA GTX 3060 GPU上每秒可验证231条日志，同时比现有技术更紧凑，并显著节省物联网设备的电池消耗。", "keywords": "安全日志, 物联网, 冷存储, 数字签名, GPU加速", "comments": "POSLO的创新之处在于其恒定大小的签名和公钥，以及利用GPU加速的批处理验证框架，这显著提升了物联网和冷存储环境中安全日志记录的性能和效率。它解决了现有方案在资源受限设备上的计算负担和在云存储端的验证延迟问题，为物联网安全审计提供了重要的实用价值。"}}
{"id": "2506.08606", "title": "RE-oriented Model Development with LLM Support and Deduction-based Verification", "authors": ["Radoslaw Klimek"], "summary": "The requirements engineering (RE) phase is pivotal in developing high-quality\nsoftware. Integrating advanced modelling techniques with large language models\n(LLMs) and formal verification in a logical style can significantly enhance\nthis process. We propose a comprehensive framework that focuses on specific\nUnified Modelling Language (UML) diagrams for preliminary system development.\nThis framework offers visualisations at various modelling stages and seamlessly\nintegrates large language models and logical reasoning engines. The behavioural\nmodels generated with the assistance of LLMs are automatically translated into\nformal logical specifications. Deductive formal verification ensures that\nlogical requirements and interrelations between software artefacts are\nthoroughly addressed. Ultimately, the framework facilitates the automatic\ngeneration of program skeletons, streamlining the transition from design to\nimplementation.", "comment": "The paper has been peer-reviewed and accepted for publication to the\n  1st International Workshop on Artificial Intelligence for Integrated\n  Development Environments (AI-IDE) of the 33rd ACM Symposium on the\n  Foundations of Software Engineering (FSE '25), June 23--27, 2025, Trondheim,\n  Norway", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08606v1", "AI": {"title_translation": "面向需求工程的模型开发：结合大型语言模型支持与演绎验证", "tldr": "本文提出一个结合大型语言模型（LLMs）和演绎验证的框架，以增强需求工程阶段的软件建模，实现从设计到实现阶段的自动化过渡。", "motivation": "需求工程（RE）阶段对于开发高质量软件至关重要。将先进建模技术与大型语言模型（LLMs）和形式化验证相结合，可以显著提升这一过程。", "method": "我们提出了一个全面的框架，专注于特定UML图以进行初步系统开发。该框架提供不同建模阶段的可视化，并无缝集成大型语言模型和逻辑推理引擎。LLM辅助生成的行为模型会自动转换为形式化逻辑规范。演绎形式化验证确保逻辑需求和软件构件之间的相互关系得到彻底处理。", "result": "该框架最终促进了程序骨架的自动生成，从而简化了从设计到实现的过渡。", "conclusion": "该框架通过结合LLM支持和演绎验证，显著增强了需求工程阶段的模型开发，实现了设计到实现的自动化过渡，从而提高了软件质量。", "translation": "需求工程（RE）阶段对于开发高质量软件至关重要。将先进建模技术与大型语言模型（LLMs）和形式化验证以逻辑方式相结合，可以显著增强这一过程。我们提出了一个全面的框架，专注于特定统一建模语言（UML）图以进行初步系统开发。该框架提供不同建模阶段的可视化，并无缝集成大型语言模型和逻辑推理引擎。在LLMs协助下生成的行为模型会自动转换为形式化逻辑规范。演绎形式化验证确保逻辑需求和软件构件之间的相互关系得到彻底处理。最终，该框架促进了程序骨架的自动生成，从而简化了从设计到实现的过渡。", "summary": "本文提出了一个面向需求工程（RE）的模型开发框架，旨在结合大型语言模型（LLMs）的支持和演绎验证来提升软件开发质量。该框架利用特定UML图进行初步系统开发，提供可视化功能，并无缝集成LLMs和逻辑推理引擎。LLM生成的行为模型被自动转换为形式化逻辑规范，并通过演绎验证确保需求的正确性。最终，该框架能够自动生成程序骨架，简化从设计到实现的流程。", "keywords": "需求工程, 大型语言模型, 演绎验证, 软件建模, UML", "comments": "该论文的创新点在于将大型语言模型与形式化验证技术相结合，应用于需求工程阶段的软件建模。这种结合有望显著提高模型开发的效率和质量，并自动化从设计到实现的部分过程。其重要性体现在提升软件开发早期阶段的准确性和一致性，减少后期错误。"}}
{"id": "2506.08434", "title": "Attention-based Learning for 3D Informative Path Planning", "authors": ["Rui Zhao", "Xingjian Zhang", "Yuhong Cao", "Yizhuo Wang", "Guillaume Sartoretti"], "summary": "In this work, we propose an attention-based deep reinforcement learning\napproach to address the adaptive informative path planning (IPP) problem in 3D\nspace, where an aerial robot equipped with a downward-facing sensor must\ndynamically adjust its 3D position to balance sensing footprint and accuracy,\nand finally obtain a high-quality belief of an underlying field of interest\nover a given domain (e.g., presence of specific plants, hazardous gas,\ngeological structures, etc.). In adaptive IPP tasks, the agent is tasked with\nmaximizing information collected under time/distance constraints, continuously\nadapting its path based on newly acquired sensor data. To this end, we leverage\nattention mechanisms for their strong ability to capture global spatial\ndependencies across large action spaces, allowing the agent to learn an\nimplicit estimation of environmental transitions. Our model builds a contextual\nbelief representation over the entire domain, guiding sequential movement\ndecisions that optimize both short- and long-term search objectives.\nComparative evaluations against state-of-the-art planners demonstrate that our\napproach significantly reduces environmental uncertainty within constrained\nbudgets, thus allowing the agent to effectively balance exploration and\nexploitation. We further show our model generalizes well to environments of\nvarying sizes, highlighting its potential for many real-world applications.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08434v1", "AI": {"title_translation": "基于注意力的3D信息路径规划学习", "tldr": "本文提出了一种基于注意力的深度强化学习方法，用于解决3D空间中的自适应信息路径规划问题，通过动态调整机器人位置来最大化信息收集并降低环境不确定性。", "motivation": "在3D空间中，空中机器人需要自适应地调整其3D位置，以平衡感知覆盖范围和精度，并最终获得感兴趣区域的高质量置信度。传统的自适应信息路径规划(IPP)任务要求智能体在时间和距离限制下最大化信息收集，并根据新获取的传感器数据持续调整路径。", "method": "本文提出了一种基于注意力的深度强化学习方法来解决3D自适应信息路径规划问题。该方法利用注意力机制捕捉大动作空间中的全局空间依赖性，从而使智能体学习环境转换的隐式估计。模型构建了一个覆盖整个域的上下文置信度表示，指导优化短期和长期搜索目标的序贯移动决策。", "result": "与最先进的规划器进行比较评估表明，该方法在受限预算内显著降低了环境不确定性，从而使智能体能够有效地平衡探索和利用。此外，该模型对不同大小的环境具有良好的泛化能力。", "conclusion": "本文提出的基于注意力的深度强化学习方法能够有效解决3D空间中的自适应信息路径规划问题，显著降低环境不确定性，并展现出良好的泛化能力，在多种实际应用中具有潜力。", "translation": "在这项工作中，我们提出了一种基于注意力的深度强化学习方法来解决3D空间中的自适应信息路径规划（IPP）问题，其中配备向下传感器的空中机器人必须动态调整其3D位置，以平衡感知覆盖范围和精度，并最终获得给定领域（例如，特定植物的存在、有害气体、地质结构等）中感兴趣底层场的高质量置信度。在自适应IPP任务中，智能体的任务是在时间/距离约束下最大化收集的信息，并根据新获取的传感器数据持续调整其路径。为此，我们利用注意力机制强大的能力来捕获大动作空间中的全局空间依赖性，从而使智能体能够学习环境转换的隐式估计。我们的模型在整个域上构建了一个上下文置信度表示，指导优化短期和长期搜索目标的序贯移动决策。与最先进的规划器进行比较评估表明，我们的方法在受限预算内显著降低了环境不确定性，从而使智能体能够有效地平衡探索和利用。我们进一步表明，我们的模型对不同大小的环境具有良好的泛化能力，突出了其在许多实际应用中的潜力。", "summary": "本文提出了一种基于注意力的深度强化学习方法，用于解决3D空间中空中机器人的自适应信息路径规划问题。该方法利用注意力机制捕捉全局空间依赖性，构建上下文置信度表示，以优化短期和长期搜索目标。实验结果表明，该方法在受限预算下显著降低了环境不确定性，有效平衡了探索与利用，并对不同大小的环境具有良好的泛化能力。", "keywords": "注意力机制, 深度强化学习, 3D信息路径规划, 自适应, 环境不确定性", "comments": "本文的创新点在于将注意力机制引入3D信息路径规划，以有效捕捉大动作空间中的全局空间依赖性，从而提升了自适应决策的能力。其重要性在于为空中机器人在复杂3D环境中进行高效信息收集提供了一种新颖且有效的解决方案，特别是在平衡感知覆盖和精度方面表现出色。该方法在降低环境不确定性和泛化能力上的表现，预示着其在实际应用中具有广阔前景。"}}
{"id": "2506.08194", "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra", "authors": ["Mateusz Michalkiewicz", "Anekha Sokhal", "Tadeusz Michalkiewicz", "Piotr Pawlikowski", "Mahsa Baktashmotlagh", "Varun Jampani", "Guha Balakrishnan"], "summary": "Monocular 3D reconstruction methods and vision-language models (VLMs)\ndemonstrate impressive results on standard benchmarks, yet their true\nunderstanding of geometric properties remains unclear. We introduce GIQ , a\ncomprehensive benchmark specifically designed to evaluate the geometric\nreasoning capabilities of vision and vision-language foundation models. GIQ\ncomprises synthetic and real-world images of 224 diverse polyhedra - including\nPlatonic, Archimedean, Johnson, and Catalan solids, as well as stellations and\ncompound shapes - covering varying levels of complexity and symmetry. Through\nsystematic experiments involving monocular 3D reconstruction, 3D symmetry\ndetection, mental rotation tests, and zero-shot shape classification tasks, we\nreveal significant shortcomings in current models. State-of-the-art\nreconstruction algorithms trained on extensive 3D datasets struggle to\nreconstruct even basic geometric forms accurately. While foundation models\neffectively detect specific 3D symmetry elements via linear probing, they\nfalter significantly in tasks requiring detailed geometric differentiation,\nsuch as mental rotation. Moreover, advanced vision-language assistants exhibit\nremarkably low accuracy on complex polyhedra, systematically misinterpreting\nbasic properties like face geometry, convexity, and compound structures. GIQ is\npublicly available, providing a structured platform to highlight and address\ncritical gaps in geometric intelligence, facilitating future progress in\nrobust, geometry-aware representation learning.", "comment": "15 pages, 4 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08194v1", "AI": {"title_translation": "GIQ：使用模拟和真实多面体对视觉基础模型的3D几何推理进行基准测试", "tldr": "当前视觉基础模型在详细的3D几何推理方面表现不佳，即使是对于基本形状，这凸显了对GIQ等更好基准的需求。", "motivation": "单目3D重建方法和视觉语言模型（VLMs）在标准基准测试中表现出色，但它们对几何属性的真正理解尚不清楚。因此，需要一个专门的基准来评估视觉和视觉语言基础模型的几何推理能力。", "method": "引入了GIQ，一个包含224种不同多面体（包括柏拉图、阿基米德、约翰逊和加泰罗尼亚立体，以及星形和复合形状）的合成和真实世界图像的综合基准。通过系统实验，包括单目3D重建、3D对称性检测、心理旋转测试和零样本形状分类任务来评估模型。", "result": "最先进的重建算法难以准确重建基本的几何形状。基础模型通过线性探测能有效检测特定3D对称元素，但在需要详细几何区分的任务（如心理旋转）中表现显著不佳。高级视觉语言助手在复杂多面体上的准确率极低，系统性地错误解释了面几何、凸性和复合结构等基本属性。", "conclusion": "GIQ揭示了当前模型在几何智能方面的显著缺陷，为未来在鲁棒、几何感知表示学习方面的进展提供了结构化平台。", "translation": "单目3D重建方法和视觉语言模型（VLMs）在标准基准测试中表现出令人印象深刻的结果，但它们对几何属性的真正理解仍不清楚。我们引入了GIQ，一个专门设计用于评估视觉和视觉语言基础模型几何推理能力的综合基准。GIQ包含224种不同多面体（包括柏拉图、阿基米德、约翰逊和加泰罗尼亚立体，以及星形和复合形状）的合成和真实世界图像，涵盖不同复杂度和对称性。通过涉及单目3D重建、3D对称性检测、心理旋转测试和零样本形状分类任务的系统实验，我们揭示了当前模型的显著缺陷。在大量3D数据集上训练的最先进重建算法难以准确重建即使是基本的几何形状。虽然基础模型通过线性探测有效地检测特定3D对称元素，但在需要详细几何区分的任务（如心理旋转）中表现显著不佳。此外，高级视觉语言助手在复杂多面体上的准确率极低，系统性地错误解释了面几何、凸性和复合结构等基本属性。GIQ已公开可用，提供了一个结构化平台来突出和解决几何智能方面的关键差距，促进未来在鲁棒、几何感知表示学习方面的进展。", "summary": "该论文引入了GIQ，一个用于评估视觉和视觉语言基础模型3D几何推理能力的综合基准。GIQ包含多样化的多面体图像，并支持通过3D重建、对称性检测和心理旋转等任务进行系统测试。实验结果揭示了当前最先进模型在准确几何重建、详细区分和理解基本属性方面的显著局限性，从而强调了改进几何感知表示学习的必要性。", "keywords": "3D几何推理, 视觉基础模型, 基准测试, 多面体, 几何智能", "comments": "这篇论文很重要，因为它解决了评估强大视觉基础模型真正几何理解方面的一个关键空白。GIQ基准及其多样化的多面体和系统任务，提供了一个急需的工具来揭示标准基准之外的局限性，从而推动更鲁棒和几何智能的AI发展。"}}
{"id": "2506.08363", "title": "FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs", "authors": ["Jun Yin", "Jing Zhong", "Pengyu Zeng", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "summary": "In the architectural design process, floorplan design is often a dynamic and\niterative process. Architects progressively draw various parts of the floorplan\naccording to their ideas and requirements, continuously adjusting and refining\nthroughout the design process. Therefore, the ability to predict a complete\nfloorplan from a partial one holds significant value in the design process.\nSuch prediction can help architects quickly generate preliminary designs,\nimprove design efficiency, and reduce the workload associated with repeated\nmodifications. To address this need, we propose FloorplanMAE, a self-supervised\nlearning framework for restoring incomplete floor plans into complete ones.\nFirst, we developed a floor plan reconstruction dataset, FloorplanNet,\nspecifically trained on architectural floor plans. Secondly, we propose a floor\nplan reconstruction method based on Masked Autoencoders (MAE), which\nreconstructs missing parts by masking sections of the floor plan and training a\nlightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy\nof FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,\nwe validated the model using real sketches from the early stages of\narchitectural design. Experimental results show that the FloorplanMAE model can\ngenerate high-quality complete floor plans from incomplete partial plans. This\nframework provides a scalable solution for floor plan generation, with broad\napplication prospects.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08363v1", "AI": {"title_translation": "FloorplanMAE：一个用于从部分输入生成完整平面图的自监督框架", "tldr": "FloorplanMAE是一个自监督框架，能从不完整输入生成高质量的完整建筑平面图，提高设计效率。", "motivation": "在建筑设计过程中，平面图设计是一个动态且迭代的过程，从部分输入预测完整平面图的能力可以帮助建筑师快速生成初步设计，提高设计效率，并减少重复修改的工作量。", "method": "本文提出了FloorplanMAE，一个用于将不完整平面图恢复为完整平面图的自监督学习框架。首先开发了专门用于建筑平面图的重建数据集FloorplanNet。其次，提出了一种基于Masked Autoencoders (MAE) 的平面图重建方法，通过遮蔽平面图的一部分并训练一个轻量级Vision Transformer (ViT) 来重建缺失部分。", "result": "实验结果表明，FloorplanMAE模型可以从不完整的部分平面图生成高质量的完整平面图，并且与最先进的基准进行了比较，并使用建筑设计早期阶段的真实草图进行了验证。", "conclusion": "FloorplanMAE框架为平面图生成提供了一个可扩展的解决方案，具有广阔的应用前景。", "translation": "在建筑设计过程中，平面图设计通常是一个动态且迭代的过程。建筑师根据他们的想法和要求逐步绘制平面图的各个部分，并在整个设计过程中不断调整和完善。因此，从部分平面图预测完整平面图的能力在设计过程中具有重要价值。这种预测可以帮助建筑师快速生成初步设计，提高设计效率，并减少重复修改带来的工作量。为了满足这一需求，我们提出了FloorplanMAE，一个用于将不完整平面图恢复为完整平面图的自监督学习框架。首先，我们开发了一个专门用于建筑平面图的平面图重建数据集FloorplanNet。其次，我们提出了一种基于Masked Autoencoders (MAE) 的平面图重建方法，该方法通过遮蔽平面图的一部分并训练一个轻量级Vision Transformer (ViT) 来重建缺失部分。我们评估了FloorplanMAE的重建精度，并与最先进的基准进行了比较。此外，我们使用建筑设计早期阶段的真实草图验证了该模型。实验结果表明，FloorplanMAE模型可以从不完整的部分平面图生成高质量的完整平面图。该框架为平面图生成提供了一个可扩展的解决方案，具有广阔的应用前景。", "summary": "本文提出了FloorplanMAE，一个自监督学习框架，旨在从部分输入生成完整的建筑平面图。针对建筑设计中平面图动态迭代的特点，该框架开发了FloorplanNet数据集，并采用基于Masked Autoencoders (MAE) 和Vision Transformer (ViT) 的方法进行平面图重建。实验证明，FloorplanMAE能够从不完整的部分平面图生成高质量的完整平面图，有效提升设计效率，并展现出广泛的应用前景。", "keywords": "平面图生成, 自监督学习, Masked Autoencoders, Vision Transformer, 建筑设计", "comments": "FloorplanMAE的创新点在于将自监督学习（特别是MAE）应用于建筑平面图的补全任务，并构建了专门的数据集FloorplanNet。这种方法能够有效提升建筑设计的效率，减少重复劳动，具有重要的实际应用价值。其可扩展性也预示着广阔的应用前景。"}}
{"id": "2506.08060", "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques", "authors": ["Asankhaya Sharma"], "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength $l$, datasets of size $\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log\n\\frac{m}{\\delta} \\right)$ or, with bounded context, $\\mathrm{O}\\left( \\frac{l\n\\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ suffice to approximate\nfine-tuned behavior across $m$ contexts within error $\\varepsilon$, where $V$\nis the vocabulary size and $\\delta$ is the failure probability. For linear\nclassification, datasets of size $\\mathrm{O}\\left( \\frac{d}{\\varepsilon}\n\\right)$ or, with fixed context, $\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log\n\\frac{1}{\\delta} \\right)$ are sufficient, where $d$ is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08060v1", "AI": {"title_translation": "通过推理时技术激发微调Transformer的能力", "tldr": "本文证明，在理想假设下，微调Transformer模型的能力可以通过基础Transformer模型在推理时技术（如上下文学习）进行近似，无需改变模型参数，并给出了实际场景下所需数据集大小的理论证明。", "motivation": "大型语言模型的监督微调（SFT）计算成本高昂。", "method": "本文通过形式化证明，在理想假设（包括无限计算资源和访问微调数据集）下，SFT获得的能力可以通过基础Transformer模型使用推理时技术（特别是上下文学习ICL）在不改变模型参数的情况下进行近似。研究进一步将这些结果推广到有限上下文长度和部分数据集访问的实际场景。", "result": "对于固定输出长度$l$的文本生成任务，在错误率$\\varepsilon$内近似微调行为，所需数据集大小为$\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log \\frac{m}{\\delta} \\right)$，或在有限上下文下为$\\mathrm{O}\\left( \\frac{l \\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$。对于线性分类，所需数据集大小为$\\mathrm{O}\\left( \\frac{d}{\\varepsilon} \\right)$，或在固定上下文下为$\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$。", "conclusion": "这些结果为大型语言模型的资源高效部署提供了理论基础，并通过检索增强生成等实用技术将理论与实际应用相结合。", "translation": "大型语言模型已经改变了自然语言处理，但监督微调（SFT）仍然是计算密集型的。本文正式证明，在理想假设（包括无限计算资源和访问微调数据集）下，通过SFT获得的能力可以通过基础Transformer模型使用推理时技术（特别是上下文学习ICL）在不改变模型参数的情况下进行近似。我们将这些结果推广到具有有限上下文长度和部分数据集访问的实际场景。对于固定输出长度$l$的文本生成任务，在错误率$\\varepsilon$内近似微调行为，需要大小为$\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log \\frac{m}{\\delta} \\right)$的数据集，或者在有限上下文下，需要大小为$\\mathrm{O}\\left( \\frac{l \\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$的数据集，其中$V$是词汇量，$\\delta$是失败概率。对于线性分类，需要大小为$\\mathrm{O}\\left( \\frac{d}{\\varepsilon} \\right)$的数据集，或者在固定上下文下，需要大小为$\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$的数据集，其中$d$是输入维度。基于Transformer的图灵完备性，这些结果为大型语言模型的资源高效部署提供了理论基础，并结合了检索增强生成等实用技术，将理论应用于实际场景。", "summary": "本文证明，在理想情况下，大型语言模型的监督微调（SFT）所获得的能力可以通过基础Transformer模型在推理时技术（如上下文学习）进行近似，而无需修改模型参数。研究进一步给出了在有限上下文和部分数据集访问的实际场景下，为达到特定近似误差所需的文本生成和线性分类任务的数据集大小的理论界限。这些发现为开发资源高效的大型语言模型部署策略提供了理论支撑。", "keywords": "Transformer, 上下文学习, 推理时技术, 监督微调, 资源高效部署", "comments": "这篇论文的创新点在于从理论上证明了通过推理时技术（如ICL）可以近似SFT的能力，这对于降低LLM的部署成本具有重要意义。它为资源受限环境下的LLM应用提供了新的思路，并强调了ICL的潜力。文章通过形式化证明和具体的数据集大小估算，提供了坚实的理论基础。"}}
{"id": "2506.08488", "title": "EtiCor++: Towards Understanding Etiquettical Bias in LLMs", "authors": ["Ashutosh Dwivedi", "Siddhant Shivdutt Singh", "Ashutosh Modi"], "summary": "In recent years, researchers have started analyzing the cultural sensitivity\nof LLMs. In this respect, Etiquettes have been an active area of research.\nEtiquettes are region-specific and are an essential part of the culture of a\nregion; hence, it is imperative to make LLMs sensitive to etiquettes. However,\nthere needs to be more resources in evaluating LLMs for their understanding and\nbias with regard to etiquettes. In this resource paper, we introduce EtiCor++,\na corpus of etiquettes worldwide. We introduce different tasks for evaluating\nLLMs for knowledge about etiquettes across various regions. Further, we\nintroduce various metrics for measuring bias in LLMs. Extensive experimentation\nwith LLMs shows inherent bias towards certain regions.", "comment": "Accepted at ACL Findings 2025, 22 pages (9 pages main content + 4\n  pages references + 9 pages appendix)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08488v1", "AI": {"title_translation": "EtiCor++: 走向理解大型语言模型中的礼仪偏见", "tldr": "本研究介绍了EtiCor++，这是一个全球礼仪语料库，旨在评估和测量大型语言模型在礼仪理解和偏见方面的表现，并发现LLM对某些地区存在固有偏见。", "motivation": "近年来，研究人员开始分析大型语言模型（LLMs）的文化敏感性，其中礼仪作为一个特定区域的文化重要组成部分，受到了广泛关注。然而，目前缺乏用于评估LLM在礼仪理解和偏见方面的资源。", "method": "本论文引入了EtiCor++，这是一个全球礼仪语料库。研究者提出了不同的任务来评估LLM在不同区域的礼仪知识，并引入了多种指标来衡量LLM中的偏见。", "result": "对大型语言模型进行的广泛实验表明，它们对某些地区存在固有的偏见。", "conclusion": "大型语言模型对某些地区存在固有的礼仪偏见，本研究介绍的EtiCor++语料库、评估任务和衡量指标有助于评估和理解这些偏见，从而推动构建对礼仪更敏感的LLM。", "translation": "近年来，研究人员开始分析大型语言模型（LLMs）的文化敏感性。在这方面，礼仪一直是一个活跃的研究领域。礼仪是区域性的，是一个地区文化的重要组成部分；因此，使LLMs对礼仪敏感至关重要。然而，在评估LLMs对礼仪的理解和偏见方面，资源仍然不足。在这篇资源论文中，我们介绍了EtiCor++，这是一个全球礼仪语料库。我们引入了不同的任务来评估LLMs在不同区域的礼仪知识。此外，我们还引入了各种指标来衡量LLMs中的偏见。对LLMs进行的广泛实验表明，它们对某些地区存在固有的偏见。", "summary": "本研究旨在解决大型语言模型（LLMs）在礼仪理解和偏见评估方面资源不足的问题。为此，论文引入了EtiCor++，一个包含全球礼仪的语料库，并设计了多种任务和指标来评估LLM的礼仪知识和偏见。实验结果显示，LLMs对特定地区存在固有的礼仪偏见。", "keywords": "礼仪偏见, 大型语言模型, EtiCor++, 文化敏感性, 语料库", "comments": "该论文通过创建EtiCor++语料库和相关评估方法，填补了LLM在文化敏感性，特别是礼仪偏见评估方面的空白，对于推动构建更具文化意识的LLM具有重要意义。"}}
{"id": "2506.08725", "title": "Stop Misusing t-SNE and UMAP for Visual Analytics", "authors": ["Hyeon Jeon", "Jeongin Park", "Sungbok Shin", "Jinwook Seo"], "summary": "Misuses of t-SNE and UMAP in visual analytics have become increasingly\ncommon. For example, although t-SNE and UMAP projections often do not\nfaithfully reflect true distances between clusters, practitioners frequently\nuse them to investigate inter-cluster relationships. In this paper, we bring\nthis issue to the surface and comprehensively investigate why such misuse\noccurs and how to prevent it. We conduct a literature review of 114 papers to\nverify the prevalence of the misuse and analyze the reasonings behind it. We\nthen execute an interview study to uncover practitioners' implicit motivations\nfor using these techniques -- rationales often undisclosed in the literature.\nOur findings indicate that misuse of t-SNE and UMAP primarily stems from\nlimited discourse on their appropriate use in visual analytics. We conclude by\nproposing future directions and concrete action items to promote more\nreasonable use of DR.", "comment": "9 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08725v1", "AI": {"title_translation": "停止滥用t-SNE和UMAP进行视觉分析", "tldr": "t-SNE和UMAP在视觉分析中常被误用，本研究通过文献综述和访谈揭示其原因在于缺乏恰当使用的讨论，并提出改进建议。", "motivation": "视觉分析中t-SNE和UMAP的误用日益普遍，例如用它们来调查集群间关系，而它们并不能忠实反映真实距离。本文旨在深入调查这种误用发生的原因以及如何预防。", "method": "研究方法包括：1. 对114篇论文进行文献综述，以验证误用的普遍性并分析其背后的原因。2. 进行访谈研究，以揭示实践者使用这些技术的隐含动机（文献中通常未披露的理由）。", "result": "研究结果表明，t-SNE和UMAP的误用主要源于对它们在视觉分析中恰当使用的讨论有限。", "conclusion": "论文最后提出了未来的方向和具体的行动项目，以促进更合理地使用降维技术（DR）。", "translation": "停止滥用t-SNE和UMAP进行视觉分析\n\n摘要：\nt-SNE和UMAP在视觉分析中的误用已变得越来越普遍。例如，尽管t-SNE和UMAP投影通常不能忠实地反映集群之间的真实距离，但实践者却经常使用它们来研究集群间的关系。在本文中，我们揭示了这个问题，并全面调查了这种误用发生的原因以及如何预防。我们对114篇论文进行了文献综述，以验证误用的普遍性并分析其背后的原因。然后，我们进行了一项访谈研究，以揭示实践者使用这些技术的隐含动机——这些理由在文献中通常未被披露。我们的研究结果表明，t-SNE和UMAP的误用主要源于对其在视觉分析中恰当使用的讨论有限。最后，我们提出了未来的方向和具体的行动项目，以促进更合理地使用降维（DR）技术。", "summary": "本文关注t-SNE和UMAP在视觉分析中日益普遍的误用现象，特别是它们在反映集群间真实距离方面的局限性。研究通过对114篇论文的文献综述和对实践者的访谈，深入探讨了误用发生的原因。结果发现，误用主要源于对这两种技术恰当使用的讨论不足。文章最后提出了促进更合理使用降维技术的未来方向和具体行动建议。", "keywords": "t-SNE, UMAP, 视觉分析, 降维, 误用", "comments": "这篇论文揭示了数据可视化领域一个重要的实际问题，即常用降维工具（t-SNE和UMAP）的误用。其创新之处在于不仅验证了误用的普遍性，还深入探究了其根源，特别是通过访谈揭示了实践者未披露的隐含动机。这对于提高数据分析的准确性和可靠性具有重要意义，并为未来的工具设计和教育提供了指导。"}}
{"id": "2506.08844", "title": "IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)", "authors": ["Siyi Sun", "David Antony Selby", "Yunchuan Huang", "Sebastian Vollmer", "Seth Flaxman", "Anisoara Calinescu"], "summary": "Missing data imputation in tabular datasets remains a pivotal challenge in\ndata science and machine learning, particularly within socioeconomic research.\nHowever, real-world socioeconomic datasets are typically subject to strict data\nprotection protocols, which often prohibit public sharing, even for synthetic\nderivatives. This severely limits the reproducibility and accessibility of\nbenchmark studies in such settings. Further, there are very few publicly\navailable synthetic datasets. Thus, there is limited availability of benchmarks\nfor systematic evaluation of imputation methods on socioeconomic datasets,\nwhether real or synthetic. In this study, we utilize the World Bank's publicly\navailable synthetic dataset, Synthetic Data for an Imaginary Country, which\nclosely mimics a real World Bank household survey while being fully public,\nenabling broad access for methodological research. With this as a starting\npoint, we derived the IMAGIC-500 dataset: we select a subset of 500k\nindividuals across approximately 100k households with 19 socioeconomic\nfeatures, designed to reflect the hierarchical structure of real-world\nhousehold surveys. This paper introduces a comprehensive missing data\nimputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,\nMNAR) and missingness ratios (10\\%, 20\\%, 30\\%, 40\\%, 50\\%). Our evaluation\nconsiders the imputation accuracy for continuous and categorical variables,\ncomputational efficiency, and impact on downstream predictive tasks, such as\nestimating educational attainment at the individual level. The results\nhighlight the strengths and weaknesses of statistical, traditional machine\nlearning, and deep learning imputation techniques, including recent\ndiffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate\nthe development of robust imputation algorithms and foster reproducible social\nscience research.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08844v1", "AI": {"title_translation": "IMAGIC-500：一个生成式虚拟国家上的缺失数据填补基准 (50万样本)", "tldr": "本文介绍了IMAGIC-500数据集，这是一个基于虚拟国家合成数据的缺失数据填补基准，旨在解决社会经济领域缺失数据填补方法评估中再现性和可访问性受限的问题，并评估了多种填补方法的性能。", "motivation": "现实世界社会经济数据集的数据保护协议限制了公开共享，导致缺失数据填补方法的基准研究缺乏再现性和可访问性，且公开可用的合成数据集稀少，因此需要一个可公开访问的、大规模的基准数据集来系统评估填补方法。", "method": "研究人员利用世界银行的“虚拟国家合成数据”派生出IMAGIC-500数据集，该数据集包含50万个个体、约10万个家庭的19个社会经济特征。在此数据集上，他们建立了全面的缺失数据填补基准，涵盖多种缺失机制（MCAR、MAR、MNAR）和缺失率（10%-50%），并评估了填补准确性、计算效率以及对下游预测任务的影响。", "result": "结果揭示了统计、传统机器学习和深度学习（包括基于扩散）等多种填补技术在不同情境下的优缺点。", "conclusion": "IMAGIC-500数据集和基准旨在推动鲁棒性填补算法的开发，并促进社会科学研究的可再现性。", "translation": "表格数据集中缺失数据填补仍然是数据科学和机器学习中的一个关键挑战，特别是在社会经济研究中。然而，现实世界的社会经济数据集通常受到严格的数据保护协议，这常常禁止公开共享，即使是合成衍生数据也不行。这严重限制了此类设置中基准研究的再现性和可访问性。此外，公开可用的合成数据集非常少。因此，对于社会经济数据集（无论是真实还是合成的）上填补方法进行系统评估的基准可用性有限。在本研究中，我们利用世界银行公开可用的合成数据集“一个虚拟国家的合成数据”，它与真实的世界银行住户调查非常相似，同时完全公开，从而使得方法论研究能够广泛访问。以此为起点，我们派生了IMAGIC-500数据集：我们选择了约10万个家庭中的50万个个体子集，包含19个社会经济特征，旨在反映真实世界住户调查的层级结构。本文介绍了IMAGIC-500上在各种缺失机制（MCAR、MAR、MNAR）和缺失率（10%、20%、30%、40%、50%）下的全面缺失数据填补基准。我们的评估考虑了连续和分类变量的填补准确性、计算效率以及对下游预测任务（例如估计个人教育程度）的影响。结果突出了统计、传统机器学习和深度学习填补技术（包括最近基于扩散的方法）的优缺点。IMAGIC-500数据集和基准旨在促进鲁棒填补算法的开发，并促进可再现的社会科学研究。", "summary": "本文介绍了IMAGIC-500数据集，这是一个针对社会经济领域缺失数据填补的综合基准。鉴于真实社会经济数据共享受限导致基准研究再现性不足的问题，研究团队利用世界银行的“虚拟国家合成数据”构建了包含50万个体和19个社会经济特征的IMAGIC-500。该基准评估了不同缺失机制和缺失率下各种统计、机器学习和深度学习填补方法的性能，包括准确性、计算效率及对下游任务的影响。IMAGIC-500旨在促进鲁棒填补算法的开发和可再现的社会科学研究。", "keywords": "缺失数据填补, 社会经济数据, 基准测试, 合成数据, IMAGIC-500", "comments": "本文通过创建一个大规模、可公开访问的合成数据集IMAGIC-500，巧妙地解决了社会经济领域真实数据共享受限导致的缺失数据填补方法评估难题，极大提升了该领域研究的再现性和可访问性。其创新之处在于提供了全面的基准测试环境，覆盖了多种缺失机制和缺失率，为评估和开发新的填补算法提供了坚实的基础。"}}
{"id": "2506.08974", "title": "Diver-Robot Communication Dataset for Underwater Hand Gesture Recognition", "authors": ["Igor Kvasić", "Derek Orbaugh Antillon", "Ðula Nađ", "Christopher Walker", "Iain Anderson", "Nikola Mišković"], "summary": "In this paper, we present a dataset of diving gesture images used for\nhuman-robot interaction underwater. By offering this open access dataset, the\npaper aims at investigating the potential of using visual detection of diving\ngestures from an autonomous underwater vehicle (AUV) as a form of communication\nwith a human diver. In addition to the image recording, the same dataset was\nrecorded using a smart gesture recognition glove. The glove uses elastomer\nsensors and on-board processing to determine the selected gesture and transmit\nthe command associated with the gesture to the AUV via acoustics. Although this\nmethod can be used under different visibility conditions and even without line\nof sight, it introduces a communication delay required for the acoustic\ntransmission of the gesture command. To compare efficiency, the glove was\nequipped with visual markers proposed in a gesture-based language called\nCADDIAN and recorded with an underwater camera in parallel to the glove's\nonboard recognition process. The dataset contains over 30,000 underwater frames\nof nearly 900 individual gestures annotated in corresponding snippet folders.\nThe dataset was recorded in a balanced ratio with five different divers in sea\nand five different divers in pool conditions, with gestures recorded at 1, 2\nand 3 metres from the camera. The glove gesture recognition statistics are\nreported in terms of average diver reaction time, average time taken to perform\na gesture, recognition success rate, transmission times and more. The dataset\npresented should provide a good baseline for comparing the performance of state\nof the art visual diving gesture recognition techniques under different\nvisibility conditions.", "comment": "28 pages, 12 figures (Elsevier Computer Networks Journal)", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.08974v1", "AI": {"title_translation": "水下手势识别的潜水员-机器人通信数据集", "tldr": "本文提出了一个用于水下潜水员-机器人手势通信的数据集，并比较了视觉识别与智能手套识别的效率。", "motivation": "旨在探索利用自主水下航行器（AUV）对潜水手势进行视觉检测作为与人类潜水员沟通方式的潜力。", "method": "本文构建了一个潜水手势图像数据集，包含超过30,000帧水下图像和近900个独立手势，通过水下摄像头和智能手势识别手套同时记录。数据由五名在海中和五名在泳池中的不同潜水员在距离摄像头1米、2米和3米处完成，并报告了手套手势识别的统计数据。", "result": "本文发布了一个名为“潜水员-机器人通信数据集”的开放获取数据集，包含超过30,000帧水下图像。同时报告了手套手势识别的统计数据，包括平均潜水员反应时间、执行手势的平均时间、识别成功率和传输时间等。", "conclusion": "所提出的数据集为比较不同能见度条件下最先进的视觉潜水手势识别技术的性能提供了一个良好的基准。", "translation": "在本文中，我们提出了一个用于水下人机交互的潜水手势图像数据集。通过提供这个开放获取的数据集，本文旨在研究使用自主水下航行器（AUV）对潜水手势进行视觉检测作为与人类潜水员沟通形式的潜力。除了图像记录，同一数据集还使用智能手势识别手套进行了记录。该手套利用弹性体传感器和板载处理来确定所选手势，并通过声学方式将与手势相关的命令传输给AUV。尽管这种方法可以在不同能见度条件下甚至没有视线的情况下使用，但它引入了声学传输手势命令所需的通信延迟。为了比较效率，该手套配备了CADDIAN手势语言中提出的视觉标记，并与手套的板载识别过程并行地用一台水下摄像机进行记录。该数据集包含超过30,000帧水下图像，其中近900个独立手势标注在相应的片段文件夹中。该数据集以平衡的比例记录，其中五名不同的潜水员在海中，五名不同的潜水员在泳池中，手势在距离摄像头1米、2米和3米处记录。手套手势识别统计数据以平均潜水员反应时间、执行手势的平均时间、识别成功率、传输时间等形式报告。所提出的数据集应为比较不同能见度条件下最先进的视觉潜水手势识别技术的性能提供一个良好的基准。", "summary": "本文介绍了“潜水员-机器人通信数据集”，旨在通过手势实现水下人机交互。研究探讨了AUV视觉识别潜水员手势的可行性，并将其与智能手势识别手套进行比较。该数据集包含超过30,000帧水下图像，来自不同环境和潜水员。研究还报告了手套识别的性能统计数据，为未来的视觉手势识别研究提供了基准。", "keywords": "水下通信, 手势识别, 人机交互, 数据集, 自主水下航行器", "comments": "该论文通过提供一个急需的开放获取数据集，在水下人机交互领域取得了创新，解决了关键的通信挑战。对视觉和基于手套的识别方法进行比较，为权衡（例如，能见度与延迟）提供了宝贵的见解。数据集的规模和多样化的记录条件（多名潜水员、海洋/泳池、不同距离）增强了其在开发鲁棒水下手势识别系统方面的实用性。"}}
{"id": "2506.08923", "title": "Mycelium: A Transformation-Embedded LSM-Tree", "authors": ["Holly Casaletto", "Jeff Lefevre", "Aldrin Montana", "Peter Alvaro"], "summary": "Compaction is a necessary, but often costly background process in\nwrite-optimized data structures like LSM-trees that reorganizes incoming data\nthat is sequentially appended to logs. In this paper, we introduce\nTransformation-Embedded LSM-trees (TE-LSM), a novel approach that transparently\nembeds a variety of data transformations into the compaction process. While\nmany others have sought to reduce the high cost of compaction, TE-LSMs leverage\nthe opportunity to embed other useful work to amortize IO costs and\namplification. We illustrate the use of a TE-LSM in Mycelium, our prototype\nbuilt on top of RocksDB that extends the compaction process through a\ncross-column-family merging mechanism. Mycelium enables seamless integration of\na transformer interface and aims to better prepare data for future accesses\nbased on access patterns. We use Mycelium to explore three types of\ntransformations: splitting column groups, converting data formats, and index\nbuilding. In addition to providing a cost model analysis, we evaluate\nMycelium's write and read performance using YCSB workloads. Our results show\nthat Mycelium incurs a 20% write throughput overhead - significantly lower than\nthe 35% to 60% overhead observed in naive approaches that perform data\ntransformations outside of compaction-while achieving up to 425% improvements\nin read latency compared to RocksDB baseline.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.08923v1", "AI": {"title_translation": "Mycelium：一种转换嵌入式LSM树", "tldr": "该论文引入了一种名为转换嵌入式LSM树（TE-LSM）的新方法，它将各种数据转换透明地嵌入到LSM树的压缩过程中，以分摊I/O成本并提高读取性能，同时保持较低的写入开销。", "motivation": "压缩是LSM树等写入优化数据结构中必需但通常成本高昂的后台过程，它会重组顺序追加到日志中的数据。现有方法致力于降低压缩的高成本，但本文利用这一机会将其他有用的数据转换嵌入到压缩过程中，以分摊I/O成本和放大，并更好地为未来的数据访问模式做准备。", "method": "论文引入了转换嵌入式LSM树（TE-LSM）的概念。Mycelium是其原型，构建在RocksDB之上，通过跨列族合并机制扩展了压缩过程。Mycelium实现了转换器接口的无缝集成，并探索了三种类型的转换：拆分列组、转换数据格式和构建索引。此外，论文还提供了成本模型分析，并使用YCSB工作负载评估了Mycelium的写入和读取性能。", "result": "Mycelium的写入吞吐量开销为20%，显著低于在压缩外部执行数据转换的朴素方法所观察到的35%到60%的开销。与RocksDB基线相比，Mycelium在读取延迟方面实现了高达425%的改进。", "conclusion": "Mycelium通过在LSM树的压缩过程中透明地嵌入数据转换，有效地分摊了I/O成本和放大，显著提高了读取性能，同时保持了可接受的写入开销，优于在压缩外部执行转换的传统方法。", "translation": "压缩是LSM树等写入优化数据结构中必需但通常成本高昂的后台过程，它会重组顺序追加到日志中的数据。在本文中，我们引入了转换嵌入式LSM树（TE-LSM），这是一种将各种数据转换透明地嵌入到压缩过程中的新方法。虽然许多人试图降低压缩的高成本，但TE-LSM利用这一机会嵌入其他有用的工作来分摊I/O成本和放大。我们通过Mycelium展示了TE-LSM的使用，Mycelium是我们基于RocksDB构建的原型，它通过跨列族合并机制扩展了压缩过程。Mycelium实现了转换器接口的无缝集成，旨在根据访问模式更好地为未来的数据访问做准备。我们使用Mycelium探索了三种类型的转换：拆分列组、转换数据格式和索引构建。除了提供成本模型分析外，我们还使用YCSB工作负载评估了Mycelium的写入和读取性能。我们的结果表明，Mycelium的写入吞吐量开销为20%——显著低于在压缩外部执行数据转换的朴素方法所观察到的35%到60%的开销——同时与RocksDB基线相比，读取延迟提高了高达425%。", "summary": "该论文提出了转换嵌入式LSM树（TE-LSM），以Mycelium原型为例，其核心思想是将数据转换直接集成到LSM树的压缩过程中。这种方法旨在通过在压缩期间执行有益工作来分摊I/O成本和放大，从而根据访问模式优化数据以供未来访问。通过在RocksDB上进行评估，并探索了列组拆分、数据格式转换和索引构建等转换类型，Mycelium展示出仅20%的写入开销，远低于外部朴素方法的35%-60%，同时读取延迟提升高达425%。", "keywords": "LSM树, 压缩, 数据转换, RocksDB, Mycelium", "comments": "这篇论文提出了一种创新性的方法，通过利用LSM树中现有且成本高昂的压缩过程来执行其他有用的数据转换。这种策略有效地分摊了成本，并提高了数据读取的准备度，解决了写入优化数据结构中的关键性能问题。其量化结果具有很强的说服力，为LSM树的优化提供了新的视角。"}}
{"id": "2506.08717", "title": "Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition", "authors": ["Mehedi Hasan Bijoy", "Dejan Porjazovski", "Tamás Grósz", "Mikko Kurimo"], "summary": "Speech Emotion Recognition (SER) is crucial for improving human-computer\ninteraction. Despite strides in monolingual SER, extending them to build a\nmultilingual system remains challenging. Our goal is to train a single model\ncapable of multilingual SER by distilling knowledge from multiple teacher\nmodels. To address this, we introduce a novel language-aware multi-teacher\nknowledge distillation method to advance SER in English, Finnish, and French.\nIt leverages Wav2Vec2.0 as the foundation of monolingual teacher models and\nthen distills their knowledge into a single multilingual student model. The\nstudent model demonstrates state-of-the-art performance, with a weighted recall\nof 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish\ndataset, surpassing fine-tuning and knowledge distillation baselines. Our\nmethod excels in improving recall for sad and neutral emotions, although it\nstill faces challenges in recognizing anger and happiness.", "comment": "Accepted to INTERSPEECH 2025 conference", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08717v1", "AI": {"title_translation": "多教师语言感知知识蒸馏用于多语言语音情感识别", "tldr": "本文提出了一种新颖的多教师语言感知知识蒸馏方法，用于训练一个单一模型进行多语言语音情感识别，在英语和芬兰语数据集上取得了最先进的性能。", "motivation": "尽管单语言语音情感识别（SER）取得了进展，但构建多语言系统仍然具有挑战性。目标是训练一个单一模型来执行多语言SER。", "method": "引入了一种新颖的语言感知多教师知识蒸馏方法。该方法以Wav2Vec2.0作为单语言教师模型的基础，然后将它们的知识蒸馏到一个单一的多语言学生模型中。", "result": "学生模型在英语数据集上取得了72.9的加权召回率，在芬兰语数据集上取得了63.4的未加权召回率，超越了微调和知识蒸馏基线。该方法在提高悲伤和中性情绪的召回率方面表现出色，但在识别愤怒和快乐方面仍面临挑战。", "conclusion": "所提出的多教师语言感知知识蒸馏方法成功地提高了多语言语音情感识别的性能，在特定语言和情感方面取得了最先进的成果，但仍需解决某些情感识别的挑战。", "translation": "语音情感识别（SER）对于改善人机交互至关重要。尽管单语言SER取得了进展，但将其扩展以构建多语言系统仍然具有挑战性。我们的目标是通过从多个教师模型中蒸馏知识来训练一个能够进行多语言SER的单一模型。为了解决这个问题，我们引入了一种新颖的语言感知多教师知识蒸馏方法，以推动英语、芬兰语和法语的SER。它利用Wav2Vec2.0作为单语言教师模型的基础，然后将它们的知识蒸馏到一个单一的多语言学生模型中。该学生模型展示了最先进的性能，在英语数据集上加权召回率为72.9，在芬兰语数据集上未加权召回率为63.4，超越了微调和知识蒸馏基线。我们的方法在提高悲伤和中性情绪的召回率方面表现出色，尽管在识别愤怒和快乐方面仍面临挑战。", "summary": "本文提出了一种新颖的多教师语言感知知识蒸馏方法，旨在解决多语言语音情感识别（SER）的挑战。该方法利用Wav2Vec2.0作为单语言教师模型的基础，并将这些教师模型的知识蒸馏到一个单一的多语言学生模型中。实验结果表明，该学生模型在英语和芬兰语数据集上取得了最先进的性能，超越了现有基线，尤其在识别悲伤和中性情绪方面表现突出，但对愤怒和快乐情绪的识别仍有待改进。", "keywords": "语音情感识别, 知识蒸馏, 多语言, Wav2Vec2.0, 深度学习", "comments": "该论文提出了一种创新的多教师知识蒸馏框架，有效地整合了多语言信息，显著提升了多语言语音情感识别的性能。其在特定情感识别方面的优势和局限性为未来的研究提供了方向。"}}
{"id": "2506.08036", "title": "Followstopper Revisited: Phase-space Lagrangian Controller for Traffic Decongestion", "authors": ["Rahul Bhadani"], "summary": "This paper revisits Followerstopper, a phase-space-based control system that\nhad demonstrated its ability to mitigate emergent traffic jams due to\nstop-and-go traffic during rush hour in the mixed-autonomy setting.\nFollowerstopper was deployed on an autonomous vehicle. The controller\nattenuates the emanant traffic waves by regulating its velocity according to\nthe relative distance and velocity of the leader car. While regulating the\nvelocity, the controller also prevents the collision of the ego vehicle with\nthe lead vehicle within the range specified by the controller's design\nparameter. The controller design is based on a configurable quadratic curve on\nrelative distance-relative velocity phase-space that allows the transition of\nthe regulated velocity from (i) no modification of input, (ii) decelerating to\nmatch the leader's velocity (iii) braking to avoid any imminent collision. In\nthis paper, we explore the phase-space properties of Followerstopper and\nprovide a detailed description of a nonlinear control law that regulates the\nreference input to Followerstopper within the physics-informed boundaries. We\nalso provide a new discussion on the nominal control law that regulates the\nreference speed to Followerstopper to avoid unrealistic and unsafe\nacceleration.", "comment": "10 figures, 11 pages", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08036v1", "AI": {"title_translation": "Followstopper再探：用于交通疏堵的相空间拉格朗日控制器", "tldr": "这篇论文重新审视了Followstopper，一个基于相空间的控制器，旨在通过调节自动驾驶车辆的速度和防止碰撞来缓解交通拥堵，并提供了详细的非线性控制律和关于名义控制的讨论，以避免不切实际和不安全的加速。", "motivation": "本文旨在重新审视并改进Followstopper，一个能够缓解交通拥堵的相空间控制系统。具体动机包括探索其相空间特性、提供详细的非线性控制律，以及讨论避免不切实际加速度的名义控制。", "method": "本文探索了Followstopper的相空间特性，并详细描述了一种在物理知情边界内调节Followstopper参考输入的非线性控制律。同时，还讨论了一种调节Followstopper参考速度以避免不切实际和不安全加速度的名义控制律。Followstopper通过基于相对距离-相对速度相空间上的可配置二次曲线来调节速度。", "result": "本文提供了一个详细的非线性控制律描述，以及关于Followstopper名义控制律的新讨论，旨在在物理知情边界内调节参考输入并避免不切实际/不安全的加速度。", "conclusion": "本文通过详细阐述非线性控制律和讨论名义控制，对Followstopper控制器进行了增强的理解和公式化，以确保在物理约束下实现更安全、更现实的交通疏堵操作。", "translation": "本文重新审视了Followerstopper，一个基于相空间的控制系统，该系统在混合自动驾驶环境下，已证明其能够在高峰期缓解因走走停停交通引起的突发性交通拥堵。Followerstopper部署在自动驾驶车辆上。该控制器通过根据前车相对距离和速度调节自身速度来衰减产生的交通波。在调节速度的同时，控制器还防止自车在控制器设计参数指定范围内与前车发生碰撞。该控制器设计基于相对距离-相对速度相空间上的可配置二次曲线，允许调节速度从 (i) 不修改输入，(ii) 减速以匹配前车速度，(iii) 刹车以避免任何即将发生的碰撞之间进行转换。在本文中，我们探索了Followstopper的相空间特性，并详细描述了一种在物理知情边界内调节Followstopper参考输入的非线性控制律。我们还对调节Followstopper参考速度以避免不切实际和不安全加速度的名义控制律进行了新的讨论。", "summary": "本文重新审视了Followstopper，一个用于混合自动驾驶环境下交通疏堵的相空间控制系统。它通过根据前车的相对距离和速度调节自动驾驶车辆的速度来衰减交通波并防止碰撞。其控制设计利用相空间上的二次曲线实现不同的速度调节模式。本文探讨了Followstopper的相空间特性，详细阐述了一种在物理知情边界内调节输入的非线性控制律，并讨论了一种避免不切实际加速度的名义控制律。", "keywords": "Followstopper, 相空间, 交通疏堵, 非线性控制, 自动驾驶车辆", "comments": "本文侧重于通过深入研究其理论基础（相空间特性）和解决实际限制（不切实际的加速度）来改进现有的控制系统（Followstopper）。这种迭代改进方法对于稳健的实际部署非常有价值。对“物理知情边界”和避免“不切实际和不安全加速度”的关注，突显了对安全性和实际应用性的高度重视。"}}
{"id": "2506.08312", "title": "Private Evolution Converges", "authors": ["Tomás González", "Giulia Fanti", "Aaditya Ramdas"], "summary": "Private Evolution (PE) is a promising training-free method for differentially\nprivate (DP) synthetic data generation. While it achieves strong performance in\nsome domains (e.g., images and text), its behavior in others (e.g., tabular\ndata) is less consistent. To date, the only theoretical analysis of the\nconvergence of PE depends on unrealistic assumptions about both the algorithm's\nbehavior and the structure of the sensitive dataset. In this work, we develop a\nnew theoretical framework to explain PE's practical behavior and identify\nsufficient conditions for its convergence. For $d$-dimensional sensitive\ndatasets with $n$ data points from a bounded domain, we prove that PE produces\nan $(\\epsilon, \\delta)$-DP synthetic dataset with expected 1-Wasserstein\ndistance of order $\\tilde{O}(d(n\\epsilon)^{-1/d})$ from the original,\nestablishing worst-case convergence of the algorithm as $n \\to \\infty$. Our\nanalysis extends to general Banach spaces as well. We also connect PE to the\nPrivate Signed Measure Mechanism, a method for DP synthetic data generation\nthat has thus far not seen much practical adoption. We demonstrate the\npractical relevance of our theoretical findings in simulations.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08312v1", "AI": {"title_translation": "私有演化收敛", "tldr": "本文提出了一个新的理论框架，证明了私有演化（PE）在差分隐私合成数据生成中的收敛性，解决了以往分析的局限性。", "motivation": "私有演化（PE）作为一种差分隐私（DP）合成数据生成方法，在某些领域表现良好但在其他领域表现不一致，且其收敛性的现有理论分析依赖于不切实际的假设。因此，需要一个更鲁棒的理论框架来解释PE的实际行为并确定其收敛的充分条件。", "method": "本文开发了一个新的理论框架来解释PE的实际行为并确定其收敛的充分条件。对于有界域中$n$个数据点的$d$维敏感数据集，作者证明了PE能生成一个期望1-Wasserstein距离为$\tilde{O}(d(n\nobreak\\epsilon)^{-1/d})$的$(\nobreak\\epsilon, \nobreak\\delta)$-DP合成数据集。该分析还扩展到一般的Banach空间，并首次将PE与私有符号测度机制（PSMM）联系起来。", "result": "研究证明，PE能生成一个期望1-Wasserstein距离为$\tilde{O}(d(n\nobreak\\epsilon)^{-1/d})$的$(\nobreak\\epsilon, \nobreak\\delta)$-DP合成数据集，从而建立了算法在$n \to \nobreak\\infty$时的最坏情况收敛性。该分析也适用于一般的Banach空间。模拟结果也证实了理论发现的实际相关性。", "conclusion": "本文为私有演化（PE）的收敛性提供了严谨的理论基础，解决了现有分析的局限性，并证明了其在差分隐私合成数据生成中的可靠性。", "translation": "私有演化（PE）是一种很有前景的、免训练的差分隐私（DP）合成数据生成方法。尽管它在某些领域（例如图像和文本）取得了强大的性能，但在其他领域（例如表格数据）其行为则不那么一致。迄今为止，关于PE收敛性的唯一理论分析依赖于对算法行为和敏感数据集结构的不切实际的假设。在这项工作中，我们开发了一个新的理论框架来解释PE的实际行为并确定其收敛的充分条件。对于来自有界域的$n$个数据点的$d$维敏感数据集，我们证明PE能生成一个期望1-Wasserstein距离为$\tilde{O}(d(n\nobreak\\epsilon)^{-1/d})$的$(\nobreak\\epsilon, \nobreak\\delta)$-DP合成数据集，从而建立了算法在$n \to \nobreak\\infty$时的最坏情况收敛性。我们的分析也扩展到一般的Banach空间。我们还将PE与私有符号测度机制（Private Signed Measure Mechanism）联系起来，这是一种迄今为止尚未得到太多实际应用的DP合成数据生成方法。我们在模拟中展示了我们理论发现的实际相关性。", "summary": "私有演化（PE）是一种用于差分隐私（DP）合成数据生成的免训练方法，但在某些领域其行为不够稳定，且现有收敛性理论分析基于不切实际的假设。本文提出了一个新的理论框架，证明了对于$d$维敏感数据集，PE能生成一个期望1-Wasserstein距离为$\tilde{O}(d(n\nobreak\\epsilon)^{-1/d})$的$(\nobreak\\epsilon, \nobreak\\delta)$-DP合成数据集，从而建立了其最坏情况收敛性。该分析还扩展到一般的Banach空间，并首次将PE与私有符号测度机制（PSMM）联系起来。模拟结果也验证了理论发现的实际相关性。", "keywords": "私有演化, 差分隐私, 合成数据, 收敛性, Wasserstein距离", "comments": "本文的创新之处在于为私有演化（PE）提供了一个更全面和严谨的理论收敛性分析，解决了以往分析中不切实际的假设问题。通过建立Wasserstein距离的界限并将其推广到Banach空间，该研究显著增强了对PE在差分隐私合成数据生成中可靠性的理解和信心。此外，与私有符号测度机制的连接也为未来的研究开辟了新的途径。"}}
{"id": "2506.08966", "title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers", "authors": ["Marek Kadlčík", "Michal Štefánik", "Timothee Mickus", "Michal Spiegel", "Josef Kuchař"], "summary": "Pretrained language models (LMs) are prone to arithmetic errors. Existing\nwork showed limited success in probing numeric values from models'\nrepresentations, indicating that these errors can be attributed to the inherent\nunreliability of distributionally learned embeddings in representing exact\nquantities. However, we observe that previous probing methods are inadequate\nfor the emergent structure of learned number embeddings with sinusoidal\npatterns.\n  In response, we propose a novel probing technique that decodes numeric values\nfrom input embeddings with near-perfect accuracy across a range of open-source\nLMs. This proves that after the sole pre-training, LMs represent numbers with\nremarkable precision. Finally, we find that the embeddings' preciseness judged\nby our probe's accuracy explains a large portion of LM's errors in elementary\narithmetic, and show that aligning the embeddings with the pattern discovered\nby our probe can mitigate these errors.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08966v1", "AI": {"title_translation": "预训练语言模型学习到惊人准确的数字表示", "tldr": "尽管预训练语言模型容易出现算术错误，但一项新颖的探测技术表明，它们实际上能以极高的精度表示数字，并且这种表示的精确度可以解释并缓解算术错误。", "motivation": "现有的工作在从语言模型中探测数字值方面成功有限，这表明算术错误归因于分布学习嵌入在表示精确数量方面固有的不可靠性。然而，作者观察到先前的探测方法不足以应对学习到的数字嵌入中出现的正弦模式的涌现结构。", "method": "提出了一种新颖的探测技术，能够以近乎完美的准确度从输入嵌入中解码数字值。", "result": "该技术在各种开源语言模型上以近乎完美的准确度解码数字值；这证明了仅经过预训练后，语言模型就能以惊人的精度表示数字；嵌入的精确度解释了语言模型在基本算术中大部分的错误；将嵌入与探测发现的模式对齐可以缓解这些错误。", "conclusion": "语言模型在预训练后能够以惊人的精度表示数字，并且通过理解和调整这些表示，可以有效缓解语言模型在算术任务中的错误。", "translation": "预训练语言模型（LMs）容易出现算术错误。现有工作在从模型的表示中探测数字值方面成功有限，这表明这些错误可以归因于分布学习嵌入在表示精确数量方面固有的不可靠性。然而，我们观察到先前的探测方法不足以应对学习到的数字嵌入中出现的正弦模式的涌现结构。作为回应，我们提出了一种新颖的探测技术，可以在一系列开源语言模型中以近乎完美的准确度从输入嵌入中解码数字值。这证明了仅经过预训练后，语言模型就能以惊人的精度表示数字。最后，我们发现由我们的探测器判断的嵌入的精确度解释了语言模型在基本算术中很大一部分的错误，并表明将嵌入与我们的探测器发现的模式对齐可以缓解这些错误。", "summary": "本文指出，尽管预训练语言模型存在算术错误且现有探测方法效果不佳，但通过引入一种新颖的探测技术，研究发现这些模型实际上能以惊人的精度表示数字。该研究进一步揭示，模型嵌入的精确度能够解释其在基本算术中的大部分错误，并且通过调整嵌入以匹配探测发现的模式，可以有效减轻这些错误。", "keywords": "预训练语言模型, 数字表示, 探测技术, 算术错误, 嵌入", "comments": "这项研究通过提出一种创新的探测技术，颠覆了先前关于预训练语言模型无法准确表示数字的普遍认知。它揭示了语言模型在预训练阶段已经学习到数字的精确表示，这对于理解语言模型的能力边界及其算术错误根源具有重要意义。此外，该研究还提供了一种潜在的解决方案来缓解语言模型的算术问题，具有重要的实践价值。"}}
{"id": "2506.08352", "title": "Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source Search in Large Language Models", "authors": ["Wentao Shi", "Yiqing Shen"], "summary": "Large language models (LLMs) can face factual limitations when responding to\ntime-sensitive queries about recent events that arise after their knowledge\nthresholds in the training corpus. Existing search-augmented approaches fall\ninto two categories, each with distinct limitations: multi-agent search\nframeworks incur substantial computational overhead by separating search\nplanning and response synthesis across multiple LLMs, while single-LLM\ntool-calling methods restrict themselves to sequential planned, single-query\nsearches from sole search sources. We present Reasoning-Search (R-Search), a\nsingle-LLM search framework that unifies multi-step planning, multi-source\nsearch execution, and answer synthesis within one coherent inference process.\nInnovatively, it structure the output into four explicitly defined components,\nincluding reasoning steps that guide the search process (<think>), a\nnatural-language directed acyclic graph that represents the search plans with\nrespect to diverse sources (<search>), retrieved results from executing the\nsearch plans (<result>), and synthesized final answers (<answer>). To enable\neffective generation of these structured outputs, we propose a specialized\nReinforcement Fine-Tuning (ReFT) method based on GRPO, together with a\nmulti-component reward function that optimizes LLM's answer correctness,\nstructural validity of the generated DAG, and adherence to the defined output\nformat. Experimental evaluation on FinSearchBench-24, SearchExpertBench-25, and\nseven Q and A benchmarks demonstrates that R-Search outperforms\nstate-of-the-art methods, while achieving substantial efficiency gains through\n70% reduction in context token usage and approximately 50% decrease in\nexecution latency. Code is available at\nhttps://github.com/wentao0429/Reasoning-search.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08352v1", "AI": {"title_translation": "大型语言模型中面向多步多源搜索推理的强化微调", "tldr": "R-Search是一个单LLM搜索框架，通过强化微调统一了多步规划、多源搜索执行和答案合成，显著提升了搜索增强型LLM的性能和效率。", "motivation": "大型语言模型（LLMs）在处理时效性查询时面临事实局限性，因为其知识截止日期在训练语料库之后。现有的搜索增强方法要么存在多智能体框架计算开销大的问题，要么单LLM工具调用方法受限于顺序的单查询搜索和单一搜索源。", "method": "本文提出了Reasoning-Search (R-Search)，一个单一LLM搜索框架，它将多步规划、多源搜索执行和答案合成统一在一个连贯的推理过程中。R-Search将输出结构化为四个明确定义的组件：指导搜索过程的推理步骤（<think>）、表示多源搜索计划的自然语言有向无环图（<search>）、执行搜索计划后检索到的结果（<result>）以及合成的最终答案（<answer>）。为有效生成这些结构化输出，作者提出了一种基于GRPO的专用强化微调（ReFT）方法，并结合了一个多组件奖励函数，该函数优化了LLM答案的正确性、生成的DAG结构有效性以及对定义输出格式的遵守。", "result": "在FinSearchBench-24、SearchExpertBench-25和七个问答基准上的实验评估表明，R-Search优于现有最先进的方法，同时通过减少70%的上下文Token使用和大约50%的执行延迟实现了显著的效率提升。", "conclusion": "R-Search通过创新的单LLM框架和强化微调方法，成功解决了现有搜索增强型LLMs的局限性，实现了在性能和效率上的显著提升，为多步多源搜索任务提供了更优的解决方案。", "translation": "大型语言模型（LLMs）在响应关于其训练语料库知识阈值之后出现的近期事件的时效性查询时，可能面临事实局限性。现有的搜索增强方法分为两类，各有其独特的局限性：多智能体搜索框架通过在多个LLM之间分离搜索规划和响应合成，产生大量的计算开销；而单LLM工具调用方法则将自身限制在顺序规划的、单一查询的、来自单一搜索源的搜索。我们提出了Reasoning-Search（R-Search），一个单一LLM搜索框架，它在一个连贯的推理过程中统一了多步规划、多源搜索执行和答案合成。创新地，它将输出结构化为四个明确定义的组件，包括指导搜索过程的推理步骤（<think>）、表示多源搜索计划的自然语言有向无环图（<search>）、执行搜索计划后检索到的结果（<result>）以及合成的最终答案（<answer>）。为了有效生成这些结构化输出，我们提出了一种基于GRPO的专用强化微调（ReFT）方法，以及一个多组件奖励函数，该函数优化了LLM答案的正确性、生成的DAG的结构有效性以及对定义输出格式的遵守。在FinSearchBench-24、SearchExpertBench-25和七个问答基准上的实验评估表明，R-Search优于现有最先进的方法，同时通过减少70%的上下文Token使用和大约50%的执行延迟实现了显著的效率提升。代码可在https://github.com/wentao0429/Reasoning-search 获取。", "summary": "该论文提出了R-Search，一个针对大型语言模型（LLMs）的单LLM搜索框架，旨在解决LLMs在处理时效性信息时的事实局限性及现有搜索增强方法的效率问题。R-Search创新性地将多步规划、多源搜索执行和答案合成整合到单一推理过程中，并结构化输出为思考、搜索、结果和答案四个组件。为优化结构化输出生成，论文引入了基于GRPO的强化微调（ReFT）方法，并设计了一个多组件奖励函数。实验证明，R-Search在多个基准测试中超越了现有技术，并显著降低了Token使用和执行延迟，提升了效率。", "keywords": "大型语言模型, 搜索增强, 强化微调, 多步搜索, 多源搜索", "comments": "这篇论文的创新点在于将多步规划、多源搜索和答案合成统一到一个单一的LLM框架中，并通过引入结构化的输出格式和专门的强化微调方法（ReFT）来优化这一过程。其提出的四组件输出结构（<think>, <search>, <result>, <answer>）为LLM的推理和搜索过程提供了清晰、可控的范式。此外，通过强化学习优化答案正确性、结构有效性和格式遵守，显著提升了模型在复杂搜索任务上的表现和效率，特别是大幅减少了上下文Token使用和执行延迟，这对于实际应用具有重要意义。"}}
{"id": "2506.08355", "title": "A Bi-Orthogonal Structure-Preserving eigensolver for large-scale linear response eigenvalue problem", "authors": ["Yu Li", "Zijing Wang", "Yong Zhang"], "summary": "The linear response eigenvalue problem, which arises from many scientific and\nengineering fields, is quite challenging numerically for large-scale\nsparse/dense system, especially when it has zero eigenvalues. Based on a direct\nsum decomposition of biorthogonal invariant subspaces and the minimization\nprinciples in the biorthogonal complement, using the structure of generalized\nnullspace, we propose a Bi-Orthogonal Structure-Preserving subspace iterative\nsolver, which is stable, efficient, and of excellent parallel scalability. The\nbiorthogonality is of essential importance and created by a modified\nGram-Schmidt biorthogonalization (MGS-Biorth) algorithm. We naturally deflate\nout converged eigenvectors by computing the rest eigenpairs in the biorthogonal\ncomplementary subspace without introducing any artificial parameters. When the\nnumber of requested eigenpairs is large, we propose a moving mechanism to\ncompute them batch by batch such that the projection matrix size is small and\nindependent of the requested eigenpair number. For large-scale problems, one\nonly needs to provide the matrix-vector product, thus waiving explicit matrix\nstorage. The numerical performance is further improved when the matrix-vector\nproduct is implemented using parallel computing. Ample numerical examples are\nprovided to demonstrate the stability, efficiency, and parallel scalability.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08355v1", "AI": {"title_translation": "大型线性响应特征值问题的双正交结构保持特征求解器", "tldr": "提出了一种双正交结构保持子空间迭代求解器，用于解决大规模线性响应特征值问题，具有稳定性、高效性和良好的并行可扩展性，尤其适用于零特征值情况。", "motivation": "线性响应特征值问题在大规模稀疏/稠密系统中存在数值挑战，尤其是在存在零特征值时。", "method": "基于双正交不变子空间的直和分解和双正交补中的最小化原理，利用广义零空间结构，提出了一种双正交结构保持子空间迭代求解器。通过修改的Gram-Schmidt双正交化 (MGS-Biorth) 算法创建双正交性。该方法通过在双正交补子空间中计算其余特征对来自然地消除收敛的特征向量，无需引入任何人工参数。当请求的特征对数量较大时，提出了一种分批计算的移动机制，使得投影矩阵大小小且独立于请求的特征对数量。对于大规模问题，只需提供矩阵-向量乘积，无需显式矩阵存储。通过并行计算实现矩阵-向量乘积进一步提高了数值性能。", "result": "提供了充足的数值例子来证明其稳定性、效率和并行可扩展性。", "conclusion": "该双正交结构保持子空间迭代求解器稳定、高效且具有出色的并行可扩展性，能够有效解决大规模线性响应特征值问题，即使存在零特征值。", "translation": "线性响应特征值问题源于许多科学和工程领域，对于大规模稀疏/稠密系统来说，其数值求解极具挑战性，尤其是在存在零特征值的情况下。基于双正交不变子空间的直和分解和双正交补中的最小化原理，并利用广义零空间的结构，我们提出了一种双正交结构保持子空间迭代求解器，该求解器稳定、高效且具有出色的并行可扩展性。双正交性至关重要，并通过改进的Gram-Schmidt双正交化 (MGS-Biorth) 算法创建。我们通过在双正交补子空间中计算其余特征对来自然地消除收敛的特征向量，而无需引入任何人工参数。当请求的特征对数量较大时，我们提出了一种移动机制来分批计算它们，使得投影矩阵的大小较小且独立于请求的特征对数量。对于大规模问题，只需提供矩阵-向量乘积，从而省去了显式矩阵存储。当矩阵-向量乘积通过并行计算实现时，数值性能得到进一步提升。提供了充足的数值例子来证明其稳定性、效率和并行可扩展性。", "summary": "本文提出了一种新型的双正交结构保持子空间迭代求解器，专门用于解决大规模线性响应特征值问题，包括存在零特征值的情况。该方法利用双正交不变子空间分解和补空间最小化原理，并通过改进的Gram-Schmidt双正交化确保双正交性。它能有效处理大规模问题，支持分批计算，并仅需矩阵-向量乘积，具有出色的稳定性、效率和并行可扩展性。", "keywords": "线性响应特征值问题, 双正交, 子空间迭代, 大规模问题, 并行计算", "comments": "该论文的创新点在于提出了一个专门针对大规模线性响应特征值问题的双正交结构保持求解器，尤其考虑了零特征值的情况。其通过MGS-Biorth算法引入双正交性，并设计了无参数的收敛特征向量消除机制和分批计算策略，有效解决了大规模计算的挑战。并行计算的集成进一步提升了其实用性。"}}
{"id": "2506.08372", "title": "Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages", "authors": ["Rishabh Ranjan", "Likhith Ayinala", "Mayank Vatsa", "Richa Singh"], "summary": "This paper introduces a novel multimodal framework for hate speech detection\nin deepfake audio, excelling even in zero-shot scenarios. Unlike previous\napproaches, our method uses contrastive learning to jointly align audio and\ntext representations across languages. We present the first benchmark dataset\nwith 127,290 paired text and synthesized speech samples in six languages:\nEnglish and five low-resource Indian languages (Hindi, Bengali, Marathi, Tamil,\nTelugu). Our model learns a shared semantic embedding space, enabling robust\ncross-lingual and cross-modal classification. Experiments on two multilingual\ntest sets show our approach outperforms baselines, achieving accuracies of\n0.819 and 0.701, and generalizes well to unseen languages. This demonstrates\nthe advantage of combining modalities for hate speech detection in synthetic\nmedia, especially in low-resource settings where unimodal models falter. The\nDataset is available at https://www.iab-rubric.org/resources.", "comment": "Accepted in Interpseech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08372v1", "AI": {"title_translation": "多模态零样本低资源语言深度伪造仇恨言论检测框架", "tldr": "本文提出了一种新颖的多模态框架，用于在零样本场景下检测深度伪造音频中的仇恨言论，特别适用于低资源语言。", "motivation": "传统的仇恨言论检测方法在深度伪造音频和低资源语言环境中表现不佳。本文旨在解决这一挑战，通过结合多模态信息来提高检测效果，尤其是在缺乏大量标注数据的情况下。", "method": "本研究引入了一种新颖的多模态框架，利用对比学习联合对齐跨语言的音频和文本表示。研究团队构建了首个包含127,290个配对文本和合成语音样本的基准数据集，涵盖英语和五种低资源印度语言（印地语、孟加拉语、马拉地语、泰米尔语、泰卢固语）。模型学习一个共享的语义嵌入空间，以实现鲁棒的跨语言和跨模态分类。", "result": "在两个多语言测试集上的实验表明，该方法优于基线模型，分别达到了0.819和0.701的准确率，并且能够很好地泛化到未见过的语言。这证明了在合成媒体中结合多模态信息进行仇恨言论检测的优势，尤其是在单模态模型表现不佳的低资源环境中。", "conclusion": "结合多模态信息对于在合成媒体中进行仇恨言论检测具有显著优势，尤其是在低资源设置下，能够有效克服单模态模型的局限性。", "translation": "本文介绍了一种新颖的多模态框架，用于在深度伪造音频中检测仇恨言论，即使在零样本场景下也表现出色。与以往的方法不同，我们的方法使用对比学习来联合对齐跨语言的音频和文本表示。我们提出了第一个基准数据集，包含127,290个配对的文本和合成语音样本，涵盖六种语言：英语和五种低资源印度语言（印地语、孟加拉语、马拉地语、泰米尔语、泰卢固语）。我们的模型学习一个共享的语义嵌入空间，从而实现鲁棒的跨语言和跨模态分类。在两个多语言测试集上的实验表明，我们的方法优于基线，分别达到了0.819和0.701的准确率，并且能够很好地泛化到未见过的语言。这证明了在合成媒体中结合模态进行仇恨言论检测的优势，尤其是在单模态模型表现不佳的低资源环境中。数据集可在 https://www.iab-rubric.org/resources 获取。", "summary": "本文提出了一种新颖的多模态零样本框架，用于在深度伪造音频中检测仇恨言论，特别关注低资源语言。该框架利用对比学习来联合对齐音频和文本表示，并引入了首个包含多种语言（包括五种低资源印度语言）的深度伪造仇恨言论检测基准数据集。实验结果表明，该模型在跨语言和跨模态分类方面表现出色，优于现有基线，并在低资源环境下展现出强大的泛化能力，证明了多模态方法在合成媒体仇恨言论检测中的有效性。", "keywords": "多模态, 深度伪造, 仇恨言论检测, 零样本, 低资源语言", "comments": "该论文的创新之处在于提出了一个新颖的多模态零样本框架，并首次构建了一个大规模多语言深度伪造仇恨言论检测数据集，尤其关注了低资源语言。这对于解决当前检测技术在合成媒体和语言多样性方面的局限性具有重要意义。其在低资源环境下的优异表现，凸显了多模态方法在实际应用中的巨大潜力。"}}
{"id": "2506.08828", "title": "Lightweight Electronic Signatures and Reliable Access Control Included in Sensor Networks to Prevent Cyber Attacks from Modifying Patient Data", "authors": ["Mishall Al-Zubaidie"], "summary": "Digital terrorism is a major cause of securing patient/healthcare providers\ndata and information. Sensitive topics that may have an impact on a patient's\nhealth or even national security include patient health records and information\non healthcare providers. Health databases and data sets have been continually\nbreached by many, regular assaults, as well as local and remote servers\nequipped with wireless sensor networks (WSNs) in diverse locations. The problem\nwas addressed by some contemporary strategies that were created to stop these\nassaults and guarantee the privacy of patient data and information transferred\nand gathered by sensors. Nevertheless, the literature analysis outlines many\nindications of weakness that persist in these methods. This study suggests a\nnovel, reliable method that bolsters the information security and data gathered\nby sensors and kept on base station datasets. The proposed approach combines a\nnumber of security mechanisms, including symmetric cryptography for encryption,\nasymmetric cryptography for access control and signatures, and the Lesamnta-LW\nmethod in the signature process. Users' information is shielded from prying\neyes by the careful application of these measures and a sound approach.\nInvestigational comparisons, security studies, and thorough results show that\nthe suggested method is better than earlier methods.", "comment": "22 pages, 11 figures, conference", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08828v1", "AI": {"title_translation": "传感器网络中包含轻量级电子签名和可靠访问控制，以防止网络攻击修改患者数据", "tldr": "本文提出一种结合对称加密、非对称加密和Lesamnta-LW方法的新型安全机制，用于传感器网络中的患者数据保护，以应对现有方法的弱点。", "motivation": "数字恐怖主义导致患者/医疗服务提供者数据面临重大安全威胁，健康数据库和数据集经常遭受网络攻击。现有策略存在弱点，无法充分保障传感器传输和收集的患者数据隐私。", "method": "提出一种新的可靠方法，增强传感器收集并存储在基站数据集上的信息安全性。该方法结合多种安全机制，包括用于加密的对称密码学、用于访问控制和签名的非对称密码学，以及签名过程中的Lesamnta-LW方法。", "result": "调查性比较、安全研究和详尽结果表明，所提出的方法优于现有方法。", "conclusion": "所提出的结合对称加密、非对称加密和Lesamnta-LW签名的新型安全方法，能有效保护传感器网络中的患者数据，并优于现有技术。", "translation": "数字恐怖主义是保护患者/医疗服务提供者数据和信息的主要原因。可能影响患者健康甚至国家安全的敏感话题包括患者健康记录和医疗服务提供者信息。健康数据库和数据集不断遭到许多常规攻击以及配备无线传感器网络（WSN）的本地和远程服务器的持续入侵。一些当代策略旨在阻止这些攻击并保证传感器传输和收集的患者数据和信息的隐私，从而解决了这个问题。然而，文献分析概述了这些方法中仍然存在的许多弱点迹象。本研究提出了一种新颖、可靠的方法，它增强了传感器收集并存储在基站数据集上的信息安全性。所提出的方法结合了多种安全机制，包括用于加密的对称密码学、用于访问控制和签名的非对称密码学，以及签名过程中的Lesamnta-LW方法。通过仔细应用这些措施和健全的方法，用户的信息可以免受窥探。调查性比较、安全研究和详尽结果表明，所提出的方法优于早期方法。", "summary": "针对数字恐怖主义对患者数据安全造成的威胁以及现有防护措施的弱点，本文提出了一种在传感器网络中保护患者数据的新型可靠方法。该方法整合了对称加密、非对称加密（用于访问控制和签名）以及Lesamnta-LW签名算法，旨在增强传感器收集并存储在基站数据集上的信息安全性。实验结果表明，该方法在安全性方面优于现有技术。", "keywords": "电子签名, 访问控制, 传感器网络, 患者数据, 网络安全", "comments": "本文针对医疗健康领域患者数据安全面临的网络攻击威胁，提出了一种结合多种密码学技术（对称加密、非对称加密、Lesamnta-LW签名）的创新安全框架。其亮点在于将这些技术应用于传感器网络环境，以增强数据完整性和访问控制。该研究的重要性在于为保护敏感医疗数据提供了更可靠的解决方案，弥补了现有方法的不足。"}}
{"id": "2506.08628", "title": "Logic Mining from Process Logs: Towards Automated Specification and Verification", "authors": ["Radoslaw Klimek", "Julia Witek"], "summary": "Logical specifications play a key role in the formal analysis of behavioural\nmodels. Automating the derivation of such specifications is particularly\nvaluable in complex systems, where manual construction is time-consuming and\nerror-prone. This article presents an approach for generating logical\nspecifications from process models discovered via workflow mining, combining\npattern-based translation with automated reasoning techniques. In contrast to\nearlier work, we evaluate the method on both general-purpose and real-case\nevent logs, enabling a broader empirical assessment. The study examines the\nimpact of data quality, particularly noise, on the structure and testability of\ngenerated specifications. Using automated theorem provers, we validate a\nvariety of logical properties, including satisfiability, internal consistency,\nand alignment with predefined requirements. The results support the\napplicability of the approach in realistic settings and its potential\nintegration into empirical software engineering practices.", "comment": "This is a draft version of the article submitted to a journal from\n  the JCR list", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08628v1", "AI": {"title_translation": "从流程日志中进行逻辑挖掘：迈向自动化规范和验证", "tldr": "本文提出了一种从流程日志中自动生成逻辑规范的方法，并对其在真实场景下的适用性进行了评估和验证。", "motivation": "在复杂系统中，手动构建行为模型的逻辑规范耗时且容易出错，因此自动化推导这些规范非常有价值。", "method": "该方法通过工作流挖掘发现流程模型，结合基于模式的转换和自动化推理技术来生成逻辑规范。它还使用自动化定理证明器来验证逻辑属性，并评估了数据质量（特别是噪声）对生成规范的影响。", "result": "研究结果支持该方法在实际环境中的适用性，并表明其有潜力整合到经验软件工程实践中。", "conclusion": "该方法能够从流程日志中有效地生成逻辑规范，并在实际场景中表现出良好的适用性，有望应用于经验软件工程实践。", "translation": "逻辑规范在行为模型的形式分析中起着关键作用。在复杂系统中，自动化推导此类规范特别有价值，因为手动构建既耗时又容易出错。本文提出了一种从通过工作流挖掘发现的流程模型中生成逻辑规范的方法，该方法结合了基于模式的转换和自动化推理技术。与早期工作相比，我们在通用和真实案例事件日志上评估了该方法，从而实现了更广泛的经验评估。本研究考察了数据质量，特别是噪声，对生成规范的结构和可测试性的影响。我们使用自动化定理证明器验证了各种逻辑属性，包括可满足性、内部一致性以及与预定义要求的一致性。结果支持该方法在实际环境中的适用性及其潜在整合到经验软件工程实践中。", "summary": "本文提出了一种通过结合模式转换和自动化推理技术，从工作流挖掘发现的流程模型中自动生成逻辑规范的方法。该研究在通用和真实事件日志上评估了该方法，并探讨了数据质量（特别是噪声）对其影响。通过自动化定理证明器，验证了生成规范的各种逻辑属性。结果表明该方法在实际应用中具有可行性，并有望融入经验软件工程实践。", "keywords": "逻辑挖掘, 流程日志, 自动化规范, 行为模型, 形式验证", "comments": "本文的创新之处在于其将模式转换与自动化推理技术相结合，实现了从流程日志中自动化生成逻辑规范。此外，研究在通用和真实案例日志上进行了广泛的实证评估，并考虑了数据质量的影响，这增强了其在实际应用中的说服力。该方法在软件工程领域自动化规范和验证方面具有重要意义。"}}
{"id": "2506.08440", "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08440v1", "AI": {"title_translation": "TGRPO：通过轨迹级组相对策略优化微调视觉-语言-动作模型", "tldr": "TGRPO提出了一种新的在线强化学习方法，通过融合步级和轨迹级优势信号来微调视觉-语言-动作（VLA）模型，在机器人操作任务中表现出更鲁棒和高效的策略。", "motivation": "现有的视觉-语言-动作（VLA）模型微调方法主要依赖于监督式微调（SFT）和静态轨迹数据集，这限制了机器人与环境的交互，无法利用实时执行反馈，并且其成功严重依赖于收集到的轨迹数据的规模和质量。强化学习（RL）提供了一种有前景的替代方案，可以实现闭环交互并将学习到的策略与任务目标直接对齐。", "method": "本文提出了轨迹级组相对策略优化（TGRPO）方法。该方法通过融合步级（step-level）和轨迹级（trajectory-level）优势信号，改进了GRPO的组级优势估计，使其更适合VLA模型的在线强化学习训练。", "result": "在libero-object基准测试的十项操作任务上的实验结果表明，TGRPO始终优于各种基线方法，能够在多个测试场景中生成更鲁棒和高效的策略。", "conclusion": "TGRPO通过融合步级和轨迹级优势信号，有效改进了VLA模型在在线强化学习训练中的表现，在机器人操作任务中生成了更鲁棒和高效的策略。", "translation": "视觉-语言-动作（VLA）模型在大型数据集上预训练后，在各种场景、任务和机器人平台上展现出强大的泛化能力。然而，这些模型在新环境中仍然需要针对特定任务进行微调，这一过程几乎完全依赖于使用静态轨迹数据集的监督式微调（SFT）。这种方法既不允许机器人与环境交互，也无法利用实时执行的反馈。此外，它们的成功关键取决于所收集轨迹的大小和质量。强化学习（RL）通过实现闭环交互和将学习到的策略直接与任务目标对齐，提供了一种有前景的替代方案。在这项工作中，我们借鉴了GRPO的思想，提出了轨迹级组相对策略优化（TGRPO）方法。通过融合步级和轨迹级优势信号，该方法改进了GRPO的组级优势估计，从而使算法更适合VLA的在线强化学习训练。在libero-object基准测试的十项操作任务上的实验结果表明，TGRPO始终优于各种基线方法，能够在多个测试场景中生成更鲁棒和高效的策略。我们的源代码可在以下网址获取：https://github.com/hahans/TGRPO", "summary": "本文提出了一种名为轨迹级组相对策略优化（TGRPO）的新型在线强化学习方法，旨在解决视觉-语言-动作（VLA）模型在陌生环境中微调时，传统监督式微调（SFT）方法依赖静态数据且无法利用实时交互反馈的局限性。TGRPO通过融合步级和轨迹级优势信号来改进GRPO的组级优势估计，从而使VLA模型能够进行更有效的在线训练。在libero-object基准测试的十项操作任务上的实验结果表明，TGRPO在生成更鲁棒和高效的策略方面，持续优于现有基线方法。", "keywords": "视觉-语言-动作模型, 强化学习, 在线微调, 策略优化, 机器人操作", "comments": "这项工作通过引入轨迹级和步级优势信号的融合，为VLA模型的在线强化学习微调提供了一个创新方案。它解决了传统SFT方法在实时交互和数据依赖性方面的局限性，提高了VLA模型在复杂机器人操作任务中的适应性和性能。其重要性在于推动了VLA模型从离线预训练向更灵活、适应性强的在线学习范式转变，为未来的机器人自主学习奠定了基础。"}}
{"id": "2506.08210", "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills.", "comment": "CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08210v1", "AI": {"title_translation": "解码器专用大型语言模型在文本到图像生成中的综合研究", "tldr": "本研究探讨了使用现代解码器专用大型语言模型（LLMs）作为文本编码器在文本到图像生成中的有效性。实验表明，通过对所有层进行层归一化平均来提取嵌入，可以显著提高与复杂提示的对齐，并且大多数LLMs在这种条件下表现优于基线T5模型。", "motivation": "许多文本到图像模型仍然使用过时的T5和CLIP作为文本编码器，这限制了其性能。本研究旨在探索使用现代解码器专用大型语言模型（LLMs）作为文本编码器，以提升文本到图像扩散模型的生成能力。", "method": "研究构建了一个标准化的训练和评估流程，以隔离和评估不同文本嵌入的效果。总共训练了27个文本到图像模型，使用了12种不同的文本编码器，分析了LLMs影响文本到图像生成的关键方面，包括嵌入提取方法、不同LLM变体和模型大小。", "result": "实验发现，使用最后一层嵌入作为条件会导致性能下降。相反，探索不同层的嵌入后发现，对所有层进行层归一化平均显著改善了与复杂提示的对齐。大多数使用这种条件设置的LLMs都优于基线T5模型，显示出在高级视觉语言推理能力方面的增强性能。", "conclusion": "使用现代解码器专用大型语言模型作为文本编码器，并通过对所有层进行层归一化平均提取嵌入，可以显著提升文本到图像生成模型的性能，尤其是在处理复杂提示和视觉语言推理方面，优于传统的T5基线模型。", "translation": "文本到图像生成和大型语言模型（LLMs）都取得了显著进展。然而，许多文本到图像模型仍采用略显过时的T5和CLIP作为其文本编码器。在这项工作中，我们研究了使用现代解码器专用LLMs作为文本到图像扩散模型的文本编码器的有效性。我们建立了一个标准化的训练和评估流程，使我们能够隔离和评估不同文本嵌入的效果。我们总共训练了27个文本到图像模型，使用了12种不同的文本编码器，以分析可能影响文本到图像生成的LLMs的关键方面，包括提取嵌入的方法、不同的LLM变体和模型大小。我们的实验表明，使用最后一层嵌入作为条件的实际方法会导致性能下降。相反，我们探索了来自不同层的嵌入，发现对所有层进行层归一化平均显著改善了与复杂提示的对齐。大多数采用这种条件的LLMs都优于基线T5模型，显示出在高级视觉语言推理能力方面的增强性能。", "summary": "本研究旨在解决当前文本到图像模型中普遍采用过时文本编码器（如T5和CLIP）的问题。通过系统地探索现代解码器专用大型语言模型（LLMs）作为文本编码器的潜力，研究构建了一个标准化的训练和评估流程，并对27个模型和12种编码器进行了广泛实验。核心发现是，传统的仅使用最后一层嵌入的方法效果不佳；相反，对所有层进行层归一化平均可以显著提升文本嵌入与复杂提示的对齐度。实验结果表明，多数采用这种新嵌入策略的LLMs在文本到图像生成任务中表现优于T5基线模型，特别是在高级视觉语言推理方面展现出更强的能力。", "keywords": "文本到图像生成, 大型语言模型, 文本编码器, 扩散模型, 嵌入提取嵌入", "comments": "该研究的创新之处在于系统地评估了现代解码器专用LLMs作为文本编码器的潜力，并揭示了传统嵌入提取方法的局限性。通过引入“所有层层归一化平均”的嵌入提取策略，显著提升了模型对复杂提示的理解和生成能力，为文本到图像领域带来了性能上的飞跃。这对于推动多模态AI的发展具有重要意义。"}}
{"id": "2506.08390", "title": "On Reasoning Strength Planning in Large Reasoning Models", "authors": ["Leheng Sheng", "An Zhang", "Zijian Wu", "Weixiang Zhao", "Changshuo Shen", "Yi Zhang", "Xiang Wang", "Tat-Seng Chua"], "summary": "Recent studies empirically reveal that large reasoning models (LRMs) can\nautomatically allocate more reasoning strengths (i.e., the number of reasoning\ntokens) for harder problems, exhibiting difficulty-awareness for better task\nperformance. While this automatic reasoning strength allocation phenomenon has\nbeen widely observed, its underlying mechanism remains largely unexplored. To\nthis end, we provide explanations for this phenomenon from the perspective of\nmodel activations. We find evidence that LRMs pre-plan the reasoning strengths\nin their activations even before generation, with this reasoning strength\ncausally controlled by the magnitude of a pre-allocated directional vector.\nSpecifically, we show that the number of reasoning tokens is predictable solely\nbased on the question activations using linear probes, indicating that LRMs\nestimate the required reasoning strength in advance. We then uncover that LRMs\nencode this reasoning strength through a pre-allocated directional vector\nembedded in the activations of the model, where the vector's magnitude\nmodulates the reasoning strength. Subtracting this vector can lead to reduced\nreasoning token number and performance, while adding this vector can lead to\nincreased reasoning token number and even improved performance. We further\nreveal that this direction vector consistently yields positive reasoning length\nprediction, and it modifies the logits of end-of-reasoning token </think> to\naffect the reasoning length. Finally, we demonstrate two potential applications\nof our findings: overthinking behavior detection and enabling efficient\nreasoning on simple problems. Our work provides new insights into the internal\nmechanisms of reasoning in LRMs and offers practical tools for controlling\ntheir reasoning behaviors. Our code is available at\nhttps://github.com/AlphaLab-USTC/LRM-plans-CoT.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08390v1", "AI": {"title_translation": "大型推理模型中的推理强度规划", "tldr": "大型推理模型在生成前就在其激活中预先规划推理强度（即token数量），这种强度由一个预分配的方向向量的幅度控制，该向量可被操纵以实现过度思考检测和高效推理等应用。", "motivation": "最近的研究发现大型推理模型（LRMs）能自动为更难的问题分配更多推理token，但其背后的机制尚不清楚，本研究旨在解释这一现象。", "method": "本研究从模型激活的角度解释LRMs的推理强度规划现象。通过线性探测器预测问题激活中的推理token数量，揭示LRMs提前估计所需推理强度。发现LRMs通过激活中预分配的方向向量来编码推理强度，其幅度调节推理强度。通过增减此向量来观察其对推理token数量和性能的影响。此外，还揭示该向量能持续预测正向推理长度并修改推理结束token </think> 的logits。", "result": "LRMs在生成前就在其激活中预先规划推理强度，这种强度由一个预分配的方向向量的幅度因果控制。推理token数量仅基于问题激活即可预测。减去该向量会降低推理token数量和性能，而添加该向量可增加推理token数量甚至提升性能。该方向向量始终产生正向推理长度预测，并修改推理结束token </think> 的logits以影响推理长度。研究还展示了两个潜在应用：过度思考行为检测和在简单问题上实现高效推理。", "conclusion": "本研究为LRMs内部推理机制提供了新见解，并为控制其推理行为提供了实用工具。", "translation": "最近的研究经验性地揭示，大型推理模型（LRMs）能够自动为更难的问题分配更多的推理强度（即推理token的数量），表现出对难度的感知，以获得更好的任务性能。尽管这种自动推理强度分配现象已被广泛观察到，但其潜在机制在很大程度上仍未被探索。为此，我们从模型激活的角度对这一现象提供了解释。我们发现有证据表明，LRMs甚至在生成之前就在其激活中预先规划了推理强度，并且这种推理强度由一个预分配的方向向量的幅度因果地控制。具体来说，我们展示了仅基于问题激活使用线性探测器就可以预测推理token的数量，这表明LRMs提前估计了所需的推理强度。然后我们揭示，LRMs通过嵌入在模型激活中的一个预分配的方向向量来编码这种推理强度，其中该向量的幅度调节推理强度。减去这个向量可以导致推理token数量和性能的降低，而添加这个向量可以导致推理token数量的增加甚至性能的提升。我们进一步揭示，这个方向向量始终产生正向的推理长度预测，并且它修改了推理结束token </think> 的logits以影响推理长度。最后，我们展示了我们发现的两个潜在应用：过度思考行为检测和在简单问题上实现高效推理。我们的工作为LRMs中推理的内部机制提供了新见解，并为控制其推理行为提供了实用工具。我们的代码可在 https://github.com/AlphaLab-USTC/LRM-plans-CoT 获取。", "summary": "该论文探讨了大型推理模型（LRMs）如何根据问题难度调整推理强度（token计数）的机制。研究揭示LRMs在激活中预先规划推理强度，并由一个预分配的方向向量的幅度控制。这种强度可以从问题激活中预测。操纵该向量可以改变推理长度和性能。这些发现为LRM的推理机制提供了见解，并为控制其行为提供了实用工具，可应用于过度思考检测和高效推理。", "keywords": "大型推理模型, 推理强度, 模型激活, 机制可解释性, 方向向量", "comments": "该论文通过深入研究大型推理模型的机制可解释性，对它们如何根据问题难度调整推理努力做出了重要贡献。识别出一个“预分配的方向向量”在激活中因果地控制推理强度是一个新颖的发现。这不仅解释了一个已观察到的现象，还为控制LRM行为提供了一个具体的手段，为优化推理效率和检测过度思考等问题开辟了途径。所展示的实际应用突出了这些机制洞察的即时实用性。"}}
{"id": "2506.08062", "title": "FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning", "authors": ["Woosung Kim", "Jinho Lee", "Jongmin Lee", "Byung-Jun Lee"], "summary": "Multi-objective reinforcement learning (MORL) aims to optimize policies in\nthe presence of conflicting objectives, where linear scalarization is commonly\nused to reduce vector-valued returns into scalar signals. While effective for\ncertain preferences, this approach cannot capture fairness-oriented goals such\nas Nash social welfare or max-min fairness, which require nonlinear and\nnon-additive trade-offs. Although several online algorithms have been proposed\nfor specific fairness objectives, a unified approach for optimizing nonlinear\nwelfare criteria in the offline setting-where learning must proceed from a\nfixed dataset-remains unexplored. In this work, we present FairDICE, the first\noffline MORL framework that directly optimizes nonlinear welfare objective.\nFairDICE leverages distribution correction estimation to jointly account for\nwelfare maximization and distributional regularization, enabling stable and\nsample-efficient learning without requiring explicit preference weights or\nexhaustive weight search. Across multiple offline benchmarks, FairDICE\ndemonstrates strong fairness-aware performance compared to existing baselines.", "comment": "Multi-objective Reinforcement Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08062v1", "AI": {"title_translation": "公平DICE：公平驱动的离线多目标强化学习", "tldr": "FairDICE是首个直接优化非线性福利目标的离线多目标强化学习框架，无需权重搜索即可实现公平感知性能。", "motivation": "传统多目标强化学习（MORL）中的线性标量化方法无法捕捉纳什社会福利或最大最小公平性等公平导向目标，这些目标需要非线性、非加性的权衡。尽管已有在线算法，但离线环境下优化非线性福利标准的统一方法仍未被探索。", "method": "提出FairDICE，一个离线MORL框架，通过分布校正估计（distribution correction estimation）直接优化非线性福利目标，同时考虑福利最大化和分布正则化，实现稳定且样本高效的学习，无需明确的偏好权重或详尽的权重搜索。", "result": "在多个离线基准测试中，FairDICE与现有基线相比，表现出强大的公平感知性能。", "conclusion": "FairDICE成功解决了离线多目标强化学习中优化非线性公平福利目标的挑战，并展示了其有效性。", "translation": "多目标强化学习（MORL）旨在存在冲突目标的情况下优化策略，其中线性标量化常用于将向量值回报转换为标量信号。虽然对某些偏好有效，但这种方法无法捕捉公平导向的目标，例如纳什社会福利或最大最小公平性，这些目标需要非线性、非加性的权衡。尽管已经提出了几种针对特定公平目标的在线算法，但在离线设置（学习必须从固定数据集进行）中优化非线性福利标准的统一方法仍未被探索。在这项工作中，我们提出了FairDICE，这是第一个直接优化非线性福利目标的离线MORL框架。FairDICE利用分布校正估计来共同考虑福利最大化和分布正则化，从而在不需要明确偏好权重或详尽权重搜索的情况下实现稳定且样本高效的学习。在多个离线基准测试中，FairDICE与现有基线相比，表现出强大的公平感知性能。", "summary": "本文提出了FairDICE，一个新颖的离线多目标强化学习框架，旨在克服传统线性标量化方法在处理非线性、非加性公平目标（如纳什社会福利和最大最小公平性）时的局限性。FairDICE通过分布校正估计，直接优化非线性福利目标，并结合福利最大化与分布正则化，实现了稳定且样本高效的学习，且无需预设偏好权重。实验结果表明，FairDICE在多个离线基准测试中展现了出色的公平感知性能。", "keywords": "离线强化学习, 多目标强化学习, 公平性, 非线性福利目标, 分布校正估计", "comments": "该论文的创新点在于提出了首个针对离线多目标强化学习的统一框架FairDICE，能够直接优化复杂的非线性公平福利目标，解决了现有方法无法有效处理此类问题的痛点。其无需显式权重或详尽搜索的特点，显著提高了实用性和效率。"}}
{"id": "2506.08686", "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths", "authors": ["Soham Poddar", "Paramita Koley", "Janardan Misra", "Sanjay Podder", "Navveen Balani", "Niloy Ganguly", "Saptarshi Ghosh"], "summary": "A significant portion of the energy consumed by Large Language Models (LLMs)\narises from their inference processes; hence developing energy-efficient\nmethods for inference is crucial. While several techniques exist for inference\noptimization, output compression remains relatively unexplored, with only a few\npreliminary efforts addressing this aspect. In this work, we first benchmark 12\ndecoder-only LLMs across 5 datasets, revealing that these models often produce\nresponses that are substantially longer than necessary. We then conduct a\ncomprehensive quality assessment of LLM responses, formally defining six\ninformation categories present in LLM responses. We show that LLMs often tend\nto include redundant or additional information besides the minimal answer. To\naddress this issue of long responses by LLMs, we explore several simple and\nintuitive prompt-engineering strategies. Empirical evaluation shows that\nappropriate prompts targeting length reduction and controlling information\ncontent can achieve significant energy optimization between 25-60\\% by reducing\nthe response length while preserving the quality of LLM responses.", "comment": "Accepted to appear at the ACL 2025 findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08686v1", "AI": {"title_translation": "简洁是可持续发展的灵魂：表征大型语言模型响应长度", "tldr": "研究发现大型语言模型（LLMs）的响应通常过长，通过提示工程可有效缩短响应长度，从而显著节省能源并保持质量。", "motivation": "大型语言模型（LLMs）的推理过程消耗大量能源，而输出压缩作为节能方法尚未得到充分探索。LLMs的响应通常比必要长很多。", "method": "首先，在5个数据集上对12个解码器-only LLMs进行了基准测试，揭示响应过长。其次，对LLM响应进行了全面的质量评估，正式定义了六种信息类别，并发现LLMs常包含冗余信息。最后，探索了几种简单直观的提示工程策略来缩短LLM响应。", "result": "经验评估表明，针对长度缩减和信息内容控制的适当提示，可以在保持LLM响应质量的同时，将响应长度缩短25-60%，从而实现显著的能源优化。", "conclusion": "通过简单的提示工程策略可以有效缩短LLM响应长度，显著降低能源消耗，同时不影响响应质量，为LLM的能源效率优化提供了新途径。", "translation": "大型语言模型（LLMs）消耗的大部分能源来自其推理过程；因此，开发节能的推理方法至关重要。尽管存在多种推理优化技术，但输出压缩仍相对未被探索，只有少数初步工作涉及这方面。在这项工作中，我们首先在5个数据集上对12个仅解码器LLMs进行了基准测试，揭示这些模型通常产生比必要长得多的响应。然后，我们对LLM响应进行了全面的质量评估，正式定义了LLM响应中存在的六种信息类别。我们发现LLMs除了最少答案外，通常倾向于包含冗余或额外信息。为了解决LLMs响应过长的问题，我们探索了几种简单直观的提示工程策略。经验评估表明，针对长度缩减和信息内容控制的适当提示，可以在保持LLM响应质量的同时，将响应长度缩短25-60%，从而实现显著的能源优化。", "summary": "本文研究了大型语言模型（LLMs）响应长度对能源消耗的影响。研究发现，LLMs在推理过程中常生成冗余或过长的响应。通过对12个LLMs进行基准测试和质量评估，作者定义了响应中的信息类别。为解决此问题，文章探索了提示工程策略，并证明通过控制响应长度和信息内容，可在保持质量的同时，将能源消耗显著降低25-60%。", "keywords": "大型语言模型, 能源效率, 提示工程, 响应长度, 输出压缩", "comments": "这项工作具有重要的实际意义，因为它直接解决了LLMs日益增长的能源消耗问题。通过简单的提示工程而非复杂的模型架构修改或训练，实现了显著的节能效果，这提供了一种成本效益高且易于实施的优化方案。其创新点在于将“简洁”与“可持续性”联系起来，并量化了响应长度对能源的影响。"}}
{"id": "2506.08805", "title": "Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration", "authors": ["Stina Klein", "Pooja Prajod", "Katharina Weitz", "Matteo Lavit Nicora", "Dimitra Tsovaltzi", "Elisabeth André"], "summary": "The integration of collaborative robots (cobots) in industrial settings\nraises concerns about worker well-being, particularly due to reduced social\ninteractions. Avatars - designed to facilitate worker interactions and\nengagement - are promising solutions to enhance the human-robot collaboration\n(HRC) experience. However, real-world perspectives on avatar-supported HRC\nremain unexplored. To address this gap, we conducted a focus group study with\nemployees from a German manufacturing company that uses cobots. Before the\ndiscussion, participants engaged with a scripted, industry-like HRC demo in a\nlab setting. This qualitative approach provided valuable insights into the\navatar's potential roles, improvements to its behavior, and practical\nconsiderations for deploying them in industrial workcells. Our findings also\nemphasize the importance of personalized communication and task assistance.\nAlthough our study's limitations restrict its generalizability, it serves as an\ninitial step in recognizing the potential of adaptive, context-aware avatar\ninteractions in real-world industrial environments.", "comment": "Accepted LBW at CHIWORK 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08805v1", "AI": {"title_translation": "在工业5.0中通过化身进行交流：一项关于人机协作的焦点小组研究", "tldr": "本研究通过对一家德国制造公司员工的焦点小组研究，探索了在工业5.0中，使用化身来增强人机协作中工人互动和福祉的潜力，发现化身在实际工业环境中可以促进个性化沟通和任务协助。", "motivation": "工业环境中协作机器人（cobots）的整合引发了对工人福祉的担忧，特别是由于社会互动减少。化身作为促进工人互动和参与的解决方案，有望增强人机协作（HRC）体验。然而，关于化身支持的HRC的真实世界视角尚未被探索。", "method": "本研究采用定性方法，对一家使用协作机器人的德国制造公司的员工进行了一项焦点小组研究。在讨论之前，参与者在实验室环境中参与了一个预设的工业化人机协作（HRC）演示。", "result": "研究提供了关于化身潜在角色、其行为改进以及在工业工作单元中部署的实际考虑因素的宝贵见解。研究结果还强调了个性化沟通和任务协助的重要性。", "conclusion": "尽管本研究的局限性限制了其普遍性，但它是在认识适应性、上下文感知化身互动在真实世界工业环境中潜力的初步步骤。", "translation": "工业环境中协作机器人（cobots）的整合引发了对工人福祉的担忧，特别是由于社会互动减少。化身——旨在促进工人互动和参与——是增强人机协作（HRC）体验的有前景的解决方案。然而，关于化身支持的HRC的真实世界视角尚未被探索。为了弥补这一空白，我们对一家使用协作机器人的德国制造公司的员工进行了一项焦点小组研究。在讨论之前，参与者在实验室环境中参与了一个预设的工业化HRC演示。这种定性方法为化身的潜在角色、其行为的改进以及在工业工作单元中部署的实际考虑因素提供了宝贵的见解。我们的研究结果还强调了个性化沟通和任务协助的重要性。尽管我们研究的局限性限制了其普遍性，但它是在认识适应性、上下文感知化身互动在真实世界工业环境中潜力的初步步骤。", "summary": "本研究旨在解决工业环境中协作机器人（cobots）导致的工人社会互动减少问题，探讨化身在增强人机协作（HRC）体验中的潜力。通过对一家德国制造公司员工进行焦点小组研究，并在实验室中进行HRC演示，研究收集了关于化身角色、行为改进和实际部署的定性见解。结果表明，化身能够促进个性化沟通和任务协助，为未来在工业环境中部署适应性、上下文感知化身互动奠定了基础。", "keywords": "人机协作, 化身, 工业5.0, 焦点小组, 协作机器人", "comments": "这项研究的创新之处在于其首次探索了在真实工业背景下，通过焦点小组研究来评估化身在人机协作中的作用，填补了该领域的空白。尽管其普遍性受限，但它为未来化身在工业5.0中改善工人福祉和效率提供了重要的初步视角。"}}
{"id": "2506.08513", "title": "Designing lensless imaging systems to maximize information capture", "authors": ["Leyla A. Kabuli", "Henry Pinkard", "Eric Markley", "Clara S. Hung", "Laura Waller"], "summary": "Mask-based lensless imaging uses an optical encoder (e.g. a phase or\namplitude mask) to capture measurements, then a computational decoding\nalgorithm to reconstruct images. In this work, we evaluate and design encoders\nbased on the information content of their measurements using mutual information\nestimation. With this approach, we formalize the object-dependent nature of\nlensless imaging and study the interdependence between object sparsity, encoder\nmultiplexing, and noise. Our analysis reveals that optimal encoder designs\nshould tailor encoder multiplexing to object sparsity for maximum information\ncapture, and that all optimally-encoded measurements share the same level of\nsparsity. Using mutual information-based optimization, we design\ninformation-optimal encoders with improved downstream reconstruction\nperformance. We validate the benefits of reduced multiplexing for dense,\nnatural images by evaluating experimental lensless imaging systems directly\nfrom captured measurements, without the need for image formation models,\nreconstruction algorithms, or ground truth images. Our comprehensive analysis\nestablishes design and engineering principles for improving lensless imaging\nsystems, and offers a model for the study of general multiplexing systems,\nespecially those with object-dependent performance.", "comment": "23 pages, 10 figures", "cate": "physics.optics", "url": "http://arxiv.org/abs/2506.08513v1", "AI": {"title_translation": "设计无透镜成像系统以最大化信息捕获", "tldr": "本研究通过互信息估计评估和设计无透镜成像系统中的编码器，发现最佳编码器应根据物体稀疏性调整多路复用，以最大化信息捕获并提升图像重建性能。", "motivation": "现有的无透镜成像系统依赖于光学编码器，本研究旨在评估和设计这些编码器，以最大化其测量的信息内容，从而提高成像性能。", "method": "本文通过互信息估计来评估和设计光学编码器。研究了物体稀疏性、编码器多路复用和噪声之间的相互依赖性，并基于互信息优化设计了信息最优的编码器。通过直接从捕获的测量值评估实验性无透镜成像系统，验证了其效果。", "result": "分析表明，最佳编码器设计应根据物体稀疏性调整编码器多路复用以最大化信息捕获，并且所有最优编码的测量都具有相同的稀疏度。设计的信息最优编码器显著提高了下游图像重建性能。", "conclusion": "本研究为改进无透镜成像系统建立了设计和工程原则，并为研究通用多路复用系统（特别是那些具有物体依赖性性能的系统）提供了一个模型。", "translation": "基于掩模的无透镜成像系统使用光学编码器（例如相位或振幅掩模）捕获测量值，然后通过计算解码算法重建图像。在这项工作中，我们利用互信息估计，根据测量的信息内容评估和设计编码器。通过这种方法，我们形式化了无透镜成像的物体依赖性，并研究了物体稀疏性、编码器多路复用和噪声之间的相互依赖关系。我们的分析表明，最佳编码器设计应根据物体稀疏性调整编码器多路复用以最大化信息捕获，并且所有最优编码的测量都具有相同的稀疏度。通过基于互信息的优化，我们设计了信息最优的编码器，从而提高了下游重建性能。我们通过直接从捕获的测量值评估实验性无透镜成像系统，验证了对密集自然图像减少多路复用的好处，而无需图像形成模型、重建算法或真实图像。我们的综合分析为改进无透镜成像系统建立了设计和工程原则，并为研究通用多路复用系统（特别是那些具有物体依赖性性能的系统）提供了一个模型。", "summary": "本文研究了基于掩模的无透镜成像系统中的光学编码器设计，旨在通过互信息估计最大化信息捕获。研究发现，最佳编码器应根据物体稀疏性调整其多路复用，以实现最大的信息量和更好的图像重建效果。这项工作为无透镜成像系统的设计提供了指导原则，并为通用多路复用系统的研究提供了模型。", "keywords": "无透镜成像, 编码器设计, 互信息, 物体稀疏性, 多路复用", "comments": "这项工作创新性地将互信息理论应用于无透镜成像系统的编码器设计，揭示了物体稀疏性与编码器多路复用之间的关键关系。其提出的设计原则不仅对无透镜成像具有重要意义，也为理解和优化其他物体依赖性多路复用系统提供了通用框架，具有较强的理论和实践价值。"}}
{"id": "2210.16402", "title": "GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity", "authors": ["Artavazd Maranjyan", "Mher Safaryan", "Peter Richtárik"], "summary": "We study a class of distributed optimization algorithms that aim to alleviate\nhigh communication costs by allowing clients to perform multiple local\ngradient-type training steps before communication. In a recent breakthrough,\nMishchenko et al. (2022) proved that local training, when properly executed,\nleads to provable communication acceleration, and this holds in the strongly\nconvex regime without relying on any data similarity assumptions. However,\ntheir ProxSkip method requires all clients to take the same number of local\ntraining steps in each communication round. We propose a redesign of the\nProxSkip method, allowing clients with ``less important'' data to get away with\nfewer local training steps without impacting the overall communication\ncomplexity of the method. In particular, we prove that our modified method,\nGradSkip, converges linearly under the same assumptions and has the same\naccelerated communication complexity, while the number of local gradient steps\ncan be reduced relative to a local condition number. We further generalize our\nmethod by extending the randomness of probabilistic alternations to arbitrary\nunbiased compression operators and by considering a generic proximable\nregularizer. This generalization, which we call GradSkip+, recovers several\nrelated methods in the literature as special cases. Finally, we present an\nempirical study on carefully designed toy problems that confirm our theoretical\nclaims.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2210.16402v3", "AI": {"title_translation": "GradSkip：具有更好计算复杂度的通信加速局部梯度方法", "tldr": "GradSkip是一种改进的分布式优化算法，通过允许客户端执行可变数量的本地梯度步骤来减少计算复杂性，同时保持通信加速。", "motivation": "现有的分布式优化算法ProxSkip在通信加速方面表现出色，但要求所有客户端在每次通信中执行相同数量的本地训练步骤，这限制了其灵活性和潜在的计算效率。", "method": "本文提出了GradSkip方法，它是ProxSkip的重新设计版本，允许“不重要”数据的客户端执行较少的本地训练步骤，同时不影响整体通信复杂度。该方法通过将概率交替的随机性扩展到任意无偏压缩算子，并考虑通用的近端正则化器，进一步推广为GradSkip+。", "result": "GradSkip在相同假设下线性收敛，具有相同的加速通信复杂度，同时相对于局部条件数可以减少本地梯度步骤。GradSkip+作为特例恢复了文献中几种相关方法。经验研究证实了理论主张。", "conclusion": "GradSkip方法成功地在保持通信加速的同时，通过允许可变的本地梯度步骤，降低了分布式优化中的计算复杂度，从而改进了ProxSkip。", "translation": "我们研究一类分布式优化算法，旨在通过允许客户端在通信前执行多个局部梯度型训练步骤来减轻高通信成本。在最近的一项突破中，Mishchenko 等人 (2022) 证明，如果执行得当，局部训练可以带来可证明的通信加速，并且这在强凸区域中成立，不依赖于任何数据相似性假设。然而，他们的 ProxSkip 方法要求所有客户端在每个通信回合中执行相同数量的局部训练步骤。我们提出了一种对 ProxSkip 方法的重新设计，允许拥有“不太重要”数据的客户端执行较少的局部训练步骤，而不会影响该方法的整体通信复杂度。特别是，我们证明了我们修改后的方法 GradSkip 在相同假设下线性收敛，并具有相同的加速通信复杂度，同时局部梯度步骤的数量可以相对于局部条件数减少。我们通过将概率交替的随机性扩展到任意无偏压缩算子并考虑通用的近端正则化器，进一步推广了我们的方法。这种泛化，我们称之为 GradSkip+，将文献中几种相关方法作为特例恢复。最后，我们对精心设计的玩具问题进行了实证研究，证实了我们的理论主张。", "summary": "本文研究了分布式优化算法，旨在通过允许客户端在通信前执行多个本地梯度训练步骤来降低高通信成本。针对现有ProxSkip方法要求所有客户端执行相同本地步骤的限制，作者提出了GradSkip。该方法允许客户端根据数据重要性执行可变数量的本地梯度步骤，并证明其在相同假设下线性收敛，保持通信加速，同时减少了本地计算量。进一步，GradSkip+泛化了该方法，涵盖了多种现有技术。实验结果证实了其理论优势。", "keywords": "分布式优化, 局部梯度方法, 通信加速, 计算复杂度, GradSkip", "comments": "GradSkip的创新之处在于通过允许客户端根据数据特性（“不重要”数据）执行可变数量的本地梯度步骤，有效地降低了分布式优化中的总计算复杂度，而不会牺牲通信效率。这对于异构数据分布的分布式学习场景具有重要意义，因为它能更好地平衡计算和通信开销。"}}
{"id": "2506.08671", "title": "Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and Design Choices", "authors": ["Junfeng Liu", "Jiarui Ye", "Mengshi Chen", "Meng Li", "Siqiang Luo"], "summary": "LSM-tree-based data stores are widely used in industry due to their\nexceptional performance. However, as data volumes grow, efficiently querying\nlarge-scale databases becomes increasingly challenging. To address this, recent\nstudies attempted to integrate learned indexes into LSM-trees to enhance lookup\nperformance, which has demonstrated promising improvements. Despite this, only\na limited range of learned index types has been considered, and the strengths\nand weaknesses of different learned indexes remain unclear, making them\ndifficult for practical use. To fill this gap, we provide a comprehensive and\nsystematic benchmark to pursue an in-depth understanding of learned indexes in\nLSM-tree systems. In this work, we summarize the workflow of 8 existing learned\nindexes and analyze the associated theoretical cost. We also identify several\nkey factors that significantly influence the performance of learned indexes and\nconclude them with a novel configuration space, including various index types,\nboundary positions, and granularity. Moreover, we implement different learned\nindex designs on a unified platform to evaluate across various configurations.\nSurprisingly, our experiments reveal several unexpected insights, such as the\nmarginal lookup enhancement when allocating a large memory budget to learned\nindexes and modest retraining overhead of learned indexes. Besides, we also\noffer practical guidelines to help developers intelligently select and tune\nlearned indexes for custom use cases.", "comment": "14 pages,12 figures", "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.08671v1", "AI": {"title_translation": "评估LSM-树系统中学习型索引：基准、洞察与设计选择", "tldr": "本文对LSM-树系统中学习型索引进行了全面基准测试，分析了其性能影响因素，并提供了实用指南，揭示了内存预算对查找性能提升有限以及重训练开销不大的意外发现。", "motivation": "LSM-树数据存储系统在处理大规模数据时，查询效率面临挑战。现有研究尝试将学习型索引集成到LSM-树中以提升查找性能，但考虑的索引类型有限，不同学习型索引的优缺点不明确，难以实际应用。本文旨在填补这一空白，深入理解学习型索引在LSM-树系统中的表现。", "method": "1. 总结了8种现有学习型索引的工作流程并分析了其理论成本。2. 识别了影响学习型索引性能的关键因素，并提出了一个包含索引类型、边界位置和粒度的新型配置空间。3. 在统一平台上实现了不同的学习型索引设计，并在各种配置下进行评估。4. 提供了实用指南，帮助开发者选择和调整学习型索引。", "result": "实验揭示了几个意外的发现：当为学习型索引分配大量内存预算时，查找性能提升有限；学习型索引的重训练开销不大。此外，还提供了实用指南，帮助开发者智能选择和调整学习型索引以适应自定义用例。", "conclusion": "本文通过全面系统的基准测试，深入理解了学习型索引在LSM-树系统中的表现，揭示了关键性能洞察，并为开发者提供了实用的选择和调优指南，填补了该领域对不同学习型索引优缺点理解不足的空白。", "translation": "基于LSM-树的数据存储系统因其卓越的性能在工业界被广泛使用。然而，随着数据量的增长，高效查询大规模数据库变得越来越具有挑战性。为了解决这个问题，最近的研究尝试将学习型索引集成到LSM-树中以增强查找性能，这已经显示出有希望的改进。尽管如此，目前只考虑了有限范围的学习型索引类型，并且不同学习型索引的优缺点仍不清楚，这使得它们难以实际使用。为了填补这一空白，我们提供了一个全面而系统的基准测试，以深入了解LSM-树系统中的学习型索引。在这项工作中，我们总结了8种现有学习型索引的工作流程并分析了相关的理论成本。我们还识别了几个显著影响学习型索引性能的关键因素，并将其归纳为一个新颖的配置空间，包括各种索引类型、边界位置和粒度。此外，我们在一个统一的平台上实现了不同的学习型索引设计，以评估各种配置。令人惊讶的是，我们的实验揭示了几个意想不到的见解，例如当为学习型索引分配大量内存预算时，查找增强效果微乎其微，以及学习型索引的重训练开销不大。此外，我们还提供了实用指南，帮助开发人员智能选择和调整学习型索引以适应自定义用例。", "summary": "本文针对LSM-树系统中学习型索引的实际应用难题，通过全面的基准测试和系统性分析，总结了8种现有学习型索引，识别了影响其性能的关键因素，并提出了一个新型配置空间。实验揭示了内存预算对查找性能提升的边际效应以及较低的重训练开销等意外洞察。最终，研究提供了实用指南，帮助开发者有效选择和调优学习型索引，以优化大规模LSM-树系统的查询性能。", "keywords": "LSM-树, 学习型索引, 基准测试, 性能评估, 数据存储", "comments": "该论文通过系统性的基准测试和深入分析，填补了LSM-树系统中学习型索引评估的空白。其创新之处在于识别了影响性能的关键因素和新型配置空间，并揭示了反直觉的实验结果，如大内存预算下查找性能提升有限。这对于实际系统设计和优化具有重要指导意义，为开发者提供了宝贵的实践指南。"}}
{"id": "2506.09023", "title": "Fine-Grained Spatially Varying Material Selection in Images", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09023v1", "AI": {"title_translation": "图像中细粒度空间变化的材料选择", "tldr": "提出一种基于ViT模型和多分辨率处理策略的图像材料选择方法，能实现细粒度、鲁棒的纹理和子纹理选择，并引入新的DuMaS数据集。", "motivation": "选择是许多图像编辑过程的第一步，能够更快、更简单地修改共享相同模态的像素。然而，现有的选择方法可能无法满足细粒度和鲁棒性的要求。本文旨在提出一种对光照和反射变化具有鲁棒性，并能实现细粒度空间变化的材料选择方法，以支持下游的图像编辑任务。", "method": "本文提出了一种基于Vision Transformer (ViT) 模型特征的材料选择方法。该方法采用多分辨率处理策略，以获得更精细和稳定的选择结果。此外，它支持纹理和子纹理两个层面的选择。为了实现这一目标，研究人员还构建了一个新的两级材料选择(DuMaS)数据集，该数据集包含超过80万张合成图像的密集标注，涵盖纹理和子纹理两个层面。", "result": "该方法比现有方法产生了更精细、更稳定的选择结果。它成功实现了纹理和子纹理两个层面的材料选择。", "conclusion": "本文提出了一种新颖的、基于ViT和多分辨率处理的材料选择方法，能够对图像进行细粒度的、鲁棒的材料选择，并且支持纹纹理和子纹理两个层面的选择。这项工作为下游图像编辑任务提供了更有效和精确的基础。", "translation": "选择是许多图像编辑过程的第一步，它能够更快、更简单地修改共享相同模态的所有像素。在这项工作中，我们提出了一种图像材料选择方法，该方法对光照和反射变化具有鲁棒性，可用于下游编辑任务。我们依赖于视觉Transformer (ViT) 模型，并利用其特征进行选择，提出了一种多分辨率处理策略，该策略比现有方法产生更精细、更稳定的选择结果。此外，我们实现了两个层面的选择：纹理和子纹理，并利用了一个新的两级材料选择(DuMaS)数据集，该数据集包含超过80万张合成图像的密集标注，涵盖纹理和子纹理两个层面。", "summary": "本文提出了一种新颖的细粒度空间变化材料选择方法，该方法利用Vision Transformer (ViT) 模型特征和多分辨率处理策略，旨在实现对图像中纹理和子纹理的鲁棒且精细的选择。该方法在光照和反射变化下表现稳定，并且引入了一个包含80多万张合成图像详细标注（涵盖纹理和子纹理级别）的新数据集DuMaS，从而为下游图像编辑任务提供了更精确的初始选择基础。", "keywords": "材料选择, 视觉Transformer, 多分辨率处理, 图像编辑, DuMaS数据集", "comments": "这篇论文的创新点在于将Vision Transformer (ViT) 模型引入到细粒度材料选择任务中，并提出了一种独特的多分辨率处理策略，以提高选择的精细度和稳定性。此外，构建了一个大规模的两级材料选择(DuMaS)数据集，这对于推动该领域的研究具有重要意义，因为它提供了纹理和子纹理层面的详细标注，有助于训练和评估更精确的模型。该方法对于下游的图像编辑任务具有重要的应用价值。"}}
{"id": "2506.08382", "title": "NAM: A Normalization Attention Model for Personalized Product Search In Fliggy", "authors": ["Shui Liu", "Mingyuan Tao", "Maofei Que", "Pan Li", "Dong Li", "Shenghua Ni", "Zhuoran Zhuang"], "summary": "Personalized product search provides significant benefits to e-commerce\nplatforms by extracting more accurate user preferences from historical\nbehaviors. Previous studies largely focused on the user factors when\npersonalizing the search query, while ignoring the item perspective, which\nleads to the following two challenges that we summarize in this paper: First,\nprevious approaches relying only on co-occurrence frequency tend to\noverestimate the conversion rates for popular items and underestimate those for\nlong-tail items, resulting in inaccurate item similarities; Second, user\npurchasing propensity is highly heterogeneous according to the popularity of\nthe target item: it is less correlated with the user's historical behavior for\na popular item and more correlated for a long-tail item. To address these\nchallenges, in this paper we propose NAM, a Normalization Attention Model,\nwhich optimizes ''when to personalize'' by utilizing Inverse Item Frequency\n(IIF) and employing a gating mechanism, as well as optimizes ''how to\npersonalize'' by normalizing the attention mechanism from a global perspective.\nThrough comprehensive experiments, we demonstrate that our proposed NAM model\nsignificantly outperforms state-of-the-art baseline models. Furthermore, we\nconducted an online A/B test at Fliggy, and obtained a significant improvement\nof 0.8% over the latest production system in conversion rate.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08382v1", "AI": {"title_translation": "NAM：一种用于飞猪个性化产品搜索的归一化注意力模型", "tldr": "本文提出了NAM模型，通过结合逆向物品频率和门控机制优化个性化时机，并通过全局归一化注意力优化个性化方式，解决了传统个性化产品搜索中流行物品和长尾物品的转化率估计不准确以及用户购买倾向异质性问题。实验证明NAM显著优于现有基线模型，并在飞猪的在线A/B测试中实现了0.8%的转化率提升。", "motivation": "现有个性化产品搜索主要关注用户因素，忽视了物品视角，导致两个挑战：1. 仅依赖共现频率会高估热门商品转化率，低估长尾商品转化率，导致物品相似度不准确。2. 用户购买倾向因目标物品流行度而异，对热门物品与用户历史行为关联度低，对长尾物品关联度高。", "method": "本文提出了归一化注意力模型（NAM），通过利用逆向物品频率（IIF）和采用门控机制来优化“何时个性化”，并通过从全局角度归一化注意力机制来优化“如何个性化”。", "result": "综合实验表明，NAM模型显著优于最先进的基线模型。此外，在飞猪进行了在线A/B测试，转化率比最新生产系统提高了0.8%。", "conclusion": "NAM模型通过创新的个性化时机和方式优化，有效解决了个性化产品搜索中的挑战，并取得了显著的性能提升，证明了其在实际应用中的有效性。", "translation": "个性化产品搜索通过从历史行为中提取更准确的用户偏好，为电子商务平台带来了显著的效益。以往的研究在个性化搜索查询时，主要侧重于用户因素，而忽略了物品视角，这导致了本文总结的以下两个挑战：首先，以往仅依赖共现频率的方法倾向于高估热门商品的转化率，低估长尾商品的转化率，导致物品相似度不准确；其次，用户购买倾向根据目标商品的流行度表现出高度的异质性：对于热门商品，它与用户历史行为的相关性较低，而对于长尾商品，相关性较高。为了解决这些挑战，本文提出了NAM，即归一化注意力模型，它通过利用逆向物品频率（IIF）和采用门控机制来优化“何时个性化”，并通过从全局角度归一化注意力机制来优化“如何个性化”。通过全面的实验，我们证明了我们提出的NAM模型显著优于最先进的基线模型。此外，我们在飞猪进行了在线A/B测试，转化率比最新生产系统显著提高了0.8%。", "summary": "本文针对个性化产品搜索中忽略物品视角导致的两个挑战——热门商品和长尾商品转化率估计不准确以及用户购买倾向异质性，提出了一种归一化注意力模型（NAM）。NAM通过引入逆向物品频率（IIF）和门控机制来决定何时进行个性化，并通过全局归一化注意力机制来优化如何进行个性化。实验结果表明，NAM模型显著优于现有基线模型，并在飞猪的在线A/B测试中使转化率提升了0.8%。", "keywords": "个性化搜索, 注意力模型, 逆向物品频率, 转化率优化, 飞猪", "comments": "NAM模型通过同时考虑用户和物品视角，并引入“何时个性化”和“如何个性化”的优化策略，有效地解决了传统个性化搜索的痛点。特别是引入逆向物品频率（IIF）来处理流行度和长尾效应，以及全局归一化注意力机制，体现了创新性。在线A/B测试的积极结果进一步验证了其在实际应用中的价值和重要性。"}}
{"id": "2506.08380", "title": "Stochastic gradient descent based variational inference for infinite-dimensional inverse problems", "authors": ["Jiaming Sui", "Junxiong Jia", "Jinglai Li"], "summary": "This paper introduces two variational inference approaches for\ninfinite-dimensional inverse problems, developed through gradient descent with\na constant learning rate. The proposed methods enable efficient approximate\nsampling from the target posterior distribution using a constant-rate\nstochastic gradient descent (cSGD) iteration. Specifically, we introduce a\nrandomization strategy that incorporates stochastic gradient noise, allowing\nthe cSGD iteration to be viewed as a discrete-time process. This transformation\nestablishes key relationships between the covariance operators of the\napproximate and true posterior distributions, thereby validating cSGD as a\nvariational inference method. We also investigate the regularization properties\nof the cSGD iteration and provide a theoretical analysis of the discretization\nerror between the approximated posterior mean and the true background function.\nBuilding on this framework, we develop a preconditioned version of cSGD to\nfurther improve sampling efficiency. Finally, we apply the proposed methods to\ntwo practical inverse problems: one governed by a simple smooth equation and\nthe other by the steady-state Darcy flow equation. Numerical results confirm\nour theoretical findings and compare the sampling performance of the two\napproaches for solving linear and non-linear inverse problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08380v1", "AI": {"title_translation": "基于随机梯度下降的无限维逆问题变分推断", "tldr": "本文提出了两种基于常数学习率随机梯度下降（cSGD）的变分推断方法，用于无限维逆问题，实现了高效的近似后验采样，并进行了理论分析和数值验证。", "motivation": "解决无限维逆问题中高效的近似后验采样问题。", "method": "引入两种基于常数学习率随机梯度下降（cSGD）的变分推断方法。采用随机化策略将cSGD视为离散时间过程，并研究其正则化特性。提供了离散化误差的理论分析。开发了预处理版本的cSGD以提高采样效率。将方法应用于简单平滑方程和稳态达西流方程。", "result": "建立了近似和真实后验分布协方差算子之间的关键关系，验证了cSGD作为变分推断方法。提供了近似后验均值与真实背景函数之间离散化误差的理论分析。数值结果证实了理论发现，并比较了两种方法在线性及非线性逆问题中的采样性能。", "conclusion": "提出的cSGD变分推断方法能够有效解决无限维逆问题，通过理论分析和数值实验验证了其有效性和采样效率。", "translation": "本文介绍了两种用于无限维逆问题的变分推断方法，这些方法通过常数学习率的梯度下降开发而成。所提出的方法能够利用常数学习率随机梯度下降（cSGD）迭代，从目标后验分布中进行高效的近似采样。具体来说，我们引入了一种随机化策略，该策略结合了随机梯度噪声，使得cSGD迭代可以被视为一个离散时间过程。这种转换建立了近似后验分布与真实后验分布的协方差算子之间的关键关系，从而验证了cSGD作为一种变分推断方法。我们还研究了cSGD迭代的正则化特性，并对近似后验均值与真实背景函数之间的离散化误差进行了理论分析。在此框架基础上，我们开发了cSGD的预处理版本，以进一步提高采样效率。最后，我们将所提出的方法应用于两个实际的逆问题：一个由简单的平滑方程控制，另一个由稳态达西流方程控制。数值结果证实了我们的理论发现，并比较了两种方法在线性和非线性逆问题求解中的采样性能。", "summary": "本文提出了两种基于常数学习率随机梯度下降（cSGD）的变分推断方法，旨在解决无限维逆问题中的高效近似后验采样。通过引入随机化策略，将cSGD迭代视为离散时间过程，并建立了近似与真实后验分布协方差算子间的关系，从而验证了其作为变分推断的有效性。研究还包括cSGD的正则化特性及离散化误差的理论分析，并开发了预处理版本以提升采样效率。数值实验在简单平滑方程和稳态达西流方程等实际问题中验证了方法的理论有效性与采样性能。", "keywords": "随机梯度下降, 变分推断, 无限维逆问题, 后验采样, 正则化", "comments": "这篇论文的创新点在于将常数学习率随机梯度下降（cSGD）应用于无限维逆问题的变分推断，并通过理论分析和随机化策略将其与离散时间过程联系起来，从而验证了其作为变分推断方法的有效性。预处理cSGD的引入进一步提高了实际应用的效率。该工作为处理复杂高维逆问题提供了新的高效计算工具和坚实的理论基础。"}}
{"id": "2506.08838", "title": "ZTaint-Havoc: From Havoc Mode to Zero-Execution Fuzzing-Driven Taint Inference", "authors": ["Yuchong Xie", "Wenhui Zhang", "Dongdong She"], "summary": "Fuzzing is a widely used technique for discovering software vulnerabilities,\nbut identifying hot bytes that influence program behavior remains challenging.\nTraditional taint analysis can track such bytes white-box, but suffers from\nscalability issue. Fuzzing-Driven Taint Inference (FTI) offers a black-box\nalternative, yet typically incurs significant runtime overhead due to extra\nprogram executions. We observe that the commonly used havoc mutation scheme in\nfuzzing can be adapted for lightweight FTI with zero extra executions. We\npresent a computational model of havoc mode, demonstrating that it can perform\nFTI while generating new test cases. Building on this, we propose ZTaint-Havoc,\na novel, efficient FTI with minimal overhead (3.84% on UniBench, 12.58% on\nFuzzBench). We further design an effective mutation algorithm utilizing the\nidentified hot bytes. Our comprehensive evaluation shows that ZTaint-Havoc,\nimplemented in AFL++, improves edge coverage by up to 33.71% on FuzzBench and\n51.12% on UniBench over vanilla AFL++, with average gains of 2.97% and 6.12% in\n24-hour fuzzing campaigns.", "comment": "To appear on 34th ISSTA", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08838v1", "AI": {"title_translation": "ZTaint-Havoc: 从Havoc模式到零执行模糊测试驱动的污点推断", "tldr": "ZTaint-Havoc 利用模糊测试中的havoc模式实现零额外执行的污点推断，显著提高了模糊测试的效率和覆盖率。", "motivation": "识别影响程序行为的热字节在模糊测试中具有挑战性。传统污点分析可扩展性差，而模糊测试驱动的污点推断(FTI)虽然是黑盒替代方案，但通常会因额外的程序执行而产生显著的运行时开销。", "method": "本文提出了ZTaint-Havoc，一种新型高效的零额外执行模糊测试驱动污点推断（FTI）方法。该方法利用模糊测试中常用的havoc变异方案，并提出了havoc模式的计算模型，证明其可以在生成新测试用例的同时执行FTI。此外，还设计了一种利用识别出的热字节的有效变异算法。", "result": "ZTaint-Havoc在UniBench上开销为3.84%，在FuzzBench上为12.58%。在AFL++中实现后，ZTaint-Havoc在FuzzBench上将边缘覆盖率提高了33.71%，在UniBench上提高了51.12%，在24小时模糊测试活动中平均增益分别为2.97%和6.12%。", "conclusion": "ZTaint-Havoc通过利用模糊测试的havoc模式，以极低的开销实现了高效的污点推断，显著提升了模糊测试的性能和代码覆盖率。", "translation": "模糊测试是一种广泛用于发现软件漏洞的技术，但识别影响程序行为的热字节仍然具有挑战性。传统的污点分析可以白盒跟踪这些字节，但存在可扩展性问题。模糊测试驱动的污点推断（FTI）提供了一种黑盒替代方案，但通常会因额外的程序执行而产生显著的运行时开销。我们观察到模糊测试中常用的havoc变异方案可以适用于轻量级FTI，且无需额外的执行。我们提出了havoc模式的计算模型，证明它可以在生成新测试用例的同时执行FTI。在此基础上，我们提出了ZTaint-Havoc，一种新颖、高效且开销极小的FTI（UniBench上为3.84%，FuzzBench上为12.58%）。我们进一步设计了一种利用已识别热字节的有效变异算法。我们的综合评估表明，在AFL++中实现的ZTaint-Havoc，在FuzzBench上比原版AFL++的边缘覆盖率提高了33.71%，在UniBench上提高了51.12%，在24小时模糊测试活动中平均增益分别为2.97%和6.12%。", "summary": "本文提出了ZTaint-Havoc，一种创新的模糊测试驱动污点推断（FTI）方法，通过利用模糊测试中的havoc变异模式，实现了零额外执行的污点分析。该方法有效解决了传统FTI因额外程序执行导致的运行时开销问题。实验结果表明，ZTaint-Havoc以极低的开销显著提升了代码覆盖率，证明了其在软件漏洞发现中的高效性和实用性。", "keywords": "模糊测试, 污点推断, Havoc模式, 零执行, 漏洞发现", "comments": "ZTaint-Havoc的创新之处在于巧妙地将模糊测试中常用的havoc变异机制与污点推断相结合，实现了零额外执行的污点分析，极大地降低了FTI的运行时开销。这对于大规模软件漏洞发现具有重要意义，因为它提高了模糊测试的效率和深度。"}}
{"id": "2506.08680", "title": "Proceedings of the 23rd International Overture Workshop", "authors": ["Hugo Daniel Macedo", "Ken Pierce"], "summary": "This volume contains the papers presented at the 23rd International Overture\nWorkshop, held on the 11th of June 2025. This event was the latest in a series\nof workshops around the Vienna Development Method (VDM), the open-source\nproject Overture, and related tools and formalisms. VDM is one of the longest\nestablished formal methods for systems development. A lively community of\nresearchers and practitioners has grown up in academia and industry has grown\naround the modelling languages (VDM-SL, VDM++, VDM-RT, CML) and tools\n(VDMTools, Overture, Crescendo, Symphony, the INTO-CPS chain, and ViennaTalk).\nTogether, these provide a platform for work on modelling and analysis\ntechnology that includes static and dynamic analysis, test generation,\nexecution support, and model checking. This workshop provided updates on the\nemerging technology of VDM/Overture, including collaboration infrastructure,\ncollaborative modelling and co-simulation for Cyber-Physical Systems.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08680v1", "AI": {"title_translation": "第23届国际Overture研讨会论文集", "tldr": "本文集收录了第23届国际Overture研讨会的论文，该研讨会聚焦于维也纳开发方法（VDM）、Overture项目及相关工具与形式化方法，并提供了新兴技术的最新进展。", "motivation": "该研讨会旨在汇集研究人员和从业者，并提供维也纳开发方法（VDM）、Overture开源项目以及相关工具、形式化方法和新兴技术的最新进展。", "method": "Not mentioned in abstract", "result": "本次研讨会提供了VDM/Overture新兴技术的最新进展，包括协作基础设施、协同建模和网络物理系统协同仿真。", "conclusion": "Not mentioned in abstract", "translation": "本卷收录了2025年6月11日举行的第23届国际Overture研讨会论文。本次活动是围绕维也纳开发方法（VDM）、开源项目Overture以及相关工具和形式化方法举行的一系列研讨会中的最新一届。VDM是系统开发领域历史最悠久的形式化方法之一。学术界和工业界围绕建模语言（VDM-SL、VDM++、VDM-RT、CML）和工具（VDMTools、Overture、Crescendo、Symphony、INTO-CPS链和ViennaTalk）形成了一个活跃的研究者和从业者社区。这些共同提供了一个建模和分析技术的工作平台，包括静态和动态分析、测试生成、执行支持和模型检查。本次研讨会提供了VDM/Overture新兴技术的最新进展，包括协作基础设施、协同建模和网络物理系统协同仿真。", "summary": "本文集收录了第23届国际Overture研讨会的论文，该研讨会是围绕维也纳开发方法（VDM）、Overture开源项目及相关形式化方法和工具举办的系列活动之一。研讨会汇聚了学术界和工业界的活跃社区，并提供了VDM/Overture新兴技术的最新进展，特别是关于协作基础设施、协同建模和网络物理系统协同仿真。", "keywords": "Overture研讨会, VDM, 形式化方法, 网络物理系统, 协同仿真", "comments": "本次研讨会强调了VDM等形式化方法和Overture等相关开源工具的持续相关性和发展。对网络物理系统和协作方面的关注表明其适应了现代复杂系统的挑战。"}}
{"id": "2506.08459", "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08459v1", "AI": {"title_translation": "自动驾驶系统安全验证的扩散模型", "tldr": "使用扩散模型生成自动驾驶系统潜在故障场景，以解决安全验证的挑战。", "motivation": "自动驾驶系统安全验证面临高风险、高成本以及潜在故障稀有性和多样性的挑战。", "method": "训练去噪扩散模型，根据任何初始交通状态生成自动驾驶车辆的潜在故障案例。", "result": "在四向交叉路口问题上的实验表明，该扩散模型能生成逼真的故障样本，并捕捉到多种潜在故障。模型无需外部训练数据集，计算资源要求不高，且不需预先了解被测系统。", "conclusion": "扩散模型可以有效地应用于交通路口等场景的自动驾驶系统安全验证。", "translation": "自动驾驶系统的安全验证极具挑战性，因为真实世界测试风险高、成本高，而且潜在故障的稀有性和多样性。为了应对这些挑战，我们训练了一个去噪扩散模型，根据任何初始交通状态生成自动驾驶车辆的潜在故障案例。在四向交叉路口问题上的实验表明，在各种场景下，该扩散模型能够生成逼真的故障样本，同时捕捉到多种潜在故障。我们的模型不需要任何外部训练数据集，可以用适度的计算资源进行训练和推理，并且不需要预先了解被测系统，适用于交通路口的安全验证。", "summary": "本文提出使用去噪扩散模型来解决自动驾驶系统安全验证中真实世界测试的挑战。该模型能根据初始交通状态生成多样且逼真的潜在故障案例，无需外部数据集，计算资源要求低，并已成功应用于交通路口的安全验证。", "keywords": "扩散模型, 自动驾驶, 安全验证, 故障生成, 交通路口", "comments": "该研究提出了一种新颖的、基于扩散模型的自动驾驶安全验证方法，解决了传统方法中数据稀缺和成本高昂的问题。其无需外部训练数据和较低的计算资源需求是重要的创新点，显示了扩散模型在生成对抗性或稀有场景方面的潜力。"}}
{"id": "2506.08214", "title": "Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation", "authors": ["Ioannis Iakovidis", "Zahra Kalantari", "Amir Hossein Payberah", "Fernando Jaramillo", "Francisco Pena Escobar"], "summary": "In recent years the wide availability of high-resolution radar satellite\nimages along with the advancement of computer vision models have enabled the\nremote monitoring of the surface area of wetlands. However, these models\nrequire large amounts of manually annotated satellite images, which are slow\nand expensive to produce. To overcome this problem, self-supervised training\nmethods have been deployed to train models without using annotated data. In\nthis paper we use a combination of deep clustering and negative sampling to\ntrain a model to segment radar satellite images into areas that separate water\nfrom land without the use of any manual annotations. Furthermore, we implement\nan ensemble version of the model to reduce variance and improve performance.\nCompared to a single fully-supervised model using the same architecture, our\nensemble of self-supervised models achieves a 0.02 improvement in the\nIntersection Over Union metric over our test dataset.", "comment": "16 pages, 9 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08214v1", "AI": {"title_translation": "使用卫星图像和自监督机器学习网络检测植被下隐藏的水", "tldr": "本文提出了一种结合深度聚类和负采样的自监督集成模型，用于在不使用手动标注数据的情况下，从雷达卫星图像中分割水域和陆地，并在IOU指标上有所提升。", "motivation": "现有的湿地遥感监测模型需要大量手动标注的卫星图像，这既耗时又昂贵。", "method": "本文结合深度聚类和负采样来训练一个自监督模型，用于分割雷达卫星图像中的水域和陆地。此外，还实现了一个集成版本的模型以减少方差并提高性能。", "result": "与使用相同架构的单一全监督模型相比，该自监督集成模型在测试数据集上的Intersection Over Union (IOU) 指标上实现了0.02的改进。", "conclusion": "通过自监督学习方法，可以有效地在没有手动标注数据的情况下，从雷达卫星图像中分割水域和陆地，并且集成模型能够进一步提升性能。", "translation": "近年来，高分辨率雷达卫星图像的广泛可用性以及计算机视觉模型的进步，使得湿地地表区域的遥感监测成为可能。然而，这些模型需要大量手动标注的卫星图像，这生产起来既缓慢又昂贵。为了克服这个问题，自监督训练方法已被用于在不使用标注数据的情况下训练模型。在本文中，我们结合深度聚类和负采样来训练一个模型，以将雷达卫星图像分割成水域和陆地，而无需任何手动标注。此外，我们还实现了一个模型的集成版本，以减少方差并提高性能。与使用相同架构的单一全监督模型相比，我们的自监督集成模型在测试数据集上的Intersection Over Union指标上实现了0.02的改进。", "summary": "本文针对现有湿地遥感监测模型对大量手动标注数据的高昂需求，提出了一种基于自监督学习的新方法。该方法结合了深度聚类和负采样，用于在没有人工标注的情况下，将雷达卫星图像中的水域与陆地进行分割。为了进一步提升性能，研究人员还引入了模型的集成版本。实验结果表明，与同架构的单一全监督模型相比，该自监督集成模型在Intersection Over Union (IOU) 指标上取得了0.02的提升。", "keywords": "自监督学习, 卫星图像, 水域检测, 深度聚类, 负采样", "comments": "这篇论文的创新点在于利用自监督学习方法解决了遥感图像分割中手动标注数据成本高昂的问题。通过结合深度聚类和负采样，并在没有标注数据的情况下实现了水陆分割，这对于湿地监测等实际应用具有重要意义。集成模型的引入也进一步提升了模型的鲁棒性和性能。"}}
{"id": "2506.08399", "title": "SafeCoT: Improving VLM Safety with Minimal Reasoning", "authors": ["Jiachen Ma", "Zhanhui Zhou", "Chao Yang", "Chaochao Lu"], "summary": "Ensuring safe and appropriate responses from vision-language models (VLMs)\nremains a critical challenge, particularly in high-risk or ambiguous scenarios.\nWe introduce SafeCoT, a lightweight, interpretable framework that leverages\nrule-based chain-of-thought (CoT) supervision to improve refusal behavior in\nVLMs. Unlike prior methods that rely on large-scale safety annotations or\ncomplex modeling, SafeCoT uses minimal supervision to help models reason about\nsafety risks and make context-aware refusals. Experiments across multiple\nbenchmarks show that SafeCoT significantly reduces overrefusal and enhances\ngeneralization, even with limited training data. Our approach offers a scalable\nsolution for aligning VLMs with safety-critical objectives.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08399v1", "AI": {"title_translation": "SafeCoT：通过最小推理提高VLM安全性", "tldr": "SafeCoT是一个轻量级、可解释的框架，利用基于规则的思维链监督来改善视觉语言模型（VLM）的拒绝行为，显著减少了过度拒绝并增强了泛化能力。", "motivation": "确保视觉语言模型（VLM）在风险高或模糊场景中提供安全和适当的响应仍然是一个严峻的挑战。", "method": "SafeCoT是一个轻量级、可解释的框架，它利用基于规则的思维链（CoT）监督来改善VLM的拒绝行为。与依赖大规模安全标注或复杂建模的现有方法不同，SafeCoT使用最少的监督来帮助模型推理安全风险并做出上下文感知的拒绝。", "result": "在多个基准测试中的实验表明，SafeCoT显著减少了过度拒绝并增强了泛化能力，即使训练数据有限。", "conclusion": "SafeCoT为使VLM与安全关键目标保持一致提供了一种可扩展的解决方案。", "translation": "确保视觉语言模型（VLM）提供安全和适当的响应仍然是一个关键挑战，特别是在高风险或模糊情境中。我们引入了SafeCoT，一个轻量级、可解释的框架，它利用基于规则的思维链（CoT）监督来改善VLM的拒绝行为。与依赖大规模安全标注或复杂建模的现有方法不同，SafeCoT使用最少的监督来帮助模型推理安全风险并做出上下文感知的拒绝。在多个基准测试中的实验表明，SafeCoT显著减少了过度拒绝并增强了泛化能力，即使训练数据有限。我们的方法为使VLM与安全关键目标保持一致提供了一种可扩展的解决方案。", "summary": "SafeCoT是一个轻量级且可解释的框架，旨在通过利用规则驱动的思维链监督来提高视觉语言模型（VLM）在风险场景下的拒绝行为。它仅需少量监督即可让模型识别安全风险并进行上下文感知拒绝，实验证明其能有效减少过度拒绝并提升泛化能力，为VLM的安全对齐提供了可扩展的方案。", "keywords": "视觉语言模型, 安全性, 思维链, 拒绝行为, 最小监督", "comments": "SafeCoT的创新之处在于其以最小的监督实现了VLM的安全对齐，避免了对大规模标注和复杂模型的依赖。其基于规则的CoT监督方法提供了解释性，并在减少过度拒绝和增强泛化能力方面表现出色，这对于VLM在实际应用中的安全性和可靠性至关重要。"}}
{"id": "2506.08063", "title": "Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift", "authors": ["Songqiao Hu", "Zeyi Liu", "Xiao He"], "summary": "The change in data distribution over time, also known as concept drift, poses\na significant challenge to the reliability of online learning methods. Existing\nmethods typically require model retraining or drift detection, both of which\ndemand high computational costs and are often unsuitable for real-time\napplications. To address these limitations, a lightweight, fast and efficient\nrandom vector functional-link network termed Lite-RVFL is proposed, capable of\nadapting to concept drift without drift detection and retraining. Lite-RVFL\nintroduces a novel objective function that assigns weights exponentially\nincreasing to new samples, thereby emphasizing recent data and enabling timely\nadaptation. Theoretical analysis confirms the feasibility of this objective\nfunction for drift adaptation, and an efficient incremental update rule is\nderived. Experimental results on a real-world safety assessment task validate\nthe efficiency, effectiveness in adapting to drift, and potential to capture\ntemporal patterns of Lite-RVFL. The source code is available at\nhttps://github.com/songqiaohu/Lite-RVFL.", "comment": "6 pages, 4 figures, accepted by the 2025 CAA Symposium on Fault\n  Detection, Supervision and Safety for Technical Processes (SAFEPROCESS 2025)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08063v1", "AI": {"title_translation": "Lite-RVFL: 一种用于概念漂移下学习的轻量级随机向量函数连接神经网络", "tldr": "Lite-RVFL是一种轻量级神经网络，无需漂移检测和再训练即可适应概念漂移，通过强调新样本实现快速适应，并在实际任务中表现出高效性和有效性。", "motivation": "现有在线学习方法在处理概念漂移时，需要模型再训练或漂移检测，导致计算成本高昂，不适用于实时应用。", "method": "提出了一种名为Lite-RVFL的轻量级、快速、高效的随机向量函数连接网络。它引入了一个新的目标函数，该函数为新样本分配指数级增长的权重，从而强调最新数据并实现及时适应。理论分析证实了该目标函数对于漂移适应的可行性，并推导出了一个高效的增量更新规则。", "result": "在真实世界安全评估任务上的实验结果验证了Lite-RVFL的效率、适应漂移的有效性以及捕获时间模式的潜力。", "conclusion": "Lite-RVFL通过其独特的目标函数和增量更新规则，提供了一种无需传统漂移检测和再训练的有效且高效的在线学习解决方案，能够成功应对概念漂移并捕获时间模式。", "translation": "数据分布随时间的变化，也称为概念漂移，对在线学习方法的可靠性构成了重大挑战。现有方法通常需要模型再训练或漂移检测，这两者都需要高计算成本，并且通常不适用于实时应用。为了解决这些限制，提出了一种轻量级、快速且高效的随机向量函数连接网络，称为Lite-RVFL，它能够在无需漂移检测和再训练的情况下适应概念漂移。Lite-RVFL引入了一种新颖的目标函数，该函数为新样本分配指数级增长的权重，从而强调最新数据并实现及时适应。理论分析证实了该目标函数对于漂移适应的可行性，并推导出了一个高效的增量更新规则。在真实世界安全评估任务上的实验结果验证了Lite-RVFL的效率、适应漂移的有效性以及捕获时间模式的潜力。源代码可在https://github.com/songqiaohu/Lite-RVFL获取。", "summary": "本文提出了一种名为Lite-RVFL的轻量级随机向量函数连接神经网络，旨在解决在线学习中概念漂移带来的挑战。与需要高计算成本的传统方法不同，Lite-RVFL通过引入一个强调新样本的创新目标函数，实现了无需漂移检测和再训练的漂移适应。理论分析和实验结果证明了Lite-RVFL在真实任务中的高效性和有效性，及其捕获时间模式的能力。", "keywords": "概念漂移, 在线学习, RVFL, 轻量级神经网络, 实时应用", "comments": "Lite-RVFL的创新点在于其无需显式漂移检测和模型再训练即可适应概念漂移的能力，这通过其独特的目标函数实现，该函数通过对新样本赋予指数级增长的权重来强调近期数据。这对于实时在线学习应用具有重要意义，因为它显著降低了计算成本。"}}
{"id": "2506.08727", "title": "Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs", "authors": ["Samarth Sikand", "Rohit Mehra", "Priyavanshi Pathania", "Nikhil Bamby", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "While Generative AI stands to be one of the fastest adopted technologies\never, studies have made evident that the usage of Large Language Models (LLMs)\nputs significant burden on energy grids and our environment. It may prove a\nhindrance to the Sustainability goals of any organization. A crucial step in\nany Sustainability strategy is monitoring or estimating the energy consumption\nof various components. While there exist multiple tools for monitoring energy\nconsumption, there is a dearth of tools/frameworks for estimating the\nconsumption or carbon emissions. Current drawbacks of both monitoring and\nestimation tools include high input data points, intrusive nature, high error\nmargin, etc. We posit that leveraging emerging LLM benchmarks and related data\npoints can help overcome aforementioned challenges while balancing accuracy of\nthe emission estimations. To that extent, we discuss the challenges of current\napproaches and present our evolving framework, R-ICE, which estimates prompt\nlevel inference carbon emissions by leveraging existing state-of-the-art(SOTA)\nbenchmark. This direction provides a more practical and non-intrusive way to\nenable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our\npromising validation results suggest that benchmark-based modelling holds great\npotential for inference emission estimation and warrants further exploration\nfrom the scientific community.", "comment": "5 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08727v1", "AI": {"title_translation": "打破ICE：探索LLM推理碳与能源估算基准的承诺与挑战", "tldr": "该研究提出了一个名为R-ICE的框架，利用LLM基准来估算LLM的推理碳排放，以应对当前能源估算工具的不足。", "motivation": "大型语言模型（LLMs）的使用对能源网和环境造成了显著负担，阻碍了组织的可持续发展目标。当前缺乏有效的工具和框架来准确、非侵入性地估算LLM的能耗或碳排放，现有工具存在数据点高、侵入性强、误差大等缺点。", "method": "该研究提出利用新兴的LLM基准和相关数据点来克服现有挑战，并平衡排放估算的准确性。具体提出了一个名为R-ICE的演进框架，通过利用现有的最先进（SOTA）基准来估算提示级别的推理碳排放。", "result": "该研究的验证结果表明，基于基准的模型在推理排放估算方面具有巨大潜力，并提供了一种更实用、非侵入性的方法，以支持动态LLM路由、碳核算等新兴用例。", "conclusion": "基于基准的模型在LLM推理碳排放估算方面具有巨大潜力，值得科学界进一步探索。", "translation": "尽管生成式AI有望成为有史以来发展最快的技术之一，但研究表明，大型语言模型（LLMs）的使用给能源电网和环境带来了沉重负担，这可能阻碍任何组织的可持续发展目标。任何可持续发展战略的关键一步是监测或估算各种组件的能耗。虽然存在多种监测能耗的工具，但用于估算能耗或碳排放的工具/框架却非常缺乏。当前监测和估算工具的缺点包括高输入数据点、侵入性、高误差范围等。我们认为，利用新兴的LLM基准和相关数据点可以帮助克服上述挑战，同时平衡排放估算的准确性。为此，我们讨论了当前方法的挑战，并提出了我们不断发展的框架R-ICE，它通过利用现有的最先进（SOTA）基准来估算提示级别的推理碳排放。这个方向提供了一种更实用和非侵入性的方式，以实现动态LLM路由、碳核算等新兴用例。我们有希望的验证结果表明，基于基准的建模在推理排放估算方面具有巨大潜力，值得科学界进一步探索。", "summary": "该论文探讨了大型语言模型（LLMs）能耗对环境可持续性的影响，并指出当前缺乏有效的碳排放估算工具。为解决此问题，作者提出了一个名为R-ICE的框架，该框架利用现有的LLM基准来估算提示级别的推理碳排放。研究结果表明，这种基于基准的建模方法在实现准确、非侵入性的碳排放估算方面具有巨大潜力，并支持如动态LLM路由和碳核算等新兴应用。", "keywords": "LLMs, 碳排放, 能源估算, 基准, R-ICE", "comments": "这篇论文解决了当前AI领域的一个重要且紧迫的问题：大型语言模型（LLMs）的能源消耗和碳排放。其创新之处在于提出了一种利用现有LLM基准进行非侵入性、提示级别碳排放估算的方法，即R-ICE框架。这为实现更可持续的AI开发和部署提供了新的视角和工具，对于未来的碳核算和绿色AI实践具有重要意义。该研究的局限性可能在于其仍处于“演进中”的框架，需要进一步的验证和广泛的应用来证明其鲁棒性和准确性。"}}
{"id": "2506.08881", "title": "From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags", "authors": ["Nicolas Grelier", "Johannes Pfau", "Nicolas Mathieu", "Stéphane Kaufmann"], "summary": "The video game industry deals with a fast-paced, competitive and almost\nunpredictable market. Trends of genres, settings and modalities change on a\nperpetual basis, studios are often one big hit or miss away from surviving or\nperishing, and hitting the pulse of the time has become one of the greatest\nchallenges for industrials, investors and other stakeholders. In this work, we\naim to support the understanding of video game trends over time based on\ndata-driven analysis, visualization and interpretation of Steam tag evolutions.\nWe confirm underlying groundwork that trends can be categorized in short-lived\nfads, contemporary fashions, or stable classics, and derived that the surge of\na trend averages at about four years in the realm of video games. After using\nindustrial experts to validate our findings, we deliver visualizations,\ninsights and an open approach of deciphering shifts in video game trends.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08881v1", "AI": {"title_translation": "从短暂潮流到经典——通过Steam标签分析视频游戏趋势演变", "tldr": "本研究通过Steam标签数据分析了视频游戏趋势的演变，将趋势分为短暂潮流、当代时尚和经典，并发现趋势的平均高涨期约为四年，为理解视频游戏市场提供了数据驱动的洞察。", "motivation": "视频游戏行业市场变化快速且难以预测，理解游戏趋势对于行业参与者至关重要。本研究旨在通过数据驱动的分析来支持对视频游戏趋势演变的理解。", "method": "本研究基于Steam标签演变的数据进行分析、可视化和解释，并使用行业专家验证研究发现。", "result": "研究证实了趋势可以分为短暂潮流、当代时尚或稳定经典，并推导出视频游戏领域趋势的平均高涨期约为四年。研究提供了可视化、洞察和一种解读视频游戏趋势转变的开放方法。", "conclusion": "本研究通过对Steam标签演变的分析，为理解视频游戏趋势提供了数据驱动的见解和工具，帮助行业内人士应对快速变化的市场。", "translation": "视频游戏行业面临着一个快节奏、竞争激烈且几乎不可预测的市场。类型、设定和模式的趋势在不断变化，工作室的生存或消亡往往取决于一次大的成功或失败，把握时代脉搏已成为行业人士、投资者和其他利益相关者面临的最大挑战之一。在这项工作中，我们旨在通过对Steam标签演变的数据驱动分析、可视化和解释，支持对视频游戏趋势随时间变化的理解。我们证实了潜在的基础工作，即趋势可以分为短命的短暂潮流、当代时尚或稳定的经典，并得出结论，在视频游戏领域，一个趋势的高涨期平均约为四年。在使用行业专家验证我们的发现后，我们提供了可视化、洞察和一种解读视频游戏趋势转变的开放方法。", "summary": "本研究通过对Steam标签演变的数据驱动分析，旨在理解视频游戏行业的趋势变化。研究将趋势分为短暂潮流、当代时尚和经典，并发现趋势的平均高涨期为四年。研究结果经过行业专家验证，并提供了可视化和解读趋势转变的方法，为视频游戏市场提供了有价值的洞察。", "keywords": "视频游戏趋势, Steam标签, 趋势演变, 数据分析, 市场预测", "comments": "这项研究通过利用Steam标签这一独特且丰富的游戏数据来源，为理解视频游戏市场的动态趋势提供了一种新颖且实用的数据驱动方法。它不仅提出了趋势分类模型，还量化了趋势的生命周期，这对于游戏开发者、投资者和市场分析师都具有重要的指导意义。通过结合专家验证，增加了研究结果的可靠性。其开放的方法也为未来的趋势分析奠定了基础。"}}
{"id": "2506.08967", "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model", "authors": ["Ailin Huang", "Bingxin Li", "Bruce Wang", "Boyong Wu", "Chao Yan", "Chengli Feng", "Heng Wang", "Hongyu Zhou", "Hongyuan Wang", "Jingbei Li", "Jianjian Sun", "Joanna Wang", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Shilei Jiang", "Tian Fei", "Wang You", "Xi Chen", "Xuerui Yang", "Yechang Huang", "Yuxiang Zhang", "Zheng Ge", "Zheng Gong", "Zhewei Huang", "Zixin Zhang", "Bin Wang", "Bo Li", "Buyun Ma", "Changxin Miao", "Changyi Wan", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Hanpeng Hu", "Haonan Jia", "Jiahao Gong", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Junzhe Lin", "Kaixiang Li", "Lei Xia", "Longlong Gu", "Ming Li", "Nie Hao", "Ranchen Ming", "Shaoliang Pang", "Siqi Liu", "Song Yuan", "Tiancheng Cao", "Wen Li", "Wenqing He", "Xu Zhao", "Xuelin Zhang", "Yanbo Yu", "Yinmin Zhong", "Yu Zhou", "Yuanwei Liang", "Yuanwei Lu", "Yuxiang Yang", "Zidong Yang", "Zili Zhang", "Binxing Jiao", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Xinhao Zhang", "Yibo Zhu", "Daxin Jiang", "Shuchang Zhou", "Chen Hu"], "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.", "comment": "12 pages, 3 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08967v1", "AI": {"title_translation": "Step-Audio-AQAA：一个完全端到端表达性大型音频语言模型", "tldr": "Step-Audio-AQAA是一个端到端的大型音频语言模型，解决了现有LALM无法直接生成自然语音响应的问题，在语音控制方面表现出色。", "motivation": "现有的大型音频语言模型（LALMs）依赖于文本输出，限制了它们直接生成自然语音响应的能力，从而阻碍了无缝的音频交互。", "method": "Step-Audio-AQAA是一个完全端到端的大型音频语言模型（LALM），用于音频查询-音频回答（AQAA）任务。它集成了双码本音频分词器用于语言和语义特征提取，一个1300亿参数的骨干LLM，以及一个神经声码器用于高保真语音合成。其后训练方法采用文本和音频交错的token输出以增强语义连贯性，并结合直接偏好优化（DPO）与模型合并来提高性能。", "result": "在StepEval-Audio-360基准测试中，Step-Audio-AQAA在语音控制方面表现出色，超越了最先进的LALM。", "conclusion": "本研究为端到端的大型音频语言模型（LALMs）提供了一个有前景的解决方案，并强调了基于token的声码器在增强AQAA任务整体性能中的关键作用。", "translation": "大型音频语言模型（LALMs）显著推动了智能人机交互的发展，然而，它们对基于文本输出的依赖限制了其直接生成自然语音响应的能力，从而阻碍了无缝的音频交互。为了解决这个问题，我们引入了Step-Audio-AQAA，一个为音频查询-音频回答（AQAA）任务设计的完全端到端LALM。该模型集成了双码本音频分词器用于语言和语义特征提取，一个1300亿参数的骨干LLM以及一个神经声码器用于高保真语音合成。我们的后训练方法采用文本和音频交错的token输出以增强语义连贯性，并结合直接偏好优化（DPO）与模型合并来提高性能。在StepEval-Audio-360基准测试中的评估表明，Step-Audio-AQAA在语音控制方面表现出色，在关键领域超越了最先进的LALM。这项工作为端到端LALM贡献了一个有前景的解决方案，并强调了基于token的声码器在增强AQAA任务整体性能中的关键作用。", "summary": "本文介绍了Step-Audio-AQAA，一个完全端到端的大型音频语言模型（LALM），旨在解决现有LALM无法直接生成自然语音响应的问题。该模型结合了双码本音频分词器、一个1300亿参数的LLM骨干和一个神经声码器，并通过文本和音频交错的token输出以及DPO与模型合并的后训练方法进行优化。在StepEval-Audio-360基准测试中，Step-Audio-AQAA在语音控制方面表现出卓越性能，超越了现有最先进的模型，为端到端LALM和基于token的声码器在AQAA任务中的应用提供了重要见解。", "keywords": "大型音频语言模型, 端到端, 语音合成, 音频查询-音频回答, 神经声码器", "comments": "Step-Audio-AQAA的创新之处在于其完全端到端的架构，直接从音频查询生成音频回答，解决了传统LALM的文本输出限制。双码本音频分词器和1300亿参数的LLM结合神经声码器的设计，以及独特的后训练方法（交错token输出和DPO与模型合并），使其在语音控制方面表现出色，对实现更自然的人机音频交互具有重要意义。"}}
{"id": "2506.08042", "title": "Continuous-Time Output Feedback Adaptive Control for Stabilization and Tracking with Experimental Results", "authors": ["Mohammad Mirtaba", "Ankit Goel"], "summary": "This paper presents a continuous-time output feedback adaptive control\ntechnique for stabilization and tracking control problems. The adaptive\ncontroller is motivated by the classical discrete-time retrospective cost\nadaptive control algorithm. The particle swarm optimization framework automates\nthe adaptive algorithm's hyper-parameter tuning. The proposed controller is\nnumerically validated in the tracking problems of a double integrator and a\nbicopter system and is experimentally validated in an attitude stabilization\nproblem. Numerical and experimental results show that the proposed controller\nis an effective technique for model-free output feedback control.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08042v1", "AI": {"title_translation": "连续时间输出反馈自适应控制用于稳定和跟踪，并附有实验结果", "tldr": "本文提出了一种连续时间输出反馈自适应控制器，用于稳定和跟踪，并通过实验验证了其有效性。", "motivation": "解决稳定和跟踪控制问题，并借鉴了经典的离散时间追溯成本自适应控制算法。", "method": "提出了一种连续时间输出反馈自适应控制技术，该技术利用粒子群优化框架自动调整自适应算法的超参数。", "result": "在双积分器和双旋翼系统的跟踪问题中进行了数值验证，并在姿态稳定问题中进行了实验验证。", "conclusion": "所提出的控制器是一种有效的无模型输出反馈控制技术。", "translation": "本文提出了一种用于稳定和跟踪控制问题的连续时间输出反馈自适应控制技术。该自适应控制器受经典的离散时间追溯成本自适应控制算法启发。粒子群优化框架自动调整自适应算法的超参数。所提出的控制器在双积分器和双旋翼系统的跟踪问题中进行了数值验证，并在姿态稳定问题中进行了实验验证。数值和实验结果表明，所提出的控制器是一种有效的无模型输出反馈控制技术。", "summary": "本文提出了一种连续时间输出反馈自适应控制技术，用于解决稳定和跟踪控制问题。该控制器受离散时间追溯成本自适应控制算法启发，并利用粒子群优化框架自动进行超参数调整。通过对双积分器和双旋翼系统的数值验证以及对姿态稳定问题的实验验证，结果表明该控制器是一种有效的无模型输出反馈控制方法。", "keywords": "连续时间,输出反馈,自适应控制,粒子群优化,无模型控制", "comments": "该论文的创新点在于将粒子群优化引入连续时间输出反馈自适应控制的超参数调优，并通过数值和实验结果验证了其在无模型控制方面的有效性，具有重要的实际应用价值。"}}
{"id": "2506.08673", "title": "Towards Fair Representation: Clustering and Consensus", "authors": ["Diptarka Chakraborty", "Kushagra Chatterjee", "Debarati Das", "Tien Long Nguyen", "Romina Nobahari"], "summary": "Consensus clustering, a fundamental task in machine learning and data\nanalysis, aims to aggregate multiple input clusterings of a dataset,\npotentially based on different non-sensitive attributes, into a single\nclustering that best represents the collective structure of the data. In this\nwork, we study this fundamental problem through the lens of fair clustering, as\nintroduced by Chierichetti et al. [NeurIPS'17], which incorporates the\ndisparate impact doctrine to ensure proportional representation of each\nprotected group in the dataset within every cluster. Our objective is to find a\nconsensus clustering that is not only representative but also fair with respect\nto specific protected attributes. To the best of our knowledge, we are the\nfirst to address this problem and provide a constant-factor approximation.\n  As part of our investigation, we examine how to minimally modify an existing\nclustering to enforce fairness -- an essential postprocessing step in many\nclustering applications that require fair representation. We develop an optimal\nalgorithm for datasets with equal group representation and near-linear time\nconstant factor approximation algorithms for more general scenarios with\ndifferent proportions of two group sizes. We complement our approximation\nresult by showing that the problem is NP-hard for two unequal-sized groups.\nGiven the fundamental nature of this problem, we believe our results on Closest\nFair Clustering could have broader implications for other clustering problems,\nparticularly those for which no prior approximation guarantees exist for their\nfair variants.", "comment": "The paper has been accepted at the Conference on Learning Theory\n  (COLT) 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08673v1", "AI": {"title_translation": "迈向公平表示：聚类与共识", "tldr": "本文首次研究公平共识聚类问题，旨在整合多个输入聚类，同时确保每个受保护群体在每个聚类中都得到比例代表。作者提供了常数因子近似算法，并开发了用于强制执行公平性的最小修改算法，证明了不等大小组情况下问题的NP-难性。", "motivation": "共识聚类旨在将多个输入聚类聚合成一个最佳代表数据集体结构的单一聚类。然而，传统的共识聚类可能无法确保数据集中每个受保护群体在每个聚类中都得到比例代表，即存在歧视性影响。因此，本文的动机是寻找一个既具有代表性又对特定受保护属性公平的共识聚类。", "method": "作者通过公平聚类的视角研究了共识聚类问题，并首次为该问题提供了常数因子近似算法。作为研究的一部分，他们还探讨了如何最小化修改现有聚类以强制实现公平性，这在许多需要公平表示的聚类应用中是一个重要的后处理步骤。具体方法包括：为具有相等组表示的数据集开发了最优算法；为具有不同比例的两个组的更一般场景开发了近线性时间常数因子近似算法；并通过证明对于两个不等大小的组，该问题是NP-难的，来补充其近似结果。", "result": "本文首次为公平共识聚类问题提供了常数因子近似算法。研究结果包括：为具有相等组表示的数据集开发了最优算法；为具有不同比例的两个组的更一般场景开发了近线性时间常数因子近似算法；并证明了对于两个不等大小的组，该问题是NP-难的。", "conclusion": "鉴于公平共识聚类问题的基础性质，本文关于最接近公平聚类（Closest Fair Clustering）的结果可能对其他聚类问题产生更广泛的影响，特别是那些其公平变体尚无先验近似保证的问题。", "translation": "共识聚类是机器学习和数据分析中的一项基本任务，旨在将数据集的多个输入聚类（可能基于不同的非敏感属性）聚合成一个单一聚类，该聚类最能代表数据的集体结构。在这项工作中，我们通过Chierichetti等人[NeurIPS'17]引入的公平聚类视角来研究这个基本问题，该视角结合了不同影响原则，以确保数据集中每个受保护群体在每个聚类中都得到比例代表。我们的目标是找到一个不仅具有代表性而且对特定受保护属性公平的共识聚类。据我们所知，我们是第一个解决这个问题并提供常数因子近似的。作为我们调查的一部分，我们研究了如何最小化修改现有聚类以强制实现公平性——这是许多需要公平表示的聚类应用中必不可少的后处理步骤。我们为具有相等组表示的数据集开发了最优算法，并为具有不同比例的两个组的更一般场景开发了近线性时间常数因子近似算法。我们通过证明对于两个不等大小的组，该问题是NP-难的，来补充我们的近似结果。鉴于这个问题的基础性质，我们相信我们关于最接近公平聚类（Closest Fair Clustering）的结果可能对其他聚类问题产生更广泛的影响，特别是那些其公平变体尚无先验近似保证的问题。", "summary": "本文研究了公平共识聚类问题，旨在将多个聚类结果整合为一个既能代表数据集体结构又对受保护群体实现比例代表的单一公平聚类。作者首次为该问题提供了常数因子近似算法，并开发了用于最小化修改现有聚类以强制实现公平性的最优和近似算法。研究还证明了在两个不等大小组的情况下，该问题是NP-难的。本文认为，其关于最接近公平聚类的结果对其他公平聚类问题具有重要意义。", "keywords": "共识聚类, 公平聚类, 近似算法, NP-难, 比例代表", "comments": "本文的创新之处在于首次将公平性概念引入共识聚类，并为该问题提供了理论上的近似保证。其提出的一系列算法，包括最优算法和近似算法，以及对问题NP-难度的证明，为后续的公平聚类研究奠定了基础。特别是对于需要后处理以强制实现公平性的应用，其最小修改算法具有重要的实践价值。该研究的发现有望为更广泛的公平机器学习问题提供新的视角和解决方案。"}}
{"id": "2506.08442", "title": "MERIT: A Merchant Incentive Ranking Model for Hotel Search & Ranking", "authors": ["Shigang Quan", "Hailong Tan", "Shui Liu", "Zhenzhe zheng", "Ruihao Zhu", "Liangyue Li", "Quan Lu", "Fan Wu"], "summary": "Online Travel Platforms (OTPs) have been working on improving their hotel\nSearch & Ranking (S&R) systems that facilitate efficient matching between\nconsumers and hotels. Existing OTPs focus almost exclusively on improving\nplatform revenue. In this work, we take a first step in incorporating hotel\nmerchants' objectives into the design of hotel S&R systems to achieve an\nincentive loop: the OTP tilts impressions and better-ranked positions to\nmerchants with high quality, and in return, the merchants provide better\nservice to consumers. Three critical design challenges need to be resolved to\nachieve this incentive loop: Matthew Effect in the consumer feedback-loop,\nunclear relation between hotel quality and performance, and conflicts between\nshort-term and long-term revenue. To address these challenges, we propose\nMERIT, a MERchant IncenTive ranking model, which can simultaneously take the\ninterests of merchants and consumers into account. We define a new Merchant\nCompetitiveness Index (MCI) to represent hotel merchant quality and propose a\nnew Merchant Tower to model the relation between MCI and ranking scores. Also,\nwe design a monotonic structure for Merchant Tower to provide a clear relation\nbetween hotel quality and performance. Finally, we propose a Multi-objective\nStratified Pairwise Loss, which can mitigate the conflicts between OTP's\nshort-term and long-term revenue. The offline experiment results indicate that\nMERIT outperforms these methods in optimizing the demands of consumers and\nmerchants. Furthermore, we conduct an online A/B test and obtain an improvement\nof 3.02% for the MCI score.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08442v1", "AI": {"title_translation": "MERIT：一种酒店搜索与排名中的商家激励排名模型", "tldr": "该研究提出MERIT模型，旨在通过激励高质量商家提供更好服务，从而平衡在线旅游平台、商家和消费者的利益，解决现有酒店搜索与排名系统仅关注平台收入的问题。", "motivation": "现有的在线旅游平台（OTPs）在酒店搜索与排名（S&R）系统设计中，几乎完全专注于提高平台收入，忽视了酒店商家的目标，未能形成一个商家提供优质服务以换取更好排名的激励循环。此外，实现这种激励循环还面临消费者反馈循环中的马太效应、酒店质量与表现之间不明确的关系以及短期与长期收入冲突等挑战。", "method": "本文提出了MERIT（MERchant IncenTive）排名模型，旨在同时考虑商家和消费者的利益。具体方法包括：定义新的商家竞争力指数（MCI）来表示酒店商家质量；提出新的商家塔（Merchant Tower）来建模MCI与排名分数之间的关系，并为其设计单调结构以明确酒店质量与表现的关系；最后，提出多目标分层成对损失（Multi-objective Stratified Pairwise Loss）以缓解OTP短期与长期收入之间的冲突。", "result": "离线实验结果表明，MERIT在优化消费者和商家需求方面优于现有方法。在线A/B测试显示，MCI分数获得了3.02%的提升。", "conclusion": "MERIT模型成功地将酒店商家的目标整合到酒店搜索与排名系统中，实现了平台、商家和消费者之间的激励循环，并在实际应用中取得了显著效果，证明了其在平衡各方利益和提升商家竞争力方面的有效性。", "translation": "在线旅游平台（OTPs）一直致力于改进其酒店搜索与排名（S&R）系统，以促进消费者和酒店之间的有效匹配。现有的OTPs几乎完全专注于提高平台收入。在这项工作中，我们首次尝试将酒店商家的目标纳入酒店S&R系统的设计中，以实现一个激励循环：OTPs向高质量商家倾斜曝光和更好的排名位置，作为回报，商家向消费者提供更好的服务。为了实现这种激励循环，需要解决三个关键的设计挑战：消费者反馈循环中的马太效应、酒店质量与表现之间不明确的关系，以及短期与长期收入之间的冲突。为了应对这些挑战，我们提出了MERIT，一个商家激励排名模型，它可以同时考虑商家和消费者的利益。我们定义了一个新的商家竞争力指数（MCI）来表示酒店商家质量，并提出了一个新的商家塔来建模MCI与排名分数之间的关系。此外，我们为商家塔设计了一个单调结构，以提供酒店质量与表现之间的明确关系。最后，我们提出了一个多目标分层成对损失，可以缓解OTP短期和长期收入之间的冲突。离线实验结果表明，MERIT在优化消费者和商家需求方面优于这些方法。此外，我们还进行了在线A/B测试，MCI分数获得了3.02%的提升。", "summary": "本文提出了一种名为MERIT的酒店搜索与排名模型，旨在解决现有在线旅游平台仅关注平台收入的问题。MERIT通过引入商家竞争力指数（MCI）和商家塔结构，并设计多目标损失函数，成功地将商家目标纳入排名系统，形成商家与消费者之间的激励循环。实验结果表明，MERIT在平衡各方利益和提升商家竞争力方面表现优异。", "keywords": "酒店搜索与排名, 商家激励, 多目标优化, 商家竞争力指数, 在线旅游平台", "comments": "这项工作具有创新性，因为它首次将酒店商家的激励机制明确地整合到在线旅游平台的搜索与排名系统中，打破了传统上以平台收入为核心的单一优化目标。通过引入MCI和创新的模型结构，MERIT不仅提升了匹配效率，也潜在地改善了消费者体验和商家服务质量，为在线平台设计提供了新的视角，具有重要的实践意义。"}}
{"id": "2506.08444", "title": "2N-storage Runge-Kutta methods: c-reflection symmetry and factorization of the Butcher tableau", "authors": ["Alexei Bazavov"], "summary": "Low-storage Runge-Kutta schemes of Williamson's type, so-called 2N-storage\nschemes, are further examined as a follow-up to the recent work. It is found\nthat the augmented Butcher tableau factorizes into a product of matrices with\nspecial properties. Those properties reveal that the 2N-storage methods of the\norder of global accuracy less than five possess a symmetry, called c-reflection\nsymmetry, i.e. most methods exist in pairs. A transformation that relates the\nButcher tableaux of the pairs is found and the fact that the c-reflected method\nsatisfies the same order conditions as the original one is proven. Numerical\nevidence that validates the analytic results is presented. Branches of\nsolutions for (5,4) methods, first explored by Carpenter and Kennedy, are\nconstructed numerically. Four new (5,4) schemes with coefficients expressed in\nradicals and one with rational coefficients are examined for illustration.\nEight new (6,4) schemes, some of which can be expressed in rationals or\nradicals, and one (8,4) scheme, are studied to understand the practical\nimplications of the c-reflection symmetry for methods with higher number of\nstages. In the absence of closed-form analytic solutions for 2N-storage\nRunge-Kutta methods of order four and above, the general symmetry properties,\nas well as some specific analytic solutions presented here, may help in\ndevelopment and optimization of 2N-storage schemes.", "comment": "41 pages, 7 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08444v1", "AI": {"title_translation": "2N-存储Runge-Kutta方法：c-反射对称性与Butcher表分解", "tldr": "本文深入研究了2N-存储Runge-Kutta方法，发现低阶方法存在c-反射对称性并能分解Butcher表，有助于开发新的数值方案。", "motivation": "作为近期工作的后续，进一步考察Williamson类型的低存储Runge-Kutta方案（即2N-存储方案）。在缺乏四阶及以上2N-存储Runge-Kutta方法闭合形式解析解的情况下，研究其性质。", "method": "研究了增广Butcher表，发现其可以分解为具有特殊性质的矩阵乘积。发现全局精度低于五阶的2N-存储方法具有c-反射对称性，并找到了关联这些方法对的Butcher表变换。证明了c-反射方法满足与原始方法相同的阶条件。通过数值证据验证了分析结果。构造并考察了新的(5,4)、(6,4)和(8,4)方案。", "result": "增广Butcher表可分解为具有特殊性质的矩阵乘积。全局精度低于五阶的2N-存储方法具有c-反射对称性，即大多数方法成对存在。找到了关联这些方法对的Butcher表变换，并证明了c-反射方法满足与原始方法相同的阶条件。数值证据验证了分析结果。数值构造了(5,4)方法的解分支。考察了四个新的(5,4)方案（其中一个具有有理系数，其余为根式系数）和八个新的(6,4)方案（部分为有理或根式），以及一个(8,4)方案。", "conclusion": "所提出的通用对称性质（特别是c-反射对称性）以及一些具体的解析解，可能有助于2N-存储方案的开发和优化，尤其是在缺乏高阶方法闭合形式解析解的情况下。", "translation": "Williamson类型的低存储Runge-Kutta方案，即所谓的2N-存储方案，作为近期工作的后续，得到了进一步的考察。研究发现，增广Butcher表可分解为具有特殊性质的矩阵乘积。这些性质揭示了全局精度低于五阶的2N-存储方法具有一种称为c-反射对称性，即大多数方法成对存在。本文发现了一种关联这些方法对的Butcher表变换，并证明了c-反射方法满足与原始方法相同的阶条件。文中提供了验证分析结果的数值证据。数值构造了Carpenter和Kennedy首次探索的(5,4)方法的解分支。为了说明，考察了四个新的(5,4)方案，其系数以根式表示，其中一个具有有理系数。研究了八个新的(6,4)方案，其中一些可以有理或根式表示，以及一个(8,4)方案，以理解c-反射对称性对具有更多阶段的方法的实际影响。在四阶及以上2N-存储Runge-Kutta方法缺乏闭合形式解析解的情况下，这里提出的通用对称性质以及一些具体的解析解，可能有助于2N-存储方案的开发和优化。", "summary": "本文深入研究了2N-存储Runge-Kutta方法，发现其增广Butcher表可分解，且全局精度低于五阶的方法表现出c-反射对称性，这意味着大多数方法成对存在。研究确定了这些方法对之间的变换关系，并证明了它们满足相同的阶条件，且提供了数值验证。此外，本文还构造并考察了多个新的(5,4)、(6,4)和(8,4)方案。这些关于对称性质和解析解的发现预计将有助于2N-存储方案的开发和优化，尤其是在高阶方法缺乏闭合形式解的情况下。", "keywords": "Runge-Kutta方法, 低存储方案, Butcher表, c-反射对称性, 数值积分", "comments": "本文通过揭示2N-存储Runge-Kutta方法中一种基本对称性（c-反射对称性），做出了重要贡献。Butcher表的分解以及成对方法的发现，简化了新方案的搜索，特别是对于难以获得解析解的高阶方法。这有望简化高效数值积分器的设计和优化。"}}
{"id": "2506.08866", "title": "SmartAttack: Air-Gap Attack via Smartwatches", "authors": ["Mordechai Guri"], "summary": "Air-gapped systems are considered highly secure against data leaks due to\ntheir physical isolation from external networks. Despite this protection,\nultrasonic communication has been demonstrated as an effective method for\nexfiltrating data from such systems. While smartphones have been extensively\nstudied in the context of ultrasonic covert channels, smartwatches remain an\nunderexplored yet effective attack vector.\n  In this paper, we propose and evaluate SmartAttack, a novel method that\nleverages smartwatches as receivers for ultrasonic covert communication in\nair-gapped environments. Our approach utilizes the built-in microphones of\nsmartwatches to capture covert signals in real time within the ultrasonic\nfrequency range of 18-22 kHz. Through experimental validation, we assess the\nfeasibility of this attack under varying environmental conditions, distances,\norientations, and noise levels. Furthermore, we analyze smartwatch-specific\nfactors that influence ultrasonic covert channels, including their continuous\npresence on the user's wrist, the impact of the human body on signal\npropagation, and the directional constraints of built-in microphones. Our\nfindings highlight the security risks posed by smartwatches in high-security\nenvironments and outline mitigation strategies to counteract this emerging\nthreat.", "comment": "Accepted to IEEE COMPSAC 2025 (SEPT)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08866v1", "AI": {"title_translation": "SmartAttack: 通过智能手表进行的空隙攻击", "tldr": "研究提出SmartAttack，利用智能手表作为接收器，通过超声波从空隙系统窃取数据，揭示了智能手表在安全环境中的新风险。", "motivation": "空隙系统被认为高度安全，但超声波通信已被证明可用于数据泄露。虽然智能手机已被广泛研究，但智能手表作为攻击载体仍未被充分探索，因此研究旨在填补这一空白。", "method": "本文提出SmartAttack，一种利用智能手表内置麦克风在18-22 kHz超声波频率范围内实时捕获隐蔽信号的方法。通过实验验证了该攻击在不同环境条件、距离、方向和噪声水平下的可行性，并分析了智能手表特定因素（如持续佩戴、人体影响和麦克风方向性）对超声波隐蔽通道的影响。", "result": "实验结果突出了智能手表在高安全环境中所构成的安全风险。", "conclusion": "智能手表可作为空隙系统数据泄露的有效攻击向量，需要制定缓解策略来应对这一新兴威胁。", "translation": "空隙系统因其与外部网络的物理隔离而被认为对数据泄露具有高度安全性。尽管有这种保护，超声波通信已被证明是从此类系统渗出数据的有效方法。虽然智能手机在超声波隐蔽通道方面已被广泛研究，但智能手表仍是一个未被充分探索但有效的攻击载体。\n在本文中，我们提出并评估了SmartAttack，这是一种利用智能手表作为接收器在空隙环境中进行超声波隐蔽通信的新方法。我们的方法利用智能手表的内置麦克风在18-22 kHz的超声波频率范围内实时捕获隐蔽信号。通过实验验证，我们评估了这种攻击在不同环境条件、距离、方向和噪声水平下的可行性。此外，我们分析了影响超声波隐蔽通道的智能手表特定因素，包括它们持续佩戴在用户手腕上、人体对信号传播的影响以及内置麦克风的方向限制。我们的研究结果突出了智能手表在高安全环境中构成的安全风险，并概述了应对这一新兴威胁的缓解策略。", "summary": "本文提出并评估了SmartAttack，一种利用智能手表内置麦克风进行超声波隐蔽通信的新方法，旨在从空隙系统中窃取数据。研究通过实验验证了该攻击在不同环境下的可行性，并分析了智能手表特有的因素（如佩戴位置和人体影响）对信号传播的影响。结果表明智能手表是高安全环境中的一个新兴安全风险，并提出了相应的缓解策略。", "keywords": "空隙系统, 超声波通信, 智能手表, 数据泄露, 隐蔽通道", "comments": "这项研究通过揭示智能手表作为空隙系统数据泄露的新型攻击向量，具有重要的创新性。它扩展了对超声波隐蔽通道的理解，并强调了在设计高安全系统时需要考虑智能手表带来的潜在风险。"}}
{"id": "2506.08688", "title": "Causality-aware Safety Testing for Autonomous Driving Systems", "authors": ["Wenbing Tang", "Mingfei Cheng", "Renzhi Wang", "Yuan Zhou", "Chengwei Liu", "Yang Liu", "Zuohua Ding"], "summary": "Simulation-based testing is essential for evaluating the safety of Autonomous\nDriving Systems (ADSs). Comprehensive evaluation requires testing across\ndiverse scenarios that can trigger various types of violations under different\nconditions. While existing methods typically focus on individual diversity\nmetrics, such as input scenarios, ADS-generated motion commands, and system\nviolations, they often fail to capture the complex interrelationships among\nthese elements. This oversight leads to gaps in testing coverage, potentially\nmissing critical issues in the ADS under evaluation. However, quantifying these\ninterrelationships presents a significant challenge. In this paper, we propose\na novel causality-aware fuzzing technique, Causal-Fuzzer, to enable efficient\nand comprehensive testing of ADSs by exploring causally diverse scenarios. The\ncore of Causal-Fuzzer is constructing a causal graph to model the\ninterrelationships among the diversities of input scenarios, ADS motion\ncommands, and system violations. Then the causal graph will guide the process\nof critical scenario generation. Specifically, Causal-Fuzzer proposes (1) a\ncausality-based feedback mechanism that quantifies the combined diversity of\ntest scenarios by assessing whether they activate new causal relationships, and\n(2) a causality-driven mutation strategy that prioritizes mutations on input\nscenario elements with higher causal impact on ego action changes and violation\noccurrence, rather than treating all elements equally. We evaluated\nCausal-Fuzzer on an industry-grade ADS Apollo, with a high-fidelity. Our\nempirical results demonstrate that Causal-Fuzzer significantly outperforms\nexisting methods in (1) identifying a greater diversity of violations, (2)\nproviding enhanced testing sufficiency with improved coverage of causal\nrelationships, and (3) achieving greater efficiency in detecting the first\ncritical scenarios.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08688v1", "AI": {"title_translation": "面向自动驾驶系统的因果感知安全测试", "tldr": "本文提出了一种新颖的因果感知模糊测试技术Causal-Fuzzer，通过建模和利用场景、动作和违规之间的因果关系，显著提高了自动驾驶系统安全测试的覆盖范围和效率。", "motivation": "现有自动驾驶系统（ADS）的仿真测试方法通常只关注单一多样性指标，未能捕捉输入场景、ADS生成的运动指令和系统违规之间复杂的相互关系，导致测试覆盖不足，可能遗漏关键问题。量化这些相互关系是一个重大挑战。", "method": "本文提出了一种新颖的因果感知模糊测试技术Causal-Fuzzer，旨在通过探索因果多样性场景来实现ADS的高效全面测试。Causal-Fuzzer的核心是构建一个因果图来建模输入场景、ADS运动指令和系统违规之间多样性的相互关系。该方法提出：(1)一种基于因果关系的反馈机制，通过评估是否激活新的因果关系来量化测试场景的组合多样性；(2)一种因果驱动的变异策略，优先对对自我行动变化和违规发生具有更高因果影响的输入场景元素进行变异。", "result": "Causal-Fuzzer在工业级ADS Apollo上进行了高保真评估。实验结果表明，Causal-Fuzzer在以下方面显著优于现有方法：(1)识别出更多样化的违规行为；(2)通过改进因果关系覆盖率提供增强的测试充分性；(3)在检测第一个关键场景方面实现更高的效率。", "conclusion": "Causal-Fuzzer通过显式考虑因果关系，为自动驾驶系统的安全测试提供了一种更高效、更全面的方法，解决了现有方法的局限性。", "translation": "基于仿真的测试对于评估自动驾驶系统（ADS）的安全性至关重要。全面的评估需要在不同条件下触发各种类型违规的多样化场景中进行测试。虽然现有方法通常侧重于个体多样性指标，例如输入场景、ADS生成的运动指令和系统违规，但它们往往未能捕捉这些元素之间复杂的相互关系。这种疏忽导致测试覆盖存在空白，可能遗漏被评估ADS中的关键问题。然而，量化这些相互关系提出了一个重大挑战。在本文中，我们提出了一种新颖的因果感知模糊测试技术Causal-Fuzzer，通过探索因果多样性场景来实现ADS的高效全面测试。Causal-Fuzzer的核心是构建一个因果图来建模输入场景、ADS运动指令和系统违规之间多样性的相互关系。然后，该因果图将指导关键场景的生成过程。具体而言，Causal-Fuzzer提出了（1）一种基于因果关系的反馈机制，通过评估测试场景是否激活新的因果关系来量化其组合多样性，以及（2）一种因果驱动的变异策略，优先对对自我行动变化和违规发生具有更高因果影响的输入场景元素进行变异，而不是平等对待所有元素。我们在工业级ADS Apollo上进行了高保真评估。我们的实证结果表明，Causal-Fuzzer在以下方面显著优于现有方法：（1）识别出更多样化的违规行为，（2）通过改进因果关系覆盖率提供增强的测试充分性，以及（3）在检测第一个关键场景方面实现更高的效率。", "summary": "本文提出了一种名为Causal-Fuzzer的新型因果感知模糊测试技术，旨在提升自动驾驶系统（ADS）的安全测试水平。与现有方法忽略输入场景、ADS动作和系统违规之间复杂因果相互关系不同，Causal-Fuzzer通过构建因果图来指导关键场景的生成。它包含一个基于因果关系的反馈机制用于多样性评估，以及一个因果驱动的变异策略以优先处理高影响元素。在工业级ADS上的实证评估表明，Causal-Fuzzer显著提升了违规多样性、测试充分性以及检测关键场景的效率。", "keywords": "因果关系, 安全测试, 自动驾驶系统, 模糊测试, 因果图", "comments": "这项工作的创新之处在于明确地建模和利用因果关系进行模糊测试，这为提高自动驾驶系统安全测试的全面性和效率提供了一种新颖的方法，填补了当前基于仿真的测试方法的关键空白。"}}
{"id": "2506.08578", "title": "Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot", "authors": ["Boyang Chen", "Xizhe Zang", "Chao Song", "Yue Zhang", "Xuehe Zhang", "Jie Zhao"], "summary": "The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design\ninspired by the rollover shape of the human foot, significantly enhances the\nenergy efficiency of the robot walking gait. However, due to the tilt of the\nsupporting leg, the error of the contact model are amplified, making robot\nstate estimation more challenging. Therefore, this paper focuses on the noise\nanalysis and state estimation for robot walking with the ESVC foot. First,\nthrough physical robot experiments, we investigate the effect of the ESVC foot\non robot measurement noise and process noise. and a noise-time regression model\nusing sliding window strategy is developed. Then, a hierarchical adaptive state\nestimator for biped robots with the ESVC foot is proposed. The state estimator\nconsists of two stages: pre-estimation and post-estimation. In the\npre-estimation stage, a data fusion-based estimation is employed to process the\nsensory data. During post-estimation, the acceleration of center of mass is\nfirst estimated, and then the noise covariance matrices are adjusted based on\nthe regression model. Following that, an EKF(Extended Kalman Filter) based\napproach is applied to estimate the centroid state during robot walking.\nPhysical experiments demonstrate that the proposed adaptive state estimator for\nbiped robot walking with the ESVC foot not only provides higher precision than\nboth EKF and Adaptive EKF, but also converges faster under varying noise\nconditions.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08578v1", "AI": {"title_translation": "仿人机器人ESVC足步态噪声分析与分层自适应机器人状态估计器", "tldr": "针对ESVC足机器人步态中状态估计的挑战，本文提出了一种分层自适应状态估计器，通过噪声分析和回归模型，实现了比EKF和自适应EKF更高的精度和更快的收敛速度。", "motivation": "ESVC足虽然显著提高了机器人行走步态的能量效率，但由于支撑腿倾斜，接触模型的误差被放大，使得机器人状态估计更具挑战性。", "method": "首先，通过物理机器人实验研究了ESVC足对机器人测量噪声和过程噪声的影响，并开发了使用滑动窗口策略的噪声-时间回归模型。然后，提出了一种分层自适应双足机器人状态估计器，该估计器包含预估计（采用基于数据融合的估计处理传感器数据）和后估计（首先估计质心加速度，然后根据回归模型调整噪声协方差矩阵，最后应用EKF估计质心状态）。", "result": "物理实验表明，所提出的自适应状态估计器在双足机器人ESVC足步态中，不仅比传统的EKF和自适应EKF具有更高的精度，而且在不同噪声条件下收敛速度更快。", "conclusion": "本文提出的分层自适应状态估计器有效解决了ESVC足机器人步态中的状态估计挑战，提供了更优越的性能，显著提高了估计精度和收敛速度。", "translation": "ESVC（基于椭圆分段变曲率）足是一种受人类足部翻转形状启发的机器人足部设计，显著提高了机器人行走步态的能量效率。然而，由于支撑腿的倾斜，接触模型的误差被放大，使得机器人状态估计更具挑战性。因此，本文重点关注ESVC足机器人行走时的噪声分析和状态估计。首先，通过物理机器人实验，我们研究了ESVC足对机器人测量噪声和过程噪声的影响，并开发了一种使用滑动窗口策略的噪声-时间回归模型。然后，提出了一种用于ESVC足双足机器人的分层自适应状态估计器。该状态估计器由两个阶段组成：预估计和后估计。在预估计阶段，采用基于数据融合的估计来处理传感器数据。在后估计阶段，首先估计质心加速度，然后根据回归模型调整噪声协方差矩阵。之后，应用基于EKF（扩展卡尔曼滤波器）的方法来估计机器人行走过程中的质心状态。物理实验表明，所提出的用于ESVC足双足机器人步态的自适应状态估计器不仅比EKF和自适应EKF提供更高的精度，而且在不同噪声条件下收敛更快。", "summary": "本文针对采用ESVC足的双足机器人行走时面临的状态估计挑战，提出了一种分层自适应状态估计器。该方法首先通过实验分析了ESVC足对机器人噪声的影响，并建立了噪声-时间回归模型。随后，设计了一个包含数据融合预估计和基于EKF后估计（结合噪声协方差调整）的两阶段估计器。实验结果表明，该估计器在精度和收敛速度上均优于传统的EKF和自适应EKF。", "keywords": "ESVC足, 机器人状态估计, 噪声分析, 自适应卡尔曼滤波, 双足机器人", "comments": "这项研究通过深入分析ESVC足对机器人噪声的影响并提出创新的分层自适应状态估计器，有效解决了机器人行走步态中复杂的状态估计问题。其贡献在于将噪声特性与估计器设计相结合，提高了估计精度和收敛速度，对仿人机器人控制和稳定性具有重要意义。"}}
{"id": "2506.08220", "title": "Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence", "authors": ["Octave Mariotti", "Zhipeng Du", "Yash Bhalgat", "Oisin Mac Aodha", "Hakan Bilen"], "summary": "Semantic correspondence (SC) aims to establish semantically meaningful\nmatches across different instances of an object category. We illustrate how\nrecent supervised SC methods remain limited in their ability to generalize\nbeyond sparsely annotated training keypoints, effectively acting as keypoint\ndetectors. To address this, we propose a novel approach for learning dense\ncorrespondences by lifting 2D keypoints into a canonical 3D space using\nmonocular depth estimation. Our method constructs a continuous canonical\nmanifold that captures object geometry without requiring explicit 3D\nsupervision or camera annotations. Additionally, we introduce SPair-U, an\nextension of SPair-71k with novel keypoint annotations, to better assess\ngeneralization. Experiments not only demonstrate that our model significantly\noutperforms supervised baselines on unseen keypoints, highlighting its\neffectiveness in learning robust correspondences, but that unsupervised\nbaselines outperform supervised counterparts when generalized across different\ndatasets.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08220v1", "AI": {"title_translation": "似曾相识：揭示监督语义对应中的泛化差距", "tldr": "现有监督语义对应方法泛化能力差，本文提出通过单目深度估计将2D关键点提升到3D空间学习密集对应，并在新数据集上表现出色，且发现无监督方法在跨数据集泛化时优于监督方法。", "motivation": "现有的监督语义对应（SC）方法在泛化到稀疏标注训练关键点之外时表现有限，更像是关键点检测器，未能有效学习语义对应的泛化能力。", "method": "本文提出一种新颖的方法，通过使用单目深度估计将2D关键点提升到规范的3D空间来学习密集对应。该方法构建了一个连续的规范流形，捕获物体几何形状，无需显式3D监督或相机标注。此外，引入了SPair-U数据集（SPair-71k的扩展，包含新颖的关键点标注）以更好地评估泛化能力。", "result": "实验证明，该模型在未见关键点上显著优于监督基线，表明其在学习鲁棒对应方面的有效性。同时，结果也显示无监督基线在跨不同数据集泛化时表现优于监督对应方法。", "conclusion": "本文提出的通过将2D关键点提升到3D空间学习密集对应的方法有效解决了监督语义对应方法的泛化能力不足问题，并在新数据集上取得了显著的性能提升。此外，研究发现无监督方法在跨数据集泛化方面具有优势。", "translation": "语义对应（SC）旨在建立不同对象类别实例之间语义上有意义的匹配。我们阐明了最近的监督SC方法在泛化到稀疏标注训练关键点之外的能力方面仍然有限，实际上更像是关键点检测器。为了解决这个问题，我们提出了一种新颖的方法，通过使用单目深度估计将2D关键点提升到规范的3D空间来学习密集对应。我们的方法构建了一个连续的规范流形，捕获物体几何形状，无需显式3D监督或相机标注。此外，我们引入了SPair-U，一个带有新颖关键点标注的SPair-71k扩展，以更好地评估泛化。实验不仅证明我们的模型在未见关键点上显著优于监督基线，突出了其在学习鲁棒对应方面的有效性，而且还表明无监督基线在跨不同数据集泛化时优于监督对应方法。", "summary": "本文揭示了监督语义对应方法在泛化能力上的不足，指出它们常退化为关键点检测器。为解决此问题，作者提出了一种新方法，通过单目深度估计将2D关键点映射到规范3D空间以学习密集对应，无需3D监督。同时引入了SPair-U数据集用于泛化评估。实验表明，该方法在未见关键点上显著优于监督基线，且无监督方法在跨数据集泛化上表现更优。", "keywords": "语义对应, 泛化能力, 单目深度估计, 3D空间, 密集对应", "comments": "本文创新性地利用单目深度估计将2D关键点提升到3D空间来解决语义对应中的泛化问题，避免了对显式3D监督的需求。引入新的数据集SPair-U有助于更全面地评估泛化能力。研究结果不仅证实了所提方法的有效性，还揭示了无监督方法在跨数据集泛化方面的潜在优势，对未来研究方向具有重要启示。"}}
{"id": "2506.08401", "title": "Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems", "authors": ["Runze Li", "Di Jin", "Xiaobao Wang", "Dongxiao He", "Bingdao Feng", "Zhen Wang"], "summary": "Graph recommendation systems have been widely studied due to their ability to\neffectively capture the complex interactions between users and items. However,\nthese systems also exhibit certain vulnerabilities when faced with attacks. The\nprevailing shilling attack methods typically manipulate recommendation results\nby injecting a large number of fake nodes and edges. However, such attack\nstrategies face two primary challenges: low stealth and high destructiveness.\nTo address these challenges, this paper proposes a novel graph backdoor attack\nmethod that aims to enhance the exposure of target items to the target user in\na covert manner, without affecting other unrelated nodes. Specifically, we\ndesign a single-node trigger generator, which can effectively expose multiple\ntarget items to the target user by inserting only one fake user node.\nAdditionally, we introduce constraint conditions between the target nodes and\nirrelevant nodes to mitigate the impact of fake nodes on the recommendation\nsystem's performance. Experimental results show that the exposure of the target\nitems reaches no less than 50% in 99% of the target users, while the impact on\nthe recommendation system's performance is controlled within approximately 5%.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08401v1", "AI": {"title_translation": "图推荐系统中单节点触发后门攻击", "tldr": "本文提出了一种新型图后门攻击方法，通过插入单个虚假用户节点，以隐蔽方式提高目标用户对目标物品的曝光度，同时将对推荐系统性能的影响降至最低。", "motivation": "现有推销攻击方法（通过注入大量虚假节点和边）面临隐蔽性低和破坏性强的挑战。本文旨在解决这些问题，提出一种更隐蔽且对系统影响更小的攻击方法。", "method": "提出了一种新颖的图后门攻击方法。具体设计了一个单节点触发生成器，仅通过插入一个虚假用户节点即可使多个目标物品暴露给目标用户。此外，引入了目标节点和不相关节点之间的约束条件，以减轻虚假节点对推荐系统性能的影响。", "result": "实验结果表明，在99%的目标用户中，目标物品的曝光率不低于50%，同时对推荐系统性能的影响控制在5%左右。", "conclusion": "该研究成功地提出了一种高效且隐蔽的单节点触发后门攻击方法，有效提升了目标物品的曝光度，同时最大限度地降低了对推荐系统整体性能的负面影响，解决了传统推销攻击的局限性。", "translation": "图推荐系统因其有效捕捉用户和物品之间复杂交互的能力而被广泛研究。然而，这些系统在面对攻击时也表现出一定的脆弱性。流行的推销攻击方法通常通过注入大量虚假节点和边来操纵推荐结果。然而，此类攻击策略面临两个主要挑战：隐蔽性低和破坏性高。为了解决这些挑战，本文提出了一种新颖的图后门攻击方法，旨在以隐蔽方式增强目标用户对目标物品的曝光度，而不影响其他不相关的节点。具体来说，我们设计了一个单节点触发生成器，它可以通过仅插入一个虚假用户节点来有效地将多个目标物品暴露给目标用户。此外，我们引入了目标节点和无关节点之间的约束条件，以减轻虚假节点对推荐系统性能的影响。实验结果表明，在99%的目标用户中，目标物品的曝光率达到不低于50%，同时对推荐系统性能的影响控制在5%左右。", "summary": "本文针对图推荐系统中现有推销攻击隐蔽性差和破坏性大的问题，提出了一种新颖的单节点触发图后门攻击方法。该方法通过设计一个单节点触发生成器，仅插入一个虚假用户节点，即可隐蔽地将多个目标物品曝光给目标用户，同时通过引入约束条件将对推荐系统性能的影响控制在5%左右，实验验证了其高效性和隐蔽性。", "keywords": "图推荐系统, 后门攻击, 单节点触发, 推销攻击, 隐蔽性", "comments": "该研究提出了一个创新性的单节点触发后门攻击，显著提高了攻击的隐蔽性和效率，解决了传统推销攻击需要大量虚假节点的问题，对理解和防御图推荐系统中的新型攻击具有重要意义。其在保持系统性能的同时实现高攻击成功率的特点值得关注。"}}
{"id": "2506.08070", "title": "Info-Coevolution: An Efficient Framework for Data Model Coevolution", "authors": ["Ziheng Qin", "Hailun Xu", "Wei Chee Yew", "Qi Jia", "Yang Luo", "Kanchan Sarkar", "Danhui Guan", "Kai Wang", "Yang You"], "summary": "Machine learning relies heavily on data, yet the continuous growth of\nreal-world data poses challenges for efficient dataset construction and\ntraining. A fundamental yet unsolved question is: given our current model and\ndata, does a new data (sample/batch) need annotation/learning? Conventional\napproaches retain all available data, leading to non-optimal data and training\nefficiency. Active learning aims to reduce data redundancy by selecting a\nsubset of samples to annotate, while it increases pipeline complexity and\nintroduces bias. In this work, we propose Info-Coevolution, a novel framework\nthat efficiently enables models and data to coevolve through online selective\nannotation with no bias. Leveraging task-specific models (and open-source\nmodels), it selectively annotates and integrates online and web data to improve\ndatasets efficiently. For real-world datasets like ImageNet-1K,\nInfo-Coevolution reduces annotation and training costs by 32\\% without\nperformance loss. It is able to automatically give the saving ratio without\ntuning the ratio. It can further reduce the annotation ratio to 50\\% with\nsemi-supervised learning. We also explore retrieval-based dataset enhancement\nusing unlabeled open-source data. Code is available at\nhttps://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.", "comment": "V1", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08070v1", "AI": {"title_translation": "信息共演：一种高效的数据模型共演框架", "tldr": "Info-Coevolution是一个高效的框架，通过在线选择性标注实现模型和数据的协同演化，显著降低了数据标注和训练成本，同时保持性能且无偏差。", "motivation": "机器学习严重依赖数据，但真实世界数据的持续增长给高效数据集构建和训练带来了挑战。传统方法保留所有数据导致效率低下，而主动学习虽然减少冗余但增加了复杂性并引入了偏差。", "method": "本文提出了Info-Coevolution框架，通过无偏差的在线选择性标注高效地实现模型和数据的协同演化。它利用特定任务模型（和开源模型）选择性地标注和整合在线及网络数据，以高效改进数据集。此外，还探索了使用未标注开源数据进行基于检索的数据集增强。", "result": "对于ImageNet-1K等真实世界数据集，Info-Coevolution在不损失性能的情况下，将标注和训练成本降低了32%。它能够自动给出节省比例而无需调整。通过半监督学习，标注比例可进一步降低至50%。", "conclusion": "Info-Coevolution框架通过高效的在线选择性标注，实现了模型和数据的无偏协同演化，显著降低了数据标注和训练成本，同时保持了性能。", "translation": "机器学习严重依赖数据，然而真实世界数据的持续增长给高效数据集构建和训练带来了挑战。一个基本但尚未解决的问题是：鉴于我们当前的模型和数据，新的数据（样本/批次）是否需要标注/学习？传统方法保留所有可用数据，导致数据和训练效率低下。主动学习旨在通过选择样本子集进行标注来减少数据冗余，但它增加了管道复杂性并引入了偏差。在这项工作中，我们提出了Info-Coevolution，一个新颖的框架，通过无偏差的在线选择性标注高效地实现模型和数据的协同演化。它利用特定任务模型（和开源模型）选择性地标注和整合在线和网络数据，以高效改进数据集。对于ImageNet-1K等真实世界数据集，Info-Coevolution在不损失性能的情况下，将标注和训练成本降低了32%。它能够自动给出节省比例而无需调整。通过半监督学习，标注比例可进一步降低至50%。我们还探索了使用未标注开源数据进行基于检索的数据集增强。代码可在https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/获取。", "summary": "针对机器学习中数据增长导致的数据集构建和训练效率低下问题，本文提出了Info-Coevolution框架。该框架通过无偏差的在线选择性标注，实现了模型和数据的协同演化。它利用现有模型选择性地标注和整合数据，有效降低了标注和训练成本，例如在ImageNet-1K上实现了32%的成本降低而无性能损失，并通过半监督学习可进一步减少标注比例至50%。该方法解决了传统方法效率低下和主动学习引入偏差的问题。", "keywords": "数据模型协同演化, 在线选择性标注, 成本效率, 机器学习, 数据集增强", "comments": "Info-Coevolution的创新之处在于它提供了一种无偏差的在线选择性标注方法，实现了数据和模型的协同演化，这在处理不断增长的现实世界数据方面具有重要意义。它不仅显著降低了数据标注和训练成本，还避免了主动学习的复杂性和偏差，具有很高的实用价值和效率提升。"}}
{"id": "2506.08738", "title": "Societal AI Research Has Become Less Interdisciplinary", "authors": ["Dror Kris Markus", "Fabrizio Gilardi", "Daria Stetsenko"], "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday\nlife, calls to align AI development with ethical and societal values have\nintensified. Interdisciplinary collaboration is often championed as a key\npathway for fostering such engagement. Yet it remains unclear whether\ninterdisciplinary research teams are actually leading this shift in practice.\nThis study analyzes over 100,000 AI-related papers published on ArXiv between\n2014 and 2024 to examine how ethical values and societal concerns are\nintegrated into technical AI research. We develop a classifier to identify\nsocietal content and measure the extent to which research papers express these\nconsiderations. We find a striking shift: while interdisciplinary teams remain\nmore likely to produce societally-oriented research, computer science-only\nteams now account for a growing share of the field's overall societal output.\nThese teams are increasingly integrating societal concerns into their papers\nand tackling a wide range of domains - from fairness and safety to healthcare\nand misinformation. These findings challenge common assumptions about the\ndrivers of societal AI and raise important questions. First, what are the\nimplications for emerging understandings of AI safety and governance if most\nsocietally-oriented research is being undertaken by exclusively technical\nteams? Second, for scholars in the social sciences and humanities: in a\ntechnical field increasingly responsive to societal demands, what distinctive\nperspectives can we still offer to help shape the future of AI?", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08738v1", "AI": {"title_translation": "社会人工智能研究的跨学科性降低", "tldr": "人工智能社会研究正越来越多地由计算机科学团队而非仅仅跨学科团队进行，这挑战了现有假设。", "motivation": "随着人工智能系统深入日常生活，对人工智能发展与伦理和社会价值观保持一致的呼声日益高涨。跨学科合作常被认为是促进这种融合的关键途径。然而，跨学科研究团队是否真正在实践中引领这一转变仍不清楚。", "method": "本研究分析了2014年至2024年间在ArXiv上发表的超过100,000篇人工智能相关论文，以检验伦理价值观和社会关注点如何融入技术人工智能研究。研究开发了一个分类器来识别社会内容，并衡量研究论文表达这些考虑的程度。", "result": "研究发现了一个显著的转变：尽管跨学科团队仍然更有可能产出面向社会的研究，但纯计算机科学团队现在在整个领域的社会产出中所占份额越来越大。这些团队正日益将社会关注点融入其论文中，并解决了从公平性和安全性到医疗保健和错误信息等广泛领域的问题。", "conclusion": "这些发现挑战了关于社会人工智能驱动因素的普遍假设，并提出了重要问题。首先，如果大多数面向社会的研究是由纯技术团队承担，这对新兴的人工智能安全和治理理解有何影响？其次，对于社会科学和人文学科的学者：在一个日益响应社会需求的技术领域中，我们还能提供哪些独特的视角来帮助塑造人工智能的未来？", "translation": "随着人工智能（AI）系统深入日常生活，要求人工智能发展与伦理和社会价值观保持一致的呼声日益高涨。跨学科合作常被认为是促进这种融合的关键途径。然而，跨学科研究团队是否真正在实践中引领这一转变仍不清楚。本研究分析了2014年至2024年间在ArXiv上发表的超过100,000篇人工智能相关论文，以检验伦理价值观和社会关注点如何融入技术人工智能研究。我们开发了一个分类器来识别社会内容，并衡量研究论文表达这些考虑的程度。我们发现了一个显著的转变：尽管跨学科团队仍然更有可能产出面向社会的研究，但纯计算机科学团队现在在整个领域的社会产出中所占份额越来越大。这些团队正日益将社会关注点融入其论文中，并解决了从公平性和安全性到医疗保健和错误信息等广泛领域的问题。这些发现挑战了关于社会人工智能驱动因素的普遍假设，并提出了重要问题。首先，如果大多数面向社会的研究是由纯技术团队承担，这对新兴的人工智能安全和治理理解有何影响？其次，对于社会科学和人文学科的学者：在一个日益响应社会需求的技术领域中，我们还能提供哪些独特的视角来帮助塑造人工智能的未来？", "summary": "本研究分析了2014年至2024年间在ArXiv上发表的逾10万篇人工智能相关论文，旨在考察伦理和社会关注点如何融入技术人工智能研究。研究发现，尽管跨学科团队仍更倾向于产出面向社会的人工智能研究，但纯计算机科学团队在整个领域的社会产出中占比日益增长，并日益将广泛的社会关注（如公平性、安全性、医疗保健、错误信息）融入其论文。这些发现挑战了对社会人工智能驱动因素的普遍假设，并引发了关于人工智能安全与治理的未来走向以及社会科学与人文学科在其中作用的重要思考。", "keywords": "人工智能伦理, 跨学科研究, 社会人工智能, 计算机科学, ArXiv", "comments": "这篇论文提供了实证证据，挑战了跨学科团队是社会人工智能研究唯一或主要驱动力的普遍看法。其发现揭示了技术团队日益融入社会关注的显著转变，这对人工智能伦理和安全的理解与治理方式具有深远影响。"}}
{"id": "2506.08892", "title": "Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams", "authors": ["Tauhid Tanjim", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "summary": "The human-robot interaction (HRI) field has recognized the importance of\nenabling robots to interact with teams. Human teams rely on effective\ncommunication for successful collaboration in time-sensitive environments.\nRobots can play a role in enhancing team coordination through real-time\nassistance. Despite significant progress in human-robot teaming research, there\nremains an essential gap in how robots can effectively communicate with action\nteams using multimodal interaction cues in time-sensitive environments. This\nstudy addresses this knowledge gap in an experimental in-lab study to\ninvestigate how multimodal robot communication in action teams affects workload\nand human perception of robots. We explore team collaboration in a medical\ntraining scenario where a robotic crash cart (RCC) provides verbal and\nnon-verbal cues to help users remember to perform iterative tasks and search\nfor supplies. Our findings show that verbal cues for object search tasks and\nvisual cues for task reminders reduce team workload and increase perceived ease\nof use and perceived usefulness more effectively than a robot with no feedback.\nOur work contributes to multimodal interaction research in the HRI field,\nhighlighting the need for more human-robot teaming research to understand best\npractices for integrating collaborative robots in time-sensitive environments\nsuch as in hospitals, search and rescue, and manufacturing applications.", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08892v1", "AI": {"title_translation": "帮助还是阻碍：理解机器人在行动团队中沟通的影响", "tldr": "本研究探讨了在时间敏感环境中，机器人多模态沟通（语言和视觉提示）对行动团队工作量和人类感知的影响，发现在特定任务中使用多模态提示能有效降低工作量并提高感知易用性和有用性。", "motivation": "尽管人机协作研究取得了显著进展，但在时间敏感环境中，机器人如何利用多模态交互线索与行动团队有效沟通仍存在一个重要空白。人类团队在时间敏感环境中依赖有效沟通进行协作，机器人可通过实时协助增强团队协调。", "method": "本研究通过一项实验室内研究来解决这一知识空白，调查了行动团队中多模态机器人沟通如何影响工作量和人类对机器人的感知。研究在一个医疗培训场景中进行，一个机器人急救车（RCC）提供语言和非语言线索，帮助用户记住执行迭代任务和搜索物资。", "result": "研究发现，用于物体搜索任务的语言线索和用于任务提醒的视觉线索，比没有反馈的机器人更能有效降低团队工作量，并提高感知易用性和感知有用性。", "conclusion": "本研究为人机交互领域的多模态交互研究做出了贡献，强调了需要更多人机协作研究来理解在医院、搜救和制造等时间敏感环境中整合协作机器人的最佳实践。", "translation": "人机交互（HRI）领域已认识到使机器人能够与团队互动的重要性。人类团队在时间敏感的环境中依赖有效的沟通进行成功的协作。机器人可以通过实时协助在增强团队协调方面发挥作用。尽管人机协作研究取得了显著进展，但在时间敏感环境中，机器人如何利用多模态交互线索与行动团队有效沟通方面仍然存在一个重要的空白。本研究通过一项实验室内研究解决了这一知识空白，以调查行动团队中多模态机器人沟通如何影响工作量和人类对机器人的感知。我们探索了在一个医疗培训场景中的团队协作，其中机器人急救车（RCC）提供语言和非语言线索，以帮助用户记住执行迭代任务和搜索物资。我们的发现表明，用于物体搜索任务的语言线索和用于任务提醒的视觉线索，比没有反馈的机器人更能有效降低团队工作量，并提高感知易用性和感知有用性。我们的工作为人机交互领域的多模态交互研究做出了贡献，强调了需要更多人机协作研究来理解在医院、搜救和制造等时间敏感环境中整合协作机器人的最佳实践。", "summary": "本研究旨在弥补人机交互领域中机器人如何在时间敏感环境中有效利用多模态交互线索与行动团队沟通的知识空白。通过一项医疗培训场景中的实验，研究发现机器人提供的语言（针对物体搜索）和视觉（针对任务提醒）提示，能够有效降低团队工作量，并提高人类对机器人的感知易用性和有用性。研究结果强调了在时间敏感环境中整合协作机器人的最佳实践的重要性。", "keywords": "机器人沟通, 多模态交互, 人机协作, 工作量, 时间敏感环境", "comments": "该研究通过实验验证了多模态机器人沟通在特定团队任务中的积极作用，尤其是在降低工作量和提升用户感知方面。其创新之处在于明确区分了不同模态（语言和视觉）在不同任务中的有效性，为未来设计更智能、更高效的协作机器人提供了具体指导。这对于人机协作在紧急或高压环境中的应用具有重要意义。"}}
{"id": "2506.08793", "title": "A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory", "authors": ["Zhuoran Zheng"], "summary": "This paper presents a novel partial differential equation (PDE) framework for\nsingle-image dehazing. By integrating the atmospheric scattering model with\nnonlocal regularization and dark channel prior, we propose the improved PDE: \\[\n-\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A) \\]\nwhere $D(\\nabla u) = (|\\nabla u| + \\epsilon)^{-1}$ is the edge-preserving\ndiffusion coefficient, $G(u)$ is the Gaussian convolution operator, and\n$\\lambda(t)$ is the adaptive regularization parameter based on transmission map\n$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\\Omega)$\nusing Lax-Milgram theorem, and implement an efficient fixed-point iteration\nscheme accelerated by PyTorch GPU computation. The experimental results\ndemonstrate that this method is a promising deghazing solution that can be\ngeneralized to the deep model paradigm.", "comment": "report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08793v1", "AI": {"title_translation": "基于偏微分方程的大气散射理论图像去雾方法", "tldr": "本文提出了一种新颖的基于偏微分方程（PDE）的单幅图像去雾方法，结合大气散射模型、非局部正则化和暗通道先验，并证明了弱解的存在唯一性，实验证明其效果良好且可推广。", "motivation": "本文旨在提出一种新颖的偏微分方程（PDE）框架，用于解决单幅图像去雾问题。", "method": "该方法通过将大气散射模型与非局部正则化和暗通道先验相结合，提出了一个改进的偏微分方程：$-\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A)$。其中，$D(\\nabla u)$是保边扩散系数，$G(u)$是高斯卷积算子，$\\lambda(t)$是基于透射图$t$的自适应正则化参数。研究通过Lax-Milgram定理证明了$H_0^1(\\Omega)$中弱解的存在性和唯一性，并实现了通过PyTorch GPU加速的有效不动点迭代方案。", "result": "实验结果表明，该方法是一种很有前景的去雾解决方案，并且可以推广到深度模型范式。", "conclusion": "该基于偏微分方程的去雾方法是一种有前景的解决方案，具有良好的性能和推广潜力。", "translation": "本文提出了一种新颖的基于偏微分方程（PDE）的单幅图像去雾框架。通过将大气散射模型与非局部正则化和暗通道先验相结合，我们提出了改进的PDE：$-\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A)$，其中$D(\\nabla u) = (|\\nabla u| + \\epsilon)^{-1}$是保边扩散系数，$G(u)$是高斯卷积算子，$\\lambda(t)$是基于透射图$t$的自适应正则化参数。我们利用Lax-Milgram定理证明了$H_0^1(\\Omega)$中弱解的存在性和唯一性，并实现了由PyTorch GPU计算加速的高效不动点迭代方案。实验结果表明，该方法是一种有前景的去雾解决方案，可以推广到深度模型范式。", "summary": "本文提出了一种新颖的基于偏微分方程（PDE）的单幅图像去雾框架。该方法将大气散射模型与非局部正则化和暗通道先验相结合，构建了一个改进的PDE模型，并详细阐述了其组成部分。研究不仅从理论上证明了该PDE弱解的存在性和唯一性，还实现了高效的GPU加速迭代求解方案。实验验证了该方法在图像去雾方面的有效性，并指出其具有推广到深度学习模型的潜力。", "keywords": "图像去雾, 偏微分方程, 大气散射理论, 暗通道先验, 非局部正则化", "comments": "该论文的创新点在于将偏微分方程框架与传统图像处理技术（如大气散射模型、暗通道先验和非局部正则化）相结合，为单幅图像去雾提供了一种新的数学建模方法。其理论证明了弱解的存在唯一性，增强了方法的严谨性。同时，结合GPU加速的实现，表明其具有实际应用潜力。该方法的可推广性到深度模型范式也预示着其未来与深度学习结合的可能性。"}}
{"id": "2506.08167", "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data", "authors": ["Sunny Gupta", "Nikita Jangid", "Amit Sethi"], "summary": "Federated Learning (FL) often suffers from severe performance degradation\nwhen faced with non-IID data, largely due to local classifier bias. Traditional\nremedies such as global model regularization or layer freezing either incur\nhigh computational costs or struggle to adapt to feature shifts. In this work,\nwe propose UniVarFL, a novel FL framework that emulates IID-like training\ndynamics directly at the client level, eliminating the need for global model\ndependency. UniVarFL leverages two complementary regularization strategies\nduring local training: Classifier Variance Regularization, which aligns\nclass-wise probability distributions with those expected under IID conditions,\neffectively mitigating local classifier bias; and Hyperspherical Uniformity\nRegularization, which encourages a uniform distribution of feature\nrepresentations across the hypersphere, thereby enhancing the model's ability\nto generalize under diverse data distributions. Extensive experiments on\nmultiple benchmark datasets demonstrate that UniVarFL outperforms existing\nmethods in accuracy, highlighting its potential as a highly scalable and\nefficient solution for real-world FL deployments, especially in\nresource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08167v1", "AI": {"title_translation": "UniVarFL：异构数据下的均匀性和方差正则化联邦学习", "tldr": "UniVarFL通过在客户端层面引入均匀性和方差正则化，解决了联邦学习在非IID数据下的性能下降问题，无需全局模型依赖，且表现优于现有方法。", "motivation": "联邦学习（FL）在面对非独立同分布（non-IID）数据时，由于局部分类器偏差，常遭遇严重的性能下降。传统方法计算成本高或难以适应特征偏移。", "method": "本文提出了UniVarFL框架，通过在客户端层面直接模拟IID训练动态，避免了对全局模型的依赖。它利用两种互补的正则化策略：分类器方差正则化（Classifier Variance Regularization）以对齐类别概率分布并缓解局部分类器偏差；以及超球面均匀性正则化（Hyperspherical Uniformity Regularization）以促进特征表示均匀分布，增强模型泛化能力。", "result": "在多个基准数据集上的广泛实验表明，UniVarFL在准确性方面优于现有方法。", "conclusion": "UniVarFL是一个高度可扩展且高效的解决方案，适用于真实世界的联邦学习部署，特别是在资源受限的环境中。", "translation": "联邦学习（FL）在面对非独立同分布（non-IID）数据时，通常会遭遇严重的性能下降，这很大程度上是由于局部分类器偏差造成的。传统的补救措施，如全局模型正则化或层冻结，要么会产生高昂的计算成本，要么难以适应特征偏移。在这项工作中，我们提出了UniVarFL，这是一种新颖的FL框架，它直接在客户端层面模拟类似IID的训练动态，从而无需依赖全局模型。UniVarFL在局部训练期间利用了两种互补的正则化策略：分类器方差正则化，它将类别概率分布与IID条件下的预期分布对齐，有效缓解了局部分类器偏差；以及超球面均匀性正则化，它鼓励特征表示在超球面上均匀分布，从而增强了模型在多样数据分布下的泛化能力。在多个基准数据集上进行的广泛实验表明，UniVarFL在准确性方面优于现有方法，突显了其作为真实世界FL部署中高度可扩展和高效解决方案的潜力，尤其是在资源受限的环境中。代码：https://github.com/sunnyinAI/UniVarFL", "summary": "UniVarFL是一个针对联邦学习中非IID数据挑战的新框架。它通过在客户端本地训练时引入分类器方差正则化和超球面均匀性正则化，模拟IID训练动态，有效缓解局部分类器偏差并增强模型泛化能力。实验证明UniVarFL在准确性上超越现有方法，是一个高效且可扩展的解决方案。", "keywords": "联邦学习, 非IID数据, 正则化, 分类器偏差, 异构数据", "comments": "该论文提出了一种新颖的客户端层面的正则化方法来解决联邦学习中的非IID问题，避免了对全局模型的依赖，降低了计算成本，并提高了模型的泛化能力。其创新性在于结合了两种互补的正则化策略，使得模型在异构数据下表现出更好的鲁棒性和准确性，对实际资源受限的FL部署具有重要意义。"}}
{"id": "2506.08211", "title": "Standard LSParameter Estimators Ensure Finite Convergence Time for Linear Regression Equations Under an Interval Excitation Assumption", "authors": ["Romeo Ortega", "Jose Guadalupe Romero", "Stanislav Aranovskiy", "Gang Tao"], "summary": "In this brief note we recall the little-known fact that, for linear\nregression equations (LRE) with intervally excited (IE) regressors, standard\nLeast Square (LS) parameter estimators ensure finite convergence time (FCT) of\nthe estimated parameters. The convergence time being equal to the time length\nneeded to comply with the IE assumption. As is well-known, IE is necessary and\nsufficient for the identifiability of the LRE-hence, it is the weakest\nassumption for the on-or off-line solution of the parameter estimation problem.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08211v1", "AI": {"title_translation": "标准最小二乘参数估计器在区间激励假设下确保线性回归方程的有限收敛时间", "tldr": "在区间激励条件下，标准最小二乘估计器能确保线性回归方程参数的有限时间收敛。", "motivation": "本研究旨在强调并回顾一个鲜为人知的事实：区间激励（IE）是线性回归方程（LRE）可识别性的必要和充分条件，也是参数估计问题最弱的假设。在此背景下，标准最小二乘（LS）参数估计器能够确保估计参数的有限收敛时间。", "method": "本文通过回顾和阐述一个已有的、鲜为人知的事实来达到其目的，而非提出新的方法或进行实验。", "result": "当回归量为区间激励（IE）时，标准最小二乘（LS）参数估计器可确保线性回归方程（LRE）中估计参数的有限收敛时间（FCT）。收敛时间等于满足IE假设所需的时间长度。", "conclusion": "标准最小二乘估计器在满足最弱的区间激励假设下，能确保线性回归方程参数的有限时间收敛，这对参数估计的效率至关重要。", "translation": "在这篇简短的笔记中，我们回顾了一个鲜为人知的事实：对于具有区间激励（IE）回归量的线性回归方程（LRE），标准最小二乘（LS）参数估计器确保估计参数的有限收敛时间（FCT）。收敛时间等于满足IE假设所需的时间长度。众所周知，IE是LRE可识别性的必要和充分条件——因此，它是参数估计问题在线或离线解决方案的最弱假设。", "summary": "本文重申并强调了一个重要事实：对于具有区间激励（IE）回归量的线性回归方程（LRE），标准最小二乘（LS）参数估计器能够确保估计参数的有限收敛时间（FCT）。收敛时间取决于满足IE假设所需的时间长度。作者指出，IE是LRE可识别性的最弱且必要的条件，这使得这一发现对于高效的参数估计具有重要意义。", "keywords": "线性回归, 最小二乘, 有限收敛时间, 区间激励, 参数估计", "comments": "这篇笔记非常有价值，因为它重新强调了标准最小二乘估计器在最弱假设下所具有的一个关键但可能被忽视的特性。它突出了最小二乘方法在线性回归中，尤其是在满足最弱可识别性条件时的鲁棒性和效率。这一洞察对于参数估计的理论理解和实际应用都非常重要。"}}
{"id": "2506.08829", "title": "Excluding an induced wheel minor in graphs without large induced stars", "authors": ["Mujin Choi", "Claire Hilaire", "Martin Milanič", "Sebastian Wiederrecht"], "summary": "We study a conjecture due to Dallard, Krnc, Kwon, Milani\\v{c}, Munaro,\n\\v{S}torgel, and Wiederrecht stating that for any positive integer $d$ and any\nplanar graph $H$, the class of all $K_{1,d}$-free graphs without $H$ as an\ninduced minor has bounded tree-independence number. A $k$-wheel is the graph\nobtained from a cycle of length $k$ by adding a vertex adjacent to all vertices\nof the cycle. We show that the conjecture of Dallard et al. is true when $H$ is\na $k$-wheel for any $k\\geq 3$. Our proof uses a generalization of the concept\nof brambles to tree-independence number. As a consequence of our main result,\nseveral important $\\mathsf{NP}$-hard problems such as Maximum Independent Set\nare tractable on $K_{1,d}$-free graphs without large induced wheel minors.\nMoreover, for fixed $d$ and $k$, we provide a polynomial-time algorithm that,\ngiven a $K_{1,d}$-free graph $G$ as input, finds an induced minor model of a\n$k$-wheel in $G$ if one exists.", "comment": "27 pages, 6 figures", "cate": "math.CO", "url": "http://arxiv.org/abs/2506.08829v1", "AI": {"title_translation": "在不含大型诱导星的图中排除诱导轮子图", "tldr": "本文证明了Dallard等人关于在不含$K_{1,d}$和特定诱导子图的图类中树独立数有界的猜想，特别是当诱导子图是$k$-轮时。这使得某些NP-难问题在该图类上变得可解。", "motivation": "本文旨在研究Dallard等人提出的一个猜想，该猜想指出对于任何正整数$d$和任何平面图$H$，不含$K_{1,d}$且不含$H$作为诱导子图的图类具有有界的树独立数。", "method": "证明使用了将荆棘（brambles）概念推广到树独立数的方法。", "result": "结果表明Dallard等人的猜想在$H$是任意$k \\geq 3$的$k$-轮时成立。主要结果的推论是，几个重要的NP-难问题（如最大独立集）在不含$K_{1,d}$且不含大型诱导轮子图的图上是可处理的。此外，对于固定的$d$和$k$，本文提供了一个多项式时间算法，用于在给定$K_{1,d}$-free图$G$的情况下，如果存在，找到$k$-轮的诱导子图模型。", "conclusion": "本文证实了Dallard等人的猜想在$H$为$k$-轮时成立，并因此证明了在特定图类上，某些NP-难问题（如最大独立集）变得可处理，同时还提供了一个相关的多项式时间算法。", "translation": "我们研究了Dallard、Krnc、Kwon、Milanič、Munaro、Štorgel和Wiederrecht提出的一个猜想，该猜想指出对于任何正整数$d$和任何平面图$H$，所有不含$K_{1,d}$且不含$H$作为诱导子图的图类具有有界的树独立数。$k$-轮图是由长度为$k$的环通过添加一个与环上所有顶点相邻的顶点而得到的图。我们证明了Dallard等人的猜想在$H$是任意$k \\geq 3$的$k$-轮时成立。我们的证明使用了将荆棘概念推广到树独立数的方法。作为我们主要结果的推论，几个重要的NP-难问题（如最大独立集）在不含$K_{1,d}$且不含大型诱导轮子图的图上是可处理的。此外，对于固定的$d$和$k$，我们提供了一个多项式时间算法，该算法在给定一个不含$K_{1,d}$的图$G$作为输入时，如果存在，找到$G$中一个$k$-轮的诱导子图模型。", "summary": "本文研究了Dallard等人提出的关于特定图类中树独立数有界的猜想，并成功证明了当排除的诱导子图是$k$-轮时该猜想成立。这一突破性结果表明，在不含$K_{1,d}$且不含大型诱导轮子图的图上，许多重要的NP-难问题（如最大独立集）变得可解，并且作者还提供了一个用于发现$k$-轮诱导子图模型的多项式时间算法。", "keywords": "诱导轮子图, 诱导星, 树独立数, $K_{1,d}$-free图, NP-难问题", "comments": "本文的创新之处在于其对Dallard等人提出的重要猜想的证明，特别是针对$k$-轮诱导子图的情况。这一证明不仅解决了图理论中的一个开放问题，更重要的是，它为在特定受限图类上解决NP-难问题提供了新的途径，具有重要的理论和实际意义。证明中对荆棘概念的推广也是一个值得关注的技术贡献。"}}
{"id": "2506.08531", "title": "TSRec: Enhancing Repeat-Aware Recommendation from a Temporal-Sequential Perspective", "authors": ["Shigang Quan", "Shui Liu", "Zhenzhe Zheng", "Fan Wu"], "summary": "Repeat consumption, such as repurchasing items and relistening songs, is a\ncommon scenario in daily life. To model repeat consumption, the repeat-aware\nrecommendation has been proposed to predict which item will be re-interacted\nbased on the user-item interactions. In this paper, we investigate various\ninherent characteristics to enhance the repeat-aware recommendation.\nSpecifically, we explore these characteristics from two aspects: one is from\nthe temporal aspect where we consider the time interval relationship in the\nuser behavior sequence; the other is from the sequential aspect where we\nconsider the sequential-level relationship in the user behavior sequence. And\nour intuition is that both the temporal pattern and sequential pattern will\nreflect users' intentions of repeat consumption. By utilizing these two\npatterns, a novel model called Temporal and Sequential repeat-aware\nRecommendation(TSRec for short) is proposed to enhance repeat-aware\nrecommendation. TSRec has three main components: 1) User-specific Temporal\nRepresentation Module (UTRM), which encodes and extracts user historical repeat\ntemporal information. 2)Item-specific Temporal Representation Module (ITRM),\nwhich incorporates item time interval information as side information to\nalleviate the data sparsity problem of user repeat behavior sequence. 3)\nSequential Repeat-Aware Module (SRAM), which represents the similarity between\nthe user's current and the last repeat sequences. Extensive experimental\nresults on three public benchmarks demonstrate the superiority of TSRec over\nstate-of-the-art methods. The implementation code is available\nhttps://anonymous.4open.science/r/TSRec-2306/.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08531v1", "AI": {"title_translation": "TSRec：从时序-序列角度增强重复感知推荐", "tldr": "TSRec是一个新模型，通过结合用户行为序列中的时间间隔和序列关系来增强重复感知推荐，并在基准测试中表现优异。", "motivation": "重复消费是日常生活中常见的场景，现有的重复感知推荐模型需要更有效地建模用户重复消费行为。本文旨在通过探索用户行为序列中的时间间隔关系和序列级关系这两种固有特性来增强重复感知推荐，因为作者认为这两种模式能够反映用户的重复消费意图。", "method": "本文提出了一种名为Temporal and Sequential repeat-aware Recommendation (TSRec) 的新型模型，以增强重复感知推荐。TSRec包含三个主要组件：1) 用户特定时间表示模块 (UTRM)，用于编码和提取用户历史重复时间信息。2) 项目特定时间表示模块 (ITRM)，将项目时间间隔信息作为辅助信息，以缓解用户重复行为序列的数据稀疏问题。3) 序列重复感知模块 (SRAM)，表示用户当前和上次重复序列之间的相似性。", "result": "在三个公共基准测试上的大量实验结果表明，TSRec优于最先进的方法。", "conclusion": "通过结合时间模式和序列模式，TSRec模型能够有效地增强重复感知推荐，并在实验中展现出优越的性能。", "translation": "重复消费，例如重复购买商品和重复收听歌曲，是日常生活中常见的场景。为了对重复消费进行建模，人们提出了重复感知推荐，旨在根据用户-商品交互预测哪些商品将被再次交互。在本文中，我们研究了各种固有特性来增强重复感知推荐。具体来说，我们从两个方面探索这些特性：一是时间方面，我们考虑用户行为序列中的时间间隔关系；二是序列方面，我们考虑用户行为序列中的序列级关系。我们的直觉是，时间模式和序列模式都将反映用户重复消费的意图。通过利用这两种模式，我们提出了一种名为时序和序列重复感知推荐（简称TSRec）的新模型，以增强重复感知推荐。TSRec包含三个主要组件：1）用户特定时间表示模块（UTRM），用于编码和提取用户历史重复时间信息。2）项目特定时间表示模块（ITRM），将项目时间间隔信息作为辅助信息，以缓解用户重复行为序列的数据稀疏问题。3）序列重复感知模块（SRAM），表示用户当前和上次重复序列之间的相似性。在三个公共基准测试上的大量实验结果表明，TSRec优于最先进的方法。实现代码可在https://anonymous.4open.science/r/TSRec-2306/获取。", "summary": "本文针对重复消费场景，提出了一种名为TSRec的新型重复感知推荐模型。该模型从时间间隔和序列关系两个维度捕捉用户行为模式，旨在更准确地预测用户的重复交互行为。TSRec包含用户特定时间表示模块、项目特定时间表示模块和序列重复感知模块三个核心组件，分别处理用户历史重复时间信息、缓解数据稀疏性以及表示序列相似性。实验结果表明，TSRec在多个公共数据集上超越了现有先进方法。", "keywords": "重复感知推荐, 时序推荐, 序列推荐, 推荐系统, 用户行为建模", "comments": "这篇论文通过引入时间间隔和序列关系来增强重复感知推荐，具有创新性。特别地，它将时间信息分为用户和项目两个维度进行建模，并利用序列相似性来捕捉重复模式，这有助于更全面地理解用户重复消费的意图。其提出的模块化设计也使得模型具有一定的可解释性。这项工作对于提升推荐系统的实际应用效果，尤其是在重复消费场景下，具有重要意义。"}}
{"id": "2506.08465", "title": "Forecasting Public Sentiments via Mean Field Games", "authors": ["Michael V. Klibanov", "Kevin McGoff", "Trung Truong"], "summary": "A mathematical model for forecasting of public sentiments via the Mean Field\nGames theory is proposed. A numerical method is developed. This is a version of\nthe so-called convexification method. Convergence analysis demonstrates the\nglobal convergence of this method. Convergence rate is established. Numerical\nexperiments demonstrate both an accurate performance of the convexification\ntechnique and some promising features of this approach.", "comment": "28 pages", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08465v1", "AI": {"title_translation": "公众情绪的平均场博弈预测", "tldr": "提出了一种基于平均场博弈的公众情绪预测数学模型和凸化数值方法，并证明了其收敛性和准确性。", "motivation": "预测公众情绪。", "method": "提出了一种基于平均场博弈理论的数学模型来预测公众情绪，并开发了一种称为凸化方法的数值方法。进行了收敛性分析以证明其全局收敛性和收敛速率。", "result": "凸化技术表现出准确的性能，并且该方法具有一些有前景的特性。该方法实现了全局收敛，并确定了收敛速率。", "conclusion": "该研究成功地提出了一个基于平均场博弈理论的数学模型和相应的凸化数值方法，用于公众情绪预测，并验证了其有效性和准确性。", "translation": "提出了一个通过平均场博弈理论预测公众情绪的数学模型。开发了一种数值方法，这是所谓的凸化方法的一个版本。收敛性分析证明了该方法的全局收敛性。建立了收敛速率。数值实验证明了凸化技术的准确性能以及该方法的一些有前景的特性。", "summary": "本文提出了一种基于平均场博弈理论的数学模型，用于预测公众情绪。为了实现这一预测，研究开发了一种数值方法，即凸化方法。通过收敛性分析，该方法被证明具有全局收敛性，并且其收敛速率也得到了确定。数值实验结果进一步证实了所提出的凸化技术具有准确的性能，并展现了该方法的应用前景。", "keywords": "平均场博弈, 公众情绪, 预测, 数值方法, 凸化方法", "comments": "这篇论文的创新点在于将平均场博弈理论应用于公众情绪预测，这为社会现象的建模提供了一个新的视角。所提出的凸化数值方法及其收敛性分析增强了模型的理论严谨性。数值实验结果表明该方法具有实用潜力。"}}
{"id": "2506.08524", "title": "Teaching Physical Awareness to LLMs through Sounds", "authors": ["Weiguo Wang", "Andy Nie", "Wenrui Zhou", "Yi Kai", "Chengchen Hu"], "summary": "Large Language Models (LLMs) have shown remarkable capabilities in text and\nmultimodal processing, yet they fundamentally lack physical\nawareness--understanding of real-world physical phenomena. In this work, we\npresent ACORN, a framework that teaches LLMs physical awareness through sound,\nfocusing on fundamental physical phenomena like the Doppler effect, multipath\neffect, and spatial relationships. To overcome data scarcity, ACORN introduce a\nphysics-based simulator combining real-world sound sources with controlled\nphysical channels to generate diverse training data. Using this simulator, we\nbuild AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an\naudio encoder that processes both magnitude and phase information. By\nconnecting our audio encoder to state-of-the-art LLMs, we demonstrate\nreasonable results in both simulated and real-world tasks, such as\nline-of-sight detection, Doppler effect estimation, and Direction-of-Arrival\nestimation, paving the way for enabling LLMs to understand physical world.", "comment": "ICML 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.08524v1", "AI": {"title_translation": "通过声音向大型语言模型教授物理感知", "tldr": "ACORN框架通过声音教授LLM物理感知，利用物理模拟器生成数据，并在模拟和现实任务中取得了合理的结果。", "motivation": "大型语言模型（LLMs）在文本和多模态处理方面表现出色，但根本上缺乏物理感知，即对真实世界物理现象的理解。", "method": "本研究提出了ACORN框架，通过声音向LLMs教授物理感知，重点关注多普勒效应、多径效应和空间关系等基本物理现象。为了克服数据稀缺问题，ACORN引入了一个结合真实声源和受控物理通道的物理模拟器来生成多样化的训练数据。利用该模拟器，构建了全面的音频问答数据集AQA-PHY，并提出了一个处理幅度和相位信息的音频编码器。", "result": "通过将音频编码器连接到最先进的LLMs，在模拟和现实世界任务（如视距检测、多普勒效应估计和到达方向估计）中展示了合理的结果。", "conclusion": "本研究为使LLMs理解物理世界铺平了道路。", "translation": "大型语言模型（LLMs）在文本和多模态处理方面表现出卓越的能力，但它们根本上缺乏物理感知——对真实世界物理现象的理解。在这项工作中，我们提出了ACORN，一个通过声音教授LLMs物理感知的框架，重点关注多普勒效应、多径效应和空间关系等基本物理现象。为了克服数据稀缺性，ACORN引入了一个结合真实世界声源和受控物理通道的物理模拟器，以生成多样化的训练数据。利用这个模拟器，我们构建了一个全面的音频问答数据集AQA-PHY，并提出了一个处理幅度和相位信息的音频编码器。通过将我们的音频编码器连接到最先进的LLMs，我们在模拟和现实世界任务（如视距检测、多普勒效应估计和到达方向估计）中展示了合理的结果，为使LLMs理解物理世界铺平了道路。", "summary": "本研究提出ACORN框架，旨在通过声音教授大型语言模型（LLMs）物理感知，以弥补其对真实世界物理现象理解的不足。为解决数据稀缺问题，ACORN利用物理模拟器生成多样化训练数据，并构建了AQA-PHY音频问答数据集。通过连接先进LLMs和处理幅度和相位信息的音频编码器，该框架在模拟和现实世界的物理感知任务中取得了良好效果，为LLMs理解物理世界奠定了基础。", "keywords": "物理感知, 大型语言模型, 声音, 模拟器, 音频问答", "comments": "这项工作创新性地提出了通过声音教授LLMs物理感知的方法，特别是在数据生成方面利用物理模拟器来克服数据稀缺问题，这对于LLMs更好地理解和交互物理世界具有重要意义。"}}
{"id": "2506.08918", "title": "Quantifying Mix Network Privacy Erosion with Generative Models", "authors": ["Vasilios Mavroudis", "Tariq Elahi"], "summary": "Modern mix networks improve over Tor and provide stronger privacy guarantees\nby robustly obfuscating metadata. As long as a message is routed through at\nleast one honest mixnode, the privacy of the users involved is safeguarded.\nHowever, the complexity of the mixing mechanisms makes it difficult to estimate\nthe cumulative privacy erosion occurring over time. This work uses a generative\nmodel trained on mixnet traffic to estimate the loss of privacy when users\ncommunicate persistently over a period of time. We train our large-language\nmodel from scratch on our specialized network traffic ``language'' and then use\nit to measure the sender-message unlinkability in various settings (e.g. mixing\nstrategies, security parameters, observation window). Our findings reveal\nnotable differences in privacy levels among mix strategies, even when they have\nsimilar mean latencies. In comparison, we demonstrate the limitations of\ntraditional privacy metrics, such as entropy and log-likelihood, in fully\ncapturing an adversary's potential to synthesize information from multiple\nobservations. Finally, we show that larger models exhibit greater sample\nefficiency and superior capabilities implying that further advancements in\ntransformers will consequently enhance the accuracy of model-based privacy\nestimates.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08918v1", "AI": {"title_translation": "使用生成模型量化混合网络隐私侵蚀", "tldr": "研究使用生成模型量化混合网络随时间推移的隐私侵蚀，发现传统指标的局限性并证明大型模型能更准确地估计隐私。", "motivation": "现代混合网络虽然增强了隐私，但其复杂性使得难以估计随时间累积的隐私侵蚀。", "method": "使用在混合网络流量上训练的生成模型（大型语言模型）来估计用户长期通信时的隐私损失，并测量发送者-消息的不可关联性。", "result": "发现不同混合策略的隐私水平存在显著差异，即使它们的平均延迟相似；传统隐私指标（如熵和对数似然）在捕捉对手从多重观察中综合信息的能力方面存在局限性；更大的模型表现出更高的样本效率和更优越的能力。", "conclusion": "进一步的Transformer模型进步将提高基于模型的隐私估计的准确性。", "translation": "现代混合网络在Tor的基础上进行了改进，通过可靠地混淆元数据提供了更强的隐私保证。只要消息通过至少一个诚实的混合节点路由，参与用户的隐私就能得到保障。然而，混合机制的复杂性使得难以估计随时间累积的隐私侵蚀。这项工作使用在混合网络流量上训练的生成模型来估计用户在一段时间内持续通信时的隐私损失。我们从头开始在专门的网络流量“语言”上训练我们的大型语言模型，然后用它来衡量各种设置（例如混合策略、安全参数、观察窗口）中的发送者-消息不可关联性。我们的研究结果揭示了不同混合策略之间的隐私水平存在显著差异，即使它们具有相似的平均延迟。相比之下，我们展示了传统隐私指标（如熵和对数似然）在完全捕捉对手从多重观察中综合信息的能力方面的局限性。最后，我们表明更大的模型表现出更高的样本效率和更优越的能力，这意味着Transformer的进一步发展将因此提高基于模型的隐私估计的准确性。", "summary": "本文提出使用在混合网络流量上训练的生成模型来量化现代混合网络中随时间累积的隐私侵蚀。研究通过测量发送者-消息不可关联性，揭示了不同混合策略的隐私差异，并指出传统隐私指标的不足。结果表明，更大的生成模型在隐私估计方面表现出更强的能力和更高的效率，预示着未来模型进步将提升隐私评估的准确性。", "keywords": "混合网络, 隐私侵蚀, 生成模型, 大型语言模型, 不可关联性", "comments": "本文的创新之处在于首次将大型生成模型应用于量化混合网络的隐私侵蚀，克服了传统指标的局限性。这为评估和设计更安全的匿名通信系统提供了新的视角和更准确的工具。其重要性在于揭示了长期通信中隐私泄露的累积效应，并强调了先进机器学习模型在隐私分析中的潜力。"}}
{"id": "2506.08790", "title": "Do Generative AI Tools Ensure Green Code? An Investigative Study", "authors": ["Samarth Sikand", "Rohit Mehra", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "Software sustainability is emerging as a primary concern, aiming to optimize\nresource utilization, minimize environmental impact, and promote a greener,\nmore resilient digital ecosystem. The sustainability or \"greenness\" of software\nis typically determined by the adoption of sustainable coding practices. With a\nmaturing ecosystem around generative AI, many software developers now rely on\nthese tools to generate code using natural language prompts. Despite their\npotential advantages, there is a significant lack of studies on the\nsustainability aspects of AI-generated code. Specifically, how environmentally\nfriendly is the AI-generated code based upon its adoption of sustainable coding\npractices? In this paper, we present the results of an early investigation into\nthe sustainability aspects of AI-generated code across three popular generative\nAI tools - ChatGPT, BARD, and Copilot. The results highlight the default\nnon-green behavior of tools for generating code, across multiple rules and\nscenarios. It underscores the need for further in-depth investigations and\neffective remediation strategies.", "comment": "4 pages. To be published in the proceedings of 2nd International\n  Workshop on Responsible AI Engineering (RAIE '24), co-located with ICSE '24,\n  Lisbon, Portugal", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08790v1", "AI": {"title_translation": "生成式AI工具能确保绿色代码吗？一项调查研究", "tldr": "一项早期调查显示，ChatGPT、BARD和Copilot等生成式AI工具默认生成的代码行为是“非绿色”的，凸显了对AI生成代码可持续性进行深入研究和制定补救策略的必要性。", "motivation": "软件可持续性日益受到关注，旨在优化资源利用和减少环境影响。尽管生成式AI工具在生成代码方面被广泛使用，但目前关于AI生成代码可持续性方面的研究显著缺乏，特别是其在采纳可持续编码实践方面的表现。", "method": "本研究对ChatGPT、BARD和Copilot这三种流行的生成式AI工具生成的代码的可持续性方面进行了早期调查。", "result": "调查结果表明，在多种规则和场景下，这些生成式AI工具默认生成的代码行为是“非绿色”的，未能充分采纳可持续编码实践。", "conclusion": "本研究强调需要对AI生成的代码的可持续性进行进一步深入调查，并制定有效的补救策略，以确保代码的环保性。", "translation": "软件可持续性正成为一个主要关注点，旨在优化资源利用、最小化环境影响，并促进一个更绿色、更具弹性的数字生态系统。软件的可持续性或“绿色程度”通常由采纳可持续编码实践来决定。随着生成式AI生态系统的日渐成熟，许多软件开发人员现在依赖这些工具通过自然语言提示来生成代码。尽管它们具有潜在优势，但关于AI生成代码可持续性方面的研究却显著不足。具体来说，基于其对可持续编码实践的采纳程度，AI生成的代码对环境有多友好？在本文中，我们展示了对ChatGPT、BARD和Copilot这三种流行的生成式AI工具生成的代码可持续性方面进行早期调查的结果。结果突出表明，在多种规则和场景下，这些工具生成代码的默认非绿色行为。它强调了需要进一步深入调查和有效的补救策略。", "summary": "本文对ChatGPT、BARD和Copilot等生成式AI工具生成的代码在可持续性方面的表现进行了早期调查。研究发现，这些工具默认生成的代码不符合可持续编码实践，即“非绿色”。这凸显了在AI辅助编程日益普及的背景下，对AI生成代码的可持续性进行深入研究和开发有效补救措施的紧迫性。", "keywords": "生成式AI, 绿色代码, 软件可持续性, 可持续编码实践, AI生成代码", "comments": "这项研究填补了生成式AI工具在软件可持续性方面的研究空白，首次揭示了这些工具在默认情况下可能未能生成“绿色”代码。其重要性在于，随着AI在软件开发中的应用越来越广泛，确保AI生成代码的环境友好性变得至关重要。研究结果为未来的AI工具设计和可持续编码实践的推广提供了宝贵的见解，并提醒开发者在使用AI工具时需要关注代码的“绿色”程度。"}}
{"id": "2506.08639", "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators", "authors": ["Amir Hossein Barjini", "Seyed Adel Alizadeh Kolagar", "Sadeq Yaqubi", "Jouni Mattila"], "summary": "This article presents a motion planning and control framework for flexible\nrobotic manipulators, integrating deep reinforcement learning (DRL) with a\nnonlinear partial differential equation (PDE) controller. Unlike conventional\napproaches that focus solely on control, we demonstrate that the desired\ntrajectory significantly influences endpoint vibrations. To address this, a DRL\nmotion planner, trained using the soft actor-critic (SAC) algorithm, generates\noptimized trajectories that inherently minimize vibrations. The PDE nonlinear\ncontroller then computes the required torques to track the planned trajectory\nwhile ensuring closed-loop stability using Lyapunov analysis. The proposed\nmethodology is validated through both simulations and real-world experiments,\ndemonstrating superior vibration suppression and tracking accuracy compared to\ntraditional methods. The results underscore the potential of combining\nlearning-based motion planning with model-based control for enhancing the\nprecision and stability of flexible robotic manipulators.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08639v1", "AI": {"title_translation": "基于深度强化学习的柔性机械臂运动规划与偏微分方程控制", "tldr": "本文提出了一种结合深度强化学习和非线性偏微分方程控制的柔性机械臂运动规划与控制框架，旨在通过优化轨迹来抑制振动并提高跟踪精度。", "motivation": "传统方法通常只关注控制，而忽略了期望轨迹对末端振动的影响。为了解决柔性机械臂的振动抑制和跟踪精度问题，本研究旨在开发一种新的运动规划和控制框架。", "method": "该方法结合了深度强化学习（DRL）和非线性偏微分方程（PDE）控制器。DRL运动规划器（使用软actor-critic (SAC) 算法训练）生成优化轨迹以最小化振动。PDE非线性控制器计算所需转矩以跟踪规划轨迹，并通过Lyapunov分析确保闭环稳定性。", "result": "通过仿真和真实世界实验验证了所提出的方法，结果表明与传统方法相比，该方法在振动抑制和跟踪精度方面表现出卓越的性能。", "conclusion": "结合基于学习的运动规划和基于模型的控制，能够显著提高柔性机械臂的精度和稳定性。", "translation": "本文提出了一种用于柔性机械臂的运动规划与控制框架，该框架将深度强化学习（DRL）与非线性偏微分方程（PDE）控制器相结合。与传统方法仅关注控制不同，我们证明了期望轨迹显著影响末端振动。为了解决这个问题，一个使用软actor-critic (SAC) 算法训练的DRL运动规划器生成了固有地最小化振动的优化轨迹。然后，PDE非线性控制器计算所需的转矩以跟踪规划的轨迹，同时使用Lyapunov分析确保闭环稳定性。所提出的方法通过仿真和真实世界实验进行了验证，与传统方法相比，展示出卓越的振动抑制和跟踪精度。结果强调了结合基于学习的运动规划与基于模型的控制在增强柔性机械臂精度和稳定性方面的潜力。", "summary": "本文提出了一种用于柔性机械臂的运动规划与控制框架，该框架将深度强化学习（DRL）与非线性偏微分方程（PDE）控制器相结合。不同于以往仅关注控制的方法，本研究强调期望轨迹对末端振动的影响，并利用DRL运动规划器生成能最大程度减少振动的优化轨迹。PDE控制器负责轨迹跟踪并确保系统稳定性。仿真和实验结果表明，该方法在振动抑制和跟踪精度上优于传统方法，证实了学习型运动规划与模型控制结合的潜力。", "keywords": "深度强化学习, 运动规划, 偏微分方程控制, 柔性机械臂, 振动抑制", "comments": "该论文的创新之处在于将深度强化学习应用于柔性机械臂的运动规划，而非仅仅是控制，从而从源头优化轨迹以抑制振动。这种学习与模型相结合的方法为柔性机械臂的精确控制和稳定性提供了新的思路，具有重要的研究价值和应用潜力。"}}
{"id": "2506.08227", "title": "A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks", "authors": ["Vishaal Udandarao", "Mehdi Cherti", "Shyamgopal Karthik", "Jenia Jitsev", "Samuel Albanie", "Matthias Bethge"], "summary": "We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for\nmeasuring compositional understanding capabilities of vision-language models\n(VLMs). We scrutinize design choices in their construction, including data\nsource (e.g. MS-COCO) and curation procedures (e.g. constructing negative\nimages/captions), uncovering several inherent biases across most benchmarks. We\nfind that blind heuristics (e.g. token-length, log-likelihood under a language\nmodel) perform on par with CLIP models, indicating that these benchmarks do not\neffectively measure compositional understanding. We demonstrate that the\nunderlying factor is a distribution asymmetry between positive and negative\nimages/captions, induced by the benchmark construction procedures. To mitigate\nthese issues, we provide a few key recommendations for constructing more robust\nvision-language compositional understanding benchmarks, that would be less\nprone to such simple attacks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08227v1", "AI": {"title_translation": "好的可丽饼不只需要糖：探究组合式视觉-语言基准中的偏见", "tldr": "该研究发现，用于衡量视觉-语言模型组合理解能力的现有基准存在固有偏见，导致简单启发式方法表现与CLIP模型相当，并提出了构建更鲁棒基准的建议。", "motivation": "研究旨在调查常用的视觉-语言模型（VLMs）组合理解能力基准（如SugarCREPE, VALSE），并揭示其构建过程中存在的固有偏见，因为这些偏见可能导致基准无法有效衡量模型的真实能力。", "method": "研究调查了17个常用的组合式视觉-语言基准，详细审查了它们的设计选择，包括数据来源和负面图像/字幕的构建过程。通过比较盲启发式方法（如token长度、语言模型下的对数似然）与CLIP模型的表现，来评估基准的有效性。", "result": "研究发现大多数基准中存在固有偏见。盲启发式方法与CLIP模型表现相当，这表明这些基准未能有效衡量组合理解能力。根本原因是基准构建过程中导致的正面和负面图像/字幕之间的分布不对称。", "conclusion": "现有视觉-语言组合理解基准存在严重偏见，无法准确评估模型的组合理解能力。为解决这些问题，研究提供了构建更鲁棒的视觉-语言组合理解基准的关键建议，以减少此类简单攻击的风险。", "translation": "我们调查了17个常用的视觉-语言模型（VLMs）组合理解能力基准（例如SugarCREPE、VALSE）。我们仔细审查了它们构建过程中的设计选择，包括数据来源（例如MS-COCO）和策展程序（例如构建负面图像/字幕），揭示了大多数基准中存在的固有偏见。我们发现，简单的启发式方法（例如token长度、语言模型下的对数似然）表现与CLIP模型相当，这表明这些基准未能有效衡量组合理解能力。我们证明，根本因素是基准构建程序导致的正面和负面图像/字幕之间的分布不对称。为了缓解这些问题，我们为构建更鲁棒、不易受此类简单攻击的视觉-语言组合理解基准提供了一些关键建议。", "summary": "本研究深入分析了17个常用的视觉-语言模型组合理解能力基准，揭示了其在数据来源和负面样本构建过程中存在的固有偏见。研究发现，简单的启发式方法在这些基准上的表现与先进的CLIP模型不相上下，这表明这些基准未能有效衡量模型的组合理解能力。核心问题在于正负样本之间存在分布不对称。为解决此问题，论文提出了构建更具鲁棒性的视觉-语言组合理解基准的关键建议。", "keywords": "视觉-语言模型, 组合理解, 基准偏见, 分布不对称, 模型评估", "comments": "这篇论文揭示了当前视觉-语言组合理解基准的严重缺陷，其重要性在于指出了现有评估方法的不可靠性。通过发现简单启发式方法能与复杂模型表现相当，论文强调了基准设计中分布偏见的关键影响。这对于视觉-语言领域的研究者而言是一个重要的警示，并为未来构建更公平、更有效的评估工具提供了宝贵的指导。"}}
{"id": "2506.08422", "title": "Transforming Expert Knowledge into Scalable Ontology via Large Language Models", "authors": ["Ikkei Itoku", "David Theil", "Evelyn Eichelsdoerfer Uehara", "Sreyoshi Bhaduri", "Junnosuke Kuroda", "Toshi Yumoto", "Alex Gil", "Natalie Perez", "Rajesh Cherukuri", "Naumaan Nayyar"], "summary": "Having a unified, coherent taxonomy is essential for effective knowledge\nrepresentation in domain-specific applications as diverse terminologies need to\nbe mapped to underlying concepts. Traditional manual approaches to taxonomy\nalignment rely on expert review of concept pairs, but this becomes\nprohibitively expensive and time-consuming at scale, while subjective\ninterpretations often lead to expert disagreements. Existing automated methods\nfor taxonomy alignment have shown promise but face limitations in handling\nnuanced semantic relationships and maintaining consistency across different\ndomains. These approaches often struggle with context-dependent concept\nmappings and lack transparent reasoning processes. We propose a novel framework\nthat combines large language models (LLMs) with expert calibration and\niterative prompt optimization to automate taxonomy alignment. Our method\nintegrates expert-labeled examples, multi-stage prompt engineering, and human\nvalidation to guide LLMs in generating both taxonomy linkages and supporting\nrationales. In evaluating our framework on a domain-specific mapping task of\nconcept essentiality, we achieved an F1-score of 0.97, substantially exceeding\nthe human benchmark of 0.68. These results demonstrate the effectiveness of our\napproach in scaling taxonomy alignment while maintaining high-quality mappings\nand preserving expert oversight for ambiguous cases.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08422v1", "AI": {"title_translation": "通过大型语言模型将专家知识转化为可扩展本体", "tldr": "该研究提出了一种结合大型语言模型（LLMs）、专家校准和迭代提示优化的新型框架，用于自动化本体对齐，在概念重要性映射任务中，F1分数达到0.97，显著超越了0.68的人工基准。", "motivation": "传统的手动本体对齐方法成本高昂且耗时，并且主观解释可能导致专家分歧。现有的自动化方法在处理细微的语义关系、保持不同领域的一致性以及处理上下文依赖的概念映射方面存在局限性，并且缺乏透明的推理过程。", "method": "提出了一种结合大型语言模型（LLMs）与专家校准和迭代提示优化相结合的新型框架，以自动化本体对齐。该方法整合了专家标注的示例、多阶段提示工程和人工验证，以指导LLMs生成本体链接和支持性理由。", "result": "在概念重要性的领域特定映射任务中，F1分数达到0.97，显著超越了0.68的人工基准。", "conclusion": "该方法在扩展本体对齐的同时，保持了高质量的映射，并在模糊情况下保留了专家监督，证明了其有效性。", "translation": "拥有统一、连贯的分类体系对于领域特定应用中有效的知识表示至关重要，因为不同的术语需要映射到潜在的概念。传统的本体对齐手动方法依赖于专家对概念对的审查，但这在规模化时变得极其昂贵和耗时，而主观解释常常导致专家分歧。现有的自动化本体对齐方法虽然显示出前景，但在处理细微的语义关系和保持不同领域的一致性方面面临局限性。这些方法通常难以处理上下文相关的概念映射，并且缺乏透明的推理过程。我们提出了一种新颖的框架，该框架结合了大型语言模型（LLMs）与专家校准和迭代提示优化，以自动化本体对齐。我们的方法整合了专家标注的示例、多阶段提示工程和人工验证，以指导LLMs生成本体链接和支持性理由。在评估我们的框架在概念重要性的领域特定映射任务时，我们取得了0.97的F1分数，大大超过了0.68的人工基准。这些结果表明了我们的方法在扩展本体对齐方面的有效性，同时保持了高质量的映射，并在模糊情况下保留了专家监督。", "summary": "本文提出了一种新颖的框架，利用大型语言模型（LLMs）结合专家校准和迭代提示优化，以自动化本体对齐过程。该方法通过整合专家标注示例、多阶段提示工程和人工验证来指导LLMs生成本体链接和支持性理由。在概念重要性映射任务中，该框架取得了0.97的F1分数，远超0.68的人工基准，证明了其在扩展本体对齐方面的有效性，同时保持了高质量的映射和专家监督。", "keywords": "本体对齐, 大型语言模型, 知识表示, 专家知识, 提示工程", "comments": "该论文创新性地将大型语言模型应用于知识表示中的本体对齐问题，解决了传统方法的可扩展性和一致性挑战。通过结合专家知识（校准、验证）与LLM的能力（迭代提示优化），该方法在自动化知识工程方面取得了显著进展，其性能远超人工基准，具有重要的实际应用价值。"}}
{"id": "2506.08113", "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting", "authors": ["Timothée Hornek Amir Sartipi", "Igor Tchappi", "Gilbert Fridgen"], "summary": "Accurate electricity price forecasting (EPF) is crucial for effective\ndecision-making in power trading on the spot market. While recent advances in\ngenerative artificial intelligence (GenAI) and pre-trained large language\nmodels (LLMs) have inspired the development of numerous time series foundation\nmodels (TSFMs) for time series forecasting, their effectiveness in EPF remains\nuncertain. To address this gap, we benchmark several state-of-the-art\npretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and\nTimeGPT--against established statistical and machine learning (ML) methods for\nEPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,\nFrance, the Netherlands, Austria, and Belgium, we generate daily forecasts with\na one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the\nTSFMs, performing on par with traditional models. However, the biseasonal MSTL\nmodel, which captures daily and weekly seasonality, stands out for its\nconsistent performance across countries and evaluation metrics, with no TSFM\nstatistically outperforming it.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08113v1", "AI": {"title_translation": "预训练时间序列模型在电价预测中的基准测试", "tldr": "论文对预训练时间序列模型在电价预测中的表现进行了基准测试，发现传统模型，特别是双季节MSTL模型，性能优于或与最新的时间序列基础模型相当。", "motivation": "准确的电价预测对于现货市场电力交易中的有效决策至关重要。尽管生成式人工智能和预训练大型语言模型启发了许多时间序列基础模型的开发，但它们在电价预测中的有效性仍不确定。", "method": "研究使用德国、法国、荷兰、奥地利和比利时2024年日前拍卖电价数据，对Chronos-Bolt、Chronos-T5、TimesFM、Moirai、Time-MoE和TimeGPT等最先进的预训练模型与已有的统计和机器学习方法进行基准测试，生成一日预测。", "result": "Chronos-Bolt和Time-MoE是时间序列基础模型中最强的，表现与传统模型相当。然而，捕获每日和每周季节性的双季节MSTL模型在不同国家和评估指标上表现一致，没有时间序列基础模型在统计上超越它。", "conclusion": "尽管最新的时间序列基础模型（TSFMs）在电价预测中表现出一定的竞争力，但它们并未统计学上超越传统的统计和机器学习方法，特别是双季节MSTL模型。这表明在电价预测领域，传统方法仍然具有强大的竞争力。", "translation": "准确的电价预测（EPF）对于现货市场电力交易中的有效决策至关重要。尽管生成式人工智能（GenAI）和预训练大型语言模型（LLMs）的最新进展启发了许多时间序列基础模型（TSFMs）的开发，用于时间序列预测，但它们在电价预测中的有效性仍不确定。为了弥补这一空白，我们对几种最先进的预训练模型——Chronos-Bolt、Chronos-T5、TimesFM、Moirai、Time-MoE和TimeGPT——与已有的统计和机器学习（ML）方法进行了电价预测的基准测试。我们使用德国、法国、荷兰、奥地利和比利时2024年日前拍卖（DAA）电价数据，生成一日预测。Chronos-Bolt和Time-MoE在时间序列基础模型中表现最强，与传统模型不相上下。然而，捕获每日和每周季节性的双季节MSTL模型因其在不同国家和评估指标上的一致表现而脱颖而出，没有时间序列基础模型在统计上超越它。", "summary": "该研究对多种预训练时间序列基础模型（TSFMs）在欧洲电价预测中的表现进行了基准测试，并将其与传统统计和机器学习方法进行比较。结果表明，虽然一些TSFMs（如Chronos-Bolt和Time-MoE）表现良好，但未能统计学上超越捕获双季节性的传统MSTL模型，凸显了传统方法在电价预测领域的持续有效性。", "keywords": "电价预测, 时间序列基础模型, 基准测试, 预训练模型, MSTL", "comments": "该研究具有重要意义，因为它首次系统地评估了新兴的时间序列基础模型在关键的电价预测任务中的表现。其创新之处在于将这些新模型与成熟的传统方法进行直接比较，揭示了尽管有先进模型出现，传统方法在某些领域仍具强大竞争力。这为未来的研究指明了方向，即在特定应用场景下，简单且具有强解释性的模型可能依然是最佳选择，而不是盲目追求最新的复杂模型。"}}
{"id": "2506.08911", "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU", "authors": ["Petar Jakuš", "Hrvoje Džapo"], "summary": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms.", "comment": "4 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.08911v1", "AI": {"title_translation": "在集成NPU的MCUX947微控制器上实现关键词识别", "tldr": "本文在集成NPU的NXP MCXN947微控制器上实现了关键词识别（KWS）系统，通过量化感知训练优化，实现了59倍的推理速度提升和97.06%的准确率，模型大小仅30.58 KB，证明了在资源受限设备上实现高效、低功耗语音交互的可行性。", "motivation": "在资源受限的设备上实现实时语音交互。", "method": "在NXP MCXN947微控制器（集成NPU）上实现关键词识别（KWS）系统。该系统结合了MFCC特征提取和CNN分类器，并使用量化感知训练进行优化，以在最小精度损失的情况下减小模型大小。", "result": "利用NPU相比仅使用CPU的执行方式，推理时间加速了59倍；系统实现了97.06%的准确率，模型大小为30.58 KB。", "conclusion": "该研究证明了在嵌入式平台上实现高效、低功耗语音接口的可行性。", "translation": "本文介绍了一种在集成神经网络处理单元（NPU）的NXP MCXN947微控制器上实现的关键词识别（KWS）系统，该系统能够在资源受限的设备上实现实时语音交互。该系统结合了MFCC特征提取和CNN分类器，并使用量化感知训练进行优化，以在最小精度损失的情况下减小模型大小。实验结果表明，与仅使用CPU执行相比，利用NPU的推理时间加速了59倍，实现了97.06%的准确率，模型大小为30.58 KB，这证明了在嵌入式平台上实现高效、低功耗语音接口的可行性。", "summary": "本文在集成NPU的NXP MCXN947微控制器上构建了一个关键词识别（KWS）系统，旨在为资源受限设备提供实时语音交互能力。该系统采用MFCC特征提取与CNN分类器相结合的方法，并通过量化感知训练进行优化，以减小模型尺寸并保持高精度。实验结果显示，与纯CPU执行相比，利用NPU可使推理速度提升59倍，同时实现97.06%的准确率和30.58 KB的模型大小，有力地证明了在嵌入式平台上实现高效、低功耗语音接口的可行性。", "keywords": "关键词识别, NPU, 微控制器, 嵌入式系统, 量化感知训练", "comments": "本文的创新点在于成功地将关键词识别系统部署到集成了NPU的微控制器上，实现了显著的推理速度提升和极小的模型尺寸，这对于资源受限的嵌入式设备至关重要。这项工作为实时、低功耗的语音交互提供了一个实用的解决方案，具有重要的工程应用价值。"}}
{"id": "2506.08809", "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08809v1", "AI": {"title_translation": "HiSin: 通过分辨率引导的渐进式推理实现高效高分辨率正弦图修复", "tldr": "HiSin是一种新的扩散模型框架，通过分辨率引导的渐进式推理，有效解决了高分辨率正弦图修复中内存和计算量大的问题，同时保持了准确性。", "motivation": "高分辨率正弦图修复对CT重建至关重要，因为缺失高频投影会导致伪影和诊断错误。现有扩散模型在高分辨率输入上的应用受限于过多的内存和计算需求。", "method": "提出HiSin框架，通过分辨率引导的渐进式推理实现高效正弦图修复。它在低分辨率下逐步提取全局结构，并将高分辨率推理推迟到小块进行，从而实现内存高效修复。此外，它还结合了频率感知块跳过和结构自适应步长分配，以减少冗余计算。", "result": "HiSin将峰值内存使用量减少了高达31.25%，推理时间减少了高达18.15%，并在不同数据集、分辨率和掩模条件下保持了修复精度。", "conclusion": "HiSin通过其创新的渐进式推理和计算优化策略，成功地解决了高分辨率正弦图修复中扩散模型面临的内存和计算效率挑战，同时保持了高准确性。", "translation": "高分辨率正弦图修复对于计算机断层扫描重建至关重要，因为缺失的高频投影可能导致可见伪影和诊断错误。扩散模型由于其鲁棒性和细节保留能力，非常适合此任务，但其在高分辨率输入上的应用受限于过多的内存和计算需求。为了解决这一限制，我们提出了HiSin，一种新颖的基于扩散的框架，通过分辨率引导的渐进式推理实现高效正弦图修复。它在低分辨率下逐步提取全局结构，并将高分辨率推理推迟到小块进行，从而实现内存高效修复。它进一步结合了频率感知块跳过和结构自适应步长分配，以减少冗余计算。实验结果表明，HiSin将峰值内存使用量减少了高达31.25%，推理时间减少了高达18.15%，并在不同数据集、分辨率和掩模条件下保持了修复精度。", "summary": "本文提出HiSin，一个基于扩散模型的新框架，用于高效高分辨率正弦图修复。针对现有扩散模型在高分辨率输入上内存和计算量大的问题，HiSin采用分辨率引导的渐进式推理，在低分辨率下提取全局结构，并在高分辨率下对小块进行推理，从而实现内存高效。此外，它还通过频率感知块跳过和结构自适应步长分配减少冗余计算。实验证明，HiSin显著降低了内存和推理时间，同时保持了修复准确性。", "keywords": "正弦图修复, 扩散模型, 高分辨率, 渐进式推理, 计算断层扫描", "comments": "HiSin的创新之处在于其分辨率引导的渐进式推理策略，有效地解决了扩散模型在高分辨率图像处理中常见的内存和计算瓶颈。通过结合多分辨率处理和计算优化技术，该方法在保持高精度修复的同时，显著提升了效率，对于CT重建等实际应用具有重要意义。"}}
{"id": "2506.08169", "title": "Federated Learning on Stochastic Neural Networks", "authors": ["Jingqiao Tang", "Ryan Bausback", "Feng Bao", "Richard Archibald"], "summary": "Federated learning is a machine learning paradigm that leverages edge\ncomputing on client devices to optimize models while maintaining user privacy\nby ensuring that local data remains on the device. However, since all data is\ncollected by clients, federated learning is susceptible to latent noise in\nlocal datasets. Factors such as limited measurement capabilities or human\nerrors may introduce inaccuracies in client data. To address this challenge, we\npropose the use of a stochastic neural network as the local model within the\nfederated learning framework. Stochastic neural networks not only facilitate\nthe estimation of the true underlying states of the data but also enable the\nquantification of latent noise. We refer to our federated learning approach,\nwhich incorporates stochastic neural networks as local models, as Federated\nstochastic neural networks. We will present numerical experiments demonstrating\nthe performance and effectiveness of our method, particularly in handling\nnon-independent and identically distributed data.", "comment": "25 pages, 19 figures, Submitted to Journal of Machine Learning for\n  Modeling and Computing", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08169v1", "AI": {"title_translation": "联邦学习在随机神经网络上的应用", "tldr": "提出一种联邦随机神经网络（FSNN）方法，通过在联邦学习中使用随机神经网络作为本地模型来解决本地数据中的潜在噪声问题，并能处理非独立同分布数据。", "motivation": "联邦学习因本地数据集中的潜在噪声（如测量限制或人为错误）而易受影响，这会引入数据不准确性。", "method": "提出在联邦学习框架中将随机神经网络（SNN）用作本地模型，并将此方法称为联邦随机神经网络（FSNN）。SNN能够估计数据的真实潜在状态并量化潜在噪声。", "result": "将通过数值实验展示该方法的性能和有效性，尤其是在处理非独立同分布数据方面的表现。", "conclusion": "Not mentioned in abstract", "translation": "联邦学习是一种机器学习范式，它利用客户端设备的边缘计算来优化模型，同时通过确保本地数据保留在设备上来维护用户隐私。然而，由于所有数据都由客户端收集，联邦学习容易受到本地数据集中潜在噪声的影响。诸如有限的测量能力或人为错误等因素可能会导致客户端数据不准确。为了解决这个挑战，我们建议在联邦学习框架内使用随机神经网络作为本地模型。随机神经网络不仅有助于估计数据的真实潜在状态，还能量化潜在噪声。我们将这种将随机神经网络作为本地模型纳入联邦学习的方法称为联邦随机神经网络。我们将展示数值实验，证明我们方法的性能和有效性，特别是在处理非独立同分布数据方面。", "summary": "这篇论文提出了一种名为联邦随机神经网络（FSNN）的新型联邦学习方法，旨在解决本地数据中普遍存在的潜在噪声问题。通过在联邦学习框架中引入随机神经网络作为本地模型，FSNN能够估计数据的真实状态并量化噪声，从而提高模型在数据不准确情况下的鲁棒性，尤其是在处理非独立同分布数据时。", "keywords": "联邦学习, 随机神经网络, 潜在噪声, 数据隐私, 非独立同分布数据", "comments": "这项工作通过引入随机神经网络来解决联邦学习中本地数据噪声的问题，具有创新性。它旨在提高联邦学习在实际应用中数据质量不佳时的鲁棒性和准确性。"}}
{"id": "2506.08272", "title": "Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids", "authors": ["Tarushri N. S."], "summary": "Universal Differential Equations (UDEs), which blend neural networks with\nphysical differential equations, have emerged as a powerful framework for\nscientific machine learning (SciML), enabling data-efficient, interpretable,\nand physically consistent modeling. In the context of smart grid systems,\nmodeling node-wise battery dynamics remains a challenge due to the\nstochasticity of solar input and variability in household load profiles.\nTraditional approaches often struggle with generalization and fail to capture\nunmodeled residual dynamics. This work proposes a UDE-based approach to learn\nnode-specific battery evolution by embedding a neural residual into a\nphysically inspired battery ODE. Synthetic yet realistic solar generation and\nload demand data are used to simulate battery dynamics over time. The neural\ncomponent learns to model unobserved or stochastic corrections arising from\nheterogeneity in node demand and environmental conditions. Comprehensive\nexperiments reveal that the trained UDE aligns closely with ground truth\nbattery trajectories, exhibits smooth convergence behavior, and maintains\nstability in long-term forecasts. These findings affirm the viability of\nUDE-based SciML approaches for battery modeling in decentralized energy\nnetworks and suggest broader implications for real-time control and\noptimization in renewable-integrated smart grids.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08272v1", "AI": {"title_translation": "智能电网中节点级电池动态的科学机器学习的通用微分方程", "tldr": "本文提出了一种基于通用微分方程（UDE）的方法，用于在智能电网中对节点级电池动态进行建模，以解决传统方法的挑战，并展示了其准确性和稳定性。", "motivation": "智能电网中节点级电池动态建模面临挑战，因为太阳能输入随机性、家庭负荷曲线可变性，以及传统方法泛化能力差、无法捕捉未建模残余动态。", "method": "提出了一种基于UDE的方法，通过将神经网络残差嵌入到物理启发的电池常微分方程（ODE）中，学习节点特定的电池演化。使用合成但真实的太阳能发电和负荷需求数据来模拟电池动态。神经网络组件学习模拟由节点需求和环境条件异质性引起的未观测或随机校正。", "result": "训练后的UDE与真实电池轨迹紧密对齐，表现出平滑的收敛行为，并在长期预测中保持稳定性。", "conclusion": "这些发现证实了基于UDE的科学机器学习方法在去中心化能源网络中电池建模的可行性，并暗示了其在可再生能源集成智能电网中实时控制和优化方面的更广泛影响。", "translation": "通用微分方程（UDEs）将神经网络与物理微分方程相结合，已成为科学机器学习（SciML）的强大框架，实现了数据高效、可解释且物理一致的建模。在智能电网系统中，由于太阳能输入的随机性和家庭负荷曲线的可变性，节点级电池动态建模仍然是一个挑战。传统方法往往难以泛化，并且无法捕捉未建模的残余动态。这项工作提出了一种基于UDE的方法，通过将神经网络残差嵌入到物理启发的电池常微分方程（ODE）中，学习节点特定的电池演化。使用合成但真实的太阳能发电和负荷需求数据来模拟电池随时间的动态。神经网络组件学习模拟由节点需求和环境条件异质性引起的未观测或随机校正。全面的实验表明，训练后的UDE与真实电池轨迹紧密对齐，表现出平滑的收敛行为，并在长期预测中保持稳定性。这些发现证实了基于UDE的科学机器学习方法在去中心化能源网络中电池建模的可行性，并暗示了其在可再生能源集成智能电网中实时控制和优化方面的更广泛影响。", "summary": "本文提出了一种利用通用微分方程（UDEs）对智能电网中节点级电池动态进行建模的新方法。该方法将神经网络与物理电池常微分方程相结合，以捕捉未观测的残余动态和异质性。通过使用合成数据进行实验，结果表明该UDE模型能够准确跟踪电池轨迹，表现出良好的收敛性和长期稳定性，验证了UDEs在去中心化能源网络中电池建模的有效性，并为智能电网的实时控制和优化提供了新的途径。", "keywords": "通用微分方程, 科学机器学习, 电池动态, 智能电网, 节点级建模", "comments": "本文创新性地将通用微分方程应用于智能电网中的节点级电池动态建模，有效解决了传统方法在处理随机性和异质性方面的局限。其结合物理模型和神经网络的混合方法，提高了模型的解释性和物理一致性，对于去中心化能源网络中的电池管理和智能电网的优化控制具有重要意义。"}}
{"id": "2506.08319", "title": "DEKC: Data-Enable Control for Tethered Space Robot Deployment in the Presence of Uncertainty via Koopman Operator Theory", "authors": ["Ao Jin", "Qinyi Wang", "Sijie Wen", "Ya Liu", "Ganghui Shen", "Panfeng Huang", "Fan Zhang"], "summary": "This work focuses the deployment of tethered space robot in the presence of\nunknown uncertainty. A data-enable framework called DEKC which contains offline\ntraining part and online execution part is proposed to deploy tethered space\nrobot in the presence of uncertainty. The main idea of this work is modeling\nthe unknown uncertainty as a dynamical system, which enables high accuracy and\nconvergence of capturing uncertainty. The core part of proposed framework is a\nproxy model of uncertainty, which is derived from data-driven Koopman theory\nand is separated with controller design. In the offline stage, the lifting\nfunctions associated with Koopman operator are parameterized with deep neural\nnetworks. Then by solving an optimization problem, the lifting functions are\nlearned from sampling data. In the online execution stage, the proxy model\ncooperates the learned lifting functions obtained in the offline phase to\ncapture the unknown uncertainty. Then the output of proxy model is compensated\nto the baseline controller such that the effect of uncertainty can be\nattenuated or even eliminated. Furthermore, considering some scenarios in which\nthe performance of proxy model may weaken, a receding-horizon scheme is\nproposed to update the proxy model online. Finally, the extensive numerical\nsimulations demonstrate the effectiveness of our proposed framework. The\nimplementation of proposed DEKC framework is publicly available at\nhttps://github.com/NPU-RCIR/DEKC.git.", "comment": "12 pages", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08319v1", "AI": {"title_translation": "DEKC：基于Koopman算子理论的不确定性系绳空间机器人部署数据使能控制", "tldr": "本文提出了一种名为DEKC的数据使能控制框架，利用Koopman算子理论处理系绳空间机器人在未知不确定性下的部署问题，通过离线训练和在线执行来捕获并补偿不确定性。", "motivation": "在未知不确定性存在的情况下，系绳空间机器人部署面临挑战，需要一种能够有效处理这些不确定性的控制方法。", "method": "本文提出了一种名为DEKC的数据使能框架，包含离线训练和在线执行两部分。其核心思想是将未知不确定性建模为一个动力学系统，并利用数据驱动的Koopman算子理论构建不确定性的代理模型。在离线阶段，通过深度神经网络参数化Koopman算子相关的提升函数，并从采样数据中学习。在线执行阶段，代理模型与学习到的提升函数协同作用捕获不确定性，并将其输出补偿给基线控制器以衰减或消除不确定性影响。此外，还提出了一种滚动优化方案以在线更新代理模型。", "result": "广泛的数值模拟结果表明，所提出的DEKC框架在处理不确定性方面是有效的。", "conclusion": "DEKC框架能够有效处理系绳空间机器人在未知不确定性下的部署问题，通过数据驱动的Koopman算子理论捕获和补偿不确定性，从而衰减甚至消除其影响。", "translation": "这项工作主要关注系绳空间机器人在存在未知不确定性情况下的部署问题。本文提出了一种名为DEKC的数据使能框架，该框架包含离线训练和在线执行两部分，用于在存在不确定性的情况下部署系绳空间机器人。这项工作的主要思想是将未知不确定性建模为一个动力学系统，这使得捕获不确定性具有高精度和收敛性。所提出框架的核心部分是不确定性的代理模型，该模型来源于数据驱动的Koopman理论，并与控制器设计分离。在离线阶段，与Koopman算子相关的提升函数通过深度神经网络进行参数化。然后通过解决一个优化问题，从采样数据中学习提升函数。在在线执行阶段，代理模型与离线阶段获得的学习到的提升函数协同作用，以捕获未知不确定性。然后将代理模型的输出补偿给基线控制器，从而可以衰减甚至消除不确定性的影响。此外，考虑到代理模型性能可能减弱的一些场景，本文提出了一种滚动优化方案来在线更新代理模型。最后，广泛的数值模拟证明了我们提出的框架的有效性。所提出的DEKC框架的实现已在https://github.com/NPU-RCIR/DEKC.git上公开可用。", "summary": "本文提出了一种名为DEKC的数据使能控制框架，用于解决系绳空间机器人在未知不确定性下的部署问题。该框架将不确定性建模为动力学系统，并利用数据驱动的Koopman算子理论构建不确定性代理模型。在离线阶段，通过深度神经网络从采样数据中学习提升函数；在线阶段，代理模型捕获不确定性并补偿给基线控制器。为了应对模型性能可能下降的情况，还引入了滚动优化方案在线更新代理模型。数值模拟验证了该框架的有效性。", "keywords": "系绳空间机器人, 不确定性, Koopman算子, 数据使能控制, 深度神经网络", "comments": "该论文的创新点在于将Koopman算子理论与深度学习相结合，构建了分离于控制器设计的不确定性代理模型，以数据驱动的方式处理复杂的未知不确定性。这种离线训练和在线执行的框架设计，以及滚动优化方案的引入，提升了系统在实际应用中的鲁棒性和适应性。对于处理具有复杂非线性动力学和未知扰动的空间机器人系统，该方法具有重要的理论和实际意义。"}}
{"id": "2506.09037", "title": "Optimizing Sparse SYK", "authors": ["Matthew Ding", "Robbie King", "Bobak T. Kiani", "Eric R. Anschuetz"], "summary": "Finding the ground state of strongly-interacting fermionic systems is often\nthe prerequisite for fully understanding both quantum chemistry and condensed\nmatter systems. The Sachdev--Ye--Kitaev (SYK) model is a representative example\nof such a system; it is particularly interesting not only due to the existence\nof efficient quantum algorithms preparing approximations to the ground state\nsuch as Hastings--O'Donnell (STOC 2022), but also known no-go results for many\nclassical ansatzes in preparing low-energy states. However, this\nquantum-classical separation is known to \\emph{not} persist when the SYK model\nis sufficiently sparsified, i.e., when terms in the model are discarded with\nprobability $1-p$, where $p=\\Theta(1/n^3)$ and $n$ is the system size. This\nraises the question of how robust the quantum and classical complexities of the\nSYK model are to sparsification.\n  In this work we initiate the study of the sparse SYK model where $p \\in\n[\\Theta(1/n^3),1]$. We show there indeed exists a certain robustness of\nsparsification. First, we prove that the quantum algorithm of\nHastings--O'Donnell for $p=1$ still achieves a constant-factor approximation to\nthe ground energy when $p\\geq\\Omega(\\log n/n)$. Additionally, we prove that\nwith high probability, Gaussian states cannot achieve better than a\n$O(\\sqrt{\\log n/pn})$-factor approximation to the true ground state energy of\nsparse SYK. This is done through a general classical circuit complexity\nlower-bound of $\\Omega(pn^3)$ for any quantum state achieving a constant-factor\napproximation. Combined, these show a provable separation between classical\nalgorithms outputting Gaussian states and efficient quantum algorithms for the\ngoal of finding approximate sparse SYK ground states when $p \\geq \\Omega(\\log\nn/n)$, extending the analogous $p=1$ result of Hastings--O'Donnell.", "comment": "27+7 pages, 1 figure. Abstract shortened to meet arxiv requirements", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09037v1", "AI": {"title_translation": "优化稀疏SYK", "tldr": "本文研究了稀疏SYK模型的量子与经典复杂性，证明了在一定稀疏度下量子算法仍优于经典算法，扩展了现有量子优势的结论。", "motivation": "寻找强相互作用费米子系统的基态对于理解量子化学和凝聚态系统至关重要。Sachdev-Ye-Kitaev (SYK) 模型是这类系统的代表，其基态近似在高效量子算法中表现出色，但经典方法则不然。然而，当SYK模型充分稀疏化时，这种量子-经典分离不再持续。因此，本文旨在探究SYK模型的量子和经典复杂性对稀疏化的鲁棒性。", "method": "本文首次系统研究了稀疏SYK模型，其中稀疏度 $p \text{ 属于 } [\\Theta(1/n^3),1]$。通过理论证明，分析了Hastings-O'Donnell量子算法在稀疏情况下的性能，并为经典高斯态算法的近似能力建立了下限，从而比较了量子与经典方法的复杂性。", "result": "1. 当 $p\\geq\\Omega(\\log n/n)$ 时，Hastings-O'Donnell的量子算法（原用于 $p=1$）仍能实现对基态能量的常数因子近似。2. 以高概率，高斯态无法实现优于 $O(\\sqrt{\\log n/pn})$ 因子近似的稀疏SYK真实基态能量。这是通过证明任何实现常数因子近似的量子态的经典电路复杂性下限为 $\\Omega(pn^3)$ 来实现的。", "conclusion": "本文证明了当 $p \\geq \\Omega(\\log n/n)$ 时，输出高斯态的经典算法与寻找近似稀疏SYK基态的高效量子算法之间存在可证明的分离，从而扩展了Hastings-O'Donnell在 $p=1$ 时的类似结论，证实了量子算法在特定稀疏度下的优势。", "translation": "寻找强相互作用费米子系统的基态通常是全面理解量子化学和凝聚态系统的先决条件。Sachdev-Ye-Kitaev (SYK) 模型是这类系统的一个典型例子；它之所以特别有趣，不仅因为存在高效的量子算法来准备基态的近似值，例如Hastings-O'Donnell (STOC 2022)，而且已知许多经典拟设在准备低能态方面存在“不可能”的结果。然而，已知当SYK模型充分稀疏化时，即当模型中的项以 $1-p$ 的概率被丢弃时，其中 $p=\\Theta(1/n^3)$ 且 $n$ 是系统大小，这种量子-经典分离不会持续。这提出了一个问题：SYK模型的量子和经典复杂性对稀疏化的鲁棒性如何？\n在这项工作中，我们首次研究了稀疏SYK模型，其中 $p \\in [\\Theta(1/n^3),1]$。我们证明了稀疏化确实存在一定的鲁棒性。首先，我们证明了Hastings-O'Donnell针对 $p=1$ 的量子算法在 $p\\geq\\Omega(\\log n/n)$ 时仍能实现对基态能量的常数因子近似。此外，我们证明了以高概率，高斯态无法实现优于 $O(\\sqrt{\\log n/pn})$ 因子近似的稀疏SYK真实基态能量。这是通过对任何实现常数因子近似的量子态建立 $\\Omega(pn^3)$ 的通用经典电路复杂性下限来实现的。结合这些结果，证明了当 $p \\geq \\Omega(\\log n/n)$ 时，输出高斯态的经典算法与寻找近似稀疏SYK基态的高效量子算法之间存在可证明的分离，从而扩展了Hastings-O'Donnell的类似 $p=1$ 结果。", "summary": "本文探讨了稀疏SYK模型的基态寻找问题，旨在理解量子与经典算法在稀疏化条件下的复杂性鲁棒性。研究发现，即使在SYK模型被稀疏化至一定程度（$p \\geq \\Omega(\\log n/n)$）时，现有的量子算法仍能有效近似其基态能量。同时，论文通过建立经典算法的性能下限，证明了经典高斯态算法在此类稀疏SYK模型中表现不佳，从而揭示了量子算法相较于经典算法在处理稀疏SYK基态问题上的显著优势。", "keywords": "SYK模型, 稀疏化, 量子算法, 基态近似, 经典复杂性", "comments": "这篇论文通过严格的数学证明，为SYK模型在稀疏化条件下的量子优势提供了理论支持。它扩展了先前关于密集SYK模型的结论，表明即使在参数空间发生变化时，量子算法在某些情况下仍能保持其计算优势。这项工作对于理解量子计算的局限性和潜力，特别是在处理强相互作用系统基态问题上，具有重要意义。"}}
{"id": "2506.08626", "title": "Leveraging LLMs to Evaluate Usefulness of Document", "authors": ["Xingzhu Wang", "Erhan Zhang", "Yiqun Chen", "Jinghan Xuan", "Yucheng Hou", "Yitong Xu", "Ying Nie", "Shuaiqiang Wang", "Dawei Yin", "Jiaxin Mao"], "summary": "The conventional Cranfield paradigm struggles to effectively capture user\nsatisfaction due to its weak correlation between relevance and satisfaction,\nalongside the high costs of relevance annotation in building test collections.\nTo tackle these issues, our research explores the potential of leveraging large\nlanguage models (LLMs) to generate multilevel usefulness labels for evaluation.\nWe introduce a new user-centric evaluation framework that integrates users'\nsearch context and behavioral data into LLMs. This framework uses a cascading\njudgment structure designed for multilevel usefulness assessments, drawing\ninspiration from ordinal regression techniques. Our study demonstrates that\nwhen well-guided with context and behavioral information, LLMs can accurately\nevaluate usefulness, allowing our approach to surpass third-party labeling\nmethods. Furthermore, we conduct ablation studies to investigate the influence\nof key components within the framework. We also apply the labels produced by\nour method to predict user satisfaction, with real-world experiments indicating\nthat these labels substantially improve the performance of satisfaction\nprediction models.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08626v1", "AI": {"title_translation": "利用大型语言模型评估文档的有用性", "tldr": "该研究提出了一种利用大型语言模型（LLMs）评估文档有用性的新框架，通过整合用户搜索上下文和行为数据，该方法能够准确评估有用性，超越第三方标注方法，并显著提升用户满意度预测模型的性能。", "motivation": "传统的Cranfield范式在捕捉用户满意度方面存在困难，因为相关性与满意度之间的关联性较弱，并且构建测试集时的相关性标注成本高昂。", "method": "研究引入了一个新的以用户为中心的评估框架，将用户的搜索上下文和行为数据整合到大型语言模型中。该框架采用受序数回归技术启发的级联判断结构，用于多级有用性评估。", "result": "实验表明，在上下文和行为信息的良好指导下，LLMs可以准确评估有用性，使该方法优于第三方标注方法。此外，通过消融研究调查了框架内关键组件的影响。将该方法生成的标签应用于预测用户满意度，实际实验表明这些标签显著提高了满意度预测模型的性能。", "conclusion": "通过整合用户上下文和行为数据，大型语言模型可以有效地评估文档的有用性，从而改进评估过程并提高用户满意度预测，解决了传统方法的局限性。", "translation": "传统Cranfield范式由于相关性和满意度之间的弱相关性以及构建测试集时相关性标注的高成本，难以有效地捕捉用户满意度。为了解决这些问题，我们的研究探索了利用大型语言模型（LLMs）生成多级有用性标签以进行评估的潜力。我们引入了一个新的以用户为中心的评估框架，该框架将用户的搜索上下文和行为数据整合到LLMs中。该框架采用受序数回归技术启发的级联判断结构，旨在进行多级有用性评估。我们的研究表明，在上下文和行为信息的良好指导下，LLMs可以准确评估有用性，使我们的方法超越第三方标注方法。此外，我们进行了消融研究，以调查框架内关键组件的影响。我们还将我们方法生成的标签应用于预测用户满意度，实际实验表明这些标签显著提高了满意度预测模型的性能。", "summary": "本文提出了一种新的评估框架，利用大型语言模型（LLMs）并结合用户搜索上下文和行为数据，生成多级文档有用性标签。该框架采用级联判断结构，旨在克服传统Cranfield范式在用户满意度评估上的不足。研究结果表明，该方法能准确评估有用性，表现优于第三方标注，并且其生成的有用性标签能显著提升用户满意度预测模型的性能。", "keywords": "LLMs, 有用性评估, 用户满意度, Cranfield范式, 行为数据", "comments": "该论文通过将用户上下文和行为数据整合到大型语言模型中来评估文档有用性，提出了一种创新方法，有效解决了传统相关性评估的局限性。其在提升用户满意度预测方面的实际应用具有重要意义。"}}
{"id": "2506.08535", "title": "Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation", "authors": ["Ronald Katende"], "summary": "We introduce the $D$-decomposition, a non-orthogonal matrix factorization of\nthe form $A \\approx P D Q$, where $P \\in \\mathbb{R}^{n \\times k}$, $D \\in\n\\mathbb{R}^{k \\times k}$, and $Q \\in \\mathbb{R}^{k \\times n}$. The\ndecomposition is defined variationally by minimizing a regularized Frobenius\nloss, allowing control over rank, sparsity, and conditioning. Unlike algebraic\nfactorizations such as LU or SVD, it is computed by alternating minimization.\nWe establish existence and perturbation stability of the solution and show that\neach update has complexity $\\mathcal{O}(n^2k)$. Benchmarks against truncated\nSVD, CUR, and nonnegative matrix factorization show improved reconstruction\naccuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,\nparticularly under sparsity and noise.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08535v1", "AI": {"title_translation": "结构化变分$D$-分解用于精确稳定的低秩近似", "tldr": "本文引入了一种新的变分$D$-分解方法，用于低秩近似，在准确性和稳定性上优于现有方法。", "motivation": "现有的代数分解方法（如LU或SVD）在低秩近似中可能难以灵活控制秩、稀疏性和条件数，且在稀疏和噪声数据下的重建精度有待提高。", "method": "本文引入了$D$-分解，这是一种非正交矩阵分解，形式为$A \\approx P D Q$，其中$P \\in \\mathbb{R}^{n \\times k}$, $D \\in \\mathbb{R}^{k \\times k}$, $Q \\in \\mathbb{R}^{k \\times n}$。该分解通过最小化正则化Frobenius损失进行变分定义，从而允许控制秩、稀疏性和条件数。其计算采用交替最小化方法，每次更新的复杂度为$\\mathcal{O}(n^2k)$。", "result": "研究建立了$D$-分解解决方案的存在性和扰动稳定性。与截断SVD、CUR和非负矩阵分解的基准测试结果表明，在MovieLens、MNIST、Olivetti Faces和基因表达矩阵等数据集上，特别是在稀疏和噪声条件下，$D$-分解在重建精度方面有所提高。", "conclusion": "$D$-分解为低秩近似提供了一种新的、更精确且稳定的方法，尤其适用于处理稀疏和噪声数据。", "translation": "我们引入了$D$-分解，这是一种非正交矩阵分解，形式为$A \\approx P D Q$，其中$P \\in \\mathbb{R}^{n \\times k}$, $D \\in \\mathbb{R}^{k \\times k}$, $Q \\in \\mathbb{R}^{k \\times n}$。该分解通过最小化正则化Frobenius损失变分定义，允许控制秩、稀疏性和条件数。与LU或SVD等代数分解不同，它通过交替最小化计算。我们建立了解决方案的存在性和扰动稳定性，并表明每次更新的复杂度为$\\mathcal{O}(n^2k)$。与截断SVD、CUR和非负矩阵分解的基准测试表明，在MovieLens、MNIST、Olivetti Faces和基因表达矩阵上，尤其是在稀疏和噪声条件下，重建精度有所提高。", "summary": "本文提出了一种名为$D$-分解的新型非正交矩阵分解方法（$A \\approx P D Q$），通过最小化正则化Frobenius损失进行变分定义，并允许控制秩、稀疏性和条件数。该方法通过交替最小化计算，并具有理论上的存在性和扰动稳定性，每次更新复杂度为$\\mathcal{O}(n^2k)$。实验结果表明，$D$-分解在MovieLens、MNIST、Olivetti Faces和基因表达矩阵等数据集上，特别是在稀疏和噪声环境下，比截断SVD、CUR和非负矩阵分解具有更高的重建精度。", "keywords": "$D$-分解, 低秩近似, 矩阵分解, 变分方法, 稀疏性", "comments": "该研究引入了一种新颖的变分矩阵分解方法，其创新之处在于通过变分定义和正则化损失函数，实现了对低秩近似中秩、稀疏性和条件数的灵活控制，这在现有代数分解方法中是难以实现的。其在稀疏和噪声数据上的优越性能表明了其在实际应用中的潜力。"}}
{"id": "2506.08922", "title": "Striking Back At Cobalt: Using Network Traffic Metadata To Detect Cobalt Strike Masquerading Command and Control Channels", "authors": ["Clément Parssegny", "Johan Mazel", "Olivier Levillain", "Pierre Chifflier"], "summary": "Off-the-shelf software for Command and Control is often used by attackers and\nlegitimate pentesters looking for discretion. Among other functionalities,\nthese tools facilitate the customization of their network traffic so it can\nmimic popular websites, thereby increasing their secrecy. Cobalt Strike is one\nof the most famous solutions in this category, used by known advanced attacker\ngroups such as \"Mustang Panda\" or \"Nobelium\". In response to these threats,\nSecurity Operation Centers and other defense actors struggle to detect Command\nand Control traffic, which often use encryption protocols such as TLS. Network\ntraffic metadata-based machine learning approaches have been proposed to detect\nencrypted malware communications or fingerprint websites over Tor network. This\npaper presents a machine learning-based method to detect Cobalt Strike Command\nand Control activity based only on widely used network traffic metadata. The\nproposed method is, to the best of our knowledge, the first of its kind that is\nable to adapt the model it uses to the observed traffic to optimize its\nperformance. This specificity permits our method to performs equally or better\nthan the state of the art while using standard features. Our method is thus\neasier to use in a production environment and more explainable.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08922v1", "AI": {"title_translation": "反击Cobalt Strike：使用网络流量元数据检测伪装的命令与控制通道", "tldr": "本文提出一种基于网络流量元数据的机器学习方法，用于检测伪装成合法流量的Cobalt Strike命令与控制(C2)通道。该方法能够自适应优化模型，性能优于现有技术且更易于生产环境部署。", "motivation": "攻击者和渗透测试人员常使用Cobalt Strike等现成工具伪装命令与控制（C2）流量，使其模仿合法网站并使用TLS等加密协议，导致安全运营中心难以检测这些隐蔽的C2活动。", "method": "本文提出了一种基于机器学习的方法，仅利用广泛使用的网络流量元数据来检测Cobalt Strike命令与控制活动。该方法独特之处在于其能够根据观察到的流量自适应地调整模型以优化性能。", "result": "所提出的方法在使用标准特征的情况下，其性能与现有技术相当或更优。", "conclusion": "该方法易于在生产环境中使用，并且更具可解释性。", "translation": "现成的命令与控制软件经常被寻求隐蔽性的攻击者和合法渗透测试人员使用。除了其他功能外，这些工具还方便定制其网络流量，使其可以模仿流行的网站，从而增加其隐蔽性。Cobalt Strike是这类中最著名的解决方案之一，被“野马熊猫”或“诺贝尔奖”等知名高级攻击者群体使用。为了应对这些威胁，安全运营中心和其他防御者努力检测命令与控制流量，这些流量通常使用TLS等加密协议。基于网络流量元数据的机器学习方法已被提出用于检测加密恶意软件通信或通过Tor网络对网站进行指纹识别。本文提出了一种基于机器学习的方法，仅依靠广泛使用的网络流量元数据来检测Cobalt Strike命令与控制活动。据我们所知，所提出的方法是同类中第一个能够根据观察到的流量调整其使用的模型以优化其性能的方法。这一特性使得我们的方法在使用标准特征的情况下，性能与现有技术相当或更优。因此，我们的方法更易于在生产环境中使用，并且更具可解释性。", "summary": "本文针对Cobalt Strike等工具伪装命令与控制（C2）流量的挑战，提出了一种创新的机器学习方法。该方法仅依赖广泛使用的网络流量元数据，并具备根据观察到的流量自适应调整模型以优化性能的能力。研究表明，该方法在使用标准特征的情况下，性能与现有技术相当或更优，同时提高了在生产环境中的可用性和可解释性。", "keywords": "Cobalt Strike, 命令与控制, 网络流量元数据, 机器学习, 自适应检测", "comments": "该论文的核心创新点在于其提出的机器学习方法能够自适应地根据观察到的流量优化模型性能，这对于在动态且不断演变的威胁环境中检测隐蔽的C2流量至关重要。此外，该方法使用标准网络流量元数据，并强调了其在生产环境中的易用性和可解释性，使其具有较高的实际应用价值和部署潜力。"}}
{"id": "2506.08812", "title": "Towards a Knowledge Base of Common Sustainability Weaknesses in Green Software Development", "authors": ["Priyavanshi Pathania", "Rohit Mehra", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "With the climate crisis looming, engineering sustainable software systems\nbecome crucial to optimize resource utilization, minimize environmental impact,\nand foster a greener, more resilient digital ecosystem. For developers, getting\naccess to automated tools that analyze code and suggest sustainabilityrelated\noptimizations becomes extremely important from a learning and implementation\nperspective. However, there is currently a dearth of such tools due to the lack\nof standardized knowledge, which serves as the foundation of these tools. In\nthis paper, we motivate the need for the development of a standard knowledge\nbase of commonly occurring sustainability weaknesses in code, and propose an\ninitial way of doing that. Furthermore, through preliminary experiments, we\ndemonstrate why existing knowledge regarding software weaknesses cannot be\nre-tagged \"as is\" to sustainability without significant due diligence, thereby\nurging further explorations in this ecologically significant domain.", "comment": "3 pages. To be published in the proceedings of 38th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2023),\n  Kirchberg, Luxembourg", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08812v1", "AI": {"title_translation": "构建绿色软件开发中常见可持续性弱点的知识库", "tldr": "本文提出为绿色软件开发中的常见可持续性弱点构建一个知识库，因为现有工具匮乏且现有软件弱点知识无法直接应用于可持续性领域。", "motivation": "气候危机使得开发可持续的软件系统至关重要，以优化资源利用、最小化环境影响。开发者需要自动化工具来分析代码并提出可持续性优化建议，但由于缺乏标准化知识，这类工具目前非常稀少。此外，现有关于软件弱点的知识无法直接重新标记并应用于可持续性领域。", "method": "本文阐述了开发一个代码中常见可持续性弱点标准知识库的必要性，并提出了初步的构建方法。此外，通过初步实验，本文论证了为什么现有关于软件弱点的知识不能“原样”重新标记到可持续性领域，需要进行大量尽职调查。", "result": "初步实验表明，现有关于软件弱点的知识不能在不进行大量尽职调查的情况下“原样”重新标记到可持续性领域。", "conclusion": "绿色软件开发中急需一个专门的常见可持续性弱点知识库，因为现有的软件弱点知识不足以直接应用于此。", "translation": "随着气候危机迫在眉睫，工程化可持续软件系统变得至关重要，以优化资源利用、最小化环境影响，并培育一个更绿色、更有弹性的数字生态系统。对于开发者而言，获得能够分析代码并提出可持续性相关优化的自动化工具，从学习和实施的角度来看变得极其重要。然而，由于缺乏作为这些工具基础的标准化知识，目前这类工具非常稀少。在本文中，我们阐述了开发一个代码中常见可持续性弱点标准知识库的必要性，并提出了初步的构建方法。此外，通过初步实验，我们论证了为什么现有关于软件弱点的知识不能在不进行大量尽职调查的情况下“原样”重新标记到可持续性领域，从而敦促在这一生态重要领域进行进一步探索。", "summary": "本文旨在解决绿色软件开发中缺乏自动化可持续性优化工具的问题。作者指出，这一不足源于缺乏标准化的可持续性弱点知识库。文章强调了构建这样一个知识库的必要性，并提出了一种初步方法。通过初步实验，研究还表明现有的通用软件弱点知识不能直接用于可持续性领域，这突显了在该生态重要领域进行进一步研究的紧迫性。", "keywords": "绿色软件开发, 可持续性弱点, 知识库, 自动化工具, 环境影响", "comments": "该论文识别了绿色软件开发工具中的一个关键空白，强调了对专业化可持续性知识的需求。其发现现有软件弱点知识不能直接用于可持续性，这一洞察非常重要，突显了软件可持续性领域的独特挑战和研究价值。"}}
{"id": "2506.08706", "title": "ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel", "authors": ["Tomasz Winiarski", "Jan Kaniuka", "Daniel Giełdowski", "Jakub Ostrysz", "Krystian Radlak", "Dmytro Kushnir"], "summary": "As robotic systems grow increasingly complex, heterogeneous, and\nsafety-critical, the need for structured development methodologies becomes\nparamount. Although frameworks like the Robot Operating System (ROS) and\nModel-Based Systems Engineering (MBSE) offer foundational tools, they often\nlack integration when used together. This paper addresses that gap by aligning\nthe widely recognized V-model development paradigm with the MeROS metamodel\nSysML-based modeling language tailored for ROS-based systems.\n  We propose a domain-specific methodology that bridges ROS-centric modelling\nwith systems engineering practices. Our approach formalises the structure,\nbehaviour, and validation processes of robotic systems using MeROS, while\nextending it with a generalized, adaptable V-model compatible with both ROS and\nROS 2. Rather than prescribing a fixed procedure, the approach supports\nproject-specific flexibility and reuse, offering guidance across all stages of\ndevelopment.\n  The approach is validated through a comprehensive case study on HeROS, a\nheterogeneous multi-robot platform comprising manipulators, mobile units, and\ndynamic test environments. This example illustrates how the MeROS-compatible\nV-model enhances traceability and system consistency while remaining accessible\nand extensible for future adaptation. The work contributes a structured,\ntool-agnostic foundation for developers and researchers seeking to apply MBSE\npractices in ROS-based projects.", "comment": "19 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08706v1", "AI": {"title_translation": "基于V模型的MeROS元模型在ROS相关机器人系统开发中的应用", "tldr": "本文提出了一种领域特定的方法，将ROS中心建模与系统工程实践相结合，通过将V模型开发范式与MeROS元模型对齐，为ROS/ROS 2机器人系统提供结构化、可追溯且灵活的开发指导。", "motivation": "随着机器人系统日益复杂、异构且对安全性要求提高，对结构化开发方法的需求变得至关重要。尽管ROS和MBSE提供了基础工具，但它们在结合使用时常常缺乏集成。本文旨在弥补这一空白。", "method": "本文提出了一种领域特定的方法，通过将广泛认可的V模型开发范式与专为ROS系统定制的SysML建模语言MeROS元模型对齐，桥接ROS中心建模与系统工程实践。该方法利用MeROS形式化机器人系统的结构、行为和验证过程，并将其扩展为与ROS和ROS 2兼容的通用、可适应的V模型，支持项目特定的灵活性和重用。", "result": "该方法通过对异构多机器人平台HeROS的综合案例研究进行了验证，该案例展示了MeROS兼容的V模型如何增强可追溯性和系统一致性，同时保持可访问性和可扩展性以供未来适应。", "conclusion": "这项工作为寻求在基于ROS的项目中应用MBSE实践的开发人员和研究人员提供了一个结构化、与工具无关的基础。", "translation": "随着机器人系统日益复杂、异构且对安全性要求提高，对结构化开发方法的需求变得至关重要。尽管诸如机器人操作系统（ROS）和基于模型的系统工程（MBSE）等框架提供了基础工具，但它们在结合使用时常常缺乏集成。本文通过将广泛认可的V模型开发范式与专为ROS系统定制的基于SysML的MeROS元模型建模语言对齐，弥补了这一空白。\n我们提出了一种领域特定的方法，将ROS中心建模与系统工程实践相结合。我们的方法利用MeROS形式化机器人系统的结构、行为和验证过程，并将其扩展为与ROS和ROS 2兼容的通用、可适应的V模型。该方法不规定固定的程序，而是支持项目特定的灵活性和重用，在开发的所有阶段提供指导。\n该方法通过对HeROS（一个包含机械臂、移动单元和动态测试环境的异构多机器人平台）的综合案例研究进行了验证。该示例说明了MeROS兼容的V模型如何增强可追溯性和系统一致性，同时保持可访问性和可扩展性以供未来适应。这项工作为寻求在基于ROS的项目中应用MBSE实践的开发人员和研究人员提供了一个结构化、与工具无关的基础。", "summary": "本文针对日益复杂的机器人系统开发，提出了一种将V模型开发范式与MeROS元模型相结合的方法。该方法旨在弥合ROS与模型化系统工程（MBSE）之间的集成差距，提供一个领域特定的开发框架，用于形式化ROS/ROS 2机器人系统的结构、行为和验证。通过一个多机器人平台案例研究验证了其有效性，证明了其在增强可追溯性和系统一致性方面的能力，并为ROS项目中的MBSE应用奠定了结构化基础。", "keywords": "ROS, V-model, MeROS, MBSE, 机器人系统开发", "comments": "本文的创新之处在于其将V模型与MeROS元模型相结合，为ROS相关机器人系统开发提供了一个急需的结构化和集成方法。它解决了ROS和MBSE在实际应用中缺乏集成的问题，通过提供一个灵活且可重用的框架，增强了系统开发的可追溯性和一致性。这项工作对于推动机器人系统工程实践具有重要意义，尤其是在应对复杂性和安全性挑战方面。"}}
{"id": "2506.08257", "title": "Highly Compressed Tokenizer Can Generate Without Training", "authors": ["L. Lao Beyer", "T. Li", "X. Chen", "S. Karaman", "K. He"], "summary": "Commonly used image tokenizers produce a 2D grid of spatially arranged\ntokens. In contrast, so-called 1D image tokenizers represent images as highly\ncompressed one-dimensional sequences of as few as 32 discrete tokens. We find\nthat the high degree of compression achieved by a 1D tokenizer with vector\nquantization enables image editing and generative capabilities through\nheuristic manipulation of tokens, demonstrating that even very crude\nmanipulations -- such as copying and replacing tokens between latent\nrepresentations of images -- enable fine-grained image editing by transferring\nappearance and semantic attributes. Motivated by the expressivity of the 1D\ntokenizer's latent space, we construct an image generation pipeline leveraging\ngradient-based test-time optimization of tokens with plug-and-play loss\nfunctions such as reconstruction or CLIP similarity. Our approach is\ndemonstrated for inpainting and text-guided image editing use cases, and can\ngenerate diverse and realistic samples without requiring training of any\ngenerative model.", "comment": "Main manuscript: 9 pages, 7 figures. Appendix: 8 pages, 9 figures. To\n  appear in the Proceedings of the 42nd International Conference on Machine\n  Learning", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08257v1", "AI": {"title_translation": "高度压缩的分词器无需训练即可生成", "tldr": "1D图像分词器通过启发式操作和基于梯度的优化，无需训练即可实现图像编辑和生成。", "motivation": "发现1D分词器潜在空间的高表达能力，并希望利用其实现无需训练的图像生成和编辑。", "method": "本文使用1D图像分词器实现图像的高度压缩（低至32个离散token），并通过对token的启发式操作（如复制和替换）实现图像编辑。在此基础上，构建了一个图像生成管道，利用基于梯度的测试时间token优化和即插即用损失函数（如重建或CLIP相似度）进行图像生成。", "result": "该方法在图像修复和文本引导图像编辑任务中得到了验证，能够生成多样化且逼真的样本，且无需训练任何生成模型。", "conclusion": "高度压缩的1D分词器结合启发式token操作和基于梯度的测试时间优化，可以实现无需训练的图像编辑和生成能力。", "translation": "常用的图像分词器会生成一个空间排列的二维token网格。相比之下，所谓的1D图像分词器将图像表示为高度压缩的一维序列，仅包含少至32个离散token。我们发现，通过向量量化实现的1D分词器所达到的高度压缩，能够通过对token的启发式操作实现图像编辑和生成能力，这表明即使是非常粗糙的操作——例如在图像潜在表示之间复制和替换token——也能通过转移外观和语义属性实现精细的图像编辑。受1D分词器潜在空间表达能力的启发，我们构建了一个图像生成管道，利用基于梯度的测试时间token优化和即插即用损失函数，例如重建或CLIP相似度。我们的方法在图像修复和文本引导图像编辑用例中得到了验证，并且无需训练任何生成模型即可生成多样化和逼真的样本。", "summary": "本文提出一种基于高度压缩1D图像分词器的方法，无需训练即可实现图像编辑和生成。该方法利用1D分词器的高压缩特性，通过启发式token操作实现精细图像编辑，并通过基于梯度的测试时间优化结合即插即用损失函数（如CLIP相似度）来生成图像。在图像修复和文本引导编辑任务中，该方法展现出生成多样化和逼真样本的能力，且无需训练任何生成模型。", "keywords": "图像分词器, 1D分词器, 无需训练, 图像生成, 图像编辑", "comments": "这项工作创新性地展示了高度压缩的1D分词器在无需传统模型训练的情况下，仅通过对潜在空间token的启发式操作和测试时间优化，即可实现高质量的图像编辑和生成。其重要性在于提供了一种计算效率更高、对训练数据和计算资源需求更低的新范式，尤其适用于资源受限或需要快速迭代的场景。"}}
{"id": "2506.08424", "title": "SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy", "authors": ["Yong Liang Goh", "Zhiguang Cao", "Yining Ma", "Jianan Zhou", "Mohammad Haroon Dupty", "Wee Sun Lee"], "summary": "Recent advances toward foundation models for routing problems have shown\ngreat potential of a unified deep model for various VRP variants. However, they\noverlook the complex real-world customer distributions. In this work, we\nadvance the Multi-Task VRP (MTVRP) setting to the more realistic yet\nchallenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce\nSHIELD, a novel model that leverages both sparsity and hierarchy principles.\nBuilding on a deeper decoder architecture, we first incorporate the\nMixture-of-Depths (MoD) technique to enforce sparsity. This improves both\nefficiency and generalization by allowing the model to dynamically select nodes\nto use or skip each decoder layer, providing the needed capacity to adaptively\nallocate computation for learning the task/distribution specific and shared\nrepresentations. We also develop a context-based clustering layer that exploits\nthe presence of hierarchical structures in the problems to produce better local\nrepresentations. These two designs inductively bias the network to identify key\nfeatures that are common across tasks and distributions, leading to\nsignificantly improved generalization on unseen ones. Our empirical results\ndemonstrate the superiority of our approach over existing methods on 9\nreal-world maps with 16 VRP variants each.", "comment": "Accepted in the 42nd International Conference of Machine Learning\n  (ICML)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08424v1", "AI": {"title_translation": "SHIELD：具有稀疏性和层次结构的多任务多分布车辆路径求解器", "tldr": "SHIELD是一种新的模型，通过利用稀疏性和层次结构原理，解决了更具挑战性的多任务多分布车辆路径问题（MTMDVRP），并在泛化能力上显著优于现有方法。", "motivation": "现有的路由问题基础模型忽略了复杂的真实世界客户分布，这促使研究者提出更现实的多任务多分布车辆路径问题（MTMDVRP）设置。", "method": "本文引入了SHIELD模型，它结合了稀疏性和层次结构原理。具体方法包括：1. 采用更深层的解码器架构，并融入Mixture-of-Depths (MoD) 技术以实现稀疏性，提高效率和泛化能力。2. 开发了一个基于上下文的聚类层，利用问题中的层次结构生成更好的局部表示。这些设计有助于网络识别跨任务和分布的共同关键特征。", "result": "SHIELD在9个真实世界地图上的16种VRP变体上，其经验结果表明优于现有方法，并在未见过的任务和分布上显著提高了泛化能力。", "conclusion": "SHIELD模型通过结合稀疏性和层次结构，成功地解决了复杂的多任务多分布车辆路径问题，并在真实世界场景中展现出卓越的泛化性能。", "translation": "最近在路由问题基础模型方面的进展显示出统一深度模型在各种VRP变体方面具有巨大潜力。然而，它们忽视了复杂的真实世界客户分布。在这项工作中，我们将多任务VRP（MTVRP）设置推进到更现实但更具挑战性的多任务多分布VRP（MTMDVRP）设置，并引入了SHIELD，一个利用稀疏性和层次结构原理的新颖模型。基于更深层的解码器架构，我们首先结合了深度混合（Mixture-of-Depths，MoD）技术以强制实现稀疏性。这通过允许模型动态选择每个解码器层要使用或跳过的节点来提高效率和泛化能力，提供了所需的容量来自适应地分配计算以学习特定于任务/分布和共享的表示。我们还开发了一个基于上下文的聚类层，该层利用问题中存在的层次结构来生成更好的局部表示。这两种设计归纳地偏置网络以识别跨任务和分布的共同关键特征，从而在未见过的任务上显著提高了泛化能力。我们的经验结果表明，在9个真实世界地图上，每种地图有16种VRP变体，我们的方法优于现有方法。", "summary": "本文提出了SHIELD模型，旨在解决更具挑战性的多任务多分布车辆路径问题（MTMDVRP），以克服现有基础模型忽视复杂客户分布的局限性。SHIELD通过深度解码器架构结合深度混合（MoD）技术实现稀疏性，提高计算效率和泛化能力，并利用基于上下文的聚类层捕获问题中的层次结构。这些创新设计使模型能够更好地识别跨任务和分布的共享特征，从而在未见过的场景中表现出显著优越的泛化性能。实验结果证明，SHIELD在多个真实世界地图和VRP变体上均优于现有方法。", "keywords": "车辆路径问题, 多任务学习, 多分布, 稀疏性, 层次结构", "comments": "SHIELD的创新之处在于其将稀疏性和层次结构原理融入多任务多分布VRP求解器中。Mixture-of-Depths (MoD) 的应用允许模型自适应地分配计算资源，提高了效率和泛化能力，这对于处理复杂且多样化的真实世界路由问题至关重要。此外，基于上下文的聚类层利用了问题固有的层次结构，进一步提升了模型的表示学习能力。该研究的重要性在于其推进了路由问题基础模型的发展，使其更能适应真实世界的复杂性和多样性。"}}
{"id": "2506.08125", "title": "Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning", "authors": ["Hanbing Liu", "Lang Cao", "Yuanyi Ren", "Mengyu Zhou", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "summary": "Large language models have demonstrated impressive reasoning capabilities,\nyet they often suffer from inefficiencies due to unnecessarily verbose or\nredundant outputs. While many works have explored reinforcement learning (RL)\nto enhance reasoning abilities, most primarily focus on improving accuracy,\nwith limited attention to reasoning efficiency. Some existing approaches\nintroduce direct length-based rewards to encourage brevity, but this often\nleads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL\nframework that advances length-based reward design to boost efficient\nreasoning. Bingo incorporates two key mechanisms: a significance-aware length\nreward, which gradually guides the model to reduce only insignificant tokens,\nand a dynamic length reward, which initially encourages elaborate reasoning for\nhard questions but decays over time to improve overall efficiency. Experiments\nacross multiple reasoning benchmarks show that Bingo improves both accuracy and\nefficiency. It outperforms the vanilla reward and several other length-based\nreward baselines in RL, achieving a favorable trade-off between accuracy and\nefficiency. These results underscore the potential of training LLMs explicitly\nfor efficient reasoning.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08125v1", "AI": {"title_translation": "Bingo：通过动态和基于重要性的强化学习提升大型语言模型的推理效率", "tldr": "Bingo是一个强化学习框架，通过引入重要性感知和动态长度奖励，有效提升了大型语言模型的推理效率和准确性，解决了现有方法在效率与准确性之间权衡的不足。", "motivation": "大型语言模型虽然展现出强大的推理能力，但常因输出冗余而效率低下。现有强化学习方法多关注准确性而非效率，少数直接基于长度的奖励会导致准确性显著下降。", "method": "本文提出了Bingo框架，一个强化学习方法，通过改进长度奖励设计来提升高效推理。Bingo包含两个核心机制：1. 重要性感知长度奖励：逐步引导模型仅减少不重要的token。2. 动态长度奖励：初期鼓励对难题进行详尽推理，但随着时间推移衰减以提高整体效率。", "result": "实验表明，Bingo在多个推理基准测试中同时提升了准确性和效率。它优于普通奖励和其他基于长度的强化学习基线，在准确性和效率之间取得了良好的权衡。", "conclusion": "这些结果强调了明确训练大型语言模型以实现高效推理的潜力。", "translation": "大型语言模型展现出令人印象深刻的推理能力，但它们常常因不必要的冗长或冗余输出而效率低下。尽管许多工作探索了强化学习（RL）以增强推理能力，但大多数主要关注提高准确性，对推理效率的关注有限。一些现有方法引入了直接基于长度的奖励以鼓励简洁性，但这通常会导致准确性显著下降。在本文中，我们提出了Bingo，一个强化学习框架，它改进了基于长度的奖励设计以促进高效推理。Bingo包含了两个关键机制：一个重要性感知长度奖励，它逐步引导模型仅减少不重要的token；以及一个动态长度奖励，它最初鼓励对难题进行详尽推理，但随着时间推移衰减以提高整体效率。在多个推理基准上的实验表明，Bingo同时提高了准确性和效率。它在RL中优于普通奖励和其他几个基于长度的奖励基线，在准确性和效率之间取得了有利的权衡。这些结果强调了明确训练大型语言模型以实现高效推理的潜力。", "summary": "本文提出了Bingo，一个基于强化学习的框架，旨在提升大型语言模型的推理效率。针对现有方法在效率与准确性之间难以兼顾的问题，Bingo引入了重要性感知长度奖励和动态长度奖励机制。前者逐步引导模型删除不重要的token，后者则在初期鼓励详尽推理并随时间衰减以提高整体效率。实验证明，Bingo在保持或提升准确性的同时，显著提高了推理效率，并在准确性与效率之间实现了更好的平衡。", "keywords": "大型语言模型, 强化学习, 推理效率, 长度奖励, Bingo", "comments": "Bingo通过创新性地结合重要性感知和动态调整的长度奖励，有效解决了大型语言模型在追求效率时牺牲准确性的难题，为LLM的高效推理训练提供了新的思路和方法，具有重要的实践意义和研究价值。"}}
{"id": "2506.08426", "title": "HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems", "authors": ["Zheng Lin", "Zhe Chen", "Xianhao Chen", "Wei Ni", "Yue Gao"], "summary": "Split federated learning (SFL) has emerged as a promising paradigm to\ndemocratize machine learning (ML) on edge devices by enabling layer-wise model\npartitioning. However, existing SFL approaches suffer significantly from the\nstraggler effect due to the heterogeneous capabilities of edge devices. To\naddress the fundamental challenge, we propose adaptively controlling batch\nsizes (BSs) and model splitting (MS) for edge devices to overcome resource\nheterogeneity. We first derive a tight convergence bound of SFL that quantifies\nthe impact of varied BSs and MS on learning performance. Based on the\nconvergence bound, we propose HASFL, a heterogeneity-aware SFL framework\ncapable of adaptively controlling BS and MS to balance communication-computing\nlatency and training convergence in heterogeneous edge networks. Extensive\nexperiments with various datasets validate the effectiveness of HASFL and\ndemonstrate its superiority over state-of-the-art benchmarks.", "comment": "16 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:2403.13101", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08426v1", "AI": {"title_translation": "HASFL：边缘计算系统中异构感知的分裂联邦学习", "tldr": "HASFL是一种异构感知的SFL框架，通过自适应控制批处理大小和模型分割，有效解决了边缘设备异构性导致的滞后效应，并在实验中表现出优越性。", "motivation": "现有的分裂联邦学习（SFL）方法由于边缘设备能力异构性，严重受滞后效应影响。", "method": "提出HASFL框架，通过自适应控制边缘设备的批处理大小（BSs）和模型分割（MS），以平衡通信-计算延迟和训练收敛。首先推导了SFL的紧密收敛界限，量化了不同BSs和MS对学习性能的影响。", "result": "大量实验（使用各种数据集）验证了HASFL的有效性，并证明其优于最先进的基准。", "conclusion": "HASFL能够有效解决边缘计算系统中SFL的异构性挑战，通过自适应控制批处理大小和模型分割，平衡通信-计算延迟和训练收敛，并实现优越的性能。", "translation": "分裂联邦学习（SFL）已成为一种有前景的范式，通过实现层级模型分区，使机器学习（ML）在边缘设备上普及。然而，现有的SFL方法由于边缘设备的异构能力而严重受滞后效应影响。为了解决这一基本挑战，我们提出自适应控制边缘设备的批处理大小（BSs）和模型分割（MS）以克服资源异构性。我们首先推导了一个SFL的紧密收敛界限，量化了不同BSs和MS对学习性能的影响。基于该收敛界限，我们提出了HASFL，一个异构感知SFL框架，能够自适应控制BS和MS，以平衡异构边缘网络中的通信-计算延迟和训练收敛。使用各种数据集进行的大量实验验证了HASFL的有效性，并证明其优于最先进的基准。", "summary": "本文提出了HASFL，一种异构感知的分裂联邦学习（SFL）框架，旨在解决边缘计算系统中SFL因设备异构性导致的滞后效应。HASFL通过自适应控制批处理大小和模型分割来平衡通信-计算延迟和训练收敛。研究首先推导了SFL的收敛界限，并基于此设计了HASFL。实验结果表明HASFL有效且优于现有方法。", "keywords": "分裂联邦学习, 异构性, 边缘计算, 滞后效应, 自适应控制", "comments": "HASFL的创新点在于其自适应控制批处理大小和模型分割以应对异构性，并首次推导了SFL的紧密收敛界限，这为理解和优化SFL在异构环境中的性能提供了理论基础。该方法对于在资源受限且多样化的边缘设备上部署高效的联邦学习具有重要意义。"}}
{"id": "2506.08366", "title": "Learning event-triggered controllers for linear parameter-varying systems from data", "authors": ["Renjie Ma", "Su Zhang", "Wenjie Liu", "Zhijian Hu", "Peng Shi"], "summary": "Nonlinear dynamical behaviours in engineering applications can be\napproximated by linear-parameter varying (LPV) representations, but obtaining\nprecise model knowledge to develop a control algorithm is difficult in\npractice. In this paper, we develop the data-driven control strategies for\nevent-triggered LPV systems with stability verifications. First, we provide the\ntheoretical analysis of ${\\theta}$-persistence of excitation for LPV systems,\nwhich leads to the feasible data-based representations. Then, in terms of the\navailable perturbed data, we derive the stability certificates for\nevent-triggered LPV systems with the aid of Petersen's lemma in the sense of\nrobust control, resulting in the computationally tractable semidefinite\nprogrammings, the feasible solutions of which yields the optimal gain\nschedulings. Besides, we generalize the data-driven eventtriggered LPV control\nmethods to the scenario of reference trajectory tracking, and discuss the\nrobust tracking stability accordingly. Finally, we verify the effectiveness of\nour theoretical derivations by numerical simulations.", "comment": "13 pages, 5 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08366v1", "AI": {"title_translation": "基于数据学习线性参数变化系统的事件触发控制器", "tldr": "本文提出了一种数据驱动的事件触发LPV系统控制策略，解决了精确模型知识获取困难的问题，并通过理论分析和数值模拟验证了其有效性。", "motivation": "在工程应用中，非线性动力学行为可以通过线性参数变化（LPV）表示来近似，但实际中很难获得精确的模型知识来开发控制算法。", "method": "首先，提供了LPV系统${\\theta}$-激励持续性的理论分析，从而实现了可行的数据表示。然后，利用可用扰动数据，借助Petersen引理，在鲁棒控制意义下推导了事件触发LPV系统的稳定性证明，得到了计算上可行的半定规划。此外，将数据驱动的事件触发LPV控制方法推广到参考轨迹跟踪场景，并讨论了鲁棒跟踪稳定性。", "result": "得到了计算上可行的半定规划，其可行解能产生最优增益调度。成功推广了数据驱动的事件触发LPV控制方法到参考轨迹跟踪场景，并讨论了鲁棒跟踪稳定性。通过数值模拟验证了理论推导的有效性。", "conclusion": "本文成功开发了数据驱动的事件触发LPV系统控制策略，解决了模型知识获取难题，并验证了其理论有效性，为实际工程应用提供了可行的解决方案。", "translation": "工程应用中的非线性动力学行为可以通过线性参数变化（LPV）表示来近似，但实际中很难获得精确的模型知识来开发控制算法。在本文中，我们开发了针对事件触发LPV系统的数据驱动控制策略，并进行了稳定性验证。首先，我们提供了LPV系统${\\theta}$-激励持续性的理论分析，这导致了可行的数据表示。然后，根据可用的扰动数据，我们借助Petersen引理，在鲁棒控制意义下推导了事件触发LPV系统的稳定性证明，从而得到了计算上可行的半定规划，其可行解能产生最优增益调度。此外，我们将数据驱动的事件触发LPV控制方法推广到参考轨迹跟踪场景，并相应地讨论了鲁棒跟踪稳定性。最后，我们通过数值模拟验证了我们理论推导的有效性。", "summary": "本文针对工程应用中LPV系统精确模型知识难以获取的问题，提出了一种数据驱动的事件触发LPV系统控制策略。研究内容包括${\\theta}$-激励持续性理论分析、基于Petersen引理的鲁棒稳定性证明（通过半定规划实现最优增益调度），并将方法推广至参考轨迹跟踪场景，讨论了鲁棒跟踪稳定性。数值模拟验证了所提理论推导的有效性。", "keywords": "事件触发控制, 线性参数变化系统, 数据驱动, 稳定性, 鲁棒控制", "comments": "本文的创新点在于将数据驱动方法应用于事件触发LPV系统控制，并提供了严谨的理论分析，包括激励持续性条件和基于鲁棒控制的稳定性证明。这为处理实际工程中模型不确定性问题提供了一种有效且计算可行的方案。"}}
{"id": "2506.08743", "title": "Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems", "authors": ["Michael Färber", "David Lamprecht", "Yuni Susanti"], "summary": "Graph Neural Networks (GNNs) have substantially advanced the field of\nrecommender systems. However, despite the creation of more than a thousand\nknowledge graphs (KGs) under the W3C standard RDF, their rich semantic\ninformation has not yet been fully leveraged in GNN-based recommender systems.\nTo address this gap, we propose a comprehensive integration of RDF KGs with\nGNNs that utilizes both the topological information from RDF object properties\nand the content information from RDF datatype properties. Our main focus is an\nin-depth evaluation of various GNNs, analyzing how different semantic feature\ninitializations and types of graph structure heterogeneity influence their\nperformance in recommendation tasks. Through experiments across multiple\nrecommendation scenarios involving multi-million-node RDF graphs, we\ndemonstrate that harnessing the semantic richness of RDF KGs significantly\nimproves recommender systems and lays the groundwork for GNN-based recommender\nsystems for the Linked Open Data cloud. The code and data are available on our\nGitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation", "comment": "Accepted at DASFAA 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08743v1", "AI": {"title_translation": "将RDF知识图谱与图神经网络结合以实现语义丰富的推荐系统", "tldr": "本研究提出了一种将RDF知识图谱的语义信息与图神经网络（GNNs）全面整合的方法，以改进推荐系统。通过实验证明，利用RDF知识图谱的语义丰富性可以显著提高推荐性能。", "motivation": "尽管W3C标准RDF下创建了上千个知识图谱，但其丰富的语义信息尚未在基于GNN的推荐系统中得到充分利用。为了解决这一空白，本研究旨在全面整合RDF知识图谱与GNN。", "method": "提出了一种将RDF知识图谱与GNN全面整合的方法，该方法利用了RDF对象属性的拓扑信息和RDF数据类型属性的内容信息。主要关注点是对各种GNN进行深入评估，分析不同语义特征初始化和图结构异构性类型如何影响其在推荐任务中的性能。", "result": "通过涉及数百万节点RDF图的多个推荐场景的实验表明，利用RDF知识图谱的语义丰富性显著改善了推荐系统。", "conclusion": "利用RDF知识图谱的语义丰富性不仅显著改善了推荐系统，也为面向链接开放数据云的基于GNN的推荐系统奠定了基础。", "translation": "图神经网络（GNNs）极大地推动了推荐系统的发展。然而，尽管在W3C标准RDF下创建了超过一千个知识图谱（KGs），但其丰富的语义信息尚未在基于GNN的推荐系统中得到充分利用。为了解决这一空白，我们提出了一种将RDF知识图谱与GNN全面整合的方法，该方法利用了RDF对象属性的拓扑信息和RDF数据类型属性的内容信息。我们的主要关注点是对各种GNN进行深入评估，分析不同语义特征初始化和图结构异构性类型如何影响其在推荐任务中的性能。通过涉及数百万节点RDF图的多个推荐场景的实验，我们证明了利用RDF知识图谱的语义丰富性显著改善了推荐系统，并为面向链接开放数据云的基于GNN的推荐系统奠定了基础。代码和数据可在我们的GitHub仓库获取：https://github.com/davidlamprecht/rdf-gnn-recommendation", "summary": "本论文提出了一种将RDF知识图谱与图神经网络（GNNs）全面整合的新方法，旨在充分利用RDF知识图谱中丰富的语义信息以改进推荐系统。研究利用RDF对象属性的拓扑信息和RDF数据类型属性的内容信息，并深入评估了不同语义特征初始化和图结构异构性对GNN在推荐任务中性能的影响。实验结果表明，充分利用RDF知识图谱的语义丰富性能够显著提升推荐系统的性能，并为未来基于GNN的链接开放数据云推荐系统奠定了基础。", "keywords": "RDF知识图谱, 图神经网络, 推荐系统, 语义信息, 链接开放数据", "comments": "这项工作通过将RDF知识图谱的语义丰富性引入GNNs，为推荐系统领域带来了创新。它解决了现有GNNs未充分利用RDF语义信息的局限性，并通过全面的实验验证了其有效性。这项研究的重要性在于其为构建更智能、更准确的推荐系统提供了新的视角和方法，尤其是在处理大规模、异构的知识图谱数据方面。为链接开放数据云上的GNN推荐系统奠定了基础，具有重要的应用前景。"}}
{"id": "2506.08670", "title": "sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation", "authors": ["Renjie Xu", "Chong Wu", "Maolin Che", "Zhuoheng Ran", "Yimin Wei", "Hong Yan"], "summary": "We propose sparseGeoHOPCA, a novel framework for sparse higher-order\nprincipal component analysis (SHOPCA) that introduces a geometric perspective\nto high-dimensional tensor decomposition. By unfolding the input tensor along\neach mode and reformulating the resulting subproblems as structured binary\nlinear optimization problems, our method transforms the original nonconvex\nsparse objective into a tractable geometric form. This eliminates the need for\nexplicit covariance estimation and iterative deflation, enabling significant\ngains in both computational efficiency and interpretability, particularly in\nhigh-dimensional and unbalanced data scenarios. We theoretically establish the\nequivalence between the geometric subproblems and the original SHOPCA\nformulation, and derive worst-case approximation error bounds based on\nclassical PCA residuals, providing data-dependent performance guarantees. The\nproposed algorithm achieves a total computational complexity of\n$O\\left(\\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\\right)$, which scales linearly with\ntensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately\nrecovers sparse supports in synthetic settings, preserves classification\nperformance under 10$\\times$ compression, and achieves high-quality image\nreconstruction on ImageNet, highlighting its robustness and versatility.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08670v1", "AI": {"title_translation": "sparseGeoHOPCA: 一种无需协方差估计的稀疏高阶PCA的几何解决方案", "tldr": "sparseGeoHOPCA是一种新的高维张量分解方法，通过几何方法实现稀疏高阶PCA，无需协方差估计，计算效率高且可解释性强。", "motivation": "现有稀疏高阶主成分分析（SHOPCA）方法可能需要显式的协方差估计和迭代降维，这在高维和不平衡数据场景下效率不高且可解释性有限。本文旨在解决这些限制，提供一种更高效且可解释的稀疏高阶PCA方法。", "method": "本文提出了sparseGeoHOPCA框架，引入几何视角处理高维张量分解。该方法通过沿每个模式展开输入张量，并将所得子问题重新表述为结构化二元线性优化问题，从而将原始非凸稀疏目标转化为可处理的几何形式。该方法消除了对显式协方差估计和迭代降维的需求。", "result": "理论上，该方法建立了几何子问题与原始SHOPCA公式的等价性，并基于经典PCA残差推导出最坏情况近似误差界，提供数据依赖的性能保证。所提出的算法总计算复杂度为$O\\left(\\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\\right)$，与张量大小呈线性关系。实验表明，sparseGeoHOPCA在合成设置中准确恢复稀疏支持，在10倍压缩下保持分类性能，并在ImageNet上实现高质量图像重建。", "conclusion": "sparseGeoHOPCA提供了一种高效、可解释且鲁棒的稀疏高阶PCA解决方案，尤其适用于高维和不平衡数据，并在理论和实践中展示了其优越性。", "translation": "我们提出了sparseGeoHOPCA，这是一种用于稀疏高阶主成分分析（SHOPCA）的新颖框架，它为高维张量分解引入了几何视角。通过沿每个模式展开输入张量并将由此产生的子问题重新表述为结构化二元线性优化问题，我们的方法将原始的非凸稀疏目标转化为可处理的几何形式。这消除了对显式协方差估计和迭代降维的需求，从而在计算效率和可解释性方面实现了显著提升，尤其是在高维和不平衡数据场景中。我们从理论上建立了几何子问题与原始SHOPCA公式之间的等价性，并基于经典PCA残差推导出最坏情况近似误差界，提供了数据依赖的性能保证。所提出的算法的总计算复杂度为$O\\left(\\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\\right)$，与张量大小呈线性关系。广泛的实验表明，sparseGeoHOPCA在合成设置中准确恢复稀疏支持，在10倍压缩下保持分类性能，并在ImageNet上实现了高质量图像重建，突出了其鲁棒性和多功能性。", "summary": "本文提出了一种名为sparseGeoHOPCA的新型稀疏高阶主成分分析（SHOPCA）框架。该方法通过引入几何视角，将高维张量分解问题转化为结构化二元线性优化问题，从而避免了协方差估计和迭代降维。sparseGeoHOPCA在计算效率和可解释性方面表现出色，特别适用于高维和不平衡数据。理论分析证明了其与原始SHOPCA的等价性并提供了性能保证，同时计算复杂度与张量大小呈线性关系。实验结果证实了其在稀疏支持恢复、分类性能保持和图像重建方面的鲁棒性和多功能性。", "keywords": "稀疏高阶PCA, 张量分解, 几何优化, 协方差估计, 高维数据", "comments": "sparseGeoHOPCA的创新之处在于引入了几何视角来解决稀疏高阶PCA问题，成功地将非凸优化转化为可处理的几何形式，并消除了传统方法中对协方差估计和迭代降维的需求。这显著提升了算法的计算效率和在高维数据场景下的可解释性，是其重要贡献。该方法在理论上提供了严格的等价性证明和误差界，并通过在多个任务上的出色表现验证了其有效性和通用性。"}}
{"id": "2506.08996", "title": "Navigating Cookie Consent Violations Across the Globe", "authors": ["Brian Tang", "Duc Bui", "Kang G. Shin"], "summary": "Online services provide users with cookie banners to accept/reject the\ncookies placed on their web browsers. Despite the increased adoption of cookie\nbanners, little has been done to ensure that cookie consent is compliant with\nprivacy laws around the globe. Prior studies have found that cookies are often\nplaced on browsers even after their explicit rejection by users. These\ninconsistencies in cookie banner behavior circumvent users' consent preferences\nand are known as cookie consent violations. To address this important problem,\nwe propose an end-to-end system, called ConsentChk, that detects and analyzes\ncookie banner behavior. ConsentChk uses a formal model to systematically detect\nand categorize cookie consent violations. We investigate eight English-speaking\nregions across the world, and analyze cookie banner behavior across 1,793\nglobally-popular websites. Cookie behavior, cookie consent violation rates, and\ncookie banner implementations are found to be highly dependent on region. Our\nevaluation reveals that consent management platforms (CMPs) and website\ndevelopers likely tailor cookie banner configurations based on their (often\nincorrect) interpretations of regional privacy laws. We discuss various root\ncauses behind these cookie consent violations. The resulting implementations\nproduce misleading cookie banners, indicating the prevalence of inconsistently\nimplemented and enforced cookie consent between various regions.", "comment": "Published at 34th USENIX Security Symposium (2025)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.08996v1", "AI": {"title_translation": "全球范围内的Cookie同意违规行为导航", "tldr": "尽管有Cookie横幅，但网站普遍存在Cookie同意违规，且违规行为因地区而异；研究提出了ConsentChk系统来检测和分析这些违规。", "motivation": "尽管Cookie横幅被广泛采用，但未能确保Cookie同意符合全球隐私法，且用户明确拒绝后仍常被放置Cookie，这导致了Cookie同意违规。", "method": "提出了端到端系统ConsentChk，它使用形式化模型系统地检测和分类Cookie同意违规。研究调查了全球八个英语区域的1,793个流行网站的Cookie横幅行为。", "result": "发现Cookie行为、Cookie同意违规率和Cookie横幅实现高度依赖于地区。评估显示，同意管理平台（CMPs）和网站开发者可能基于对区域隐私法（常有误）的解释来定制Cookie横幅配置。", "conclusion": "最终的实现产生了误导性的Cookie横幅，表明不同地区之间Cookie同意的实施和执行普遍存在不一致性。", "translation": "在线服务为用户提供Cookie横幅，以接受/拒绝放置在其网络浏览器上的Cookie。尽管Cookie横幅的使用日益增多，但为确保Cookie同意符合全球隐私法所做的工作却很少。先前的研究发现，即使在用户明确拒绝后，Cookie也常常被放置在浏览器上。Cookie横幅行为中的这些不一致性规避了用户的同意偏好，并被称为Cookie同意违规。为了解决这个重要问题，我们提出了一个端到端系统，名为ConsentChk，用于检测和分析Cookie横幅行为。ConsentChk使用形式化模型系统地检测和分类Cookie同意违规。我们调查了全球八个英语区域，并分析了1,793个全球流行网站的Cookie横幅行为。结果发现，Cookie行为、Cookie同意违规率和Cookie横幅实现高度依赖于地区。我们的评估显示，同意管理平台（CMPs）和网站开发者可能根据他们对区域隐私法（通常是错误的）的解释来定制Cookie横幅配置。我们讨论了这些Cookie同意违规背后的各种根本原因。由此产生的实现会产生误导性的Cookie横幅，这表明不同地区之间Cookie同意的实施和执行普遍存在不一致性。", "summary": "本文提出了一种名为ConsentChk的端到端系统，旨在检测和分析网站上的Cookie同意违规行为。研究发现，尽管Cookie横幅广泛存在，但许多网站未能遵守全球隐私法，即使在用户明确拒绝后仍放置Cookie。通过对全球八个英语区域1,793个网站的分析，揭示了Cookie行为和违规率的地域依赖性，并指出同意管理平台和开发者常因对区域法律的错误理解而导致误导性Cookie横幅和不一致的实施。", "keywords": "Cookie同意违规, 隐私法, Cookie横幅, ConsentChk, 地域差异", "comments": "该论文通过提出ConsentChk系统，提供了一种系统性的方法来检测和分类Cookie同意违规，填补了现有研究的空白。其对全球范围内大量网站的实证分析揭示了Cookie同意实施的地域差异和普遍存在的违规问题，强调了隐私法合规性在实践中的挑战，对隐私保护和Web开发具有重要意义。"}}
{"id": "2506.08860", "title": "On The Impact of Merge Request Deviations on Code Review Practices", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "summary": "Code review is a key practice in software engineering, ensuring quality and\ncollaboration. However, industrial Merge Request (MR) workflows often deviate\nfrom standardized review processes, with many MRs serving non-review purposes\n(e.g., drafts, rebases, or dependency updates). We term these cases deviations\nand hypothesize that ignoring them biases analytics and undermines ML models\nfor review analysis.\n  We identify seven deviation categories, occurring in 37.02% of MRs, and\npropose a few-shot learning detection method (91% accuracy). By excluding\ndeviations, ML models predicting review completion time improve performance in\n53.33% of cases (up to 2.25x) and exhibit significant shifts in feature\nimportance (47% overall, 60% top-*k*).\n  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven\ndetection approach, and (3) empirical evidence of their impact on ML-based\nreview analytics. This work aids practitioners in optimizing review efforts and\nensuring reliable insights.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08860v1", "AI": {"title_translation": "合并请求偏差对代码审查实践的影响", "tldr": "本研究识别了代码审查中合并请求的偏差，并提出了一种少样本学习检测方法，发现排除这些偏差可以显著提高代码审查分析中机器学习模型的性能和特征重要性。", "motivation": "工业合并请求(MR)工作流经常偏离标准化的审查流程，许多MR用于非审查目的。研究者假设忽略这些偏差会偏倚分析结果并损害用于审查分析的机器学习模型。", "method": "研究者识别了七种合并请求偏差类别，并提出了一种准确率为91%的少样本学习检测方法。通过排除这些偏差，他们评估了机器学习模型在预测审查完成时间方面的性能改进。", "result": "研究发现37.02%的合并请求存在偏差。排除偏差后，预测审查完成时间的机器学习模型在53.33%的案例中性能得到提升（最高达2.25倍），并且特征重要性发生了显著变化（总体47%，top-k特征60%）。", "conclusion": "本研究的贡献包括：(1) 合并请求偏差的分类法，(2) 一种AI驱动的检测方法，以及 (3) 它们对基于机器学习的审查分析影响的实证证据。这项工作有助于实践者优化审查工作并确保可靠的洞察。", "translation": "代码审查是软件工程中的一项关键实践，它能确保质量和协作。然而，工业合并请求（MR）工作流经常偏离标准化的审查流程，许多MR用于非审查目的（例如，草稿、rebase或依赖更新）。我们将这些情况称为偏差，并假设忽略它们会偏倚分析结果并损害用于审查分析的机器学习模型。\n我们识别了七种偏差类别，它们出现在37.02%的MR中，并提出了一种少样本学习检测方法（准确率为91%）。通过排除偏差，预测审查完成时间的机器学习模型在53.33%的案例中性能得到提升（最高达2.25倍），并且特征重要性发生了显著变化（总体47%，top-k特征60%）。\n我们的贡献包括：(1) MR偏差的分类法，(2) 一种AI驱动的检测方法，以及 (3) 它们对基于ML的审查分析影响的实证证据。这项工作有助于实践者优化审查工作并确保可靠的洞察。", "summary": "本论文探讨了工业合并请求（MR）工作流中偏离标准代码审查实践的偏差，并假设这些偏差会影响分析和机器学习模型。研究识别了七种偏差类别，发现它们在MR中普遍存在（37.02%），并开发了一种准确率为91%的少样本学习检测方法。通过排除这些偏差，机器学习模型在预测代码审查完成时间方面显示出显著的性能提升（最高2.25倍）和特征重要性变化。这项工作为优化代码审查实践和提高数据分析可靠性提供了分类法、AI检测工具和实证证据。", "keywords": "代码审查, 合并请求, 偏差检测, 机器学习, 少样本学习", "comments": "这项研究通过识别和量化代码审查中常见的“非审查”合并请求偏差，填补了一个重要的空白。其创新之处在于提出了一个实用的偏差分类法，并开发了一种基于少样本学习的有效检测方法。该研究的实证结果有力地证明了忽略这些偏差对机器学习模型性能和特征重要性的负面影响。这对于依赖数据驱动决策的软件工程实践者来说具有重要意义，有助于他们获得更准确的分析结果和更可靠的模型洞察。"}}
{"id": "2506.08708", "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08708v1", "AI": {"title_translation": "PhyBlock：一个通过3D积木组装进行物理理解和规划的渐进式基准", "tldr": "PhyBlock是一个新的基准，通过3D积木组装任务评估视觉-语言模型（VLMs）的物理理解和规划能力，发现VLMs在复杂空间推理和高层规划方面存在显著局限性。", "motivation": "尽管视觉-语言模型（VLMs）在具身智能体的推理和规划方面展现出有前景的能力，但它们在理解物理现象，特别是在结构化3D环境中，其能力仍然受到严重限制，因此需要弥补这一差距。", "method": "引入了PhyBlock，一个渐进式基准，旨在通过机器人3D积木组装任务来评估VLMs的物理理解和规划能力。PhyBlock整合了新颖的四级认知层次组装任务以及有针对性的视觉问答（VQA）样本，共同旨在评估渐进式空间推理和基本物理理解，包括物体属性、空间关系和整体场景理解。PhyBlock包含2600个积木任务（400个组装任务，2200个VQA任务），并从部分完成、故障诊断和规划鲁棒性三个关键维度评估模型。对21个最先进的VLM进行了基准测试。", "result": "VLMs的性能在高层规划和推理能力方面表现出明显的局限性，导致随着任务复杂性增加，性能显著下降。错误分析揭示了空间定位和依赖推理方面的持续困难。链式思考提示（chain-of-thought prompting）提供的改进微乎其微，表明空间任务严重依赖模型的直观理解。", "conclusion": "PhyBlock被定位为一个统一的测试平台，旨在推动具身推理，弥合视觉-语言理解与现实世界物理问题解决之间的鸿沟。", "translation": "尽管视觉-语言模型（VLMs）在具身智能体的推理和规划方面展现出有前景的能力，但它们在理解物理现象，尤其是在结构化3D环境中，其能力仍然受到严重限制。为了弥补这一差距，我们引入了PhyBlock，一个渐进式基准，旨在通过机器人3D积木组装任务来评估VLMs的物理理解和规划能力。PhyBlock整合了一种新颖的四级认知层次组装任务以及有针对性的视觉问答（VQA）样本，共同旨在评估渐进式空间推理和基本物理理解，包括物体属性、空间关系和整体场景理解。PhyBlock包含2600个积木任务（400个组装任务，2200个VQA任务），并从三个关键维度评估模型：部分完成、故障诊断和规划鲁棒性。我们对21个最先进的VLM进行了基准测试，强调了它们在基于物理的多步规划中的优势和局限性。我们的实证结果表明，VLMs的性能在高层规划和推理能力方面表现出明显的局限性，导致随着任务复杂性的增长，性能显著下降。错误分析揭示了空间定位和依赖推理方面的持续困难。令人惊讶的是，链式思考提示（chain-of-thought prompting）提供的改进微乎其微，这表明空间任务严重依赖模型的直观理解。我们将PhyBlock定位为一个统一的测试平台，以推进具身推理，弥合视觉-语言理解与现实世界物理问题解决之间的鸿沟。", "summary": "本文介绍了PhyBlock，一个新颖的基准，旨在通过3D积木组装任务评估视觉-语言模型（VLMs）的物理理解和规划能力。该基准包含组装和VQA任务，并从多维度评估VLMs。研究发现，尽管VLMs在具身智能体方面表现出潜力，但在高层规划和复杂空间推理上存在显著局限性，且链式思考提示效果不佳，凸显了直观理解的重要性。PhyBlock有望成为促进具身推理和现实世界物理问题解决的统一测试平台。", "keywords": "物理理解, 3D积木组装, 视觉-语言模型, 具身推理, 基准", "comments": "PhyBlock通过引入四级认知层次任务和VQA样本，提供了一个系统化的框架来评估VLMs在复杂物理理解和规划方面的能力，其创新性在于为具身智能体领域提供了一个急需的、结构化的基准。研究结果清晰地揭示了当前SOTA VLMs在处理复杂空间推理和多步规划时的局限性，特别是链式思考提示效果不佳的发现，对未来模型设计具有重要指导意义。这对于推动具身推理和弥合视觉-语言理解与实际物理问题解决之间的差距至关重要。"}}
{"id": "2506.08279", "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage", "authors": ["Aditi Sundararaman", "Amogh Adishesha", "Andrew Jaegle", "Dan Bigioi", "Hyoung-Kyu Song", "Jon Kyl", "Justin Mao", "Kevin Lan", "Mojtaba Komeili", "ShahRukh Athar", "Sheila Babayan", "Stanislau Beliasau", "William Buchwalter"], "summary": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links).", "comment": "Technical report website: mirage.app/research/seeing-voices, product\n  website: mirage.app", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08279v1", "AI": {"title_translation": "看见声音：使用Mirage从音频生成A-Roll视频", "tldr": "Mirage是一个音频到视频的基础模型，可以根据音频输入生成逼真、富有表现力的A-Roll视频，并可与文本到语音结合使用。", "motivation": "当前视频生成方法要么忽略声音，要么专注于受限的应用领域（如重新配音），而视频的强大之处在于听觉与视觉的和谐结合。因此，需要一个能从音频生成高质量视频的通用模型。", "method": "本文提出了Mirage，一个基于自注意力机制的音频到视频生成基础模型。该模型可以从头开始训练，或在现有权重的基础上进行训练。它采用统一的方法，使其在保持通用性的同时，生成的主观质量优于结合特定音频架构或损失组件的方法。", "result": "Mirage能够根据音频输入生成逼真、富有表现力的输出图像。当在包含语音的人员A-roll视频上进行训练并以包含语音的音频为条件时，Mirage可以生成人们传达输入音频中隐含表演的视频。与文本到语音（TTS）结合时，Mirage能产生引人注目的多模态视频。", "conclusion": "Mirage提供了一种统一且通用的音频到视频生成方法，能够生成高质量的输出，并且在主观质量上优于现有方法，从而实现了声音与视觉的和谐整合。", "translation": "从专业电影制作到用户生成内容，创作者和消费者早已认识到视频的力量取决于我们所听（视频的音轨）与所看（视频的图像序列）的和谐整合。当前的视频生成方法要么忽略声音以专注于通用但无声的图像序列生成，要么同时处理视觉和音频元素，但专注于受限的应用领域，例如重新配音。我们引入了Mirage，一个音频到视频的基础模型，它擅长根据音频输入从头开始生成逼真、富有表现力的输出图像。当与现有的语音合成方法（文本到语音，即TTS）结合时，Mirage能产生引人注目的多模态视频。当在人们说话（A-roll）的音视频素材上进行训练，并以包含语音的音频为条件时，Mirage能生成人们对输入音频中隐含表演进行可信演绎的视频。我们的核心技术贡献是训练基于自注意力机制的音频到视频生成模型的统一方法，无论是从头开始还是给定现有权重。这种方法使得Mirage在音频到视频生成方面保持通用性，同时产生比那些结合特定音频架构或特定于人物、语音或图像或音频捕获细节的损失组件的方法更优越的主观质量输出。我们鼓励读者亲自观看和聆听Mirage的结果（请参阅论文和评论以获取链接）。", "summary": "该论文介绍了Mirage，一个创新的音频到视频基础模型，旨在解决当前视频生成中音频与视觉整合不足的问题。Mirage能够根据音频输入生成逼真且富有表现力的A-Roll视频，尤其擅长从语音音频生成人物讲话的视频。其核心贡献在于提出了一种统一的自注意力训练方法，使得模型在保持通用性的同时，生成的主观质量优于现有方法。Mirage与文本到语音技术结合时，能产生引人注目的多模态视频。", "keywords": "音频到视频生成, A-roll视频, 基础模型, 自注意力, 多模态视频", "comments": "Mirage的创新之处在于其作为音频到视频基础模型的通用性和高质量输出。它通过统一的自注意力训练方法，克服了现有方法在音频-视觉整合方面的局限性，特别是在生成逼真人物讲话视频方面的表现突出。这对于电影制作和用户生成内容领域都具有重要意义，因为它为从音频驱动视频内容创建开辟了新途径。"}}
{"id": "2506.08446", "title": "A Survey on Large Language Models for Mathematical Reasoning", "authors": ["Peng-Yuan Wang", "Tian-Shuo Liu", "Chenyang Wang", "Yi-Di Wang", "Shu Yan", "Cheng-Xing Jia", "Xu-Hui Liu", "Xin-Wei Chen", "Jia-Cheng Xu", "Ziniu Li", "Yang Yu"], "summary": "Mathematical reasoning has long represented one of the most fundamental and\nchallenging frontiers in artificial intelligence research. In recent years,\nlarge language models (LLMs) have achieved significant advances in this area.\nThis survey examines the development of mathematical reasoning abilities in\nLLMs through two high-level cognitive phases: comprehension, where models gain\nmathematical understanding via diverse pretraining strategies, and answer\ngeneration, which has progressed from direct prediction to step-by-step\nChain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical\nreasoning, ranging from training-free prompting to fine-tuning approaches such\nas supervised fine-tuning and reinforcement learning, and discuss recent work\non extended CoT and \"test-time scaling\". Despite notable progress, fundamental\nchallenges remain in terms of capacity, efficiency, and generalization. To\naddress these issues, we highlight promising research directions, including\nadvanced pretraining and knowledge augmentation techniques, formal reasoning\nframeworks, and meta-generalization through principled learning paradigms. This\nsurvey tries to provide some insights for researchers interested in enhancing\nreasoning capabilities of LLMs and for those seeking to apply these techniques\nto other domains.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08446v1", "AI": {"title_translation": "大型语言模型在数学推理方面的综述", "tldr": "本文综述了大型语言模型在数学推理方面的发展，包括理解和答案生成阶段，提升方法以及面临的挑战和未来方向。", "motivation": "数学推理是人工智能研究中一个基础且具挑战性的前沿领域，大型语言模型（LLMs）在此领域取得了显著进展，因此有必要对LLMs的数学推理能力进行全面审查。", "method": "本综述从理解和答案生成两个高级认知阶段审视了LLMs数学推理能力的发展，回顾了从无训练提示到微调（如监督微调和强化学习）的增强数学推理方法，并讨论了扩展思维链（CoT）和“测试时缩放”的最新工作。", "result": "大型语言模型在数学推理方面取得了显著进展，但仍面临容量、效率和泛化等方面的基本挑战。", "conclusion": "尽管取得了显著进展，但在容量、效率和泛化方面仍存在基本挑战。未来的研究方向包括高级预训练、知识增强技术、形式化推理框架和通过原则性学习范式实现的元泛化。本综述旨在为相关研究人员提供见解。", "translation": "数学推理长期以来一直是人工智能研究中最基础和最具挑战性的前沿领域之一。近年来，大型语言模型（LLMs）在该领域取得了显著进展。本综述通过两个高级认知阶段审视了LLMs数学推理能力的发展：理解阶段，模型通过多样化的预训练策略获得数学理解；以及答案生成阶段，该阶段从直接预测发展到逐步的思维链（CoT）推理。我们回顾了增强数学推理的方法，从无需训练的提示到微调方法，如监督微调和强化学习，并讨论了关于扩展CoT和“测试时缩放”的最新工作。尽管取得了显著进展，但在容量、效率和泛化方面仍存在基本挑战。为了解决这些问题，我们强调了有前景的研究方向，包括高级预训练和知识增强技术、形式化推理框架以及通过原则性学习范式实现的元泛化。本综述旨在为有兴趣增强LLMs推理能力的研究人员以及寻求将这些技术应用于其他领域的研究人员提供一些见解。", "summary": "本综述探讨了大型语言模型（LLMs）在数学推理领域的发展。它从理解和答案生成两个认知阶段分析了LLMs的数学能力，并回顾了从提示工程到微调等多种增强方法。尽管取得了显著进步，但LLMs在容量、效率和泛化方面仍面临挑战。文章最后提出了未来的研究方向，以期克服这些限制。", "keywords": "大型语言模型, 数学推理, 思维链, 预训练, 微调", "comments": "这篇综述系统地梳理了LLM在数学推理领域的发展现状、关键技术和面临的挑战，并指明了未来研究方向，对于该领域的研究人员具有重要的参考价值。其创新点在于将LLM的数学推理能力划分为理解和答案生成两个认知阶段进行分析，并全面总结了提升方法。"}}
{"id": "2506.08139", "title": "Nearness of Neighbors Attention for Regression in Supervised Finetuning", "authors": ["Aviad Susman", "Mayte Suárez-Fariñas", "Joseph T Colonel"], "summary": "It is common in supervised machine learning to combine the feature extraction\ncapabilities of neural networks with the predictive power of traditional\nalgorithms, such as k-nearest neighbors (k-NN) or support vector machines. This\nprocedure involves performing supervised fine-tuning (SFT) on a\ndomain-appropriate feature extractor, followed by training a traditional\npredictor on the resulting SFT embeddings. When used in this manner,\ntraditional predictors often deliver increased performance over the SFT model\nitself, despite the fine-tuned feature extractor yielding embeddings\nspecifically optimized for prediction by the neural network's final dense\nlayer. This suggests that directly incorporating traditional algorithms into\nSFT as prediction layers may further improve performance. However, many\ntraditional algorithms have not been implemented as neural network layers due\nto their non-differentiable nature and their unique optimization requirements.\nAs a step towards solving this problem, we introduce the Nearness of Neighbors\nAttention (NONA) regression layer. NONA uses the mechanics of neural network\nattention and a novel learned attention-masking scheme to yield a\ndifferentiable proxy of the k-NN regression algorithm. Results on multiple\nunstructured datasets show improved performance over both dense layer\nprediction and k-NN on SFT embeddings for regression.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08139v1", "AI": {"title_translation": "监督微调中用于回归的近邻注意力", "tldr": "引入了一种名为近邻注意力（NONA）的新型神经网络层，它作为k-NN回归算法的可微分代理，在监督微调中提高了回归性能。", "motivation": "在监督机器学习中，神经网络的特征提取能力与传统算法（如k-NN）结合使用时，传统预测器通常比监督微调（SFT）模型本身表现更好。这表明将传统算法直接整合到SFT中作为预测层可能进一步提高性能，但由于其不可微性和独特的优化要求，许多传统算法尚未作为神经网络层实现。", "method": "本文引入了近邻注意力（NONA）回归层。NONA利用神经网络注意力的机制和一种新颖的学习注意力掩蔽方案，以生成k-NN回归算法的可微分代理。", "result": "在多个非结构化数据集上的结果显示，与密集层预测和SFT嵌入上的k-NN相比，NONA在回归任务上表现出更好的性能。", "conclusion": "近邻注意力（NONA）层成功地将k-NN回归算法的可微分代理整合到神经网络中，并在监督微调中实现了优于传统方法的回归性能提升。", "translation": "在监督机器学习中，将神经网络的特征提取能力与传统算法（如k-NN或支持向量机）的预测能力相结合是很常见的。此过程涉及对领域合适的特征提取器进行监督微调（SFT），然后在使用SFT嵌入训练传统预测器。尽管微调后的特征提取器产生的嵌入是专门为神经网络最终密集层的预测而优化的，但以这种方式使用时，传统预测器通常比SFT模型本身提供更高的性能。这表明将传统算法直接整合到SFT中作为预测层可能进一步提高性能。然而，由于其不可微性和独特的优化要求，许多传统算法尚未作为神经网络层实现。为了解决这个问题，我们引入了近邻注意力（NONA）回归层。NONA利用神经网络注意力的机制和一种新颖的学习注意力掩蔽方案，以生成k-NN回归算法的可微分代理。在多个非结构化数据集上的结果显示，与密集层预测和SFT嵌入上的k-NN相比，NONA在回归任务上表现出更好的性能。", "summary": "本文提出了一种名为近邻注意力（NONA）的新型神经网络回归层，旨在解决传统算法（如k-NN）虽在监督微调（SFT）后表现优异但难以直接整合到神经网络中的问题。NONA通过结合神经网络注意力机制和学习注意力掩蔽方案，成功地创建了一个k-NN回归算法的可微分代理。实验结果表明，在多个非结构化数据集上，NONA在回归任务上的表现优于传统的密集层预测和SFT嵌入上的k-NN。", "keywords": "近邻注意力, k-NN回归, 监督微调, 神经网络层, 注意力机制", "comments": "这项工作具有创新性，因为它成功地将传统上难以集成到神经网络中的非可微算法（如k-NN）转化为可微分的层。NONA层提供了一种将传统算法的优势直接融入深度学习模型的新途径，可能为未来的混合模型设计开辟新的研究方向。其重要性在于，它可能使模型在保持神经网络强大特征提取能力的同时，利用传统算法的预测优势，从而提高整体性能。"}}
{"id": "2506.08899", "title": "From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis", "authors": ["Elias Horner", "Cristinel Mateis", "Guido Governatori", "Agata Ciabattoni"], "summary": "We present a novel approach to the automated semantic analysis of legal texts\nusing large language models (LLMs), targeting their transformation into formal\nrepresentations in Defeasible Deontic Logic (DDL). We propose a structured\npipeline that segments complex normative language into atomic snippets,\nextracts deontic rules, and evaluates them for syntactic and semantic\ncoherence. Our methodology is evaluated across various LLM configurations,\nincluding prompt engineering strategies, fine-tuned models, and multi-stage\npipelines, focusing on legal norms from the Australian Telecommunications\nConsumer Protections Code. Empirical results demonstrate promising alignment\nbetween machine-generated and expert-crafted formalizations, showing that LLMs\n- particularly when prompted effectively - can significantly contribute to\nscalable legal informatics.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08899v1", "AI": {"title_translation": "从法律文本到可废止道义逻辑通过大型语言模型：一项自动化语义分析研究", "tldr": "本研究提出一种利用大型语言模型（LLMs）将法律文本自动转换为可废止道义逻辑（DDL）形式化表示的新方法，并展示了其在法律规范自动化分析中的潜力。", "motivation": "现有法律文本的语义分析自动化程度不高，需要一种将复杂法律规范转换为形式化表示的方法，以实现可扩展的法律信息学。", "method": "提出一个结构化管道，该管道将复杂的规范语言分割成原子片段，提取道义规则，并评估其句法和语义连贯性。该方法通过不同的LLM配置（包括提示工程策略、微调模型和多阶段管道）进行评估。", "result": "经验结果表明，机器生成的形式化与专家制作的形式化之间具有良好的一致性，表明LLMs（尤其是在有效提示下）可以显著促进可扩展的法律信息学。", "conclusion": "大型语言模型能够有效地将法律文本转换为可废止道义逻辑的正式表示，从而显著推动法律信息学的自动化和可扩展性。", "translation": "我们提出了一种使用大型语言模型（LLMs）对法律文本进行自动化语义分析的新方法，旨在将其转换为可废止道义逻辑（DDL）中的形式化表示。我们提出了一个结构化管道，该管道将复杂的规范语言分割成原子片段，提取道义规则，并评估其句法和语义连贯性。我们的方法在各种LLM配置下进行了评估，包括提示工程策略、微调模型和多阶段管道，重点关注澳大利亚电信消费者保护法规中的法律规范。经验结果表明，机器生成的形式化与专家制作的形式化之间具有令人鼓舞的一致性，表明LLMs——特别是在有效提示下——可以显著促进可扩展的法律信息学。", "summary": "本文介绍了一种利用大型语言模型（LLMs）对法律文本进行自动化语义分析的创新方法，旨在将其转换为可废止道义逻辑（DDL）的正式表示。研究构建了一个结构化流程，用于分解法律规范、提取道义规则并验证其一致性。通过对不同LLM配置的评估，包括提示工程和微调，结果显示LLMs在将法律文本形式化方面展现出与专家水平高度一致的潜力，预示着其在可扩展法律信息学领域的巨大应用前景。", "keywords": "法律文本, 大型语言模型, 可废止道义逻辑, 语义分析, 法律信息学", "comments": "这项研究通过将LLMs应用于法律文本的形式化分析，为法律信息学领域带来了创新。其提出的结构化管道和对不同LLM配置的评估，为自动化法律语义分析提供了有价值的见解，有望显著提高法律规范处理的效率和可扩展性。"}}
{"id": "2506.08462", "title": "Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing", "authors": ["Christos Margadji", "Sebastian W. Pattinson"], "summary": "Industrial processes must be robust and adaptable, as environments and tasks\nare often unpredictable, while operational errors remain costly and difficult\nto detect. AI-based control systems offer a path forward, yet typically depend\non supervised learning with extensive labelled datasets, which limits their\nability to generalize across variable and data-scarce industrial settings.\nFoundation models could enable broader reasoning and knowledge integration, but\nrarely deliver the quantitative precision demanded by engineering applications.\nHere, we introduceControl and Interpretation of Production via Hybrid Expertise\nand Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming\nto replicate human-like reasoning for industrial control, instantiated in a\ncommercial-grade 3D printer. It integrates a process expert, a regression model\nenabling quantitative characterization of system states required for\nengineering tasks. CIPHER also incorporates retrieval-augmented generation to\naccess external expert knowledge and support physics-informed, chain-of-thought\nreasoning. This hybrid architecture exhibits strong generalization to\nout-of-distribution tasks. It interprets visual or textual inputs from process\nmonitoring, explains its decisions, and autonomously generates precise machine\ninstructions, without requiring explicit annotations. CIPHER thus lays the\nfoundations for autonomous systems that act with precision, reason with\ncontext, and communicate decisions transparently, supporting safe and trusted\ndeployment in industrial settings.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08462v1", "AI": {"title_translation": "制造业中用于感知、解释和自主行动的混合推理", "tldr": "提出CIPHER，一个混合推理VLA模型框架，用于工业控制，具有强大的泛化能力，无需显式标注。", "motivation": "工业过程需要鲁棒性和适应性，但环境和任务不可预测，操作错误代价高昂且难以检测。现有AI控制系统依赖大量标注数据，泛化能力有限。基础模型缺乏工程应用所需的定量精度。", "method": "引入CIPHER（Control and Interpretation of Production via Hybrid Expertise and Reasoning）：一个视觉-语言-动作（VLA）模型框架。它整合了过程专家、实现系统状态定量表征的回归模型，并结合检索增强生成以访问外部专家知识和支持物理信息、思维链推理。", "result": "该混合架构对分布外任务表现出强大的泛化能力。它能解释来自过程监控的视觉或文本输入，解释其决策，并自主生成精确的机器指令，无需显式标注。", "conclusion": "CIPHER为自主系统奠定了基础，使其能够精确行动、结合上下文推理并透明地沟通决策，支持在工业环境中的安全可靠部署。", "translation": "工业过程必须稳健且适应性强，因为环境和任务通常不可预测，而操作错误代价高昂且难以检测。基于AI的控制系统提供了一条前进的道路，但通常依赖于有大量标注数据的监督学习，这限制了它们在多变和数据稀缺的工业环境中泛化的能力。基础模型可以实现更广泛的推理和知识整合，但很少能提供工程应用所需的定量精度。\n在此，我们介绍了通过混合专业知识和推理进行生产控制和解释（CIPHER）：一个旨在复制人类在工业控制中推理能力的视觉-语言-动作（VLA）模型框架，并在一台商用级3D打印机中进行了实例化。它整合了一个过程专家和一个回归模型，该模型能够对工程任务所需的系统状态进行定量表征。CIPHER还结合了检索增强生成来访问外部专家知识，并支持物理信息化的思维链推理。这种混合架构对分布外任务表现出强大的泛化能力。它能够解释来自过程监控的视觉或文本输入，解释其决策，并自主生成精确的机器指令，而无需显式标注。因此，CIPHER为自主系统奠定了基础，使其能够精确行动、结合上下文推理并透明地沟通决策，支持在工业环境中的安全和可信部署。", "summary": "本文提出了CIPHER，一个用于工业控制的视觉-语言-动作（VLA）混合推理框架。它通过结合过程专家、回归模型和检索增强生成，解决了传统AI系统在数据稀缺和泛化能力方面的局限性以及基础模型缺乏定量精度的问题。CIPHER无需显式标注即可解释输入、解释决策并生成精确指令，在商用3D打印机上展示了强大的泛化能力，为工业自主系统的安全部署奠定了基础。", "keywords": "混合推理, 工业控制, 视觉-语言-动作模型, 泛化能力, 自主系统", "comments": "这篇论文的创新点在于其混合推理架构，结合了符号推理、机器学习和检索增强生成，以解决工业控制中鲁棒性、泛化能力和定量精度的挑战。通过模拟人类的推理过程，CIPHER有望在数据稀缺的工业场景中实现高效、透明且可信赖的自主控制。其在商用3D打印机上的实例化展示了其在实际应用中的潜力。"}}
{"id": "2506.08433", "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "authors": ["Hernán Maina", "Nicolás Wolovick", "Luciana Benotti"], "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precisions and data parallelization strategies impacts both training\nspeed (as a proxy to energy and hardware consumption) and model accuracy, with\nthe goal of facilitating domain adaptation in low-resource environments. Our\nfindings are relevant to any setting where energy efficiency, accessibility, or\nlimited hardware availability are key concerns.", "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08433v1", "AI": {"title_translation": "低资源域适应，同时最大限度地减少能源和硬件资源消耗", "tldr": "本文评估了不同数值精度和数据并行策略对域适应训练速度和模型准确性的影响，旨在促进低资源环境下的域适应。", "motivation": "训练大型语言模型（LLMs）在能源、硬件和标注数据方面成本高昂，且模型常受主流文化和价值观影响。尽管域适应是解决模型文化对齐问题的有效策略，但其计算成本高昂，成为低资源研究团队的障碍。", "method": "本文通过评估不同数值精度和数据并行策略如何影响训练速度（作为能源和硬件消耗的替代指标）和模型准确性，以期在低资源环境下促进域适应。", "result": "抽象中未提及具体结果，但指出研究发现与关注能源效率、可访问性或有限硬件可用性的任何场景都相关。", "conclusion": "本文的发现对于关注能源效率、可访问性或硬件资源有限的场景具有重要意义，有助于在低资源环境下进行域适应。", "translation": "训练大型语言模型（LLMs）在能源、硬件和标注数据方面成本高昂，通常会导致模型立场根植于主流文化和价值观（Santy et al., 2023）。域适应已成为一种有前景的策略，可以更好地使模型与不同的文化和价值语境对齐（Hershcovich et al., 2022），但其计算成本仍然是一个显著障碍，特别是对于缺乏大型基础设施的研究团队。在本文中，我们评估了不同数值精度和数据并行化策略的使用如何影响训练速度（作为能源和硬件消耗的替代指标）和模型准确性，旨在促进低资源环境下的域适应。我们的发现与任何关注能源效率、可访问性或有限硬件可用性的场景都相关。", "summary": "大型语言模型训练成本高昂且存在文化偏见，域适应虽能改善对齐但计算成本仍是瓶颈。本文旨在解决低资源环境下的域适应问题，通过评估不同数值精度和数据并行策略对训练速度（代表能耗和硬件消耗）和模型准确性的影响。研究结果对能源效率、可访问性和硬件受限环境具有普遍适用性。", "keywords": "域适应, 低资源, 大型语言模型, 能源效率, 硬件消耗", "comments": "这篇论文的创新点在于它试图解决LLM域适应过程中的实际资源限制问题，特别是能源和硬件消耗。它通过探索数值精度和数据并行化策略，为低资源环境下的研究团队提供了实用的解决方案，这对于推动LLM的普惠性和可持续发展具有重要意义。"}}
{"id": "2506.08418", "title": "RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation", "authors": ["Taiqin Chen", "Zikun Zhou", "Zheng Fang", "Wenzhen Zou", "Kanjun Liu", "Ke Chen", "Yongbing Zhang", "Yaowei Wang"], "summary": "The radio map represents the spatial distribution of spectrum resources\nwithin a region, supporting efficient resource allocation and interference\nmitigation. However, it is difficult to construct a dense radio map as a\nlimited number of samples can be measured in practical scenarios. While\nexisting works have used deep learning to estimate dense radio maps from sparse\nsamples, they are hard to integrate with the physical characteristics of the\nradio map. To address this challenge, we cast radio map estimation as the\nsparse signal recovery problem. A physical propagation model is further\nincorporated to decompose the problem into multiple factor optimization\nsub-problems, thereby reducing recovery complexity. Inspired by the existing\ncompressive sensing methods, we propose the Radio Deep Unfolding Network\n(RadioDUN) to unfold the optimization process, achieving adaptive parameter\nadjusting and prior fitting in a learnable manner. To account for the radio\npropagation characteristics, we develop a dynamic reweighting module (DRM) to\nadaptively model the importance of each factor for the radio map. Inspired by\nthe shadowing factor in the physical propagation model, we integrate\nobstacle-related factors to express the obstacle-induced signal stochastic\ndecay. The shadowing loss is further designed to constrain the factor\nprediction and act as a supplementary supervised objective, which enhances the\nperformance of RadioDUN. Extensive experiments have been conducted to\ndemonstrate that the proposed method outperforms the state-of-the-art methods.\nOur code will be made publicly available upon publication.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08418v1", "AI": {"title_translation": "RadioDUN: 一种物理启发的深度展开网络用于无线电图估计", "tldr": "RadioDUN是一个物理启发的深度展开网络，通过将无线电图估计视为稀疏信号恢复问题，并结合物理传播模型和动态重加权模块，从稀疏样本中估计密集无线电图，性能优于现有方法。", "motivation": "难以从有限样本构建密集无线电图；现有深度学习方法难以整合无线电图的物理特性。", "method": "将无线电图估计视为稀疏信号恢复问题；引入物理传播模型将问题分解为多个因子优化子问题；提出RadioDUN网络展开优化过程，实现自适应参数调整和先验拟合；开发动态重加权模块(DRM)自适应建模每个因子对无线电图的重要性；整合障碍相关因子表达障碍引起的信号随机衰减；设计阴影损失约束因子预测并作为辅助监督目标。", "result": "所提出的方法优于现有最先进的方法。", "conclusion": "RadioDUN能够有效且准确地从稀疏样本中估计无线电图，并通过整合物理特性和创新的模块设计，显著提升了性能。", "translation": "无线电图表示一个区域内频谱资源的空间分布，支持高效的资源分配和干扰缓解。然而，在实际场景中只能测量有限数量的样本，因此很难构建密集的无线电图。尽管现有工作已使用深度学习从稀疏样本中估计密集的无线电图，但它们难以与无线电图的物理特性相结合。为了解决这一挑战，我们将无线电图估计视为稀疏信号恢复问题。进一步结合物理传播模型将问题分解为多个因子优化子问题，从而降低了恢复复杂度。受现有压缩感知方法的启发，我们提出了无线电深度展开网络（RadioDUN）来展开优化过程，以可学习的方式实现自适应参数调整和先验拟合。为了考虑无线电传播特性，我们开发了一个动态重加权模块（DRM）来自适应地建模每个因子对无线电图的重要性。受物理传播模型中阴影因子的启发，我们整合了障碍相关因子来表达障碍引起的信号随机衰减。进一步设计了阴影损失来约束因子预测并作为辅助监督目标，从而增强了RadioDUN的性能。广泛的实验表明，所提出的方法优于现有最先进的方法。我们的代码将在发布后公开。", "summary": "本文提出了一种名为RadioDUN的物理启发深度展开网络，用于从稀疏测量样本中估计密集无线电图。该方法将无线电图估计建模为稀疏信号恢复问题，并整合了物理传播模型以分解问题。RadioDUN通过展开优化过程，实现自适应参数调整和先验拟合。此外，引入动态重加权模块和阴影损失，以更好地融入无线电传播特性和障碍物影响，实验证明其性能优于现有先进方法。", "keywords": "无线电图估计, 深度展开网络, 稀疏信号恢复, 物理传播模型, 动态重加权", "comments": "这篇论文通过将深度学习与无线电传播的物理特性相结合，提出了一种新颖的无线电图估计方法，解决了现有深度学习方法难以整合物理特性的痛点。其深度展开网络架构、动态重加权模块和阴影损失的设计体现了创新性，有望在实际无线电资源管理中发挥重要作用。"}}
{"id": "2506.08404", "title": "Compact Amplified Laser Power Stabilization Using Robust Active Disturbance Rejection Control with Sensor Noise Decoupling", "authors": ["Yanpei Shi", "Jingxuan Zhang", "Zhuo Shi", "Chenyao Zhang", "Yuze Guo", "Rui Feng"], "summary": "Laser power instability, encompassing random jitter and slow drift, severely\nlimits the performance of optically pumped magnetometers (OPMs) in detecting\nultra-weak magnetic fields, especially in large-scale OPM arrays for\nmagnetoencephalography. Although a unified amplified laser (AL) architecture\nimproves integration, fluctuations in the pump beam progressively degrade\nperformance across all channels, exacerbated by environmental disturbances and\nsystem uncertainties. To address this challenge, this paper presents a compact\nAL power stabilization approach based on an innovative dual-loop active\ndisturbance rejection control (DLADRC) strategy, while integrating a\ncomprehensive quantitative stability analysis through novel exponential decay\nestimates for extended state observers (ESOs) and control error dynamics. As\nvalidated through physical experimental results, the proposed method\nsignificantly improves AL's long-term stability with sensor noise decoupling,\nachieving an over 85.7% reduction in 1-hour power instability and a tenfold\ndecrease in Allan variance for correlation times 10^2 s--10^3 s, compared to\nstandard ADRC. Crucially, the strategy demonstrates robust effectiveness across\ndiverse operating scenarios, enabling AL-based OPM systems to achieve their\nfull potential in high-sensitivity biomagnetic field detection.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08404v1", "AI": {"title_translation": "采用鲁棒自抗扰控制和传感器噪声解耦的紧凑型放大激光功率稳定技术", "tldr": "本文提出了一种基于双环自抗扰控制（DLADRC）的紧凑型放大激光（AL）功率稳定方法，通过传感器噪声解耦显著提高了AL的长期稳定性，并在生物磁场检测中实现高灵敏度。", "motivation": "激光功率不稳定（包括随机抖动和慢漂移）严重限制了光泵磁力计（OPMs）在检测超弱磁场方面的性能，尤其是在用于脑磁图的大规模OPM阵列中。尽管统一的放大激光（AL）架构改善了集成度，但泵浦光束的波动会逐渐降低所有通道的性能，并因环境扰动和系统不确定性而加剧。", "method": "本文提出了一种基于创新性双环自抗扰控制（DLADRC）策略的紧凑型放大激光（AL）功率稳定方法。该方法通过新型扩张状态观测器（ESOs）和控制误差动力学的指数衰减估计，整合了全面的定量稳定性分析，并实现了传感器噪声解耦。", "result": "通过物理实验验证，所提出的方法显著提高了AL的长期稳定性，并实现了传感器噪声解耦。与标准自抗扰控制（ADRC）相比，1小时功率不稳定性降低了85.7%以上，在10^2 s-10^3 s相关时间内的阿伦方差降低了十倍。", "conclusion": "该策略在不同操作场景下均表现出鲁棒的有效性，使基于AL的OPM系统能够充分发挥其在高灵敏度生物磁场检测中的潜力。", "translation": "激光功率不稳定，包括随机抖动和慢漂移，严重限制了光泵磁力计（OPMs）在检测超弱磁场方面的性能，尤其是在用于脑磁图的大规模OPM阵列中。尽管统一的放大激光（AL）架构改善了集成度，但泵浦光束的波动会逐渐降低所有通道的性能，并因环境扰动和系统不确定性而加剧。为了解决这一挑战，本文提出了一种基于创新性双环自抗扰控制（DLADRC）策略的紧凑型AL功率稳定方法，同时通过新型扩张状态观测器（ESOs）和控制误差动力学的指数衰减估计，整合了全面的定量稳定性分析。通过物理实验结果验证，所提出的方法显著提高了AL的长期稳定性，并实现了传感器噪声解耦，与标准自抗扰控制（ADRC）相比，1小时功率不稳定性降低了85.7%以上，在10^2 s-10^3 s相关时间内的阿伦方差降低了十倍。至关重要的是，该策略在不同操作场景下均表现出鲁棒的有效性，使基于AL的OPM系统能够充分发挥其在高灵敏度生物磁场检测中的潜力。", "summary": "本文针对光泵磁力计（OPMs）中激光功率不稳定性对性能的限制，提出了一种紧凑型放大激光（AL）功率稳定方法。该方法基于创新的双环自抗扰控制（DLADRC）策略，并结合了全面的定量稳定性分析和传感器噪声解耦。实验结果表明，与标准自抗扰控制相比，该方法将1小时功率不稳定性降低了85.7%以上，并将阿伦方差降低了十倍，从而显著提高了AL的长期稳定性，并使OPM系统在高灵敏度生物磁场检测中发挥最大潜力。", "keywords": "激光功率稳定, 自抗扰控制, 传感器噪声解耦, 光泵磁力计, 阿伦方差", "comments": "本文提出了一种新颖的双环自抗扰控制（DLADRC）策略，结合了传感器噪声解耦和全面的稳定性分析，解决了放大激光功率稳定性在光泵磁力计（OPMs）中的关键问题。其创新性在于将DLADRC应用于激光功率稳定，并取得了显著的性能提升，特别是对长期稳定性和抗扰动能力的改善。这对于高灵敏度生物磁场检测等应用具有重要意义。"}}
{"id": "2506.08774", "title": "Multimodal Representation Alignment for Cross-modal Information Retrieval", "authors": ["Fan Xu", "Luis A. Leiva"], "summary": "Different machine learning models can represent the same underlying concept\nin different ways. This variability is particularly valuable for in-the-wild\nmultimodal retrieval, where the objective is to identify the corresponding\nrepresentation in one modality given another modality as input. This challenge\ncan be effectively framed as a feature alignment problem. For example, given a\nsentence encoded by a language model, retrieve the most semantically aligned\nimage based on features produced by an image encoder, or vice versa. In this\nwork, we first investigate the geometric relationships between visual and\ntextual embeddings derived from both vision-language models and combined\nunimodal models. We then align these representations using four standard\nsimilarity metrics as well as two learned ones, implemented via neural\nnetworks. Our findings indicate that the Wasserstein distance can serve as an\ninformative measure of the modality gap, while cosine similarity consistently\noutperforms alternative metrics in feature alignment tasks. Furthermore, we\nobserve that conventional architectures such as multilayer perceptrons are\ninsufficient for capturing the complex interactions between image and text\nrepresentations. Our study offers novel insights and practical considerations\nfor researchers working in multimodal information retrieval, particularly in\nreal-world, cross-modal applications.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.08774v1", "AI": {"title_translation": "多模态表示对齐用于跨模态信息检索", "tldr": "研究多模态表示对齐以改进跨模态信息检索，发现Wasserstein距离可衡量模态差距，余弦相似度在特征对齐中表现最佳，且传统架构不足以捕捉复杂交互。", "motivation": "不同的机器学习模型可以以不同方式表示相同的底层概念，这在野外多模态检索中尤其有价值，目标是根据一种模态输入识别另一种模态中对应的表示。这个挑战可以有效地被框架为特征对齐问题。", "method": "首先调查了来自视觉-语言模型和组合单模态模型的视觉和文本嵌入之间的几何关系。然后使用四种标准相似度指标以及两种通过神经网络实现的学习指标来对齐这些表示。", "result": "Wasserstein距离可以作为模态差距的一个信息量大的度量；余弦相似度在特征对齐任务中始终优于其他替代指标；传统架构（如多层感知机）不足以捕捉图像和文本表示之间复杂的交互。", "conclusion": "本研究为多模态信息检索领域的研究人员提供了新颖的见解和实际考虑，特别是在真实世界的跨模态应用中。", "translation": "不同的机器学习模型可以以不同的方式表示相同的底层概念。这种可变性对于野外多模态检索尤其有价值，其目标是在给定一种模态作为输入的情况下，识别另一种模态中对应的表示。这个挑战可以有效地被框架为特征对齐问题。例如，给定一个由语言模型编码的句子，根据图像编码器产生的特征检索语义最对齐的图像，反之亦然。在这项工作中，我们首先研究了来自视觉-语言模型和组合单模态模型的视觉和文本嵌入之间的几何关系。然后，我们使用四种标准相似度指标以及两种通过神经网络实现的学习指标来对齐这些表示。我们的研究结果表明，Wasserstein距离可以作为模态差距的一个信息量大的度量，而余弦相似度在特征对齐任务中始终优于其他替代指标。此外，我们观察到传统架构（如多层感知机）不足以捕捉图像和文本表示之间复杂的交互。我们的研究为多模态信息检索领域的研究人员提供了新颖的见解和实际考虑，特别是在真实世界的跨模态应用中。", "summary": "本文探讨了多模态信息检索中的特征对齐问题，旨在根据一种模态输入检索另一种模态的对应表示。研究分析了视觉和文本嵌入之间的几何关系，并评估了多种标准及学习的相似度指标进行表示对齐。结果显示，Wasserstein距离能有效衡量模态间隙，余弦相似度在特征对齐上表现最佳，而传统网络架构不足以处理复杂的模态交互。研究为真实世界跨模态应用提供了实用见解。", "keywords": "多模态检索, 特征对齐, 视觉-语言模型, Wasserstein距离, 余弦相似度", "comments": "本文深入探讨了多模态信息检索中的核心问题——表示对齐。其创新点在于系统性地比较了多种相似度度量，并指出Wasserstein距离在衡量模态差距方面的潜力，以及余弦相似度在实际对齐中的优越性。此外，对传统网络架构局限性的观察也为未来的模型设计提供了方向。这项工作对于理解和改进跨模态检索系统具有重要的理论和实践意义。"}}
{"id": "2506.08980", "title": "AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code Generation", "authors": ["Kaifeng He", "Mingwei Liu", "Chong Wang", "Zike Li", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "summary": "Code generation with large language models (LLMs) is highly sensitive to\ntoken selection during decoding, particularly at uncertain decision points that\ninfluence program logic. While standard strategies like greedy and beam search\ntreat all tokens uniformly, they overlook code-specific uncertainty patterns,\nleading to suboptimal performance. This paper presents an empirical study\nrevealing that many generation errors stem from ranking mistakes at\nhigh-uncertainty steps, where the correct token is present but not top-ranked.\n  Motivated by these findings, we propose AdaDec, an uncertainty-guided\nadaptive decoding framework that integrates a token-level pause-then-rerank\nmechanism driven by token uncertainty (Shannon entropy). AdaDec learns\nmodel-specific uncertainty thresholds and applies a lookahead-based reranking\nstrategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks\nshow that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding,\noutperforms or matches beam search, and reduces computational cost and latency\nthrough efficient, selective pausing. Our results highlight the promise of\nuncertainty-aware adaptive decoding for improving the reliability and\nefficiency of LLM-based code generation.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.08980v1", "AI": {"title_translation": "AdaDec：不确定性引导的LLM代码生成自适应解码", "tldr": "AdaDec提出了一种不确定性引导的自适应解码框架，通过在不确定性高的决策点进行暂停-重排机制，显著提高了LLM代码生成的准确性和效率。", "motivation": "现有标准解码策略（如贪婪搜索和束搜索）在LLM代码生成中未能充分考虑代码特有的不确定性模式，导致次优性能。研究发现，许多生成错误源于高不确定性步骤中的排名错误，即正确令牌存在但未排在首位。", "method": "本文提出了AdaDec，一个不确定性引导的自适应解码框架。它通过令牌不确定性（香农熵）驱动的令牌级暂停-然后-重排机制，学习模型特定的不确定性阈值，并在不确定性高时应用基于前瞻的重排策略。", "result": "在HumanEval和MBPP基准测试中，AdaDec将Pass@1准确率比贪婪解码提高了15.5%，优于或与束搜索相当，并通过高效、选择性暂停降低了计算成本和延迟。", "conclusion": "不确定性感知的自适应解码对于提高基于LLM的代码生成的可靠性和效率具有重要前景。", "translation": "大型语言模型（LLM）的代码生成对解码过程中的令牌选择高度敏感，尤其是在影响程序逻辑的不确定决策点。虽然贪婪搜索和束搜索等标准策略统一处理所有令牌，但它们忽视了代码特有的不确定性模式，导致次优性能。本文提出了一项实证研究，揭示了许多生成错误源于高不确定性步骤中的排名错误，即正确的令牌存在但未排在首位。受这些发现的启发，我们提出了AdaDec，一个不确定性引导的自适应解码框架，它集成了由令牌不确定性（香农熵）驱动的令牌级暂停-然后-重排机制。AdaDec学习模型特定的不确定性阈值，并在不确定性高时应用基于前瞻的重排策略。在HumanEval和MBPP基准测试上的实验表明，AdaDec将Pass@1准确率比贪婪解码提高了15.5%，优于或与束搜索相当，并通过高效、选择性暂停降低了计算成本和延迟。我们的结果突出了不确定性感知的自适应解码在提高基于LLM的代码生成的可靠性和效率方面的潜力。", "summary": "本研究提出AdaDec，一种不确定性引导的自适应解码框架，旨在解决大型语言模型（LLM）代码生成中因不确定性决策点令牌选择敏感性导致的性能问题。通过实证研究发现，许多生成错误源于高不确定性步骤中的排名错误。AdaDec引入了基于香农熵的令牌级暂停-重排机制，并在高不确定性时应用前瞻性重排策略。实验结果显示，AdaDec在HumanEval和MBPP基准测试上将Pass@1准确率提升高达15.5%，且优于或匹配束搜索，同时有效降低了计算成本和延迟。这表明不确定性感知的自适应解码能显著提升LLM代码生成的可靠性和效率。", "keywords": "LLM代码生成, 自适应解码, 不确定性引导, 令牌重排, 香农熵", "comments": "AdaDec的创新之处在于其不确定性引导的自适应解码策略，通过在关键决策点进行选择性暂停和重排，有效解决了LLM代码生成中因不确定性导致的排名错误问题。这种方法不仅显著提高了生成准确性，还在计算效率上有所提升，为LLM在代码生成领域的应用提供了有价值的优化方向。其对不确定性的利用提供了一个新颖的视角来改进解码过程。"}}
{"id": "2506.08756", "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning", "authors": ["Octavio Arriaga", "Rebecca Adam", "Melvin Laux", "Lisa Gutzeit", "Marco Ragni", "Jan Peters", "Frank Kirchner"], "summary": "Real-world robotic applications, from autonomous exploration to assistive\ntechnologies, require adaptive, interpretable, and data-efficient learning\nparadigms. While deep learning architectures and foundation models have driven\nsignificant advances in diverse robotic applications, they remain limited in\ntheir ability to operate efficiently and reliably in unknown and dynamic\nenvironments. In this position paper, we critically assess these limitations\nand introduce a conceptual framework for combining data-driven learning with\ndeliberate, structured reasoning. Specifically, we propose leveraging\ndifferentiable physics for efficient world modeling, Bayesian inference for\nuncertainty-aware decision-making, and meta-learning for rapid adaptation to\nnew tasks. By embedding physical symbolic reasoning within neural models,\nrobots could generalize beyond their training data, reason about novel\nsituations, and continuously expand their knowledge. We argue that such hybrid\nneuro-symbolic architectures are essential for the next generation of\nautonomous systems, and to this end, we provide a research roadmap to guide and\naccelerate their development.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08756v1", "AI": {"title_translation": "贝叶斯逆物理学在神经符号机器人学习中的应用", "tldr": "本立场论文提出了一种结合数据驱动学习和结构化推理的神经符号机器人学习概念框架，旨在解决现有深度学习模型在未知动态环境中运行效率和可靠性有限的问题，并为未来自主系统发展提供了研究路线图。", "motivation": "现有深度学习架构和基础模型在未知和动态环境中高效可靠运行的能力有限，而现实世界的机器人应用需要自适应、可解释和数据高效的学习范式。", "method": "提出一个概念框架，结合可微分物理学进行高效世界建模，贝叶斯推理进行不确定性感知决策，以及元学习实现对新任务的快速适应。通过将物理符号推理嵌入到神经模型中。", "result": "机器人能够超越训练数据进行泛化，推理新颖情况，并持续扩展其知识。", "conclusion": "这种混合神经符号架构对于下一代自主系统至关重要，并为此提供了一份研究路线图以指导和加速其发展。", "translation": "现实世界的机器人应用，从自主探索到辅助技术，都需要自适应、可解释和数据高效的学习范式。虽然深度学习架构和基础模型在各种机器人应用中取得了重大进展，但它们在未知和动态环境中高效可靠运行的能力仍然有限。在这篇立场论文中，我们批判性地评估了这些局限性，并引入了一个结合数据驱动学习和深思熟虑的结构化推理的概念框架。具体来说，我们建议利用可微分物理学进行高效的世界建模，利用贝叶斯推理进行不确定性感知决策，以及利用元学习实现对新任务的快速适应。通过将物理符号推理嵌入到神经模型中，机器人可以超越其训练数据进行泛化，推理新颖情况，并不断扩展其知识。我们认为这种混合神经符号架构对于下一代自主系统至关重要，为此，我们提供了一份研究路线图以指导和加速其发展。", "summary": "本立场论文提出了一种结合数据驱动学习和结构化推理的神经符号机器人学习概念框架，旨在解决现有深度学习模型在未知动态环境中效率和可靠性有限的问题。该框架利用可微分物理学进行世界建模、贝叶斯推理进行不确定性感知决策以及元学习进行快速适应，使机器人能够更好地泛化、推理新情况并持续学习，为未来自主系统发展提供了研究路线图。", "keywords": "神经符号学习, 机器人, 贝叶斯推理, 可微分物理学, 元学习", "comments": "这篇立场论文提出了一个重要的研究方向，即结合符号推理和神经网络的神经符号方法，以克服纯数据驱动方法的局限性。其创新点在于将可微分物理学、贝叶斯推理和元学习整合到一个统一的框架中，旨在提高机器人的适应性、可解释性和数据效率。对于未来需要处理复杂、未知环境的自主系统而言，这种混合架构具有重要意义。"}}
{"id": "2506.08297", "title": "SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging", "authors": ["Nhat Thanh Tran", "Fanghui Xue", "Shuai Zhang", "Jiancheng Lyu", "Yunling Zheng", "Yingyong Qi", "Jack Xin"], "summary": "Attention is the critical component of a transformer. Yet the quadratic\ncomputational complexity of vanilla full attention in the input size and the\ninability of its linear attention variant to focus have been challenges for\ncomputer vision tasks. We provide a mathematical definition of generalized\nattention and formulate both vanilla softmax attention and linear attention\nwithin the general framework. We prove that generalized attention disperses,\nthat is, as the number of keys tends to infinity, the query assigns equal\nweights to all keys. Motivated by the dispersion property and recent\ndevelopment of Mamba form of attention, we design Scalable and Efficient Mamba\nlike Attention (SEMA) which utilizes token localization to avoid dispersion and\nmaintain focusing, complemented by theoretically consistent arithmetic\naveraging to capture global aspect of attention. We support our approach on\nImagenet-1k where classification results show that SEMA is a scalable and\neffective alternative beyond linear attention, outperforming recent vision\nMamba models on increasingly larger scales of images at similar model parameter\nsizes.", "comment": "15 pages, figures 3", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08297v1", "AI": {"title_translation": "SEMA: 一种通过Token定位和平均实现的类Mamba可扩展高效注意力机制", "tldr": "SEMA是一种新的类Mamba注意力机制，通过Token定位和平均解决了传统注意力的二次计算复杂度和线性注意力的分散问题，在图像分类任务上表现优于现有模型。", "motivation": "传统Transformer的注意力机制存在二次计算复杂度问题，而线性注意力变体则无法保持聚焦。受注意力分散特性和Mamba形式注意力的启发，本文旨在解决这些挑战。", "method": "本文首先给出了广义注意力的数学定义，并将vanilla softmax注意力和线性注意力纳入其中。通过证明广义注意力的分散特性，即当键的数量趋于无穷时，查询会平均分配权重给所有键。受此启发，设计了可扩展高效的类Mamba注意力（SEMA），它利用Token定位来避免分散并保持聚焦，并通过理论上一致的算术平均来捕获注意力的全局方面。", "result": "在Imagenet-1k上的图像分类结果表明，SEMA是一种超越线性注意力的可扩展且有效的替代方案，在相似模型参数大小下，在越来越大的图像尺度上优于最近的视觉Mamba模型。", "conclusion": "SEMA通过创新的Token定位和平均机制，成功解决了传统注意力的计算复杂度和线性注意力的聚焦不足问题，为计算机视觉任务提供了一种高效且可扩展的注意力替代方案。", "translation": "注意力是Transformer的关键组成部分。然而，vanilla全注意力机制在输入大小上的二次计算复杂性及其线性注意力变体无法聚焦的问题一直是计算机视觉任务的挑战。我们提供了广义注意力的数学定义，并将vanilla softmax注意力和线性注意力都在通用框架内进行表述。我们证明了广义注意力会分散，即当键的数量趋于无穷时，查询会向所有键分配相等的权重。受分散特性和Mamba形式注意力最新发展的启发，我们设计了可扩展高效的类Mamba注意力（SEMA），它利用Token定位来避免分散并保持聚焦，并辅以理论上一致的算术平均来捕获注意力的全局方面。我们在Imagenet-1k上支持我们的方法，分类结果表明SEMA是线性注意力之外的一种可扩展且有效的替代方案，在相似模型参数大小下，在越来越大的图像尺度上优于最近的视觉Mamba模型。", "summary": "本文提出了一种名为SEMA（Scalable and Efficient Mamba like Attention）的新型注意力机制，旨在解决传统注意力机制的二次计算复杂度和线性注意力机制聚焦不足的问题。通过对广义注意力的数学定义和对其分散特性的证明，SEMA利用Token定位来维持聚焦，并通过算术平均来捕获全局信息。实验结果表明，在图像分类任务中，SEMA在可扩展性和有效性方面均优于现有的线性注意力和视觉Mamba模型。", "keywords": "注意力机制, Mamba, Token定位, 图像分类, 可扩展性", "comments": "SEMA的创新点在于结合了Token定位和算术平均，有效解决了注意力机制中的分散问题，并保持了其聚焦能力。其在Imagenet-1k上的优异表现，表明它为未来计算机视觉任务中的注意力机制提供了一个有前景的高效替代方案。"}}
{"id": "2506.08140", "title": "AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists", "authors": ["Yifei Li", "Hanane Nour Moussa", "Ziru Chen", "Shijie Chen", "Botao Yu", "Mingyi Xue", "Benjamin Burns", "Tzu-Yao Chiu", "Vishal Dey", "Zitong Lu", "Chen Wei", "Qianheng Zhang", "Tianyu Zhang", "Song Gao", "Xuhui Huang", "Xia Ning", "Nesreen K. Ahmed", "Ali Payani", "Huan Sun"], "summary": "Despite long-standing efforts in accelerating scientific discovery with AI,\nbuilding AI co-scientists remains challenging due to limited high-quality data\nfor training and evaluation. To tackle this data scarcity issue, we present\nAutoSDT, an automatic pipeline that collects high-quality coding tasks in\nreal-world data-driven discovery workflows. AutoSDT leverages the coding\ncapabilities and parametric knowledge of LLMs to search for diverse sources,\nselect ecologically valid tasks, and synthesize accurate task instructions and\ncode solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404\ncoding tasks for data-driven discovery that covers four scientific disciplines\nand 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the\nonly automatically collected and the largest open dataset for data-driven\nscientific discovery. Expert feedback on a subset of 256 tasks shows the\neffectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,\nand 92.2% of the synthesized programs are functionally correct. Trained on\nAutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show\nsubstantial improvement on two challenging data-driven discovery benchmarks,\nScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches\nthe same level of performance as GPT-4o on ScienceAgentBench with a success\nrate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it\nlifts the hypothesis matching score to 8.1, bringing a 17.4% relative\nimprovement and closing the gap between open-weight models and GPT-4o.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08140v1", "AI": {"title_translation": "AutoSDT：将数据驱动的发现任务扩展到开放式合作科学家", "tldr": "AutoSDT提出了一个自动管道，用于收集高质量的数据驱动科学发现编码任务，构建了最大的开放数据集AutoSDT-5K，并显著提升了LLM在科学发现基准上的性能，使其接近GPT-4o。", "motivation": "现有AI辅助科学发现面临高质量数据稀缺的挑战，限制了AI合作科学家的发展。", "method": "提出了AutoSDT，一个自动管道，利用LLM的编码能力和参数知识来搜索多样化来源、选择生态有效任务、合成准确的任务指令和代码解决方案，从而收集高质量的编码任务。基于此方法构建了AutoSDT-5K数据集。", "result": "构建了AutoSDT-5K，一个包含5404个编码任务的数据集，涵盖四个科学学科和756个Python包，是目前最大且唯一自动收集的数据驱动科学发现开放数据集。专家反馈显示，93%的任务具有生态有效性，92.2%的合成程序功能正确。在AutoSDT-5K上训练的Qwen2.5-Coder-Instruct LLM系列（AutoSDT-Coder）在ScienceAgentBench和DiscoveryBench两个基准测试中表现出显著改进。AutoSDT-Coder-32B在ScienceAgentBench上的成功率达到7.8%，是其基础模型的两倍，达到与GPT-4o相同的性能水平。在DiscoveryBench上，假设匹配得分提升至8.1，相对提升17.4%，缩小了开源模型与GPT-4o之间的差距。", "conclusion": "AutoSDT及其构建的AutoSDT-5K数据集有效解决了数据稀缺问题，显著提升了大型语言模型在数据驱动科学发现任务上的性能，使其成为AI合作科学家领域的重要进展。", "translation": "尽管在人工智能加速科学发现方面付出了长期努力，但由于训练和评估所需的高质量数据有限，构建人工智能合作科学家仍然充满挑战。为了解决数据稀缺问题，我们提出了AutoSDT，一个自动管道，用于收集真实世界数据驱动发现工作流中的高质量编码任务。AutoSDT利用大型语言模型的编码能力和参数知识来搜索多样化来源，选择生态有效任务，并合成准确的任务指令和代码解决方案。利用我们的管道，我们构建了AutoSDT-5K，一个包含5404个数据驱动发现编码任务的数据集，涵盖四个科学学科和756个独特的Python包。据我们所知，AutoSDT-5K是唯一一个自动收集的、也是最大的数据驱动科学发现开放数据集。对256个任务子集的专家反馈显示了AutoSDT的有效性：93%收集到的任务具有生态有效性，92.2%合成的程序功能正确。在AutoSDT-5K上训练的Qwen2.5-Coder-Instruct大型语言模型系列，被称为AutoSDT-Coder，在ScienceAgentBench和DiscoveryBench这两个具有挑战性的数据驱动发现基准测试中显示出显著改进。最值得注意的是，AutoSDT-Coder-32B在ScienceAgentBench上的成功率达到7.8%，是其基础模型的两倍，达到了与GPT-4o相同的性能水平。在DiscoveryBench上，它将假设匹配得分提高到8.1，带来了17.4%的相对提升，并缩小了开源模型与GPT-4o之间的差距。", "summary": "本文介绍了AutoSDT，一个旨在解决AI科学发现中高质量数据稀缺问题的自动化管道。AutoSDT能够自动收集和生成数据驱动科学发现工作流中的高质量编码任务。通过AutoSDT，研究人员构建了AutoSDT-5K数据集，这是目前最大的自动收集的开放数据集。基于此数据集训练的AutoSDT-Coder模型在多个科学发现基准测试中表现出色，显著提升了性能，并缩小了与顶尖闭源模型如GPT-4o的差距，展示了其在推动AI合作科学家发展方面的潜力。", "keywords": "数据驱动发现, LLM, 科学发现, 数据集, AutoSDT", "comments": "这项工作通过自动化数据收集和任务生成，有效解决了AI科学发现领域长期存在的高质量数据稀缺问题。AutoSDT-5K数据集的创建及其对开源LLM性能的显著提升，为构建更强大的AI合作科学家奠定了基础，具有重要的实际意义和创新性。其方法论利用LLM的编码和知识能力，为未来类似的数据集构建提供了新的范式。"}}
{"id": "2506.08945", "title": "Who is using AI to code? Global diffusion and impact of generative AI", "authors": ["Simone Daniotti", "Johannes Wachs", "Xiangnan Feng", "Frank Neffke"], "summary": "Generative coding tools promise big productivity gains, but uneven uptake\ncould widen skill and income gaps. We train a neural classifier to spot\nAI-generated Python functions in 80 million GitHub commits (2018-2024) by\n200,000 developers and track how fast--and where--these tools take hold. By\nDecember 2024, AI wrote an estimated 30.1% of Python functions from U.S.\ncontributors, versus 24.3% in Germany, 23.2% in France, 21.6% in India, 15.4%\nin Russia and 11.7% in China. Newer GitHub users use AI more than veterans,\nwhile male and female developers adopt at similar rates. Within-developer\nfixed-effects models show that moving to 30% AI use raises quarterly commits by\n2.4%. Coupling this effect with occupational task and wage data puts the annual\nvalue of AI-assisted coding in the United States at $9.6-$14.4 billion, rising\nto $64-$96 billion if we assume higher estimates of productivity effects\nreported by randomized control trials. Moreover, generative AI prompts learning\nand innovation, leading to increases in the number of new libraries and library\ncombinations that programmers use. In short, AI usage is already widespread but\nhighly uneven, and the intensity of use, not only access, drives measurable\ngains in output and exploration.", "comment": "42 pages", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.08945v1", "AI": {"title_translation": "谁在使用AI编程？生成式AI的全球扩散与影响", "tldr": "生成式AI编程工具的采用已广泛但分布不均，对开发者生产力有显著提升，并带来数十亿至千亿美元的经济价值。", "motivation": "生成式编程工具虽有望带来巨大的生产力提升，但其不均衡的普及可能加剧技能和收入差距，因此有必要研究这些工具的全球扩散和影响。", "method": "研究训练了一个神经网络分类器，用于识别GitHub上8000万次提交（2018-2024年）中由AI生成的Python函数，涉及20万名开发者。此外，还使用开发者内部的固定效应模型来评估AI使用对提交量的影响，并结合职业任务和工资数据估算经济价值。", "result": "截至2024年12月，美国贡献者约有30.1%的Python函数由AI编写，德国为24.3%，法国23.2%，印度21.6%，俄罗斯15.4%，中国11.7%。新GitHub用户比老用户更多使用AI，男女开发者采用率相似。开发者内部固定效应模型显示，AI使用率提高到30%可使季度提交量增加2.4%。结合职业任务和工资数据，AI辅助编程在美国的年价值估计为96亿至144亿美元，若采用随机对照试验中更高的生产力估计，则可达640亿至960亿美元。此外，生成式AI还促进了学习和创新，导致程序员使用的新库和库组合数量增加。", "conclusion": "AI编程工具的使用已经非常普遍，但其普及程度高度不均。使用强度而非仅仅是可访问性，是驱动产出和探索方面可衡量收益的关键因素。", "translation": "生成式编程工具承诺带来巨大的生产力提升，但其不均衡的普及可能加剧技能和收入差距。我们训练了一个神经网络分类器，用于识别20万名开发者在8000万次GitHub提交（2018-2024年）中由AI生成的Python函数，并追踪这些工具的普及速度和地点。截至2024年12月，美国贡献者约有30.1%的Python函数由AI编写，而德国为24.3%，法国23.2%，印度21.6%，俄罗斯15.4%，中国11.7%。新的GitHub用户比资深用户更多地使用AI，而男性和女性开发者的采用率相似。开发者内部的固定效应模型显示，将AI使用率提高到30%可使季度提交量增加2.4%。将这种效应与职业任务和工资数据结合起来，美国AI辅助编程的年价值估计为96亿至144亿美元，如果我们假设随机对照试验报告的更高生产力效应估计，则可升至640亿至960亿美元。此外，生成式AI促进了学习和创新，导致程序员使用的新库和库组合数量增加。简而言之，AI的使用已经很普遍但高度不均，并且使用强度，而不仅仅是可访问性，驱动着产出和探索方面的可衡量收益。", "summary": "本研究通过训练神经网络分类器分析GitHub数据，揭示了生成式AI编程工具的全球扩散和影响。结果显示，AI编程工具的采用率在全球范围内存在显著差异，且新用户比老用户更倾向于使用AI。研究还量化了AI使用对开发者生产力的提升作用，并估算了其巨大的经济价值，同时指出AI促进了编程领域的学习和创新。核心发现是AI使用虽已普及但分布不均，且使用强度而非仅是可访问性，是驱动产出和创新的关键。", "keywords": "生成式AI, 编程工具, 生产力, GitHub, 扩散", "comments": "这项研究通过大规模数据分析，首次在全球范围内量化了生成式AI编程工具的扩散程度和经济影响，其创新之处在于使用了神经网络分类器来识别AI生成的代码，并结合经济模型评估了其价值。研究结果对于理解AI在软件开发领域的变革性作用及其潜在的社会经济影响具有重要意义，尤其是在生产力提升和技能差距方面。"}}
{"id": "2506.08555", "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement", "authors": ["Xinyue Niu", "Akira Furui"], "summary": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems.", "comment": "6 pages, 3 figures. This work has been accepted for presentation at\n  the IEEE Engineering in Medicine and Biology Conference (EMBC) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08555v1", "AI": {"title_translation": "面向跨受试者肌电图模式识别：通过双分支对抗性特征解耦实现", "tldr": "一种双分支对抗性神经网络通过将肌电图特征解耦为模式特定和受试者特定组件，实现了鲁棒的、无需校准的跨受试者肌电图模式识别。", "motivation": "跨受试者肌电图（EMG）模式识别面临着巨大的挑战，这归因于受试者间的可变性。传统方法依赖于耗时且不切实际的特定受试者校准数据，这阻碍了其大规模、真实世界的部署。", "method": "本文提出了一种端到端的双分支对抗性神经网络，通过将EMG特征解耦为模式特定和受试者特定组件，同时执行模式识别和个体识别。模式特定组件实现无需模型校准的新用户鲁棒模式识别，而受试者特定组件支持任务不变的生物识别等下游应用。", "result": "实验结果表明，所提出的模型在来自未见过用户的数据上取得了鲁棒的性能，在跨受试者场景中优于各种基线方法。", "conclusion": "这项研究为无需模型校准的跨受试者肌电图模式识别提供了新视角，并突出了所提出模型在更广泛应用（例如任务无关的生物识别系统）方面的潜力。", "translation": "跨受试者肌电图（EMG）模式识别面临着巨大挑战，这归因于肌肉解剖结构、电极放置和信号特性方面存在的受试者间差异。传统方法依赖于特定受试者校准数据来使模型适应新用户，这种方法既耗时又不利于大规模、真实世界的部署。本文提出了一种通过特征解耦消除校准要求的方法，从而实现有效的跨受试者泛化。我们提出了一种端到端的双分支对抗性神经网络，通过将肌电图特征解耦为模式特定和受试者特定组件，同时执行模式识别和个体识别。模式特定组件有助于新用户无需模型校准即可进行鲁棒的模式识别，而受试者特定组件则支持下游应用，例如任务不变的生物识别。实验结果表明，所提出的模型在来自未见过用户的数据上取得了鲁棒的性能，在跨受试者场景中优于各种基线方法。总的来说，这项研究为无需模型校准的跨受试者肌电图模式识别提供了新视角，并突出了所提出模型在更广泛应用（例如任务无关的生物识别系统）方面的潜力。", "summary": "本文针对跨受试者肌电图（EMG）模式识别中存在的受试者间差异和传统校准方法不切实际的问题，提出了一种通过特征解耦来消除校准需求的方法。该方法引入了一个端到端的双分支对抗性神经网络，能够将EMG特征解耦为模式特定和受试者特定组件。这种解耦使得新用户无需模型校准即可实现鲁棒的模式识别，并促进了任务不变的生物识别等下游应用。实验结果表明，该模型在未见过用户的数据上表现出优越的鲁棒性能，超越了各种基线方法，为无需校准的跨受试者EMG模式识别和更广泛的应用提供了新颖的视角。", "keywords": "EMG模式识别, 跨受试者, 特征解耦, 对抗性神经网络, 免校准", "comments": "该论文提出了一种创新的方法，通过双分支对抗性网络进行特征解耦，克服了跨受试者EMG模式识别中特定受试者校准的局限性。这种方法为实际部署提供了可行的解决方案。通过解耦特征同时实现鲁棒模式识别和个体识别是一项重大进步，为生物识别系统等更广泛的应用开辟了道路。"}}
{"id": "2506.08414", "title": "Theoretical Foundations of Waste Factor and Waste Figure with Applications to Fixed Wireless Access and Relay Systems", "authors": ["Nurullah Sevim", "Mostafa Ibrahim", "Sabit Ekin", "Theodore S. Rappaport"], "summary": "The exponential rise in energy consumption across wireless communication\nsystems, particularly in anticipation of next-generation wireless systems,\nnecessitates rigorous frameworks for evaluating and optimizing energy\nefficiency. This paper revisits and expands the concept of the Waste Factor\n(W), or Waste Figure (WF) in decibel scale, as a unifying metric that captures\nboth utilized and wasted power in cascaded communication systems. Building upon\nits foundation in system-level power modeling, we integrate the Waste Factor\ninto a refined formulation of the Consumption Factor (CF), the ratio of data\nrate to total consumed power, linking it directly to Shannon's theoretical\nlimit on energy per bit. This analysis introduces additive energy waste into\nthe classical energy-per-bit derivation through the Waste Factor term.\n  We derive closed-form expressions for energy-per-bit expenditure in both\ndirect and relay-assisted links and develop a decision rule to determine which\ncommunication path is more energy efficient under given conditions. While not\nmodeled explicitly, Reflective Intelligent Surfaces (RIS) can be interpreted as\na special case of relay-based architectures within this unified formulation,\nsuggesting broader applicability of the Waste Factor framework to emerging 6G\nuse cases. The framework is then extended to a Fixed Wireless Access (FWA)\nscenario, where uplink and downlink asymmetries, traffic directionality, and\ncomponent inefficiencies are jointly considered to analyze energy-optimal\ndeployment strategies.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08414v1", "AI": {"title_translation": "废弃因子和废弃系数的理论基础及其在固定无线接入和中继系统中的应用", "tldr": "本文提出了废弃因子/系数作为统一的能量效率度量，并将其应用于固定无线接入和中继系统，以优化能耗。", "motivation": "随着无线通信系统能耗的指数级增长，尤其是在下一代无线系统背景下，需要严格的框架来评估和优化能量效率。", "method": "本文重新审视并扩展了废弃因子(W)或废弃系数(WF)的概念，作为捕获级联通信系统中已利用和浪费功率的统一指标。研究将废弃因子整合到功耗因子(CF)的精确公式中，并将其与香农的每比特能量理论极限直接关联，引入了附加能量浪费的概念。推导了直接链路和中继辅助链路中每比特能量消耗的闭式表达式，并开发了决策规则来确定在给定条件下哪种通信路径更节能。该框架被扩展到固定无线接入(FWA)场景，联合考虑了上行和下行不对称性、流量方向性和组件效率低下，以分析能量最优的部署策略。", "result": "推导出了直接链路和中继辅助链路中每比特能量消耗的闭式表达式，并开发了用于确定更节能通信路径的决策规则。废弃因子框架可以解释反射智能表面(RIS)作为中继架构的特例，表明其在6G用例中的广泛适用性。分析了固定无线接入(FWA)场景下的能量最优部署策略，考虑了上行/下行不对称性、流量方向性和组件效率低下。", "conclusion": "废弃因子/系数提供了一个统一且严格的框架，用于评估和优化无线通信系统的能量效率，特别是在固定无线接入和中继系统中，并有望应用于新兴的6G技术。", "translation": "无线通信系统，特别是下一代无线系统，能源消耗呈指数级增长，这使得评估和优化能源效率的严格框架变得必不可少。本文重新审视并扩展了废弃因子（W）或以分贝标度表示的废弃系数（WF）的概念，将其作为一种统一的度量指标，用于捕捉级联通信系统中已利用和浪费的功率。在系统级功率建模的基础上，我们将废弃因子整合到功耗因子（CF）（数据速率与总消耗功率之比）的精炼公式中，将其与香农的每比特能量理论极限直接关联。通过废弃因子项，本分析将附加能量浪费引入经典的每比特能量推导中。\n我们推导了直接链路和中继辅助链路中每比特能量消耗的闭式表达式，并开发了一个决策规则来确定在给定条件下哪种通信路径更节能。虽然未明确建模，但反射智能表面（RIS）在该统一公式中可以被解释为基于中继架构的特例，这表明废弃因子框架对新兴的6G用例具有更广泛的适用性。该框架随后被扩展到固定无线接入（FWA）场景，其中联合考虑了上行和下行不对称性、流量方向性和组件效率低下，以分析能量最优的部署策略。", "summary": "本文提出了废弃因子（Waste Factor, W）或废弃系数（Waste Figure, WF）作为衡量级联通信系统能量效率的统一指标，该指标能捕捉已利用和浪费的功率。研究将废弃因子整合到功耗因子（Consumption Factor, CF）的公式中，并将其与香农的每比特能量理论极限相关联，引入了附加能量浪费的概念。论文推导了直接链路和中继辅助链路的每比特能量消耗闭式表达式，并提出了节能路径选择的决策规则。此外，该框架被扩展应用于固定无线接入（FWA）场景，分析了上行/下行不对称性、流量方向性和组件效率低下对能量最优部署策略的影响，并指出其对6G用例的潜在适用性。", "keywords": "能量效率, 废弃因子, 固定无线接入, 中继系统, 6G", "comments": "这篇论文的创新点在于提出了“废弃因子”这一统一指标，并将其与香农理论极限结合，为评估和优化无线通信系统的能量效率提供了一个新的、更全面的理论框架。其对固定无线接入和中继系统的应用，以及对RIS和6G的潜在适用性，都显示出其重要的实际意义和前瞻性。"}}
{"id": "2503.19009", "title": "Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval", "authors": ["Arun Reddy", "Alexander Martin", "Eugene Yang", "Andrew Yates", "Kate Sanders", "Kenton Murray", "Reno Kriz", "Celso M. de Melo", "Benjamin Van Durme", "Rama Chellappa"], "summary": "In this work, we tackle the problem of text-to-video retrieval (T2VR).\nInspired by the success of late interaction techniques in text-document,\ntext-image, and text-video retrieval, our approach, Video-ColBERT, introduces a\nsimple and efficient mechanism for fine-grained similarity assessment between\nqueries and videos. Video-ColBERT is built upon 3 main components: a\nfine-grained spatial and temporal token-wise interaction, query and visual\nexpansions, and a dual sigmoid loss during training. We find that this\ninteraction and training paradigm leads to strong individual, yet compatible,\nrepresentations for encoding video content. These representations lead to\nincreases in performance on common text-to-video retrieval benchmarks compared\nto other bi-encoder methods.", "comment": "Accepted at CVPR 2025. 13 pages, 4 figures. Approved for public\n  release: distribution unlimited", "cate": "cs.CV", "url": "http://arxiv.org/abs/2503.19009v1", "AI": {"title_translation": "视频-ColBERT：用于文本到视频检索的上下文晚期交互", "tldr": "Video-ColBERT提出了一种新的晚期交互机制，通过改进表示学习，显著提升了文本到视频检索的性能。", "motivation": "旨在解决文本到视频检索（T2VR）问题，并引入一种简单高效的机制，用于查询和视频之间的细粒度相似性评估，其灵感来源于晚期交互技术在文本-文档、文本-图像和文本-视频检索领域的成功应用。", "method": "提出的方法是Video-ColBERT，它基于三个主要组件：细粒度的时空令牌级交互、查询和视觉扩展，以及训练过程中的双sigmoid损失。", "result": "该交互和训练范式能够生成强大且兼容的视频内容编码表示。与其他的双编码器方法相比，这些表示在常见的文本到视频检索基准上提升了性能。", "conclusion": "Video-ColBERT通过其创新的晚期交互机制和训练范式，成功地提高了文本到视频检索的性能，证明了其在生成高质量视频内容表示方面的有效性。", "translation": "在这项工作中，我们解决了文本到视频检索（T2VR）问题。受文本-文档、文本-图像和文本-视频检索中晚期交互技术成功的启发，我们的方法Video-ColBERT引入了一种简单高效的机制，用于查询和视频之间的细粒度相似性评估。Video-ColBERT建立在三个主要组件之上：细粒度的时空令牌级交互、查询和视觉扩展，以及训练过程中的双sigmoid损失。我们发现这种交互和训练范式能够生成强大且兼容的视频内容编码表示。与其他双编码器方法相比，这些表示在常见的文本到视频检索基准上提升了性能。", "summary": "本文针对文本到视频检索（T2VR）问题，提出了Video-ColBERT方法。该方法借鉴了晚期交互技术的成功经验，设计了一种简单高效的机制，用于实现查询与视频间的细粒度相似性评估。Video-ColBERT的核心包括细粒度的时空令牌级交互、查询及视觉扩展，以及双sigmoid损失训练。实验结果表明，这种交互和训练范式能够生成高质量的视频内容表示，并在主流文本到视频检索基准上超越了其他双编码器方法。", "keywords": "文本到视频检索, 晚期交互, Video-ColBERT, 表示学习, 多模态检索", "comments": "这篇论文的创新点在于将ColBERT的晚期交互思想引入到文本到视频检索领域，并针对视频数据的时空特性进行了组件设计（细粒度时空令牌级交互、查询和视觉扩展）。其重要性在于通过生成强大的兼容性表示，有效提升了T2VR的性能，为未来的多模态检索研究提供了新的方向。"}}
{"id": "2506.08921", "title": "Enabling stratified sampling in high dimensions via nonlinear dimensionality reduction", "authors": ["Gianluca Geraci", "Daniele E. Schiavazzi", "Andrea Zanoni"], "summary": "We consider the problem of propagating the uncertainty from a possibly large\nnumber of random inputs through a computationally expensive model. Stratified\nsampling is a well-known variance reduction strategy, but its application, thus\nfar, has focused on models with a limited number of inputs due to the\nchallenges of creating uniform partitions in high dimensions. To overcome these\nchallenges, we perform stratification with respect to the uniform distribution\ndefined over the unit interval, and then derive the corresponding strata in the\noriginal space using nonlinear dimensionality reduction. We show that our\napproach is effective in high dimensions and can be used to further reduce the\nvariance of multifidelity Monte Carlo estimators.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08921v1", "AI": {"title_translation": "通过非线性降维在高维空间中实现分层抽样", "tldr": "本文提出一种通过非线性降维在高维空间中实现分层抽样的方法，以解决传统分层抽样在高维空间中的挑战，并有效降低蒙特卡罗估计的方差。", "motivation": "传统分层抽样作为一种有效的方差削减策略，其应用受限于输入维度较少的情况，因为在高维空间中创建均匀划分面临挑战。本文旨在克服这些挑战。", "method": "作者首先在单位区间上定义的均匀分布进行分层，然后利用非线性降维技术，在原始空间中推导出相应的分层。", "result": "该方法在高维空间中表现出有效性，并且可以进一步降低多保真蒙特卡罗估计器的方差。", "conclusion": "通过将分层抽样与非线性降维相结合，可以有效地将分层抽样应用于高维问题，并显著提升蒙特卡罗模拟的效率和准确性。", "translation": "我们考虑将可能大量随机输入的 UQ 通过计算昂贵的模型进行传播的问题。分层抽样是一种著名的方差削减策略，但迄今为止，由于在高维空间中创建均匀划分的挑战，其应用主要集中在输入数量有限的模型上。为了克服这些挑战，我们针对定义在单位区间上的均匀分布进行分层，然后利用非线性降维在原始空间中推导出相应的分层。我们表明，我们的方法在高维空间中是有效的，并且可以用于进一步降低多保真蒙特卡罗估计器的方差。", "summary": "本文提出一种解决高维空间中分层抽样挑战的新方法。通过在单位区间上进行分层并结合非线性降维技术，该方法能够在原始高维空间中有效地构建分层，从而在高维问题中实现有效的方差削减，并可用于降低多保真蒙特卡罗估计器的方差。", "keywords": "分层抽样, 非线性降维, 高维, 方差削减, 蒙特卡罗", "comments": "该论文的创新之处在于将非线性降维技术引入分层抽样，有效解决了传统分层抽样在高维空间中的应用难题，显著扩展了其适用范围。这对于处理具有大量不确定性输入的复杂计算模型具有重要意义，尤其是在需要高效不确定性传播和方差削减的领域。"}}
{"id": "2506.08400", "title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks", "authors": ["Luel Hagos Beyene", "Vivek Verma", "Min Ma", "Jesujoba O. Alabi", "Fabian David Schmidt", "Joyce Nakatumba-Nabende", "David Ifeoluwa Adelani"], "summary": "Large Language models (LLMs) have demonstrated impressive performance on a\nwide range of tasks, including in multimodal settings such as speech. However,\ntheir evaluation is often limited to English and a few high-resource languages.\nFor low-resource languages, there is no standardized evaluation benchmark. In\nthis paper, we address this gap by introducing mSTEB, a new benchmark to\nevaluate the performance of LLMs on a wide range of tasks covering language\nidentification, text classification, question answering, and translation tasks\non both speech and text modalities. We evaluated the performance of leading\nLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open\nmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in\nperformance between high-resource and low-resource languages, especially for\nlanguages spoken in Africa and Americas/Oceania. Our findings show that more\ninvestment is needed to address their under-representation in LLMs coverage.", "comment": "working paper", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08400v1", "AI": {"title_translation": "mSTEB：大规模多语言评估LLM在语音和文本任务上的表现", "tldr": "mSTEB是一个新的基准，用于评估LLM在语音和文本任务上对大量语言（包括低资源语言）的性能，发现高资源和低资源语言之间存在显著性能差距。", "motivation": "现有的大型语言模型（LLM）评估主要局限于英语和少数高资源语言，对于低资源语言缺乏标准化的评估基准。", "method": "引入mSTEB基准，用于评估LLM在语音和文本模态上的多种任务，包括语言识别、文本分类、问答和翻译任务。评估了领先的LLM（如Gemini 2.0 Flash和GPT-4o (Audio)）和SOTA开源模型（如Qwen 2 Audio和Gemma 3 27B）。", "result": "评估结果显示，高资源语言和低资源语言之间存在显著的性能差距，特别是非洲、美洲/大洋洲地区的语言。", "conclusion": "需要投入更多资源来解决低资源语言在LLM覆盖中的代表性不足问题。", "translation": "大型语言模型（LLM）在包括语音等多种任务中表现出色。然而，它们的评估通常局限于英语和少数高资源语言。对于低资源语言，目前没有标准化的评估基准。在本文中，我们通过引入mSTEB解决了这一空白，mSTEB是一个新的基准，用于评估LLM在语音和文本模态上涵盖语言识别、文本分类、问答和翻译任务等广泛任务中的性能。我们评估了领先的LLM，如Gemini 2.0 Flash和GPT-4o (Audio)，以及最先进的开源模型，如Qwen 2 Audio和Gemma 3 27B的性能。我们的评估显示，高资源语言和低资源语言之间存在巨大性能差距，特别是非洲和美洲/大洋洲地区使用的语言。我们的研究结果表明，需要更多的投资来解决这些语言在LLM覆盖中的代表性不足问题。", "summary": "mSTEB是一个新推出的基准，旨在解决大型语言模型（LLM）在低资源语言评估方面缺乏标准化工具的问题。该基准涵盖语音和文本模态下的语言识别、文本分类、问答和翻译等多种任务。通过对Gemini 2.0 Flash、GPT-4o (Audio)等领先模型以及Qwen 2 Audio、Gemma 3 27B等开源模型的评估，研究发现高资源语言与低资源语言之间存在显著的性能差距，尤其是在非洲和美洲/大洋洲地区。这表明LLM在低资源语言覆盖方面仍需更多投入。", "keywords": "LLM评估, 多语言, 语音和文本, 低资源语言, 基准测试", "comments": "该论文通过引入mSTEB基准，填补了LLM在低资源语言评估方面的空白，具有重要的实践意义。其创新之处在于提供了涵盖语音和文本的多模态、大规模多语言评估框架。研究结果明确指出了当前LLM在低资源语言方面存在的不足，为未来的模型开发和资源投入提供了明确的方向。"}}
{"id": "2506.09002", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "comment": "13 pages, 5 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09002v1", "AI": {"title_translation": "通过混合程序分析和大型语言模型提升Rust单元测试覆盖率", "tldr": "PALM结合程序分析和LLM，显著提升Rust单元测试覆盖率，效果接近人工，为自动化测试提供新途径。", "motivation": "传统的搜索式软件测试(SBST)和符号执行方法在处理复杂程序单元时难以实现高覆盖率，而现有利用大型语言模型(LLM)生成测试的方法则受限于固定提示，导致编译成功率和覆盖率较低。", "method": "本文提出PALM方法，该方法通过程序分析识别函数中的分支条件，并将其组合成路径约束。这些约束与相关上下文信息被用于构建提示，以指导大型语言模型生成高覆盖率的单元测试。", "result": "在2-3小时内，PALM显著提高了测试覆盖率，某些项目整体覆盖率增加了50%以上；其生成的测试平均覆盖率达到75.77%，与人工水平（71.30%）相当。在提交的91个针对新代码的PALM生成单元测试中，有80个被接受。", "conclusion": "将程序分析与人工智能相结合，能够有效提升自动化软件测试的效率和覆盖率，为未来该领域的研究开辟了新的方向。", "translation": "单元测试对于确保软件的可靠性和正确性至关重要。经典的基于搜索的软件测试（SBST）方法和基于符号执行的方法在生成单元测试时，由于难以处理复杂的程序单元（如分支条件和外部依赖），常常无法实现高覆盖率。最近的工作越来越多地利用大型语言模型（LLM）来生成测试用例，通过提供更好的上下文和纠正模型输出中的错误来提高测试生成的质量。然而，这些方法依赖于固定的提示，导致编译成功率和覆盖率相对较低。本文提出了PALM，一种利用大型语言模型（LLM）来增强高覆盖率单元测试生成的方法。PALM执行程序分析以识别函数内的分支条件，然后将其组合成路径约束。这些约束和相关的上下文信息用于构建提示，以指导LLM生成单元测试。我们在10个开源Rust crate中实现了该方法并进行了评估。实验结果表明，在短短两到三个小时内，PALM可以显著提高测试覆盖率，与经典方法相比，某些情况下整体项目覆盖率增加了50%以上，其生成的测试平均覆盖率为75.77%，与人工努力（71.30%）相当，突出了LLM在自动化测试生成中的潜力。我们提交了91个针对新代码的PALM生成单元测试。在这些提交中，80个被接受，5个被拒绝，6个仍在等待审查。结果证明了程序分析与AI集成是有效的，并为自动化软件测试的未来研究开辟了新途径。", "summary": "本文提出PALM，一种结合混合程序分析和大型语言模型（LLMs）的方法，旨在提升Rust单元测试的覆盖率。PALM通过程序分析识别函数中的分支条件并构建路径约束，然后利用这些约束和相关上下文信息构建提示，指导LLM生成高覆盖率的单元测试。实验结果表明，PALM在短时间内显著提高了Rust项目的测试覆盖率，其生成的测试平均覆盖率高达75.77%，与人工水平相当，且大量生成的测试被实际接受，验证了该方法在自动化测试领域的有效性和潜力。", "keywords": "单元测试, Rust, 大型语言模型, 程序分析, 测试覆盖率", "comments": "PALM的创新之处在于其混合方法，结合了传统的程序分析与新兴的LLM技术，解决了传统方法难以处理复杂程序单元的问题，并克服了现有LLM方法固定提示的局限性。其在Rust项目上的显著效果，特别是与人工水平相当的覆盖率，突显了LLM在自动化测试领域的巨大潜力，为软件质量保证提供了新的高效工具。"}}
{"id": "2506.08795", "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning", "authors": ["Kaijie Shi", "Wanglong Lu", "Hanli Zhao", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "summary": "Limb loss affects millions globally, impairing physical function and reducing\nquality of life. Most traditional surface electromyographic (sEMG) and\nsemi-autonomous methods require users to generate myoelectric signals for each\ncontrol, imposing physically and mentally taxing demands. This study aims to\ndevelop a fully autonomous control system that enables a prosthetic hand to\nautomatically grasp and release objects of various shapes using only a camera\nattached to the wrist. By placing the hand near an object, the system will\nautomatically execute grasping actions with a proper grip force in response to\nthe hand's movements and the environment. To release the object being grasped,\njust naturally place the object close to the table and the system will\nautomatically open the hand. Such a system would provide individuals with limb\nloss with a very easy-to-use prosthetic control interface and greatly reduce\nmental effort while using. To achieve this goal, we developed a teleoperation\nsystem to collect human demonstration data for training the prosthetic hand\ncontrol model using imitation learning, which mimics the prosthetic hand\nactions from human. Through training the model using only a few objects' data\nfrom one single participant, we have shown that the imitation learning\nalgorithm can achieve high success rates, generalizing to more individuals and\nunseen objects with a variation of weights. The demonstrations are available at\n\\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08795v1", "AI": {"title_translation": "通过模仿学习实现无需生物信号的自主假肢手控制", "tldr": "该研究提出了一种基于摄像头的、无需生物信号的自主假肢手控制系统，通过模仿学习实现，并表现出高成功率和泛化能力。", "motivation": "全球数百万人受肢体缺失影响，导致身体功能受损并降低生活质量。大多数传统的假肢控制方法（如表面肌电图和半自主方法）要求用户为每次控制生成肌电信号，这带来了身体和精神上的巨大负担。因此，需要开发一种易于使用且完全自主的假肢控制系统。", "method": "研究人员开发了一个远程操作系统来收集人类演示数据，并利用模仿学习训练假肢手控制模型。该系统仅通过连接在手腕上的摄像头，根据手部动作和环境自动执行抓取和释放物体。通过将手靠近物体，系统将自动执行抓取动作；将物体靠近桌面，系统将自动打开手。", "result": "模仿学习算法取得了高成功率，即使仅使用单个参与者的少量物体数据进行训练，也能泛化到更多个体和未见过的物体，同时具有不同的权重。", "conclusion": "基于模仿学习和腕部摄像头的无需生物信号的自主假肢手控制系统，为肢体缺失者提供了一个易于使用的控制界面，大大减轻了他们的精神负担。", "translation": "全球数百万人受肢体缺失影响，导致身体功能受损并降低生活质量。大多数传统的表面肌电图（sEMG）和半自主方法要求用户为每次控制生成肌电信号，这带来了身体和精神上的巨大负担。本研究旨在开发一个完全自主的控制系统，使假肢手仅通过连接在手腕上的摄像头，就能自动抓取和释放各种形状的物体。通过将手靠近物体，系统将根据手部动作和环境自动执行具有适当抓握力的抓取动作。要释放被抓取的物体，只需自然地将物体靠近桌面，系统就会自动张开手。这样的系统将为肢体缺失者提供一个非常易于使用的假肢控制界面，并大大减少使用时的精神负担。为实现这一目标，我们开发了一个远程操作系统来收集人类演示数据，用于通过模仿学习训练假肢手控制模型，该模型模仿人类的假肢手动作。通过仅使用来自单个参与者的少量物体数据对模型进行训练，我们已经表明模仿学习算法可以实现高成功率，并能泛化到更多个体和未见过的物体，同时具有不同的权重。演示视频可在 \\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand} 查看。", "summary": "本研究提出了一种新型的无需生物信号的自主假肢手控制系统。该系统利用腕部摄像头和模仿学习，实现物体的自动抓取和释放，与传统肌电图方法相比，显著降低了用户负担。该系统在不同用户和未见过物体上均表现出高成功率和泛化能力。", "keywords": "假肢手, 自主控制, 模仿学习, 无生物信号, 计算机视觉", "comments": "这篇论文通过向真正的自主假肢控制迈进，实现了重要进展，将用户从生物信号生成的认知和身体负担中解放出来。其使用模仿学习，并在极少训练数据（单个参与者、少量物体）下实现强大的泛化能力，尤其具有创新性和实际应用前景。"}}
{"id": "2506.08299", "title": "OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal", "authors": ["Kangning Yang", "Ling Ouyang", "Huiming Sun", "Jie Cai", "Lan Fu", "Jiaming Ding", "Chiu Man Ho", "Zibo Meng"], "summary": "Reflection removal technology plays a crucial role in photography and\ncomputer vision applications. However, existing techniques are hindered by the\nlack of high-quality in-the-wild datasets. In this paper, we propose a novel\nparadigm for collecting reflection datasets from a fresh perspective. Our\napproach is convenient, cost-effective, and scalable, while ensuring that the\ncollected data pairs are of high quality, perfectly aligned, and represent\nnatural and diverse scenarios. Following this paradigm, we collect a\nReal-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which\ncontains 1,000 high-quality transmission-reflection image pairs collected in\nthe wild. Through the analysis of several reflection removal methods and\nbenchmark evaluation experiments on our dataset, we demonstrate its\neffectiveness in improving robustness in challenging real-world environments.\nOur dataset is available at https://github.com/caijie0620/OpenRR-1k.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08299v1", "AI": {"title_translation": "OpenRR-1k：一个用于真实世界反射去除的可扩展数据集", "tldr": "提出了一种新的范式来收集高质量、对齐的真实世界反射数据集OpenRR-1k，并证明其能提高反射去除方法的鲁棒性。", "motivation": "现有反射去除技术受限于缺乏高质量的“野外”数据集。", "method": "本文提出了一种方便、经济高效且可扩展的新范式来收集反射数据集，确保数据对高质量、完美对齐且代表自然多样场景。遵循此范式，研究者收集了一个名为OpenRR-1k的数据集，其中包含1000对在野外收集的高质量透射-反射图像。", "result": "通过对几种反射去除方法的分析以及在OpenRR-1k数据集上的基准评估实验，证明了该数据集在提高挑战性真实世界环境中反射去除方法鲁棒性方面的有效性。", "conclusion": "OpenRR-1k数据集能够有效提高反射去除方法在复杂真实世界环境中的鲁棒性。", "translation": "反射去除技术在摄影和计算机视觉应用中扮演着关键角色。然而，现有技术受限于缺乏高质量的“野外”数据集。在本文中，我们提出了一种从全新视角收集反射数据集的新范式。我们的方法方便、经济高效且可扩展，同时确保收集到的数据对高质量、完美对齐，并代表自然多样的场景。遵循此范式，我们收集了一个真实世界、多样化、像素对齐的数据集（命名为OpenRR-1k数据集），其中包含1000对在野外收集的高质量透射-反射图像。通过对几种反射去除方法的分析以及在我们数据集上的基准评估实验，我们证明了其在提高挑战性真实世界环境中鲁棒性方面的有效性。我们的数据集可在https://github.com/caijie0620/OpenRR-1k获取。", "summary": "针对现有反射去除技术缺乏高质量“野外”数据集的问题，本文提出了一种新颖、方便、经济且可扩展的数据集收集范式。基于此范式，研究者构建了OpenRR-1k数据集，包含1000对高质量、真实世界、多样化且像素对齐的透射-反射图像。实验证明，该数据集能有效提升反射去除方法在复杂真实环境中的鲁棒性。", "keywords": "反射去除, 数据集, OpenRR-1k, 真实世界, 图像处理", "comments": "这篇论文的创新点在于提出了一个收集高质量真实世界反射数据集的新范式，解决了现有数据集的不足。OpenRR-1k数据集的发布对于推动反射去除技术在实际应用中的发展具有重要意义，因为它提供了在野外收集的、对齐且多样化的数据，有助于训练和评估更鲁棒的模型。"}}
{"id": "2506.08486", "title": "RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being", "authors": ["Rahatara Ferdousi", "M Anwar Hossain"], "summary": "The rise of large language models (LLMs) has created new possibilities for\ndigital twins in healthcare. However, the deployment of such systems in\nconsumer health contexts raises significant concerns related to hallucination,\nbias, lack of transparency, and ethical misuse. In response to recommendations\nfrom health authorities such as the World Health Organization (WHO), we propose\nResponsible Health Twin (RHealthTwin), a principled framework for building and\ngoverning AI-powered digital twins for well-being assistance. RHealthTwin\nprocesses multimodal inputs that guide a health-focused LLM to produce safe,\nrelevant, and explainable responses. At the core of RHealthTwin is the\nResponsible Prompt Engine (RPE), which addresses the limitations of traditional\nLLM configuration. Conventionally, users input unstructured prompt and the\nsystem instruction to configure the LLM, which increases the risk of\nhallucination. In contrast, RPE extracts predefined slots dynamically to\nstructure both inputs. This guides the language model to generate responses\nthat are context aware, personalized, fair, reliable, and explainable for\nwell-being assistance. The framework further adapts over time through a\nfeedback loop that updates the prompt structure based on user satisfaction. We\nevaluate RHealthTwin across four consumer health domains including mental\nsupport, symptom triage, nutrition planning, and activity coaching. RPE\nachieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and\nBERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical\ncompliance and instruction-following metrics using LLM-as-judge evaluation,\noutperforming baseline strategies. We envision RHealthTwin as a forward-looking\nfoundation for responsible LLM-based applications in health and well-being.", "comment": "18 pages, 12 figures, IEEE EMBS JBHI", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08486v1", "AI": {"title_translation": "RHealthTwin：迈向负责任的多模态数字孪生以实现个性化健康", "tldr": "RHealthTwin是一个负责任的多模态数字孪生框架，通过负责任的提示引擎解决LLM在医疗健康领域的幻觉、偏见等问题，提供安全、相关、可解释的个性化健康协助，并在多个健康领域表现出色。", "motivation": "大型语言模型（LLMs）在医疗健康领域的数字孪生应用中面临幻觉、偏见、缺乏透明度和道德滥用等重大问题。为响应世界卫生组织（WHO）等健康机构的建议，需要开发一个原则性的框架来构建和管理AI驱动的健康数字孪生。", "method": "本文提出了负责任健康孪生（RHealthTwin）框架，该框架处理多模态输入，以指导面向健康的LLM生成安全、相关且可解释的响应。其核心是负责任提示引擎（RPE），它通过动态提取预定义槽来结构化输入，从而克服了传统LLM配置中无结构化提示导致幻觉增加的局限性。RPE使语言模型能够生成上下文感知、个性化、公平、可靠和可解释的响应。此外，该框架通过基于用户满意度的反馈循环进行自适应更新。", "result": "RPE在基准数据集上取得了最先进的结果：BLEU = 0.41，ROUGE-L = 0.63，BERTScore = 0.89。通过LLM-as-judge评估，在道德合规性和指令遵循指标上均超过90%，优于基线策略。该框架已在心理支持、症状分类、营养规划和活动指导等四个消费者健康领域进行了评估。", "conclusion": "RHealthTwin被设想为未来负责任的基于LLM的健康与福祉应用的开创性基础。", "translation": "大型语言模型（LLMs）的兴起为医疗健康领域的数字孪生带来了新的可能性。然而，此类系统在消费者健康环境中的部署引发了与幻觉、偏见、缺乏透明度和道德滥用相关的重大担忧。为响应世界卫生组织（WHO）等健康机构的建议，我们提出了负责任健康孪生（RHealthTwin），这是一个用于构建和管理AI驱动的个性化健康数字孪生的原则性框架。RHealthTwin处理多模态输入，以指导面向健康的LLM生成安全、相关且可解释的响应。RHealthTwin的核心是负责任提示引擎（RPE），它解决了传统LLM配置的局限性。传统上，用户输入非结构化提示和系统指令来配置LLM，这增加了幻觉的风险。相比之下，RPE动态提取预定义槽来结构化输入，从而引导语言模型生成上下文感知、个性化、公平、可靠和可解释的响应，以提供健康协助。该框架还通过一个反馈循环随时间进行适应性调整，该循环根据用户满意度更新提示结构。我们评估了RHealthTwin在心理支持、症状分类、营养规划和活动指导等四个消费者健康领域中的表现。RPE在基准数据集上取得了最先进的结果，BLEU = 0.41，ROUGE-L = 0.63，BERTScore = 0.89。此外，我们使用LLM-as-judge评估，在道德合规性和指令遵循指标上均超过90%，优于基线策略。我们设想RHealthTwin将成为未来负责任的基于LLM的健康与福祉应用的开创性基础。", "summary": "RHealthTwin是一个针对个性化健康的负责任多模态数字孪生框架，旨在解决大型语言模型在医疗健康领域部署时面临的幻觉、偏见和道德问题。该框架通过核心的负责任提示引擎（RPE）结构化多模态输入，引导LLM生成安全、相关、可解释且个性化的响应。RHealthTwin在心理支持、症状分类、营养规划和活动指导等多个健康领域进行了评估，并在性能和道德合规性方面均取得了最先进的成果，为负责任的LLM健康应用奠定了基础。", "keywords": "数字孪生, 负责任AI, 大型语言模型, 医疗健康, 提示工程", "comments": "RHealthTwin的创新之处在于其核心的“负责任提示引擎（RPE）”，它通过结构化输入有效解决了LLM在医疗健康领域面临的幻觉、偏见和缺乏透明度等关键挑战。该框架不仅关注模型性能，更强调伦理合规性和用户满意度，并通过反馈循环实现自适应，这对于LLM在敏感领域的安全和负责任部署具有重要意义。其多模态输入处理和个性化健康协助的特性也增加了其实用价值。"}}
{"id": "2506.08143", "title": "Accelerating Spectral Clustering under Fairness Constraints", "authors": ["Francesco Tonin", "Alex Lambert", "Johan A. K. Suykens", "Volkan Cevher"], "summary": "Fairness of decision-making algorithms is an increasingly important issue. In\nthis paper, we focus on spectral clustering with group fairness constraints,\nwhere every demographic group is represented in each cluster proportionally as\nin the general population. We present a new efficient method for fair spectral\nclustering (Fair SC) by casting the Fair SC problem within the difference of\nconvex functions (DC) framework. To this end, we introduce a novel variable\naugmentation strategy and employ an alternating direction method of multipliers\ntype of algorithm adapted to DC problems. We show that each associated\nsubproblem can be solved efficiently, resulting in higher computational\nefficiency compared to prior work, which required a computationally expensive\neigendecomposition. Numerical experiments demonstrate the effectiveness of our\napproach on both synthetic and real-world benchmarks, showing significant\nspeedups in computation time over prior art, especially as the problem size\ngrows. This work thus represents a considerable step forward towards the\nadoption of fair clustering in real-world applications.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08143v1", "AI": {"title_translation": "加速公平性约束下的谱聚类", "tldr": "本文提出了一种新的高效方法（Fair SC），通过将公平性约束下的谱聚类问题纳入凸函数差（DC）框架来解决，并采用变量增广策略和ADMM算法，显著提高了计算效率，尤其是在问题规模增大时。", "motivation": "决策算法的公平性问题日益重要，本文关注于具有群体公平性约束的谱聚类问题，即每个聚类中各人口群体的比例与总人口中相同。", "method": "提出了一种新的高效的公平谱聚类（Fair SC）方法，通过将Fair SC问题转换为凸函数差（DC）框架。引入了一种新颖的变量增广策略，并采用了适用于DC问题的交替方向乘子法（ADMM）算法。", "result": "数值实验表明，该方法在合成和真实世界基准测试上均有效，与现有技术相比，计算时间显著加快，尤其是在问题规模增大时。", "conclusion": "这项工作代表了在现实世界应用中采纳公平聚类向前迈出了重要一步。", "translation": "决策算法的公平性是一个日益重要的问题。在本文中，我们关注具有群体公平性约束的谱聚类，其中每个聚类中每个人口群体的代表比例与总人口中相同。我们通过将公平谱聚类（Fair SC）问题置于凸函数差（DC）框架内，提出了一种新的高效的公平谱聚类方法。为此，我们引入了一种新颖的变量增广策略，并采用了一种适用于DC问题的交替方向乘子法（ADMM）算法。我们表明，每个相关的子问题都可以高效求解，与需要计算昂贵的特征分解的先前工作相比，计算效率更高。数值实验证明了我们方法在合成和真实世界基准测试上的有效性，显示出比现有技术显著的计算时间加速，尤其是随着问题规模的增长。因此，这项工作代表了在现实世界应用中采纳公平聚类向前迈出了重要一步。", "summary": "本文提出了一种加速公平性约束下谱聚类的新方法（Fair SC）。该方法将公平谱聚类问题建模为凸函数差（DC）问题，并引入了创新的变量增广策略和交替方向乘子法（ADMM）算法。与传统方法相比，新方法避免了昂贵的特征分解，显著提高了计算效率，并在实验中展现出优越的性能和速度提升，为公平聚类在实际应用中的推广奠定了基础。", "keywords": "谱聚类, 公平性约束, 凸函数差, 计算效率, 群体公平", "comments": "本文的创新点在于将公平谱聚类问题巧妙地转化到凸函数差（DC）框架下，并设计了高效的ADMM算法来求解，有效规避了传统方法中计算量大的特征分解步骤。这极大地提升了算法的计算效率，使其在处理大规模数据时更具实用性，是推动公平聚类在实际应用中落地的关键一步。"}}
{"id": "2506.08836", "title": "Advancing STT for Low-Resource Real-World Speech", "authors": ["Flavio D'Intino", "Hans-Peter Hutter"], "summary": "Swiss German is a low-resource language represented by diverse dialects that\ndiffer significantly from Standard German and from each other, lacking a\nstandardized written form. As a result, transcribing Swiss German involves\ntranslating into Standard German. Existing datasets have been collected in\ncontrolled environments, yielding effective speech-to-text (STT) models, but\nthese models struggle with spontaneous conversational speech.\n  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour\nannotated speech corpus featuring real-world long-audio recordings from 39\nSwiss German radio and TV stations. It captures spontaneous speech across all\nmajor Swiss dialects recorded in various realistic environments and overcomes\nthe limitation of prior sentence-level corpora.\n  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,\nachieving notable enhancements over previous zero-shot performance metrics.\nImprovements in word error rate (WER) ranged from 19% to 33%, while BLEU scores\nincreased between 8% and 40%. The best fine-tuned model, large-v3, achieved a\nWER of 17.1% and a BLEU score of 74.8. This advancement is crucial for\ndeveloping effective and robust STT systems for Swiss German and other\nlow-resource languages in real-world contexts.", "comment": "Conference: HCI International 2025, 20 pages, 4 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08836v1", "AI": {"title_translation": "推进低资源真实世界语音的语音转文本技术", "tldr": "该研究介绍了SRB-300数据集，这是一个包含300小时瑞士德语真实世界语音的语料库，用于改进低资源语言的语音转文本（STT）模型。通过在该数据集上微调OpenAI Whisper模型，显著提高了WER和BLEU分数，有助于开发更鲁棒的STT系统。", "motivation": "瑞士德语是一种低资源语言，其方言多样且缺乏标准化书写形式，现有STT模型在受控环境下表现良好，但难以处理自发性会话语音。因此，需要新的数据集和方法来改进针对低资源语言的STT系统，使其适用于真实世界场景。", "method": "本研究引入了SRB-300数据集，这是一个300小时的标注语音语料库，包含来自39个瑞士德语广播电台和电视台的真实世界长音频录音，捕捉了各种现实环境中主要瑞士方言的自发性语音。研究人员在该数据集上微调了多个OpenAI Whisper模型。", "result": "通过在SRB-300数据集上微调，OpenAI Whisper模型的性能较之前的零样本表现有显著提升。词错误率（WER）改善了19%至33%，BLEU分数提高了8%至40%。其中，最佳微调模型large-v3的WER达到17.1%，BLEU分数达到74.8。", "conclusion": "这项进展对于开发针对瑞士德语及其他低资源语言在真实世界环境中有效且鲁棒的语音转文本（STT）系统至关重要。", "translation": "瑞士德语是一种低资源语言，其方言多样，与标准德语及彼此之间差异显著，且缺乏标准化的书面形式。因此，转录瑞士德语通常需要翻译成标准德语。现有数据集是在受控环境中收集的，虽然能产生有效的语音转文本（STT）模型，但这些模型在处理自发性会话语音时表现不佳。\n\n因此，本文引入了新的SRB-300数据集，这是一个300小时的标注语音语料库，包含来自39个瑞士德语广播电台和电视台的真实世界长音频录音。它捕捉了在各种现实环境中录制的所有主要瑞士方言的自发性语音，并克服了先前句子级语料库的局限性。\n\n我们基于SRB-300数据集对多个OpenAI Whisper模型进行了微调，与之前的零样本性能指标相比，取得了显著的提升。词错误率（WER）的改善范围为19%至33%，而BLEU分数提高了8%至40%。最佳微调模型large-v3的WER达到17.1%，BLEU分数达到74.8。这一进展对于在真实世界环境中开发针对瑞士德语及其他低资源语言的有效且鲁棒的STT系统至关重要。", "summary": "本研究针对瑞士德语这一低资源语言，其现有语音转文本（STT）模型难以处理自发性会话语音的问题，引入了SRB-300数据集。该数据集是一个300小时的标注语料库，包含来自广播电台和电视台的真实世界长音频，覆盖了瑞士德语的主要方言和多种现实环境下的自发性语音。通过在该数据集上微调OpenAI Whisper模型，研究取得了显著的性能提升，词错误率（WER）降低了19%至33%，BLEU分数提高了8%至40%。特别是最佳模型large-v3达到了17.1%的WER和74.8的BLEU分数。这项工作为开发针对低资源语言的鲁棒STT系统提供了关键进展。", "keywords": "低资源语言, 语音转文本, 瑞士德语, SRB-300数据集, Whisper模型", "comments": "本文的创新之处在于构建了一个大规模的、真实世界的、包含自发性语音的低资源语言（瑞士德语）数据集SRB-300。这一数据集的引入有效弥补了现有受控环境数据集的不足，并显著提升了OpenAI Whisper模型在处理真实世界自发性语音时的性能。研究结果表明，高质量、多样化的真实世界数据对于开发鲁棒的语音识别系统至关重要，为其他低资源语言的STT发展提供了有益的参考。"}}
{"id": "2506.08637", "title": "Stability estimates for adaptive focused time-frequency transforms", "authors": ["Pierre Warion", "Bruno Torrésani"], "summary": "This contribution is a follow-up of a recent paper by the authors on\nadaptive, non-linear time-frequency transforms, focusing on the STFT based\ntransforms. The adaptivity is provided by a focus function, that depends on the\nanalyzed function or signal, and that adapts dynamically the time-frequency\nresolution of the analysis. Sticking to the continuous case setting, this work\nprovides new stability results for the transform (stability with respect with\nthe focus function). It also investigates in some details focus functions based\nupon regularized R{\\'e}nyi entropies and show corresponding continuity results.", "comment": null, "cate": "math.CA", "url": "http://arxiv.org/abs/2506.08637v1", "AI": {"title_translation": "自适应聚焦时频变换的稳定性估计", "tldr": "本文提供了自适应聚焦时频变换的稳定性结果，特别是与聚焦函数相关的稳定性。", "motivation": "本文是对作者近期关于自适应非线性时频变换（特别是基于STFT的变换）的后续研究，旨在提供关于这些变换的稳定性结果。", "method": "该方法通过一个依赖于分析函数或信号的聚焦函数来动态调整时频分辨率。研究在连续情况下，提供了关于变换（相对于聚焦函数）的新稳定性结果，并详细研究了基于正则化Rényi熵的聚焦函数，展示了相应的连续性结果。", "result": "本文提供了变换（相对于聚焦函数）的新稳定性结果，并展示了基于正则化Rényi熵的聚焦函数的相应连续性结果。", "conclusion": "本文为自适应聚焦时频变换提供了重要的稳定性估计和连续性结果，特别是在连续情况下，增强了对这些变换特性的理解。", "translation": "本文是作者近期关于自适应非线性时频变换（主要基于STFT变换）的后续研究。自适应性由一个聚焦函数提供，该函数取决于被分析的函数或信号，并动态调整分析的时频分辨率。在连续情况下，这项工作为变换（相对于聚焦函数）提供了新的稳定性结果。它还详细研究了基于正则化Rényi熵的聚焦函数，并展示了相应的连续性结果。", "summary": "本文作为之前研究的延续，主要关注自适应聚焦时频变换的稳定性。通过引入一个动态调整时频分辨率的聚焦函数，本文在连续情况下为该变换提供了新的稳定性估计，并深入探讨了基于正则化Rényi熵的聚焦函数及其连续性。", "keywords": "时频变换, 稳定性估计, 自适应, 聚焦函数, Rényi熵", "comments": "本文在自适应时频分析领域具有重要意义，它解决了自适应变换的关键稳定性问题。通过引入聚焦函数并分析其稳定性，特别是与Rényi熵的结合，为理解和应用这些高级信号处理工具奠定了坚实的基础。"}}
{"id": "2506.08509", "title": "Predictive reinforcement learning based adaptive PID controller", "authors": ["Chaoqun Ma", "Zhiyong Zhang"], "summary": "Purpose: This study aims to address the challenges of controlling unstable\nand nonlinear systems by proposing an adaptive PID controller based on\npredictive reinforcement learning (PRL-PID), where the PRL-PID combines the\nadvantages of both data-driven and model-driven approaches.\nDesign/methodology/approach: A predictive reinforcement learning framework is\nintroduced, incorporating action smooth strategy to suppress overshoot and\noscillations, and a hierarchical reward function to support training. Findings:\nExperimental results show that the PRL-PID controller achieves superior\nstability and tracking accuracy in nonlinear, unstable, and strongly coupled\nsystems, consistently outperforming existing RL-tuned PID methods while\nmaintaining excellent robustness and adaptability across diverse operating\nconditions. Originality/Value: By adopting predictive learning, the proposed\nPRL-PID integrates system model priors into data-driven control, enhancing both\nthe control framework's training efficiency and the controller's stability. As\na result, PRL-PID provides a balanced blend of model-based and data-driven\napproaches, delivering robust, high-performance control.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08509v1", "AI": {"title_translation": "基于预测强化学习的自适应PID控制器", "tldr": "本文提出了一种基于预测强化学习（PRL-PID）的自适应PID控制器，结合数据驱动和模型驱动的优势，有效控制不稳定和非线性系统，并优于现有方法。", "motivation": "控制不稳定和非线性系统面临挑战。", "method": "引入预测强化学习框架，包含动作平滑策略以抑制过冲和振荡，以及分层奖励函数以支持训练。", "result": "PRL-PID控制器在非线性、不稳定和强耦合系统中表现出卓越的稳定性和跟踪精度，持续优于现有RL调优的PID方法，并在不同操作条件下保持出色的鲁棒性和适应性。", "conclusion": "PRL-PID通过将系统模型先验知识融入数据驱动控制，平衡了基于模型和数据驱动的方法，提供了鲁棒、高性能的控制，提高了训练效率和控制器稳定性。", "translation": "目的：本研究旨在通过提出一种基于预测强化学习的自适应PID控制器（PRL-PID）来解决控制不稳定和非线性系统的挑战，PRL-PID结合了数据驱动和模型驱动方法的优点。\n设计/方法/途径：引入了一个预测强化学习框架，其中包含了用于抑制过冲和振荡的动作平滑策略，以及用于支持训练的分层奖励函数。\n发现：实验结果表明，PRL-PID控制器在非线性、不稳定和强耦合系统中实现了卓越的稳定性和跟踪精度，持续优于现有RL调优的PID方法，同时在不同操作条件下保持了出色的鲁棒性和适应性。\n原创性/价值：通过采用预测学习，所提出的PRL-PID将系统模型先验知识集成到数据驱动控制中，从而提高了控制框架的训练效率和控制器的稳定性。因此，PRL-PID提供了基于模型和数据驱动方法的平衡融合，实现了鲁棒、高性能的控制。", "summary": "本文提出了一种基于预测强化学习（PRL-PID）的自适应PID控制器，旨在解决不稳定和非线性系统的控制难题。该控制器融合了数据驱动和模型驱动的优点，通过引入动作平滑策略和分层奖励函数进行训练。实验结果表明，PRL-PID在稳定性、跟踪精度、鲁棒性和适应性方面均优于现有RL调优的PID方法，尤其适用于非线性、不稳定和强耦合系统。", "keywords": "预测强化学习, 自适应PID控制器, 非线性系统控制, 数据驱动, 模型驱动", "comments": "该论文的创新之处在于将预测学习引入强化学习，使PRL-PID能够整合系统模型先验知识，从而在数据驱动控制中实现模型和数据的平衡，提高了训练效率和控制器稳定性。其价值在于为复杂、动态系统的鲁棒高性能控制提供了一种有效的新方法。"}}
{"id": "2506.08354", "title": "Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "summary": "This position paper argues that the text embedding research community should\nmove beyond surface meaning and embrace implicit semantics as a central\nmodeling goal. Text embedding models have become foundational in modern NLP,\npowering a wide range of applications and drawing increasing research\nattention. Yet, much of this progress remains narrowly focused on surface-level\nsemantics. In contrast, linguistic theory emphasizes that meaning is often\nimplicit, shaped by pragmatics, speaker intent, and sociocultural context.\nCurrent embedding models are typically trained on data that lacks such depth\nand evaluated on benchmarks that reward the capture of surface meaning. As a\nresult, they struggle with tasks requiring interpretive reasoning, speaker\nstance, or social meaning. Our pilot study highlights this gap, showing that\neven state-of-the-art models perform only marginally better than simplistic\nbaselines on implicit semantics tasks. To address this, we call for a paradigm\nshift: embedding research should prioritize more diverse and linguistically\ngrounded training data, design benchmarks that evaluate deeper semantic\nunderstanding, and explicitly frame implicit meaning as a core modeling\nobjective, better aligning embeddings with real-world language complexity.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08354v1", "AI": {"title_translation": "文本嵌入应捕捉隐式语义，而非仅仅表层含义", "tldr": "文本嵌入研究应超越表层语义，以隐式语义为核心建模目标，因为现有模型在需要深层理解的任务上表现不佳。", "motivation": "目前的文本嵌入研究过于狭隘地关注表层语义，忽视了语言学理论中强调的由语用、说话者意图和社会文化语境塑造的隐式含义。这导致现有模型在需要解释性推理、说话者立场或社会意义的任务上表现不佳。", "method": "本文是一篇立场论文，通过一项初步研究（pilot study）揭示了现有模型在隐式语义任务上的不足。它提出了一种范式转变的方法，即优先考虑更多样化和基于语言学的训练数据，设计评估深层语义理解的基准，并将隐式意义明确地作为核心建模目标。", "result": "一项初步研究表明，即使是最先进的模型在隐式语义任务上的表现也仅略优于简单的基线模型，突显了这一差距。", "conclusion": "文本嵌入研究应进行范式转变：优先使用更多样化和基于语言学的训练数据，设计评估更深层语义理解的基准，并明确将隐式意义作为核心建模目标，以更好地使嵌入与真实世界的语言复杂性保持一致。", "translation": "这篇立场论文认为，文本嵌入研究社区应该超越表层含义，将隐式语义作为核心建模目标。文本嵌入模型已成为现代自然语言处理的基础，为广泛的应用提供支持，并吸引了越来越多的研究关注。然而，大部分进展仍然狭隘地集中在表层语义上。相比之下，语言学理论强调意义往往是隐式的，受语用、说话者意图和社会文化语境的影响。当前的嵌入模型通常在缺乏这种深度的T数据上进行训练，并在奖励捕捉表层含义的基准上进行评估。结果，它们在需要解释性推理、说话者立场或社会意义的任务上表现不佳。我们的初步研究突出了这一差距，表明即使是最先进的模型在隐式语义任务上的表现也仅略优于简单的基线。为了解决这个问题，我们呼吁一场范式转变：嵌入研究应优先考虑更多样化和基于语言学的训练数据，设计评估更深层语义理解的基准，并明确将隐式意义作为核心建模目标，从而更好地使嵌入与真实世界的语言复杂性保持一致。", "summary": "这篇立场论文主张文本嵌入研究应将重点从表层语义转向隐式语义。论文指出，当前的文本嵌入模型过度关注表层含义，忽视了语言学中关键的隐式意义，导致它们在需要深层理解的任务上表现不足。一项初步研究证实了这一缺陷，显示现有模型在隐式语义任务上的表现不佳。为此，论文呼吁进行范式转变，建议采用更多样化和语言学基础的训练数据，开发评估深层语义的基准，并将隐式意义明确设定为核心建模目标，以更好地捕捉真实世界的语言复杂性。", "keywords": "文本嵌入, 隐式语义, 表层含义, 自然语言处理, 语言学理论", "comments": "这篇论文具有重要意义，因为它指出了当前文本嵌入模型的一个关键局限性，并为未来的研究提供了明确的方向。它强调了开发能够捕捉更深层、语境化语义的嵌入模型的必要性，而不仅仅是表层词义。这对于推动NLP在理解人类语言的细微差别和复杂性方面至关重要，并促使研究人员重新思考数据收集和模型评估策略。"}}
{"id": "2506.08937", "title": "Asymptotic error distribution for stochastic Runge--Kutta methods of strong order one", "authors": ["Diancong Jin"], "summary": "This work gives the asymptotic error distribution of the stochastic\nRunge--Kutta (SRK) method of strong order $1$ applied to Stratonovich-type\nstochastic differential equations. For dealing with the implicitness introduced\nin the diffusion term, we provide a framework to derive the asymptotic error\ndistribution of diffusion-implicit or fully implicit numerical methods, which\nenables us to construct a fully explicit numerical method sharing the same\nasymptotic error distribution as the SRK method. Further, we show for the\nmultiplicative noise that the limit distribution $U(T)$ satisfies $\\mathbf\nE|U(T)|^2\\le e^{L_1T}(1+\\eta_1)T^3$ for some $\\eta_1$ only depending on the\ncoefficients of the SRK method. Thus, we infer that $\\eta_1$ is the key\nparameter reflecting the growth rate of the mean-square error of the SRK\nmethod. Especially, among the SRK methods of strong order $1$, those of weak\norder $2$ ($\\eta_1=0$) share the unified asymptotic error distribution and have\nthe smallest mean-square errors after a long time. This property is also found\nfor the case of additive noise. It seems that we are the first to give the\nasymptotic error distribution of fully implicit numerical methods for SDEs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.08937v1", "AI": {"title_translation": "强一阶随机Runge--Kutta方法的渐近误差分布", "tldr": "本文研究了应用于Stratonovich型随机微分方程的强一阶随机Runge--Kutta (SRK) 方法的渐近误差分布，并提出了一种处理隐式扩散项的框架，发现弱二阶SRK方法具有最小的均方误差。", "motivation": "为了处理扩散项中引入的隐式性，并推导扩散隐式或全隐式数值方法的渐近误差分布。", "method": "提供了一个框架来推导扩散隐式或全隐式数值方法的渐近误差分布，并构建了一个与SRK方法具有相同渐近误差分布的全显式数值方法。", "result": "构建了一个与SRK方法具有相同渐近误差分布的全显式数值方法。对于乘性噪声，极限分布U(T)满足E|U(T)|^2≤e^{L_1T}(1+η_1)T^3，其中η_1是反映SRK方法均方误差增长率的关键参数。在强一阶SRK方法中，弱二阶方法（η_1=0）具有统一的渐近误差分布，并且在长时间后具有最小的均方误差。加性噪声情况下也发现了此特性。", "conclusion": "η_1是反映SRK方法均方误差增长率的关键参数。在强一阶SRK方法中，弱二阶方法具有统一的渐近误差分布，并且在长时间后具有最小的均方误差，这表明它们是这类方法中的最优选择。", "translation": "这项工作给出了应用于Stratonovich型随机微分方程的强一阶随机Runge--Kutta (SRK) 方法的渐近误差分布。为了处理扩散项中引入的隐式性，我们提供了一个框架来推导扩散隐式或全隐式数值方法的渐近误差分布，这使得我们能够构建一个与SRK方法具有相同渐近误差分布的全显式数值方法。此外，我们证明了对于乘性噪声，极限分布U(T)满足E|U(T)|^2≤e^{L_1T}(1+η_1)T^3，其中η_1仅取决于SRK方法的系数。因此，我们推断η_1是反映SRK方法均方误差增长率的关键参数。特别是，在强一阶SRK方法中，弱二阶方法（η_1=0）共享统一的渐近误差分布，并且在长时间后具有最小的均方误差。加性噪声情况下也发现了此特性。似乎我们是第一个给出SDE全隐式数值方法渐近误差分布的。", "summary": "本文研究了应用于Stratonovich型随机微分方程的强一阶随机Runge--Kutta (SRK) 方法的渐近误差分布。为处理扩散项的隐式性，作者提出了一个推导扩散隐式或全隐式数值方法渐近误差分布的框架，并据此构建了一个与SRK方法拥有相同渐近误差分布的全显式方法。研究发现，一个名为η_1的参数是反映SRK方法均方误差增长率的关键。特别地，在所有强一阶SRK方法中，弱二阶方法（η_1=0）不仅共享统一的渐近误差分布，而且在长时间后表现出最小的均方误差。", "keywords": "随机Runge--Kutta方法, 渐近误差分布, 随机微分方程, 强一阶, 弱二阶", "comments": "本文的主要创新在于首次给出了随机微分方程全隐式数值方法的渐近误差分布，并提出了一个通用的框架来分析此类方法的误差。研究发现，在强一阶SRK方法中，弱二阶方法在均方误差方面表现最优，这对于数值方法的设计和选择具有重要的指导意义。"}}
{"id": "2506.08564", "title": "Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?", "authors": ["Tuukka Törö", "Antti Suni", "Juraj Šimko"], "summary": "Investigating linguistic relationships on a global scale requires analyzing\ndiverse features such as syntax, phonology and prosody, which evolve at varying\nrates influenced by internal diversification, language contact, and\nsociolinguistic factors. Recent advances in machine learning (ML) offer\ncomplementary alternatives to traditional historical and typological\napproaches. Instead of relying on expert labor in analyzing specific linguistic\nfeatures, these new methods enable the exploration of linguistic variation\nthrough embeddings derived directly from speech, opening new avenues for\nlarge-scale, data-driven analyses.\n  This study employs embeddings from the fine-tuned XLS-R self-supervised\nlanguage identification model voxlingua107-xls-r-300m-wav2vec, to analyze\nrelationships between 106 world languages based on speech recordings. Using\nlinear discriminant analysis (LDA), language embeddings are clustered and\ncompared with genealogical, lexical, and geographical distances. The results\ndemonstrate that embedding-based distances align closely with traditional\nmeasures, effectively capturing both global and local typological patterns.\nChallenges in visualizing relationships, particularly with hierarchical\nclustering and network-based methods, highlight the dynamic nature of language\nchange.\n  The findings show potential for scalable analyses of language variation based\non speech embeddings, providing new perspectives on relationships among\nlanguages. By addressing methodological considerations such as corpus size and\nlatent space dimensionality, this approach opens avenues for studying\nlow-resource languages and bridging macro- and micro-level linguistic\nvariation. Future work aims to extend these methods to underrepresented\nlanguages and integrate sociolinguistic variation for a more comprehensive\nunderstanding of linguistic diversity.", "comment": "27 pages, 11 figures (+5 supplementary), submitted to PLOS One", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08564v1", "AI": {"title_translation": "邻居与亲属：语音嵌入如何反映世界范围内的语言联系？", "tldr": "本研究利用语音嵌入（来自XLS-R模型）分析了106种世界语言之间的关系，发现其结果与传统语言学度量高度一致，为大规模语言变异分析提供了新视角。", "motivation": "传统语言学方法分析全球语言关系需要大量专家劳动，且难以处理大规模数据。机器学习的最新进展为通过语音嵌入直接探索语言变异提供了新的、数据驱动的途径。", "method": "本研究使用经过微调的XLS-R自监督语言识别模型voxlingua107-xls-r-300m-wav2vec产生的语音嵌入，分析了106种世界语言。利用线性判别分析（LDA）对语言嵌入进行聚类，并与谱系、词汇和地理距离进行比较。", "result": "结果表明，基于嵌入的距离与传统测量方法高度一致，能有效捕捉全球和局部类型学模式。但可视化关系（特别是使用层次聚类和基于网络的方法）存在挑战，凸显了语言变化的动态性。", "conclusion": "语音嵌入在可扩展的语言变异分析方面显示出巨大潜力，为语言间关系提供了新视角。该方法通过解决语料库大小和潜在空间维度等问题，为研究低资源语言和连接宏观与微观语言变异开辟了道路。", "translation": "调查全球范围内的语言关系需要分析语法、音系和韵律等多种特征，这些特征以不同的速率演变，受内部多样化、语言接触和社会语言学因素的影响。机器学习（ML）的最新进展为传统的历史和类型学方法提供了补充替代方案。这些新方法不再依赖专家劳动来分析特定的语言特征，而是通过直接从语音中提取的嵌入来探索语言变异，为大规模、数据驱动的分析开辟了新途径。\n本研究采用经过微调的XLS-R自监督语言识别模型voxlingua107-xls-r-300m-wav2vec的嵌入，基于语音记录分析了106种世界语言之间的关系。使用线性判别分析（LDA），对语言嵌入进行聚类，并与谱系、词汇和地理距离进行比较。结果表明，基于嵌入的距离与传统测量方法高度一致，有效捕捉了全球和局部类型学模式。在可视化关系方面遇到的挑战，特别是使用层次聚类和基于网络的方法时，突出了语言变化的动态性。\n研究结果显示了基于语音嵌入进行语言变异可扩展分析的潜力，为语言间关系提供了新视角。通过解决语料库大小和潜在空间维度等方法论问题，该方法为研究低资源语言和连接宏观与微观语言变异开辟了途径。未来的工作旨在将这些方法扩展到代表性不足的语言，并整合社会语言学变异，以更全面地理解语言多样性。", "summary": "这项研究利用预训练的语音嵌入（来自XLS-R模型）分析了106种世界语言之间的关系，旨在为大规模语言变异研究提供数据驱动的替代方案。通过线性判别分析，研究发现语音嵌入捕捉到的语言间距离与传统的谱系、词汇和地理距离高度吻合，证明了其在反映全球和局部语言模式方面的有效性。尽管可视化复杂关系存在挑战，但该方法为可扩展的语言分析提供了新视角，并有望应用于低资源语言研究。", "keywords": "语音嵌入, 语言关系, XLS-R, 线性判别分析, 语言变异", "comments": "这篇论文创新性地将先进的自监督语音嵌入技术应用于大规模语言关系分析，为传统语言学研究提供了强大的计算工具。其重要性在于验证了机器学习方法在捕捉复杂语言连接方面的有效性，并为低资源语言研究开辟了新途径。然而，论文也指出可视化复杂语言动态性仍是挑战，这可能限制了对细微语言演变过程的直观理解。"}}
{"id": "2506.08347", "title": "Differentially Private Relational Learning with Entity-level Privacy Guarantees", "authors": ["Yinan Huang", "Haoteng Ying", "Eli Chien", "Rongzhe Wei", "Pan Li"], "summary": "Learning with relational and network-structured data is increasingly vital in\nsensitive domains where protecting the privacy of individual entities is\nparamount. Differential Privacy (DP) offers a principled approach for\nquantifying privacy risks, with DP-SGD emerging as a standard mechanism for\nprivate model training. However, directly applying DP-SGD to relational\nlearning is challenging due to two key factors: (i) entities often participate\nin multiple relations, resulting in high and difficult-to-control sensitivity;\nand (ii) relational learning typically involves multi-stage, potentially\ncoupled (interdependent) sampling procedures that make standard privacy\namplification analyses inapplicable. This work presents a principled framework\nfor relational learning with formal entity-level DP guarantees. We provide a\nrigorous sensitivity analysis and introduce an adaptive gradient clipping\nscheme that modulates clipping thresholds based on entity occurrence frequency.\nWe also extend the privacy amplification results to a tractable subclass of\ncoupled sampling, where the dependence arises only through sample sizes. These\ncontributions lead to a tailored DP-SGD variant for relational data with\nprovable privacy guarantees. Experiments on fine-tuning text encoders over\ntext-attributed network-structured relational data demonstrate the strong\nutility-privacy trade-offs of our approach. Our code is available at\nhttps://github.com/Graph-COM/Node_DP.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08347v1", "AI": {"title_translation": "差分隐私关系学习与实体级隐私保障", "tldr": "本文提出了一种针对关系数据的差分隐私SGD变体，通过解决实体参与多关系导致的高敏感度和耦合采样问题，实现了形式化的实体级隐私保障。", "motivation": "在敏感领域，关系和网络结构化数据的学习至关重要，但直接将DP-SGD应用于关系学习面临挑战：实体多重参与关系导致高敏感度；关系学习中的多阶段耦合采样使标准隐私放大分析失效。", "method": "提出了一个具有形式化实体级DP保障的关系学习框架。具体方法包括：严格的敏感度分析，引入基于实体出现频率调节裁剪阈值的自适应梯度裁剪方案，以及将隐私放大结果扩展到可处理的耦合采样子类（依赖仅通过样本大小产生）。这些贡献共同形成了一个为关系数据量身定制的DP-SGD变体。", "result": "在文本归因网络结构化关系数据上微调文本编码器的实验表明，该方法具有强大的效用-隐私权衡。", "conclusion": "该工作提供了一个为关系数据量身定制的DP-SGD变体，具有可证明的隐私保障，有效解决了在关系数据上应用差分隐私的挑战。", "translation": "关系和网络结构化数据的学习在保护个体实体隐私至关重要的敏感领域中变得越来越重要。差分隐私（DP）为量化隐私风险提供了一种原则性方法，其中DP-SGD已成为私有模型训练的标准机制。然而，由于两个关键因素，直接将DP-SGD应用于关系学习具有挑战性：(i) 实体通常参与多个关系，导致敏感度高且难以控制；(ii) 关系学习通常涉及多阶段、可能耦合（相互依赖）的采样过程，这使得标准隐私放大分析不适用。这项工作提出了一个具有形式化实体级DP保障的关系学习的原则性框架。我们提供了严格的敏感度分析，并引入了一种自适应梯度裁剪方案，该方案根据实体出现频率调节裁剪阈值。我们还将隐私放大结果扩展到可处理的耦合采样子类，其中依赖性仅通过样本大小产生。这些贡献导致了一种为关系数据量身定制的DP-SGD变体，具有可证明的隐私保障。在文本归因网络结构化关系数据上微调文本编码器的实验证明了我们方法的强大效用-隐私权衡。我们的代码可在https://github.com/Graph-COM/Node_DP 获取。", "summary": "本文针对关系学习中应用差分隐私（DP-SGD）的挑战，提出了一种新的框架。该框架通过严格的敏感度分析、自适应梯度裁剪以及对特定耦合采样场景的隐私放大扩展，开发了一个为关系数据定制的DP-SGD变体，实现了可证明的实体级隐私保障。实验证明了该方法在效用和隐私保护之间取得了良好的平衡。", "keywords": "差分隐私, 关系学习, 实体级隐私, DP-SGD, 敏感度分析", "comments": "这项工作的创新之处在于为关系数据量身定制了差分隐私SGD，解决了传统DP-SGD在多关系和耦合采样场景下的局限性。其提出的自适应梯度裁剪和隐私放大扩展方法，为在复杂关系数据上实现严格的实体级隐私保障提供了可行的方案，对于推动敏感领域的关系数据分析具有重要意义。"}}
{"id": "2506.08713", "title": "Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure", "authors": ["Fariz Ikhwantri", "Dusica Marijan"], "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08713v1", "AI": {"title_translation": "基于保证案例结构的多跳自然语言推理可解释性合规检测", "tldr": "本文提出了一种名为EXCLAIM的基于自然语言推理（NLI）的合规检测方法，通过将保证案例结构表述为多跳推理，以实现可解释和可追溯的合规检测，并利用大型语言模型生成保证案例数据来解决数据稀缺问题，展示了NLI在自动化法规合规过程中的潜力。", "motivation": "确保复杂系统符合法规通常需要通过声明-论证-证据框架检查保证案例的有效性。这个过程中的挑战包括法律和技术文本的复杂性、模型解释的需求以及保证案例数据访问受限。", "method": "本文提出了一种基于自然语言推理（NLI）的合规检测方法：EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM)。该方法将保证案例的声明-论证-证据结构表述为多跳推理，以实现可解释和可追溯的合规检测。为了解决保证案例数量有限的问题，研究人员使用大型语言模型（LLM）生成了保证案例。此外，还引入了衡量覆盖率和结构一致性的指标。", "result": "研究人员在GDPR要求的多跳推理任务中，通过案例研究展示了生成保证案例的有效性。结果强调了基于NLI的方法在自动化法规合规过程中的潜力。", "conclusion": "基于NLI的方法在自动化法规合规过程中具有显著潜力，通过多跳推理和LLM生成数据，可以实现可解释和可追溯的合规检测。", "translation": "确保复杂系统符合法规通常需要通过声明-论证-证据框架检查保证案例的有效性。这个过程中的挑战包括法律和技术文本的复杂性、模型解释的需求以及保证案例数据访问受限。我们提出了一种基于自然语言推理（NLI）的合规检测方法：EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM)。我们将保证案例的声明-论证-证据结构表述为多跳推理，以实现可解释和可追溯的合规检测。我们通过使用大型语言模型（LLMs）生成保证案例来解决保证案例数量有限的问题。我们引入了衡量覆盖率和结构一致性的指标。我们以案例研究的形式，在GDPR要求的多跳推理任务中展示了生成保证案例的有效性。我们的结果强调了基于NLI的方法在自动化法规合规过程中的潜力。", "summary": "本文提出了一种名为EXCLAIM的基于自然语言推理（NLI）的合规检测方法，旨在解决复杂系统合规性检查中法律文本复杂、模型解释需求和数据受限等挑战。EXCLAIM将保证案例的声明-论证-证据结构公式化为多跳推理，以实现可解释和可追溯的合规检测。为应对保证案例数据稀缺问题，研究利用大型语言模型生成数据，并引入了衡量覆盖率和结构一致性的指标。通过GDPR要求的案例研究，验证了生成保证案例在多跳推理任务中的有效性，突显了NLI方法在自动化法规合规流程中的巨大潜力。", "keywords": "合规检测, 自然语言推理, 多跳推理, 保证案例, 大型语言模型", "comments": "该论文通过将保证案例结构与多跳自然语言推理相结合，提供了一种新颖且可解释的合规检测方法。利用大型语言模型生成训练数据是解决领域数据稀缺性的重要创新点。这对于自动化复杂法规遵从性检查具有重要意义，尤其是在需要高可解释性和可追溯性的场景中。"}}
{"id": "2506.08822", "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency", "authors": ["Yifei Su", "Ning Liu", "Dong Chen", "Zhen Zhao", "Kun Wu", "Meng Li", "Zhiyuan Xu", "Zhengping Che", "Jian Tang"], "summary": "Generative modeling-based visuomotor policies have been widely adopted in\nrobotic manipulation attributed to their ability to model multimodal action\ndistributions. However, the high inference cost of multi-step sampling limits\ntheir applicability in real-time robotic systems. To address this issue,\nexisting approaches accelerate the sampling process in generative\nmodeling-based visuomotor policies by adapting acceleration techniques\noriginally developed for image generation. Despite this progress, a major\ndistinction remains: image generation typically involves producing independent\nsamples without temporal dependencies, whereas robotic manipulation involves\ngenerating time-series action trajectories that require continuity and temporal\ncoherence. To effectively exploit temporal information in robotic manipulation,\nwe propose FreqPolicy, a novel approach that first imposes frequency\nconsistency constraints on flow-based visuomotor policies. Our work enables the\naction model to capture temporal structure effectively while supporting\nefficient, high-quality one-step action generation. We introduce a frequency\nconsistency constraint that enforces alignment of frequency-domain action\nfeatures across different timesteps along the flow, thereby promoting\nconvergence of one-step action generation toward the target distribution. In\naddition, we design an adaptive consistency loss to capture structural temporal\nvariations inherent in robotic manipulation tasks. We assess FreqPolicy on 53\ntasks across 3 simulation benchmarks, proving its superiority over existing\none-step action generators. We further integrate FreqPolicy into the\nvision-language-action (VLA) model and achieve acceleration without performance\ndegradation on the 40 tasks of Libero. Besides, we show efficiency and\neffectiveness in real-world robotic scenarios with an inference frequency\n93.5Hz. The code will be publicly available.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08822v1", "AI": {"title_translation": "FreqPolicy: 基于频率一致性的高效流式视觉运动策略", "tldr": "FreqPolicy通过引入频率一致性约束，实现了高效、高质量的单步动作生成，解决了生成模型在机器人操作中高推理成本和时间依赖性问题。", "motivation": "现有的基于生成模型的视觉运动策略在机器人操作中因多步采样的高推理成本而受限，且现有加速方法未充分考虑机器人操作中动作轨迹的时间连续性和相干性。", "method": "提出FreqPolicy，一种新颖的方法，首次对基于流的视觉运动策略施加频率一致性约束，使动作模型有效捕获时间结构并支持高效、高质量的单步动作生成。引入频率一致性约束以强制流中不同时间步的频域动作特征对齐，促进单步动作生成收敛到目标分布。设计自适应一致性损失以捕获机器人操作任务中固有的结构性时间变化。", "result": "FreqPolicy在3个模拟基准的53项任务上评估，优于现有单步动作生成器。将其集成到视觉-语言-动作（VLA）模型中，在Libero的40项任务上实现加速且无性能下降。在真实世界机器人场景中展现效率和有效性，推理频率达到93.5Hz。", "conclusion": "FreqPolicy通过引入频率一致性约束和自适应一致性损失，成功解决了生成模型在机器人操作中高推理成本和时间依赖性问题，实现了高效、高性能的视觉运动策略。", "translation": "基于生成模型的视觉运动策略因其能够模拟多模态动作分布而被广泛应用于机器人操作中。然而，多步采样的高推理成本限制了它们在实时机器人系统中的适用性。为了解决这个问题，现有方法通过调整最初为图像生成开发的速度加速技术来加速基于生成模型的视觉运动策略中的采样过程。尽管取得了进展，但一个主要区别仍然存在：图像生成通常涉及生成没有时间依赖性的独立样本，而机器人操作涉及生成需要连续性和时间相干性的时间序列动作轨迹。为了有效利用机器人操作中的时间信息，我们提出了FreqPolicy，一种新颖的方法，它首次对基于流的视觉运动策略施加频率一致性约束。我们的工作使动作模型能够有效捕获时间结构，同时支持高效、高质量的单步动作生成。我们引入了一种频率一致性约束，强制流中不同时间步的频域动作特征对齐，从而促进单步动作生成收敛到目标分布。此外，我们设计了一种自适应一致性损失来捕获机器人操作任务中固有的结构性时间变化。我们在3个模拟基准的53项任务上评估了FreqPolicy，证明其优于现有单步动作生成器。我们进一步将FreqPolicy集成到视觉-语言-动作（VLA）模型中，并在Libero的40项任务上实现了加速而没有性能下降。此外，我们还在真实世界机器人场景中展示了效率和有效性，推理频率达到93.5Hz。代码将公开可用。", "summary": "本文提出了FreqPolicy，一种新颖的基于流的视觉运动策略，通过引入频率一致性约束和自适应一致性损失，解决了现有生成模型在机器人操作中高推理成本和时间依赖性问题。FreqPolicy能够有效捕获动作的时间结构，支持高效、高质量的单步动作生成，并在模拟和真实世界机器人任务中展现出优越的性能和效率。", "keywords": "Visuomotor Policy, Frequency Consistency, Robotic Manipulation, Flow-based Model, Real-time Control", "comments": "FreqPolicy的创新点在于首次将频率一致性约束引入流式视觉运动策略，有效解决了机器人操作中动作时间序列的连续性和相干性问题，同时实现了高效的单步动作生成，对于实时机器人系统具有重要意义。"}}
{"id": "2506.08324", "title": "Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating", "authors": ["Guandong Li", "Mengxia Ye"], "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more effectively extract\nand fuse spatial context with fine spectral information in hyperspectral image\n(HSI) classification, this paper proposes a novel network architecture called\nSTNet. The core advantage of STNet stems from the dual innovative design of its\nSpatial-Spectral Transformer module: first, the fundamental explicit decoupling\nof spatial and spectral attention ensures targeted capture of key information\nin HSI; second, two functionally distinct gating mechanisms perform intelligent\nregulation at both the fusion level of attention flows (adaptive attention\nfusion gating) and the internal level of feature transformation (GFFN). This\ncharacteristic demonstrates superior feature extraction and fusion capabilities\ncompared to traditional convolutional neural networks, while reducing\noverfitting risks in small-sample and high-noise scenarios. STNet enhances\nmodel representation capability without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.", "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15155,\n  arXiv:2504.13045, arXiv:2503.23472", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08324v1", "AI": {"title_translation": "高光谱图像分类：基于Transformer的光谱-空间注意力解耦与自适应门控", "tldr": "本文提出STNet，通过Transformer模块解耦光谱空间注意力并引入自适应门控机制，有效解决高光谱图像分类中过拟合和泛化能力差的问题，在多个数据集上表现优越，超越主流方法。", "motivation": "深度神经网络在高光谱图像分类中面临高维数据、地物稀疏分布和光谱冗余等挑战，这些问题常导致分类过拟合和泛化能力受限，因此需要更有效地提取和融合空间上下文与精细光谱信息。", "method": "本文提出了一种名为STNet的新型网络架构，其核心是双重创新设计的空间-光谱Transformer模块。该模块首先明确解耦空间和光谱注意力，以实现对高光谱图像中关键信息的有针对性捕获；其次，引入两种功能不同的门控机制（自适应注意力融合门控和GFFN），分别在注意力流的融合层面和特征转换的内部层面进行智能调节，以增强特征提取和融合能力。", "result": "STNet在IN、UP和KSC数据集上表现出卓越的性能，超越了主流的高光谱图像分类方法。它还展示了比传统卷积神经网络更优的特征提取和融合能力，并有效降低了小样本和高噪声场景下的过拟合风险，同时在不增加网络深度或宽度的情况下增强了模型的表示能力。", "conclusion": "STNet通过其创新的光谱-空间注意力解耦和自适应门控机制，成功解决了高光谱图像分类中深度学习面临的挑战，显著提升了模型的特征提取与融合能力、泛化性能，并有效抑制了过拟合。", "translation": "深度神经网络在高光谱图像分类中面临若干挑战，包括高维数据、地物稀疏分布和光谱冗余，这些问题常导致分类过拟合和泛化能力受限。为了更有效地在高光谱图像（HSI）分类中提取和融合空间上下文与精细光谱信息，本文提出了一种新颖的网络架构——STNet。STNet的核心优势源于其空间-光谱Transformer模块的双重创新设计：首先，空间和光谱注意力的基本明确解耦确保了对HSI中关键信息的有针对性捕获；其次，两种功能不同的门控机制在注意力流的融合层面（自适应注意力融合门控）和特征转换的内部层面（GFFN）执行智能调节。这一特性表明，与传统卷积神经网络相比，STNet具有卓越的特征提取和融合能力，同时降低了小样本和高噪声场景下的过拟合风险。STNet在不增加网络深度或宽度的情况下增强了模型表示能力。所提出的方法在IN、UP和KSC数据集上表现出卓越的性能，优于主流高光谱图像分类方法。", "summary": "本文针对高光谱图像分类中深度神经网络面临的高维、稀疏和冗余数据导致的过拟合及泛化能力不足问题，提出了一种名为STNet的新型网络架构。STNet的核心在于其创新的空间-光谱Transformer模块，该模块通过明确解耦空间和光谱注意力，并引入自适应注意力融合门控与GFFN两种门控机制，实现了对关键信息的精准捕获及高效的特征提取与融合。实验结果表明，STNet在多个标准数据集上性能优越，有效降低了过拟合风险，并提升了模型的泛化能力。", "keywords": "高光谱图像分类, Transformer, 注意力解耦, 自适应门控, STNet", "comments": "STNet的创新之处在于其明确解耦空间和光谱注意力，并通过双重门控机制进行智能调节，这使得模型能够更精准地捕获高光谱图像中的关键信息，并有效融合空间和光谱特征。这种设计不仅提高了特征提取和融合能力，还显著降低了在小样本和高噪声条件下的过拟合风险，提升了模型的泛化能力，同时避免了增加网络复杂度，具有重要的实际应用价值。"}}
{"id": "2506.08518", "title": "FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching", "authors": ["Sunny Gupta", "Nikita Jangid", "Shounak Das", "Amit Sethi"], "summary": "Domain Generalization (DG) seeks to train models that perform reliably on\nunseen target domains without access to target data during training. While\nrecent progress in smoothing the loss landscape has improved generalization,\nexisting methods often falter under long-tailed class distributions and\nconflicting optimization objectives. We introduce FedTAIL, a federated domain\ngeneralization framework that explicitly addresses these challenges through\nsharpness-guided, gradient-aligned optimization. Our method incorporates a\ngradient coherence regularizer to mitigate conflicts between classification and\nadversarial objectives, leading to more stable convergence. To combat class\nimbalance, we perform class-wise sharpness minimization and propose a\ncurvature-aware dynamic weighting scheme that adaptively emphasizes\nunderrepresented tail classes. Furthermore, we enhance conditional distribution\nalignment by integrating sharpness-aware perturbations into entropy\nregularization, improving robustness under domain shift. FedTAIL unifies\noptimization harmonization, class-aware regularization, and conditional\nalignment into a scalable, federated-compatible framework. Extensive\nevaluations across standard domain generalization benchmarks demonstrate that\nFedTAIL achieves state-of-the-art performance, particularly in the presence of\ndomain shifts and label imbalance, validating its effectiveness in both\ncentralized and federated settings. Code: https://github.com/sunnyinAI/FedTail", "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic\n  Workflows CFAgentic @ ICML'25", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08518v1", "AI": {"title_translation": "FEDTAIL：基于锐度引导梯度匹配的联邦长尾域泛化", "tldr": "FedTAIL是一个联邦域泛化框架，通过锐度引导的梯度匹配来解决长尾类分布和冲突优化目标下的域泛化挑战，并在多个基准测试中达到了最先进的性能。", "motivation": "现有的域泛化方法在长尾类分布和冲突优化目标下表现不佳，尽管平滑损失函数的方法有所改进，但仍需解决在训练期间无法访问目标数据的情况下，模型在未见目标域上可靠性能的问题。", "method": "FedTAIL通过以下方式解决挑战：1. 引入梯度一致性正则化器，以减轻分类和对抗性目标之间的冲突，实现更稳定的收敛。2. 执行类级别锐度最小化，并提出一种曲率感知动态加权方案，自适应地强调代表性不足的尾部类别。3. 通过将锐度感知扰动整合到熵正则化中，增强条件分布对齐，提高域偏移下的鲁棒性。", "result": "FedTAIL在标准域泛化基准测试中取得了最先进的性能，特别是在存在域偏移和标签不平衡的情况下，验证了其在集中式和联邦设置中的有效性。", "conclusion": "FedTAIL通过统一优化协调、类感知正则化和条件对齐，成功解决了联邦长尾域泛化中的关键挑战，并在各种设置下实现了卓越的泛化能力。", "translation": "域泛化 (DG) 旨在训练模型，使其在训练期间无需访问目标数据的情况下，也能在未见的目标域上可靠地执行。尽管最近在平滑损失函数方面取得了进展，改善了泛化能力，但现有方法在长尾类分布和冲突优化目标下往往表现不佳。我们引入了 FedTAIL，一个联邦域泛化框架，通过锐度引导的梯度对齐优化明确解决了这些挑战。我们的方法结合了梯度一致性正则化器，以减轻分类和对抗性目标之间的冲突，从而实现更稳定的收敛。为了对抗类别不平衡，我们执行类级别锐度最小化，并提出了一种曲率感知动态加权方案，自适应地强调代表性不足的尾部类别。此外，我们通过将锐度感知扰动整合到熵正则化中，增强了条件分布对齐，提高了域偏移下的鲁棒性。FedTAIL 将优化协调、类感知正则化和条件对齐统一到一个可扩展的、联邦兼容的框架中。对标准域泛化基准的广泛评估表明，FedTAIL 实现了最先进的性能，特别是在存在域偏移和标签不平衡的情况下，验证了其在集中式和联邦设置中的有效性。代码：https://github.com/sunnyinAI/FedTail", "summary": "本文介绍了FedTAIL，一个联邦域泛化框架，旨在解决长尾类分布和冲突优化目标下的挑战。FedTAIL通过梯度一致性正则化器、类级别锐度最小化和曲率感知动态加权方案，以及将锐度感知扰动整合到熵正则化中来增强条件分布对齐。该方法在标准域泛化基准测试中表现出最先进的性能，尤其是在域偏移和标签不平衡的场景下，验证了其在集中式和联邦环境中的有效性。", "keywords": "联邦学习, 域泛化, 长尾分布, 梯度匹配, 锐度引导", "comments": "FedTAIL的创新之处在于它将锐度引导的梯度匹配引入联邦域泛化，有效解决了长尾分布和优化目标冲突的问题。其结合梯度一致性、类感知锐度最小化和曲率感知加权，以及锐度感知扰动进行条件对齐，形成了一个全面且可扩展的框架。该方法在实际应用中，尤其是在数据隐私和异构性是关键问题的联邦学习场景下，具有重要意义。"}}
{"id": "2506.08146", "title": "Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields", "authors": ["Vahidullah Taç", "Amirhossein Amiri-Hezaveh", "Manuel K. Rausch", "Grace N. Bechtel", "Francisco Sahli Costabal", "Adrian Buganza Tepole"], "summary": "We propose a new framework for identifying mechanical properties of\nheterogeneous materials without a closed-form constitutive equation. Given a\nfull-field measurement of the displacement field, for instance as obtained from\ndigital image correlation (DIC), a continuous approximation of the strain field\nis obtained by training a neural network that incorporates Fourier features to\neffectively capture sharp gradients in the data. A physics-based data-driven\nmethod built upon ordinary neural differential equations (NODEs) is employed to\ndiscover constitutive equations. The NODE framework can represent arbitrary\nmaterials while satisfying constraints in the theory of constitutive equations\nby default. To account for heterogeneity, a hyper-network is defined, where the\ninput is the material coordinate system, and the output is the NODE-based\nconstitutive equation. The parameters of the hyper-network are optimized by\nminimizing a multi-objective loss function that includes penalty terms for\nviolations of the strong form of the equilibrium equations of elasticity and\nthe associated Neumann boundary conditions. We showcase the framework with\nseveral numerical examples, including heterogeneity arising from variations in\nmaterial parameters, spatial transitions from isotropy to anisotropy, material\nidentification in the presence of noise, and, ultimately, application to\nexperimental data. As the numerical results suggest, the proposed approach is\nrobust and general in identifying the mechanical properties of heterogeneous\nmaterials with very few assumptions, making it a suitable alternative to\nclassical inverse methods.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08146v1", "AI": {"title_translation": "基于超网络神经ODE场的全数据驱动逆超弹性", "tldr": "本文提出了一种基于神经ODE和超网络的全数据驱动框架，用于识别异质材料的力学性能，无需闭合形式的本构方程。该框架通过数值算例和实验数据验证了其鲁棒性和通用性。", "motivation": "现有方法在识别没有闭合形式本构方程的异质材料力学性能方面存在挑战，本文旨在解决这一问题。", "method": "1. 使用包含傅里叶特征的神经网络从全场位移测量中近似应变场。2. 采用基于常微分神经方程（NODEs）的物理驱动数据驱动方法来发现本构方程，该方法能满足本构方程理论约束。3. 定义一个超网络，其输入为材料坐标系，输出为基于NODE的本构方程，以处理材料异质性。4. 通过最小化包含弹性平衡方程强形式和诺依曼边界条件违反惩罚项的多目标损失函数来优化超网络参数。", "result": "该框架在多种数值算例中表现出色，包括材料参数变化、各向同性到各向异性转变、噪声下的材料识别以及实验数据应用。数值结果表明，该方法在识别异质材料力学性能方面具有鲁棒性和通用性，且假设条件极少。", "conclusion": "所提出的方法在识别异质材料力学性能方面具有鲁棒性、通用性和极少假设的特点，是经典逆方法的合适替代方案。", "translation": "我们提出了一种新的框架，用于识别没有闭合形式本构方程的异质材料的力学性能。给定位移场的全场测量，例如从数字图像相关（DIC）获得的数据，通过训练一个包含傅里叶特征的神经网络来获得应变场的连续近似，以有效捕捉数据中的急剧梯度。采用基于常微分神经方程（NODEs）的物理驱动数据驱动方法来发现本构方程。NODE框架可以表示任意材料，同时默认满足本构方程理论中的约束。为了解释异质性，定义了一个超网络，其中输入是材料坐标系，输出是基于NODE的本构方程。超网络的参数通过最小化一个多目标损失函数进行优化，该损失函数包括对弹性平衡方程强形式和相关诺依曼边界条件违反的惩罚项。我们通过几个数值例子展示了该框架，包括材料参数变化引起的异质性、从各向同性到各向异性的空间转变、存在噪声情况下的材料识别，以及最终应用于实验数据。数值结果表明，所提出的方法在识别异质材料的力学性能方面具有鲁棒性和通用性，且假设非常少，使其成为经典逆方法的合适替代方案。", "summary": "本文提出了一种新颖的全数据驱动逆超弹性框架，专门用于识别无需闭合形式本构方程的异质材料力学性能。该方法利用带有傅里叶特征的神经网络从全场位移测量中近似应变场。其核心在于采用基于常微分神经方程（NODEs）的物理驱动数据驱动方法来发现本构方程，该方法能固有地满足理论约束。为处理材料异质性，框架引入了一个超网络，将材料坐标映射到基于NODE的本构方程。框架参数通过最小化一个惩罚违反平衡方程和边界条件的多目标损失函数进行优化。数值算例（包括含噪声情况和实验数据应用）验证了该方法的鲁棒性和通用性，使其成为传统逆方法的有力替代方案。", "keywords": "逆超弹性, 数据驱动, 神经ODE, 超网络, 异质材料", "comments": "本文通过将神经ODE和超网络集成到材料表征的逆问题中，提出了一种创新方法。其“全数据驱动”的特性，结合处理异质材料的能力以及通过NODE隐式满足物理约束，是重要的进步。该框架对噪声的鲁棒性以及在实验数据上的适用性突显了其在实践中的潜力。它通过提供一种灵活的方法来识别复杂材料特性而无需严格假设，解决了材料科学中的关键挑战。"}}
{"id": "2506.08890", "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments.", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08890v1", "AI": {"title_translation": "人机协作现场部署：口头与非口头交流的比较", "tldr": "研究比较了机器人在急救场景中口头和非口头交流对医护人员工作负荷和态度的影响，发现口头交流能显著降低精神负荷和努力。", "motivation": "医护人员在医院面临快速获取医疗用品的挑战，可能导致医疗错误和患者护理延迟。机器人急救推车（RCCs）显示出辅助医疗任务的潜力，但缺乏对现实环境中何种通信方式最有效且干扰最小的探索。", "method": "采用受试者间实验设计，在复苏场景中比较了机器人急救推车的口头和非口头物品搜索通信与标准急救推车的效果，以了解机器人通信对工作负荷和在工作场所使用机器人态度的影响。", "result": "研究结果表明，与视觉提示和传统急救推车相比，口头交流显著降低了精神负荷和努力。尽管与机器人协作时的挫败感略高于传统推车。", "conclusion": "这些研究见解为高风险环境中的人机协作提供了宝贵的启示。", "translation": "医护人员（HCWs）在医院面临挑战，例如从急救车中快速取回医疗用品，这可能导致医疗错误和患者护理延迟。机器人急救推车（RCCs）在医疗任务中通过引导式物品搜索和任务提醒显示出辅助医疗团队的潜力。然而，在现实世界环境中，关于何种通信方式最有效且对患者护理干扰最小的探索有限。为了弥补这一空白，我们进行了一项受试者间实验，在复苏场景中比较了RCC的口头和非口头物品搜索通信与标准急救推车的效果，以了解机器人通信对工作负荷和在工作场所使用机器人态度的影响。我们的研究结果表明，与视觉提示和传统急救推车相比，口头交流显著降低了精神负荷和努力。尽管与机器人协作时的挫败感略高于传统推车，但这些研究见解为高风险环境中的人机协作提供了宝贵的启示。", "summary": "本研究旨在解决医疗环境中人机协作通信方式的有效性问题。通过一项比较机器人急救推车口头和非口头通信的实验，发现口头交流能显著减轻医护人员的精神负荷和努力，尽管机器人协作可能带来轻微的挫败感。研究强调了在关键医疗场景中优化人机通信的重要性。", "keywords": "人机协作, 口头交流, 非口头交流, 机器人急救推车, 医疗环境", "comments": "这项研究通过对比口头和非口头交流在人机协作中的效果，为高风险医疗环境中的机器人应用提供了实用见解。其创新之处在于将机器人急救推车引入实际复苏场景进行实验，并量化了不同通信方式对工作负荷的影响。研究结果强调了口头交流的优势，对未来设计更高效、更人性化的医疗机器人具有重要指导意义。"}}
{"id": "2506.08871", "title": "Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery", "authors": ["Victor M. Tenorio", "Madeline Navarro", "Samuel Rey", "Santiago Segarra", "Antonio G. Marques"], "summary": "Graph Neural Networks (GNNs) often struggle with heterophilic data, where\nconnected nodes may have dissimilar labels, as they typically assume homophily\nand rely on local message passing. To address this, we propose creating\nalternative graph structures by linking nodes with similar structural\nattributes (e.g., role-based or global), thereby fostering higher label\nhomophily on these new graphs. We theoretically prove that GNN performance can\nbe improved by utilizing graphs with fewer false positive edges (connections\nbetween nodes of different classes) and that considering multiple graph views\nincreases the likelihood of finding such beneficial structures. Building on\nthese insights, we introduce Structure-Guided GNN (SG-GNN), an architecture\nthat processes the original graph alongside the newly created structural\ngraphs, adaptively learning to weigh their contributions. Extensive experiments\non various benchmark datasets, particularly those with heterophilic\ncharacteristics, demonstrate that our SG-GNN achieves state-of-the-art or\nhighly competitive performance, highlighting the efficacy of exploiting\nstructural information to guide GNNs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08871v1", "AI": {"title_translation": "通过结构引导的邻居发现适应异质图数据", "tldr": "GNN在异质图数据上表现不佳。本文提出SG-GNN，通过创建基于结构相似性的替代图结构来提高GNN在异质数据集上的性能。", "motivation": "图神经网络（GNNs）在异质数据上常常遇到困难，因为它们通常假设同质性并依赖局部消息传递，导致当连接节点具有不同标签时表现不佳。", "method": "本文提出通过连接具有相似结构属性（如基于角色或全局）的节点来创建替代图结构，以促进新图上更高的标签同质性。理论上证明，利用具有较少错误正边（不同类别节点之间的连接）的图可以提高GNN性能，并且考虑多视图图增加了找到此类有益结构的可能性。在此基础上，引入了结构引导GNN（SG-GNN），该架构能够同时处理原始图和新创建的结构图，并自适应地学习权衡它们的贡献。", "result": "在各种基准数据集，特别是具有异质性特征的数据集上进行的广泛实验表明，SG-GNN 实现了最先进或极具竞争力的性能。", "conclusion": "利用结构信息可以有效地指导GNNs，从而提高其在异质图数据上的表现。", "translation": "图神经网络 (GNN) 在异质数据上常常表现不佳，因为连接的节点可能具有不同的标签，这通常是因为它们假设同质性并依赖局部消息传递。为了解决这个问题，我们提出通过连接具有相似结构属性（例如，基于角色或全局）的节点来创建替代图结构，从而在这些新图上促进更高的标签同质性。我们从理论上证明，通过利用具有较少错误正边（不同类别节点之间的连接）的图，可以提高 GNN 的性能，并且考虑多视图图增加了找到此类有益结构的可能性。基于这些见解，我们引入了结构引导 GNN (SG-GNN)，这是一种处理原始图和新创建的结构图的架构，能够自适应地学习权衡它们的贡献。在各种基准数据集，特别是具有异质性特征的数据集上进行的广泛实验表明，我们的 SG-GNN 实现了最先进或极具竞争力的性能，突出了利用结构信息指导 GNN 的有效性。", "summary": "图神经网络（GNNs）在处理异质图数据时面临挑战，因为它们通常假设同质性。为解决此问题，本文提出了结构引导GNN（SG-GNN），通过连接具有相似结构属性的节点来创建替代图结构，从而在新图上提高标签同质性。研究从理论上证明，使用具有较少错误正边的图以及考虑多视图图可以改善GNN性能。SG-GNN能够自适应地结合原始图和新构建的结构图，并在异质数据集上取得了最先进或极具竞争力的表现，突显了利用结构信息指导GNN的有效性。", "keywords": "异质图, 图神经网络, 结构引导GNN, 图结构, 同质性", "comments": "本文的创新之处在于通过基于结构相似性修改图结构本身来解决异质性问题，而不仅仅是调整GNN架构。对多视图图和错误正边进行理论论证也是一个亮点。"}}
{"id": "2506.08530", "title": "The Invariant Zonotopic Set-Membership Filter for State Estimation on Groups", "authors": ["Tao Li", "Yi Li", "Lulin Zhang", "Jiuxiang Dong"], "summary": "The invariant filtering theory based on the group theory has been successful\nin statistical filtering methods. However, there exists a class of state\nestimation problems with unknown statistical properties of noise disturbances,\nand it is worth discussing whether the invariant observer still has performance\nadvantages. In this paper, considering the problem of state estimation with\nunknown but bounded noise disturbances, an Invariant Zonotopic Set-Membership\nFilter (InZSMF) method on groups is innovatively proposed, which extends the\ninvariant filtering theory to the field of non-statistical filtering\nrepresented by set-membership filtering. Firstly, the InZSMF method transforms\nthe state space from the traditional Euclidean vector space to the Lie group\nspace to construct group affine discrete systems with unknown but bounded noise\nuncertainty defined by the zonotope on groups. Secondly, the nonlinear observer\non the group is defined and the corresponding linearized estimation error is\nderived. Then, two observer gain tuning algorithms under the InZSMF method are\nproposed, respectively, the pole configuration method and the F-radius\noptimization method. Finally, through simulation experiments, it is shown that\nthe InZSMF state estimation method is generally superior to the traditional\nZonotopic Set-Membership Filter (ZSMF) state estimation method. Especially,\nwhen the initial estimations are imprecise, the convergence speed of state\nestimation, the accuracy of set-membership center estimation, and the average\ninterval area of zonotopic estimation of the InZSMF method are significantly\nbetter than those of the ZSMF method.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08530v1", "AI": {"title_translation": "群上状态估计的不变带状集员滤波器", "tldr": "本文提出了一种不变带状集员滤波器（InZSMF），用于解决群上存在未知但有界噪声干扰的状态估计问题。该方法将不变滤波理论扩展到非统计集员滤波领域，并通过仿真证明其在收敛速度和估计精度方面优于传统方法，尤其是在初始估计不精确的情况下。", "motivation": "不变滤波理论在统计滤波方法中已取得成功，但对于噪声干扰统计特性未知的一类状态估计问题，其性能优势尚不明确。本文旨在解决噪声干扰未知但有界的状态估计问题。", "method": "InZSMF方法首先将状态空间从欧几里得向量空间变换到李群空间，构建带有带状体定义噪声不确定性的群仿射离散系统。其次，定义了群上的非线性观测器并推导了线性化估计误差。然后，提出了极点配置和F半径优化两种观测器增益调整算法。", "result": "仿真实验表明，InZSMF状态估计方法总体上优于传统的带状集员滤波器（ZSMF）方法。特别是在初始估计不精确时，InZSMF方法的收敛速度、集员中心估计精度和带状估计的平均区间面积均显著优于ZSMF方法。", "conclusion": "不变带状集员滤波器（InZSMF）成功将不变滤波理论扩展到群上状态估计的非统计集员滤波领域，并在存在未知但有界噪声扰动时表现出优越的性能，特别是在初始条件不精确的情况下。", "translation": "不变滤波理论基于群论已在统计滤波方法中取得成功。然而，存在一类噪声干扰统计特性未知状态估计问题，值得讨论不变观测器是否仍具有性能优势。在本文中，考虑到噪声干扰未知但有界的状态估计问题，创新性地提出了一种群上的不变带状集员滤波器（InZSMF）方法，将不变滤波理论扩展到以集员滤波为代表的非统计滤波领域。首先，InZSMF方法将状态空间从传统的欧几里得向量空间变换到李群空间，以构建在群上由带状体定义的未知但有界噪声不确定性的群仿射离散系统。其次，定义了群上的非线性观测器，并推导了相应的线性化估计误差。然后，提出了InZSMF方法下的两种观测器增益调整算法，分别是极点配置方法和F半径优化方法。最后，通过仿真实验表明，InZSMF状态估计方法总体上优于传统的带状集员滤波器（ZSMF）状态估计方法。特别是，当初始估计不精确时，InZSMF方法的状态估计收敛速度、集员中心估计精度和带状估计的平均区间面积均显著优于ZSMF方法。", "summary": "本文提出了一种新颖的不变带状集员滤波器（InZSMF），旨在解决群上存在未知但有界噪声干扰的状态估计问题。该方法创新性地将不变滤波理论扩展到集员滤波领域，通过将状态空间转换到李群空间，并提出了两种观测器增益调整算法。仿真结果表明，InZSMF在收敛速度和估计精度方面优于传统方法，尤其在初始估计不精确时表现出显著优势。", "keywords": "不变滤波, 集员滤波, 状态估计, 李群, 带状体", "comments": "该论文创新性地将不变滤波理论从统计方法扩展到非统计的集员滤波领域，这对于处理噪声特性未知但有界的状态估计问题具有重要意义。利用李群理论进行状态空间变换以及提出两种观测器增益调整算法是其主要亮点。仿真结果表明其在初始估计不精确情况下的性能优势，突出了其潜在的实际应用价值。"}}
{"id": "2506.08479", "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$", "authors": ["Chihiro Taguchi", "Seiji Maekawa", "Nikita Bhutani"], "summary": "Retrieval-augmented generation (RAG) and long-context language models (LCLMs)\nboth address context limitations of LLMs in open-domain question answering\n(QA). However, optimal external context to retrieve remains an open problem:\nfixing the retrieval size risks either wasting tokens or omitting key evidence.\nExisting adaptive methods like Self-RAG and Self-Route rely on iterative LLM\nprompting and perform well on factoid QA, but struggle with aggregation QA,\nwhere the optimal context size is both unknown and variable. We present\nAdaptive-$k$ retrieval, a simple and effective single-pass method that\nadaptively selects the number of passages based on the distribution of the\nsimilarity scores between the query and the candidate passages. It does not\nrequire model fine-tuning, extra LLM inferences or changes to existing\nretriever-reader pipelines. On both factoid and aggregation QA benchmarks,\nAdaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x\nfewer tokens than full-context input, yet still retrieves 70% of relevant\npassages. It improves accuracy across five LCLMs and two embedding models,\nhighlighting that dynamically adjusting context size leads to more efficient\nand accurate QA.", "comment": "26 pages, 16 tables, 5 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08479v1", "AI": {"title_translation": "高效长文本问答上下文选择：无需微调，无需迭代，只需自适应-$k$", "tldr": "Adaptive-$k$是一种无需微调或迭代的单次检索方法，可根据相似度分数自适应选择上下文大小，从而提高长文本问答的效率和准确性。", "motivation": "现有RAG和LCLM方法在开放域问答中存在上下文限制，固定检索大小会导致浪费token或遗漏关键证据。Self-RAG和Self-Route等现有自适应方法依赖迭代LLM提示，且在最佳上下文大小未知且可变的聚合问答中表现不佳。", "method": "提出Adaptive-$k$检索，这是一种简单有效的单次方法，它根据查询与候选段落之间相似度分数的分布来自适应选择段落数量。该方法不需要模型微调、额外的LLM推理或对现有检索器-阅读器管道的更改。", "result": "在事实性和聚合问答基准测试中，Adaptive-$k$匹配或优于固定-$k$基线，同时使用的token比全上下文输入少10倍，但仍检索到70%的相关段落。它提高了五种LCLM和两种嵌入模型的准确性。", "conclusion": "动态调整上下文大小可以带来更高效和准确的问答。", "translation": "检索增强生成（RAG）和长上下文语言模型（LCLMs）都解决了LLMs在开放域问答（QA）中上下文限制的问题。然而，检索最佳外部上下文仍然是一个开放问题：固定检索大小要么浪费token，要么遗漏关键证据。像Self-RAG和Self-Route等现有自适应方法依赖于迭代的LLM提示，在事实性问答中表现良好，但在聚合问答中却表现不佳，因为最佳上下文大小既未知又可变。我们提出了Adaptive-$k$检索，这是一种简单有效的单次方法，它根据查询与候选段落之间相似度分数的分布来自适应选择段落数量。它不需要模型微调、额外的LLM推理或对现有检索器-阅读器管道的更改。在事实性和聚合问答基准测试中，Adaptive-$k$匹配或优于固定-$k$基线，同时使用的token比全上下文输入少10倍，但仍检索到70%的相关段落。它提高了五种LCLM和两种嵌入模型的准确性，突出表明动态调整上下文大小可以带来更高效和准确的问答。", "summary": "该论文介绍了Adaptive-$k$检索，这是一种无需微调或迭代的单次方法，用于长文本问答中的高效上下文选择。针对现有方法在固定检索大小和聚合问答中的局限性，Adaptive-$k$根据查询与候选段落的相似度分数分布自适应地确定检索的段落数量。实验表明，该方法在事实性和聚合问答基准测试中表现优异，显著减少了token使用量，同时保持了高相关段落检索率，并提高了多种长上下文语言模型和嵌入模型的准确性，证明了动态上下文调整的有效性。", "keywords": "上下文选择, 检索增强生成, 长上下文语言模型, 自适应检索, 问答", "comments": "该研究提出了一种无需微调和迭代的上下文选择方法，简化了长文本问答的流程。其创新之处在于利用相似度分数分布自适应地确定上下文大小，有效解决了现有方法在聚合问答中的不足。该方法在效率和准确性方面均有显著提升，对RAG和LCLM的应用具有重要意义。"}}
{"id": "2506.08383", "title": "Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest", "authors": ["Jiaqi Chen", "Rongbin Ye"], "summary": "With the rapid expansion of Internet of Things (IoT) networks, detecting\nmalicious traffic in real-time has become a critical cybersecurity challenge.\nThis research addresses the detection challenges by presenting a comprehensive\nempirical analysis of machine learning techniques for malware detection using\nthe IoT-23 dataset provided by the Stratosphere Laboratory. We address the\nsignificant class imbalance within the dataset through three resampling\nstrategies. We implement and compare a few machine learning techniques. Our\nfindings demonstrate that the combination of appropriate imbalance treatment\ntechniques with ensemble methods, particularly gcForest, achieves better\ndetection performance compared to traditional approaches. This work contributes\nsignificantly to the development of more intelligent and efficient automated\nthreat detection systems for IoT environments, helping to secure critical\ninfrastructure against sophisticated cyber attacks while optimizing\ncomputational resource usage.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08383v1", "AI": {"title_translation": "网络威胁检测：利用深度森林解决类别不平衡数据问题", "tldr": "本研究通过使用重采样策略和集成方法（特别是gcForest）来解决物联网恶意流量检测中的类别不平衡问题，结果表明这种组合优于传统方法。", "motivation": "随着物联网（IoT）网络的快速扩张，实时检测恶意流量已成为一个关键的网络安全挑战。本研究旨在解决这一检测难题。", "method": "本研究对使用Stratosphere Laboratory提供的IoT-23数据集进行恶意软件检测的机器学习技术进行了全面的实证分析。通过三种重采样策略解决了数据集中显著的类别不平衡问题，并实现和比较了几种机器学习技术，重点是集成方法，特别是gcForest。", "result": "研究发现，适当的不平衡处理技术与集成方法（特别是gcForest）相结合，比传统方法取得了更好的检测性能。", "conclusion": "这项工作对开发更智能、更高效的物联网环境自动化威胁检测系统做出了重大贡献，有助于保护关键基础设施免受复杂的网络攻击，同时优化计算资源使用。", "translation": "随着物联网（IoT）网络的快速扩张，实时检测恶意流量已成为一个关键的网络安全挑战。本研究通过对使用Stratosphere Laboratory提供的IoT-23数据集进行恶意软件检测的机器学习技术进行全面的实证分析来解决检测挑战。我们通过三种重采样策略解决了数据集中显著的类别不平衡问题。我们实现并比较了几种机器学习技术。我们的研究结果表明，适当的不平衡处理技术与集成方法（特别是gcForest）相结合，比传统方法取得了更好的检测性能。这项工作对开发更智能、更高效的物联网环境自动化威胁检测系统做出了重大贡献，有助于保护关键基础设施免受复杂的网络攻击，同时优化计算资源使用。", "summary": "本研究旨在解决物联网网络中实时恶意流量检测的挑战，特别是数据集中存在的类别不平衡问题。通过对IoT-23数据集进行机器学习技术实证分析，研究人员采用了三种重采样策略来处理数据不平衡，并比较了多种机器学习方法。结果表明，将不平衡处理技术与集成方法（尤其是gcForest）结合使用，能够显著提高检测性能，优于传统方法。这项工作为开发更智能、高效的物联网威胁检测系统提供了重要支持。", "keywords": "网络威胁检测, 类别不平衡, 深度森林, 物联网, 机器学习", "comments": "该论文解决了物联网安全领域的一个重要且实际的问题：恶意流量检测中的类别不平衡。其创新之处在于结合了重采样技术与gcForest等集成方法，为解决这一普遍挑战提供了有效的解决方案。这对于提高物联网环境中威胁检测系统的准确性和效率具有重要意义。"}}
{"id": "2506.08840", "title": "MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains", "authors": ["Dewei Wang", "Xinmiao Wang", "Xinzhe Liu", "Jiyuan Shi", "Yingnan Zhao", "Chenjia Bai", "Xuelong Li"], "summary": "Humanoid robots have demonstrated robust locomotion capabilities using\nReinforcement Learning (RL)-based approaches. Further, to obtain human-like\nbehaviors, existing methods integrate human motion-tracking or motion prior in\nthe RL framework. However, these methods are limited in flat terrains with\nproprioception only, restricting their abilities to traverse challenging\nterrains with human-like gaits. In this work, we propose a novel framework\nusing a mixture of latent residual experts with multi-discriminators to train\nan RL policy, which is capable of traversing complex terrains in controllable\nlifelike gaits with exteroception. Our two-stage training pipeline first\nteaches the policy to traverse complex terrains using a depth camera, and then\nenables gait-commanded switching between human-like gait patterns. We also\ndesign gait rewards to adjust human-like behaviors like robot base height.\nSimulation and real-world experiments demonstrate that our framework exhibits\nexceptional performance in traversing complex terrains, and achieves seamless\ntransitions between multiple human-like gait patterns.", "comment": "9 pages, 5 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08840v1", "AI": {"title_translation": "MoRE：用于复杂地形类人步态学习的残差专家混合模型", "tldr": "提出了一种名为MoRE的残差专家混合框架，结合多鉴别器和两阶段训练，使人形机器人能够在复杂地形上学习并切换可控的类人步态。", "motivation": "现有的人形机器人强化学习方法在平坦地形上表现良好，但由于仅依赖本体感受，难以在复杂地形上实现类人步态。", "method": "本文提出了一种 novel 框架，使用潜在残差专家的混合模型和多鉴别器来训练强化学习策略。该框架采用两阶段训练：首先利用深度摄像头学习穿越复杂地形，然后通过步态指令实现类人步态模式的切换。此外，还设计了步态奖励来调整类人行为。", "result": "仿真和实际实验表明，该框架在穿越复杂地形方面表现出色，并实现了多种类人步态模式之间的无缝过渡。", "conclusion": "MoRE框架通过结合残差专家混合模型和多鉴别器，成功使人形机器人在复杂地形上学习并实现可控的类人步态，克服了现有方法的局限性。", "translation": "人形机器人已经展示了使用强化学习（RL）方法的强大运动能力。此外，为了获得类人行为，现有方法将人类运动跟踪或运动先验集成到RL框架中。然而，这些方法仅限于平坦地形，并且仅依赖本体感受，这限制了它们在具有类人步态的挑战性地形上穿越的能力。在这项工作中，我们提出了一种 novel 框架，使用潜在残差专家的混合模型和多鉴别器来训练RL策略，该策略能够以可控的类人步态和外部感知穿越复杂地形。我们的两阶段训练管道首先教会策略使用深度摄像头穿越复杂地形，然后实现类人步态模式之间的步态指令切换。我们还设计了步态奖励来调整机器人基座高度等类人行为。仿真和实际实验表明，我们的框架在穿越复杂地形方面表现出色，并实现了多种类人步态模式之间的无缝过渡。", "summary": "本文提出了MoRE（残差专家混合模型）框架，旨在解决人形机器人在复杂地形上实现类人步态的挑战。该框架结合了潜在残差专家和多鉴别器，并采用两阶段训练：首先利用深度摄像头学习地形穿越，然后通过步态指令实现类人步态模式的切换。实验结果表明，MoRE在复杂地形上表现出色，并能实现多种类人步态模式的无缝过渡。", "keywords": "人形机器人, 强化学习, 复杂地形, 类人步态, 残差专家混合", "comments": "该论文的创新点在于提出了一个结合残差专家混合模型和多鉴别器的强化学习框架，并采用两阶段训练方法，有效解决了人形机器人在复杂地形上实现可控类人步态的难题。引入外部感知（深度摄像头）并实现步态模式切换是其重要贡献，有望推动人形机器人在更复杂环境中的应用。"}}
{"id": "2506.08327", "title": "Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera", "authors": ["Yuto Kase", "Kai Ishibe", "Ryoma Yasuda", "Yudai Washida", "Sakiko Hashimoto"], "summary": "In racket sports, such as tennis, locating the ball's position at impact is\nimportant in clarifying player and equipment characteristics, thereby aiding in\npersonalized equipment design. High-speed cameras are used to measure the\nimpact location; however, their excessive memory consumption limits prolonged\nscene capture, and manual digitization for position detection is time-consuming\nand prone to human error. These limitations make it difficult to effectively\ncapture the entire playing scene, hindering the ability to analyze the player's\nperformance. We propose a method for locating the tennis ball impact on the\nracket in real time using an event camera. Event cameras efficiently measure\nbrightness changes (called `events') with microsecond accuracy under high-speed\nmotion while using lower memory consumption. These cameras enable users to\ncontinuously monitor their performance over extended periods. Our method\nconsists of three identification steps: time range of swing, timing at impact,\nand contours of ball and racket. Conventional computer vision techniques are\nutilized along with an original event-based processing to detect the timing at\nimpact (PATS: the amount of polarity asymmetry in time symmetry). The results\nof the experiments were within the permissible range for measuring tennis\nplayers' performance. Moreover, the computation time was sufficiently short for\nreal-time applications.", "comment": "17 pages, 10 figures, 3 tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08327v1", "AI": {"title_translation": "使用事件相机实时定位网球拍上的网球撞击点", "tldr": "本文提出一种利用事件相机实时定位网球拍上击球点的方法，解决了传统高速相机在内存消耗和手动处理方面的局限性。", "motivation": "在网球等球拍运动中，定位击球点对于分析球员和装备特性、辅助个性化装备设计至关重要。传统高速相机存在内存消耗大、捕获时间受限、手动数字化耗时且易出错等问题，难以有效捕捉整个比赛场景并分析球员表现。", "method": "提出一种使用事件相机实时定位网球拍上击球点的方法。事件相机能以微秒级精度高效测量亮度变化（称为“事件”），且内存消耗低，适用于高速运动并支持长时间连续监测。该方法包含三个识别步骤：挥拍时间范围、击球时刻以及球和球拍的轮廓。结合传统计算机视觉技术和原创的基于事件的处理方法（PATS：时间对称性中的极性不对称量）来检测击球时刻。", "result": "实验结果表明，该方法在测量网球运动员表现方面处于可接受的范围内。此外，计算时间足够短，适用于实时应用。", "conclusion": "综上所述，本文提出的基于事件相机的实时网球击球点定位方法有效且实用，能够满足性能测量要求，并克服了传统方法的局限性，为长时间连续监测提供了可能。", "translation": "在网球等球拍运动中，定位击球点对于明确球员和装备特性至关重要，从而有助于个性化装备设计。高速相机被用于测量击球点；然而，其过度的内存消耗限制了长时间的场景捕获，并且用于位置检测的手动数字化既耗时又容易出现人为错误。这些局限性使得难以有效捕捉整个比赛场景，从而阻碍了分析球员表现的能力。我们提出了一种使用事件相机实时定位网球拍上网球撞击点的方法。事件相机能够以微秒级精度在高速运动下高效测量亮度变化（称为“事件”），同时使用更低的内存消耗。这些相机使用户能够长时间连续监测其表现。我们的方法包括三个识别步骤：挥拍时间范围、击球时刻以及球和球拍的轮廓。结合传统计算机视觉技术和一种原创的基于事件的处理方法来检测击球时刻（PATS：时间对称性中的极性不对称量）。实验结果在测量网球运动员表现的允许范围内。此外，计算时间足够短，适用于实时应用。", "summary": "本文提出了一种利用事件相机实时定位网球拍上击球点的新方法，旨在克服传统高速相机在内存消耗和数据处理方面的局限性。该方法通过识别挥拍时间范围、击球时刻（利用原创的PATS事件处理技术）以及球拍和球的轮廓，实现了在高速运动场景下的高精度和低内存消耗。实验结果表明，该方法不仅精度满足网球运动员表现测量的要求，而且计算速度足以支持实时应用，为长时间连续监测提供了可行方案。", "keywords": "网球, 事件相机, 击球点定位, 实时, PATS", "comments": "这项研究的创新之处在于首次将事件相机应用于网球击球点的实时定位，有效解决了传统高速相机内存大、处理慢的痛点。其提出的PATS事件处理方法是核心创新点。该技术对于运动员训练、个性化装备设计和运动表现分析具有重要意义，尤其是在需要长时间连续监测的场景中展现出巨大潜力。"}}
{"id": "2506.08532", "title": "Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness", "authors": ["Yanwei Gong", "Xiaolin Chang"], "summary": "The rapid growth of the low-altitude economy has driven the widespread\nadoption of unmanned aerial vehicles (UAVs). This growing deployment presents\nnew challenges for UAV trajectory planning in complex urban environments.\nHowever, existing studies often overlook key factors, such as urban airspace\nconstraints and economic efficiency, which are essential in low-altitude\neconomy contexts. Deep reinforcement learning (DRL) is regarded as a promising\nsolution to these issues, while its practical adoption remains limited by low\nlearning efficiency. To overcome this limitation, we propose a novel UAV\ntrajectory planning framework that combines DRL with large language model (LLM)\nreasoning to enable safe, compliant, and economically viable path planning.\nExperimental results demonstrate that our method significantly outperforms\nexisting baselines across multiple metrics, including data collection rate,\ncollision avoidance, successful landing, regulatory compliance, and energy\nefficiency. These results validate the effectiveness of our approach in\naddressing UAV trajectory planning key challenges under constraints of the\nlow-altitude economy networking.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08532v1", "AI": {"title_translation": "低空空域安全经济型无人机轨迹规划：一种结合合规意识的混合DRL-LLM方法", "tldr": "该研究提出了一种结合深度强化学习（DRL）和大型语言模型（LLM）的混合方法，用于在低空空域进行安全、经济且符合规定的无人机轨迹规划，并实验证明其优于现有基线。", "motivation": "低空经济的快速发展导致无人机广泛应用，但在复杂城市环境中，现有无人机轨迹规划研究常忽略城市空域限制和经济效率等关键因素。深度强化学习（DRL）虽有潜力，但学习效率低，限制了其应用。", "method": "本文提出一种结合深度强化学习（DRL）和大型语言模型（LLM）推理的新型无人机轨迹规划框架，旨在实现安全、合规且经济可行的路径规划。", "result": "实验结果表明，该方法在数据收集率、避碰、成功着陆、法规遵从性和能源效率等多个指标上显著优于现有基线。", "conclusion": "该方法有效解决了低空经济网络约束下无人机轨迹规划的关键挑战。", "translation": "低空经济的快速增长推动了无人机的广泛应用。这种日益增长的部署对复杂城市环境中的无人机轨迹规划提出了新的挑战。然而，现有研究往往忽视城市空域限制和经济效率等关键因素，而这些在低空经济背景下至关重要。深度强化学习（DRL）被认为是解决这些问题的一个有前景的方案，但其在实际应用中受限于低学习效率。为了克服这一限制，我们提出了一种新颖的无人机轨迹规划框架，该框架将DRL与大型语言模型（LLM）推理相结合，以实现安全、合规且经济可行的路径规划。实验结果表明，我们的方法在数据收集率、避碰、成功着陆、法规遵从性和能源效率等多个指标上显著优于现有基线。这些结果验证了我们方法在解决低空经济网络约束下无人机轨迹规划关键挑战方面的有效性。", "summary": "为应对低空经济背景下无人机轨迹规划面临的安全、经济和合规挑战，本研究提出了一种混合DRL-LLM框架。该框架结合了深度强化学习的高效决策能力与大型语言模型的推理能力，以克服传统DRL学习效率低的局限。实验证明，该方法在多项关键性能指标上显著优于现有基线，有效提升了无人机在复杂低空环境中的安全、经济和合规性。", "keywords": "无人机轨迹规划, 深度强化学习, 大型语言模型, 低空空域, 合规意识", "comments": "该论文的创新点在于将DRL与LLM结合，以解决无人机轨迹规划中学习效率低和复杂约束难以建模的问题。LLM的引入可能增强了对法规和经济因素的理解与推理能力，使其规划更具“合规意识”。这种混合方法为未来智能体在复杂、高约束环境中的决策提供了新思路，具有重要的实际应用价值。"}}
{"id": "2506.08164", "title": "BLUR: A Bi-Level Optimization Approach for LLM Unlearning", "authors": ["Hadi Reisizadeh", "Jinghan Jia", "Zhiqi Bu", "Bhanukiran Vinzamuri", "Anil Ramakrishna", "Kai-Wei Chang", "Volkan Cevher", "Sijia Liu", "Mingyi Hong"], "summary": "Enabling large language models (LLMs) to unlearn knowledge and capabilities\nacquired during training has proven vital for ensuring compliance with data\nregulations and promoting ethical practices in generative AI. Although there\nare growing interests in developing various unlearning algorithms, it remains\nunclear how to best formulate the unlearning problem. The most popular\nformulation uses a weighted sum of forget and retain loss, but it often leads\nto performance degradation due to the inherent trade-off between forget and\nretain losses. In this work, we argue that it is important to model the\nhierarchical structure of the unlearning problem, where the forget problem\n(which \\textit{unlearns} certain knowledge and/or capabilities) takes priority\nover the retain problem (which preserves model utility). This hierarchical\nstructure naturally leads to a bi-level optimization formulation where the\nlower-level objective focuses on minimizing the forget loss, while the\nupper-level objective aims to maintain the model's utility. Based on this new\nformulation, we propose a novel algorithm, termed Bi-Level UnleaRning\n(\\texttt{BLUR}), which not only possesses strong theoretical guarantees but\nmore importantly, delivers superior performance. In particular, our extensive\nexperiments demonstrate that \\texttt{BLUR} consistently outperforms all the\nstate-of-the-art algorithms across various unlearning tasks, models, and\nmetrics. Codes are available at\nhttps://github.com/OptimAI-Lab/BLURLLMUnlearning.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08164v1", "AI": {"title_translation": "BLUR：一种用于LLM遗忘的双层优化方法", "tldr": "BLUR提出了一种新颖的双层优化方法来解决大型语言模型（LLM）的遗忘问题，该方法在理论上具有强保证，并在实践中优于现有最先进算法。", "motivation": "为了遵守数据法规和促进生成式AI的伦理实践，使大型语言模型（LLM）遗忘知识和能力至关重要。尽管开发各种遗忘算法的兴趣日益增长，但如何最好地 формулировать 遗忘问题仍不清楚。最流行的公式使用遗忘损失和保留损失的加权和，但这通常由于遗忘和保留损失之间固有的权衡而导致性能下降。", "method": "本研究提出了一种双层优化方法，称为Bi-Level UnleaRning (BLUR)。该方法认为遗忘问题具有层次结构，其中遗忘问题（遗忘某些知识和/或能力）优先于保留问题（保持模型效用）。这种层次结构自然导致了双层优化公式，其中下层目标侧重于最小化遗忘损失，而上层目标旨在保持模型的效用。", "result": "BLUR算法不仅具有强大的理论保证，更重要的是，它提供了卓越的性能。在各种遗忘任务、模型和指标的广泛实验中，BLUR始终优于所有最先进的算法。", "conclusion": "通过引入一种新颖的双层优化方法，BLUR成功解决了LLM遗忘问题中遗忘与保留之间的权衡，并展示了在性能上的显著提升，为该领域设定了新的基准。", "translation": "使大型语言模型（LLM）遗忘在训练期间获得的知识和能力，对于确保符合数据法规和促进生成式AI中的伦理实践至关重要。尽管对开发各种遗忘算法的兴趣日益增长，但如何最好地 формулировать 遗忘问题仍不清楚。最流行的公式使用遗忘损失和保留损失的加权和，但这通常由于遗忘和保留损失之间固有的权衡而导致性能下降。在这项工作中，我们认为对遗忘问题的层次结构进行建模非常重要，其中遗忘问题（遗忘某些知识和/或能力）优先于保留问题（保持模型效用）。这种层次结构自然导致了双层优化公式，其中下层目标侧重于最小化遗忘损失，而上层目标旨在保持模型的效用。基于这种新公式，我们提出了一种新颖的算法，称为Bi-Level UnleaRning（BLUR），它不仅具有强大的理论保证，更重要的是，它提供了卓越的性能。特别是，我们的大量实验表明，BLUR在各种遗忘任务、模型和指标上始终优于所有最先进的算法。代码可在 https://github.com/OptimAI-Lab/BLURLLMUnlearning 获取。", "summary": "该论文提出了BLUR，一种用于大型语言模型（LLM）遗忘的新型双层优化方法。研究指出，现有遗忘算法在遗忘和保留损失之间存在权衡，导致性能下降。BLUR通过将遗忘问题建模为具有优先级的层次结构来解决此问题，其中遗忘优先于保留。该方法将遗忘损失最小化作为下层目标，模型效用保持作为上层目标。实验证明，BLUR在多个遗忘任务、模型和指标上均优于现有最先进算法，并具有强大的理论保证。", "keywords": "LLM遗忘, 双层优化, 遗忘算法, 模型效用, 深度学习", "comments": "BLUR通过引入双层优化框架来解决LLM遗忘中遗忘与保留之间的固有冲突，这是一个重要的创新。这种方法不仅提供了理论上的强保证，而且在实践中也表现出卓越的性能，有望成为LLM遗忘领域的新范式。其对遗忘问题层次结构的建模是其核心贡献。"}}
{"id": "2506.08579", "title": "Toward Low-Altitude Airspace Management and UAV Operations: Requirements, Architecture and Enabling Technologies", "authors": ["Guiyang Luo", "Jinglin Li", "Qixun Zhang", "Zhiyong Feng", "Quan Yuan", "Yijing Lin", "Hui Zhang", "Nan Cheng", "Ping Zhang"], "summary": "The low-altitude economy (LAE) is rapidly advancing toward intelligence,\nconnectivity, and coordination, bringing new challenges in dynamic airspace\nmanagement, unmanned aerial vehicle (UAV) operation, and security management.\nExisting systems remain fragmented and lack effective coordination. To bridge\nthese gaps, we propose UTICN (Ubiquitous and Trusted Intelligent\nCellular-native Network) for LAE, a unified cellular-native architecture that\nintegrates multi-domain sensing, high-precision positioning, intelligent\naircraft-to-everything communication, dynamic airspace management, and UAV\noperational services. UTICN introduces key technologies such as integrated\nsensing and communication (ISAC), passive and active positioning, intelligent\nmachine communication, swarm coordination, and control-data decoupled\nmanagement frameworks. We demonstrate UTICN's feasibility through two use\ncases, i.e., a city-level LAE management platform and a multi-frequency\ncollaborative ISAC system. This work provides a fundamental reference for\nbuilding a unified operational foundation and airspace management architecture\nfor the LAE.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08579v1", "AI": {"title_translation": "低空空域管理与无人机运行：需求、架构与使能技术", "tldr": "本文提出UTICN，一个统一的蜂窝原生网络架构，旨在解决低空经济（LAE）中动态空域管理和无人机（UAV）运行的挑战，并通过整合多项关键技术实现智能化、协调化的低空空域服务。", "motivation": "低空经济（LAE）的快速发展带来了动态空域管理、无人机（UAV）运行和安全管理的新挑战，而现有系统存在碎片化且缺乏有效协调的问题。", "method": "提出UTICN（无处不在、可信的智能蜂窝原生网络）作为LAE的统一蜂窝原生架构，该架构集成了多域感知、高精度定位、智能空对地通信、动态空域管理和无人机运行服务。UTICN引入了集成感知与通信（ISAC）、被动和主动定位、智能机器通信、蜂群协调以及控制-数据解耦管理框架等关键技术。", "result": "通过两个用例（城市级LAE管理平台和多频协作ISAC系统）证明了UTICN的可行性。", "conclusion": "这项工作为构建低空经济（LAE）的统一运营基础和空域管理架构提供了基础性参考。", "translation": "低空经济（LAE）正迅速向智能化、互联化和协调化方向发展，这给动态空域管理、无人机（UAV）运行和安全管理带来了新的挑战。现有系统仍然碎片化，缺乏有效的协调。为了弥补这些差距，我们为LAE提出了UTICN（无处不在且可信的智能蜂窝原生网络），这是一个统一的蜂窝原生架构，集成了多域感知、高精度定位、智能空对地通信、动态空域管理和无人机运行服务。UTICN引入了集成感知与通信（ISAC）、被动和主动定位、智能机器通信、蜂群协调以及控制-数据解耦管理框架等关键技术。我们通过两个用例（即城市级LAE管理平台和多频协作ISAC系统）证明了UTICN的可行性。这项工作为构建LAE的统一运营基础和空域管理架构提供了基础性参考。", "summary": "本文针对低空经济（LAE）快速发展带来的空域管理和无人机操作挑战，提出了一种名为UTICN（无处不在且可信的智能蜂窝原生网络）的统一蜂窝原生架构。UTICN旨在整合多域感知、高精度定位、智能通信、动态空域管理及无人机服务，并通过集成感知与通信（ISAC）等关键技术，解决现有系统碎片化和协调不足的问题。文章通过两个用例验证了UTICN的可行性，为LAE的统一运营基础和空域管理架构提供了参考。", "keywords": "低空经济, 无人机操作, 空域管理, 蜂窝原生网络, 集成感知与通信", "comments": "该论文的创新点在于提出了UTICN这一统一的蜂窝原生架构，旨在整合多种前沿技术（如ISAC、蜂群协调等）以解决低空经济发展中的空域管理和无人机操作碎片化问题。其重要性在于为未来低空空域的智能化、协同化管理提供了潜在的系统级解决方案和基础性参考。"}}
{"id": "2506.08771", "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery", "authors": ["Yuni Susanti", "Michael Färber"], "summary": "Inferring causal relationships between variable pairs is crucial for\nunderstanding multivariate interactions in complex systems. Knowledge-based\ncausal discovery -- which involves inferring causal relationships by reasoning\nover the metadata of variables (e.g., names or textual context) -- offers a\ncompelling alternative to traditional methods that rely on observational data.\nHowever, existing methods using Large Language Models (LLMs) often produce\nunstable and inconsistent results, compromising their reliability for causal\ninference. To address this, we introduce a novel approach that integrates\nKnowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.\nOur approach identifies informative metapath-based subgraphs within KGs and\nfurther refines the selection of these subgraphs using Learning-to-Rank-based\nmodels. The top-ranked subgraphs are then incorporated into zero-shot prompts,\nimproving the effectiveness of LLMs in inferring the causal relationship.\nExtensive experiments on biomedical and open-domain datasets demonstrate that\nour method outperforms most baselines by up to 44.4 points in F1 scores,\nevaluated across diverse LLMs and KGs. Our code and datasets are available on\nGitHub: https://github.com/susantiyuni/path-to-causality", "comment": "Accepted at KDD 2025 (full research paper)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08771v1", "AI": {"title_translation": "因果关系路径：在知识图谱中寻找信息子图以进行基于知识的因果发现", "tldr": "本文提出一种将知识图谱与大型语言模型结合的新方法，通过识别和精炼知识图谱中的信息子图来提高基于知识的因果发现的准确性和稳定性，并在实验中表现优异。", "motivation": "推断变量对之间的因果关系对理解复杂系统至关重要。尽管基于知识的因果发现提供了传统方法的替代方案，但现有使用大型语言模型（LLMs）的方法常产生不稳定和不一致的结果，影响其可靠性。", "method": "本文提出一种将知识图谱（KGs）与大型语言模型（LLMs）集成的新方法。该方法首先在KGs中识别基于元路径的信息子图，然后使用Learning-to-Rank模型精炼子图选择，并将排名靠前的子图整合到零样本提示中，以提高LLMs的因果推理效果。", "result": "在生物医学和开放域数据集上的广泛实验表明，该方法在F1分数上比大多数基线提高了高达44.4分，并在不同的LLMs和KGs上进行了评估。", "conclusion": "该方法成功解决了现有LLM在因果推理中结果不稳定和不一致的问题，显著提高了基于知识的因果发现的准确性和可靠性。", "translation": "推断变量对之间的因果关系对于理解复杂系统中的多变量相互作用至关重要。基于知识的因果发现——通过对变量的元数据（例如名称或文本上下文）进行推理来推断因果关系——为依赖观测数据的传统方法提供了一个引人注目的替代方案。然而，现有使用大型语言模型（LLMs）的方法通常会产生不稳定和不一致的结果，从而损害其因果推理的可靠性。为了解决这个问题，我们引入了一种将知识图谱（KGs）与LLMs相结合的新方法，以增强基于知识的因果发现。我们的方法识别KGs中基于元路径的信息子图，并使用基于学习排序的模型进一步精炼这些子图的选择。然后将排名靠前的子图整合到零样本提示中，从而提高LLMs在推断因果关系方面的有效性。在生物医学和开放域数据集上的广泛实验表明，我们的方法在F1分数上比大多数基线提高了高达44.4分，并在不同的LLMs和KGs上进行了评估。我们的代码和数据集可在GitHub上获取：https://github.com/susantiyuni/path-to-causality", "summary": "本文提出一种新颖的基于知识的因果发现方法，旨在解决现有大型语言模型（LLMs）在此任务中表现不稳定和不一致的问题。该方法创新性地将知识图谱（KGs）与LLMs结合，通过识别和精炼KGs中的信息子图，并将其融入LLMs的零样本提示中，显著提升了因果推理的准确性。实验结果表明，该方法在多个数据集上均优于现有基线。", "keywords": "因果发现, 知识图谱, 大型语言模型, 子图, 元路径", "comments": "本文的创新点在于将知识图谱的结构化知识与大型语言模型的语义理解能力相结合，有效解决了LLM在因果发现中结果不稳定的问题，为知识驱动的因果推理提供了一个可靠且高效的范式。其方法论中的子图识别和排序机制也具有通用性，可能适用于其他需要从复杂知识结构中提取相关信息以辅助LLM推理的任务。"}}
{"id": "2506.08435", "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings", "authors": ["Mingyuan Fan", "Fuyi Wang", "Cen Chen", "Jianying Zhou"], "summary": "Federated learning (FL) enables collaborative model training among multiple\nclients without the need to expose raw data. Its ability to safeguard privacy,\nat the heart of FL, has recently been a hot-button debate topic. To elaborate,\nseveral studies have introduced a type of attacks known as gradient leakage\nattacks (GLAs), which exploit the gradients shared during training to\nreconstruct clients' raw data. On the flip side, some literature, however,\ncontends no substantial privacy risk in practical FL environments due to the\neffectiveness of such GLAs being limited to overly relaxed conditions, such as\nsmall batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that\nclients' data can still be effectively reconstructed, even within realistic FL\nenvironments. Upon revisiting GLAs, we recognize that their performance\nfailures stem from their inability to handle the gradient matching problem. To\nalleviate the performance bottlenecks identified above, we develop FedLeak,\nwhich introduces two novel techniques, partial gradient matching and gradient\nregularization. Moreover, to evaluate the performance of FedLeak in real-world\nFL environments, we formulate a practical evaluation protocol grounded in a\nthorough review of extensive FL literature and industry practices. Under this\nprotocol, FedLeak can still achieve high-fidelity data reconstruction, thereby\nunderscoring the significant vulnerability in FL systems and the urgent need\nfor more effective defense methods.", "comment": "Accepted to Usenix Security 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08435v1", "AI": {"title_translation": "提升梯度泄露攻击：现实联邦学习设置中的数据重建", "tldr": "本文提出FedLeak，一种新的梯度泄露攻击，能在现实联邦学习环境下高精度重建客户端数据，揭示了FL系统的显著隐私漏洞。", "motivation": "联邦学习（FL）的隐私保护能力备受争议。虽然梯度泄露攻击（GLAs）被提出用于重建客户端数据，但有研究认为其在现实FL环境中效果有限，仅限于过于宽松的条件。本文旨在弥合这一差距，证明即使在现实FL环境中，客户端数据仍能被有效重建。", "method": "作者重新审视了GLAs，发现其性能不佳是由于无法处理梯度匹配问题。为解决这一瓶颈，他们开发了FedLeak，引入了两种新颖技术：部分梯度匹配（partial gradient matching）和梯度正则化（gradient regularization）。此外，为了在现实FL环境中评估FedLeak的性能，他们基于对FL文献和行业实践的全面审查，制定了一个实用的评估协议。", "result": "在所制定的评估协议下，FedLeak仍能实现高保真度的数据重建。", "conclusion": "本文强调了联邦学习系统存在显著的隐私漏洞，并迫切需要更有效的防御方法。", "translation": "联邦学习（FL）允许多个客户端协作训练模型，而无需暴露原始数据。其保护隐私的能力是FL的核心，最近成为一个热门的争论话题。具体来说，一些研究引入了一种称为梯度泄露攻击（GLAs）的攻击类型，该攻击利用训练期间共享的梯度来重建客户端的原始数据。另一方面，一些文献却认为在实际的FL环境中不存在实质性的隐私风险，因为此类GLA的有效性仅限于过于宽松的条件，例如小批量大小和对客户端数据分布的了解。\n本文通过经验证明，即使在现实的FL环境中，客户端数据仍然可以被有效重建，从而弥合了这一关键差距。在重新审视GLA时，我们认识到它们的性能失败源于它们无法处理梯度匹配问题。为了缓解上述性能瓶颈，我们开发了FedLeak，它引入了两种新颖的技术：部分梯度匹配和梯度正则化。此外，为了评估FedLeak在真实世界FL环境中的性能，我们基于对大量FL文献和行业实践的彻底审查，制定了一个实用的评估协议。在该协议下，FedLeak仍然可以实现高保真度的数据重建，从而强调了FL系统中存在的显著漏洞以及对更有效防御方法的迫切需求。", "summary": "本文旨在解决联邦学习（FL）中梯度泄露攻击（GLAs）在现实条件下有效性受限的争议。研究发现现有GLAs因无法处理梯度匹配问题而表现不佳。为此，作者开发了FedLeak，该方法引入了部分梯度匹配和梯度正则化两种新颖技术，以实现高保真数据重建。通过制定一套基于行业实践的评估协议，本文实验证明FedLeak即使在现实FL环境下也能有效重建客户端数据，从而揭示了FL系统在隐私保护方面的显著漏洞，并强调了开发更有效防御措施的紧迫性。", "keywords": "联邦学习, 梯度泄露攻击, 数据重建, 隐私保护, FedLeak", "comments": "本文通过提出FedLeak，有效地解决了现有梯度泄露攻击在现实联邦学习环境中表现不佳的问题。其创新点在于引入了部分梯度匹配和梯度正则化技术，并制定了更贴近实际的评估协议。这项工作对于揭示联邦学习系统的隐私脆弱性具有重要意义，将促使研究人员开发更鲁棒的隐私保护机制。"}}
{"id": "2506.08851", "title": "Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization", "authors": ["Sepehr Samavi", "Garvish Bhutani", "Florian Shkurti", "Angela P. Schoellig"], "summary": "Safe and efficient navigation in crowded environments remains a critical\nchallenge for robots that provide a variety of service tasks such as food\ndelivery or autonomous wheelchair mobility. Classical robot crowd navigation\nmethods decouple human motion prediction from robot motion planning, which\nneglects the closed-loop interactions between humans and robots. This lack of a\nmodel for human reactions to the robot plan (e.g. moving out of the way) can\ncause the robot to get stuck. Our proposed Safe and Interactive Crowd\nNavigation (SICNav) method is a bilevel Model Predictive Control (MPC)\nframework that combines prediction and planning into one optimization problem,\nexplicitly modeling interactions among agents. In this paper, we present a\nsystems overview of the crowd navigation platform we use to deploy SICNav in\npreviously unseen indoor and outdoor environments. We provide a preliminary\nanalysis of the system's operation over the course of nearly 7 km of autonomous\nnavigation over two hours in both indoor and outdoor environments.", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics\n  (non-archival)", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08851v1", "AI": {"title_translation": "在现场部署SICNav：使用MPC和双层优化实现安全交互式人群导航", "tldr": "SICNav是一种基于MPC和双层优化的机器人人群导航方法，已在室内外环境中成功部署和测试。", "motivation": "机器人在拥挤环境中安全高效导航仍是关键挑战，传统方法忽略人机闭环交互，导致机器人容易受阻。", "method": "本文提出了安全交互式人群导航（SICNav）方法，这是一个双层模型预测控制（MPC）框架，它将人类运动预测与机器人运动规划结合为一个优化问题，明确建模了智能体之间的交互。", "result": "该系统已在未知的室内和室外环境中部署，并进行了近7公里、两小时的自主导航，提供了系统操作的初步分析。", "conclusion": "SICNav方法在实际室内外拥挤环境中实现了安全和交互式的机器人导航，初步部署结果验证了其有效性。", "translation": "在拥挤环境中安全高效地导航对于提供送餐或自动轮椅移动等各种服务任务的机器人来说仍然是一个严峻的挑战。经典的机器人人群导航方法将人类运动预测与机器人运动规划解耦，这忽略了人类和机器人之间的闭环交互。这种缺乏对人类对机器人计划反应（例如，让路）的模型可能导致机器人卡住。我们提出的安全交互式人群导航（SICNav）方法是一个双层模型预测控制（MPC）框架，它将预测和规划结合到一个优化问题中，明确建模智能体之间的交互。在本文中，我们介绍了用于在以前未见的室内外环境中部署SICNav的人群导航平台的系统概述。我们提供了该系统在室内外环境中近7公里、两小时自主导航过程中运行的初步分析。", "summary": "本文提出了一种名为SICNav的安全交互式人群导航方法，该方法采用双层模型预测控制（MPC）框架，旨在解决传统机器人导航方法中忽略人机交互导致机器人卡顿的问题。SICNav将人类运动预测与机器人运动规划整合到一个优化问题中，显式建模了智能体间的交互。文章详细介绍了用于在未知室内外环境中部署SICNav的平台，并提供了该系统在近7公里、两小时自主导航期间运行的初步分析。", "keywords": "人群导航, 模型预测控制, 双层优化, 机器人, 人机交互", "comments": "该研究通过提出结合预测和规划的SICNav方法，有效解决了机器人人群导航中人机交互的挑战，并提供了实际部署和测试的初步结果，具有重要的实践意义。"}}
{"id": "2506.08351", "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08351v1", "AI": {"title_translation": "多少引导才够：重新审视无分类器引导文本到图像扩散模型中的自适应引导", "tldr": "本文提出了一种名为Step AG的简单普适自适应引导策略，通过将无分类器引导限制在去噪的前几个步骤，可在不牺牲图像质量和对齐度的情况下，将文本到图像扩散模型的生成速度平均提升20%到30%。", "motivation": "现有的文本到图像扩散模型中，无分类器引导（CFG）虽然是主流方法，但其计算成本高昂（需要两倍的模型前向传播步骤）。此外，先前的自适应引导方法缺乏扎实的分析和经验结果，无法普遍应用于各种扩散模型。", "method": "本文提出了一种名为Step AG的简单且普适的自适应引导策略。该方法的核心思想是将无分类器引导（CFG）限制在去噪过程的最初几个步骤。", "result": "实验结果表明，仅在去噪的前几个步骤中限制无分类器引导就足以生成高质量、条件良好的图像。该方法实现了20%到30%的平均速度提升，并且这种改进在不同的推理步数和包括视频生成模型在内的各种模型设置下都保持一致。", "conclusion": "本文提出的Step AG方法是一种优越的自适应引导策略，能够显著提高文本到图像扩散模型的生成速度，同时保持图像质量和文本对齐度，且具有普适性。", "translation": "随着文本到图像生成扩散模型的快速发展，无分类器引导已成为最普遍的条件化方法。然而，这种方法固有地需要两倍于无条件生成的模型前向传播步骤，导致成本显著提高。虽然之前的研究引入了自适应引导的概念，但它缺乏扎实的分析和经验结果，使得之前的方法无法应用于通用扩散模型。在这项工作中，我们提出了应用自适应引导的另一种视角，并提出了Step AG，这是一种简单、普适的自适应引导策略。我们的评估侧重于图像质量和图像-文本对齐，结果表明将无分类器引导限制在去噪的前几个步骤足以生成高质量、条件良好的图像，实现了20%到30%的平均速度提升。这种改进在不同的设置（如推理步数）和包括视频生成模型在内的各种模型中都保持一致，突出了我们方法的优越性。", "summary": "本文针对文本到图像扩散模型中无分类器引导（CFG）计算成本高的问题，提出了一种名为Step AG的普适性自适应引导策略。该策略通过仅在去噪过程的早期阶段应用CFG，显著提升了模型生成速度（平均20%-30%），同时保持了图像质量和文本对齐度，并且在多种模型和设置下均表现出优越性。", "keywords": "自适应引导, 无分类器引导, 扩散模型, 文本到图像生成, 效率提升", "comments": "本文创新性地提出了一种简单而有效的自适应引导策略Step AG，解决了无分类器引导在扩散模型中计算成本高昂的问题。其核心思想是洞察到无分类器引导在去噪初期对图像质量和对齐度的关键作用，从而在后期阶段可以减少其应用，这对于提高扩散模型的推理效率具有重要意义。该方法的普适性和在视频生成模型上的表现也增加了其应用价值。"}}
{"id": "2506.08580", "title": "HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning", "authors": ["Yang Lv", "Jinlong Lei", "Peng Yi"], "summary": "Two-stage Colonel Blotto game represents a typical adversarial resource\nallocation problem, in which two opposing agents sequentially allocate\nresources in a network topology across two phases: an initial resource\ndeployment followed by multiple rounds of dynamic reallocation adjustments. The\nsequential dependency between game stages and the complex constraints imposed\nby the graph topology make it difficult for traditional approaches to attain a\nglobally optimal strategy. To address these challenges, we propose a\nhierarchical graph Transformer framework called HGformer. By incorporating an\nenhanced graph Transformer encoder with structural biases and a two-agent\nhierarchical decision model, our approach enables efficient policy generation\nin large-scale adversarial environments. Moreover, we design a layer-by-layer\nfeedback reinforcement learning algorithm that feeds the long-term returns from\nlower-level decisions back into the optimization of the higher-level strategy,\nthus bridging the coordination gap between the two decision-making stages.\nExperimental results demonstrate that, compared to existing hierarchical\ndecision-making or graph neural network methods, HGformer significantly\nimproves resource allocation efficiency and adversarial payoff, achieving\nsuperior overall performance in complex dynamic game scenarios.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08580v1", "AI": {"title_translation": "HGFormer：一种基于强化学习的两阶段布洛托上校博弈分层图Transformer框架", "tldr": "HGFormer是一个分层图Transformer框架，通过强化学习解决了两阶段布洛托上校博弈中的资源分配问题，显著提高了效率和对抗收益。", "motivation": "两阶段布洛托上校博弈是一种典型的对抗性资源分配问题，传统方法难以获得全局最优策略，因为游戏阶段之间的顺序依赖性和图拓扑施加的复杂约束。", "method": "我们提出了一个名为HGFormer的分层图Transformer框架。它包含一个带有结构偏差的增强图Transformer编码器和一个双智能体分层决策模型。此外，我们设计了一种逐层反馈强化学习算法，将低级决策的长期回报反馈给高级策略的优化，从而弥合了两个决策阶段之间的协调差距。", "result": "实验结果表明，与现有分层决策或图神经网络方法相比，HGFormer显著提高了资源分配效率和对抗收益，在复杂动态博弈场景中实现了卓越的整体性能。", "conclusion": "HGFormer在复杂动态博弈场景中表现出卓越的整体性能，通过有效解决两阶段布洛托上校博弈中的资源分配和策略生成挑战。", "translation": "两阶段布洛托上校博弈代表了一个典型的对抗性资源分配问题，其中两个对抗性智能体在网络拓扑中分两个阶段顺序分配资源：初始资源部署，随后是多轮动态重新分配调整。游戏阶段之间的顺序依赖性和图拓扑施加的复杂约束使得传统方法难以获得全局最优策略。为了解决这些挑战，我们提出了一个名为HGFormer的分层图Transformer框架。通过结合一个带有结构偏差的增强图Transformer编码器和一个双智能体分层决策模型，我们的方法能够在大规模对抗环境中实现高效的策略生成。此外，我们设计了一种逐层反馈强化学习算法，将低级决策的长期回报反馈给高级策略的优化，从而弥合了两个决策阶段之间的协调差距。实验结果表明，与现有分层决策或图神经网络方法相比，HGFormer显著提高了资源分配效率和对抗收益，在复杂动态博弈场景中实现了卓越的整体性能。", "summary": "本研究提出了HGFormer，一个分层图Transformer框架，旨在解决两阶段布洛托上校博弈中的资源分配和策略生成挑战。该框架结合了增强型图Transformer编码器和双智能体分层决策模型，并引入了逐层反馈强化学习算法来优化跨阶段协调。实验证明，HGFormer在资源分配效率和对抗收益方面优于现有方法，在复杂动态博弈场景中表现出色。", "keywords": "两阶段布洛托上校博弈, 分层图Transformer, 强化学习, 资源分配, 对抗性博弈", "comments": "HGFormer的创新之处在于其结合了分层图Transformer结构和独特的逐层反馈强化学习算法，有效地解决了两阶段布洛托上校博弈中序列依赖和复杂拓扑带来的挑战。这种方法为大规模对抗性资源分配问题提供了新的解决方案，具有重要的理论和实践意义。"}}
{"id": "2506.08608", "title": "Q-learning-based Hierarchical Cooperative Local Search for Steelmaking-continuous Casting Scheduling Problem", "authors": ["Yang Lv", "Rong Hu", "Bin Qian", "Jian-Bo Yang"], "summary": "The steelmaking continuous casting scheduling problem (SCCSP) is a critical\nand complex challenge in modern steel production, requiring the coordinated\nassignment and sequencing of steel charges across multiple production stages.\nEfficient scheduling not only enhances productivity but also significantly\nreduces energy consumption. However, both traditional heuristics (e.g.,\ntwo-stage local search) and recent metaheuristics often struggle to adapt to\nthe dynamic characteristics of practical SCCSP instances. To address these\nlimitations, this paper introduces a novel Q learning based hierarchical\ncooperative local search framework, termed HierC_Q, aimed at minimizing the\nweighted sum of the maximum completion time and the average waiting time in\nSCCSP. The core contributions of HierC_Q are twofold. First, considering the\nintrinsic coupling properties of the SCCSP, a dedicated reward function is\nproposed based on a novel coupling measure (CM), guiding the exploration\nprocess towards promising regions of the solution space. Second, a hierarchical\narchitecture is devised, comprising two distinct tiers: the learn to improve\n(L2I) tier and the \"disturb to renovate\" (D2R) tier. The L2I tier performs deep\nexploitation within promising regions using two independent Q-learning-based\nlocal search frameworks (QLSFs) tailored for subproblems, along with a synergy\nQLSF designed for the main problem. To enhance the effectiveness of local\nsearch, a validity evaluation approach and a speed-up evaluation method are\nalso intro-duced, grounded in a detailed study of the problem's structure.\nMeanwhile, the D2R tier incorporates a perturbation and construction based\nsolution renewal strategy to mitigate the risk of premature convergence. The\nsuperiority and effectiveness of HierC_Q are demonstrated through extensive\ncomparisons with eleven local search frameworks and nine state-of-the-art\nalgorithms.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08608v1", "AI": {"title_translation": "基于Q学习的分层协同局部搜索解决炼钢-连铸调度问题", "tldr": "本文提出了一种基于Q学习的分层协同局部搜索框架（HierC_Q），以解决炼钢-连铸调度问题，通过创新的奖励函数和分层架构（L2I和D2R）显著提高了调度效率并避免了早熟收敛，性能优于现有方法。", "motivation": "炼钢-连铸调度问题（SCCSP）是现代钢铁生产中一个关键且复杂的挑战，现有启发式和元启发式方法难以适应其实际动态特性，导致生产力受限且能耗较高。", "method": "本文提出了一种名为HierC_Q的基于Q学习的分层协同局部搜索框架，旨在最小化最大完工时间和平均等待时间的加权和。其核心贡献包括：1. 提出了一种基于新型耦合度量（CM）的专用奖励函数，以利用SCCSP的内在耦合特性并指导搜索。2. 设计了一个分层架构，包含“学习改进”（L2I）层和“扰动创新”（D2R）层。L2I层利用两个独立的Q学习局部搜索框架（QLSFs）进行子问题深度开发，并结合一个协同QLSF处理主问题，同时引入有效性评估和加速评估方法。D2R层则采用基于扰动和构造的解决方案更新策略，以避免早熟收敛。", "result": "HierC_Q在广泛的比较中，与十一个局部搜索框架和九个最先进的算法相比，展示了其优越性和有效性。", "conclusion": "本文提出的HierC_Q框架通过其创新的奖励函数和分层协同机制，有效解决了炼钢-连铸调度问题中的复杂性和动态适应性挑战，显著提升了调度性能，并克服了传统方法的局限性。", "translation": "炼钢-连铸调度问题（SCCSP）是现代钢铁生产中一个关键且复杂的挑战，需要协调分配和排序多个生产阶段的钢水。高效调度不仅能提高生产率，还能显著降低能耗。然而，传统的启发式方法（例如两阶段局部搜索）和最近的元启发式方法往往难以适应实际SCCSP实例的动态特性。为了解决这些限制，本文引入了一种新颖的基于Q学习的分层协同局部搜索框架，称为HierC_Q，旨在最小化SCCSP中最大完工时间和平均等待时间的加权和。HierC_Q的核心贡献有两方面。首先，考虑到SCCSP固有的耦合特性，提出了一种基于新型耦合度量（CM）的专用奖励函数，指导探索过程走向解空间中有希望的区域。其次，设计了一种分层架构，包括两个不同的层级：“学习改进”（L2I）层和“扰动创新”（D2R）层。L2I层利用两个独立的针对子问题的基于Q学习的局部搜索框架（QLSFs）以及一个为主问题设计的协同QLSF，在有希望的区域内进行深度开发。为了提高局部搜索的有效性，还基于对问题结构的详细研究，引入了有效性评估方法和加速评估方法。同时，D2R层结合了基于扰动和构造的解决方案更新策略，以减轻早熟收敛的风险。通过与十一个局部搜索框架和九个最先进算法的广泛比较，证明了HierC_Q的优越性和有效性。", "summary": "本文针对炼钢-连铸调度问题（SCCSP）中现有方法难以适应动态特性的问题，提出了一种名为HierC_Q的基于Q学习的分层协同局部搜索框架。该框架通过引入基于耦合度量（CM）的专用奖励函数来引导搜索，并设计了两层分层架构：L2I层利用多个Q学习局部搜索框架进行深度开发，D2R层通过扰动策略避免早熟收敛。实验结果表明，HierC_Q在调度性能上显著优于多种现有局部搜索和最先进算法。", "keywords": "炼钢-连铸调度问题, Q学习, 分层协同局部搜索, 耦合度量, 启发式算法", "comments": "这篇论文通过结合Q学习和分层协同局部搜索，为解决复杂的炼钢-连铸调度问题提供了一个创新性框架。其创新点在于引入了考虑问题耦合特性的奖励函数，以及L2I和D2R两层架构，有效平衡了探索与开发，并避免了早熟收敛。该方法在实际工业调度中具有重要应用价值，有望提高生产效率并降低能耗。"}}
{"id": "2506.08813", "title": "Numerical stability of force-gradient integrators and their Hessian-free variants in lattice QCD simulations", "authors": ["Kevin Schäfers", "Jacob Finkenrath", "Michael Günther", "Francesco Knechtli"], "summary": "We investigate the numerical stability of force-gradient integrators and\ntheir Hessian-free variants within the molecular dynamics step of the\nHamiltonian Monte Carlo algorithm in lattice QCD simulations. A linear\nstability analysis of (Hessian-free) force-gradient integrators is conducted by\ninvestigating the harmonic oscillator as a test equation. By performing\ndetailed stability investigations for the entire family of self-adjoint\nintegrators with up to eleven exponentials per time step, we detect promising\nintegrator variants that are providing a good trade-off between accuracy and\nnumerical stability. Simulations for the two-dimensional Schwinger model\ndemonstrate that there are no significant differences in the stability domain\nof a force-gradient integrator and its Hessian-free counterpart. Furthermore,\nlattice QCD simulations are conducted to emphasize the significance of\nnumerical stability as a metric for evaluating the computational efficiency of\nintegrators when applied to lattice QCD simulations.", "comment": "18 pages, 6 figures", "cate": "hep-lat", "url": "http://arxiv.org/abs/2506.08813v1", "AI": {"title_translation": "格点QCD模拟中力梯度积分器及其无Hessian变体的数值稳定性", "tldr": "本文研究了格点QCD模拟中力梯度积分器及其无Hessian变体的数值稳定性，通过线性稳定性分析和模拟发现了一些有前景的积分器，并强调了数值稳定性作为评估计算效率指标的重要性。", "motivation": "在格点QCD模拟的哈密顿蒙特卡洛算法的分子动力学步骤中，研究力梯度积分器及其无Hessian变体的数值稳定性，并强调数值稳定性作为评估计算效率指标的重要性。", "method": "通过对谐振子进行线性稳定性分析，检测了自伴随积分器家族的稳定性。此外，还进行了二维Schwinger模型和格点QCD模拟。", "result": "发现了一些在精度和数值稳定性之间取得良好平衡的有前景的积分器变体。力梯度积分器及其无Hessian对应物的稳定性域没有显著差异。强调了数值稳定性作为评估格点QCD模拟中积分器计算效率的指标的重要性。", "conclusion": "数值稳定性是评估格点QCD模拟中积分器计算效率的一个重要指标。", "translation": "我们研究了格点QCD模拟中哈密顿蒙特卡洛算法的分子动力学步骤中力梯度积分器及其无Hessian变体的数值稳定性。通过将谐振子作为测试方程，对（无Hessian）力梯度积分器进行了线性稳定性分析。通过对每个时间步最多包含十一个指数项的整个自伴随积分器家族进行详细的稳定性研究，我们发现了一些有前景的积分器变体，它们在精度和数值稳定性之间提供了良好的权衡。二维Schwinger模型的模拟表明，力梯度积分器及其无Hessian对应物的稳定性域没有显著差异。此外，还进行了格点QCD模拟，以强调数值稳定性作为评估应用于格点QCD模拟的积分器计算效率指标的重要性。", "summary": "本文在格点QCD模拟的哈密顿蒙特卡洛算法框架内，深入探讨了力梯度积分器及其无Hessian变体的数值稳定性。通过对谐振子的线性稳定性分析，研究了自伴随积分器家族，并识别出在精度和稳定性之间实现良好平衡的积分器变体。研究表明，力梯度积分器与其无Hessian对应物在稳定性域上没有显著差异。此外，格点QCD模拟进一步证实了数值稳定性在评估积分器计算效率方面的重要性。", "keywords": "力梯度积分器, 数值稳定性, 格点QCD, 哈密顿蒙特卡洛, Schwinger模型", "comments": "这项研究对于优化格点QCD模拟的计算效率具有重要意义，因为它识别了具有良好数值稳定性的积分器，并强调了将其作为评估指标的重要性。特别是，发现无Hessian变体与标准力梯度积分器具有相似的稳定性域，这可能为未来的算法开发提供灵活性。"}}
{"id": "2506.08856", "title": "Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation", "authors": ["Jonathan P. King", "Harnoor Ahluwalia", "Michael Zhang", "Nancy S. Pollard"], "summary": "This work presents a fast anytime algorithm for computing globally optimal\nindependent contact regions (ICRs). ICRs are regions such that one contact\nwithin each region enables a valid grasp. Locations of ICRs can provide\nguidance for grasp and manipulation planning, learning, and policy transfer.\nHowever, ICRs for modern applications have been little explored, in part due to\nthe expense of computing them, as they have a search space exponential in the\nnumber of contacts. We present a divide and conquer algorithm based on\nincremental n-dimensional Delaunay triangulation that produces results with\nbounded suboptimality in times sufficient for real-time planning. This paper\npresents the base algorithm for grasps where contacts lie within a plane. Our\nexperiments show substantial benefits over competing grasp quality metrics and\nspeedups of 100X and more for competing approaches to computing ICRs. We\nexplore robustness of a policy guided by ICRs and outline a path to general 3D\nimplementation. Code will be released on publication to facilitate further\ndevelopment and applications.", "comment": "Submitted to IEEE Conference on Humanoid Robots", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08856v1", "AI": {"title_translation": "鲁棒抓取与操作中全局最优独立接触区域的快速估计", "tldr": "提出了一种快速算法，用于计算全局最优独立接触区域（ICRs），显著提高了计算ICRs的速度，可用于实时抓取规划。", "motivation": "独立接触区域（ICRs）对于抓取和操作规划、学习及策略迁移至关重要，但由于计算成本高昂（搜索空间随接触点数量呈指数增长），在现代应用中探索不足。", "method": "提出了一种基于增量n维Delaunay三角剖分的“分而治之”算法，该算法能够以实时规划所需的时间生成具有有界次优性的结果。本文介绍了接触点位于平面内的抓取基础算法。", "result": "实验表明，与竞争性的抓取质量指标相比具有显著优势，并且计算ICRs的速度比现有方法快100倍甚至更多。研究了ICRs引导策略的鲁棒性，并概述了实现通用3D实现的方法。", "conclusion": "本文提出了一种高效的算法来计算ICRs，解决了其计算成本高的问题，为实时抓取和操作规划提供了可能，并展示了其在鲁棒性方面的潜力。", "translation": "这项工作提出了一种快速的随时可用算法，用于计算全局最优独立接触区域（ICRs）。ICRs是这样的区域：每个区域内的一个接触点即可实现有效抓取。ICRs的位置可以为抓取和操作规划、学习以及策略转移提供指导。然而，由于计算成本高昂（其搜索空间随接触点数量呈指数增长），现代应用中的ICRs探索甚少。我们提出了一种基于增量n维Delaunay三角剖分的“分而治之”算法，该算法能够在实时规划所需的时间内生成具有有界次优性的结果。本文介绍了接触点位于平面内的抓取基础算法。我们的实验表明，与竞争性的抓取质量指标相比具有显著优势，并且计算ICRs的速度比现有方法快100倍甚至更多。我们探讨了ICRs引导策略的鲁棒性，并概述了实现通用3D实现的方法。代码将在发布时公开，以促进进一步的开发和应用。", "summary": "本文提出了一种名为“快速随时可用算法”的新方法，用于高效计算全局最优独立接触区域（ICRs）。ICRs对机器人抓取和操作规划至关重要，但传统的计算方法因其指数级的搜索空间而效率低下。该算法采用基于增量n维Delaunay三角剖分的“分而治之”策略，能够在实时规划的时间限制内提供具有有界次优性的结果。实验证明，该算法在平面接触抓取方面表现出色，计算速度比现有方法快100倍以上，并提升了抓取策略的鲁棒性，为未来3D实现奠定了基础。", "keywords": "独立接触区域, 抓取规划, 机器人操作, Delaunay三角剖分, 实时算法", "comments": "这篇论文的创新点在于提出了一个高效的算法来解决ICRs计算成本高昂的问题，这对于实时机器人抓取和操作具有重要意义。其“分而治之”结合Delaunay三角剖分的思想，有效降低了计算复杂性，实现了显著的性能提升。虽然目前主要针对平面接触抓取，但作者已明确指出向3D实现的路径，这预示着该方法在未来有更广泛的应用潜力。代码的开源也为后续研究和工业应用提供了便利。"}}
{"id": "2506.08356", "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding", "authors": ["Shivang Chopra", "Lingchao Mao", "Gabriela Sanchez-Rodriguez", "Andrew J Feola", "Jing Li", "Zsolt Kira"], "summary": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08356v1", "AI": {"title_translation": "MedMoE：用于医学视觉-语言理解的模态专用专家混合模型", "tldr": "MedMoE是一个视觉-语言处理框架，它利用专家混合（MoE）模块，根据诊断上下文动态调整视觉表示，以适应不同的医学成像模态，从而在无需模态特定监督的情况下提高视觉-语言理解能力。", "motivation": "大多数现有的医学领域视觉-语言框架在局部特征提取方面采用统一策略，忽视了模态特异性需求，而不同的医学成像模态以不同空间分辨率捕获诊断信息。", "method": "MedMoE是一个模块化、可扩展的视觉-语言处理框架。它包含一个基于报告类型进行条件化的专家混合（MoE）模块，该模块将多尺度图像特征通过专门的专家分支进行路由，这些分支经过训练以捕获模态特定的视觉语义。这些专家在源自Swin Transformer骨干的特征金字塔上操作，从而实现对临床相关区域的空间自适应注意力。该框架在推理时无需模态特定监督即可生成与文本描述对齐的局部视觉表示。", "result": "MedMoE在多样化的医学基准测试中，提高了跨成像模态的对齐和检索性能。", "conclusion": "模态专用视觉表示在临床视觉-语言系统中具有重要价值。", "translation": "不同的医学成像模态以不同的空间分辨率捕获诊断信息，从粗略的全局模式到细粒度的局部结构。然而，医学领域大多数现有的视觉-语言框架在局部特征提取方面采用统一策略，忽视了模态特异性需求。在这项工作中，我们提出了MedMoE，一个模块化、可扩展的视觉-语言处理框架，它根据诊断上下文动态调整视觉表示。MedMoE包含一个基于报告类型进行条件化的专家混合（MoE）模块，该模块将多尺度图像特征通过专门的专家分支进行路由，这些分支经过训练以捕获模态特定的视觉语义。这些专家在源自Swin Transformer骨干的特征金字塔上操作，从而实现对临床相关区域的空间自适应注意力。该框架在推理时无需模态特定监督即可生成与文本描述对齐的局部视觉表示。在多样化的医学基准测试上的实证结果表明，MedMoE提高了跨成像模态的对齐和检索性能，强调了模态专用视觉表示在临床视觉-语言系统中的价值。", "summary": "MedMoE通过引入模态专用专家混合（MoE）模块，解决了现有医学视觉-语言框架在局部特征提取中忽视模态特异性需求的局限性。该框架根据诊断上下文动态调整视觉表示，利用从Swin Transformer骨干派生的多尺度特征训练的专家分支来捕获模态特定的视觉语义。它在多样化的医学成像模态上提升了对齐和检索性能，突显了专用视觉表示在临床视觉-语言理解中的重要性。", "keywords": "医学视觉-语言, 专家混合, 模态专用, Swin Transformer, 医学影像", "comments": "MedMoE的创新在于其使用MoE模块动态调整视觉表示，解决了医学视觉-语言处理中对模态特异性需求的关键忽视。这种方法在推理时无需模态特定监督即可提高性能，使其适用于多样化的临床环境。"}}
{"id": "2506.08627", "title": "FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings", "authors": ["Douwe Geurtjens", "Xixi Lu"], "summary": "Conformance checking is a fundamental task of process mining, which\nquantifies the extent to which the observed process executions match a\nnormative process model. The state-of-the-art approaches compute alignments by\nexploring the state space formed by the synchronous product of the process\nmodel and the trace. This often leads to state space explosion, particularly\nwhen the model exhibits a high degree of choice and concurrency. Moreover, as\nalignments inherently impose a sequential structure, they fail to fully\nrepresent the concurrent behavior present in many real-world processes. To\naddress these limitations, this paper proposes a new technique for computing\npartial-order alignments {on the fly using directed Petri net unfoldings, named\nFoldA. We evaluate our technique on 485 synthetic model-log pairs and compare\nit against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6\nbenchmark pairs. The results show that our unfolding alignment, although it\nrequires more computation time, generally reduces the number of queued states\nand provides a more accurate representation of concurrency.", "comment": "Conditionally accepted at BPM 2025", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08627v1", "AI": {"title_translation": "FoldA：使用有向网展开计算偏序对齐", "tldr": "本文提出了一种名为FoldA的新技术，它使用有向Petri网展开来计算偏序对齐，以解决现有对齐方法在处理并发行为和状态空间爆炸方面的局限性。", "motivation": "现有的依从性检查方法通过探索模型和轨迹同步乘积形成的状态空间来计算对齐，这经常导致状态空间爆炸，尤其当模型具有高度选择和并发性时。此外，对齐本质上强加了顺序结构，未能充分表示许多真实世界过程中存在的并发行为。", "method": "本文提出了一种名为FoldA的新技术，用于使用有向Petri网展开实时计算偏序对齐。", "result": "尽管FoldA需要更多的计算时间，但它通常减少了排队状态的数量，并提供了更准确的并发表示。", "conclusion": "FoldA技术通过使用有向Petri网展开计算偏序对齐，有效解决了传统对齐方法在处理并发性时的局限性，并减少了状态空间问题，尽管计算时间有所增加。", "translation": "一致性检查是过程挖掘的一项基本任务，它量化了观察到的过程执行与规范过程模型匹配的程度。最先进的方法通过探索过程模型和轨迹同步乘积形成的状态空间来计算对齐。这通常会导致状态空间爆炸，特别是在模型表现出高度选择和并发性时。此外，由于对齐本质上强加了顺序结构，它们未能充分表示许多真实世界过程中存在的并发行为。为了解决这些限制，本文提出了一种名为FoldA的新技术，用于使用有向Petri网展开实时计算偏序对齐。我们在485个合成模型-日志对上评估了我们的技术，并将其与13个真实模型-日志对和6个基准对上的Astar和Dijkstra对齐进行了比较。结果表明，我们的展开对齐，尽管需要更多的计算时间，但通常减少了排队状态的数量，并提供了更准确的并发表示。", "summary": "本文提出了一种名为FoldA的新技术，用于使用有向Petri网展开实时计算偏序对齐。该方法旨在解决现有依从性检查方法中状态空间爆炸和无法充分表示并发行为的问题。通过对合成和真实数据集的评估，结果显示FoldA虽然计算时间较长，但能显著减少排队状态并更准确地表示并发性。", "keywords": "偏序对齐, Petri网展开, 依从性检查, 过程挖掘, 并发性", "comments": "FoldA通过引入偏序对齐和有向Petri网展开，创新性地解决了传统依从性检查中状态空间爆炸和并发性表示不足的重大挑战。尽管计算时间增加，但其在准确表示并发行为和减少状态数量方面的优势，使其在过程挖掘领域具有重要意义。"}}
{"id": "2506.08676", "title": "Linguistic Ordered Weighted Averaging based deep learning pooling for fault diagnosis in a wastewater treatment plant", "authors": ["Alicia Beneyto-Rodriguez", "Gregorio I. Sainz-Palmero", "Marta Galende-Hernández", "María J. Fuente"], "summary": "Nowadays, water reuse is a serious challenge to help address water shortages.\nHere, the wastewater treatment plants (WWTP) play a key role, and its proper\noperation is mandatory. So, fault diagnosis is a key activity for these plants.\nTheir high complexity and large-scale require of smart methodologies for that\nfault diagnosis and safety operation. All these large-scale and complex\nindustrial processes are monitored, allowing the data collection about the\nplant operation, so data driven approaches for fault diagnosis can be applied.\nA popular approach to fault diagnosis is deep learning-based methodologies.\nHere, a fault diagnosis methodology is proposed for a WWTP using a new\nlinguistic Ordered Weighted Averaging (OWA) pooling based Deep Convolutional\nNeural Network (DCNN) and a sliding and overlapping time window. This window\nslides over input data based on the monitoring sampling time, then the\ndiagnosis is carried out by the linguistic OWA pooling based DCNN. This\nalternative linguistic pooling uses well-known linguistic OWA quantifiers,\nwhich permit terms such as \\textsl{Most, AtLeast, etc.}, supplying new\nintuitive options for the pooling tasks. This sliding time window and the OWA\npooling based network permit a better and earlier fault diagnosis, at each\nsampling time, using a few monitoring samples and a fewer learning iterations\nthan DCNN standard pooling. Several linguistic OWA operators have been checked\nwith a benchmark for WWTPs. A set of 5 fault types has been used, taking into\naccount 140 variables sampled at 15 minutes time intervals. The performance has\nbeen over $91\\%$ for $Accuracy$, $Recall$ or $F1-Score$, and better than other\ncompetitive methodologies. Moreover, these linguistic OWA operators for DCNN\npooling have shown a better performance than the standard \\textsl{Max} and\n\\textsl{Average} options.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08676v1", "AI": {"title_translation": "基于语言有序加权平均的深度学习池化在污水处理厂故障诊断中的应用", "tldr": "本文提出了一种基于语言有序加权平均（OWA）池化的新型深度卷积神经网络（DCNN），用于污水处理厂的故障诊断，与标准方法相比，该方法能以更少的样本和迭代实现更好、更早的诊断。", "motivation": "水资源短缺使得水资源再利用成为一项严峻挑战，其中污水处理厂（WWTP）的正常运行至关重要。因此，故障诊断是这些复杂、大规模工厂的关键活动，需要智能的数据驱动方法。深度学习是故障诊断的流行方法。", "method": "本文提出了一种基于新型语言有序加权平均（OWA）池化和滑动重叠时间窗口的深度卷积神经网络（DCNN）故障诊断方法。该方法利用语言OWA量词（如“大多数”、“至少”）提供直观的池化选项，并通过基于语言OWA池化的DCNN对滑动时间窗口上的输入数据进行诊断。", "result": "该方法在每个采样时间都能实现更好、更早的故障诊断，并且比标准DCNN池化使用更少的监测样本和学习迭代。在WWTPs基准测试中，其准确率、召回率或F1-分数均超过91%，且优于其他竞争方法。此外，这些语言OWA算子在DCNN池化中表现出比标准Max和Average选项更好的性能。", "conclusion": "基于语言OWA池化的深度卷积神经网络为污水处理厂的故障诊断提供了一种稳健且有效的解决方案，与传统深度学习池化方法相比，其性能和效率均有所提高。", "translation": "如今，水资源再利用是解决水资源短缺的一项严峻挑战。其中，污水处理厂（WWTP）发挥着关键作用，其正常运行是强制性的。因此，故障诊断是这些工厂的一项关键活动。其高度复杂性和大规模运行需要智能方法来进行故障诊断和安全操作。所有这些大规模、复杂的工业过程都受到监控，从而可以收集有关工厂运行的数据，因此可以应用数据驱动的故障诊断方法。深度学习方法是故障诊断的一种流行方法。本文提出了一种用于污水处理厂的故障诊断方法，该方法采用了一种新的基于语言有序加权平均（OWA）池化的深度卷积神经网络（DCNN）和滑动重叠时间窗口。该窗口根据监测采样时间在输入数据上滑动，然后由基于语言OWA池化的DCNN进行诊断。这种替代的语言池化使用了众所周知的语言OWA量词，例如“大多数”、“至少”等，为池化任务提供了新的直观选择。这种滑动时间窗口和基于OWA池化的网络允许在每个采样时间进行更好、更早的故障诊断，与DCNN标准池化相比，使用更少的监测样本和更少的学习迭代。我们已经使用一个WWTP基准测试了多种语言OWA算子。使用了5种故障类型，考虑了以15分钟时间间隔采样的140个变量。在准确率、召回率或F1-分数方面，性能超过91%，并且优于其他竞争方法。此外，这些用于DCNN池化的语言OWA算子表现出比标准“最大”和“平均”选项更好的性能。", "summary": "本文提出了一种新颖的污水处理厂（WWTP）故障诊断方法，该方法结合了深度卷积神经网络（DCNN）、新型语言有序加权平均（OWA）池化技术和滑动时间窗口。该方法利用语言量词实现更直观的池化操作。与标准DCNN池化相比，所提出的方法能够以更少的数据样本和迭代实现更早、更准确的故障诊断，并在WWTP基准测试中取得了超过91%的准确率、召回率和F1-分数，优于传统方法。", "keywords": "故障诊断, 污水处理厂, 深度学习, 有序加权平均, 语言池化", "comments": "本文的创新之处在于将语言OWA池化集成到DCNN中用于故障诊断，提供了一种更直观、更高效的特征聚合方式。语言量词的使用提供了灵活性，而其在资源消耗更少的情况下实现性能提升的特点，凸显了其在污水处理厂等复杂工业系统实时监测中的实际应用价值。"}}
{"id": "2506.08991", "title": "Do Concept Replacement Techniques Really Erase Unacceptable Concepts?", "authors": ["Anudeep Das", "Gurjot Singh", "Prach Chantasantitam", "N. Asokan"], "summary": "Generative models, particularly diffusion-based text-to-image (T2I) models,\nhave demonstrated astounding success. However, aligning them to avoid\ngenerating content with unacceptable concepts (e.g., offensive or copyrighted\ncontent, or celebrity likenesses) remains a significant challenge. Concept\nreplacement techniques (CRTs) aim to address this challenge, often by trying to\n\"erase\" unacceptable concepts from models. Recently, model providers have\nstarted offering image editing services which accept an image and a text prompt\nas input, to produce an image altered as specified by the prompt. These are\nknown as image-to-image (I2I) models. In this paper, we first use an I2I model\nto empirically demonstrate that today's state-of-the-art CRTs do not in fact\nerase unacceptable concepts. Existing CRTs are thus likely to be ineffective in\nemerging I2I scenarios, despite their proven ability to remove unwanted\nconcepts in T2I pipelines, highlighting the need to understand this discrepancy\nbetween T2I and I2I settings. Next, we argue that a good CRT, while replacing\nunacceptable concepts, should preserve other concepts specified in the inputs\nto generative models. We call this fidelity. Prior work on CRTs have neglected\nfidelity in the case of unacceptable concepts. Finally, we propose the use of\ntargeted image-editing techniques to achieve both effectiveness and fidelity.\nWe present such a technique, AntiMirror, and demonstrate its viability.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08991v1", "AI": {"title_translation": "概念替换技术真的能消除不可接受的概念吗？", "tldr": "本研究发现，尽管概念替换技术（CRTs）在文本到图像（T2I）模型中有效，但在图像到图像（I2I）模型中并不能真正消除不可接受的概念。作者提出了AntiMirror作为一种有效且忠实的概念替换技术。", "motivation": "生成模型（特别是扩散模型）在避免生成不可接受内容（如冒犯性或版权内容、名人肖像）方面面临对齐挑战。概念替换技术（CRTs）旨在解决此问题，但其有效性，尤其是在图像到图像（I2I）场景中的有效性，受到质疑。", "method": "作者首先使用图像到图像（I2I）模型实证证明了现有概念替换技术（CRTs）未能消除不可接受的概念。接着，他们提出了一个好的CRT应在替换概念的同时保留输入中其他概念的“忠实度”原则。最后，他们提出并展示了一种名为AntiMirror的定向图像编辑技术，以实现有效性和忠实度。", "result": "研究表明，当前最先进的概念替换技术（CRTs）实际上并未消除不可接受的概念，这导致它们在图像到图像（I2I）场景中可能无效，尽管它们在文本到图像（T2I）管道中已被证明有效。此外，他们提出了一种名为AntiMirror的定向图像编辑技术，并证明了其可行性，该技术旨在实现概念替换的有效性和忠实度。", "conclusion": "本研究得出结论，当前的概念替换技术在图像到图像（I2I）场景中未能有效消除不可接受的概念，这与它们在文本到图像（T2I）设置中的表现存在差异。为了解决这一问题，研究强调了理解T2I和I2I之间差异的必要性，并提出了一种新的、有效的、且能保持忠实度的定向图像编辑技术AntiMirror，证明了其可行性。", "translation": "生成模型，特别是基于扩散的文本到图像（T2I）模型，已展示出惊人的成功。然而，将它们对齐以避免生成带有不可接受概念（例如，冒犯性或受版权保护的内容，或名人肖像）的内容仍然是一个重大挑战。概念替换技术（CRTs）旨在解决这一挑战，通常通过尝试从模型中“擦除”不可接受的概念。最近，模型提供商已开始提供图像编辑服务，该服务接受图像和文本提示作为输入，以生成按提示指定修改的图像。这些被称为图像到图像（I2I）模型。在本文中，我们首先使用一个I2I模型实证证明了当今最先进的CRTs实际上并未消除不可接受的概念。因此，尽管现有CRTs已被证明能够消除T2I管道中不需要的概念，但它们在新兴的I2I场景中可能无效，这突显了理解T2I和I2I设置之间这种差异的必要性。接下来，我们认为一个好的CRT在替换不可接受概念的同时，应保留生成模型输入中指定的其他概念。我们称之为忠实度。以前关于CRTs的工作在不可接受概念的情况下忽略了忠实度。最后，我们提出使用定向图像编辑技术来实现有效性和忠实度。我们提出了一种这样的技术，AntiMirror，并展示了其可行性。", "summary": "本研究探讨了概念替换技术（CRTs）在消除生成模型中不可接受概念的有效性，特别是在图像到图像（I2I）场景下。研究发现，尽管CRTs在文本到图像（T2I）模型中表现良好，但它们在I2I设置中未能真正“擦除”这些概念。论文强调了理解T2I和I2I之间差异的重要性，并提出一个好的CRT应具备“忠实度”（即在替换概念的同时保留其他输入概念）。为此，作者提出并验证了一种名为AntiMirror的定向图像编辑技术，旨在实现有效性和忠实度。", "keywords": "概念替换技术, 图像到图像, 文本到图像, 生成模型, AntiMirror", "comments": "本文的创新之处在于首次实证揭示了当前概念替换技术在图像到图像（I2I）场景下的局限性，即它们未能真正消除不可接受的概念，这与它们在文本到图像（T2I）场景下的表现形成鲜明对比。这对于理解生成模型对齐的复杂性至关重要。此外，论文提出了“忠实度”这一重要概念，并引入了AntiMirror这一新的定向图像编辑技术，为解决I2I场景中的概念替换问题提供了有前景的方向。这项工作对于开发更安全、更可控的生成模型具有重要意义。"}}
{"id": "2506.08868", "title": "MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation", "authors": ["Marco Ruggia"], "summary": "MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone\nthat is fully actuated, meaning it can control its orientation independently of\nits position. MOMAV is also highly symmetrical, making its flight efficiency\nlargely unaffected by its current orientation. These characteristics are\nachieved by a novel drone design where six rotor arms align with the vertices\nof an octahedron, and where each arm can actively rotate along its long axis.\nVarious standout features of MOMAV are presented: The high flight efficiency\ncompared to arm configuration of other fully-actuated drones, the design of an\noriginal rotating arm assembly featuring slip-rings used to enable continuous\narm rotation, and a novel control allocation algorithm based on sequential\nquadratic programming (SQP) used to calculate throttle and arm-angle setpoints\nin flight. Flight tests have shown that MOMAV is able to achieve remarkably low\nmean position/orientation errors of 6.6mm, 2.1{\\deg} ({\\sigma}: 3.0mm,\n1.0{\\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\\deg} ({\\sigma}:\n8.6mm, 2.0{\\deg}) when sweeping orientation setpoints.", "comment": "12 pages, 12 figures, preprint", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08868v1", "AI": {"title_translation": "MOMAV：一种采用优化控制分配的高度对称全驱动多旋翼无人机", "tldr": "MOMAV是一种新型全驱动、高度对称的多旋翼无人机，通过独特的八面体臂设计和SQP控制算法实现高效飞行和精确控制，飞行测试显示其定位和姿态误差极低。", "motivation": "本研究旨在开发一种新型全驱动多旋翼无人机，以克服现有全驱动无人机在飞行效率和姿态独立控制方面的局限性，实现高度对称和高效率的飞行。", "method": "MOMAV采用新颖的无人机设计，六个旋翼臂与八面体的顶点对齐，且每个臂可沿长轴主动旋转。设计了带滑环的原创旋转臂组件以实现连续臂旋转。采用基于序列二次规划（SQP）的新型控制分配算法来计算飞行中的油门和臂角设定点。", "result": "飞行测试显示，MOMAV在扫描位置设定点时，平均位置/姿态误差分别为6.6毫米和2.1度（标准差：3.0毫米，1.0度）；在扫描姿态设定点时，误差分别为11.8毫米和3.3度（标准差：8.6毫米，2.0度），表现出极低的误差。", "conclusion": "MOMAV通过其创新的设计和优化的控制分配算法，实现了高精度、高效率的飞行控制，能够独立且精确地控制位置和姿态，验证了其设计的有效性和卓越的控制性能。", "translation": "MOMAV（Marco的全向微型飞行器）是一种全驱动多旋翼无人机，这意味着它可以独立于其位置控制其姿态。MOMAV也高度对称，这使得其飞行效率在很大程度上不受其当前姿态的影响。这些特性是通过一种新颖的无人机设计实现的，其中六个旋翼臂与八面体的顶点对齐，并且每个臂可以沿其长轴主动旋转。MOMAV的各种突出特点被提出：与其他全驱动无人机臂配置相比的高飞行效率，采用滑环实现连续臂旋转的原创旋转臂组件设计，以及一种基于序列二次规划（SQP）的新型控制分配算法，用于在飞行中计算油门和臂角设定点。飞行测试表明，MOMAV在扫描位置设定点时能够实现极低的平均位置/姿态误差，分别为6.6毫米、2.1度（标准差：3.0毫米、1.0度），在扫描姿态设定点时则为11.8毫米、3.3度（标准差：8.6毫米、2.0度）。", "summary": "本论文介绍了一种名为MOMAV的新型全驱动多旋翼无人机，其特点是高度对称和独立的位置/姿态控制能力。通过创新的八面体臂配置、可旋转臂设计（带滑环）以及基于序列二次规划（SQP）的优化控制分配算法，MOMAV实现了高飞行效率和精确控制。飞行测试结果表明，MOMAV在位置和姿态设定点扫描中均表现出极低的位置/姿态误差，验证了其设计的有效性和卓越的控制性能。", "keywords": "全驱动无人机, 多旋翼, 优化控制, 对称性, SQP", "comments": "MOMAV的创新点在于其独特的八面体旋翼臂配置和可旋转臂设计，结合优化的SQP控制分配算法，解决了传统全驱动无人机在效率和独立控制方面的挑战。其高度对称性保证了飞行效率不受姿态影响，而精确的飞行测试结果则证明了其在复杂任务中的潜力，具有重要的工程和应用价值。"}}
{"id": "2506.08361", "title": "Image Demoiréing Using Dual Camera Fusion on Mobile Phones", "authors": ["Yanting Mei", "Zhilu Zhang", "Xiaohe Wu", "Wangmeng Zuo"], "summary": "When shooting electronic screens, moir\\'e patterns usually appear in captured\nimages, which seriously affects the image quality. Existing image demoir\\'eing\nmethods face great challenges in removing large and heavy moir\\'e. To address\nthe issue, we propose to utilize Dual Camera fusion for Image Demoir\\'eing\n(DCID), \\ie, using the ultra-wide-angle (UW) image to assist the moir\\'e\nremoval of wide-angle (W) image. This is inspired by two motivations: (1) the\ntwo lenses are commonly equipped with modern smartphones, (2) the UW image\ngenerally can provide normal colors and textures when moir\\'e exists in the W\nimage mainly due to their different focal lengths. In particular, we propose an\nefficient DCID method, where a lightweight UW image encoder is integrated into\nan existing demoir\\'eing network and a fast two-stage image alignment manner is\npresent. Moreover, we construct a large-scale real-world dataset with diverse\nmobile phones and monitors, containing about 9,000 samples. Experiments on the\ndataset show our method performs better than state-of-the-art methods. Code and\ndataset are available at https://github.com/Mrduckk/DCID.", "comment": "ICME 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08361v1", "AI": {"title_translation": "使用双摄像头融合在手机上进行图像去摩尔纹", "tldr": "当拍摄电子屏幕时，图像中常出现摩尔纹。本文提出了一种双摄像头融合（DCID）方法，利用超广角图像辅助去除广角图像中的摩尔纹，并展示了优于现有方法的性能。", "motivation": "1. 拍摄电子屏幕时，图像中出现的摩尔纹严重影响图像质量，现有方法难以去除大而重的摩尔纹。2. 现代智能手机普遍配备广角和超广角双镜头。3. 超广角图像由于焦距不同，在广角图像出现摩尔纹时通常能提供正常的颜色和纹理。", "method": "1. 提出利用双摄像头融合进行图像去摩尔纹（DCID），即使用超广角（UW）图像辅助广角（W）图像的摩尔纹去除。2. 将一个轻量级UW图像编码器集成到现有去摩尔纹网络中。3. 提出一种快速的两阶段图像对齐方式。4. 构建了一个包含约9,000个样本的大规模真实世界数据集，涵盖多种手机和显示器。", "result": "在所构建的数据集上的实验表明，所提出的方法优于最先进的方法。", "conclusion": "该方法利用双摄像头融合有效去除摩尔纹，性能优于最先进的方法。", "translation": "当拍摄电子屏幕时，捕获的图像中通常会出现摩尔纹，这严重影响了图像质量。现有的图像去摩尔纹方法在去除大而重的摩尔纹方面面临巨大挑战。为了解决这个问题，我们提出利用双摄像头融合进行图像去摩尔纹（DCID），即使用超广角（UW）图像辅助广角（W）图像的摩尔纹去除。这受到两个动机的启发：（1）现代智能手机普遍配备了这两个镜头，（2）当广角图像中存在摩尔纹时，超广角图像通常可以提供正常的颜色和纹理，这主要是由于它们焦距不同。特别是，我们提出了一种高效的DCID方法，其中将一个轻量级UW图像编码器集成到现有的去摩尔纹网络中，并提出了一种快速的两阶段图像对齐方式。此外，我们构建了一个包含约9,000个样本的大规模真实世界数据集，其中包含各种手机和显示器。在该数据集上的实验表明，我们的方法优于最先进的方法。代码和数据集可在https://github.com/Mrduckk/DCID获取。", "summary": "本文旨在解决电子屏幕图像中摩尔纹严重影响图像质量的问题，特别是现有方法难以处理的重度摩尔纹。作者提出了一种名为双摄像头融合图像去摩尔纹（DCID）的新方法，利用智能手机上常见的广角和超广角双镜头。该方法利用超广角图像（通常因焦距不同而无摩尔纹）辅助去除广角图像中的摩尔纹。DCID方法将一个轻量级超广角图像编码器集成到现有网络中，并采用快速两阶段图像对齐。此外，研究人员构建了一个大型真实世界数据集。实验结果表明，DCID方法优于现有最先进的方法。", "keywords": "图像去摩尔纹, 双摄像头融合, 摩尔纹, 手机摄影, 图像质量", "comments": "该论文通过利用现代智能手机的双摄像头设置，提出了一种创新的图像去摩尔纹方法，这是一种实用且利用硬件优势的解决方案。利用超广角图像辅助去摩尔纹的动机是合理的，因为它利用了不同的光学特性。构建大规模真实世界数据集是一项重要贡献，解决了图像处理研究中的常见挑战，并为未来的工作提供了宝贵资源。所提出方法的效率和优于最先进方法的性能，凸显了其在移动摄影中实际应用的潜力。"}}
{"id": "2506.08630", "title": "Modular Recurrence in Contextual MDPs for Universal Morphology Control", "authors": ["Laurens Engwegen", "Daan Brinks", "Wendelin Böhmer"], "summary": "A universal controller for any robot morphology would greatly improve\ncomputational and data efficiency. By utilizing contextual information about\nthe properties of individual robots and exploiting their modular structure in\nthe architecture of deep reinforcement learning agents, steps have been made\ntowards multi-robot control. Generalization to new, unseen robots, however,\nremains a challenge. In this paper we hypothesize that the relevant contextual\ninformation is partially observable, but that it can be inferred through\ninteractions for better generalization to contexts that are not seen during\ntraining. To this extent, we implement a modular recurrent architecture and\nevaluate its generalization performance on a large set of MuJoCo robots. The\nresults show a substantial improved performance on robots with unseen dynamics,\nkinematics, and topologies, in four different environments.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08630v1", "AI": {"title_translation": "上下文MDP中模块化递归用于通用形态控制", "tldr": "本文提出了一种模块化递归架构，用于推断部分可观测的上下文信息，以更好地将通用机器人控制器泛化到未见过的机器人形态，并在新机器人上显示出改进的性能。", "motivation": "为了提高计算和数据效率，实现一个适用于任何机器人形态的通用控制器，并解决泛化到新的、未见过的机器人的挑战。", "method": "假设相关的上下文信息是部分可观测的，但可以通过交互推断出来。为此，实现了一个模块化递归架构。", "result": "在四种不同环境下，在具有未见过的动力学、运动学和拓扑结构的机器人上，性能得到了显著提升。", "conclusion": "模块化递归架构能够有效推断部分可观测的上下文信息，从而使通用机器人控制器更好地泛化到未见过的形态。", "translation": "通用机器人形态控制器将极大地提高计算和数据效率。通过利用关于单个机器人属性的上下文信息，并利用其在深度强化学习代理架构中的模块化结构，多机器人控制已取得进展。然而，泛化到新的、未见过的机器人仍然是一个挑战。在本文中，我们假设相关的上下文信息是部分可观测的，但可以通过交互推断出来，从而更好地泛化到训练期间未见过的上下文。为此，我们实现了一个模块化递归架构，并在大量MuJoCo机器人上评估其泛化性能。结果显示，在四种不同环境下，该架构在具有未见过的动力学、运动学和拓扑结构的机器人上表现出显著的性能提升。", "summary": "本文旨在解决通用机器人控制器泛化到未见形态的挑战。它提出了一种模块化递归架构，通过交互推断部分可观测的上下文信息。在MuJoCo机器人上进行的评估表明，该方法在多种环境中对各种未见过的机器人动力学、运动学和拓扑结构展现出显著的泛化性能提升，从而推动了多机器人控制效率的进步。", "keywords": "通用形态控制, 上下文MDPs, 模块化递归架构, 泛化, 强化学习", "comments": "本文的创新之处在于其关于推断部分可观测上下文信息以实现通用机器人控制的假设。模块化递归架构有效地解决了向未见机器人形态泛化的挑战，这是实现真正通用控制器的关键一步。其对多样化机器人属性（动力学、运动学、拓扑结构）的适用性突显了其在多机器人系统和强化学习领域的潜在影响。"}}
{"id": "2506.08176", "title": "FedGA-Tree: Federated Decision Tree using Genetic Algorithm", "authors": ["Anh V Nguyen", "Diego Klabjan"], "summary": "In recent years, with rising concerns for data privacy, Federated Learning\nhas gained prominence, as it enables collaborative training without the\naggregation of raw data from participating clients. However, much of the\ncurrent focus has been on parametric gradient-based models, while nonparametric\ncounterparts such as decision tree are relatively understudied. Existing\nmethods for adapting decision trees to Federated Learning generally combine a\ngreedy tree-building algorithm with differential privacy to produce a global\nmodel for all clients. These methods are limited to classification trees and\ncategorical data due to the constraints of differential privacy. In this paper,\nwe explore an alternative approach that utilizes Genetic Algorithm to\nfacilitate the construction of personalized decision trees and accommodate\ncategorical and numerical data, thus allowing for both classification and\nregression trees. Comprehensive experiments demonstrate that our method\nsurpasses decision trees trained solely on local data and a benchmark\nalgorithm.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08176v1", "AI": {"title_translation": "FedGA-Tree：使用遗传算法的联邦决策树", "tldr": "FedGA-Tree提出了一种利用遗传算法构建联邦决策树的新方法，克服了现有方法的局限性，并实现了个性化决策树的构建，在分类和回归任务上表现优异。", "motivation": "鉴于数据隐私问题的日益突出，联邦学习受到了广泛关注。然而，当前联邦学习的研究主要集中在参数化模型上，对决策树等非参数化模型的关注相对较少。现有的联邦决策树方法通常结合贪婪树构建算法和差分隐私，但受限于分类树和分类数据。本文旨在探索一种替代方法来解决这些限制。", "method": "本文提出了一种替代方法，利用遗传算法来促进个性化决策树的构建，并适应分类和数值数据，从而支持分类树和回归树。", "result": "全面的实验表明，该方法优于仅在本地数据上训练的决策树和基准算法。", "conclusion": "FedGA-Tree成功地将遗传算法应用于联邦学习中的决策树构建，克服了传统联邦决策树方法的局限性，实现了个性化决策树，并支持分类和回归任务，且性能优越。", "translation": "近年来，随着数据隐私问题的日益突出，联邦学习受到了广泛关注，因为它能够在不聚合参与客户端原始数据的情况下实现协同训练。然而，当前的大部分研究都集中在参数化梯度模型上，而决策树等非参数化模型相对研究不足。现有将决策树应用于联邦学习的方法通常将贪婪树构建算法与差分隐私相结合，为所有客户端生成一个全局模型。这些方法由于差分隐私的限制，仅限于分类树和分类数据。在本文中，我们探索了一种替代方法，利用遗传算法来促进个性化决策树的构建，并适应分类和数值数据，从而允许分类树和回归树。全面的实验表明，我们的方法优于仅在本地数据上训练的决策树和基准算法。", "summary": "本文针对联邦学习中决策树研究不足且现有方法存在局限性的问题，提出了一种名为FedGA-Tree的新方法。该方法利用遗传算法构建个性化决策树，支持分类和回归任务，并能处理分类和数值数据，克服了传统方法对分类数据和分类树的限制。实验结果表明，FedGA-Tree的性能优于本地训练的决策树和现有基准算法。", "keywords": "联邦学习, 决策树, 遗传算法, 数据隐私, 个性化决策树", "comments": "该论文的创新点在于将遗传算法引入联邦学习中的决策树构建，解决了现有基于差分隐私的联邦决策树方法在数据类型和任务类型上的局限性，实现了个性化决策树的构建。这对于保护数据隐私同时利用决策树进行复杂数据分析具有重要意义。"}}
{"id": "2506.08689", "title": "Efficient Uncertainty Propagation with Guarantees in Wasserstein Distance", "authors": ["Eduardo Figueiredo", "Steven Adams", "Peyman Mohajerin Esfahani", "Luca Laurenti"], "summary": "In this paper, we consider the problem of propagating an uncertain\ndistribution by a possibly non-linear function and quantifying the resulting\nuncertainty. We measure the uncertainty using the Wasserstein distance, and for\na given input set of distributions close in the Wasserstein distance, we\ncompute a set of distributions centered at a discrete distribution that is\nguaranteed to contain the pushforward of any distribution in the input set. Our\napproach is based on approximating a nominal distribution from the input set to\na discrete support distribution for which the exact computation of the\npushforward distribution is tractable, thus guaranteeing computational\nefficiency to our approach. Then, we rely on results from semi-discrete optimal\ntransport and distributional robust optimization to show that for any $\\epsilon\n> 0$ the error introduced by our approach can be made smaller than $\\epsilon$.\nCritically, in the context of dynamical systems, we show how our results allow\none to efficiently approximate the distribution of a stochastic dynamical\nsystem with a discrete support distribution for a possibly infinite horizon\nwhile bounding the resulting approximation error. We empirically investigate\nthe effectiveness of our framework on various benchmarks, including a 10-D\nnon-linear system, showing the effectiveness of our approach in quantifying\nuncertainty in linear and non-linear stochastic systems.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08689v1", "AI": {"title_translation": "基于Wasserstein距离的有效不确定性传播与保证", "tldr": "本文提出了一种有效且具有理论保证的方法，用于通过非线性函数传播不确定性分布，并使用Wasserstein距离量化结果不确定性，特别适用于随机动力系统。", "motivation": "本文旨在解决通过可能非线性函数传播不确定分布并量化其结果不确定性的问题。", "method": "该方法基于将输入分布集中的标称分布近似为离散支持分布，从而可以精确计算推前分布。它利用半离散最优传输和分布鲁棒优化理论来保证误差界限，并确保计算效率。", "result": "该方法保证计算出的离散分布集包含输入集中任何分布的推前，并且引入的误差可以小于任意给定的$\\epsilon > 0$。在动力系统背景下，该方法能有效近似随机动力系统分布，并能限制近似误差。经验结果表明其在量化线性和非线性随机系统不确定性方面的有效性。", "conclusion": "本文提出的框架能够有效地传播和量化通过非线性函数的不确定性，并提供理论保证，特别适用于随机动力系统的不确定性量化。", "translation": "在本文中，我们考虑了通过可能非线性函数传播不确定性分布并量化所得不确定性的问题。我们使用Wasserstein距离来衡量不确定性，并且对于给定的一组在Wasserstein距离上接近的输入分布，我们计算了一组以离散分布为中心的分布，这些分布保证包含输入集中任何分布的推前。我们的方法基于将输入集中的标称分布近似为离散支持分布，对于该分布，推前分布的精确计算是可处理的，从而保证了我们方法的计算效率。然后，我们依赖于半离散最优传输和分布鲁棒优化的结果来表明，对于任何$\\epsilon > 0$，我们方法引入的误差可以小于$\\epsilon$。至关重要的是，在动力系统背景下，我们展示了我们的结果如何允许以离散支持分布有效近似随机动力系统的分布，即使是无限时间范围，同时限制了由此产生的近似误差。我们通过各种基准（包括一个10维非线性系统）经验性地研究了我们框架的有效性，展示了我们方法在线性和非线性随机系统中量化不确定性的有效性。", "summary": "本文提出了一种在Wasserstein距离下有效传播不确定性分布并量化其不确定性的方法。该方法通过将输入分布近似为离散支持分布，并结合半离散最优传输和分布鲁棒优化，确保了计算效率和误差保证。它能够将推前分布包含在一个以离散分布为中心的集合中，并且误差可以任意小。该方法在随机动力系统的不确定性量化方面表现出有效性，包括高维非线性系统。", "keywords": "不确定性传播, Wasserstein距离, 离散支持分布, 随机动力系统, 最优传输", "comments": "本文的创新点在于提供了一种在Wasserstein距离下传播不确定性的高效且具有理论保证的方法。通过将连续分布近似为离散支持分布，解决了非线性函数下推前分布计算的难题，并利用最优传输理论提供了严格的误差界限。这对于需要高置信度不确定性量化的实际应用，如动力系统控制和分析，具有重要意义。"}}
{"id": "2506.08391", "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding", "authors": ["Woohyeon Park", "Woojin Kim", "Jaeik Kim", "Jaeyoung Do"], "summary": "Despite significant advancements in Vision-Language Models (VLMs), the\nperformance of existing VLMs remains hindered by object hallucination, a\ncritical challenge to achieving accurate visual understanding. To address this\nissue, we propose SECOND: Selective and Contrastive Decoding, a novel approach\nthat enables VLMs to effectively leverage multi-scale visual information with\nan object-centric manner, closely aligning with human visual perception. SECOND\nprogressively selects and integrates multi-scale visual information,\nfacilitating a more precise interpretation of images. By contrasting these\nvisual information iteratively, SECOND significantly reduces perceptual\nhallucinations and outperforms a wide range of benchmarks. Our theoretical\nanalysis and experiments highlight the largely unexplored potential of\nmulti-scale application in VLMs, showing that prioritizing and contrasting\nacross scales outperforms existing methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08391v1", "AI": {"title_translation": "第二：通过选择性和对比解码减轻视觉-语言模型中的感知幻觉", "tldr": "提出SECOND方法，通过选择性和对比解码利用多尺度视觉信息，有效减少视觉-语言模型中的感知幻觉。", "motivation": "现有视觉-语言模型（VLMs）的性能受限于物体幻觉，这是实现准确视觉理解的关键挑战。", "method": "本文提出了一种名为SECOND（选择性和对比解码）的新方法，旨在解决视觉-语言模型（VLMs）中普遍存在的物体幻觉问题。SECOND通过以物体为中心的方式，逐步选择和整合多尺度视觉信息，并通过迭代对比这些信息来减少感知幻觉。", "result": "SECOND显著减少了感知幻觉，并在广泛的基准测试中表现优异。理论分析和实验表明，在VLMs中优先和对比不同尺度的信息优于现有方法。", "conclusion": "多尺度信息在视觉-语言模型中具有巨大的未开发潜力，通过优先处理和对比不同尺度的信息可以有效减轻感知幻觉并提升模型性能。", "translation": "尽管视觉-语言模型（VLMs）取得了显著进展，但现有VLMs的性能仍然受到物体幻觉的阻碍，这是实现准确视觉理解的关键挑战。为了解决这个问题，我们提出了SECOND：选择性和对比解码，这是一种新颖的方法，使VLM能够以物体为中心有效利用多尺度视觉信息，这与人类视觉感知密切相关。SECOND逐步选择和整合多尺度视觉信息，促进了对图像更精确的解释。通过迭代对比这些视觉信息，SECOND显著减少了感知幻觉，并在广泛的基准测试中表现优异。我们的理论分析和实验突出了多尺度应用在VLMs中 largely 未开发的潜力，表明优先处理和对比不同尺度的信息优于现有方法。", "summary": "本文提出了一种名为SECOND（选择性和对比解码）的新方法，旨在解决视觉-语言模型（VLMs）中普遍存在的物体幻觉问题。SECOND通过以物体为中心的方式，有效利用并迭代对比多尺度视觉信息，从而更精确地解释图像并显著减少感知幻觉。实验结果表明，SECOND在多个基准测试中表现出色，并揭示了多尺度信息在VLMs中的巨大潜力。", "keywords": "视觉-语言模型, 感知幻觉, 多尺度信息, 选择性解码, 对比解码", "comments": "这篇论文提出了一种新颖的解码方法，通过模拟人类视觉处理多尺度信息的方式来解决视觉-语言模型中的感知幻觉问题。其创新点在于强调了多尺度信息在VLM中的重要性，并提出了选择性与对比解码机制，这对于提高VLM的视觉理解准确性具有重要意义。"}}
{"id": "2506.08745", "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "authors": ["Kongcheng Zhang", "Qi Yao", "Shunyu Liu", "Yingjie Wang", "Baisheng Lai", "Jieping Ye", "Mingli Song", "Dacheng Tao"], "summary": "Recent advances of Reinforcement Learning (RL) have highlighted its potential\nin complex reasoning tasks, yet effective training often relies on external\nsupervision, which limits the broader applicability. In this work, we propose a\nnovel self-rewarding reinforcement learning framework to enhance Large Language\nModel (LLM) reasoning by leveraging the consistency of intermediate reasoning\nstates across different reasoning trajectories. Our key insight is that correct\nresponses often exhibit consistent trajectory patterns in terms of model\nlikelihood: their intermediate reasoning states tend to converge toward their\nown final answers (high consistency) with minimal deviation toward other\ncandidates (low volatility). Inspired by this observation, we introduce CoVo,\nan intrinsic reward mechanism that integrates Consistency and Volatility via a\nrobust vector-space aggregation strategy, complemented by a curiosity bonus to\npromote diverse exploration. CoVo enables LLMs to perform RL in a\nself-rewarding manner, offering a scalable pathway for learning to reason\nwithout external supervision. Extensive experiments on diverse reasoning\nbenchmarks show that CoVo achieves performance comparable to or even surpassing\nsupervised RL. Our code is available at https://github.com/sastpg/CoVo.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08745v1", "AI": {"title_translation": "一致路径通向真理：用于LLM推理的自奖励强化学习", "tldr": "本文提出了一种名为CoVo的自奖励强化学习框架，通过利用中间推理状态的一致性来增强大型语言模型（LLM）的推理能力，实现了无需外部监督的强大性能。", "motivation": "当前强化学习在复杂推理任务中的应用受限于对外部监督的依赖，这限制了其广泛适用性。", "method": "本文提出了一种新颖的自奖励强化学习框架，通过利用不同推理轨迹中中间推理状态的一致性来增强大型语言模型（LLM）的推理能力。其核心思想是，正确响应在模型似然方面表现出一致的轨迹模式：它们的中间推理状态倾向于收敛到自己的最终答案（高一致性），同时对其他候选答案的偏离最小（低波动性）。受此启发，引入了CoVo，这是一种内在奖励机制，通过鲁棒的向量空间聚合策略整合了一致性和波动性，并辅以好奇心奖励以促进多样化探索。", "result": "在各种推理基准上的广泛实验表明，CoVo实现了与监督式强化学习相当甚至超越的性能。", "conclusion": "CoVo为LLM提供了一种可扩展的、无需外部监督的学习推理途径。", "translation": "强化学习（RL）的最新进展凸显了其在复杂推理任务中的潜力，然而，有效的训练往往依赖于外部监督，这限制了其更广泛的适用性。在这项工作中，我们提出了一种新颖的自奖励强化学习框架，通过利用不同推理轨迹中中间推理状态的一致性来增强大型语言模型（LLM）的推理能力。我们的关键见解是，正确响应在模型似然方面表现出一致的轨迹模式：它们的中间推理状态倾向于收敛到自己的最终答案（高一致性），同时对其他候选答案的偏离最小（低波动性）。受此观察启发，我们引入了CoVo，这是一种内在奖励机制，通过鲁棒的向量空间聚合策略整合了一致性和波动性，并辅以好奇心奖励以促进多样化探索。CoVo使LLM能够以自奖励的方式执行RL，为无需外部监督的学习推理提供了一条可扩展的途径。在各种推理基准上的广泛实验表明，CoVo实现了与监督式RL相当甚至超越的性能。我们的代码可在https://github.com/sastpg/CoVo获取。", "summary": "本文提出了一种名为CoVo的新型自奖励强化学习框架，旨在通过利用中间推理状态的一致性和波动性来增强大型语言模型（LLM）的推理能力。该方法通过引入一种结合一致性和波动性的内在奖励机制，并辅以好奇心奖励，使得LLM能够以自奖励的方式进行强化学习，从而摆脱对外部监督的依赖。实验证明，CoVo在各种推理基准上取得了与监督式强化学习相当甚至更优的性能，为无需外部监督的LLM推理学习提供了一条可扩展的途径。", "keywords": "自奖励强化学习, LLM推理, 一致性, 波动性, 内在奖励", "comments": "CoVo的创新之处在于其自奖励机制，通过利用模型自身中间推理状态的一致性和波动性作为内在奖励，有效解决了传统强化学习对外部监督的依赖问题。这对于推动LLM在复杂推理任务中实现更自主、更可扩展的学习具有重要意义。"}}
{"id": "2506.08201", "title": "Correlated Noise Mechanisms for Differentially Private Learning", "authors": ["Krishna Pillutla", "Jalaj Upadhyay", "Christopher A. Choquette-Choo", "Krishnamurthy Dvijotham", "Arun Ganesh", "Monika Henzinger", "Jonathan Katz", "Ryan McKenna", "H. Brendan McMahan", "Keith Rush", "Thomas Steinke", "Abhradeep Thakurta"], "summary": "This monograph explores the design and analysis of correlated noise\nmechanisms for differential privacy (DP), focusing on their application to\nprivate training of AI and machine learning models via the core primitive of\nestimation of weighted prefix sums. While typical DP mechanisms inject\nindependent noise into each step of a stochastic gradient (SGD) learning\nalgorithm in order to protect the privacy of the training data, a growing body\nof recent research demonstrates that introducing (anti-)correlations in the\nnoise can significantly improve privacy-utility trade-offs by carefully\ncanceling out some of the noise added on earlier steps in subsequent steps.\nSuch correlated noise mechanisms, known variously as matrix mechanisms,\nfactorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when\napplied to learning algorithms, have also been influential in practice, with\nindustrial deployment at a global scale.", "comment": "212 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08201v1", "AI": {"title_translation": "差分隐私学习中的相关噪声机制", "tldr": "本专著探讨了用于差分隐私（DP）的相关噪声机制的设计与分析，特别关注其在AI和机器学习模型私有训练中的应用，通过引入（反）相关噪声显著改善隐私-效用权衡。", "motivation": "典型的差分隐私（DP）机制在随机梯度下降（SGD）学习算法的每一步中注入独立噪声以保护训练数据隐私，但近期研究表明，在噪声中引入（反）相关性可以通过仔细抵消后续步骤中早期步骤添加的部分噪声，显著改善隐私-效用权衡。", "method": "本专著探索了用于差分隐私（DP）的相关噪声机制的设计和分析，重点关注其通过加权前缀和估计这一核心原语在AI和机器学习模型私有训练中的应用。文中提及的机制包括矩阵机制、分解机制和DP-Follow-the-Regularized-Leader（DP-FTRL）。", "result": "引入（反）相关噪声可以显著改善隐私-效用权衡，其通过仔细抵消后续步骤中早期步骤添加的部分噪声实现。这些相关噪声机制，如矩阵机制、分解机制和DP-FTRL，在实践中具有影响力，并已在全球范围内进行工业部署。", "conclusion": "Not mentioned in abstract", "translation": "本专著探讨了差分隐私（DP）中相关噪声机制的设计与分析，重点关注其通过加权前缀和估计这一核心原语在AI和机器学习模型私有训练中的应用。虽然典型的DP机制在随机梯度（SGD）学习算法的每一步中注入独立噪声以保护训练数据的隐私，但近期越来越多的研究表明，在噪声中引入（反）相关性可以通过仔细抵消后续步骤中早期步骤添加的部分噪声，显著改善隐私-效用权衡。当应用于学习算法时，这类被称为矩阵机制、分解机制和DP-Follow-the-Regularized-Leader（DP-FTRL）的相关噪声机制在实践中也产生了重要影响，并已在全球范围内部署。", "summary": "本专著深入探讨了差分隐私（DP）中相关噪声机制的设计与分析，特别强调其在AI和机器学习模型私有训练中的应用。它指出，与传统注入独立噪声的DP机制不同，引入（反）相关噪声能有效抵消噪声，从而显著优化隐私-效用权衡。文中提及的矩阵机制、分解机制和DP-FTRL等相关噪声机制已被工业界广泛采纳并全球部署，证明了其在实际应用中的重要性。", "keywords": "差分隐私, 相关噪声, 机器学习, 隐私-效用权衡, SGD", "comments": "这篇专著探讨了差分隐私领域的一个重要且实用的方向。通过引入相关噪声而非独立噪声，该方法能够有效提高隐私保护下的模型效用，解决了传统DP机制在隐私-效用权衡上的局限性。其在工业界的全球部署也证明了其实用价值和影响力。"}}
{"id": "2506.08719", "title": "Efficient Learning of Vehicle Controller Parameters via Multi-Fidelity Bayesian Optimization: From Simulation to Experiment", "authors": ["Yongpeng Zhao", "Maik Pfefferkorn", "Maximilian Templer", "Rolf Findeisen"], "summary": "Parameter tuning for vehicle controllers remains a costly and time-intensive\nchallenge in automotive development. Traditional approaches rely on extensive\nreal-world testing, making the process inefficient. We propose a multi-fidelity\nBayesian optimization approach that efficiently learns optimal controller\nparameters by leveraging both low-fidelity simulation data and a very limited\nnumber of real-world experiments. Our approach significantly reduces the need\nfor manual tuning and expensive field testing while maintaining the standard\ntwo-stage development workflow used in industry. The core contribution is the\nintegration of an auto-regressive multi-fidelity Gaussian process model into\nBayesian optimization, enabling knowledge transfer between different fidelity\nlevels without requiring additional low-fidelity evaluations during real-world\ntesting. We validate our approach through both simulation studies and realworld\nexperiments. The results demonstrate that our method achieves high-quality\ncontroller performance with only very few real-world experiments, highlighting\nits potential as a practical and scalable solution for intelligent vehicle\ncontrol tuning in industrial applications.", "comment": "8 pages, 8 figures, accepted for IEEE IV 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08719v1", "AI": {"title_translation": "车辆控制器参数的多保真贝叶斯优化高效学习：从仿真到实验", "tldr": "提出一种多保真贝叶斯优化方法，结合仿真和少量实际实验，高效地学习车辆控制器参数，显著减少调优成本和时间。", "motivation": "车辆控制器参数调优在汽车开发中耗时且成本高昂，传统方法依赖大量实际测试，效率低下。", "method": "提出一种多保真贝叶斯优化方法，利用低保真仿真数据和少量实际实验数据来学习最优控制器参数。核心贡献是将自回归多保真高斯过程模型集成到贝叶斯优化中，实现不同保真度级别之间的知识迁移，无需在实际测试中进行额外的低保真评估。", "result": "该方法仅需极少量实际实验即可实现高质量的控制器性能。", "conclusion": "该方法为工业应用中智能车辆控制调优提供了一个实用且可扩展的解决方案。", "translation": "车辆控制器参数调优在汽车开发中仍然是一个成本高昂且耗时的挑战。传统方法依赖于大量的实际测试，导致过程效率低下。我们提出了一种多保真贝叶斯优化方法，该方法通过利用低保真仿真数据和数量非常有限的实际实验，高效地学习最优控制器参数。我们的方法显著减少了手动调优和昂贵的现场测试的需求，同时保持了行业中使用的标准两阶段开发工作流程。核心贡献是将自回归多保真高斯过程模型集成到贝叶斯优化中，从而实现不同保真度级别之间的知识迁移，而无需在实际测试期间进行额外的低保真评估。我们通过仿真研究和实际实验验证了我们的方法。结果表明，我们的方法仅需极少量的实际实验即可实现高质量的控制器性能，突出了其作为工业应用中智能车辆控制调优的实用且可扩展解决方案的潜力。", "summary": "本文提出一种多保真贝叶斯优化方法，旨在解决车辆控制器参数调优耗时且成本高昂的问题。该方法结合低保真仿真数据和少量实际实验，通过将自回归多保真高斯过程模型集成到贝叶斯优化中，实现不同保真度层级间的知识高效迁移。实验结果表明，该方法仅需极少量实际实验即可获得高质量的控制器性能，为工业应用中的智能车辆控制调优提供了一种实用且可扩展的解决方案。", "keywords": "多保真贝叶斯优化, 车辆控制器, 参数调优, 仿真, 实际实验", "comments": "这篇论文的创新点在于将自回归多保真高斯过程模型集成到贝叶斯优化中，从而在不同保真度级别之间实现了高效的知识迁移，显著减少了昂贵的实际测试需求。这对于汽车开发等需要大量实验验证的领域具有重要意义，有望大幅提升开发效率并降低成本。"}}
{"id": "2506.08931", "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks", "authors": ["Yixuan Li", "Yutang Lin", "Jieming Cui", "Tengyu Liu", "Wei Liang", "Yixin Zhu", "Siyuan Huang"], "summary": "Humanoid teleoperation plays a vital role in demonstrating and collecting\ndata for complex humanoid-scene interactions. However, current teleoperation\nsystems face critical limitations: they decouple upper- and lower-body control\nto maintain stability, restricting natural coordination, and operate open-loop\nwithout real-time position feedback, leading to accumulated drift. The\nfundamental challenge is achieving precise, coordinated whole-body\nteleoperation over extended durations while maintaining accurate global\npositioning. Here we show that an MoE-based teleoperation system, CLONE, with\nclosed-loop error correction enables unprecedented whole-body teleoperation\nfidelity, maintaining minimal positional drift over long-range trajectories\nusing only head and hand tracking from an MR headset. Unlike previous methods\nthat either sacrifice coordination for stability or suffer from unbounded\ndrift, CLONE learns diverse motion skills while preventing tracking error\naccumulation through real-time feedback, enabling complex coordinated movements\nsuch as ``picking up objects from the ground.'' These results establish a new\nmilestone for whole-body humanoid teleoperation for long-horizon humanoid-scene\ninteraction tasks.", "comment": "18 pages, 13 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.08931v1", "AI": {"title_translation": "CLONE：用于长周期任务的全身人形机器人闭环遥操作", "tldr": "CLONE是一个基于MoE的闭环遥操作系统，它通过实时反馈纠正错误，实现了高精度、低漂移的全身人形机器人遥操作，适用于复杂的长周期任务。", "motivation": "当前的人形机器人遥操作系统存在局限性：为了保持稳定性，它们将上半身和下半身控制分离，限制了自然协调；它们采用开环操作，没有实时位置反馈，导致累积漂移。核心挑战在于在保持精确全局定位的同时，实现长时间内精确、协调的全身遥操作。", "method": "本文提出了一个基于MoE（Mixture-of-Experts）的遥操作系统CLONE，该系统通过闭环误差校正，仅使用MR头显的头部和手部跟踪数据，即可实现全身遥操作。CLONE学习多样化的运动技能，并通过实时反馈防止跟踪误差累积。", "result": "CLONE实现了前所未有的全身遥操作保真度，在长距离轨迹上保持了最小的位置漂移。它能够实现复杂的协调运动，例如“从地面捡起物体”。", "conclusion": "CLONE为长周期人形机器人-场景交互任务的全身人形机器人遥操作树立了一个新的里程碑。", "translation": "人形机器人遥操作在演示和收集复杂人形机器人与场景交互的数据方面发挥着至关重要的作用。然而，当前的遥操作系统面临着严峻的局限性：它们为了保持稳定性而将上半身和下半身控制解耦，限制了自然协调性；它们采用开环操作，没有实时位置反馈，导致累积漂移。根本挑战在于在保持精确全局定位的同时，实现长时间内精确、协调的全身遥操作。本文展示了一个基于MoE的遥操作系统CLONE，该系统通过闭环误差校正，仅使用MR头显的头部和手部跟踪数据，即可实现前所未有的全身遥操作保真度，在长距离轨迹上保持了最小的位置漂移。与以往牺牲协调性以求稳定或遭受无限制漂移的方法不同，CLONE在学习多样化运动技能的同时，通过实时反馈防止跟踪误差累积，从而实现诸如“从地面捡起物体”等复杂协调运动。这些结果为长周期人形机器人-场景交互任务的全身人形机器人遥操作树立了一个新的里程碑。", "summary": "本文介绍了一个名为CLONE的MoE（Mixture-of-Experts）基闭环遥操作系统，旨在解决当前人形机器人遥操作系统中存在的上下身控制解耦和开环操作导致的漂移问题。CLONE通过实时反馈纠正误差，仅利用MR头显的头部和手部跟踪数据，即可实现高精度、低漂移的全身人形机器人遥操作。该系统能够学习多样化的运动技能并执行复杂的协调动作，例如从地面拾取物体，为长周期人形机器人-场景交互任务的全身遥操作设定了新的标准。", "keywords": "人形机器人, 遥操作, 闭环控制, MoE, 误差校正", "comments": "CLONE的创新之处在于其结合了MoE架构和闭环误差校正，克服了传统遥操作系统中稳定性和协调性难以兼顾的难题。通过仅使用MR头显的轻量级跟踪数据，实现了高保真度的全身控制，这对于未来复杂人形机器人任务的数据收集和演示具有重要意义。该方法有效解决了长期任务中的位置漂移问题，提升了遥操作的实用性和可靠性。"}}
{"id": "2506.08747", "title": "A Sample Efficient Conditional Independence Test in the Presence of Discretization", "authors": ["Boyang Sun", "Yu Yao", "Xinshuai Dong", "Zongfang Liu", "Tongliang Liu", "Yumou Qiu", "Kun Zhang"], "summary": "In many real-world scenarios, interested variables are often represented as\ndiscretized values due to measurement limitations. Applying Conditional\nIndependence (CI) tests directly to such discretized data, however, can lead to\nincorrect conclusions. To address this, recent advancements have sought to\ninfer the correct CI relationship between the latent variables through\nbinarizing observed data. However, this process inevitably results in a loss of\ninformation, which degrades the test's performance. Motivated by this, this\npaper introduces a sample-efficient CI test that does not rely on the\nbinarization process. We find that the independence relationships of latent\ncontinuous variables can be established by addressing an over-identifying\nrestriction problem with Generalized Method of Moments (GMM). Based on this\ninsight, we derive an appropriate test statistic and establish its asymptotic\ndistribution correctly reflecting CI by leveraging nodewise regression.\nTheoretical findings and Empirical results across various datasets demonstrate\nthat the superiority and effectiveness of our proposed test. Our code\nimplementation is provided in https://github.com/boyangaaaaa/DCT", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08747v1", "AI": {"title_translation": "存在离散化情况下的样本高效条件独立性检验", "tldr": "针对离散化数据，提出了一种无需二值化的样本高效条件独立性（CI）检验方法，通过GMM和节点回归确保了检验的准确性和有效性。", "motivation": "在许多实际场景中，由于测量限制，变量常以离散化值表示。直接将条件独立性（CI）检验应用于此类离散化数据会导致错误结论。现有方法通过二值化观察数据来推断潜在变量的CI关系，但这会不可避免地导致信息损失，从而降低检验性能。", "method": "本文引入了一种不依赖于二值化过程的样本高效CI检验方法。研究发现，可以通过广义矩方法（GMM）解决过度识别约束问题来建立潜在连续变量的独立性关系。基于此，利用节点回归（nodewise regression）推导出了一个合适的检验统计量，并确立了其正确反映CI的渐近分布。", "result": "理论发现和跨各种数据集的实证结果均表明，所提出的检验方法具有优越性和有效性。", "conclusion": "本文提出了一种在存在离散化数据时，无需二值化即可准确进行条件独立性检验的新方法，并通过理论和实证验证了其优越性，克服了传统方法的信息损失问题。", "translation": "在许多现实场景中，由于测量限制，相关变量通常表示为离散值。然而，直接将条件独立性（CI）检验应用于此类离散化数据可能会导致不正确的结论。为了解决这个问题，最近的进展试图通过对观测数据进行二值化来推断潜在变量之间正确的CI关系。然而，这个过程不可避免地导致信息损失，从而降低了检验的性能。受此启发，本文引入了一种不依赖于二值化过程的样本高效CI检验。我们发现，通过广义矩方法（GMM）解决过度识别约束问题，可以建立潜在连续变量的独立性关系。基于这一见解，我们利用节点回归推导出了一个合适的检验统计量，并建立了其正确反映CI的渐近分布。跨各种数据集的理论发现和实证结果表明了我们提出的检验的优越性和有效性。我们的代码实现可在https://github.com/boyangaaaaa/DCT中找到。", "summary": "针对条件独立性（CI）检验在离散化数据处理中面临的挑战，即现有方法直接应用会导致错误结论或二值化处理造成信息损失并降低性能，本文提出了一种样本高效的CI检验方法。该方法不依赖于数据二值化，而是创新性地通过广义矩方法（GMM）解决过度识别约束问题来推断潜在连续变量的独立性关系，并结合节点回归推导出合适的检验统计量。理论分析和多数据集的实证结果均证明了所提方法的优越性和有效性。", "keywords": "条件独立性检验, 离散化数据, 样本高效, 广义矩方法, 节点回归", "comments": "这篇论文的创新点在于提出了一个无需二值化处理的样本高效条件独立性检验方法，有效解决了传统方法在处理离散化数据时信息损失和性能下降的问题。通过引入广义矩方法和节点回归，该方法能够更准确地推断潜在连续变量的独立性关系，对于实际应用中受测量限制的离散化数据分析具有重要的理论和实践意义。"}}
{"id": "2506.08720", "title": "Minimal Order Recovery through Rank-adaptive Identification", "authors": ["Frédéric Zheng", "Yassir Jedra", "Alexandre Proutière"], "summary": "This paper addresses the problem of identifying linear systems from noisy\ninput-output trajectories. We introduce Thresholded Ho-Kalman, an algorithm\nthat leverages a rank-adaptive procedure to estimate a Hankel-like matrix\nassociated with the system. This approach optimally balances the trade-off\nbetween accurately inferring key singular values and minimizing approximation\nerrors for the rest. We establish finite-sample Frobenius norm error bounds for\nthe estimated Hankel matrix. Our algorithm further recovers both the system\norder and its Markov parameters, and we provide upper bounds for the sample\ncomplexity required to identify the system order and finite-time error bounds\nfor estimating the Markov parameters. Interestingly, these bounds match those\nachieved by state-of-the-art algorithms that assume prior knowledge of the\nsystem order.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08720v1", "AI": {"title_translation": "通过秩自适应识别实现最小阶恢复", "tldr": "本文提出了一种名为Thresholded Ho-Kalman的算法，通过秩自适应过程从噪声输入输出轨迹中识别线性系统，并在不知道系统阶数的情况下实现了与现有最优算法相当的误差界限。", "motivation": "本文旨在解决从噪声输入输出轨迹中识别线性系统的问题。", "method": "本文引入了Thresholded Ho-Kalman算法。该算法利用秩自适应过程来估计与系统相关的类Hankel矩阵。这种方法能够最优地平衡准确推断关键奇异值和最小化其余近似误差之间的权衡。", "result": "该算法建立了估计Hankel矩阵的有限样本Frobenius范数误差界限。它还能恢复系统阶数及其马尔可夫参数，并提供了识别系统阶数所需的样本复杂度上限以及估计马尔可夫参数的有限时间误差界限。这些界限与假设已知系统阶数的现有最优算法所达到的界限相匹配。", "conclusion": "Thresholded Ho-Kalman算法在不知道系统阶数的情况下，在识别线性系统方面达到了与已知系统阶数的最先进算法相当的性能。", "translation": "本文解决了从噪声输入输出轨迹中识别线性系统的问题。我们引入了Thresholded Ho-Kalman算法，该算法利用秩自适应过程来估计与系统相关的类Hankel矩阵。这种方法最优地平衡了准确推断关键奇异值和最小化其余近似误差之间的权衡。我们为估计的Hankel矩阵建立了有限样本Frobenius范数误差界限。我们的算法进一步恢复了系统阶数及其马尔可夫参数，并且我们提供了识别系统阶数所需的样本复杂度上限以及估计马尔可夫参数的有限时间误差界限。有趣的是，这些界限与假设已知系统阶数的现有最优算法所达到的界限相匹配。", "summary": "本文提出了一种名为Thresholded Ho-Kalman的新算法，用于从噪声数据中识别线性系统。该算法通过秩自适应方法估计类Hankel矩阵，并在未知系统阶数的情况下，实现了与已知系统阶数的最先进算法相当的误差界限，能够恢复系统阶数和马尔可夫参数。", "keywords": "线性系统识别, 秩自适应, Hankel矩阵, Ho-Kalman, 最小阶恢复", "comments": "该论文的创新点在于提出了Thresholded Ho-Kalman算法，该算法能够在不知道系统阶数的情况下，实现与已知系统阶数的现有最优算法相当的性能。这对于实际应用中系统阶数未知的情况具有重要意义。"}}
{"id": "2506.08429", "title": "Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring", "authors": ["Mingjie Xu", "Andrew Estornell", "Hongzheng Yang", "Yuzhi Zhao", "Zhaowei Zhu", "Qi Xuan", "Jiaheng Wei"], "summary": "The application of visual instruction tuning and other post-training\ntechniques has significantly enhanced the capabilities of Large Language Models\n(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with\nmore comprehensive visual language datasets. However, the effectiveness of VLMs\nis highly dependent on large-scale, high-quality datasets that ensure precise\nrecognition and accurate reasoning. Two key challenges hinder progress: (1)\nnoisy alignments between images and the corresponding text, which leads to\nmisinterpretation, and (2) ambiguous or misleading text, which obscures visual\ncontent. To address these challenges, we propose SCALE (Single modality data\nquality and Cross modality Alignment Evaluation), a novel quality-driven data\nselection pipeline for VLM instruction tuning datasets. Specifically, SCALE\nintegrates a cross-modality assessment framework that first assigns each data\nentry to its appropriate vision-language task, generates general and\ntask-specific captions (covering scenes, objects, style, etc.), and evaluates\nthe alignment, clarity, task rarity, text coherence, and image clarity of each\nentry based on the generated captions. We reveal that: (1) current unimodal\nquality assessment methods evaluate one modality while overlooking the rest,\nwhich can underestimate samples essential for specific tasks and discard the\nlower-quality instances that help build model robustness; and (2) appropriately\ngenerated image captions provide an efficient way to transfer the image-text\nmultimodal task into a unified text modality.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08429v1", "AI": {"title_translation": "减少数据实现更好推理：通过统一模态评分增强视觉语言模型", "tldr": "本文提出SCALE框架，通过跨模态评估和数据选择，解决VLM训练数据中的噪声和歧义问题，实现用更少数据提升VLM推理能力。", "motivation": "视觉语言模型（VLMs）的有效性高度依赖大规模高质量数据集，但现有数据存在图像-文本噪声对齐和文本歧义/误导性问题。此外，当前的单模态质量评估方法忽视其他模态，可能错误地丢弃对模型鲁棒性有益的低质量样本。", "method": "提出SCALE（单模态数据质量和跨模态对齐评估），一个质量驱动的数据选择流程。它整合了跨模态评估框架，首先将数据分配到相应的视觉-语言任务，然后生成通用和任务特定的图像描述，并基于这些描述评估每个数据条目的对齐性、清晰度、任务稀有度、文本连贯性和图像清晰度。", "result": "揭示了两个关键点：1) 当前的单模态质量评估方法在评估一个模态时忽略了其余模态，这可能低估了特定任务所需的样本，并丢弃了有助于建立模型鲁棒性的低质量实例；2) 适当生成的图像描述提供了一种有效的方式，将图像-文本多模态任务转换为统一的文本模态。", "conclusion": "通过SCALE框架，该研究有效解决了VLM训练数据中的质量问题，证明了跨模态评估和基于生成描述的方法能更有效地进行数据选择，从而用更少数据提升VLM的推理能力和鲁棒性。", "translation": "论文题目：减少数据实现更好推理：通过统一模态评分增强视觉语言模型\n论文摘要：视觉指令微调和其他后训练技术的应用显著增强了大型语言模型（LLMs）在视觉理解方面的能力，通过更全面的视觉语言数据集丰富了视觉-语言模型（VLMs）。然而，VLMs的有效性高度依赖于大规模、高质量的数据集，以确保精确识别和准确推理。阻碍进展的两个关键挑战是：(1) 图像与相应文本之间的噪声对齐，这导致误解；(2) 模糊或误导性文本，这模糊了视觉内容。为了解决这些挑战，我们提出了SCALE（单模态数据质量和跨模态对齐评估），一个新颖的质量驱动数据选择流程，用于VLM指令微调数据集。具体来说，SCALE整合了一个跨模态评估框架，该框架首先将每个数据条目分配到其适当的视觉-语言任务，生成通用和任务特定的描述（涵盖场景、对象、风格等），并根据生成的描述评估每个条目的对齐性、清晰度、任务稀有度、文本连贯性和图像清晰度。我们揭示了：(1) 当前的单模态质量评估方法在评估一个模态时忽略了其余模态，这可能低估了特定任务所需的样本，并丢弃了有助于建立模型鲁棒性的低质量实例；(2) 适当生成的图像描述提供了一种有效的方式，将图像-文本多模态任务转换为统一的文本模态。", "summary": "本文提出SCALE框架，旨在通过统一模态评分解决视觉语言模型（VLMs）训练数据中存在的噪声对齐和文本歧义问题。SCALE通过跨模态评估，生成图像描述并评估数据质量，有效筛选出高质量数据，从而在减少数据量的情况下提升VLMs的推理能力和鲁棒性，同时指出当前单模态评估的局限性。", "keywords": "视觉语言模型, 数据选择, 跨模态评估, 数据质量, 统一模态评分", "comments": "本文的创新点在于提出了一个结合单模态质量和跨模态对齐评估的数据选择流程（SCALE），特别是利用生成式图像描述将多模态评估转换为统一的文本模态评估，这为处理大规模VLM训练数据中的质量问题提供了一个高效且新颖的思路。它强调了数据质量而非单纯数量的重要性，并指出当前数据评估方法的不足，对未来VLM数据构建具有重要指导意义。"}}
{"id": "2506.08216", "title": "What makes an Ensemble (Un) Interpretable?", "authors": ["Shahaf Bassan", "Guy Amir", "Meirav Zehavi", "Guy Katz"], "summary": "Ensemble models are widely recognized in the ML community for their limited\ninterpretability. For instance, while a single decision tree is considered\ninterpretable, ensembles of trees (e.g., boosted trees) are often treated as\nblack-boxes. Despite this folklore recognition, there remains a lack of\nrigorous mathematical understanding of what particularly makes an ensemble\n(un)-interpretable, including how fundamental factors like the (1) *number*,\n(2) *size*, and (3) *type* of base models influence its interpretability. In\nthis work, we seek to bridge this gap by applying concepts from computational\ncomplexity theory to study the challenges of generating explanations for\nvarious ensemble configurations. Our analysis uncovers nuanced complexity\npatterns influenced by various factors. For example, we demonstrate that under\nstandard complexity assumptions like P$\\neq$NP, interpreting ensembles remains\nintractable even when base models are of constant size. Surprisingly, the\ncomplexity changes drastically with the number of base models: small ensembles\nof decision trees are efficiently interpretable, whereas interpreting ensembles\nwith even a constant number of linear models remains intractable. We believe\nthat our findings provide a more robust foundation for understanding the\ninterpretability of ensembles, emphasizing the benefits of examining it through\na computational complexity lens.", "comment": "To appear in ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08216v1", "AI": {"title_translation": "是什么让集成模型（不可）解释？", "tldr": "本文运用计算复杂性理论，严格分析了影响集成模型可解释性的因素。", "motivation": "尽管集成模型因其有限的可解释性而广为人知，但对于究竟是什么使得集成模型（不可）解释，以及基模型的数量、大小和类型等基本因素如何影响其可解释性，目前仍缺乏严格的数学理解。", "method": "作者通过应用计算复杂性理论的概念，研究了为各种集成配置生成解释所面临的挑战。", "result": "分析揭示了受多种因素影响的细微复杂性模式。例如，在P≠NP等标准复杂性假设下，即使基模型是常数大小，解释集成模型仍然是难以处理的。令人惊讶的是，复杂性随着基模型数量的变化而急剧变化：少量决策树的集成是可高效解释的，而解释包含甚至常数数量线性模型的集成仍然是难以处理的。", "conclusion": "研究结果为理解集成模型的可解释性提供了更坚实的基础，强调了通过计算复杂性视角审视其益处。", "translation": "集成模型因其有限的可解释性而在机器学习社区中广受认可。例如，虽然单个决策树被认为是可解释的，但树的集成（例如，提升树）通常被视为黑盒。尽管有这种普遍的认识，但对于究竟是什么使得集成模型（不可）解释，包括基本因素如基模型的（1）数量、（2）大小和（3）类型如何影响其可解释性，仍然缺乏严格的数学理解。在这项工作中，我们试图通过应用计算复杂性理论的概念来研究为各种集成配置生成解释的挑战，从而弥补这一空白。我们的分析揭示了受各种因素影响的细微复杂性模式。例如，我们证明在标准复杂性假设（如P≠NP）下，即使基模型是常数大小，解释集成模型仍然是难以处理的。令人惊讶的是，复杂性随着基模型数量的变化而急剧变化：少量决策树的集成是可高效解释的，而解释包含甚至常数数量线性模型的集成仍然是难以处理的。我们相信我们的发现为理解集成模型的可解释性提供了更坚实的基础，强调了通过计算复杂性视角审视其益处。", "summary": "本文通过计算复杂性理论深入探讨了集成模型可解释性的影响因素，旨在弥补现有理解的不足。研究发现，基模型的数量、大小和类型显著影响解释的复杂性，例如，在P≠NP假设下，即使基模型很小，解释集成模型也可能难以处理；而少量决策树的集成是可解释的，但少量线性模型的集成则不然。这为理解集成模型的可解释性提供了新的理论框架。", "keywords": "集成模型, 可解释性, 计算复杂性, 决策树, 线性模型", "comments": "本文通过应用计算复杂性理论，为严格分析集成模型的可解释性提供了新颖视角，超越了传统认知，为理解这些模型提供了更坚实的数学基础。"}}
{"id": "2506.08724", "title": "Future Deployment and Flexibility of Distributed Energy Resources in the Distribution Grids of Switzerland", "authors": ["Lorenzo Zapparoli", "Alfredo Oneto", "María Parajeles Herrera", "Blazhe Gjorgiev", "Gabriela Hug", "Giovanni Sansavini"], "summary": "The decarbonization goals worldwide drive the energy transition of power\ndistribution grids, which operate under increasingly volatile conditions and\ncloser to their technical limits. In this context, localized operational data\nwith high temporal and spatial resolution is essential for their effective\nplanning and regulation. Nevertheless, information on grid-connected\ndistributed energy resources, such as electric vehicles, photovoltaic systems,\nand heat pumps, is often fragmented, inconsistent, and unavailable. This work\nintroduces a comprehensive database of distributed energy resources and\nnon-controllable loads allocated in Switzerland's medium- and low-voltage\ndistribution grid models, covering over 2 million points of connection.\nRemarkably, this data specifies the flexibility capabilities of the\ncontrollable devices, with a set of projections aligned with national forecasts\nfor 2030, 2040, and 2050. The database supports studies on flexibility\nprovision of distributed energy resources, distribution grid resilience, and\nnational energy policy, among other topics. Importantly, its modular structure\nallows users to extract national- and local-scale information across medium-\nand low-voltage systems, enabling broad applicability across locations.", "comment": "The dataset can be accessed here:\n  https://doi.org/10.5281/zenodo.15056134", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08724v1", "AI": {"title_translation": "瑞士配电网中分布式能源的未来部署与灵活性", "tldr": "本文介绍了一个全面的分布式能源数据库，涵盖瑞士200多万个连接点，并包含可控设备的灵活性能力和未来预测，以支持电网规划和能源政策。", "motivation": "全球脱碳目标推动能源转型，导致配电网在日益波动的条件下运行并接近技术极限。然而，关于电网连接的分布式能源（如电动汽车、光伏系统和热泵）的信息通常是碎片化、不一致且不可用的，而高时间空间分辨率的本地化运行数据对于有效的规划和管理至关重要。", "method": "本文引入了一个全面的分布式能源和不可控负荷数据库，这些资源和负荷被分配到瑞士的中低压配电网模型中。该数据库覆盖了超过200万个连接点，并详细说明了可控设备的灵活性能力，同时包含与国家2030年、2040年和2050年预测相符的一系列预测。", "result": "该研究构建了一个覆盖瑞士中低压配电网模型中200多万个连接点的分布式能源和不可控负荷的综合数据库。该数据显著地明确了可控设备的灵活性能力，并提供了一系列与2030年、2040年和2050年国家预测相符的展望。", "conclusion": "所开发的数据库支持分布式能源灵活性供应、配电网弹性以及国家能源政策等方面的研究。其模块化结构允许用户在中低压系统中提取国家和地方尺度的信息，从而实现广泛的适用性。", "translation": "全球的脱碳目标推动着电力配电网的能源转型，配电网在日益波动的条件下运行并接近其技术极限。在此背景下，具有高时间空间分辨率的本地化运行数据对于其有效的规划和管理至关重要。然而，关于电网连接的分布式能源，例如电动汽车、光伏系统和热泵的信息，通常是碎片化、不一致且不可用的。这项工作引入了一个全面的分布式能源和不可控负荷数据库，这些资源和负荷被分配到瑞士的中低压配电网模型中，覆盖了超过200万个连接点。值得注意的是，这些数据详细说明了可控设备的灵活性能力，并包含与国家2030年、2040年和2050年预测相符的一系列展望。该数据库支持分布式能源灵活性供应、配电网弹性以及国家能源政策等方面的研究。重要的是，其模块化结构允许用户在中低压系统中提取国家和地方尺度的信息，从而实现广泛的适用性。", "summary": "为应对全球脱碳目标和配电网日益波动的运行条件，本文提出了一个针对瑞士中低压配电网的综合分布式能源（DER）和不可控负荷数据库。该数据库包含超过200万个连接点的数据，并首次详细说明了可控DER的灵活性能力，同时提供了与国家未来预测（2030、2040、2050年）一致的展望。该数据库旨在支持DER灵活性供应、电网弹性以及国家能源政策等方面的研究，其模块化设计确保了在不同尺度上的广泛适用性。", "keywords": "分布式能源, 灵活性, 配电网, 数据库, 瑞士", "comments": "本文的主要创新在于构建了一个覆盖范围广、数据分辨率高且包含未来预测的瑞士分布式能源数据库。该数据库特别强调了可控设备的灵活性能力，这对于电网规划、韧性分析和能源政策制定至关重要。其模块化结构增强了实用性，使其能够支持多尺度研究。这项工作为能源转型中的电网管理提供了重要的基础数据支持。"}}
{"id": "2506.08456", "title": "Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance", "authors": ["June Suk Choi", "Kyungmin Lee", "Sihyun Yu", "Yisol Choi", "Jinwoo Shin", "Kimin Lee"], "summary": "Recent text-to-video (T2V) models have demonstrated strong capabilities in\nproducing high-quality, dynamic videos. To improve the visual controllability,\nrecent works have considered fine-tuning pre-trained T2V models to support\nimage-to-video (I2V) generation. However, such adaptation frequently suppresses\nmotion dynamics of generated outputs, resulting in more static videos compared\nto their T2V counterparts. In this work, we analyze this phenomenon and\nidentify that it stems from the premature exposure to high-frequency details in\nthe input image, which biases the sampling process toward a shortcut trajectory\nthat overfits to the static appearance of the reference image. To address this,\nwe propose adaptive low-pass guidance (ALG), a simple fix to the I2V model\nsampling procedure to generate more dynamic videos without compromising\nper-frame image quality. Specifically, ALG adaptively modulates the frequency\ncontent of the conditioning image by applying low-pass filtering at the early\nstage of denoising. Extensive experiments demonstrate that ALG significantly\nimproves the temporal dynamics of generated videos, while preserving image\nfidelity and text alignment. Especially, under VBench-I2V test suite, ALG\nachieves an average improvement of 36% in dynamic degree without a significant\ndrop in video quality or image fidelity.", "comment": "Preprint. Under review. Project page available at\n  http://choi403.github.io/ALG", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08456v1", "AI": {"title_translation": "通过自适应低通引导增强图像到视频模型的运动动态", "tldr": "I2V模型在适应T2V时常抑制运动动态。本文分析原因，提出自适应低通引导(ALG)，通过早期去噪阶段对条件图像进行低通滤波，显著提升生成视频的动态性。", "motivation": "现有的图像到视频（I2V）模型通过微调文本到视频（T2V）模型实现，但这种适应性常常抑制生成视频的运动动态，导致视频比T2V模型生成的更静态。本文旨在解决这一问题。", "method": "提出自适应低通引导（ALG），这是一种对I2V模型采样过程的简单修正。具体来说，ALG通过在去噪的早期阶段应用低通滤波，自适应地调节条件图像的频率内容，以生成更具动态的视频。", "result": "实验证明ALG显著改善了生成视频的时间动态性，同时保持了图像保真度和文本对齐。在VBench-I2V测试套件下，ALG在动态程度上平均提高了36%，且视频质量或图像保真度没有显著下降。", "conclusion": "ALG能够有效解决I2V模型中运动动态受抑制的问题，在不牺牲图像质量的前提下生成更具动态的视频，为I2V生成带来了显著改进。", "translation": "最近的文本到视频（T2V）模型在生成高质量、动态视频方面表现出强大能力。为了提高视觉可控性，最近的工作考虑对预训练的T2V模型进行微调以支持图像到视频（I2V）生成。然而，这种适应性通常会抑制生成输出的运动动态，导致与T2V模型相比，视频更加静态。在这项工作中，我们分析了这一现象，并发现它源于输入图像过早地暴露于高频细节，这使得采样过程偏向于过拟合参考图像静态外观的捷径轨迹。为了解决这个问题，我们提出了自适应低通引导（ALG），这是一种对I2V模型采样过程的简单修复，可以在不损害每帧图像质量的情况下生成更具动态的视频。具体来说，ALG通过在去噪的早期阶段应用低通滤波，自适应地调制条件图像的频率内容。大量实验表明，ALG显著改善了生成视频的时间动态性，同时保持了图像保真度和文本对齐。特别是，在VBench-I2V测试套件下，ALG在动态程度上平均提高了36%，且视频质量或图像保真度没有显著下降。", "summary": "本文探讨了图像到视频（I2V）模型在从文本到视频（T2V）模型微调时运动动态受抑制的问题。研究发现，这源于输入图像高频细节的过早暴露导致采样过程过拟合静态外观。为解决此问题，作者提出了自适应低通引导（ALG），通过在去噪早期阶段对条件图像进行低通滤波，有效增强了生成视频的动态性，同时保持了图像质量和文本对齐，并在VBench-I2V测试中实现了显著的动态度提升。", "keywords": "图像到视频生成, 运动动态, 低通滤波, 自适应引导, 文本到视频", "comments": "这篇论文提出了一种新颖且有效的解决I2V模型运动动态不足的方法。通过分析问题根源并引入自适应低通滤波，ALG在不牺牲图像质量的前提下显著提升了视频的动态感，为I2V生成领域带来了重要的改进，具有较高的实用价值。"}}
{"id": "2506.08800", "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents", "authors": ["Irene Testini", "José Hernández-Orallo", "Lorenzo Pacchiardi"], "summary": "Data science aims to extract insights from data to support decision-making\nprocesses. Recently, Large Language Models (LLMs) are increasingly used as\nassistants for data science, by suggesting ideas, techniques and small code\nsnippets, or for the interpretation of results and reporting. Proper automation\nof some data-science activities is now promised by the rise of LLM agents,\ni.e., AI systems powered by an LLM equipped with additional affordances--such\nas code execution and knowledge bases--that can perform self-directed actions\nand interact with digital environments. In this paper, we survey the evaluation\nof LLM assistants and agents for data science. We find (1) a dominant focus on\na small subset of goal-oriented activities, largely ignoring data management\nand exploratory activities; (2) a concentration on pure assistance or fully\nautonomous agents, without considering intermediate levels of human-AI\ncollaboration; and (3) an emphasis on human substitution, therefore neglecting\nthe possibility of higher levels of automation thanks to task transformation.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08800v1", "AI": {"title_translation": "测量数据科学自动化：AI助手和代理评估工具的调查", "tldr": "对数据科学中大型语言模型（LLM）助手和代理的评估进行了一项调查，发现当前的评估主要集中在有限的目标导向型任务和人类替代上，忽视了数据管理、探索性活动以及人机协作。", "motivation": "数据科学旨在从数据中提取见解以支持决策。大型语言模型（LLM）正越来越多地被用作数据科学助手，而LLM代理的兴起有望实现数据科学活动的自动化。本文旨在调查这些LLM助手和代理的评估现状。", "method": "本文对数据科学中LLM助手和代理的评估进行了调查。", "result": "调查发现：(1) 评估主要集中在一小部分目标导向型活动上，很大程度上忽略了数据管理和探索性活动；(2) 评估集中于纯粹的辅助或完全自主的代理，没有考虑人机协作的中间级别；(3) 评估强调人类替代，因此忽略了通过任务转换实现更高自动化水平的可能性。", "conclusion": "目前对数据科学中LLM助手和代理的评估是有限的，主要关注特定任务和人类替代，而忽视了更广泛的数据科学活动和人机协作。", "translation": "数据科学旨在从数据中提取见解以支持决策过程。最近，大型语言模型 (LLM) 越来越多地用作数据科学助手，通过提供想法、技术和小型代码片段，或用于结果解释和报告。LLM 代理的兴起——即由 LLM 驱动并配备额外功能（如代码执行和知识库）的 AI 系统，能够执行自主行动并与数字环境交互——现在有望实现某些数据科学活动的适当自动化。在本文中，我们调查了数据科学中 LLM 助手和代理的评估。我们发现：(1) 主要关注一小部分面向目标的活动，很大程度上忽略了数据管理和探索性活动；(2) 集中于纯粹的辅助或完全自主的代理，没有考虑人机协作的中间级别；以及 (3) 强调人类替代，因此忽略了通过任务转换实现更高自动化水平的可能性。", "summary": "本文调查了数据科学领域大型语言模型（LLM）助手和代理的评估情况。研究发现，当前的评估存在三方面局限：过度关注狭窄的目标导向型任务，忽视数据管理和探索性活动；仅集中于纯粹的辅助或完全自主代理，忽略人机协作的中间层级；以及强调人类替代，而非通过任务转换实现更高自动化水平的可能性。", "keywords": "数据科学自动化, LLM代理, AI助手, 评估工具, 人机协作", "comments": "该论文指出了当前数据科学领域AI助手和代理评估方法中的关键空白。其重要性在于识别了未来研究和开发应重点关注的领域，以实现更全面有效的自动化，特别是强调人机协作和更广泛的数据科学活动。其创新之处在于系统地调查了评估现状并指出了这些被忽视的方面。"}}
{"id": "2506.08226", "title": "Mondrian: Transformer Operators via Domain Decomposition", "authors": ["Arthur Feeney", "Kuei-Hsiang Huang", "Aparna Chandramowlishwaran"], "summary": "Operator learning enables data-driven modeling of partial differential\nequations (PDEs) by learning mappings between function spaces. However, scaling\ntransformer-based operator models to high-resolution, multiscale domains\nremains a challenge due to the quadratic cost of attention and its coupling to\ndiscretization. We introduce \\textbf{Mondrian}, transformer operators that\ndecompose a domain into non-overlapping subdomains and apply attention over\nsequences of subdomain-restricted functions. Leveraging principles from domain\ndecomposition, Mondrian decouples attention from discretization. Within each\nsubdomain, it replaces standard layers with expressive neural operators, and\nattention across subdomains is computed via softmax-based inner products over\nfunctions. The formulation naturally extends to hierarchical windowed and\nneighborhood attention, supporting both local and global interactions. Mondrian\nachieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating\nresolution scaling without retraining. These results highlight the promise of\ndomain-decomposed attention for scalable and general-purpose neural operators.", "comment": "26 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08226v1", "AI": {"title_translation": "Mondrian：通过域分解的Transformer算子", "tldr": "Mondrian是一个新的Transformer算子模型，通过域分解解决高分辨率、多尺度领域中注意力机制的二次成本问题，实现了无需重新训练的分辨率扩展，并在PDEs上表现出色。", "motivation": "现有的基于Transformer的算子模型在扩展到高分辨率、多尺度领域时面临挑战，原因在于注意力机制的二次成本及其与离散化的耦合。", "method": "引入Mondrian，它将一个领域分解为不重叠的子领域，并在子领域限制函数序列上应用注意力。Mondrian利用域分解原理，将注意力与离散化解耦，用表达性神经算子替换子领域内的标准层，并通过基于softmax的函数内积计算子领域间的注意力。该方法自然地扩展到分层窗口和邻域注意力，支持局部和全局交互。", "result": "Mondrian在Allen-Cahn和Navier-Stokes PDEs上取得了强大的性能，并展示了无需重新训练的分辨率扩展能力。", "conclusion": "域分解注意力机制对于可扩展和通用神经算子具有广阔前景。", "translation": "算子学习通过学习函数空间之间的映射，实现偏微分方程（PDEs）的数据驱动建模。然而，将基于Transformer的算子模型扩展到高分辨率、多尺度领域仍然是一个挑战，原因在于注意力机制的二次成本及其与离散化的耦合。我们引入了Mondrian，这是一种Transformer算子，它将一个领域分解为不重叠的子领域，并在子领域受限函数序列上应用注意力。Mondrian利用域分解原理，将注意力与离散化解耦。在每个子领域内，它用表达性神经算子替换标准层，并通过基于softmax的函数内积计算子领域间的注意力。这种公式自然地扩展到分层窗口和邻域注意力，支持局部和全局交互。Mondrian在Allen-Cahn和Navier-Stokes PDEs上取得了强大性能，展示了无需重新训练的分辨率扩展能力。这些结果突出了域分解注意力机制在可扩展和通用神经算子方面的广阔前景。", "summary": "Mondrian是一种新型的Transformer算子模型，旨在解决传统Transformer在处理高分辨率、多尺度PDEs时注意力机制的二次成本和与离散化耦合的问题。它通过将领域分解为不重叠的子领域，并在子领域函数序列上应用注意力，从而将注意力与离散化解耦。Mondrian在子领域内使用神经算子，并通过函数内积计算子领域间的注意力。该模型在Allen-Cahn和Navier-Stokes PDEs上表现出色，并能实现无需重新训练的分辨率扩展，表明域分解注意力是构建可扩展和通用神经算子的有效途径。", "keywords": "Transformer算子, 域分解, 神经算子, 偏微分方程, 注意力机制", "comments": "Mondrian通过引入域分解的概念，有效地解决了Transformer模型在处理高分辨率PDEs时面临的计算复杂度和离散化耦合问题，这在神经算子领域是一个重要的创新。其无需重新训练即可实现分辨率扩展的能力，极大地提升了模型的实用性和泛化性。"}}
{"id": "2506.08861", "title": "Distributed component-level modeling and control of energy dynamics in electric power systems", "authors": ["Hiya Gada", "Rupamathi Jaddivada", "Marija Ilic"], "summary": "The widespread deployment of power electronic-based technologies is\ntransforming modern power systems into fast, nonlinear, and heterogeneous\nsystems. Conventional modeling and control approaches, rooted in quasi-static\nanalysis and centralized control, are inadequate for these converter-dominated\nsystems, which operate on fast timescales and involve proprietary models of\ndiverse components. This paper adopts and extends a previously introduced\nenergy space modeling framework grounded in energy conservation principles to\naddress these challenges. We generalize the notion of a port interaction\nvariable, which encodes energy exchange between interconnected, heterogeneous\ncomponents in a unified and physically intuitive manner. A multilayered\ndistributed control architecture is proposed, wherein the nonlinear physical\ndynamics of each component are lifted to a higher-level linear energy space\nthrough well-defined mappings. Distributed controllers are designed in this\nenergy space using only local states and minimal neighbor information via port\ninteraction variables. Two control designs, energy-based feedback linearizing\ncontrol (FBLC) and sliding mode control (SMC), are proven to achieve asymptotic\nconvergence to reference outputs. The approach is validated on two systems: an\ninverter-controlled RLC circuit and a synchronous generator connected to a\nload. In both cases, energy-based control improves transient response and\nreduces control effort.", "comment": "Submitted to Automatica for possible publication", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08861v1", "AI": {"title_translation": "电力系统中能量动态的分布式组件级建模与控制", "tldr": "针对现代电力系统快速、非线性、异构的特点，本文提出并扩展了一种基于能量空间建模框架和多层分布式控制架构，通过能量反馈线性化控制和滑模控制，实现了对参考输出的渐近收敛，并提高了暂态响应和降低了控制工作量。", "motivation": "现代电力系统因电力电子技术广泛部署而变得快速、非线性、异构，传统基于准静态分析和集中控制的方法已不足以应对这些快速时间尺度和涉及专有模型的转换器主导系统。", "method": "本文采用并扩展了基于能量守恒原理的能量空间建模框架，推广了端口交互变量的概念。提出了一种多层分布式控制架构，将每个组件的非线性物理动力学提升到更高层的线性能量空间。在能量空间中，利用局部状态和最小邻居信息设计了分布式控制器，并证明了能量反馈线性化控制（FBLC）和滑模控制（SMC）两种设计可以实现对参考输出的渐近收敛。", "result": "该方法在逆变器控制的RLC电路和同步发电机连接负载的两个系统上得到了验证。在这两种情况下，基于能量的控制都改善了暂态响应并降低了控制工作量。", "conclusion": "基于能量的分布式组件级建模和控制方法能够有效应对现代电力系统的挑战，提高系统性能。", "translation": "电力电子技术在电力系统中的广泛部署正在将现代电力系统转变为快速、非线性、异构的系统。传统的基于准静态分析和集中控制的建模与控制方法不足以应对这些由转换器主导的系统，这些系统运行在快速时间尺度上，并涉及各种组件的专有模型。本文采用并扩展了之前引入的基于能量守恒原理的能量空间建模框架来解决这些挑战。我们推广了端口交互变量的概念，它以统一且物理直观的方式编码了互连异构组件之间的能量交换。提出了一种多层分布式控制架构，其中每个组件的非线性物理动力学通过明确定义的映射提升到更高层的线性能量空间。分布式控制器在此能量空间中设计，仅使用局部状态和通过端口交互变量获得的最小邻居信息。能量反馈线性化控制（FBLC）和滑模控制（SMC）两种控制设计被证明可以实现对参考输出的渐近收敛。该方法在两个系统上进行了验证：一个逆变器控制的RLC电路和一个连接到负载的同步发电机。在这两种情况下，基于能量的控制都改善了暂态响应并降低了控制工作量。", "summary": "本文针对现代电力系统快速、非线性、异构的特点，提出并扩展了一种基于能量守恒原理的能量空间建模框架和多层分布式控制架构。通过将非线性物理动力学映射到线性能量空间，并利用局部信息设计分布式控制器，实现了对参考输出的渐近收敛。实验验证表明，该方法在改善暂态响应和降低控制工作量方面表现优异。", "keywords": "能量空间建模, 分布式控制, 电力系统, 能量反馈线性化控制, 滑模控制", "comments": "这篇论文的创新点在于将能量守恒原理与分布式控制架构相结合，提出了能量空间建模框架来解决电力电子主导的现代电力系统的复杂性。通过将非线性系统提升到线性能量空间进行控制设计，简化了控制难度。这种方法对于提高电力系统在快速时间尺度下的性能和鲁棒性具有重要意义。"}}
{"id": "2506.08470", "title": "MARMOT: Masked Autoencoder for Modeling Transient Imaging", "authors": ["Siyuan Shen", "Ziheng Wang", "Xingyue Peng", "Suan Xia", "Ruiqian Li", "Shiying Li", "Jingyi Yu"], "summary": "Pretrained models have demonstrated impressive success in many modalities\nsuch as language and vision. Recent works facilitate the pretraining paradigm\nin imaging research. Transients are a novel modality, which are captured for an\nobject as photon counts versus arrival times using a precisely time-resolved\nsensor. In particular for non-line-of-sight (NLOS) scenarios, transients of\nhidden objects are measured beyond the sensor's direct line of sight. Using\nNLOS transients, the majority of previous works optimize volume density or\nsurfaces to reconstruct the hidden objects and do not transfer priors learned\nfrom datasets. In this work, we present a masked autoencoder for modeling\ntransient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a\nself-supervised model pretrianed on massive and diverse NLOS transient\ndatasets. Using a Transformer-based encoder-decoder, MARMOT learns features\nfrom partially masked transients via a scanning pattern mask (SPM), where the\nunmasked subset is functionally equivalent to arbitrary sampling, and predicts\nfull measurements. Pretrained on TransVerse-a synthesized transient dataset of\n500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature\ntransfer or decoder finetuning. Comprehensive experiments are carried out in\ncomparisons with state-of-the-art methods. Quantitative and qualitative results\ndemonstrate the efficiency of our MARMOT.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08470v1", "AI": {"title_translation": "MARMOT：瞬态成像建模的掩码自编码器", "tldr": "MARMOT是一种用于瞬态成像的掩码自编码器，通过在大量瞬态数据集上进行自监督预训练，有效处理非视距场景并适应下游任务。", "motivation": "现有方法在处理非视距（NLOS）瞬态成像时，通常优化体密度或表面来重建隐藏物体，但未能利用从数据集中学习到的先验知识。为了解决这一问题，并促进瞬态成像领域预训练范式的应用，本工作提出MARMOT。", "method": "本文提出了MARMOT，一个用于瞬态成像建模的掩码自编码器。MARMOT是一个自监督模型，在大量多样的NLOS瞬态数据集上进行预训练。它使用基于Transformer的编码器-解码器，通过扫描模式掩码（SPM）从部分掩蔽的瞬态数据中学习特征，并预测完整的测量结果。未被掩蔽的子集在功能上等同于任意采样。MARMOT在包含50万个3D模型的合成瞬态数据集TransVerse上进行预训练。", "result": "定量和定性结果表明了MARMOT的效率，并且与其他最先进的方法进行了比较。", "conclusion": "MARMOT模型通过在大规模数据集上的自监督预训练，能够有效地处理非视距应用，并通过直接特征迁移或解码器微调适应下游成像任务。", "translation": "预训练模型在语言和视觉等多种模态中取得了令人瞩目的成功。最近的工作促进了成像研究中的预训练范式。瞬态是一种新颖的模态，其通过使用精确时间分辨传感器捕获物体作为光子计数与到达时间的关系。特别是在非视距（NLOS）场景中，隐藏物体的瞬态是在传感器直视范围之外测量的。利用NLOS瞬态，大多数先前的工作优化体密度或表面来重建隐藏物体，并且不转移从数据集中学习到的先验知识。在这项工作中，我们提出了一个用于瞬态成像建模的掩码自编码器，即MARMOT，以促进NLOS应用。我们的MARMOT是一个自监督模型，在海量多样的NLOS瞬态数据集上进行预训练。MARMOT使用基于Transformer的编码器-解码器，通过扫描模式掩码（SPM）从部分掩蔽的瞬态中学习特征，其中未掩蔽的子集在功能上等同于任意采样，并预测完整的测量结果。在TransVerse（一个包含50万个3D模型的合成瞬态数据集）上预训练后，MARMOT通过直接特征迁移或解码器微调适应下游成像任务。与最先进的方法进行了全面的实验比较。定量和定性结果证明了我们MARMOT的效率。", "summary": "本文提出了一种名为MARMOT的掩码自编码器，用于瞬态成像建模，特别针对非视距（NLOS）场景。与以往不利用先验知识的方法不同，MARMOT是一个自监督模型，通过在大量NLOS瞬态数据集上进行预训练，并利用基于Transformer的编码器-解码器和扫描模式掩码来学习特征并预测完整测量。MARMOT在大型合成数据集TransVerse上预训练后，能够通过特征迁移或微调适应各种下游成像任务。实验结果验证了其高效性。", "keywords": "瞬态成像, 掩码自编码器, 自监督学习, 非视距, 预训练", "comments": "MARMOT的创新之处在于将掩码自编码器和自监督预训练范式引入到瞬态成像这一新颖模态中，特别是针对非视距场景。通过在大规模数据集上进行预训练，模型能够学习到通用的瞬态特征，并有效解决传统方法缺乏先验知识利用的问题，这对于提高瞬态成像的重建效率和泛化能力具有重要意义。"}}
{"id": "2506.08872", "title": "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task", "authors": ["Nataliya Kosmyna", "Eugene Hauptmann", "Ye Tong Yuan", "Jessica Situ", "Xian-Hao Liao", "Ashly Vivian Beresnitzky", "Iris Braunstein", "Pattie Maes"], "summary": "This study explores the neural and behavioral consequences of LLM-assisted\nessay writing. Participants were divided into three groups: LLM, Search Engine,\nand Brain-only (no tools). Each completed three sessions under the same\ncondition. In a fourth session, LLM users were reassigned to Brain-only group\n(LLM-to-Brain), and Brain-only users were reassigned to LLM condition\n(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18\ncompleting session 4. We used electroencephalography (EEG) to assess cognitive\nload during essay writing, and analyzed essays using NLP, as well as scoring\nessays with the help from human teachers and an AI judge. Across groups, NERs,\nn-gram patterns, and topic ontology showed within-group homogeneity. EEG\nrevealed significant differences in brain connectivity: Brain-only participants\nexhibited the strongest, most distributed networks; Search Engine users showed\nmoderate engagement; and LLM users displayed the weakest connectivity.\nCognitive activity scaled down in relation to external tool use. In session 4,\nLLM-to-Brain participants showed reduced alpha and beta connectivity,\nindicating under-engagement. Brain-to-LLM users exhibited higher memory recall\nand activation of occipito-parietal and prefrontal areas, similar to Search\nEngine users. Self-reported ownership of essays was the lowest in the LLM group\nand the highest in the Brain-only group. LLM users also struggled to accurately\nquote their own work. While LLMs offer immediate convenience, our findings\nhighlight potential cognitive costs. Over four months, LLM users consistently\nunderperformed at neural, linguistic, and behavioral levels. These results\nraise concerns about the long-term educational implications of LLM reliance and\nunderscore the need for deeper inquiry into AI's role in learning.", "comment": "206 pages, 92 figures, 4 tables and appendix", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08872v1", "AI": {"title_translation": "使用ChatGPT时的大脑：使用AI助手进行论文写作任务时的认知负债积累", "tldr": "LLM辅助的论文写作导致认知参与度不足和长期表现不佳。", "motivation": "本研究旨在探讨LLM辅助论文写作的神经和行为后果，以解决对LLM依赖的长期教育影响的担忧。", "method": "参与者分为LLM组、搜索引擎组和纯脑力组，每组完成三次会话。在第四次会话中，LLM用户被重新分配到纯脑力组，纯脑力用户被重新分配到LLM条件。研究共涉及54名参与者（前三会话）和18名参与者（第四会话）。研究使用脑电图（EEG）评估认知负荷，并结合NLP、人类教师和AI评判对论文进行分析和评分。", "result": "各组在命名实体识别、n-gram模式和主题本体方面表现出组内同质性。EEG显示纯脑力参与者的大脑连接最强且分布最广，搜索引擎用户适中，LLM用户最弱，认知活动随外部工具使用而减少。在第四次会话中，LLM-to-Brain参与者表现出α和β连接减少（参与度不足），而Brain-to-LLM用户表现出更高的记忆回忆和枕叶-顶叶和前额叶区域的激活。LLM组的论文自我报告所有权最低，纯脑力组最高。LLM用户难以准确引用自己的作品。在四个月内，LLM用户在神经、语言和行为层面持续表现不佳。", "conclusion": "尽管大型语言模型（LLMs）提供了即时便利，但它们可能带来潜在的认知成本，并引发对LLM依赖的长期教育影响的担忧。", "translation": "本研究探讨了LLM辅助论文写作的神经和行为后果。参与者分为三组：LLM组、搜索引擎组和纯脑力组（不使用工具）。每组在相同条件下完成了三次会话。在第四次会话中，LLM用户被重新分配到纯脑力组（LLM-to-Brain），纯脑力用户被重新分配到LLM条件（Brain-to-LLM）。共有54名参与者参加了第1-3次会话，其中18名完成了第4次会话。我们使用脑电图（EEG）评估论文写作期间的认知负荷，并使用自然语言处理（NLP）分析论文，以及在人类教师和AI评判的帮助下对论文进行评分。在各组中，命名实体识别（NERs）、n-gram模式和主题本体显示出组内同质性。EEG显示脑连接存在显著差异：纯脑力参与者表现出最强、分布最广的网络；搜索引擎用户表现出中度参与；LLM用户表现出最弱的连接。认知活动随着外部工具的使用而减少。在第4次会话中，LLM-to-Brain参与者表现出α和β连接减少，表明参与度不足。Brain-to-LLM用户表现出更高的记忆回忆和枕叶-顶叶和前额叶区域的激活，类似于搜索引擎用户。论文的自我报告所有权在LLM组中最低，在纯脑力组中最高。LLM用户也难以准确引用自己的作品。虽然LLM提供了即时便利，但我们的研究结果强调了潜在的认知成本。在四个月的时间里，LLM用户在神经、语言和行为层面持续表现不佳。这些结果引发了对LLM依赖的长期教育影响的担忧，并强调需要更深入地探究AI在学习中的作用。", "summary": "本研究通过EEG和论文分析，探讨了使用大型语言模型（LLMs）进行论文写作的神经和行为影响。研究发现，与纯脑力组和搜索引擎组相比，LLM用户表现出较弱的大脑连接和认知参与度不足。长期来看，LLM用户在神经、语言和行为指标上持续表现不佳，同时自我报告的论文所有权较低，且难以引用自己的作品。研究结果表明，尽管LLMs提供便利，但可能带来显著的认知成本，并引发对其长期教育影响的担忧。", "keywords": "LLM, 认知负荷, 论文写作, 脑电图, 认知负债", "comments": "本研究通过结合脑电图和纵向数据，为过度依赖大型语言模型（LLMs）进行学术任务可能产生的认知负面影响提供了关键的实证证据。其跨学科方法（神经科学、自然语言处理、教育学）增强了研究结果的说服力。提出的“认知负债”概念是对观察到的长期表现不佳的有力比喻。这项研究对教育政策和未来AI在学习中的整合提出了重要的关注点。"}}
{"id": "2506.08228", "title": "Scaling Laws of Motion Forecasting and Planning -- A Technical Report", "authors": ["Mustafa Baniodeh", "Kratarth Goel", "Scott Ettinger", "Carlos Fuertes", "Ari Seff", "Tim Shen", "Cole Gulino", "Chenjie Yang", "Ghassen Jerfel", "Dokook Choe", "Rui Wang", "Vinutha Kallem", "Sergio Casas", "Rami Al-Rfou", "Benjamin Sapp", "Dragomir Anguelov"], "summary": "We study the empirical scaling laws of a family of encoder-decoder\nautoregressive transformer models on the task of joint motion forecasting and\nplanning in the autonomous driving domain. Using a 500 thousand hours driving\ndataset, we demonstrate that, similar to language modeling, model performance\nimproves as a power-law function of the total compute budget, and we observe a\nstrong correlation between model training loss and model evaluation metrics.\nMost interestingly, closed-loop metrics also improve with scaling, which has\nimportant implications for the suitability of open-loop metrics for model\ndevelopment and hill climbing. We also study the optimal scaling of the number\nof transformer parameters and the training data size for a training\ncompute-optimal model. We find that as the training compute budget grows,\noptimal scaling requires increasing the model size 1.5x as fast as the dataset\nsize. We also study inference-time compute scaling, where we observe that\nsampling and clustering the output of smaller models makes them competitive\nwith larger models, up to a crossover point beyond which a larger models\nbecomes more inference-compute efficient. Overall, our experimental results\ndemonstrate that optimizing the training and inference-time scaling properties\nof motion forecasting and planning models is a key lever for improving their\nperformance to address a wide variety of driving scenarios. Finally, we briefly\nstudy the utility of training on general logged driving data of other agents to\nimprove the performance of the ego-agent, an important research area to address\nthe scarcity of robotics data for large capacity models training.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08228v1", "AI": {"title_translation": "运动预测与规划的缩放定律——一份技术报告", "tldr": "本文研究了自动驾驶领域运动预测和规划中Transformer模型的经验缩放定律，发现模型性能随计算预算呈幂律增长，并提出了训练和推理时间最优缩放的策略。", "motivation": "研究自动驾驶领域中联合运动预测和规划任务的编码器-解码器自回归Transformer模型的经验缩放定律，以理解模型性能如何随计算资源扩展。", "method": "使用50万小时的驾驶数据集，对一系列编码器-解码器自回归Transformer模型进行实证研究；分析模型性能与总计算预算、训练损失与评估指标之间的关系；研究训练计算最优模型中Transformer参数数量和训练数据大小的最佳缩放比例；研究推理时间计算缩放，并探讨通过采样和聚类小模型输出来提高其竞争力的方法；简要研究了利用其他智能体的通用驾驶数据来提高自我智能体性能的效用。", "result": "模型性能随总计算预算呈幂律函数改进，与语言建模类似；模型训练损失与模型评估指标之间存在强相关性；闭环指标也随缩放而改进，对开环指标的适用性有重要影响；随着训练计算预算的增长，最佳缩放要求模型大小的增长速度比数据集大小快1.5倍；在推理时间计算缩放方面，通过采样和聚类小模型的输出，可以使其与大模型具有竞争力，直到一个交叉点，超过该点后大模型变得更具推理计算效率。", "conclusion": "优化运动预测和规划模型的训练和推理时间缩放特性是提高其性能以应对各种驾驶场景的关键杠杆。", "translation": "我们研究了自动驾驶领域中联合运动预测和规划任务上，一系列编码器-解码器自回归Transformer模型的经验缩放定律。通过使用一个50万小时的驾驶数据集，我们证明，类似于语言建模，模型性能随着总计算预算的增加呈幂律函数改进，并且我们观察到模型训练损失与模型评估指标之间存在强相关性。最有趣的是，闭环指标也随着缩放而改进，这对开环指标在模型开发和优化中的适用性具有重要意义。我们还研究了训练计算最优模型中Transformer参数数量和训练数据大小的最佳缩放比例。我们发现，随着训练计算预算的增长，最佳缩放要求模型大小的增长速度比数据集大小快1.5倍。我们还研究了推理时间计算缩放，我们观察到对较小模型的输出进行采样和聚类可以使它们与较大模型具有竞争力，直到一个交叉点，超过该点后，较大模型变得更具推理计算效率。总的来说，我们的实验结果表明，优化运动预测和规划模型的训练和推理时间缩放特性是提高其性能以应对各种驾驶场景的关键杠杆。最后，我们简要研究了在其他智能体的通用日志驾驶数据上进行训练以提高自我智能体性能的效用，这是一个重要的研究领域，旨在解决大型容量模型训练中机器人数据稀缺的问题。", "summary": "本技术报告深入探讨了自动驾驶领域中运动预测与规划任务的Transformer模型的经验缩放定律。研究发现，模型性能随计算预算呈幂律增长，且训练损失与评估指标强相关。特别地，闭环指标也随缩放而改善。论文进一步分析了训练计算最优的模型参数与数据规模的最佳配比，指出模型大小应比数据集增长快1.5倍。此外，研究还探讨了推理时间的缩放，发现小模型通过特定策略可与大模型媲美，直至某临界点。整体而言，优化训练和推理的缩放特性是提升模型性能的关键。", "keywords": "缩放定律, 运动预测, 规划, Transformer, 自动驾驶", "comments": "这篇论文对自动驾驶领域的运动预测和规划模型在大规模数据和计算下的行为进行了系统性的实证研究，揭示了性能的幂律缩放规律，并提供了训练和推理时间最优缩放的实用指导。其关于闭环指标随缩放改进的发现对于实际系统开发具有重要指导意义，而对模型大小与数据量最佳配比的量化发现也为未来的模型设计提供了宝贵参考。同时，简要提及利用其他智能体数据解决数据稀缺问题，也指明了未来研究方向。"}}
{"id": "2506.08903", "title": "HabSim: Architecture for modelling disruptions, propagation, detection and repair in deep space habitats", "authors": ["Luca Vaccino", "Alana K. Lund", "Shirley J. Dyke", "Mohsen Azimi", "Ethan Vallerga"], "summary": "Establishing long-term human settlements in deep space presents significant\nchallenges. Harsh environmental conditions, such as extreme temperature\nfluctuations, micrometeorite impacts, seismic activity, and exposure to solar\nand cosmic radiation pose obstacles to the design and operation of habitat\nsystems. Prolonged mission duration and the vast distances from Earth introduce\nfurther complications in the form of delayed communication and limited\nresources, making autonomy especially desirable.\n  Enabling simulation of the consequences of disruptions and their propagation\nthrough the various habitat subsystems is important for the development of\nautonomous and resilient space habitats. While existing simulation tools can\nassist in modeling some of these aspects, the integration of damage\npropagation, detection and repair in a computational model is rarely\nconsidered. This paper introduces and demonstrates a simulation architecture\ndesigned to model these aspects efficiently. By combining physics-based and\nphenomenological models, our approach balances computational efficiency with\nmodel fidelity. Furthermore, by coordinating subsystems operating at different\ntime scales, we achieve real-time simulation capabilities.\n  After describing the architecture, we demonstrate its application within\nHabSim, a space habitat system model developed by the NASA-funded Resilient\nExtraterrestrial Habitat Institute (RETHi). In these scenarios we consider fire\nhazard propagation within a lunar habitat to illustrate both how our\narchitecture supports the modeling of disruption propagation, detection, and\nrepair in a simulation environment and how the HabSim model can be leveraged\nfor through stochastic simulations to support resilience assessment. The\narchitecture developed herein is efficient and scalable, enabling researchers\nto gain insight into resilience, autonomy and decision-making.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08903v1", "AI": {"title_translation": "HabSim：用于模拟深空栖息地中断、传播、检测和修复的架构", "tldr": "本文提出并演示了一种名为HabSim的模拟架构，用于高效地模拟深空栖息地中破坏的传播、检测和修复，以支持弹性栖息地的设计和自主性。", "motivation": "在深空建立长期人类定居点面临严峻挑战，包括恶劣环境、通信延迟和资源有限。现有模拟工具很少考虑计算模型中破坏传播、检测和修复的集成，而这对于开发自主和弹性空间栖息地至关重要。", "method": "本文引入并演示了一种模拟架构，旨在高效地模拟破坏传播、检测和修复。该方法结合了基于物理的模型和现象学模型，平衡了计算效率和模型保真度。通过协调在不同时间尺度下运行的子系统，实现了实时模拟能力。", "result": "该架构在NASA资助的RETHi开发的深空栖息地系统模型HabSim中得到了应用。通过模拟月球栖息地内的火灾危险传播场景，展示了该架构如何支持模拟中断传播、检测和修复，以及HabSim模型如何通过随机模拟用于弹性评估。", "conclusion": "所开发的架构高效且可扩展，使研究人员能够深入了解栖息地的弹性、自主性和决策过程。", "translation": "在深空建立长期人类定居点面临重大挑战。恶劣的环境条件，如极端温度波动、微流星体撞击、地震活动以及太阳和宇宙辐射的暴露，对栖息地系统的设计和运行构成障碍。任务持续时间的延长以及与地球的巨大距离带来了通信延迟和资源有限等进一步的复杂性，使得自主性尤为重要。\n\n实现对破坏后果及其在各种栖息地子系统中的传播进行模拟，对于开发自主和弹性的空间栖息地至关重要。虽然现有模拟工具可以协助模拟其中一些方面，但计算模型中破坏传播、检测和修复的集成却很少被考虑。本文介绍并演示了一种旨在高效模拟这些方面的模拟架构。通过结合基于物理的模型和现象学模型，我们的方法在计算效率和模型保真度之间取得了平衡。此外，通过协调在不同时间尺度下运行的子系统，我们实现了实时模拟能力。\n\n在描述了该架构之后，我们演示了其在HabSim中的应用，HabSim是NASA资助的弹性地外栖息地研究所（RETHi）开发的太空栖息地系统模型。在这些场景中，我们考虑了月球栖息地内的火灾危险传播，以说明我们的架构如何支持在模拟环境中对破坏传播、检测和修复进行建模，以及如何利用HabSim模型通过随机模拟来支持弹性评估。本文开发的架构高效且可扩展，使研究人员能够深入了解弹性、自主性和决策。", "summary": "本文介绍了一种名为HabSim的模拟架构，旨在解决深空栖息地在极端环境下建立长期人类定居点所面临的挑战。该架构通过结合物理和现象学模型，并协调不同时间尺度的子系统，实现了对破坏传播、检测和修复的高效实时模拟。文章通过在NASA资助的RETHi开发的HabSim模型中模拟月球栖息地的火灾危险传播，展示了其在支持弹性评估和自主性方面的应用。该架构被证明是高效和可扩展的，有助于提升对深空栖息地弹性、自主性和决策的理解。", "keywords": "深空栖息地, 模拟架构, 破坏传播, 弹性评估, HabSim", "comments": "这项工作通过提出一种集成破坏传播、检测和修复的模拟架构，填补了现有模拟工具的空白。其结合物理和现象学模型以平衡效率和保真度，以及实现实时模拟的能力，是重要的创新点。该架构对于未来深空栖息地的设计和运行，特别是提升其弹性和自主性具有重要意义。"}}
{"id": "2506.08493", "title": "Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization", "authors": ["Qilin Yin", "Wei Lu", "Xiangyang Luo", "Xiaochun Cao"], "summary": "Most research efforts in the multimedia forensics domain have focused on\ndetecting forgery audio-visual content and reached sound achievements. However,\nthese works only consider deepfake detection as a classification task and\nignore the case where partial segments of the video are tampered with. Temporal\nforgery localization (TFL) of small fake audio-visual clips embedded in real\nvideos is still challenging and more in line with realistic application\nscenarios. To resolve this issue, we propose a universal context-aware\ncontrastive learning framework (UniCaCLF) for TFL. Our approach leverages\nsupervised contrastive learning to discover and identify forged instants by\nmeans of anomaly detection, allowing for the precise localization of temporal\nforged segments. To this end, we propose a novel context-aware perception layer\nthat utilizes a heterogeneous activation operation and an adaptive context\nupdater to construct a context-aware contrastive objective, which enhances the\ndiscriminability of forged instant features by contrasting them with genuine\ninstant features in terms of their distances to the global context. An\nefficient context-aware contrastive coding is introduced to further push the\nlimit of instant feature distinguishability between genuine and forged instants\nin a supervised sample-by-sample manner, suppressing the cross-sample influence\nto improve temporal forgery localization performance. Extensive experimental\nresults over five public datasets demonstrate that our proposed UniCaCLF\nsignificantly outperforms the state-of-the-art competing algorithms.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08493v1", "AI": {"title_translation": "上下文感知TFL：一种用于时间伪造定位的通用上下文感知对比学习框架", "tldr": "本文提出了一种名为UniCaCLF的通用上下文感知对比学习框架，通过异常检测和上下文感知对比目标，显著提高了视频中时间伪造片段的定位性能。", "motivation": "多媒体取证领域的大多数研究都集中在检测伪造的音视频内容，但这些工作通常将深度伪造检测视为分类任务，忽略了视频部分片段被篡改的情况。在真实视频中嵌入的小型伪造音视频片段的时间伪造定位（TFL）仍然具有挑战性，但更符合实际应用场景。", "method": "本文提出了一种通用的上下文感知对比学习框架（UniCaCLF）用于时间伪造定位。该方法利用监督对比学习，通过异常检测来发现和识别伪造瞬间，从而实现对时间伪造片段的精确本地化。为此，文中提出了一种新颖的上下文感知感知层，该层利用异构激活操作和自适应上下文更新器来构建上下文感知对比目标，通过将伪造瞬间特征与真实瞬间特征相对于其到全局上下文的距离进行对比，增强了伪造瞬间特征的可区分性。此外，还引入了一种高效的上下文感知对比编码，以监督的逐样本方式进一步推动真实和伪造瞬间之间特征可区分性的极限，抑制了跨样本影响，从而提高了时间伪造定位性能。", "result": "在五个公共数据集上的大量实验结果表明，我们提出的UniCaCLF显著优于最先进的竞争算法。", "conclusion": "本文提出的UniCaCLF框架通过结合上下文感知对比学习和异常检测，有效解决了视频中时间伪造定位的挑战，并取得了优于现有技术的性能。", "translation": "大多数多媒体取证领域的研究工作都集中在检测伪造的音视频内容并取得了显著成就。然而，这些工作只将深度伪造检测视为一个分类任务，而忽略了视频部分片段被篡改的情况。在真实视频中嵌入的小型伪造音视频片段的时间伪造定位（TFL）仍然具有挑战性，但更符合实际应用场景。为了解决这个问题，我们提出了一种用于TFL的通用上下文感知对比学习框架（UniCaCLF）。我们的方法利用监督对比学习，通过异常检测来发现和识别伪造瞬间，从而实现对时间伪造片段的精确本地化。为此，我们提出了一种新颖的上下文感知感知层，该层利用异构激活操作和自适应上下文更新器来构建上下文感知对比目标，通过将伪造瞬间特征与真实瞬间特征相对于其到全局上下文的距离进行对比，增强了伪造瞬间特征的可区分性。此外，还引入了一种高效的上下文感知对比编码，以监督的逐样本方式进一步推动真实和伪造瞬间之间特征可区分性的极限，抑制了跨样本影响，从而提高了时间伪造定位性能。在五个公共数据集上的大量实验结果表明，我们提出的UniCaCLF显著优于最先进的竞争算法。", "summary": "本文提出了一种名为UniCaCLF的通用上下文感知对比学习框架，旨在解决视频中时间伪造片段的精确定位问题。与传统仅关注分类的深度伪造检测不同，UniCaCLF利用监督对比学习和异常检测机制，通过新颖的上下文感知感知层和高效的上下文感知对比编码，增强了伪造与真实瞬间特征的可区分性。该方法在五个公共数据集上表现出显著优于现有技术的性能，为时间伪造定位提供了有效解决方案。", "keywords": "时间伪造定位, 上下文感知, 对比学习, 深度伪造检测, 异常检测", "comments": "该论文的创新点在于提出了一个通用的上下文感知对比学习框架（UniCaCLF），专门用于解决时间伪造定位（TFL）这一更具挑战性和实际意义的任务。通过结合监督对比学习、异常检测以及独特的上下文感知感知层和对比编码，该方法有效地增强了伪造瞬间特征的区分度，克服了传统分类方法的局限性。其在多个数据集上超越SOTA的表现，表明了该方法在多媒体取证领域的潜力和重要性。"}}
{"id": "2506.08898", "title": "Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation", "authors": ["Mingfeng Fan", "Jianan Zhou", "Yifeng Zhang", "Yaoxin Wu", "Jinbiao Chen", "Guillaume Adrien Sartoretti"], "summary": "Recent deep reinforcement learning methods have achieved remarkable success\nin solving multi-objective combinatorial optimization problems (MOCOPs) by\ndecomposing them into multiple subproblems, each associated with a specific\nweight vector. However, these methods typically treat all subproblems equally\nand solve them using a single model, hindering the effective exploration of the\nsolution space and thus leading to suboptimal performance. To overcome the\nlimitation, we propose POCCO, a novel plug-and-play framework that enables\nadaptive selection of model structures for subproblems, which are subsequently\noptimized based on preference signals rather than explicit reward values.\nSpecifically, we design a conditional computation block that routes subproblems\nto specialized neural architectures. Moreover, we propose a preference-driven\noptimization algorithm that learns pairwise preferences between winning and\nlosing solutions. We evaluate the efficacy and versatility of POCCO by applying\nit to two state-of-the-art neural methods for MOCOPs. Experimental results\nacross four classic MOCOP benchmarks demonstrate its significant superiority\nand strong generalization.", "comment": "22 pages, 6 figures, under review", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08898v1", "AI": {"title_translation": "偏好驱动的条件计算多目标组合优化", "tldr": "针对多目标组合优化问题（MOCOPs），现有方法因平等处理子问题而性能不佳。本文提出POCCO框架，通过自适应模型选择和偏好驱动优化来解决此限制，显著提高了性能和泛化能力。", "motivation": "最近的深度强化学习方法在解决多目标组合优化问题（MOCOPs）时，通常将所有子问题同等对待并使用单一模型进行求解，这阻碍了对解空间的有效探索，从而导致次优性能。", "method": "本文提出了POCCO，一个新颖的即插即用框架，它能够为子问题自适应选择模型结构，然后根据偏好信号而非显式奖励值进行优化。具体而言，设计了一个条件计算块，将子问题路由到专门的神经网络架构。此外，提出了一种偏好驱动的优化算法，学习赢家和输家解决方案之间的成对偏好。", "result": "通过将其应用于两种最先进的MOCOPs神经方法，评估了POCCO的有效性和多功能性。在四个经典MOCOP基准上的实验结果表明，其具有显著的优越性和强大的泛化能力。", "conclusion": "POCCO通过自适应模型选择和偏好驱动优化，有效克服了现有深度强化学习方法在解决多目标组合优化问题时面临的局限性，从而显著提高了性能和泛化能力。", "translation": "最近的深度强化学习方法通过将多目标组合优化问题（MOCOPs）分解为多个子问题，每个子问题关联一个特定的权重向量，从而在解决MOCOPs方面取得了显著成功。然而，这些方法通常将所有子问题同等对待，并使用单一模型进行求解，这阻碍了对解空间的有效探索，从而导致次优性能。为了克服这一限制，我们提出了POCCO，一个新颖的即插即用框架，它能够为子问题自适应选择模型结构，然后根据偏好信号而非显式奖励值进行优化。具体而言，我们设计了一个条件计算块，将子问题路由到专门的神经网络架构。此外，我们提出了一种偏好驱动的优化算法，学习赢家和输家解决方案之间的成对偏好。通过将其应用于两种最先进的MOCOPs神经方法，我们评估了POCCO的有效性和多功能性。在四个经典MOCOP基准上的实验结果表明，其具有显著的优越性和强大的泛化能力。", "summary": "本文提出了一种名为POCCO的新型即插即用框架，旨在解决现有深度强化学习方法在多目标组合优化问题（MOCOPs）中存在的局限性。当前方法通常对所有子问题一视同仁，导致探索不足和性能次优。POCCO通过引入条件计算块，能够为不同的子问题自适应选择专门的神经网络架构。此外，它采用一种偏好驱动的优化算法，学习解决方案之间的成对偏好，从而无需显式奖励。实验结果表明，POCCO在多个经典MOCOP基准测试上展现出显著的优越性和强大的泛化能力。", "keywords": "多目标组合优化, 深度强化学习, 条件计算, 偏好驱动优化, 自适应选择", "comments": "该论文的创新点在于引入了条件计算和偏好驱动的优化方法来处理多目标组合优化问题。通过允许模型根据子问题自适应地选择计算路径，并利用偏好信号而非硬性奖励，POCCO有望更有效地探索解空间，从而提升复杂优化问题的求解质量和效率。这种模块化和自适应的设计为未来的多目标优化研究提供了新的思路。"}}
{"id": "2506.08231", "title": "Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework", "authors": ["Melissa Estevez", "Nisha Singh", "Lauren Dyson", "Blythe Adamson", "Qianyu Yuan", "Megan W. Hildner", "Erin Fidyk", "Olive Mbah", "Farhad Khan", "Kathi Seidl-Rathkopf", "Aaron B. Cohen"], "summary": "Large language models (LLMs) are increasingly used to extract clinical data\nfrom electronic health records (EHRs), offering significant improvements in\nscalability and efficiency for real-world data (RWD) curation in oncology.\nHowever, the adoption of LLMs introduces new challenges in ensuring the\nreliability, accuracy, and fairness of extracted data, which are essential for\nresearch, regulatory, and clinical applications. Existing quality assurance\nframeworks for RWD and artificial intelligence do not fully address the unique\nerror modes and complexities associated with LLM-extracted data. In this paper,\nwe propose a comprehensive framework for evaluating the quality of clinical\ndata extracted by LLMs. The framework integrates variable-level performance\nbenchmarking against expert human abstraction, automated verification checks\nfor internal consistency and plausibility, and replication analyses comparing\nLLM-extracted data to human-abstracted datasets or external standards. This\nmultidimensional approach enables the identification of variables most in need\nof improvement, systematic detection of latent errors, and confirmation of\ndataset fitness-for-purpose in real-world research. Additionally, the framework\nsupports bias assessment by stratifying metrics across demographic subgroups.\nBy providing a rigorous and transparent method for assessing LLM-extracted RWD,\nthis framework advances industry standards and supports the trustworthy use of\nAI-powered evidence generation in oncology research and practice.", "comment": "18 pages, 3 tables, 1 figure", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08231v1", "AI": {"title_translation": "确保精选EHR衍生数据的可靠性：LLM/ML提取信息和数据准确性验证（VALID）框架", "tldr": "本文提出了一个名为VALID的综合框架，用于评估LLM从电子健康记录（EHR）中提取的临床数据的质量，以解决现有框架在确保数据可靠性、准确性和公平性方面的不足。", "motivation": "大型语言模型（LLM）在从电子健康记录（EHR）中提取临床数据方面应用日益广泛，显著提高了肿瘤学领域真实世界数据（RWD）整理的可扩展性和效率。然而，LLM的采用引入了确保所提取数据可靠性、准确性和公平性的新挑战，而这些对于研究、监管和临床应用至关重要。现有的RWD和人工智能质量保证框架未能完全解决与LLM提取数据相关的独特错误模式和复杂性。", "method": "本文提出了一个用于评估LLM提取临床数据质量的综合框架。该框架整合了针对专家人工抽象的变量级性能基准测试、用于内部一致性和合理性的自动化验证检查，以及将LLM提取数据与人工抽象数据集或外部标准进行比较的复制分析。", "result": "该多维度方法能够识别最需要改进的变量，系统地检测潜在错误，并确认数据集在真实世界研究中的适用性。此外，该框架通过按人口统计学亚组分层指标来支持偏倚评估。", "conclusion": "通过提供一种严谨透明的LLM提取真实世界数据评估方法，该框架提升了行业标准，并支持在肿瘤学研究和实践中可信赖地使用AI驱动的证据生成。", "translation": "大型语言模型（LLM）正越来越多地用于从电子健康记录（EHR）中提取临床数据，这显著提高了肿瘤学领域真实世界数据（RWD）整理的可扩展性和效率。然而，LLM的采用在确保所提取数据的可靠性、准确性和公平性方面带来了新的挑战，而这些对于研究、监管和临床应用至关重要。现有的RWD和人工智能质量保证框架未能完全解决与LLM提取数据相关的独特错误模式和复杂性。在本文中，我们提出了一个用于评估LLM提取临床数据质量的综合框架。该框架整合了针对专家人工抽象的变量级性能基准测试、用于内部一致性和合理性的自动化验证检查，以及将LLM提取数据与人工抽象数据集或外部标准进行比较的复制分析。这种多维度方法能够识别最需要改进的变量，系统地检测潜在错误，并确认数据集在真实世界研究中的适用性。此外，该框架通过按人口统计学亚组分层指标来支持偏倚评估。通过提供一种严谨透明的LLM提取真实世界数据评估方法，该框架提升了行业标准，并支持在肿瘤学研究和实践中可信赖地使用AI驱动的证据生成。", "summary": "本文提出了一项名为VALID的综合框架，旨在解决大型语言模型（LLMs）从电子健康记录（EHRs）中提取临床数据时面临的可靠性、准确性和公平性挑战。该框架通过整合变量级性能基准测试、自动化验证检查和复制分析，以识别错误、评估数据适用性并支持偏倚评估，从而提升了AI驱动证据生成的行业标准。", "keywords": "LLMs, EHR, 数据可靠性, 框架, 肿瘤学", "comments": "该论文提出了一种及时且重要的框架，以解决LLM在医疗数据提取中日益增长的应用所带来的数据质量挑战。其创新之处在于结合了人工基准、自动化检查和复制分析的多维方法，这对于确保AI在敏感的临床研究和实践中的可信赖应用至关重要。该框架对偏倚评估的支持也增加了其价值。"}}
{"id": "2506.08975", "title": "Quantitative Indices for Improving Metro Load Curve, Using Distributed Generation", "authors": ["Masoud Behbahani", "Alireza Fereidunian"], "summary": "This paper promises the idea of using DG (Distributed Generation) to improve\nthe Metro load curve. Public transportation systems are often based on gasoline\nand diesel. However, with the gradual development in usage of the Metro and\nmonorail, a new load with heavy demand, inappropriate load curve and middle LF\n(Load factor) is added to the electricity grid. In addition to supply problem\nof this massive consumer, the Metro load curve is another problem, which has a\nrelatively low LF. Furthermore, Metro load peak hours coincide with the peaks\nof national grid. Improvement of the load curve is well-known in electrical\nengineering literature, which depending on the type of load curve, offers\ngeneral recommendations in three approaches; DSM (Demand Side Management), DS\n(Distributed Storage) and DG. In this paper, to achieve quantitative indices of\nimprovement for Metro load curve using DG, firstly based on the analysis of\nvolume and consumption pattern of the main loads in Metro, the typical load\ncurve has been extracted. Using this curve, the result of using DG is shown by\nquantitative parameters which represent the significant improvement in load\ncurve. These parameters can be used to calculate economic indicators such as\ninitial cost and ROI (Return of Investment).", "comment": "Preprint of the accepted paper for the EPDC 2014", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08975v1", "AI": {"title_translation": "地铁负荷曲线的量化改进指标，使用分布式发电", "tldr": "本文提出使用分布式发电（DG）改善地铁负荷曲线，通过量化指标展示了显著改进。", "motivation": "地铁系统作为一种新的大需求负荷，其负荷曲线不佳，负荷率（LF）较低，且负荷高峰与国家电网高峰重合，给电网带来供电问题。因此，需要改进地铁负荷曲线。", "method": "首先，通过分析地铁主要负荷的体积和消耗模式，提取了典型的地铁负荷曲线。然后，利用该曲线，通过量化参数展示了使用分布式发电（DG）后的负荷曲线改善效果。", "result": "使用分布式发电（DG）后，地铁负荷曲线得到了显著改善，并通过量化参数进行了展示。这些参数可用于计算初始成本和投资回报率等经济指标。", "conclusion": "本文通过量化指标证明了分布式发电（DG）能够显著改善地铁负荷曲线，并且这些量化参数可以用于经济性评估。", "translation": "本文提出了利用分布式发电（DG）改善地铁负荷曲线的设想。公共交通系统通常以汽油和柴油为基础。然而，随着地铁和单轨交通使用的逐步发展，电网中增加了一种具有高需求、不合适的负荷曲线和中等负荷率（LF）的新负荷。除了这种大规模消费者的供电问题外，地铁负荷曲线是另一个问题，其负荷率相对较低。此外，地铁负荷高峰与国家电网高峰重合。负荷曲线的改进在电气工程文献中广为人知，根据负荷曲线的类型，通常提供三种方法的通用建议：需求侧管理（DSM）、分布式储能（DS）和分布式发电（DG）。在本文中，为了获得使用DG改善地铁负荷曲线的量化指标，首先基于对地铁主要负荷的体积和消耗模式的分析，提取了典型的负荷曲线。利用该曲线，通过量化参数显示了使用DG的结果，这些参数代表了负荷曲线的显著改善。这些参数可用于计算初始成本和投资回报率（ROI）等经济指标。", "summary": "本文探讨了利用分布式发电（DG）技术改善地铁系统负荷曲线的方法。鉴于地铁作为大需求负荷对电网造成的压力，以及其不佳的负荷曲线和与电网高峰重合的问题，研究人员首先分析了地铁主要负荷的模式，提取了典型的负荷曲线。在此基础上，通过量化参数展示了DG对负荷曲线的显著优化效果，并指出这些参数可用于经济性评估。", "keywords": "分布式发电, 地铁负荷曲线, 负荷管理, 量化指标, 负荷率", "comments": "本文的创新之处在于提出了使用分布式发电（DG）来解决地铁负荷曲线不佳的问题，并引入了量化指标来衡量改进效果，这为后续的经济性分析奠定了基础。这对于城市轨道交通的能源管理和电网稳定具有重要意义。"}}
{"id": "2506.08512", "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Zihao Liu", "Linlin Yang"], "summary": "Video Temporal Grounding (VTG), which aims to localize video clips\ncorresponding to natural language queries, is a fundamental yet challenging\ntask in video understanding. Existing Transformer-based methods often suffer\nfrom redundant attention and suboptimal multi-modal alignment. To address these\nlimitations, we propose MLVTG, a novel framework that integrates two key\nmodules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba\nblocks as a backbone instead of Transformers to model temporal dependencies and\nextract robust video representations for multi-modal alignment. LLMRefiner\nleverages the specific frozen layer of a pre-trained Large Language Model (LLM)\nto implicitly transfer semantic priors, enhancing multi-modal alignment without\nfine-tuning. This dual alignment strategy, temporal modeling via structured\nstate-space dynamics and semantic purification via textual priors, enables more\nprecise localization. Extensive experiments on QVHighlights, Charades-STA, and\nTVSum demonstrate that MLVTG achieves state-of-the-art performance and\nsignificantly outperforms existing baselines.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08512v1", "AI": {"title_translation": "MLVTG：基于Mamba的特征对齐和LLM驱动的多模态视频时间定位纯化", "tldr": "MLVTG提出一种基于Mamba和LLM的新框架，用于多模态视频时间定位，解决了现有方法的冗余注意力和次优对齐问题，并取得了SOTA性能。", "motivation": "现有的基于Transformer的视频时间定位方法存在冗余注意力和次优的多模态对齐问题。", "method": "提出MLVTG框架，包含MambaAligner和LLMRefiner两个核心模块。MambaAligner使用Vision Mamba块作为骨干网络，建模时间依赖性并提取鲁棒的视频表示进行多模态对齐。LLMRefiner利用预训练LLM的特定冻结层隐式传递语义先验，无需微调即可增强多模态对齐。这种结合结构化状态空间动态和文本先验的双重对齐策略，实现了更精确的定位。", "result": "在QVHighlights、Charades-STA和TVSum数据集上取得了最先进的性能，并显著优于现有基线。", "conclusion": "MLVTG通过结合Mamba和LLM的独特方法，有效解决了视频时间定位中的挑战，并实现了卓越的性能。", "translation": "视频时间定位（VTG）旨在定位与自然语言查询对应的视频片段，是视频理解中一项基础但具有挑战性的任务。现有的基于Transformer的方法通常存在冗余注意力和次优的多模态对齐问题。为了解决这些限制，我们提出了MLVTG，一个整合了两个关键模块的新颖框架：MambaAligner和LLMRefiner。MambaAligner使用堆叠的Vision Mamba块作为骨干网络，而不是Transformer来建模时间依赖性并提取用于多模态对齐的鲁棒视频表示。LLMRefiner利用预训练大型语言模型（LLM）的特定冻结层来隐式传递语义先验，无需微调即可增强多模态对齐。这种双重对齐策略，即通过结构化状态空间动态进行时间建模和通过文本先验进行语义纯化，实现了更精确的定位。在QVHighlights、Charades-STA和TVSum上的大量实验表明，MLVTG取得了最先进的性能，并显著优于现有基线。", "summary": "本文针对视频时间定位（VTG）任务中现有Transformer方法面临的冗余注意力和次优多模态对齐问题，提出了MLVTG框架。MLVTG包含MambaAligner和LLMRefiner两个核心模块。MambaAligner利用Vision Mamba建模时间依赖并提取视频表示，LLMRefiner则通过LLM的冻结层引入语义先验以增强对齐。这种结合结构化状态空间动态和文本先验的双重对齐策略，有效提升了定位精度。实验结果表明，MLVTG在多个基准数据集上均达到了最先进的性能。", "keywords": "视频时间定位, Mamba, LLM, 多模态对齐, 视频理解", "comments": "这篇论文的创新点在于结合了Mamba架构来解决Transformer在时间建模中的冗余注意力问题，并通过利用预训练LLM的语义先验来提升多模态对齐，且无需微调LLM，这在计算效率和模型泛化性上具有优势。该方法为视频理解领域的多模态对齐提供了新的思路。"}}
{"id": "2506.08957", "title": "IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections", "authors": ["Yash Ranjan", "Rahul Sengupta", "Anand Rangarajan", "Sanjay Ranka"], "summary": "Traffic simulators are widely used to study the operational efficiency of\nroad infrastructure, but their rule-based approach limits their ability to\nmimic real-world driving behavior. Traffic intersections are critical\ncomponents of the road infrastructure, both in terms of safety risk (nearly 28%\nof fatal crashes and 58% of nonfatal crashes happen at intersections) as well\nas the operational efficiency of a road corridor. This raises an important\nquestion: can we create a data-driven simulator that can mimic the macro- and\nmicro-statistics of the driving behavior at a traffic intersection? Deep\nGenerative Modeling-based trajectory prediction models provide a good starting\npoint to model the complex dynamics of vehicles at an intersection. But they\nare not tested in a \"live\" micro-simulation scenario and are not evaluated on\ntraffic engineering-related metrics. In this study, we propose traffic\nengineering-related metrics to evaluate generative trajectory prediction models\nand provide a simulation-in-the-loop pipeline to do so. We also provide a\nmulti-headed self-attention-based trajectory prediction model that incorporates\nthe signal information, which outperforms our previous models on the evaluation\nmetrics.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08957v1", "AI": {"title_translation": "IntTrajSim：信号交叉口多车辆驾驶模拟的轨迹预测", "tldr": "本文提出IntTrajSim，一个用于在信号交叉口模拟多车辆驾驶轨迹的数据驱动预测模型。它通过引入交通工程相关指标、模拟循环评估流程以及基于多头自注意力机制的预测模型，解决了传统规则模拟器无法准确模仿真实驾驶行为的问题，并在评估中表现优异。", "motivation": "交通模拟器在研究道路基础设施效率方面广泛使用，但其基于规则的方法限制了模仿真实驾驶行为的能力。交通交叉口是道路基础设施的关键组成部分，涉及高安全风险和运行效率问题。现有的深度生成模型轨迹预测模型尚未在“实时”微观模拟场景中进行测试，也未根据交通工程相关指标进行评估，这促使研究人员寻求一个能够模仿交通交叉口驾驶行为宏观和微观统计数据的数据驱动模拟器。", "method": "本研究提出交通工程相关的指标来评估生成式轨迹预测模型，并提供一个“模拟循环”（simulation-in-the-loop）管道来实施评估。此外，还提出一个结合信号信息的多头自注意力机制的轨迹预测模型。", "result": "提出的多头自注意力轨迹预测模型在评估指标上优于之前的模型。", "conclusion": "Not mentioned in abstract", "translation": "交通模拟器被广泛用于研究道路基础设施的运行效率，但其基于规则的方法限制了它们模仿真实世界驾驶行为的能力。交通交叉口是道路基础设施的关键组成部分，无论是在安全风险（近28%的致命事故和58%的非致命事故发生在交叉口）还是道路走廊的运行效率方面。这提出了一个重要问题：我们能否创建一个数据驱动的模拟器，能够模仿交通交叉口驾驶行为的宏观和微观统计数据？基于深度生成模型的轨迹预测模型为建模交叉口车辆的复杂动态提供了一个良好的起点。但它们尚未在“实时”微观模拟场景中进行测试，也未根据交通工程相关指标进行评估。在本研究中，我们提出了交通工程相关的指标来评估生成式轨迹预测模型，并提供了一个“模拟循环”管道来完成这项工作。我们还提供了一个结合信号信息的多头自注意力机制的轨迹预测模型，该模型在评估指标上优于我们之前的模型。", "summary": "本文提出IntTrajSim，一个用于模拟信号交叉口多车辆驾驶的轨迹预测框架。针对传统交通模拟器在模仿真实驾驶行为上的局限性，作者提出了一套交通工程相关的评估指标和一个“模拟循环”评估管道。此外，还开发了一个融合信号信息的多头自注意力轨迹预测模型。实验结果表明，该模型在所提出的评估指标上表现优异，能更好地模拟交叉口复杂驾驶行为，从而推动数据驱动的交通模拟发展。", "keywords": "轨迹预测, 交通模拟, 信号交叉口, 深度生成模型, 自注意力机制", "comments": "本文的创新之处在于将先进的深度学习轨迹预测技术（特别是结合信号信息的多头自注意力机制）应用于交通模拟领域，并引入了专门针对交通工程的评估指标和“模拟循环”评估范式。这不仅提升了交通模拟器模仿真实世界驾驶行为的能力，也为交通规划、安全分析和智能交通系统开发提供了更精确的工具。其对交叉口复杂动态的关注和数据驱动方法的应用是其重要贡献。"}}
{"id": "2506.08240", "title": "Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations", "authors": ["Dongkyu Cho", "Rumi Chunara"], "summary": "Data augmentation is a promising tool for enhancing out-of-distribution\ngeneralization, where the key is to produce diverse, challenging variations of\nthe source domain via costly targeted augmentations that maximize its\ngeneralization effect. Conversely, random augmentation is inexpensive but is\ndeemed suboptimal due to its limited effect. In this paper, we revisit random\naugmentation and explore methods to address its shortcomings. We show that the\nstochastic nature of random augmentation can produce a set of colliding\naugmentations that distorts the learned features, similar to catastrophic\nforgetting. We propose a simple solution that improves the generalization\neffect of random augmentation by addressing forgetting, which displays strong\ngeneralization performance across various single source domain generalization\n(sDG) benchmarks.", "comment": "12 pages, 6 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08240v1", "AI": {"title_translation": "处理邪恶双胞胎：通过解决多样化增强的灾难性遗忘来改进随机增强", "tldr": "本文通过解决随机增强中的灾难性遗忘问题，显著提升了其泛化能力。", "motivation": "数据增强是提高分布外泛化能力的有效工具，但成本高昂的目标增强是关键。随机增强虽然成本低廉，但因效果有限而被认为是次优的。本文旨在解决随机增强的缺点，提高其泛化效果。", "method": "研究表明，随机增强的随机性会产生一组“冲突增强”，导致学习到的特征扭曲，类似于灾难性遗忘。本文提出了一种简单的解决方案，通过解决这种遗忘问题来改善随机增强的泛化效果。", "result": "所提出的解决方案在各种单一源域泛化（sDG）基准测试中显示出强大的泛化性能。", "conclusion": "通过解决随机增强中类似于灾难性遗忘的特征扭曲问题，可以显著提高其在分布外泛化任务中的性能。", "translation": "数据增强是增强分布外泛化的一种有前景的工具，其关键在于通过昂贵的目标增强来生成源域的多样化、具有挑战性的变体，从而最大限度地提高其泛化效果。相反，随机增强成本低廉，但因其效果有限而被认为是次优的。在本文中，我们重新审视了随机增强，并探索了解决其缺点的方法。我们表明，随机增强的随机性会产生一组冲突的增强，从而扭曲学习到的特征，类似于灾难性遗忘。我们提出了一种简单的解决方案，通过解决遗忘问题来提高随机增强的泛化效果，该方案在各种单一源域泛化（sDG）基准测试中表现出强大的泛化性能。", "summary": "本文重新审视了随机数据增强，指出其随机性可能导致类似于灾难性遗忘的特征扭曲问题。为解决这一问题，论文提出了一种简单的方法来改进随机增强的泛化能力，并在多个单一源域泛化基准测试中验证了其有效性，表明通过解决遗忘问题可以显著提升随机增强的性能。", "keywords": "随机增强, 灾难性遗忘, 数据增强, 域泛化, 分布外泛化", "comments": "这篇论文的创新点在于它挑战了随机增强效果不佳的普遍看法，并提出了一种简单有效的方法来改善其性能。通过将随机增强中的特征扭曲现象与灾难性遗忘联系起来，并提出解决方案，该研究为廉价且高效的分布外泛化提供了新的视角，具有重要的实践意义。"}}
{"id": "2506.08983", "title": "Online Learning Control Strategies for Industrial Processes with Application for Loosening and Conditioning", "authors": ["Yue Wu", "Jianfu Cao", "Ye Cao"], "summary": "This paper proposes a novel adaptive Koopman Model Predictive Control (MPC)\nframework, termed HPC-AK-MPC, designed to address the dual challenges of\ntime-varying dynamics and safe operation in complex industrial processes. The\nframework integrates two core strategies: online learning and\nhistorically-informed safety constraints. To contend with process\ntime-variance, a Recursive Extended Dynamic Mode Decomposition (rEDMDc)\ntechnique is employed to construct an adaptive Koopman model capable of\nupdating its parameters from real-time data, endowing the controller with the\nability to continuously learn and track dynamic changes. To tackle the critical\nissue of safe operation under model uncertainty, we introduce a novel\nHistorical Process Constraint (HPC) mechanism. This mechanism mines successful\noperational experiences from a historical database and, by coupling them with\nthe confidence level of the online model, generates a dynamic \"safety corridor\"\nfor the MPC optimization problem. This approach transforms implicit expert\nknowledge into explicit, adaptive constraints, establishing a dynamic balance\nbetween pursuing optimal performance and ensuring robust safety. The proposed\nHPC-AK-MPC method is applied to a real-world tobacco loosening and conditioning\nprocess and systematically validated using an \"advisor mode\" simulation\nframework with industrial data. Experimental results demonstrate that, compared\nto historical operations, the proposed method significantly improves the\nProcess Capability Index (Cpk) for key quality variables across all tested\nbatches, proving its substantial potential in enhancing control performance\nwhile guaranteeing operational safety.", "comment": "19pages,6figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.08983v1", "AI": {"title_translation": "工业过程在线学习控制策略及其在松散和调节中的应用", "tldr": "本文提出了一种结合在线学习和历史安全约束的自适应Koopman模型预测控制框架（HPC-AK-MPC），用于解决复杂工业过程中的时变动态和安全操作挑战，并在烟草加工中取得了显著性能提升和安全保障。", "motivation": "解决复杂工业过程中时变动态和安全操作的双重挑战。", "method": "本文提出了一种名为HPC-AK-MPC的新型自适应Koopman模型预测控制框架。该框架整合了两种核心策略：在线学习和历史信息安全约束。通过递归扩展动态模式分解（rEDMDc）技术构建自适应Koopman模型，使其能够从实时数据更新参数以应对过程时变性。引入历史过程约束（HPC）机制，从历史数据库中挖掘成功的操作经验，并结合在线模型的置信度，生成动态“安全走廊”用于MPC优化问题，从而将隐式专家知识转化为显式的自适应约束。", "result": "所提出的HPC-AK-MPC方法应用于真实的烟草松散和调节过程，并通过“顾问模式”仿真框架和工业数据进行了系统验证。实验结果表明，与历史操作相比，该方法显著提高了所有测试批次关键质量变量的过程能力指数（Cpk）。", "conclusion": "该方法在增强控制性能的同时保证了操作安全性方面显示出巨大的潜力。", "translation": "本文提出了一种新颖的自适应Koopman模型预测控制（MPC）框架，称为HPC-AK-MPC，旨在解决复杂工业过程中时变动态和安全操作的双重挑战。该框架整合了两种核心策略：在线学习和历史信息安全约束。为了应对过程时变性，采用了递归扩展动态模式分解（rEDMDc）技术来构建一个自适应Koopman模型，该模型能够从实时数据更新其参数，赋予控制器持续学习和跟踪动态变化的能力。为了解决模型不确定性下安全操作的关键问题，我们引入了一种新颖的历史过程约束（HPC）机制。该机制从历史数据库中挖掘成功的操作经验，并通过将其与在线模型的置信水平耦合，为MPC优化问题生成一个动态的“安全走廊”。这种方法将隐式专家知识转化为显式的自适应约束，在追求最佳性能和确保鲁棒安全性之间建立了动态平衡。所提出的HPC-AK-MPC方法应用于真实的烟草松散和调节过程，并使用工业数据通过“顾问模式”仿真框架进行了系统验证。实验结果表明，与历史操作相比，所提出的方法显著提高了所有测试批次关键质量变量的过程能力指数（Cpk），证明了其在增强控制性能同时保证操作安全性方面的巨大潜力。", "summary": "本文提出了一种名为HPC-AK-MPC的自适应Koopman模型预测控制框架，旨在解决工业过程中的时变动态和安全操作问题。该框架结合了使用rEDMDc的在线学习机制和基于历史数据的安全约束（HPC），以实现模型参数的自适应更新和动态安全边界的生成。实验在真实的烟草松散和调节过程中进行，结果表明该方法显著提高了过程能力指数（Cpk），证明了其在提升控制性能和保障操作安全方面的有效性。", "keywords": "Koopman模型预测控制, 在线学习, 安全约束, 工业过程, 过程能力指数", "comments": "本文的创新点在于将在线学习的Koopman模型预测控制与基于历史经验的安全约束相结合，有效地解决了工业过程中模型不确定性和安全操作的挑战。通过将隐式专家知识转化为显式约束，提供了一种新颖且实用的方法来平衡性能优化和安全保障。其在实际工业数据上的验证增加了研究的可信度。"}}
{"id": "2506.08460", "title": "MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning", "authors": ["Yihong Guo", "Yu Yang", "Pan Xu", "Anqi Liu"], "summary": "We study the off-dynamics offline reinforcement learning problem, where the\ngoal is to learn a policy from offline datasets collected from source and\ntarget domains with mismatched transition. Existing off-dynamics offline RL\nmethods typically either filter source transitions that resemble those of the\ntarget domain or apply reward augmentation to source data, both constrained by\nthe limited transitions available from the target domain. As a result, the\nlearned policy is unable to explore target domain beyond the offline datasets.\nWe propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that\naddresses this limitation by enabling exploration of the target domain via\nlearned dynamics. MOBODY generates new synthetic transitions in the target\ndomain through model rollouts, which are used as data augmentation during\noffline policy learning. Unlike existing model-based methods that learn\ndynamics from a single domain, MOBODY tackles the challenge of mismatched\ndynamics by leveraging both source and target datasets. Directly merging these\ndatasets can bias the learned model toward source dynamics. Instead, MOBODY\nlearns target dynamics by discovering a shared latent representation of states\nand transitions across domains through representation learning. To stabilize\ntraining, MOBODY incorporates a behavior cloning loss that regularizes the\npolicy. Specifically, we introduce a Q-weighted behavior cloning loss that\nregularizes the policy toward actions with high target-domain Q-values, rather\nthan uniformly imitating all actions in the dataset. These Q-values are learned\nfrom an enhanced target dataset composed of offline target data, augmented\nsource data, and rollout data from the learned target dynamics. We evaluate\nMOBODY on MuJoCo benchmarks and show that it significantly outperforms\nstate-of-the-art baselines, with especially pronounced improvements in\nchallenging scenarios.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08460v1", "AI": {"title_translation": "MOBODY：基于模型的离线异动力学强化学习", "tldr": "MOBODY是一种基于模型的离线异动力学强化学习算法，通过学习跨域共享的潜在动力学并在目标域生成合成数据，显著提高了在目标域探索和性能。", "motivation": "现有的离线异动力学强化学习方法受限于目标域有限的转换数据，导致学习到的策略无法在离线数据集之外探索目标域。", "method": "MOBODY通过学习目标域的动力学模型，利用模型rollout生成新的合成转换数据进行数据增强。它通过学习跨域（源域和目标域）状态和转换的共享潜在表示来应对不匹配的动力学问题，避免直接合并数据集的偏差。此外，MOBODY引入了一种Q值加权的模仿学习损失来稳定训练，该损失将策略正则化到具有高目标域Q值的动作。", "result": "MOBODY在MuJoCo基准测试中显著优于最先进的基线方法，尤其在具有挑战性的场景中表现出更明显的改进。", "conclusion": "MOBODY有效解决了离线异动力学强化学习中目标域探索受限的问题，通过其创新的动力学学习和策略正则化方法，在性能上超越了现有技术。", "translation": "我们研究了异动力学离线强化学习问题，其目标是从源域和目标域收集的离线数据集中学习策略，这两个域具有不匹配的转换。现有的异动力学离线强化学习方法通常要么过滤与目标域相似的源转换，要么对源数据应用奖励增强，这两种方法都受限于目标域可用的有限转换。因此，学习到的策略无法在离线数据集之外探索目标域。我们提出了MOBODY，一种基于模型的离线异动力学强化学习算法，通过学习到的动力学实现对目标域的探索，从而解决了这一限制。MOBODY通过模型rollout在目标域中生成新的合成转换，这些转换在离线策略学习期间用作数据增强。与现有仅从单一域学习动力学的模型基方法不同，MOBODY通过利用源域和目标域数据集来解决不匹配动力学的挑战。直接合并这些数据集可能会使学习到的模型偏向源动力学。相反，MOBODY通过表示学习发现跨域状态和转换的共享潜在表示来学习目标动力学。为了稳定训练，MOBODY结合了行为克隆损失来正则化策略。具体来说，我们引入了一种Q值加权的行为克隆损失，它将策略正则化到具有高目标域Q值的动作，而不是统一模仿数据集中的所有动作。这些Q值是从一个增强的目标数据集中学习的，该数据集由离线目标数据、增强的源数据以及从学习到的目标动力学中获得的rollout数据组成。我们在MuJoCo基准测试中评估了MOBODY，结果表明它显著优于最先进的基线方法，在具有挑战性的场景中表现出尤其显著的改进。", "summary": "本文提出了MOBODY，一种基于模型的离线异动力学强化学习算法，旨在解决源域和目标域转换不匹配的问题。现有方法因目标域数据稀缺而限制了策略探索。MOBODY通过学习跨域共享的潜在动力学来生成目标域的合成转换数据，从而实现目标域的探索。为了稳定训练，它引入了Q值加权的模仿学习损失，将策略导向高Q值的动作。实验结果表明，MOBODY在MuJoCo基准测试中显著优于现有SOTA方法。", "keywords": "离线强化学习, 异动力学, 模型基方法, 领域适应, 行为克隆", "comments": "MOBODY的创新之处在于其独特的跨域动力学学习方法，通过共享潜在表示来处理异动力学，避免了传统方法中直接合并数据集带来的偏差。此外，Q值加权的行为克隆损失也为策略正则化提供了新的思路，使其能够更有效地利用增强的数据集。这对于解决离线RL中数据稀缺和域适应的挑战具有重要意义。"}}
{"id": "2506.08526", "title": "Robust Visual Localization via Semantic-Guided Multi-Scale Transformer", "authors": ["Zhongtao Tian", "Wenhao Huang", "Zhidong Chen", "Xiao Wei Sun"], "summary": "Visual localization remains challenging in dynamic environments where\nfluctuating lighting, adverse weather, and moving objects disrupt appearance\ncues. Despite advances in feature representation, current absolute pose\nregression methods struggle to maintain consistency under varying conditions.\nTo address this challenge, we propose a framework that synergistically combines\nmulti-scale feature learning with semantic scene understanding. Our approach\nemploys a hierarchical Transformer with cross-scale attention to fuse geometric\ndetails and contextual cues, preserving spatial precision while adapting to\nenvironmental changes. We improve the performance of this architecture with\nsemantic supervision via neural scene representation during training, guiding\nthe network to learn view-invariant features that encode persistent structural\ninformation while suppressing complex environmental interference. Experiments\non TartanAir demonstrate that our approach outperforms existing pose regression\nmethods in challenging scenarios with dynamic objects, illumination changes,\nand occlusions. Our findings show that integrating multi-scale processing with\nsemantic guidance offers a promising strategy for robust visual localization in\nreal-world dynamic environments.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08526v1", "AI": {"title_translation": "语义引导多尺度Transformer的鲁棒视觉定位", "tldr": "提出了一种结合多尺度特征学习和语义场景理解的Transformer框架，通过语义监督学习视图不变特征，在动态环境中实现了鲁棒的视觉定位，优于现有方法。", "motivation": "视觉定位在光照波动、恶劣天气和移动物体等动态环境中仍具挑战性，现有绝对姿态回归方法难以在不同条件下保持一致性。", "method": "提出一个结合多尺度特征学习和语义场景理解的框架。采用带有跨尺度注意力的分层Transformer来融合几何细节和上下文线索。通过训练期间的神经场景表示进行语义监督，引导网络学习视图不变特征，编码持久结构信息并抑制环境干扰。", "result": "在TartanAir数据集上的实验表明，该方法在动态物体、光照变化和遮挡等挑战性场景中优于现有姿态回归方法。", "conclusion": "将多尺度处理与语义引导相结合，为真实世界动态环境中的鲁棒视觉定位提供了一个有前景的策略。", "translation": "视觉定位在动态环境中仍然具有挑战性，其中光照波动、恶劣天气和移动物体会干扰外观线索。尽管特征表示取得了进展，但当前的绝对姿态回归方法在不同条件下难以保持一致性。为了解决这一挑战，我们提出了一个协同结合多尺度特征学习和语义场景理解的框架。我们的方法采用带有跨尺度注意力的分层Transformer来融合几何细节和上下文线索，在保持空间精度的同时适应环境变化。我们通过训练期间的神经场景表示进行语义监督来提高该架构的性能，指导网络学习编码持久结构信息并抑制复杂环境干扰的视图不变特征。在TartanAir上的实验表明，我们的方法在动态物体、光照变化和遮挡等挑战性场景中优于现有的姿态回归方法。我们的发现表明，将多尺度处理与语义引导相结合，为真实世界动态环境中的鲁棒视觉定位提供了一个有前景的策略。", "summary": "该论文提出了一种名为“语义引导多尺度Transformer”的新型框架，旨在解决动态环境下视觉定位的鲁棒性问题。该框架结合了多尺度特征学习和语义场景理解，利用分层Transformer与跨尺度注意力来融合几何和上下文信息。通过语义监督，网络能够学习视图不变特征，从而在光照变化、恶劣天气和移动物体等复杂环境中实现更稳定的定位。实验结果表明，该方法在挑战性场景中显著优于现有姿态回归方法。", "keywords": "视觉定位, 语义引导, 多尺度Transformer, 鲁棒性, 姿态回归", "comments": "该论文的创新点在于将多尺度Transformer与语义监督相结合，通过学习视图不变特征来增强视觉定位在动态环境中的鲁棒性。这种结合几何和语义信息的方法有效地解决了传统方法在复杂环境下的局限性，为实际应用中的视觉定位提供了新的思路和更可靠的解决方案。"}}
{"id": "2506.08963", "title": "Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics", "authors": ["Yash Ranjan", "Rahul Sengupta", "Anand Rangarajan", "Sanjay Ranka"], "summary": "Traffic Intersections are vital to urban road networks as they regulate the\nmovement of people and goods. However, they are regions of conflicting\ntrajectories and are prone to accidents. Deep Generative models of traffic\ndynamics at signalized intersections can greatly help traffic authorities\nbetter understand the efficiency and safety aspects. At present, models are\nevaluated on computational metrics that primarily look at trajectory\nreconstruction errors. They are not evaluated online in a `live'\nmicrosimulation scenario. Further, these metrics do not adequately consider\ntraffic engineering-specific concerns such as red-light violations, unallowed\nstoppage, etc. In this work, we provide a comprehensive analytics tool to\ntrain, run, and evaluate models with metrics that give better insights into\nmodel performance from a traffic engineering point of view. We train a\nstate-of-the-art multi-vehicle trajectory forecasting model on a large dataset\ncollected by running a calibrated scenario of a real-world urban intersection.\nWe then evaluate the performance of the prediction models, online in a\nmicrosimulator, under unseen traffic conditions. We show that despite using\nideally-behaved trajectories as input, and achieving low trajectory\nreconstruction errors, the generated trajectories show behaviors that break\ntraffic rules. We introduce new metrics to evaluate such undesired behaviors\nand present our results.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08963v1", "AI": {"title_translation": "评估交通路口动态的生成式车辆轨迹模型", "tldr": "现有生成式车辆轨迹模型评估方法不足，本研究提出一种新的分析工具和交通工程视角下的评估指标，发现即使模型重构误差低，生成的轨迹仍可能违反交通规则。", "motivation": "现有的交通轨迹生成模型评估主要依赖轨迹重构误差，但未在实时微观仿真场景中进行在线评估，也未充分考虑闯红灯、违规停车等交通工程特定问题。", "method": "本研究提供了一个全面的分析工具，用于训练、运行和评估模型。该工具使用从交通工程角度提供更好洞察力的新指标。作者在一个真实城市路口的校准场景数据集上训练了一个最先进的多车辆轨迹预测模型，并在微观仿真器中、在未见过的交通条件下在线评估了其性能。", "result": "尽管使用理想行为轨迹作为输入并实现了低轨迹重构误差，但生成的轨迹仍表现出违反交通规则的行为。", "conclusion": "本文引入了新的指标来评估这些不期望的行为，并展示了相关结果，强调了从交通工程角度评估生成模型的重要性。", "translation": "交通路口对于城市道路网络至关重要，因为它们调节着人流和物流的运动。然而，它们是轨迹冲突的区域，容易发生事故。信号交叉口交通动态的深度生成模型可以极大地帮助交通管理部门更好地理解效率和安全方面。目前，模型主要通过轨迹重建误差等计算指标进行评估。它们没有在“实时”微观仿真场景中进行在线评估。此外，这些指标没有充分考虑交通工程特定的问题，例如闯红灯、不允许停车等。在这项工作中，我们提供了一个全面的分析工具，用于训练、运行和评估模型，其指标可以从交通工程角度更好地洞察模型性能。我们使用在真实城市交叉口校准场景中收集的大型数据集训练了一个最先进的多车辆轨迹预测模型。然后，我们在微观仿真器中、在未见过的交通条件下在线评估了预测模型的性能。我们表明，尽管使用理想行为轨迹作为输入，并实现了低轨迹重建误差，但生成的轨迹仍表现出违反交通规则的行为。我们引入了新的指标来评估这些不期望的行为，并展示了我们的结果。", "summary": "本文旨在解决现有生成式车辆轨迹模型评估方法在交通路口动态方面存在的不足。研究人员开发了一个全面的分析工具，该工具包含从交通工程角度出发的新评估指标。通过在一个真实路口数据集上训练最先进的轨迹预测模型，并在微观仿真器中进行在线评估，发现即使模型在轨迹重构上表现良好，其生成的轨迹仍可能违反交通规则。因此，本文引入了专门用于评估这类不期望行为的新指标。", "keywords": "车辆轨迹模型, 交通路口, 生成式模型, 在线评估, 交通工程指标", "comments": "这篇论文的创新点在于强调了生成式轨迹模型在实际交通工程应用中面临的挑战，即模型即使在传统计算指标上表现良好，也可能生成不符合交通规则的轨迹。通过引入在线微观仿真评估和交通工程特定的新指标，为未来轨迹生成模型的开发和评估提供了更全面、更实用的方向。"}}
{"id": "2506.08243", "title": "Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic", "authors": ["Zhenjiang Mao", "Artem Bisliouk", "Rohith Reddy Nama", "Ivan Ruchkin"], "summary": "Large Language Models (LLMs) have shown impressive performance in\nmathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.\nHowever, they tend to produce highly confident yet incorrect outputs, which\nposes significant risks in domains like education, where users may lack the\nexpertise to assess reasoning steps. To address this, we propose a structured\nframework that models stepwise confidence as a temporal signal and evaluates it\nusing Signal Temporal Logic (STL). In particular, we define formal STL-based\nconstraints to capture desirable temporal properties and compute robustness\nscores that serve as structured, interpretable confidence estimates. Our\napproach also introduces a set of uncertainty reshaping strategies to enforce\nsmoothness, monotonicity, and causal consistency across the reasoning\ntrajectory. Experiments show that our approach consistently improves\ncalibration metrics and provides more reliable uncertainty estimates than\nconventional confidence aggregation and post-hoc calibration.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08243v1", "AI": {"title_translation": "置信度的时序化：使用信号时序逻辑评估思维链推理", "tldr": "本研究提出了一种使用信号时序逻辑（STL）评估思维链（CoT）推理中逐步置信度的方法，以解决大型语言模型（LLM）过度自信的问题，并实验证明其能提高校准和不确定性估计的可靠性。", "motivation": "大型语言模型（LLMs）在数学推理任务中使用思维链（CoT）提示时表现出色，但它们倾向于产生高度自信但错误的输出，这在教育等领域带来了重大风险，因为用户可能缺乏评估推理步骤的专业知识。", "method": "本研究提出了一个结构化框架，将逐步置信度建模为时序信号，并使用信号时序逻辑（STL）进行评估。具体来说，定义了基于STL的正式约束来捕捉期望的时序属性，并计算作为结构化、可解释置信度估计的鲁棒性分数。该方法还引入了一组不确定性重塑策略，以在推理轨迹中强制执行平滑性、单调性和因果一致性。", "result": "实验表明，该方法始终改进了校准指标，并提供了比传统置信度聚合和事后校准更可靠的不确定性估计。", "conclusion": "该方法通过将逐步置信度建模为时序信号并使用信号时序逻辑进行评估，有效地解决了大型语言模型在思维链推理中过度自信的问题，从而提供了更可靠的不确定性估计和改进的校准。", "translation": "大型语言模型（LLMs）在思维链（CoT）提示的引导下，在数学推理任务中展现出令人印象深刻的性能。然而，它们倾向于产生高度自信但错误的输出，这在教育等领域带来了重大风险，因为用户可能缺乏评估推理步骤的专业知识。为了解决这个问题，我们提出了一个结构化框架，将逐步置信度建模为时序信号，并使用信号时序逻辑（STL）对其进行评估。特别是，我们定义了基于STL的正式约束来捕捉期望的时序属性，并计算作为结构化、可解释置信度估计的鲁棒性分数。我们的方法还引入了一组不确定性重塑策略，以在推理轨迹中强制执行平滑性、单调性和因果一致性。实验表明，我们的方法始终改进了校准指标，并提供了比传统置信度聚合和事后校准更可靠的不确定性估计。", "summary": "本论文提出了一个新颖的框架，通过将大型语言模型（LLMs）在思维链（CoT）推理中的逐步置信度视为时序信号，并利用信号时序逻辑（STL）进行评估，以解决LLMs过度自信的问题。该框架定义了STL约束以捕捉理想的时序属性，计算可解释的鲁棒性分数作为置信度估计，并引入不确定性重塑策略以确保推理轨迹的平滑性、单调性和因果一致性。实验结果表明，该方法显著改善了校准指标，并提供了比现有方法更可靠的不确定性估计。", "keywords": "大型语言模型, 思维链, 信号时序逻辑, 置信度校准, 不确定性估计", "comments": "这项研究的创新之处在于将时序逻辑（STL）应用于评估大型语言模型思维链推理中的逐步置信度，这提供了一种新颖且形式化的方法来解决LLM过度自信的挑战。通过将置信度视为时序信号并引入不确定性重塑策略，该方法为提高LLM在关键应用领域（如教育）的可靠性和可信度提供了一个有价值的工具。其提出的结构化、可解释的置信度估计也为LLM的调试和改进提供了新的视角。"}}
{"id": "2506.08529", "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\\times$RTX 4090s", "authors": ["Xijun Wang", "Xin Li", "Bingchen Li", "Zhibo Chen"], "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.", "comment": "Project page: https://kopperx.github.io/projects/liftvsr", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08529v1", "AI": {"title_translation": "LiftVSR：通过混合时间建模将图像扩散提升至视频超分辨率，仅需4块RTX 4090显卡", "tldr": "LiftVSR是一个高效的视频超分辨率（VSR）框架，它利用图像扩散（PixArt-$\\alpha$）并引入了混合时间建模（DTA+AMC），仅使用4块RTX 4090显卡就实现了最先进的性能，解决了现有VSR方法计算成本高昂和时间连贯性有限的问题。", "motivation": "现有扩散模型在视频超分辨率（VSR）中面临时间连贯性有限和计算成本过高（通常需要超过8块NVIDIA A100-80G GPU）的问题，尤其是在处理长视频时。", "method": "本文提出了LiftVSR，一个高效的VSR框架，利用PixArt-$\\alpha$的图像扩散先验。它引入了一种混合时间建模机制，包含：(i) 动态时间注意力（DTA），用于短帧段内的细粒度时间建模；(ii) 注意力记忆缓存（AMC），用于跨段的长期时间建模。此外，还引入了非对称采样策略以稳定推理过程中的缓存交互并减轻特征不匹配。", "result": "LiftVSR仅使用4块RTX 4090 GPU就达到了最先进的视频超分辨率结果。在多个典型VSR基准测试中，LiftVSR以显著更低的计算成本实现了令人印象深刻的性能。", "conclusion": "LiftVSR有效地解决了基于扩散的VSR方法中时间连贯性不足和计算成本过高的问题，以高效率展现了最先进的性能。", "translation": "扩散模型通过精心设计的时间建模来确保帧间一致性，显著提升了视频超分辨率（VSR）的感知质量。然而，现有方法通常存在时间连贯性有限和计算成本过高（例如，通常需要超过8块NVIDIA A100-80G GPU）的问题，特别是对于长视频。在这项工作中，我们提出了LiftVSR，一个高效的VSR框架，它利用并提升了PixArt-$\\alpha$的图像扩散先验，仅使用4块RTX 4090 GPU就达到了最先进的结果。为了平衡长期一致性和效率，我们引入了一种混合时间建模机制，将时间学习分解为两个互补的组件：(i) 动态时间注意力（DTA），用于短帧段内的细粒度时间建模（即，低复杂性），以及 (ii) 注意力记忆缓存（AMC），用于跨段的长期时间建模（即，一致性）。具体来说，DTA在多头查询和键令牌中识别跨帧的多个令牌流，以在值令牌中扭曲帧间上下文。AMC通过缓存单元自适应地聚合历史片段信息，以最小的开销确保长期连贯性。为了进一步稳定推理过程中的缓存交互，我们引入了一种非对称采样策略，以减轻由不同扩散采样步骤引起的特征不匹配。在几个典型的VSR基准上进行的广泛实验表明，LiftVSR以显著降低的计算成本实现了令人印象深刻的性能。", "summary": "LiftVSR是一个高效的视频超分辨率框架，它利用图像扩散（PixArt-$\\alpha$）来克服现有方法计算成本高昂和时间连贯性有限的缺点。该框架引入了一种混合时间建模方法，结合了用于短期细粒度一致性的动态时间注意力（DTA）和用于长期连贯性的注意力记忆缓存（AMC），并辅以非对称采样策略。LiftVSR仅使用4块RTX 4090显卡就达到了最先进的结果，并显著降低了计算资源需求。", "keywords": "视频超分辨率, 扩散模型, 时间建模, 计算效率, LiftVSR", "comments": "LiftVSR的创新点在于其混合时间建模机制（DTA和AMC）以及非对称采样策略，有效地平衡了长期时间一致性与计算效率。其重要性在于，通过显著降低计算成本（仅需4块RTX 4090），使得高感知质量的视频超分辨率技术变得更加实用和可及，解决了基于扩散的VSR在实际应用中的一个关键限制。"}}
{"id": "2506.08970", "title": "A Survey of Link Prediction in N-ary Knowledge Graphs", "authors": ["Jiyao Wei", "Saiping Guan", "Da Li", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "summary": "N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph\ndesigned to efficiently represent complex real-world facts. Unlike traditional\nknowledge graphs, where a fact typically involves two entities, NKGs can\ncapture n-ary facts containing more than two entities. Link prediction in NKGs\naims to predict missing elements within these n-ary facts, which is essential\nfor completing NKGs and improving the performance of downstream applications.\nThis task has recently gained significant attention. In this paper, we present\nthe first comprehensive survey of link prediction in NKGs, providing an\noverview of the field, systematically categorizing existing methods, and\nanalyzing their performance and application scenarios. We also outline\npromising directions for future research.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.08970v1", "AI": {"title_translation": "N元知识图谱中链接预测的综述", "tldr": "这是一篇关于N元知识图谱中链接预测的首次全面综述，旨在概述该领域、分类现有方法并指出未来研究方向。", "motivation": "N元知识图谱（NKGs）能够表示复杂的现实世界事实，而链接预测在NKGs中对于补全缺失元素、提高下游应用性能至关重要。由于该任务最近受到了广泛关注，因此需要一篇全面的综述。", "method": "本文首次全面综述了N元知识图谱中的链接预测，系统地分类了现有方法，并分析了它们的性能和应用场景。", "result": "综述提供了该领域的概述，对现有方法进行了系统分类，分析了它们的性能和应用场景，并指出了未来有前景的研究方向。", "conclusion": "N元知识图谱中的链接预测是一个重要且日益受到关注的领域，本文通过提供全面的综述，为该领域的理解、现有方法的评估以及未来研究方向的探索奠定了基础。", "translation": "N元知识图谱（NKGs）是一种专门的知识图谱，旨在高效表示复杂的现实世界事实。与传统知识图谱中事实通常涉及两个实体不同，NKGs可以捕获包含两个以上实体的N元事实。NKGs中的链接预测旨在预测这些N元事实中的缺失元素，这对于补全NKGs和提高下游应用程序的性能至关重要。这项任务最近受到了广泛关注。在本文中，我们首次对NKGs中的链接预测进行了全面综述，提供了该领域的概述，系统地分类了现有方法，并分析了它们的性能和应用场景。我们还概述了未来研究的有前景方向。", "summary": "本文首次全面综述了N元知识图谱（NKGs）中的链接预测任务。NKGs能够表示包含多于两个实体的复杂N元事实，而链接预测对于补全NKGs和提升下游应用性能至关重要。该综述系统地分类了现有链接预测方法，分析了它们的性能和应用场景，并指出了未来的研究方向。", "keywords": "N元知识图谱, 链接预测, 知识图谱, 综述, 复杂事实", "comments": "这篇论文作为N元知识图谱中链接预测领域的首次全面综述，具有重要意义，它为研究人员提供了该领域的结构化概览、现有方法的分类及性能分析，并指明了未来的研究方向，对该领域的进一步发展具有指导作用。"}}
{"id": "2506.08244", "title": "Parameter-free approximate equivariance for tasks with finite group symmetry", "authors": ["Riccardo Ali", "Pietro Liò", "Jamie Vicary"], "summary": "Equivariant neural networks incorporate symmetries through group actions,\nembedding them as an inductive bias to improve performance on a wide variety of\ntasks. However, existing equivariant methods can be computationally intensive,\nwith high parameter counts, and are often tied to a specific architecture. We\npropose a simple zero-parameter approach that imposes approximate equivariance\nfor a finite group in the latent representation, as an additional term in the\nloss function. We conduct experiments which allow the network to learn a group\nrepresentation on the latent space, and show in every case it prefers to learn\nthe regular representation. Fixing this action on the latent space, this yields\na simple method to impose approximate equivariance as an additional loss\npenalty. We benchmark our approach on three datasets and compare it against\nseveral existing equivariant methods, showing that in many cases it achieves\nsimilar or better performance for a fraction of the parameters.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08244v1", "AI": {"title_translation": "有限群对称任务的无参数近似等变性", "tldr": "提出了一种无参数的近似等变方法，通过损失函数项在潜在表示中施加有限群对称性，在减少参数的同时实现了有竞争力的性能。", "motivation": "现有的等变神经网络计算量大、参数多，并且通常与特定架构绑定。", "method": "提出了一种简单的零参数方法，通过在损失函数中添加一个项，在潜在表示中对有限群施加近似等变性。该方法允许网络在潜在空间学习群表示，并发现它倾向于学习正则表示。", "result": "实验表明网络倾向于学习正则表示。在三个数据集上与现有等变方法进行基准测试，结果显示在许多情况下，该方法以更少的参数实现了相似或更好的性能。", "conclusion": "该研究提出了一种简单、无参数的方法来施加近似等变性，通过在潜在空间中学习群表示并将其作为损失惩罚项，有效地解决了现有等变方法的计算和参数效率问题，并在多个任务上展现出有竞争力的性能。", "translation": "等变神经网络通过群作用融入对称性，将其作为归纳偏置以提高各种任务的性能。然而，现有的等变方法可能计算密集，参数量大，并且通常与特定架构绑定。我们提出了一种简单的零参数方法，通过在损失函数中增加一个额外项，在潜在表示中对有限群施加近似等变性。我们进行了实验，允许网络在潜在空间上学习群表示，并表明在每种情况下它都倾向于学习正则表示。固定潜在空间上的这种作用，这提供了一种简单的方法来将近似等变性作为额外的损失惩罚。我们在三个数据集上对我们的方法进行了基准测试，并将其与几种现有等变方法进行比较，结果表明在许多情况下，它以一小部分参数实现了相似或更好的性能。", "summary": "这篇论文提出了一种创新的无参数方法，通过在潜在表示中引入一个损失项来强制实现有限群的近似等变性。该方法解决了现有等变神经网络计算开销大和参数量多的问题。实验证明，该网络倾向于学习正则表示，并且在多个基准测试中，与现有方法相比，它以更少的参数实现了相似或更优的性能。", "keywords": "等变神经网络, 近似等变性, 无参数, 损失函数, 群对称性", "comments": "这项工作的主要创新在于提出了一个无参数的近似等变方法，通过修改损失函数而不是网络架构来引入对称性，这显著降低了计算复杂性和参数量。其重要性在于提供了一个更高效、更通用的等变学习范式，摆脱了特定架构的限制。在某些情况下，其性能甚至优于参数更多的现有方法，这表明了其在实际应用中的巨大潜力。"}}
{"id": "2506.08463", "title": "How to Provably Improve Return Conditioned Supervised Learning?", "authors": ["Zhishuai Liu", "Yu Yang", "Ruhan Wang", "Pan Xu", "Dongruo Zhou"], "summary": "In sequential decision-making problems, Return-Conditioned Supervised\nLearning (RCSL) has gained increasing recognition for its simplicity and\nstability in modern decision-making tasks. Unlike traditional offline\nreinforcement learning (RL) algorithms, RCSL frames policy learning as a\nsupervised learning problem by taking both the state and return as input. This\napproach eliminates the instability often associated with temporal difference\n(TD) learning in offline RL. However, RCSL has been criticized for lacking the\nstitching property, meaning its performance is inherently limited by the\nquality of the policy used to generate the offline dataset. To address this\nlimitation, we propose a principled and simple framework called Reinforced\nRCSL. The key innovation of our framework is the introduction of a concept we\ncall the in-distribution optimal return-to-go. This mechanism leverages our\npolicy to identify the best achievable in-dataset future return based on the\ncurrent state, avoiding the need for complex return augmentation techniques.\nOur theoretical analysis demonstrates that Reinforced RCSL can consistently\noutperform the standard RCSL approach. Empirical results further validate our\nclaims, showing significant performance improvements across a range of\nbenchmarks.", "comment": "25 pages, 4 figures, 12 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08463v1", "AI": {"title_translation": "如何证明性地改进基于回报的监督学习？", "tldr": "提出Reinforced RCSL框架，通过引入in-distribution optimal return-to-go机制，克服了传统RCSL性能受限于数据集质量的缺点，并证明其优于标准RCSL。", "motivation": "传统的回报条件监督学习（RCSL）虽然简单稳定，但缺乏“拼接”特性，其性能受限于生成离线数据集的策略质量。", "method": "提出了一个名为Reinforced RCSL的原则性简单框架。其关键创新是引入了“in-distribution optimal return-to-go”概念，该机制利用策略识别当前状态下数据集中可实现的最优未来回报，避免了复杂的回报增强技术。", "result": "理论分析表明Reinforced RCSL能持续优于标准RCSL方法。实证结果进一步验证了这些主张，在多个基准测试中显示出显著的性能改进。", "conclusion": "Reinforced RCSL通过引入in-distribution optimal return-to-go机制，有效解决了传统RCSL的性能局限性，并在理论和实践上证明了其优越性。", "translation": "在顺序决策问题中，回报条件监督学习（RCSL）因其在现代决策任务中的简单性和稳定性而受到越来越多的认可。与传统的离线强化学习（RL）算法不同，RCSL通过将状态和回报都作为输入，将策略学习框定为一个监督学习问题。这种方法消除了离线RL中通常与时序差分（TD）学习相关的不稳定性。然而，RCSL因缺乏拼接特性而受到批评，这意味着其性能本质上受限于用于生成离线数据集的策略质量。为了解决这一限制，我们提出了一个原则性且简单的框架，称为Reinforced RCSL。我们框架的关键创新是引入了一个我们称之为“in-distribution optimal return-to-go”的概念。这种机制利用我们的策略来识别基于当前状态在数据集中可实现的最佳未来回报，避免了对复杂回报增强技术的需要。我们的理论分析表明，Reinforced RCSL可以持续优于标准RCSL方法。实证结果进一步验证了我们的主张，在多个基准测试中显示出显著的性能改进。", "summary": "回报条件监督学习（RCSL）在顺序决策问题中因其简单和稳定而受到认可，但其性能受限于离线数据集的质量。为解决这一局限性，本文提出了Reinforced RCSL框架，核心创新在于引入了“in-distribution optimal return-to-go”概念，该机制允许策略识别数据集中可实现的最优未来回报。理论分析和实证结果均表明，Reinforced RCSL显著优于标准RCSL。", "keywords": "Return-Conditioned Supervised Learning, Reinforced RCSL, Offline Reinforcement Learning, In-distribution Optimal Return-to-go, Sequential Decision-Making", "comments": "该论文通过提出Reinforced RCSL框架，巧妙地解决了RCSL缺乏“拼接”特性、性能受限于离线数据集质量的局限性。其核心创新“in-distribution optimal return-to-go”概念，避免了复杂的回报增强技术，使得方法更具普适性和简洁性。理论证明和实证结果的结合，增强了其说服力，对离线强化学习领域具有重要意义。"}}
{"id": "2506.08541", "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching", "authors": ["Qi Yan", "Brian Zhang", "Yutong Zhang", "Daniel Yang", "Joshua White", "Di Chen", "Jiachao Liu", "Langechuan Liu", "Binnan Zhuang", "Shaoshuai Shi", "Renjie Liao"], "summary": "Efficient and accurate motion prediction is crucial for ensuring safety and\ninformed decision-making in autonomous driving, particularly under dynamic\nreal-world conditions that necessitate multi-modal forecasts. We introduce\nTrajFlow, a novel flow matching-based motion prediction framework that\naddresses the scalability and efficiency challenges of existing generative\ntrajectory prediction methods. Unlike conventional generative approaches that\nemploy i.i.d. sampling and require multiple inference passes to capture diverse\noutcomes, TrajFlow predicts multiple plausible future trajectories in a single\npass, significantly reducing computational overhead while maintaining coherence\nacross predictions. Moreover, we propose a ranking loss based on the\nPlackett-Luce distribution to improve uncertainty estimation of predicted\ntrajectories. Additionally, we design a self-conditioning training technique\nthat reuses the model's own predictions to construct noisy inputs during a\nsecond forward pass, thereby improving generalization and accelerating\ninference. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across\nvarious key metrics, underscoring its effectiveness for safety-critical\nautonomous driving applications. The code and other details are available on\nthe project website https://traj-flow.github.io/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08541v1", "AI": {"title_translation": "TrajFlow：通过流匹配进行多模态运动预测", "tldr": "TrajFlow是一个基于流匹配的运动预测框架，它通过单次推理预测多条轨迹，解决了现有生成方法的可伸缩性和效率问题，并在Waymo数据集上实现了最先进的性能。", "motivation": "在自动驾驶中，高效准确的运动预测对于确保安全和做出明智决策至关重要，尤其是在需要多模态预测的动态真实世界条件下。", "method": "TrajFlow是一个新颖的基于流匹配的运动预测框架。它通过单次推理预测多条合理的未来轨迹，显著降低了计算开销。该方法还提出了一个基于Plackett-Luce分布的排序损失来改进预测轨迹的不确定性估计，并设计了一种自条件训练技术，通过重用模型自身的预测来构建噪声输入，从而提高泛化能力并加速推理。", "result": "在大型Waymo开放运动数据集（WOMD）上进行的广泛实验表明，TrajFlow在各种关键指标上实现了最先进的性能。", "conclusion": "TrajFlow通过其创新的流匹配方法，在多模态运动预测方面取得了显著的进步，解决了现有方法的效率和可伸缩性挑战，并为自动驾驶应用提供了有效的解决方案。", "translation": "高效准确的运动预测对于确保自动驾驶中的安全和明智决策至关重要，尤其是在需要多模态预测的动态真实世界条件下。我们引入了TrajFlow，一个新颖的基于流匹配的运动预测框架，它解决了现有生成轨迹预测方法的可伸缩性和效率挑战。与采用独立同分布（i.i.d.）采样并需要多次推理才能捕获不同结果的传统生成方法不同，TrajFlow通过单次推理预测多条合理的未来轨迹，显著降低了计算开销，同时保持了预测之间的一致性。此外，我们提出了一种基于Plackett-Luce分布的排序损失，以改进预测轨迹的不确定性估计。此外，我们设计了一种自条件训练技术，该技术在第二次前向传播中重用模型自身的预测来构建噪声输入，从而提高泛化能力并加速推理。在大型Waymo开放运动数据集（WOMD）上进行的广泛实验表明，TrajFlow在各种关键指标上实现了最先进的性能，突显了其在安全关键型自动驾驶应用中的有效性。代码和其他详细信息可在项目网站https://traj-flow.github.io/上获取。", "summary": "TrajFlow是一个新颖的基于流匹配的多模态运动预测框架，旨在解决现有生成方法在自动驾驶中面临的效率和可伸缩性问题。它通过单次推理预测多条未来轨迹，显著减少了计算开销。为提高预测不确定性估计，该方法引入了基于Plackett-Luce分布的排序损失，并采用自条件训练技术来增强泛化能力和加速推理。在Waymo开放运动数据集上的实验结果表明，TrajFlow在多项关键指标上达到了最先进的性能，证明了其在安全关键型自动驾驶应用中的有效性。", "keywords": "运动预测, 流匹配, 多模态, 自动驾驶, TrajFlow", "comments": "TrajFlow的创新之处在于其采用流匹配方法实现单次推理多模态运动预测，这显著提升了效率并解决了现有生成模型的多样本推理问题。其引入的排序损失和自条件训练技术进一步优化了不确定性估计和模型泛化能力。该工作对于自动驾驶领域中轨迹预测的实时性和准确性具有重要意义。"}}
{"id": "2506.09038", "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions", "authors": ["Polina Kirichenko", "Mark Ibrahim", "Kamalika Chaudhuri", "Samuel J. Bell"], "summary": "For Large Language Models (LLMs) to be reliably deployed in both everyday and\nhigh-stakes domains, knowing when not to answer is equally critical as\nanswering correctly. Real-world user queries, which can be underspecified,\nill-posed, or fundamentally unanswerable, require LLMs to reason about\nuncertainty and selectively abstain -- i.e., refuse to answer definitively.\nHowever, abstention remains understudied, without a systematic evaluation\nframework for modern LLMs. In this work, we introduce AbstentionBench, a\nlarge-scale benchmark for holistically evaluating abstention across 20 diverse\ndatasets, including questions with unknown answers, underspecification, false\npremises, subjective interpretations, and outdated information. Evaluating 20\nfrontier LLMs reveals abstention is an unsolved problem, and one where scaling\nmodels is of little use. While recent reasoning LLMs have shown impressive\nresults in complex problem solving, surprisingly, we find that reasoning\nfine-tuning degrades abstention (by $24\\%$ on average), even for math and\nscience domains on which reasoning models are explicitly trained. We find that\nwhile a carefully crafted system prompt can boost abstention in practice, it\ndoes not resolve models' fundamental inability to reason about uncertainty. We\nrelease AbstentionBench to foster research into advancing LLM reliability.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09038v1", "AI": {"title_translation": "弃权基准：推理大型语言模型在无法回答的问题上失败", "tldr": "LLMs在无法回答的问题上表现不佳，推理微调甚至会降低其弃权能力，表明LLM在不确定性推理方面存在根本性缺陷。", "motivation": "为了可靠部署大型语言模型（LLMs），它们在面对无法回答的问题时需要学会“弃权”（拒绝回答）。然而，目前对此研究不足，缺乏系统性评估框架。", "method": "引入AbstentionBench，一个大规模基准测试，包含20个多样化数据集，用于全面评估LLM的弃权能力。", "result": "评估20个前沿LLM发现弃权是一个未解决的问题，且模型规模扩大帮助不大。令人惊讶的是，推理微调反而会使弃权能力平均下降24%，即使是在其专门训练的数学和科学领域。虽然精心设计的系统提示可以提升弃权表现，但无法解决模型在不确定性推理方面的根本性缺陷。", "conclusion": "弃权是LLM可靠性面临的未解决问题，推理能力并不能改善这一点。AbstentionBench的发布旨在促进LLM可靠性方面的研究。", "translation": "为了使大型语言模型（LLM）能够在日常和高风险领域可靠部署，了解何时不回答与正确回答同样重要。现实世界的用户查询可能是未明确的、不明确的或根本无法回答的，这要求LLM对不确定性进行推理并选择性地弃权——即明确拒绝回答。然而，弃权问题仍未得到充分研究，缺乏针对现代LLM的系统评估框架。在这项工作中，我们引入了AbstentionBench，一个大规模基准测试，用于全面评估20个不同数据集上的弃权能力，包括答案未知、规范不足、错误前提、主观解释和过时信息的问题。对20个前沿LLM的评估表明，弃权是一个未解决的问题，并且模型规模的扩大对此帮助不大。尽管最近的推理LLM在解决复杂问题方面表现出色，但令人惊讶的是，我们发现推理微调会降低弃权能力（平均下降24%），即使是在推理模型明确训练的数学和科学领域。我们发现，虽然精心设计的系统提示可以在实践中提高弃权能力，但它不能解决模型在不确定性推理方面的根本性缺陷。我们发布AbstentionBench，以促进LLM可靠性方面的研究。", "summary": "该研究引入了AbstentionBench，一个大规模基准测试，旨在评估大型语言模型（LLM）在面对无法回答问题时的弃权能力。通过对20个前沿LLM的评估，发现弃权是一个未解决的难题，且模型规模的提升或推理微调（反而降低24%）都未能有效改善。尽管系统提示有所帮助，但未能解决LLM在不确定性推理方面的根本性缺陷。该工作强调了LLM在可靠性方面的不足，并发布AbstentionBench以推动相关研究。", "keywords": "大型语言模型, 弃权, 无法回答问题, 评估, 基准测试", "comments": "这项工作提出了一种新的、系统的评估框架AbstentionBench，填补了LLM弃权能力评估的空白。其创新之处在于揭示了推理能力与弃权能力之间的负相关性，这与直觉相反，并指出了LLM在不确定性推理方面的深层缺陷。论文的重要性在于为未来研究LLM可靠性提供了一个关键的工具和方向。"}}
{"id": "2506.08255", "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense", "authors": ["Patryk Krukowski", "Łukasz Gorczyca", "Piotr Helm", "Kamil Książek", "Przemysław Spurek"], "summary": "Traditional deep neural networks suffer from several limitations, including\ncatastrophic forgetting. When models are adapted to new datasets, they tend to\nquickly forget previously learned knowledge. Another significant issue is the\nlack of robustness to even small perturbations in the input data. In practice,\nwe can often easily perform adversarial attacks and change the network's\npredictions, adding minimal noise to the input. Dedicated architectures and\ntraining procedures can solve each of the above problems separately.\nUnfortunately, currently, no model can simultaneously address both catastrophic\nforgetting and vulnerability to adversarial attacks. We introduce SHIELD\n(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel\napproach that integrates a hypernetwork-based continual learning approach with\ninterval arithmetic. SHIELD use the hypernetwork to transfer trainable task\nembedding vectors into the weights of a target model dedicated to specific\ndata. This paradigm allows for the dynamic generation of separate networks for\neach subtask, while the hypernetwork aggregates and analyzes information across\nall tasks. The target model takes in the input a data sample with a defined\ninterval range, and by creating a hypercube, produces a prediction for the\ngiven range. Therefore, such target models provide strict guarantees against\nall possible attacks for data samples within the interval range. Our approach\nenhances security without sacrificing network adaptability, addressing the\noverlooked challenge of safety in continual learning.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08255v1", "AI": {"title_translation": "SHIELD: 安全超网络用于增量扩展学习防御", "tldr": "SHIELD结合超网络和区间算术，同时解决灾难性遗忘和对抗性攻击问题。", "motivation": "传统深度神经网络存在灾难性遗忘和对输入数据微小扰动缺乏鲁棒性（易受对抗性攻击）的问题。现有模型无法同时解决这两个挑战。", "method": "本文引入了SHIELD（安全超网络用于增量扩展学习防御），该方法将基于超网络的持续学习与区间算术相结合。SHIELD使用超网络将可训练的任务嵌入向量转移到特定数据目标模型的权重中，为每个子任务动态生成单独的网络，同时超网络聚合和分析所有任务的信息。目标模型接收具有定义区间范围的输入数据样本，并通过创建超立方体为给定范围生成预测，从而提供对抗所有可能攻击的严格保证。", "result": "SHIELD在不牺牲网络适应性的情况下增强了安全性，解决了持续学习中被忽视的安全性挑战。它能同时解决灾难性遗忘和对抗性攻击问题，并对给定区间范围内的所有可能攻击提供严格保证。", "conclusion": "SHIELD提供了一种新颖的方法，可以同时解决深度学习中的灾难性遗忘和对抗性攻击这两个关键挑战，并在持续学习中提升安全性。", "translation": "传统深度神经网络存在一些局限性，包括灾难性遗忘。当模型适应新数据集时，它们往往会迅速忘记先前学习到的知识。另一个重要问题是即使输入数据出现微小扰动也缺乏鲁棒性。在实践中，我们通常可以轻松地执行对抗性攻击，通过向输入添加最小噪声来改变网络的预测。专门的架构和训练过程可以分别解决上述每个问题。不幸的是，目前没有模型可以同时解决灾难性遗忘和对抗性攻击的脆弱性。我们引入了SHIELD（安全超网络用于增量扩展学习防御），这是一种将基于超网络的持续学习方法与区间算术相结合的新颖方法。SHIELD使用超网络将可训练的任务嵌入向量转移到专用特定数据的目标模型权重中。这种范式允许为每个子任务动态生成单独的网络，同时超网络聚合和分析所有任务的信息。目标模型接收具有定义区间范围的输入数据样本，并通过创建超立方体为给定范围生成预测。因此，此类目标模型对区间范围内的数据样本提供针对所有可能攻击的严格保证。我们的方法在不牺牲网络适应性的情况下增强了安全性，解决了持续学习中被忽视的安全性挑战。", "summary": "SHIELD是一种新颖的方法，它将基于超网络的持续学习与区间算术相结合，旨在同时解决深度神经网络中的灾难性遗忘和对抗性攻击问题。通过为每个子任务动态生成专用网络并利用区间算术提供严格的安全保证，SHIELD在保持网络适应性的同时增强了安全性。", "keywords": "灾难性遗忘, 对抗性攻击, 超网络, 持续学习, 区间算术", "comments": "SHIELD的创新之处在于其将持续学习和对抗性鲁棒性结合在一个统一框架中，这在之前是未被充分解决的挑战。通过引入区间算术，它为对抗性攻击提供了数学上的严格保证，这对于实际应用中的安全至关重要。"}}
{"id": "2506.08543", "title": "Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs", "authors": ["Bowei Tian", "Xuntao Lyu", "Meng Liu", "Hongyi Wang", "Ang Li"], "summary": "High-level representations have become a central focus in enhancing AI\ntransparency and control, shifting attention from individual neurons or\ncircuits to structured semantic directions that align with human-interpretable\nconcepts. Motivated by the Linear Representation Hypothesis (LRH), we propose\nthe Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned\ndirections originate in the input space and are selectively amplified with\nincreasing depth. We then introduce the Spectral Principal Path (SPP)\nframework, which formalizes how deep networks progressively distill linear\nrepresentations along a small set of dominant spectral directions. Building on\nthis framework, we further demonstrate the multimodal robustness of these\nrepresentations in Vision-Language Models (VLMs). By bridging theoretical\ninsights with empirical validation, this work advances a structured theory of\nrepresentation formation in deep networks, paving the way for improving AI\nrobustness, fairness, and transparency.", "comment": "arXiv admin note: text overlap with arXiv:2503.22720", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08543v1", "AI": {"title_translation": "谱主路径的收敛性：深度网络如何从噪声输入中提取线性表示", "tldr": "本文提出了输入空间线性假设（ISLH）和谱主路径（SPP）框架，解释了深度网络如何从噪声输入中提取并放大与概念对齐的线性表示，并验证了这些表示在视觉语言模型中的多模态鲁棒性。", "motivation": "为了增强AI的透明度和可控性，研究人员将关注点从单个神经元转向与人类可解释概念对齐的结构化语义方向。受线性表示假设（LRH）的启发，本文旨在理解深度网络如何从输入空间中提取并选择性地放大这些与概念对齐的线性表示。", "method": "本文提出了输入空间线性假设（ISLH），认为概念对齐的方向源于输入空间并随深度增加而被选择性放大。在此基础上，引入了谱主路径（SPP）框架，该框架形式化了深度网络如何沿着少量主导谱方向逐步提取线性表示。", "result": "本文在视觉语言模型（VLMs）中进一步证明了这些线性表示的多模态鲁棒性。", "conclusion": "通过结合理论见解和经验验证，这项工作推进了深度网络中表示形成的有结构理论，为提高AI的鲁棒性、公平性和透明度铺平了道路。", "translation": "高层表示已成为增强AI透明度和控制的核心焦点，将注意力从单个神经元或电路转向与人类可解释概念对齐的结构化语义方向。受线性表示假设（LRH）的启发，我们提出了输入空间线性假设（ISLH），它认为与概念对齐的方向起源于输入空间，并随着深度增加而被选择性放大。然后，我们引入了谱主路径（SPP）框架，该框架形式化了深度网络如何沿着少量主导谱方向逐步提取线性表示。在此框架的基础上，我们进一步证明了这些表示在视觉语言模型（VLMs）中的多模态鲁棒性。通过将理论见解与经验验证相结合，这项工作推进了深度网络中表示形成的结构化理论，为提高AI的鲁棒性、公平性和透明度铺平了道路。", "summary": "本文提出了输入空间线性假设（ISLH），认为深度网络中与概念对齐的方向源于输入空间并随深度增加而被选择性放大。在此基础上，引入了谱主路径（SPP）框架，以形式化深度网络如何逐步提取线性表示。研究还证明了这些表示在视觉语言模型中的多模态鲁棒性。这项工作为深度网络中的表示形成提供了一个结构化理论，有助于提升AI的鲁棒性、公平性和透明度。", "keywords": "谱主路径, 线性表示, 深度网络, 视觉语言模型, AI透明度", "comments": "这项研究通过提出ISLH和SPP框架，为理解深度网络如何从噪声输入中提取和精炼线性表示提供了新的理论视角。它不仅在理论上有所创新，还通过在VLMs中验证其多模态鲁棒性，展示了其潜在的实际应用价值，特别是在AI透明度、鲁棒性和公平性方面具有重要意义。"}}
{"id": "2506.09049", "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "comment": "Project page: https://faceong.github.io/VIKI-R/", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09049v1", "AI": {"title_translation": "VIKI-R：通过强化学习协调具身多智能体协作", "tldr": "论文提出了VIKI-Bench，一个用于具身多智能体协作的层次基准，并引入了VIKI-R，一个两阶段框架，通过微调VLM和强化学习来解决协调具身多智能体的挑战，实验证明其性能优于基线方法并能促进异构智能体间的组合协作模式。", "motivation": "协调动态环境中多个具身智能体仍然是人工智能的核心挑战，需要感知驱动的推理和可扩展的协作策略。现有基于VLM的方法在支持多样具身类型方面存在局限。", "method": "1. 引入VIKI-Bench，首个为具身多智能体协作量身定制的层次基准，包含三个结构化级别：智能体激活、任务规划和轨迹感知。它包括多样机器人具身、多视角视觉观测和结构化监督信号。2. 提出VIKI-R，一个两阶段框架：首先使用Chain-of-Thought标注的演示微调预训练的视觉语言模型（VLM），然后进行多层次奖励信号下的强化学习。", "result": "1. VIKI-R在所有任务级别上显著优于基线方法。2. 强化学习使得异构智能体之间出现了组合协作模式。", "conclusion": "VIKI-Bench和VIKI-R共同为推动具身AI系统中多智能体、视觉驱动的协作提供了统一的测试平台和方法。", "translation": "在动态环境中协调多个具身智能体仍然是人工智能的核心挑战，这需要感知驱动的推理和可扩展的协作策略。虽然最近的工作利用大型语言模型（LLM）进行多智能体规划，但很少有工作开始探索视觉语言模型（VLM）进行视觉推理。然而，这些基于VLM的方法在支持多样具身类型方面仍然有限。在这项工作中，我们引入了VIKI-Bench，这是第一个专为具身多智能体协作量身定制的层次基准，具有三个结构化级别：智能体激活、任务规划和轨迹感知。VIKI-Bench包括多样化的机器人具身、多视角视觉观测和结构化监督信号，以评估基于视觉输入的推理。为了展示VIKI-Bench的实用性，我们提出了VIKI-R，一个两阶段框架，它首先使用Chain-of-Thought标注的演示微调预训练的视觉语言模型（VLM），然后在多层次奖励信号下进行强化学习。我们的大量实验表明，VIKI-R在所有任务级别上显著优于基线方法。此外，我们还表明强化学习能够促使异构智能体之间出现组合协作模式。VIKI-Bench和VIKI-R共同为推动具身AI系统中多智能体、视觉驱动的协作提供了统一的测试平台和方法。", "summary": "本文针对具身多智能体在动态环境中的协调挑战，提出了VIKI-Bench，一个包含智能体激活、任务规划和轨迹感知三个层次的具身多智能体协作基准，支持多样具身类型和视觉输入。在此基础上，引入了VIKI-R框架，该框架通过Chain-of-Thought微调VLM并结合多层次强化学习。实验结果表明，VIKI-R在各项任务中显著超越基线方法，并能促进异构智能体间的组合协作。VIKI-Bench和VIKI-R共同为具身AI中视觉驱动的多智能体协作提供了统一的测试和方法。", "keywords": "具身多智能体协作, 强化学习, 视觉语言模型, 基准, VIKI-R", "comments": "这篇论文的创新点在于提出了一个专门用于具身多智能体协作的层次化基准VIKI-Bench，填补了现有VLM方法在支持多样具身类型方面的空白。同时，VIKI-R框架结合了VLM的视觉推理能力和强化学习的策略学习能力，有效地解决了复杂协作问题，并能促使异构智能体间出现更高级的协作模式，对具身AI领域具有重要意义。"}}
{"id": "2506.08266", "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints", "authors": ["Yaswanth Chittepu", "Blossom Metevier", "Will Schwarzer", "Austin Hoag", "Scott Niekum", "Philip S. Thomas"], "summary": "Existing approaches to language model alignment often treat safety as a\ntradeoff against helpfulness, which can lead to unacceptable responses in\nsensitive domains. To ensure reliable performance in such settings, we propose\nHigh-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a\nmethod that provides high-confidence safety guarantees while maximizing\nhelpfulness. Similar to previous methods, HC-RLHF explicitly decouples human\npreferences into helpfulness and harmlessness (safety), which are learned by\ntraining a reward model and a cost model, respectively. It then employs a\ntwo-step process to find safe solutions. In the first step, it optimizes the\nreward function under an intentionally pessimistic version of the cost\nconstraint. In the second step, the trained model undergoes a safety test to\nverify whether its performance stays within an upper-confidence bound of the\nactual cost constraint. We provide a theoretical analysis of HC-RLHF, including\nproof that it will not return an unsafe solution with a probability greater\nthan a user-specified threshold. For our empirical analysis, we apply HC-RLHF\nto align three different language models (Qwen2-1.5B, Qwen2.5-3B, and\nLLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF\nproduces safe models with high probability and can improve harmlessness and\nhelpfulness compared to previous methods.", "comment": "20 pages, 6 figures, 4 tables, Second Reinforcement Learning\n  Conference (RLC 2025)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08266v1", "AI": {"title_translation": "基于高置信度安全约束的人类反馈强化学习", "tldr": "HC-RLHF是一种新方法，通过解耦帮助性和无害性并采用两步优化和安全测试流程，在最大化帮助性的同时提供高置信度安全保证，解决了现有语言模型对齐方法中安全与帮助性之间的权衡问题。", "motivation": "现有的语言模型对齐方法通常将安全性视为与帮助性之间的权衡，这可能导致在敏感领域出现不可接受的响应。为了确保在此类设置中的可靠性能，需要一种能够提供高置信度安全保证同时最大化帮助性的方法。", "method": "本文提出了高置信度安全人类反馈强化学习（HC-RLHF）方法。它将人类偏好解耦为帮助性（通过奖励模型学习）和无害性/安全性（通过成本模型学习）。HC-RLHF采用两步过程：首先，在悲观成本约束下优化奖励函数；其次，对训练后的模型进行安全测试，验证其性能是否在实际成本约束的上限置信区间内。该方法提供了理论分析，证明其返回不安全解决方案的概率不会超过用户指定阈值。", "result": "经验分析表明，HC-RLHF能够以高概率生成安全的模型，并且与现有方法相比，可以改善语言模型的无害性和帮助性。该方法已应用于对齐Qwen2-1.5B、Qwen2.5-3B和LLaMa3.2-3B三种不同的语言模型。", "conclusion": "HC-RLHF成功地解决了语言模型对齐中安全性与帮助性之间的权衡问题，通过提供高置信度安全保证，同时提升了模型的无害性和帮助性。", "translation": "现有语言模型对齐方法通常将安全性视为与帮助性之间的权衡，这可能导致在敏感领域出现不可接受的响应。为了确保在此类设置中的可靠性能，我们提出了高置信度安全人类反馈强化学习（HC-RLHF），这是一种在最大化帮助性的同时提供高置信度安全保证的方法。与之前的方法类似，HC-RLHF明确地将人类偏好解耦为帮助性（通过训练奖励模型学习）和无害性/安全性（通过训练成本模型学习）。然后，它采用两步过程来寻找安全解决方案。第一步，在有意悲观的成本约束下优化奖励函数。第二步，对训练后的模型进行安全测试，以验证其性能是否保持在实际成本约束的上限置信区间内。我们提供了HC-RLHF的理论分析，包括证明它返回不安全解决方案的概率不会大于用户指定的阈值。在我们的经验分析中，我们将HC-RLHF应用于将三种不同的语言模型（Qwen2-1.5B、Qwen2.5-3B和LLaMa3.2-3B）与人类偏好对齐。我们的结果表明，HC-RLHF以高概率生成安全的模型，并且与现有方法相比，可以改善无害性和帮助性。", "summary": "本文提出了一种名为高置信度安全人类反馈强化学习（HC-RLHF）的新方法，旨在解决语言模型对齐中安全性与帮助性之间的权衡问题。该方法通过将人类偏好解耦为帮助性和无害性，并分别训练奖励模型和成本模型。HC-RLHF采用两步优化流程：首先在悲观成本约束下优化奖励，然后进行严格的安全测试以确保模型满足高置信度安全约束。理论分析证明了其安全性保证，并通过对多种语言模型的实证评估，验证了HC-RLHF在提升模型安全性和帮助性方面的有效性。", "keywords": "强化学习, 人类反馈, 安全约束, 语言模型对齐, 高置信度", "comments": "HC-RLHF的创新之处在于其对安全性的高置信度保证，通过明确的成本模型和两步优化及验证过程，解决了传统RLHF在敏感领域可能面临的安全风险。这种方法在确保语言模型可靠性方面具有重要意义，尤其是在需要高度安全性的应用场景中。理论分析的加入也增加了其方法论的严谨性。"}}
{"id": "2506.08553", "title": "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge", "authors": ["Agnese Taluzzi", "Davide Gesualdi", "Riccardo Santambrogio", "Chiara Plizzari", "Francesca Palermo", "Simone Mentasti", "Matteo Matteucci"], "summary": "This report presents SceneNet and KnowledgeNet, our approaches developed for\nthe HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with\na multi-modal large language model (MLLM) to capture fine-grained object\ninteractions, spatial relationships, and temporally grounded events. In\nparallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge\nto introduce high-level semantic connections between entities, enabling\nreasoning beyond directly observable visual evidence. Each method demonstrates\ndistinct strengths across the seven categories of the HD-EPIC benchmark, and\ntheir combination within our framework results in an overall accuracy of 44.21%\non the challenge, highlighting its effectiveness for complex egocentric VQA\ntasks.", "comment": "Technical report for the HD-EPIC VQA Challenge 2025 (1st place)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08553v1", "AI": {"title_translation": "从像素到图谱：利用场景图和知识图谱解决HD-EPIC VQA挑战", "tldr": "本文介绍了为HD-EPIC VQA挑战2025开发的SceneNet和KnowledgeNet方法，通过结合场景图和知识图谱处理复杂的以自我为中心的VQA任务，并在挑战中取得了44.21%的准确率。", "motivation": "为解决HD-EPIC VQA挑战中的复杂以自我为中心的视觉问答任务，需要捕捉细粒度对象交互、空间关系、时间事件以及高层语义连接，从而提升模型对视觉证据以外信息的推理能力。", "method": "本文提出了SceneNet和KnowledgeNet两种方法。SceneNet利用多模态大语言模型（MLLM）生成的场景图来捕获细粒度对象交互、空间关系和时间性事件。KnowledgeNet则整合ConceptNet的外部常识知识，引入实体之间的高层语义连接，以实现超越直接可观察视觉证据的推理。这两种方法被结合使用。", "result": "在HD-EPIC基准测试的七个类别中，每种方法都展示了独特的优势。将两种方法结合在提出的框架中，在挑战中取得了44.21%的总体准确率。", "conclusion": "结合场景图（SceneNet）和知识图谱（KnowledgeNet）的方法对于解决复杂的以自我为中心的视觉问答（VQA）任务是有效且具有竞争力的。", "translation": "本报告介绍了SceneNet和KnowledgeNet，这是我们为HD-EPIC VQA挑战2025开发的方法。SceneNet利用多模态大型语言模型（MLLM）生成的场景图来捕获细粒度对象交互、空间关系和时间性事件。同时，KnowledgeNet整合了ConceptNet的外部常识知识，引入实体之间的高级语义连接，从而实现超越直接可观察视觉证据的推理。每种方法在HD-EPIC基准测试的七个类别中都表现出独特的优势，并且它们在我们的框架内的结合在挑战中取得了44.21%的总体准确率，突显了其对于复杂以自我为中心的VQA任务的有效性。", "summary": "本文介绍了为HD-EPIC VQA挑战2025设计的SceneNet和KnowledgeNet方法。SceneNet利用MLLM生成的场景图捕获细粒度视觉信息，而KnowledgeNet则融入外部常识知识进行高层语义推理。这两种方法在HD-EPIC基准测试的七个类别中均表现出独特优势，其组合框架在挑战中取得了44.21%的准确率，证明了其在复杂以自我为中心的VQA任务中的有效性。", "keywords": "场景图, 知识图谱, VQA, HD-EPIC, 以自我为中心", "comments": "该论文通过结合场景图和外部知识图谱（ConceptNet）来解决复杂的以自我为中心的视觉问答（VQA）任务，具有创新性。特别是SceneNet利用MLLM生成场景图，以及KnowledgeNet引入常识知识进行推理，提升了模型对细粒度交互和高层语义的理解能力，这对于超越传统VQA的挑战性场景至关重要。44.21%的准确率表明了其在HD-EPIC VQA挑战中的竞争力。"}}
{"id": "2506.09050", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.", "comment": "36 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09050v1", "AI": {"title_translation": "ALE-Bench：面向长周期目标驱动算法工程的基准", "tldr": "ALE-Bench是一个新的基准，用于评估AI在长周期算法工程中解决复杂优化问题的能力，揭示了大型语言模型与人类在一致性方面的差距。", "motivation": "本研究旨在评估AI系统在包裹递送路线规划、人员调度、工厂生产计划和电网平衡等领域中，对复杂优化问题进行算法工程的性能。同时，该基准的引入是为了促进未来AI在该领域的进步，特别是在需要长周期和一致性解决能力的场景。", "method": "研究引入了ALE-Bench，这是一个用于评估AI系统在基于分数的算法编程竞赛中表现的新基准。该基准借鉴了AtCoder启发式竞赛中的真实任务，提出了计算上困难且没有已知精确解的优化问题。与短时、通过/失败的编码基准不同，ALE-Bench鼓励在长时间范围内进行迭代解决方案优化。其软件框架支持利用测试运行反馈和可视化的交互式智能体架构。研究还对前沿大型语言模型（LLMs）进行了评估。", "result": "对前沿大型语言模型（LLMs）的评估显示，虽然它们在特定问题上表现出高性能，但在跨问题的一致性和长周期问题解决能力方面，与人类相比仍存在显著差距。", "conclusion": "本基准的引入对于促进未来AI在算法工程领域的进步至关重要，尤其是在提升AI系统解决长周期、复杂优化问题时的一致性方面。", "translation": "人工智能系统在包裹递送路线规划、人员调度、工厂生产计划和电网平衡等领域的复杂优化问题算法工程中表现如何？我们引入了ALE-Bench，这是一个用于评估AI系统在基于分数的算法编程竞赛中表现的新基准。ALE-Bench借鉴了AtCoder启发式竞赛中的真实任务，提出了计算上困难且没有已知精确解的优化问题。与短时、通过/失败的编码基准不同，ALE-Bench鼓励在长时间范围内进行迭代解决方案优化。我们的软件框架支持利用测试运行反馈和可视化的交互式智能体架构。我们对前沿大型语言模型（LLMs）的评估显示，虽然它们在特定问题上表现出高性能，但在跨问题的一致性和长周期问题解决能力方面，与人类相比仍存在显著差距。这凸显了需要这个基准来促进未来AI的进步。", "summary": "ALE-Bench是一个新颖的基准，旨在评估AI系统在计算困难且需要长周期迭代优化的算法工程问题中的表现，其任务来源于真实的AtCoder启发式竞赛。该基准强调解决方案的迭代改进和交互式智能体架构，与传统的短时编码评估有所不同。对前沿大型语言模型的初步评估表明，尽管它们在特定问题上表现良好，但在解决跨问题的一致性和长周期问题方面，与人类相比仍存在明显差距，这强调了该基准对于推动未来AI发展的必要性。", "keywords": "ALE-Bench, 算法工程, 优化问题, AI评估, 长周期", "comments": "ALE-Bench通过关注长周期、迭代式的算法工程，填补了当前AI评估中的一个关键空白，这与现实世界的复杂优化问题高度相关。研究发现大型语言模型在一致性和长周期问题解决能力上与人类存在差距，这一洞察具有重要意义，指明了未来AI研究除了简单的通过/失败编码挑战之外，需要关注的关键方向。"}}
{"id": "2506.08267", "title": "Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression", "authors": ["Mansooreh Montazerin", "Majd Al Aawar", "Antonio Ortega", "Ajitesh Srivastava"], "summary": "Symbolic regression (SR) aims to discover closed-form mathematical\nexpressions that accurately describe data, offering interpretability and\nanalytical insight beyond standard black-box models. Existing SR methods often\nrely on population-based search or autoregressive modeling, which struggle with\nscalability and symbolic consistency. We introduce LIES (Logarithm, Identity,\nExponential, Sine), a fixed neural network architecture with interpretable\nprimitive activations that are optimized to model symbolic expressions. We\ndevelop a framework to extract compact formulae from LIES networks by training\nwith an appropriate oversampling strategy and a tailored loss function to\npromote sparsity and to prevent gradient instability. After training, it\napplies additional pruning strategies to further simplify the learned\nexpressions into compact formulae. Our experiments on SR benchmarks show that\nthe LIES framework consistently produces sparse and accurate symbolic formulae\noutperforming all baselines. We also demonstrate the importance of each design\ncomponent through ablation studies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08267v1", "AI": {"title_translation": "用于符号回归的LIES网络稀疏可解释深度学习", "tldr": "该论文引入了LIES网络，一种用于符号回归的深度学习方法，可生成稀疏、可解释的数学表达式，优于现有方法。", "motivation": "符号回归旨在为数据寻找可解释的数学表达式，但现有方法（基于群体的搜索、自回归建模）在可扩展性和符号一致性方面存在困难。", "method": "该论文引入了LIES（对数、恒等、指数、正弦），这是一种具有可解释原始激活函数的固定神经网络架构。它使用适当的过采样策略和定制的损失函数进行训练，以促进稀疏性并防止梯度不稳定。训练后，应用额外的剪枝策略来简化表达式。", "result": "LIES框架在SR基准测试上始终生成稀疏且准确的符号公式，优于所有基线。消融研究证实了每个设计组件的重要性。", "conclusion": "LIES框架通过生成稀疏、准确和可解释的数学表达式，有效解决了符号回归的挑战，并表现出优于基线的性能。", "translation": "符号回归 (SR) 旨在发现能够准确描述数据的闭式数学表达式，提供超越标准黑盒模型的可解释性和分析洞察。现有的 SR 方法通常依赖于基于群体的搜索或自回归建模，这些方法在可扩展性和符号一致性方面存在困难。我们引入了 LIES（对数、恒等、指数、正弦），这是一种固定的神经网络架构，具有可解释的原始激活函数，经过优化以建模符号表达式。我们开发了一个框架，通过使用适当的过采样策略和定制的损失函数进行训练，从 LIES 网络中提取紧凑的公式，以促进稀疏性并防止梯度不稳定。训练后，它会应用额外的剪枝策略，进一步将学习到的表达式简化为紧凑的公式。我们在 SR 基准测试上的实验表明，LIES 框架始终生成稀疏且准确的符号公式，优于所有基线。我们还通过消融研究证明了每个设计组件的重要性。", "summary": "本文提出LIES（对数、恒等、指数、正弦）网络，一种用于符号回归的新型深度学习架构。LIES网络使用可解释的原始激活函数，并采用特定的过采样策略和损失函数进行训练，以确保稀疏性和稳定性。训练后剪枝进一步简化了学习到的表达式。实验表明，LIES始终生成稀疏、准确和可解释的数学公式，优于现有的符号回归方法，并解决了它们在可扩展性和一致性方面的局限性。", "keywords": "符号回归, 深度学习, 可解释人工智能, 稀疏模型, LIES网络", "comments": "这篇论文通过将深度学习与可解释性和稀疏性相结合，为符号回归提供了一种创新方法，这些在黑盒模型中通常具有挑战性。具有原始激活函数的固定架构以及明确的稀疏性促进机制是其主要优势。"}}
{"id": "2506.08441", "title": "Time-Aware World Model for Adaptive Prediction and Control", "authors": ["Anh N. Nhu", "Sanghyun Son", "Ming Lin"], "summary": "In this work, we introduce the Time-Aware World Model (TAWM), a model-based\napproach that explicitly incorporates temporal dynamics. By conditioning on the\ntime-step size, {\\Delta}t, and training over a diverse range of {\\Delta}t\nvalues -- rather than sampling at a fixed time-step -- TAWM learns both high-\nand low-frequency task dynamics across diverse control problems. Grounded in\nthe information-theoretic insight that the optimal sampling rate depends on a\nsystem's underlying dynamics, this time-aware formulation improves both\nperformance and data efficiency. Empirical evaluations show that TAWM\nconsistently outperforms conventional models across varying observation rates\nin a variety of control tasks, using the same number of training samples and\niterations. Our code can be found online at:\ngithub.com/anh-nn01/Time-Aware-World-Model.", "comment": "Paper accepted to ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08441v1", "AI": {"title_translation": "时间感知世界模型用于自适应预测和控制", "tldr": "TAWM是一种模型方法，通过在不同时间步长上训练，提高了预测和控制任务的性能和数据效率。", "motivation": "传统的模型训练通常在固定时间步长下进行，这可能无法有效捕捉系统的高频和低频动态，导致性能和数据效率不足。", "method": "本文提出了一种时间感知世界模型（TAWM），它通过显式地整合时间动态，并以时间步长Δt为条件，在各种Δt值范围内进行训练，而不是固定采样，从而学习高频和低频任务动态。", "result": "TAWM在各种控制任务中，使用相同的训练样本和迭代次数，始终优于传统模型，并在不同观测速率下表现出更好的性能和数据效率。", "conclusion": "时间感知世界模型（TAWM）通过考虑时间动态和多样化的时间步长训练，显著提高了模型在自适应预测和控制任务中的性能和数据效率。", "translation": "在这项工作中，我们引入了时间感知世界模型（TAWM），这是一种明确包含时间动态的基于模型的方法。通过以时间步长Δt为条件，并在各种Δt值范围内进行训练——而不是以固定时间步长采样——TAWM学习了各种控制问题中的高频和低频任务动态。基于信息论的洞察，即最佳采样率取决于系统的底层动态，这种时间感知公式提高了性能和数据效率。实证评估表明，TAWM在各种控制任务中，使用相同数量的训练样本和迭代次数，始终优于不同观测速率下的传统模型。我们的代码可以在线获取：github.com/anh-nn01/Time-Aware-World-Model。", "summary": "本文提出了一种名为时间感知世界模型（TAWM）的新型模型，它通过显式考虑时间步长并在多样化的时间步长范围内进行训练，以捕捉高频和低频任务动态。实验证明，TAWM在多种控制任务中，相较于传统模型，在性能和数据效率上均有显著提升。", "keywords": "时间感知世界模型, 自适应控制, 预测, 时间动态, 数据效率", "comments": "TAWM的创新之处在于其时间感知特性，通过在不同时间步长上进行训练，克服了传统模型在固定采样率下的局限性，从而更全面地学习系统动态。这对于需要处理多尺度时间动态的自适应预测和控制问题具有重要意义，有望提升模型在复杂真实世界场景中的鲁棒性和效率。"}}
{"id": "2506.00160", "title": "Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement", "authors": ["Qihui Fan", "Enfu Nan", "Wenbo Li", "Lei Lu", "Pu Zhao", "Yanzhi Wang"], "summary": "The growing popularity of social deduction game systems for both business\napplications and AI research has greatly benefited from the rapid advancements\nin Large Language Models (LLMs), which now demonstrate stronger reasoning and\npersuasion capabilities. Especially with the raise of DeepSeek R1 and V3\nmodels, LLMs should enable a more engaging experience for human players in\nLLM-agent-based social deduction games like Werewolf. Previous works either\nfine-tuning, advanced prompting engineering, or additional experience pool to\nachieve engaging text-format Werewolf game experience. We propose a novel yet\nstraightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)\nmodels designed for enhanced compatibility with various LLM models, and\nimproved user engagement. We argue with ever enhancing LLM reasoning, extra\ncomponents will be unnecessary in the case of Werewolf.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.00160v1", "AI": {"title_translation": "狼人杀：一个带有TTS的简单游戏框架，用于提升用户参与度", "tldr": "提出一个结合TTS的简单LLM-based狼人杀游戏系统，旨在提升用户参与度，并认为随着LLM推理能力的增强，额外组件在狼人杀游戏中是不必要的。", "motivation": "随着大型语言模型（LLMs）推理和说服能力的增强，它们应能为基于LLM代理的社交推理游戏（如狼人杀）中的人类玩家提供更具吸引力的体验。现有方法通常需要微调、高级提示工程或额外的经验池来达到良好的文本格式游戏体验。", "method": "我们提出一个新颖且直接的基于LLM的狼人杀游戏系统，该系统集成了经过优化的文本转语音（TTS）模型，旨在增强与各种LLM模型的兼容性并提高用户参与度。", "result": "Not mentioned in abstract", "conclusion": "随着LLM推理能力的不断增强，在狼人杀游戏中，额外的组件（如微调、高级提示工程或经验池）将变得不必要。", "translation": "狼人杀：一个带有TTS的简单游戏框架，用于提升用户参与度\n\n社交推理游戏系统在商业应用和人工智能研究中日益普及，这极大地受益于大型语言模型（LLMs）的快速发展，LLMs现在展现出更强的推理和说服能力。特别是随着DeepSeek R1和V3模型的出现，LLMs应该能够为基于LLM代理的社交推理游戏（如狼人杀）中的人类玩家提供更具吸引力的体验。以往的工作通过微调、高级提示工程或额外的经验池来实现引人入胜的文本格式狼人杀游戏体验。我们提出了一种新颖而直接的基于LLM的狼人杀游戏系统，该系统集成了经过优化的文本转语音（TTS）模型，旨在增强与各种LLM模型的兼容性并提高用户参与度。我们认为，随着LLM推理能力的不断增强，在狼人杀游戏中，额外的组件将变得不必要。", "summary": "这篇论文提出了一个新颖且直接的基于大型语言模型（LLM）的狼人杀游戏系统，该系统集成了经过优化的文本转语音（TTS）模型。其主要目标是提升用户参与度，并增强与不同LLM模型的兼容性。作者认为，鉴于LLM日益增强的推理能力，以往工作中所需的微调、高级提示工程或额外经验池等组件，在狼人杀游戏中将不再是必需的。", "keywords": "大型语言模型, 狼人杀, 文本转语音, 用户参与度, 社交推理游戏", "comments": "这篇论文的创新点在于提出了一个“简单直接”的LLM-based狼人杀框架，并引入了TTS技术以提升用户体验，这与以往需要复杂调优的方法形成对比。其重要性在于，如果该框架确实能达到预期效果，将大大降低开发高质量LLM驱动社交游戏系统的门槛，并可能推动LLM在更广泛交互式应用中的普及。一个潜在的局限性是，抽象中没有提供具体的实验结果来支持其关于“额外组件不必要”的论点。"}}
{"id": "2506.08270", "title": "SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space", "authors": ["Zitong Huang", "Mansooreh Montazerin", "Ajitesh Srivastava"], "summary": "Designing neural networks typically relies on manual trial and error or a\nneural architecture search (NAS) followed by weight training. The former is\ntime-consuming and labor-intensive, while the latter often discretizes\narchitecture search and weight optimization. In this paper, we propose a\nfundamentally different approach that simultaneously optimizes both the\narchitecture and the weights of a neural network. Our framework first trains a\nuniversal multi-scale autoencoder that embeds both architectural and parametric\ninformation into a continuous latent space, where functionally similar neural\nnetworks are mapped closer together. Given a dataset, we then randomly\ninitialize a point in the embedding space and update it via gradient descent to\nobtain the optimal neural network, jointly optimizing its structure and\nweights. The optimization process incorporates sparsity and compactness\npenalties to promote efficient models. Experiments on synthetic regression\ntasks demonstrate that our method effectively discovers sparse and compact\nneural networks with strong performance.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08270v1", "AI": {"title_translation": "SWAT-NN：在潜在空间中同时训练神经网络的权重和架构", "tldr": "SWAT-NN提出了一种在新颖的连续潜在空间中同时优化神经网络架构和权重的方法，旨在发现高效模型。", "motivation": "传统神经网络设计方法（手动试错或NAS后训练权重）耗时、耗力且将架构搜索与权重优化离散化，本研究旨在解决这些局限性。", "method": "提出SWAT-NN框架，通过训练一个通用的多尺度自编码器，将架构和参数信息嵌入到连续潜在空间中。然后，在给定数据集上，通过梯度下降在潜在空间中联合优化网络结构和权重，并引入稀疏性和紧凑性惩罚。", "result": "在合成回归任务上的实验表明，该方法能有效发现性能强劲的稀疏紧凑型神经网络。", "conclusion": "SWAT-NN提出了一种同时优化神经网络架构和权重的新颖方法，能有效发现高效且高性能的神经网络。", "translation": "设计神经网络通常依赖于手动试错或神经架构搜索（NAS）后再进行权重训练。前者耗时耗力，后者则常常将架构搜索和权重优化离散化。在本文中，我们提出了一种根本不同的方法，可以同时优化神经网络的架构和权重。我们的框架首先训练一个通用的多尺度自编码器，将架构和参数信息嵌入到一个连续的潜在空间中，其中功能相似的神经网络被映射得更近。给定一个数据集，我们随后在嵌入空间中随机初始化一个点，并通过梯度下降更新它以获得最优神经网络，从而联合优化其结构和权重。优化过程中加入了稀疏性和紧凑性惩罚，以促进高效模型的生成。在合成回归任务上的实验表明，我们的方法能有效发现性能强劲的稀疏紧凑型神经网络。", "summary": "本文提出了SWAT-NN，一种用于同时优化神经网络架构和权重的新型框架。该方法通过训练一个通用的多尺度自编码器，将网络信息嵌入到连续潜在空间中，并通过梯度下降结合稀疏性和紧凑性惩罚进行联合优化。实验证明，SWAT-NN能有效发现稀疏、紧凑且性能优异的神经网络。", "keywords": "神经网络架构搜索, 权重训练, 潜在空间, 同时优化, 稀疏神经网络", "comments": "SWAT-NN通过在连续潜在空间中统一架构搜索和权重优化，为神经网络设计提供了一个创新方向，有望解决传统NAS的局限性。其在合成任务上的成功表明了该方法在生成高效、紧凑模型方面的潜力。"}}
{"id": "2506.08455", "title": "The interplay of robustness and generalization in quantum machine learning", "authors": ["Julian Berberich", "Tobias Fellner", "Christian Holm"], "summary": "While adversarial robustness and generalization have individually received\nsubstantial attention in the recent literature on quantum machine learning,\ntheir interplay is much less explored. In this chapter, we address this\ninterplay for variational quantum models, which were recently proposed as\nfunction approximators in supervised learning. We discuss recent results\nquantifying both robustness and generalization via Lipschitz bounds, which\nexplicitly depend on model parameters. Thus, they give rise to a\nregularization-based training approach for robust and generalizable quantum\nmodels, highlighting the importance of trainable data encoding strategies. The\npractical implications of the theoretical results are demonstrated with an\napplication to time series analysis.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.08455v1", "AI": {"title_translation": "量子机器学习中鲁棒性与泛化性之间的相互作用", "tldr": "本文探讨了量子机器学习中变分量子模型的鲁棒性与泛化性之间的相互作用，通过Lipschitz界限进行量化，并提出了一种基于正则化的训练方法，强调了可训练数据编码策略的重要性，并通过时间序列分析进行了演示。", "motivation": "尽管对抗性鲁棒性和泛化性在量子机器学习中得到了广泛关注，但它们之间的相互作用却鲜有探索。", "method": "针对变分量子模型，通过Lipschitz界限量化鲁棒性和泛化性，这些界限明确依赖于模型参数。这引出了一种基于正则化的训练方法，用于构建鲁棒且可泛化的量子模型，并强调了可训练数据编码策略的重要性。", "result": "提出了通过Lipschitz界限量化鲁棒性和泛化性的方法，并由此发展出一种基于正则化的训练方法，以实现鲁棒且可泛化的量子模型。理论结果的实际意义通过时间序列分析应用得到证明。", "conclusion": "本文强调了在变分量子模型中，通过参数相关的Lipschitz界限量化和正则化训练，可以同时实现鲁棒性和泛化性，并且可训练的数据编码策略至关重要。", "translation": "尽管对抗性鲁棒性和泛化性在近期量子机器学习文献中各自受到了广泛关注，但它们之间的相互作用却鲜有探索。在本章中，我们针对变分量子模型探讨了这种相互作用，这些模型最近被提出作为监督学习中的函数逼近器。我们讨论了通过Lipschitz界限量化鲁棒性和泛化性的最新结果，这些界限明确依赖于模型参数。因此，它们催生了一种基于正则化的训练方法，用于构建鲁棒且可泛化的量子模型，突出了可训练数据编码策略的重要性。理论结果的实际意义通过时间序列分析的应用得到了证明。", "summary": "本章探讨了量子机器学习中变分量子模型的鲁棒性与泛化性之间的相互作用。通过Lipschitz界限量化这两个特性，这些界限与模型参数相关联。基于此，文章提出了一种正则化训练方法来构建鲁棒且泛化能力强的量子模型，并强调了可训练数据编码策略的重要性。文章通过时间序列分析应用展示了理论结果的实际意义。", "keywords": "量子机器学习, 鲁棒性, 泛化性, 变分量子模型, Lipschitz界限", "comments": "这项研究通过量化鲁棒性和泛化性之间的相互作用，并提出基于Lipschitz界限的正则化训练方法，为构建更可靠和高效的量子机器学习模型提供了新的视角。强调可训练数据编码策略的重要性是其创新点之一。"}}
{"id": "2506.08562", "title": "Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection", "authors": ["Duc Thanh Pham", "Hong Dang Nguyen", "Nhat Minh Nguyen Quoc", "Linh Ngo Van", "Sang Dinh Viet", "Duc Anh Nguyen"], "summary": "Recently, object detection models have witnessed notable performance\nimprovements, particularly with transformer-based models. However, new objects\nfrequently appear in the real world, requiring detection models to continually\nlearn without suffering from catastrophic forgetting. Although Incremental\nObject Detection (IOD) has emerged to address this challenge, these existing\nmodels are still not practical due to their limited performance and prolonged\ninference time. In this paper, we introduce a novel framework for IOD, called\nHier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both\nefficiency and competitive performance by leveraging Neural Collapse for\nimbalance dataset and Hierarchical relation of classes' labels.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08562v1", "AI": {"title_translation": "针对类别增量目标检测的分层神经崩塌检测Transformer", "tldr": "本文提出了Hier-DETR，一个利用神经崩塌和类别层级关系的新型增量目标检测框架，旨在解决现有IOD模型性能受限和推理时间长的问题，从而实现高效且有竞争力的表现。", "motivation": "现有增量目标检测（IOD）模型在持续学习新对象时面临灾难性遗忘问题，且性能有限、推理时间长，导致不实用。", "method": "本文引入了一个名为Hier-DETR（分层神经崩塌检测Transformer）的新型IOD框架，通过利用不平衡数据集的神经崩塌特性和类别标签的层级关系来解决问题。", "result": "该框架旨在确保效率和具有竞争力的性能。", "conclusion": "Hier-DETR通过结合神经崩塌和类别层级关系，为增量目标检测提供了一个更高效、性能更优的解决方案。", "translation": "近期，目标检测模型取得了显著的性能提升，特别是基于Transformer的模型。然而，现实世界中新对象频繁出现，要求检测模型能够持续学习而不受灾难性遗忘的影响。尽管增量目标检测（IOD）已应运而生以应对这一挑战，但现有模型由于其有限的性能和过长的推理时间，仍然不实用。本文中，我们引入了一个新颖的IOD框架，名为Hier-DETR：分层神经崩塌检测Transformer，通过利用不平衡数据集的神经崩塌和类别标签的层级关系，确保了效率和具有竞争力的性能。", "summary": "本文提出了一种名为Hier-DETR的新型增量目标检测（IOD）框架，旨在解决现有IOD模型在处理新对象时性能受限和推理时间过长的问题。Hier-DETR通过结合不平衡数据集的神经崩塌特性和类别标签的层级关系，以实现高效且具有竞争力的检测性能，从而克服了灾难性遗忘的挑战。", "keywords": "增量目标检测, 神经崩塌, Transformer, 灾难性遗忘, 类别层级关系", "comments": "这篇论文的创新点在于将神经崩塌（Neural Collapse）和类别层级关系（Hierarchical relation of classes' labels）引入到增量目标检测中，以解决现有IOD模型在效率和性能上的局限性。这种结合可能为处理数据不平衡和提高模型在连续学习场景下的实用性提供了新的思路。"}}
{"id": "2506.04760", "title": "Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "summary": "Large Language Models (LLMs) have shown potential in generating hypothetical\ndocuments for query expansion, thereby enhancing information retrieval\nperformance. However, the efficacy of this method is highly dependent on the\nquality of the generated documents, which often requires complex prompt\nstrategies and the integration of advanced dense retrieval techniques. This can\nbe both costly and computationally intensive. To mitigate these limitations, we\nexplore the use of zero-shot LLM-based query expansion to improve sparse\nretrieval, particularly for learned sparse retrievers. We introduce a novel\nfusion ranking framework, Exp4Fuse, which enhances the performance of sparse\nretrievers through an indirect application of zero-shot LLM-based query\nexpansion. Exp4Fuse operates by simultaneously considering two retrieval\nroutes-one based on the original query and the other on the LLM-augmented\nquery. It then generates two ranked lists using a sparse retriever and fuses\nthem using a modified reciprocal rank fusion method. We conduct extensive\nevaluations of Exp4Fuse against leading LLM-based query expansion methods and\nadvanced retrieval techniques on three MS MARCO-related datasets and seven\nlow-resource datasets. Experimental results reveal that Exp4Fuse not only\nsurpasses existing LLM-based query expansion methods in enhancing sparse\nretrievers but also, when combined with advanced sparse retrievers, achieves\nSOTA results on several benchmarks. This highlights the superior performance\nand effectiveness of Exp4Fuse in improving query expansion for sparse\nretrieval.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.04760v1", "AI": {"title_translation": "Exp4Fuse：一种基于大型语言模型查询扩展的增强稀疏检索的排序融合框架", "tldr": "Exp4Fuse利用零样本大型语言模型进行查询扩展，并通过新颖的排序融合框架增强稀疏检索性能，超越现有方法并在多个基准测试中达到SOTA。", "motivation": "现有的大型语言模型（LLM）查询扩展方法高度依赖于生成文档的质量，通常需要复杂的提示策略和先进的密集检索技术，这导致成本高昂且计算密集。", "method": "本文提出了一个名为Exp4Fuse的新型融合排序框架，通过间接应用零样本LLM查询扩展来增强稀疏检索器的性能。Exp4Fuse同时考虑原始查询和LLM增强查询两种检索路径，生成两个排序列表，然后使用改进的倒数排名融合方法将它们融合。", "result": "实验结果表明，Exp4Fuse不仅在增强稀疏检索器方面超越了现有基于LLM的查询扩展方法，而且当与先进的稀疏检索器结合时，在多个基准测试中取得了最先进（SOTA）的结果。", "conclusion": "Exp4Fuse在改进稀疏检索的查询扩展方面表现出卓越的性能和有效性。", "translation": "大型语言模型（LLMs）在生成用于查询扩展的假设文档方面显示出潜力，从而提高了信息检索性能。然而，这种方法的功效高度依赖于生成文档的质量，这通常需要复杂的提示策略和先进的密集检索技术的集成。这可能既昂贵又计算密集。为了减轻这些限制，我们探索使用零样本LLM查询扩展来改进稀疏检索，特别是针对学习型稀疏检索器。我们引入了一个新颖的融合排序框架Exp4Fuse，它通过间接应用零样本LLM查询扩展来增强稀疏检索器的性能。Exp4Fuse通过同时考虑两条检索路径——一条基于原始查询，另一条基于LLM增强查询来操作。然后，它使用稀疏检索器生成两个排序列表，并使用改进的倒数排名融合方法将它们融合。我们在三个MS MARCO相关数据集和七个低资源数据集上对Exp4Fuse与领先的基于LLM的查询扩展方法和先进的检索技术进行了广泛评估。实验结果表明，Exp4Fuse不仅在增强稀疏检索器方面超越了现有基于LLM的查询扩展方法，而且当与先进的稀疏检索器结合时，在多个基准测试中取得了最先进（SOTA）的结果。这突出了Exp4Fuse在改进稀疏检索的查询扩展方面的卓越性能和有效性。", "summary": "本文提出了一种名为Exp4Fuse的排序融合框架，旨在通过间接利用零样本大型语言模型（LLM）进行查询扩展来提升稀疏检索性能。该框架通过同时处理原始查询和LLM增强查询生成两个排名列表，并使用改进的倒数排名融合方法进行整合。实验证明，Exp4Fuse不仅优于现有LLM查询扩展方法，而且与先进稀疏检索器结合时，在多个基准测试中取得了最先进的（SOTA）结果，验证了其在稀疏检索查询扩展方面的优越性和有效性。", "keywords": "稀疏检索, 查询扩展, 大型语言模型, 排序融合, 信息检索", "comments": "Exp4Fuse的创新之处在于它通过排序融合间接应用零样本LLM进行查询扩展，有效规避了直接生成高质量假设文档的复杂性和高成本。这种方法在不牺牲性能的前提下，降低了LLM在信息检索中应用的门槛，具有重要的实际意义。其在多个数据集上达到SOTA的结果，进一步证明了其有效性和竞争力。"}}
{"id": "2506.08566", "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations", "authors": ["Yibo Cui", "Liang Xie", "Yu Zhao", "Jiawei Sun", "Erwei Yin"], "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate\nenvironments by integrating visual perception and natural language\ninstructions, yet faces significant challenges due to the scarcity of\nfine-grained cross-modal alignment annotations. Existing datasets primarily\nfocus on global instruction-trajectory matching, neglecting\nsub-instruction-level and entity-level alignments critical for accurate\nnavigation action decision-making. To address this limitation, we propose\nFCA-NIG, a generative framework that automatically constructs navigation\ninstructions with dual-level fine-grained cross-modal annotations. In this\nframework, an augmented trajectory is first divided into sub-trajectories,\nwhich are then processed through GLIP-based landmark detection, crafted\ninstruction construction, OFA-Speaker based R2R-like instruction generation,\nand CLIP-powered entity selection, generating sub-instruction-trajectory pairs\nwith entity-landmark annotations. Finally, these sub-pairs are aggregated to\nform a complete instruction-trajectory pair. The framework generates the\nFCA-R2R dataset, the first large-scale augmentation dataset featuring precise\nsub-instruction-sub-trajectory and entity-landmark alignments. Extensive\nexperiments demonstrate that training with FCA-R2R significantly improves the\nperformance of multiple state-of-the-art VLN agents, including SF, EnvDrop,\nRecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances\nagents' state awareness and decision accuracy, while entity-landmark alignment\nfurther boosts navigation performance and generalization. These results\nhighlight the effectiveness of FCA-NIG in generating high-quality, scalable\ntraining data without manual annotation, advancing fine-grained cross-modal\nlearning in complex navigation tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08566v1", "AI": {"title_translation": "生成结合细粒度对齐注释的视觉-语言导航指令", "tldr": "本文提出了FCA-NIG框架，自动生成包含双层细粒度跨模态对齐注释的视觉-语言导航指令和数据集FCA-R2R，显著提升了现有VLN智能体的导航性能。", "motivation": "现有的视觉-语言导航(VLN)数据集主要关注全局指令-轨迹匹配，忽略了子指令级和实体级的对齐，这对于准确的导航动作决策至关重要。本文旨在解决缺乏细粒度跨模态对齐注释的问题。", "method": "本文提出了FCA-NIG生成框架，自动构建具有双层细粒度跨模态注释的导航指令。该框架首先将增强轨迹划分为子轨迹，然后通过基于GLIP的地标检测、指令构建、基于OFA-Speaker的R2R类指令生成以及CLIP驱动的实体选择，生成带有实体-地标注释的子指令-轨迹对。最后，这些子对被聚合形成完整的指令-轨迹对。该框架生成了FCA-R2R数据集。", "result": "FCA-NIG框架生成了FCA-R2R数据集，这是第一个具有精确子指令-子轨迹和实体-地标对齐的大规模增强数据集。实验表明，使用FCA-R2R进行训练显著提高了多种最先进VLN智能体（包括SF、EnvDrop、RecBERT和HAMT）的性能。结合子指令-轨迹对齐增强了智能体的状态感知和决策准确性，而实体-地标对齐进一步提升了导航性能和泛化能力。", "conclusion": "FCA-NIG框架能够生成高质量、可扩展的训练数据，无需手动标注，有效解决了细粒度跨模态对齐注释的稀缺问题，推动了复杂导航任务中细粒度跨模态学习的进展。", "translation": "视觉-语言导航（VLN）使智能体能够通过整合视觉感知和自然语言指令来导航环境，但由于细粒度跨模态对齐注释的稀缺性而面临重大挑战。现有数据集主要关注全局指令-轨迹匹配，忽略了对准确导航动作决策至关重要的子指令级和实体级对齐。为了解决这一限制，我们提出了FCA-NIG，一个生成框架，它自动构建具有双层细粒度跨模态注释的导航指令。在这个框架中，首先将增强的轨迹划分为子轨迹，然后通过基于GLIP的地标检测、精心设计的指令构建、基于OFA-Speaker的R2R类指令生成以及CLIP驱动的实体选择进行处理，生成带有实体-地标注释的子指令-轨迹对。最后，这些子对被聚合形成完整的指令-轨迹对。该框架生成了FCA-R2R数据集，这是第一个具有精确子指令-子轨迹和实体-地标对齐的大规模增强数据集。广泛的实验表明，使用FCA-R2R进行训练显著提高了多种最先进VLN智能体（包括SF、EnvDrop、RecBERT和HAMT）的性能。结合子指令-轨迹对齐增强了智能体的状态感知和决策准确性，而实体-地标对齐进一步提升了导航性能和泛化能力。这些结果突出了FCA-NIG在无需手动注释的情况下生成高质量、可扩展训练数据的有效性，推动了复杂导航任务中细粒度跨模态学习的进展。", "summary": "本文提出FCA-NIG框架，旨在解决视觉-语言导航(VLN)中细粒度跨模态对齐注释稀缺的挑战。FCA-NIG能自动生成包含子指令-子轨迹和实体-地标双层细粒度对齐注释的导航指令。通过该框架生成的FCA-R2R数据集，是首个大规模细粒度对齐增强数据集。实验证明，FCA-R2R显著提升了现有VLN智能体的性能，表明其在无需手动标注的情况下生成高质量训练数据的有效性，从而促进了复杂导航任务中的细粒度跨模态学习。", "keywords": "视觉-语言导航, 细粒度对齐, 数据集生成, 跨模态学习, FCA-NIG", "comments": "该论文的创新点在于提出了一个无需手动标注即可自动生成带有双层细粒度对齐注释的视觉-语言导航训练数据的框架FCA-NIG，并构建了首个大规模细粒度对齐增强数据集FCA-R2R。这对于解决VLN领域中细粒度注释稀缺的痛点具有重要意义，能够有效提升现有模型的性能和泛化能力，为未来的研究提供了高质量的数据支持。"}}
{"id": "2506.05695", "title": "Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "summary": "Knowledge Distillation (KD) compresses large language models (LLMs) by\ntransferring the teacher model's capabilities to a smaller student model,\nreducing inference cost and memory usage while maintaining performance.\nHowever, existing KD methods for LLMs often fail to prevent significant shifts\nin the student model's distribution during training, leading to issues such as\ncatastrophic forgetting, mode collapse, and training-inference mismatch. To\naddress these challenges, we propose a novel, plug-in curriculum learning\nframework inspired by the strength training principle of \"progressive overload\"\n(POCL), which can be seamlessly integrated into existing white-box KD\napproaches with minimal computational overhead. The framework comprises two\ncore components: (1) a difficulty measurer that ranks and partitions training\nsamples from easy to hard, and (2) a training scheduler that incrementally\nintroduces these subsets into the distillation process at fixed intervals while\napplying loss functions with progressively rising temperatures. By starting\nwith the easiest samples and progressively increasing the difficulty, the\napproach enhances both the stability and efficiency of learning. Extensive\nexperiments in instruction-following settings demonstrate that POCL\nconsistently improves the performance of distilled student models across\nvarious white-box KD methods and model families. Our findings highlight the\neffectiveness of sorted training samples in KD for LLMs. More generally, our\nwork demonstrates how to structure training data within the KD process to\nenhance the stability and performance of distilled LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.05695v1", "AI": {"title_translation": "逐步增强！通过课程学习框架增强大型语言模型的知识蒸馏", "tldr": "提出了一种渐进式课程学习框架（POCL），用于大型语言模型（LLM）的知识蒸馏，以防止训练过程中学生模型分布发生显著偏移，从而提高学习的稳定性和效率。", "motivation": "现有的大型语言模型（LLM）知识蒸馏（KD）方法在训练过程中未能有效防止学生模型分布的显著偏移，导致灾难性遗忘、模式崩溃和训练-推理不匹配等问题。", "method": "提出了一种受“渐进式超负荷”力量训练原则启发的、可即插即用的课程学习框架（POCL），可无缝集成到现有白盒KD方法中。该框架包含两个核心组件：1）一个难度衡量器，用于对训练样本进行从易到难的排序和分区；2）一个训练调度器，以固定间隔逐步将这些子集引入蒸馏过程，并应用温度逐步升高的损失函数。", "result": "在指令遵循设置下的广泛实验表明，POCL持续改进了各种白盒KD方法和模型家族中蒸馏学生模型的性能。研究结果强调了排序训练样本在LLM知识蒸馏中的有效性。", "conclusion": "本研究表明，在知识蒸馏过程中合理组织训练数据可以有效增强蒸馏大型语言模型的稳定性和性能。", "translation": "知识蒸馏（KD）通过将教师模型的能力转移到较小的学生模型来压缩大型语言模型（LLM），从而降低推理成本和内存使用，同时保持性能。然而，现有用于LLM的KD方法通常未能阻止学生模型在训练过程中发生显著的分布偏移，导致灾难性遗忘、模式崩溃和训练-推理不匹配等问题。为了解决这些挑战，我们提出了一种新颖的、可即插即用的课程学习框架，其灵感来源于“渐进式超负荷”（POCL）的力量训练原则，可以以最小的计算开销无缝集成到现有的白盒KD方法中。该框架包括两个核心组件：1）一个难度衡量器，用于将训练样本从易到难进行排序和分区；2）一个训练调度器，以固定间隔逐步将这些子集引入蒸馏过程，同时应用温度逐步升高的损失函数。通过从最简单的样本开始并逐步增加难度，该方法提高了学习的稳定性和效率。在指令遵循设置下的广泛实验表明，POCL持续改进了各种白盒KD方法和模型家族中蒸馏学生模型的性能。我们的发现强调了排序训练样本在LLM知识蒸馏中的有效性。更普遍地说，我们的工作展示了如何在KD过程中组织训练数据，以增强蒸馏LLM的稳定性和性能。", "summary": "本文提出了一种名为POCL的新型课程学习框架，旨在增强大型语言模型（LLM）的知识蒸馏（KD）效果。为解决现有KD方法中学生模型分布偏移导致的灾难性遗忘等问题，POCL引入了一个难度衡量器对训练样本进行排序，并利用一个训练调度器逐步引入这些样本，同时提升损失函数的温度。实验证明，POCL显著提高了不同KD方法和模型架构下蒸馏LLM的稳定性和性能。", "keywords": "知识蒸馏, 大型语言模型, 课程学习, 渐进式超负荷, 模型压缩", "comments": "该论文的创新之处在于将力量训练中的“渐进式超负荷”原则应用于LLM知识蒸馏的课程学习中，提供了一种即插即用的解决方案，以提高训练的稳定性和效率。其在不同KD方法和模型家族中的普适性是其关键优势。"}}
{"id": "2506.08274", "title": "The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks", "authors": ["João Manoel Herrera Pinheiro", "Suzana Vilas Boas de Oliveira", "Thiago Henrique Segreto Silva", "Pedro Antonio Rabelo Saraiva", "Enzo Ferreira de Souza", "Leonardo André Ambrosio", "Marcelo Becker"], "summary": "This research addresses the critical lack of comprehensive studies on feature\nscaling by systematically evaluating 12 scaling techniques - including several\nless common transformations - across 14 different Machine Learning algorithms\nand 16 datasets for classification and regression tasks. We meticulously\nanalyzed impacts on predictive performance (using metrics such as accuracy,\nMAE, MSE, and $R^2$) and computational costs (training time, inference time,\nand memory usage). Key findings reveal that while ensemble methods (such as\nRandom Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)\ndemonstrate robust performance largely independent of scaling, other widely\nused models such as Logistic Regression, SVMs, TabNet, and MLPs show\nsignificant performance variations highly dependent on the chosen scaler. This\nextensive empirical analysis, with all source code, experimental results, and\nmodel parameters made publicly available to ensure complete transparency and\nreproducibility, offers model-specific crucial guidance to practitioners on the\nneed for an optimal selection of feature scaling techniques.", "comment": "27 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08274v1", "AI": {"title_translation": "机器学习中特征缩放的影响：对回归和分类任务的影响", "tldr": "该研究系统评估了特征缩放对机器学习模型性能和计算成本的影响，发现集成方法对缩放不敏感，而其他模型则高度依赖缩放技术。", "motivation": "现有的特征缩放综合研究严重不足，促使本研究系统评估不同缩放技术对机器学习模型的影响。", "method": "研究系统评估了12种特征缩放技术，涉及14种不同的机器学习算法和16个数据集，涵盖分类和回归任务。分析了预测性能（准确率、MAE、MSE、R^2）和计算成本（训练时间、推理时间、内存使用）。", "result": "主要发现是，集成方法（如随机森林、XGBoost、CatBoost和LightGBM）的性能在很大程度上独立于特征缩放。然而，其他广泛使用的模型（如逻辑回归、SVM、TabNet和MLP）的性能则高度依赖于所选的缩放器。", "conclusion": "这项广泛的实证分析为从业者提供了关于最佳选择特征缩放技术的模型特定关键指导。", "translation": "这项研究通过系统评估12种缩放技术（包括几种不常见的变换），在14种不同的机器学习算法和16个数据集上，针对分类和回归任务，解决了特征缩放综合研究严重缺乏的问题。我们仔细分析了对预测性能（使用准确率、MAE、MSE和R^2等指标）和计算成本（训练时间、推理时间、内存使用）的影响。主要发现表明，虽然集成方法（如随机森林以及XGBoost、CatBoost和LightGBM等梯度提升模型）表现出强大的性能，在很大程度上独立于缩放，但其他广泛使用的模型，如逻辑回归、支持向量机、TabNet和多层感知器，则表现出显著的性能变化，高度依赖于所选的缩放器。这项广泛的实证分析，其所有源代码、实验结果和模型参数均已公开，以确保完全透明和可重复性，为从业者提供了关于最佳选择特征缩放技术的模型特定关键指导。", "summary": "本研究旨在弥补特征缩放综合研究的不足，系统评估了12种缩放技术对14种机器学习算法在分类和回归任务中的影响。研究分析了预测性能和计算成本。核心发现是集成方法对特征缩放不敏感，而逻辑回归、SVM、TabNet和MLP等模型则高度依赖缩放技术。研究结果为从业者选择特征缩放技术提供了模型特定指导。", "keywords": "特征缩放, 机器学习, 回归, 分类, 性能", "comments": "这项研究的创新之处在于其对特征缩放进行了迄今为止最全面的系统评估，涵盖了广泛的缩放技术、机器学习算法和数据集。其公开所有代码和数据的做法极大地增强了研究的透明度和可重复性。该研究对于机器学习从业者具有重要实践指导意义，填补了该领域的一个关键空白。"}}
{"id": "2506.08591", "title": "Diversity-Guided MLP Reduction for Efficient Large Vision Transformers", "authors": ["Chengchao Shen", "Hourun Zhu", "Gongfan Fang", "Jianxin Wang", "Xinchao Wang"], "summary": "Transformer models achieve excellent scaling property, where the performance\nis improved with the increment of model capacity. However, large-scale model\nparameters lead to an unaffordable cost of computing and memory. We analyze\npopular transformer architectures and find that multilayer perceptron (MLP)\nmodules take up the majority of model parameters. To this end, we focus on the\nrecoverability of the compressed models and propose a Diversity-Guided MLP\nReduction (DGMR) method to significantly reduce the parameters of large vision\ntransformers with only negligible performance degradation. Specifically, we\nconduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons\nof MLP hidden layer, while preserving weight diversity for better performance\nrecover during distillation. Compared to the model trained from scratch, our\npruned model only requires 0.06\\% data of LAION-2B (for the training of large\nvision transformers) without labels (ImageNet-1K) to recover the original\nperformance. Experimental results on several state-of-the-art large vision\ntransformers demonstrate that our method achieves a more than 57.0\\% parameter\nand FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),\nour method accomplishes a 71.5\\% parameter and FLOPs reduction without\nperformance degradation. The source code and trained weights are available at\nhttps://github.com/visresearch/DGMR.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08591v1", "AI": {"title_translation": "高效大型视觉Transformer的多样性引导MLP缩减", "tldr": "提出DGMR方法，通过多样性引导的MLP剪枝显著减少大型视觉Transformer的参数和计算量，同时保持性能。", "motivation": "大型Transformer模型参数量大，导致计算和内存成本高昂，其中多层感知机（MLP）模块占据了大部分参数。", "method": "提出多样性引导MLP缩减（DGMR）方法。具体通过Gram-Schmidt权重剪枝策略，消除MLP隐藏层中的冗余神经元，同时保留权重多样性以在蒸馏过程中更好地恢复性能。", "result": "该方法在多个最先进的大型视觉Transformer上实现了超过57.0%的参数和FLOPs近乎无损的缩减。对于EVA-CLIP-E (4.4B)，实现了71.5%的参数和FLOPs缩减且无性能下降。恢复原始性能仅需LAION-2B数据（无标签）的0.06%。", "conclusion": "DGMR方法能够显著且高效地减少大型视觉Transformer的参数和计算量，同时保持其性能，有效解决了模型过大带来的成本问题。", "translation": "Transformer模型展现出卓越的扩展性，其性能随模型容量的增加而提升。然而，大规模模型参数导致了高昂的计算和内存成本。我们分析了流行的Transformer架构，发现多层感知机（MLP）模块占据了模型参数的大部分。为此，我们关注压缩模型的恢复性，并提出了一种多样性引导MLP缩减（DGMR）方法，以显著减少大型视觉Transformer的参数，同时仅导致可忽略的性能下降。具体来说，我们采用Gram-Schmidt权重剪枝策略来消除MLP隐藏层的冗余神经元，同时保留权重多样性，以便在蒸馏过程中更好地恢复性能。与从头训练的模型相比，我们的剪枝模型仅需要LAION-2B（用于训练大型视觉Transformer）0.06%的无标签数据（ImageNet-1K）即可恢复原始性能。在几个最先进的大型视觉Transformer上的实验结果表明，我们的方法以近乎无损的方式实现了超过57.0%的参数和FLOPs缩减。值得注意的是，对于EVA-CLIP-E（4.4B），我们的方法在不降低性能的情况下完成了71.5%的参数和FLOPs缩减。源代码和训练权重可在https://github.com/visresearch/DGMR 获取。", "summary": "本文提出了一种名为多样性引导MLP缩减（DGMR）的新方法，旨在解决大型视觉Transformer模型参数量过大导致的计算和内存成本问题。通过分析发现MLP模块是主要参数来源，DGMR采用Gram-Schmidt权重剪枝策略，有效去除MLP隐藏层冗余神经元，同时通过保留权重多样性来确保性能恢复。实验证明，该方法能在仅需少量无标签数据恢复性能的情况下，实现大型视觉Transformer参数和FLOPs的显著（超过57%，最高71.5%）且近乎无损的缩减。", "keywords": "大型视觉Transformer, MLP缩减, 模型剪枝, Gram-Schmidt, 权重多样性", "comments": "这项工作通过专注于MLP模块的优化，提出了一种新颖的剪枝策略，有效地解决了大型视觉Transformer的部署成本问题。其创新点在于结合了Gram-Schmidt剪枝和多样性保留机制，确保了在大幅度压缩的同时性能的维持和快速恢复，对于推动大型模型在资源受限环境下的应用具有重要意义。"}}
{"id": "2506.06363", "title": "ChemGraph: An Agentic Framework for Computational Chemistry Workflows", "authors": ["Thang D. Pham", "Aditya Tanikanti", "Murat Keçeli"], "summary": "Atomistic simulations are essential tools in chemistry and materials science,\naccelerating the discovery of novel catalysts, energy storage materials, and\npharmaceuticals. However, running these simulations remains challenging due to\nthe wide range of computational methods, diverse software ecosystems, and the\nneed for expert knowledge and manual effort for the setup, execution, and\nvalidation stages. In this work, we present ChemGraph, an agentic framework\npowered by artificial intelligence and state-of-the-art simulation tools to\nstreamline and automate computational chemistry and materials science\nworkflows. ChemGraph leverages graph neural network-based foundation models for\naccurate yet computationally efficient calculations and large language models\n(LLMs) for natural language understanding, task planning, and scientific\nreasoning to provide an intuitive and interactive interface. Users can perform\ntasks such as molecular structure generation, single-point energy, geometry\noptimization, vibrational analysis, and thermochemistry calculations with\nmethods ranging from tight-binding and machine learning interatomic potentials\nto density functional theory or wave function theory-based methods. We evaluate\nChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs\n(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,\nwhile more complex tasks benefit from using larger models like GPT-4o.\nImportantly, we show that decomposing complex tasks into smaller subtasks\nthrough a multi-agent framework enables smaller LLM models to match or exceed\nGPT-4o's performance in specific scenarios.", "comment": null, "cate": "physics.chem-ph", "url": "http://arxiv.org/abs/2506.06363v1", "AI": {"title_translation": "ChemGraph：一个用于计算化学工作流程的智能体框架", "tldr": "ChemGraph是一个由AI驱动的智能体框架，旨在自动化和简化计算化学工作流程，它结合了图神经网络和大型语言模型，并在基准任务上表现出色，尤其是在多智能体分解复杂任务时。", "motivation": "原子模拟在化学和材料科学中至关重要，但其运行复杂且需要专业知识和大量手动工作，这限制了其广泛应用和效率。", "method": "提出了ChemGraph框架，它结合了AI（图神经网络基础模型用于精确高效计算，大型语言模型用于自然语言理解、任务规划和科学推理）和先进模拟工具。它通过多智能体框架将复杂任务分解为更小的子任务。", "result": "ChemGraph在13个基准任务上进行了评估。较小的LLM（如GPT-4o-mini、Claude-3.5-haiku、Qwen2.5-14B）在简单工作流程中表现良好，而复杂任务则受益于GPT-4o等大型模型。重要的是，通过多智能体框架将复杂任务分解为子任务，较小的LLM模型在特定场景下能达到或超越GPT-4o的性能。", "conclusion": "ChemGraph成功地自动化和简化了计算化学工作流程，通过结合图神经网络和大型语言模型提供了直观的交互界面。多智能体分解策略显著提升了小型LLM处理复杂任务的能力，使其在计算化学领域具有广阔的应用前景。", "translation": "原子模拟是化学和材料科学中不可或缺的工具，它加速了新型催化剂、储能材料和药物的发现。然而，运行这些模拟仍然具有挑战性，原因在于计算方法种类繁多、软件生态系统多样化，以及在设置、执行和验证阶段需要专业知识和手动操作。在这项工作中，我们提出了ChemGraph，一个由人工智能和最先进的模拟工具驱动的智能体框架，旨在简化和自动化计算化学和材料科学工作流程。ChemGraph利用基于图神经网络的基础模型进行准确且计算高效的计算，并利用大型语言模型（LLMs）进行自然语言理解、任务规划和科学推理，从而提供直观和交互式的界面。用户可以执行分子结构生成、单点能量、几何优化、振动分析和热化学计算等任务，方法涵盖紧束缚和机器学习原子间势到密度泛函理论或基于波函数理论的方法。我们评估了ChemGraph在13个基准任务上的表现，并证明较小的LLM（GPT-4o-mini、Claude-3.5-haiku、Qwen2.5-14B）在简单工作流程中表现良好，而更复杂的任务则受益于使用GPT-4o等大型模型。重要的是，我们表明，通过多智能体框架将复杂任务分解为更小的子任务，使得较小的LLM模型在特定场景下能够匹配或超越GPT-4o的性能。", "summary": "ChemGraph是一个创新的AI驱动智能体框架，旨在自动化和简化计算化学和材料科学的工作流程。它结合了图神经网络基础模型进行高效计算，并利用大型语言模型进行任务规划和科学推理。该框架支持多种计算任务和方法，并在13项基准测试中表现出色。研究发现，通过多智能体分解复杂任务，小型LLM也能达到或超越大型LLM的性能，这极大地提升了计算化学模拟的效率和可访问性。", "keywords": "计算化学, 智能体框架, 大型语言模型, 图神经网络, 自动化", "comments": "该论文提出了一种新颖的智能体框架ChemGraph，它有效地整合了AI（GNN和LLM）与计算化学模拟。其创新点在于利用LLM进行高级任务规划和推理，并引入多智能体分解策略，显著提升了小型LLM处理复杂任务的能力，降低了对超大型模型的依赖。这对于推动计算化学的自动化和普及具有重要意义。"}}
{"id": "2506.08292", "title": "From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium", "authors": ["Xie Yi", "Zhanke Zhou", "Chentao Cao", "Qiyu Niu", "Tongliang Liu", "Bo Han"], "summary": "Multi-agent frameworks can substantially boost the reasoning power of large\nlanguage models (LLMs), but they typically incur heavy computational costs and\nlack convergence guarantees. To overcome these challenges, we recast multi-LLM\ncoordination as an incomplete-information game and seek a Bayesian Nash\nequilibrium (BNE), in which each agent optimally responds to its probabilistic\nbeliefs about the strategies of others. We introduce Efficient Coordination via\nNash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that\nmarries distributed reasoning with centralized final output. Under ECON, each\nLLM independently selects responses that maximize its expected reward,\nconditioned on its beliefs about co-agents, without requiring costly\ninter-agent exchanges. We mathematically prove that ECON attains a markedly\ntighter regret bound than non-equilibrium multi-agent schemes. Empirically,\nECON outperforms existing multi-LLM approaches by 11.2% on average across six\nbenchmarks spanning complex reasoning and planning tasks. Further experiments\ndemonstrate ECON's ability to flexibly incorporate additional models,\nconfirming its scalability and paving the way toward larger, more powerful\nmulti-LLM ensembles. The code is publicly available at:\nhttps://github.com/tmlr-group/ECON.", "comment": "Accepted by ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08292v1", "AI": {"title_translation": "从辩论到均衡：基于信念的多智能体LLM通过贝叶斯纳什均衡进行推理", "tldr": "提出ECON框架，通过贝叶斯纳什均衡实现多智能体LLM高效推理，解决了计算成本高和收敛性问题，并在多个基准测试中表现优异。", "motivation": "现有的多智能体LLM框架计算成本高昂且缺乏收敛性保证，限制了其推理能力的提升。", "method": "将多LLM协调重构为不完全信息博弈，并寻求贝叶斯纳什均衡（BNE）。引入了高效协调纳什均衡（ECON）框架，这是一种分层强化学习范式，结合了分布式推理和集中式最终输出。ECON中的每个LLM根据其对其他智能体策略的概率信念独立选择响应，以最大化预期奖励，无需昂贵的智能体间交换。", "result": "证明了ECON比非均衡多智能体方案具有更紧密的遗憾界。在六个涵盖复杂推理和规划任务的基准测试中，ECON平均优于现有多LLM方法11.2%。实验还表明ECON能够灵活地整合额外的模型，证实了其可扩展性。", "conclusion": "ECON框架通过引入博弈论概念和分层强化学习，有效解决了多智能体LLM推理中的效率和收敛性问题，展现出卓越的性能和良好的可扩展性。", "translation": "多智能体框架可以显著提升大型语言模型（LLMs）的推理能力，但它们通常会产生高昂的计算成本且缺乏收敛性保证。为了克服这些挑战，我们将多LLM协调重构为不完全信息博弈，并寻求贝叶斯纳什均衡（BNE），其中每个智能体对其关于其他智能体策略的概率信念做出最优响应。我们引入了通过纳什均衡进行高效协调（ECON）框架，这是一种分层强化学习范式，它将分布式推理与集中式最终输出相结合。在ECON下，每个LLM独立选择响应，以最大化其预期奖励，并以其对合作智能体的信念为条件，而无需昂贵的智能体间交换。我们数学证明了ECON比非均衡多智能体方案获得了明显更紧密的遗憾界。经验上，ECON在涵盖复杂推理和规划任务的六个基准测试中，平均优于现有多LLM方法11.2%。进一步的实验证明了ECON灵活整合额外模型的能力，证实了其可扩展性，并为更大、更强大的多LLM集成铺平了道路。代码已公开发布于：https://github.com/tmlr-group/ECON。", "summary": "本文提出了ECON框架，通过将多LLM协调建模为不完全信息博弈并寻求贝叶斯纳什均衡，旨在解决多智能体LLM推理中高计算成本和缺乏收敛性的问题。ECON是一种分层强化学习范式，允许LLM独立优化响应，无需频繁通信。数学证明和实验结果表明，ECON在遗憾界方面表现更优，并在多个基准测试中显著超越现有方法，同时展现出良好的可扩展性。", "keywords": "多智能体LLM, 贝叶斯纳什均衡, 强化学习, 高效协调, 可扩展性", "comments": "这篇论文的创新点在于将博弈论中的贝叶斯纳什均衡引入到多智能体LLM的协调中，通过构建一个分层强化学习框架（ECON），有效地解决了传统多智能体LLM面临的计算效率和收敛性问题。其“信念驱动”的机制减少了智能体间的通信开销，并通过数学证明和实验验证了其优越的性能和可扩展性，为未来构建更强大的多LLM系统提供了新的范式。"}}
{"id": "2506.08979", "title": "Rethinking Range-View LiDAR Segmentation in Adverse Weather", "authors": ["Longyu Yang", "Ping Hu", "Lu Zhang", "Jun Liu", "Yap-Peng Tan", "Heng Tao Shen", "Xiaofeng Zhu"], "summary": "LiDAR segmentation has emerged as an important task to enrich multimedia\nexperiences and analysis. Range-view-based methods have gained popularity due\nto their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08979v1", "AI": {"title_translation": "恶劣天气下距离视图激光雷达分割的再思考", "tldr": "本文提出了一种模块化轻量级框架，通过分离处理几何属性和反射强度，显著提高了距离视图激光雷达分割模型在恶劣天气下的泛化能力。", "motivation": "距离视图激光雷达分割方法在恶劣天气条件下的泛化性能尚未得到充分探索，限制了其在现实世界环境中的可靠性。", "method": "提出了一种模块化、轻量级的框架，不改变现有模型的核心架构。该方法将标准距离视图网络的初始主干块重构为两个分支，分别处理几何属性和反射强度。具体包括：几何异常抑制（GAS）模块以减少天气引起的空间噪声影响；反射率畸变校准（RDC）模块通过记忆引导自适应实例归一化校正反射率畸变。处理后的特征融合后传递给原始分割管道。", "result": "在不同基准和基线模型上进行的广泛实验表明，该方法在最小推理开销的情况下显著提高了对恶劣天气的泛化能力。", "conclusion": "该研究为现实世界的激光雷达分割提供了一个实用且有效的解决方案。", "translation": "激光雷达分割已成为丰富多媒体体验和分析的一项重要任务。基于距离视图的方法因其高计算效率和与实时部署的兼容性而广受欢迎。然而，它们在恶劣天气条件下的泛化性能仍未得到充分探索，限制了其在现实世界环境中的可靠性。在这项工作中，我们识别并分析了影响距离视图激光雷达分割在恶劣天气下泛化的独特挑战。为了应对这些挑战，我们提出了一种模块化、轻量级的框架，在不改变现有模型核心架构的情况下增强鲁棒性。我们的方法将标准距离视图网络的初始主干块重构为两个分支，分别处理几何属性和反射强度。具体来说，几何异常抑制（GAS）模块减少了天气引起的空间噪声的影响，反射率畸变校准（RDC）模块通过记忆引导自适应实例归一化校正反射率畸变。处理后的特征随后被融合并传递到原始分割管道。在不同基准和基线模型上进行的广泛实验表明，我们的方法在最小推理开销的情况下显著提高了对恶劣天气的泛化能力，为现实世界的激光雷达分割提供了一个实用且有效的解决方案。", "summary": "本文针对恶劣天气下距离视图激光雷达分割的泛化挑战，提出了一种模块化轻量级框架。该框架通过将网络初始主干块分为几何属性和反射强度处理两个分支，并引入几何异常抑制（GAS）模块和反射率畸变校准（RDC）模块，有效减少了天气引起的噪声和畸变。实验证明，该方法在保持低推理开销的同时，显著提升了模型在恶劣天气下的泛化性能，为实际应用提供了可靠方案。", "keywords": "激光雷达分割, 距离视图, 恶劣天气, 泛化, 鲁棒性", "comments": "本文提出了一种新颖且实用的方法来解决距离视图激光雷达在恶劣天气下的泛化问题。其创新点在于通过双分支设计分别处理几何和反射特性，并引入了GAS和RDC模块，有效地抑制了天气干扰。该框架的模块化和轻量级特性使其易于集成到现有模型中，且推理开销小，对于提升激光雷达在复杂环境中的可靠性具有重要意义。"}}
{"id": "2506.08596", "title": "Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems", "authors": ["Guyang Zhang", "Waleed Abdulla"], "summary": "Transformers have become the architecture of choice for learning long-range\ndependencies, yet their adoption in hyperspectral imaging (HSI) is still\nemerging. We reviewed more than 300 papers published up to 2025 and present the\nfirst end-to-end survey dedicated to Transformer-based HSI classification. The\nstudy categorizes every stage of a typical pipeline-pre-processing, patch or\npixel tokenization, positional encoding, spatial-spectral feature extraction,\nmulti-head self-attention variants, skip connections, and loss design-and\ncontrasts alternative design choices with the unique spatial-spectral\nproperties of HSI. We map the field's progress against persistent obstacles:\nscarce labeled data, extreme spectral dimensionality, computational overhead,\nand limited model explainability. Finally, we outline a research agenda\nprioritizing valuable public data sets, lightweight on-edge models,\nillumination and sensor shifts robustness, and intrinsically interpretable\nattention mechanisms. Our goal is to guide researchers in selecting, combining,\nor extending Transformer components that are truly fit for purpose for\nnext-generation HSI applications.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08596v1", "AI": {"title_translation": "Transformer遇上高光谱成像：模型、挑战与开放问题的综合研究", "tldr": "本文是对Transformer在高光谱成像（HSI）分类中应用的首次端到端综述，回顾了300多篇论文，分类了典型流程的各个阶段，并提出了未来研究议程。", "motivation": "尽管Transformer在学习长距离依赖方面表现出色，但其在高光谱成像（HSI）领域的应用仍处于新兴阶段。本研究旨在指导研究人员选择、组合或扩展适合下一代HSI应用的Transformer组件。", "method": "本研究回顾了截至2025年发表的300多篇论文，并首次提供了专门针对基于Transformer的HSI分类的端到端综述。研究对典型流程的每个阶段进行了分类，包括预处理、块或像素token化、位置编码、空谱特征提取、多头自注意力变体、跳跃连接和损失设计，并将替代设计选择与HSI独特的空谱特性进行了对比。", "result": "研究对基于Transformer的HSI分类典型流程的每个阶段进行了详细分类，并将其与HSI独特的空谱特性进行了对比。此外，研究还揭示了该领域在稀缺标记数据、极端光谱维度、计算开销和有限模型可解释性等持续障碍方面取得的进展。", "conclusion": "本研究提出了一个优先级的研究议程，包括有价值的公共数据集、轻量级边缘模型、光照和传感器漂移的鲁棒性以及内在可解释的注意力机制。目标是指导研究人员选择、组合或扩展真正适用于下一代HSI应用的Transformer组件。", "translation": "Transformer已成为学习长距离依赖的首选架构，但其在高光谱成像（HSI）中的应用仍处于新兴阶段。我们回顾了截至2025年发表的300多篇论文，并首次提出了专门针对基于Transformer的HSI分类的端到端综述。该研究对典型流程的每个阶段——预处理、块或像素token化、位置编码、空谱特征提取、多头自注意力变体、跳跃连接和损失设计——进行了分类，并将替代设计选择与HSI独特的空谱特性进行了对比。我们将该领域的进展与持续存在的障碍进行了映射：稀缺的标记数据、极端光谱维度、计算开销和有限的模型可解释性。最后，我们概述了一个研究议程，优先考虑有价值的公共数据集、轻量级边缘模型、光照和传感器漂移的鲁棒性以及内在可解释的注意力机制。我们的目标是指导研究人员选择、组合或扩展真正适用于下一代HSI应用的Transformer组件。", "summary": "本文是对Transformer在高光谱成像（HSI）分类中应用的首次全面综述，涵盖了300多篇论文。研究详细分类了Transformer在高光谱成像处理流程中的各个阶段，并对比了不同的设计选择。同时，文章指出了该领域面临的挑战，如数据稀缺、维度灾难、计算成本和模型可解释性不足。最后，论文提出了未来的研究方向，旨在推动Transformer在下一代HSI应用中的发展。", "keywords": "Transformer, 高光谱成像, HSI分类, 综述, 挑战", "comments": "这是一篇非常有价值的综述性论文，填补了Transformer在高光谱成像领域应用方面系统性研究的空白。其创新之处在于首次提供了端到端的全面视角，并对整个处理流程进行了细致的分类和对比。论文不仅总结了现有进展，更重要的是明确指出了当前面临的挑战并给出了具体的研究议程，对未来该领域的研究具有重要的指导意义。其全面性、前瞻性和实用性是其主要亮点。"}}
{"id": "2506.07675", "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "authors": ["Yuyang Song", "Hanxu Yan", "Jiale Lao", "Yibo Wang", "Yufei Li", "Yuanchun Zhou", "Jianguo Wang", "Mingjie Tang"], "summary": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle.", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.07675v1", "AI": {"title_translation": "QUITE：一个超越规则的LLM代理查询重写系统", "tldr": "QUITE是一个基于LLM代理的查询重写系统，它通过多代理框架、重写中间件和提示注入技术，克服了传统规则方法的局限性，显著提高了SQL查询性能和重写覆盖范围。", "motivation": "现有SQL查询重写方法主要依赖预定义规则，但存在难以发现和验证新规则、无法泛化到新查询模式以及无法表达某些重写技术的局限性，导致性能下降。人类专家重写能力强但不可扩展，而大型语言模型（LLMs）具备接近人类的语义和推理能力，但直接应用存在幻觉问题，可能导致非等价和次优查询。", "method": "本文提出了QUITE（query rewrite）系统，一个无训练且反馈感知的基于LLM代理的系统。该系统包含三个关键设计：1. 设计一个由有限状态机（FSM）控制的多代理框架，使LLMs能够使用外部工具并利用实时数据库反馈增强重写过程。2. 开发一个重写中间件，以增强LLMs生成优化查询等效项的能力。3. 采用一种新颖的提示注入技术，以改进重写查询的执行计划。", "result": "QUITE将查询执行时间比最先进的方法缩短了高达35.8%，并且比现有方法多生成了24.1%的重写，覆盖了早期系统未处理的查询案例。", "conclusion": "QUITE通过利用LLM代理和创新的架构，成功地扩展了SQL查询重写的范围和效率，超越了传统的基于规则的方法，并显著提高了查询性能。", "translation": "查询重写将SQL查询转换为语义等价的、运行效率更高的形式。现有方法主要依赖预定义的重写规则，但它们只能处理有限的查询子集，并可能导致性能下降。这种局限性源于基于规则的查询重写面临的三个挑战：(1) 难以发现和验证新规则，(2) 固定重写规则无法泛化到新的查询模式，以及 (3) 某些重写技术无法表示为固定规则。鉴于人类专家表现出显著更好的重写能力但存在可扩展性问题，以及大型语言模型（LLMs）展示出接近人类水平的语义和推理能力，我们提出了一种使用LLMs超越规则重写SQL查询的新方法。由于LLMs的幻觉问题，直接应用LLMs通常会导致非等价和次优查询。为了解决这个问题，我们提出了QUITE（query rewrite），一个无训练且反馈感知的基于LLM代理的系统，它将SQL查询重写为语义等价的形式，性能显著提高，并且与基于规则的方法相比，覆盖了更广泛的查询模式和重写策略。首先，我们设计了一个由有限状态机（FSM）控制的多代理框架，使LLMs能够使用外部工具，并通过实时数据库反馈增强重写过程。其次，我们开发了一个重写中间件，以增强LLMs生成优化查询等效项的能力。最后，我们采用一种新颖的提示注入技术，以改进重写查询的执行计划。广泛的实验表明，QUITE将查询执行时间比最先进的方法缩短了高达35.8%，并且比现有方法多生成了24.1%的重写，覆盖了早期系统未处理的查询案例。", "summary": "本文提出QUITE，一个基于LLM代理的SQL查询重写系统，旨在克服传统规则方法的局限性。QUITE通过多代理框架、重写中间件和提示注入技术，有效利用LLM的语义能力，同时解决其幻觉问题。实验结果表明，QUITE显著提升了查询执行效率和重写覆盖率，超越了现有SOTA方法。", "keywords": "SQL查询重写, LLM代理, 数据库优化, 查询性能, QUITE", "comments": "QUITE的创新之处在于其将LLM的强大语义理解和推理能力引入到SQL查询重写领域，并通过多代理框架、实时反馈和提示注入等机制有效抑制了LLM的幻觉问题。这种结合外部工具和反馈的LLM代理范式，为其他需要精确性和可靠性的领域提供了新的思路。其重要性在于显著提升了查询优化能力和覆盖范围，有望在数据库性能调优中发挥关键作用。"}}
{"id": "2506.08295", "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?", "authors": ["Zhanke Zhou", "Xiao Feng", "Zhaocheng Zhu", "Jiangchao Yao", "Sanmi Koyejo", "Bo Han"], "summary": "While existing benchmarks probe the reasoning abilities of large language\nmodels (LLMs) across diverse domains, they predominantly assess passive\nreasoning, providing models with all the information needed to reach a\nsolution. By contrast, active reasoning-where an LLM must interact with\nexternal systems to acquire missing evidence or data-has received little\nsystematic attention. To address this shortfall, we present AR-Bench, a novel\nbenchmark designed explicitly to evaluate an LLM's active reasoning skills.\nAR-Bench comprises three task families-detective cases, situation puzzles, and\nguessing numbers-that together simulate real-world, agentic scenarios and\nmeasure performance across commonsense, logical, and symbolic reasoning\nchallenges. Empirical evaluation on AR-Bench demonstrates that contemporary\nLLMs exhibit pronounced difficulties with active reasoning: they frequently\nfail to acquire or leverage the information needed to solve tasks. This gap\nhighlights a stark divergence between their passive and active reasoning\nabilities. Moreover, ablation studies indicate that even advanced strategies,\nsuch as tree-based searching or post-training approaches, yield only modest\ngains and fall short of the levels required for real-world deployment.\nCollectively, these findings highlight the critical need to advance methodology\nfor active reasoning, e.g., incorporating interactive learning, real-time\nfeedback loops, and environment-aware objectives for training. The benchmark is\npublicly available at: https://github.com/tmlr-group/AR-Bench.", "comment": "Accepted by ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08295v1", "AI": {"title_translation": "从被动推理到主动推理：大型语言模型在信息不完整的情况下能否提出正确的问题？", "tldr": "现有基准主要评估LLMs的被动推理能力，本研究引入AR-Bench基准评估LLMs的主动推理能力，发现LLMs在此方面表现不佳，凸显了开发新方法的必要性。", "motivation": "现有LLM推理能力基准主要评估被动推理，即向模型提供所有必要信息。然而，LLM需要与外部系统交互以获取缺失证据或数据的主动推理能力却鲜有系统关注。", "method": "提出了一个名为AR-Bench的新型基准，专门用于评估LLM的主动推理能力。AR-Bench包含侦探案例、情境谜题和数字猜测三个任务系列，它们共同模拟真实世界的代理场景，并衡量LLM在常识、逻辑和符号推理挑战中的表现。", "result": "对AR-Bench的实证评估表明，当前LLM在主动推理方面表现出显著困难，它们经常未能获取或利用解决任务所需的信息。这突显了LLM被动和主动推理能力之间的巨大差异。消融研究表明，即使是树形搜索或后训练等高级策略也仅能带来适度提升，远未达到实际部署所需水平。", "conclusion": "这些发现共同强调了推进主动推理方法学的迫切需求，例如在训练中整合交互式学习、实时反馈循环和环境感知目标。", "translation": "尽管现有基准在不同领域探测大型语言模型（LLM）的推理能力，但它们主要评估被动推理，即向模型提供解决问题所需的所有信息。相比之下，主动推理——LLM必须与外部系统交互以获取缺失证据或数据——却很少受到系统关注。为了弥补这一不足，我们提出了AR-Bench，一个专门设计用于评估LLM主动推理技能的新型基准。AR-Bench包含三个任务系列——侦探案例、情境谜题和数字猜测——它们共同模拟真实世界的代理场景，并衡量在常识、逻辑和符号推理挑战中的表现。对AR-Bench的实证评估表明，当代LLM在主动推理方面表现出显著困难：它们经常未能获取或利用解决任务所需的信息。这一差距凸显了它们被动和主动推理能力之间的巨大差异。此外，消融研究表明，即使是高级策略，如基于树的搜索或后训练方法，也仅能带来适度的提升，远未达到实际部署所需的水平。总的来说，这些发现强调了推进主动推理方法学的迫切需求，例如在训练中整合交互式学习、实时反馈循环和环境感知目标。该基准可在以下网址公开获取：https://github.com/tmlr-group/AR-Bench。", "summary": "本文针对大型语言模型（LLM）主动推理能力评估的不足，提出了一个名为AR-Bench的新型基准。该基准包含三类任务，旨在模拟LLM在信息不完整情况下通过与外部系统交互来获取信息的真实世界场景。研究发现，现有LLM在主动推理方面表现显著不足，即使是先进策略也效果有限，这表明LLM的被动与主动推理能力存在巨大差异，并强调了开发新方法以提升主动推理能力的重要性。", "keywords": "大型语言模型, 主动推理, 基准测试, 信息不完整, AR-Bench", "comments": "这篇论文通过引入AR-Bench基准，创新性地填补了LLM主动推理能力系统评估的空白。它揭示了LLM在主动信息获取和利用方面的显著局限性，即便在被动推理表现优异的情况下。这项工作的重要性在于它明确指出了当前LLM能力的一个关键瓶颈，并为未来研究指明了方向，即需要开发更具交互性、环境感知和反馈驱动的训练方法，以使LLM更接近真实世界的代理能力。"}}
{"id": "2506.08997", "title": "SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction", "authors": ["Fabian Immel", "Jan-Hendrik Pauls", "Richard Fehler", "Frank Bieder", "Jonas Merkert", "Christoph Stiller"], "summary": "Autonomous vehicles rely on detailed and accurate environmental information\nto operate safely. High definition (HD) maps offer a promising solution, but\ntheir high maintenance cost poses a significant barrier to scalable deployment.\nThis challenge is addressed by online HD map construction methods, which\ngenerate local HD maps from live sensor data. However, these methods are\ninherently limited by the short perception range of onboard sensors. To\novercome this limitation and improve general performance, recent approaches\nhave explored the use of standard definition (SD) maps as prior, which are\nsignificantly easier to maintain. We propose SDTagNet, the first online HD map\nconstruction method that fully utilizes the information of widely available SD\nmaps, like OpenStreetMap, to enhance far range detection accuracy. Our approach\nintroduces two key innovations. First, in contrast to previous work, we\nincorporate not only polyline SD map data with manually selected classes, but\nadditional semantic information in the form of textual annotations. In this\nway, we enrich SD vector map tokens with NLP-derived features, eliminating the\ndependency on predefined specifications or exhaustive class taxonomies. Second,\nwe introduce a point-level SD map encoder together with orthogonal element\nidentifiers to uniformly integrate all types of map elements. Experiments on\nArgoverse 2 and nuScenes show that this boosts map perception performance by up\nto +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP\n(+20%) w.r.t. previous approaches that already use SD map priors. Code is\navailable at https://github.com/immel-f/SDTagNet", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08997v1", "AI": {"title_translation": "SDTagNet：利用文本标注导航地图进行在线高清地图构建", "tldr": "SDTagNet是首个利用文本标注的标准清晰度（SD）地图信息来增强在线高清（HD）地图构建中远距离检测精度的方法，显著优于现有方法。", "motivation": "自动驾驶汽车依赖详细准确的环境信息，但高清（HD）地图的高昂维护成本阻碍了其大规模部署。虽然在线HD地图构建方法能从实时传感器数据生成局部HD地图，但受限于车载传感器感知范围短。现有方法使用标准清晰度（SD）地图作为先验信息，但仅限于手动选择类别的折线数据。", "method": "我们提出了SDTagNet，这是首个充分利用广泛可用的SD地图（如OpenStreetMap）信息的在线HD地图构建方法。该方法引入了两项关键创新：1. 除了折线SD地图数据外，还结合了文本标注形式的额外语义信息，通过NLP派生特征丰富SD矢量地图令牌，消除了对预定义规范或详尽类别分类的依赖。2. 引入了点级别SD地图编码器和正交元素标识符，以统一整合所有类型的地图元素。", "result": "在Argoverse 2和nuScenes数据集上的实验表明，SDTagNet将地图感知性能提升了高达+5.9 mAP（+45%）（相对于没有先验信息的地图构建）和高达+3.2 mAP（+20%）（相对于已使用SD地图先验的现有方法）。", "conclusion": "SDTagNet通过充分利用文本标注的SD地图信息，显著提高了在线HD地图构建的远距离检测精度和整体性能，为自动驾驶汽车的地图构建提供了更有效和可扩展的解决方案。", "translation": "自动驾驶汽车依赖详细准确的环境信息才能安全运行。高清（HD）地图提供了一个有前景的解决方案，但其高昂的维护成本对其大规模部署构成了重大障碍。在线HD地图构建方法通过从实时传感器数据生成局部HD地图来解决这一挑战。然而，这些方法固有地受限于车载传感器短促的感知范围。为了克服这一限制并提高整体性能，最近的方法探索了使用标准清晰度（SD）地图作为先验信息，这些地图的维护成本要低得多。我们提出了SDTagNet，这是第一个充分利用广泛可用的SD地图（如OpenStreetMap）信息的在线HD地图构建方法，以提高远距离检测精度。我们的方法引入了两项关键创新。首先，与以前的工作不同，我们不仅包含了手动选择类别的折线SD地图数据，还包含了以文本标注形式的额外语义信息。通过这种方式，我们用NLP派生的特征丰富了SD矢量地图令牌，消除了对预定义规范或详尽类别分类的依赖。其次，我们引入了点级别SD地图编码器以及正交元素标识符，以统一整合所有类型的地图元素。在Argoverse 2和nuScenes上的实验表明，这使得地图感知性能相对于没有先验信息的地图构建提高了高达+5.9 mAP（+45%），相对于已经使用SD地图先验的现有方法提高了高达+3.2 mAP（+20%）。代码可在https://github.com/immel-f/SDTagNet获取。", "summary": "自动驾驶汽车需要详细的高清地图，但其高昂的维护成本限制了部署。针对在线高清地图构建受限于传感器感知范围短的问题，本文提出了SDTagNet。该方法是首个充分利用文本标注的标准清晰度（SD）地图信息来增强远距离检测精度的方法。SDTagNet创新性地结合了SD地图的折线数据和NLP派生的文本标注语义信息，并引入了点级别编码器统一整合地图元素。实验结果表明，SDTagNet在地图感知性能上取得了显著提升，比无先验方法提高了45%，比现有使用SD地图先验的方法提高了20%。", "keywords": "高清地图, 在线构建, 标准清晰度地图, 文本标注, 自动驾驶", "comments": "该论文的创新之处在于其首次将文本标注的SD地图信息引入在线高清地图构建，并通过NLP技术从文本中提取语义特征，极大地丰富了地图的先验信息，摆脱了对预定义分类的依赖。点级别编码器的引入也确保了不同类型地图元素的统一集成。这项工作为降低高清地图的维护成本和提高自动驾驶的感知能力提供了一个重要且实用的解决方案，具有重要的应用价值。"}}
{"id": "2506.08611", "title": "Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation", "authors": ["Shiji Zhao", "Chi Chen", "Ranjie Duan", "Xizhe Wang", "Xingxing Wei"], "summary": "Adversarial Training (AT) is widely recognized as an effective approach to\nenhance the adversarial robustness of Deep Neural Networks. As a variant of AT,\nAdversarial Robustness Distillation (ARD) has shown outstanding performance in\nenhancing the robustness of small models. However, both AT and ARD face robust\nfairness issue: these models tend to display strong adversarial robustness\nagainst some classes (easy classes) while demonstrating weak adversarial\nrobustness against others (hard classes). This paper explores the underlying\nfactors of this problem and points out the smoothness degree of soft labels for\ndifferent classes significantly impacts the robust fairness from both empirical\nobservation and theoretical analysis. Based on the above exploration, we\npropose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge\nDistillation framework to enhance the adversarial robust fairness.\nSpecifically, ABSLD adaptively reduces the student's error risk gap between\ndifferent classes, which is accomplished by adjusting the class-wise smoothness\ndegree of teacher's soft labels during the training process, and the adjustment\nis managed by assigning varying temperatures to different classes.\nAdditionally, as a label-based approach, ABSLD is highly adaptable and can be\nintegrated with the sample-based methods. Extensive experiments demonstrate\nABSLD outperforms state-of-the-art methods on the comprehensive performance of\nrobustness and fairness.", "comment": "arXiv admin note: text overlap with arXiv:2312.05508", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08611v1", "AI": {"title_translation": "通过抗偏差软标签蒸馏实现类间公平对抗训练", "tldr": "对抗训练（AT）和对抗鲁棒性蒸馏（ARD）存在类间对抗鲁棒性不公平问题。本文提出了一种名为抗偏差软标签蒸馏（ABSLD）的新方法，通过调整不同类别软标签的平滑度来提升鲁棒公平性。", "motivation": "对抗训练（AT）和对抗鲁棒性蒸馏（ARD）都面临鲁棒性公平性问题，即模型对某些类别（易类）表现出强大的对抗鲁棒性，而对其他类别（难类）则表现出较弱的对抗鲁棒性。本文旨在探究这一问题的潜在因素并提出解决方案。", "method": "本文提出了一种在知识蒸馏框架内的抗偏差软标签蒸馏（ABSLD）方法，以增强对抗鲁棒性公平性。ABSLD通过在训练过程中调整教师模型软标签的类间平滑度（通过为不同类别分配不同的温度），自适应地减少学生模型在不同类别之间的错误风险差距。作为一种基于标签的方法，ABSLD具有高度适应性，可以与基于样本的方法相结合。", "result": "广泛的实验表明，ABSLD在鲁棒性和公平性的综合性能上优于现有最先进的方法。", "conclusion": "本文通过探究软标签平滑度对鲁棒性公平性的影响，提出了一种新颖的抗偏差软标签蒸馏（ABSLD）方法。该方法通过自适应调整类间软标签平滑度有效解决了对抗训练中的类间鲁棒性公平性问题，并展现出优越的性能。", "translation": "对抗训练（AT）被广泛认为是增强深度神经网络对抗鲁棒性的有效方法。作为AT的一种变体，对抗鲁棒性蒸馏（ARD）在增强小型模型鲁棒性方面表现出色。然而，AT和ARD都面临鲁棒性公平性问题：这些模型倾向于对某些类别（容易类别）表现出强大的对抗鲁棒性，而对其他类别（困难类别）则表现出较弱的对抗鲁棒性。本文探讨了这一问题的潜在因素，并指出不同类别软标签的平滑度在经验观察和理论分析上都显著影响鲁棒性公平性。基于上述探索，我们提出在知识蒸馏框架内使用抗偏差软标签蒸馏（ABSLD）来增强对抗鲁棒性公平性。具体而言，ABSLD通过在训练过程中调整教师模型软标签的类间平滑度（通过为不同类别分配不同的温度来完成），自适应地减少学生模型在不同类别之间的错误风险差距。此外，作为一种基于标签的方法，ABSLD具有高度适应性，可以与基于样本的方法相结合。广泛的实验表明，ABSLD在鲁棒性和公平性的综合性能上优于现有最先进的方法。", "summary": "本文旨在解决对抗训练（AT）和对抗鲁棒性蒸馏（ARD）中存在的类间鲁棒性不公平问题，即模型对不同类别的对抗鲁棒性表现不均衡。研究发现，不同类别软标签的平滑度是影响鲁棒公平性的关键因素。基于此，作者提出了抗偏差软标签蒸馏（ABSLD）方法。ABSLD在知识蒸馏框架下，通过自适应调整教师模型软标签的类间平滑度（为不同类别分配不同温度），以减小学生模型在不同类别间的错误风险差距，从而增强对抗鲁棒公平性。实验证明，ABSLD在鲁棒性和公平性方面均优于现有SOTA方法，并且具有良好的兼容性。", "keywords": "对抗训练, 鲁棒性公平性, 知识蒸馏, 软标签, 偏差校正", "comments": "本文创新性地从软标签平滑度这一新颖角度出发，深入探讨了对抗训练中的类间鲁棒性公平性问题，并提供了理论和实证支持。所提出的ABSLD方法通过自适应地调整类间软标签平滑度，提供了一种有效且灵活的解决方案，能够与现有基于样本的方法结合，提升了其实用性。该研究对于实现更公平、更鲁棒的深度学习模型具有重要意义。"}}
{"id": "2506.08298", "title": "H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs", "authors": ["Trung-Kien Nguyen", "Heng Ping", "Shixuan Li", "Peiyu Zhang", "Nikos Kanakaris", "Nicholas Kotov", "Paul Bogdan"], "summary": "The growing interests and applications of graph learning in diverse domains\nhave propelled the development of a unified model generalizing well across\ndifferent graphs and tasks, known as the Graph Foundation Model (GFM). Existing\nresearch has leveraged text-attributed graphs (TAGs) to tackle the\nheterogeneity in node features among graphs. However, they primarily focus on\nhomogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple\ntypes of nodes/edges reside, underexplored. To enhance the capabilities and\napplications of GFM, we introduce H$^2$GFM, a novel framework designed to\ngeneralize across both HoTAGs and HeTAGs. Our model projects diverse\nmeta-relations among graphs under a unified textual space, and employs a\ncontext encoding to capture spatial and higher-order semantic relationships. To\nachieve robust node representations, we propose a novel context-adaptive graph\ntransformer (CGT), effectively capturing information from both context\nneighbors and their relationships. Furthermore, we employ a mixture of CGT\nexperts to capture the heterogeneity in structural patterns among graph types.\nComprehensive experiments on a wide range of HoTAGs and HeTAGs as well as\nlearning scenarios demonstrate the effectiveness of our model.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08298v1", "AI": {"title_translation": "H$^2$GFM：迈向统一文本属性图上的同质性和异质性", "tldr": "H$^2$GFM是一个新的图基础模型框架，旨在统一处理同质和异质文本属性图，通过上下文编码和自适应图Transformer实现鲁棒的节点表示。", "motivation": "现有的图基础模型（GFM）主要关注同质文本属性图（HoTAGs），而对存在多种类型节点/边的异质文本属性图（HeTAGs）的探索不足，这限制了GFM的能力和应用。", "method": "提出了H$^2$GFM框架，它将图中的不同元关系投影到统一的文本空间，并采用上下文编码来捕获空间和高阶语义关系。此外，引入了上下文自适应图Transformer (CGT) 来捕获上下文邻居及其关系的信息，并使用CGT专家混合来捕获图类型之间结构模式的异质性。", "result": "在广泛的HoTAGs和HeTAGs以及学习场景上的综合实验证明了模型的有效性。", "conclusion": "H$^2$GFM成功地统一了同质和异质文本属性图的处理，增强了图基础模型的能力和应用范围。", "translation": "图学习在不同领域日益增长的兴趣和应用推动了统一模型的发展，该模型在不同图和任务中具有良好的泛化能力，被称为图基础模型（GFM）。现有研究利用文本属性图（TAGs）来解决图中节点特征的异质性。然而，它们主要关注同质TAGs（HoTAGs），而对存在多种类型节点/边的异质TAGs（HeTAGs）的探索不足。为了增强GFM的能力和应用，我们引入了H$^2$GFM，这是一个旨在在HoTAGs和HeTAGs上泛化的新型框架。我们的模型将图中不同的元关系投影到统一的文本空间中，并采用上下文编码来捕获空间和高阶语义关系。为了获得鲁棒的节点表示，我们提出了一种新颖的上下文自适应图Transformer（CGT），有效地捕获来自上下文邻居及其关系的信息。此外，我们采用CGT专家混合来捕获图类型之间结构模式的异质性。在广泛的HoTAGs和HeTAGs以及学习场景上的综合实验证明了我们模型的有效性。", "summary": "本文提出了H$^2$GFM框架，旨在解决现有图基础模型在处理异质文本属性图（HeTAGs）方面的不足，从而统一处理同质和异质文本属性图（HoTAGs和HeTAGs）。H$^2$GFM通过将元关系投影到统一文本空间、采用上下文编码以及引入上下文自适应图Transformer（CGT）及其专家混合来捕获不同图类型的结构异质性，从而实现鲁棒的节点表示。实验证明了该模型在多种图类型和学习场景下的有效性。", "keywords": "文本属性图, 图基础模型, 同质图, 异质图, 图Transformer", "comments": "该论文的创新点在于提出了H$^2$GFM框架，首次尝试统一处理同质和异质文本属性图，这对于扩展图基础模型在现实世界复杂图数据上的应用具有重要意义。通过引入上下文编码和自适应图Transformer，模型能够更全面地捕获图结构和语义信息。"}}
{"id": "2506.08612", "title": "Data-Efficient Challenges in Visual Inductive Priors: A Retrospective", "authors": ["Robert-Jan Bruintjes", "Attila Lengyel", "Osman Semih Kayhan", "Davide Zambrano", "Nergis Tömen", "Hadi Jamali-Rad", "Jan van Gemert"], "summary": "Deep Learning requires large amounts of data to train models that work well.\nIn data-deficient settings, performance can be degraded. We investigate which\nDeep Learning methods benefit training models in a data-deficient setting, by\norganizing the \"VIPriors: Visual Inductive Priors for Data-Efficient Deep\nLearning\" workshop series, featuring four editions of data-impaired challenges.\nThese challenges address the problem of training deep learning models for\ncomputer vision tasks with limited data. Participants are limited to training\nmodels from scratch using a low number of training samples and are not allowed\nto use any form of transfer learning. We aim to stimulate the development of\nnovel approaches that incorporate prior knowledge to improve the data\nefficiency of deep learning models. Successful challenge entries make use of\nlarge model ensembles that mix Transformers and CNNs, as well as heavy data\naugmentation. Novel prior knowledge-based methods contribute to success in some\nentries.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08612v1", "AI": {"title_translation": "视觉归纳先验中的数据高效挑战：回顾", "tldr": "本文回顾了“VIPriors”系列挑战赛，旨在探索在数据匮乏环境下训练深度学习模型的方法，发现模型集成、数据增强和新颖的先验知识方法是成功的关键。", "motivation": "深度学习模型通常需要大量数据才能表现良好，但在数据匮乏的环境中，模型性能会下降。本文旨在调查哪些深度学习方法能在此类数据受限环境下有效地训练模型。", "method": "通过组织四届“VIPriors：视觉归纳先验用于数据高效深度学习”系列研讨会，并设置数据受限挑战赛。参与者被限制使用少量训练样本从头开始训练模型，且不允许使用任何形式的迁移学习。", "result": "成功的挑战赛参赛作品利用了大型模型集成（混合Transformer和CNN），以及大量数据增强。新颖的基于先验知识的方法在某些参赛作品中也取得了成功。", "conclusion": "在数据受限的深度学习任务中，模型集成、数据增强以及结合新颖的先验知识是提高数据效率的有效途径。", "translation": "深度学习需要大量数据来训练表现良好的模型。在数据不足的环境中，性能可能会下降。我们通过组织“VIPriors：用于数据高效深度学习的视觉归纳先验”系列研讨会，其中包括四届数据受损挑战赛，来调查哪些深度学习方法有利于在数据不足的环境中训练模型。这些挑战赛旨在解决在数据有限的情况下训练用于计算机视觉任务的深度学习模型的问题。参与者被限制使用少量训练样本从头开始训练模型，并且不允许使用任何形式的迁移学习。我们的目标是刺激新方法的开发，这些方法将先验知识融入其中，以提高深度学习模型的数据效率。成功的挑战赛参赛作品利用了混合Transformer和CNN的大型模型集成，以及大量数据增强。新颖的基于先验知识的方法在某些参赛作品中也取得了成功。", "summary": "本文回顾了“VIPriors”系列挑战赛，该挑战赛旨在探索在数据量有限的计算机视觉任务中训练深度学习模型的方法。通过限制参与者使用少量数据从头训练模型且禁止迁移学习，挑战赛鼓励开发结合先验知识的新方法。结果显示，成功的策略包括使用大型模型集成（结合Transformer和CNN）和大量数据增强，同时新颖的基于先验知识的方法也显示出有效性。", "keywords": "数据高效深度学习, 视觉归纳先验, 数据增强, 模型集成, 数据匮乏", "comments": "这篇论文通过举办实际挑战赛来深入探讨数据高效的深度学习方法，提供了一个实证性的回顾。其创新点在于通过严格限制训练条件（禁止迁移学习，少量数据）来推动真正的数据效率解决方案。这对于资源受限或需要快速部署模型的场景具有重要意义。"}}
{"id": "2506.08309", "title": "Learnable Spatial-Temporal Positional Encoding for Link Prediction", "authors": ["Katherine Tieu", "Dongqi Fu", "Zihao Li", "Ross Maciejewski", "Jingrui He"], "summary": "Accurate predictions rely on the expressiveness power of graph deep learning\nframeworks like graph neural networks and graph transformers, where a\npositional encoding mechanism has become much more indispensable in recent\nstate-of-the-art works to record the canonical position information. However,\nthe current positional encoding is limited in three aspects: (1) most\npositional encoding methods use pre-defined, and fixed functions, which are\ninadequate to adapt to the complex attributed graphs; (2) a few pioneering\nworks proposed the learnable positional encoding but are still limited to the\nstructural information, not considering the real-world time-evolving\ntopological and feature information; (3) most positional encoding methods are\nequipped with transformers' attention mechanism to fully leverage their\ncapabilities, where the dense or relational attention is often unaffordable on\nlarge-scale structured data. Hence, we aim to develop Learnable\nSpatial-Temporal Positional Encoding in an effective and efficient manner and\npropose a simple temporal link prediction model named L-STEP. Briefly, for\nL-STEP, we (1) prove the proposed positional learning scheme can preserve the\ngraph property from the spatial-temporal spectral viewpoint, (2) verify that\nMLPs can fully exploit the expressiveness and reach transformers' performance\non that encoding, (3) change different initial positional encoding inputs to\nshow robustness, (4) analyze the theoretical complexity and obtain less\nempirical running time than SOTA, and (5) demonstrate its temporal link\nprediction out-performance on 13 classic datasets and with 10 algorithms in\nboth transductive and inductive settings using 3 different sampling strategies.\nAlso, \\name\\ obtains the leading performance in the newest large-scale TGB\nbenchmark. Our code is available at https://github.com/kthrn22/L-STEP.", "comment": "Accepted by ICML 2025. 28 pages, 1 figures, 22 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08309v1", "AI": {"title_translation": "用于链接预测的可学习时空位置编码", "tldr": "本文提出了一种名为L-STEP的可学习时空位置编码方法，用于时间链接预测，解决了现有位置编码在适应复杂图、考虑时变信息和处理大规模数据方面的局限性，并实现了卓越的性能和效率。", "motivation": "现有的位置编码方法存在三方面局限性：1) 大多使用预定义和固定函数，难以适应复杂属性图；2) 少数可学习方法仅限于结构信息，未考虑实时演化的拓扑和特征信息；3) 大多数方法依赖于Transformer的注意力机制，但在大规模结构化数据上成本过高。", "method": "本文提出了一种名为L-STEP的简单时间链接预测模型，旨在有效且高效地开发可学习时空位置编码。L-STEP通过以下方式实现：1) 证明所提出的位置学习方案可以从时空谱角度保留图属性；2) 验证MLP可以在该编码上充分利用表达能力并达到Transformer的性能；3) 改变不同的初始位置编码输入以显示鲁棒性；4) 分析理论复杂度并获得比现有技术更少的经验运行时间。", "result": "L-STEP在13个经典数据集上，使用10种算法，在转导和归纳设置下，通过3种不同的采样策略，在时间链接预测方面表现出色。同时，L-STEP在最新的大规模TGB基准测试中也取得了领先性能。理论分析表明其复杂度较低，且经验运行时间优于SOTA。", "conclusion": "L-STEP通过引入可学习的时空位置编码，有效解决了现有图深度学习框架在处理复杂、时变和大规模图数据时位置编码的局限性，并在时间链接预测任务上实现了卓越的性能和效率，证明了其在图属性保留、表达能力和实际应用中的优越性。", "translation": "准确的预测依赖于图深度学习框架（如图神经网络和图Transformer）的表达能力，其中位置编码机制在最近的最新工作中变得不可或缺，用于记录规范的位置信息。然而，当前的位置编码在三个方面存在局限性：(1) 大多数位置编码方法使用预定义和固定函数，不足以适应复杂的属性图；(2) 少数开创性工作提出了可学习的位置编码，但仍限于结构信息，未考虑真实世界中随时间演变的拓扑和特征信息；(3) 大多数位置编码方法配备了Transformer的注意力机制以充分利用其能力，但在大规模结构化数据上，密集或关系注意力通常是难以承受的。因此，我们旨在以有效和高效的方式开发可学习的时空位置编码，并提出了一个简单的时间链接预测模型，名为L-STEP。简而言之，对于L-STEP，我们(1) 证明了所提出的位置学习方案可以从时空谱角度保留图属性；(2) 验证了MLP可以在该编码上充分利用表达能力并达到Transformer的性能；(3) 改变不同的初始位置编码输入以显示鲁棒性；(4) 分析了理论复杂度并获得了比SOTA更少的经验运行时间；(5) 在13个经典数据集上，使用10种算法，在转导和归纳设置下，通过3种不同的采样策略，证明了其在时间链接预测方面的出色表现。此外，L-STEP在最新的大规模TGB基准测试中也取得了领先性能。我们的代码可在https://github.com/kthrn22/L-STEP获取。", "summary": "本文提出了一种名为L-STEP的可学习时空位置编码方法，旨在解决现有图深度学习中位置编码在处理复杂、时变和大规模图数据时的局限性。L-STEP能够从时空谱角度保留图属性，并使MLP达到Transformer的性能。实验证明，L-STEP在时间链接预测任务上，于多个经典数据集和大型基准测试中均展现出卓越的性能和更高的效率。", "keywords": "可学习位置编码, 链接预测, 时空图, 图神经网络, 效率", "comments": "该论文的创新点在于提出了可学习的时空位置编码，有效地解决了现有位置编码在处理复杂图和时变信息时的不足，并避免了Transformer在处理大规模数据时的计算瓶颈。通过结合MLP的表达能力和新颖的编码方式，L-STEP在效率和性能上均超越了现有技术，对于未来图深度学习在动态图上的应用具有重要意义。"}}
{"id": "2506.08613", "title": "SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything", "authors": ["Joost van Dalen", "Yuki M. Asano", "Marc Russwurm"], "summary": "This work proposes SAMSelect, an algorithm to obtain a salient three-channel\nvisualization for multispectral images. We develop SAMSelect and show its use\nfor marine scientists visually interpreting floating marine debris in\nSentinel-2 imagery. These debris are notoriously difficult to visualize due to\ntheir compositional heterogeneity in medium-resolution imagery. Out of these\ndifficulties, a visual interpretation of imagery showing marine debris remains\na common practice by domain experts, who select bands and spectral indices on a\ncase-by-case basis informed by common practices and heuristics. SAMSelect\nselects the band or index combination that achieves the best classification\naccuracy on a small annotated dataset through the Segment Anything Model. Its\ncentral assumption is that the three-channel visualization achieves the most\naccurate segmentation results also provide good visual information for\nphoto-interpretation.\n  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine\ndebris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets\nfrom the Plastic Litter Project. This reveals the potential of new previously\nunused band combinations (e.g., a normalized difference index of B8, B2), which\ndemonstrate improved performance compared to literature-based indices. We\ndescribe the algorithm in this paper and provide an open-source code repository\nthat will be helpful for domain scientists doing visual photo interpretation,\nespecially in the marine field.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08613v1", "AI": {"title_translation": "SAMSelect：一种使用Segment Anything进行海洋垃圾可视化的光谱指数搜索", "tldr": "SAMSelect算法通过Segment Anything模型自动选择多光谱图像的最佳三通道可视化组合，以提高海洋垃圾的目视判读准确性。", "motivation": "由于海洋垃圾的组成异质性，在中分辨率图像中对其进行可视化识别非常困难。领域专家通常需要根据经验手动选择波段和光谱指数进行目视判读，这既耗时又依赖专业知识。", "method": "本文提出了SAMSelect算法，它通过Segment Anything模型在一个小型标注数据集上选择能实现最佳分类准确率的波段或指数组合，从而为多光谱图像提供显著的三通道可视化。其核心假设是，能够获得最准确分割结果的三通道可视化也能为照片判读提供良好的视觉信息。", "result": "SAMSelect在包含加纳阿克拉和南非德班的通用海洋垃圾以及Plastic Litter Project部署的塑料目标的三幅Sentinel-2场景中进行了评估。结果表明，该方法能够发现新的、以前未使用的波段组合（例如，B8和B2的归一化差异指数），与基于文献的指数相比，这些组合表现出更好的性能。", "conclusion": "SAMSelect算法为海洋科学家目视判读Sentinel-2图像中的浮动海洋垃圾提供了一种有效且自动化的方法，特别是通过发现性能优于传统方法的新的波段组合。该算法及其开源代码库将有助于领域专家进行目视照片判读，尤其是在海洋领域。", "translation": "这项工作提出了SAMSelect，一种用于获取多光谱图像显著三通道可视化的算法。我们开发了SAMSelect并展示了它在海洋科学家目视判读Sentinel-2图像中浮动海洋垃圾方面的应用。由于这些垃圾在中分辨率图像中的组成异质性，它们的可视化识别非常困难。尽管存在这些困难，显示海洋垃圾的图像的目视判读仍然是领域专家的一种常见做法，他们根据常见实践和启发式方法逐案选择波段和光谱指数。SAMSelect通过Segment Anything模型在一个小型标注数据集上选择能实现最佳分类准确率的波段或指数组合。其核心假设是，能够获得最准确分割结果的三通道可视化也能为照片判读提供良好的视觉信息。我们在包含加纳阿克拉和南非德班的通用海洋垃圾以及Plastic Litter Project部署的塑料目标的三幅Sentinel-2场景中评估了SAMSelect。这揭示了新的、以前未使用的波段组合（例如，B8和B2的归一化差异指数）的潜力，与基于文献的指数相比，这些组合表现出改进的性能。我们在本文中描述了该算法，并提供了一个开源代码库，这将有助于进行目视照片判读的领域科学家，特别是在海洋领域。", "summary": "本文提出SAMSelect算法，旨在为多光谱图像生成显著的三通道可视化，以辅助海洋科学家在Sentinel-2图像中目视判读浮动海洋垃圾。针对海洋垃圾难以可视化的问题，SAMSelect利用Segment Anything模型，通过在小型标注数据集上选择实现最佳分类准确率的波段或光谱指数组合，从而自动优化可视化效果。实验在加纳阿克拉和南非德班的Sentinel-2场景以及部署的塑料目标上进行，结果显示SAMSelect能够发现优于传统方法的新的波段组合，例如B8和B2的归一化差异指数。该研究提供了开源代码，对海洋领域的目视判读工作具有重要意义。", "keywords": "SAMSelect, 海洋垃圾, Sentinel-2, Segment Anything, 光谱指数", "comments": "SAMSelect的创新之处在于其自动化的波段/指数选择机制，摆脱了领域专家手动选择的繁琐和主观性，并通过利用Segment Anything模型实现了高效的图像分割和可视化优化。其重要性在于显著提升了海洋垃圾目视判读的准确性和效率，特别是在处理组成异质性高的目标时。该方法还发现了新的、性能更优的波段组合，为遥感应用开辟了新视角。开源代码的提供将极大促进该领域的研究和实际应用。"}}
{"id": "2506.08619", "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering", "authors": ["Gonçalo Dias Pais", "Valter Piedade", "Moitreya Chatterjee", "Marcus Greiff", "Pedro Miraldo"], "summary": "Several variants of Neural Radiance Fields (NeRFs) have significantly\nimproved the accuracy of synthesized images and surface reconstruction of 3D\nscenes/objects. In all of these methods, a key characteristic is that none can\ntrain the neural network with every possible input data, specifically, every\npixel and potential 3D point along the projection rays due to scalability\nissues. While vanilla NeRFs uniformly sample both the image pixels and 3D\npoints along the projection rays, some variants focus only on guiding the\nsampling of the 3D points along the projection rays. In this paper, we leverage\nthe implicit surface representation of the foreground scene and model a\nprobability density function in a 3D image projection space to achieve a more\ntargeted sampling of the rays toward regions of interest, resulting in improved\nrendering. Additionally, a new surface reconstruction loss is proposed for\nimproved performance. This new loss fully explores the proposed 3D image\nprojection space model and incorporates near-to-surface and empty space\ncomponents. By integrating our novel sampling strategy and novel loss into\ncurrent state-of-the-art neural implicit surface renderers, we achieve more\naccurate and detailed 3D reconstructions and improved image rendering,\nespecially for the regions of interest in any given scene.", "comment": "Accepted in ECCV 2024", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08619v1", "AI": {"title_translation": "一种概率引导的神经隐式表面渲染采样器", "tldr": "本文提出了一种概率引导采样器和新的损失函数，用于神经隐式表面渲染，旨在通过更具针对性的光线采样来提高3D重建和图像渲染的准确性和细节，尤其是在感兴趣区域。", "motivation": "现有的神经辐射场（NeRFs）及其变体在训练时面临可扩展性问题，无法对所有可能的输入数据（如投影光线上的每个像素和潜在的3D点）进行训练。虽然一些方法引导3D点采样，但仍需要一种更具针对性的光线采样方法来聚焦感兴趣区域，从而改善渲染效果。", "method": "本文提出了一种新颖的方法：1. 利用前景场景的隐式表面表示。2. 在3D图像投影空间中建模概率密度函数，以实现对感兴趣区域更具针对性的光线采样。3. 提出了一种新的表面重建损失函数，该损失充分利用了所提出的3D图像投影空间模型，并结合了近表面和空白空间组件。这些创新策略被整合到当前最先进的神经隐式表面渲染器中。", "result": "该方法实现了改进的渲染效果、更准确和详细的3D重建，以及特别是在任何给定场景中对感兴趣区域的图像渲染质量的提升。", "conclusion": "通过将新颖的概率引导采样策略和新的表面重建损失函数整合到现有神经隐式表面渲染器中，本文实现了更准确、更详细的3D重建和改进的图像渲染，特别是对于感兴趣区域。", "translation": "神经辐射场（NeRFs）的几种变体显著提高了3D场景/物体合成图像的准确性和表面重建能力。在所有这些方法中，一个关键特点是由于可扩展性问题，没有一种方法能够使用所有可能的输入数据（特别是投影光线上的每个像素和潜在的3D点）来训练神经网络。虽然传统的NeRFs均匀采样图像像素和投影光线上的3D点，但一些变体只专注于引导投影光线上的3D点采样。在本文中，我们利用前景场景的隐式表面表示，并在3D图像投影空间中建模一个概率密度函数，以实现对感兴趣区域更具针对性的光线采样，从而改善渲染效果。此外，为了提高性能，我们提出了一种新的表面重建损失。这种新损失充分利用了所提出的3D图像投影空间模型，并结合了近表面和空白空间组件。通过将我们新颖的采样策略和新颖的损失整合到当前最先进的神经隐式表面渲染器中，我们实现了更准确、更详细的3D重建和改进的图像渲染，特别是对于任何给定场景中的感兴趣区域。", "summary": "本文提出了一种用于神经隐式表面渲染的概率引导采样器和新的表面重建损失函数。通过在3D图像投影空间中建模概率密度函数，该方法实现了更具针对性的光线采样，从而改进了渲染效果和更准确的3D重建，尤其是在感兴趣区域，解决了现有NeRF变体的可扩展性和采样效率问题。", "keywords": "神经隐式表面, 概率引导采样, NeRF, 3D重建, 图像渲染", "comments": "该论文提出了一种创新方法，通过概率引导采样策略和新的表面重建损失函数，提高了神经隐式表面渲染的效率和准确性。其核心创新在于能够针对感兴趣区域进行采样，并充分利用3D图像投影空间，这解决了现有基于NeRF的方法中的关键可扩展性限制，有望实现更详细和准确的重建，尤其适用于复杂场景。"}}
{"id": "2506.08316", "title": "Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion", "authors": ["Alan N. Amin", "Nate Gruver", "Andrew Gordon Wilson"], "summary": "Discrete diffusion models, like continuous diffusion models, generate\nhigh-quality samples by gradually undoing noise applied to datapoints with a\nMarkov process. Gradual generation in theory comes with many conceptual\nbenefits; for example, inductive biases can be incorporated into the noising\nMarkov process, and access to improved sampling algorithms. In practice,\nhowever, the consistently best performing discrete diffusion model is,\nsurprisingly, masking diffusion, which does not denoise gradually. Here we\nexplain the superior performance of masking diffusion by noting that it makes\nuse of a fundamental difference between continuous and discrete Markov\nprocesses: discrete Markov processes evolve by discontinuous jumps at a fixed\nrate and, unlike other discrete diffusion models, masking diffusion builds in\nthe known distribution of jump times and only learns where to jump to. We show\nthat we can similarly bake in the known distribution of jump times into any\ndiscrete diffusion model. The resulting models - schedule-conditioned discrete\ndiffusion (SCUD) - generalize classical discrete diffusion and masking\ndiffusion. By applying SCUD to models with noising processes that incorporate\ninductive biases on images, text, and protein data, we build models that\noutperform masking.", "comment": "Code available at: https://github.com/AlanNawzadAmin/SCUD", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08316v1", "AI": {"title_translation": "为什么掩蔽扩散有效：基于跳跃调度改进离散扩散", "tldr": "掩蔽扩散模型之所以表现优异，是因为它利用了离散马尔可夫过程的跳跃调度。本文提出了一种新的模型——调度条件离散扩散（SCUD），通过同样利用跳跃时间分布，可以超越掩蔽扩散。", "motivation": "离散扩散模型在理论上应通过逐步去噪获得优势，但实践中表现最佳的却是非逐步去噪的掩蔽扩散。本文的动机是解释掩蔽扩散的优越性能，并在此基础上开发出性能更优的模型。", "method": "作者通过指出掩蔽扩散利用了离散马尔可夫过程在固定速率下不连续跳跃的特性，并内置了已知的跳跃时间分布，来解释其优越性。他们进一步提出了一种新的方法，即调度条件离散扩散（SCUD），它能够将已知的跳跃时间分布同样融入到任何离散扩散模型中。", "result": "SCUD模型概括了经典的离散扩散和掩蔽扩散。通过将SCUD应用于在图像、文本和蛋白质数据上具有归纳偏置的去噪过程模型，他们构建的模型超越了掩蔽扩散。", "conclusion": "掩蔽扩散的卓越性能源于其对跳跃调度的利用。通过在任何离散扩散模型中融入已知的跳跃时间分布，可以构建出更优越的调度条件离散扩散（SCUD）模型，并在多种数据类型上超越现有最佳性能。", "translation": "离散扩散模型，如连续扩散模型，通过马尔可夫过程逐步消除应用于数据点的噪声来生成高质量样本。理论上，逐步生成具有许多概念上的优势；例如，归纳偏置可以被纳入去噪马尔可夫过程，并可以访问改进的采样算法。然而，在实践中，性能始终最佳的离散扩散模型，令人惊讶地是掩蔽扩散，它不逐步去噪。在此，我们通过指出掩蔽扩散利用了连续和离散马尔可夫过程之间的一个根本区别来解释其卓越性能：离散马尔可夫过程以固定速率通过不连续跳跃演变，并且，与其他离散扩散模型不同，掩蔽扩散构建了已知的跳跃时间分布，并且只学习跳到哪里。我们表明，我们可以类似地将已知的跳跃时间分布融入到任何离散扩散模型中。由此产生的模型——调度条件离散扩散（SCUD）——概括了经典的离散扩散和掩蔽扩散。通过将SCUD应用于在图像、文本和蛋白质数据上整合了归纳偏置的去噪过程模型，我们构建了超越掩蔽的模型。", "summary": "离散扩散模型通常通过逐步去噪生成高质量样本，但令人惊讶的是，表现最佳的却是非逐步去噪的掩蔽扩散。本文解释了掩蔽扩散的优越性，指出它利用了离散马尔可夫过程不连续跳跃的特性，并融入了已知的跳跃时间分布。在此基础上，作者提出了调度条件离散扩散（SCUD）模型，该模型能够将跳跃时间分布融入到任何离散扩散模型中，从而概括了经典离散扩散和掩蔽扩散。实验表明，SCUD模型在图像、文本和蛋白质数据上均超越了掩蔽扩散。", "keywords": "掩蔽扩散, 离散扩散, 跳跃调度, 马尔可夫过程, SCUD", "comments": "该论文为掩蔽扩散模型的经验成功提供了理论解释，弥合了理论与实践之间的鸿沟。提出的SCUD框架是一个重要的泛化，不仅提升了模型性能，也为离散扩散模型的设计开辟了新的方向。"}}
{"id": "2506.08629", "title": "ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network", "authors": ["Feixiang Du", "Shengkun Wu"], "summary": "In the past decade, Convolutional Neural Networks (CNNs) and Transformers\nhave achieved wide applicaiton in semantic segmentation tasks. Although CNNs\nwith Transformer models greatly improve performance, the global context\nmodeling remains inadequate. Recently, Mamba achieved great potential in vision\ntasks, showing its advantages in modeling long-range dependency. In this paper,\nwe propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,\ndubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based\nframework to address their complementary weaknesses. Specifically, We design a\nEnhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to\nimprove the representations ability of feature, We devise a Multi-Scale\nAttention Unit (MSAU) to integrate multi-scale feature aggregation, spatial\naggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion\nModule (FFM) merges diverse level feature, significantly enhancing segmented\naccuracy. Extensive experiments on two representative datasets demonstrate that\nthe proposed model excels in accuracy and efficiency balance, achieving 70.6%\nmIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M\nparameters and 8.27G FLOPs on a single RTX 3090 GPU platform.", "comment": "16 pages, 2 figures, 4 tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08629v1", "AI": {"title_translation": "ECMNet：基于高效CNN-Mamba网络的轻量级语义分割", "tldr": "ECMNet提出了一种轻量级的CNN-Mamba混合网络，用于语义分割，通过结合CNN和Mamba的优势，在准确性和效率之间取得了良好的平衡。", "motivation": "过去的十年里，CNN和Transformer在语义分割任务中应用广泛，但Transformer模型虽然提升了性能，其全局上下文建模仍显不足。最近，Mamba在视觉任务中展现出巨大潜力，尤其擅长建模长距离依赖。因此，本文旨在结合CNN和Mamba的优势，解决现有模型的不足。", "method": "本文提出了一种名为ECMNet的轻量级高效CNN-Mamba网络，用于语义分割。ECMNet在一个基于胶囊的框架中巧妙地结合了CNN和Mamba，以弥补它们各自的弱点。具体来说，设计了一个增强双注意力块（EDAB）用于轻量级瓶颈，一个多尺度注意力单元（MSAU）来整合多尺度特征聚合、空间聚合和通道聚合，以及一个Mamba增强特征融合模块（FFM）来合并不同级别的特征，显著提高分割精度。", "result": "ECMNet在Cityscapes数据集上达到了70.6%的mIoU，在CamVid测试数据集上达到了73.6%的mIoU，参数量为0.87M，FLOPs为8.27G，在一块RTX 3090 GPU平台上实现。模型在准确性和效率之间取得了出色的平衡。", "conclusion": "本文提出的ECMNet在语义分割任务中表现出优异的准确性和效率平衡，通过巧妙地结合CNN和Mamba的优势，有效提升了分割性能。", "translation": "在过去的十年中，卷积神经网络（CNN）和Transformer在语义分割任务中取得了广泛应用。尽管结合Transformer模型的CNN大大提高了性能，但全局上下文建模仍然不足。最近，Mamba在视觉任务中展现出巨大潜力，显示出其在建模长距离依赖方面的优势。本文提出了一种用于语义分割的轻量级高效CNN-Mamba网络，命名为ECMNet。ECMNet在一个基于胶囊的框架中巧妙地结合了CNN和Mamba，以解决它们互补的弱点。具体来说，我们设计了一个增强双注意力块（EDAB）用于轻量级瓶颈。为了提高特征的表示能力，我们设计了一个多尺度注意力单元（MSAU）来整合多尺度特征聚合、空间聚合和通道聚合。此外，一个Mamba增强特征融合模块（FFM）合并了不同级别的特征，显著提高了分割精度。在两个代表性数据集上进行的大量实验表明，所提出的模型在准确性和效率平衡方面表现出色，在Cityscapes上实现了70.6%的mIoU，在CamVid测试数据集上实现了73.6%的mIoU，参数量为0.87M，FLOPs为8.27G，在单个RTX 3090 GPU平台上运行。", "summary": "本文提出了一种名为ECMNet的轻量级CNN-Mamba混合网络，专为语义分割任务设计。该网络通过在胶囊框架中巧妙结合CNN和Mamba的优势，旨在解决现有模型全局上下文建模不足的问题。ECMNet包含增强双注意力块（EDAB）、多尺度注意力单元（MSAU）和Mamba增强特征融合模块（FFM），以提升特征表示和融合能力。实验结果显示，ECMNet在Cityscapes和CamVid数据集上取得了高mIoU，同时保持了较低的参数量和计算复杂度，证明了其在准确性和效率上的平衡。", "keywords": "语义分割, CNN-Mamba, 轻量级网络, 全局上下文建模, 混合网络", "comments": "ECMNet的创新之处在于其将CNN和Mamba这两种强大的架构巧妙地结合在一个轻量级框架中，旨在弥补它们各自的局限性并发挥协同效应。特别是引入Mamba来增强长距离依赖建模，同时利用CNN的局部特征提取能力。其在保证高精度的同时实现了低参数量和计算量，这对于实际应用，尤其是在资源受限的环境下，具有重要意义。该研究为未来轻量级高效的视觉模型设计提供了新的思路。"}}
{"id": "2506.08326", "title": "Graph Prompting for Graph Learning Models: Recent Advances and Future Directions", "authors": ["Xingbo Fu", "Zehong Wang", "Zihan Chen", "Jiazheng Li", "Yaochen Zhu", "Zhenyu Lei", "Cong Shen", "Yanfang Ye", "Chuxu Zhang", "Jundong Li"], "summary": "Graph learning models have demonstrated great prowess in learning expressive\nrepresentations from large-scale graph data in a wide variety of real-world\nscenarios. As a prevalent strategy for training powerful graph learning models,\nthe \"pre-training, adaptation\" scheme first pre-trains graph learning models on\nunlabeled graph data in a self-supervised manner and then adapts them to\nspecific downstream tasks. During the adaptation phase, graph prompting emerges\nas a promising approach that learns trainable prompts while keeping the\npre-trained graph learning models unchanged. In this paper, we present a\nsystematic review of recent advancements in graph prompting. First, we\nintroduce representative graph pre-training methods that serve as the\nfoundation step of graph prompting. Next, we review mainstream techniques in\ngraph prompting and elaborate on how they design learnable prompts for graph\nprompting. Furthermore, we summarize the real-world applications of graph\nprompting from different domains. Finally, we discuss several open challenges\nin existing studies with promising future directions in this field.", "comment": "Accepted by KDD 2025 Tutorial/Survey Track", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08326v1", "AI": {"title_translation": "图学习模型的图提示：最新进展与未来方向", "tldr": "本文系统性回顾了图学习模型中图提示技术的最新进展，包括预训练方法、提示设计、实际应用以及未来挑战。", "motivation": "图学习模型在处理大规模图数据方面表现出色，而图提示作为一种在预训练模型不变的情况下学习可训练提示的适应策略，正日益受到关注。因此，有必要对图提示的最新进展进行系统性回顾。", "method": "本文首先介绍了作为图提示基础的代表性图预训练方法，接着回顾了图提示的主流技术及其可学习提示的设计方式，然后总结了图提示在不同领域的实际应用，最后讨论了现有研究中存在的开放挑战和未来方向。", "result": "本文系统性地回顾了图提示的最新进展，包括其基础的图预训练方法、主流技术、实际应用，并指出了开放挑战和未来方向。", "conclusion": "现有图提示研究仍面临一些开放挑战，但该领域未来发展前景广阔。", "translation": "图学习模型在各种现实场景中，在从大规模图数据中学习富有表现力的表示方面展现出强大的能力。作为训练强大图学习模型的普遍策略，“预训练、适应”方案首先以自监督方式在未标记的图数据上预训练图学习模型，然后将其适应于特定的下游任务。在适应阶段，图提示作为一种有前景的方法出现，它学习可训练的提示，同时保持预训练的图学习模型不变。在本文中，我们系统地回顾了图提示的最新进展。首先，我们介绍了作为图提示基础的代表性图预训练方法。接下来，我们回顾了图提示的主流技术，并详细阐述了它们如何为图提示设计可学习的提示。此外，我们总结了图提示在不同领域的实际应用。最后，我们讨论了现有研究中的几个开放挑战以及该领域有前景的未来方向。", "summary": "这篇综述系统性地回顾了图学习模型中图提示技术的最新进展。文章首先介绍了作为基础的图预训练方法，随后深入探讨了图提示的主流技术及其可学习提示的设计，并总结了其在不同领域的实际应用。最后，文章还讨论了该领域面临的开放挑战和未来的研究方向。", "keywords": "图提示, 图学习模型, 预训练, 自监督学习, 综述", "comments": "这篇综述为图学习领域中的图提示技术提供了全面的概览，特别是在“预训练-适应”范式下，图提示作为一种不改变预训练模型而进行适应的有效手段。其创新之处在于系统性地梳理了现有技术，并明确指出了未来的研究方向，对该领域的研究人员具有重要的参考价值。"}}
{"id": "2506.08632", "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08632v1", "AI": {"title_translation": "RoboSwap: 一种用于无监督机器人手臂替换的GAN驱动视频扩散框架", "tldr": "RoboSwap是一个新的框架，结合GAN和扩散模型，用于在不同视频环境中无监督地替换机器人手臂，解决了机器人学习中数据稀缺和跨平台泛化的问题。", "motivation": "现有生成模型在视频合成和编辑方面取得了进展，但高质量、多样化的数据集稀缺，阻碍了视频条件下的机器人学习和跨平台泛化。本文旨在解决在不同视频中替换机器人手臂的挑战，这是实现跨实体学习的关键一步。", "method": "RoboSwap框架针对非配对数据进行操作，首先将机器人手臂从背景中分割出来。然后，训练一个非配对GAN模型将一个机器人手臂转换为另一个。转换后的手臂与原始视频背景融合，并通过扩散模型进行细化，以增强连贯性、运动真实感和物体交互。GAN和扩散阶段是独立训练的。", "result": "实验表明，RoboSwap在三个基准测试中，在结构连贯性和运动一致性方面均优于最先进的视频和图像编辑模型。", "conclusion": "RoboSwap为生成可靠的跨实体机器人学习数据提供了一个鲁棒的解决方案。", "translation": "生成模型最近的进展彻底改变了视频合成和编辑。然而，多样化、高质量数据集的稀缺性持续阻碍视频条件下的机器人学习，限制了跨平台泛化能力。在这项工作中，我们解决了将一个视频中的机器人手臂替换为另一个的挑战：这是跨实体学习的关键一步。与之前依赖于相同环境设置下配对视频演示的方法不同，我们提出的框架RoboSwap在来自不同环境的非配对数据上运行，从而减轻了数据收集需求。RoboSwap引入了一种新颖的视频编辑流程，整合了GAN和扩散模型，结合了它们各自的优势。具体来说，我们将机器人手臂从背景中分割出来，并训练一个非配对GAN模型将一个机器人手臂转换为另一个。转换后的手臂与原始视频背景融合，并通过扩散模型进行细化，以增强连贯性、运动真实感和物体交互。GAN和扩散阶段是独立训练的。我们的实验表明，RoboSwap在三个基准测试中，在结构连贯性和运动一致性方面均优于最先进的视频和图像编辑模型，从而为机器人学习中生成可靠的跨实体数据提供了鲁棒的解决方案。", "summary": "RoboSwap是一个创新的视频扩散框架，旨在解决机器人学习中视频数据稀缺和跨平台泛化的问题。它通过结合GAN和扩散模型，实现了在不同环境背景下无监督地替换视频中的机器人手臂。该方法首先分割手臂，然后使用非配对GAN进行转换，最后通过扩散模型进行融合和细化，以确保视频的连贯性和运动真实感。实验证明，RoboSwap在结构连贯性和运动一致性方面优于现有技术，为生成可靠的跨实体机器人学习数据提供了有效方案。", "keywords": "机器人手臂替换, GAN, 扩散模型, 无监督学习, 视频编辑", "comments": "RoboSwap的创新之处在于其将GAN和扩散模型结合起来，用于解决机器人学习中视频数据稀缺和跨实体泛化的核心问题。通过允许在非配对数据上进行训练，极大地降低了数据收集的门槛。这种无监督的机器人手臂替换能力对于生成多样化的训练数据至关重要，有望推动机器人学习在更广泛应用中的发展。"}}
{"id": "2506.08337", "title": "A Simple Analysis of Discretization Error in Diffusion Models", "authors": ["Juhyeok Choi", "Chenglin Fan"], "summary": "Diffusion models, formulated as discretizations of stochastic differential\nequations (SDEs), achieve state-of-the-art generative performance. However,\nexisting analyses of their discretization error often rely on complex\nprobabilistic tools. In this work, we present a simplified theoretical\nframework for analyzing the Euler--Maruyama discretization of\nvariance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models\n(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion\nprocess. Our approach leverages Gr\\\"onwall's inequality to derive a convergence\nrate of $ \\mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly\nstreamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise\nin the discretization can be replaced by a discrete random variable (e.g.,\nRademacher or uniform noise) without sacrificing convergence guarantees-an\ninsight with practical implications for efficient sampling. Experiments\nvalidate our theory, showing that (1) the error scales as predicted, (2)\ndiscrete noise achieves comparable sample quality to Gaussian noise, and (3)\nincorrect noise scaling degrades performance. By unifying simplified analysis\nand discrete noise substitution, our work bridges theoretical rigor with\npractical efficiency in diffusion-based generative modeling.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08337v1", "AI": {"title_translation": "扩散模型中离散化误差的简单分析", "tldr": "本文提出了一个简化理论框架来分析扩散模型中的离散化误差，并证明了离散噪声可以替代高斯噪声，且误差收敛率为$ \\mathcal{O}(1/T^{1/2}) $。", "motivation": "现有扩散模型离散化误差的分析方法过于复杂，依赖于复杂的概率工具。", "method": "本文提出了一个简化的理论框架，用于分析去噪扩散概率模型（DDPMs）中方差保持随机微分方程（VP-SDEs）的Euler-Maruyama离散化。该方法利用Gr\"onwall不等式推导出收敛速率，并证明了高斯噪声可以用离散随机变量（如Rademacher或均匀噪声）替代而不影响收敛保证。", "result": "在Lipschitz假设下，推导出了$ \\mathcal{O}(1/T^{1/2}) $的收敛速率。实验验证了理论，显示误差按预测比例缩放，离散噪声实现了与高斯噪声相当的样本质量，并且不正确的噪声缩放会降低性能。", "conclusion": "通过统一简化的分析和离散噪声替换，本文工作弥合了扩散生成建模中的理论严谨性与实际效率之间的差距。", "translation": "扩散模型，被表述为随机微分方程（SDEs）的离散化，实现了最先进的生成性能。然而，现有对其离散化误差的分析通常依赖于复杂的概率工具。在这项工作中，我们提出了一个简化的理论框架，用于分析去噪扩散概率模型（DDPMs）中方差保持SDEs（VP-SDEs）的Euler-Maruyama离散化，其中$ T $表示扩散过程中的去噪步骤数。我们的方法利用Gr\"onwall不等式在Lipschitz假设下推导出$ \\mathcal{O}(1/T^{1/2}) $的收敛速率，显著简化了先前的证明。此外，我们证明了离散化中的高斯噪声可以被离散随机变量（例如，Rademacher或均匀噪声）替代，而不会牺牲收敛保证——这一见解对高效采样具有实际意义。实验验证了我们的理论，表明（1）误差按预测比例缩放，（2）离散噪声实现了与高斯噪声相当的样本质量，以及（3）不正确的噪声缩放会降低性能。通过统一简化的分析和离散噪声替换，我们的工作弥合了扩散生成建模中的理论严谨性与实际效率之间的差距。", "summary": "本文提出了一个简化的理论框架，用于分析扩散模型中Euler-Maruyama离散化的误差。研究利用Gr\"onwall不等式推导了$ \\mathcal{O}(1/T^{1/2}) $的收敛速率，并证明了高斯噪声可以被离散随机变量替代而不损失收敛性。实验结果验证了理论预测，并展示了离散噪声在样本质量上与高斯噪声相当，为扩散模型的高效采样提供了实践指导。", "keywords": "扩散模型, 离散化误差, Euler-Maruyama, Grönwall不等式, 离散噪声", "comments": "这项工作通过简化理论分析并引入离散噪声替换，显著提升了扩散模型的理论理解和实际应用效率。其创新之处在于利用Gr\"onwall不等式简化了复杂证明，并提供了使用离散噪声替代高斯噪声的理论依据，这对于高效采样具有重要意义。该研究成功地将理论严谨性与实践效率相结合，为生成模型领域带来了进步。"}}
{"id": "2506.08635", "title": "SurfR: Surface Reconstruction with Multi-scale Attention", "authors": ["Siddhant Ranade", "Gonçalo Dias Pais", "Ross Tyler Whitaker", "Jacinto C. Nascimento", "Pedro Miraldo", "Srikumar Ramalingam"], "summary": "We propose a fast and accurate surface reconstruction algorithm for\nunorganized point clouds using an implicit representation. Recent learning\nmethods are either single-object representations with small neural models that\nallow for high surface details but require per-object training or generalized\nrepresentations that require larger models and generalize to newer shapes but\nlack details, and inference is slow. We propose a new implicit representation\nfor general 3D shapes that is faster than all the baselines at their optimum\nresolution, with only a marginal loss in performance compared to the\nstate-of-the-art. We achieve the best accuracy-speed trade-off using three key\ncontributions. Many implicit methods extract features from the point cloud to\nclassify whether a query point is inside or outside the object. First, to speed\nup the reconstruction, we show that this feature extraction does not need to\nuse the query point at an early stage (lazy query). Second, we use a parallel\nmulti-scale grid representation to develop robust features for different noise\nlevels and input resolutions. Finally, we show that attention across scales can\nprovide improved reconstruction results.", "comment": "Accepted in 3DV 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08635v1", "AI": {"title_translation": "SurfR: 基于多尺度注意力的表面重建", "tldr": "SurfR提出了一种快速准确的无序点云隐式表面重建算法，通过惰性查询、多尺度网格和跨尺度注意力实现了最佳的精度-速度权衡。", "motivation": "现有的表面重建学习方法要么需要逐对象训练以获得高细节，要么虽然通用但缺乏细节且推理缓慢。本研究旨在开发一种既快速又准确，且能泛化到新形状的隐式表面重建方法。", "method": "SurfR算法采用了一种新的通用3D形状隐式表示。其核心贡献包括：1) 惰性查询：特征提取无需在早期阶段使用查询点，以加速重建。2) 并行多尺度网格表示：用于开发适用于不同噪声水平和输入分辨率的鲁棒特征。3) 跨尺度注意力：用于提供改进的重建结果。", "result": "SurfR在最佳分辨率下比所有基线方法都快，与最先进的方法相比性能损失微乎其微，并实现了最佳的精度-速度权衡。", "conclusion": "该论文提出的SurfR算法通过惰性查询、多尺度网格和跨尺度注意力，为无序点云的隐式表面重建提供了卓越的精度-速度权衡。", "translation": "我们提出了一种使用隐式表示的无序点云快速准确表面重建算法。最近的学习方法要么是单对象表示，使用小型神经模型允许高表面细节但需要逐对象训练，要么是通用表示，需要更大的模型并能泛化到新形状但缺乏细节且推理缓慢。我们提出了一种新的通用3D形状隐式表示，它在最佳分辨率下比所有基线方法都快，与最先进的方法相比，性能损失仅是微乎其微。我们通过三项关键贡献实现了最佳的精度-速度权衡。许多隐式方法从点云中提取特征，以分类查询点是在对象内部还是外部。首先，为了加速重建，我们表明这种特征提取不需要在早期阶段使用查询点（惰性查询）。其次，我们使用并行多尺度网格表示来开发适用于不同噪声水平和输入分辨率的鲁棒特征。最后，我们展示了跨尺度注意力可以提供改进的重建结果。", "summary": "本文介绍了SurfR，一种利用隐式表示从无序点云进行快速准确表面重建的新颖算法。为解决现有方法在细节保留与计算效率之间的权衡问题，SurfR提出了一种新的隐式表示，实现了卓越的精度-速度平衡。其关键创新包括：用于加速特征提取的“惰性查询”，用于生成鲁棒特征的并行多尺度网格表示，以及用于提高重建质量的跨尺度注意力。实验结果表明，该方法比基线方法更快，且与最先进方法相比性能损失甚微。", "keywords": "表面重建, 隐式表示, 点云, 多尺度注意力, 惰性查询", "comments": "该论文通过解决计算效率和细节保留之间的权衡，提出了一种创新的表面重建方法。其中“惰性查询”的概念对于加速推理尤其巧妙。多尺度网格和跨尺度注意力的结合是处理不同输入质量和提高重建精度的有效方式。其专注于实现更好的精度-速度权衡，对于实际应用具有高度相关性。"}}
{"id": "2506.08340", "title": "Dynamical System Optimization", "authors": ["Emo Todorov"], "summary": "We develop an optimization framework centered around a core idea: once a\n(parametric) policy is specified, control authority is transferred to the\npolicy, resulting in an autonomous dynamical system. Thus we should be able to\noptimize policy parameters without further reference to controls or actions,\nand without directly using the machinery of approximate Dynamic Programming and\nReinforcement Learning. Here we derive simpler algorithms at the autonomous\nsystem level, and show that they compute the same quantities as policy\ngradients and Hessians, natural gradients, proximal methods. Analogs to\napproximate policy iteration and off-policy learning are also available. Since\npolicy parameters and other system parameters are treated uniformly, the same\nalgorithms apply to behavioral cloning, mechanism design, system\nidentification, learning of state estimators. Tuning of generative AI models is\nnot only possible, but is conceptually closer to the present framework than to\nReinforcement Learning.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08340v1", "AI": {"title_translation": "动力系统优化", "tldr": "本文提出一种新的优化框架，通过将控制权转移给参数化策略，形成自主动力系统，从而在无需近似动态规划或强化学习的情况下，直接优化策略参数，并证明其算法与现有方法等效，且适用范围广泛。", "motivation": "传统的优化方法，特别是近似动态规划和强化学习，在优化策略参数时可能过于复杂。本文的动机是开发一种更简单、更直接的算法，在自主动力系统层面优化策略参数，避免直接使用复杂的控制或动作机制。", "method": "该框架的核心思想是：一旦指定了参数化策略，控制权就转移给该策略，形成一个自主动力系统。这样，策略参数的优化可以在不参考控制或动作的情况下进行，也无需直接使用近似动态规划和强化学习的复杂机制。算法在自主系统层面推导，并证明它们计算的结果与策略梯度、Hessian、自然梯度和近端方法相同。", "result": "结果表明，推导出的算法能够计算与策略梯度、Hessian、自然梯度和近端方法相同的量。此外，该框架还提供了近似策略迭代和离策略学习的类似物。由于策略参数和其他系统参数被统一处理，相同的算法也适用于行为克隆、机制设计、系统识别和状态估计器学习。生成式AI模型的调整不仅可能，而且在概念上比强化学习更接近本框架。", "conclusion": "本文提出了一个基于自主动力系统的优化框架，能够通过更简单的算法有效优化策略参数，避免了强化学习和动态规划的复杂性。该框架的算法与现有先进方法等效，并具有广泛的适用性，尤其在生成式AI模型调整方面显示出概念上的优势。", "translation": "我们开发了一个以核心思想为中心的优化框架：一旦指定了（参数化的）策略，控制权就转移给该策略，从而形成一个自主动力系统。因此，我们应该能够在不进一步参考控制或动作，以及不直接使用近似动态规划和强化学习机制的情况下优化策略参数。在这里，我们在自主系统层面推导了更简单的算法，并表明它们计算出的量与策略梯度、Hessian、自然梯度、近端方法相同。近似策略迭代和离策略学习的类似物也可用。由于策略参数和其他系统参数被统一处理，相同的算法也适用于行为克隆、机制设计、系统识别、状态估计器学习。生成式AI模型的调整不仅可能，而且在概念上比强化学习更接近本框架。", "summary": "本文提出了一个名为“动力系统优化”的新框架，其核心在于将控制权转移给一个预设的参数化策略，从而创建一个自主动力系统。这种方法允许直接优化策略参数，避免了传统近似动态规划和强化学习的复杂性。研究表明，该框架下的算法能够计算与策略梯度、Hessian和自然梯度等现有方法相同的优化量，并且提供了近似策略迭代和离策略学习的对应机制。该框架具有广泛的适用性，包括行为克隆、机制设计、系统识别和状态估计器学习，并被认为在概念上比强化学习更适合生成式AI模型的调整。", "keywords": "动力系统优化, 策略优化, 自主系统, 强化学习替代, 生成式AI", "comments": "本文的创新之处在于提出了一种新颖的优化范式，通过构建自主动力系统来简化策略参数的优化过程，避免了强化学习和动态规划的复杂性。其重要性在于提供了一种更通用、更简洁的框架，能够统一处理多种优化问题，并特别指出其在生成式AI模型调整方面的潜在优势，这可能为AI模型的训练和优化开辟新的途径。"}}
{"id": "2506.08640", "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned", "authors": ["Yichong Lu", "Yuzhuo Tian", "Zijin Jiang", "Yikun Zhao", "Yuanbo Yang", "Hao Ouyang", "Haoji Hu", "Huimin Yu", "Yujun Shen", "Yiyi Liao"], "summary": "Humans intuitively perceive object shape and orientation from a single image,\nguided by strong priors about canonical poses. However, existing 3D generative\nmodels often produce misaligned results due to inconsistent training data,\nlimiting their usability in downstream tasks. To address this gap, we introduce\nthe task of orientation-aligned 3D object generation: producing 3D objects from\nsingle images with consistent orientations across categories. To facilitate\nthis, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D\nmodels spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two\nrepresentative 3D generative models based on multi-view diffusion and 3D\nvariational autoencoder frameworks to produce aligned objects that generalize\nwell to unseen objects across various categories. Experimental results\ndemonstrate the superiority of our method over post-hoc alignment approaches.\nFurthermore, we showcase downstream applications enabled by our aligned object\ngeneration, including zero-shot object orientation estimation via\nanalysis-by-synthesis and efficient arrow-based object rotation manipulation.", "comment": "Project Page: https://xdimlab.github.io/Orientation_Matters", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08640v1", "AI": {"title_translation": "方向很重要：让3D生成模型方向对齐", "tldr": "现有3D生成模型常因数据不一致导致方向未对齐，本文提出方向对齐的3D物体生成任务，构建了Objaverse-OA数据集，并基于此微调了多视图扩散和3D变分自编码模型，实现了优于后处理方法的对齐生成，并支持零样本方向估计和高效旋转操作等下游应用。", "motivation": "现有的3D生成模型由于训练数据不一致，经常产生方向未对齐的结果，这限制了它们在下游任务中的可用性。", "method": "引入了方向对齐的3D物体生成任务；构建了一个包含14,832个方向对齐的3D模型的Objaverse-OA数据集；利用Objaverse-OA数据集微调了基于多视图扩散和3D变分自编码框架的两种代表性3D生成模型。", "result": "实验结果表明，该方法优于后处理对齐方法。此外，展示了对齐物体生成所支持的下游应用，包括通过合成分析进行的零样本物体方向估计以及基于箭头的有效物体旋转操作。", "conclusion": "本文提出的方法有效解决了3D生成模型中物体方向未对齐的问题，通过构建对齐数据集和微调现有模型，实现了优越的对齐效果，并为零样本方向估计和高效物体旋转等下游应用提供了支持。", "translation": "人类凭直觉从单一图像中感知物体形状和方向，这得益于关于标准姿态的强大先验知识。然而，现有的3D生成模型由于训练数据不一致，经常产生未对齐的结果，限制了它们在下游任务中的可用性。为了弥补这一空白，我们引入了方向对齐的3D物体生成任务：从单一图像生成在不同类别中具有一致方向的3D物体。为了促进这一点，我们构建了Objaverse-OA，一个包含1008个类别、14,832个方向对齐的3D模型的数据集。利用Objaverse-OA，我们微调了基于多视图扩散和3D变分自编码框架的两种代表性3D生成模型，以生成对齐的物体，这些物体能够很好地泛化到各种类别中未见过的物体。实验结果证明了我们的方法优于后处理对齐方法。此外，我们展示了由我们对齐物体生成所支持的下游应用，包括通过合成分析进行的零样本物体方向估计以及高效的基于箭头的物体旋转操作。", "summary": "本文针对现有3D生成模型生成物体方向未对齐的问题，提出了方向对齐的3D物体生成任务。为此，作者构建了大规模方向对齐的Objaverse-OA数据集，并基于此微调了多视图扩散和3D变分自编码模型。实验证明，该方法在生成对齐物体方面优于传统后处理方法，并支持零样本物体方向估计和高效物体旋转等多种下游应用，显著提升了3D生成模型的实用性。", "keywords": "3D生成模型, 方向对齐, Objaverse-OA, 多视图扩散, 3D变分自编码", "comments": "本文创新性地提出了“方向对齐的3D物体生成”这一重要任务，并构建了目前最大的方向对齐3D数据集Objaverse-OA，为解决现有3D生成模型输出物体方向不一致的痛点提供了关键资源和方法。通过在现有模型上进行微调，不仅提高了生成结果的可用性，还拓展了新的下游应用，具有较高的实用价值和研究意义。"}}
{"id": "2506.08649", "title": "Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Youwei Lu"], "summary": "Video memorability refers to the ability of videos to be recalled after\nviewing, playing a crucial role in creating content that remains memorable.\nExisting models typically focus on extracting multimodal features to predict\nvideo memorability scores but often fail to fully utilize motion cues. The\nrepresentation of motion features is compromised during the fine-tuning phase\nof the motion feature extractor due to a lack of labeled data. In this paper,\nwe introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal\nvideo memorability prediction model designed to enhance the representation of\nmotion features. We tackle the challenge of improving motion feature\nrepresentation by leveraging text description similarities across videos to\nestablish positive and negative motion sample sets for a given target. This\nenhancement allows the model to learn similar feature representations for\nsemantically related motion content, resulting in more accurate memorability\npredictions. Our model achieves state-of-the-art performance on two video\nmemorability prediction datasets. Moreover, the potential applications of video\nmemorability prediction have been underexplored. To address this gap, we\npresent Memorability Weighted Correction for Video Summarization (MWCVS), using\nvideo memorability prediction to reduce subjectivity in video summarization\nlabels. Experimental results on two video summarization datasets demonstrate\nthe effectiveness of MWCVS, showcasing the promising applications of video\nmemorability prediction.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08649v1", "AI": {"title_translation": "使用文本-运动跨模态对比损失增强视频记忆性预测及其在视频摘要中的应用", "tldr": "提出TMCCL模型改善视频记忆性预测，并通过MWCVS应用于视频摘要，实现了SOTA性能。", "motivation": "现有视频记忆性预测模型未能充分利用运动线索，且运动特征在微调阶段因缺乏标注数据而受损；视频记忆性预测的潜在应用尚未被充分探索。", "method": "引入Text-Motion Cross-modal Contrastive Loss (TMCCL)，利用文本描述相似性建立正负运动样本集以增强运动特征表示。提出Memorability Weighted Correction for Video Summarization (MWCVS)，将视频记忆性预测应用于视频摘要，以减少摘要标签的主观性。", "result": "TMCCL在两个视频记忆性预测数据集上取得了最先进的性能。MWCVS在两个视频摘要数据集上证明了其有效性。", "conclusion": "本研究通过TMCCL显著提升了视频记忆性预测的准确性，并通过MWCVS展示了视频记忆性预测在视频摘要等应用中的巨大潜力。", "translation": "视频记忆性是指视频在观看后被回忆起来的能力，在创建令人难忘的内容方面发挥着关键作用。现有模型通常侧重于提取多模态特征来预测视频记忆性得分，但往往未能充分利用运动线索。在运动特征提取器的微调阶段，由于缺乏标记数据，运动特征的表示受到损害。在本文中，我们引入了文本-运动跨模态对比损失（TMCCL），这是一种多模态视频记忆性预测模型，旨在增强运动特征的表示。我们通过利用视频间的文本描述相似性来为给定目标建立正负运动样本集，从而解决了改善运动特征表示的挑战。这种增强使模型能够学习语义相关运动内容的相似特征表示，从而实现更准确的记忆性预测。我们的模型在两个视频记忆性预测数据集上取得了最先进的性能。此外，视频记忆性预测的潜在应用尚未得到充分探索。为了弥补这一差距，我们提出了用于视频摘要的记忆性加权校正（MWCVS），利用视频记忆性预测来减少视频摘要标签中的主观性。在两个视频摘要数据集上的实验结果证明了MWCVS的有效性，展示了视频记忆性预测的广阔应用前景。", "summary": "本文提出Text-Motion Cross-modal Contrastive Loss (TMCCL) 模型，通过利用文本描述相似性增强运动特征表示，显著提升了视频记忆性预测的准确性。同时，引入Memorability Weighted Correction for Video Summarization (MWCVS)，将视频记忆性预测应用于视频摘要，有效降低了标签主观性，并在多个数据集上验证了其先进性和有效性。", "keywords": "视频记忆性预测, 跨模态对比学习, 运动特征, 视频摘要, TMCCL, MWCVS", "comments": "该论文的创新点在于提出了TMCCL以解决视频记忆性预测中运动特征利用不足的问题，并开创性地将视频记忆性预测应用于视频摘要，展示了其在实际应用中的巨大潜力，为减少人工标注的主观性提供了新思路。"}}
{"id": "2506.08353", "title": "An Adaptive Method Stabilizing Activations for Enhanced Generalization", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "summary": "We introduce AdaAct, a novel optimization algorithm that adjusts learning\nrates according to activation variance. Our method enhances the stability of\nneuron outputs by incorporating neuron-wise adaptivity during the training\nprocess, which subsequently leads to better generalization -- a complementary\napproach to conventional activation regularization methods. Experimental\nresults demonstrate AdaAct's competitive performance across standard image\nclassification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing\nit with other state-of-the-art methods. Importantly, AdaAct effectively bridges\nthe gap between the convergence speed of Adam and the strong generalization\ncapabilities of SGD, all while maintaining competitive execution times. Code is\navailable at https://github.com/hseung88/adaact.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08353v1", "AI": {"title_translation": "一种稳定激活以增强泛化能力的自适应方法", "tldr": "AdaAct是一种根据激活方差调整学习率的优化算法，能稳定神经元输出，提升泛化能力，并在图像分类任务上表现出色，同时兼顾Adam的收敛速度和SGD的泛化能力。", "motivation": "旨在通过根据激活方差调整学习率来增强神经元输出的稳定性，从而提升模型的泛化能力，作为传统激活正则化方法的补充。", "method": "引入AdaAct，一种新型优化算法，它通过在训练过程中根据激活方差调整学习率，并结合神经元级别的自适应性来增强神经元输出的稳定性。", "result": "AdaAct在标准图像分类基准（CIFAR和ImageNet）上表现出有竞争力的性能，与最先进的方法相当。它有效地弥合了Adam的收敛速度和SGD的强大泛化能力之间的差距，同时保持了有竞争力的执行时间。", "conclusion": "AdaAct是一种有效的优化算法，通过稳定激活来提高模型的泛化能力，同时兼顾训练效率和模型性能。", "translation": "我们引入AdaAct，这是一种根据激活方差调整学习率的新型优化算法。我们的方法通过在训练过程中引入神经元级别的自适应性来增强神经元输出的稳定性，从而带来更好的泛化能力——这是对传统激活正则化方法的一种补充方法。实验结果表明，AdaAct在标准图像分类基准测试中表现出有竞争力的性能。我们在CIFAR和ImageNet上评估了AdaAct，并将其与其他最先进的方法进行比较。重要的是，AdaAct有效地弥合了Adam的收敛速度和SGD强大的泛化能力之间的差距，同时保持了有竞争力的执行时间。代码可在https://github.com/hseung88/adaact 获取。", "summary": "本文提出了AdaAct，一种基于激活方差调整学习率的新型优化算法，旨在通过稳定神经元输出来提升模型的泛化能力。该方法是对传统激活正则化方法的补充。实验证明，AdaAct在图像分类任务上表现出色，既能达到Adam的快速收敛，又能保持SGD的优秀泛化能力，且执行效率高。", "keywords": "AdaAct, 优化算法, 泛化能力, 激活稳定, 学习率自适应", "comments": "AdaAct的创新点在于其独特的学习率调整机制，即根据激活方差进行神经元级别的自适应调整，这提供了一种新颖的稳定激活的方法，与传统的正则化手段形成互补。其重要性在于，它成功地结合了两种主流优化器（Adam和SGD）的优点，即快速收敛和优异泛化，这对于深度学习模型的训练效率和最终性能具有重要意义。"}}
{"id": "2506.08650", "title": "Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping", "authors": ["Peter Grönquist", "Stepan Tulyakov", "Dengxin Dai"], "summary": "Achieving consistent color reproduction across multiple cameras is essential\nfor seamless image fusion and Image Processing Pipeline (ISP) compatibility in\nmodern devices, but it is a challenging task due to variations in sensors and\noptics. Existing raw-to-raw conversion methods face limitations such as poor\nadaptability to changing illumination, high computational costs, or impractical\nrequirements such as simultaneous camera operation and overlapping\nfields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,\nphysically-informed approach that simulates raw images under specified\nillumination to estimate transformations between devices. The NPM effectively\nadapts to varying illumination conditions, can be initialized with physical\nmeasurements, and supports training with or without paired data. Experiments on\npublic datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent\nstate-of-the-art methods, providing robust chromatic consistency across\ndifferent sensors and optical systems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08650v1", "AI": {"title_translation": "超越校准：用于原始图像到原始图像映射的物理信息学习", "tldr": "一种新的物理信息模型（NPM），用于原始图像到原始图像的转换，实现了跨摄像头的鲁棒色彩一致性，能适应不同的光照条件，并优于现有最先进的方法。", "motivation": "在现代设备中，实现多摄像头之间一致的色彩再现对于无缝图像融合和图像处理流水线（ISP）兼容性至关重要，但由于传感器和光学器件的差异，这是一项具有挑战性的任务。现有的原始图像到原始图像转换方法面临局限性，例如对光照变化的适应性差、计算成本高或不切实际的要求（如同步摄像头操作和重叠视场）。", "method": "本文引入了神经物理模型（NPM），这是一种轻量级、物理信息驱动的方法，通过模拟特定光照下的原始图像来估计设备之间的变换。NPM能有效地适应不同的光照条件，可以用物理测量进行初始化，并支持有或无配对数据进行训练。", "result": "在NUS和BeyondRGB等公共数据集上的实验表明，NPM优于最近最先进的方法，在不同传感器和光学系统之间提供了鲁棒的色度一致性。", "conclusion": "神经物理模型（NPM）是原始图像到原始图像映射的卓越解决方案，提供鲁棒的色度一致性、对光照变化的适应性以及灵活的训练选项，优于当前最先进的方法。", "translation": "在现代设备中，实现多摄像头之间一致的色彩再现对于无缝图像融合和图像处理流水线（ISP）兼容性至关重要，但由于传感器和光学器件的差异，这是一项具有挑战性的任务。现有的原始图像到原始图像转换方法面临局限性，例如对光照变化的适应性差、计算成本高或不切实际的要求（如同步摄像头操作和重叠视场）。本文引入了神经物理模型（NPM），这是一种轻量级、物理信息驱动的方法，通过模拟特定光照下的原始图像来估计设备之间的变换。NPM能有效地适应不同的光照条件，可以用物理测量进行初始化，并支持有或无配对数据进行训练。在NUS和BeyondRGB等公共数据集上的实验表明，NPM优于最近最先进的方法，在不同传感器和光学系统之间提供了鲁棒的色度一致性。", "summary": "本文提出了一种名为神经物理模型（NPM）的轻量级、物理信息学习方法，用于原始图像到原始图像的映射。该方法解决了现有方法在多摄像头之间实现一致色彩再现时面临的挑战，例如对不同光照条件适应性差和计算成本高。NPM通过模拟特定光照下的原始图像来估计设备变换，能够很好地适应光照变化，可以用物理测量进行初始化，并支持有或无配对数据训练。在公共数据集上的实验结果表明，NPM优于现有最先进的方法，在不同传感器和光学系统之间实现了鲁棒的色度一致性。", "keywords": "原始图像映射, 色彩再现, 神经物理模型, 色度一致性, 物理信息学习", "comments": "该论文的创新之处在于将物理信息整合到神经模型（NPM）中，用于原始图像到原始图像的映射，显著提高了对不同光照的适应性，并提供了灵活的训练选项（配对/非配对数据）。这种方法解决了现有方法的关键局限性，使其在多摄像头系统中具有高度实用性。其轻量级特性和卓越性能标志着在实现鲁棒色彩一致性方面迈出了重要一步。"}}
{"id": "2506.08360", "title": "NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "summary": "Adaptive gradient methods are computationally efficient and converge quickly,\nbut they often suffer from poor generalization. In contrast, second-order\nmethods enhance convergence and generalization but typically incur high\ncomputational and memory costs. In this work, we introduce NysAct, a scalable\nfirst-order gradient preconditioning method that strikes a balance between\nstate-of-the-art first-order and second-order optimization methods. NysAct\nleverages an eigenvalue-shifted Nystrom method to approximate the activation\ncovariance matrix, which is used as a preconditioning matrix, significantly\nreducing time and memory complexities with minimal impact on test accuracy. Our\nexperiments show that NysAct not only achieves improved test accuracy compared\nto both first-order and second-order methods but also demands considerably less\ncomputational resources than existing second-order methods. Code is available\nat https://github.com/hseung88/nysact.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08360v1", "AI": {"title_translation": "NysAct: 一种使用Nystrom近似的可扩展预处理梯度下降方法", "tldr": "NysAct是一种可扩展的预处理梯度下降方法，它通过Nystrom近似平衡了一阶和二阶优化方法的优势，在提高测试准确性的同时显著降低了计算资源消耗。", "motivation": "自适应梯度方法计算高效且收敛快，但泛化能力差；二阶方法能增强收敛性和泛化性，但计算和内存成本高。本研究旨在寻找一种能兼顾两者优点的优化方法。", "method": "本文提出了NysAct，一种可扩展的一阶梯度预处理方法。NysAct利用特征值移位的Nystrom方法来近似激活协方差矩阵，并将其用作预处理矩阵，从而在对测试准确性影响最小的情况下，显著降低了时间和内存复杂度。", "result": "实验表明，NysAct不仅比现有的一阶和二阶方法获得了更高的测试准确性，而且比现有的二阶方法需要更少的计算资源。", "conclusion": "NysAct成功地在优化性能和计算效率之间取得了平衡，提供了一种在提高测试准确性的同时显著降低计算和内存成本的有效预处理梯度下降方法。", "translation": "自适应梯度方法计算效率高且收敛速度快，但通常泛化能力较差。相比之下，二阶方法能增强收敛性和泛化性，但通常会产生高昂的计算和内存成本。在这项工作中，我们引入了NysAct，一种可扩展的一阶梯度预处理方法，它在最先进的一阶和二阶优化方法之间取得了平衡。NysAct利用特征值移位的Nystrom方法来近似激活协方差矩阵，该矩阵用作预处理矩阵，从而在对测试准确性影响最小的情况下显著降低了时间和内存复杂度。我们的实验表明，NysAct不仅比一阶和二阶方法都取得了更高的测试准确性，而且比现有二阶方法所需的计算资源少得多。代码可在https://github.com/hseung88/nysact获取。", "summary": "NysAct是一种新颖的可扩展一阶梯度预处理方法，旨在解决现有优化方法在效率和泛化能力之间的权衡问题。它通过使用特征值移位的Nystrom方法近似激活协方差矩阵作为预处理矩阵，显著降低了计算和内存成本，同时保持或提高了测试准确性。实验证明NysAct在准确性和资源消耗方面均优于或平衡了一阶和二阶方法。", "keywords": "NysAct, 预处理梯度下降, Nystrom近似, 优化, 可扩展", "comments": "NysAct的创新之处在于其巧妙地结合了Nystrom近似来预处理梯度下降，从而有效地桥接了一阶方法的高效性和二阶方法的良好泛化性。这种方法在保证性能的同时，显著降低了计算成本，使其在实际应用中具有很高的价值和可扩展性。"}}
{"id": "2506.08666", "title": "LLaVA-c: Continual Improved Visual Instruction Tuning", "authors": ["Wenzhuo Liu", "Fei Zhu", "Haiyang Guo", "Longhui Wei", "Cheng-Lin Liu"], "summary": "Multimodal models like LLaVA-1.5 achieve state-of-the-art visual\nunderstanding through visual instruction tuning on multitask datasets, enabling\nstrong instruction-following and multimodal performance. However, multitask\nlearning faces challenges such as task balancing, requiring careful adjustment\nof data proportions, and expansion costs, where new tasks risk catastrophic\nforgetting and need costly retraining. Continual learning provides a promising\nalternative to acquiring new knowledge incrementally while preserving existing\ncapabilities. However, current methods prioritize task-specific performance,\nneglecting base model degradation from overfitting to specific instructions,\nwhich undermines general capabilities. In this work, we propose a simple but\neffective method with two modifications on LLaVA-1.5: spectral-aware\nconsolidation for improved task balance and unsupervised inquiry regularization\nto prevent base model degradation. We evaluate both general and task-specific\nperformance across continual pretraining and fine-tuning. Experiments\ndemonstrate that LLaVA-c consistently enhances standard benchmark performance\nand preserves general capabilities. For the first time, we show that\ntask-by-task continual learning can achieve results that match or surpass\nmultitask joint learning. The code will be publicly released.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08666v1", "AI": {"title_translation": "LLaVA-c: 持续改进的视觉指令微调", "tldr": "LLaVA-c提出了一种持续学习方法，通过改进LLaVA-1.5来解决多任务学习中的灾难性遗忘和现有持续学习方法的模型退化问题，首次实现了逐任务持续学习媲美甚至超越多任务联合学习的效果。", "motivation": "多模态模型在多任务数据集上进行视觉指令微调取得了SOTA性能，但多任务学习面临任务平衡和扩展成本（灾难性遗忘）的挑战。现有的持续学习方法则优先考虑任务特定性能，忽视了基础模型因过拟合特定指令而导致的泛化能力下降。", "method": "提出了LLaVA-c，在LLaVA-1.5上进行了两项修改：1. 谱感知整合（spectral-aware consolidation）以改善任务平衡；2. 无监督查询正则化（unsupervised inquiry regularization）以防止基础模型退化。", "result": "LLaVA-c持续提升了标准基准测试性能，并保留了泛化能力。首次证明了逐任务持续学习可以达到或超越多任务联合学习的结果。", "conclusion": "LLaVA-c通过其创新的持续学习方法有效地解决了多模态模型在多任务学习和现有持续学习中的挑战，实现了性能提升和泛化能力保持，为视觉指令微调提供了一个有前景的替代方案。", "translation": "多模态模型如LLaVA-1.5通过在多任务数据集上进行视觉指令微调，实现了最先进的视觉理解能力，从而具备强大的指令遵循和多模态性能。然而，多任务学习面临任务平衡（需要仔细调整数据比例）和扩展成本（新任务面临灾难性遗忘且需要昂贵的再训练）等挑战。持续学习提供了一种有前景的替代方案，可以增量地获取新知识，同时保留现有能力。然而，当前的方法优先考虑任务特定性能，忽视了基础模型因过拟合特定指令而导致的退化，这损害了泛化能力。在这项工作中，我们提出了一种简单但有效的方法，对LLaVA-1.5进行了两项修改：用于改善任务平衡的谱感知整合，以及用于防止基础模型退化的无监督查询正则化。我们评估了持续预训练和微调的通用和任务特定性能。实验表明，LLaVA-c持续增强了标准基准测试性能并保留了泛化能力。我们首次展示了逐任务持续学习可以达到或超越多任务联合学习的结果。代码将公开发布。", "summary": "本文提出了LLaVA-c，一个基于LLaVA-1.5的持续改进视觉指令微调模型。针对多任务学习中的任务平衡和灾难性遗忘问题，以及现有持续学习方法中基础模型泛化能力退化的问题，LLaVA-c引入了谱感知整合和无监督查询正则化。实验结果表明，LLaVA-c不仅提升了基准性能，还有效保持了模型的泛化能力，并首次证明了逐任务持续学习可以媲美甚至超越传统的多任务联合学习。", "keywords": "持续学习, 视觉指令微调, 多模态模型, 灾难性遗忘, LLaVA", "comments": "本文的创新点在于提出了两种新颖的修改（谱感知整合和无监督查询正则化），有效解决了多模态模型在持续学习中的核心挑战，即任务平衡和基础模型退化。其重要性在于首次证明了逐任务持续学习可以达到甚至超越多任务联合学习的性能，这为未来多模态模型的增量学习和扩展提供了新的范式，降低了模型更新和维护的成本。"}}
{"id": "2506.08049", "title": "Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting", "authors": ["Tengfei Lyu", "Weijia Zhang", "Hao Liu"], "summary": "Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions\nfrom several weeks to months in advance, presents significant challenges due to\nthe chaotic dynamics of atmospheric systems and complex interactions across\nmultiple scales. Current approaches often fail to explicitly model underlying\nphysical processes and teleconnections that are crucial at S2S timescales. We\nintroduce TelePiT, a novel deep learning architecture that enhances global S2S\nforecasting through integrated multi-scale physics and teleconnection\nawareness. Our approach consists of three key components: (1) Spherical\nHarmonic Embedding, which accurately encodes global atmospheric variables onto\nspherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which\nexplicitly captures atmospheric physical processes across multiple learnable\nfrequency bands; (3) Teleconnection-Aware Transformer, which models critical\nglobal climate interactions through tactfully injecting teleconnection patterns\ninto the self-attention. Extensive experiments demonstrate that TelePiT\nsignificantly outperforms state-of-the-art data-driven baselines and\noperational numerical weather prediction systems, with remarkable improvements\nfor atmospheric variables including a 57.7% reduction in RMSE for 2-meter\ntemperature compared to previous best models.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.08049v1", "AI": {"title_translation": "用于全球次季节到季节预报的物理信息遥相关感知Transformer", "tldr": "TelePiT是一个新型深度学习模型，通过整合多尺度物理信息和遥相关感知，显著提高了全球次季节到季节预报的准确性，超越了现有最先进的模型。", "motivation": "次季节到季节 (S2S) 预报由于大气系统的混沌动力学和多尺度复杂相互作用而面临巨大挑战。现有方法通常未能明确建模S2S时间尺度上至关重要的潜在物理过程和遥相关。", "method": "本文引入了TelePiT，一个增强全球S2S预报的新型深度学习架构。它包含三个关键组件：1) 球谐嵌入，用于准确编码全球大气变量到球形几何；2) 多尺度物理信息神经ODE，用于明确捕捉多个可学习频段的大气物理过程；3) 遥相关感知Transformer，通过巧妙地将遥相关模式注入自注意力机制来建模关键的全球气候相互作用。", "result": "广泛的实验表明，TelePiT显著优于最先进的数据驱动基线和业务数值天气预报系统。与之前最佳模型相比，2米温度的RMSE降低了57.7%，显示出对大气变量的显著改进。", "conclusion": "TelePiT通过整合物理信息和遥相关感知，为全球次季节到季节预报提供了一个有效且优越的深度学习解决方案，显著提高了预报准确性。", "translation": "次季节到季节（S2S）预报，即提前数周到数月预测气候条件，由于大气系统的混沌动力学和多尺度复杂相互作用而面临重大挑战。当前方法通常未能明确建模S2S时间尺度上至关重要的潜在物理过程和遥相关。我们引入了TelePiT，一种新颖的深度学习架构，通过整合多尺度物理信息和遥相关感知来增强全球S2S预报。我们的方法由三个关键组件组成：(1) 球谐嵌入，它将全球大气变量准确编码到球形几何上；(2) 多尺度物理信息神经ODE，它明确捕捉多个可学习频率带上的大气物理过程；(3) 遥相关感知Transformer，它通过巧妙地将遥相关模式注入自注意力机制来建模关键的全球气候相互作用。广泛的实验表明，TelePiT显著优于最先进的数据驱动基线和业务数值天气预报系统，对大气变量有显著改进，其中2米温度的RMSE比以前最佳模型降低了57.7%。", "summary": "本文提出了TelePiT，一种新颖的深度学习模型，旨在改进全球次季节到季节（S2S）预报。该模型通过整合球谐嵌入、多尺度物理信息神经ODE和遥相关感知Transformer，有效处理大气系统的复杂动力学和遥相关。实验证明，TelePiT在S2S预报任务上显著优于现有最先进的数据驱动和数值天气预报系统，尤其是在2米温度预报方面取得了显著的RMSE降低。", "keywords": "次季节到季节预报, 深度学习, 物理信息, 遥相关, Transformer", "comments": "TelePiT的创新之处在于其将物理信息和遥相关模式巧妙地融入到Transformer架构中，这对于理解和预测复杂的大气系统至关重要。其结合球谐嵌入、神经ODE和注意力机制的方法提供了一个全面的框架，能够捕捉多尺度物理过程和全球相互作用。这项工作对S2S预报领域具有重要意义，可能为更准确、更可靠的长期天气和气候预测铺平道路。"}}
{"id": "2506.08365", "title": "AlphaFold Database Debiasing for Robust Inverse Folding", "authors": ["Cheng Tan", "Zhenxiao Cao", "Zhangyang Gao", "Siyuan Li", "Yufei Huang", "Stan Z. Li"], "summary": "The AlphaFold Protein Structure Database (AFDB) offers unparalleled\nstructural coverage at near-experimental accuracy, positioning it as a valuable\nresource for data-driven protein design. However, its direct use in training\ndeep models that are sensitive to fine-grained atomic geometry, such as inverse\nfolding, exposes a critical limitation. Comparative analysis of structural\nfeature distributions reveals that AFDB structures exhibit distinct statistical\nregularities, reflecting a systematic geometric bias that deviates from the\nconformational diversity found in experimentally determined structures from the\nProtein Data Bank (PDB). While AFDB structures are cleaner and more idealized,\nPDB structures capture the intrinsic variability and physical realism essential\nfor generalization in downstream tasks. To address this discrepancy, we\nintroduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct\nnative-like conformations from intentionally corrupted backbone geometries. By\ntraining the model to recover plausible structural states, DeSAE implicitly\ncaptures a more robust and natural structural manifold. At inference, applying\nDeSAE to AFDB structures produces debiased structures that significantly\nimprove inverse folding performance across multiple benchmarks. This work\nhighlights the critical impact of subtle systematic biases in predicted\nstructures and presents a principled framework for debiasing, significantly\nboosting the performance of structure-based learning tasks like inverse\nfolding.", "comment": "Under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08365v1", "AI": {"title_translation": "AlphaFold 数据库去偏以实现鲁棒的逆向折叠", "tldr": "AlphaFold数据库（AFDB）在蛋白质结构预测方面表现出色，但其系统性几何偏差限制了其在敏感深度学习任务（如逆向折叠）中的直接应用。本研究引入了一种去偏结构自编码器（DeSAE），通过学习从损坏的骨架几何形状中重建类天然构象来解决这一问题，从而显著提高了逆向折叠的性能。", "motivation": "AlphaFold数据库（AFDB）虽然提供了广泛的蛋白质结构覆盖和接近实验的准确性，但其结构存在系统性的几何偏差，与实验确定的PDB结构中的构象多样性存在差异。这种偏差使得AFDB结构在训练对精细原子几何敏感的深度模型（如逆向折叠）时，其直接使用暴露了关键限制，导致模型泛化能力不足。", "method": "为了解决AFDB的几何偏差问题，本研究引入了一种去偏结构自编码器（DeSAE）。DeSAE通过学习从有意损坏的骨架几何形状中重建类天然构象来训练。通过这种方式，模型隐式地捕获了一个更鲁棒和自然的结构流形。", "result": "在推理阶段，将DeSAE应用于AFDB结构可以生成去偏的结构。这些去偏结构在多个基准测试中显著提高了逆向折叠的性能。", "conclusion": "本研究强调了预测结构中细微系统偏差的关键影响，并提出了一个有原则的去偏框架。该框架显著提升了基于结构的深度学习任务（如逆向折叠）的性能。", "translation": "AlphaFold 蛋白质结构数据库 (AFDB) 提供了无与伦比的结构覆盖范围和接近实验的准确性，使其成为数据驱动蛋白质设计的宝贵资源。然而，将其直接用于训练对精细原子几何敏感的深度模型（例如逆向折叠）时，暴露出一个关键限制。结构特征分布的比较分析表明，AFDB 结构表现出独特的统计规律性，反映出一种系统性的几何偏差，偏离了蛋白质数据库 (PDB) 中实验确定的结构中发现的构象多样性。虽然 AFDB 结构更干净、更理想化，但 PDB 结构捕捉了下游任务中泛化所必需的内在变异性和物理真实性。为了解决这种差异，我们引入了一种去偏结构自编码器 (DeSAE)，它学习从有意损坏的骨架几何形状中重建类天然构象。通过训练模型恢复合理的结构状态，DeSAE 隐式地捕获了一个更鲁棒和自然的结构流形。在推理时，将 DeSAE 应用于 AFDB 结构可以产生去偏的结构，这些结构显著改善了多个基准测试中的逆向折叠性能。这项工作突出了预测结构中细微系统偏差的关键影响，并提出了一个有原则的去偏框架，显著提升了基于结构的深度学习任务（如逆向折叠）的性能。", "summary": "本研究关注AlphaFold蛋白质结构数据库（AFDB）在用于训练深度学习模型（特别是逆向折叠）时存在的系统性几何偏差问题。通过比较AFDB和实验性PDB结构的特征分布，发现AFDB结构虽然更理想化，但缺乏PDB结构中固有的变异性和物理真实性。为解决此问题，论文提出了一种去偏结构自编码器（DeSAE），它通过从受损骨架几何中学习重建类天然构象来捕获更鲁棒的结构流形。将DeSAE应用于AFDB结构后，显著改善了逆向折叠任务的性能，凸显了预测结构中偏差的关键影响以及去偏方法的有效性。", "keywords": "AlphaFold数据库, 逆向折叠, 蛋白质结构, 去偏, 自编码器", "comments": "这项工作具有重要意义，因为它解决了利用大规模计算预测结构数据库（如AlphaFold DB）进行下游任务时面临的实际挑战。通过引入DeSAE，研究人员提供了一个通用的去偏框架，这对于依赖于精细结构信息的深度学习模型至关重要。其创新点在于通过自编码器学习重建“原生”构象来隐式纠正偏差，这比显式地建模偏差更为优雅和有效。该方法有望提高蛋白质设计和工程领域中基于结构的学习任务的可靠性和泛化能力。"}}
{"id": "2506.08678", "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction", "authors": ["Juan Yeo", "Soonwoo Cha", "Jiwoo Song", "Hyunbin Jin", "Taesup Kim"], "summary": "Vision-language models such as CLIP have recently propelled open-vocabulary\ndense prediction tasks by enabling recognition of a broad range of visual\nconcepts. However, CLIP still struggles with fine-grained, region-level\nunderstanding, hindering its effectiveness on these dense prediction tasks. We\nidentify two pivotal factors required to address this limitation: semantic\ncoherence and fine-grained vision-language alignment. Current adaptation\nmethods often improve fine-grained alignment at the expense of semantic\ncoherence, and often rely on extra modules or supervised fine-tuning. To\novercome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel\napproach that simultaneously enhances semantic coherence and fine-grained\nalignment by leveraging own knowledge of a model across all representation\nlevels. Unlike prior methods, ATAS uses only unlabeled images and an internal\nself-distillation process to refine representations of CLIP vision encoders,\npreserving local semantic consistency while sharpening local detail\nrecognition. On open-vocabulary object detection and semantic segmentation\nbenchmarks, ATAS achieves substantial performance gains, outperforming baseline\nCLIP models. These results validate the effectiveness of our approach and\nunderscore the importance of jointly maintaining semantic coherence and\nfine-grained alignment for advanced open-vocabulary dense prediction.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08678v1", "AI": {"title_translation": "ATAS：任意到任意自蒸馏，用于增强开放词汇密集预测", "tldr": "ATAS通过自蒸馏解决了CLIP在开放词汇密集预测中细粒度理解不足的问题，在无标签数据下同时提升了语义一致性和细粒度对齐，并取得了显著性能提升。", "motivation": "尽管视觉-语言模型如CLIP推动了开放词汇密集预测任务的发展，但CLIP在细粒度、区域级理解方面仍有不足，这限制了其在密集预测任务上的有效性。现有适应方法通常以牺牲语义一致性为代价来改善细粒度对齐，且常依赖额外模块或有监督微调。", "method": "本文提出了Any-to-Any Self-Distillation (ATAS) 方法。该方法通过利用模型自身在所有表示层面的知识，同时增强语义一致性和细粒度对齐。ATAS仅使用未标记图像和内部自蒸馏过程来优化CLIP视觉编码器的表示，旨在保持局部语义一致性的同时提高局部细节识别能力。", "result": "在开放词汇目标检测和语义分割基准测试中，ATAS取得了显著的性能提升，表现优于基线CLIP模型。", "conclusion": "本研究结果验证了ATAS方法的有效性，并强调了在高级开放词汇密集预测中同时保持语义一致性和细粒度对齐的重要性。", "translation": "视觉-语言模型如CLIP最近通过实现对广泛视觉概念的识别，推动了开放词汇密集预测任务的发展。然而，CLIP在细粒度、区域级理解方面仍然存在困难，这阻碍了其在这些密集预测任务上的有效性。我们确定了解决这一限制所需的两个关键因素：语义一致性和细粒度视觉-语言对齐。当前的适应方法通常以牺牲语义一致性为代价来改善细粒度对齐，并且通常依赖于额外的模块或有监督微调。为了克服这些问题，我们提出了任意到任意自蒸馏（ATAS），这是一种新颖的方法，通过利用模型自身在所有表示层面的知识，同时增强语义一致性和细粒度对齐。与现有方法不同，ATAS仅使用未标记图像和内部自蒸馏过程来优化CLIP视觉编码器的表示，从而在保持局部语义一致性的同时提高局部细节识别能力。在开放词汇目标检测和语义分割基准测试中，ATAS取得了显著的性能提升，优于基线CLIP模型。这些结果验证了我们方法的有效性，并强调了在高级开放词汇密集预测中同时保持语义一致性和细粒度对齐的重要性。", "summary": "ATAS是一种新颖的自蒸馏方法，旨在解决CLIP模型在开放词汇密集预测任务中细粒度理解不足的问题。该方法通过利用模型自身在所有表示层面的知识，在无标签图像上进行内部自蒸馏，同时增强语义一致性和细粒度视觉-语言对齐，从而优化CLIP视觉编码器的表示。实验结果表明，ATAS在开放词汇目标检测和语义分割基准测试中显著优于基线CLIP模型，验证了其有效性。", "keywords": "开放词汇密集预测, 自蒸馏, CLIP, 语义一致性, 细粒度对齐", "comments": "ATAS的创新之处在于其“任意到任意”的自蒸馏机制，以及仅依赖未标记数据进行训练，这使其在实际应用中更具灵活性和普适性。该方法同时关注语义一致性和细粒度对齐，解决了现有方法中一个关键的权衡问题。"}}
{"id": "2506.08379", "title": "Reinforce LLM Reasoning through Multi-Agent Reflection", "authors": ["Yurun Yuan", "Tengyang Xie"], "summary": "Leveraging more test-time computation has proven to be an effective way to\nboost the reasoning capabilities of large language models (LLMs). Among various\nmethods, the verify-and-improve paradigm stands out for enabling dynamic\nsolution exploration and feedback incorporation. However, existing approaches\noften suffer from restricted feedback spaces and lack of coordinated training\nof different parties, leading to suboptimal performance. To address this, we\nmodel this multi-turn refinement process as a Markov Decision Process and\nintroduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement\nlearning algorithm that trains an actor-critic LLM system to iteratively refine\nanswers via direct preference learning on self-generated data. Theoretically,\nDPSDP can match the performance of any policy within the training distribution.\nEmpirically, we instantiate DPSDP with various base models and show\nimprovements on both in- and out-of-distribution benchmarks. For example, on\nbenchmark MATH 500, majority voting over five refinement steps increases\nfirst-turn accuracy from 58.2% to 63.2% with Ministral-based models. An\nablation study further confirms the benefits of multi-agent collaboration and\nout-of-distribution generalization.", "comment": "International Conference on Machine Learning (ICML), 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08379v1", "AI": {"title_translation": "通过多智能体反思强化LLM推理", "tldr": "本文提出了一种名为DPSDP的强化学习算法，通过训练一个actor-critic LLM系统，利用多智能体反思迭代优化答案，从而提升LLM的推理能力。", "motivation": "现有的LLM推理验证和改进方法存在反馈空间受限以及不同参与方缺乏协同训练的问题，导致性能不佳。", "method": "论文将多轮细化过程建模为马尔可夫决策过程，并引入了DPSDP（Direct Policy Search by Dynamic Programming）算法。该算法是一个强化学习方法，通过对自生成数据进行直接偏好学习，训练一个actor-critic LLM系统来迭代细化答案。", "result": "DPSDP在理论上可以匹配训练分布内任何策略的性能。在经验上，它在内部分布和外部分布基准测试中均显示出改进。例如，在MATH 500基准测试中，通过五步细化后的多数投票，基于Ministral模型的首次准确率从58.2%提高到63.2%。消融研究也证实了多智能体协作和域外泛化的益处。", "conclusion": "通过建模多轮细化过程并引入DPSDP强化学习算法，可以有效提升LLM的推理能力，解决了现有方法反馈空间受限和协同训练不足的问题，并展示了多智能体协作的优势。", "translation": "利用更多的测试时计算已被证明是提升大型语言模型（LLM）推理能力的有效方法。在各种方法中，验证-改进范式因其能够实现动态解决方案探索和反馈整合而脱颖而出。然而，现有方法通常存在反馈空间受限和不同参与方缺乏协同训练的问题，导致性能不佳。为了解决这个问题，我们将这种多轮细化过程建模为马尔可夫决策过程，并引入了DPSDP（通过动态规划的直接策略搜索），这是一种强化学习算法，它训练一个actor-critic LLM系统，通过对自生成数据进行直接偏好学习来迭代细化答案。理论上，DPSDP可以匹配训练分布内任何策略的性能。在经验上，我们用各种基础模型实例化了DPSDP，并展示了在内部分布和外部分布基准测试上的改进。例如，在MATH 500基准测试中，对五个细化步骤进行多数投票，基于Ministral模型的首次准确率从58.2%提高到63.2%。一项消融研究进一步证实了多智能体协作和域外泛化的益处。", "summary": "本文提出了一种名为DPSDP的强化学习算法，旨在通过模拟多智能体反思过程来增强大型语言模型（LLMs）的推理能力。该方法将多轮细化建模为马尔可夫决策过程，并训练一个actor-critic LLM系统，通过对自生成数据进行直接偏好学习来迭代优化答案。实验结果表明，DPSDP在多种基准测试上均实现了显著的性能提升，尤其是在数学推理任务上，并验证了多智能体协作和泛化能力的有效性。", "keywords": "LLM推理, 强化学习, 多智能体, 反思, DPSDP", "comments": "这篇论文通过引入强化学习和多智能体协作来解决LLM推理中反馈空间受限和协同训练不足的问题，具有创新性。将细化过程建模为MDP并采用直接偏好学习是其核心亮点，理论和经验结果都支持了其有效性，尤其是在提升数学推理能力方面。多智能体反思的范式对于未来的LLM能力提升具有重要意义。"}}
{"id": "2506.08690", "title": "CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities", "authors": ["Hugo Porta", "Emanuele Dalsasso", "Jessica L. McCarty", "Devis Tuia"], "summary": "Canada experienced in 2023 one of the most severe wildfire seasons in recent\nhistory, causing damage across ecosystems, destroying communities, and emitting\nlarge quantities of CO2. This extreme wildfire season is symptomatic of a\nclimate-change-induced increase in the length and severity of the fire season\nthat affects the boreal ecosystem. Therefore, it is critical to empower\nwildfire management in boreal communities with better mitigation solutions.\nWildfire probability maps represent an important tool for understanding the\nlikelihood of wildfire occurrence and the potential severity of future\nwildfires. The massive increase in the availability of Earth observation data\nhas enabled the development of deep learning-based wildfire forecasting models,\naiming at providing precise wildfire probability maps at different spatial and\ntemporal scales. A main limitation of such methods is their reliance on\ncoarse-resolution environmental drivers and satellite products, leading to\nwildfire occurrence prediction of reduced resolution, typically around $\\sim\n0.1${\\deg}. This paper presents a benchmark dataset: CanadaFireSat, and\nbaseline methods for high-resolution: 100 m wildfire forecasting across Canada,\nleveraging multi-modal data from high-resolution multi-spectral satellite\nimages (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and\nenvironmental factors (ERA5 reanalysis data). Our experiments consider two\nmajor deep learning architectures. We observe that using multi-modal temporal\ninputs outperforms single-modal temporal inputs across all metrics, achieving a\npeak performance of 60.3% in F1 score for the 2023 wildfire season, a season\nnever seen during model training. This demonstrates the potential of\nmulti-modal deep learning models for wildfire forecasting at high-resolution\nand continental scale.", "comment": "34 pages, 11 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08690v1", "AI": {"title_translation": "CanadaFireSat：迈向高分辨率多模态森林火灾预测", "tldr": "本文介绍了CanadaFireSat数据集和基线方法，利用多模态数据实现加拿大高分辨率（100米）森林火灾预测，实验表明多模态时间输入优于单模态输入。", "motivation": "2023年加拿大经历了有史以来最严重的森林火灾季节，凸显了气候变化对火灾季节长度和严重性的影响。为更好地管理北方生态系统的森林火灾，急需高分辨率的火灾概率图，但现有方法受限于粗分辨率数据，导致预测精度不足。", "method": "本文提出了一个基准数据集CanadaFireSat和基线方法，用于加拿大100米高分辨率森林火灾预测。该方法利用多模态数据，包括高分辨率多光谱卫星图像（Sentinel-2 L1C）、中分辨率卫星产品（MODIS）和环境因素（ERA5再分析数据）。实验采用了两种主要的深度学习架构。", "result": "实验结果显示，使用多模态时间输入在所有指标上均优于单模态时间输入。在模型训练中未曾出现过的2023年火灾季节，F1分数峰值达到60.3%。", "conclusion": "多模态深度学习模型在实现高分辨率和大陆尺度的森林火灾预测方面具有巨大潜力。", "translation": "2023年，加拿大经历了近代历史上最严重的森林火灾季节之一，对生态系统造成破坏，摧毁了社区，并排放了大量的二氧化碳。这个极端火灾季节是气候变化导致火灾季节长度和严重性增加的症状，影响着北方生态系统。因此，通过更好的缓解方案来增强北方社区的森林火灾管理至关重要。森林火灾概率图是理解火灾发生可能性和未来火灾潜在严重性的重要工具。地球观测数据可用性的大幅增加使得基于深度学习的森林火灾预测模型得以开发，旨在提供不同空间和时间尺度上的精确森林火灾概率图。此类方法的一个主要局限性在于它们依赖于粗分辨率的环境驱动因素和卫星产品，导致火灾发生预测的分辨率降低，通常约为0.1度。本文提出了一个基准数据集：CanadaFireSat，以及用于加拿大高分辨率：100米森林火灾预测的基线方法，利用高分辨率多光谱卫星图像（Sentinel-2 L1C）、中分辨率卫星产品（MODIS）和环境因素（ERA5再分析数据）的多模态数据。我们的实验考虑了两种主要的深度学习架构。我们观察到，在所有指标上，使用多模态时间输入优于单模态时间输入，在2023年火灾季节（模型训练期间从未见过的季节）实现了60.3%的F1分数峰值。这表明了多模态深度学习模型在高分辨率和大陆尺度森林火灾预测方面的潜力。", "summary": "本文针对加拿大2023年严峻的森林火灾形势，提出CanadaFireSat数据集和基于多模态深度学习的高分辨率（100米）森林火灾预测方法。该方法整合了高分辨率多光谱卫星图像、中分辨率卫星产品和环境再分析数据。实验证明，多模态时间输入在火灾预测性能上显著优于单模态输入，尤其是在未见过的火灾季节中表现出色，凸显了其在高分辨率、大陆尺度火灾预测中的应用潜力。", "keywords": "森林火灾预测, 多模态深度学习, CanadaFireSat, 高分辨率, 地球观测数据", "comments": "该论文通过引入CanadaFireSat数据集和多模态深度学习方法，解决了现有森林火灾预测模型分辨率低的问题，实现了100米的高分辨率预测，这是一个显著的创新。其利用Sentinel-2 L1C等高分辨率数据，并结合多种数据源进行融合，提升了预测的准确性，尤其是在应对未见过的新火灾季节时表现出的鲁棒性，对于气候变化背景下的森林火灾管理具有重要意义。"}}
{"id": "2506.08059", "title": "CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction", "authors": ["Huong Van Le", "Weibin Ren", "Junhong Kim", "Yukyung Yun", "Young Bin Park", "Young Jun Kim", "Bok Kyung Han", "Inho Choi", "Jong IL Park", "Hwi-Yeol Yun", "Jae-Mun Choi"], "summary": "Caco-2 permeability serves as a critical in vitro indicator for predicting\nthe oral absorption of drug candidates during early-stage drug discovery. To\nenhance the accuracy and efficiency of computational predictions, we\nsystematically investigated the impact of eight molecular feature\nrepresentation types including 2D/3D descriptors, structural fingerprints, and\ndeep learning-based embeddings combined with automated machine learning\ntechniques to predict Caco-2 permeability. Using two datasets of differing\nscale and diversity (TDC benchmark and curated OCHEM data), we assessed model\nperformance across representations and identified PaDEL, Mordred, and RDKit\ndescriptors as particularly effective for Caco-2 prediction. Notably, the\nAutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,\nfor both PaDEL and Mordred representations, the incorporation of 3D descriptors\nresulted in a 15.73% reduction in MAE compared to using 2D features alone, as\nconfirmed by feature importance analysis. These findings highlight the\neffectiveness of AutoML approaches in ADMET modeling and offer practical\nguidance for feature selection in data-limited prediction tasks.", "comment": "49 pages, 11 figures", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.08059v1", "AI": {"title_translation": "CaliciBoost：性能驱动的分子表示评估，用于Caco-2渗透性预测", "tldr": "该研究通过结合自动化机器学习技术，系统评估了八种分子表示类型（包括2D/3D描述符、结构指纹和深度学习嵌入）对Caco-2渗透性预测的影响。结果表明PaDEL、Mordred和RDKit描述符以及3D特征特别有效，其中CaliciBoost模型表现最佳。", "motivation": "为了提高药物早期发现阶段Caco-2渗透性计算预测的准确性和效率，Caco-2渗透性是预测药物口服吸收的关键体外指标。", "method": "研究系统地调查了八种分子特征表示类型（包括2D/3D描述符、结构指纹和基于深度学习的嵌入），并将其与自动化机器学习（AutoML）技术相结合，用于预测Caco-2渗透性。研究使用了TDC基准和精选的OCHEM数据两个不同规模和多样性的数据集来评估模型性能。", "result": "研究发现PaDEL、Mordred和RDKit描述符对Caco-2预测特别有效。基于AutoML的模型CaliciBoost取得了最佳的MAE性能。对于PaDEL和Mordred表示，结合3D描述符相比单独使用2D特征，MAE降低了15.73%。", "conclusion": "AutoML方法在ADMET建模中是有效的，并且研究结果为数据受限的预测任务中的特征选择提供了实用指导。", "translation": "Caco-2渗透性是药物早期发现阶段预测候选药物口服吸收的关键体外指标。为了提高计算预测的准确性和效率，我们系统地研究了八种分子特征表示类型（包括2D/3D描述符、结构指纹和基于深度学习的嵌入）结合自动化机器学习技术对Caco-2渗透性预测的影响。我们使用两个不同规模和多样性的数据集（TDC基准和精选的OCHEM数据），评估了不同表示类型的模型性能，并确定PaDEL、Mordred和RDKit描述符对Caco-2预测特别有效。值得注意的是，基于AutoML的模型CaliciBoost取得了最佳的MAE性能。此外，对于PaDEL和Mordred表示，结合3D描述符相比单独使用2D特征，MAE降低了15.73%，这通过特征重要性分析得到了证实。这些发现突出了AutoML方法在ADMET建模中的有效性，并为数据受限的预测任务中的特征选择提供了实用指导。", "summary": "本论文评估了八种分子表示方法（包括2D/3D描述符、结构指纹和深度学习嵌入）结合自动化机器学习技术在Caco-2渗透性预测中的性能。研究基于两个数据集，发现PaDEL、Mordred和RDKit描述符特别有效，其中AutoML模型CaliciBoost实现了最佳的MAE性能。值得注意的是，结合3D描述符显著提高了预测精度。这些发现强调了AutoML在ADMET建模中的潜力，并为特征选择提供了实用指导。", "keywords": "Caco-2渗透性, 分子表示, AutoML, 药物发现, ADMET建模", "comments": "该论文全面评估了用于Caco-2渗透性预测的分子表示方法，展示了AutoML在优化模型性能方面的强大能力。发现3D描述符能显著提高预测精度尤其富有洞察力，为药物发现提供了实用指导。使用多种表示类型和数据集增加了研究的稳健性。"}}
{"id": "2506.08691", "title": "VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism", "authors": ["Congzhi Zhang", "Jiawei Peng", "Zhenglin Wang", "Yilong Lai", "Haowen Sun", "Heng Chang", "Fei Ma", "Weijiang Yu"], "summary": "Large Vision-Language Models (LVLMs) have shown exceptional performance in\nmultimodal tasks, but their effectiveness in complex visual reasoning is still\nconstrained, especially when employing Chain-of-Thought prompting techniques.\nIn this paper, we propose VReST, a novel training-free approach that enhances\nReasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.\nVReST meticulously traverses the reasoning landscape by establishing a search\ntree, where each node encapsulates a reasoning step, and each path delineates a\ncomprehensive reasoning sequence. Our innovative multimodal Self-Reward\nmechanism assesses the quality of reasoning steps by integrating the utility of\nsub-questions, answer correctness, and the relevance of vision-language clues,\nall without the need for additional models. VReST surpasses current prompting\nmethods and secures state-of-the-art performance across three multimodal\nmathematical reasoning benchmarks. Furthermore, it substantiates the efficacy\nof test-time scaling laws in multimodal tasks, offering a promising direction\nfor future research.", "comment": "Accepted by ACL 2025 main", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08691v1", "AI": {"title_translation": "VReST：通过树搜索和自奖励机制增强大型视觉语言模型的推理能力", "tldr": "VReST通过蒙特卡洛树搜索和自奖励机制，在无需训练的情况下显著提升了大型视觉语言模型在复杂视觉推理任务上的性能，并在多模态数学推理基准上达到了最先进水平。", "motivation": "大型视觉语言模型（LVLMs）在多模态任务中表现出色，但在复杂的视觉推理方面，尤其是在使用思维链提示技术时，其有效性仍然受到限制。", "method": "本文提出了一种名为VReST的无需训练的方法，通过蒙特卡洛树搜索（Monte Carlo Tree Search）和自奖励机制（Self-Reward mechanisms）来增强LVLMs的推理能力。VReST通过建立搜索树来遍历推理过程，其中每个节点代表一个推理步骤，每条路径表示一个完整的推理序列。其创新的多模态自奖励机制通过整合子问题的效用、答案的正确性以及视觉语言线索的相关性来评估推理步骤的质量，而无需额外模型。", "result": "VReST超越了当前的提示方法，并在三个多模态数学推理基准上取得了最先进的性能。此外，它还证实了测试时缩放定律在多模态任务中的有效性。", "conclusion": "VReST通过结合树搜索和自奖励机制，有效提升了大型视觉语言模型在复杂推理任务上的表现，并为未来研究提供了有前景的方向，特别是在多模态任务中测试时缩放定律的应用。", "translation": "大型视觉语言模型（LVLMs）在多模态任务中表现出色，但其在复杂视觉推理方面的有效性仍然受到限制，尤其是在采用思维链提示技术时。在本文中，我们提出了VReST，一种新颖的无需训练的方法，它通过蒙特卡洛树搜索和自奖励机制增强了LVLMs的推理能力。VReST通过建立一个搜索树来精心地遍历推理过程，其中每个节点封装一个推理步骤，每条路径描绘一个全面的推理序列。我们创新的多模态自奖励机制通过整合子问题的效用、答案的正确性以及视觉语言线索的相关性来评估推理步骤的质量，所有这些都无需额外模型。VReST超越了当前的提示方法，并在三个多模态数学推理基准上取得了最先进的性能。此外，它证实了测试时缩放定律在多模态任务中的有效性，为未来的研究提供了有前景的方向。", "summary": "VReST是一种无需训练的新方法，旨在通过结合蒙特卡洛树搜索和创新的多模态自奖励机制，增强大型视觉语言模型（LVLMs）在复杂视觉推理任务中的表现。该方法通过构建推理搜索树并评估推理步骤的质量，解决了LVLMs在复杂推理方面的局限性。VReST在多个多模态数学推理基准上实现了最先进的性能，并验证了测试时缩放定律在多模态任务中的潜力。", "keywords": "大型视觉语言模型, 视觉推理, 树搜索, 自奖励机制, 多模态学习", "comments": "这篇论文通过引入无需训练的树搜索和自奖励机制，为提升大型视觉语言模型的复杂推理能力提供了一个新颖且有效的方法。其创新之处在于将蒙特卡洛树搜索应用于推理路径的探索，并设计了无需额外模型的自奖励机制来评估推理步骤的质量，这显著降低了训练成本。在多模态数学推理任务上取得SOTA性能，并验证测试时缩放定律，显示了其重要性和未来研究潜力。"}}
{"id": "2506.08388", "title": "Reinforcement Learning Teachers of Test Time Scaling", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "summary": "Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework.", "comment": "Preprint", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08388v1", "AI": {"title_translation": "测试时缩放的强化学习教师", "tldr": "本文提出了一种名为强化学习教师（RLTs）的新框架，通过生成详细解释来有效指导学生语言模型，解决了传统强化学习在语言模型训练中的探索难题，并在蒸馏和冷启动任务上超越了更大的模型。", "motivation": "现有的用强化学习训练推理语言模型（LMs）的方法，其单点正确性依赖于LM在初始化时能否进行探索并解决任务，且LMs的主要用途是作为教师来蒸馏新学生和冷启动未来的RL迭代，而非直接部署。", "method": "引入了强化学习教师（RLTs）框架，旨在产生最有效的下游蒸馏。RLTs被同时提供问题和解决方案，任务是连接点并为学生量身定制详细解释。通过将每个解释反馈给学生并测试学生对问题解决方案的理解来获得密集奖励，从而训练RLTs。", "result": "在竞赛和研究生级别的任务中，一个7B RLT的原始输出比现有收集和后处理更大规模LM推理轨迹的蒸馏和冷启动管道提供了更高的最终性能。此外，RLTs在训练更大的学生时以及零样本应用于分布外任务时仍能保持其有效性。", "conclusion": "强化学习教师（RLTs）通过避免RL的探索挑战并专注于有效的下游蒸馏，为RL推理框架带来了新的效率和可重用性。", "translation": "用强化学习（RL）训练推理语言模型（LMs）以实现单点正确性，本质上依赖于LM在初始化时能够进行探索并解决其任务。此外，推理LMs的一个关键用例是充当教师，用于蒸馏新学生和冷启动未来的RL迭代，而不是自身被部署。基于这些考虑，我们引入了一个新框架，通过训练一类新的强化学习教师（RLTs）来避免RL的探索挑战，这些教师专注于产生最有效的下游蒸馏。RLTs被同时提示问题和解决方案，其任务是简单地“连接点”，提供为学生量身定制的详细解释。我们通过将每个解释反馈给学生并测试其对问题解决方案的理解来获得密集奖励，从而训练RLTs。在实践中，一个7B RLT的原始输出在竞赛和研究生级别的任务上，比现有收集和后处理大几个数量级LM推理轨迹的蒸馏和冷启动管道提供了更高的最终性能。此外，RLTs在训练更大的学生时以及零样本应用于分布外任务时仍能保持其有效性，为RL推理框架带来了新的效率和可重用性。", "summary": "本文提出了一种名为强化学习教师（RLTs）的新型框架，旨在解决传统强化学习训练推理语言模型时面临的探索难题。RLTs被设计为通过接收问题和解决方案，并生成详细解释来有效指导学生语言模型。这种方法通过评估学生对解释的理解来提供密集奖励。实验结果表明，一个7B的RLT在蒸馏和冷启动任务上的表现优于使用更大规模语言模型的现有方法，并且在训练更大模型和处理分布外任务时依然有效，显著提升了RL推理框架的效率和可重用性。", "keywords": "强化学习, 语言模型, 知识蒸馏, 教师模型, 解释生成", "comments": "本文提出了一种新颖且实用的方法来解决强化学习在语言模型训练中面临的探索效率问题。通过将重点从直接解决任务转移到生成高质量的教学解释，RLTs有效地利用了已有的解决方案信息，从而避免了耗时的探索过程。这种“教师”范式不仅提高了训练效率，还使得小模型能够指导大模型，并展现出良好的泛化能力，对于未来高效的语言模型训练和知识蒸馏具有重要意义。"}}
{"id": "2506.08694", "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning", "authors": ["Mohammadreza Salehi", "Shashanka Venkataramanan", "Ioana Simion", "Efstratios Gavves", "Cees G. M. Snoek", "Yuki M Asano"], "summary": "Dense self-supervised learning has shown great promise for learning pixel-\nand patch-level representations, but extending it to videos remains challenging\ndue to the complexity of motion dynamics. Existing approaches struggle as they\nrely on static augmentations that fail under object deformations, occlusions,\nand camera movement, leading to inconsistent feature learning over time. We\npropose a motion-guided self-supervised learning framework that clusters dense\npoint tracks to learn spatiotemporally consistent representations. By\nleveraging an off-the-shelf point tracker, we extract long-range motion\ntrajectories and optimize feature clustering through a momentum-encoder-based\noptimal transport mechanism. To ensure temporal coherence, we propagate cluster\nassignments along tracked points, enforcing feature consistency across views\ndespite viewpoint changes. Integrating motion as an implicit supervisory\nsignal, our method learns representations that generalize across frames,\nimproving robustness in dynamic scenes and challenging occlusion scenarios. By\ninitializing from strong image-pretrained models and leveraging video data for\ntraining, we improve state-of-the-art by 1% to 6% on six image and video\ndatasets and four evaluation benchmarks. The implementation is publicly\navailable at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main", "comment": "preprint", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08694v1", "AI": {"title_translation": "MoSiC: 密集自监督学习的最优传输运动轨迹", "tldr": "MoSiC提出了一种运动引导的自监督学习框架，通过利用运动轨迹和最优传输机制，解决了视频密集自监督学习中特征不一致的问题，并在多个图像和视频数据集上取得了SOTA性能提升。", "motivation": "现有密集自监督学习方法难以扩展到视频，因为它们依赖静态数据增强，在物体形变、遮挡和摄像机移动下表现不佳，导致特征学习随时间不一致。", "method": "本文提出了一种运动引导的自监督学习框架，通过聚类密集点轨迹来学习时空一致的表示。该方法利用现成的点跟踪器提取长程运动轨迹，并通过基于动量编码器的最优传输机制优化特征聚类。为确保时间连贯性，沿着跟踪点传播聚类分配，强制跨视图的特征一致性，即使在视点变化下也是如此。将运动作为隐式监督信号。", "result": "通过从强大的图像预训练模型初始化并利用视频数据进行训练，该方法在六个图像和视频数据集以及四个评估基准上，将现有技术水平提高了1%到6%。", "conclusion": "MoSiC框架通过整合运动信息作为隐式监督信号，有效地解决了视频密集自监督学习中的挑战，学习到跨帧泛化的鲁棒表示，并在动态场景和遮挡情景下表现出更高的鲁棒性。", "translation": "密集自监督学习在学习像素级和补丁级表示方面展现出巨大前景，但由于运动动态的复杂性，将其扩展到视频仍然具有挑战性。现有方法因依赖静态增强而在物体形变、遮挡和摄像机移动下失效，导致特征学习随时间不一致，从而面临困难。我们提出了一种运动引导的自监督学习框架，该框架聚类密集点轨迹以学习时空一致的表示。通过利用现成的点跟踪器，我们提取长程运动轨迹，并通过基于动量编码器的最优传输机制优化特征聚类。为确保时间连贯性，我们沿着跟踪点传播聚类分配，强制跨视图的特征一致性，尽管视点发生变化。通过将运动作为隐式监督信号，我们的方法学习到的表示能够跨帧泛化，提高了在动态场景和挑战性遮挡情景下的鲁棒性。通过从强大的图像预训练模型初始化并利用视频数据进行训练，我们在六个图像和视频数据集和四个评估基准上将现有技术水平提高了1%到6%。该实现已在我们的GitHub仓库公开：https://github.com/SMSD75/MoSiC/tree/main", "summary": "MoSiC提出了一种新颖的运动引导自监督学习框架，旨在解决视频中密集自监督学习的挑战。针对现有方法在动态场景下特征不一致的问题，MoSiC利用点跟踪器提取长程运动轨迹，并通过基于动量编码器的最优传输机制优化特征聚类。该方法通过沿跟踪点传播聚类分配来确保时间连贯性，从而学习到时空一致且跨帧泛化的表示。实验结果表明，MoSiC在多个图像和视频数据集上，相对于现有技术实现了1%到6%的性能提升。", "keywords": "自监督学习, 视频, 运动轨迹, 最优传输, 密集表示", "comments": "MoSiC的创新点在于将运动轨迹作为隐式监督信号引入自监督学习，并通过最优传输机制实现特征聚类，有效解决了视频数据中复杂的运动动态和时间一致性问题。这种方法显著提升了在动态场景和遮挡下的表示鲁棒性，并在多个基准上取得了显著的性能提升，为视频自监督学习领域提供了新的思路。"}}
{"id": "2506.08397", "title": "Spatiotemporal deep learning models for detection of rapid intensification in cyclones", "authors": ["Vamshika Sutar", "Amandeep Singh", "Rohitash Chandra"], "summary": "Cyclone rapid intensification is the rapid increase in cyclone wind\nintensity, exceeding a threshold of 30 knots, within 24 hours. Rapid\nintensification is considered an extreme event during a cyclone, and its\noccurrence is relatively rare, contributing to a class imbalance in the\ndataset. A diverse array of factors influences the likelihood of a cyclone\nundergoing rapid intensification, further complicating the task for\nconventional machine learning models. In this paper, we evaluate deep learning,\nensemble learning and data augmentation frameworks to detect cyclone rapid\nintensification based on wind intensity and spatial coordinates. We note that\nconventional data augmentation methods cannot be utilised for generating\nspatiotemporal patterns replicating cyclones that undergo rapid\nintensification. Therefore, our framework employs deep learning models to\ngenerate spatial coordinates and wind intensity that replicate cyclones to\naddress the class imbalance problem of rapid intensification. We also use a\ndeep learning model for the classification module within the data augmentation\nframework to differentiate between rapid and non-rapid intensification events\nduring a cyclone. Our results show that data augmentation improves the results\nfor rapid intensification detection in cyclones, and spatial coordinates play a\ncritical role as input features to the given models. This paves the way for\nresearch in synthetic data generation for spatiotemporal data with extreme\nevents.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08397v1", "AI": {"title_translation": "旋风快速增强检测的时空深度学习模型", "tldr": "本文提出并评估了使用深度学习和数据增强框架来检测旋风快速增强，并通过生成时空数据来解决类别不平衡问题，结果表明数据增强有效且空间坐标是关键特征。", "motivation": "旋风快速增强是极端事件，发生相对罕见，导致数据集中存在类别不平衡问题，且多种因素影响其发生，使传统机器学习模型难以处理。", "method": "评估了深度学习、集成学习和数据增强框架来检测旋风快速增强。为了解决类别不平衡问题，该框架利用深度学习模型生成复制旋风的时空坐标和风强度数据。同时，使用深度学习模型作为数据增强框架内的分类模块，以区分快速和非快速增强事件。", "result": "数据增强改善了旋风快速增强检测的结果，且空间坐标作为输入特征对给定模型起着关键作用。", "conclusion": "这为极端事件时空数据的合成数据生成研究铺平了道路。", "translation": "旋风快速增强是指旋风风速在24小时内迅速增加，超过30节的阈值。快速增强被认为是旋风期间的极端事件，其发生相对罕见，导致数据集中存在类别不平衡。多种因素影响旋风发生快速增强的可能性，这进一步使传统机器学习模型的任务复杂化。在本文中，我们评估了深度学习、集成学习和数据增强框架，以基于风强度和空间坐标检测旋风快速增强。我们注意到，传统的数据增强方法不能用于生成复制快速增强旋风的时空模式。因此，我们的框架采用深度学习模型生成复制旋风的空间坐标和风强度，以解决快速增强的类别不平衡问题。我们还在数据增强框架内使用深度学习模型作为分类模块，以区分旋风期间的快速和非快速增强事件。我们的结果表明，数据增强改善了旋风快速增强检测的结果，并且空间坐标作为给定模型的输入特征起着关键作用。这为极端事件时空数据的合成数据生成研究铺平了道路。", "summary": "本文针对旋风快速增强这一罕见且存在类别不平衡问题的极端事件检测，提出并评估了结合深度学习、集成学习和数据增强的框架。为克服传统数据增强局限性，该框架利用深度学习模型生成合成的时空数据以解决类别不平衡问题，并使用深度学习模型进行分类。研究结果表明，数据增强有效提升了快速增强检测性能，且空间坐标是关键输入特征，为极端时空数据的合成生成研究开辟了新途径。", "keywords": "旋风快速增强, 深度学习, 数据增强, 时空数据, 类别不平衡", "comments": "这项研究通过引入基于深度学习的合成时空数据生成来解决旋风快速增强检测中的类别不平衡问题，具有创新性。它强调了数据增强和空间坐标在处理极端事件时空数据方面的潜力，为未来合成数据研究提供了方向。"}}
{"id": "2506.08699", "title": "ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds", "authors": ["Frederik Hagelskjaer"], "summary": "This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose\nestimation network for colorless point clouds. The pose estimation is\ncalculated from center and top points of the object, predicted by the neural\nnetwork. The network is trained on synthetic data, and tested on a benchmark\ndataset, where it demonstrates state-of-the-art performance and outperforms all\ncolorless methods. The network is able to run inference in only 250\nmilliseconds making it usable in many scenarios. Project page with code at\narrowpose.github.io", "comment": "6 pages, 5 figures, 4 tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08699v1", "AI": {"title_translation": "ArrowPose：无色点云的分割、检测和5自由度姿态估计网络", "tldr": "ArrowPose是一种用于无色点云的快速5自由度姿态估计网络，在基准数据集上表现出最先进的性能，并且推理速度快。", "motivation": "解决无色点云中物体快速检测和5自由度姿态估计的问题。", "method": "本文提出了ArrowPose网络，通过神经网络预测物体的中心点和顶部点来计算5自由度姿态。该网络在合成数据上进行训练，并在基准数据集上进行测试。", "result": "在基准数据集上展示了最先进的性能，超越了所有无色方法。该网络能够在250毫秒内完成推理。", "conclusion": "ArrowPose是一个高效且实用的无色点云姿态估计算法，其快速的推理能力使其适用于多种场景。", "translation": "本文提出了一种用于无色点云的快速检测和5自由度（DoF）姿态估计网络。姿态估计是通过神经网络预测的物体中心点和顶部点计算得出的。该网络在合成数据上进行训练，并在基准数据集上进行测试，展示了最先进的性能，超越了所有无色方法。该网络能够在250毫秒内完成推理，使其适用于许多场景。项目页面和代码位于arrowpose.github.io。", "summary": "ArrowPose是一种用于无色点云的神经网络，它通过预测物体的中心点和顶部点来实现快速检测和5自由度姿态估计。该网络在合成数据上训练，并在基准数据集上验证，表现出领先的性能和高效的推理速度（250毫秒），使其适用于多种实际应用。", "keywords": "无色点云, 姿态估计, 深度学习, 物体检测, 5自由度", "comments": "该论文提出了一种高效的无色点云姿态估计方法，其创新点在于从中心点和顶部点预测姿态，并实现了极快的推理速度，这对于实时应用至关重要。其在合成数据上训练的能力也为实际部署提供了便利。"}}
{"id": "2506.08066", "title": "WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection", "authors": ["Alexander Stepikin", "Evgenia Romanenkova", "Alexey Zaytsev"], "summary": "Change Point Detection (CPD) aims to identify moments of abrupt distribution\nshifts in data streams. Real-world high-dimensional CPD remains challenging due\nto data pattern complexity and violation of common assumptions. Resorting to\nstandalone deep neural networks, the current state-of-the-art detectors have\nyet to achieve perfect quality. Concurrently, ensembling provides more robust\nsolutions, boosting the performance. In this paper, we investigate ensembles of\ndeep change point detectors and realize that standard prediction aggregation\ntechniques, e.g., averaging, are suboptimal and fail to account for problem\npeculiarities. Alternatively, we introduce WWAggr -- a novel task-specific\nmethod of ensemble aggregation based on the Wasserstein distance. Our procedure\nis versatile, working effectively with various ensembles of deep CPD models.\nMoreover, unlike existing solutions, we practically lift a long-standing\nproblem of the decision threshold selection for CPD.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.08066v1", "AI": {"title_translation": "WWAggr：一种基于窗口 Wasserstein 的集成变点检测聚合方法", "tldr": "WWAggr是一种基于Wasserstein距离的新型集成变点检测聚合方法，旨在解决高维数据中的挑战并简化决策阈值选择。", "motivation": "现有深度神经网络变点检测器在高维数据中表现不完美，且标准预测聚合技术（如平均）在集成变点检测中次优，未能考虑到问题特殊性。", "method": "本文引入了WWAggr，一种基于Wasserstein距离的新型任务特定集成聚合方法，用于深度变点检测模型集成。", "result": "WWAggr与各种深度CPD模型集成都能有效工作，并且解决了变点检测中决策阈值选择的长期问题。", "conclusion": "WWAggr通过引入基于Wasserstein距离的聚合方法，提升了集成变点检测的性能，并解决了决策阈值选择的难题。", "translation": "变点检测（CPD）旨在识别数据流中分布突然变化的时刻。由于数据模式的复杂性和常见假设的违反，真实世界中的高维CPD仍然具有挑战性。依赖于独立的深度神经网络，当前的最新检测器尚未达到完美的质量。同时，集成提供了更鲁棒的解决方案，提高了性能。在本文中，我们研究了深度变点检测器的集成，并认识到标准的预测聚合技术，例如平均，是次优的，并且未能考虑到问题的特殊性。作为替代，我们引入了WWAggr——一种基于Wasserstein距离的新型任务特定集成聚合方法。我们的程序用途广泛，与各种深度CPD模型集成都能有效工作。此外，与现有解决方案不同，我们实际解决了CPD中决策阈值选择的长期问题。", "summary": "本文提出了一种名为WWAggr的新型集成聚合方法，用于解决高维数据流中的变点检测挑战。该方法基于Wasserstein距离，旨在改进现有深度学习变点检测器集成中次优的聚合技术。WWAggr能够与多种深度CPD模型有效配合，并解决了变点检测中长期存在的决策阈值选择问题，从而提高了检测性能。", "keywords": "变点检测, 集成学习, Wasserstein距离, 数据流, 聚合", "comments": "该论文的创新点在于提出了基于Wasserstein距离的任务特定聚合方法WWAggr，解决了集成变点检测中标准聚合方法的局限性，并实际克服了决策阈值选择这一长期难题，对高维数据流的变点检测具有重要意义。"}}
{"id": "2506.08409", "title": "FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion", "authors": ["Fred Xu", "Song Jiang", "Zijie Huang", "Xiao Luo", "Shichang Zhang", "Adrian Chen", "Yizhou Sun"], "summary": "Taxonomy Expansion, which models complex concepts and their relations, can be\nformulated as a set representation learning task. The generalization of set,\nfuzzy set, incorporates uncertainty and measures the information within a\nsemantic concept, making it suitable for concept modeling. Existing works\nusually model sets as vectors or geometric objects such as boxes, which are not\nclosed under set operations. In this work, we propose a sound and efficient\nformulation of set representation learning based on its volume approximation as\na fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),\nsatisfies all set operations and compactly approximates the underlying fuzzy\nset, hence preserving information while being efficient to learn, relying on\nminimum neural architecture. We empirically demonstrate the power of FUSE on\nthe task of taxonomy expansion, where FUSE achieves remarkable improvements up\nto 23% compared with existing baselines. Our work marks the first attempt to\nunderstand and efficiently compute the embeddings of fuzzy sets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08409v1", "AI": {"title_translation": "FUSE：基于测度论紧凑模糊集表示的分类学扩展", "tldr": "FUSE提出了一种基于测度论紧凑模糊集表示的分类学扩展方法，解决了现有集合表示不封闭的问题，并在分类学扩展任务上取得了显著改进。", "motivation": "现有的集合表示方法（如向量或几何对象）在集合操作下不封闭，且在概念建模中未能充分利用模糊集处理不确定性和测量信息的能力。", "method": "提出Fuzzy Set Embedding (FUSE)，一种基于模糊集体积近似的集合表示学习框架。FUSE满足所有集合操作，紧凑地近似底层模糊集，高效学习且依赖最小神经网络架构。", "result": "在分类学扩展任务中，FUSE比现有基线提高了高达23%。", "conclusion": "FUSE是首次尝试理解和高效计算模糊集嵌入的工作，并在分类学扩展中表现出色。", "translation": "分类学扩展，即对复杂概念及其关系进行建模，可以被表述为一种集合表示学习任务。集合的泛化形式——模糊集，融合了不确定性并测量语义概念中的信息，使其适用于概念建模。现有工作通常将集合建模为向量或几何对象（如盒子），这些方法在集合操作下不封闭。在这项工作中，我们提出了一种基于其体积近似作为模糊集的集合表示学习的健全且高效的公式。由此产生的嵌入框架，即模糊集嵌入（FUSE），满足所有集合操作，并紧凑地近似底层模糊集，因此在高效学习的同时保留了信息，且仅依赖于最小的神经网络架构。我们通过经验证明了FUSE在分类学扩展任务上的强大能力，FUSE与现有基线相比取得了高达23%的显著改进。我们的工作标志着首次尝试理解和高效计算模糊集嵌入。", "summary": "FUSE是一种新颖的基于测度论紧凑模糊集表示的分类学扩展方法。它通过将集合表示学习公式化为模糊集的体积近似，解决了传统集合表示在集合操作下不封闭的问题。FUSE框架满足所有集合操作，高效且紧凑地近似模糊集，并在分类学扩展任务上实现了高达23%的性能提升。", "keywords": "分类学扩展, 模糊集, 集合表示学习, 测度论, FUSE", "comments": "该工作创新性地将模糊集的概念引入到集合表示学习中，并通过测度论解决了现有方法在集合操作下不封闭的局限性。其在分类学扩展任务上的显著性能提升表明了模糊集嵌入的巨大潜力，为未来处理不确定性概念和信息测量提供了新的视角。"}}
{"id": "2506.08704", "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering", "authors": ["Xiaohan Zhang", "Sitong Wang", "Yushen Yan", "Yi Yang", "Mingda Xu", "Qi Liu"], "summary": "High-quality novel view synthesis for large-scale scenes presents a\nchallenging dilemma in 3D computer vision. Existing methods typically partition\nlarge scenes into multiple regions, reconstruct a 3D representation using\nGaussian splatting for each region, and eventually merge them for novel view\nrendering. They can accurately render specific scenes, yet they do not\ngeneralize effectively for two reasons: (1) rigid spatial partition techniques\nstruggle with arbitrary camera trajectories, and (2) the merging of regions\nresults in Gaussian overlap to distort texture details. To address these\nchallenges, we propose TraGraph-GS, leveraging a trajectory graph to enable\nhigh-precision rendering for arbitrarily large-scale scenes. We present a\nspatial partitioning method for large-scale scenes based on graphs, which\nincorporates a regularization constraint to enhance the rendering of textures\nand distant objects, as well as a progressive rendering strategy to mitigate\nartifacts caused by Gaussian overlap. Experimental results demonstrate its\nsuperior performance both on four aerial and four ground datasets and highlight\nits remarkable efficiency: our method achieves an average improvement of 1.86\ndB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to\nstate-of-the-art approaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08704v1", "AI": {"title_translation": "TraGraph-GS：基于轨迹图的任意大规模场景高斯泼溅渲染", "tldr": "TraGraph-GS提出了一种基于轨迹图的高斯泼溅方法，旨在解决大规模场景新颖视图合成中现有方法的泛化性差和纹理失真问题，实现了更高的渲染精度和效率。", "motivation": "现有大规模场景新颖视图合成方法存在两个主要问题：1) 刚性空间划分技术难以适应任意相机轨迹；2) 区域合并导致高斯重叠，从而扭曲纹理细节。这些问题导致现有方法泛化能力差。", "method": "本文提出了TraGraph-GS方法，利用轨迹图实现任意大规模场景的高精度渲染。该方法包括基于图的大规模场景空间划分方法，并引入正则化约束以增强纹理和远距离物体的渲染效果，以及渐进式渲染策略以减轻高斯重叠导致的伪影。", "result": "实验结果表明，TraGraph-GS在四个空中和四个地面数据集上均表现出卓越的性能和显著的效率。与现有最先进方法相比，在空中数据集上PSNR平均提高了1.86 dB，在地面数据集上平均提高了1.62 dB。", "conclusion": "TraGraph-GS通过引入轨迹图和新的空间划分及渲染策略，有效解决了大规模场景新颖视图合成中的泛化性和纹理失真问题，显著提高了渲染质量和效率。", "translation": "大规模场景高质量新颖视图合成是3D计算机视觉中一个具有挑战性的难题。现有方法通常将大场景划分为多个区域，对每个区域使用高斯泼溅重建3D表示，并最终合并它们进行新颖视图渲染。它们可以准确渲染特定场景，但由于两个原因未能有效泛化：(1) 刚性空间划分技术难以处理任意相机轨迹，以及 (2) 区域合并导致高斯重叠以扭曲纹理细节。为了解决这些挑战，我们提出了TraGraph-GS，利用轨迹图实现任意大规模场景的高精度渲染。我们提出了一种基于图的大规模场景空间划分方法，该方法结合了正则化约束以增强纹理和远距离物体的渲染，以及一种渐进式渲染策略以减轻高斯重叠引起的伪影。实验结果表明，它在四个空中和四个地面数据集上均表现出卓越的性能，并突出了其显著的效率：与最先进的方法相比，我们的方法在空中数据集上PSNR平均提高了1.86 dB，在地面数据集上平均提高了1.62 dB。", "summary": "TraGraph-GS提出了一种基于轨迹图的高斯泼溅方法，旨在解决大规模场景新颖视图合成中现有方法的泛化性差和纹理细节失真问题。通过引入基于图的空间划分、正则化约束和渐进式渲染策略，该方法能够处理任意相机轨迹并减轻高斯重叠伪影。实验证明，TraGraph-GS在渲染质量和效率上均优于现有技术，显著提升了PSNR值。", "keywords": "大规模场景渲染, 高斯泼溅, 轨迹图, 新颖视图合成, 3D计算机视觉", "comments": "TraGraph-GS的创新点在于引入轨迹图来解决大规模场景渲染中的相机轨迹适应性问题，并通过图结构的空间划分和渐进式渲染策略有效解决了高斯重叠导致的纹理失真，这对于提高大规模场景新颖视图合成的实用性和泛化能力具有重要意义。"}}
{"id": "2506.08412", "title": "Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics", "authors": ["Saraa Ali", "Aleksandr Khizhik", "Stepan Svirin", "Artem Ryzhikov", "Denis Derkach"], "summary": "The application of machine learning (ML) algorithms in the intelligent\ndiagnosis of three-phase engines has the potential to significantly enhance\ndiagnostic performance and accuracy. Traditional methods largely rely on\nsignature analysis, which, despite being a standard practice, can benefit from\nthe integration of advanced ML techniques. In our study, we innovate by\ncombining ML algorithms with a novel unsupervised anomaly generation\nmethodology that takes into account the engine physics model. We propose\nSignature-Guided Data Augmentation (SGDA), an unsupervised framework that\nsynthesizes physically plausible faults directly in the frequency domain of\nhealthy current signals. Guided by Motor Current Signature Analysis, SGDA\ncreates diverse and realistic anomalies without resorting to computationally\nintensive simulations. This hybrid approach leverages the strengths of both\nsupervised ML and unsupervised signature analysis, achieving superior\ndiagnostic accuracy and reliability along with wide industrial application. The\nfindings highlight the potential of our approach to contribute significantly to\nthe field of engine diagnostics, offering a robust and efficient solution for\nreal-world applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08412v1", "AI": {"title_translation": "学习听懂损坏的电机：用于感应电机诊断的签名引导数据增强", "tldr": "该研究结合机器学习算法与一种新颖的无监督异常生成方法（SGDA），通过在健康电流信号的频域中直接合成物理上合理的故障，提高了感应电机诊断的性能和准确性，无需计算密集型仿真。", "motivation": "传统的电机诊断方法主要依赖于签名分析，但结合先进的机器学习技术可以显著提高诊断性能和准确性。现有方法在生成物理上合理的故障数据方面存在挑战，尤其是在避免计算密集型仿真方面。", "method": "提出了一种名为签名引导数据增强（SGDA）的无监督框架。该方法在健康电流信号的频域中直接合成物理上合理的故障。SGDA通过电机电流签名分析进行引导，创建多样且真实的异常，而无需进行计算密集型仿真。", "result": "实现了卓越的诊断准确性和可靠性，并具有广泛的工业应用潜力。", "conclusion": "该方法有望显著促进发动机诊断领域的发展，为实际应用提供一个鲁棒且高效的解决方案。", "translation": "机器学习（ML）算法在三相发动机智能诊断中的应用，有潜力显著提高诊断性能和准确性。传统方法主要依赖于签名分析，尽管这是一种标准做法，但通过整合先进的ML技术可以从中受益。在本研究中，我们通过将ML算法与一种新颖的、考虑发动机物理模型的无监督异常生成方法相结合进行创新。我们提出了签名引导数据增强（SGDA），这是一个无监督框架，可以直接在健康电流信号的频域中合成物理上合理的故障。在电机电流签名分析的引导下，SGDA创建了多样化且真实的异常，而无需借助计算密集型仿真。这种混合方法利用了监督ML和无监督签名分析的优势，实现了卓越的诊断准确性和可靠性，并具有广泛的工业应用。研究结果突出了我们方法在发动机诊断领域做出重大贡献的潜力，为实际应用提供了鲁棒而高效的解决方案。", "summary": "本文提出了一种名为签名引导数据增强（SGDA）的新型无监督框架，旨在提高感应电机的故障诊断能力。该方法将机器学习算法与发动机物理模型相结合，能够在健康电流信号的频域中直接合成物理上合理的故障数据。通过电机电流签名分析的引导，SGDA无需计算密集型仿真即可生成多样且真实的异常，从而显著提升了诊断准确性和可靠性，为工业应用提供了鲁棒高效的解决方案。", "keywords": "感应电机诊断, 数据增强, 机器学习, 异常生成, 签名分析", "comments": "该论文的创新之处在于其提出的SGDA方法，它有效地结合了传统签名分析的物理知识与机器学习的数据增强能力。通过在频域直接合成故障信号，SGDA解决了机器学习在电机故障诊断中常见的训练数据稀缺问题，同时避免了耗时耗力的物理仿真，这对于实际工业应用具有重要意义和实用价值。"}}
{"id": "2506.08710", "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting", "authors": ["Mengjiao Ma", "Qi Ma", "Yue Li", "Jiahuan Cheng", "Runyi Yang", "Bin Ren", "Nikola Popovic", "Mingqiang Wei", "Nicu Sebe", "Luc Van Gool", "Theo Gevers", "Martin R. Oswald", "Danda Pani Paudel"], "summary": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient\nencoding of scene geometry, appearance, and semantics. Moreover, grounding\nlanguage in 3D scenes has proven to be an effective strategy for 3D scene\nunderstanding. Current Language Gaussian Splatting line of work fall into three\nmain groups: (i) per-scene optimization-based, (ii) per-scene\noptimization-free, and (iii) generalizable approach. However, most of them are\nevaluated only on rendered 2D views of a handful of scenes and viewpoints close\nto the training views, limiting ability and insight into holistic 3D\nunderstanding. To address this gap, we propose the first large-scale benchmark\nthat systematically assesses these three groups of methods directly in 3D\nspace, evaluating on 1060 scenes across three indoor datasets and one outdoor\ndataset. Benchmark results demonstrate a clear advantage of the generalizable\nparadigm, particularly in relaxing the scene-specific limitation, enabling fast\nfeed-forward inference on novel scenes, and achieving superior segmentation\nperformance. We further introduce GaussianWorld-49K a carefully curated 3DGS\ndataset comprising around 49K diverse indoor and outdoor scenes obtained from\nmultiple sources, with which we demonstrate the generalizable approach could\nharness strong data priors. Our codes, benchmark, and datasets will be made\npublic to accelerate research in generalizable 3DGS scene understanding.", "comment": "15 pages, codes, data and benchmark will be released", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08710v1", "AI": {"title_translation": "SceneSplat++：一个用于语言高斯泼溅的大型数据集和综合基准", "tldr": "本文提出了SceneSplat++，一个大型数据集和基准，用于评估3D语言高斯泼溅方法，发现可泛化方法表现最佳，并引入GaussianWorld-49K数据集以利用数据先验。", "motivation": "当前的语言高斯泼溅方法主要在渲染的2D视图上评估，场景和视点有限，这限制了对整体3D理解的能力和洞察力。", "method": "本文提出了第一个大规模基准，直接在3D空间中系统地评估了三种主要的语言高斯泼溅方法（基于每场景优化、免每场景优化、可泛化方法），并在1060个场景上进行了评估。此外，本文还引入了GaussianWorld-49K数据集，包含约49K个多样化的室内外3DGS场景。", "result": "基准测试结果表明，可泛化范式具有明显优势，特别是在放宽场景特定限制、在新型场景上实现快速前向推理以及实现卓越分割性能方面。通过GaussianWorld-49K数据集，本文证明了可泛化方法可以利用强大的数据先验。", "conclusion": "可泛化的3D高斯泼溅方法在3D场景理解中表现出优越性，并且通过大规模数据集可以进一步提升其性能。本文的基准、数据集和代码将促进该领域的研究。", "translation": "3D高斯泼溅（3DGS）作为场景几何、外观和语义的高性能、高效编码方式。此外，在3D场景中接地语言已被证明是3D场景理解的有效策略。当前语言高斯泼溅的工作主要分为三类：（i）基于每场景优化的方法，（ii）免每场景优化的方法，以及（iii）可泛化方法。然而，它们中的大多数仅在少数场景的渲染2D视图和接近训练视图的视点上进行评估，这限制了对整体3D理解的能力和洞察力。为了解决这一空白，我们提出了第一个大规模基准，它直接在3D空间中系统地评估这三类方法，在三个室内数据集和一个室外数据集的1060个场景上进行评估。基准测试结果表明，可泛化范式具有明显优势，特别是在放宽场景特定限制、在新型场景上实现快速前向推理以及实现卓越分割性能方面。我们进一步引入了GaussianWorld-49K，这是一个精心策划的3DGS数据集，包含从多个来源获得的约49K个多样化室内外场景，我们通过它证明了可泛化方法可以利用强大的数据先验。我们的代码、基准和数据集将公开发布，以加速可泛化3DGS场景理解的研究。", "summary": "本文提出了SceneSplat++，一个大型基准数据集，旨在解决当前语言高斯泼溅方法评估的局限性。该基准直接在3D空间中对三种主要方法进行系统评估，涵盖1060个场景。实验结果表明，可泛化方法在处理新场景和分割性能方面具有显著优势。此外，本文还引入了GaussianWorld-49K，一个包含约49K个多样化3DGS场景的数据集，进一步证明了可泛化方法能有效利用数据先验。所有代码、基准和数据集都将公开，以促进可泛化3DGS场景理解的研究。", "keywords": "3D Gaussian Splatting, 语言高斯泼溅, 数据集, 基准测试, 可泛化方法", "comments": "该论文通过引入大规模3D评估基准和大型数据集，解决了当前语言高斯泼溅领域评估不足和数据稀缺的问题，对于推动可泛化3DGS场景理解研究具有重要意义。特别强调可泛化方法的优势，有望加速该领域的实际应用。"}}
{"id": "2506.08073", "title": "Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy", "authors": ["Yu Liu", "Utkarsh Pratiush", "Kamyar Barakati", "Hiroshi Funakubo", "Ching-Che Lin", "Jaegyu Kim", "Lane W. Martin", "Sergei V. Kalinin"], "summary": "Ferroelectric polarization switching underpins the functional performance of\na wide range of materials and devices, yet its dependence on complex local\nmicrostructural features renders systematic exploration by manual or grid-based\nspectroscopic measurements impractical. Here, we introduce a multi-objective\nkernel-learning workflow that infers the microstructural rules governing\nswitching behavior directly from high-resolution imaging data. Applied to\nautomated piezoresponse force microscopy (PFM) experiments, our framework\nefficiently identifies the key relationships between domain-wall configurations\nand local switching kinetics, revealing how specific wall geometries and defect\ndistributions modulate polarization reversal. Post-experiment analysis projects\nabstract reward functions, such as switching ease and domain symmetry, onto\nphysically interpretable descriptors including domain configuration and\nproximity to boundaries. This enables not only high-throughput active learning,\nbut also mechanistic insight into the microstructural control of switching\nphenomena. While demonstrated for ferroelectric domain switching, our approach\nprovides a powerful, generalizable tool for navigating complex,\nnon-differentiable design spaces, from structure-property correlations in\nmolecular discovery to combinatorial optimization across diverse imaging\nmodalities.", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.08073v1", "AI": {"title_translation": "帕累托前沿上的畴翻转：自动化压电力显微镜中的多目标深度核学习", "tldr": "本文提出了一种结合自动化压电力显微镜的多目标核学习工作流程，有效识别铁电极化翻转的微结构规则，提供机制洞察并实现高通量主动学习。", "motivation": "铁电极化翻转的系统探索因其对复杂局部微结构特征的依赖性以及手动或基于网格的光谱测量的不切实际性而面临挑战。", "method": "引入了一种多目标核学习工作流程，直接从高分辨率成像数据中推断控制翻转行为的微结构规则。该框架应用于自动化压电力显微镜（PFM）实验，并通过实验后分析将抽象奖励函数投射到可物理解释的描述符上。", "result": "该框架有效地识别了畴壁配置与局部翻转动力学之间的关键关系，揭示了特定畴壁几何形状和缺陷分布如何调节极化反转。它实现了高通量主动学习，并提供了对翻转现象微结构控制的机制洞察。", "conclusion": "该方法为探索复杂的、不可微分的设计空间提供了一种强大且可推广的工具，可应用于分子发现中的结构-性质关联和跨多种成像模式的组合优化，超越了铁电畴翻转的范畴。", "translation": "铁电极化翻转是各种材料和器件功能性能的基础，但其对复杂局部微结构特征的依赖性使得通过手动或基于网格的光谱测量进行系统探索变得不切实际。本文引入了一种多目标核学习工作流程，该流程直接从高分辨率成像数据中推断控制翻转行为的微结构规则。应用于自动化压电力显微镜（PFM）实验时，我们的框架有效地识别了畴壁配置与局部翻转动力学之间的关键关系，揭示了特定畴壁几何形状和缺陷分布如何调节极化反转。实验后分析将抽象的奖励函数（例如翻转难易程度和畴对称性）投射到可物理解释的描述符上，包括畴配置和与边界的接近程度。这不仅实现了高通量主动学习，还提供了对翻转现象微结构控制的机制洞察。尽管本文展示的是铁电畴翻转，但我们的方法为探索复杂的、不可微分的设计空间提供了一种强大且可推广的工具，从分子发现中的结构-性质关联到跨多种成像模式的组合优化。", "summary": "本文提出了一种结合自动化压电力显微镜（PFM）的多目标核学习工作流程，以克服手动探索铁电极化翻转的挑战。该方法有效地从高分辨率图像中推断微结构规则，识别畴壁配置与翻转动力学之间的关系。该方法促进了高通量主动学习，并提供了对微结构如何控制翻转的机制性见解，为复杂设计空间（不限于铁电材料）提供了一个可推广的工具。", "keywords": "铁电极化翻转, 多目标核学习, 自动化压电力显微镜, 微结构控制, 主动学习", "comments": "该论文将多目标核学习创新性地应用于自动化压电力显微镜，解决了材料科学中的一个重要挑战。直接从成像数据推断微结构规则并将其与翻转行为联系起来的能力具有重要价值。其对其他复杂、不可微分设计空间的普适性突显了其在铁电领域之外的潜在影响，为各个领域的主动学习和机制发现提供了强大的工具。"}}
{"id": "2506.08415", "title": "Improved Scaling Laws in Linear Regression via Data Reuse", "authors": ["Licong Lin", "Jingfeng Wu", "Peter L. Bartlett"], "summary": "Neural scaling laws suggest that the test error of large language models\ntrained online decreases polynomially as the model size and data size increase.\nHowever, such scaling can be unsustainable when running out of new data. In\nthis work, we show that data reuse can improve existing scaling laws in linear\nregression. Specifically, we derive sharp test error bounds on $M$-dimensional\nlinear models trained by multi-pass stochastic gradient descent (multi-pass\nSGD) on $N$ data with sketched features. Assuming that the data covariance has\na power-law spectrum of degree $a$, and that the true parameter follows a prior\nwith an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show\nthat multi-pass SGD achieves a test error of $\\Theta(M^{1-b} + L^{(1-b)/a})$,\nwhere $L \\lesssim N^{a/b}$ is the number of iterations. In the same setting,\none-pass SGD only attains a test error of $\\Theta(M^{1-b} + N^{(1-b)/a})$ (see\ne.g., Lin et al., 2024). This suggests an improved scaling law via data reuse\n(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are\nalso provided to verify our theoretical findings.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08415v1", "AI": {"title_translation": "通过数据重用改进线性回归中的缩放定律", "tldr": "当新数据有限时，数据重用可以改善线性回归中的缩放定律。", "motivation": "神经缩放定律表明测试误差随模型和数据大小增加而多项式下降，但新数据耗尽时这种缩放不可持续。本文旨在通过数据重用解决数据稀缺性问题，以改进现有缩放定律。", "method": "本文通过多遍随机梯度下降（multi-pass SGD）训练M维线性模型。假设数据协方差和真实参数都遵循幂律谱，推导了精确的测试误差界限，并与单遍SGD进行了比较。数值模拟用于验证理论结果。", "result": "多遍SGD实现了更低的测试误差 $\\Theta(M^{1-b} + L^{(1-b)/a})$，其中 $L \\lesssim N^{a/b}$ 是迭代次数。相比之下，单遍SGD的测试误差为 $\\Theta(M^{1-b} + N^{(1-b)/a})$。这表明在数据受限的情况下，通过数据重用（即选择 $L>N$）可以改进缩放定律。数值模拟验证了理论发现。", "conclusion": "数据重用（通过多遍SGD实现）可以显著改善线性回归中的缩放定律，尤其是在新数据受限的情况下，通过实现比单遍SGD更低的测试误差界限。", "translation": "神经缩放定律表明，在线训练的大型语言模型的测试误差会随着模型大小和数据大小的增加而多项式下降。然而，当新数据耗尽时，这种缩放可能变得不可持续。在这项工作中，我们展示了数据重用可以改进线性回归中现有的缩放定律。具体来说，我们推导了在具有草图特征的 $N$ 个数据上通过多遍随机梯度下降（multi-pass SGD）训练的 $M$ 维线性模型的精确测试误差界限。假设数据协方差具有度为 $a$ 的幂律谱，并且真实参数遵循具有对齐的度为 $b-a$ 的幂律谱的先验（其中 $a > b > 1$），我们表明多遍 SGD 实现了 $\\Theta(M^{1-b} + L^{(1-b)/a})$ 的测试误差，其中 $L \\lesssim N^{a/b}$ 是迭代次数。在相同设置下，单遍 SGD 只能达到 $\\Theta(M^{1-b} + N^{(1-b)/a})$ 的测试误差（参见例如 Lin et al., 2024）。这表明在数据受限的情况下，通过数据重用（即选择 $L>N$）可以改进缩放定律。本文还提供了数值模拟来验证我们的理论发现。", "summary": "本文研究了数据重用如何改善线性回归中的缩放定律，特别是在新数据稀缺时。通过在具有特定数据假设的线性模型上使用多遍随机梯度下降，作者推导出了一个新的测试误差界限，该界限优于单遍SGD，表明数据重用在数据受限场景中能带来改进的缩放。数值模拟支持了这些理论发现。", "keywords": "数据重用, 缩放定律, 线性回归, 多遍SGD, 测试误差", "comments": "本文通过理论推导，证明了数据重用这种实用策略能够克服数据稀缺性在实现更好缩放定律方面的限制，这对于在获取新数据成本高昂或不可用时训练大型模型具有重要意义。精确误差界限的推导为研究提供了坚实的理论基础。"}}
{"id": "2506.08729", "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces", "authors": ["Dieuwertje Alblas", "Patryk Rygiel", "Julian Suk", "Kaj O. Kappe", "Marieke Hofman", "Christoph Brune", "Kak Khee Yeung", "Jelmer M. Wolterink"], "summary": "Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the\nabdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current\nclinical guidelines recommend elective surgical repair when the maximum AAA\ndiameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet\nthese criteria are periodically monitored, with surveillance intervals based on\nthe maximum AAA diameter. However, this diameter does not take into account the\ncomplex relation between the 3D AAA shape and its growth, making standardized\nintervals potentially unfit. Personalized AAA growth predictions could improve\nmonitoring strategies. We propose to use an SE(3)-symmetric transformer model\nto predict AAA growth directly on the vascular model surface enriched with\nlocal, multi-physical features. In contrast to other works which have\nparameterized the AAA shape, this representation preserves the vascular\nsurface's anatomical structure and geometric fidelity. We train our model using\na longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24\nAAA patients at irregularly sampled intervals. After training, our model\npredicts AAA growth to the next scan moment with a median diameter error of\n1.18 mm. We further demonstrate our model's utility to identify whether a\npatient will become eligible for elective repair within two years (acc = 0.93).\nFinally, we evaluate our model's generalization on an external validation set\nconsisting of 25 CTAs from 7 AAA patients from a different hospital. Our\nresults show that local directional AAA growth prediction from the vascular\nsurface is feasible and may contribute to personalized surveillance strategies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08729v1", "AI": {"title_translation": "腹主动脉瘤表面局部生长预测的几何深度学习", "tldr": "使用SE(3)对称Transformer模型，结合几何深度学习，直接在腹主动脉瘤表面预测其局部生长，以改进个性化监测策略。", "motivation": "腹主动脉瘤（AAA）目前的临床指南基于最大直径进行监测，但这种方法未考虑AAA复杂3D形状与生长的关系，导致监测间隔可能不适合。个性化AAA生长预测能够显著改进监测策略。", "method": "本研究提出一个SE(3)对称Transformer模型，用于直接在血管模型表面预测AAA生长，该表面富含局部多物理特征，并保留了血管表面的解剖结构和几何保真度。模型在一个包含24名AAA患者的113次计算断层扫描血管造影（CTA）的纵向数据集上进行训练，这些扫描是在不规则间隔采样的。", "result": "模型能够预测下一次扫描时刻的AAA生长，中位直径误差为1.18毫米。此外，模型还能以0.93的准确率识别患者是否将在两年内符合择期修复条件。模型还在由来自不同医院的7名AAA患者的25次CTA组成的外部验证集上展示了其泛化能力。", "conclusion": "从血管表面进行局部定向AAA生长预测是可行的，并且可能有助于实现个性化的监测策略。", "translation": "腹主动脉瘤（AAA）是腹主动脉进行性局灶性扩张。AAA可能破裂，存活率仅为20%。目前的临床指南建议当男性最大AAA直径超过55毫米或女性超过50毫米时进行择期手术修复。不符合这些标准的患者会定期监测，监测间隔基于最大AAA直径。然而，该直径并未考虑3D AAA形状与其生长之间的复杂关系，使得标准化间隔可能不适用。个性化AAA生长预测可以改善监测策略。我们建议使用SE(3)对称Transformer模型，直接在富含局部多物理特征的血管模型表面预测AAA生长。与其他对AAA形状进行参数化的工作不同，这种表示保留了血管表面的解剖结构和几何保真度。我们使用包含24名AAA患者在不规则采样间隔下进行的113次计算机断层血管造影（CTA）扫描的纵向数据集训练我们的模型。训练后，我们的模型预测下一次扫描时的AAA生长，中位直径误差为1.18毫米。我们进一步展示了我们的模型识别患者是否将在两年内符合择期修复条件的效用（准确率=0.93）。最后，我们在一个外部验证集上评估了我们的模型泛化能力，该验证集由来自不同医院的7名AAA患者的25次CTA组成。我们的结果表明，从血管表面进行局部定向AAA生长预测是可行的，并且可能有助于个性化监测策略。", "summary": "该研究提出一种基于几何深度学习的SE(3)对称Transformer模型，用于在腹主动脉瘤（AAA）表面直接预测其局部生长。针对当前基于最大直径的监测方法未能充分考虑AAA复杂3D形状与生长的局限性，该模型利用富含局部多物理特征的血管表面表示，以保持解剖结构和几何保真度。模型在包含24名患者113次CTA扫描的纵向数据集上训练，实现了中位直径预测误差1.18毫米，并能以0.93的准确率识别患者是否在两年内需择期修复。外部验证进一步证实了其泛化能力，表明该方法为个性化AAA监测策略提供了可行途径。", "keywords": "腹主动脉瘤, 几何深度学习, 生长预测, Transformer, 个性化监测", "comments": "本文的创新点在于将几何深度学习（特别是SE(3)对称Transformer）应用于腹主动脉瘤的局部生长预测，直接在3D血管表面进行分析，克服了传统方法对AAA形状参数化可能导致的几何信息损失。这对于实现更精准的个性化监测和早期干预具有重要意义，有望改进临床实践。"}}
{"id": "2506.08417", "title": "Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood", "authors": ["Qingmao Yao", "Zhichao Lei", "Tianyuan Chen", "Ziyue Yuan", "Xuefan Chen", "Jianxiang Liu", "Faguo Wu", "Xiao Zhang"], "summary": "Offline Reinforcement Learning (RL) struggles with distributional shifts,\nleading to the $Q$-value overestimation for out-of-distribution (OOD) actions.\nExisting methods address this issue by imposing constraints; however, they\noften become overly conservative when evaluating OOD regions, which constrains\nthe $Q$-function generalization. This over-constraint issue results in poor\n$Q$-value estimation and hinders policy improvement. In this paper, we\nintroduce a novel approach to achieve better $Q$-value estimation by enhancing\n$Q$-function generalization in OOD regions within Convex Hull and its\nNeighborhood (CHN). Under the safety generalization guarantees of the CHN, we\npropose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by\nsmoothing them with neighboring in-sample $Q$-values. We theoretically show\nthat SBO approximates true $Q$-values for both in-sample and OOD actions within\nthe CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),\nempirically alleviates the over-constraint issue, achieving near-accurate\n$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing\nstate-of-the-art methods in both performance and computational efficiency.", "comment": "ICLR 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08417v1", "AI": {"title_translation": "具有凸包及其邻域平滑OOD泛化的离线强化学习", "tldr": "本文提出了一种名为SQOG的新方法，通过在凸包及其邻域（CHN）中引入平滑Bellman算子（SBO）来解决离线强化学习中Q值过高估计和泛化不足的问题，并在D4RL基准测试中表现出色。", "motivation": "离线强化学习（RL）面临分布偏移问题，导致对样本外（OOD）动作的Q值高估。现有方法通过施加约束来解决，但往往在评估OOD区域时过于保守，限制了Q函数泛化，从而导致Q值估计不佳并阻碍策略改进。", "method": "本文提出了一种新方法，通过在凸包及其邻域（CHN）中增强Q函数泛化来改善Q值估计。在CHN的安全泛化保证下，提出了一种平滑Bellman算子（SBO），通过用相邻的样本内Q值平滑OOD Q值来更新它们。实际算法是平滑Q函数OOD泛化（SQOG）。", "result": "理论上，SBO在CHN内近似样本内和OOD动作的真实Q值。实践中，SQOG算法有效缓解了过度约束问题，实现了接近准确的Q值估计。在D4RL基准测试中，SQOG在性能和计算效率上均优于现有最先进的方法。", "conclusion": "该研究通过引入Smooth Bellman Operator (SBO)和Smooth Q-function OOD Generalization (SQOG)算法，成功解决了离线强化学习中OOD动作的Q值过高估计和Q函数泛化不足的问题，在理论和实践中均显示出优越性。", "translation": "离线强化学习（RL）在处理分布偏移时遇到困难，导致对样本外（OOD）动作的Q值高估。现有方法通过施加约束来解决这个问题；然而，它们在评估OOD区域时往往变得过于保守，这限制了Q函数的泛化。这种过度约束问题导致Q值估计不佳并阻碍策略改进。在本文中，我们引入了一种新颖的方法，通过增强在凸包及其邻域（CHN）内的OOD区域中的Q函数泛化来获得更好的Q值估计。在CHN的安全泛化保证下，我们提出了平滑Bellman算子（SBO），它通过用相邻的样本内Q值平滑OOD Q值来更新它们。我们从理论上证明，SBO近似于CHN内样本内和OOD动作的真实Q值。我们的实用算法，平滑Q函数OOD泛化（SQOG），经验性地缓解了过度约束问题，实现了接近准确的Q值估计。在D4RL基准测试中，SQOG在性能和计算效率上均优于现有最先进的方法。", "summary": "本文提出了一种名为SQOG的新型离线强化学习方法，旨在解决现有方法在处理OOD动作时Q值过高估计和泛化不足的问题。该方法在凸包及其邻域（CHN）内通过引入平滑Bellman算子（SBO）来增强Q函数的泛化能力，SBO通过平滑OOD Q值与相邻的样本内Q值来更新它们。理论分析表明SBO能近似真实Q值，实验结果表明SQOG在D4RL基准测试上显著优于现有SOTA方法，表现出更高的性能和计算效率。", "keywords": "离线强化学习, Q值估计, OOD泛化, 平滑Bellman算子, 凸包及其邻域", "comments": "该论文通过引入“平滑Bellman算子”和“凸包及其邻域”的概念，为离线RL中的Q值过高估计和OOD泛化问题提供了一个新颖且有效的解决方案。其创新点在于通过平滑机制，在保证安全性的前提下，提升了Q函数在OOD区域的泛化能力，避免了传统方法过于保守的缺点。理论保证和经验性表现都证明了其重要性。"}}
{"id": "2506.08735", "title": "InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba", "authors": ["Yuhang Wang", "Jun Li", "Zhijian Wu", "Jianhua Xu"], "summary": "Within the family of convolutional neural networks, InceptionNeXt has shown\nexcellent competitiveness in image classification and a number of downstream\ntasks. Built on parallel one-dimensional strip convolutions, however, it\nsuffers from limited ability of capturing spatial dependencies along different\ndimensions and fails to fully explore spatial modeling in local neighborhood.\nBesides, inherent locality constraints of convolution operations are\ndetrimental to effective global context modeling. To overcome these\nlimitations, we propose a novel backbone architecture termed InceptionMamba in\nthis study. More specifically, the traditional one-dimensional strip\nconvolutions are replaced by orthogonal band convolutions in our InceptionMamba\nto achieve cohesive spatial modeling. Furthermore, global contextual modeling\ncan be achieved via a bottleneck Mamba module, facilitating enhanced\ncross-channel information fusion and enlarged receptive field. Extensive\nevaluations on classification and various downstream tasks demonstrate that the\nproposed InceptionMamba achieves state-of-the-art performance with superior\nparameter and computational efficiency. The source code will be available at\nhttps://github.com/Wake1021/InceptionMamba.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08735v1", "AI": {"title_translation": "InceptionMamba: 一种具有大带卷积和瓶颈Mamba的高效混合网络", "tldr": "InceptionMamba是一种新型混合网络，结合了带卷积和Mamba模块，旨在改善空间和全局上下文建模，并在各种任务上以高效率实现了SOTA性能。", "motivation": "InceptionNeXt虽然在图像分类和下游任务中表现出色，但其基于并行一维条形卷积的架构在捕获不同维度空间依赖性方面能力有限，未能充分探索局部邻域的空间建模，且卷积操作固有的局部性限制不利于有效的全局上下文建模。", "method": "InceptionMamba用正交带卷积取代了传统的一维条形卷积，以实现内聚空间建模。此外，通过瓶颈Mamba模块实现了全局上下文建模，促进了增强的跨通道信息融合和增大的感受野。", "result": "在分类和各种下游任务上的广泛评估表明，所提出的InceptionMamba以卓越的参数和计算效率实现了最先进的性能。", "conclusion": "InceptionMamba通过引入正交带卷积和瓶颈Mamba模块，有效克服了传统卷积神经网络在空间依赖性和全局上下文建模方面的局限性，从而在性能和效率上都达到了SOTA水平。", "translation": "在卷积神经网络家族中，InceptionNeXt在图像分类和许多下游任务中表现出卓越的竞争力。然而，它基于并行一维条形卷积，在捕获不同维度空间依赖性方面能力有限，并且未能充分探索局部邻域的空间建模。此外，卷积操作固有的局部性限制不利于有效的全局上下文建模。为了克服这些局限性，本研究提出了一种新颖的骨干网络架构，命名为InceptionMamba。更具体地说，在我们的InceptionMamba中，传统的​​一维条形卷积被正交带卷积取代，以实现内聚空间建模。此外，通过瓶颈Mamba模块可以实现全局上下文建模，从而促进增强的跨通道信息融合和增大的感受野。在分类和各种下游任务上的广泛评估表明，所提出的InceptionMamba以卓越的参数和计算效率实现了最先进的性能。源代码将发布在https://github.com/Wake1021/InceptionMamba。", "summary": "InceptionMamba是一种新颖的混合网络，旨在克服InceptionNeXt的局限性。它通过引入正交带卷积来增强空间建模，并结合瓶颈Mamba模块以实现有效的全局上下文捕获和跨通道融合。该架构在各种视觉任务中展示了最先进的性能和卓越的效率。", "keywords": "InceptionMamba, 混合网络, 带卷积, 瓶颈Mamba, 图像分类", "comments": "该论文提出了一种创新的混合架构，将类CNN的带卷积与Mamba模块相结合，解决了传统卷积在捕获空间依赖性和全局上下文方面的已知局限性。这种混合方法似乎是实现高效高性能视觉骨干网络的一个有前途的方向。"}}
{"id": "2506.08419", "title": "Online Learning-guided Learning Rate Adaptation via Gradient Alignment", "authors": ["Ruichen Jiang", "Ali Kavis", "Aryan Mokhtari"], "summary": "The performance of an optimizer on large-scale deep learning models depends\ncritically on fine-tuning the learning rate, often requiring an extensive grid\nsearch over base learning rates, schedules, and other hyperparameters. In this\npaper, we propose a principled framework called GALA (Gradient Alignment-based\nLearning rate Adaptation), which dynamically adjusts the learning rate by\ntracking the alignment between consecutive gradients and using a local\ncurvature estimate. Guided by the convergence analysis, we formulate the\nproblem of selecting the learning rate as a one-dimensional online learning\nproblem. When paired with an online learning algorithm such as\nFollow-the-Regularized-Leader, our method produces a flexible, adaptive\nlearning rate schedule that tends to increase when consecutive gradients are\naligned and decrease otherwise. We establish a data-adaptive convergence rate\nfor normalized SGD equipped with GALA in the smooth, nonconvex setting.\nEmpirically, common optimizers such as SGD and Adam, when augmented with GALA,\ndemonstrate robust performance across a wide range of initial learning rates\nand perform competitively without the need for tuning.", "comment": "24 pages, 5 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08419v1", "AI": {"title_translation": "基于梯度对齐的在线学习引导学习率自适应", "tldr": "GALA是一种在线学习引导的学习率自适应框架，通过跟踪梯度对齐和局部曲率来动态调整学习率，显著减少了深度学习优化器的学习率调优需求，并提高了鲁棒性。", "motivation": "深度学习模型优化器的性能严重依赖于学习率的精细调整，这通常需要对基础学习率、调度和其他超参数进行大量的网格搜索。", "method": "本文提出了GALA（Gradient Alignment-based Learning rate Adaptation）框架，通过跟踪连续梯度之间的对齐和使用局部曲率估计来动态调整学习率。该问题被公式化为一个一维在线学习问题，并与Follow-the-Regularized-Leader等在线学习算法结合使用。GALA在梯度对齐时增加学习率，否则减少学习率。", "result": "GALA能够生成灵活的自适应学习率调度。在光滑非凸环境下，为配备GALA的归一化SGD建立了数据自适应收敛率。实验表明，增强了GALA的SGD和Adam等常用优化器在各种初始学习率下表现出鲁棒性能，并且无需调优即可具有竞争力。", "conclusion": "GALA框架通过在线学习引导的学习率自适应，显著提高了深度学习优化器的性能和鲁棒性，减少了对学习率进行广泛调优的需求。", "translation": "大规模深度学习模型优化器的性能关键取决于学习率的精细调整，这通常需要对基础学习率、调度和其他超参数进行广泛的网格搜索。在本文中，我们提出了一个名为GALA（Gradient Alignment-based Learning rate Adaptation）的原则性框架，它通过跟踪连续梯度之间的对齐并使用局部曲率估计来动态调整学习率。在收敛性分析的指导下，我们将学习率选择问题公式化为一个一维在线学习问题。当与Follow-the-Regularized-Leader等在线学习算法结合使用时，我们的方法会产生一个灵活的自适应学习率调度，当连续梯度对齐时倾向于增加，否则减少。我们为配备GALA的归一化SGD在光滑、非凸环境下建立了数据自适应收敛率。经验上，当SGD和Adam等常用优化器与GALA结合时，在各种初始学习率下都表现出鲁棒性能，并且无需调优即可具有竞争力。", "summary": "本文提出了一种名为GALA（Gradient Alignment-based Learning rate Adaptation）的新型框架，旨在解决深度学习中学习率调优的挑战。GALA通过跟踪连续梯度对齐和局部曲率来动态调整学习率，并将此问题建模为一维在线学习。结合在线学习算法，GALA能自适应地调整学习率，在梯度对齐时增加，否则减少。研究证明了其在非凸设置下的收敛性，并经验性地展示了其在广泛初始学习率下的鲁棒性和竞争力，显著减少了对学习率进行手动调优的需求。", "keywords": "学习率自适应, 梯度对齐, 在线学习, 深度学习优化, GALA", "comments": "GALA的创新之处在于将学习率调整问题转化为在线学习问题，并利用梯度对齐这一直观的信号来动态调整学习率。这大大减轻了深度学习模型训练中超参数调优的负担，提高了优化器的鲁棒性和易用性。其理论收敛性证明和经验性能提升都表明了该方法的有效性和实用价值。"}}
{"id": "2506.08772", "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation", "authors": ["Jiayi Song", "Kaiyu Li", "Xiangyong Cao", "Deyu Meng"], "summary": "Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nWe propose that Vision Foundation Models (VFMs), pre-trained on vast and\ndiverse datasets, possess robust generalization capabilities that can\neffectively bridge this distribution gap and provide strong semantic priors for\nSSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and\nFusion), a novel framework that leverages the powerful semantic knowledge\nembedded in VFMs to guide semi-supervised learning in remote sensing.\nSpecifically, RS-MTDF employs multiple frozen VFMs (\\textit{e.g.}, DINOv2 and\nCLIP) as expert teachers, utilizing feature-level distillation to align student\nfeatures with their robust representations. To further enhance discriminative\npower, the distilled knowledge is seamlessly fused into the student decoder.\nExtensive experiments on three challenging remote sensing datasets (ISPRS\nPotsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves\nstate-of-the-art performance. Notably, our method outperforms existing\napproaches across various label ratios on LoveDA and secures the highest IoU in\nthe majority of semantic categories. These results underscore the efficacy of\nmulti-teacher VFM guidance in significantly enhancing both generalization and\nsemantic understanding for remote sensing segmentation. Ablation studies\nfurther validate the contribution of each proposed module.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08772v1", "AI": {"title_translation": "RS-MTDF：用于遥感半监督语义分割的多教师蒸馏与融合", "tldr": "RS-MTDF是一种新的框架，它利用多个预训练的视觉基础模型（VFM）作为专家教师，通过特征级蒸馏和知识融合来指导遥感图像的半监督语义分割，并在多个数据集上实现了最先进的性能。", "motivation": "遥感图像中的语义分割依赖于昂贵且耗时的像素级标注。现有的半监督语义分割（SSS）方法难以解决有限标注数据和大量未标注数据之间的分布不匹配问题，导致泛化能力不佳。", "method": "本文提出了RS-MTDF（Multi-Teacher Distillation and Fusion）框架。该方法利用多个冻结的视觉基础模型（如DINOv2和CLIP）作为专家教师，通过特征级蒸馏将学生模型的特征与教师的鲁棒表示对齐。此外，蒸馏的知识被无缝融合到学生解码器中，以增强判别能力。", "result": "在ISPRS Potsdam、LoveDA和DeepGlobe三个遥感数据集上的大量实验表明，RS-MTDF始终取得了最先进的性能。值得注意的是，该方法在LoveDA数据集的各种标签比例下均优于现有方法，并在大多数语义类别中获得了最高的IoU。", "conclusion": "RS-MTDF通过多教师VFM指导显著增强了遥感分割的泛化能力和语义理解能力，证明了其有效性。", "translation": "遥感图像中的语义分割对各种应用至关重要，但其性能严重依赖于大规模、高质量的像素级标注，而这些标注的获取是众所周知的昂贵且耗时的。半监督语义分割（SSS）提供了一种有前景的替代方案来缓解这种数据依赖。然而，现有的SSS方法常常难以解决有限标注数据和大量未标注数据之间固有的分布不匹配问题，导致次优的泛化能力。我们提出，在大量多样化数据集上预训练的视觉基础模型（VFMs）拥有强大的泛化能力，可以有效弥合这种分布差距，并为SSS提供强大的语义先验。受此启发，我们引入了RS-MTDF（多教师蒸馏与融合），这是一个新颖的框架，它利用VFMs中嵌入的强大语义知识来指导遥感领域的半监督学习。具体来说，RS-MTDF采用多个冻结的VFMs（例如DINOv2和CLIP）作为专家教师，利用特征级蒸馏来使学生特征与其鲁棒表示对齐。为了进一步增强判别能力，蒸馏的知识被无缝融合到学生解码器中。在三个具有挑战性的遥感数据集（ISPRS Potsdam、LoveDA和DeepGlobe）上进行的大量实验表明，RS-MTDF始终取得了最先进的性能。值得注意的是，我们的方法在LoveDA数据集的各种标签比例下均优于现有方法，并在大多数语义类别中获得了最高的IoU。这些结果强调了多教师VFM指导在显著增强遥感分割的泛化能力和语义理解方面的功效。消融研究进一步验证了每个所提模块的贡献。", "summary": "本文提出了一种名为RS-MTDF的新型框架，旨在解决遥感图像半监督语义分割中对大量标注数据的依赖和数据分布不匹配问题。RS-MTDF利用多个预训练的视觉基础模型（VFM）作为专家教师，通过特征级蒸馏和知识融合来指导学生模型，以利用VFM的强大语义知识。在多个挑战性遥感数据集上的实验表明，RS-MTDF在泛化能力和语义理解方面均取得了最先进的性能，验证了其在遥感半监督语义分割中的有效性。", "keywords": "遥感图像, 半监督语义分割, 知识蒸馏, 视觉基础模型, 多教师学习", "comments": "RS-MTDF的创新之处在于利用多个视觉基础模型（VFM）作为教师进行知识蒸馏，有效解决了半监督学习中数据分布不匹配的问题。这种多教师和特征级蒸馏的方法为遥感图像的半监督语义分割提供了一个强大的解决方案，显著降低了对昂贵像素级标注的依赖，具有重要的应用价值。"}}
{"id": "2506.08777", "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting", "authors": ["Keyi Liu", "Weidong Yang", "Ben Fei", "Ying He"], "summary": "Self-supervised learning (SSL) for point cloud pre-training has become a\ncornerstone for many 3D vision tasks, enabling effective learning from\nlarge-scale unannotated data. At the scene level, existing SSL methods often\nincorporate volume rendering into the pre-training framework, using RGB-D\nimages as reconstruction signals to facilitate cross-modal learning. This\nstrategy promotes alignment between 2D and 3D modalities and enables the model\nto benefit from rich visual cues in the RGB-D inputs. However, these approaches\nare limited by their reliance on implicit scene representations and high memory\ndemands. Furthermore, since their reconstruction objectives are applied only in\n2D space, they often fail to capture underlying 3D geometric structures. To\naddress these challenges, we propose Gaussian2Scene, a novel scene-level SSL\nframework that leverages the efficiency and explicit nature of 3D Gaussian\nSplatting (3DGS) for pre-training. The use of 3DGS not only alleviates the\ncomputational burden associated with volume rendering but also supports direct\n3D scene reconstruction, thereby enhancing the geometric understanding of the\nbackbone network. Our approach follows a progressive two-stage training\nstrategy. In the first stage, a dual-branch masked autoencoder learns both 2D\nand 3D scene representations. In the second stage, we initialize training with\nreconstructed point clouds and further supervise learning using the geometric\nlocations of Gaussian primitives and rendered RGB images. This process\nreinforces both geometric and cross-modal learning. We demonstrate the\neffectiveness of Gaussian2Scene across several downstream 3D object detection\ntasks, showing consistent improvements over existing pre-training methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08777v1", "AI": {"title_translation": "Gaussian2Scene：通过3D高斯泼溅自监督学习进行3D场景表示学习", "tldr": "Gaussian2Scene是一个新的场景级自监督学习框架，它利用3D高斯泼溅来解决现有方法的局限性，通过两阶段训练策略增强3D几何和跨模态学习，并在3D目标检测任务中显示出一致的改进。", "motivation": "现有的点云预训练自监督学习方法在场景级别上常依赖隐式场景表示和高内存需求，且其2D重建目标未能有效捕获底层的3D几何结构。", "method": "我们提出了Gaussian2Scene，一个新颖的场景级自监督学习框架，利用3D高斯泼溅（3DGS）进行预训练，以提高效率和显式性。该方法采用渐进式两阶段训练策略：第一阶段，双分支掩码自编码器学习2D和3D场景表示；第二阶段，使用重建的点云初始化训练，并利用高斯基元的几何位置和渲染的RGB图像进行进一步监督学习，以强化几何和跨模态学习。", "result": "Gaussian2Scene在多个下游3D目标检测任务中展现出有效性，相比现有预训练方法有持续改进。", "conclusion": "Gaussian2Scene通过利用3D高斯泼溅，有效解决了现有自监督学习方法在3D场景表示学习中的局限性，增强了模型的几何理解和跨模态学习能力，从而在3D目标检测等任务中取得了显著性能提升。", "translation": "点云预训练的自监督学习（SSL）已成为许多3D视觉任务的基石，能够从大规模未标注数据中进行有效学习。在场景级别，现有的SSL方法通常将体渲染纳入预训练框架，使用RGB-D图像作为重建信号以促进跨模态学习。这种策略促进了2D和3D模态之间的对齐，并使模型能够从RGB-D输入中丰富的视觉线索中受益。然而，这些方法受限于对隐式场景表示的依赖和高内存需求。此外，由于它们的重建目标仅应用于2D空间，它们往往未能捕获底层的3D几何结构。为了解决这些挑战，我们提出了Gaussian2Scene，一个新颖的场景级SSL框架，它利用3D高斯泼溅（3DGS）的效率和显式性进行预训练。使用3DGS不仅减轻了与体渲染相关的计算负担，还支持直接的3D场景重建，从而增强了骨干网络的几何理解。我们的方法遵循渐进式两阶段训练策略。在第一阶段，一个双分支掩码自编码器学习2D和3D场景表示。在第二阶段，我们使用重建的点云初始化训练，并进一步利用高斯基元的几何位置和渲染的RGB图像进行监督学习。这个过程强化了几何和跨模态学习。我们展示了Gaussian2Scene在几个下游3D目标检测任务中的有效性，显示出比现有预训练方法一致的改进。", "summary": "本文提出了Gaussian2Scene，一个针对3D场景表示学习的新型自监督学习框架。该框架旨在解决现有方法在隐式表示、高内存消耗以及对3D几何结构捕获不足的问题。Gaussian2Scene利用3D高斯泼溅（3DGS）进行预训练，以提供更高效、显式的3D场景重建。其采用两阶段训练策略：首先通过双分支掩码自编码器学习2D和3D表示，随后利用重建点云、高斯基元几何位置和渲染RGB图像进行进一步监督，以强化几何和跨模态学习。实验证明，Gaussian2Scene在3D目标检测任务中优于现有预训练方法。", "keywords": "3D场景表示, 自监督学习, 3D高斯泼溅, 点云预训练, 3D目标检测", "comments": "该论文的创新点在于将3D高斯泼溅引入到自监督学习框架中，解决了传统体渲染方法在效率和3D几何捕获上的不足。通过显式的3DGS表示和两阶段训练策略，模型能够更好地理解3D几何并促进跨模态学习，这对于提升3D视觉任务的性能具有重要意义。"}}
{"id": "2506.08780", "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models", "authors": ["Isaac Corley", "Lakshay Sharma", "Ruth Crasto"], "summary": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08780v1", "AI": {"title_translation": "Landsat-Bench：Landsat基础模型的数据集和基准", "tldr": "引入Landsat-Bench，一套用于Landsat基础模型的基准数据集，并证明SSL4EO-L预训练模型在下游任务中表现优于ImageNet预训练模型。", "motivation": "现有Landsat数据缺乏基准，这限制了基于Landsat的地理空间基础模型（GFM）的进展。", "method": "本文引入了Landsat-Bench，一套包含Landsat影像的三个基准数据集——EuroSAT-L、BigEarthNet-L和LC100-L，这些基准改编自现有遥感数据集。研究建立了通用架构和在SSL4EO-L数据集上预训练的Landsat基础模型的基线和标准化评估方法。", "result": "SSL4EO-L预训练的GFM比ImageNet预训练模型能为下游任务提取更好的表示，在EuroSAT-L和BigEarthNet-L上分别获得了+4% OA和+5.1% mAP的性能提升。", "conclusion": "SSL4EO-L预训练对于Landsat基础模型在下游任务中提取更好的表示是有效的，并能带来显著的性能提升。", "translation": "Landsat计划提供了50多年来全球一致的地球影像。然而，缺乏针对这些数据的基准限制了基于Landsat的地理空间基础模型（GFM）的进展。在本文中，我们引入了Landsat-Bench，一套包含Landsat影像的三个基准，这些基准改编自现有的遥感数据集——EuroSAT-L、BigEarthNet-L和LC100-L。我们建立了通用架构和在SSL4EO-L数据集上预训练的Landsat基础模型的基线和标准化评估方法。值得注意的是，我们提供了证据表明，与ImageNet相比，SSL4EO-L预训练的GFM能为下游任务提取更好的表示，包括在EuroSAT-L和BigEarthNet-L上分别获得了+4% OA和+5.1% mAP的性能提升。", "summary": "本文介绍了Landsat-Bench，一套包含EuroSAT-L、BigEarthNet-L和LC100-L三个数据集的Landsat图像基准，旨在解决Landsat数据在地理空间基础模型开发中缺乏基准的问题。研究建立了基线和标准化评估方法，并证明了在SSL4EO-L数据集上预训练的Landsat基础模型在下游任务中比ImageNet模型能提取出更好的特征表示，性能显著提升。", "keywords": "Landsat, 地理空间基础模型, 基准, 预训练, 遥感", "comments": "这篇论文通过提供专门的基准数据集，解决了Landsat数据在地理空间基础模型开发中的关键瓶颈。它为Landsat基础模型的评估和比较提供了一个标准化的平台，并证明了特定领域预训练（如SSL4EO-L）的有效性，对于推动地球观测领域的基础模型发展具有重要意义。"}}
{"id": "2506.08147", "title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models", "authors": ["Muhammad Usman", "Muhammad Ahmad", "M. Shahiki Tash", "Irina Gelbukh", "Rolando Quintero Tellez", "Grigori Sidorov"], "summary": "Social media platforms are critical spaces for public discourse, shaping\nopinions and community dynamics, yet their widespread use has amplified harmful\ncontent, particularly hate speech, threatening online safety and inclusivity.\nWhile hate speech detection has been extensively studied in languages like\nEnglish and Spanish, Urdu remains underexplored, especially using\ntranslation-based approaches. To address this gap, we introduce a trilingual\ndataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and\nSpanish (3,162 samples), collected via keyword filtering, with a balanced\ndistribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology\nleverages attention layers as a precursor to transformer-based models and large\nlanguage models (LLMs), enhancing feature extraction for multilingual hate\nspeech detection. For non-transformer models, we use TF-IDF for feature\nextraction. The dataset is benchmarked using state-of-the-art models, including\nGPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models\nlike SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,\nfollowing rigorous guidelines, ensured high dataset quality, achieving a\nFleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5\nTurbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of\n0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for\nUrdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).\nThese results reflect improvements of 8.75% in English (over SVM baseline\n0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM\nbaseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline\n0.82). Our framework offers a robust solution for multilingual hate speech\ndetection, fostering safer digital communities worldwide.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08147v1", "AI": {"title_translation": "使用基于翻译的大型语言模型进行社交媒体多语言仇恨言论检测", "tldr": "本研究引入了一个新的三语（英语、乌尔都语、西班牙语）推文数据集，并利用基于注意力层的大型语言模型（如GPT-3.5 Turbo和Qwen 2.5 72B）进行多语言仇恨言论检测，取得了显著的性能提升。", "motivation": "社交媒体平台上的仇恨言论日益增多，威胁到在线安全和包容性。尽管英语和西班牙语的仇恨言论检测已被广泛研究，但乌尔都语在这方面，特别是使用基于翻译的方法，仍未得到充分探索。", "method": "研究引入了一个包含10,193条推文的三语数据集（英语、乌尔都语、西班牙语），通过关键词过滤收集并平衡了仇恨/非仇恨标签。方法利用注意力层作为Transformer模型和大型语言模型（LLMs）的前置，增强特征提取。对于非Transformer模型，使用TF-IDF进行特征提取。数据集使用GPT-3.5 Turbo、Qwen 2.5 72B、SVM、BERT和RoBERTa等模型进行基准测试。数据集质量由三名标注者确保，Fleiss' Kappa达到0.821。", "result": "该方法在英语（GPT-3.5 Turbo）上宏F1分数为0.87，西班牙语（GPT-3.5 Turbo）为0.85，乌尔都语（Qwen 2.5 72B）为0.81，以及联合多语言模型（Qwen 2.5 72B）为0.88。与SVM基线相比，英语、西班牙语、乌尔都语和联合多语言模型的性能分别提高了8.75%、8.97%、5.19%和7.32%。", "conclusion": "本研究提出的框架为多语言仇恨言论检测提供了一个强大的解决方案，有助于在全球范围内建立更安全的数字社区。", "translation": "社交媒体平台是公共讨论的关键空间，塑造着观点和社区动态，然而其广泛使用却加剧了有害内容，特别是仇恨言论，威胁到在线安全和包容性。尽管仇恨言论检测在英语和西班牙语等语言中得到了广泛研究，但乌尔都语仍然未被充分探索，尤其是在使用基于翻译的方法时。为了弥补这一空白，我们引入了一个包含10,193条推文的三语数据集，分别包含英语（3,834个样本）、乌尔都语（3,197个样本）和西班牙语（3,162个样本），通过关键词过滤收集，并平衡了4,849个仇恨标签和5,344个非仇恨标签。我们的方法利用注意力层作为Transformer模型和大型语言模型（LLMs）的前置，增强了多语言仇恨言论检测的特征提取能力。对于非Transformer模型，我们使用TF-IDF进行特征提取。该数据集使用最先进的模型进行基准测试，包括GPT-3.5 Turbo和Qwen 2.5 72B，以及传统的机器学习模型如SVM和其他Transformer模型（例如BERT、RoBERTa）。三名标注者遵循严格的指导方针，确保了高数据集质量，Fleiss' Kappa达到了0.821。我们整合注意力层与GPT-3.5 Turbo和Qwen 2.5 72B的方法取得了优异的性能，英语（GPT-3.5 Turbo）的宏F1分数为0.87，西班牙语（GPT-3.5 Turbo）为0.85，乌尔都语（Qwen 2.5 72B）为0.81，联合多语言模型（Qwen 2.5 72B）为0.88。这些结果反映出与SVM基线相比，英语（基线0.80）提高了8.75%，西班牙语（基线0.78）提高了8.97%，乌尔都语（基线0.77）提高了5.19%，联合多语言模型（基线0.82）提高了7.32%。我们的框架为多语言仇恨言论检测提供了一个强大的解决方案，有助于在全球范围内建立更安全的数字社区。", "summary": "本研究旨在解决社交媒体中多语言仇恨言论检测的挑战，特别是针对未充分探索的乌尔都语。为此，作者构建了一个包含英语、乌尔都语和西班牙语的三语推文数据集。研究提出了一种结合注意力层与大型语言模型（如GPT-3.5 Turbo和Qwen 2.5 72B）的方法，以增强特征提取和检测性能。实验结果表明，该方法在多语言环境下，特别是乌尔都语上，相比传统方法取得了显著的F1分数提升，为构建更安全的数字社区提供了有效的解决方案。", "keywords": "多语言仇恨言论检测, 大型语言模型, 社交媒体, 乌尔都语, 翻译方法", "comments": "该论文的创新点在于构建了一个新的三语仇恨言论数据集，并特别关注了资源稀缺的乌尔都语。其方法结合了注意力机制与前沿的大型语言模型，在多语言环境下展现了强大的性能。研究结果的提升百分比清晰地展示了所提方法的有效性。这项工作对于促进全球数字社区的安全性和包容性具有重要意义。"}}
{"id": "2506.08438", "title": "Learning to Lead: Incentivizing Strategic Agents in the Dark", "authors": ["Yuchen Wu", "Xinyi Zhong", "Zhuoran Yang"], "summary": "We study an online learning version of the generalized principal-agent model,\nwhere a principal interacts repeatedly with a strategic agent possessing\nprivate types, private rewards, and taking unobservable actions. The agent is\nnon-myopic, optimizing a discounted sum of future rewards and may strategically\nmisreport types to manipulate the principal's learning. The principal,\nobserving only her own realized rewards and the agent's reported types, aims to\nlearn an optimal coordination mechanism that minimizes strategic regret. We\ndevelop the first provably sample-efficient algorithm for this challenging\nsetting. Our approach features a novel pipeline that combines (i) a delaying\nmechanism to incentivize approximately myopic agent behavior, (ii) an\ninnovative reward angle estimation framework that uses sector tests and a\nmatching procedure to recover type-dependent reward functions, and (iii) a\npessimistic-optimistic LinUCB algorithm that enables the principal to explore\nefficiently while respecting the agent's incentive constraints. We establish a\nnear optimal $\\tilde{O}(\\sqrt{T}) $ regret bound for learning the principal's\noptimal policy, where $\\tilde{O}(\\cdot) $ omits logarithmic factors. Our\nresults open up new avenues for designing robust online learning algorithms for\na wide range of game-theoretic settings involving private types and strategic\nagents.", "comment": "81 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08438v1", "AI": {"title_translation": "学会领导：在黑暗中激励战略代理", "tldr": "本文研究了广义委托-代理模型的在线学习版本，其中委托人与具有私人类型、私人奖励和采取不可观测行动的战略代理人重复交互。代理人是非短视的，会策略性地误报类型以操纵委托人的学习。委托人旨在学习一个最优协调机制以最小化战略遗憾。本文开发了第一个可证明的样本高效算法，实现了接近最优的$\\tilde{O}(\\sqrt{T})$ 遗憾界限。", "motivation": "委托人与具有私人类型、私人奖励和采取不可观测行动的战略代理人重复交互。代理人是非短视的，会策略性地误报类型以操纵委托人的学习。委托人仅观察其自身实现的奖励和代理人报告的类型，旨在学习一个最优协调机制以最小化战略遗憾。", "method": "本文开发了一个新颖的流水线，结合了：(i) 一种延迟机制，以激励近似短视的代理人行为；(ii) 一个创新的奖励角度估计框架，该框架使用扇形测试和匹配程序来恢复依赖于类型的奖励函数；(iii) 一个悲观-乐观的LinUCB算法，使委托人能够在尊重代理人激励约束的同时有效探索。", "result": "实现了学习委托人最优策略的接近最优的$\\tilde{O}(\\sqrt{T})$ 遗憾界限，其中$\\tilde{O}(\\cdot)$ 省略了对数因子。", "conclusion": "我们的结果为在涉及私人类型和战略代理人的广泛博弈论设置中设计鲁棒的在线学习算法开辟了新途径。", "translation": "我们研究了广义委托-代理模型的在线学习版本，其中委托人与具有私人类型、私人奖励和采取不可观测行动的战略代理人重复交互。代理人是非短视的，优化未来奖励的折现和，并可能策略性地误报类型以操纵委托人的学习。委托人仅观察其自身实现的奖励和代理人报告的类型，旨在学习一个最优协调机制以最小化战略遗憾。我们开发了第一个可证明的样本高效算法，用于这种具有挑战性的设置。我们的方法采用了一个新颖的流水线，结合了 (i) 一种延迟机制，以激励近似短视的代理人行为，(ii) 一个创新的奖励角度估计框架，该框架使用扇形测试和匹配程序来恢复依赖于类型的奖励函数，以及 (iii) 一个悲观-乐观的LinUCB算法，使委托人能够在尊重代理人激励约束的同时有效探索。我们为学习委托人最优策略建立了接近最优的 $\\tilde{O}(\\sqrt{T}) $ 遗憾界限，其中 $\\tilde{O}(\\cdot) $ 省略了对数因子。我们的结果为在涉及私人类型和战略代理人的广泛博弈论设置中设计鲁棒的在线学习算法开辟了新途径。", "summary": "本文研究了广义委托-代理模型的在线学习问题，其中委托人与拥有私人信息并可能策略性操纵学习的非短视代理人进行交互。为了解决委托人学习最优协调机制以最小化战略遗憾的挑战，作者提出了一种新的样本高效算法。该算法结合了延迟机制、奖励角度估计框架和悲观-乐观的LinUCB算法。实验结果表明，该方法在学习委托人最优策略方面达到了接近最优的遗憾界限，为涉及私人类型和战略代理人的博弈论在线学习算法设计提供了新思路。", "keywords": "在线学习, 委托-代理模型, 战略代理, 激励机制, 遗憾界限", "comments": "本文的创新之处在于它是第一个为这种具有挑战性的设置（即委托人与具有私人类型和策略性行为的代理人进行在线交互）开发的可证明的样本高效算法。它通过结合三种新颖的技术：延迟机制、创新的奖励角度估计和悲观-乐观的LinUCB算法，有效地解决了代理人操纵学习和激励兼容性问题。这项工作对于设计更鲁棒的在线学习算法在各种博弈论场景中具有重要意义。"}}
{"id": "2506.08784", "title": "HomographyAD: Deep Anomaly Detection Using Self Homography Learning", "authors": ["Jongyub Seok", "Chanjin Kang"], "summary": "Anomaly detection (AD) is a task that distinguishes normal and abnormal data,\nwhich is important for applying automation technologies of the manufacturing\nfacilities. For MVTec dataset that is a representative AD dataset for\nindustrial environment, many recent works have shown remarkable performances.\nHowever, the existing anomaly detection works have a limitation of showing good\nperformance for fully-aligned datasets only, unlike real-world industrial\nenvironments. To solve this limitation, we propose HomographyAD, a novel deep\nanomaly detection methodology based on the ImageNet-pretrained network, which\nis specially designed for actual industrial dataset. Specifically, we first\nsuggest input foreground alignment using the deep homography estimation method.\nIn addition, we fine-tune the model by self homography learning to learn\nadditional shape information from normal samples. Finally, we conduct anomaly\ndetection based on the measure of how far the feature of test sample is from\nthe distribution of the extracted normal features. By applying our proposed\nmethod to various existing AD approaches, we show performance enhancement\nthrough extensive experiments.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08784v1", "AI": {"title_translation": "同态AD：使用自同态学习的深度异常检测", "tldr": "提出HomographyAD，一种基于自同态学习的深度异常检测方法，旨在解决现有方法在未对齐真实工业数据上的性能限制，并通过实验证明其能提升现有AD方法的性能。", "motivation": "现有异常检测方法在MVTec等对齐数据集上表现良好，但在真实工业环境中，数据并非完全对齐，导致现有方法性能受限。", "method": "提出HomographyAD，基于ImageNet预训练网络。具体步骤包括：1) 使用深度同态估计方法进行输入前景对齐；2) 通过自同态学习对模型进行微调，以从正常样本中学习额外的形状信息；3) 根据测试样本特征与提取的正常特征分布的距离进行异常检测。", "result": "将所提出的方法应用于各种现有AD方法，并通过大量实验证明了性能提升。", "conclusion": "HomographyAD通过解决数据未对齐的问题，有效提升了工业环境中异常检测的性能。", "translation": "异常检测（AD）是区分正常和异常数据的任务，这对于制造设施自动化技术的应用至关重要。对于MVTec数据集（一个代表性的工业环境AD数据集），许多近期工作已显示出卓越的性能。然而，现有的异常检测工作存在一个局限性，即仅能对完全对齐的数据集显示出良好性能，这与真实世界的工业环境不同。为了解决这一局限性，我们提出了HomographyAD，一种基于ImageNet预训练网络的新型深度异常检测方法，该方法专门为实际工业数据集设计。具体来说，我们首先建议使用深度同态估计方法进行输入前景对齐。此外，我们通过自同态学习对模型进行微调，以从正常样本中学习额外的形状信息。最后，我们根据测试样本特征与提取的正常特征分布的距离来执行异常检测。通过将我们提出的方法应用于各种现有AD方法，我们通过广泛的实验展示了性能提升。", "summary": "本文提出了一种名为HomographyAD的新型深度异常检测方法，旨在解决现有方法在处理真实世界中未对齐工业数据时的性能瓶颈。该方法利用ImageNet预训练网络，并通过深度同态估计实现输入前景对齐，同时通过自同态学习从正常样本中获取形状信息。最终，通过测量测试样本特征与正常特征分布的距离进行异常检测。实验结果表明，HomographyAD能够有效提升现有异常检测方法的性能。", "keywords": "异常检测, 深度学习, 同态估计, 自同态学习, 工业视觉", "comments": "HomographyAD的创新点在于引入深度同态估计和自同态学习来解决工业异常检测中数据未对齐的实际问题。这对于提升自动化技术在复杂工业环境中的应用具有重要意义。该方法通过预处理和特征学习的结合，增强了模型的鲁棒性，使其更适用于真实场景。"}}
{"id": "2506.08796", "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08796v1", "AI": {"title_translation": "流式多样且高效：通过随机速度场采样学习动量流匹配", "tldr": "本文提出了一种名为Discretized-RF的新型整流流模型（也称为动量流模型），它通过离散化直线路径并引入速度噪声来解决现有整流流模型在多样性和多尺度噪声建模方面的局限性，实现了多样且高效的轨迹生成，并能持续生成高质量和多样化的结果。", "motivation": "尽管整流流（RF）模型在直线路径采样方面具有高效率优势，尤其是在生成高质量图像方面表现出色（如Flux 1.0和SD 3.0），但其直线连接方式导致了两个主要问题：1) 多样性问题，因为直线路径覆盖的采样空间受限；2) 多尺度噪声建模问题，因为直线流仅需优化常数速度场。", "method": "本文提出了Discretized-RF，这是一种新型的整流流家族（也称为动量流模型），它通过将直线路径离散化为一系列可变速度场子路径（即“动量场”）来扩展搜索空间，尤其是在接近噪声分布时。与以往直接在数据上叠加噪声不同，Discretized-RF在子路径的速度上引入噪声以改变其方向，从而提高多样性和多尺度噪声建模能力。", "result": "在多个代表性数据集上的实验结果表明，通过采样随机速度场学习动量流匹配将产生既多样又高效的轨迹，并且能够持续生成高质量和多样化的结果。", "conclusion": "通过引入Discretized-RF，将直线路径离散化并对速度引入噪声，可以显著提高整流流模型的采样多样性和多尺度噪声建模能力，从而生成更优质和多样化的图像。", "translation": "最近，整流流（RF）因其在直线路径采样方面的高效率优势，成为基于流的扩散模型中的新SOTA，尤其是一系列RF模型（如Flux 1.0和SD 3.0）生成的惊人图像。尽管噪声数据分布和自然数据分布之间的直线连接是直观、快速且易于优化的，但它仍然不可避免地导致：1) 多样性问题，因为直线路径仅覆盖相当受限的采样空间。2) 多尺度噪声建模问题，因为直线流仅需优化两个分布$\\bm\\pi_0$和$\\bm\\pi_1$之间的常数速度场$\\bm v$。在这项工作中，我们提出了Discretized-RF，这是一种新型的整流流家族（也称为动量流模型，因为它们在每个扩散步骤中都参考了先前的速度分量和随机速度分量），它将直线路径离散化为一系列可变速度场子路径（即“动量场”）以扩展搜索空间，尤其是在接近噪声分布$p_\\text{noise}$时。与以往直接在$\\bm x$上叠加噪声不同，我们我们在子路径的速度$\\bm v$上引入噪声以改变其方向，从而提高多样性和多尺度噪声建模能力。在几个代表性数据集上的实验结果表明，通过采样随机速度场学习动量流匹配将产生既多样又高效的轨迹，并且能够持续生成高质量和多样化的结果。代码可在https://github.com/liuruixun/momentum-fm获得。", "summary": "本文提出Discretized-RF，一种新的整流流模型，通过将直线路径离散为可变速度场子路径（动量场）并对速度引入噪声，解决了传统整流流在多样性和多尺度噪声建模上的局限性。实验证明，该方法能生成多样且高效的轨迹，并持续产生高质量、多样化的结果。", "keywords": "整流流, 动量流, 扩散模型, 随机速度场, 多样性", "comments": "这篇论文通过引入“动量流”的概念，巧妙地解决了现有整流流模型在生成多样性和多尺度噪声建模方面的核心局限性。通过对速度场进行离散化和引入噪声，扩展了模型的搜索空间，这是一个重要的创新点，有望推动基于流的生成模型进一步发展。"}}
{"id": "2506.08797", "title": "HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation", "authors": ["Ziyao Huang", "Zixiang Zhou", "Juan Cao", "Yifeng Ma", "Yi Chen", "Zejing Rao", "Zhiyong Xu", "Hongmei Wang", "Qin Lin", "Yuan Zhou", "Qinglin Lu", "Fan Tang"], "summary": "To address key limitations in human-object interaction (HOI) video generation\n-- specifically the reliance on curated motion data, limited generalization to\nnovel objects/scenarios, and restricted accessibility -- we introduce\nHunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.\nHunyuanVideo-HOMA enhances controllability and reduces dependency on precise\ninputs through sparse, decoupled motion guidance. It encodes appearance and\nmotion signals into the dual input space of a multimodal diffusion transformer\n(MMDiT), fusing them within a shared context space to synthesize temporally\nconsistent and physically plausible interactions. To optimize training, we\nintegrate a parameter-space HOI adapter initialized from pretrained MMDiT\nweights, preserving prior knowledge while enabling efficient adaptation, and a\nfacial cross-attention adapter for anatomically accurate audio-driven lip\nsynchronization. Extensive experiments confirm state-of-the-art performance in\ninteraction naturalness and generalization under weak supervision. Finally,\nHunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and\ninteractive object manipulation, supported by a user-friendly demo interface.\nThe project page is at https://anonymous.4open.science/w/homa-page-0FBE/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08797v1", "AI": {"title_translation": "HunyuanVideo-HOMA：多模态驱动人体动画中的通用人-物交互", "tldr": "HunyuanVideo-HOMA是一个弱条件多模态驱动框架，用于生成逼真且可泛化的人-物交互视频，解决了现有方法对精确输入和特定数据的依赖。", "motivation": "为了解决人-物交互（HOI）视频生成中对精心策划的动作数据的依赖、对新颖物体/场景的泛化能力有限以及可访问性受限等关键限制。", "method": "本文引入了HunyuanVideo-HOMA，一个弱条件多模态驱动框架。它通过稀疏、解耦的动作引导增强可控性，并将外观和动作信号编码到多模态扩散Transformer（MMDiT）的双输入空间中，在共享上下文空间中融合以合成时间一致且物理合理的人-物交互。为优化训练，集成了从预训练MMDiT权重初始化的参数空间HOI适配器和用于音频驱动唇形同步的面部交叉注意力适配器。", "result": "广泛的实验证实了HunyuanVideo-HOMA在弱监督下，交互自然度和泛化能力达到了最先进水平。此外，它还在文本条件生成和交互式物体操作中展示了多功能性。", "conclusion": "HunyuanVideo-HOMA在弱监督下，在交互自然度和泛化方面实现了最先进的性能，并展现了在文本条件生成和交互式物体操作方面的多功能性。", "translation": "为了解决人-物交互（HOI）视频生成中的关键限制——特别是对精心策划的动作数据的依赖、对新颖物体/场景的泛化能力有限以及可访问性受限——我们引入了HunyuanVideo-HOMA，一个弱条件多模态驱动框架。HunyuanVideo-HOMA通过稀疏、解耦的动作引导增强了可控性并减少了对精确输入的依赖。它将外观和动作信号编码到多模态扩散Transformer（MMDiT）的双输入空间中，并在共享上下文空间中融合它们，以合成时间一致且物理上合理的人-物交互。为了优化训练，我们集成了一个从预训练MMDiT权重初始化的参数空间HOI适配器，保留了先验知识同时实现了高效适应，以及一个用于解剖学上精确的音频驱动唇形同步的面部交叉注意力适配器。广泛的实验证实了在弱监督下，其在交互自然度和泛化方面的最先进性能。最后，HunyuanVideo-HOMA在文本条件生成和交互式物体操作中展示了多功能性，并得到了用户友好的演示界面的支持。项目页面位于 https://anonymous.4open.science/w/homa-page-0FBE/。", "summary": "HunyuanVideo-HOMA是一个创新的多模态驱动框架，旨在克服人-物交互（HOI）视频生成中对数据依赖、泛化能力弱及可访问性差的问题。该框架通过稀疏、解耦的动作引导，将外观和动作信号融合于多模态扩散Transformer（MMDiT）中，以生成时间一致且物理合理的人-物交互。其引入了HOI适配器和面部交叉注意力适配器以优化训练和实现唇形同步。实验证明，HunyuanVideo-HOMA在弱监督下实现了交互自然度和泛化能力的领先水平，并展现了在文本条件生成和物体操作方面的多功能性。", "keywords": "人-物交互, 多模态驱动, 扩散Transformer, 视频生成, 弱监督", "comments": "这项工作通过引入弱条件多模态驱动框架，显著提升了人-物交互视频生成的通用性和可控性，尤其是在减少对精确输入依赖和提高泛化能力方面具有创新性。其结合MMDiT、参数空间HOI适配器和面部交叉注意力适配器的设计，有效解决了现有方法的局限性，为逼真的人体动画生成提供了新的思路和强大的工具。"}}
{"id": "2506.08464", "title": "MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "summary": "Second-order optimization methods for training neural networks, such as KFAC,\nexhibit superior convergence by utilizing curvature information of loss\nlandscape. However, it comes at the expense of high computational burden. In\nthis work, we analyze the two components that constitute the layer-wise Fisher\ninformation matrix (FIM) used in KFAC: the Kronecker factors related to\nactivations and pre-activation gradients. Based on empirical observations on\ntheir eigenspectra, we propose efficient approximations for them, resulting in\na computationally efficient optimization method called MAC. To the best of our\nknowledge, MAC is the first algorithm to apply the Kronecker factorization to\nthe FIM of attention layers used in transformers and explicitly integrate\nattention scores into the preconditioning. We also study the convergence\nproperty of MAC on nonlinear neural networks and provide two conditions under\nwhich it converges to global minima. Our extensive evaluations on various\nnetwork architectures and datasets show that the proposed method outperforms\nKFAC and other state-of-the-art methods in terms of accuracy, end-to-end\ntraining time, and memory usage. Code is available at\nhttps://github.com/hseung88/mac.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08464v1", "AI": {"title_translation": "MAC：一种使用平均激活近似曲率的有效梯度预处理方法", "tldr": "MAC是一种高效的二阶优化方法，通过近似曲率信息来加速神经网络训练，并优于现有方法。", "motivation": "神经网络训练中的二阶优化方法（如KFAC）虽然在收敛性上表现优越，但计算负担过高。", "method": "本文分析了KFAC中层级Fisher信息矩阵（FIM）的两个组成部分（激活和预激活梯度），并基于其特征谱的经验观察，提出了高效的近似方法，从而开发出计算高效的优化方法MAC。MAC是首个将Kronecker分解应用于Transformer中注意力层FIM并明确整合注意力分数到预处理中的算法。此外，还研究了MAC在非线性神经网络上的收敛特性，并提供了其收敛到全局最小值的两个条件。", "result": "所提出的MAC方法在准确性、端到端训练时间和内存使用方面均优于KFAC和其他最先进方法。", "conclusion": "MAC是一种计算高效且性能优越的神经网络二阶优化方法，通过创新性地近似曲率信息，有效解决了传统二阶方法的计算瓶颈，尤其适用于包括Transformer在内的各种网络架构。", "translation": "神经网络训练的二阶优化方法，例如KFAC，通过利用损失景观的曲率信息展现出卓越的收敛性。然而，这伴随着高计算负担。在这项工作中，我们分析了构成KFAC中层级Fisher信息矩阵（FIM）的两个组成部分：与激活和预激活梯度相关的Kronecker因子。基于对其特征谱的经验观察，我们提出了对它们的高效近似，从而得到了一种计算高效的优化方法，称为MAC。据我们所知，MAC是第一个将Kronecker分解应用于Transformer中使用的注意力层的FIM，并明确将注意力分数整合到预处理中的算法。我们还研究了MAC在非线性神经网络上的收敛特性，并提供了它收敛到全局最小值的两个条件。我们对各种网络架构和数据集进行的广泛评估表明，所提出的方法在准确性、端到端训练时间和内存使用方面优于KFAC和其他最先进方法。代码可在https://github.com/hseung88/mac 获取。", "summary": "本文提出了一种名为MAC的计算高效的二阶优化方法，用于神经网络训练。该方法通过对KFAC中Fisher信息矩阵的Kronecker因子进行高效近似，有效解决了传统二阶方法计算负担高的问题。MAC首次将Kronecker分解应用于Transformer的注意力层FIM，并整合注意力分数。实验结果表明，MAC在准确性、训练时间和内存使用上均优于KFAC及其他现有方法。", "keywords": "二阶优化, 梯度预处理, Fisher信息矩阵, Kronecker分解, 神经网络训练", "comments": "MAC的创新之处在于其对Fisher信息矩阵的Kronecker因子进行了高效近似，显著降低了二阶优化方法的计算成本，同时保持甚至超越了现有方法的性能。特别是在Transformer注意力层中的应用，使其对大型模型训练具有重要意义。此外，对收敛性的理论分析也增加了其可靠性。"}}
{"id": "2506.08473", "title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin", "authors": ["Shuo Yang", "Qihui Zhang", "Yuyang Liu", "Yue Huang", "Xiaojun Jia", "Kunpeng Ning", "Jiayu Yao", "Jigang Wang", "Hailiang Dai", "Yibing Song", "Li Yuan"], "summary": "Large language models (LLMs) are vulnerable to safety risks during\nfine-tuning, where small amounts of malicious or harmless data can compromise\nsafeguards. In this paper, building on the concept of alignment direction --\ndefined by the weight difference between aligned and unaligned models -- we\nobserve that perturbations along this direction preserve model safety. In\ncontrast, perturbations along directions orthogonal to this alignment are\nstrongly linked to harmful direction perturbations, rapidly degrading safety\nand framing the parameter space as a narrow safety basin. Based on this\ninsight, we propose a methodology for safety fine-tuning called AsFT (Anchoring\nSafety in Fine-Tuning), which integrates a regularization term into the\ntraining objective. This term uses the alignment direction as an anchor to\nsuppress updates in harmful directions, ensuring that fine-tuning is\nconstrained within the narrow safety basin. Extensive experiments on multiple\ndatasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by\n7.60 percent, improving model performance by 3.44 percent, and maintaining\nrobust performance across various experimental settings. Code is available at\nhttps://github.com/PKU-YuanGroup/AsFT", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08473v1", "AI": {"title_translation": "AsFT: 在狭窄安全盆地内锚定大语言模型微调过程中的安全性", "tldr": "AsFT是一种新的微调方法，通过在训练目标中加入正则化项，利用对齐方向作为锚点，抑制有害方向的更新，从而在微调过程中保持大语言模型的安全性，并显著降低有害行为。", "motivation": "大语言模型（LLMs）在微调过程中容易出现安全风险，少量恶意或无害数据就可能破坏安全防护措施。", "method": "该研究基于对齐方向（对齐模型与未对齐模型之间的权重差异）的概念，观察到沿此方向的扰动能保持模型安全性，而正交方向的扰动与有害方向紧密相关。基于此洞察，提出了一种名为AsFT（Anchoring Safety in Fine-Tuning）的安全微调方法。该方法在训练目标中整合了一个正则化项，利用对齐方向作为锚点来抑制有害方向的更新，确保微调过程被约束在狭窄的安全盆地内。", "result": "AsFT在多个数据集上进行了广泛实验，结果显示其优于Safe LoRA，有害行为减少了7.60%，模型性能提高了3.44%，并且在各种实验设置下均保持了稳健的性能。", "conclusion": "AsFT方法通过利用对齐方向作为锚点并引入正则化项，有效地在LLM微调过程中锚定安全性，显著降低了有害行为并提升了模型性能。", "translation": "大型语言模型（LLM）在微调过程中容易受到安全风险的影响，少量恶意或无害的数据都可能损害其安全防护。本文基于对齐方向（定义为对齐模型和未对齐模型之间的权重差异）的概念，我们观察到沿此方向的扰动可以保持模型的安全性。相反，沿与此对齐方向正交的方向的扰动与有害方向的扰动密切相关，会迅速降低安全性，并将参数空间构架为一个狭窄的安全盆地。基于这一见解，我们提出了一种名为 AsFT（Anchoring Safety in Fine-Tuning）的安全微调方法，该方法将一个正则化项集成到训练目标中。该项利用对齐方向作为锚点来抑制有害方向的更新，确保微调被约束在狭窄的安全盆地内。在多个数据集上进行的广泛实验表明，AsFT 优于 Safe LoRA，有害行为减少了 7.60%，模型性能提高了 3.44%，并在各种实验设置下保持了稳健的性能。代码可在 https://github.com/PKU-YuanGroup/AsFT 获取。", "summary": "本研究提出了一种名为AsFT（Anchoring Safety in Fine-Tuning）的新方法，旨在解决大语言模型在微调过程中面临的安全风险。通过观察到沿特定“对齐方向”的扰动能保持模型安全性，而正交方向的扰动则会迅速降低安全性，作者将参数空间定义为狭窄的安全盆地。AsFT通过在训练目标中引入一个正则化项，以对齐方向为锚点，抑制有害方向的更新，从而将微调过程限制在安全范围内。实验结果表明，AsFT在减少有害行为和提升模型性能方面均优于现有方法。", "keywords": "大语言模型, 安全微调, 对齐方向, 正则化, 安全盆地", "comments": "AsFT的创新点在于识别并利用了“对齐方向”这一关键概念，将其作为锚点来指导LLM的微调过程，有效解决了微调中固有的安全脆弱性。该方法通过引入正则化项，以一种巧妙且可控的方式将安全性融入到训练目标中，为大模型安全微调提供了一个有前景的解决方案。其在降低有害行为和提升性能方面的双重优势，凸显了其在实际应用中的重要性。"}}
{"id": "2506.08817", "title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought", "authors": ["Shuyi Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Pengwei Wang", "Zhongyuan Wang", "Hongxuan Ma", "Shanghang Zhang"], "summary": "Video content comprehension is essential for various applications, ranging\nfrom video analysis to interactive systems. Despite advancements in large-scale\nvision-language models (VLMs), these models often struggle to capture the\nnuanced, spatiotemporal details essential for thorough video analysis. To\naddress this gap, we introduce Video-CoT, a groundbreaking dataset designed to\nenhance spatiotemporal understanding using Chain-of-Thought (CoT)\nmethodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal\nquestion-answer pairs and 23,000 high-quality CoT-annotated samples, providing\na solid foundation for evaluating spatiotemporal understanding in video\ncomprehension. Additionally, we provide a comprehensive benchmark for assessing\nthese tasks, with each task featuring 750 images and tailored evaluation\nmetrics. Our extensive experiments reveal that current VLMs face significant\nchallenges in achieving satisfactory performance, high-lighting the\ndifficulties of effective spatiotemporal understanding. Overall, the Video-CoT\ndataset and benchmark open new avenues for research in multimedia understanding\nand support future innovations in intelligent systems requiring advanced video\nanalysis capabilities. By making these resources publicly available, we aim to\nencourage further exploration in this critical area. Project\nwebsite:https://video-cot.github.io/ .", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08817v1", "AI": {"title_translation": "视频-CoT：一个基于思维链的视频时空理解综合数据集", "tldr": "引入Video-CoT数据集，用于提升视频时空理解，发现现有VLM在此方面表现不佳。", "motivation": "现有大规模视觉语言模型（VLMs）难以捕捉视频中细微的时空细节，这对于彻底的视频分析至关重要。", "method": "引入Video-CoT数据集，包含192,000个细粒度时空问答对和23,000个高质量CoT标注样本。同时提供一个综合基准，每个任务包含750张图像和定制评估指标。", "result": "实验表明，当前VLMs在实现令人满意的性能方面面临显著挑战，突显了有效时空理解的困难。", "conclusion": "Video-CoT数据集和基准为多媒体理解研究开辟了新途径，并支持未来需要高级视频分析能力的智能系统创新。", "translation": "视频内容理解对于从视频分析到交互系统等各种应用至关重要。尽管大规模视觉语言模型（VLMs）取得了进展，但这些模型通常难以捕捉彻底视频分析所需的细微时空细节。为了解决这一差距，我们引入了Video-CoT，这是一个旨在利用思维链（CoT）方法增强时空理解的开创性数据集。Video-CoT包含192,000个细粒度时空问答对和23,000个高质量的CoT标注样本，为评估视频理解中的时空理解提供了坚实的基础。此外，我们提供了一个评估这些任务的综合基准，每个任务包含750张图像和定制的评估指标。我们广泛的实验表明，当前的VLMs在实现令人满意的性能方面面临显著挑战，突显了有效时空理解的困难。总的来说，Video-CoT数据集和基准为多媒体理解研究开辟了新途径，并支持未来需要高级视频分析能力的智能系统创新。通过公开这些资源，我们旨在鼓励在这一关键领域进行进一步探索。项目网站：https://video-cot.github.io/。", "summary": "本文介绍了Video-CoT数据集，旨在解决现有视觉语言模型在视频时空细节理解方面的不足。该数据集包含大量的细粒度时空问答对和思维链标注样本，并提供了一个综合基准用于评估。实验结果表明，当前VLMs在时空理解方面表现不佳，凸显了这一领域的挑战。Video-CoT有望推动多媒体理解和智能系统视频分析能力的发展。", "keywords": "视频理解, 时空理解, 思维链, 数据集, 视觉语言模型", "comments": "Video-CoT数据集通过引入思维链（CoT）方法和大规模细粒度时空问答对，为视频时空理解领域提供了重要的资源。其创新性在于针对现有VLMs的弱点，构建了一个专门用于评估和提升模型时空推理能力的新型基准。该工作的意义在于揭示了当前模型在复杂视频理解任务上的局限性，并为未来的研究提供了明确的方向和评估工具。"}}
{"id": "2506.08184", "title": "Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08184v1", "AI": {"title_translation": "无法遗忘：主动干扰揭示了LLM超越上下文长度的工作记忆限制", "tldr": "大型语言模型（LLMs）难以应对主动干扰，倾向于检索旧的、已被覆盖的信息，这表明其存在超越上下文长度限制的工作记忆瓶颈。", "motivation": "尽管人们日益认识到大型语言模型（LLMs）中的信息检索与生成能力交织而非简单查找，且通常认为更长的上下文能改善检索，但上下文内部干扰的影响仍未得到充分研究。本研究旨在解决这一问题。", "method": "本研究改编了认知科学中的主动干扰（PI）范式，并引入了PI-LLM评估方法。该方法顺序流式传输语义相关的键值更新，并仅查询最终值。", "result": "LLM的检索准确性随着干扰的累积呈对数线性下降至零，错误源于检索到先前被覆盖的值。通过提示工程尝试减轻干扰的效果有限。", "conclusion": "这些发现揭示了LLM在解开干扰和灵活操纵信息方面的根本性限制，表明存在超越简单上下文访问的工作记忆瓶颈。这要求采取方法来增强模型在检索过程中抑制不相关内容的能力。", "translation": "大型语言模型（LLMs）中的信息检索正日益被认为是与生成能力交织在一起，而非仅仅是查找。虽然通常认为更长的上下文会改善检索，但上下文内部干扰的影响仍未得到充分研究。为了解决这个问题，我们改编了认知科学中的主动干扰（PI）范式，即早期信息会干扰对新更新信息的召回。在人类中，对这种干扰的敏感性与工作记忆容量呈负相关。我们引入了PI-LLM，这是一种评估方法，它按顺序流式传输语义相关的键值更新，并且只查询最终值。尽管这些最终值明确地位于查询之前，但随着干扰的累积，LLM的检索准确性呈对数线性下降至零；错误源于检索到先前被覆盖的值。通过提示工程（例如，指示模型忽略早期输入）来减轻干扰的尝试效果有限。这些发现揭示了LLM在解开干扰和灵活操纵信息方面的基本限制，表明存在超越简单上下文访问的工作记忆瓶颈。这要求采取方法来增强模型在检索过程中抑制不相关内容的能力。", "summary": "本论文通过改编认知科学中的主动干扰（PI）范式，研究了大型语言模型（LLMs）中上下文内部干扰这一未被充分研究的问题。研究引入了PI-LLM，这是一种评估方法，用于流式传输语义相关的键值更新并仅查询最新值。结果表明，随着干扰的累积，LLM的检索准确性呈对数线性下降，错误主要来自检索到先前被覆盖的信息。提示工程在减轻这种干扰方面效果有限，这表明LLM存在一个独立于上下文长度的根本性工作记忆瓶颈，突显了其抑制不相关内容和解开干扰的能力不足。", "keywords": "主动干扰, 大型语言模型, 工作记忆, 信息检索, 上下文长度", "comments": "这篇论文创新性地将认知科学中的主动干扰范式应用于LLM评估，揭示了LLM超越上下文长度的显著工作记忆限制。提示工程效果不佳的发现，暗示了LLM在架构或学习机制上可能存在更深层次的挑战。这项工作对于指导未来研究至关重要，旨在开发具有更好选择性注意和干扰抑制机制的LLM，这对于更稳健和可靠的长上下文应用至关重要。"}}
{"id": "2506.08835", "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics", "authors": ["Shravan Nayak", "Mehar Bhatia", "Xiaofeng Zhang", "Verena Rieser", "Lisa Anne Hendricks", "Sjoerd van Steenkiste", "Yash Goyal", "Karolina Stańczak", "Aishwarya Agrawal"], "summary": "The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts. In this work, we present the first study to\nsystematically quantify the alignment of T2I models and evaluation metrics with\nrespect to both explicit as well as implicit cultural expectations. To this\nend, we introduce CulturalFrames, a novel benchmark designed for rigorous human\nevaluation of cultural representation in visual generations. Spanning 10\ncountries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,\n3637 corresponding images generated by 4 state-of-the-art T2I models, and over\n10k detailed human annotations. We find that T2I models not only fail to meet\nthe more challenging implicit expectations but also the less challenging\nexplicit expectations. Across models and countries, cultural expectations are\nmissed an average of 44% of the time. Among these failures, explicit\nexpectations are missed at a surprisingly high average rate of 68%, while\nimplicit expectation failures are also significant, averaging 49%. Furthermore,\nwe demonstrate that existing T2I evaluation metrics correlate poorly with human\njudgments of cultural alignment, irrespective of their internal reasoning.\nCollectively, our findings expose critical gaps, providing actionable\ndirections for developing more culturally informed T2I models and evaluation\nmethodologies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08835v1", "AI": {"title_translation": "CulturalFrames：评估文生图模型和评估指标中的文化期望一致性", "tldr": "本研究引入了CulturalFrames基准，系统地量化了文生图模型在文化期望方面的表现，发现现有模型在显性和隐性文化期望方面均表现不佳，且现有评估指标与人类判断相关性差。", "motivation": "文生图模型在生成视觉内容方面的普及引发了对其准确代表多样文化背景能力的担忧。本研究旨在系统量化文生图模型和评估指标与文化期望的一致性。", "method": "本研究引入了CulturalFrames，这是一个新颖的基准，用于对视觉生成中的文化表征进行严格的人工评估。该基准涵盖10个国家和5个社会文化领域，包含983个提示、4个最先进文生图模型生成的3637张相应图像以及超过1万个人工详细标注。", "result": "研究发现，文生图模型不仅未能满足更具挑战性的隐性期望，也未能满足挑战性较低的显性期望。模型和国家之间，文化期望平均有44%的时间未被满足。在这些失败中，显性期望未被满足的平均比例高达68%，而隐性期望失败的平均比例也达到49%。此外，现有文生图评估指标与人类文化一致性判断的相关性很差。", "conclusion": "本研究的发现揭示了文生图模型和评估方法中的关键差距，为开发更具文化意识的文生图模型和评估方法提供了可行的方向。", "translation": "随着文生图（T2I）模型作为视觉内容生成工具的日益普及，人们对其准确代表多样文化背景的能力产生了担忧。在这项工作中，我们首次系统地量化了T2I模型和评估指标在显性和隐性文化期望方面的一致性。为此，我们引入了CulturalFrames，这是一个旨在对视觉生成中的文化表征进行严格人工评估的新型基准。CulturalFrames涵盖10个国家和5个社会文化领域，包含983个提示、4个最先进的T2I模型生成的3637张相应图像，以及超过1万个详细的人工标注。我们发现T2I模型不仅未能满足更具挑战性的隐性期望，也未能满足挑战性较低的显性期望。跨模型和国家，文化期望平均有44%的时间未被满足。在这些失败中，显性期望未被满足的平均比例高达68%，而隐性期望失败的平均比例也达到49%。此外，我们证明现有T2I评估指标与人类对文化一致性的判断相关性很差，无论其内部推理如何。总的来说，我们的发现暴露了关键差距，为开发更具文化意识的T2I模型和评估方法提供了可行的方向。", "summary": "本研究介绍了CulturalFrames，一个用于评估文生图模型文化代表性的新型基准。通过对10个国家、5个社会文化领域的983个提示、3637张图像和1万多个人工标注进行分析，发现现有文生图模型在显性和隐性文化期望方面均表现不佳，平均有44%的文化期望未被满足。此外，现有评估指标与人类文化一致性判断的相关性很差。这些发现揭示了文生图模型和评估方法中的关键不足，并为未来发展提供了方向。", "keywords": "文生图模型, 文化期望, CulturalFrames, 人工评估, 评估指标", "comments": "这项研究的创新之处在于首次系统地量化了文生图模型在文化期望方面的表现，并引入了专门用于此目的的大规模人工评估基准CulturalFrames。其重要性在于揭示了当前最先进文生图模型在文化多样性表示方面的严重缺陷，以及现有评估指标的不足，为未来开发更具文化敏感性的AI模型提供了明确的方向。"}}
{"id": "2506.08505", "title": "Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations", "authors": ["Shahaf Bassan", "Yizhak Yisrael Elboher", "Tobias Ladner", "Matthias Althoff", "Guy Katz"], "summary": "Despite significant advancements in post-hoc explainability techniques for\nneural networks, many current methods rely on heuristics and do not provide\nformally provable guarantees over the explanations provided. Recent work has\nshown that it is possible to obtain explanations with formal guarantees by\nidentifying subsets of input features that are sufficient to determine that\npredictions remain unchanged using neural network verification techniques.\nDespite the appeal of these explanations, their computation faces significant\nscalability challenges. In this work, we address this gap by proposing a novel\nabstraction-refinement technique for efficiently computing provably sufficient\nexplanations of neural network predictions. Our method abstracts the original\nlarge neural network by constructing a substantially reduced network, where a\nsufficient explanation of the reduced network is also provably sufficient for\nthe original network, hence significantly speeding up the verification process.\nIf the explanation is in sufficient on the reduced network, we iteratively\nrefine the network size by gradually increasing it until convergence. Our\nexperiments demonstrate that our approach enhances the efficiency of obtaining\nprovably sufficient explanations for neural network predictions while\nadditionally providing a fine-grained interpretation of the network's\npredictions across different abstraction levels.", "comment": "To appear in ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08505v1", "AI": {"title_translation": "解释，或快或慢：可证明解释的抽象与细化", "tldr": "本文提出了一种新颖的抽象-细化技术，用于高效计算神经网络预测的可证明充分解释，解决了现有方法的可伸缩性挑战。", "motivation": "尽管事后可解释性技术取得了进展，但许多现有方法依赖启发式，缺乏形式化的可证明保证，且计算可证明解释面临显著的可伸缩性挑战。", "method": "本文提出了一种新的抽象-细化技术。该方法通过构建一个大幅缩小的网络来抽象原始大型神经网络，其中缩小网络的充分解释对原始网络也具有可证明的充分性，从而显著加速验证过程。如果缩小网络上的解释不充分，则通过逐步增加网络大小迭代细化，直至收敛。", "result": "实验表明，该方法提高了获取神经网络预测可证明充分解释的效率，并提供了跨不同抽象级别的网络预测的细粒度解释。", "conclusion": "本文提出的抽象-细化技术显著提高了神经网络预测可证明充分解释的计算效率，并提供了多层次的解释能力。", "translation": "尽管神经网络事后可解释性技术取得了显著进步，但许多现有方法依赖启发式，且不提供所提供解释的形式化可证明保证。最近的工作表明，通过识别足以确定预测不变的输入特征子集，并使用神经网络验证技术，可以获得具有形式化保证的解释。尽管这些解释具有吸引力，但其计算面临显著的可伸缩性挑战。在这项工作中，我们通过提出一种新颖的抽象-细化技术来解决这一空白，以高效计算神经网络预测的可证明充分解释。我们的方法通过构建一个大幅缩小的网络来抽象原始大型神经网络，其中缩小网络的充分解释对原始网络也具有可证明的充分性，从而显著加速验证过程。如果缩小网络上的解释不充分，我们通过逐步增加网络大小迭代细化网络，直至收敛。我们的实验表明，我们的方法提高了获取神经网络预测可证明充分解释的效率，同时还提供了跨不同抽象级别的网络预测的细粒度解释。", "summary": "本文提出了一种名为“抽象-细化”的新技术，旨在高效计算神经网络预测的可证明充分解释。针对现有可解释性方法缺乏形式化保证和可伸缩性差的问题，该方法通过将大型神经网络抽象为一个缩小网络，并证明缩小网络的解释对原网络同样有效，从而加速验证过程。当解释不充分时，该方法会迭代细化网络。实验结果表明，该方法显著提高了获取可证明解释的效率，并能提供多层次的细粒度解释。", "keywords": "可解释性AI, 神经网络, 可证明解释, 抽象-细化, 可伸缩性", "comments": "该论文解决了可证明解释在计算效率上的关键瓶颈，通过抽象-细化策略在保证解释有效性的前提下提升了实用性。其创新点在于将神经网络验证技术与分层解释相结合，为构建更可靠、更高效的XAI系统提供了有价值的思路。"}}
{"id": "2506.08849", "title": "Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis", "authors": ["Jingguo Qu", "Xinyang Han", "Tonghuan Xiao", "Jia Ai", "Juan Wu", "Tong Zhao", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Yingınst"], "summary": "Medical ultrasonography is an essential imaging technique for examining\nsuperficial organs and tissues, including lymph nodes, breast, and thyroid. It\nemploys high-frequency ultrasound waves to generate detailed images of the\ninternal structures of the human body. However, manually contouring regions of\ninterest in these images is a labor-intensive task that demands expertise and\noften results in inconsistent interpretations among individuals.\nVision-language foundation models, which have excelled in various computer\nvision applications, present new opportunities for enhancing ultrasound image\nanalysis. Yet, their performance is hindered by the significant differences\nbetween natural and medical imaging domains. This research seeks to overcome\nthese challenges by developing domain adaptation methods for vision-language\nfoundation models. In this study, we explore the fine-tuning pipeline for\nvision-language foundation models by utilizing large language model as text\nrefiner with special-designed adaptation strategies and task-driven heads. Our\napproach has been extensively evaluated on six ultrasound datasets and two\ntasks: segmentation and classification. The experimental results show that our\nmethod can effectively improve the performance of vision-language foundation\nmodels for ultrasound image analysis, and outperform the existing\nstate-of-the-art vision-language and pure foundation models. The source code of\nthis study is available at\n\\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08849v1", "AI": {"title_translation": "用于下一代医用超声图像分析的视觉-语言基础模型自适应", "tldr": "本研究开发了一种领域自适应方法，通过结合大型语言模型作为文本精炼器和特殊设计的自适应策略，显著提高了视觉-语言基础模型在医学超声图像分析中的性能，并在分割和分类任务上超越了现有最先进模型。", "motivation": "医用超声图像中的区域手动勾勒耗时且需要专业知识，易导致个体间解释不一致。尽管视觉-语言基础模型在计算机视觉中表现出色，但由于自然图像与医学图像之间的显著差异，它们在医学领域表现受限。本研究旨在克服这些挑战。", "method": "本研究通过利用大型语言模型作为文本精炼器，并结合特殊设计的自适应策略和任务驱动头，探索了视觉-语言基础模型的微调流程，以开发领域自适应方法。", "result": "实验结果表明，该方法能有效提高视觉-语言基础模型在超声图像分析中的性能，并在六个超声数据集和分割、分类两项任务上优于现有最先进的视觉-语言模型和纯基础模型。", "conclusion": "本研究开发的领域自适应方法能够有效提升视觉-语言基础模型在医学超声图像分析中的表现，并超越现有最先进模型。", "translation": "医学超声检查是检查淋巴结、乳腺和甲状腺等浅表器官和组织的重要成像技术。它利用高频超声波生成人体内部结构的详细图像。然而，在这些图像中手动勾勒感兴趣区域是一项劳动密集型任务，需要专业知识，并且经常导致个体之间解释不一致。在各种计算机视觉应用中表现出色的视觉-语言基础模型为增强超声图像分析提供了新的机会。然而，它们的性能受到自然图像和医学成像领域之间显著差异的阻碍。本研究旨在通过开发视觉-语言基础模型的领域自适应方法来克服这些挑战。在本研究中，我们利用大型语言模型作为文本精炼器，结合特殊设计的自适应策略和任务驱动头，探索了视觉-语言基础模型的微调流程。我们的方法已在六个超声数据集和分割、分类两项任务上进行了广泛评估。实验结果表明，我们的方法可以有效提高视觉-语言基础模型在超声图像分析中的性能，并优于现有最先进的视觉-语言模型和纯基础模型。本研究的源代码可在 \\href{https://github.com/jinggqu/NextGen-UIA}{GitHub} 获取。", "summary": "本研究旨在解决视觉-语言基础模型在医学超声图像分析中因领域差异导致的性能限制。通过开发一种领域自适应方法，该方法利用大型语言模型作为文本精炼器，并结合特殊的自适应策略和任务驱动头来微调视觉-语言基础模型。在六个超声数据集的分割和分类任务上的广泛评估表明，该方法显著提高了模型的性能，并超越了现有最先进的视觉-语言和纯基础模型。", "keywords": "医学超声, 视觉-语言基础模型, 领域自适应, 分割, 分类", "comments": "这项研究的创新之处在于其将大型语言模型引入视觉-语言基础模型的微调流程中，作为文本精炼器，以更好地适应医学超声图像的特定领域。这种结合领域自适应策略和任务驱动头的方法，有效解决了医学与自然图像之间的领域鸿沟问题，为下一代医学图像分析提供了有力的工具。其在多个数据集和任务上的优异表现，证明了其在临床应用中的潜力。"}}
{"id": "2506.08514", "title": "DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training", "authors": ["Jacob Piland", "Chris Sweet", "Adam Czakja"], "summary": "Class Activation Mapping (CAM) and its gradient-based variants (e.g.,\nGradCAM) have become standard tools for explaining Convolutional Neural Network\n(CNN) predictions. However, these approaches typically focus on individual\nlogits, while for neural networks using softmax, the class membership\nprobability estimates depend \\textit{only} on the \\textit{differences} between\nlogits, not on their absolute values. This disconnect leaves standard CAMs\nvulnerable to adversarial manipulation, such as passive fooling, where a model\nis trained to produce misleading CAMs without affecting decision performance.\nWe introduce \\textbf{Salience-Hoax Activation Maps (SHAMs)}, an\n\\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM\nrobustness under adversarial conditions. To address the passive fooling\nvulnerability, we then propose \\textbf{DiffGradCAM}, a novel, lightweight, and\ncontrastive approach to class activation mapping that is both non-suceptible to\npassive fooling, but also matches the output of standard CAM methods such as\nGradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a\nnew framework for probing and improving the robustness of saliency-based\nexplanations. We validate both contributions across multi-class tasks with few\nand many classes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08514v1", "AI": {"title_translation": "DiffGradCAM：一种抗对抗训练的通用类激活图", "tldr": "现有CAM方法易受对抗性操纵，本文提出SHAM作为基准来评估鲁棒性，并引入DiffGradCAM，一种新型抗被动欺骗的CAM方法，共同建立了一个探测和改进显著性解释鲁棒性的新框架。", "motivation": "现有的类激活映射（CAM）及其梯度变体（如GradCAM）是解释CNN预测的标准工具。然而，这些方法通常关注单个logit，而使用softmax的神经网络中，类成员概率估计仅取决于logit之间的差异。这种脱节使得标准CAM容易受到对抗性操纵，例如“被动欺骗”，即模型在不影响决策性能的情况下产生误导性的CAM。", "method": "本文引入了“显著性欺骗激活图（SHAMs）”，这是一种熵感知的被动欺骗形式，用作对抗条件下CAM鲁棒性的基准。为了解决被动欺骗的漏洞，作者提出了“DiffGradCAM”，一种新颖、轻量级且对比性的类激活映射方法，它既不受被动欺骗的影响，又在非对抗情况下与GradCAM等标准CAM方法的输出相匹配。", "result": "SHAM和DiffGradCAM共同建立了一个探测和改进基于显著性解释鲁棒性的新框架。作者在具有少量和大量类的多类任务中验证了这两项贡献。", "conclusion": "DiffGradCAM提供了一种对被动欺骗具有鲁棒性的类激活映射方法，并且在非对抗性条件下与现有方法兼容，同时SHAM提供了一个评估CAM鲁棒性的新基准，共同推进了显著性解释的鲁棒性研究。", "translation": "类激活映射（CAM）及其基于梯度的变体（例如GradCAM）已成为解释卷积神经网络（CNN）预测的标准工具。然而，这些方法通常关注单个logit，而对于使用softmax的神经网络，类成员概率估计仅取决于logit之间的差异，而不是它们的绝对值。这种脱节使得标准CAM容易受到对抗性操纵，例如被动欺骗，即在不影响决策性能的情况下，训练模型产生误导性的CAM。我们引入了显著性欺骗激活图（SHAMs），这是一种熵感知的被动欺骗形式，用作对抗条件下CAM鲁棒性的基准。为了解决被动欺骗漏洞，我们随后提出了DiffGradCAM，这是一种新颖、轻量级且对比性的类激活映射方法，它既不受被动欺骗的影响，又在非对抗情况下与GradCAM等标准CAM方法的输出相匹配。SHAM和DiffGradCAM共同建立了一个探测和改进基于显著性解释鲁棒性的新框架。我们在具有少量和大量类的多类任务中验证了这两项贡献。", "summary": "本文针对现有类激活映射（CAM）方法易受“被动欺骗”式对抗性操纵的问题，提出了一种新的框架。该框架包括：1) **SHAMs**，一种熵感知的被动欺骗形式，作为评估CAM鲁棒性的基准；2) **DiffGradCAM**，一种新型、轻量级且对比性的CAM方法，它能抵抗被动欺骗，并在非对抗条件下与传统CAM方法效果一致。这项工作为提高显著性解释的鲁棒性提供了新的工具和视角。", "keywords": "类激活映射, 对抗性鲁棒性, 可解释人工智能, 被动欺骗, DiffGradCAM", "comments": "这篇论文通过引入SHAMs和DiffGradCAM，解决了现有CAM方法在对抗环境下的脆弱性问题。SHAMs提供了一个量化“被动欺骗”的新基准，而DiffGradCAM则提供了一种实用的、更鲁棒的解释工具。其创新性在于从logit差异的角度出发，开发出一种对对抗性操纵更具抵抗力的CAM变体，这对于提高AI模型可解释性的可靠性和安全性具有重要意义。"}}
{"id": "2506.08854", "title": "Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning", "authors": ["Junzhuo Liu", "Markus Eckstein", "Zhixiang Wang", "Friedrich Feuerhake", "Dorit Merhof"], "summary": "Spatial transcriptomics is a technology that captures gene expression levels\nat different spatial locations, widely used in tumor microenvironment analysis\nand molecular profiling of histopathology, providing valuable insights into\nresolving gene expression and clinical diagnosis of cancer. Due to the high\ncost of data acquisition, large-scale spatial transcriptomics data remain\nchallenging to obtain. In this study, we develop a contrastive learning-based\ndeep learning method to predict spatially resolved gene expression from\nwhole-slide images. Evaluation across six different disease datasets\ndemonstrates that, compared to existing studies, our method improves Pearson\nCorrelation Coefficient (PCC) in the prediction of highly expressed genes,\nhighly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%\nrespectively. Further analysis indicates that our method preserves gene-gene\ncorrelations and applies to datasets with limited samples. Additionally, our\nmethod exhibits potential in cancer tissue localization based on biomarker\nexpression.", "comment": "20 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08854v1", "AI": {"title_translation": "基于跨模态掩码重建和对比学习的组织病理学空间转录组表达预测", "tldr": "该研究开发了一种基于对比学习的深度学习方法，用于从组织病理学图像预测空间分辨的基因表达，与现有研究相比，该方法在多个疾病数据集上显著提高了高表达基因、高变异基因和标记基因的预测准确性，并适用于样本量有限的数据集，同时在癌症组织定位方面也显示出潜力。", "motivation": "空间转录组学技术在肿瘤微环境分析和组织病理学分子图谱分析中具有重要价值，但由于数据获取成本高昂，大规模空间转录组学数据难以获取。本研究旨在解决这一挑战。", "method": "本研究开发了一种基于对比学习的深度学习方法，通过跨模态掩码重建从全玻片图像预测空间分辨的基因表达。", "result": "在六个不同疾病数据集上的评估表明，与现有研究相比，该方法在高表达基因、高变异基因和标记基因的预测方面，皮尔逊相关系数（PCC）分别提高了6.27%、6.11%和11.26%。此外，该方法还能保持基因-基因相关性，适用于样本量有限的数据集，并在基于生物标志物表达的癌症组织定位方面展现出潜力。", "conclusion": "本研究提出的基于对比学习的深度学习方法能够有效且准确地从组织病理学图像预测空间分辨的基因表达，克服了数据获取成本高的限制，并展现出在癌症诊断和研究中的应用前景。", "translation": "空间转录组学是一种捕获不同空间位置基因表达水平的技术，广泛应用于肿瘤微环境分析和组织病理学分子图谱分析，为解析基因表达和癌症临床诊断提供了宝贵见解。由于数据获取成本高昂，大规模空间转录组学数据仍然难以获得。在这项研究中，我们开发了一种基于对比学习的深度学习方法，用于从全玻片图像预测空间分辨的基因表达。对六个不同疾病数据集的评估表明，与现有研究相比，我们的方法在高表达基因、高变异基因和标记基因的预测中，皮尔逊相关系数（PCC）分别提高了6.27%、6.11%和11.26%。进一步分析表明，我们的方法保留了基因-基因相关性，并适用于样本量有限的数据集。此外，我们的方法在基于生物标志物表达的癌症组织定位方面也显示出潜力。", "summary": "本研究提出了一种基于对比学习的深度学习方法，旨在从组织病理学全玻片图像中预测空间分辨的基因表达，以解决空间转录组数据获取成本高昂的挑战。该方法在六个不同疾病数据集上进行了评估，结果显示其在高表达基因、高变异基因和标记基因的预测准确性上显著优于现有研究。此外，该方法还能保持基因间的相关性，适用于样本量有限的数据集，并为基于生物标志物表达的癌症组织定位提供了新的可能性。", "keywords": "空间转录组学, 基因表达预测, 组织病理学, 对比学习, 深度学习", "comments": "该研究的创新之处在于利用对比学习和跨模态掩码重建技术，从组织病理学图像预测空间转录组表达，有效解决了空间转录组数据获取成本高和数据量稀缺的问题。其重要性在于，这项技术有望使空间基因表达分析更加便捷和经济，从而推动肿瘤微环境分析、分子分型和癌症诊断等领域的发展。"}}
{"id": "2506.08516", "title": "NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis", "authors": ["Mouadh Yagoubi", "David Danan", "Milad Leyli-Abadi", "Ahmed Mazari", "Jean-Patrick Brunet", "Abbas Kabalan", "Fabien Casenave", "Yuxin Ma", "Giovanni Catalani", "Jean Fesquet", "Jacob Helwig", "Xuan Zhang", "Haiyang Yu", "Xavier Bertrand", "Frederic Tost", "Michael Baurheim", "Joseph Morlier", "Shuiwang Ji"], "summary": "The integration of machine learning (ML) into the physical sciences is\nreshaping computational paradigms, offering the potential to accelerate\ndemanding simulations such as computational fluid dynamics (CFD). Yet,\npersistent challenges in accuracy, generalization, and physical consistency\nhinder the practical deployment of ML models in scientific domains. To address\nthese limitations and systematically benchmark progress, we organized the\nML4CFD competition, centered on surrogate modeling for aerodynamic simulations\nover two-dimensional airfoils. The competition attracted over 240 teams, who\nwere provided with a curated dataset generated via OpenFOAM and evaluated\nthrough a multi-criteria framework encompassing predictive accuracy, physical\nfidelity, computational efficiency, and out-of-distribution generalization.\nThis retrospective analysis reviews the competition outcomes, highlighting\nseveral approaches that outperformed baselines under our global evaluation\nscore. Notably, the top entry exceeded the performance of the original OpenFOAM\nsolver on aggregate metrics, illustrating the promise of ML-based surrogates to\noutperform traditional solvers under tailored criteria. Drawing from these\nresults, we analyze the key design principles of top submissions, assess the\nrobustness of our evaluation framework, and offer guidance for future\nscientific ML challenges.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08516v1", "AI": {"title_translation": "NeurIPS 2024 ML4CFD 竞赛：结果与回顾性分析", "tldr": "本文回顾了NeurIPS 2024 ML4CFD竞赛的结果和分析，该竞赛旨在通过机器学习（ML）加速计算流体动力学（CFD）模拟。竞赛结果表明，ML代理模型在特定评估标准下能够超越传统CFD求解器。", "motivation": "机器学习（ML）在物理科学中的应用，尤其是在加速计算流体动力学（CFD）等高要求模拟方面，具有巨大潜力。然而，ML模型在科学领域的实际部署面临准确性、泛化性和物理一致性等持续挑战。为了解决这些局限性并系统地评估进展，本文组织了ML4CFD竞赛。", "method": "组织了ML4CFD竞赛，专注于二维翼型气动模拟的代理建模。竞赛吸引了240多支队伍，提供了通过OpenFOAM生成的精选数据集，并通过包含预测准确性、物理保真度、计算效率和分布外泛化等多标准框架进行评估。本文对竞赛结果进行了回顾性分析。", "result": "多项方法表现优于基线，其中排名第一的参赛作品在综合指标上超越了原始OpenFOAM求解器的性能。这表明在特定标准下，基于ML的代理模型有望超越传统求解器。本文还分析了顶级提交作品的关键设计原则，并评估了评估框架的稳健性。", "conclusion": "基于机器学习的代理模型在特定标准下有望超越传统CFD求解器，这为科学领域ML模型的实际部署提供了前景。竞赛结果提供了关于有效ML模型设计和未来科学ML挑战的指导。", "translation": "机器学习（ML）融入物理科学正在重塑计算范式，提供了加速计算流体动力学（CFD）等高要求模拟的潜力。然而，准确性、泛化性和物理一致性方面的持续挑战阻碍了ML模型在科学领域的实际部署。为了解决这些局限性并系统地评估进展，我们组织了ML4CFD竞赛，该竞赛以二维翼型气动模拟的代理建模为中心。竞赛吸引了240多支队伍，他们获得了通过OpenFOAM生成的精选数据集，并通过包含预测准确性、物理保真度、计算效率和分布外泛化等多标准框架进行评估。本次回顾性分析回顾了竞赛结果，重点介绍了在我们全球评估得分下表现优于基线的几种方法。值得注意的是，排名第一的参赛作品在综合指标上超越了原始OpenFOAM求解器的性能，这说明了基于ML的代理模型在特定标准下超越传统求解器的前景。根据这些结果，我们分析了顶级提交作品的关键设计原则，评估了我们评估框架的稳健性，并为未来的科学ML挑战提供了指导。", "summary": "NeurIPS 2024 ML4CFD竞赛旨在系统评估机器学习在计算流体动力学（CFD）代理建模中的进展。该竞赛吸引了240多支团队，使用OpenFOAM数据集，并采用多标准框架评估预测准确性、物理保真度、计算效率和泛化能力。回顾性分析显示，多个方法表现优于基线，其中表现最佳的ML模型在综合指标上甚至超越了传统OpenFOAM求解器。研究强调了ML代理模型在特定条件下超越传统求解器的潜力，并分析了顶级提交的关键设计原则，为未来的科学ML挑战提供了指导。", "keywords": "机器学习, 计算流体动力学, 代理建模, 竞赛, 气动模拟", "comments": "这项工作通过组织ML4CFD竞赛，为机器学习在计算流体动力学领域的应用提供了一个重要的基准。比赛结果显著地展示了ML模型在某些情况下能够超越传统数值求解器的潜力，这对于加速科学模拟具有重要意义。文章对顶级方法的分析及其对未来挑战的指导，对于推动科学ML领域的发展具有创新性和重要性。"}}
{"id": "2506.08862", "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams", "authors": ["Zike Wu", "Qi Yan", "Xuanyu Yi", "Lele Wang", "Renjie Liao"], "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08862v1", "AI": {"title_translation": "StreamSplat：面向未校准视频流的在线动态3D重建", "tldr": "StreamSplat是一个实时、全前馈框架，能将未校准视频流在线转换为动态3D高斯Splatting表示，在动态场景重建质量和效率上优于现有方法。", "motivation": "现有方法难以同时解决从未校准视频流实时重建动态3D场景的三个关键挑战：实时处理未校准输入、准确建模动态场景演化、以及保持长期稳定性和计算效率。", "method": "提出StreamSplat，一个全前馈框架，能将任意长度的未校准视频流在线转换为动态3D高斯Splatting (3DGS) 表示。其主要创新包括：在静态编码器中采用概率采样机制进行3DGS位置预测，以及在动态解码器中采用双向形变场以实现鲁棒高效的动态建模。", "result": "在静态和动态基准测试中，StreamSplat在重建质量和动态场景建模方面持续优于现有工作，并独特地支持任意长度视频流的在线重建。", "conclusion": "StreamSplat是首个支持从任意长度未校准视频流在线重建动态3D高斯Splatting表示的全前馈框架，在性能和效率上均表现出色。", "translation": "从非校准视频流实时重建动态3D场景对于众多实际应用至关重要。然而，现有方法难以共同解决三个关键挑战：1) 实时处理非校准输入，2) 准确建模动态场景演化，以及 3) 保持长期稳定性和计算效率。为此，我们引入了StreamSplat，这是首个全前馈框架，能够在线将任意长度的非校准视频流转换为动态3D高斯Splatting (3DGS) 表示，并能够从时间局部观测中恢复场景动态。我们提出了两项关键技术创新：静态编码器中的概率采样机制用于3DGS位置预测，以及动态解码器中的双向形变场，以实现鲁棒高效的动态建模。在静态和动态基准测试上的广泛实验表明，StreamSplat在重建质量和动态场景建模方面始终优于现有工作，同时独特地支持任意长度视频流的在线重建。代码和模型可在 https://github.com/nickwzk/StreamSplat 获取。", "summary": "StreamSplat是一种新颖的全前馈框架，旨在解决从非校准视频流实时在线重建动态3D场景的挑战。它通过创新的概率采样机制和双向形变场，将视频流高效转换为动态3D高斯Splatting表示，并在重建质量和动态建模方面超越了现有方法，同时支持任意长度视频的在线处理。", "keywords": "动态3D重建, 未校准视频流, 在线重建, 3D高斯Splatting, StreamSplat", "comments": "StreamSplat的创新之处在于其全前馈架构，能够在线处理未校准的视频流并生成动态3D高斯Splatting表示，这对于实时动态场景重建具有重要意义。其在鲁棒性和效率方面的提升，以及对任意长度视频流的支持，使其在实际应用中具有广阔前景。"}}
{"id": "2506.08523", "title": "Leveraging chaos in the training of artificial neural networks", "authors": ["Pedro Jiménez-González", "Miguel C. Soriano", "Lucas Lacasa"], "summary": "Traditional algorithms to optimize artificial neural networks when confronted\nwith a supervised learning task are usually exploitation-type relaxational\ndynamics such as gradient descent (GD). Here, we explore the dynamics of the\nneural network trajectory along training for unconventionally large learning\nrates. We show that for a region of values of the learning rate, the GD\noptimization shifts away from purely exploitation-like algorithm into a regime\nof exploration-exploitation balance, as the neural network is still capable of\nlearning but the trajectory shows sensitive dependence on initial conditions --\nas characterized by positive network maximum Lyapunov exponent --.\nInterestingly, the characteristic training time required to reach an acceptable\naccuracy in the test set reaches a minimum precisely in such learning rate\nregion, further suggesting that one can accelerate the training of artificial\nneural networks by locating at the onset of chaos. Our results -- initially\nillustrated for the MNIST classification task -- qualitatively hold for a range\nof supervised learning tasks, learning architectures and other hyperparameters,\nand showcase the emergent, constructive role of transient chaotic dynamics in\nthe training of artificial neural networks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08523v1", "AI": {"title_translation": "在人工神经网络训练中利用混沌", "tldr": "通过利用大步长梯度下降引入的混沌动力学，可以加速人工神经网络的训练，因为这能在探索-利用之间达到平衡。", "motivation": "传统神经网络优化算法（如梯度下降）通常是利用型松弛动力学，研究者探索了在非常大的学习率下神经网络训练轨迹的动力学，以寻找加速训练的方法。", "method": "研究者探索了在非常大的学习率下人工神经网络训练过程中轨迹的动力学，并发现当学习率达到一定范围时，梯度下降优化从纯粹的利用型算法转变为探索-利用平衡的模式，并通过正的网络最大Lyapunov指数来表征其对初始条件的敏感依赖性。", "result": "结果表明，在某个学习率区域内，梯度下降优化从纯粹的利用型算法转变为探索-利用平衡的模式，此时神经网络仍能学习，但轨迹显示出对初始条件的敏感依赖性（表现为正的网络最大Lyapunov指数）。有趣的是，达到可接受准确率所需的特征训练时间恰好在该学习率区域达到最小值。这些结果在MNIST分类任务中得到初步验证，并定性地适用于一系列监督学习任务、学习架构和其他超参数。", "conclusion": "研究表明，通过定位在混沌的起点，可以加速人工神经网络的训练，揭示了瞬态混沌动力学在人工神经网络训练中出现的建设性作用。", "translation": "传统上，当面对监督学习任务时，优化人工神经网络的算法通常是利用型松弛动力学，例如梯度下降（GD）。在这里，我们探索了在非常大的学习率下神经网络训练轨迹的动力学。我们发现，对于某个学习率范围，GD优化从纯粹的利用型算法转变为探索-利用平衡的模式，因为神经网络仍然能够学习，但其轨迹显示出对初始条件的敏感依赖性——以正的网络最大Lyapunov指数为特征。有趣的是，在测试集中达到可接受准确性所需的特征训练时间恰好在该学习率区域达到最小值，这进一步表明通过定位在混沌的起点可以加速人工神经网络的训练。我们的结果——最初在MNIST分类任务中得到说明——定性地适用于一系列监督学习任务、学习架构和其他超参数，并展示了瞬态混沌动力学在人工神经网络训练中出现的建设性作用。", "summary": "本研究探索了在非常大的学习率下人工神经网络训练中梯度下降的动力学。发现当学习率达到一定范围时，优化过程从纯粹的利用转变为探索-利用平衡，此时训练轨迹表现出混沌特性（正Lyapunov指数）。重要的是，达到可接受准确率所需的训练时间在此混沌区域达到最小值，表明利用混沌的起点可以显著加速神经网络训练。该发现适用于多种监督学习任务和网络配置，揭示了瞬态混沌动力学在神经网络训练中的积极作用。", "keywords": "人工神经网络, 梯度下降, 混沌, 学习率, 训练加速", "comments": "这篇论文的创新点在于提出了利用混沌动力学来加速神经网络训练的新颖视角，突破了传统梯度下降仅作为利用型算法的局限。通过引入大步长，使得训练过程进入探索-利用平衡的混沌区域，显著缩短了训练时间。这为深度学习优化提供了一个全新的方向，可能启发更多基于动力学系统理论的优化方法。其重要性在于提供了一种普遍适用的加速训练策略，且对多种任务和架构均有效。潜在的局限性可能在于如何精确控制和利用这种“混沌边缘”，以及其在更复杂、更大规模模型上的稳定性与泛化能力。"}}
{"id": "2506.08887", "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval", "authors": ["Leqi Shen", "Guoqiang Gong", "Tianxiang Hao", "Tao He", "Yifeng Zhang", "Pengzhang Liu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.", "comment": "CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08887v1", "AI": {"title_translation": "DiscoVLA：用于参数高效视频文本检索的视觉、语言和对齐差异消减", "tldr": "DiscoVLA通过同时解决视觉、语言和对齐差异，改进了CLIP模型在视频文本检索中的参数高效适应性，并在MSRVTT上取得了显著提升。", "motivation": "现有方法在将图像文本预训练模型CLIP应用于视频文本检索时，主要关注视觉差异，而忽略了语言和对齐差异，导致视频级理解不足。", "method": "本文提出了DiscoVLA，通过引入图像-视频特征融合来整合图像级和视频级特征，解决视觉和语言差异。此外，生成伪图像字幕以学习细粒度图像级对齐。为缓解对齐差异，提出了图像到视频对齐蒸馏，利用图像级对齐知识增强视频级对齐。", "result": "广泛的实验证明了DiscoVLA的优越性。特别是在MSRVTT数据集上，使用CLIP (ViT-B/16)时，DiscoVLA在R@1指标上超越了现有方法1.5%，最终达到50.5%的R@1分数。", "conclusion": "DiscoVLA通过同时解决视觉、语言和对齐差异，显著提升了参数高效视频文本检索的性能，证明了全面解决这些差异的重要性。", "translation": "图像-文本预训练模型CLIP在视频-文本检索中的参数高效适应性是一个突出的研究领域。虽然CLIP专注于图像级的视觉-语言匹配，但视频-文本检索需要视频级的全面理解。从图像级到视频级的转换中出现了三个关键差异：视觉、语言和对齐。然而，现有方法主要关注视觉，而忽视了语言和对齐。在本文中，我们提出了视觉、语言和对齐差异消减（DiscoVLA），它同时缓解了所有这三个差异。具体来说，我们引入了图像-视频特征融合来整合图像级和视频级特征，有效解决了视觉和语言差异。此外，我们生成伪图像字幕以学习细粒度的图像级对齐。为了缓解对齐差异，我们提出了图像到视频对齐蒸馏，它利用图像级对齐知识来增强视频级对齐。广泛的实验证明了我们DiscoVLA的优越性。特别是在MSRVTT数据集上，使用CLIP (ViT-B/16)时，DiscoVLA在R@1指标上超越了现有方法1.5%，最终达到50.5%的R@1分数。代码可在https://github.com/LunarShen/DsicoVLA获取。", "summary": "本文提出DiscoVLA，旨在解决将图像文本预训练模型CLIP应用于视频文本检索时面临的视觉、语言和对齐三大差异。DiscoVLA通过图像-视频特征融合技术整合多级特征以解决视觉和语言差异，并利用伪图像字幕和图像到视频对齐蒸馏来提升图像级和视频级对齐。实验结果表明，DiscoVLA在MSRVTT数据集上取得了显著的性能提升，验证了其在参数高效视频文本检索中的有效性。", "keywords": "视频文本检索, 参数高效, CLIP, 差异消减, 多模态学习", "comments": "该论文的创新之处在于其全面性，它不仅关注视觉差异，还同时解决了语言和对齐差异，这在之前的研究中常被忽视。通过引入图像-视频特征融合和图像到视频对齐蒸馏等机制，DiscoVLA有效地将图像级知识迁移到视频级任务中，提高了参数高效视频文本检索的性能。其方法对于理解和解决多模态模型在不同粒度数据间迁移的挑战具有重要意义。"}}
{"id": "2506.08234", "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions", "authors": ["Yu-Ang Lee", "Guan-Ting Yi", "Mei-Yi Liu", "Jui-Chao Lu", "Guan-Bo Yang", "Yun-Nung Chen"], "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.", "comment": "15 pages, 4 figures, 1 table", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08234v1", "AI": {"title_translation": "复合AI系统优化：方法、挑战与未来方向综述", "tldr": "本文综述了复合AI系统优化领域的方法、挑战和未来方向，涵盖了数值和基于语言的技术。", "motivation": "随着大型语言模型（LLMs）和AI系统的发展，复合AI系统在执行复杂任务方面变得越来越熟练。然而，其日益增长的复杂性在优化单个组件及其交互方面带来了新的挑战，因此需要对该领域的最新进展进行系统回顾。", "method": "本文提供了一个系统综述，回顾了复合AI系统优化方面的最新进展，涵盖了数值和基于语言的技术。论文形式化了复合AI系统优化的概念，并根据几个关键维度对现有方法进行了分类。", "result": "本文形式化了复合AI系统优化的概念，分类了现有方法，并强调了该领域开放的研究挑战和未来的发展方向。", "conclusion": "复合AI系统优化是一个快速发展的领域，面临着优化复杂多组件交互的新挑战。本综述通过系统回顾、概念形式化和方法分类，为该领域的未来研究提供了清晰的路线图和有待解决的挑战。", "translation": "近期大型语言模型（LLMs）和AI系统的发展导致了复杂AI工作流设计和优化方面的范式转变。通过集成多个组件，复合AI系统在执行复杂任务方面变得越来越熟练。然而，随着这些系统复杂性的增长，在优化单个组件以及它们之间的交互方面出现了新的挑战。虽然监督微调（SFT）和强化学习（RL）等传统优化方法仍然是基础，但自然语言反馈的兴起引入了有前景的新方法，特别是对于优化不可微分系统。本文系统回顾了复合AI系统优化方面的最新进展，涵盖了数值和基于语言的技术。我们形式化了复合AI系统优化的概念，根据几个关键维度对现有方法进行了分类，并强调了在这个快速发展的领域中开放的研究挑战和未来方向。一份调查论文列表可在 https://github.com/MiuLab/AISysOpt-Survey 公开获取。", "summary": "本论文对复合AI系统优化领域进行了全面的系统综述。鉴于大型语言模型和AI系统在复杂任务中的应用日益广泛，以及随之而来的优化多组件交互的挑战，该综述形式化了复合AI系统优化的概念，分类了现有方法（包括数值和基于语言的技术），并指出了未来的研究方向和开放挑战，以促进该领域的发展。", "keywords": "复合AI系统优化, 大型语言模型, 自然语言反馈, 系统综述, AI工作流", "comments": "这篇综述论文的重要性在于它系统地梳理了一个新兴且关键的领域——复合AI系统优化。它不仅总结了现有技术，还识别了挑战并指明了未来方向，这对于研究人员理解和进入该领域具有重要的指导意义。特别是考虑到自然语言反馈在优化不可微分系统中的潜力，这篇综述为该领域的研究提供了宝贵的视角。"}}
{"id": "2506.08533", "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)", "authors": ["Nihal Acharya Adde", "Alexandra Gianzina", "Hanno Gottschalk", "Andreas Ebert"], "summary": "This paper introduces Evolutionary Multi-Objective Network Architecture\nSearch (EMNAS) for the first time to optimize neural network architectures in\nlarge-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses\ngenetic algorithms to automate network design, tailored to enhance rewards and\nreduce model size without compromising performance. Additionally,\nparallelization techniques are employed to accelerate the search, and\nteacher-student methodologies are implemented to ensure scalable optimization.\nThis research underscores the potential of transfer learning as a robust\nframework for optimizing performance across iterative learning processes by\neffectively leveraging knowledge from earlier generations to enhance learning\nefficiency and stability in subsequent generations. Experimental results\ndemonstrate that tailored EMNAS outperforms manually designed models, achieving\nhigher rewards with fewer parameters. The findings of these strategies\ncontribute positively to EMNAS for RL in autonomous driving, advancing the\nfield toward better-performing networks suitable for real-world scenarios.", "comment": "Published at ESANN 2025 Conference", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08533v1", "AI": {"title_translation": "强化学习中鲁棒的进化多目标网络架构搜索 (EMNAS-RL)", "tldr": "本文首次将进化多目标网络架构搜索 (EMNAS) 应用于自动驾驶大规模强化学习，以优化网络架构，实现更高奖励和更小模型，并采用并行化和师生方法加速和确保可扩展性。", "motivation": "优化大规模强化学习（特别是自动驾驶）中的神经网络架构，以提高奖励并减少模型大小，同时不影响性能。", "method": "本文引入EMNAS，使用遗传算法自动化网络设计，并采用并行化技术加速搜索，以及师生方法确保可扩展优化。研究还利用迁移学习，有效利用早期世代的知识来增强后续世代的学习效率和稳定性。", "result": "实验结果表明，定制的EMNAS优于手动设计的模型，以更少的参数获得了更高的奖励。", "conclusion": "EMNAS策略对自动驾驶领域的强化学习做出了积极贡献，推动了该领域向适用于实际场景的更高性能网络发展。", "translation": "本文首次引入进化多目标网络架构搜索（EMNAS），以优化大规模强化学习（RL）中用于自动驾驶（AD）的神经网络架构。EMNAS使用遗传算法自动化网络设计，旨在提高奖励并减少模型大小，同时不影响性能。此外，还采用了并行化技术来加速搜索，并实施了师生方法以确保可扩展优化。这项研究强调了迁移学习作为一种鲁棒框架的潜力，通过有效利用早期世代的知识来提高后续世代的学习效率和稳定性，从而在迭代学习过程中优化性能。实验结果表明，定制的EMNAS优于手动设计的模型，以更少的参数实现了更高的奖励。这些策略的发现对自动驾驶领域的强化学习EMNAS做出了积极贡献，推动该领域向适用于实际场景的更高性能网络发展。", "summary": "本文首次将进化多目标网络架构搜索（EMNAS）应用于自动驾驶的大规模强化学习，旨在通过遗传算法优化神经网络架构，以实现更高奖励和更小模型。研究通过并行化和师生方法加速并确保优化过程的可扩展性，并利用迁移学习提升学习效率和稳定性。实验证明，EMNAS在参数更少的情况下取得了比手动设计模型更高的性能，为强化学习在实际应用中设计高性能网络提供了有效途径。", "keywords": "进化多目标网络架构搜索, 强化学习, 自动驾驶, 遗传算法, 迁移学习", "comments": "这篇论文的创新点在于首次将EMNAS应用于大规模强化学习，特别是自动驾驶领域，并通过多目标优化、并行化和师生方法解决了网络架构搜索中的效率和可扩展性问题。其重要性在于提供了一种自动设计高性能、小尺寸神经网络的方法，这对于资源受限的实际强化学习部署（如自动驾驶）至关重要。"}}
{"id": "2506.08894", "title": "Product of Experts for Visual Generation", "authors": ["Yunzhi Zhang", "Carson Murtuza-Lanier", "Zizhang Li", "Yilun Du", "Jiajun Wu"], "summary": "Modern neural models capture rich priors and have complementary knowledge\nover shared data domains, e.g., images and videos. Integrating diverse\nknowledge from multiple sources -- including visual generative models, visual\nlanguage models, and sources with human-crafted knowledge such as graphics\nengines and physics simulators -- remains under-explored. We propose a Product\nof Experts (PoE) framework that performs inference-time knowledge composition\nfrom heterogeneous models. This training-free approach samples from the product\ndistribution across experts via Annealed Importance Sampling (AIS). Our\nframework shows practical benefits in image and video synthesis tasks, yielding\nbetter controllability than monolithic methods and additionally providing\nflexible user interfaces for specifying visual generation goals.", "comment": "Project page: https://product-of-experts.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08894v1", "AI": {"title_translation": "视觉生成领域的专家乘积模型", "tldr": "提出一种免训练的专家乘积（PoE）框架，通过退火重要性采样（AIS）整合异构模型知识，以提高图像和视频生成的可控性。", "motivation": "现代神经网络模型在共享数据领域（如图像和视频）捕获丰富的先验知识并具有互补性，但整合来自多种来源（包括视觉生成模型、视觉语言模型以及图形引擎和物理模拟器等人工知识来源）的异构知识仍未得到充分探索。", "method": "提出专家乘积（PoE）框架，在推理时进行知识组合。这是一种免训练方法，通过退火重要性采样（AIS）从专家乘积分布中采样。", "result": "该框架在图像和视频合成任务中显示出实际效益，比单一方法具有更好的可控性，并提供灵活的用户界面来指定视觉生成目标。", "conclusion": "专家乘积（PoE）框架通过整合异构模型知识，显著提高了视觉生成任务的可控性和用户交互灵活性，证明了其在实际应用中的潜力。", "translation": "现代神经网络模型捕获丰富的先验知识，并在共享数据领域（例如图像和视频）拥有互补知识。整合来自多个来源——包括视觉生成模型、视觉语言模型以及具有人工知识的来源（如图形引擎和物理模拟器）——的异构知识仍未得到充分探索。我们提出了一种专家乘积（PoE）框架，该框架在推理时执行来自异构模型的知识组合。这种免训练方法通过退火重要性采样（AIS）从专家乘积分布中进行采样。我们的框架在图像和视频合成任务中显示出实际效益，比单一方法具有更好的可控性，并额外提供了灵活的用户界面来指定视觉生成目标。", "summary": "本文提出了一种名为“专家乘积（PoE）”的框架，旨在解决视觉生成领域中整合异构知识的挑战。该框架无需训练，通过退火重要性采样（AIS）在推理时有效组合来自不同模型的知识，包括视觉生成模型、视觉语言模型以及人工知识源。实验结果表明，PoE框架在图像和视频合成任务中表现出优于传统单一方法的可控性，并提供了灵活的用户界面，使用户能更精确地定义生成目标。", "keywords": "专家乘积, 视觉生成, 知识组合, 退火重要性采样, 可控性", "comments": "这篇论文的创新点在于提出了一个免训练的专家乘积框架，用于在推理时整合来自多种异构模型的知识。这种方法提高了视觉生成的可控性，并为用户提供了更灵活的交互方式，这对于复杂视觉内容生成具有重要意义。其免训练的特性也降低了应用门槛。"}}
{"id": "2506.08235", "title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning", "authors": ["Shashidhar Reddy Javaji", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Nikhil Muralidhar", "Zining Zhu"], "summary": "Large language models (LLMs) are increasingly being used for complex research\ntasks such as literature review, idea generation, and scientific paper\nanalysis, yet their ability to truly understand and process the intricate\nrelationships within complex research papers, such as the logical links between\nclaims and supporting evidence remains largely unexplored. In this study, we\npresent CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'\ncapabilities in scientific claim-evidence extraction and validation, a task\nthat reflects deeper comprehension of scientific argumentation. We\nsystematically compare three approaches which are inspired by divide and\nconquer approaches, across six diverse LLMs, highlighting model-specific\nstrengths and weaknesses in scientific comprehension. Through evaluation\ninvolving over 300 claim-evidence pairs across multiple research domains, we\nreveal significant limitations in LLMs' ability to process complex scientific\ncontent. Our results demonstrate that closed-source models like GPT-4 and\nClaude consistently outperform open-source counterparts in precision and recall\nacross claim-evidence identification tasks. Furthermore, strategically designed\nthree-pass and one-by-one prompting approaches significantly improve LLMs'\nabilities to accurately link dispersed evidence with claims, although this\ncomes at increased computational cost. CLAIM-BENCH sets a new standard for\nevaluating scientific comprehension in LLMs, offering both a diagnostic tool\nand a path forward for building systems capable of deeper, more reliable\nreasoning across full-length papers.", "comment": "21 pages, 6 figures, Under review", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08235v1", "AI": {"title_translation": "AI能否验证科学？基准测试LLM以实现准确的科学主张→证据推理", "tldr": "本研究引入CLAIM-BENCH基准，评估大型语言模型（LLM）在科学主张-证据推理方面的能力，发现LLM在处理复杂科学内容方面存在显著局限性，但封闭源代码模型表现更好，且特定提示策略能提高准确性。", "motivation": "大型语言模型（LLM）在文献综述、思想生成和科学论文分析等研究任务中应用日益广泛，但它们理解和处理复杂研究论文中（如主张与支持证据之间的逻辑联系）错综复杂关系的能力仍未得到充分探索。", "method": "本研究提出了CLAIM-BENCH，一个用于评估LLM在科学主张-证据提取和验证能力方面的综合基准。研究系统地比较了三种受分而治之方法启发的策略，并对六个不同的LLM进行了评估。评估涉及300多对跨多个研究领域的主张-证据对。", "result": "结果表明，LLM在处理复杂科学内容方面存在显著局限性。GPT-4和Claude等封闭源代码模型在主张-证据识别任务的准确率和召回率方面始终优于开源模型。此外，精心设计的三遍和逐一提示方法显著提高了LLM准确链接分散证据与主张的能力，尽管这会增加计算成本。", "conclusion": "CLAIM-BENCH为评估LLM的科学理解能力设定了新标准，既提供了诊断工具，也为构建能够在完整论文中进行更深入、更可靠推理的系统指明了前进方向。", "translation": "大型语言模型（LLM）正越来越多地被用于复杂的科研任务，例如文献综述、想法生成和科学论文分析。然而，它们真正理解和处理复杂研究论文中错综复杂关系的能力，例如主张与支持证据之间的逻辑联系，仍未得到充分探索。在本研究中，我们提出了CLAIM-BENCH，一个用于评估LLM在科学主张-证据提取和验证能力方面的综合基准，这项任务反映了对科学论证的更深层次理解。我们系统地比较了三种受分而治之方法启发的策略，并对六个不同的LLM进行了评估，突出了模型在科学理解方面的特定优势和劣势。通过涉及跨多个研究领域的300多对主张-证据对的评估，我们揭示了LLM在处理复杂科学内容方面存在的显著局限性。我们的结果表明，GPT-4和Claude等封闭源代码模型在主张-证据识别任务的准确率和召回率方面始终优于开源模型。此外，精心设计的三遍和逐一提示方法显著提高了LLM准确链接分散证据与主张的能力，尽管这会增加计算成本。CLAIM-BENCH为评估LLM的科学理解能力设定了新标准，既提供了诊断工具，也为构建能够在完整论文中进行更深入、更可靠推理的系统指明了前进方向。", "summary": "本研究介绍了CLAIM-BENCH，这是一个评估大型语言模型（LLM）在科学主张-证据提取和验证方面能力的综合基准。通过对六个LLM和300多对主张-证据对的评估，研究发现LLM在处理复杂科学内容方面存在显著局限性。尽管如此，封闭源代码模型（如GPT-4和Claude）表现优异，且特定的多遍提示策略能显著提高准确性，但代价是计算成本增加。CLAIM-BENCH旨在成为诊断工具和未来构建更可靠科学推理系统的基础。", "keywords": "大型语言模型, 科学验证, 主张-证据推理, CLAIM-BENCH, 基准测试", "comments": "这篇论文的创新之处在于提出了一个专门用于评估LLM科学主张-证据推理能力的基准——CLAIM-BENCH，填补了LLM在深层科学理解方面评估空白。研究发现LLM在处理复杂科学内容上的局限性，这对于LMM在科学研究中的应用具有重要警示意义。同时，指出了封闭源代码模型的优势和特定提示策略的有效性，为LLM的改进提供了方向。其重要性在于为未来开发更可靠、更深入的科学推理AI系统奠定了基础。"}}
{"id": "2506.08551", "title": "DeepForm: Reasoning Large Language Model for Communication System Formulation", "authors": ["Panlong Wu", "Ting Wang", "Yifei Zhong", "Haoqi Zhang", "Zitong Wang", "Fangxin Wang"], "summary": "Communication system formulation is critical for advancing 6G and future\nwireless technologies, yet it remains a complex, expertise-intensive task.\nWhile Large Language Models (LLMs) offer potential, existing general-purpose\nmodels often lack the specialized domain knowledge, nuanced reasoning\ncapabilities, and access to high-quality, domain-specific training data\nrequired for adapting a general LLM into an LLM specially for communication\nsystem formulation. To bridge this gap, we introduce DeepForm, the first\nreasoning LLM specially for automated communication system formulation. We\npropose the world-first large-scale, open-source dataset meticulously curated\nfor this domain called Communication System Formulation Reasoning Corpus\n(CSFRC). Our framework employs a two-stage training strategy: first, Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;\nsecond, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based\non ReMax, to cultivate advanced modeling capabilities and elicit sophisticated\nreasoning patterns like self-correction and verification. Extensive experiments\ndemonstrate that our model achieves state-of-the-art performance, significantly\noutperforming larger proprietary LLMs on diverse senerios. We will release\nrelated resources to foster further research in this area after the paper is\naccepted.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08551v1", "AI": {"title_translation": "深度表单：用于通信系统公式化的推理大型语言模型", "tldr": "DeepForm是首个专为通信系统公式化设计的推理LLM，通过大规模数据集和两阶段训练（SFT+CoT，RL+C-ReMax）实现SOTA性能，超越现有LLM。", "motivation": "通信系统公式化对于推进6G和未来无线技术至关重要，但它仍然是一项复杂且需要专业知识的任务。现有通用LLM缺乏专业领域知识、细致的推理能力以及高质量、领域特定训练数据，无法适应此任务。", "method": "本文引入了DeepForm，这是首个专门用于自动化通信系统公式化的推理大型语言模型。为此，作者构建了世界上第一个大规模、开源的通信系统公式化推理语料库（CSFRC）。DeepForm采用两阶段训练策略：首先，使用思维链（CoT）数据进行监督微调（SFT）以提炼领域知识；其次，提出了一种基于ReMax的新颖的基于规则的强化学习（RL）算法C-ReMax，以培养高级建模能力并引出自我纠正和验证等复杂推理模式。", "result": "广泛的实验表明，DeepForm模型取得了最先进的性能，在各种场景下显著优于更大的专有LLM。", "conclusion": "DeepForm成功地将LLM应用于复杂的通信系统公式化任务，并通过其独特的数据集和训练策略实现了卓越的性能，解决了现有LLM在该领域面临的挑战。", "translation": "通信系统公式化对于推进6G和未来无线技术至关重要，但它仍然是一项复杂且需要专业知识的任务。尽管大型语言模型（LLM）提供了潜力，但现有通用模型通常缺乏将通用LLM调整为专门用于通信系统公式化的LLM所需的专业领域知识、细致的推理能力以及对高质量、领域特定训练数据的访问。为了弥合这一差距，我们引入了DeepForm，这是首个专门用于自动化通信系统公式化的推理LLM。我们提出了世界上第一个为此领域精心策划的大规模、开源数据集，称为通信系统公式化推理语料库（CSFRC）。我们的框架采用两阶段训练策略：首先，使用思维链（CoT）数据进行监督微调（SFT）以提炼领域知识；其次，一种基于ReMax的新颖的基于规则的强化学习（RL）算法C-ReMax，用于培养高级建模能力并引出自我纠正和验证等复杂推理模式。广泛的实验表明，我们的模型取得了最先进的性能，在各种场景下显著优于更大的专有LLM。论文被接受后，我们将发布相关资源以促进该领域的进一步研究。", "summary": "本文介绍了DeepForm，这是首个专为自动化通信系统公式化设计的推理大型语言模型。针对通用LLM在这一专业领域知识和推理能力不足的问题，作者构建了大规模开源数据集CSFRC，并提出了两阶段训练策略：首先通过SFT和CoT数据学习领域知识，然后利用基于ReMax的C-ReMax强化学习算法培养高级推理能力（如自我纠正）。实验证明DeepForm在性能上超越了现有的大型专有LLM。", "keywords": "通信系统公式化, 大型语言模型, 推理LLM, 强化学习, CSFRC", "comments": "DeepForm的创新点在于其是首个针对通信系统公式化任务的专用推理LLM。其重要性体现在解决了通用LLM在此领域专业知识和推理能力不足的痛点，并通过构建大规模领域特定数据集CSFRC和采用两阶段训练策略（SFT+CoT和C-ReMax RL）有效地提升了模型性能。特别是引入CoT和C-ReMax以培养复杂推理模式，有望推动LLM在特定工程领域的应用。"}}
{"id": "2506.08896", "title": "WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos", "authors": ["Negin Ghamsarian", "Raphael Sznitman", "Klaus Schoeffmann", "Jens Kowal"], "summary": "To meet the growing demand for systematic surgical training, wetlab\nenvironments have become indispensable platforms for hands-on practice in\nophthalmology. Yet, traditional wetlab training depends heavily on manual\nperformance evaluations, which are labor-intensive, time-consuming, and often\nsubject to variability. Recent advances in computer vision offer promising\navenues for automated skill assessment, enhancing both the efficiency and\nobjectivity of surgical education. Despite notable progress in ophthalmic\nsurgical datasets, existing resources predominantly focus on real surgeries or\nisolated tasks, falling short of supporting comprehensive skill evaluation in\ncontrolled wetlab settings. To address these limitations, we introduce WetCat,\nthe first dataset of wetlab cataract surgery videos specifically curated for\nautomated skill assessment. WetCat comprises high-resolution recordings of\nsurgeries performed by trainees on artificial eyes, featuring comprehensive\nphase annotations and semantic segmentations of key anatomical structures.\nThese annotations are meticulously designed to facilitate skill assessment\nduring the critical capsulorhexis and phacoemulsification phases, adhering to\nstandardized surgical skill assessment frameworks. By focusing on these\nessential phases, WetCat enables the development of interpretable, AI-driven\nevaluation tools aligned with established clinical metrics. This dataset lays a\nstrong foundation for advancing objective, scalable surgical education and sets\na new benchmark for automated workflow analysis and skill assessment in\nophthalmology training. The dataset and annotations are publicly available in\nSynapse https://www.synapse.org/Synapse:syn66401174/files.", "comment": "9 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08896v1", "AI": {"title_translation": "WetCat: 自动化湿实验室白内障手术视频中的技能评估", "tldr": "WetCat是首个针对湿实验室白内障手术视频的自动化技能评估数据集，旨在解决传统评估的局限性，提高外科培训的效率和客观性。", "motivation": "传统的湿实验室培训依赖手动、耗时且主观的性能评估，而现有眼科手术数据集未能支持在受控湿实验室环境中进行全面的技能评估。", "method": "研究团队引入了WetCat数据集，这是第一个专门为自动化技能评估而策划的湿实验室白内障手术视频数据集。它包含由受训者在人工眼上进行手术的高分辨率录像，并附有全面的阶段注释和关键解剖结构的语义分割，尤其关注撕囊和超声乳化阶段。", "result": "WetCat数据集已创建并公开可用，它能够促进开发与既定临床指标相符的可解释、AI驱动的评估工具。", "conclusion": "WetCat数据集为推进客观、可扩展的外科教育奠定了坚实基础，并为眼科培训中的自动化工作流程分析和技能评估树立了新基准。", "translation": "为了满足对系统性手术培训日益增长的需求，湿实验室环境已成为眼科实践中不可或缺的平台。然而，传统的湿实验室培训严重依赖手动性能评估，这既耗费人力、耗时，又常常存在变异性。计算机视觉的最新进展为自动化技能评估提供了有前景的途径，从而提高了外科教育的效率和客观性。尽管眼科手术数据集取得了显著进展，但现有资源主要侧重于真实手术或孤立任务，未能支持在受控湿实验室环境中进行全面的技能评估。为了解决这些局限性，我们推出了WetCat，这是第一个专门为自动化技能评估而策划的湿实验室白内障手术视频数据集。WetCat包含由受训者在人工眼上进行手术的高分辨率录像，具有全面的阶段注释和关键解剖结构的语义分割。这些注释经过精心设计，旨在促进在关键的撕囊和超声乳化阶段进行技能评估，并遵循标准化的手术技能评估框架。通过专注于这些基本阶段，WetCat能够开发出与既定临床指标相符的可解释的、AI驱动的评估工具。该数据集为推进客观、可扩展的外科教育奠定了坚实的基础，并为眼科培训中的自动化工作流程分析和技能评估树立了新基准。数据集和注释可在Synapse公开获取：https://www.synapse.org/Synapse:syn66401174/files。", "summary": "WetCat数据集旨在解决传统湿实验室白内障手术技能评估中手动、耗时且主观的局限性。该数据集是首个专门为自动化技能评估而设计的湿实验室白内障手术视频集，包含高分辨率录像、详细的阶段注释和关键结构语义分割，重点关注撕囊和超声乳化阶段。WetCat支持开发与临床指标一致的AI驱动评估工具，为眼科培训中的客观、可扩展的技能评估和工作流程分析奠定了基础。", "keywords": "WetCat, 白内障手术, 技能评估, 湿实验室, 计算机视觉", "comments": "这项工作通过创建首个湿实验室白内障手术视频数据集WetCat，填补了现有数据集的空白，为自动化手术技能评估提供了宝贵的资源。其创新性在于专注于受控环境下的综合技能评估，并提供详细的注释，有望显著提高外科培训的效率和客观性。该数据集的公开可用性也促进了该领域的研究和发展。"}}
{"id": "2506.08572", "title": "The Geometries of Truth Are Orthogonal Across Tasks", "authors": ["Waiss Azizian", "Michael Kirchhof", "Eugene Ndiaye", "Louis Bethune", "Michal Klein", "Pierre Ablin", "Marco Cuturi"], "summary": "Large Language Models (LLMs) have demonstrated impressive generalization\ncapabilities across various tasks, but their claim to practical relevance is\nstill mired by concerns on their reliability. Recent works have proposed\nexamining the activations produced by an LLM at inference time to assess\nwhether its answer to a question is correct. Some works claim that a \"geometry\nof truth\" can be learned from examples, in the sense that the activations that\ngenerate correct answers can be distinguished from those leading to mistakes\nwith a linear classifier. In this work, we underline a limitation of these\napproaches: we observe that these \"geometries of truth\" are intrinsically\ntask-dependent and fail to transfer across tasks. More precisely, we show that\nlinear classifiers trained across distinct tasks share little similarity and,\nwhen trained with sparsity-enforcing regularizers, have almost disjoint\nsupports. We show that more sophisticated approaches (e.g., using mixtures of\nprobes and tasks) fail to overcome this limitation, likely because activation\nvectors commonly used to classify answers form clearly separated clusters when\nexamined across tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08572v1", "AI": {"title_translation": "真理的几何形状在不同任务中是正交的", "tldr": "本文发现，用于评估大型语言模型（LLM）答案正确性的“真理几何形状”是任务依赖的，无法跨任务迁移，这限制了现有方法的应用。", "motivation": "大型语言模型（LLM）的实用性仍受其可靠性问题的困扰。近期工作提出通过检查LLM推理时的激活来评估其答案的正确性，并声称可以学习到一种“真理几何形状”，即通过线性分类器区分正确答案和错误答案的激活。本文旨在指出这些方法的局限性。", "method": "作者观察到“真理几何形状”本质上是任务依赖的，并且无法跨任务转移。他们更精确地展示了在不同任务上训练的线性分类器相似性很小，并且在施加稀疏强制正则化器时具有几乎不相交的支持。他们还尝试了更复杂的方法（例如，使用探针和任务的混合）来克服这一限制。", "result": "研究发现，用于评估LLM答案正确性的“真理几何形状”是固有的任务依赖性，并且无法跨任务转移。具体而言，在不同任务上训练的线性分类器相似性很小，并且在施加稀疏强制正则化器时，它们的支持几乎不相交。更复杂的方法（例如，使用探针和任务的混合）也未能克服这一限制，这可能是因为用于分类答案的激活向量在跨任务检查时形成了明显分离的簇。", "conclusion": "本文得出结论，用于评估大型语言模型答案正确性的“真理几何形状”是任务依赖的，并且无法跨任务迁移，这表明现有基于激活的方法在泛化能力上存在根本性限制。", "translation": "大型语言模型（LLM）在各种任务中表现出令人印象深刻的泛化能力，但其在实际应用中的相关性仍因对其可靠性的担忧而受阻。最近的工作提出检查LLM在推理时产生的激活，以评估其对问题的答案是否正确。一些工作声称可以从示例中学习到一种“真理几何形状”，即生成正确答案的激活可以与导致错误的激活通过线性分类器区分开来。在这项工作中，我们强调了这些方法的局限性：我们观察到这些“真理几何形状”本质上是任务依赖的，并且无法跨任务转移。更准确地说，我们表明在不同任务上训练的线性分类器共享很少的相似性，并且当使用稀疏强制正则化器训练时，它们的支持几乎不相交。我们表明，更复杂的方法（例如，使用探针和任务的混合）未能克服这一限制，这可能是因为通常用于分类答案的激活向量在跨任务检查时形成了明显分离的簇。", "summary": "本文研究了大型语言模型（LLM）中用于评估答案正确性的“真理几何形状”的跨任务泛化能力。研究发现，这种“真理几何形状”是任务依赖的，无法跨任务迁移。具体而言，在不同任务上训练的线性分类器显示出极低的相似性，并且在应用稀疏正则化时，其支持集几乎不相交。即使采用更复杂的探测方法也未能克服这一限制，这可能是由于激活向量在不同任务之间形成了明显分离的簇。", "keywords": "大型语言模型, 可靠性, 激活, 真理几何形状, 任务依赖", "comments": "本文揭示了当前LLM激活分析方法的一个重要局限性，即“真理几何形状”的任务依赖性。这对于依赖激活来评估LLM可靠性和可解释性的研究具有重要意义，表明未来的研究需要开发能够处理或克服这种任务特异性的新方法。其贡献在于明确指出了现有方法的边界，并为未来研究指明了方向。"}}
{"id": "2506.08900", "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis", "authors": ["José Morano", "Botond Fazekas", "Emese Sükei", "Ronald Fecso", "Taha Emre", "Markus Gumpinger", "Georg Faustmann", "Marzieh Oghbaie", "Ursula Schmidt-Erfurth", "Hrvoje Bogunović"], "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08900v1", "AI": {"title_translation": "MIRAGE：用于全面视网膜OCT图像分析的多模态基础模型和基准", "tldr": "MIRAGE是一个多模态基础模型，用于视网膜OCT图像分析，并在分类和分割任务上表现优于现有模型和方法。", "motivation": "AI在眼科图像分析中需要大量标注，且现有模型在独立、未见数据上表现不佳。尽管基础模型有潜力，但现有眼科基础模型缺乏广泛验证，特别是分割任务，且仅关注单一成像模态。", "method": "本文提出了MIRAGE，一个用于分析OCT和扫描激光眼底镜（SLO）图像的新型多模态基础模型。此外，还提出了一个包含OCT/SLO分类和分割任务的新评估基准，并与通用和专业基础模型以及分割方法进行了比较。", "result": "MIRAGE在分类和分割两类任务中均表现出优越性。", "conclusion": "MIRAGE适用于作为开发稳健视网膜OCT图像分析AI系统的基础。", "translation": "人工智能（AI）已成为辅助临床医生分析眼科图像（如光学相干断层扫描（OCT））的基本工具。然而，开发AI模型通常需要大量的标注，并且现有模型在独立的、未见过的数据上往往表现不佳。基础模型（FM）是经过大量未标记数据集训练的大型AI模型，在克服这些挑战方面显示出希望。尽管如此，现有的眼科基础模型缺乏广泛的验证，特别是对于分割任务，并且只关注单一成像模态。在此背景下，我们提出了MIRAGE，一种用于分析OCT和扫描激光眼底镜（SLO）图像的新型多模态FM。此外，我们提出了一个新的评估基准，包含OCT/SLO分类和分割任务。与通用和专业FM以及分割方法的比较表明，MIRAGE在这两类任务中均表现出优越性，突出了其作为开发稳健视网膜OCT图像分析AI系统基础的适用性。MIRAGE和评估基准均已公开：https://github.com/j-morano/MIRAGE。", "summary": "本文提出了MIRAGE，一个用于视网膜OCT和SLO图像分析的多模态基础模型，旨在解决现有AI模型在眼科图像分析中对大量标注的需求和在独立数据上的性能不足问题。研究团队还构建了一个新的评估基准，包含分类和分割任务。实验结果表明，MIRAGE在两类任务中均优于现有通用和专业基础模型以及分割方法，证明了其在开发稳健视网膜AI系统方面的潜力。", "keywords": "多模态基础模型, 视网膜OCT, 图像分析, 分类, 分割", "comments": "MIRAGE的创新之处在于其多模态（OCT和SLO）处理能力以及作为基础模型，旨在减少对大量标注数据的依赖。其提出的新评估基准也为未来研究提供了统一的比较平台。这对于眼科AI领域，特别是视网膜图像分析，具有重要意义，有望推动更稳健、泛化能力更强的AI系统发展。"}}
{"id": "2506.08574", "title": "SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models", "authors": ["Alvise Dei Rossi", "Matteo Metaldi", "Michal Bechny", "Irina Filchenko", "Julia van der Meer", "Markus H. Schmidt", "Claudio L. A. Bassetti", "Athina Tzovara", "Francesca D. Faraci", "Luigi Fiorillo"], "summary": "Despite advances in deep learning for automatic sleep staging, clinical\nadoption remains limited due to challenges in fair model evaluation,\ngeneralization across diverse datasets, model bias, and variability in human\nannotations. We present SLEEPYLAND, an open-source sleep staging evaluation\nframework designed to address these barriers. It includes more than 22'0000\nhours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain\n(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,\nand hardware setups. We release pre-trained models based on high-performing SoA\narchitectures and evaluate them under standardized conditions across single-\nand multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble\ncombining models across architectures and channel setups via soft voting.\nSOMNUS achieves robust performance across twenty-four different datasets, with\nmacro-F1 scores between 68.7% and 87.2%, outperforming individual models in\n94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including\ncases where compared models were trained ID while SOMNUS treated the same data\nas OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked\nto age, gender, AHI, and PLMI, showing that while ensemble improves robustness,\nno model architecture consistently minimizes bias in performance and clinical\nmarkers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,\nDOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on\nDOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus\nthan any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA\ncohorts). Finally, we introduce ensemble disagreement metrics - entropy and\ninter-model divergence based - predicting regions of scorer disagreement with\nROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.", "comment": "41 pages, 4 Figures, 7 Tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08574v1", "AI": {"title_translation": "SLEEPYLAND：信任始于对自动睡眠分期模型的公平评估", "tldr": "SLEEPYLAND是一个开源的睡眠分期评估框架，旨在解决自动睡眠分期模型评估中的挑战，并通过SOMNUS集成模型实现了在多数据集上的鲁棒性能，甚至超越了人类专家。", "motivation": "尽管深度学习在自动睡眠分期方面取得了进展，但由于公平模型评估、跨不同数据集的泛化能力、模型偏差以及人类标注的可变性等挑战，临床应用仍然有限。", "method": "本文提出了SLEEPYLAND，一个开源的睡眠分期评估框架，包含超过22万小时的域内（ID）和8.4万小时的域外（OOD）睡眠记录。研究发布了基于高性能SoA架构的预训练模型，并在单通道和多通道EEG/EOG配置下进行标准化评估。引入了SOMNUS，一个通过软投票结合不同架构和通道设置模型的集成方法。此外，还使用了集成不一致性指标（基于熵和模型间差异）来预测标注者不一致的区域。", "result": "SOMNUS在24个不同数据集上实现了鲁棒性能，宏观F1分数介于68.7%至87.2%之间，在94.9%的情况下优于单个模型。SOMNUS超越了先前的SoA方法，即使在SOMNUS将数据视为OOD而其他模型将其视为ID训练的情况下也是如此。在BSWR子集上量化了与年龄、性别、AHI和PLMI相关的模型偏差，发现集成虽然提高了鲁棒性，但没有模型架构能持续最小化性能和临床标记估计中的偏差。在OOD多标注数据集（DOD-H, DOD-O）上，SOMNUS超过了最佳人类评分者（DOD-H上MF1 85.2% vs 80.8%，DOD-O上80.2% vs 75.9%），并且比任何单个专家更能重现评分者共识。集成不一致性指标预测评分者不一致区域的ROC AUC高达0.828。", "conclusion": "SLEEPYLAND框架和SOMNUS集成模型为自动睡眠分期模型的公平评估提供了解决方案，显著提升了模型在多样化数据集上的泛化能力和鲁棒性，并能更好地捕捉人类标注者共识和不确定性。", "translation": "尽管深度学习在自动睡眠分期方面取得了进展，但由于公平模型评估、跨不同数据集的泛化能力、模型偏差以及人类标注的可变性等挑战，临床应用仍然有限。我们提出了SLEEPYLAND，一个旨在解决这些障碍的开源睡眠分期评估框架。它包括超过22万小时的域内（ID）睡眠记录和超过8.4万小时的域外（OOD）睡眠记录，涵盖了广泛的年龄、睡眠-觉醒障碍和硬件设置。我们发布了基于高性能SoA架构的预训练模型，并在单通道和多通道EEG/EOG配置下对它们进行了标准化评估。我们引入了SOMNUS，一个通过软投票结合不同架构和通道设置模型的集成方法。SOMNUS在24个不同数据集上实现了鲁棒性能，宏观F1分数介于68.7%至87.2%之间，在94.9%的情况下优于单个模型。值得注意的是，SOMNUS超越了先前的SoA方法，甚至包括在比较模型进行ID训练而SOMNUS将相同数据视为OOD处理的情况。使用BSWR数据集的一个子集（N=6'633），我们量化了与年龄、性别、AHI和PLMI相关的模型偏差，结果表明，尽管集成提高了鲁棒性，但没有模型架构能持续最小化性能和临床标记估计中的偏差。在OOD多标注数据集（DOD-H, DOD-O）上的评估中，SOMNUS超过了最佳人类评分者，例如在DOD-H上MF1为85.2%对比80.8%，在DOD-O上为80.2%对比75.9%，比任何单个专家都能更好地再现评分者共识（健康/OSA队列的k = 0.89/0.85和ACS = 0.95/0.94）。最后，我们引入了集成不一致性指标——基于熵和模型间差异——预测评分者不一致的区域，ROC AUC高达0.828，为人类不确定性提供了数据驱动的代理。", "summary": "本研究介绍了SLEEPYLAND，一个用于自动睡眠分期模型公平评估的开源框架，旨在解决现有评估挑战。该框架包含大量域内和域外睡眠数据，并发布了基于先进架构的预训练模型。研究提出了一种名为SOMNUS的集成模型，该模型通过软投票结合不同架构和通道设置，在24个数据集上表现出卓越的鲁棒性，宏观F1分数高达87.2%，并显著优于单个模型，甚至超越了人类专家。此外，研究还量化了模型偏差，并引入了集成不一致性指标，用于预测人类标注者不确定区域，为临床应用提供了更可信赖的工具。", "keywords": "自动睡眠分期, 模型评估, SLEEPYLAND, SOMNUS, 深度学习", "comments": "该论文通过构建一个大规模、多样化的评估框架SLEEPYLAND，并提出创新的SOMNUS集成模型，显著提升了自动睡眠分期模型的评估公平性和实际应用中的泛化能力与鲁棒性。其超越人类专家性能的成果令人印象深刻，并且对模型偏差的量化以及引入集成不一致性指标，都为未来可信赖的AI医疗应用提供了重要思路和工具。这项工作对于推动自动睡眠分期技术从实验室走向临床具有重要意义。"}}
{"id": "2506.08906", "title": "Hyperbolic Dual Feature Augmentation for Open-Environment", "authors": ["Peilin Yu", "Yuwei Wu", "Zhi Gao", "Xiaomeng Fan", "Shuo Yang", "Yunde Jia"], "summary": "Feature augmentation generates novel samples in the feature space, providing\nan effective way to enhance the generalization ability of learning algorithms\nwith hyperbolic geometry. Most hyperbolic feature augmentation is confined to\nclosed-environment, assuming the number of classes is fixed (\\emph{i.e.}, seen\nclasses) and generating features only for these classes. In this paper, we\npropose a hyperbolic dual feature augmentation method for open-environment,\nwhich augments features for both seen and unseen classes in the hyperbolic\nspace. To obtain a more precise approximation of the real data distribution for\nefficient training, (1) we adopt a neural ordinary differential equation\nmodule, enhanced by meta-learning, estimating the feature distributions of both\nseen and unseen classes; (2) we then introduce a regularizer to preserve the\nlatent hierarchical structures of data in the hyperbolic space; (3) we also\nderive an upper bound for the hyperbolic dual augmentation loss, allowing us to\ntrain a hyperbolic model using infinite augmentations for seen and unseen\nclasses. Extensive experiments on five open-environment tasks:\nclass-incremental learning, few-shot open-set recognition, few-shot learning,\nzero-shot learning, and general image classification, demonstrate that our\nmethod effectively enhances the performance of hyperbolic algorithms in\nopen-environment.", "comment": "arXiv admin note: text overlap with arXiv:2207.03824,\n  arXiv:2304.11855 by other authors", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08906v1", "AI": {"title_translation": "开放环境下双曲对偶特征增强", "tldr": "本文提出了一种用于开放环境的双曲对偶特征增强方法，通过增强已知和未知类别的特征来提高双曲算法的泛化能力。", "motivation": "大多数双曲特征增强方法局限于封闭环境，即假设类别数量固定且仅为已知类别生成特征。为了解决这一限制，本文提出了一种适用于开放环境的双曲对偶特征增强方法。", "method": "本文提出了一种双曲对偶特征增强方法，具体包括：1) 采用结合元学习的神经常微分方程模块，估计已知和未知类别的特征分布；2) 引入正则化器以保留数据在双曲空间中的潜在层次结构；3) 推导双曲对偶增强损失的上限，以实现对已知和未知类别的无限增强训练。", "result": "在类增量学习、少样本开集识别、少样本学习、零样本学习和通用图像分类这五项开放环境任务上的大量实验表明，该方法有效提升了双曲算法在开放环境中的性能。", "conclusion": "本文提出的双曲对偶特征增强方法，通过在双曲空间中对已知和未知类别进行特征增强，显著提高了双曲算法在各种开放环境任务中的性能和泛化能力。", "translation": "特征增强在特征空间中生成新样本，提供了一种有效的方法来增强双曲几何学习算法的泛化能力。大多数双曲特征增强局限于封闭环境，假设类别数量固定（即已知类别）并且仅为这些类别生成特征。在本文中，我们提出了一种用于开放环境的双曲对偶特征增强方法，该方法在双曲空间中增强已知和未知类别的特征。为了获得更精确的真实数据分布近似以进行高效训练，(1) 我们采用了一个由元学习增强的神经常微分方程模块，估计已知和未知类别的特征分布；(2) 我们随后引入了一个正则化器，以保留数据在双曲空间中的潜在层次结构；(3) 我们还推导了双曲对偶增强损失的上限，允许我们使用无限增强来训练已知和未知类别的双曲模型。在五项开放环境任务：类增量学习、少样本开集识别、少样本学习、零样本学习和通用图像分类上进行的广泛实验表明，我们的方法有效增强了双曲算法在开放环境中的性能。", "summary": "本文提出了一种新颖的双曲对偶特征增强方法，旨在解决现有双曲特征增强方法仅限于封闭环境的问题。该方法在双曲空间中同时增强已知和未知类别的特征，通过结合神经常微分方程模块、结构保留正则化器以及推导损失上限来实现高效训练和无限增强。实验结果表明，该方法显著提升了双曲算法在多种开放环境任务中的性能。", "keywords": "双曲几何, 特征增强, 开放环境, 神经常微分方程, 元学习", "comments": "这项工作在将双曲特征增强技术扩展到更具挑战性的开放环境方面具有创新性。它通过引入对未知类别的特征增强，并结合先进的建模和优化技术，有效地提升了模型的泛化能力。其贡献在于为处理现实世界中不断变化的类别数据提供了新的视角和方法。"}}
{"id": "2506.08577", "title": "Diffusion-based Time Series Forecasting for Sewerage Systems", "authors": ["Nicholas A. Pearson", "Francesca Cairoli", "Luca Bortolussi", "Davide Russo", "Francesca Zanello"], "summary": "We introduce a novel deep learning approach that harnesses the power of\ngenerative artificial intelligence to enhance the accuracy of contextual\nforecasting in sewerage systems. By developing a diffusion-based model that\nprocesses multivariate time series data, our system excels at capturing complex\ncorrelations across diverse environmental signals, enabling robust predictions\neven during extreme weather events. To strengthen the model's reliability, we\nfurther calibrate its predictions with a conformal inference technique,\ntailored for probabilistic time series data, ensuring that the resulting\nprediction intervals are statistically reliable and cover the true target\nvalues with a desired confidence level. Our empirical tests on real sewerage\nsystem data confirm the model's exceptional capability to deliver reliable\ncontextual predictions, maintaining accuracy even under severe weather\nconditions.", "comment": "Accepted for presentation at the 13th Urban Drainage Modelling\n  Conference, Innsbruck (Austria), September 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08577v1", "AI": {"title_translation": "基于扩散模型的污水系统时间序列预测", "tldr": "本文提出了一种新颖的基于扩散模型的深度学习方法，用于提高污水系统情境预测的准确性，即使在极端天气下也能提供可靠的预测，并通过共形推断校准预测结果。", "motivation": "为了提高污水系统情境预测的准确性，本文利用生成式人工智能的强大能力来处理多元时间序列数据并捕捉复杂的相关性，从而实现即使在极端天气事件中也能进行稳健预测。", "method": "本文开发了一种基于扩散模型的深度学习方法，用于处理多元时间序列数据，以捕捉不同环境信号之间的复杂相关性。此外，该模型通过为概率时间序列数据量身定制的共形推断技术进行预测校准，以确保预测区间在统计上可靠并以期望的置信水平覆盖真实目标值。", "result": "在真实的污水系统数据上的实证测试证实，该模型即使在恶劣天气条件下也能提供可靠的情境预测，并保持准确性。", "conclusion": "该研究提出的基于扩散模型的深度学习方法，结合共形推断技术，能够为污水系统提供准确且统计可靠的时间序列预测，即使在极端天气条件下也表现出色。", "translation": "我们引入了一种新颖的深度学习方法，利用生成式人工智能的力量来提高污水系统情境预测的准确性。通过开发一个处理多元时间序列数据的扩散模型，我们的系统擅长捕捉不同环境信号之间的复杂相关性，即使在极端天气事件中也能实现稳健预测。为了增强模型的可靠性，我们进一步使用为概率时间序列数据量身定制的共形推断技术校准其预测，确保所得的预测区间在统计上可靠，并以期望的置信水平覆盖真实目标值。我们对真实污水系统数据的实证测试证实了该模型提供可靠情境预测的卓越能力，即使在恶劣天气条件下也能保持准确性。", "summary": "本文介绍了一种新颖的深度学习方法，该方法利用生成式人工智能的扩散模型来提高污水系统情境预测的准确性。该模型能够处理多元时间序列数据并捕捉复杂相关性，即使在极端天气下也能提供稳健预测。通过共形推断技术校准预测结果，确保了预测区间的统计可靠性。在真实污水系统数据上的实证测试验证了该模型在恶劣天气条件下提供可靠情境预测的卓越能力。", "keywords": "扩散模型, 时间序列预测, 污水系统, 生成式AI, 共形推断", "comments": "本文的创新点在于将生成式人工智能中的扩散模型应用于污水系统的时间序列预测，并结合共形推断技术增强了预测的可靠性。这对于在复杂且受极端天气影响的环境中进行关键基础设施预测具有重要意义。该方法通过捕捉多元时间序列数据的复杂相关性，提升了预测的鲁棒性。"}}
{"id": "2506.08908", "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "authors": ["Jiajun Li", "Yue Ma", "Xinyu Zhang", "Qingyan Wei", "Songhua Liu", "Linfeng Zhang"], "summary": "Recent studies on Visual Autoregressive (VAR) models have highlighted that\nhigh-frequency components, or later steps, in the generation process contribute\ndisproportionately to inference latency. However, the underlying computational\nredundancy involved in these steps has yet to be thoroughly investigated. In\nthis paper, we conduct an in-depth analysis of the VAR inference process and\nidentify two primary sources of inefficiency: step redundancy and unconditional\nbranch redundancy. To address step redundancy, we propose an automatic\nstep-skipping strategy that selectively omits unnecessary generation steps to\nimprove efficiency. For unconditional branch redundancy, we observe that the\ninformation gap between the conditional and unconditional branches is minimal.\nLeveraging this insight, we introduce unconditional branch replacement, a\ntechnique that bypasses the unconditional branch to reduce computational cost.\nNotably, we observe that the effectiveness of acceleration strategies varies\nsignificantly across different samples. Motivated by this, we propose SkipVAR,\na sample-adaptive framework that leverages frequency information to dynamically\nselect the most suitable acceleration strategy for each instance. To evaluate\nthe role of high-frequency information, we introduce high-variation benchmark\ndatasets that test model sensitivity to fine details. Extensive experiments\nshow SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall\nacceleration and 2.62x speedup on the GenEval benchmark, maintaining model\nquality. These results confirm the effectiveness of frequency-aware,\ntraining-free adaptive acceleration for scalable autoregressive image\ngeneration. Our code is available at https://github.com/fakerone-li/SkipVAR and\nhas been publicly released.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08908v1", "AI": {"title_translation": "SkipVAR: 通过自适应频率感知跳过加速视觉自回归建模", "tldr": "SkipVAR通过识别并解决视觉自回归模型中的计算冗余，实现了显著加速并保持了图像生成质量。", "motivation": "视觉自回归（VAR）模型在生成后期步骤或高频分量中存在推断延迟过高的问题，且其潜在的计算冗余尚未被彻底研究。", "method": "本文深入分析VAR推断过程，识别出“步骤冗余”和“无条件分支冗余”两种低效率来源。为解决步骤冗余，提出自动步骤跳过策略；为解决无条件分支冗余，引入无条件分支替换技术。在此基础上，提出SkipVAR，一个样本自适应框架，利用频率信息动态选择最合适的加速策略。同时引入高变异基准数据集以评估高频信息的作用。", "result": "SkipVAR在保持模型质量（平均SSIM超过0.88）的同时，在GenEval基准上实现了高达1.81倍的整体加速和2.62倍的速度提升。", "conclusion": "频率感知、无需训练的自适应加速对于可扩展的自回归图像生成是有效的。", "translation": "最近对视觉自回归（VAR）模型的研究强调，生成过程中的高频分量或后期步骤对推断延迟造成了不成比例的贡献。然而，这些步骤中涉及的潜在计算冗余尚未被彻底研究。在本文中，我们对VAR推断过程进行了深入分析，并识别出两种主要的低效率来源：步骤冗余和无条件分支冗余。为了解决步骤冗余，我们提出了一种自动步骤跳过策略，选择性地省略不必要的生成步骤以提高效率。对于无条件分支冗余，我们观察到条件分支和无条件分支之间的信息差距很小。利用这一见解，我们引入了无条件分支替换，这是一种绕过无条件分支以降低计算成本的技术。值得注意的是，我们观察到加速策略的有效性在不同样本之间差异显著。受此启发，我们提出了SkipVAR，一个样本自适应框架，它利用频率信息动态选择最适合每个实例的加速策略。为了评估高频信息的作用，我们引入了高变异基准数据集，用于测试模型对精细细节的敏感性。大量实验表明，SkipVAR在保持模型质量的同时，实现了超过0.88的平均SSIM，在GenEval基准上实现了高达1.81倍的整体加速和2.62倍的速度提升。这些结果证实了频率感知、无需训练的自适应加速对于可扩展的自回归图像生成的有效性。我们的代码可在https://github.com/fakerone-li/SkipVAR 获取并已公开。", "summary": "本文深入分析了视觉自回归（VAR）模型推断过程中的计算冗余，识别出“步骤冗余”和“无条件分支冗余”。为解决这些问题，作者提出了自动步骤跳过策略和无条件分支替换技术。在此基础上，开发了SkipVAR框架，一个利用频率信息动态选择最佳加速策略的样本自适应方法。实验证明，SkipVAR在保持图像生成质量的同时，显著提高了VAR模型的推理速度。", "keywords": "视觉自回归模型, 推断加速, 频率感知, 自适应跳过, 计算冗余", "comments": "该论文创新性地从计算冗余的角度分析了视觉自回归模型的效率问题，并提出了针对性的解决方案。特别是其样本自适应的SkipVAR框架，通过频率感知动态调整加速策略，显著提升了模型的推理速度，同时维持了生成质量，对于可扩展的自回归图像生成具有重要意义。无需训练的特点也增加了其实用性。"}}
{"id": "2506.08260", "title": "Automatic Generation of Inference Making Questions for Reading Comprehension Assessments", "authors": ["Wanjing Anya Ma", "Michael Flor", "Zuowei Wang"], "summary": "Inference making is an essential but complex skill in reading comprehension\n(RC). Some inferences require resolving references across sentences, and some\nrely on using prior knowledge to fill in the detail that is not explicitly\nwritten in the text. Diagnostic RC questions can help educators provide more\neffective and targeted reading instruction and interventions for school-age\nstudents. We introduce a taxonomy of inference types for RC and use it to\nanalyze the distribution of items within a diagnostic RC item bank. Next, we\npresent experiments using GPT-4o to generate bridging-inference RC items for\ngiven reading passages via few-shot prompting, comparing conditions with and\nwithout chain-of-thought prompts. Generated items were evaluated on three\naspects: overall item quality, appropriate inference type, and LLM reasoning,\nachieving high inter-rater agreements above 0.90. Our results show that GPT-4o\nproduced 93.8% good-quality questions suitable for operational use in grade\n3-12 contexts; however, only 42.6% of the generated questions accurately\nmatched the targeted inference type. We conclude that combining automatic item\ngeneration with human judgment offers a promising path toward scalable,\nhigh-quality diagnostic RC assessments.", "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA 2025), co-located with the ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08260v1", "AI": {"title_translation": "阅读理解评估中推理题的自动生成", "tldr": "本文研究了使用GPT-4o自动生成阅读理解中的推理题，发现其能生成高质量题目，但推理类型匹配度有待提高，建议结合人工判断。", "motivation": "推理是阅读理解（RC）中一项重要但复杂的技能。诊断性阅读理解问题可以帮助教育工作者为学龄学生提供更有效和有针对性的阅读指导和干预。", "method": "本文引入了一种阅读理解推理类型的分类法，并用它来分析诊断性阅读理解题库中题目的分布。随后，使用GPT-4o通过少样本提示生成给定阅读段落的桥接推理阅读理解题目，并比较了有无思维链提示的条件。生成的题目在整体题目质量、适当的推理类型和LLM推理三个方面进行了评估。", "result": "GPT-4o生成了93.8%的优质题目，适合在3-12年级环境中使用；然而，只有42.6%的生成题目准确匹配了目标推理类型。", "conclusion": "结合自动题目生成与人工判断为可扩展、高质量的诊断性阅读理解评估提供了一条有前景的途径。", "translation": "推理是阅读理解（RC）中一项重要但复杂的技能。有些推理需要解决跨句的指代，有些则依赖于使用先验知识来填补文本中未明确写出的细节。诊断性阅读理解问题可以帮助教育工作者为学龄学生提供更有效和有针对性的阅读指导和干预。我们引入了一种阅读理解推理类型的分类法，并用它来分析诊断性阅读理解题库中题目的分布。接下来，我们介绍了使用GPT-4o通过少样本提示为给定阅读段落生成桥接推理阅读理解题目的实验，并比较了有无思维链提示的条件。生成的题目在三个方面进行了评估：整体题目质量、适当的推理类型和LLM推理，并获得了高于0.90的高评分者间一致性。我们的结果表明，GPT-4o生成了93.8%的优质题目，适合在3-12年级环境中使用；然而，只有42.6%的生成题目准确匹配了目标推理类型。我们得出结论，将自动题目生成与人工判断相结合为可扩展、高质量的诊断性阅读理解评估提供了一条有前景的途径。", "summary": "本文研究了使用GPT-4o自动生成阅读理解中的推理题。作者首先提出了推理类型的分类法，并利用GPT-4o通过少样本提示生成桥接推理题。实验结果显示，GPT-4o能生成高比例的优质题目（93.8%），但目标推理类型的匹配度较低（42.6%）。研究认为，结合自动生成与人工判断是实现可扩展、高质量诊断性阅读理解评估的有效途径。", "keywords": "阅读理解, 推理题生成, GPT-4o, 自动评估, 诊断性评估", "comments": "这项研究展示了大型语言模型（如GPT-4o）在教育评估领域，特别是在自动生成高质量诊断性阅读理解题目方面的巨大潜力。其创新之处在于将LLM应用于复杂推理题的生成，并提出了详细的评估标准。尽管在推理类型匹配度上存在局限性，但93.8%的题目质量足以支持实际应用，这表明了LLM在提高教育资源生产效率方面的价值。论文提出的结合人工判断的混合模式，为克服当前LLM的局限性并确保评估质量提供了实用的解决方案。"}}
{"id": "2506.08600", "title": "CALT: A Library for Computer Algebra with Transformer", "authors": ["Hiroshi Kera", "Shun Arakawa", "Yuta Sato"], "summary": "Recent advances in artificial intelligence have demonstrated the learnability\nof symbolic computation through end-to-end deep learning. Given a sufficient\nnumber of examples of symbolic expressions before and after the target\ncomputation, Transformer models - highly effective learners of\nsequence-to-sequence functions - can be trained to emulate the computation.\nThis development opens up several intriguing challenges and new research\ndirections, which require active contributions from the symbolic computation\ncommunity. In this work, we introduce Computer Algebra with Transformer (CALT),\na user-friendly Python library designed to help non-experts in deep learning\ntrain models for symbolic computation tasks.", "comment": "ISSAC 2025 Short Communications", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08600v1", "AI": {"title_translation": "CALT：一个基于Transformer的计算机代数库", "tldr": "CALT是一个Python库，旨在帮助非深度学习专家使用Transformer模型进行符号计算任务。", "motivation": "深度学习在符号计算方面的进展带来了新的挑战和研究方向，需要符号计算社区的积极贡献。然而，非深度学习专家可能难以有效利用Transformer模型进行相关任务。", "method": "本文介绍了CALT（Computer Algebra with Transformer），一个用户友好的Python库，专门设计用于帮助非深度学习专家训练用于符号计算任务的Transformer模型。", "result": "本文的结果是推出了CALT库，一个用于计算机代数与Transformer结合的Python库。", "conclusion": "CALT库的推出旨在降低非深度学习专家利用Transformer模型进行符号计算任务的门槛，从而促进该领域的研究和应用。", "translation": "人工智能的最新进展表明，符号计算可以通过端到端深度学习进行学习。给定足够数量的目标计算前后符号表达式的示例，Transformer模型——一种高效的序列到序列函数学习器——可以被训练来模拟该计算。这一发展带来了几个有趣的挑战和新的研究方向，需要符号计算社区的积极贡献。在这项工作中，我们介绍了CALT（Computer Algebra with Transformer），一个用户友好的Python库，旨在帮助非深度学习专家训练用于符号计算任务的模型。", "summary": "本文介绍了CALT（Computer Algebra with Transformer），一个用户友好的Python库，旨在帮助非深度学习专家利用Transformer模型进行符号计算任务。该库的开发旨在应对深度学习在符号计算中带来的新挑战和研究方向，促进更广泛的社区参与。", "keywords": "计算机代数, Transformer, 符号计算, 深度学习, Python库", "comments": "CALT的创新之处在于其作为连接深度学习和符号计算的桥梁，降低了非深度学习专家进入该领域的门槛。这对于促进符号计算社区在AI领域的参与和探索新的研究方向具有重要意义，因为它将复杂的模型训练过程封装为用户友好的接口。"}}
{"id": "2506.08915", "title": "Inherently Faithful Attention Maps for Vision Transformers", "authors": ["Ananthu Aniraj", "Cassio F. Dantas", "Dino Ienco", "Diego Marcos"], "summary": "We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08915v1", "AI": {"title_translation": "Vision Transformer的内在忠实注意力图", "tldr": "提出一种基于注意力机制的两阶段框架，通过学习二值注意力掩码确保只有相关区域影响预测，从而提高模型对虚假相关性和分布外背景的鲁棒性。", "motivation": "目标感知受上下文影响大，在分布外背景下易产生偏差表示。同时，许多图像级任务需要识别相关区域，但也常需要上下文信息，这产生了矛盾。", "method": "提出一个两阶段框架：第一阶段处理完整图像以发现目标部分并识别任务相关区域；第二阶段利用输入注意力掩码将感受野限制在这些区域，进行聚焦分析并过滤虚假信息。两阶段联合训练，第二阶段优化第一阶段。", "result": "在各种基准测试中，该方法显著提高了对虚假相关性和分布外背景的鲁棒性。", "conclusion": "该方法通过确保只有被关注的图像区域影响预测，有效解决了上下文偏差问题，提高了Vision Transformer的鲁棒性。", "translation": "我们引入了一种基于注意力的方法，该方法使用学习到的二值注意力掩码来确保只有被关注的图像区域影响预测。上下文会强烈影响目标感知，有时会导致有偏见的表示，特别是在目标出现在分布外背景中时。同时，许多图像级以目标为中心的任务需要识别相关区域，这通常需要上下文。为了解决这个难题，我们提出了一个两阶段框架：第一阶段处理完整图像以发现目标部分并识别任务相关区域，而第二阶段利用输入注意力掩码将其感受野限制在这些区域，从而实现聚焦分析，同时过滤掉潜在的虚假信息。两个阶段联合训练，允许第二阶段细化第一阶段。在各种基准测试中进行的广泛实验表明，我们的方法显著提高了对虚假相关性和分布外背景的鲁棒性。", "summary": "本文提出一种名为“内在忠实注意力图”的基于注意力机制的两阶段方法，用于Vision Transformer。该方法通过学习二值注意力掩码，确保仅被关注的图像区域影响预测，以解决上下文对目标感知的潜在偏差问题。第一阶段识别任务相关区域，第二阶段利用注意力掩码聚焦分析这些区域并过滤无关信息。联合训练的两阶段框架显著提高了模型对虚假相关性和分布外背景的鲁棒性。", "keywords": "注意力机制, Vision Transformer, 鲁棒性, 二值注意力掩码, 上下文偏差", "comments": "该论文创新性地提出了一种两阶段的注意力框架，通过强制性的二值注意力掩码确保了模型预测的“忠实性”，有效解决了Vision Transformer在复杂上下文和分布外背景下的鲁棒性问题。这种显式地限制感受野的方法对于提高模型可解释性和减少虚假相关性具有重要意义。"}}
{"id": "2506.08927", "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions", "authors": ["David Acuna", "Ximing Lu", "Jaehun Jung", "Hyunwoo Kim", "Amlan Kar", "Sanja Fidler", "Yejin Choi"], "summary": "Recent research in vision-language models (VLMs) has centered around the\npossibility of equipping them with implicit long-form chain-of-thought\nreasoning -- akin to the success observed in language models -- via\ndistillation and reinforcement learning. But what about the non-reasoning\nmodels already trained and deployed across the internet? Should we simply\nabandon them, or is there hope for a search mechanism that can elicit hidden\nknowledge and induce long reasoning traces -- without any additional training\nor supervision? In this paper, we explore this possibility using a Monte Carlo\nTree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer\npairs into the model's output stream. We show that framing reasoning as a\nsearch process -- where subquestions act as latent decisions within a broader\ninference trajectory -- helps the model \"connect the dots\" between fragmented\nknowledge and produce extended reasoning traces in non-reasoning models. We\nevaluate our method across three benchmarks and observe consistent\nimprovements. Notably, our approach yields a 2% overall improvement on\nMMMU-PRO, including a significant 9% gain in Liberal Arts.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08927v1", "AI": {"title_translation": "苏格拉底式-MCTS：通过提出正确问题进行测试时视觉推理", "tldr": "Socratic-MCTS提出了一种无需额外训练或监督的蒙特卡洛树搜索（MCTS）启发算法，通过注入子问题-子答案对，使现有非推理视觉-语言模型能够进行长篇推理，并在多个基准测试中取得了显著改进。", "motivation": "当前视觉-语言模型（VLMs）的研究主要集中于通过蒸馏和强化学习来赋予模型隐式长篇思维链推理能力。然而，对于互联网上已训练和部署的非推理模型，需要一种机制来在不进行额外训练或监督的情况下，诱导其进行长篇推理并挖掘隐藏知识。", "method": "本文提出了一种受蒙特卡洛树搜索（MCTS）启发的算法，称为Socratic-MCTS。该方法通过将子问题-子答案对注入到模型的输出流中，将推理视为一个搜索过程，其中子问题充当更广泛推理轨迹中的潜在决策，从而帮助模型连接碎片化知识并生成扩展的推理痕迹。", "result": "该方法在三个基准测试中均取得了持续改进。值得注意的是，在MMMU-PRO上总体提高了2%，其中在文科方面显著提高了9%。", "conclusion": "通过将推理框架为搜索过程，并注入子问题-子答案对，Socratic-MCTS能够使现有的非推理模型在不进行额外训练或监督的情况下实现长篇视觉推理，并在多个基准测试中展示了显著的性能提升。", "translation": "最近视觉-语言模型（VLMs）的研究主要集中于赋予它们隐式的长篇思维链推理能力——类似于语言模型中观察到的成功——通过蒸馏和强化学习。但是，对于互联网上已经训练和部署的非推理模型呢？我们应该简单地放弃它们，还是有一种搜索机制可以引出隐藏知识并诱导长篇推理痕迹——而无需任何额外的训练或监督？在本文中，我们探索了这种可能性，使用一种受蒙特卡洛树搜索（MCTS）启发的算法，该算法将子问题-子答案对注入到模型的输出流中。我们表明，将推理框定为搜索过程——其中子问题充当更广泛推理轨迹中的潜在决策——有助于模型“连接”碎片化知识，并在非推理模型中产生扩展的推理痕迹。我们在三个基准测试中评估了我们的方法，并观察到持续的改进。值得注意的是，我们的方法在MMMU-PRO上总体提高了2%，其中包括文科方面显著提高了9%。", "summary": "本文提出了Socratic-MCTS，一种受MCTS启发的算法，旨在使现有的非推理视觉-语言模型在测试时无需额外训练或监督即可进行长篇视觉推理。通过将推理视为一个搜索过程，并注入子问题-子答案对，该方法帮助模型整合碎片化知识。实验结果表明，Socratic-MCTS在多个基准测试中取得了持续改进，尤其在MMMU-PRO上总体提升2%，并在文科方面取得了9%的显著提升。", "keywords": "视觉推理, 蒙特卡洛树搜索, 视觉-语言模型, 思维链, 测试时推理", "comments": "Socratic-MCTS的创新之处在于它提供了一种无需重新训练或微调现有模型即可增强其推理能力的方法，这对于已部署的大量非推理模型具有重要意义。通过将推理过程转化为一个搜索问题，并巧妙地利用子问题引导模型生成更长的推理链，该方法为模型在测试时进行复杂推理提供了一条新途径。其“无需额外训练或监督”的特点是其核心优势，大大降低了应用门槛。"}}
{"id": "2506.08607", "title": "Sample Efficient Demonstration Selection for In-Context Learning", "authors": ["Kiran Purohit", "V Venktesh", "Sourangshu Bhattacharya", "Avishek Anand"], "summary": "The in-context learning paradigm with LLMs has been instrumental in advancing\na wide range of natural language processing tasks. The selection of few-shot\nexamples (exemplars / demonstration samples) is essential for constructing\neffective prompts under context-length budget constraints. In this paper, we\nformulate the exemplar selection task as a top-m best arms identification\nproblem. A key challenge in this setup is the exponentially large number of\narms that need to be evaluated to identify the m-best arms. We propose CASE\n(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient\nselective exploration strategy that maintains a shortlist of \"challenger\" arms,\nwhich are current candidates for the top-m arms. In each iteration, only one of\nthe arms from this shortlist or the current topm set is pulled, thereby\nreducing sample complexity and, consequently, the number of LLM evaluations.\nFurthermore, we model the scores of exemplar subsets (arms) using a\nparameterized linear scoring function, leading to stochastic linear bandits\nsetting. CASE achieves remarkable efficiency gains of up to 7x speedup in\nruntime while requiring 7x fewer LLM calls (87% reduction) without sacrificing\nperformance compared to state-of-the-art exemplar selection methods. We release\nour code and data at https://github.com/kiranpurohit/CASE", "comment": "Accepted at ICML 2025 , 24 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08607v1", "AI": {"title_translation": "上下文学习中高效示范选择", "tldr": "提出了一种名为CASE的新型采样高效选择探索策略，用于上下文学习中的示例选择，显著减少了LLM评估次数并提高了运行效率。", "motivation": "在上下文学习范式中，选择少量示例对于在上下文长度预算限制下构建有效提示至关重要。当前的主要挑战是需要评估的“臂”数量呈指数级增长，以识别最佳的m个“臂”。", "method": "将示例选择任务表述为识别前m个最佳“臂”的问题。提出了一种名为CASE（Challenger Arm Sampling for Exemplar selection）的采样高效选择探索策略。CASE维护一个“挑战者”臂的候选列表，每次迭代只从该列表或当前前m个集合中抽取一个臂，从而减少采样复杂性和LLM评估次数。此外，使用参数化线性评分函数对示例子集（臂）的得分进行建模，将其转化为随机线性多臂强盗设置。", "result": "CASE在运行时实现了高达7倍的加速，同时LLM调用次数减少了7倍（降低87%），且在性能上与最先进的示例选择方法相比没有牺牲。", "conclusion": "CASE是一种高效的示例选择方法，显著减少了上下文学习中LLM的评估成本和运行时间，同时保持了性能。", "translation": "大型语言模型（LLMs）的上下文学习范式在推动广泛的自然语言处理任务方面发挥了重要作用。少量示例（范例/演示样本）的选择对于在上下文长度预算限制下构建有效提示至关重要。在本文中，我们将示例选择任务表述为识别前m个最佳“臂”的问题。在这种设置中，一个关键挑战是需要评估的“臂”数量呈指数级增长，以识别最佳的m个“臂”。我们提出了CASE（Challenger Arm Sampling for Exemplar selection），这是一种新颖的采样高效选择探索策略，它维护一个“挑战者”臂的候选列表，这些是当前前m个臂的候选。在每次迭代中，只从该候选列表或当前前m个集合中抽取一个臂，从而减少了采样复杂性，进而减少了LLM评估的次数。此外，我们使用参数化线性评分函数对示例子集（臂）的得分进行建模，从而形成了随机线性多臂强盗设置。与最先进的示例选择方法相比，CASE在运行时实现了高达7倍的加速，同时LLM调用次数减少了7倍（降低87%），且在性能上没有牺牲。我们已在https://github.com/kiranpurohit/CASE发布了代码和数据。", "summary": "本文提出了一种名为CASE（Challenger Arm Sampling for Exemplar selection）的采样高效策略，用于大型语言模型（LLMs）的上下文学习中的示例选择。该方法将示例选择视为识别前m个最佳“臂”的问题，并通过维护一个“挑战者”臂列表并选择性地进行探索，显著减少了LLM评估次数和运行时间。实验结果表明，CASE在不牺牲性能的情况下，实现了高达7倍的加速和87%的LLM调用次数减少。", "keywords": "上下文学习, 示例选择, 采样效率, 大型语言模型, 多臂强盗", "comments": "这项工作在优化LLM的实际应用中具有重要意义，通过减少昂贵的LLM调用次数，极大地提高了上下文学习的效率。将示例选择建模为多臂强盗问题并引入“挑战者臂”的概念是其创新之处。该方法的通用性可能使其适用于其他需要高效样本选择的场景。"}}
{"id": "2506.08933", "title": "What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities", "authors": ["Wendong Bu", "Yang Wu", "Qifan Yu", "Minghe Gao", "Bingchen Miao", "Zhenkui Zhang", "Kaihang Pan", "Yunfei Li", "Mengze Li", "Wei Ji", "Juncheng Li", "Siliang Tang", "Yueting Zhuang"], "summary": "As multimodal large language models (MLLMs) advance, MLLM-based virtual\nagents have demonstrated remarkable performance. However, existing benchmarks\nface significant limitations, including uncontrollable task complexity,\nextensive manual annotation with limited scenarios, and a lack of\nmultidimensional evaluation. In response to these challenges, we introduce\nOmniBench, a self-generating, cross-platform, graph-based benchmark with an\nautomated pipeline for synthesizing tasks of controllable complexity through\nsubtask composition. To evaluate the diverse capabilities of virtual agents on\nthe graph, we further present OmniEval, a multidimensional evaluation framework\nthat includes subtask-level evaluation, graph-based metrics, and comprehensive\ntests across 10 capabilities. Our synthesized dataset contains 36k\ngraph-structured tasks across 20 scenarios, achieving a 91\\% human acceptance\nrate. Training on our graph-structured data shows that it can more efficiently\nguide agents compared to manually annotated data. We conduct multidimensional\nevaluations for various open-source and closed-source models, revealing their\nperformance across various capabilities and paving the way for future\nadvancements. Our project is available at https://omni-bench.github.io/.", "comment": "Accepted by ICML 2025 (Oral)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08933v1", "AI": {"title_translation": "虚拟智能体应用有何限制？OmniBench：一个可扩展的多维度基准，用于评估虚拟智能体的基本能力", "tldr": "引入OmniBench，一个自生成、跨平台、基于图的多维度基准测试平台，用于评估虚拟智能体的能力，解决了现有基准的局限性，并展示了其在高效指导智能体和揭示模型性能方面的潜力。", "motivation": "现有评估虚拟智能体的基准面临诸多限制，包括任务复杂度不可控、手动标注工作量大且场景有限，以及缺乏多维度评估。", "method": "论文提出了OmniBench，一个自生成、跨平台、基于图的基准测试平台，通过子任务组合的自动化流水线来合成可控复杂度的任务。此外，还提出了OmniEval，一个多维度评估框架，包括子任务级评估、基于图的指标和对10种能力的综合测试。", "result": "论文合成了一个包含3.6万个图结构任务的数据集，涵盖20个场景，人类接受率达到91%。在这些图结构数据上进行训练比手动标注数据能更有效地指导智能体。对各种开源和闭源模型进行多维度评估，揭示了它们在不同能力上的表现。", "conclusion": "OmniBench通过提供一个可扩展、多维度且高效的基准测试平台，解决了现有虚拟智能体评估的局限性，为未来的智能体能力提升铺平了道路。", "translation": "随着多模态大语言模型（MLLMs）的进步，基于MLLM的虚拟智能体展现出卓越的性能。然而，现有基准面临显著局限性，包括任务复杂度不可控、大量手动标注且场景有限，以及缺乏多维度评估。为应对这些挑战，我们引入了OmniBench，一个自生成、跨平台、基于图的基准测试平台，其自动化流水线通过子任务组合合成可控复杂度的任务。为了评估虚拟智能体在图上的多样化能力，我们进一步提出了OmniEval，一个多维度评估框架，包括子任务级评估、基于图的指标，以及对10种能力的综合测试。我们合成的数据集包含20个场景中的3.6万个图结构任务，实现了91%的人类接受率。在我们图结构数据上进行的训练表明，与手动标注数据相比，它能更有效地指导智能体。我们对各种开源和闭源模型进行了多维度评估，揭示了它们在各种能力上的表现，并为未来的进步铺平了道路。我们的项目可在 https://omni-bench.github.io/ 获取。", "summary": "本文针对当前虚拟智能体评估基准存在的任务复杂度不可控、标注成本高和缺乏多维度评估等问题，提出了OmniBench。OmniBench是一个自生成、跨平台、基于图的基准测试平台，通过自动化流水线合成可控复杂度的任务，并结合OmniEval多维度评估框架，全面评估智能体在10种能力上的表现。研究表明，其合成的3.6万个图结构任务数据集（人类接受率91%）能更高效地指导智能体，并通过对多模型的评估揭示了它们的性能，为未来智能体发展奠定了基础。", "keywords": "虚拟智能体, 基准测试, 多模态大语言模型, OmniBench, 多维度评估", "comments": "这篇论文的创新点在于提出了一个自生成、基于图的、可扩展的多维度基准测试平台OmniBench，有效解决了现有虚拟智能体评估基准的痛点。其自动化任务合成和多维度评估框架OmniEval具有重要意义，能够高效地评估和指导虚拟智能体的能力发展，特别是在数据生成和训练效率方面的优势显著。"}}
{"id": "2506.08277", "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Prachi Jindal", "Satya Sai Srinath Namburi", "Maneesh Singh", "Tanmoy Chakraborty", "Bapi S. Raju", "Manish Gupta"], "summary": "Recent voxel-wise multimodal brain encoding studies have shown that\nmultimodal large language models (MLLMs) exhibit a higher degree of brain\nalignment compared to unimodal models in both unimodal and multimodal stimulus\nsettings. More recently, instruction-tuned multimodal models have shown to\ngenerate task-specific representations that align strongly with brain activity.\nHowever, prior work evaluating the brain alignment of MLLMs has primarily\nfocused on unimodal settings or relied on non-instruction-tuned multimodal\nmodels for multimodal stimuli. To address this gap, we investigated brain\nalignment, that is, measuring the degree of predictivity of neural activity\nrecorded while participants were watching naturalistic movies (video along with\naudio) with representations derived from MLLMs. We utilized\ninstruction-specific embeddings from six video and two audio instruction-tuned\nMLLMs. Experiments with 13 video task-specific instructions show that\ninstruction-tuned video MLLMs significantly outperform non-instruction-tuned\nmultimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for\nboth video and audio tasks using language-guided instructions shows clear\ndisentanglement in task-specific representations from MLLMs, leading to precise\ndifferentiation of multimodal functional processing in the brain. We also find\nthat MLLM layers align hierarchically with the brain, with early sensory areas\nshowing strong alignment with early layers, while higher-level visual and\nlanguage regions align more with middle to late layers. These findings provide\nclear evidence for the role of task-specific instructions in improving the\nalignment between brain activity and MLLMs, and open new avenues for mapping\njoint information processing in both the systems. We make the code publicly\navailable [https://github.com/subbareddy248/mllm_videos].", "comment": "39 pages, 22 figures", "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.08277v1", "AI": {"title_translation": "指令调优的视频-音频模型阐明大脑中的功能特化", "tldr": "指令调优的视频-音频多模态大语言模型（MLLMs）与大脑活动对齐度更高，显示出任务特定的功能特化和分层对齐。", "motivation": "先前的多模态大语言模型（MLLMs）脑对齐研究主要集中在单模态设置或依赖非指令调优的多模态模型进行多模态刺激，这在理解指令调优的MLLMs与多模态脑活动的对齐方面存在空白。", "method": "研究人员通过测量参与者观看自然电影（视频和音频）时记录的神经活动与指令特定嵌入的MLLMs（包括六个视频和两个音频指令调优的MLLMs）所导出的表征之间的预测程度来调查大脑对齐。实验使用了13个视频任务特定的指令。", "result": "指令调优的视频MLLMs显著优于非指令调优的多模态模型（15%）和单模态模型（20%）。使用语言引导指令评估MLLMs的视频和音频任务表明，MLLMs的任务特定表征清晰解耦，从而精确区分大脑中的多模态功能处理。此外，MLLM层与大脑分层对齐，早期感觉区域与早期层强对齐，而高级视觉和语言区域与中后期层更对齐。", "conclusion": "任务特定的指令在改善大脑活动与多模态大语言模型（MLLMs）之间的对齐方面发挥着关键作用，并为绘制两个系统中的联合信息处理开辟了新途径。", "translation": "最近的体素级多模态脑编码研究表明，多模态大语言模型（MLLMs）在单模态和多模态刺激设置下，与单模态模型相比，展现出更高程度的脑对齐。最近，指令调优的多模态模型已被证明能生成与大脑活动高度对齐的任务特定表征。然而，评估MLLMs脑对齐的先前工作主要集中在单模态设置或依赖非指令调优的多模态模型进行多模态刺激。为了弥补这一空白，我们研究了脑对齐，即测量参与者观看自然电影（视频和音频）时记录的神经活动与MLLMs导出的表征之间的预测程度。我们利用了来自六个视频和两个音频指令调优的MLLMs的指令特定嵌入。针对13个视频任务特定指令的实验表明，指令调优的视频MLLMs显著优于非指令调优的多模态模型（15%）和单模态模型（20%）。我们使用语言引导指令对视频和音频任务的MLLMs进行评估，结果显示MLLMs的任务特定表征清晰解耦，从而精确区分大脑中的多模态功能处理。我们还发现MLLM层与大脑分层对齐，早期感觉区域与早期层强对齐，而高级视觉和语言区域与中后期层更对齐。这些发现为任务特定指令在改善大脑活动与MLLMs对齐方面的作用提供了清晰证据，并为绘制两个系统中的联合信息处理开辟了新途径。我们已将代码公开可用[https://github.com/subbareddy248/mllm_videos]。", "summary": "本文研究了指令调优的多模态大语言模型（MLLMs）在自然电影刺激下与大脑的对齐情况。研究发现，指令调优的MLLMs，特别是针对视频任务的模型，在预测神经活动方面显著优于非指令调优模型和单模态模型。研究还揭示，这些模型的任务特定表征表现出清晰的解耦，从而能够精确区分大脑中的多模态功能处理，并且MLLM层与大脑区域呈分层对齐。这项工作强调了任务特定指令对于改善大脑与MLLM对齐的重要性，并为理解联合信息处理开辟了新途径。", "keywords": "指令调优MLLMs, 脑对齐, 多模态处理, 功能特化, 自然电影", "comments": "该论文的创新之处在于利用指令调优的多模态大语言模型（MLLMs）进行多模态脑编码，展示了卓越的脑对齐能力，并阐明了功能特化。这对于理解大脑如何处理复杂的多模态信息以及开发更符合大脑工作方式的人工智能模型具有重要意义。"}}
{"id": "2506.08618", "title": "HSG-12M: A Large-Scale Spatial Multigraph Dataset", "authors": ["Xianquan Yan", "Hakan Akgün", "Kenji Kawaguchi", "N. Duane Loh", "Ching Hua Lee"], "summary": "Existing graph benchmarks assume non-spatial, simple edges, collapsing\nphysically distinct paths into a single link. We introduce HSG-12M, the first\nlarge-scale dataset of $\\textbf{spatial multigraphs}-$graphs embedded in a\nmetric space where multiple geometrically distinct trajectories between two\nnodes are retained as separate edges. HSG-12M contains 11.6 million static and\n5.1 million dynamic $\\textit{Hamiltonian spectral graphs}$ across 1401\ncharacteristic-polynomial classes, derived from 177 TB of spectral potential\ndata. Each graph encodes the full geometry of a 1-D crystal's energy spectrum\non the complex plane, producing diverse, physics-grounded topologies that\ntranscend conventional node-coordinate datasets. To enable future extensions,\nwe release $\\texttt{Poly2Graph}$: a high-performance, open-source pipeline that\nmaps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with\npopular GNNs expose new challenges in learning from multi-edge geometry at\nscale. Beyond its practical utility, we show that spectral graphs serve as\nuniversal topological fingerprints of polynomials, vectors, and matrices,\nforging a new algebra-to-graph link. HSG-12M lays the groundwork for\ngeometry-aware graph learning and new opportunities of data-driven scientific\ndiscovery in condensed matter physics and beyond.", "comment": "39 pages, 13 figures, 3 tables. Code & pipeline:\n  [https://github.com/sarinstein-yan/Poly2Graph] Dataset:\n  [https://github.com/sarinstein-yan/HSG-12M] Dataset released under CC BY 4.0", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08618v1", "AI": {"title_translation": "HSG-12M：一个大规模空间多图数据集", "tldr": "HSG-12M是一个新的大规模空间多图数据集，旨在解决现有图基准假设非空间、简单边的问题，并提供了将1-D晶体哈密顿量映射到谱图的开源工具。", "motivation": "现有图基准假设非空间、简单边，将物理上不同的路径合并为单个链接，这无法保留两个节点之间多个几何上不同的轨迹。", "method": "引入了HSG-12M，这是第一个大规模空间多图数据集，其中图嵌入在度量空间中，并保留了两个节点之间多个几何上不同的轨迹作为单独的边。HSG-12M包含1160万个静态和510万个动态哈密顿谱图，源自177TB的谱势数据。同时发布了Poly2Graph，一个高性能的开源管道，用于将任意1-D晶体哈密顿量映射到谱图。", "result": "HSG-12M数据集是第一个大规模空间多图数据集，包含多样化的、基于物理的拓扑结构。对流行GNN的基准测试揭示了从大规模多边几何中学习的新挑战。谱图被证明可以作为多项式、向量和矩阵的通用拓扑指纹，建立了代数到图的新链接。", "conclusion": "HSG-12M为几何感知图学习以及凝聚态物理及其他领域的数据驱动科学发现奠定了基础，并提供了新的机会。", "translation": "现有图基准假设非空间、简单边，将物理上不同的路径合并为单个链接。我们引入了HSG-12M，这是第一个大规模空间多图数据集——嵌入在度量空间中的图，其中两个节点之间多个几何上不同的轨迹被保留为单独的边。HSG-12M包含1160万个静态和510万个动态哈密顿谱图，跨越1401个特征多项式类别，源自177TB的谱势数据。每个图编码了复平面上1-D晶体能谱的完整几何形状，产生了超越传统节点坐标数据集的多样化、基于物理的拓扑结构。为了实现未来的扩展，我们发布了Poly2Graph：一个高性能的开源管道，用于将任意1-D晶体哈密顿量映射到谱图。对流行GNN的基准测试揭示了从大规模多边几何中学习的新挑战。除了其实用价值之外，我们还表明谱图可以作为多项式、向量和矩阵的通用拓扑指纹，建立了代数到图的新链接。HSG-12M为几何感知图学习以及凝聚态物理及其他领域的数据驱动科学发现奠定了基础。", "summary": "本文介绍了HSG-12M，一个解决现有图基准局限性的大规模空间多图数据集。该数据集包含1160万个静态和510万个动态哈密顿谱图，源自大量物理数据，编码了1-D晶体能谱的几何形状。为方便扩展，作者发布了开源工具Poly2Graph。研究表明，HSG-12M对图神经网络的学习提出了新挑战，并且谱图可作为多项式、向量和矩阵的通用拓扑指纹，为几何感知图学习和数据驱动科学发现奠定了基础。", "keywords": "空间多图, HSG-12M, 谱图, 图神经网络, 数据集", "comments": "本文的创新之处在于引入了HSG-12M，这是第一个大规模的空间多图数据集，它打破了传统图基准的简单边假设，保留了物理上更丰富的几何信息。其重要性在于为几何感知图学习提供了新的基础，并揭示了现有GNN在处理复杂多边几何数据时的局限性，从而推动了该领域的发展。同时，将谱图作为通用拓扑指纹的概念也为代数与图的连接开辟了新途径。"}}
{"id": "2506.08949", "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08949v1", "AI": {"title_translation": "SSS：基于高效提示的半监督SAM-2医学图像分割", "tldr": "SSS提出了一种基于SAM-2的半监督学习方法，通过判别性特征增强和物理约束滑动窗口提示生成，显著提升了医学图像分割性能。", "motivation": "在医学影像领域，有效利用大规模未标记数据并减少对高质量像素级标注的依赖是一个关键挑战。半监督学习（SSL）通过促进知识迁移来增强未标记数据的利用，显著提高了全监督模型的性能，是医学图像分析中一个非常有前景的研究方向。", "method": "本文提出了SSS（Semi-Supervised SAM-2），一种利用SAM-2强大特征提取能力来发现未标记医学图像中潜在知识的新方法。该方法基于单流“弱到强”一致性正则化框架，引入了判别性特征增强（DFE）机制，以进一步探索多视图不同数据增强策略引入的特征差异。通过利用多尺度增强技术中的特征相似性和不相似性，该方法重建并建模特征，从而有效优化显著区域。此外，还开发了一个结合物理约束与滑动窗口（PCSW）机制的提示生成器，为未标记数据生成输入提示，满足SAM-2对额外提示的要求。", "result": "广泛的实验证明了所提出的方法在两个多标签数据集ACDC和BHSD上进行半监督医学图像分割的优越性。值得注意的是，SSS在BHSD数据集上取得了53.15的平均Dice分数，超过了之前最先进的方法3.65 Dice。", "conclusion": "SSS通过结合SAM-2的强大能力、创新的特征增强机制和高效的提示生成器，在半监督医学图像分割方面取得了显著的SOTA性能，有效解决了利用未标记数据进行医学图像分析的挑战。", "translation": "在信息爆炸的时代，有效利用大规模未标记数据同时最大限度地减少对高质量像素级标注的依赖，仍然是医学影像领域的一个关键挑战。半监督学习（SSL）通过促进知识迁移来增强未标记数据的利用，显著提高了全监督模型的性能，并成为医学图像分析中一个极具前景的研究方向。受视觉基础模型（例如SAM-2）提供丰富先验知识能力的启发，我们提出了SSS（Semi-Supervised SAM-2），一种新颖的方法，它利用SAM-2强大的特征提取能力来发掘未标记医学图像中的潜在知识，从而有效增强全监督医学图像分割的特征支持。具体而言，本文在单流“弱到强”一致性正则化框架的基础上，引入了判别性特征增强（DFE）机制，以进一步探索多视图不同数据增强策略引入的特征差异。通过利用多尺度增强技术中的特征相似性和不相似性，该方法重建并建模特征，从而有效优化显著区域。此外，还开发了一个结合物理约束与滑动窗口（PCSW）机制的提示生成器，为未标记数据生成输入提示，满足SAM-2对额外提示的要求。广泛的实验证明了所提出的方法在两个多标签数据集（即ACDC和BHSD）上进行半监督医学图像分割的优越性。值得注意的是，SSS在BHSD上取得了53.15的平均Dice分数，超过了之前最先进的方法3.65 Dice。代码将可在https://github.com/AIGeeksGroup/SSS获取。", "summary": "本文提出了SSS（Semi-Supervised SAM-2），一种用于医学图像分割的半监督学习新范式。该方法利用SAM-2的强大特征提取能力，结合单流“弱到强”一致性正则化框架，并引入了判别性特征增强（DFE）机制来利用多视图数据增强带来的特征差异。此外，还开发了一个结合物理约束与滑动窗口（PCSW）的提示生成器，以满足SAM-2对未标记数据提示的需求。在ACDC和BHSD两个多标签数据集上的实验结果表明，SSS在半监督医学图像分割方面表现出优越性，特别是在BHSD上，其平均Dice分数达到53.15，超越了现有最先进方法3.65 Dice。", "keywords": "半监督学习, 医学图像分割, SAM-2, 判别性特征增强, 提示生成器", "comments": "该论文提出了一种新颖的半监督学习方法SSS，将Vision Foundation Model SAM-2引入到医学图像分割领域，利用其强大的特征提取能力。其创新点在于引入了判别性特征增强（DFE）机制来利用多视图特征差异，并设计了物理约束滑动窗口（PCSW）提示生成器来高效处理未标记数据。这些机制有效地解决了医学图像分割中数据标注成本高昂的问题，并显著提升了模型性能，具有重要的实际应用价值和研究意义。"}}
{"id": "2506.08641", "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers", "authors": ["Simon Roschmann", "Quentin Bouniot", "Vasilii Feofanov", "Ievgen Redko", "Zeynep Akata"], "summary": "Time series classification is a fundamental task in healthcare and industry,\nyet the development of time series foundation models (TSFMs) remains limited by\nthe scarcity of publicly available time series datasets. In this work, we\npropose Time Vision Transformer (TiViT), a framework that converts time series\ninto images to leverage the representational power of frozen Vision\nTransformers (ViTs) pretrained on large-scale image datasets. First, we\ntheoretically motivate our approach by analyzing the 2D patching of ViTs for\ntime series, showing that it can increase the number of label-relevant tokens\nand reduce the sample complexity. Second, we empirically demonstrate that TiViT\nachieves state-of-the-art performance on standard time series classification\nbenchmarks by utilizing the hidden representations of large OpenCLIP models. We\nexplore the structure of TiViT representations and find that intermediate\nlayers with high intrinsic dimension are the most effective for time series\nclassification. Finally, we assess the alignment between TiViT and TSFM\nrepresentation spaces and identify a strong complementarity, with further\nperformance gains achieved by combining their features. Our findings reveal yet\nanother direction for reusing vision representations in a non-visual domain.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08641v1", "AI": {"title_translation": "用于分类的时间序列表示隐藏在预训练的视觉 Transformer 中", "tldr": "TiViT 将时间序列转换为图像，利用预训练的 ViT 实现最先进的时间序列分类，证明 ViT 的隐藏表示功能强大且与 TSFM 互补。", "motivation": "时间序列基础模型（TSFM）的开发受到公开时间序列数据集稀缺的限制。", "method": "提出 TiViT 框架，将时间序列转换为图像，利用预训练的视觉 Transformer (ViT) 的表示能力。理论上分析 ViT 的 2D 分块对时间序列的影响，证明其能增加标签相关 token 数量并降低样本复杂度。", "result": "TiViT 在标准时间序列分类基准上实现了最先进的性能。发现具有高内在维度的中间层对时间序列分类最有效。TiViT 和 TSFM 表示空间之间存在强互补性，结合两者特征可进一步提高性能。", "conclusion": "研究结果揭示了在非视觉领域重用视觉表示的又一个方向。", "translation": "时间序列分类是医疗保健和工业领域的一项基本任务，但时间序列基础模型（TSFM）的开发仍受限于公开时间序列数据集的稀缺性。在这项工作中，我们提出了时间视觉 Transformer (TiViT)，这是一个将时间序列转换为图像以利用预训练在大型图像数据集上的冻结视觉 Transformer (ViT) 的表示能力的框架。首先，我们通过分析 ViT 对时间序列的 2D 分块，从理论上阐述了我们的方法，表明它可以增加标签相关 token 的数量并降低样本复杂度。其次，我们凭经验证明，TiViT 通过利用大型 OpenCLIP 模型的隐藏表示，在标准时间序列分类基准上实现了最先进的性能。我们探索了 TiViT 表示的结构，发现具有高内在维度的中间层对时间序列分类最有效。最后，我们评估了 TiViT 和 TSFM 表示空间之间的对齐程度，并确定了强大的互补性，通过结合它们的特征实现了进一步的性能提升。我们的发现揭示了在非视觉领域重用视觉表示的又一个方向。", "summary": "本文提出 TiViT 框架，通过将时间序列转换为图像，利用预训练的视觉 Transformer (ViT) 的表示能力，解决时间序列基础模型数据稀缺的问题。该方法在理论上证明了 ViT 2D 分块对时间序列分类的益处，并在经验上展示了 TiViT 在标准基准测试中达到最先进的性能。研究还发现，ViT 的中间层对时间序列分类最有效，且 TiViT 表示与时间序列基础模型具有很强的互补性，为在非视觉领域重用视觉表示提供了新方向。", "keywords": "时间序列分类, 视觉 Transformer, 表示学习, 基础模型, 跨领域迁移", "comments": "本文提出一种创新方法，通过创造性地重用预训练的视觉 Transformer，解决了时间序列基础模型的数据稀缺问题。将时间序列转换为图像以利用强大的视觉模型的想法非常新颖且具有影响力，为跨领域迁移学习开辟了新途径。理论和经验验证增强了其重要性，特别是发现了与现有时间序列基础模型的互补性。"}}
{"id": "2506.08953", "title": "Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF", "authors": ["Anirudh Nanduri", "Siyuan Huang", "Rama Chellappa"], "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nwide range of biometric tasks, including face and body recognition. In this\nwork, we adapt a ViT model pretrained on visible (VIS) imagery to the\nchallenging problem of cross-spectral body recognition, which involves matching\nimages captured in the visible and infrared (IR) domains. Recent ViT\narchitectures have explored incorporating additional embeddings beyond\ntraditional positional embeddings. Building on this idea, we integrate Side\nInformation Embedding (SIE) and examine the impact of encoding domain and\ncamera information to enhance cross-spectral matching. Surprisingly, our\nresults show that encoding only camera information - without explicitly\nincorporating domain information - achieves state-of-the-art performance on the\nLLCM dataset. While occlusion handling has been extensively studied in\nvisible-spectrum person re-identification (Re-ID), occlusions in\nvisible-infrared (VI) Re-ID remain largely underexplored - primarily because\nexisting VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly\nfeature full-body, unoccluded images. To address this gap, we analyze the\nimpact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain\nFace (IJB-MDF) dataset, which provides a diverse set of visible and infrared\nimages captured at various distances, enabling cross-range, cross-spectral\nevaluations.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08953v1", "AI": {"title_translation": "带旁信息嵌入的跨光谱人体识别：LLCM基准测试及IJB-MDF上距离引起的遮挡分析", "tldr": "本文将预训练的Vision Transformer模型应用于跨光谱人体识别，并引入旁信息嵌入（SIE）以增强匹配效果。研究发现，仅编码相机信息即可在LLCM数据集上达到SOTA性能。此外，论文还分析了IJB-MDF数据集上距离引起的遮挡对可见光-红外重识别的影响。", "motivation": "Vision Transformers（ViTs）在生物识别任务中表现出色，但跨光谱人体识别（可见光与红外图像匹配）仍具挑战性。现有可见光-红外重识别数据集多为无遮挡图像，导致距离引起的遮挡问题未被充分探索。", "method": "本文将一个在可见光（VIS）图像上预训练的Vision Transformer（ViT）模型应用于跨光谱人体识别。在此基础上，集成了旁信息嵌入（SIE），并检验了编码域信息和相机信息对增强跨光谱匹配的影响。此外，利用IARPA Janus Benchmark Multi-Domain Face（IJB-MDF）数据集分析了距离引起的遮挡影响。", "result": "令人惊讶的是，仅编码相机信息（未明确包含域信息）即可在LLCM数据集上实现最先进的性能。研究还分析了IJB-MDF数据集上距离引起的遮挡对可见光-红外重识别的影响。", "conclusion": "本文证明了Vision Transformer结合旁信息嵌入在跨光谱人体识别中的有效性，并指出仅相机信息编码即可达到SOTA性能。此外，强调了在现有数据集下，对距离引起的遮挡进行分析的重要性，为未来可见光-红外重识别研究提供了新方向。", "translation": "Vision Transformers (ViTs) 在包括人脸和人体识别在内的广泛生物识别任务中表现出令人印象深刻的性能。在这项工作中，我们将一个在可见光（VIS）图像上预训练的ViT模型应用于具有挑战性的跨光谱人体识别问题，该问题涉及匹配在可见光和红外（IR）域捕获的图像。最近的ViT架构探索了在传统位置嵌入之外引入额外嵌入。在此基础上，我们集成了旁信息嵌入（SIE）并检验了编码域和相机信息对增强跨光谱匹配的影响。令人惊讶的是，我们的结果显示，仅编码相机信息——而无需明确包含域信息——即可在LLCM数据集上实现最先进的性能。尽管遮挡处理在可见光谱行人重识别（Re-ID）中得到了广泛研究，但可见光-红外（VI）Re-ID中的遮挡仍未被充分探索——这主要是因为现有的VI-ReID数据集，例如LLCM、SYSU-MM01和RegDB，主要包含全身、无遮挡图像。为了解决这一差距，我们利用IARPA Janus Benchmark Multi-Domain Face（IJB-MDF）数据集分析了距离引起的遮挡影响，该数据集提供了在不同距离捕获的各种可见光和红外图像，从而能够进行跨距离、跨光谱评估。", "summary": "本文将预训练的Vision Transformer模型应用于具有挑战性的跨光谱人体识别任务，即匹配可见光和红外图像。通过集成旁信息嵌入（SIE），研究发现仅编码相机信息即可在LLCM数据集上实现最先进的性能。此外，鉴于现有可见光-红外重识别数据集在遮挡处理方面的不足，本文利用IJB-MDF数据集分析了距离引起的遮挡对跨光谱评估的影响，填补了该研究空白。", "keywords": "跨光谱人体识别, Vision Transformers, 旁信息嵌入, 遮挡分析, LLCM, IJB-MDF", "comments": "该论文在跨光谱人体识别领域具有创新性，特别是在旁信息嵌入的应用上。发现仅相机信息即可达到SOTA性能是一个令人惊讶且重要的结果，可能简化未来模型设计。此外，对距离引起的遮挡问题的探索弥补了现有数据集的不足，为可见光-红外重识别的实际应用提供了更全面的视角。"}}
{"id": "2506.08644", "title": "Semi-gradient DICE for Offline Constrained Reinforcement Learning", "authors": ["Woosung Kim", "JunHo Seo", "Jongmin Lee", "Byung-Jun Lee"], "summary": "Stationary Distribution Correction Estimation (DICE) addresses the mismatch\nbetween the stationary distribution induced by a policy and the target\ndistribution required for reliable off-policy evaluation (OPE) and policy\noptimization. DICE-based offline constrained RL particularly benefits from the\nflexibility of DICE, as it simultaneously maximizes return while estimating\ncosts in offline settings. However, we have observed that recent approaches\ndesigned to enhance the offline RL performance of the DICE framework\ninadvertently undermine its ability to perform OPE, making them unsuitable for\nconstrained RL scenarios. In this paper, we identify the root cause of this\nlimitation: their reliance on a semi-gradient optimization, which solves a\nfundamentally different optimization problem and results in failures in cost\nestimation. Building on these insights, we propose a novel method to enable OPE\nand constrained RL through semi-gradient DICE. Our method ensures accurate cost\nestimation and achieves state-of-the-art performance on the offline constrained\nRL benchmark, DSRL.", "comment": "Constrained Offline Reinforcement Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08644v1", "AI": {"title_translation": "离线约束强化学习的半梯度DICE", "tldr": "现有半梯度DICE方法在离线约束RL中进行OPE时成本估计失败，本文提出了改进的半梯度DICE方法，解决了这个问题并达到SOTA性能。", "motivation": "现有DICE框架中旨在提高离线RL性能的半梯度优化方法，无意中损害了其执行OPE的能力，导致成本估计失败，使其不适用于约束RL场景。", "method": "作者识别了现有半梯度DICE方法在离线约束RL中成本估计失败的根本原因（其依赖半梯度优化导致解决不同优化问题），并在此基础上提出了一种新颖的半梯度DICE方法，以实现准确的离策略评估（OPE）和约束强化学习。", "result": "该方法确保了准确的成本估计，并在离线约束RL基准DSRL上取得了最先进的性能。", "conclusion": "半梯度优化是现有DICE方法在约束强化学习中成本估计失败的根源，通过提出新的半梯度DICE方法，可以实现准确的成本估计并达到最先进的性能。", "translation": "策略引起的平稳分布与可靠的离策略评估（OPE）和策略优化所需的目标分布之间的不匹配是平稳分布校正估计（DICE）解决的问题。基于DICE的离线约束强化学习（RL）特别受益于DICE的灵活性，因为它在离线设置中同时最大化回报并估计成本。然而，我们观察到，旨在增强DICE框架离线RL性能的最新方法无意中损害了其执行OPE的能力，使其不适用于约束RL场景。在本文中，我们确定了这一限制的根本原因：它们依赖于半梯度优化，这解决了根本上不同的优化问题，并导致成本估计失败。基于这些见解，我们提出了一种新颖的方法，通过半梯度DICE实现OPE和约束RL。我们的方法确保了准确的成本估计，并在离线约束RL基准DSRL上取得了最先进的性能。", "summary": "本文研究了DICE（平稳分布校正估计）在离线约束强化学习中的应用。作者指出，现有旨在提高DICE离线RL性能的半梯度优化方法，因其解决不同优化问题而导致成本估计失败，从而不适用于约束RL场景。为解决此问题，论文提出了改进的半梯度DICE方法，该方法能确保准确的成本估计，并在DSRL离线约束RL基准上取得了最先进的性能。", "keywords": "离线强化学习, 约束强化学习, DICE, 半梯度优化, 成本估计", "comments": "这篇论文解决了DICE框架在离线约束强化学习中一个关键的实际问题，即半梯度优化导致的成本估计不准确。通过深入分析问题根源并提出改进方法，它不仅提升了DICE在约束RL场景下的适用性，也为未来相关研究提供了新的视角。其在DSRL基准上达到SOTA性能，证明了方法的有效性和重要性。"}}
{"id": "2506.08955", "title": "Segment Concealed Objects with Incomplete Supervision", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models.", "comment": "IEEE TPAMI", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08955v1", "AI": {"title_translation": "不完全监督下的隐匿物体分割", "tldr": "本文提出了SEE，一个统一的均值教师框架，结合SAM和伪标签策略，以及混合粒度特征分组模块，用于不完全监督的隐匿物体分割，实现了最先进的性能。", "motivation": "不完全监督隐匿物体分割（ISCOS）面临两大挑战：1) 不完全标注训练数据提供的监督有限；2) 由于隐匿场景中固有的相似性，难以将隐匿物体与背景区分开来。", "method": "本文提出了首个统一的ISCOS方法SEE。为解决不完全监督问题，SEE采用统一的均值教师框架，利用视觉基础模型SAM，通过教师模型生成的粗略掩码生成伪标签。为减轻低质量分割掩码的影响，引入了一系列伪标签生成、存储和监督策略，以确保鲁棒的网络训练。为解决固有相似性问题，设计了混合粒度特征分组模块，通过聚类相似特征来促进分割一致性，实现更完整的分割。", "result": "在多个ISCOS任务中验证了方法的有效性，实验结果表明所提出的方法实现了最先进的性能。此外，SEE可以作为即插即用解决方案，增强现有模型的性能。", "conclusion": "本文提出的统一方法SEE有效解决了不完全监督隐匿物体分割中的有限监督和固有相似性挑战，取得了最先进的性能，并具有即插即用能力。", "translation": "不完全监督隐匿物体分割 (ISCOS) 涉及分割那些无缝融入周围环境的物体，同时利用不完全标注的数据（例如弱标注和半标注）进行模型训练。这项任务仍然极具挑战性，原因在于 (1) 不完全标注的训练数据提供的监督有限，以及 (2) 由于隐匿场景中固有的相似性，难以将隐匿物体与背景区分开来。在本文中，我们引入了第一个用于 ISCOS 的统一方法来解决这些挑战。为了解决不完全监督的问题，我们提出了一个统一的均值教师框架 SEE，它利用视觉基础模型“Segment Anything Model (SAM)”以教师模型生成的粗略掩码作为提示来生成伪标签。为了减轻低质量分割掩码的影响，我们引入了一系列伪标签生成、存储和监督策略。这些策略旨在生成信息丰富的伪标签，存储生成的最佳伪标签，并选择最可靠的组件来指导学生模型，从而确保鲁棒的网络训练。此外，为了解决固有相似性问题，我们设计了一个混合粒度特征分组模块，该模块将不同粒度的特征进行分组并聚合这些结果。通过聚类相似特征，该模块促进了分割的一致性，有助于对单物体和多物体图像进行更完整的分割。我们在多个 ISCOS 任务中验证了我们方法的有效性，实验结果表明我们的方法实现了最先进的性能。此外，SEE 可以作为即插即用解决方案，增强现有模型的性能。", "summary": "本文解决了不完全监督隐匿物体分割 (ISCOS) 这一挑战性任务，其难点在于有限的监督和物体与背景固有的相似性。作者提出了 SEE，一个统一的均值教师框架，该框架利用 SAM 结合鲁棒的伪标签生成策略。为解决固有的相似性问题，引入了一个混合粒度特征分组模块。实验表明 SEE 实现了最先进的性能，并可作为即插即用的解决方案。", "keywords": "不完全监督隐匿物体分割, 均值教师框架, SAM, 伪标签, 特征分组", "comments": "该论文解决了在不完全监督下分割隐匿物体这一非常实际且具有挑战性的问题。其创新点在于将均值教师框架与强大的 SAM 相结合以生成伪标签，并辅以精心设计的策略来确保伪标签质量。混合粒度特征分组模块是解决固有相似性问题的另一个关键创新，这对于隐匿物体分割至关重要。SEE 的即插即用特性表明了其广泛适用性和对现有模型的潜在影响。"}}
{"id": "2506.08645", "title": "Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach", "authors": ["Youqi Wu", "Jingwei Zhang", "Farzan Farnia"], "summary": "Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved\npromising results in aligning representations across modalities. However, these\nembeddings could underperform compared to state-of-the-art single-modality\nembeddings on modality-specific tasks. On the other hand, single-modality\nembeddings excel in their domains but lack cross-modal alignment capabilities.\nIn this work, we focus on the problem of unifying cross-modality and\nsingle-modality embeddings to achieve the performance of modality-expert\nembedding within individual modalities while preserving cross-modal alignment.\nTo this end, we propose RP-KrossFuse, a method that leverages a random\nprojection-based Kronecker product to integrate cross-modal embeddings with\nsingle-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise\nsimilarity scores of the fused embeddings and operates efficiently in a\nspecified kernel space and supports scalable implementations via random Fourier\nfeatures for shift-invariant kernels such as the Gaussian kernel. We\ndemonstrate the effectiveness of RP-KrossFuse through several numerical\nexperiments, combining CLIP embeddings with uni-modal image and text\nembeddings. Our numerical results indicate that RP-KrossFuse achieves\ncompetitive modality-specific performance while retaining cross-modal\nalignment, bridging the gap between cross-modal and single-modality embeddings.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08645v1", "AI": {"title_translation": "融合跨模态和单模态表示：一种克罗内克积方法", "tldr": "本文提出了RP-KrossFuse，一种利用克罗内克积融合跨模态和单模态嵌入的方法，旨在在模态特定任务和跨模态对齐方面都达到良好性能。", "motivation": "跨模态嵌入在对齐方面表现出色但在模态特定任务上表现不佳，而单模态嵌入在各自领域表现优秀但缺乏跨模态对齐能力。本文旨在统一这两种嵌入，以同时实现模态专家级性能和跨模态对齐。", "method": "RP-KrossFuse是一种利用基于随机投影的克罗内克积来集成跨模态嵌入和单模态嵌入的方法。它旨在融合融合嵌入的样本对相似性分数，并在指定的核空间中高效运行，通过随机傅里叶特征支持可扩展实现，适用于高斯核等平移不变核。", "result": "数值实验表明，RP-KrossFuse在保持跨模态对齐的同时，实现了有竞争力的模态特定性能，弥合了跨模态和单模态嵌入之间的差距。", "conclusion": "RP-KrossFuse通过在保持跨模态对齐的同时实现有竞争力的模态特定性能，弥合了跨模态和单模态嵌入之间的差距。", "translation": "跨模态嵌入，如CLIP、BLIP及其变体，在对齐不同模态的表示方面取得了可喜的成果。然而，与最先进的单模态嵌入相比，这些嵌入在模态特定任务上可能表现不佳。另一方面，单模态嵌入在其领域内表现出色，但缺乏跨模态对齐能力。在这项工作中，我们专注于统一跨模态和单模态嵌入的问题，以在保持跨模态对齐的同时，在单个模态内达到模态专家嵌入的性能。为此，我们提出了RP-KrossFuse，一种利用基于随机投影的克罗内克积来集成跨模态嵌入和单模态嵌入的方法。RP-KrossFuse旨在融合融合嵌入的样本对相似性分数，并在指定的核空间中高效运行，通过随机傅里叶特征支持可扩展实现，适用于高斯核等平移不变核。我们通过结合CLIP嵌入与单模态图像和文本嵌入的多个数值实验证明了RP-KrossFuse的有效性。我们的数值结果表明，RP-KrossFuse在保持跨模态对齐的同时实现了有竞争力的模态特定性能，弥合了跨模态和单模态嵌入之间的差距。", "summary": "本文提出了一种名为RP-KrossFuse的新方法，该方法利用基于随机投影的克罗内克积有效结合跨模态和单模态嵌入。针对现有嵌入的局限性——跨模态嵌入缺乏模态特定能力，而单模态嵌入缺乏跨模态对齐——RP-KrossFuse旨在在这两个方面都实现强大性能。通过数值实验，该方法展示了其在保持关键跨模态对齐的同时，获得有竞争力的模态特定结果的能力，从而弥合了显著的性能差距。", "keywords": "跨模态嵌入, 单模态嵌入, 克罗内克积, 表示融合, RP-KrossFuse", "comments": "这篇论文解决了一个多模态学习中非常实际的问题，即跨模态对齐和模态特定性能之间的权衡。使用克罗内克积与随机投影在相似性分数级别融合表示是一种有趣的方法，可能提供灵活且可扩展的解决方案。"}}
{"id": "2506.08956", "title": "Data Augmentation For Small Object using Fast AutoAugment", "authors": ["DaeEun Yoon", "Semin Kim", "SangWook Yoo", "Jongha Lee"], "summary": "In recent years, there has been tremendous progress in object detection\nperformance. However, despite these advances, the detection performance for\nsmall objects is significantly inferior to that of large objects. Detecting\nsmall objects is one of the most challenging and important problems in computer\nvision. To improve the detection performance for small objects, we propose an\noptimal data augmentation method using Fast AutoAugment. Through our proposed\nmethod, we can quickly find optimal augmentation policies that can overcome\ndegradation when detecting small objects, and we achieve a 20% performance\nimprovement on the DOTA dataset.", "comment": "Accepted and published in the USB Proceedings of the 20th\n  International Conference on Modeling Decisions for Artificial Intelligence\n  (MDAI 2023), Ume{\\aa}, Sweden, June 19--22, 2023, ISBN 978-91-527-7293-5,\n  pp.\\ 12--21", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08956v1", "AI": {"title_translation": "使用Fast AutoAugment进行小目标数据增强", "tldr": "本文提出了一种使用Fast AutoAugment的优化数据增强方法，以解决小目标检测性能不佳的问题，并在DOTA数据集上实现了20%的性能提升。", "motivation": "尽管目标检测性能取得了显著进展，但小目标检测的性能仍远低于大目标。检测小目标是计算机视觉领域最具挑战性且重要的问题之一，因此需要改进其检测性能。", "method": "本文提出了一种利用Fast AutoAugment的优化数据增强方法。通过该方法，可以快速找到克服小目标检测性能下降的最佳增强策略。", "result": "通过所提出的方法，能够快速找到最佳增强策略，并在DOTA数据集上实现了20%的性能提升。", "conclusion": "所提出的基于Fast AutoAugment的数据增强方法能够有效提升小目标检测性能。", "translation": "近年来，目标检测性能取得了巨大进展。然而，尽管有这些进步，小目标的检测性能仍显著低于大目标。检测小目标是计算机视觉中最具挑战性且重要的问题之一。为了提高小目标的检测性能，我们提出了一种使用Fast AutoAugment的优化数据增强方法。通过我们提出的方法，我们可以快速找到能够克服检测小目标时性能下降的最佳增强策略，并且我们在DOTA数据集上实现了20%的性能提升。", "summary": "针对小目标检测性能远低于大目标的挑战，本文提出了一种利用Fast AutoAugment的优化数据增强方法。该方法旨在快速发现能够有效提升小目标检测性能的最佳数据增强策略，并在DOTA数据集上取得了20%的性能提升。", "keywords": "数据增强, 小目标检测, Fast AutoAugment, 目标检测, DOTA数据集", "comments": "本文的创新点在于将Fast AutoAugment应用于小目标检测的数据增强，有效解决了小目标检测中的性能下降问题。20%的性能提升表明了该方法在实际应用中的巨大潜力，对于提升遥感图像等领域的小目标检测精度具有重要意义。"}}
{"id": "2506.08652", "title": "JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset", "authors": ["Mahesh Godavarti"], "summary": "Transformers have demonstrated remarkable success in sequence modeling, yet\neffectively incorporating positional information remains a challenging and\nactive area of research. In this paper, we introduce JoFormer, a journey-based\nTransformer architecture grounded in a recently proposed non-commutative\nalgebra for composing transformations across positions. JoFormer represents\nrelative positions through learnable directional transforms that are\nsequentially composed along the input, thereby extending and generalizing\nexisting approaches based on relative position representations. We derive the\nJoFormer attention mechanism from first principles and show that it subsumes\nstandard methods such as rotary transformations as special cases. To evaluate\nits effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny\nShakespeare character-level language modeling task. Our results demonstrate\nthat\n  JoFormer consistently achieves lower perplexity and faster convergence,\nhighlighting the advantages of its more expressive, journey-based treatment of\nposition. Notably, the per-token JoFormer is still a primitive, conceptual\nvariant with layer-independent angles, yet it already demonstrates strong\nperformance-underscoring its promise as a proof of concept for more expressive\narchitectures. We conclude by discussing how JoFormer offers a principled\napproach to integrating positional structure into Transformer architectures.\nThe code used in this work is available at\nhttps://github.com/mahesh-godavarti/joformer.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08652v1", "AI": {"title_translation": "JoFormer（基于旅程的Transformer）：在Tiny Shakespeare数据集上的理论与实证分析", "tldr": "JoFormer是一种新的Transformer架构，通过可学习的方向性变换表示相对位置，在Tiny Shakespeare数据集上比RoFormer取得了更低的困惑度和更快的收敛速度。", "motivation": "尽管Transformer在序列建模中取得了巨大成功，但有效整合位置信息仍然是一个具有挑战性且活跃的研究领域。", "method": "本文引入了JoFormer，这是一种基于旅程的Transformer架构，它基于一种新提出的非交换代数来组合跨位置的变换。JoFormer通过可学习的方向性变换来表示相对位置，这些变换沿输入顺序组合，从而扩展和概括了现有方法。其注意力机制从第一性原理推导，并包含标准方法（如旋转变换）作为特例。", "result": "JoFormer在Tiny Shakespeare字符级语言建模任务上与RoFormer基线相比，始终实现了更低的困惑度（perplexity）和更快的收敛速度。即使是原始的、概念性的每token JoFormer变体也表现出强大的性能。", "conclusion": "JoFormer为将位置结构整合到Transformer架构中提供了一种原则性的方法，并证明了其作为更具表现力架构概念验证的潜力。", "translation": "Transformer在序列建模方面取得了显著成功，然而，有效整合位置信息仍然是一个具有挑战性且活跃的研究领域。在本文中，我们引入了JoFormer，这是一种基于旅程的Transformer架构，它基于最近提出的一种用于组合跨位置变换的非交换代数。JoFormer通过可学习的方向性变换来表示相对位置，这些变换沿输入顺序组合，从而扩展和概括了现有基于相对位置表示的方法。我们从第一性原理推导了JoFormer的注意力机制，并表明它将旋转变换等标准方法作为特例包含在内。为了评估其有效性，我们将JoFormer与RoFormer基线在Tiny Shakespeare字符级语言建模任务上进行了比较。我们的结果表明，JoFormer始终实现了更低的困惑度和更快的收敛速度，突显了其更具表现力的、基于旅程的位置处理的优势。值得注意的是，每token JoFormer仍然是一个原始的、概念性的变体，具有与层无关的角度，但它已经展示出强大的性能——这强调了其作为更具表现力架构概念验证的潜力。最后，我们讨论了JoFormer如何提供一种将位置结构整合到Transformer架构中的原则性方法。本工作使用的代码可在https://github.com/mahesh-godavarti/joformer获得。", "summary": "本文提出了一种名为JoFormer的新型Transformer架构，旨在有效整合序列建模中的位置信息。JoFormer基于一种非交换代数，通过学习可学习的方向性变换来表示和组合相对位置，从而概括并扩展了现有方法。实验表明，在Tiny Shakespeare数据集上，JoFormer相较于RoFormer基线，在字符级语言建模任务中表现出更低的困惑度和更快的收敛速度，证明了其基于旅程的位置处理方法的有效性和潜力。", "keywords": "Transformer, 位置编码, JoFormer, 序列建模, 相对位置", "comments": "JoFormer的创新之处在于其采用了一种基于非交换代数的“旅程式”方法来处理位置信息，这提供了一种新颖且原则性的方式来表示和组合相对位置，并能够概括现有方法（如旋转变换）。其在Tiny Shakespeare数据集上的实证结果显示出性能提升，证明了其作为更具表现力架构概念验证的潜力。"}}
{"id": "2506.08964", "title": "ORIDa: Object-centric Real-world Image Composition Dataset", "authors": ["Jinwoo Kim", "Sangmin Han", "Jinho Jeong", "Jiwoo Choi", "Dongyoung Kim", "Seon Joo Kim"], "summary": "Object compositing, the task of placing and harmonizing objects in images of\ndiverse visual scenes, has become an important task in computer vision with the\nrise of generative models. However, existing datasets lack the diversity and\nscale required to comprehensively explore real-world scenarios. We introduce\nORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,\nreal-captured dataset containing over 30,000 images featuring 200 unique\nobjects, each of which is presented across varied positions and scenes. ORIDa\nhas two types of data: factual-counterfactual sets and factual-only scenes. The\nfactual-counterfactual sets consist of four factual images showing an object in\ndifferent positions within a scene and a single counterfactual (or background)\nimage of the scene without the object, resulting in five images per scene. The\nfactual-only scenes include a single image containing an object in a specific\ncontext, expanding the variety of environments. To our knowledge, ORIDa is the\nfirst publicly available dataset with its scale and complexity for real-world\nimage composition. Extensive analysis and experiments highlight the value of\nORIDa as a resource for advancing further research in object compositing.", "comment": "Accepted at CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08964v1", "AI": {"title_translation": "ORIDa：以物体为中心的真实世界图像合成数据集", "tldr": "ORIDa是一个大型的、真实拍摄的图像合成数据集，包含3万多张图像和200个独特物体，旨在解决现有数据集缺乏多样性和规模的问题，以推动物体合成研究。", "motivation": "随着生成模型的兴起，物体合成已成为计算机视觉中的重要任务。然而，现有数据集缺乏探索真实世界场景所需的多样性和规模，这限制了该领域的研究进展。", "method": "本文介绍了ORIDa（以物体为中心的真实世界图像合成数据集），一个大规模、真实拍摄的数据集。它包含超过30,000张图像和200个独特的物体，每个物体都呈现在不同的位置和场景中。ORIDa有两种数据类型：事实-反事实集（每场景五张图像，包括有物体和无物体的图像）和仅事实场景（包含特定上下文中的物体图像）。", "result": "ORIDa是目前已知第一个具有如此规模和复杂性的公开真实世界图像合成数据集。广泛的分析和实验表明，ORIDa是推动物体合成进一步研究的宝贵资源。", "conclusion": "ORIDa数据集的引入为物体合成研究提供了急需的大规模、多样化的真实世界数据，有望显著推动该领域的发展，解决现有数据集的局限性。", "translation": "物体合成，即在不同视觉场景的图像中放置和协调物体的任务，随着生成模型的兴起已成为计算机视觉中的一项重要任务。然而，现有数据集缺乏全面探索真实世界场景所需的多样性和规模。我们引入了ORIDa（以物体为中心的真实世界图像合成数据集），一个大规模、真实拍摄的数据集，包含超过30,000张图像，涉及200个独特的物体，每个物体都呈现在不同的位置和场景中。ORIDa有两种类型的数据：事实-反事实集和仅事实场景。事实-反事实集包含四张在场景中不同位置显示物体的真实图像和一张没有物体的场景反事实（或背景）图像，每个场景共五张图像。仅事实场景包含一张在特定上下文中包含物体的图像，扩展了环境的多样性。据我们所知，ORIDa是第一个具有如此规模和复杂性的公开真实世界图像合成数据集。广泛的分析和实验突出了ORIDa作为推动物体合成进一步研究的资源的价值。", "summary": "本文介绍了ORIDa，一个大规模、真实拍摄的以物体为中心的图像合成数据集，旨在解决现有数据集在多样性和规模上的不足。ORIDa包含超过3万张图像和200个独特物体，数据分为事实-反事实集和仅事实场景，提供了丰富的真实世界物体合成数据。该数据集的发布有望推动物体合成领域的进一步研究。", "keywords": "物体合成, 图像合成, 数据集, 真实世界, 生成模型", "comments": "ORIDa数据集的创新之处在于其大规模和真实世界的特性，以及独特的事实-反事实数据结构，这对于训练和评估真实世界图像合成模型至关重要。它的发布填补了现有数据集的空白，将极大地促进计算机视觉领域中物体合成任务的研究进展。"}}
{"id": "2506.08968", "title": "ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations", "authors": ["Amirreza Rouhi", "Solmaz Arezoomandan", "Knut Peterson", "Joseph T. Woods", "David K. Han"], "summary": "Object detection models typically rely on predefined categories, limiting\ntheir ability to identify novel objects in open-world scenarios. To overcome\nthis constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,\na training-free, self-refining framework for open-world object labeling. ADAM\nleverages large language models (LLMs) to generate candidate labels for unknown\nobjects based on contextual information from known entities within a scene.\nThese labels are paired with visual embeddings from CLIP to construct an\nEmbedding-Label Repository (ELR) that enables inference without category\nsupervision. For a newly encountered unknown object, ADAM retrieves visually\nsimilar instances from the ELR and applies frequency-based voting and\ncross-modal re-ranking to assign a robust label. To further enhance\nconsistency, we introduce a self-refinement loop that re-evaluates repository\nlabels using visual cohesion analysis and k-nearest-neighbor-based majority\nre-labeling. Experimental results on the COCO and PASCAL datasets demonstrate\nthat ADAM effectively annotates novel categories using only visual and\ncontextual signals, without requiring any fine-tuning or retraining.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08968v1", "AI": {"title_translation": "ADAM：使用大型语言模型进行上下文感知标注的自主发现和标注模型", "tldr": "ADAM是一个无需训练、自完善的框架，利用大型语言模型和视觉嵌入实现开放世界对象检测的上下文感知标注，无需预定义类别或微调。", "motivation": "现有对象检测模型依赖预定义类别，限制了其在开放世界场景中识别新颖对象的能力。", "method": "ADAM是一个无需训练、自完善的开放世界对象标注框架。它利用大型语言模型（LLMs）根据场景中已知实体的上下文信息生成未知对象的候选标签。这些标签与来自CLIP的视觉嵌入配对，构建一个嵌入-标签存储库（ELR），从而实现无需类别监督的推理。对于新遇到的未知对象，ADAM从ELR中检索视觉相似的实例，并应用基于频率的投票和跨模态重排序来分配鲁棒的标签。为了增强一致性，引入了一个自完善循环，通过视觉凝聚力分析和基于k-最近邻的多数重新标注来重新评估存储库标签。", "result": "在COCO和PASCAL数据集上的实验结果表明，ADAM仅使用视觉和上下文信号就能有效标注新颖类别，无需任何微调或再训练。", "conclusion": "ADAM成功地通过提供一个无需训练、自完善的开放世界对象标注框架，解决了对象检测中预定义类别的限制。", "translation": "对象检测模型通常依赖预定义类别，这限制了它们在开放世界场景中识别新颖对象的能力。为了克服这一限制，我们引入了ADAM：自主发现和标注模型，这是一个无需训练、自完善的开放世界对象标注框架。ADAM利用大型语言模型（LLMs）根据场景中已知实体的上下文信息为未知对象生成候选标签。这些标签与来自CLIP的视觉嵌入配对，以构建一个嵌入-标签存储库（ELR），从而实现无需类别监督的推理。对于新遇到的未知对象，ADAM从ELR中检索视觉相似的实例，并应用基于频率的投票和跨模态重排序来分配鲁棒的标签。为了进一步增强一致性，我们引入了一个自完善循环，通过视觉凝聚力分析和基于k-最近邻的多数重新标注来重新评估存储库标签。在COCO和PASCAL数据集上的实验结果表明，ADAM仅使用视觉和上下文信号就能有效标注新颖类别，无需任何微调或再训练。", "summary": "ADAM是一种无需训练、自完善的开放世界对象标注框架，旨在解决传统对象检测模型对预定义类别的依赖问题。它利用大型语言模型生成未知对象的上下文感知标签，并结合CLIP视觉嵌入构建嵌入-标签存储库。通过频率投票、跨模态重排序和自完善循环，ADAM能够有效地为新颖类别分配鲁棒标签，无需任何微调，并在COCO和PASCAL数据集上表现出有效性。", "keywords": "开放世界对象检测, 大型语言模型, 上下文感知标注, 无需训练, 自完善", "comments": "ADAM的创新之处在于其无需训练的自完善框架，通过结合大型语言模型和视觉嵌入，实现了开放世界中新颖对象的上下文感知标注。这种方法克服了传统检测模型对预定义类别的限制，为开放世界对象检测提供了一种新颖且高效的解决方案，尤其是在无需微调或再训练方面具有重要意义。"}}
{"id": "2506.08660", "title": "Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness", "authors": ["Jinkwan Jang", "Hyungjin Park", "Jinmyeong Choi", "Taesup Kim"], "summary": "Real-world time series data are inherently multivariate, often exhibiting\ncomplex inter-channel dependencies. Each channel is typically sampled at its\nown period and is prone to missing values due to various practical and\noperational constraints. These characteristics pose fundamental challenges\nrelated to channel dependency, sampling asynchrony, and missingness, all of\nwhich must be addressed to enable robust and reliable forecasting in practical\nsettings. However, most existing architectures are built on oversimplified\nassumptions, such as identical sampling periods across channels and fully\nobserved inputs at test time, which often do not hold in real-world scenarios.\nTo bridge this gap, we propose ChannelTokenFormer, a Transformer-based\nforecasting model with a flexible architecture designed to explicitly capture\ncross-channel interactions, accommodate channel-wise asynchronous sampling, and\neffectively handle missing values. Extensive experiments on three benchmark\ndatasets modified to reflect practical settings, along with one real-world\nindustrial dataset, demonstrate the superior robustness and accuracy of\nChannelTokenFormer under challenging real-world conditions.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08660v1", "AI": {"title_translation": "迈向鲁棒的真实世界多元时间序列预测：一个统一的依赖性、异步性和缺失性处理框架", "tldr": "ChannelTokenFormer是一个基于Transformer的模型，用于处理真实世界多元时间序列预测中的通道依赖性、异步采样和缺失值问题，并在挑战性条件下表现出卓越的鲁棒性和准确性。", "motivation": "真实世界的时间序列数据通常是多元的，具有复杂的通道间依赖性，每个通道的采样周期不同，且容易出现缺失值。现有的大多数架构都基于简化假设（如相同采样周期和完全观测输入），这在真实世界场景中往往不成立，导致无法实现鲁棒可靠的预测。", "method": "本文提出了ChannelTokenFormer，一个基于Transformer的预测模型。它具有灵活的架构，旨在明确捕获跨通道交互、适应通道级异步采样并有效处理缺失值。", "result": "在三个修改后的基准数据集和一个真实世界工业数据集上进行的大量实验表明，ChannelTokenFormer在挑战性的真实世界条件下表现出卓越的鲁棒性和准确性。", "conclusion": "ChannelTokenFormer通过统一处理依赖性、异步性和缺失性，为真实世界多元时间序列预测提供了一个鲁棒且准确的解决方案。", "translation": "真实世界的时间序列数据本质上是多元的，通常表现出复杂的通道间依赖性。每个通道通常以其自己的周期采样，并且由于各种实际和操作限制而容易出现缺失值。这些特征带来了与通道依赖性、采样异步性和缺失性相关的基本挑战，所有这些都必须解决才能在实际设置中实现鲁棒可靠的预测。然而，大多数现有架构都建立在过于简化的假设之上，例如通道间相同的采样周期和测试时完全观测的输入，这在真实世界场景中通常不成立。为了弥合这一差距，我们提出了ChannelTokenFormer，一个基于Transformer的预测模型，其灵活的架构旨在明确捕获跨通道交互、适应通道级异步采样并有效处理缺失值。在三个修改以反映实际设置的基准数据集以及一个真实世界工业数据集上进行的广泛实验表明，ChannelTokenFormer在挑战性的真实世界条件下表现出卓越的鲁棒性和准确性。", "summary": "本文提出了ChannelTokenFormer，一个基于Transformer的统一框架，旨在解决真实世界多元时间序列预测中的核心挑战，包括复杂的通道间依赖性、异步采样和缺失值。与现有模型简化假设不同，ChannelTokenFormer的灵活架构能够显式捕获跨通道交互，并有效处理异步性和缺失数据。实验证明，该模型在模拟和真实世界的复杂条件下均展现出卓越的预测鲁棒性和准确性。", "keywords": "多元时间序列预测, Transformer, 异步性, 缺失值, 通道依赖性", "comments": "该论文的创新之处在于提出了一个统一的Transformer框架ChannelTokenFormer，能够同时处理真实世界多元时间序列数据中固有的依赖性、异步性和缺失值这三大挑战。这对于提高实际应用中时间序列预测的鲁棒性和可靠性具有重要意义，解决了现有模型在复杂真实场景中性能不足的问题。"}}
{"id": "2506.08990", "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Dongyun Liang", "Jing Qin", "Liansheng Wang"], "summary": "Medical vision-language alignment through cross-modal contrastive learning\nshows promising performance in image-text matching tasks, such as retrieval and\nzero-shot classification. However, conventional cross-modal contrastive\nlearning (CLIP-based) methods suffer from suboptimal visual representation\ncapabilities, which also limits their effectiveness in vision-language\nalignment. In contrast, although the models pretrained via multimodal masked\nmodeling struggle with direct cross-modal matching, they excel in visual\nrepresentation. To address this contradiction, we propose ALTA (ALign Through\nAdapting), an efficient medical vision-language alignment method that utilizes\nonly about 8% of the trainable parameters and less than 1/5 of the\ncomputational consumption required for masked record modeling. ALTA achieves\nsuperior performance in vision-language matching tasks like retrieval and\nzero-shot classification by adapting the pretrained vision model from masked\nrecord modeling. Additionally, we integrate temporal-multiview radiograph\ninputs to enhance the information consistency between radiographs and their\ncorresponding descriptions in reports, further improving the vision-language\nalignment. Experimental evaluations show that ALTA outperforms the\nbest-performing counterpart by over 4% absolute points in text-to-image\naccuracy and approximately 6% absolute points in image-to-text retrieval\naccuracy. The adaptation of vision-language models during efficient alignment\nalso promotes better vision and language understanding. Code is publicly\navailable at https://github.com/DopamineLcy/ALTA.", "comment": "TMI 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.08990v1", "AI": {"title_translation": "通过适应掩码视觉模型实现高效医学视觉-语言对齐", "tldr": "ALTA通过适应掩码视觉模型实现高效医学视觉-语言对齐，计算量更少，性能优于传统方法。", "motivation": "传统的跨模态对比学习（基于CLIP）方法在视觉表示能力方面表现不佳，限制了其在视觉-语言对齐中的有效性。而多模态掩码建模预训练的模型虽然不擅长直接的跨模态匹配，但在视觉表示方面表现出色，存在矛盾。", "method": "提出了ALTA（ALign Through Adapting）方法，通过适应来自掩码记录建模的预训练视觉模型，实现高效医学视觉-语言对齐，仅使用约8%的可训练参数和不到1/5的计算消耗。此外，整合了时间-多视图X射线输入，以增强X射线图像与其报告描述之间的信息一致性。", "result": "实验评估表明，ALTA在文本到图像准确率方面超越最佳对比方法4%以上，在图像到文本检索准确率方面超越约6%。", "conclusion": "在高效对齐过程中对视觉-语言模型的适应也促进了更好的视觉和语言理解。", "translation": "通过跨模态对比学习实现的医学视觉-语言对齐在图像-文本匹配任务（如检索和零样本分类）中表现出有希望的性能。然而，传统的跨模态对比学习（基于CLIP）方法存在视觉表示能力不足的问题，这也限制了它们在视觉-语言对齐中的有效性。相比之下，尽管通过多模态掩码建模预训练的模型在直接跨模态匹配方面表现不佳，但它们在视觉表示方面表现出色。为了解决这一矛盾，我们提出了ALTA（ALign Through Adapting），一种高效的医学视觉-语言对齐方法，它仅使用掩码记录建模所需可训练参数的约8%和不到1/5的计算消耗。ALTA通过适应来自掩码记录建模的预训练视觉模型，在检索和零样本分类等视觉-语言匹配任务中取得了卓越的性能。此外，我们整合了时间-多视图X射线输入，以增强X射线图像与其报告描述之间的信息一致性，进一步改善了视觉-语言对齐。实验评估表明，ALTA在文本到图像准确率方面超越最佳对比方法4%以上，在图像到文本检索准确率方面超越约6%。在高效对齐过程中对视觉-语言模型的适应也促进了更好的视觉和语言理解。代码已公开在https://github.com/DopamineLcy/ALTA。", "summary": "本文提出ALTA，一种高效的医学视觉-语言对齐方法，旨在解决传统基于CLIP方法视觉表示能力不足的问题。ALTA通过适应预训练的掩码视觉模型，利用更少的参数和计算资源，并在医学图像-文本匹配任务（如检索和零样本分类）中表现出色。此外，该方法还整合了时间-多视图X射线输入以增强信息一致性，最终提高了视觉和语言的理解能力。", "keywords": "医学视觉-语言对齐, 掩码视觉模型, 对比学习, 图像-文本匹配, ALTA", "comments": "ALTA的创新之处在于高效地适应掩码视觉模型进行视觉-语言对齐，解决了传统对比学习的局限性，同时利用了掩码模型的强大视觉表示能力。其在参数和计算方面的效率对于医学应用非常实用。"}}
{"id": "2506.08669", "title": "Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search", "authors": ["Dongge Han", "Menglin Xia", "Daniel Madrigal Diaz", "Samuel Kessler", "Ankur Mallick", "Xuchao Zhang", "Mirian Del Carmen Hipolito Garcia", "Jin Xu", "Victor Rühle", "Saravan Rajmohan"], "summary": "Small language models (SLMs) offer promising and efficient alternatives to\nlarge language models (LLMs). However, SLMs' limited capacity restricts their\nreasoning capabilities and makes them sensitive to prompt variations. To\naddress these challenges, we propose a novel framework that enhances SLM\nreasoning capabilities through LLM generated blueprints. The blueprints provide\nstructured, high-level reasoning guides that help SLMs systematically tackle\nrelated problems. Furthermore, our framework integrates a prompt template\nsearch mechanism to mitigate the SLMs' sensitivity to prompt variations. Our\nframework demonstrates improved SLM performance across various tasks, including\nmath (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves\nthe reasoning capabilities of SLMs without increasing model size or requiring\nadditional training, offering a lightweight and deployment-friendly solution\nfor on-device or resource-constrained environments.", "comment": "TTODLer-FM Workshop@ICML'25 (Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08669v1", "AI": {"title_translation": "利用蓝图和提示模板搜索增强小型语言模型的推理能力", "tldr": "该论文提出了一种利用大型语言模型生成的蓝图和提示模板搜索机制来增强小型语言模型推理能力的新框架，无需增加模型大小或额外训练。", "motivation": "小型语言模型（SLMs）虽然是大型语言模型（LLMs）的有前景的替代品，但其有限的能力限制了推理能力，并使其对提示变化敏感。", "method": "我们提出了一个新颖的框架，通过LLM生成的蓝图来增强SLM的推理能力。蓝图提供结构化、高级的推理指南。此外，该框架集成了提示模板搜索机制，以减轻SLM对提示变化的敏感性。", "result": "我们的框架在各种任务（包括数学GSM8K、编码MBPP和逻辑推理BBH）上展示了SLM性能的提高。该方法在不增加模型大小或需要额外训练的情况下提高了SLM的推理能力。", "conclusion": "该研究提出了一种轻量级且易于部署的解决方案，通过利用LLM生成的蓝图和提示模板搜索，有效提升了小型语言模型的推理能力，适用于设备端或资源受限环境。", "translation": "小型语言模型（SLMs）为大型语言模型（LLMs）提供了有前景且高效的替代方案。然而，SLMs有限的能力限制了它们的推理能力，并使其对提示变化敏感。为了解决这些挑战，我们提出了一个新颖的框架，通过LLM生成的蓝图来增强SLM的推理能力。蓝图提供结构化、高级的推理指南，帮助SLMs系统地解决相关问题。此外，我们的框架集成了提示模板搜索机制，以减轻SLMs对提示变化的敏感性。我们的框架在各种任务（包括数学（GSM8K）、编码（MBPP）和逻辑推理（BBH））上展示了SLM性能的提高。我们的方法在不增加模型大小或需要额外训练的情况下提高了SLMs的推理能力，为设备端或资源受限环境提供了一种轻量级且易于部署的解决方案。", "summary": "该论文提出了一种新颖的框架，旨在通过利用大型语言模型（LLMs）生成的蓝图和集成的提示模板搜索机制，提升小型语言模型（SLMs）的推理能力。蓝图作为结构化的高级推理指南，帮助SLMs系统解决问题，而提示模板搜索则缓解了SLMs对提示变化的敏感性。该框架在数学、编码和逻辑推理等多种任务上显著提升了SLMs的性能，且无需增加模型大小或进行额外训练，为资源受限环境提供了一种高效且可部署的解决方案。", "keywords": "小型语言模型, 推理能力, 蓝图, 提示模板搜索, LLM", "comments": "这项工作具有创新性，因为它提出了一种无需增加模型大小或额外训练即可增强SLM推理能力的方法，这对于资源受限环境非常重要。通过结合LLM生成的蓝图和提示模板搜索，该方法有效地弥补了SLMs在推理和提示敏感性方面的不足，提供了一个轻量级的部署友好型解决方案。"}}
{"id": "2506.08349", "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving", "authors": ["Yuxuan Zhou", "Xien Liu", "Chenwei Yan", "Chen Ning", "Xiao Zhang", "Boxun Li", "Xiangling Fu", "Shijin Wang", "Guoping Hu", "Yu Wang", "Ji Wu"], "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious medical benchmarks, but their capabilities across different cognitive\nlevels remain underexplored. Inspired by Bloom's Taxonomy, we propose a\nmulti-cognitive-level evaluation framework for assessing LLMs in the medical\ndomain in this study. The framework integrates existing medical datasets and\nintroduces tasks targeting three cognitive levels: preliminary knowledge grasp,\ncomprehensive knowledge application, and scenario-based problem solving. Using\nthis framework, we systematically evaluate state-of-the-art general and medical\nLLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.\nOur findings reveal a significant performance decline as cognitive complexity\nincreases across evaluated models, with model size playing a more critical role\nin performance at higher cognitive levels. Our study highlights the need to\nenhance LLMs' medical capabilities at higher cognitive levels and provides\ninsights for developing LLMs suited to real-world medical applications.", "comment": "20 pages, 11 figures. Accepted by ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08349v1", "AI": {"title_translation": "评估大型语言模型跨多认知水平的能力：从医学知识掌握到基于场景的问题解决", "tldr": "大型语言模型在医学领域中随着认知复杂性的增加，性能显著下降；模型大小对高认知水平任务的表现更为关键。", "motivation": "大型语言模型（LLM）在各种医学基准测试中表现出色，但其在不同认知水平上的能力尚未得到充分探索。本研究旨在评估LLM在医学领域中跨不同认知水平的能力。", "method": "研究提出了一个受布鲁姆分类学启发的“多认知水平评估框架”，该框架整合了现有医学数据集，并引入了针对初步知识掌握、综合知识应用和基于场景的问题解决三个认知水平的任务。研究使用此框架系统地评估了Llama、Qwen、Gemma、Phi、GPT和DeepSeek等六个主要家族的SOTA通用和医学LLM。", "result": "研究发现，随着认知复杂性的增加，所评估模型的性能显著下降。模型大小在较高认知水平上的性能中扮演着更关键的角色。", "conclusion": "研究强调需要提高LLM在较高认知水平上的医学能力，并为开发适用于真实世界医疗应用的LLM提供了见解。", "translation": "大型语言模型（LLM）在各种医学基准测试中表现出色，但它们在不同认知水平上的能力仍未得到充分探索。受布鲁姆分类学的启发，本研究提出了一个多认知水平评估框架，用于评估医学领域中的LLM。该框架整合了现有的医学数据集，并引入了针对三个认知水平的任务：初步知识掌握、综合知识应用和基于场景的问题解决。利用该框架，我们系统地评估了来自六个主要家族（Llama、Qwen、Gemma、Phi、GPT和DeepSeek）的SOTA通用和医学LLM。我们的发现揭示，随着认知复杂性的增加，所评估模型中的性能显著下降，其中模型大小在较高认知水平上的性能中扮演着更关键的角色。我们的研究强调需要提高LLM在较高认知水平上的医学能力，并为开发适用于真实世界医疗应用的LLM提供了见解。", "summary": "本研究提出一个受布鲁姆分类学启发的医学领域多认知水平LLM评估框架，旨在评估LLM在初步知识掌握、综合知识应用和基于场景的问题解决等不同认知水平上的能力。研究发现，LLM的性能随认知复杂性增加而显著下降，且模型大小对高认知水平任务的表现更为关键。该研究强调了提升LLM在高认知水平上医学能力的重要性，并为开发适用于真实世界医疗应用的LLM提供了指导。", "keywords": "LLMs, 医学领域, 认知水平, 布鲁姆分类学, 评估框架", "comments": "该研究的创新之处在于提出了一个针对医学领域LLM的多认知水平评估框架，超越了简单的知识回忆，深入到复杂的医学问题解决能力评估。这为理解LLM在关键领域的能力和局限性提供了更细致的视角，对未来LLM在医疗领域的应用开发具有重要指导意义。"}}
{"id": "2506.08681", "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling", "authors": ["Phuc Minh Nguyen", "Ngoc-Hieu Nguyen", "Duy H. M. Nguyen", "Anji Liu", "An Mai", "Binh T. Nguyen", "Daniel Sonntag", "Khoa D. Doan"], "summary": "Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization\n(DPO) have emerged as alternatives to the standard Reinforcement Learning from\nHuman Feedback (RLHF) for aligning large language models (LLMs) with human\nvalues. However, these methods are more susceptible to over-optimization, in\nwhich the model drifts away from the reference policy, leading to degraded\nperformance as training progresses. This paper proposes a novel\nimportance-sampling approach to mitigate the over-optimization problem of\noffline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective\nwith an importance ratio that accounts for the reference policy distribution.\nIS-DAAs additionally avoid the high variance issue associated with importance\nsampling by clipping the importance ratio to a maximum value. Our extensive\nexperiments demonstrate that IS-DAAs can effectively mitigate\nover-optimization, especially under low regularization strength, and achieve\nbetter performance than other methods designed to address this problem. Our\nimplementations are provided publicly at this link.", "comment": "First version", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08681v1", "AI": {"title_translation": "使用重要性采样减轻直接对齐算法中的奖励过度优化", "tldr": "本文提出了一种名为IS-DAAs的新型重要性采样方法，以减轻直接对齐算法（DAAs）中奖励过度优化的问题，并证明其能有效提高性能。", "motivation": "直接对齐算法（DAAs）如DPO在对齐大型语言模型（LLMs）方面表现出色，但它们容易出现过度优化问题，导致模型偏离参考策略，性能下降。", "method": "本文提出了一种名为IS-DAAs（Importance Sampling Direct Alignment Algorithms）的新方法。该方法通过将DAA目标与考虑参考策略分布的重要性比率相乘来解决过度优化问题。为了避免高方差问题，IS-DAAs还会将重要性比率裁剪到最大值。", "result": "广泛的实验表明，IS-DAAs能够有效减轻过度优化，尤其是在低正则化强度下，并且比其他旨在解决此问题的方法取得了更好的性能。", "conclusion": "IS-DAAs通过引入重要性采样和比率裁剪，成功解决了直接对齐算法中的奖励过度优化问题，提升了模型性能。", "translation": "直接对齐算法（DAAs），如直接偏好优化（DPO），已成为标准的人类反馈强化学习（RLHF）的替代方案，用于将大型语言模型（LLMs）与人类价值观对齐。然而，这些方法更容易受到过度优化的影响，即模型偏离参考策略，导致训练过程中性能下降。本文提出了一种新颖的重要性采样方法来减轻离线DAA的过度优化问题。这种方法称为（IS-DAAs），它将DAA目标乘以一个考虑参考策略分布的重要性比率。IS-DAAs还通过将重要性比率裁剪到最大值来避免与重要性采样相关的高方差问题。我们广泛的实验表明，IS-DAAs可以有效减轻过度优化，尤其是在低正则化强度下，并比其他旨在解决此问题的方法取得更好的性能。我们的实现已在此链接公开提供。", "summary": "本文针对直接对齐算法（DAAs）中普遍存在的奖励过度优化问题，提出了一种基于重要性采样的新方法——IS-DAAs。该方法通过引入考虑参考策略分布的重要性比率，并对该比率进行裁剪以避免高方差，从而有效缓解了模型训练过程中性能下降的问题。实验结果表明，IS-DAAs在减轻过度优化和提升性能方面优于现有方法。", "keywords": "直接对齐算法, 奖励过度优化, 重要性采样, 大型语言模型, DPO", "comments": "这项研究提出了一种新颖且实用的方法来解决DAA中的关键问题。通过引入重要性采样和裁剪，它有效地平衡了模型与参考策略的对齐，对于提升LLM的对齐效果具有重要意义。"}}
{"id": "2506.09022", "title": "Do MIL Models Transfer?", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "comment": "ICML 2025 (Spotlight). 20 pages, 8 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09022v1", "AI": {"title_translation": "MIL模型是否可迁移？", "tldr": "预训练的MIL模型在计算病理学中表现出强大的迁移能力，即使在不同器官和任务之间也能一致地优于从头开始训练的模型。", "motivation": "多实例学习（MIL）在计算病理学（CPath）中常面临小型、弱监督临床数据集的挑战。与自然语言处理和传统计算机视觉领域不同，MIL模型的迁移能力尚不明确，本研究旨在解决这一问题。", "method": "本研究通过评估11个模型在21个预训练任务中的形态学和分子亚型预测能力，系统地评估了预训练MIL模型的迁移学习能力。", "result": "结果表明，预训练的MIL模型，即使在与目标任务不同的器官上进行训练，也始终优于从头开始训练的模型。此外，在泛癌数据集上进行预训练能够实现跨器官和任务的强大泛化能力，并且在数据量显著少于幻灯片基础模型的情况下表现更优。", "conclusion": "这些发现突出了MIL模型强大的适应性，并证明了利用迁移学习来提高计算病理学性能的益处。", "translation": "多实例学习（MIL）是计算病理学（CPath）中的一个基石方法，用于从千兆像素组织图像中生成具有临床意义的玻片级嵌入。然而，MIL常在小型、弱监督的临床数据集中表现不佳。与自然语言处理和传统计算机视觉等广泛使用迁移学习来解决数据稀缺问题的领域不同，MIL模型的迁移能力仍然知之甚少。在本研究中，我们通过评估11个模型在21个预训练任务中的形态学和分子亚型预测能力，系统地评估了预训练MIL模型的迁移学习能力。我们的结果显示，预训练的MIL模型，即使在与目标任务不同的器官上进行训练，也始终优于从头开始训练的模型。此外，在泛癌数据集上进行预训练能够实现跨器官和任务的强大泛化能力，并且在数据量显著少于幻灯片基础模型的情况下表现更优。这些发现突出了MIL模型强大的适应性，并证明了利用迁移学习来提高CPath性能的益处。最后，我们提供了一个标准化MIL模型实现和流行CPath任务预训练模型权重的资源，可在https://github.com/mahmoodlab/MIL-Lab获取。", "summary": "本研究探讨了多实例学习（MIL）模型在计算病理学（CPath）中的迁移能力，该领域常面临数据稀缺的挑战。通过评估11个MIL模型在21个预训练任务上的表现，研究表明预训练的MIL模型即使在不同器官间迁移，其性能也持续优于从头训练的模型。值得注意的是，在泛癌数据集上进行预训练能够实现出色的跨器官和任务泛化能力，且所需数据量显著少于幻灯片基础模型。这些发现强调了MIL模型的强大适应性以及迁移学习在提升CPath性能方面的显著优势。研究还提供了一个标准化MIL模型实现和预训练模型权重的资源。", "keywords": "多实例学习, 迁移学习, 计算病理学, 泛癌, 巨像素图像", "comments": "这篇论文解决了计算病理学中的一个关键问题，即MIL模型的迁移能力，这对于解决临床数据稀缺性至关重要。发现泛癌预训练特别有效且数据高效，这具有重要意义，可能为更稳健和可泛化的CPath模型铺平道路。提供标准化资源也有助于提高研究的可复现性和未来的发展。"}}
{"id": "2506.08698", "title": "Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data", "authors": ["Boyu Xie", "Tangtang Xie"], "summary": "With the development of smart grids, High-Dimensional and Incomplete (HDI)\nPower Load Monitoring (PLM) data challenges the performance of Power Load\nForecasting (PLF) models. In this paper, we propose a potential\ncharacterization model VAE-LF based on Variational Autoencoder (VAE) for\nefficiently representing and complementing PLM missing data. VAE-LF learns a\nlow-dimensional latent representation of the data using an Encoder-Decoder\nstructure by splitting the HDI PLM data into vectors and feeding them\nsequentially into the VAE-LF model, and generates the complementary data.\nExperiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark\nmodels in both 5% and 10% sparsity test cases, with significantly lower RMSE\nand MAE, and especially outperforms on low sparsity ratio data. The method\nprovides an efficient data-completion solution for electric load management in\nsmart grids.", "comment": "9 pages, 2 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08698v1", "AI": {"title_translation": "基于变分自编码器的电力负荷监测数据高效表示潜在特征分析方法", "tldr": "本文提出了一种基于变分自编码器（VAE）的潜在表征模型VAE-LF，用于高效表示和补充高维不完整（HDI）电力负荷监测（PLM）数据，实验证明其在数据补全方面优于现有基准模型。", "motivation": "随着智能电网的发展，高维不完整（HDI）的电力负荷监测（PLM）数据对电力负荷预测（PLF）模型的性能提出了挑战。", "method": "本文提出了一种名为VAE-LF的基于变分自编码器（VAE）的潜在特征模型，用于高效表示和补充PLM缺失数据。VAE-LF使用编码器-解码器结构学习数据的低维潜在表示，通过将HDI PLM数据分割成向量并按顺序输入VAE-LF模型来生成补充数据。", "result": "在UK-DALE数据集上的实验表明，VAE-LF在5%和10%稀疏度测试案例中均优于其他基准模型，RMSE和MAE显著降低，尤其在低稀疏度数据上表现更佳。", "conclusion": "该方法为智能电网中的电力负荷管理提供了一种高效的数据补全解决方案。", "translation": "随着智能电网的发展，高维不完整（HDI）电力负荷监测（PLM）数据对电力负荷预测（PLF）模型的性能提出了挑战。在本文中，我们提出了一种基于变分自编码器（VAE）的潜在表征模型VAE-LF，用于高效表示和补充PLM缺失数据。VAE-LF通过将HDI PLM数据分割成向量并按顺序输入VAE-LF模型，使用编码器-解码器结构学习数据的低维潜在表示，并生成补充数据。在UK-DALE数据集上的实验表明，VAE-LF在5%和10%稀疏度测试案例中均优于其他基准模型，RMSE和MAE显著降低，尤其在低稀疏度数据上表现更佳。该方法为智能电网中的电力负荷管理提供了一种高效的数据补全解决方案。", "summary": "针对智能电网中高维不完整电力负荷监测（PLM）数据对负荷预测模型性能的挑战，本文提出了一种基于变分自编码器（VAE）的VAE-LF模型。该模型利用编码器-解码器结构学习PLM数据的低维潜在表示，并能有效补充缺失数据。在UK-DALE数据集上的实验结果表明，VAE-LF在不同稀疏度下均优于其他基准模型，显著降低了RMSE和MAE，尤其在低稀疏度数据上表现出色，为智能电网的电力负荷管理提供了高效的数据补全方案。", "keywords": "变分自编码器, 电力负荷监测, 数据补全, 潜在特征分析, 智能电网", "comments": "本文提出了一种创新的基于VAE的数据补全方法，有效解决了智能电网中高维不完整电力负荷数据的表示和补全问题。其在稀疏数据上的优异表现，特别是对低稀疏度数据的改进，显示了该方法在实际应用中的巨大潜力，对于提升电力负荷管理效率具有重要意义。"}}
{"id": "2506.09024", "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging", "authors": ["Felix Wagner", "Pramit Saha", "Harry Anthony", "J. Alison Noble", "Konstantinos Kamnitsas"], "summary": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard training data\nafter deployment or assume that test samples and training data are centrally\nstored together, an assumption that rarely holds in real-world settings. This\nis because shipping training data with the deployed model is usually impossible\ndue to the size of training databases, as well as proprietary or privacy\nconstraints. We introduce the Isolation Network, an OOD detection framework\nthat quantifies the difficulty of separating a target test sample from the\ntraining data by solving a binary classification task. We then propose\nDecentralized Isolation Networks (DIsoN), which enables the comparison of\ntraining and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code will be available upon\nacceptance at: *****", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09024v1", "AI": {"title_translation": "DIsoN: 医疗影像中异常检测的去中心化隔离网络", "tldr": "DIsoN是一种去中心化的异常检测方法，旨在解决医疗影像中模型部署时由于数据隐私和大小限制导致训练数据无法共享的问题。它通过仅交换模型参数来实现训练数据和测试数据的比较，并在多个医疗影像数据集上表现良好。", "motivation": "在医疗影像等安全关键领域安全部署机器学习模型，需要检测训练期间未见的输入（即异常检测），以防止不可靠的预测。现有异常检测方法要么在部署后丢弃训练数据，要么假设测试样本和训练数据集中存储在一起，这在现实世界中很少成立，因为训练数据库的庞大尺寸以及专有或隐私限制使得随模型传输训练数据通常不可能。", "method": "本文引入了隔离网络（Isolation Network），这是一种通过解决二元分类任务来量化目标测试样本与训练数据分离难度的异常检测框架。在此基础上，提出了去中心化隔离网络（DIsoN），通过仅在训练和部署的远程计算节点之间交换模型参数，实现在无法共享数据时进行训练数据和测试数据的比较。进一步将DIsoN扩展为类别条件式，仅将目标样本与其预测类别的训练数据进行比较。", "result": "DIsoN在四个医疗影像数据集（皮肤病学、胸部X光、乳腺超声、组织病理学）上的12项异常检测任务中进行了评估。结果表明，DIsoN在尊重数据隐私的同时，表现优于现有方法。", "conclusion": "DIsoN这一去中心化异常检测框架，为机器学习开发者提供了一种新型服务：随模型提供远程、安全地利用其训练数据进行异常检测服务。", "translation": "在医疗影像等安全关键领域安全部署机器学习（ML）模型，需要检测训练期间未见的输入（即异常检测），以防止不可靠的预测。部署后有效的异常检测可以受益于对训练数据的访问，从而能够直接比较测试样本和训练数据分布以识别差异。然而，最先进的异常检测方法要么在部署后丢弃训练数据，要么假设测试样本和训练数据集中存储在一起，这在现实世界中很少成立。这是因为训练数据库的庞大尺寸以及专有或隐私限制通常使得随部署模型传输训练数据成为不可能。我们引入了隔离网络（Isolation Network），这是一种异常检测框架，通过解决二元分类任务来量化目标测试样本与训练数据分离的难度。然后，我们提出了去中心化隔离网络（DIsoN），它通过仅在训练和部署的远程计算节点之间交换模型参数，实现在无法共享数据时比较训练数据和测试数据。我们进一步将DIsoN扩展为类别条件式，仅将目标样本与其预测类别的训练数据进行比较。我们在四个医疗影像数据集（皮肤病学、胸部X光、乳腺超声、组织病理学）上的12项异常检测任务中评估了DIsoN。DIsoN在尊重数据隐私的同时，表现优于现有方法。这个去中心化异常检测框架为ML开发者提供了一种新型服务：随模型提供远程、安全地利用其训练数据进行异常检测服务。代码将在接受后提供：*****", "summary": "本文提出了一种名为DIsoN（Decentralized Isolation Networks）的去中心化异常检测框架，旨在解决医疗影像等安全关键领域中，机器学习模型部署后，由于数据隐私和数据量限制而无法共享训练数据的问题。DIsoN基于隔离网络的概念，通过解决二元分类任务来量化测试样本与训练数据的分离难度。其核心创新在于，它允许通过仅交换模型参数而非原始数据，在远程节点之间进行训练数据和测试数据的比较。DIsoN还引入了类别条件机制，以更精确地进行比较。在多个医疗影像数据集上的评估表明，DIsoN在保持数据隐私的同时，性能优于现有方法。该框架为机器学习开发者提供了一种新的服务模式，即安全地远程利用训练数据进行异常检测。", "keywords": "异常检测, 医疗影像, 去中心化学习, 隐私保护, 隔离网络", "comments": "DIsoN的创新之处在于其去中心化的异常检测方法，有效解决了医疗影像等领域中数据隐私和数据量过大导致训练数据无法共享的关键问题。它通过仅交换模型参数来实现数据比较，从而在不侵犯隐私的前提下提高了模型部署的安全性。这对于安全关键型AI应用具有重要意义，并开辟了新的服务模式。"}}
{"id": "2506.09027", "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "authors": ["Runqian Wang", "Kaiming He"], "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09027v1", "AI": {"title_translation": "扩散与分散：带有表示正则化的图像生成", "tldr": "本文提出了一种名为“分散损失”的简单即插即用正则化器，用于改进基于扩散的生成模型，通过鼓励内部表示在隐藏空间中分散，从而在ImageNet数据集上实现一致的性能提升。", "motivation": "过去十年中，基于扩散的生成模型与表示学习的进展基本独立。这些扩散模型通常依赖于基于回归的目标，并且普遍缺乏显式正则化。", "method": "本文提出了一种名为“分散损失”（Dispersive Loss）的简单即插即用正则化器。该损失函数鼓励内部表示在隐藏空间中分散，类似于对比自监督学习，但关键区别在于它不需要正样本对，因此不干扰用于回归的采样过程。与表示对齐（REPA）方法相比，该方法是自包含和极简的，不需要预训练、额外参数或外部数据。", "result": "在ImageNet数据集上对一系列模型进行了评估，结果显示与广泛使用和强大的基线相比，实现了持续的改进。", "conclusion": "本文的工作有望帮助弥合生成建模和表示学习之间的鸿沟。", "translation": "在过去十年中，基于扩散的生成模型的发展与表示学习的进展基本独立。这些扩散模型通常依赖于基于回归的目标，并且普遍缺乏显式正则化。在这项工作中，我们提出了“分散损失”（Dispersive Loss），一个简单的即插即用正则化器，可有效改进基于扩散的生成模型。我们的损失函数鼓励内部表示在隐藏空间中分散，类似于对比自监督学习，但关键区别在于它不需要正样本对，因此不干扰用于回归的采样过程。与最近的表示对齐（REPA）方法相比，我们的方法是自包含和极简的，不需要预训练、额外参数和外部数据。我们在ImageNet数据集上对一系列模型评估了分散损失，并报告了与广泛使用和强大的基线相比的一致改进。我们希望我们的工作将有助于弥合生成建模和表示学习之间的鸿沟。", "summary": "本研究引入了一种名为“分散损失”的新型即插即用正则化器，旨在提升基于扩散的生成模型性能。该损失函数通过促使内部表示在隐藏空间中分散，类似于对比学习，但无需正样本对，从而避免干扰采样过程。与现有方法相比，分散损失具有自包含、极简的特点，无需预训练或额外数据。在ImageNet数据集上的实验证明，该方法能持续改进多种模型，并有望促进生成建模与表示学习的融合。", "keywords": "扩散模型, 表示学习, 正则化, 分散损失, 图像生成", "comments": "该论文提出了一种新颖且简洁的正则化方法“分散损失”，有效地解决了扩散模型中缺乏显式正则化的问题。其创新点在于无需正样本对即可实现表示分散，且具备即插即用、自包含和极简的特性，降低了应用门槛。这项工作对于弥合生成建模和表示学习之间的差距具有重要意义，有望推动这两个领域的协同发展。"}}
{"id": "2506.08737", "title": "Exploration by Random Reward Perturbation", "authors": ["Haozhe Ma", "Guoji Fu", "Zhengding Luo", "Jiele Wu", "Tze-Yun Leong"], "summary": "We introduce Random Reward Perturbation (RRP), a novel exploration strategy\nfor reinforcement learning (RL). Our theoretical analyses demonstrate that\nadding zero-mean noise to environmental rewards effectively enhances policy\ndiversity during training, thereby expanding the range of exploration. RRP is\nfully compatible with the action-perturbation-based exploration strategies,\nsuch as $\\epsilon$-greedy, stochastic policies, and entropy regularization,\nproviding additive improvements to exploration effects. It is general,\nlightweight, and can be integrated into existing RL algorithms with minimal\nimplementation effort and negligible computational overhead. RRP establishes a\ntheoretical connection between reward shaping and noise-driven exploration,\nhighlighting their complementary potential. Experiments show that RRP\nsignificantly boosts the performance of Proximal Policy Optimization and Soft\nActor-Critic, achieving higher sample efficiency and escaping local optima\nacross various tasks, under both sparse and dense reward scenarios.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08737v1", "AI": {"title_translation": "随机奖励扰动探索", "tldr": "随机奖励扰动（RRP）是一种新的强化学习探索策略，通过向环境奖励添加零均值噪声来增强策略多样性，提高探索效果，并显著提升现有RL算法的性能。", "motivation": "为了解决强化学习中探索不足的问题，并有效增强训练过程中的策略多样性，从而扩大探索范围。", "method": "引入随机奖励扰动（RRP）策略，通过向环境奖励添加零均值噪声来增强策略多样性。该方法与基于动作扰动的探索策略兼容，且通用、轻量，易于集成到现有RL算法中。", "result": "RRP显著提升了近端策略优化（PPO）和软 Actor-Critic（SAC）的性能，在各种任务中（包括稀疏和密集奖励场景下）实现了更高的样本效率并成功逃离局部最优。", "conclusion": "随机奖励扰动（RRP）是一种有效且通用的探索策略，通过引入奖励噪声显著改善了强化学习算法的性能和探索能力，并在理论上连接了奖励塑造和噪声驱动探索。", "translation": "我们引入了随机奖励扰动（RRP），一种新颖的强化学习（RL）探索策略。我们的理论分析表明，向环境奖励添加零均值噪声能有效增强训练期间的策略多样性，从而扩大探索范围。RRP与基于动作扰动的探索策略完全兼容，例如 $\\epsilon$-greedy、随机策略和熵正则化，为探索效果提供了附加改进。它通用、轻量，并且可以以最小的实现工作量和可忽略的计算开销集成到现有的RL算法中。RRP在奖励塑造和噪声驱动探索之间建立了理论联系，突出了它们的互补潜力。实验表明，RRP显著提升了近端策略优化（Proximal Policy Optimization）和软 Actor-Critic（Soft Actor-Critic）的性能，在稀疏和密集奖励场景下的各种任务中实现了更高的样本效率并逃离了局部最优。", "summary": "本文提出随机奖励扰动（RRP），一种新颖的强化学习探索策略。RRP通过向环境奖励添加零均值噪声来有效增强策略多样性，从而扩大探索范围。该方法与现有探索策略兼容，通用且轻量，易于集成。RRP在理论上建立了奖励塑造与噪声驱动探索之间的联系。实验证明，RRP显著提升了PPO和SAC的性能，提高了样本效率，并在多种任务中成功逃离局部最优。", "keywords": "随机奖励扰动, 强化学习, 探索, 策略多样性, 奖励塑造", "comments": "RRP的创新性在于其通过奖励扰动而非常见的动作扰动来实现探索，并能与现有方法叠加，提供额外的探索增益。其通用性、轻量级以及与奖励塑造的理论联系是其重要亮点。该方法为解决强化学习中的探索难题提供了一个有效且易于实现的方案。"}}
{"id": "2506.09035", "title": "Princeton365: A Diverse Dataset with Accurate Camera Pose", "authors": ["Karhan Kayan", "Stamatis Alexandropoulos", "Rishabh Jain", "Yiming Zuo", "Erich Liang", "Jia Deng"], "summary": "We introduce Princeton365, a large-scale diverse dataset of 365 videos with\naccurate camera pose. Our dataset bridges the gap between accuracy and data\ndiversity in current SLAM benchmarks by introducing a novel ground truth\ncollection framework that leverages calibration boards and a 360-camera. We\ncollect indoor, outdoor, and object scanning videos with synchronized monocular\nand stereo RGB video outputs as well as IMU. We further propose a new scene\nscale-aware evaluation metric for SLAM based on the the optical flow induced by\nthe camera pose estimation error. In contrast to the current metrics, our new\nmetric allows for comparison between the performance of SLAM methods across\nscenes as opposed to existing metrics such as Average Trajectory Error (ATE),\nallowing researchers to analyze the failure modes of their methods. We also\npropose a challenging Novel View Synthesis benchmark that covers cases not\ncovered by current NVS benchmarks, such as fully non-Lambertian scenes with\n360-degree camera trajectories. Please visit\nhttps://princeton365.cs.princeton.edu for the dataset, code, videos, and\nsubmission.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09035v1", "AI": {"title_translation": "Princeton365：一个包含精确相机姿态的多样化数据集", "tldr": "引入了Princeton365数据集，一个大规模、多样化且具有精确相机姿态的数据集，并提出了新的SLAM评估指标和新颖视图合成（NVS）基准，以弥补现有基准在准确性和数据多样性方面的不足。", "motivation": "现有SLAM基准在准确性和数据多样性之间存在差距，本研究旨在通过引入一个新数据集来弥合这一差距。", "method": "引入了Princeton365数据集，包含365个视频，具有精确的相机姿态。采用新颖的地面实况采集框架，利用校准板和360度相机。采集了室内、室外和物体扫描视频，同步单目和立体RGB视频输出以及IMU数据。提出了一种基于相机姿态估计误差引起的光流的场景尺度感知SLAM评估指标。提出了一种具有挑战性的新颖视图合成基准，涵盖了当前NVS基准未涵盖的案例，如完全非朗伯场景和360度相机轨迹。", "result": "Princeton365数据集弥合了当前SLAM基准在准确性和数据多样性之间的差距。提出的新SLAM指标允许跨场景比较SLAM方法的性能，并分析其失效模式。新颖视图合成基准涵盖了现有NVS基准未涵盖的挑战性案例，如完全非朗伯场景和360度相机轨迹。", "conclusion": "Princeton365提供了一个多样化且具有精确相机姿态的大规模数据集，以及用于SLAM和新颖视图合成的创新评估工具，从而推动了相关领域的研究，解决了现有基准的局限性。", "translation": "我们介绍了Princeton365，一个包含365个视频的大规模多样化数据集，具有精确的相机姿态。我们的数据集通过引入一个利用校准板和360度相机的新颖地面实况采集框架，弥合了当前SLAM基准在准确性和数据多样性之间的差距。我们采集了室内、室外和物体扫描视频，并同步了单目和立体RGB视频输出以及IMU数据。我们进一步提出了一种基于相机姿态估计误差引起的光流的场景尺度感知SLAM评估指标。与当前指标不同，我们的新指标允许在不同场景之间比较SLAM方法的性能，而不是像平均轨迹误差（ATE）等现有指标那样，从而使研究人员能够分析其方法的失效模式。我们还提出了一个具有挑战性的新颖视图合成基准，涵盖了当前NVS基准未涵盖的案例，例如具有360度相机轨迹的完全非朗伯场景。请访问https://princeton365.cs.princeton.edu获取数据集、代码、视频和提交信息。", "summary": "本文介绍了Princeton365，一个包含365个视频的大规模多样化数据集，其相机姿态精确。该数据集通过利用校准板和360度相机的新颖地面实况采集框架，弥合了当前SLAM基准在准确性和数据多样性方面的差距。数据集包含同步的单目和立体RGB视频以及IMU数据，涵盖室内、室外和物体扫描场景。此外，论文提出了一种基于相机姿态估计误差引起的光流的场景尺度感知SLAM评估指标，允许跨场景性能比较和失效模式分析。还提出了一个具有挑战性的新颖视图合成基准，涵盖了现有基准中未涉及的案例，如完全非朗伯场景和360度相机轨迹。", "keywords": "Princeton365, SLAM, 数据集, 相机姿态, 新颖视图合成", "comments": "该论文的创新点在于引入了一个大规模、多样化且具有高精度相机姿态的全新数据集，弥补了现有SLAM基准的不足。其独特的地面实况采集方法，结合校准板和360度相机，是其关键创新。此外，提出的新评估指标对于分析SLAM方法的失效模式具有重要意义，而新颖视图合成基准则扩展了该领域的研究范围，尤其是在处理非朗伯场景和360度轨迹方面。这对于推动SLAM和NVS领域的发展具有重要价值。"}}
{"id": "2506.08373", "title": "Draft-based Approximate Inference for LLMs", "authors": ["Kevin Galim", "Ethan Ewer", "Wonjun Kang", "Minjae Lee", "Hyung Il Koo", "Kangwook Lee"], "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08373v1", "AI": {"title_translation": "基于草稿的LLM近似推理", "tldr": "本文提出了一种利用小型草稿模型更准确地预测令牌和KV对重要性的LLM近似推理框架，通过SpecKV和SpecPC两种实例化方法，在长上下文LLM推理中实现了更高的准确性和效率。", "motivation": "由于Transformer的二次计算复杂度和线性内存复杂度，优化长上下文大型语言模型（LLMs）的推理变得越来越重要。现有近似方法对令牌或KV对重要性的预测通常不够精确。", "method": "提出了一种新颖的LLM近似推理框架，利用小型草稿模型更准确地预测令牌和KV对的重要性。该框架包含两种实例化方法：(i) SpecKV，利用草稿输出来准确评估每个KV对的重要性，从而实现更有效的KV缓存丢弃；(ii) SpecPC，使用草稿模型的注意力激活来识别和丢弃不重要的提示令牌。这是首次将草稿模型用于近似LLM推理加速。", "result": "理论和实证分析表明，草稿模型和目标模型的注意力模式之间存在很强的相关性。在长上下文基准上的大量实验显示，所提出的方法始终比现有基线获得更高的准确性，同时保持了内存使用、延迟和吞吐量的相同改进。", "conclusion": "所提出的基于草稿的近似推理方法（SpecKV和SpecPC）通过更准确地预测令牌和KV对的重要性，显著提高了长上下文LLM推理的准确性，同时保持了与现有方法相同的内存、延迟和吞吐量效率提升，证明了该方法的有效性。", "translation": "由于Transformer的二次计算复杂度和线性内存复杂度，优化长上下文大型语言模型（LLMs）的推理变得越来越重要。现有的近似方法，例如键值（KV）缓存丢弃、稀疏注意力以及提示压缩，通常依赖于对令牌或KV对重要性的粗略预测。我们提出了一种新颖的LLM近似推理框架，该框架利用小型草稿模型更准确地预测令牌和KV对的重要性。具体来说，我们介绍了我们提出的框架的两种实例化：（i）SpecKV，它利用草稿输出来准确评估每个KV对的重要性，从而实现更有效的KV缓存丢弃；（ii）SpecPC，它使用草稿模型的注意力激活来识别和丢弃不重要的提示令牌。据我们所知，这是首次使用草稿模型进行LLM近似推理加速的工作，将其效用扩展到传统的无损推测解码之外。我们通过理论和实证分析来论证我们的方法，并展示了草稿模型和目标模型的注意力模式之间存在很强的相关性。在长上下文基准上的大量实验表明，我们的方法始终比现有基线获得更高的准确性，同时保持了内存使用、延迟和吞吐量的相同改进。我们的代码可在https://github.com/furiosa-ai/draft-based-approx-llm获取。", "summary": "本文介绍了一种新颖的LLM近似推理框架，其核心是利用小型草稿模型更准确地预测令牌和键值（KV）对的重要性。该框架提出了两种具体实现：SpecKV用于更有效地丢弃KV缓存，SpecPC用于识别并丢弃不重要的提示令牌。研究表明，该方法在长上下文基准测试中，相较于现有基线，在保持同等内存、延迟和吞吐量优化的前提下，显著提升了推理准确性。这是首次将草稿模型应用于LLM近似推理加速。", "keywords": "LLMs, 近似推理, 草稿模型, KV缓存, 提示压缩", "comments": "这项工作具有创新性，因为它首次将草稿模型应用于LLM的近似推理加速，扩展了其在传统无损推测解码之外的效用。通过利用草稿模型更准确地预测令牌和KV对的重要性，该方法显著提高了KV缓存丢弃和提示压缩的准确性，解决了现有方法依赖粗略预测的局限性。强大的实验结果证明了其在高效部署长上下文LLM方面的实际重要性。"}}
{"id": "2506.08740", "title": "Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports", "authors": ["Sidhika Balachandar", "Shuvom Sadhuka", "Bonnie Berger", "Emma Pierson", "Nikhil Garg"], "summary": "Graph neural networks (GNNs) are widely used in urban spatiotemporal\nforecasting, such as predicting infrastructure problems. In this setting,\ngovernment officials wish to know in which neighborhoods incidents like\npotholes or rodent issues occur. The true state of incidents (e.g., street\nconditions) for each neighborhood is observed via government inspection\nratings. However, these ratings are only conducted for a sparse set of\nneighborhoods and incident types. We also observe the state of incidents via\ncrowdsourced reports, which are more densely observed but may be biased due to\nheterogeneous reporting behavior. First, for such settings, we propose a\nmultiview, multioutput GNN-based model that uses both unbiased rating data and\nbiased reporting data to predict the true latent state of incidents. Second, we\ninvestigate a case study of New York City urban incidents and collect,\nstandardize, and make publicly available a dataset of 9,615,863 crowdsourced\nreports and 1,041,415 government inspection ratings over 3 years and across 139\ntypes of incidents. Finally, we show on both real and semi-synthetic data that\nour model can better predict the latent state compared to models that use only\nreporting data or models that use only rating data, especially when rating data\nis sparse and reports are predictive of ratings. We also quantify demographic\nbiases in crowdsourced reporting, e.g., higher-income neighborhoods report\nproblems at higher rates. Our analysis showcases a widely applicable approach\nfor latent state prediction using heterogeneous, sparse, and biased data.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08740v1", "AI": {"title_translation": "使用图神经网络进行城市事件预测：整合政府评级和众包报告", "tldr": "该研究提出了一种多视图、多输出的图神经网络模型，结合政府评级和众包报告来预测城市事件的真实潜在状态，并在纽约市事件数据上验证了其优越性。", "motivation": "政府官员需要了解城市中基础设施问题（如坑洼、鼠患）的发生地点。虽然政府检查评级提供了真实状态，但数据稀疏；众包报告数据更密集，但存在偏差。因此，需要一种模型来有效整合这两种异构数据源，以准确预测事件的真实潜在状态。", "method": "该研究提出了一种多视图、多输出的基于图神经网络（GNN）的模型，该模型同时利用无偏的政府评级数据和有偏的众包报告数据来预测事件的真实潜在状态。此外，研究还收集、标准化并公开了一个包含9,615,863条众包报告和1,041,415条政府检查评级的数据集，涵盖纽约市3年间139种事件类型。", "result": "在真实和半合成数据上，该模型与仅使用报告数据或仅使用评级数据的模型相比，能更好地预测潜在状态，尤其是在评级数据稀疏且报告能预测评级的情况下。研究还量化了众包报告中的人口统计偏差，例如，高收入社区报告问题的比例更高。", "conclusion": "该研究提出的方法提供了一种广泛适用的方法，用于使用异构、稀疏和有偏数据进行潜在状态预测。", "translation": "图神经网络（GNN）广泛应用于城市时空预测，例如预测基础设施问题。在这种情况下，政府官员希望知道坑洼或鼠患等事件发生在哪些社区。每个社区事件的真实状态（例如街道状况）通过政府检查评级进行观察。然而，这些评级仅针对稀疏的社区和事件类型进行。我们还通过众包报告观察事件状态，这些报告观察更密集，但可能由于异构报告行为而存在偏差。首先，针对这种情况，我们提出了一种多视图、多输出的基于GNN的模型，该模型利用无偏的评级数据和有偏的报告数据来预测事件的真实潜在状态。其次，我们调查了纽约市城市事件的案例研究，并收集、标准化并公开了一个包含9,615,863条众包报告和1,041,415条政府检查评级的数据集，历时3年，涵盖139种事件类型。最后，我们在真实和半合成数据上表明，与仅使用报告数据或仅使用评级数据的模型相比，我们的模型可以更好地预测潜在状态，尤其是在评级数据稀疏且报告能够预测评级的情况下。我们还量化了众包报告中的人口统计偏差，例如，高收入社区报告问题的比例更高。我们的分析展示了一种使用异构、稀疏和有偏数据进行潜在状态预测的广泛适用方法。", "summary": "该论文提出了一种基于图神经网络的多视图、多输出模型，用于城市事件预测。该模型创新性地整合了政府稀疏但无偏的检查评级数据和众包密集但有偏的报告数据，以预测城市事件的真实潜在状态。研究以纽约市的城市事件为例，构建了一个大规模的公开数据集，并在真实和半合成数据上验证了所提模型的优越性，尤其在评级数据稀疏时表现突出。此外，论文还分析了众包报告中的人口统计偏差，强调了其方法在处理异构、稀疏和有偏数据进行潜在状态预测方面的广泛适用性。", "keywords": "图神经网络, 城市事件预测, 众包数据, 政府评级, 数据融合", "comments": "该论文的创新点在于提出了一个多视图、多输出的GNN模型，有效地整合了两种具有不同特性（稀疏/密集，无偏/有偏）的数据源来预测城市事件的潜在状态，这在实际应用中非常重要。此外，论文构建并公开了一个大规模的真实世界数据集，这对于后续研究具有重要的推动作用。研究还关注了众包数据中存在的偏差问题，并进行了量化分析，增加了研究的深度和实用性。"}}
{"id": "2506.09040", "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09040v1", "AI": {"title_translation": "自回归语义视觉重建有助于VLM更好地理解", "tldr": "本文提出自回归语义视觉重建（ASVR），通过联合学习视觉和文本模态，显著提升了大型视觉-语言模型（LVLMs）的理解能力，尤其是在重建图像的语义表示而非原始视觉外观时。", "motivation": "典型的大型视觉-语言模型（LVLMs）仅对文本序列应用自回归监督，没有将视觉模态充分融入学习过程，导致：1) 无法利用无配对字幕的图像；2) 字幕可能遗漏关键视觉细节；3) 某些以视觉为中心的内容无法通过文本充分传达。这使得当前LVLMs可能忽视细粒度视觉信息。", "method": "引入自回归语义视觉重建（ASVR），在一个统一的自回归框架内实现视觉和文本模态的联合学习。ASVR通过自回归重建图像的语义表示来增强理解，而不是重建原始视觉外观。即使模型输入连续图像特征，也能有效重建离散语义token。", "result": "自回归重建图像的原始视觉外观不会增强甚至可能损害多模态理解。相反，自回归重建图像的语义表示能持续改善理解。即使模型输入连续图像特征，也能有效重建离散语义token，并在广泛的多模态理解基准上带来稳定一致的改进。该方法在不同数据规模（556k-2M）和LLM骨干网络类型上均实现了显著性能提升。具体而言，ASVR使LLaVA-1.5在14个多模态基准上的平均分数提高了5%。", "conclusion": "通过自回归语义视觉重建（ASVR），特别是重建图像的语义表示，可以有效弥补当前大型视觉-语言模型在视觉理解方面的不足，显著提升其多模态理解能力。", "translation": "典型的大型视觉-语言模型（LVLMs）仅对文本序列应用自回归监督，没有将视觉模态充分融入学习过程。这导致了三个关键限制：（1）无法利用没有附带字幕的图像，（2）字幕可能遗漏关键视觉细节的风险，以及（3）某些以视觉为中心的内容无法通过文本充分传达的挑战。因此，当前的LVLMs通常优先考虑视觉到语言的对齐，而可能忽视细粒度的视觉信息。尽管一些先前的工作探索了自回归图像生成，但有效利用自回归视觉监督来增强图像理解仍然是一个开放的挑战。在本文中，我们引入了自回归语义视觉重建（ASVR），它在一个统一的自回归框架内实现了视觉和文本模态的联合学习。我们发现自回归重建图像的原始视觉外观并不能增强甚至可能损害多模态理解。相比之下，自回归重建图像的语义表示持续改善了理解。值得注意的是，我们发现即使模型输入连续图像特征，它们也能有效地重建离散语义token，从而在广泛的多模态理解基准上带来稳定一致的改进。我们的方法在不同数据规模（556k-2M）和LLM骨干网络类型上均实现了显著的性能提升。具体而言，ASVR使LLaVA-1.5在14个多模态基准上的平均分数提高了5%。代码可在https://github.com/AlenjandroWang/ASVR获取。", "summary": "本文提出自回归语义视觉重建（ASVR）方法，旨在解决现有大型视觉-语言模型（LVLMs）在视觉模态利用不足的问题。ASVR在一个统一的自回归框架下联合学习视觉和文本模态，通过自回归重建图像的语义表示而非原始像素来显著提升LVLMs的多模态理解能力。实验证明，ASVR在多种基准测试和不同规模数据上均取得了稳定且显著的性能提升，例如使LLaVA-1.5的平均分数提高了5%。", "keywords": "自回归学习, 语义视觉重建, 视觉-语言模型, 多模态理解, 图像理解", "comments": "这篇论文的创新点在于提出了“自回归语义视觉重建”这一概念，并证明了重建图像的语义表示比重建原始像素更能有效提升VLMs的理解能力。它解决了现有VLMs在视觉信息利用上的局限性，特别是那些没有字幕的图像或文本难以描述的视觉细节。这一方法为未来多模态模型的训练提供了新的方向，强调了语义层面的视觉理解对于模型性能的重要性。"}}
{"id": "2506.08764", "title": "On the Stability of the Jacobian Matrix in Deep Neural Networks", "authors": ["Benjamin Dadoun", "Soufiane Hayou", "Hanan Salam", "Mohamed El Amine Seddik", "Pierre Youssef"], "summary": "Deep neural networks are known to suffer from exploding or vanishing\ngradients as depth increases, a phenomenon closely tied to the spectral\nbehavior of the input-output Jacobian. Prior work has identified critical\ninitialization schemes that ensure Jacobian stability, but these analyses are\ntypically restricted to fully connected networks with i.i.d. weights. In this\nwork, we go significantly beyond these limitations: we establish a general\nstability theorem for deep neural networks that accommodates sparsity (such as\nthat introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.\ninduced by training). Our results rely on recent advances in random matrix\ntheory, and provide rigorous guarantees for spectral stability in a much\nbroader class of network models. This extends the theoretical foundation for\ninitialization schemes in modern neural networks with structured and dependent\nrandomness.", "comment": "16 pages, 26 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08764v1", "AI": {"title_translation": "深度神经网络中雅可比矩阵的稳定性研究", "tldr": "本文提出了一种通用的深度神经网络稳定性定理，解决了现有研究在处理稀疏性和非独立同分布权重时的局限性，为更广泛的网络模型提供了严格的谱稳定性保证。", "motivation": "深度神经网络随着深度增加会出现梯度爆炸或消失问题，这与输入-输出雅可比矩阵的谱行为密切相关。现有工作在确保雅可比稳定性方面存在局限性，通常仅限于全连接网络和独立同分布权重。", "method": "本文利用随机矩阵理论的最新进展，建立了一个通用的深度神经网络稳定性定理，该定理能够适应稀疏性（例如剪枝引入的）和非独立同分布、弱相关权重（例如训练引起的）。", "result": "提供了在更广泛的网络模型中谱稳定性的严格保证。", "conclusion": "扩展了现代神经网络中（具有结构化和依赖性随机性的）初始化方案的理论基础。", "translation": "深度神经网络随着深度增加会遭受梯度爆炸或消失的问题，这种现象与输入-输出雅可比矩阵的谱行为密切相关。先前的研究已经确定了确保雅可比稳定性关键的初始化方案，但这些分析通常仅限于具有独立同分布权重的全连接网络。在这项工作中，我们显著超越了这些限制：我们为深度神经网络建立了一个通用的稳定性定理，该定理能够适应稀疏性（例如剪枝引入的）和非独立同分布、弱相关权重（例如训练引起的）。我们的结果依赖于随机矩阵理论的最新进展，并为更广泛的网络模型中的谱稳定性提供了严格的保证。这扩展了现代神经网络中（具有结构化和依赖性随机性的）初始化方案的理论基础。", "summary": "本文针对深度神经网络中梯度爆炸/消失问题，提出了一个通用的雅可比矩阵稳定性定理。该定理克服了现有研究仅限于独立同分布权重的局限，能够处理稀疏性及训练引起的非独立同分布、弱相关权重。通过利用随机矩阵理论，研究为更广泛的网络模型提供了严格的谱稳定性保证，从而扩展了现代神经网络初始化方案的理论基础。", "keywords": "深度神经网络, 雅可比矩阵, 稳定性, 随机矩阵理论, 梯度消失/爆炸", "comments": "这项工作在理论上具有重要意义，它扩展了深度神经网络稳定性分析的范围，使其能够应用于更接近实际训练场景（如稀疏网络和训练后权重）的模型。利用随机矩阵理论是其创新点。"}}
{"id": "2506.09042", "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "comment": "Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao: Equal contribution.\n  Only the core contributors are listed. The full list of contributors can be\n  found in Appendix A of this paper", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09042v1", "AI": {"title_translation": "Cosmos-Drive-Dreams：基于世界基础模型的可扩展合成驾驶数据生成", "tldr": "该论文介绍了Cosmos-Drive-Dreams，一个利用NVIDIA Cosmos世界基础模型生成高保真、具有挑战性的驾驶场景的合成数据生成管道，有助于缓解长尾分布问题并提高自动驾驶（AV）任务的泛化能力。", "motivation": "收集和标注用于自动驾驶（AV）系统的真实世界数据既耗时又昂贵，尤其难以捕获对训练和测试至关重要的罕见边缘案例。", "method": "论文提出了Cosmos-Drive-Dreams，一个合成数据生成（SDG）管道。它由Cosmos-Drive提供支持，后者是NVIDIA Cosmos世界基础模型在驾驶领域的专用模型套件，能够生成可控、高保真、多视角和时空一致的驾驶视频。该管道旨在扩展高保真和挑战性驾驶数据集的数量和多样性。", "result": "生成的合成数据有助于缓解长尾分布问题，并增强3D车道检测、3D目标检测和驾驶策略学习等下游任务的泛化能力。该管道工具包、数据集和模型权重已开源。", "conclusion": "Cosmos-Drive-Dreams通过生成可扩展和多样化的合成驾驶数据，有效解决了真实世界数据收集的挑战，提高了泛化能力，并缓解了自动驾驶系统中的长尾问题。", "translation": "收集和标注用于安全关键物理AI系统（如自动驾驶汽车（AV））的真实世界数据既耗时又昂贵。捕获在AV系统训练和测试中起关键作用的罕见边缘案例尤其具有挑战性。为了解决这一挑战，我们引入了Cosmos-Drive-Dreams——一个合成数据生成（SDG）管道，旨在生成具有挑战性的场景，以促进下游任务，如感知和驾驶策略训练。该管道由Cosmos-Drive提供支持，Cosmos-Drive是NVIDIA Cosmos世界基础模型在驾驶领域的专用模型套件，能够生成可控、高保真、多视角和时空一致的驾驶视频。我们通过将Cosmos-Drive-Dreams应用于扩展具有高保真和挑战性场景的驾驶数据集的数量和多样性来展示这些模型的实用性。实验表明，我们生成的数据有助于缓解长尾分布问题，并增强3D车道检测、3D目标检测和驾驶策略学习等下游任务的泛化能力。我们通过NVIDIA的Cosmos平台开源了我们的管道工具包、数据集和模型权重。项目页面：https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "summary": "本论文介绍了Cosmos-Drive-Dreams，一个旨在克服自动驾驶汽车真实世界数据收集限制（特别是罕见边缘案例）的合成数据生成（SDG）管道。该管道利用Cosmos-Drive（源自NVIDIA Cosmos世界基础模型的专用模型套件）生成可控、高保真、多视角和时空一致的驾驶视频。Cosmos-Drive-Dreams的实用性体现在其能够扩展具有挑战性驾驶场景的数量和多样性。实验结果表明，生成的合成数据有效缓解了长尾分布问题，并增强了3D车道检测、3D目标检测和驾驶策略学习等关键下游任务的泛化能力。作者还开源了他们的工具包、数据集和模型权重。", "keywords": "合成数据生成, 自动驾驶, 世界基础模型, 驾驶场景, 长尾分布", "comments": "这篇论文提出了一种创新方法，解决了自动驾驶系统数据稀缺性和多样性的关键挑战。通过利用世界基础模型进行合成数据生成，它提供了一种可扩展且经济高效的解决方案，尤其适用于难以捕获的边缘案例。所展示的在长尾分布问题和泛化能力方面的改进对于自动驾驶汽车的鲁棒性和可靠性至关重要。开源资源将极大地造福研究社区。"}}
{"id": "2506.08837", "title": "Design Patterns for Securing LLM Agents against Prompt Injections", "authors": ["Luca Beurer-Kellner", "Beat Buesser Ana-Maria Creţu", "Edoardo Debenedetti", "Daniel Dobos", "Daniel Fabian", "Marc Fischer", "David Froelicher", "Kathrin Grosse", "Daniel Naeff", "Ezinwanne Ozoani", "Andrew Paverd", "Florian Tramèr", "Václav Volhejn"], "summary": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08837v1", "AI": {"title_translation": "LLM智能体抵御提示注入攻击的设计模式", "tldr": "本文提出了一套有原则的设计模式，旨在提高大型语言模型（LLM）驱动的AI智能体对提示注入攻击的防御能力。", "motivation": "随着大型语言模型（LLM）驱动的AI智能体变得越来越通用和强大，确保其安全性已成为一项严峻的挑战。其中，提示注入攻击是一种尤其危险的威胁，特别是在智能体被授予工具访问权限或处理敏感信息时。", "method": "本文提出了一套有原则的设计模式，用于构建对提示注入具有可证明抵抗力的AI智能体。研究系统地分析了这些模式，讨论了它们在实用性和安全性方面的权衡，并通过一系列案例研究说明了其在现实世界中的适用性。", "result": "提出了对提示注入具有可证明抵抗力的设计模式，并通过一系列案例研究展示了这些模式在现实世界中的适用性。", "conclusion": "本文提出的设计模式能够有效提高大型语言模型（LLM）智能体对提示注入攻击的防御能力，为构建更安全的AI智能体提供了有价值的指导。", "translation": "随着大型语言模型（LLM）驱动的AI智能体变得越来越通用且能够处理广泛的任务，确保其安全性已成为一项严峻的挑战。其中最紧迫的威胁之一是提示注入攻击，这种攻击利用了智能体对自然语言输入的依赖性——当智能体被授予工具访问权限或处理敏感信息时，这种威胁尤其危险。在这项工作中，我们提出了一套有原则的设计模式，用于构建对提示注入具有可证明抵抗力的AI智能体。我们系统地分析了这些模式，讨论了它们在实用性和安全性方面的权衡，并通过一系列案例研究说明了它们在现实世界中的适用性。", "summary": "本文针对大型语言模型（LLM）驱动的AI智能体所面临的提示注入攻击这一关键安全威胁，提出了一套有原则的设计模式。这些设计模式旨在为AI智能体提供可证明的提示注入抵抗力。研究对这些模式进行了系统分析，探讨了它们在实用性和安全性之间的权衡，并通过一系列案例研究展示了其在现实世界中的应用潜力。", "keywords": "LLM智能体安全, 提示注入, 设计模式, AI安全, 大型语言模型", "comments": "这篇论文的创新之处在于提出了专门针对LLM智能体提示注入攻击的“有原则的设计模式”，并强调了“可证明抵抗力”这一重要特性。这种结构化的安全设计方法对于提升AI智能体在处理敏感数据或拥有工具访问权限时的安全性具有重要实践意义。"}}
{"id": "2506.09045", "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.", "comment": "Project Page: https://zehong-ma.github.io/MagCache", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09045v1", "AI": {"title_translation": "磁力缓存：基于幅度感知的快速视频生成", "tldr": "MagCache通过发现残差输出的幅度规律，实现了视频扩散模型加速，仅需单样本校准，并显著提升了生成速度和视觉质量。", "motivation": "现有视频扩散模型加速技术依赖统一的启发式或时间嵌入变体，需要大量校准样本，且存在提示词特异性过拟合导致输出不一致的风险。", "method": "发现了一个统一的幅度定律：连续残差输出的幅度比在大多数时间步单调稳定下降，在最后几步迅速下降。基于此，提出了Magnitude-aware Cache (MagCache)，通过误差建模机制和自适应缓存策略，自适应跳过不重要的时间步，且仅需单个样本进行校准。", "result": "MagCache在Open-Sora和Wan 2.1上分别实现了2.1倍和2.68倍的加速，同时保持了卓越的视觉保真度。在相似计算预算下，LPIPS、SSIM和PSNR指标上显著优于现有方法。", "conclusion": "MagCache通过利用新发现的幅度定律和创新的自适应缓存策略，克服了现有视频生成加速技术的局限性，提供了更高效、更鲁棒且高质量的解决方案。", "translation": "现有视频扩散模型的加速技术通常依赖统一的启发式方法或时间嵌入变体来跳过时间步并重用缓存特征。这些方法通常需要通过精心设计的提示词进行广泛校准，并且由于提示词特异性过拟合而存在输出不一致的风险。在本文中，我们引入了一个新颖且鲁棒的发现：在不同模型和提示词中观察到统一的幅度定律。具体来说，连续残差输出的幅度比在大多数时间步中单调稳定下降，而在最后几个时间步中迅速下降。利用这一见解，我们引入了一种幅度感知缓存（MagCache），它使用误差建模机制和自适应缓存策略自适应地跳过不重要的时间步。与现有方法需要数十个精心设计的样本进行校准不同，MagCache仅需要单个样本进行校准。实验结果表明，MagCache在Open-Sora和Wan 2.1上分别实现了2.1倍和2.68倍的加速，同时保持了卓越的视觉保真度。在可比的计算预算下，它在LPIPS、SSIM和PSNR方面显著优于现有方法。", "summary": "本文提出了一种名为MagCache的快速视频生成方法，旨在解决现有视频扩散模型加速技术效率低下和鲁棒性差的问题。通过发现一个统一的残差输出幅度定律，MagCache利用误差建模和自适应缓存策略智能跳过不重要的时间步。该方法仅需单样本校准，显著优于现有技术，在Open-Sora和Wan 2.1上实现了2.1倍和2.68倍的加速，同时保持了高质量的视觉效果。", "keywords": "视频生成, 扩散模型, 加速, 幅度感知缓存, MagCache", "comments": "这篇论文的创新点在于发现了视频扩散模型中残差输出的统一幅度定律，并基于此设计了高效的MagCache加速机制。其优势在于仅需单样本校准，大大降低了校准成本和复杂性，解决了现有方法对大量精心设计样本的依赖及其导致的过拟合问题，显著提升了视频生成速度和质量。"}}
{"id": "2506.08403", "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration", "authors": ["Weiya Li", "Junjie Chen", "Bei Li", "Boyang Liu", "Zichen Wen", "Nuanqiao Shan", "Xiaoqian Liu", "Anping Liu", "Huajie Liu", "Youyan Wang", "Wujiuge Yin", "Hu Song", "Bing Huang", "Zhiyuan Xia", "Jialiang Chen", "Linfeng Zhang"], "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.", "comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08403v1", "AI": {"title_translation": "TACTIC：认知理论交互协作的翻译智能体", "tldr": "TACTIC是一个受认知启发的LLM多智能体翻译框架，通过模拟人类翻译认知过程，在FLORES-200和WMT24基准测试中实现了SOTA性能。", "motivation": "现有的多智能体翻译框架在很大程度上忽视了认知翻译研究中的基本见解，即人类译者如何运用不同的认知策略进行翻译，如平衡直译和意译、根据语境细化表达和迭代评估输出。", "method": "本文提出了一个名为TACTIC的认知启发多智能体框架，它包含六个功能独特的智能体，模仿人类翻译行为中的关键认知过程，包括起草、细化、评估、评分、语境推理和外部知识收集。该框架通过模拟交互式和理论基础的翻译工作流程来利用LLM的能力。", "result": "在FLORES-200和WMT24基准测试的多种语言对上，TACTIC持续实现了最先进的性能。使用DeepSeek-V3作为基础模型，TACTIC平均超越GPT-4.1 +0.6 XCOMET和+1.18 COMETKIWI-23。与DeepSeek-R1相比，它进一步提升了+0.84 XCOMET和+2.99 COMETKIWI-23。", "conclusion": "TACTIC框架通过模拟交互式和理论基础的翻译工作流程，有效利用了大型语言模型的全部能力来实现高质量翻译，并在多个基准测试中达到了最先进的性能。", "translation": "机器翻译长期以来一直是自然语言处理的核心任务。随着大型语言模型（LLM）的快速发展，翻译质量取得了显著进步。然而，充分发挥LLM的翻译潜力仍然是一个开放的挑战。最近的研究探索了多智能体系统，将复杂的翻译任务分解为协作子任务，在通过智能体合作和专业化提升翻译质量方面显示出初步前景。尽管如此，现有的多智能体翻译框架在很大程度上忽视了认知翻译研究中的基本见解。这些见解强调了人类译者如何运用不同的认知策略，例如平衡直译和意译、根据语境细化表达以及迭代评估输出。为了解决这一限制，我们提出了一个名为TACTIC的认知启发多智能体框架，其全称为Translation Agents with Cognitive-Theoretic Interactive Collaboration。该框架包含六个功能独特的智能体，它们模仿人类翻译行为中观察到的关键认知过程。这些智能体包括起草、细化、评估、评分、语境推理和外部知识收集。通过模拟交互式和理论基础的翻译工作流程，TACTIC有效利用了LLM的全部能力来实现高质量翻译。在FLORES-200和WMT24基准测试中，对多种语言对的实验结果表明，我们的方法持续实现了最先进的性能。使用DeepSeek-V3作为基础模型，TACTIC平均超越GPT-4.1 +0.6 XCOMET和+1.18 COMETKIWI-23。与DeepSeek-R1相比，它进一步提升了+0.84 XCOMET和+2.99 COMETKIWI-23。代码可在https://github.com/weiyali126/TACTIC获取。", "summary": "这篇论文提出了一个名为TACTIC的认知启发多智能体翻译框架，旨在充分发挥大型语言模型在翻译方面的潜力。该框架通过模仿人类翻译的认知过程，设计了六个功能不同的智能体（包括起草、细化、评估、评分、语境推理和外部知识收集）。实验结果表明，TACTIC在FLORES-200和WMT24基准测试中，使用DeepSeek-V3作为基础模型，性能显著优于GPT-4.1和DeepSeek-R1等现有SOTA模型，证明了其在高质量翻译方面的有效性。", "keywords": "机器翻译, 大型语言模型, 多智能体系统, 认知翻译, TACTIC", "comments": "这项工作通过将认知翻译理论融入多智能体系统，为LLM驱动的机器翻译带来了创新。它不仅提升了翻译质量，还为理解和模拟人类翻译过程提供了新的视角，具有重要的研究价值和应用潜力。其模块化的智能体设计也为未来的研究提供了可扩展的框架。"}}
{"id": "2506.08850", "title": "Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing", "authors": ["Amin Avan", "Akramul Azim", "Qusay Mahmoud"], "summary": "Soft real-time applications are becoming increasingly complex, posing\nsignificant challenges for scheduling offloaded tasks in edge computing\nenvironments while meeting task timing constraints. Moreover, the exponential\ngrowth of the search space, presence of multiple objectives and parameters, and\nhighly dynamic nature of edge computing environments further exacerbate the\ncomplexity of task scheduling. As a result, schedulers based on heuristic and\nmetaheuristic algorithms frequently encounter difficulties in generating\noptimal or near-optimal task schedules due to their constrained ability to\nadapt to the dynamic conditions and complex environmental characteristics of\nedge computing. Accordingly, reinforcement learning algorithms have been\nincorporated into schedulers to address the complexity and dynamic conditions\ninherent in task scheduling in edge computing. However, a significant\nlimitation of reinforcement learning algorithms is the prolonged learning time\nrequired to adapt to new environments and to address medium- and large-scale\nproblems. This challenge arises from the extensive global action space and\nfrequent random exploration of irrelevant actions. Therefore, this study\nproposes Agile Reinforcement learning (aRL), in which the RL-agent performs\ninformed exploration and executes only relevant actions. Consequently, the\npredictability of the RL-agent is enhanced, leading to rapid adaptation and\nconvergence, which positions aRL as a suitable candidate for scheduling the\ntasks of soft real-time applications in edge computing. The experiments\ndemonstrate that the combination of informed exploration and action-masking\nmethods enables aRL to achieve a higher hit-ratio and converge faster than the\nbaseline approaches.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08850v1", "AI": {"title_translation": "边缘计算中实时任务调度的敏捷强化学习", "tldr": "针对边缘计算中实时任务调度面临的挑战，本研究提出了敏捷强化学习（aRL），通过智能探索和相关动作执行，显著缩短学习时间并提高调度性能。", "motivation": "软实时应用日益复杂，边缘计算中卸载任务调度面临巨大挑战，包括搜索空间指数增长、多目标、动态环境和难以满足时序约束。传统的启发式和元启发式算法难以适应动态环境，而现有强化学习算法学习时间过长，无法快速适应新环境和解决中大型问题，因为它们存在庞大的全局动作空间和频繁的无关动作随机探索问题。", "method": "提出了敏捷强化学习（aRL），其中RL代理执行智能探索并仅执行相关动作。具体方法结合了智能探索（informed exploration）和动作掩蔽（action-masking）。", "result": "实验表明，智能探索和动作掩蔽方法的结合使aRL比基线方法实现了更高的命中率和更快的收敛速度。", "conclusion": "aRL通过增强RL代理的可预测性，实现快速适应和收敛，使其成为边缘计算中软实时应用任务调度的合适选择。", "translation": "软实时应用日益复杂，对边缘计算环境中卸载任务的调度提出了严峻挑战，同时需要满足任务的时序约束。此外，搜索空间的指数级增长、多目标和参数的存在以及边缘计算环境的高度动态性进一步加剧了任务调度的复杂性。因此，基于启发式和元启发式算法的调度器由于其适应动态条件和边缘计算复杂环境特征的能力有限，经常难以生成最优或接近最优的任务调度。为此，强化学习算法已被引入调度器中，以解决边缘计算中任务调度固有的复杂性和动态条件。然而，强化学习算法的一个显著局限性是适应新环境以及解决中大型问题所需的学习时间过长。这一挑战源于庞大的全局动作空间和频繁的无关动作随机探索。因此，本研究提出了敏捷强化学习（aRL），其中RL代理执行智能探索并仅执行相关动作。因此，RL代理的可预测性得到增强，从而实现快速适应和收敛，这使得aRL成为边缘计算中软实时应用任务调度的合适候选方案。实验表明，智能探索和动作掩蔽方法的结合使aRL比基线方法实现了更高的命中率和更快的收敛速度。", "summary": "本文针对边缘计算中实时任务调度面临的复杂性和动态性挑战，提出了一种名为敏捷强化学习（aRL）的新方法。鉴于传统启发式算法适应性差以及现有强化学习算法学习时间过长的局限性，aRL通过智能探索和动作掩蔽机制，使RL代理能够更有效地选择相关动作，从而显著缩短学习时间、加速收敛并提高任务调度性能。实验结果验证了aRL在命中率和收敛速度方面优于基线方法。", "keywords": "敏捷强化学习, 边缘计算, 实时任务调度, 智能探索, 动作掩蔽", "comments": "本文提出了一种新颖的敏捷强化学习框架aRL，有效解决了传统强化学习在边缘计算实时任务调度中学习时间过长的问题。其核心创新在于引入了智能探索和动作掩蔽机制，这显著提升了RL代理的效率和可预测性。该方法对于提升边缘计算系统在动态环境下的实时任务调度能力具有重要意义。"}}
{"id": "2506.08882", "title": "Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data", "authors": ["Dimitrios Amaxilatis", "Themistoklis Sarantakos", "Ioannis Chatzigiannakis", "Georgios Mylonas"], "summary": "In this work, we explore the application of recent data imputation techniques\nto enhance monitoring and management of water distribution networks using smart\nwater meters, based on data derived from a real-world IoT water grid monitoring\ndeployment. Despite the detailed data produced by such meters, data gaps due to\ntechnical issues can significantly impact operational decisions and efficiency.\nOur results, by comparing various imputation methods, such as k-Nearest\nNeighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate\nthat effective data imputation can substantially enhance the quality of the\ninsights derived from water consumption data as we study their effect on\naccuracy and reliability of water metering data to provide solutions in\napplications like leak detection and predictive maintenance scheduling.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08882v1", "AI": {"title_translation": "填补空白：在不完整的水表数据中应用数据插补", "tldr": "本文探讨了在智能水表收集到的不完整水计量数据中应用数据插补技术，以提高水分配网络的监控和管理，并展示了其在泄漏检测和预测性维护等应用中的有效性。", "motivation": "尽管智能水表能产生详细数据，但技术问题导致的数据空白会严重影响水务运营决策和效率，因此需要有效的数据插补方法来提升数据质量和洞察力。", "method": "本文比较了多种数据插补方法，包括k-最近邻（k-Nearest Neighbors）、MissForest、Transformer和循环神经网络（Recurrent Neural Networks）。", "result": "研究结果表明，有效的数据插补可以显著提高从水消耗数据中获得的洞察力质量，并提升水表数据的准确性和可靠性。", "conclusion": "有效的数据插补可以为泄漏检测和预测性维护调度等应用提供解决方案，从而增强水分配网络的监控和管理。", "translation": "在这项工作中，我们基于一个真实世界的物联网水网监控部署数据，探索了应用最新数据插补技术来增强使用智能水表对水分配网络的监控和管理。尽管此类水表能产生详细数据，但由于技术问题导致的数据空白会严重影响运营决策和效率。我们的结果通过比较各种插补方法，如k-最近邻、MissForest、Transformer和循环神经网络，表明有效的数据插补可以显著提高从水消耗数据中获得的洞察力质量，因为我们研究了它们对水计量数据准确性和可靠性的影响，从而为泄漏检测和预测性维护调度等应用提供解决方案。", "summary": "本研究旨在解决智能水表数据中存在的空白问题，通过应用k-最近邻、MissForest、Transformer和循环神经网络等数据插补技术，提升水分配网络的监控和管理效率。研究结果表明，有效的数据插补能显著提高水消耗数据的质量、准确性和可靠性，为泄漏检测和预测性维护等应用提供支持。", "keywords": "数据插补, 智能水表, 水分配网络, 数据质量, 泄漏检测", "comments": "该论文的创新点在于将先进的数据插补技术应用于实际的智能水表数据，以解决数据不完整性问题。这对于提升水务管理的效率和决策质量具有重要意义，尤其是在泄漏检测和预测性维护方面。"}}
{"id": "2506.08889", "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning", "authors": ["Yizhao Gao", "Shuming Guo", "Shijie Cao", "Yuqing Xia", "Yu Cheng", "Lei Wang", "Lingxiao Ma", "Yutao Sun", "Tianzhu Ye", "Li Dong", "Hayden Kwok-Hay So", "Yu Hua", "Ting Cao", "Fan Yang", "Mao Yang"], "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08889v1", "AI": {"title_translation": "SeerAttention-R：长推理的稀疏注意力自适应", "tldr": "SeerAttention-R 是一种用于长推理的稀疏注意力框架，通过自蒸馏门控机制学习注意力稀疏性，实现了近乎无损的推理精度和显著的解码速度提升。", "motivation": "针对推理模型的长解码问题，现有注意力机制效率低下，需要一种能够有效处理长序列并保持性能的稀疏注意力框架。", "method": "引入 SeerAttention-R，它扩展自 SeerAttention，通过自蒸馏门控机制学习注意力稀疏性，并移除查询池化以适应自回归解码。它是一个轻量级插件，无需修改现有预训练模型参数。", "result": "在 AIME 基准测试中，SeerAttention-R 在 0.4B token 上训练，在 4K token 预算下，即使在大稀疏注意力块大小 (64/128) 下也能保持近乎无损的推理精度。使用 TileLang 开发的稀疏解码内核在 H100 GPU 上以 90% 稀疏度实现了比 FlashAttention-3 高达 9 倍的接近理论速度提升。", "conclusion": "SeerAttention-R 提供了一种高效且性能优越的稀疏注意力解决方案，显著提升了长推理模型的解码速度和效率，同时保持了高精度。", "translation": "我们引入了 SeerAttention-R，一个专门为推理模型的长解码量身定制的稀疏注意力框架。SeerAttention-R 扩展自 SeerAttention，保留了通过自蒸馏门控机制学习注意力稀疏性的设计，同时移除了查询池化以适应自回归解码。SeerAttention-R 采用轻量级插件式门控，灵活性高，可以轻松集成到现有预训练模型中，而无需修改原始参数。我们证明，SeerAttention-R 仅在 0.4B token 上训练，在 AIME 基准测试中，在 4K token 预算下，即使在大的稀疏注意力块大小（64/128）下也能保持近乎无损的推理精度。使用 TileLang，我们开发了一个高度优化的稀疏解码内核，在 H100 GPU 上以 90% 的稀疏度实现了比 FlashAttention-3 高达 9 倍的接近理论速度提升。代码可在 https://github.com/microsoft/SeerAttention 获取。", "summary": "本文介绍了 SeerAttention-R，一种为长推理模型解码设计的稀疏注意力框架。它通过自蒸馏门控机制学习注意力稀疏性，并适应自回归解码。作为轻量级插件，它易于集成。实验表明，SeerAttention-R 在保持近乎无损推理精度的同时，实现了显著的解码速度提升，在 90% 稀疏度下比 FlashAttention-3 快 9 倍。", "keywords": "稀疏注意力, 长推理, 自蒸馏门控, 解码加速, SeerAttention-R", "comments": "SeerAttention-R 的创新之处在于其针对长推理的稀疏注意力自适应，特别是通过自蒸馏门控机制学习稀疏性，并能够作为轻量级插件集成到现有模型中。其在推理精度保持和解码速度提升方面（高达 9 倍加速）的表现非常重要，为处理长序列的推理任务提供了高效的解决方案。"}}
{"id": "2506.08902", "title": "Intention-Conditioned Flow Occupancy Models", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08902v1", "AI": {"title_translation": "意图条件流占用模型", "tldr": "该论文提出了InFOM，一种用于预训练大型强化学习模型的新方法，通过使用流匹配预测以用户意图为条件的未来状态，并展示了显著的性能改进。", "motivation": "将大规模预训练应用于强化学习（RL）可以解决样本效率和鲁棒性等核心挑战，但在RL背景下预训练大型模型面临动作具有长期依赖性的根本性挑战。", "method": "本文构建了一个概率模型，利用流匹配来预测代理在未来将访问的状态（即占用度量）。为了增加模型的表达能力并实现适应性，模型中包含了一个捕获用户意图的潜在变量。该方法被称为意图条件流占用模型（InFOM）。", "result": "与替代的预训练方法相比，在36个基于状态和4个基于图像的基准任务上的实验表明，所提出的方法在回报方面实现了1.8倍的中位数改进，并将成功率提高了36%。", "conclusion": "本文提出的意图条件流占用模型（InFOM）通过建模以用户意图为条件的未来状态占用，有效地解决了强化学习中预训练大型模型的挑战，从而显著提高了性能。", "translation": "大规模预训练已经从根本上改变了当今机器学习研究的方式：大型基础模型经过一次训练，然后社区中的任何人（包括那些没有数据或计算资源从头开始训练模型的人）都可以使用它们来适应和微调特定任务。将相同的框架应用于强化学习（RL）具有吸引力，因为它为解决RL中的核心挑战（包括样本效率和鲁棒性）提供了引人注目的途径。然而，在RL背景下预训练大型模型仍然存在一个根本性挑战：动作具有长期依赖性，因此训练一个能够进行跨时间推理的基础模型非常重要。生成式AI的最新进展为建模高度复杂分布提供了新工具。在本文中，我们构建了一个概率模型，使用流匹配来预测代理在时间上遥远的未来将访问哪些状态（即占用度量）。由于大型数据集通常由许多执行不同任务的不同用户构建，我们在模型中包含了一个捕获用户意图的潜在变量。这种意图增加了我们模型的表达能力，并通过广义策略改进实现了适应。我们将我们提出的方法称为意图条件流占用模型（InFOM）。与替代的预训练方法相比，我们在36个基于状态和4个基于图像的基准任务上的实验表明，所提出的方法在回报方面实现了1.8倍的中位数改进，并将成功率提高了36%。网站：https://chongyi-zheng.github.io/infom 代码：https://github.com/chongyi-zheng/infom", "summary": "本文介绍了意图条件流占用模型（InFOM），这是一种用于预训练大型强化学习（RL）模型的新型概率模型。它利用流匹配来预测未来状态占用，并引入一个捕获用户意图的潜在变量，从而解决了RL预训练中长期动作依赖性的挑战。InFOM在多项基准任务上展现出显著的性能提升，实现了1.8倍的回报中位数改进和36%的成功率提升，使得大规模预训练在RL领域更加有效。", "keywords": "强化学习, 预训练, 流匹配, 占用模型, 用户意图", "comments": "该论文通过引入“意图条件流占用模型（InFOM）”提出了一种创新性的方法来预训练强化学习中的大型模型。其核心创新在于使用流匹配来建模未来状态占用，并关键性地融入了用户意图的潜在变量。这解决了RL预训练中与长期依赖性和多样化用户任务相关的核心挑战，从而实现了更具表达能力的模型和更好的适应性。显著的实验结果（1.8倍回报改进，36%成功率提升）突显了其在推动大规模RL基础模型方面的实际重要性。"}}
{"id": "2506.08916", "title": "Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)", "authors": ["Maria-Veronica Ciocanel", "John T. Nardini", "Kevin B. Flores", "Erica M. Rutter", "Suzanne S. Sindi", "Alexandria Volkening"], "summary": "Agent-based modeling (ABM) is a powerful tool for understanding\nself-organizing biological systems, but it is computationally intensive and\noften not analytically tractable. Equation learning (EQL) methods can derive\ncontinuum models from ABM data, but they typically require extensive\nsimulations for each parameter set, raising concerns about generalizability. In\nthis work, we extend EQL to Multi-experiment equation learning (ME-EQL) by\nintroducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns\nindividual models for each parameter set and connects them via interpolation,\nand embedded structure ME-EQL (ES ME-EQL), which builds a unified model library\nacross parameters. We demonstrate these methods using a birth--death mean-field\nmodel and an on-lattice agent-based model of birth, death, and migration with\nspatial structure. Our results show that both methods significantly reduce the\nrelative error in recovering parameters from agent-based simulations, with OAT\nME-EQL offering better generalizability across parameter space. Our findings\nhighlight the potential of equation learning from multiple experiments to\nenhance the generalizability and interpretability of learned models for complex\nbiological systems.", "comment": "31 pages, 10 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08916v1", "AI": {"title_translation": "使用多实验方程学习 (ME-EQL) 增强模型发现跨参数空间的泛化能力", "tldr": "引入多实验方程学习 (ME-EQL) 方法，通过处理多个参数集的数据，显著提高了从ABM数据中学习连续模型时的泛化能力和参数恢复准确性。", "motivation": "现有的方程学习 (EQL) 方法在从基于智能体的模型 (ABM) 数据中推导连续模型时，通常需要为每个参数集进行大量模拟，导致泛化能力差。", "method": "本文提出了多实验方程学习 (ME-EQL)，包括两种方法：一次一个ME-EQL (OAT ME-EQL)，它为每个参数集学习单独的模型并通过插值连接它们；以及嵌入式结构ME-EQL (ES ME-EQL)，它在不同参数之间构建统一的模型库。这些方法通过一个出生-死亡平均场模型和一个具有空间结构的出生、死亡和迁移的格点智能体模型进行了验证。", "result": "两种ME-EQL方法都显著降低了从智能体模拟中恢复参数的相对误差。其中，OAT ME-EQL在跨参数空间的泛化能力方面表现更佳。", "conclusion": "从多个实验中进行方程学习具有增强复杂生物系统学习模型的泛化能力和可解释性的潜力。", "translation": "基于智能体的建模（ABM）是理解自组织生物系统的强大工具，但其计算密集且通常难以进行分析处理。方程学习（EQL）方法可以从ABM数据中推导连续模型，但它们通常需要为每个参数集进行大量模拟，引发了对泛化能力的担忧。在这项工作中，我们通过引入两种方法将EQL扩展到多实验方程学习（ME-EQL）：一次一个ME-EQL（OAT ME-EQL），它为每个参数集学习单独的模型并通过插值连接它们；以及嵌入式结构ME-EQL（ES ME-EQL），它在参数之间构建统一的模型库。我们使用一个出生-死亡平均场模型和一个具有空间结构的出生、死亡和迁移的格点智能体模型演示了这些方法。我们的结果表明，这两种方法都显著降低了从基于智能体模拟中恢复参数的相对误差，其中OAT ME-EQL在跨参数空间的泛化能力方面表现更好。我们的发现突出了从多个实验中进行方程学习的潜力，以增强复杂生物系统学习模型的泛化能力和可解释性。", "summary": "本文提出了一种名为多实验方程学习 (ME-EQL) 的新方法，旨在解决现有方程学习 (EQL) 在从基于智能体的模型 (ABM) 数据推导连续模型时泛化能力差的问题。ME-EQL包含两种策略：一次一个ME-EQL (OAT ME-EQL) 和嵌入式结构ME-EQL (ES ME-EQL)。实验结果表明，ME-EQL显著提高了从ABM模拟中恢复参数的准确性，特别是OAT ME-EQL在跨参数空间表现出更好的泛化能力，从而增强了学习模型的泛化性和可解释性。", "keywords": "方程学习, 多实验, 泛化能力, 基于智能体的建模, 参数空间", "comments": "这项工作通过引入多实验方程学习 (ME-EQL) 显著提升了方程学习 (EQL) 在处理复杂生物系统时的实用性。其创新点在于从多个参数集的数据中学习，而非单独处理，从而解决了EQL在泛化能力上的关键限制。OAT ME-EQL和ES ME-EQL这两种方法的提出，为从ABM数据中高效、准确地提取连续模型提供了新的途径，对于理解和预测自组织生物系统具有重要意义。"}}
{"id": "2506.08928", "title": "Local MDI+: Local Feature Importances for Tree-Based Models", "authors": ["Zhongyuan Liang", "Zachary T. Rewolinski", "Abhineet Agarwal", "Tiffany M. Tang", "Bin Yu"], "summary": "Tree-based ensembles such as random forests remain the go-to for tabular data\nover deep learning models due to their prediction performance and computational\nefficiency. These advantages have led to their widespread deployment in\nhigh-stakes domains, where interpretability is essential for ensuring\ntrustworthy predictions. This has motivated the development of popular local\n(i.e. sample-specific) feature importance (LFI) methods such as LIME and\nTreeSHAP. However, these approaches rely on approximations that ignore the\nmodel's internal structure and instead depend on potentially unstable\nperturbations. These issues are addressed in the global setting by MDI+, a\nfeature importance method which exploits an equivalence between decision trees\nand linear models on a transformed node basis. However, the global MDI+ scores\nare not able to explain predictions when faced with heterogeneous individual\ncharacteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel\nextension of the MDI+ framework to the sample specific setting. LMDI+\noutperforms existing baselines LIME and TreeSHAP in identifying\ninstance-specific signal features, averaging a 10% improvement in downstream\ntask performance across twelve real-world benchmark datasets. It further\ndemonstrates greater stability by consistently producing similar instance-level\nfeature importance rankings across multiple random forest fits. Finally, LMDI+\nenables local interpretability use cases, including the identification of\ncloser counterfactuals and the discovery of homogeneous subgroups.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08928v1", "AI": {"title_translation": "局部MDI+：基于树模型的局部特征重要性", "tldr": "提出Local MDI+ (LMDI+)，一种新的局部特征重要性方法，解决了现有方法（如LIME和TreeSHAP）的局限性，在性能和稳定性上均优于它们，并支持局部可解释性应用。", "motivation": "树模型在表格数据上表现优异，但在高风险领域需要可解释性。现有局部特征重要性方法（如LIME、TreeSHAP）依赖近似和不稳定扰动，且全局MDI+无法解释异构个体预测。因此需要一种更准确、稳定的局部特征重要性方法。", "method": "提出Local MDI+ (LMDI+)，它是MDI+框架向样本特定设置的扩展。LMDI+利用了决策树和线性模型在转换节点基础上的等价性来计算局部特征重要性。", "result": "LMDI+在识别实例特定信号特征方面优于LIME和TreeSHAP，在12个真实世界基准数据集上，下游任务性能平均提高10%。它还通过在多次随机森林拟合中始终产生相似的实例级别特征重要性排名，展现出更高的稳定性。", "conclusion": "Local MDI+ (LMDI+)是一种有效且稳定的局部特征重要性方法，克服了现有方法的局限性，提升了解释性能，并支持多种局部可解释性应用。", "translation": "树模型集成（如随机森林）由于其预测性能和计算效率，仍然是表格数据而非深度学习模型的首选。这些优势使其在高风险领域得到广泛部署，而可解释性对于确保可信预测至关重要。这促使了LIME和TreeSHAP等流行的局部（即样本特定）特征重要性（LFI）方法的发展。然而，这些方法依赖于忽略模型内部结构并依赖于潜在不稳定扰动的近似值。这些问题在全局设置中通过MDI+得到解决，MDI+是一种特征重要性方法，它利用决策树和线性模型在转换节点基础上的等价性。然而，当面临异构个体特征时，全局MDI+分数无法解释预测。为了解决这一空白，我们提出了Local MDI+ (LMDI+)，这是MDI+框架向样本特定设置的一种新颖扩展。LMDI+在识别实例特定信号特征方面优于现有基线LIME和TreeSHAP，在12个真实世界基准数据集上，下游任务性能平均提高10%。它通过在多次随机森林拟合中始终产生相似的实例级别特征重要性排名，进一步展示了更高的稳定性。最后，LMDI+支持局部可解释性用例，包括识别更接近的反事实和发现同质子组。", "summary": "本文提出了一种名为Local MDI+ (LMDI+) 的新型局部特征重要性方法，旨在解决现有方法（如LIME和TreeSHAP）在解释树模型预测时存在的近似和不稳定性问题。LMDI+是全局MDI+框架的样本特定扩展，它利用了决策树与线性模型在转换节点上的等价性。实验结果表明，LMDI+在识别实例特定信号特征方面优于LIME和TreeSHAP，在下游任务性能上平均提高了10%，并表现出更高的稳定性。此外，LMDI+还支持多种局部可解释性应用，如反事实识别和同质子组发现。", "keywords": "局部特征重要性, 树模型, 可解释性, MDI+, LMDI+", "comments": "本文创新性地将全局MDI+的思想扩展到局部特征重要性，解决了现有局部可解释性方法（如LIME和TreeSHAP）的局限性，即依赖近似和不稳定扰动。通过利用模型内部结构，LMDI+在性能和稳定性上都取得了显著提升，并为局部可解释性应用提供了新的工具。这对于高风险领域中树模型的可信部署具有重要意义。"}}
{"id": "2506.08480", "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "summary": "Text-to-image models often struggle to generate images that precisely match\ntextual prompts. Prior research has extensively studied the evaluation of\nimage-text alignment in text-to-image generation. However, existing evaluations\nprimarily focus on agreement with human assessments, neglecting other critical\nproperties of a trustworthy evaluation framework. In this work, we first\nidentify two key aspects that a reliable evaluation should address. We then\nempirically demonstrate that current mainstream evaluation frameworks fail to\nfully satisfy these properties across a diverse range of metrics and models.\nFinally, we propose recommendations for improving image-text alignment\nevaluation.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08480v1", "AI": {"title_translation": "重新思考文本到图像模型中图像-文本对齐的自动评估", "tldr": "本文重新审视了文本到图像模型中图像-文本对齐的现有评估方法，指出其不足并提出改进建议。", "motivation": "现有的文本到图像模型中图像-文本对齐的评估方法主要关注与人类评估的一致性，但忽略了可信评估框架的其他关键属性。", "method": "本文首先确定了可靠评估应解决的两个关键方面，然后通过实证证明了当前主流评估框架未能完全满足这些属性，最后提出了改进图像-文本对齐评估的建议。", "result": "研究表明，当前主流的图像-文本对齐评估框架未能充分满足可靠评估所需的关键属性。", "conclusion": "为了改进文本到图像模型中图像-文本对齐的评估，需要重新考虑现有框架并采纳新的建议。", "translation": "文本到图像模型在生成与文本提示精确匹配的图像时常常面临挑战。先前的研究已经广泛探讨了文本到图像生成中图像-文本对齐的评估。然而，现有评估主要侧重于与人类评估的一致性，忽略了可信评估框架的其他关键属性。在这项工作中，我们首先确定了可靠评估应解决的两个关键方面。然后，我们通过实证证明了当前主流评估框架未能在一系列指标和模型中完全满足这些属性。最后，我们提出了改进图像-文本对齐评估的建议。", "summary": "本文重新审视了文本到图像模型中图像-文本对齐的自动评估。研究指出，现有评估方法过度依赖与人类判断的一致性，却忽视了可信评估框架的其他重要属性。作者识别出可靠评估的两个关键方面，并通过实证证明当前主流评估框架未能满足这些方面。最终，论文提出了改进图像-文本对齐评估的建议。", "keywords": "文本到图像模型, 图像-文本对齐, 自动评估, 评估框架, 可信度", "comments": "本文指出了当前文本到图像模型评估中存在的重要问题，即过度依赖人类评估而忽视了其他关键属性。其创新之处在于识别并实证验证了现有评估框架的不足，并提出了具体的改进建议，这对于推动文本到图像生成领域评估方法的进步具有重要意义。"}}
{"id": "2506.08936", "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models", "authors": ["Amina Mollaysa", "Artem Moskale", "Pushpak Pati", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "summary": "We present BioLangFusion, a simple approach for integrating pre-trained DNA,\nmRNA, and protein language models into unified molecular representations.\nMotivated by the central dogma of molecular biology (information flow from gene\nto transcript to protein), we align per-modality embeddings at the biologically\nmeaningful codon level (three nucleotides encoding one amino acid) to ensure\ndirect cross-modal correspondence. BioLangFusion studies three standard fusion\ntechniques: (i) codon-level embedding concatenation, (ii) entropy-regularized\nattention pooling inspired by multiple-instance learning, and (iii) cross-modal\nmulti-head attention -- each technique providing a different inductive bias for\ncombining modality-specific signals. These methods require no additional\npre-training or modification of the base models, allowing straightforward\nintegration with existing sequence-based foundation models. Across five\nmolecular property prediction tasks, BioLangFusion outperforms strong unimodal\nbaselines, showing that even simple fusion of pre-trained models can capture\ncomplementary multi-omic information with minimal overhead.", "comment": "Proceedings of ICML 2025 Workshop on Multi-modal Foundation\n  Proceedings of ICML 2025 Workshop on Multi-modal Foundation Proceedings of\n  ICML 2025 Workshop on Multi-modal Foundation Models and Large Language Models\n  for Life Sciences", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08936v1", "AI": {"title_translation": "BioLangFusion：DNA、mRNA和蛋白质语言模型的多模态融合", "tldr": "BioLangFusion是一个简单的方法，它将预训练的DNA、mRNA和蛋白质语言模型整合到统一的分子表示中，通过在密码子水平对齐嵌入并使用三种融合技术，在分子特性预测任务中优于单模态基线。", "motivation": "该研究的动机是分子生物学的中心法则（信息从基因流向转录本再到蛋白质），旨在将预训练的DNA、mRNA和蛋白质语言模型整合为统一的分子表示。", "method": "BioLangFusion通过在生物学上具有意义的密码子水平（三个核苷酸编码一个氨基酸）对齐每个模态的嵌入，以确保直接的跨模态对应。它研究了三种标准的融合技术：(i) 密码子级别的嵌入拼接，(ii) 受多实例学习启发的熵正则化注意力池化，以及(iii) 跨模态多头注意力。这些方法无需额外的预训练或修改基础模型。", "result": "在五项分子特性预测任务中，BioLangFusion的表现优于强大的单模态基线。", "conclusion": "研究表明，即使是预训练模型的简单融合也能以最小的开销捕获互补的多组学信息。", "translation": "我们提出了BioLangFusion，一种将预训练的DNA、mRNA和蛋白质语言模型整合到统一分子表示中的简单方法。受分子生物学中心法则（信息从基因流向转录本再到蛋白质）的启发，我们在生物学上有意义的密码子水平（三个核苷酸编码一个氨基酸）对齐每个模态的嵌入，以确保直接的跨模态对应。BioLangFusion研究了三种标准融合技术：(i) 密码子级别嵌入拼接，(ii) 受多实例学习启发的熵正则化注意力池化，以及(iii) 跨模态多头注意力——每种技术都为结合特定模态信号提供了不同的归纳偏置。这些方法不需要额外的预训练或修改基础模型，可以直接与现有的基于序列的基础模型集成。在五项分子特性预测任务中，BioLangFusion优于强大的单模态基线，表明即使是预训练模型的简单融合也能以最小的开销捕获互补的多组学信息。", "summary": "BioLangFusion提出了一种将预训练的DNA、mRNA和蛋白质语言模型融合到统一分子表示中的方法。该方法在密码子水平对齐多模态嵌入，并探索了拼接、注意力池化和多头注意力三种融合技术。它无需对基础模型进行额外修改或预训练，并在分子特性预测任务中显著超越了单模态基线，证明了简单融合也能高效捕获互补的多组学信息。", "keywords": "多模态融合, 语言模型, DNA, mRNA, 蛋白质", "comments": "BioLangFusion的创新之处在于其简洁性和高效性。它没有采用复杂的预训练或模型修改，而是通过在生物学上合理的密码子级别进行对齐和应用标准融合技术，实现了多模态信息的有效整合。这对于现有序列基础模型的集成性强，且在性能上超越了单模态基线，显示了其在生物信息学领域，尤其是在分子特性预测任务中的实用性和潜力。"}}
{"id": "2506.08487", "title": "Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models", "authors": ["Sumanth Manduru", "Carlotta Domeniconi"], "summary": "The rapid adoption of Small Language Models (SLMs) for on-device and\nresource-constrained deployments has outpaced our understanding of their\nethical risks. To the best of our knowledge, we present the first large-scale\naudit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an\noverlooked \"middle tier\" between BERT-class encoders and flagship LLMs. Our\nevaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma\n3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we\nanalyze both utility and fairness across ambiguous and disambiguated contexts.\nThis evaluation reveals three key insights. First, competence and fairness need\nnot be antagonistic: Phi models achieve F1 scores exceeding 90 percent while\nexhibiting minimal bias, showing that efficient and ethical NLP is attainable.\nSecond, social bias varies significantly by architecture: Qwen 2.5 models may\nappear fair, but this often reflects vacuous neutrality, random guessing, or\nevasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2\nmodels exhibit stronger stereotypical bias, suggesting overconfidence rather\nthan neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ\nquantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but\nincreases disability-related bias in Phi-4-Mini by over 7 percentage points.\nThese insights provide practical guidance for the responsible deployment of\nSLMs in applications demanding fairness and efficiency, particularly benefiting\nsmall enterprises and resource-constrained environments.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08487v1", "AI": {"title_translation": "公平并非沉默：揭示小型语言模型中的空洞中立性", "tldr": "首次对0.5到50亿参数的小型语言模型进行大规模公平性审计，发现能力与公平性并非对立，偏见因模型架构而异，且模型压缩会引入权衡。", "motivation": "小型语言模型（SLMs）的快速普及超出了我们对其伦理风险的理解，尤其是在设备端和资源受限部署中，目前缺乏对0.5到50亿参数的SLMs的公平性大规模审计。", "method": "研究团队对包括Qwen 2.5、LLaMA 3.2、Gemma 3和Phi家族在内的九个开源指令调优SLMs（0.5到50亿参数）进行了大规模审计。他们使用BBQ基准在零样本提示下，分析了模糊和明确上下文中的效用和公平性。", "result": "1. 能力和公平性并非对立：Phi模型在F1分数上超过90%，同时表现出最小的偏见。2. 社会偏见因架构而异：Qwen 2.5模型可能表面公平，但往往是空洞中立、随机猜测或规避行为；LLaMA 3.2模型则表现出更强的刻板偏见。3. 压缩引入了细微的权衡：4位AWQ量化提高了LLaMA 3.2-3B在模糊设置下的F1分数，但使Phi-4-Mini与残疾相关的偏见增加了7个百分点以上。", "conclusion": "这些发现为在需要公平性和效率的应用中负责任地部署SLMs提供了实用指导，特别有利于小型企业和资源受限的环境。", "translation": "小型语言模型（SLM）在设备端和资源受限部署中的快速采用，已经超越了我们对其伦理风险的理解。据我们所知，我们首次对参数量介于0.5到50亿的指令微调SLM进行了大规模审计——这是一个被BERT类编码器和旗舰级大型语言模型（LLM）之间忽视的“中间层”。我们的评估包括来自Qwen 2.5、LLaMA 3.2、Gemma 3和Phi家族的九个开源模型。在零样本提示下，我们使用BBQ基准分析了模糊和明确上下文中的实用性和公平性。这项评估揭示了三个关键见解。首先，能力和公平性不必是对立的：Phi模型在F1分数上超过90%，同时表现出最小的偏见，表明高效和道德的自然语言处理是可实现的。其次，社会偏见因架构而异：Qwen 2.5模型可能看起来公平，但这往往反映的是空洞的中立性、随机猜测或规避行为，而非真正的伦理对齐。相比之下，LLaMA 3.2模型表现出更强的刻板偏见，这表明是过度自信而非中立。第三，压缩引入了细微的权衡：4位AWQ量化在模糊设置下提高了LLaMA 3.2-3B的F1分数，但使Phi-4-Mini中与残疾相关的偏见增加了7个百分点以上。这些见解为在要求公平性和效率的应用中负责任地部署SLM提供了实用指导，特别有利于小型企业和资源受限的环境。", "summary": "该研究首次对0.5到50亿参数的小型语言模型（SLMs）进行了大规模伦理公平性审计，填补了该领域研究空白。通过评估Qwen 2.5、LLaMA 3.2、Gemma 3和Phi家族的九个模型，研究发现能力与公平性并非对立（Phi模型表现出色），社会偏见因模型架构而异（Qwen 2.5表现为空洞中立，LLaMA 3.2则表现出刻板偏见），且模型压缩会带来复杂的权衡。这些发现为SLMs的负责任部署提供了实践指导。", "keywords": "小型语言模型, 公平性, 伦理风险, 模型审计, 社会偏见", "comments": "该研究首次对被忽视的“中间层”SLMs（0.5到50亿参数）进行了大规模公平性审计，具有重要创新性。它挑战了能力和公平性对立的普遍观念，并揭示了不同模型架构和压缩技术对公平性的复杂影响，为SLM的伦理部署提供了宝贵的实践指导，尤其对资源受限环境下的应用具有指导意义。"}}
{"id": "2506.08939", "title": "KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting", "authors": ["Hang Ye", "Gaoxiang Duan", "Haoran Zeng", "Yangxin Zhu", "Lingxue Meng", "Xiaoying Zheng", "Yongxin Zhu"], "summary": "Multivariate long-term and efficient time series forecasting is a key\nrequirement for a variety of practical applications, and there are complex\ninterleaving time dynamics in time series data that require decomposition\nmodeling. Traditional time series decomposition methods are single and rely on\nfixed rules, which are insufficient for mining the potential information of the\nseries and adapting to the dynamic characteristics of complex series. On the\nother hand, the Transformer-based models for time series forecasting struggle\nto effectively model long sequences and intricate dynamic relationships due to\ntheir high computational complexity. To overcome these limitations, we\nintroduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to\ndynamically extract trend and seasonal components. It further integrates a\nHybrid Frequency-Time Decomposition module (HFTD) to further decompose Series\ninto frequency-domain and time-domain. These components are coupled with\nmulti-scale Mamba-based KarmaBlock to efficiently process global and local\ninformation in a coordinated manner. Experiments on eight real-world datasets\nfrom diverse domains well demonstrated that KARMA significantly outperforms\nmainstream baseline methods in both predictive accuracy and computational\nefficiency. Code and full results are available at this repository:\nhttps://github.com/yedadasd/KARMA", "comment": "10 pages,3 figures, published to WASA2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08939v1", "AI": {"title_translation": "KARMA：一种用于多元长期时间序列预测的多级分解混合Mamba框架", "tldr": "KARMA是一个多级分解混合Mamba框架，用于高效准确地进行多元长期时间序列预测，通过动态分解和Mamba模块解决了传统方法和Transformer的局限性。", "motivation": "多元长期高效时间序列预测在实际应用中至关重要，但时间序列数据中存在复杂的交织时间动态，需要分解建模。传统时间序列分解方法单一且依赖固定规则，不足以挖掘序列的潜在信息并适应复杂序列的动态特性。另一方面，基于Transformer的时间序列预测模型由于计算复杂度高，难以有效建模长序列和复杂的动态关系。", "method": "KARMA引入了自适应时间通道分解模块（ATCD）来动态提取趋势和季节性分量。它进一步集成了混合频时分解模块（HFTD）以将序列进一步分解为频域和时域。这些分量与基于多尺度Mamba的KarmaBlock耦合，以协调方式高效处理全局和局部信息。", "result": "在八个来自不同领域的真实世界数据集上的实验表明，KARMA在预测准确性和计算效率方面显著优于主流基线方法。", "conclusion": "KARMA通过其多级分解和混合Mamba架构，有效解决了多元长期时间序列预测中传统方法和Transformer模型的局限性，实现了卓越的预测性能和计算效率。", "translation": "多元长期高效时间序列预测是各种实际应用的关键要求，时间序列数据中存在复杂的交织时间动态，需要分解建模。传统时间序列分解方法单一且依赖固定规则，不足以挖掘序列的潜在信息并适应复杂序列的动态特性。另一方面，基于Transformer的时间序列预测模型由于其高计算复杂度，难以有效建模长序列和复杂的动态关系。为了克服这些限制，我们引入了KARMA，它带有一个自适应时间通道分解模块（ATCD）以动态提取趋势和季节性分量。它进一步集成了混合频时分解模块（HFTD）以将序列进一步分解为频域和时域。这些分量与基于多尺度Mamba的KarmaBlock耦合，以协调方式高效处理全局和局部信息。在来自不同领域的八个真实世界数据集上的实验充分证明，KARMA在预测准确性和计算效率方面显著优于主流基线方法。代码和完整结果可在以下存储库中获取：https://github.com/yedadasd/KARMA", "summary": "KARMA是一个新颖的多级分解混合Mamba框架，专为解决多元长期时间序列预测中的挑战而设计。它通过自适应时间通道分解（ATCD）动态提取趋势和季节性成分，并通过混合频时分解（HFTD）进一步将序列分解到频域和时域。这些分解后的成分与多尺度Mamba-based KarmaBlock结合，以高效处理全局和局部信息。实验证明，KARMA在预测准确性和计算效率上均优于现有主流方法。", "keywords": "时间序列预测, 多元, Mamba, 分解, 长期预测", "comments": "KARMA的创新之处在于结合了动态多级分解策略和Mamba架构，有效解决了传统时序分解方法的局限性以及Transformer模型在处理长序列时的计算效率问题。其自适应分解模块和混合频时分解能更精细地捕捉时间序列的复杂动态，而Mamba模块则保证了处理长序列时的效率。这对于需要长期、高效预测的实际应用具有重要意义。"}}
{"id": "2506.08961", "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation", "authors": ["Chenxu Wang", "Huaping Liu"], "summary": "Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have\nbeen widely studied in various threat models; however, few consider\nenvironmental state perturbations, which are natural in embodied scenarios. To\nimprove the robustness of DRL agents, we formulate the problem of environmental\nstate perturbation, introducing a preliminary non-targeted attack method as a\ncalibration adversary, and then propose a defense framework, named Boosted\nAdversarial Training (BAT), which first tunes the agents via supervised\nlearning to avoid catastrophic failure and subsequently adversarially trains\nthe agent with reinforcement learning. Extensive experimental results\nsubstantiate the vulnerability of mainstream agents under environmental state\nperturbations and the effectiveness of our proposed attack. The defense results\ndemonstrate that while existing robust reinforcement learning algorithms may\nnot be suitable, our BAT framework can significantly enhance the robustness of\nagents against environmental state perturbations across various situations.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08961v1", "AI": {"title_translation": "针对环境状态扰动的鲁棒深度强化学习", "tldr": "本文研究深度强化学习中环境状态扰动的鲁棒性问题，提出了一种攻击方法和名为BAT的防御框架，实验证明了其有效性。", "motivation": "现有研究较少考虑深度强化学习（DRL）中环境状态扰动下的鲁棒性，而这在具身场景中是自然存在的。", "method": "首先，提出环境状态扰动问题，并引入一种初步的非目标攻击方法作为校准对抗。然后，提出一种名为Boosted Adversarial Training (BAT) 的防御框架，该框架首先通过监督学习调整智能体以避免灾难性故障，随后通过强化学习对智能体进行对抗训练。", "result": "实验结果证实了主流智能体在环境状态扰动下的脆弱性以及所提出攻击方法的有效性。防御结果表明，现有鲁棒强化学习算法可能不适用，而BAT框架可以显著增强智能体在各种情况下对抗环境状态扰动的鲁棒性。", "conclusion": "本文提出的BAT框架能够显著增强深度强化学习智能体对抗环境状态扰动的鲁棒性。", "translation": "对抗性攻击和深度强化学习（DRL）中的鲁棒性已在各种威胁模型中得到广泛研究；然而，很少有研究考虑环境状态扰动，这在具身场景中是自然存在的。为了提高DRL智能体的鲁棒性，我们提出了环境状态扰动问题，引入了一种初步的非目标攻击方法作为校准对抗，然后提出了一种名为Boosted Adversarial Training (BAT) 的防御框架，该框架首先通过监督学习调整智能体以避免灾难性故障，随后通过强化学习对智能体进行对抗训练。大量的实验结果证实了主流智能体在环境状态扰动下的脆弱性以及我们提出的攻击的有效性。防御结果表明，虽然现有鲁棒强化学习算法可能不适用，但我们的BAT框架可以显著增强智能体在各种情况下对抗环境状态扰动的鲁棒性。", "summary": "本文关注深度强化学习（DRL）在环境状态扰动下的鲁棒性问题，指出现有研究对此关注不足。为此，作者首先定义了环境状态扰动问题，并提出了一种非目标攻击方法。在此基础上，提出了一种名为Boosted Adversarial Training (BAT) 的防御框架，该框架结合了监督学习和对抗性强化学习。实验证明了主流DRL智能体在环境状态扰动下的脆弱性，并验证了所提攻击和BAT防御框架的有效性，表明BAT能显著提升智能体的鲁棒性。", "keywords": "深度强化学习, 鲁棒性, 环境状态扰动, 对抗训练, BAT框架", "comments": "本文创新性地将环境状态扰动这一现实问题引入深度强化学习的鲁棒性研究中，填补了现有威胁模型研究的空白。提出的BAT框架结合了监督学习和对抗训练，为提升DRL智能体在复杂环境中的鲁棒性提供了有效途径，具有重要的理论和实践意义。"}}
{"id": "2506.08500", "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs", "authors": ["Arie Cattan", "Alon Jacovi", "Ori Ram", "Jonathan Herzig", "Roee Aharoni", "Sasha Goldshtein", "Eran Ofek", "Idan Szpektor", "Avi Caciularu"], "summary": "Retrieval Augmented Generation (RAG) is a commonly used approach for\nenhancing large language models (LLMs) with relevant and up-to-date\ninformation. However, the retrieved sources can often contain conflicting\ninformation and it remains unclear how models should address such\ndiscrepancies. In this work, we first propose a novel taxonomy of knowledge\nconflict types in RAG, along with the desired model behavior for each type. We\nthen introduce CONFLICTS, a high-quality benchmark with expert annotations of\nconflict types in a realistic RAG setting. CONFLICTS is the first benchmark\nthat enables tracking progress on how models address a wide range of knowledge\nconflicts. We conduct extensive experiments on this benchmark, showing that\nLLMs often struggle to appropriately resolve conflicts between sources. While\nprompting LLMs to explicitly reason about the potential conflict in the\nretrieved documents significantly improves the quality and appropriateness of\ntheir responses, substantial room for improvement in future research remains.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08500v1", "AI": {"title_translation": "DRAGged into Conflicts：检测和解决搜索增强型LLM中的冲突来源", "tldr": "RAG中的检索源常含冲突信息，本文提出知识冲突分类法和CONFLCTS基准，发现LLM在解决冲突上表现不佳，但显式提示推理能显著改善。", "motivation": "检索增强生成（RAG）中检索到的信息可能包含冲突，且不清楚模型应如何处理这些差异。", "method": "本文首先提出了RAG中知识冲突类型的新分类法，并定义了每种类型所需的模型行为。然后，引入了CONFLCTS，一个高质量的基准数据集，包含专家在真实RAG设置下对冲突类型的注释。最后，在该基准上进行了广泛实验，并通过提示LLM显式推理检索文档中潜在冲突来改善响应。", "result": "实验表明，大型语言模型（LLM）通常难以适当解决来源之间的冲突。然而，提示LLM显式推理检索文档中潜在的冲突可以显著提高其响应的质量和适当性。", "conclusion": "尽管通过显式提示LLM推理潜在冲突能显著改善其响应质量，但在检索增强生成（RAG）中解决知识冲突方面，未来研究仍有很大的改进空间。", "translation": "检索增强生成（RAG）是一种常用的方法，用于通过相关和最新的信息来增强大型语言模型（LLM）。然而，检索到的来源通常可能包含冲突信息，并且目前尚不清楚模型应如何处理这些差异。在这项工作中，我们首先提出了RAG中知识冲突类型的新分类法，以及每种类型所需的模型行为。然后，我们引入了CONFLCTS，一个高质量的基准数据集，包含专家在真实RAG设置下对冲突类型的注释。CONFLCTS是第一个能够跟踪模型如何解决各种知识冲突进展的基准。我们对这个基准进行了广泛的实验，结果表明LLM在适当解决来源之间的冲突方面常常表现不佳。虽然提示LLM明确推理检索文档中潜在的冲突可以显著提高其响应的质量和适当性，但在未来的研究中仍有很大的改进空间。", "summary": "本文研究了检索增强生成（RAG）中检索源可能包含冲突信息的问题。作者首先提出了RAG中知识冲突类型的新分类法，并定义了每种类型所需的模型行为。接着，他们引入了CONFLCTS，一个带有专家注释的高质量基准，用于评估模型解决知识冲突的能力。实验表明，当前LLM在处理冲突信息方面存在困难，但通过显式提示LLM对冲突进行推理，可以显著提升模型响应的质量和适当性。研究指出，未来仍有很大的改进空间。", "keywords": "检索增强生成, 知识冲突, 大型语言模型, 基准, 冲突解决", "comments": "这篇论文通过提出一个新颖的知识冲突分类法和高质量的CONFLCTS基准，为RAG领域中处理信息冲突提供了一个重要的研究方向。它揭示了当前LLM在解决冲突方面的局限性，并提出了一个有效的提示策略，为未来的研究奠定了基础，具有重要的实践和理论意义。"}}
{"id": "2506.08965", "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO", "authors": ["Yiyang Zhao", "Huiyu Bai", "Xuejiao Zhao"], "summary": "The ability to train high-performing reward models with few-shot data is\ncritical for enhancing the efficiency and scalability of Reinforcement Learning\nfrom Human Feedback (RLHF). We propose a data augmentation and expansion\nframework that enables generative reward models trained on small datasets to\nachieve comparable performance to those trained on large-scale datasets.\nTraditional methods to train a generative reward model, such as Direct\nPreference Optimization (DPO), are constrained by inefficiencies in sample\npairing and limited data diversity. This work introduces preference refinement,\nwhich employs Chain-of-Thought (CoT) sampling to uncover diverse and\nhigh-quality preference relationships. It also incorporates a perplexity-based\nscoring mechanism to assign nuanced preference levels and utilizes Multi-level\nDirect Preference Optimization (M-DPO) to enable the model to capture\nfiner-grained preference differences between samples. Experimental results\ndemonstrate that the proposed method significantly enhances data efficiency and\nmodel performance, enabling reward models trained in a few-shot setting to\nachieve results on par with those trained on large-scale datasets. This study\nunderscores the potential of data-efficient strategies in advancing reward\nmodel optimization, offering a robust solution for low-resource RLHF\napplications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08965v1", "AI": {"title_translation": "GFRIEND：通过高效DPO生成式少样本奖励推断", "tldr": "提出GFRIEND框架，通过数据增强和M-DPO，使少样本奖励模型达到大规模数据集训练的效果，提高RLHF效率。", "motivation": "训练高性能奖励模型，尤其是在少样本数据条件下，对于提高人类反馈强化学习（RLHF）的效率和可扩展性至关重要。传统的直接偏好优化（DPO）等方法受到样本配对效率低下和数据多样性有限的限制，导致在小数据集上训练的奖励模型性能不足。", "method": "提出一个名为GFRIEND的数据增强和扩展框架。该框架引入了“偏好细化”机制，利用思维链（CoT）采样来发现多样化和高质量的偏好关系。同时，它结合了基于困惑度的评分机制来分配细致的偏好等级，并采用多级直接偏好优化（M-DPO）使模型能够捕获样本之间更细粒度的偏好差异。", "result": "实验结果表明，所提出的GFRIEND方法显著提高了数据效率和模型性能，使得在少样本设置下训练的奖励模型能够达到与在大规模数据集上训练的模型相当的性能。", "conclusion": "这项研究强调了数据高效策略在推进奖励模型优化方面的潜力，为低资源RLHF应用提供了一个鲁棒的解决方案。", "translation": "训练高性能奖励模型的能力，尤其是在少样本数据条件下，对于提高人类反馈强化学习（RLHF）的效率和可扩展性至关重要。我们提出了一个数据增强和扩展框架，该框架使得在小数据集上训练的生成式奖励模型能够达到与在大规模数据集上训练的模型相当的性能。传统的训练生成式奖励模型的方法，例如直接偏好优化（DPO），受到样本配对效率低下和数据多样性有限的限制。这项工作引入了偏好细化，它采用思维链（CoT）采样来发现多样化和高质量的偏好关系。它还结合了基于困惑度的评分机制来分配细致的偏好等级，并利用多级直接偏好优化（M-DPO）使模型能够捕获样本之间更细粒度的偏好差异。实验结果表明，所提出的方法显著提高了数据效率和模型性能，使得在少样本设置下训练的奖励模型能够达到与在大规模数据集上训练的模型相当的性能。这项研究强调了数据高效策略在推进奖励模型优化方面的潜力，为低资源RLHF应用提供了一个鲁棒的解决方案。", "summary": "本文提出了GFRIEND框架，旨在解决RLHF中少样本数据下训练高性能奖励模型的问题。针对传统DPO方法在样本配对效率和数据多样性上的限制，GFRIEND引入了偏好细化机制，通过思维链采样获取多样化偏好关系，并利用基于困惑度的评分和多级DPO来捕捉更细粒度的偏好。实验证明，该方法显著提升了数据效率和模型性能，使少样本训练的奖励模型达到与大规模数据集训练模型相当的效果，为低资源RLHF应用提供了有效方案。", "keywords": "少样本学习, 奖励模型, DPO, RLHF, 数据增强", "comments": "这篇论文通过引入偏好细化、基于困惑度的评分机制和M-DPO，创新性地解决了RLHF中奖励模型在少样本数据下的训练效率和性能问题。其核心贡献在于通过数据增强和精细化偏好学习，使得仅用少量数据即可达到传统大规模数据训练的效果，这对于资源受限的RLHF应用具有重要意义，极大地提升了RLHF的实用性和可扩展性。"}}
{"id": "2506.08504", "title": "CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations", "authors": ["Divyaksh Shukla", "Ritesh Baviskar", "Dwijesh Gohil", "Aniket Tiwari", "Atul Shree", "Ashutosh Modi"], "summary": "Discourse parsing is an important task useful for NLU applications such as\nsummarization, machine comprehension, and emotion recognition. The current\ndiscourse parsing datasets based on conversations consists of written English\ndialogues restricted to a single domain. In this resource paper, we introduce\nCoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in\nconversations. The corpus (code-mixed in Hindi and English) has both audio and\ntranscribed text and is annotated with nine discourse relations. We experiment\nwith various SoTA baseline models; the poor performance of SoTA models\nhighlights the challenges of multi-domain code-mixed corpus, pointing towards\nthe need for developing better models for such realistic settings.", "comment": "Accepted at ACL Findings 2025 (16 pages: 5 pages main content + 3\n  pages references + 8 pages appendix)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08504v1", "AI": {"title_translation": "CoMuMDR: 语码混合、多模态、多领域对话语篇分析语料库", "tldr": "CoMuMDR是一个新的语码混合、多模态、多领域对话语篇分析语料库，现有最先进模型在该语料库上表现不佳，表明需要开发更好的模型。", "motivation": "现有的对话语篇分析数据集仅限于书面英语对话，且仅限于单一领域，这限制了其在NLU应用中的效用。本研究旨在解决这一数据限制。", "method": "本文介绍了CoMuMDR语料库，这是一个语码混合（印地语和英语）、多模态（包含音频和转录文本）的多领域对话语篇分析语料库。该语料库已标注了九种语篇关系。研究者还使用各种最先进的基线模型进行了实验。", "result": "最先进的基线模型在CoMuMDR语料库上的表现不佳，这突显了多领域语码混合语料库的挑战性。", "conclusion": "最先进模型在CoMuMDR语料库上的糟糕表现表明，需要为这种现实设置开发更好的模型，以应对语码混合和多领域对话的复杂性。", "translation": "语篇分析是自然语言理解（NLU）应用（如摘要、机器阅读理解和情感识别）的重要任务。目前基于对话的语篇分析数据集由书面英语对话组成，且仅限于单一领域。在这篇资源论文中，我们介绍了CoMuMDR：一个用于对话语篇分析的语码混合、多模态、多领域语料库。该语料库（印地语和英语语码混合）同时包含音频和转录文本，并标注了九种语篇关系。我们用各种最先进的基线模型进行了实验；最先进模型表现不佳，这突出了多领域语码混合语料库的挑战，指出了需要为这种现实设置开发更好的模型。", "summary": "本文介绍了CoMuMDR，一个用于对话语篇分析的语码混合（印地语和英语）、多模态（音频和文本）和多领域语料库，旨在解决现有数据集的局限性。通过实验发现，当前最先进的模型在该语料库上表现不佳，这强调了开发更适用于现实语码混合多领域对话场景的模型的必要性。", "keywords": "语篇分析, 语码混合, 多模态, 多领域, 对话语料库", "comments": "CoMuMDR语料库的创新之处在于其语码混合、多模态和多领域的特性，这使其比现有数据集更具挑战性和现实性。它为语篇分析领域提供了一个急需的、更复杂的数据集，并明确指出了现有模型在处理此类复杂数据时的局限性，为未来的研究指明了方向。"}}
{"id": "2506.08977", "title": "Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data", "authors": ["Victoria Hankemeier", "Malte Schilling"], "summary": "Developments in Deep Learning have significantly improved time series\nforecasting by enabling more accurate modeling of complex temporal dependencies\ninherent in sequential data. The effectiveness of such models is often\ndemonstrated on limited sets of specific real-world data. Although this allows\nfor comparative analysis, it still does not demonstrate how specific data\ncharacteristics align with the architectural strengths of individual models.\nOur research aims at uncovering clear connections between time series\ncharacteristics and particular models. We introduce a novel dataset generated\nusing Gaussian Processes, specifically designed to display distinct, known\ncharacteristics for targeted evaluations of model adaptability to them.\nFurthermore, we present TimeFlex, a new model that incorporates a modular\narchitecture tailored to handle diverse temporal dynamics, including trends and\nperiodic patterns. This model is compared to current state-of-the-art models,\noffering a deeper understanding of how models perform under varied time series\nconditions.", "comment": "Accepted at IJCNN25, Code: https://github.com/vicky-hnk/time-flex", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08977v1", "AI": {"title_translation": "时间序列预测的定制架构：评估深度学习模型在高斯过程生成数据上的表现", "tldr": "研究旨在通过高斯过程生成的数据集，评估深度学习模型在时间序列预测中的表现，并引入了新模型TimeFlex以更好地适应不同时间序列特征。", "motivation": "现有的深度学习时间序列预测模型通常在有限的真实世界数据上进行评估，这无法充分揭示特定数据特征与模型架构优势之间的明确联系。因此，本研究旨在揭示时间序列特征与特定模型之间的清晰联系。", "method": "研究引入了一个使用高斯过程生成的新数据集，该数据集专门设计用于显示独特且已知的时间序列特征，以便有针对性地评估模型的适应性。此外，研究提出了一个新的模块化架构模型TimeFlex，旨在处理多样化的时间动态，并将其与当前最先进的模型进行比较，以深入了解模型在不同时间序列条件下的表现。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "深度学习的发展通过更准确地建模序列数据中固有的复杂时间依赖性，显著改善了时间序列预测。此类模型的有效性通常在有限的特定真实世界数据集上得到证明。尽管这允许进行比较分析，但它仍然未能证明特定数据特征如何与单个模型的架构优势相匹配。我们的研究旨在揭示时间序列特征与特定模型之间的清晰联系。我们引入了一个使用高斯过程生成的新颖数据集，该数据集专门设计用于显示独特、已知的特征，以便有针对性地评估模型对这些特征的适应性。此外，我们提出了TimeFlex，一个结合了模块化架构的新模型，该架构旨在处理多样化的时间动态，包括趋势和周期性模式。该模型与当前最先进的模型进行了比较，从而提供了对模型在不同时间序列条件下表现的更深入理解。", "summary": "这项研究旨在解决深度学习时间序列预测模型在有限真实世界数据上评估的局限性，以揭示数据特征与模型架构之间的联系。为此，研究团队创建了一个基于高斯过程的新数据集，该数据集具有明确定义的时间序列特征。同时，他们提出了一个名为TimeFlex的新型模块化模型，专门用于处理不同的时间动态。通过将TimeFlex与现有先进模型进行比较，研究旨在更深入地理解模型在各种时间序列条件下的性能。", "keywords": "时间序列预测, 深度学习, 高斯过程, 定制架构, TimeFlex", "comments": "这项研究通过引入高斯过程生成的数据集，为评估深度学习模型在不同时间序列特征下的表现提供了一个新颖且受控的环境，这弥补了现有研究依赖有限真实世界数据的不足。提出的TimeFlex模型具有模块化架构，有望更好地适应多样化的时间动态，这对于提高时间序列预测的泛化能力具有重要意义。"}}
{"id": "2506.08978", "title": "Propositional Logic for Probing Generalization in Neural Networks", "authors": ["Anna Langedijk", "Jaap Jumelet", "Willem Zuidema"], "summary": "The extent to which neural networks are able to acquire and represent\nsymbolic rules remains a key topic of research and debate. Much current work\nfocuses on the impressive capabilities of large language models, as well as\ntheir often ill-understood failures on a wide range of reasoning tasks. In this\npaper, in contrast, we investigate the generalization behavior of three key\nneural architectures (Transformers, Graph Convolution Networks and LSTMs) in a\ncontrolled task rooted in propositional logic. The task requires models to\ngenerate satisfying assignments for logical formulas, making it a structured\nand interpretable setting for studying compositionality. We introduce a\nbalanced extension of an existing dataset to eliminate superficial patterns and\nenable testing on unseen operator combinations. Using this dataset, we evaluate\nthe ability of the three architectures to generalize beyond the training\ndistribution. While all models perform well in-distribution, we find that\ngeneralization to unseen patterns, particularly those involving negation,\nremains a significant challenge. Transformers fail to apply negation\ncompositionally, unless structural biases are introduced. Our findings\nhighlight persistent limitations in the ability of standard architectures to\nlearn systematic representations of logical operators, suggesting the need for\nstronger inductive biases to support robust rule-based reasoning.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08978v1", "AI": {"title_translation": "用于探测神经网络泛化能力的命题逻辑", "tldr": "神经网络在命题逻辑任务中，尤其是在处理否定时，难以实现组合泛化，这凸显了其在学习系统逻辑表示方面的局限性。", "motivation": "当前研究多关注大型语言模型的能力与推理任务中的失败。本文旨在通过一个受控的命题逻辑任务，研究Transformer、图卷积网络和LSTM等关键神经网络架构的泛化行为，以理解它们获取和表示符号规则的能力。", "method": "研究了Transformer、图卷积网络和LSTM的泛化能力。使用基于命题逻辑的受控任务，要求模型为逻辑公式生成满足赋值。引入了一个现有数据集的平衡扩展，以消除表面模式并测试未见过的运算符组合。评估了模型泛化到训练分布之外的能力。", "result": "所有模型在分布内表现良好。泛化到未见过的模式（特别是涉及否定的模式）仍然是一个重大挑战。除非引入结构性偏差，否则Transformer无法组合地应用否定。", "conclusion": "标准架构在学习逻辑运算符的系统表示方面存在持续限制，这表明需要更强的归纳偏差来支持稳健的基于规则的推理。", "translation": "神经网络在获取和表示符号规则方面的能力仍然是研究和争论的关键话题。当前的大部分工作都集中在大型语言模型令人印象深刻的能力，以及它们在各种推理任务中经常难以理解的失败。与此相反，本文研究了三种关键神经网络架构（Transformer、图卷积网络和LSTM）在基于命题逻辑的受控任务中的泛化行为。该任务要求模型为逻辑公式生成满足赋值，使其成为一个结构化且可解释的设置，用于研究组合性。我们引入了现有数据集的一个平衡扩展，以消除表面模式并支持在未见过的运算符组合上进行测试。使用该数据集，我们评估了这三种架构泛化到训练分布之外的能力。虽然所有模型在分布内都表现良好，但我们发现泛化到未见过的模式，特别是涉及否定的模式，仍然是一个重大挑战。除非引入结构性偏差，否则 Transformer 无法组合地应用否定。我们的发现突出了标准架构在学习逻辑运算符的系统表示方面存在的持续限制，这表明需要更强的归纳偏差来支持稳健的基于规则的推理。", "summary": "本文研究了Transformer、图卷积网络和LSTM在受控命题逻辑任务（即为逻辑公式生成满足赋值）中的泛化能力。通过使用一个测试未见运算符组合的平衡数据集，研究发现模型虽然在分布内数据上表现良好，但在组合泛化，特别是处理否定方面，存在显著困难。研究结果表明，标准神经网络架构在学习逻辑运算符的系统表示方面存在局限性，强调了需要更强的归纳偏差以实现稳健的基于规则的推理。", "keywords": "命题逻辑, 神经网络, 泛化, 组合性, 归纳偏差", "comments": "这篇论文为当前神经网络架构在组合泛化，特别是在逻辑推理任务中的局限性提供了宝贵的见解。受控的设置和对命题逻辑的关注为理解这些模型的不足提供了一个清晰的框架。即使Transformer在没有引入偏差的情况下也难以组合地应用否定的发现是重要的。这项工作突出了神经网络在获取系统符号表示方面面临的基本挑战，并强调了探索更强归纳偏差以实现稳健逻辑推理的重要性。"}}
{"id": "2506.08982", "title": "On Finetuning Tabular Foundation Models", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev"], "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08982v1", "AI": {"title_translation": "关于微调表格基础模型", "tldr": "本文系统评估了表格基础模型（TabPFNv2）的微调策略，发现全面微调是最佳实践，并揭示了其内部机制如何通过改进样本相似度提升性能，在特定数据集上达到SOTA。", "motivation": "表格深度学习中的基础模型是一个新兴方向，尤其是TabPFNv2在小规模数据集上表现出色。然而，针对表格基础模型的最佳微调方法及其对模型内部机制的影响尚未被充分探索。鉴于现有研究结果不一致以及TabPFNv2独特的架构，有必要进行新的调查。", "method": "本文首先系统评估了在不同数据集上各种微调策略，以确定时间效率和有效性方面的最佳实践。随后，通过与检索增强模型进行类比，深入研究了微调如何改变TabPFNv2的内部机制。", "result": "研究发现，全面微调是TabPFNv2在时间效率和有效性方面最实用的解决方案。微调的成功在于梯度优化后，测试对象查询表示与上下文训练对象键表示的点积能更准确地反映它们的目标相似性。这种改进的相似性使得微调后的TabPFNv2能够通过适当加权相关上下文样本来更好地近似目标依赖，从而改进基于检索的预测逻辑。在多达5万个对象的多数数据集上，微调TabPFNv2观察到性能提升。在具有I.I.D.分割的学术数据集上，微调后的TabPFNv2达到了最先进的性能。但在具有逐渐时间漂移和丰富特征集的非I.I.D.数据集上，TabPFNv2的稳定性较差，现有方法仍表现更好。", "conclusion": "本文系统评估了表格基础模型TabPFNv2的微调方法，确立了全面微调作为实用且高效的策略，并深入分析了其内部机制，揭示了微调通过提升样本相似度来改进预测。尽管在I.I.D.数据集上表现出色，但在时间漂移数据集上的稳定性仍需改进。", "translation": "基础模型是表格深度学习中一个新兴的研究方向。值得注意的是，TabPFNv2最近声称在使用上下文学习范式的小规模数据集上，其性能优于传统的基于GBDT的方法，该范式不调整模型参数以适应目标数据集。然而，适应表格基础模型的最佳微调方法，以及这种适应如何重塑其内部机制，仍未得到充分探索。尽管先前的研究已经探讨了早期基础模型的微调，但不一致的发现和TabPFNv2独特的架构使得有必要进行新的调查。为了解决这些问题，我们首先系统地评估了在不同数据集上的各种微调策略。我们的发现确立了全面微调是TabPFNv2在时间效率和有效性方面最实用的解决方案。然后，我们通过与检索增强模型进行类比，研究了微调如何改变TabPFNv2的内部机制。我们揭示了微调成功的关键在于，在基于梯度的适应之后，测试对象的查询表示和上下文训练对象的键表示的点积更准确地反映了它们的目标相似性。这种改进的相似性使得微调后的TabPFNv2能够通过适当加权相关的上下文样本来更好地近似目标依赖，从而改进基于检索的预测逻辑。从实践角度来看，我们成功地在多达5万个对象的多种数据集上对TabPFNv2进行了微调，并在几乎所有任务上都观察到性能提升。更精确地说，在具有I.I.D.（独立同分布）分割的学术数据集上，微调使TabPFNv2达到了最先进的性能，而在具有逐渐时间漂移和丰富特征集的数据集上，TabPFNv2的稳定性较差，现有方法仍然更优。", "summary": "本文探讨了表格基础模型TabPFNv2的微调方法。研究系统评估了多种微调策略，发现全面微调在效率和效果上表现最佳。通过分析其内部机制，揭示了微调通过提高查询-键表示的相似度来改进上下文学习中的检索式预测逻辑。实践中，微调在多达5万个对象的数据集上提升了TabPFNv2的性能，使其在I.I.D.学术数据集上达到最先进水平，但在具有时间漂移的复杂数据集上稳定性有待提高。", "keywords": "表格基础模型, 微调, TabPFNv2, 上下文学习, 检索增强模型", "comments": "该研究创新性地探讨了表格基础模型的微调，并深入剖析了其内部机制，揭示了微调如何通过改进样本相似度来增强模型的检索能力，这对于理解和优化这类模型具有重要意义。同时，研究也指出了TabPFNv2在处理时间漂移数据集时的局限性，为未来的研究方向提供了线索。"}}
{"id": "2506.08989", "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "authors": ["Xiao Liang", "Zhong-Zhi Li", "Yeyun Gong", "Yang Wang", "Hengyuan Zhang", "Yelong Shen", "Ying Nian Wu", "Weizhu Chen"], "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor training large language models (LLMs) on complex reasoning tasks, such as\nmathematical problem solving. A prerequisite for the scalability of RLVR is a\nhigh-quality problem set with precise and verifiable answers. However, the\nscarcity of well-crafted human-labeled math problems and limited-verification\nanswers in existing distillation-oriented synthetic datasets limit their\neffectiveness in RL. Additionally, most problem synthesis strategies\nindiscriminately expand the problem set without considering the model's\ncapabilities, leading to low efficiency in generating useful questions. To\nmitigate this issue, we introduce a Self-aware Weakness-driven problem\nSynthesis framework (SwS) that systematically identifies model deficiencies and\nleverages them for problem augmentation. Specifically, we define weaknesses as\nquestions that the model consistently fails to learn through its iterative\nsampling during RL training. We then extract the core concepts from these\nfailure cases and synthesize new problems to strengthen the model's weak areas\nin subsequent augmented training, enabling it to focus on and gradually\novercome its weaknesses. Without relying on external knowledge distillation,\nour framework enables robust generalization byempowering the model to\nself-identify and address its weaknesses in RL, yielding average performance\ngains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning\nbenchmarks.", "comment": "Reinforcement Learning; Large Language Models; LLM Reasoning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.08989v1", "AI": {"title_translation": "SwS：强化学习中用于LLM推理的自感知弱点驱动问题合成", "tldr": "SwS框架通过识别LLM在RL训练中的弱点并合成新问题来解决这些弱点，从而提高LLM在复杂推理任务上的性能。", "motivation": "RLVR训练LLM需要高质量且可验证的问题集，但现有数据集缺乏高质量人工标注和可验证答案，且大多数问题合成策略未考虑模型能力，导致生成有用问题的效率低下。", "method": "本文提出SwS（自感知弱点驱动问题合成）框架，系统地识别模型缺陷。弱点被定义为模型在RL训练中通过迭代采样持续未能学习的问题。框架从这些失败案例中提取核心概念，并合成新问题以强化模型的薄弱领域，在随后的增强训练中使模型专注于并克服其弱点，无需外部知识蒸馏。", "result": "在8个主流推理基准测试中，使7B模型平均性能提高10.0%，32B模型平均性能提高7.7%。", "conclusion": "SwS框架通过使模型自识别和解决其在强化学习中的弱点，实现了强大的泛化能力，并显著提高了LLM在复杂推理任务上的性能。", "translation": "可验证奖励强化学习（RLVR）已被证明在训练大型语言模型（LLM）处理复杂推理任务（例如数学问题解决）方面是有效的。RLVR可扩展性的一个先决条件是拥有高质量、具有精确可验证答案的问题集。然而，现有面向蒸馏的合成数据集中精心制作的人工标注数学问题稀缺以及可验证答案有限，限制了它们在强化学习中的有效性。此外，大多数问题合成策略不加区分地扩展问题集，没有考虑模型的能力，导致生成有用问题的效率低下。为了缓解这个问题，我们引入了一个自感知弱点驱动问题合成框架（SwS），它系统地识别模型缺陷并利用它们进行问题增强。具体来说，我们将弱点定义为模型在强化学习训练过程中通过迭代采样持续未能学习的问题。然后，我们从这些失败案例中提取核心概念，并合成新问题，以在随后的增强训练中强化模型的薄弱领域，使其能够专注于并逐步克服其弱点。我们的框架无需依赖外部知识蒸馏，通过赋予模型在强化学习中自识别和解决其弱点的能力，实现了强大的泛化能力，在八个主流推理基准测试中，使7B和32B模型分别获得了10.0%和7.7%的平均性能提升。", "summary": "SwS是一个针对LLM推理的强化学习问题合成框架。它通过识别LLM在RL训练中反复失败的问题作为弱点，并从中提取核心概念来合成新问题。这种方法旨在增强模型在薄弱领域的学习，无需外部知识蒸馏，显著提高了LLM在复杂推理任务上的性能。", "keywords": "强化学习, LLM推理, 问题合成, 弱点驱动, 自感知", "comments": "SwS的创新之处在于其“自感知弱点驱动”的问题合成策略，这与传统不区分地扩展问题集的方法形成对比。通过专注于模型未能掌握的知识点，SwS提高了训练效率和模型泛化能力，对于解决高质量训练数据稀缺性问题具有重要意义。"}}
{"id": "2506.09007", "title": "Branched Schrödinger Bridge Matching", "authors": ["Sophia Tang", "Yinuo Zhang", "Alexander Tong", "Pranam Chatterjee"], "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09007v1", "AI": {"title_translation": "分支薛定谔桥匹配", "tldr": "引入了分支薛定谔桥匹配（BranchSBM）框架，用于学习分支薛定谔桥，以解决现有生成模型在处理多模态或分支演化轨迹方面的局限性。", "motivation": "现有的流匹配和薛定谔桥匹配等方法，在生成模型中预测初始分布和目标分布之间的中间轨迹时，仅限于单峰过渡，无法捕捉从共同起源到多个不同结果的分支或发散演化。", "method": "本文提出了分支薛定谔桥匹配（BranchSBM）框架，该框架通过参数化多个时间依赖的速度场和生长过程，来学习分支薛定谔桥，从而能够表示群体层面发散到多个终末分布的过程。", "result": "BranchSBM不仅更具表现力，而且对于涉及多路径表面导航、模拟同质祖细胞状态的细胞命运分叉以及模拟细胞对扰动的发散响应等任务至关重要。", "conclusion": "BranchSBM通过学习分支薛定谔桥，有效解决了现有生成模型在处理多模态和分支演化轨迹方面的局限性，并在多种实际应用中展现出其必要性和优越性。", "translation": "预测初始分布和目标分布之间的中间轨迹是生成建模中的一个核心问题。现有的方法，例如流匹配和薛定谔桥匹配，通过建模单一随机路径来有效学习两个分布之间的映射。然而，这些方法本质上仅限于单峰过渡，无法捕捉从共同起源到多个不同结果的分支或发散演化。为了解决这个问题，我们引入了分支薛定谔桥匹配（BranchSBM），这是一个学习分支薛定谔桥的新颖框架。BranchSBM参数化了多个时间依赖的速度场和生长过程，从而能够表示群体层面发散到多个终末分布的过程。我们表明，BranchSBM不仅更具表现力，而且对于涉及多路径表面导航、模拟同质祖细胞状态的细胞命运分叉以及模拟细胞对扰动的发散响应等任务至关重要。", "summary": "本文针对现有生成模型在处理多模态或分支轨迹方面的局限性，提出了一种名为分支薛定谔桥匹配（BranchSBM）的新框架。BranchSBM通过参数化多个时间依赖的速度场和生长过程来学习分支薛定谔桥，从而能够表示群体层面发散到多个终末分布的过程。研究表明，BranchSBM不仅表现力更强，而且对于多路径导航、细胞命运分叉建模和细胞响应模拟等任务至关重要。", "keywords": "薛定谔桥匹配, 分支轨迹, 生成模型, 多模态, 细胞命运", "comments": "BranchSBM在生成模型领域具有重要创新性，它克服了传统薛定谔桥匹配方法在处理多模态或分支轨迹时的固有局限。通过引入多路径建模能力，它为生物学（如细胞命运分化）和物理学（如多路径导航）等领域的复杂系统建模提供了强大的新工具，具有广泛的应用潜力。"}}
{"id": "2506.09010", "title": "Effective Data Pruning through Score Extrapolation", "authors": ["Sebastian Schmidt", "Prasanga Dhungel", "Christoffer Löffler", "Björn Nieth", "Stephan Günnemann", "Leo Schwinn"], "summary": "Training advanced machine learning models demands massive datasets, resulting\nin prohibitive computational costs. To address this challenge, data pruning\ntechniques identify and remove redundant training samples while preserving\nmodel performance. Yet, existing pruning techniques predominantly require a\nfull initial training pass to identify removable samples, negating any\nefficiency benefits for single training runs. To overcome this limitation, we\nintroduce a novel importance score extrapolation framework that requires\ntraining on only a small subset of data. We present two initial approaches in\nthis framework - k-nearest neighbors and graph neural networks - to accurately\npredict sample importance for the entire dataset using patterns learned from\nthis minimal subset. We demonstrate the effectiveness of our approach for 2\nstate-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different\ndatasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training\nparadigms (supervised, unsupervised, and adversarial). Our results indicate\nthat score extrapolation is a promising direction to scale expensive score\ncalculation methods, such as pruning, data attribution, or other tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09010v1", "AI": {"title_translation": "分数外推法实现高效数据剪枝", "tldr": "提出一种新的分数外推框架，仅需在小部分数据上训练即可预测整个数据集的样本重要性，从而显著提升数据剪枝等昂贵分数计算方法的效率。", "motivation": "训练先进机器学习模型需要海量数据，导致计算成本过高。现有数据剪枝技术虽能去除冗余样本，但通常需要完整的初始训练过程来识别可移除样本，这抵消了单次训练运行的效率优势。", "method": "引入一种新颖的重要性分数外推框架，该框架仅需在数据的少量子集上进行训练。在该框架中，作者提出了两种初始方法：k-最近邻和图神经网络，利用从最小子集学习到的模式，准确预测整个数据集的样本重要性。", "result": "该方法在两种最先进的剪枝方法（动态不确定性和TDDS）、四种不同数据集（CIFAR-10、CIFAR-100、Places-365和ImageNet）以及三种训练范式（监督、无监督和对抗性）上均表现出有效性。", "conclusion": "结果表明，分数外推是扩展昂贵分数计算方法（如剪枝、数据归因或其他任务）的一个有前景的方向。", "translation": "训练先进的机器学习模型需要海量数据集，这导致了高昂的计算成本。为了解决这一挑战，数据剪枝技术旨在识别并移除冗余训练样本，同时保持模型性能。然而，现有的剪枝技术主要要求进行完整的初始训练才能识别可移除样本，这抵消了单次训练运行的任何效率优势。为了克服这一限制，我们引入了一种新颖的重要性分数外推框架，该框架仅需在数据的少量子集上进行训练。我们在此框架中提出了两种初始方法——k-最近邻和图神经网络——利用从这个最小子集学习到的模式，准确预测整个数据集的样本重要性。我们证明了我们的方法对于两种最先进的剪枝方法（动态不确定性和TDDS）、四种不同数据集（CIFAR-10、CIFAR-100、Places-365和ImageNet）以及三种训练范式（监督、无监督和对抗性）的有效性。我们的结果表明，分数外推是扩展昂贵分数计算方法（如剪枝、数据归因或其他任务）的一个有前景的方向。", "summary": "本文提出了一种名为“分数外推”的新颖数据剪枝框架，旨在解决现有剪枝技术需要完整训练周期才能识别冗余样本的效率问题。该框架通过仅在数据的一个小部分子集上进行训练，并利用k-最近邻和图神经网络等方法外推预测整个数据集的样本重要性。实验证明，该方法在多种剪枝技术、数据集和训练范式下均能有效识别和移除冗余数据，显著降低了计算成本，并为未来扩展昂贵分数计算任务提供了新方向。", "keywords": "数据剪枝, 分数外推, 机器学习, 计算效率, 样本重要性", "comments": "这篇论文通过引入分数外推框架，巧妙地解决了现有数据剪枝方法效率低下的核心问题。其创新点在于无需完整的初始训练即可预测样本重要性，这对于大规模机器学习模型的训练成本控制具有重要意义。该方法在多种场景下的广泛验证也增强了其普适性和实用价值，为数据归因等其他昂贵计算任务提供了新的思路。"}}
{"id": "2506.09016", "title": "SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning", "authors": ["Ruiqi Zhang", "Daman Arora", "Song Mei", "Andrea Zanette"], "summary": "Training large language models with reinforcement learning (RL) against\nverifiable rewards significantly enhances their reasoning abilities, yet\nremains computationally expensive due to inefficient uniform prompt sampling.\nWe introduce Selective Prompting with Efficient Estimation of Difficulty\n(SPEED), an adaptive online RL curriculum that selectively chooses training\nexamples of intermediate difficulty to maximize learning efficiency.\nTheoretically, we establish that intermediate-difficulty prompts improve the\ngradient estimator's signal-to-noise ratio, accelerating convergence.\nEmpirically, our efficient implementation leads to 2x to 6x faster training\nwithout degrading accuracy, requires no manual tuning, and integrates\nseamlessly into standard RL algorithms.", "comment": "pre-print", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09016v1", "AI": {"title_translation": "SPEED-RL：通过在线课程学习加速推理模型的训练", "tldr": "SPEED-RL引入了一种名为SPEED的自适应在线强化学习课程，通过选择中等难度的训练样本，将大型语言模型的强化学习训练速度提高了2到6倍，同时不影响准确性，且无需手动调整。", "motivation": "尽管使用强化学习（RL）对大型语言模型进行训练可以显著增强其推理能力，但由于低效的统一提示采样，其计算成本仍然很高。", "method": "本文引入了名为“选择性提示与难度高效估计”（SPEED）的自适应在线RL课程。该方法选择性地选取中等难度的训练样本，以最大化学习效率。理论上，该方法证明了中等难度的提示能够提高梯度估计器的信噪比，从而加速收敛。", "result": "在经验上，SPEED的高效实现使得训练速度提高了2到6倍，且没有降低准确性。此外，该方法无需手动调优，并能无缝集成到标准的RL算法中。", "conclusion": "SPEED-RL通过引入选择性提示的在线课程学习方法，显著提高了强化学习训练推理模型的效率，同时保持了准确性，并具有良好的通用性和易用性。", "translation": "通过可验证奖励对大型语言模型进行强化学习（RL）训练可以显著增强其推理能力，但由于低效的统一提示采样，其计算成本仍然很高。我们引入了“难度高效估计的选择性提示”（SPEED），这是一种自适应在线RL课程，它选择性地选择中等难度的训练样本以最大化学习效率。理论上，我们证明了中等难度的提示可以提高梯度估计器的信噪比，从而加速收敛。在经验上，我们高效的实现使得训练速度提高了2到6倍，而不会降低准确性，无需手动调优，并且可以无缝集成到标准的RL算法中。", "summary": "本文提出了SPEED-RL，一种利用自适应在线强化学习课程（SPEED）来加速大型语言模型推理能力训练的方法。通过选择中等难度的提示样本，SPEED理论上能提高梯度估计的信噪比，并经验性地实现了2到6倍的训练速度提升，同时保持准确性，无需手动调整，且易于集成到现有RL算法中，解决了RL训练计算成本高的问题。", "keywords": "强化学习, 大型语言模型, 课程学习, 推理模型, 训练效率", "comments": "SPEED-RL的创新点在于其引入的SPEED在线课程学习方法，通过动态选择中等难度的训练样本，有效解决了大型语言模型强化学习训练中计算成本高昂和效率低下的问题。该方法不仅有坚实的理论基础，而且在实践中展现出显著的加速效果（2-6倍），同时保持了模型性能，且无需手动调优，极大地提高了其实用性和易用性。这对于推动RL在LLM推理能力训练中的应用具有重要意义。"}}
{"id": "2506.09018", "title": "Edit Flows: Flow Matching with Edit Operations", "authors": ["Marton Havasi", "Brian Karrer", "Itai Gat", "Ricky T. Q. Chen"], "summary": "Autoregressive generative models naturally generate variable-length\nsequences, while non-autoregressive models struggle, often imposing rigid,\ntoken-wise structures. We propose Edit Flows, a non-autoregressive model that\novercomes these limitations by defining a discrete flow over sequences through\nedit operations-insertions, deletions, and substitutions. By modeling these\noperations within a Continuous-time Markov Chain over the sequence space, Edit\nFlows enable flexible, position-relative generation that aligns more closely\nwith the structure of sequence data. Our training method leverages an expanded\nstate space with auxiliary variables, making the learning process efficient and\ntractable. Empirical results show that Edit Flows outperforms both\nautoregressive and mask models on image captioning and significantly\noutperforms the mask construction in text and code generation.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09018v1", "AI": {"title_translation": "编辑流：基于编辑操作的流匹配", "tldr": "Edit Flows是一种非自回归模型，通过离散流和编辑操作生成可变长度序列，并在图像描述、文本和代码生成等任务中表现优异。", "motivation": "自回归生成模型能自然生成可变长度序列，而非自回归模型则难以处理，常强制施加僵硬的、基于词元（token-wise）的结构。本文旨在克服这些限制。", "method": "提出Edit Flows，这是一种非自回归模型，通过编辑操作（插入、删除和替换）在序列上定义离散流。它通过序列空间上的连续时间马尔可夫链建模这些操作，实现灵活的、位置相关的生成。训练方法利用带有辅助变量的扩展状态空间，使学习过程高效且易于处理。", "result": "实验结果表明，Edit Flows在图像描述任务中优于自回归模型和掩码模型，并在文本和代码生成中显著优于掩码构建方法。", "conclusion": "Edit Flows是一种强大的非自回归生成模型，能够灵活地进行可变长度序列的位置相关生成，并展现出卓越的性能。", "translation": "自回归生成模型自然地生成可变长度序列，而非自回归模型则面临困难，通常强加僵硬的、基于词元的结构。我们提出了Edit Flows，这是一种非自回归模型，通过编辑操作（插入、删除和替换）在序列上定义离散流来克服这些限制。通过在序列空间中的连续时间马尔可夫链内建模这些操作，Edit Flows实现了灵活的、位置相关的生成，这更符合序列数据的结构。我们的训练方法利用带有辅助变量的扩展状态空间，使学习过程高效且易于处理。经验结果表明，Edit Flows在图像描述方面优于自回归模型和掩码模型，并在文本和代码生成方面显著优于掩码构建。", "summary": "Edit Flows是一种新颖的非自回归生成模型，旨在解决现有非自回归方法在处理可变长度序列方面的局限性。它通过在序列空间中基于编辑操作（插入、删除和替换）的离散流，并利用连续时间马尔可夫链进行建模来实现。该模型的高效训练得益于辅助变量的扩展状态空间。实验结果表明，Edit Flows在图像描述、文本生成和代码生成任务中均优于自回归模型和掩码模型，提供了灵活且位置相关的序列生成能力。", "keywords": "Edit Flows, 非自回归模型, 生成模型, 序列生成, 离散流", "comments": "该论文的创新之处在于在非自回归框架中引入离散流和编辑操作来处理可变长度序列，这解决了传统非自回归模型的一个关键挑战。利用连续时间马尔可夫链和扩展状态空间进行高效训练也是一个重要的贡献。"}}
{"id": "2506.08552", "title": "Efficient Post-Training Refinement of Latent Reasoning in Large Language Models", "authors": ["Xinyuan Wang", "Dongjie Wang", "Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Sixun Dong", "Kunpeng Liu", "Yanjie Fu"], "summary": "Reasoning is a key component of language understanding in Large Language\nModels. While Chain-of-Thought prompting enhances performance via explicit\nintermediate steps, it suffers from sufficient token overhead and a fixed\nreasoning trajectory, preventing step-wise refinement. Recent advances in\nlatent reasoning address these limitations by refining internal reasoning\nprocesses directly in the model's latent space, without producing explicit\noutputs. However, a key challenge remains: how to effectively update reasoning\nembeddings during post-training to guide the model toward more accurate\nsolutions. To overcome this challenge, we propose a lightweight post-training\nframework that refines latent reasoning trajectories using two novel\nstrategies: 1) Contrastive reasoning feedback, which compares reasoning\nembeddings against strong and weak baselines to infer effective update\ndirections via embedding enhancement; 2) Residual embedding refinement, which\nstabilizes updates by progressively integrating current and historical\ngradients, enabling fast yet controlled convergence. Extensive experiments and\ncase studies are conducted on five reasoning benchmarks to demonstrate the\neffectiveness of the proposed framework. Notably, a 5\\% accuracy gain on MathQA\nwithout additional training.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08552v1", "AI": {"title_translation": "大型语言模型中潜在推理的有效后训练优化", "tldr": "本文提出了一种轻量级后训练框架，通过对比推理反馈和残差嵌入优化，有效提升大型语言模型中潜在推理的准确性，无需额外训练。", "motivation": "现有思维链提示法存在token开销大和推理轨迹固定等问题，阻碍了逐步优化。尽管潜在推理能解决这些限制，但如何在后训练期间有效更新推理嵌入以引导模型获得更准确的解决方案仍是关键挑战。", "method": "提出了一种轻量级后训练框架，使用两种新颖策略优化潜在推理轨迹：1) 对比推理反馈，通过比较推理嵌入与强弱基线来推断有效更新方向；2) 残差嵌入优化，通过逐步整合当前和历史梯度来稳定更新，实现快速且受控的收敛。", "result": "在五个推理基准上进行了广泛实验和案例研究，证明了所提出框架的有效性。尤其在MathQA上实现了5%的准确率提升，无需额外训练。", "conclusion": "该框架有效解决了大型语言模型中潜在推理的后训练更新挑战，显著提升了推理准确性，且无需额外训练。", "translation": "大型语言模型中的推理是语言理解的关键组成部分。虽然思维链提示通过明确的中间步骤提高了性能，但它存在足够的token开销和固定的推理轨迹问题，阻碍了逐步优化。最近在潜在推理方面的进展通过直接在模型的潜在空间中优化内部推理过程来解决这些限制，而无需产生明确的输出。然而，一个关键挑战依然存在：如何在后训练期间有效更新推理嵌入以引导模型获得更准确的解决方案。为了克服这一挑战，我们提出了一种轻量级后训练框架，该框架使用两种新颖的策略优化潜在推理轨迹：1) 对比推理反馈，通过将推理嵌入与强弱基线进行比较，通过嵌入增强推断有效的更新方向；2) 残差嵌入优化，通过逐步整合当前和历史梯度来稳定更新，从而实现快速且受控的收敛。在五个推理基准上进行了广泛的实验和案例研究，以证明所提出框架的有效性。值得注意的是，MathQA上实现了5%的准确率提升，无需额外训练。", "summary": "本文提出了一种轻量级后训练框架，旨在解决大型语言模型中潜在推理的有效更新问题。该框架通过对比推理反馈和残差嵌入优化两种新颖策略，优化模型内部的潜在推理轨迹。实验证明，该方法在多个推理基准上表现出显著效果，尤其在MathQA上实现了5%的准确率提升，且无需额外训练。", "keywords": "大型语言模型, 潜在推理, 后训练, 对比学习, 残差嵌入", "comments": "这项工作提出了一种新颖且高效的后训练方法，用于优化大型语言模型中的潜在推理能力。其创新之处在于引入了对比推理反馈和残差嵌入优化，有效解决了潜在推理嵌入更新的挑战，且实现了显著的性能提升，特别是“无需额外训练”这一点，体现了其高效性和实用价值。"}}
{"id": "2506.09026", "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs", "authors": ["Amrith Setlur", "Matthew Y. R. Yang", "Charlie Snell", "Jeremy Greer", "Ian Wu", "Virginia Smith", "Max Simchowitz", "Aviral Kumar"], "summary": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09026v1", "AI": {"title_translation": "e3：学习探索使LLM的测试时计算可外推", "tldr": "e3提出了一种新的训练方法，通过情境探索使LLM在测试时计算上具有更好的外推能力，从而在更难的问题上表现更好。", "motivation": "现有的大型语言模型在测试时计算的外推性（即在超出训练令牌预算的情况下，长时间思考以解决更难问题时的性能提升）方面表现不佳。", "method": "e3方法通过训练LLM进行情境探索来实现外推性，具体包括三个关键要素：1) 链接LLM具有不对称能力的技能（如验证与生成），以实现情境搜索；2) 利用错误轨迹的“负”梯度来放大强化学习过程中的探索；3) 通过专门设计的课程，在训练期间将任务难度与训练令牌预算耦合，以构建情境探索。", "result": "e3方法产生的1.7B模型在AIME'25和HMMT'25分数上表现最佳，并且能够外推到训练令牌预算的2倍。e3-1.7B模型不仅获得了高pass@1分数，而且相对于基础模型，pass@k也有所提高。", "conclusion": "通过训练LLM进行情境探索，可以显著提高其在测试时计算上的外推能力，从而在解决复杂问题时表现出更好的性能。", "translation": "测试时扩展为通过在推理时利用更多计算来改善LLM推理提供了一条有前景的道路；然而，这种范式的真正潜力在于外推（即，随着LLM“思考”时间更长，超出其训练时的最大令牌预算，在难题上的性能会提高）。令人惊讶的是，我们发现大多数现有的推理模型外推能力不佳。我们表明，实现外推的一种方法是训练LLM执行情境探索：训练LLM通过链接操作（例如生成、验证、细化等）或在提交答案之前测试多个假设来有效利用其测试时预算。为了实现情境探索，我们确定了e3配方中的三个关键要素：（1）链接基础LLM具有不对称能力的技能，例如将验证（容易）与生成（困难）链接起来，作为实现情境搜索的一种方式；（2）利用不正确轨迹的“负”梯度来放大强化学习过程中的探索，从而产生更长的搜索轨迹，链接额外的非对称性；以及（3）通过专门设计的课程，在训练期间将任务难度与训练令牌预算耦合，以构建情境探索。我们的e3配方根据AIME'25和HMMT'25分数产生了已知最佳的1.7B模型，并可外推到训练令牌预算的2倍。我们的e3-1.7B模型不仅获得了高pass@1分数，而且相对于基础模型，pass@k也有所提高。", "summary": "本研究发现，尽管测试时计算扩展有潜力改善LLM推理，但现有模型在处理超出训练预算的复杂问题时外推能力不足。为解决此问题，论文提出了e3方法，通过训练LLM进行情境探索，使其能有效利用测试时预算。e3包含三个核心要素：链接LLM不对称技能进行情境搜索、利用错误轨迹的负梯度增强探索，以及将任务难度与训练令牌预算耦合以结构化探索。实验结果表明，e3方法在1.7B模型上取得了SOTA性能，并展现出优异的外推能力，将训练令牌预算扩展了两倍，同时提高了pass@1和pass@k分数。", "keywords": "LLM, 外推性, 情境探索, 测试时计算, 强化学习", "comments": "e3方法通过引入“情境探索”的概念，为LLM在测试时计算的外推性问题提供了创新的解决方案。其核心在于训练模型如何“思考”和“探索”，而非仅仅生成答案。利用不对称技能链接、负梯度强化探索以及课程学习等策略，显著提升了模型处理复杂问题的能力，特别是其在超出训练范围的计算预算下的性能，这对于未来LLM在实际应用中处理更困难、更需要深度推理的任务具有重要意义。"}}
{"id": "2506.09034", "title": "FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed", "authors": ["Sizhe Dang", "Yangyang Guo", "Yanjun Zhao", "Haishan Ye", "Xiaodong Zheng", "Guang Dai", "Ivor Tsang"], "summary": "Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:\nthe backward pass of first-order optimizers like Adam increases memory usage to\nmore than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order\n(ZO) optimizers avoid this cost by estimating gradients only from forward\npasses, yet existing methods like MeZO usually require many more steps to\nconverge. Can this trade-off between speed and memory in ZO be fundamentally\nimproved? Normalized-SGD demonstrates strong empirical performance with greater\nmemory efficiency than Adam. In light of this, we introduce FZOO, a Fast\nZeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward\npasses needed for convergence by employing batched one-sided estimates that\nadapt step sizes based on the standard deviation of batch losses. It also\naccelerates per-batch computation through the use of Rademacher random vector\nperturbations coupled with CUDA's parallel processing. Extensive experiments on\ndiverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,\nacross 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms\nMeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For\nRoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy\nand an 18 times reduction in forward passes compared to MeZO, achieving\nconvergence speeds comparable to Adam. We also provide theoretical analysis\nproving FZOO's formal equivalence to a normalized-SGD update rule and its\nconvergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling\neven larger memory savings. Overall, our results make single-GPU, high-speed,\nfull-parameter fine-tuning practical and point toward future work on\nmemory-efficient pre-training.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09034v1", "AI": {"title_translation": "FZOO：用于微调大型语言模型的快速零阶优化器，实现Adam级速度", "tldr": "FZOO是一种快速零阶优化器，解决了LLM微调中的内存瓶颈和现有零阶优化器的收敛慢问题，实现了与Adam相当的速度和更好的准确性。", "motivation": "微调大型语言模型（LLMs）时，一阶优化器（如Adam）的向后传播会导致GPU内存瓶颈，内存使用量是推理的10倍以上。虽然零阶（ZO）优化器通过仅从前向传播估计梯度来避免此成本，但现有方法（如MeZO）通常需要更多步骤才能收敛。因此，需要改进零阶优化器在速度和内存之间的权衡。", "method": "本文引入了FZOO，一种快速零阶优化器，通过采用批处理单侧估计（根据批次损失的标准偏差调整步长）来减少收敛所需的总前向传播次数。同时，利用Rademacher随机向量扰动结合CUDA并行处理加速每个批次的计算。理论上，FZOO被证明与归一化SGD更新规则等效，并提供收敛保证。它还能与PEFT技术无缝集成。", "result": "在包括RoBERTa-large、OPT（350M-66B）、Phi-2和Llama3在内的多种模型上，跨11项任务进行了广泛实验。平均而言，FZOO在准确性上比MeZO高出3%，同时前向传播次数减少3倍。对于RoBERTa-large，FZOO在准确性上平均提高了5.6%，前向传播次数比MeZO减少了18倍，实现了与Adam相当的收敛速度。", "conclusion": "FZOO的结果使得单GPU、高速、全参数微调大型语言模型变得实用，并为未来内存高效的预训练工作指明了方向。", "translation": "微调大型语言模型（LLMs）经常面临GPU内存瓶颈：一阶优化器（如Adam）的向后传播会使内存使用量增加到推理水平的10倍以上（例如，OPT-30B需要633 GB）。零阶（ZO）优化器通过仅从前向传播估计梯度来避免此成本，但现有方法（如MeZO）通常需要更多步骤才能收敛。零阶优化器在速度和内存之间的这种权衡能否从根本上得到改善？归一化SGD在经验上表现出强大的性能，并且比Adam具有更高的内存效率。鉴于此，我们引入了FZOO，一种实现Adam级速度的快速零阶优化器。FZOO通过采用批处理单侧估计（根据批次损失的标准偏差调整步长）来减少收敛所需的总前向传播次数。它还通过使用Rademacher随机向量扰动结合CUDA的并行处理来加速每个批次的计算。在包括RoBERTa-large、OPT（350M-66B）、Phi-2和Llama3在内的多种模型上，跨11项任务进行的广泛实验验证了FZOO的有效性。平均而言，FZOO在准确性上比MeZO高出3%，同时所需的前向传播次数减少3倍。对于RoBERTa-large，与MeZO相比，FZOO在准确性上平均提高了5.6%，前向传播次数减少了18倍，实现了与Adam相当的收敛速度。我们还提供了理论分析，证明FZOO与归一化SGD更新规则的正式等效性及其收敛保证。FZOO与PEFT技术无缝集成，从而实现更大的内存节省。总的来说，我们的结果使单GPU、高速、全参数微调变得实用，并为未来内存高效的预训练工作指明了方向。", "summary": "本文提出了FZOO，一种快速零阶优化器，旨在解决大型语言模型微调中GPU内存瓶颈和现有零阶优化器收敛慢的问题。FZOO通过批处理单侧估计和Rademacher随机向量扰动结合CUDA并行处理来加速收敛并提高效率。实验证明，FZOO在准确性上优于MeZO，并显著减少了前向传播次数，实现了与Adam相当的收敛速度，使得单GPU全参数微调成为可能。", "keywords": "零阶优化器, 大型语言模型, 微调, 内存效率, Adam", "comments": "本文的创新之处在于提出了一种零阶优化器FZOO，它在保持内存效率的同时，显著提升了收敛速度，使其达到甚至超越了一阶优化器Adam的水平。这对于解决大型语言模型微调中的GPU内存限制问题具有重要意义，使得在单GPU上进行全参数微调变得更加实用和高效。研究还提供了理论保证，增加了其可信度。"}}
{"id": "2506.08569", "title": "Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata", "authors": ["Erwan Plantec", "Gautier Hamon", "Mayalen Etcheverry", "Bert Wang-Chak Chan", "Pierre-Yves Oudeyer", "Clément Moulin-Frier"], "summary": "Central to the artificial life endeavour is the creation of artificial\nsystems spontaneously generating properties found in the living world such as\nautopoiesis, self-replication, evolution and open-endedness. While numerous\nmodels and paradigms have been proposed, cellular automata (CA) have taken a\nvery important place in the field notably as they enable the study of\nphenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia\nhave been showed to produce life-like patterns reminiscent, on an aesthetic and\nontological point of view, of biological organisms we call creatures. We\npropose in this paper Flow-Lenia, a mass conservative extension of Lenia. We\npresent experiments demonstrating its effectiveness in generating\nspatially-localized patters (SLPs) with complex behaviors and show that the\nupdate rule parameters can be optimized to generate complex creatures showing\nbehaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed\nthe parameters of the model, defining the properties of the emerging patterns,\nwithin its own dynamics thus allowing for multispecies simulations. By using\nthe evolutionary activity framework as well as other metrics, we shed light on\nthe emergent evolutionary dynamics taking place in this system.", "comment": "This manuscript has been accepted for publication in the Artificial\n  Life journal (https://direct.mit.edu/artl)", "cate": "nlin.CG", "url": "http://arxiv.org/abs/2506.08569v1", "AI": {"title_translation": "Flow-Lenia：质量守恒连续细胞自动机中的涌现进化动力学", "tldr": "本文提出了Flow-Lenia，一种Lenia的质量守恒扩展，能够生成具有复杂行为的空间局部模式，并支持多物种模拟和涌现进化动力学研究。", "motivation": "人工生命领域的核心是创建能自发产生生命世界特性的系统，如自组织、自我复制、进化和开放性。细胞自动机（CA）在模拟这些现象方面占据重要地位，特别是连续细胞自动机Lenia已被证明能产生类生命模式。本文旨在通过提出Flow-Lenia来进一步探索和实现这些复杂生命现象的涌现。", "method": "本文提出了Flow-Lenia，一种Lenia的质量守恒扩展。通过实验证明其在生成具有复杂行为的空间局部模式（SLPs）方面的有效性。此外，研究还展示了如何优化更新规则参数以生成具有特定行为的复杂生物，并允许将定义涌现模式属性的模型参数嵌入其自身动力学中，从而实现多物种模拟。通过使用进化活动框架及其他指标来揭示系统中发生的涌现进化动力学。", "result": "Flow-Lenia能有效生成具有复杂行为的空间局部模式（SLPs）。更新规则参数可被优化以生成展示特定行为的复杂生物。Flow-Lenia允许将模型参数嵌入其自身动力学中，从而实现多物种模拟。该系统展现了涌现的进化动力学。", "conclusion": "Flow-Lenia作为Lenia的质量守恒扩展，成功地展示了生成复杂类生命模式、支持多物种模拟以及涌现进化动力学的潜力，为理解和研究人工生命现象提供了新途径。", "translation": "人工生命研究的核心在于创建能够自发产生生命世界特性的系统，例如自组织、自我复制、进化和开放性。尽管已经提出了许多模型和范式，但细胞自动机（CA）在该领域占据了非常重要的地位，尤其因为它能够研究自我复制和自组织等现象。像Lenia这样的连续细胞自动机已被证明能够产生在美学和本体论观点上类似于我们称之为“生物”的真实生物的类生命模式。本文提出了Flow-Lenia，它是Lenia的质量守恒扩展。我们通过实验证明了它在生成具有复杂行为的空间局部模式（SLPs）方面的有效性，并表明可以优化更新规则参数以生成展示特定行为的复杂生物。此外，我们还展示了Flow-Lenia允许我们将定义涌现模式属性的模型参数嵌入其自身的动力学中，从而实现多物种模拟。通过使用进化活动框架以及其他指标，我们揭示了该系统中发生的涌现进化动力学。", "summary": "本文提出Flow-Lenia，一种Lenia的质量守恒扩展，旨在创建能自发产生生命世界特性的系统。该模型能有效生成具有复杂行为的空间局部模式，并可通过参数优化产生复杂生物。Flow-Lenia还支持将模型参数嵌入其自身动力学中，从而实现多物种模拟。通过应用进化活动框架，研究揭示了该系统中涌现的进化动力学，为人工生命研究提供了新的工具和见解。", "keywords": "Flow-Lenia, 细胞自动机, 涌现进化, 质量守恒, 人工生命", "comments": "Flow-Lenia的创新之处在于其质量守恒特性以及将模型参数嵌入自身动力学中以实现多物种模拟的能力，这对于研究复杂生命系统中的相互作用和涌现进化动力学具有重要意义。该工作为探索开放式进化和人工生命现象提供了有力的计算模型。"}}
{"id": "2506.09044", "title": "The Decoupled Risk Landscape in Performative Prediction", "authors": ["Javier Sanguino", "Thomas Kehrenberg", "Jose A. Lozano", "Novi Quadrianto"], "summary": "Performative Prediction addresses scenarios where deploying a model induces a\ndistribution shift in the input data, such as individuals modifying their\nfeatures and reapplying for a bank loan after rejection. Literature has had a\ntheoretical perspective giving mathematical guarantees for convergence (either\nto the stable or optimal point). We believe that visualization of the loss\nlandscape can complement this theoretical advances with practical insights.\nTherefore, (1) we introduce a simple decoupled risk visualization method\ninspired in the two-step process that performative prediction is. Our approach\nvisualizes the risk landscape with respect to two parameter vectors: model\nparameters and data parameters. We use this method to propose new properties of\nthe interest points, to examine how existing algorithms traverse the risk\nlandscape and perform under more realistic conditions, including strategic\nclassification with non-linear models. (2) Building on this decoupled risk\nvisualization, we introduce a novel setting - extended Performative Prediction\n- which captures scenarios where the distribution reacts to a model different\nfrom the decision-making one, reflecting the reality that agents often lack\nfull access to the deployed model.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09044v1", "AI": {"title_translation": "表演性预测中的解耦风险格局", "tldr": "本文提出了一种解耦风险可视化方法，用于分析表演性预测中模型部署引起的分布漂移，并引入了“扩展表演性预测”的新设置，以捕捉更真实的场景。", "motivation": "现有关于表演性预测的理论研究缺乏实践洞察，因此需要通过可视化损失格局来补充理论进展，以提供实际见解。", "method": "本文提出了一种受表演性预测两步过程启发的简单解耦风险可视化方法，该方法根据模型参数和数据参数来可视化风险格局。在此基础上，还引入了一种新颖的“扩展表演性预测”设置，用于捕捉分布对与决策模型不同的模型做出反应的场景。", "result": "该方法被用于提出兴趣点的新特性，并检查现有算法在更现实的条件下（包括非线性模型下的策略性分类）如何遍历风险格局和执行。此外，“扩展表演性预测”设置能够捕捉更真实的场景，即代理通常无法完全访问已部署的模型。", "conclusion": "通过引入解耦风险可视化方法和“扩展表演性预测”设置，本文为理解和分析表演性预测中的模型诱导分布漂移提供了新的工具和更现实的框架。", "translation": "表演性预测解决了模型部署导致输入数据分布发生漂移的场景，例如个人在被银行贷款拒绝后修改其特征并重新申请贷款。现有文献从理论角度为收敛性（无论是稳定点还是最优解）提供了数学保证。我们认为，损失格局的可视化可以补充这些理论进展，提供实际见解。因此，(1) 我们引入了一种简单的解耦风险可视化方法，其灵感来源于表演性预测的两步过程。我们的方法根据两个参数向量：模型参数和数据参数来可视化风险格局。我们使用此方法来提出兴趣点的新特性，并检查现有算法如何遍历风险格局以及在更现实的条件下（包括非线性模型下的策略性分类）的表现。(2) 在此解耦风险可视化的基础上，我们引入了一种新颖的设置——扩展表演性预测——它捕捉了分布对与决策模型不同的模型做出反应的场景，反映了代理通常无法完全访问已部署模型的现实情况。", "summary": "表演性预测研究模型部署导致输入数据分布漂移的场景。为弥补现有理论进展在实践洞察方面的不足，本文提出了一种解耦风险可视化方法，通过模型参数和数据参数来展现风险格局。利用此方法，论文提出了兴趣点的新特性，并分析了现有算法在非线性模型下策略性分类等更真实条件下的表现。此外，本文还引入了“扩展表演性预测”这一新设置，以捕捉代理无法完全访问已部署模型时分布反应的真实场景。", "keywords": "表演性预测, 风险格局, 解耦, 分布漂移, 策略性分类", "comments": "本文的创新之处在于提出了解耦风险可视化方法和“扩展表演性预测”的新设置。前者通过将模型参数和数据参数分离可视化，为理解表演性预测的复杂性提供了新的视角；后者则考虑了代理对模型信息不对称的现实情况，使得模型分析更具实用价值。这些贡献有望为表演性预测领域的理论和实践分析提供更强大、更准确的工具。"}}
{"id": "2506.09048", "title": "Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations", "authors": ["Yuxin Dong", "Jiachen Jiang", "Zhihui Zhu", "Xia Ning"], "summary": "Task vectors offer a compelling mechanism for accelerating inference in\nin-context learning (ICL) by distilling task-specific information into a\nsingle, reusable representation. Despite their empirical success, the\nunderlying principles governing their emergence and functionality remain\nunclear. This work proposes the Linear Combination Conjecture, positing that\ntask vectors act as single in-context demonstrations formed through linear\ncombinations of the original ones. We provide both theoretical and empirical\nsupport for this conjecture. First, we show that task vectors naturally emerge\nin linear transformers trained on triplet-formatted prompts through loss\nlandscape analysis. Next, we predict the failure of task vectors on\nrepresenting high-rank mappings and confirm this on practical LLMs. Our\nfindings are further validated through saliency analyses and parameter\nvisualization, suggesting an enhancement of task vectors by injecting multiple\nones into few-shot prompts. Together, our results advance the understanding of\ntask vectors and shed light on the mechanisms underlying ICL in\ntransformer-based models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09048v1", "AI": {"title_translation": "理解上下文学习中的任务向量：出现、功能和局限性", "tldr": "本研究提出了线性组合猜想，解释了上下文学习中任务向量的出现和功能，并揭示了其局限性，同时提出了增强方法。", "motivation": "尽管任务向量在上下文学习（ICL）中加速推理方面取得了经验上的成功，但其出现和功能的潜在原理仍不清楚。", "method": "本研究提出了“线性组合猜想”，认为任务向量是通过原始上下文示例的线性组合形成的单个上下文演示。研究通过损失景观分析在线性Transformer中展示了任务向量的自然出现，并预测了任务向量在表示高秩映射时的失败，并在实际LLM上进行了验证。此外，还通过显著性分析和参数可视化进一步验证了发现。", "result": "研究结果表明，任务向量在线性Transformer中自然出现。同时，预测并证实了任务向量在表示高秩映射时的失败。此外，研究还通过显著性分析和参数可视化验证了发现，并提出通过在少样本提示中注入多个任务向量可以增强其性能。", "conclusion": "本研究的结果增进了对任务向量的理解，并阐明了Transformer模型中上下文学习的内在机制。", "translation": "任务向量提供了一种引人注目的机制，通过将任务特定信息提炼成单一、可重用的表示形式，从而加速上下文学习（ICL）中的推理。尽管它们取得了经验上的成功，但其出现和功能的潜在原理仍不清楚。这项工作提出了线性组合猜想，认为任务向量是通过原始上下文演示的线性组合形成的单个上下文演示。我们为这一猜想提供了理论和经验支持。首先，我们通过损失景观分析表明，任务向量在用三元组格式提示训练的线性Transformer中自然出现。接下来，我们预测了任务向量在表示高秩映射时的失败，并在实际大型语言模型（LLM）上证实了这一点。我们的发现通过显著性分析和参数可视化得到进一步验证，这表明通过向少样本提示中注入多个任务向量可以增强任务向量。总而言之，我们的研究结果增进了对任务向量的理解，并阐明了基于Transformer模型中ICL的潜在机制。", "summary": "本研究深入探讨了上下文学习（ICL）中任务向量的出现、功能和局限性。论文提出了“线性组合猜想”，认为任务向量是原始上下文示例的线性组合。通过理论和实验验证，研究揭示了任务向量在线性Transformer中的自然出现，并预测了其在高秩映射表示上的失效。研究结果增进了对任务向量及其在Transformer模型ICL机制中的作用的理解，并提出了通过注入多个任务向量来增强其性能的潜力。", "keywords": "任务向量, 上下文学习, 线性组合猜想, Transformer, 大型语言模型", "comments": "该论文通过提出“线性组合猜想”为任务向量的理解提供了新的理论框架，并通过理论和实验相结合的方式，深入探讨了其出现、功能和局限性，特别是在高秩映射上的失效分析，为实际LLM的应用提供了指导。同时，提出的通过注入多个任务向量来增强性能的思路也具有创新性。"}}
{"id": "2506.08700", "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts", "authors": ["Ruiran Su", "Jiasheng Si", "Zhijiang Guo", "Janet B. Pierrehumbert"], "summary": "Scientific fact-checking has mostly focused on text and tables, overlooking\nscientific charts, which are key for presenting quantitative evidence and\nstatistical reasoning. We introduce ClimateViz, the first large-scale benchmark\nfor scientific fact-checking using expert-curated scientific charts. ClimateViz\ncontains 49,862 claims linked to 2,896 visualizations, each labeled as support,\nrefute, or not enough information. To improve interpretability, each example\nincludes structured knowledge graph explanations covering trends, comparisons,\nand causal relations. We evaluate state-of-the-art multimodal language models,\nincluding both proprietary and open-source systems, in zero-shot and few-shot\nsettings. Results show that current models struggle with chart-based reasoning:\neven the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to\n77.8 percent accuracy in label-only settings, far below human performance (89.3\nand 92.7 percent). Explanation-augmented outputs improve performance in some\nmodels. We released our dataset and code alongside the paper.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08700v1", "AI": {"title_translation": "气候可视化：一个用于科学图表统计推理和事实核查的基准", "tldr": "引入ClimateViz，首个用于科学图表事实核查的大规模基准数据集，发现现有模型在图表推理方面表现不佳。", "motivation": "科学事实核查主要集中在文本和表格，忽略了作为定量证据和统计推理关键的科学图表。", "method": "引入ClimateViz数据集，包含49,862个声明和2,896个可视化图表，每个都有支持、反驳或信息不足的标签，并包含知识图谱解释。评估了最先进的多模态语言模型在零样本和少样本设置下的表现。", "result": "现有模型在图表推理方面表现不佳；即使是最好的系统，如Gemini 2.5和InternVL 2.5，在仅标签设置下也只能达到76.2%至77.8%的准确率，远低于人类表现（89.3%和92.7%）。解释增强的输出在某些模型中提高了性能。", "conclusion": "现有模型在基于图表的科学事实核查方面仍有显著提升空间，需要进一步研究以缩小与人类表现的差距。", "translation": "科学事实核查主要集中在文本和表格，而忽略了作为呈现定量证据和统计推理关键的科学图表。我们引入了ClimateViz，这是第一个使用专家策划的科学图表进行科学事实核查的大规模基准数据集。ClimateViz包含49,862个声明，链接到2,896个可视化图表，每个都标记为支持、反驳或信息不足。为了提高可解释性，每个示例都包含结构化知识图谱解释，涵盖趋势、比较和因果关系。我们评估了最先进的多模态语言模型，包括专有和开源系统，在零样本和少样本设置下的表现。结果显示，当前模型在基于图表的推理方面表现不佳：即使是最好的系统，如Gemini 2.5和InternVL 2.5，在仅标签设置下也只能达到76.2%至77.8%的准确率，远低于人类表现（89.3%和92.7%）。解释增强的输出在某些模型中提高了性能。我们随论文发布了数据集和代码。", "summary": "本文介绍了ClimateViz，一个用于科学图表事实核查的首个大规模基准数据集，旨在弥补现有事实核查研究对科学图表关注不足的空白。该数据集包含大量图表声明及其标签和结构化解释。研究评估了当前多模态语言模型在该任务上的表现，发现它们在图表推理方面仍远低于人类水平，表明该领域仍有巨大改进空间。", "keywords": "科学图表, 事实核查, 统计推理, 基准数据集, 多模态语言模型", "comments": "该论文通过引入首个大规模科学图表事实核查基准数据集ClimateViz，填补了科学事实核查领域的一个重要空白，强调了对科学图表进行推理和核查的重要性。它揭示了当前先进多模态模型在该任务上的局限性，为未来研究指明了方向，具有重要的实践和研究价值。"}}
{"id": "2506.08594", "title": "Solving excited states for long-range interacting trapped ions with neural networks", "authors": ["Yixuan Ma", "Chang Liu", "Weikang Li", "Shun-Yao Zhang", "L. -M. Duan", "Yukai Wu", "Dong-Ling Deng"], "summary": "The computation of excited states in strongly interacting quantum many-body\nsystems is of fundamental importance. Yet, it is notoriously challenging due to\nthe exponential scaling of the Hilbert space dimension with the system size.\nHere, we introduce a neural network-based algorithm that can simultaneously\noutput multiple low-lying excited states of a quantum many-body spin system in\nan accurate and efficient fashion. This algorithm, dubbed the neural quantum\nexcited-state (NQES) algorithm, requires no explicit orthogonalization of the\nstates and is generally applicable to higher dimensions. We demonstrate,\nthrough concrete examples including the Haldane-Shastry model with all-to-all\ninteractions, that the NQES algorithm is capable of efficiently computing\nmultiple excited states and their related observable expectations. In addition,\nwe apply the NQES algorithm to two classes of long-range interacting\ntrapped-ion systems in a two-dimensional Wigner crystal. For non-decaying\nall-to-all interactions with alternating signs, our computed low-lying excited\nstates bear spatial correlation patterns similar to those of the ground states,\nwhich closely match recent experimental observations that the\nquasi-adiabatically prepared state accurately reproduces analytical\nground-state correlations. For a system of up to 300 ions with power-law\ndecaying antiferromagnetic interactions, we successfully uncover its gap\nscaling and correlation features. Our results establish a scalable and\nefficient algorithm for computing excited states of interacting quantum\nmany-body systems, which holds potential applications ranging from benchmarking\nquantum devices to photoisomerization.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.08594v1", "AI": {"title_translation": "用神经网络求解长程相互作用囚禁离子的激发态", "tldr": "本文提出了一种基于神经网络的算法（NQES），用于高效准确地计算强相互作用量子多体系统（特别是长程相互作用囚禁离子）的多个低能激发态。", "motivation": "在强相互作用量子多体系统中计算激发态具有根本重要性，但由于希尔伯特空间维度随系统尺寸呈指数增长，这项任务极具挑战性。", "method": "本文介绍了一种名为神经量子激发态（NQES）的基于神经网络的算法。该算法能够准确有效地同时输出量子多体自旋系统的多个低能激发态，不需要对态进行显式正交化，并且普遍适用于更高维度。", "result": "NQES算法被证明能够有效地计算Haldane-Shastry模型等具体示例中的多个激发态及其相关的可观测期望值。此外，该算法还应用于二维Wigner晶体中的两类长程相互作用囚禁离子系统。对于具有交替符号的非衰减全对全相互作用，计算出的低能激发态具有与基态相似的空间关联模式，这与最近的实验观察结果吻合。对于多达300个离子、具有幂律衰减反铁磁相互作用的系统，该算法成功揭示了其能隙标度律和关联特征。", "conclusion": "研究结果建立了一种可扩展且高效的算法，用于计算相互作用量子多体系统的激发态，这在量子设备基准测试到光异构化等领域具有潜在应用。", "translation": "在强相互作用量子多体系统中计算激发态具有根本重要性。然而，由于希尔伯特空间维度随系统尺寸呈指数增长，这项任务极具挑战性。本文介绍了一种基于神经网络的算法，该算法能够准确有效地同时输出量子多体自旋系统的多个低能激发态。这种算法被称为神经量子激发态（NQES）算法，它不需要对态进行显式正交化，并且普遍适用于更高维度。我们通过具体示例（包括具有全对全相互作用的Haldane-Shastry模型）证明，NQES算法能够有效地计算多个激发态及其相关的可观测期望值。此外，我们将NQES算法应用于二维Wigner晶体中的两类长程相互作用囚禁离子系统。对于具有交替符号的非衰减全对全相互作用，我们计算出的低能激发态具有与基态相似的空间关联模式，这与最近的实验观察结果（准绝热制备的态准确再现了解析基态关联）非常吻合。对于多达300个离子、具有幂律衰减反铁磁相互作用的系统，我们成功揭示了其能隙标度律和关联特征。我们的结果建立了一种可扩展且高效的算法，用于计算相互作用量子多体系统的激发态，这在量子设备基准测试到光异构化等领域具有潜在应用。", "summary": "本文提出了一种新颖的基于神经网络的神经量子激发态（NQES）算法，用于准确高效地计算强相互作用量子多体系统（特别是长程相互作用囚禁离子）的多个低能激发态。与传统方法不同，NQES避免了显式正交化，并能很好地随系统尺寸扩展。该算法通过Haldane-Shastry模型和多达300个囚禁离子系统等各种示例进行了验证，成功再现了实验观测结果并揭示了能隙标度律和关联模式等关键特征。这项工作为量子多体物理学提供了一种可扩展的工具，具有广泛的应用前景。", "keywords": "神经网络, 激发态, 量子多体系统, 囚禁离子, NQES算法", "comments": "这篇论文通过利用神经网络解决激发态计算这一具有挑战性的问题，在计算量子多体物理学领域取得了重大进展。NQES算法能够避免显式正交化并适用于更高维度是其关键创新点。它在处理300个囚禁离子等大型系统并再现实验结果方面的成功，突显了其实用性和可扩展性，为理解复杂的量子现象和量子设备基准测试开辟了新途径。"}}
{"id": "2506.08030", "title": "MOSS: Multi-Objective Optimization for Stable Rule Sets", "authors": ["Brian Liu", "Rahul Mazumder"], "summary": "We present MOSS, a multi-objective optimization framework for constructing\nstable sets of decision rules. MOSS incorporates three important criteria for\ninterpretability: sparsity, accuracy, and stability, into a single\nmulti-objective optimization framework. Importantly, MOSS allows a practitioner\nto rapidly evaluate the trade-off between accuracy and stability in sparse rule\nsets in order to select an appropriate model. We develop a specialized cutting\nplane algorithm in our framework to rapidly compute the Pareto frontier between\nthese two objectives, and our algorithm scales to problem instances beyond the\ncapabilities of commercial optimization solvers. Our experiments show that MOSS\noutperforms state-of-the-art rule ensembles in terms of both predictive\nperformance and stability.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.08030v1", "AI": {"title_translation": "MOSS：稳定规则集的多目标优化", "tldr": "MOSS是一个多目标优化框架，用于构建稳定的决策规则集，通过专门的割平面算法平衡稀疏规则集的准确性和稳定性，并在实验中优于现有技术。", "motivation": "现有方法在构建可解释的决策规则集时，难以同时平衡稀疏性、准确性和稳定性。MOSS旨在提供一个多目标优化框架，使实践者能够快速评估稀疏规则集中准确性和稳定性之间的权衡，从而选择合适的模型。", "method": "MOSS采用多目标优化框架，将稀疏性、准确性和稳定性三个可解释性标准整合在一起。它开发了一种专门的割平面算法，用于快速计算准确性和稳定性这两个目标之间的帕累托前沿，并且该算法能够扩展到商业优化求解器无法处理的问题实例。", "result": "实验结果表明，MOSS在预测性能和稳定性方面均优于最先进的规则集成方法。", "conclusion": "MOSS通过其多目标优化框架和专门的割平面算法，成功地在稀疏规则集中平衡了准确性和稳定性，并在性能上超越了现有技术，为构建可解释的决策规则集提供了一个有效且高效的解决方案。", "translation": "我们提出了MOSS，一个用于构建稳定决策规则集的多目标优化框架。MOSS将可解释性的三个重要标准：稀疏性、准确性和稳定性，整合到一个单一的多目标优化框架中。重要的是，MOSS允许实践者快速评估稀疏规则集中准确性和稳定性之间的权衡，以便选择合适的模型。我们在框架中开发了一种专门的割平面算法，以快速计算这两个目标之间的帕累托前沿，并且我们的算法可以扩展到商业优化求解器无法处理的问题实例。我们的实验表明，MOSS在预测性能和稳定性方面均优于最先进的规则集成方法。", "summary": "MOSS是一个创新的多目标优化框架，旨在构建稳定的决策规则集。它整合了稀疏性、准确性和稳定性这三个关键的可解释性标准。该框架允许用户快速评估准确性和稳定性之间的权衡。MOSS引入了一种专门的割平面算法，能够高效计算帕累托前沿，并处理大规模问题。实验证明，MOSS在预测性能和稳定性方面均优于现有规则集成方法。", "keywords": "多目标优化, 决策规则, 稳定性, 可解释性, 割平面算法", "comments": "MOSS的创新之处在于其将稀疏性、准确性和稳定性这三个关键的可解释性指标整合到一个多目标优化框架中，并通过专门的割平面算法高效地计算帕累托前沿。这对于需要权衡模型性能和可解释性的领域具有重要意义。该方法在解决大规模问题方面的能力也超越了商业求解器，显示出其强大的实用性。"}}
{"id": "2506.08647", "title": "Summarization for Generative Relation Extraction in the Microbiome Domain", "authors": ["Oumaima El Khettari", "Solen Quiniou", "Samuel Chaffron"], "summary": "We explore a generative relation extraction (RE) pipeline tailored to the\nstudy of interactions in the intestinal microbiome, a complex and low-resource\nbiomedical domain. Our method leverages summarization with large language\nmodels (LLMs) to refine context before extracting relations via\ninstruction-tuned generation. Preliminary results on a dedicated corpus show\nthat summarization improves generative RE performance by reducing noise and\nguiding the model. However, BERT-based RE approaches still outperform\ngenerative models. This ongoing work demonstrates the potential of generative\nmethods to support the study of specialized domains in low-resources setting.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08647v1", "AI": {"title_translation": "针对微生物组领域生成式关系抽取的摘要方法", "tldr": "在低资源微生物组领域，利用LLM摘要改进生成式关系抽取，但BERT模型仍表现更优，展示了生成式方法的潜力。", "motivation": "在复杂的低资源生物医学领域（如肠道微生物组）研究相互作用时，关系抽取面临挑战。", "method": "探索了一种生成式关系抽取（RE）流程，该方法利用大型语言模型（LLMs）进行摘要，以在通过指令调整生成抽取关系之前提炼上下文。", "result": "初步结果表明，摘要通过减少噪声和指导模型，提高了生成式RE的性能。然而，基于BERT的关系抽取方法仍然优于生成模型。", "conclusion": "生成式方法有潜力支持低资源环境下专业领域的研究。", "translation": "我们探索了一种生成式关系抽取（RE）流程，该流程专为研究肠道微生物组中的相互作用而设计，这是一个复杂且资源匮乏的生物医学领域。我们的方法利用大型语言模型（LLM）进行摘要，以在通过指令调整生成抽取关系之前提炼上下文。在专用语料库上的初步结果表明，摘要通过减少噪声和指导模型，提高了生成式RE的性能。然而，基于BERT的RE方法仍然优于生成模型。这项正在进行的工作展示了生成式方法在低资源环境下支持专业领域研究的潜力。", "summary": "本文探讨了一种针对低资源微生物组领域中关系抽取的生成式方法。该方法利用大型语言模型（LLMs）进行文本摘要，以优化关系抽取的上下文。初步结果显示，摘要处理有助于提升生成式关系抽取的性能，尽管当前BERT基线模型仍表现更优。研究表明，生成式方法在处理专业且资源受限的领域中具有应用前景。", "keywords": "关系抽取, 生成式模型, 微生物组, 大型语言模型, 摘要", "comments": "该研究的创新点在于将摘要技术引入生成式关系抽取流程，以应对低资源领域的挑战。其重要性在于探索了LLMs在生物医学领域关系抽取中的应用潜力，尤其是在数据稀缺的专业领域。尽管初步结果显示BERT模型仍占优势，但该工作为生成式方法在该领域的进一步发展奠定了基础，并指出了未来的研究方向。"}}
{"id": "2506.08065", "title": "Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions", "authors": ["Ye Zhu", "Duo Xu", "Zhiwei Deng", "Jonathon C. Tan", "Olga Russakovsky"], "summary": "We study Diffusion Schr\\\"odinger Bridge (DSB) models in the context of\ndynamical astrophysical systems, specifically tackling observational inverse\nprediction tasks within Giant Molecular Clouds (GMCs) for star formation. We\nintroduce the Astro-DSB model, a variant of DSB with the pairwise domain\nassumption tailored for astrophysical dynamics. By investigating its learning\nprocess and prediction performance in both physically simulated data and in\nreal observations (the Taurus B213 data), we present two main takeaways. First,\nfrom the astrophysical perspective, our proposed paired DSB method improves\ninterpretability, learning efficiency, and prediction performance over\nconventional astrostatistical and other machine learning methods. Second, from\nthe generative modeling perspective, probabilistic generative modeling reveals\nimprovements over discriminative pixel-to-pixel modeling in Out-Of-Distribution\n(OOD) testing cases of physical simulations with unseen initial conditions and\ndifferent dominant physical processes. Our study expands research into\ndiffusion models beyond the traditional visual synthesis application and\nprovides evidence of the models' learning abilities beyond pure data\nstatistics, paving a path for future physics-aware generative models which can\nalign dynamics between machine learning and real (astro)physical systems.", "comment": "Preprint. Code will be available at\n  https://github.com/L-YeZhu/AstroDSB", "cate": "astro-ph.IM", "url": "http://arxiv.org/abs/2506.08065v1", "AI": {"title_translation": "动态扩散薛定谔桥在天体物理观测反演中的应用", "tldr": "本文将扩散薛定谔桥模型应用于天体物理观测反演任务，提出了Astro-DSB模型，在可解释性、学习效率和预测性能方面优于传统方法，并展示了生成模型在OOD测试中的优势。", "motivation": "解决天体物理动力学系统（特别是巨分子云中恒星形成）中的观测逆预测任务，并改进现有方法的解释性、效率和性能。", "method": "引入了Astro-DSB模型，这是DSB的一个变体，具有针对天体物理动力学量身定制的成对域假设。通过物理模拟数据和真实观测数据（金牛座B213数据）进行学习过程和预测性能研究。", "result": "1. 从天体物理学角度看，提出的成对DSB方法在可解释性、学习效率和预测性能方面优于传统天体统计学和其他机器学习方法。2. 从生成建模角度看，概率生成建模在具有未见初始条件和不同主导物理过程的物理模拟的域外（OOD）测试案例中，表现优于判别式像素到像素建模。", "conclusion": "本研究将扩散模型的研究扩展到传统视觉合成应用之外，并提供了模型超越纯数据统计的学习能力的证据，为未来能够对齐机器学习与真实（天体）物理系统之间动力学的物理感知生成模型铺平了道路。", "translation": "我们研究了动态天体物理系统背景下的扩散薛定谔桥（DSB）模型，特别是在巨分子云（GMC）中恒星形成观测逆预测任务。我们引入了Astro-DSB模型，它是DSB的一个变体，具有为天体物理动力学量身定制的成对域假设。通过研究其在物理模拟数据和真实观测数据（金牛座B213数据）中的学习过程和预测性能，我们得出两个主要结论。首先，从天体物理学的角度来看，我们提出的成对DSB方法在可解释性、学习效率和预测性能方面优于传统的天体统计学和其他机器学习方法。其次，从生成建模的角度来看，概率生成建模在具有未见初始条件和不同主导物理过程的物理模拟的域外（OOD）测试案例中，表现出优于判别式像素到像素建模的改进。我们的研究将扩散模型的研究扩展到传统视觉合成应用之外，并提供了模型超越纯数据统计的学习能力的证据，为未来能够对齐机器学习与真实（天体）物理系统之间动力学的物理感知生成模型铺平了道路。", "summary": "本文针对天体物理观测中的逆预测任务，引入了Astro-DSB模型，一个专门用于天体物理动力学的扩散薛定谔桥变体。研究表明，该模型在可解释性、学习效率和预测性能上优于传统方法，并且在域外测试中，概率生成建模展现出比判别式模型更优的性能。这项工作拓展了扩散模型在非视觉合成领域的应用，并为构建物理感知的生成模型奠定了基础。", "keywords": "扩散薛定谔桥, 天体物理反演, Astro-DSB, 生成模型, 巨分子云", "comments": "本文的创新在于将扩散薛定谔桥模型应用于天体物理观测反演这一非传统领域，并提出了定制化的Astro-DSB模型。其重要性在于提升了天体物理数据分析的可解释性、效率和泛化能力，尤其是在处理OOD数据方面。这为未来结合物理知识的机器学习模型发展提供了新的方向。"}}
{"id": "2506.08121", "title": "Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence", "authors": ["Qi Feng", "Gu Wang"], "summary": "We introduce a continuous policy-value iteration algorithm where the\napproximations of the value function of a stochastic control problem and the\noptimal control are simultaneously updated through Langevin-type dynamics. This\nframework applies to both the entropy-regularized relaxed control problems and\nthe classical control problems, with infinite horizon. We establish policy\nimprovement and demonstrate convergence to the optimal control under the\nmonotonicity condition of the Hamiltonian. By utilizing Langevin-type\nstochastic differential equations for continuous updates along the policy\niteration direction, our approach enables the use of distribution sampling and\nnon-convex learning techniques in machine learning to optimize the value\nfunction and identify the optimal control simultaneously.", "comment": "37 pages", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.08121v1", "AI": {"title_translation": "随机控制问题的连续策略与价值迭代及其收敛性", "tldr": "本文提出了一种连续策略-价值迭代算法，通过Langevin型动力学同时更新随机控制问题的价值函数和最优控制，并证明了其收敛性。", "motivation": "解决随机控制问题中的价值函数和最优控制的同步更新问题，并利用机器学习技术进行优化。", "method": "引入了一种连续策略-价值迭代算法，通过Langevin型动力学同时更新价值函数和最优控制。该框架适用于熵正则化松弛控制问题和经典控制问题，且具有无限时间范围。利用Langevin型随机微分方程进行连续更新，使得该方法能够利用分布采样和非凸学习技术。", "result": "建立了策略改进，并在哈密顿量单调性条件下证明了收敛到最优控制。", "conclusion": "本文提出的连续策略-价值迭代算法在哈密顿量单调性条件下能够收敛到最优控制，并且能够利用机器学习中的分布采样和非凸学习技术进行优化。", "translation": "我们引入了一种连续策略-价值迭代算法，其中随机控制问题的价值函数和最优控制的近似值通过Langevin型动力学同时更新。该框架适用于熵正则化松弛控制问题和经典控制问题，且具有无限时间范围。我们建立了策略改进，并在哈密顿量单调性条件下证明了收敛到最优控制。通过利用Langevin型随机微分方程进行沿策略迭代方向的连续更新，我们的方法使得在机器学习中利用分布采样和非凸学习技术来同时优化价值函数和识别最优控制成为可能。", "summary": "本文提出了一种新颖的连续策略-价值迭代算法，该算法利用Langevin型动力学同时更新随机控制问题的价值函数和最优控制。该方法适用于多种控制问题，并在哈密顿量单调性假设下证明了其收敛性，从而能够利用机器学习中的分布采样和非凸学习技术进行优化。", "keywords": "连续策略迭代, 价值迭代, 随机控制, Langevin动力学, 收敛性", "comments": "该论文的创新点在于将Langevin型动力学引入到策略-价值迭代过程中，实现了价值函数和最优控制的同步连续更新。这使得利用机器学习中的先进技术（如分布采样和非凸学习）来解决随机控制问题成为可能，具有重要的理论和实践意义。"}}
{"id": "2506.08127", "title": "Constrained Pareto Set Identification with Bandit Feedback", "authors": ["Cyrille Kone", "Emilie Kaufmann", "Laura Richert"], "summary": "In this paper, we address the problem of identifying the Pareto Set under\nfeasibility constraints in a multivariate bandit setting. Specifically, given a\n$K$-armed bandit with unknown means $\\mu_1, \\dots, \\mu_K \\in \\mathbb{R}^d$, the\ngoal is to identify the set of arms whose mean is not uniformly worse than that\nof another arm (i.e., not smaller for all objectives), while satisfying some\nknown set of linear constraints, expressing, for example, some minimal\nperformance on each objective. Our focus lies in fixed-confidence\nidentification, for which we introduce an algorithm that significantly\noutperforms racing-like algorithms and the intuitive two-stage approach that\nfirst identifies feasible arms and then their Pareto Set. We further prove an\ninformation-theoretic lower bound on the sample complexity of any algorithm for\nconstrained Pareto Set identification, showing that the sample complexity of\nour approach is near-optimal. Our theoretical results are supported by an\nextensive empirical evaluation on a series of benchmarks.", "comment": "To appear in Proceedings of ICML2025", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.08127v1", "AI": {"title_translation": "带老虎机反馈的受限帕累托集识别", "tldr": "本文提出了一种在多变量老虎机环境下识别受限帕累托集的算法，该算法性能显著优于现有方法且样本复杂度接近最优。", "motivation": "在多变量老虎机设置中，需要识别满足特定线性约束的帕累托集，即那些均值不比其他臂在所有目标上都差的臂。现有方法（如赛跑式算法和两阶段方法）效率不高。", "method": "论文提出了一种针对固定置信度识别的算法，用于在多变量老虎机设置下识别受限帕累托集。此外，还证明了该算法的样本复杂度接近信息论下界。", "result": "该算法在固定置信度识别方面显著优于赛跑式算法和直观的两阶段方法（先识别可行臂再识别帕累托集）。理论结果表明，该方法的样本复杂度接近最优，并通过广泛的经验评估得到支持。", "conclusion": "本文提出了一种有效且样本复杂度接近最优的算法，用于在多变量老虎机设置下识别受限帕累托集。", "translation": "在本文中，我们解决了在多变量老虎机设置中识别受限帕累托集的问题。具体来说，给定一个具有未知均值 $\\mu_1, \\dots, \\mu_K \\in \\mathbb{R}^d$ 的 K 臂老虎机，目标是识别那些其均值不比其他臂在所有目标上都差（即不小于所有目标）的臂集，同时满足一些已知的线性约束集，例如，每个目标上的最小性能。我们的重点在于固定置信度识别，为此我们引入了一种算法，该算法显著优于赛跑式算法和首先识别可行臂然后识别其帕累托集的直观两阶段方法。我们进一步证明了任何受限帕累托集识别算法在样本复杂度上的信息论下界，表明我们方法的样本复杂度接近最优。我们的理论结果得到了在一系列基准测试上广泛的经验评估的支持。", "summary": "本文研究了在多变量老虎机环境下识别受限帕累托集的问题，其中目标是识别满足已知线性约束的帕累托臂集。针对固定置信度识别，我们提出了一种新算法，该算法在性能上显著优于现有方法，并且通过理论证明和实证评估，其样本复杂度被证实接近信息论下界。", "keywords": "帕累托集识别, 带约束优化, 多变量老虎机, 样本复杂度, 固定置信度识别", "comments": "本文的创新点在于提出了一个在受限帕累托集识别问题上具有显著性能提升和近最优样本复杂度的算法。这对于多目标优化和决策制定在不确定环境下的应用具有重要意义。理论分析和实证结果相互支持，增加了研究的可信度。"}}
{"id": "2506.08712", "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization", "authors": ["Hee Suk Yoon", "Eunseop Yoon", "Mark A. Hasegawa-Johnson", "Sungwoong Kim", "Chang D. Yoo"], "summary": "We introduce ConfPO, a method for preference learning in Large Language\nModels (LLMs) that identifies and optimizes preference-critical tokens based\nsolely on the training policy's confidence, without requiring any auxiliary\nmodels or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as\nDirect Preference Optimization (DPO), which uniformly adjust all token\nprobabilities regardless of their relevance to preference, ConfPO focuses\noptimization on the most impactful tokens. This targeted approach improves\nalignment quality while mitigating overoptimization (i.e., reward hacking) by\nusing the KL divergence budget more efficiently. In contrast to recent\ntoken-level methods that rely on credit-assignment models or AI annotators,\nraising concerns about scalability and reliability, ConfPO is simple,\nlightweight, and model-free. Experimental results on challenging alignment\nbenchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO\nconsistently outperforms uniform DAAs across various LLMs, delivering better\nalignment with zero additional computational overhead.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08712v1", "AI": {"title_translation": "ConfPO：利用策略模型置信度进行大型语言模型偏好优化中的关键令牌选择", "tldr": "ConfPO 是一种大型语言模型偏好优化方法，通过根据策略模型置信度选择和优化关键令牌，提高了对齐质量，同时没有额外计算开销，优于现有方法。", "motivation": "现有的直接对齐算法（如 DPO）统一调整所有令牌的概率，导致 KL 散度预算使用效率低下并可能过度优化。同时，其他令牌级方法依赖辅助模型或 AI 标注器，存在可伸缩性和可靠性问题。ConfPO 旨在通过有针对性的优化解决这些限制。", "method": "ConfPO 是一种用于大型语言模型 (LLM) 偏好学习的方法，它仅根据训练策略的置信度识别和优化偏好关键令牌。该方法不依赖任何辅助模型或计算，简单、轻量且无需模型。它通过将优化重点放在最具影响力的令牌上，更有效地利用 KL 散度预算，从而提高对齐质量并减轻过度优化。", "result": "在 AlpacaEval 2 和 Arena-Hard 等具有挑战性的对齐基准测试中，ConfPO 在各种 LLM 上始终优于统一的直接对齐算法 (DAA)，实现了更好的对齐，且没有额外的计算开销。", "conclusion": "ConfPO 是一种有效、高效且简单的大型语言模型偏好优化方法，它通过基于策略置信度关注关键令牌来提高对齐质量，且无需额外的计算资源。", "translation": "我们引入了 ConfPO，这是一种用于大型语言模型 (LLM) 偏好学习的方法，它仅根据训练策略的置信度识别和优化偏好关键令牌，无需任何辅助模型或计算。与以前的直接对齐算法 (DAA)（例如直接偏好优化 (DPO)）不同，后者不分青红皂白地调整所有令牌的概率，无论它们与偏好的相关性如何，ConfPO 将优化重点放在最具影响力的令牌上。这种有针对性的方法通过更有效地利用 KL 散度预算来提高对齐质量，同时减轻过度优化（即奖励黑客）。与最近依赖信用分配模型或 AI 注释器而引起可伸缩性和可靠性担忧的令牌级方法相反，ConfPO 简单、轻量且无需模型。在具有挑战性的对齐基准（包括 AlpacaEval 2 和 Arena-Hard）上的实验结果表明，ConfPO 在各种 LLM 上始终优于统一 DAA，以零额外计算开销实现更好的对齐。", "summary": "ConfPO 是一种针对大型语言模型偏好优化的新方法，它通过识别和优化基于策略模型置信度的“偏好关键令牌”来提高对齐质量。与 DPO 等传统方法不同，ConfPO 避免了对所有令牌的统一调整，从而更有效地利用 KL 散度预算并减轻过度优化。ConfPO 无需辅助模型，因此简单、轻量且高效。实验证明，ConfPO 在多个基准测试中优于现有方法，且没有额外的计算成本。", "keywords": "大型语言模型, 偏好优化, 令牌选择, 策略置信度, 直接对齐算法", "comments": "ConfPO 的创新之处在于其无需辅助模型即可进行令牌级优化的能力，通过利用策略模型的置信度来识别关键令牌，从而提高了 LLM 偏好优化的效率和效果。其简单、轻量和无需额外计算开销的特点使其具有很高的实用价值和可扩展性，解决了现有方法在可伸缩性和可靠性方面的担忧。"}}
{"id": "2506.08726", "title": "Improved LLM Agents for Financial Document Question Answering", "authors": ["Nelvin Tan", "Zian Seng", "Liang Zhang", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "summary": "Large language models (LLMs) have shown impressive capabilities on numerous\nnatural language processing tasks. However, LLMs still struggle with numerical\nquestion answering for financial documents that include tabular and textual\ndata. Recent works have showed the effectiveness of critic agents (i.e.,\nself-correction) for this task given oracle labels. Building upon this\nframework, this paper examines the effectiveness of the traditional critic\nagent when oracle labels are not available, and show, through experiments, that\nthis critic agent's performance deteriorates in this scenario. With this in\nmind, we present an improved critic agent, along with the calculator agent\nwhich outperforms the previous state-of-the-art approach (program-of-thought)\nand is safer. Furthermore, we investigate how our agents interact with each\nother, and how this interaction affects their performance.", "comment": "12 pages, 5 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08726v1", "AI": {"title_translation": "改进的LLM代理用于金融文档问答", "tldr": "大型语言模型（LLM）在金融文档的数值问答方面表现不佳，特别是在缺乏预言标签的情况下。本文提出了改进的批评代理和计算器代理，它们超越了现有最先进的方法，并且更安全。", "motivation": "大型语言模型（LLM）在处理包含表格和文本数据的金融文档的数值问答方面仍然面临挑战。现有的批评代理虽然有效，但需要预言标签（oracle labels），在缺乏这些标签时性能会下降。", "method": "本文首先检验了在没有预言标签的情况下传统批评代理的有效性。在此基础上，提出了一种改进的批评代理和一个新的计算器代理。此外，研究了这些代理之间如何相互作用以及这种相互作用如何影响它们的性能。", "result": "实验表明，在没有预言标签的情况下，传统批评代理的性能会下降。本文提出的改进批评代理和计算器代理超越了之前最先进的方法（思维程序），并且更安全。", "conclusion": "本文提出的改进批评代理和计算器代理有效提升了LLM在金融文档数值问答任务上的性能，在不依赖预言标签的情况下，提供了比现有方法更安全、更准确的解决方案。", "translation": "大型语言模型（LLM）在众多自然语言处理任务中展现出令人印象深刻的能力。然而，LLM在处理包含表格和文本数据的金融文档的数值问答方面仍然面临挑战。最近的研究表明，在给定预言标签（oracle labels）的情况下，批评代理（即自我纠正）对这项任务是有效的。在此框架的基础上，本文研究了在没有预言标签的情况下传统批评代理的有效性，并通过实验表明在这种情况下该批评代理的性能会下降。考虑到这一点，我们提出了一种改进的批评代理，以及一个计算器代理，它超越了之前最先进的方法（思维程序，program-of-thought）并且更安全。此外，我们还研究了我们的代理之间如何相互作用，以及这种相互作用如何影响它们的性能。", "summary": "本文解决了大型语言模型（LLM）在金融文档数值问答方面的挑战。研究发现，在缺乏预言标签时，传统的批评代理性能会下降。为解决此问题，论文提出了一种改进的批评代理和一个新颖的计算器代理。实验证明，这些新代理在性能上超越了现有最先进方法（思维程序），并提供了更高的安全性。论文还探讨了这些代理之间的相互作用及其对性能的影响。", "keywords": "LLM代理, 金融文档, 问答, 批评代理, 计算器代理", "comments": "该论文的创新之处在于，它解决了LLM在金融数值问答中对预言标签（oracle labels）的依赖问题，这在实际应用中是一个重要限制。引入专门的计算器代理是一种实用且有效的方法。关注安全性和在无预言标签情况下超越现有SOTA，使得这项工作对将LLM应用于金融等关键领域具有重要意义。"}}
{"id": "2506.08276", "title": "LEANN: A Low-Storage Vector Index", "authors": ["Yichuan Wang", "Shu Liu", "Zhifei Li", "Yongji Wu", "Ziming Mao", "Yilong Zhao", "Xiao Yan", "Zhiying Xu", "Yang Zhou", "Ion Stoica", "Sewon Min", "Matei Zaharia", "Joseph E. Gonzalez"], "summary": "Embedding-based search is widely used in applications such as recommendation\nand retrieval-augmented generation (RAG). Recently, there is a growing demand\nto support these capabilities over personal data stored locally on devices.\nHowever, maintaining the necessary data structure associated with the\nembedding-based search is often infeasible due to its high storage overhead.\nFor example, indexing 100 GB of raw data requires 150 to 700 GB of storage,\nmaking local deployment impractical. Reducing this overhead while maintaining\nsearch quality and latency becomes a critical challenge. In this paper, we\npresent LEANN, a storage-efficient approximate nearest neighbor (ANN) search\nindex optimized for resource-constrained personal devices. LEANN combines a\ncompact graph-based structure with an efficient on-the-fly recomputation\nstrategy to enable fast and accurate retrieval with minimal storage overhead.\nOur evaluation shows that LEANN reduces index size to under 5% of the original\nraw data, achieving up to 50 times smaller storage than standard indexes, while\nmaintaining 90% top-3 recall in under 2 seconds on real-world question\nanswering benchmarks.", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.08276v1", "AI": {"title_translation": "LEANN：一种低存储向量索引", "tldr": "LEANN是一种针对资源受限个人设备的低存储ANN索引，通过结合紧凑图结构和即时重计算，将索引大小减少到原始数据的5%以下，存储空间比标准索引小50倍，同时保持高召回率和低延迟。", "motivation": "基于嵌入的搜索（如推荐和RAG）在本地设备上的应用日益增长，但其高存储开销使得部署不切实际（例如，索引100GB数据需要150-700GB存储）。因此，在保持搜索质量和延迟的同时，减少存储开销成为一个关键挑战。", "method": "LEANN结合了紧凑的图结构和高效的即时重计算策略，以实现快速准确的检索和最小的存储开销。", "result": "LEANN将索引大小减少到原始原始数据不到5%，比标准索引存储空间小50倍。在真实世界的问答基准测试中，它在2秒内保持了90%的前3召回率。", "conclusion": "LEANN成功地解决了在资源受限的个人设备上部署基于嵌入的搜索所面临的存储挑战，通过显著减少索引大小，同时保持了高搜索性能。", "translation": "基于嵌入的搜索广泛应用于推荐和检索增强生成（RAG）等应用程序。最近，在本地设备上支持这些功能的需求日益增长。然而，由于其高存储开销，维护与基于嵌入的搜索相关的数据结构通常是不可行的。例如，索引100GB的原始数据需要150到700GB的存储空间，这使得本地部署不切实际。在保持搜索质量和延迟的同时减少这种开销成为一个关键挑战。在本文中，我们提出了LEANN，这是一种为资源受限的个人设备优化的存储高效近似最近邻（ANN）搜索索引。LEANN结合了紧凑的基于图的结构和高效的即时重计算策略，以最小的存储开销实现快速准确的检索。我们的评估表明，LEANN将索引大小减少到原始原始数据的5%以下，比标准索引存储空间小50倍，同时在真实世界的问答基准测试中，在2秒内保持了90%的前3召回率。", "summary": "本论文提出了一种名为LEANN的低存储向量索引，旨在解决基于嵌入的搜索在资源受限个人设备上部署时面临的巨大存储开销问题。LEANN通过结合紧凑的图结构和高效的即时重计算策略，显著降低了索引大小，实验证明其索引大小可降至原始数据的5%以下，存储空间比现有标准索引小50倍，同时在真实世界问答任务中保持了高召回率和低延迟。", "keywords": "低存储, 向量索引, ANN, 嵌入式搜索, 个人设备", "comments": "LEANN在解决本地设备上嵌入式搜索的存储瓶颈方面具有重要意义，其创新点在于结合了紧凑图结构和即时重计算，实现了极高的存储效率。这对于推动AI应用在边缘设备上的普及具有积极作用，特别是在个人数据处理和隐私保护方面。该方法展示了在严格资源限制下实现高性能ANN搜索的可行性。"}}
{"id": "2506.08753", "title": "Factors affecting the in-context learning abilities of LLMs for dialogue state tracking", "authors": ["Pradyoth Hegde", "Santosh Kesiraju", "Jan Švec", "Šimon Sedláček", "Bolaji Yusuf", "Oldřich Plchot", "Deepak K T", "Jan Černocký"], "summary": "This study explores the application of in-context learning (ICL) to the\ndialogue state tracking (DST) problem and investigates the factors that\ninfluence its effectiveness. We use a sentence embedding based k-nearest\nneighbour method to retrieve the suitable demonstrations for ICL. The selected\ndemonstrations, along with the test samples, are structured within a template\nas input to the LLM. We then conduct a systematic study to analyse the impact\nof factors related to demonstration selection and prompt context on DST\nperformance. This work is conducted using the MultiWoZ2.4 dataset and focuses\nprimarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and\nLlama3.2-3B-Instruct models. Our findings provide several useful insights on\nin-context learning abilities of LLMs for dialogue state tracking.", "comment": "Accepted to Interspeech 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08753v1", "AI": {"title_translation": "影响LLMs对话状态跟踪上下文学习能力的因素", "tldr": "本研究探讨了影响大型语言模型（LLMs）在对话状态跟踪（DST）中上下文学习（ICL）能力的因素，并提供了一些有用的见解。", "motivation": "本研究旨在探索上下文学习（ICL）在对话状态跟踪（DST）问题中的应用，并调查影响其有效性的因素。", "method": "研究采用基于句子嵌入的k-近邻方法来检索适合ICL的示例。选定的示例和测试样本被构建在模板中作为LLM的输入。研究对示例选择和提示上下文相关因素对DST性能的影响进行了系统性分析。研究使用了MultiWoZ2.4数据集，并主要关注OLMo-7B-instruct、Mistral-7B-Instruct-v0.3和Llama3.2-3B-Instruct模型。", "result": "研究结果为LLMs在对话状态跟踪中的上下文学习能力提供了几点有用的见解。", "conclusion": "本研究的发现为LLMs在对话状态跟踪中的上下文学习能力提供了有用的见解。", "translation": "这项研究探讨了上下文学习（ICL）在对话状态跟踪（DST）问题中的应用，并调查了影响其有效性的因素。我们使用基于句子嵌入的k-近邻方法来检索适合ICL的示例。选定的示例与测试样本一起被结构化在一个模板中，作为LLM的输入。然后，我们进行了一项系统性研究，分析了与示例选择和提示上下文相关的因素对DST性能的影响。这项工作使用MultiWoZ2.4数据集进行，主要关注OLMo-7B-instruct、Mistral-7B-Instruct-v0.3和Llama3.2-3B-Instruct模型。我们的研究结果为LLMs在对话状态跟踪中的上下文学习能力提供了几点有用的见解。", "summary": "本研究深入探讨了大型语言模型（LLMs）的上下文学习（ICL）能力在对话状态跟踪（DST）中的应用及其影响因素。通过采用基于句子嵌入的k-近邻方法选择示例，并将这些示例与测试样本一同作为LLM输入，研究系统性地分析了示例选择和提示上下文对DST性能的影响。研究基于MultiWoZ2.4数据集，并使用了OLMo-7B-instruct、Mistral-7B-Instruct-v0.3和Llama3.2-3B-Instruct等模型，最终为LLMs在DST中的ICL能力提供了宝贵的见解。", "keywords": "上下文学习, 对话状态跟踪, 大型语言模型, 示例选择, 提示工程", "comments": "这项研究通过系统性地分析示例选择和提示上下文对LLMs在对话状态跟踪中上下文学习能力的影响，为该领域提供了重要的见解。其创新之处在于将基于句子嵌入的k-近邻方法应用于ICL的示例选择，并对多种LLMs进行了实证研究，这对于理解和优化LLMs在复杂对话任务中的表现具有重要意义。"}}
{"id": "2506.08325", "title": "Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces", "authors": ["Marcos Matabuena", "Rahul Ghosal", "Pavlo Mozharovskyi", "Oscar Hernan Madrid Padilla", "Jukka-Pekka Onnela"], "summary": "Depth measures are powerful tools for defining level sets in emerging,\nnon--standard, and complex random objects such as high-dimensional multivariate\ndata, functional data, and random graphs. Despite their favorable theoretical\nproperties, the integration of depth measures into regression modeling to\nprovide prediction regions remains a largely underexplored area of research. To\naddress this gap, we propose a novel, model-free uncertainty quantification\nalgorithm based on conditional depth measures--specifically, conditional kernel\nmean embeddings and an integrated depth measure. These new algorithms can be\nused to define prediction and tolerance regions when predictors and responses\nare defined in separable Hilbert spaces. The use of kernel mean embeddings\nensures faster convergence rates in prediction region estimation. To enhance\nthe practical utility of the algorithms with finite samples, we also introduce\na conformal prediction variant that provides marginal, non-asymptotic\nguarantees for the derived prediction regions. Additionally, we establish both\nconditional and unconditional consistency results, as well as fast convergence\nrates in certain homoscedastic settings. We evaluate the finite--sample\nperformance of our model in extensive simulation studies involving various\ntypes of functional data and traditional Euclidean scenarios. Finally, we\ndemonstrate the practical relevance of our approach through a digital health\napplication related to physical activity, aiming to provide personalized\nrecommendations", "comment": "arXiv admin note: substantial text overlap with arXiv:2405.13970", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.08325v1", "AI": {"title_translation": "分离希尔伯特空间中回归模型不确定性量化的无模型核共形深度度量算法", "tldr": "本文提出了一种基于条件深度度量（包括条件核均值嵌入和集成深度度量）的新型无模型不确定性量化算法，用于在可分离希尔伯特空间中为回归模型定义预测和容忍区域，并提供有限样本保证和更快的收敛速度。", "motivation": "尽管深度度量在定义复杂随机对象的水平集方面具有良好的理论特性，但将其整合到回归模型中以提供预测区域的研究仍未得到充分探索。", "method": "我们提出了一种新颖的、无模型的、基于条件深度度量（特别是条件核均值嵌入和集成深度度量）的不确定性量化算法。该算法可用于在可分离希尔伯特空间中定义预测和容忍区域。为增强有限样本下的实用性，我们引入了提供边际非渐近保证的共形预测变体。", "result": "该方法利用核均值嵌入确保了预测区域估计的更快收敛速度。我们建立了条件和无条件一致性结果，以及在某些同方差设置下的快速收敛率。在广泛的模拟研究中，模型在各种函数数据和传统欧几里得场景中表现出良好的有限样本性能。此外，通过一个数字健康应用（与身体活动相关），该方法展示了实际相关性，旨在提供个性化推荐。", "conclusion": "本文成功提出了一个结合深度度量与回归建模的无模型不确定性量化算法，为复杂数据类型提供了具有理论保证和良好实际性能的预测区域。", "translation": "深度度量是定义新兴、非标准和复杂随机对象（如高维多元数据、函数数据和随机图）中水平集的强大工具。尽管它们具有良好的理论特性，但将深度度量整合到回归模型中以提供预测区域仍然是一个很大程度上未被充分探索的研究领域。为了弥补这一空白，我们提出了一种新颖的、无模型的、基于条件深度度量（特别是条件核均值嵌入和集成深度度量）的不确定性量化算法。这些新算法可用于当预测变量和响应变量在可分离希尔伯特空间中定义时，定义预测和容忍区域。核均值嵌入的使用确保了预测区域估计中更快的收敛速度。为了增强算法在有限样本下的实用性，我们还引入了一种共形预测变体，它为派生的预测区域提供了边际、非渐近保证。此外，我们建立了条件和无条件一致性结果，以及在某些同方差设置下的快速收敛率。我们在涉及各种类型函数数据和传统欧几里得场景的广泛模拟研究中评估了我们模型在有限样本下的性能。最后，我们通过一个与身体活动相关的数字健康应用，展示了我们方法的实际相关性，旨在提供个性化推荐。", "summary": "本文提出了一种新颖的无模型不确定性量化算法，将条件深度度量（包括条件核均值嵌入和集成深度度量）应用于回归模型，以在可分离希尔伯特空间中构建预测和容忍区域。该算法利用核均值嵌入实现更快的收敛速度，并引入共形预测变体以提供有限样本下的非渐近保证。研究建立了理论一致性与收敛性结果，并通过广泛模拟和数字健康应用验证了其在复杂数据类型上的有效性和实用性。", "keywords": "深度度量, 不确定性量化, 回归模型, 核共形预测, 希尔伯特空间", "comments": "该论文的创新之处在于将深度度量与回归模型相结合，解决了在复杂数据（如函数数据和希尔伯特空间数据）中进行不确定性量化的问题。通过引入核均值嵌入，显著提升了预测区域估计的收敛速度。此外，结合共形预测确保了有限样本下的理论保证，增强了算法的实用性。这为处理高维和非标准数据类型中的预测不确定性提供了一个有力的工具。"}}
{"id": "2506.08338", "title": "midr: Learning from Black-Box Models by Maximum Interpretation Decomposition", "authors": ["Ryoichi Asashiba", "Reiji Kozuma", "Hirokazu Iwasawa"], "summary": "The use of appropriate methods of Interpretable Machine Learning (IML) and\neXplainable Artificial Intelligence (XAI) is essential for adopting black-box\npredictive models in fields where model and prediction explainability is\nrequired. As a novel tool for interpreting black-box models, we introduce the R\npackage midr, which implements Maximum Interpretation Decomposition (MID). MID\nis a functional decomposition approach that derives a low-order additive\nrepresentation of a black-box model by minimizing the squared error between the\nmodel's prediction function and this additive representation. midr enables\nlearning from black-box models by constructing a global surrogate model with\nadvanced analytical capabilities. After reviewing related work and the\ntheoretical foundation of MID, we demonstrate the package's usage and discuss\nsome of its key features.", "comment": "20 pages, 10 figures", "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.08338v1", "AI": {"title_translation": "midr: 通过最大解释分解从黑盒模型中学习", "tldr": "midr是一个R软件包，它实现了最大解释分解（MID），通过构建全局代理模型来解释黑盒机器学习模型，从而提高其可解释性。", "motivation": "在需要模型和预测可解释性的领域中，采用黑盒预测模型时，使用适当的可解释机器学习（IML）和可解释人工智能（XAI）方法至关重要。", "method": "本文介绍了R软件包midr，该软件包实现了最大解释分解（MID）。MID是一种函数分解方法，通过最小化模型预测函数与加性表示之间的平方误差，来导出黑盒模型的低阶加性表示。midr通过构建具有高级分析功能的全局代理模型，实现从黑盒模型中学习。", "result": "本文回顾了相关工作和MID的理论基础，并演示了midr软件包的用法，讨论了其一些关键特性。", "conclusion": "midr软件包提供了一种新颖且重要的工具，通过最大解释分解（MID）来解释黑盒模型，这对于在需要可解释性的领域中采用黑盒模型至关重要。", "translation": "在需要模型和预测可解释性的领域中，采用黑盒预测模型时，使用适当的可解释机器学习（IML）和可解释人工智能（XAI）方法至关重要。作为一种解释黑盒模型的新工具，我们引入了R软件包midr，它实现了最大解释分解（MID）。MID是一种函数分解方法，通过最小化模型预测函数与该加性表示之间的平方误差，来导出黑盒模型的低阶加性表示。midr通过构建具有高级分析功能的全局代理模型，从而实现从黑盒模型中学习。在回顾了相关工作和MID的理论基础之后，我们演示了该软件包的用法并讨论了它的一些关键特性。", "summary": "本文介绍了R软件包midr，它实现了最大解释分解（MID），这是一种函数分解方法，通过最小化平方误差来构建黑盒模型的低阶加性表示，从而有效地创建了一个全局代理模型。midr旨在为黑盒模型提供必要的解释性，这对于它们在需要可解释性的领域中的应用至关重要。文章还演示了该软件包的用法和关键特性。", "keywords": "可解释机器学习, 可解释人工智能, 黑盒模型, 最大解释分解, R软件包", "comments": "该论文的创新之处在于引入了midr R软件包和MID方法，为解释黑盒模型提供了一种新的函数分解方法。这对于在关键应用中增加复杂AI模型的信任和采用至关重要，尤其是在可解释性至关重要的领域。专注于构建具有高级分析功能的全局代理模型是其主要优势。"}}
{"id": "2506.08362", "title": "Solving Convex-Concave Problems with $\\tilde{\\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity", "authors": ["Lesi Chen", "Chengchang Liu", "Luo Luo", "Jingzhao Zhang"], "summary": "Previous algorithms can solve convex-concave minimax problems $\\min_{x \\in\n\\mathcal{X}} \\max_{y \\in \\mathcal{Y}} f(x,y)$ with\n$\\mathcal{O}(\\epsilon^{-2/3})$ second-order oracle calls using Newton-type\nmethods. This result has been speculated to be optimal because the upper bound\nis achieved by a natural generalization of the optimal first-order method. In\nthis work, we show an improved upper bound of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$ by generalizing the optimal second-order\nmethod for convex optimization to solve the convex-concave minimax problem. We\nfurther apply a similar technique to lazy Hessian algorithms and show that our\nproposed algorithm can also be seen as a second-order ``Catalyst'' framework\n(Lin et al., JMLR 2018) that could accelerate any globally convergent\nalgorithms for solving minimax problems.", "comment": "COLT 2025", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.08362v1", "AI": {"title_translation": "求解具有 $\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$ 二阶预言机复杂度的凸凹问题", "tldr": "提出一种新的算法，将求解凸凹极小极大问题的二阶预言机复杂度从 $\\mathcal{O}(\\epsilon^{-2/3})$ 改进到 $\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$。", "motivation": "以前的算法使用牛顿型方法求解凸凹极小极大问题，二阶预言机调用次数为 $\\mathcal{O}(\\epsilon^{-2/3})$，并且该结果曾被推测为最优。", "method": "通过将凸优化中的最优二阶方法推广到求解凸凹极小极大问题，并进一步将类似技术应用于惰性Hessian算法，提出了一种新的算法。该算法也可被视为一个二阶“Catalyst”框架。", "result": "实现了 $\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$ 的改进上界。", "conclusion": "所提出的算法可以作为二阶“Catalyst”框架，加速任何求解极小极大问题的全局收敛算法。", "translation": "以前的算法可以使用牛顿型方法以 $\\mathcal{O}(\\epsilon^{-2/3})$ 的二阶预言机调用次数来解决凸凹极小极大问题 $\\min_{x \\in \\mathcal{X}} \\max_{y \\in \\mathcal{Y}} f(x,y)$。这个结果曾被推测为最优，因为其上界是通过最优一阶方法的自然推广实现的。在这项工作中，我们通过将凸优化中的最优二阶方法推广到解决凸凹极小极大问题，展示了一个改进的 $\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$ 上界。我们进一步将类似的技术应用于惰性Hessian算法，并表明我们提出的算法也可以被视为一个二阶“Catalyst”框架（Lin et al., JMLR 2018），可以加速任何全局收敛的极小极大问题求解算法。", "summary": "本文提出了一种新的算法，用于求解凸凹极小极大问题，将其二阶预言机复杂度从先前算法的 $\\mathcal{O}(\\epsilon^{-2/3})$ 改进到 $\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$。该方法通过推广凸优化中的最优二阶方法实现，并可作为加速其他全局收敛极小极大问题算法的二阶“Catalyst”框架。", "keywords": "凸凹问题, 极小极大问题, 二阶复杂度, 优化算法, Catalyst框架", "comments": "这项工作通过突破之前被认为是最优的复杂度上界，为凸凹极小极大问题的求解带来了显著进步。其创新点在于将最优二阶方法推广到更复杂的凸凹设置，并将其与“Catalyst”框架联系起来，这表明了其在加速现有算法方面的潜力。"}}
{"id": "2506.08381", "title": "TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses", "authors": ["He Yang", "Fei Ren", "Hai-Sui Yu", "Xueyu Geng", "Pei-Zhi Zhuang"], "summary": "Accuracy and efficiency of the conventional physics-informed neural network\n(PINN) need to be improved before it can be a competitive alternative for soil\nconsolidation analyses. This paper aims to overcome these limitations by\nproposing a highly accurate and efficient physics-informed machine learning\n(PIML) approach, termed time-stepping physics-informed extreme learning machine\n(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into\nnumerous time intervals, which helps overcome the limitation of PIELM in\nsolving differential equations with sharp gradients. To accelerate network\ntraining, the solution is approximated by a single-layer feedforward extreme\nlearning machine (ELM), rather than using a fully connected neural network in\nPINN. The input layer weights of the ELM network are generated randomly and\nfixed during the training process. Subsequently, the output layer weights are\ndirectly computed by solving a system of linear equations, which significantly\nenhances the training efficiency compared to the time-consuming gradient\ndescent method in PINN. Finally, the superior performance of TS-PIELM is\ndemonstrated by solving three typical Terzaghi consolidation problems. Compared\nto PINN, results show that the computational efficiency and accuracy of the\nnovel TS-PIELM framework are improved by more than 1000 times and 100 times for\none-dimensional cases, respectively. This paper provides compelling evidence\nthat PIML can be a powerful tool for computational geotechnics.", "comment": null, "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.08381v1", "AI": {"title_translation": "TS-PIELM：时步物理信息极限学习机促进土体固结分析", "tldr": "TS-PIELM通过结合时步方法和极限学习机，显著提高了土体固结分析的计算效率和准确性，解决了传统物理信息神经网络（PINN）的局限性。", "motivation": "传统物理信息神经网络（PINN）在土体固结分析中存在准确性和效率不足的问题，使其难以成为一个有竞争力的替代方案，因此需要改进。", "method": "本文提出了一种名为时步物理信息极限学习机（TS-PIELM）的方法。该框架将固结过程划分为多个时间间隔，以处理具有陡峭梯度的微分方程。为加速训练，模型采用单层前馈极限学习机（ELM）进行近似，其输入层权重随机生成并固定，输出层权重通过求解线性方程组直接计算，从而避免了PINN中耗时的梯度下降法。", "result": "与PINN相比，TS-PIELM在处理一维土体固结问题时，计算效率提高了1000倍以上，准确性提高了100倍以上。该方法在解决三个典型的太沙基固结问题中表现出卓越的性能。", "conclusion": "TS-PIELM作为一种物理信息机器学习（PIML）方法，被证明是计算岩土工程领域的强大工具。", "translation": "传统物理信息神经网络（PINN）的准确性和效率需要提高，才能成为土体固结分析的具有竞争力的替代方案。本文旨在通过提出一种高精度、高效率的物理信息机器学习（PIML）方法，即时步物理信息极限学习机（TS-PIELM），来克服这些局限性。在TS-PIELM框架中，固结过程被划分为许多时间间隔，这有助于克服PIELM在求解具有陡峭梯度的微分方程方面的局限性。为了加速网络训练，解决方案由单层前馈极限学习机（ELM）近似，而不是像PINN那样使用全连接神经网络。ELM网络的输入层权重是随机生成并在训练过程中固定的。随后，通过求解线性方程组直接计算输出层权重，这与PINN中耗时的梯度下降方法相比，显著提高了训练效率。最后，通过解决三个典型的太沙基固结问题，证明了TS-PIELM的优越性能。与PINN相比，结果表明，新型TS-PIELM框架在处理一维情况时，计算效率和准确性分别提高了1000倍以上和100倍以上。本文提供了令人信服的证据，表明PIML可以成为计算岩土工程的强大工具。", "summary": "本文提出了一种名为TS-PIELM的新型物理信息机器学习方法，旨在解决传统物理信息神经网络（PINN）在土体固结分析中存在的准确性和效率问题。TS-PIELM框架将固结过程划分为多个时间间隔，并通过使用极限学习机（ELM）近似解来加速网络训练，其中输出层权重通过求解线性方程组直接计算，避免了耗时的梯度下降法。实验结果表明，与PINN相比，TS-PIELM在处理一维太沙基固结问题时，计算效率提高了1000倍以上，准确性提高了100倍以上，证明了PIML在计算岩土工程领域的巨大潜力。", "keywords": "土体固结, 物理信息机器学习, 极限学习机, 时步法, 计算岩土工程", "comments": "该论文的创新之处在于将时步方法与极限学习机（ELM）相结合，用于物理信息建模。这种方法显著提高了效率和准确性，尤其对于土体固结这类具有陡峭梯度的问题，克服了传统PINN的局限性。这使得物理信息机器学习在工程应用中更具实用性。"}}
{"id": "2506.08827", "title": "The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation", "authors": ["Francisco Vargas", "Alejandro González Coene", "Gaston Escalante", "Exequiel Lobón", "Manuel Pulido"], "summary": "The extraction of information about traffic accidents from legal documents is\ncrucial for quantifying insurance company costs. Extracting entities such as\npercentages of physical and/or psychological disability and the involved\ncompensation amounts is a challenging process, even for experts, due to the\nsubtle arguments and reasoning in the court decision. A two-step procedure is\nproposed: first, segmenting the document identifying the most relevant\nsegments, and then extracting the entities. For text segmentation, two\nmethodologies are compared: a classic method based on regular expressions and a\nsecond approach that divides the document into blocks of n-tokens, which are\nthen vectorized using multilingual models for semantic searches\n(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models\n(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to\nthe selected segments for entity extraction. For the LLaMA models, fine-tuning\nis performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a\nsignificant number of hallucinations in extractions which are an important\ncontention point for named entity extraction. This work shows that these\nhallucinations are substantially reduced after finetuning the model. The\nperformance of the methodology based on segment vectorization and subsequent\nuse of LLMs significantly surpasses the classic method which achieves an\naccuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning\nachieves the highest accuracy 79.4%, surpassing its base version 61.7%.\nNotably, the base LLaMA-3 8B model already performs comparably to the finetuned\nLLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model\ndevelopment. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08827v1", "AI": {"title_translation": "LLaMA微调对法律文件中命名实体提取中幻觉的影响", "tldr": "研究表明，对LLaMA模型进行微调可以显著减少在法律文档命名实体提取中的幻觉，并提高准确性，优于传统方法和未微调的LLaMA模型。", "motivation": "从法律文件中提取交通事故相关信息（如残疾百分比和赔偿金额）对量化保险公司成本至关重要，但由于法庭判决中的细微论证和推理，即使对专家来说也是一个挑战。LLaMA模型在提取过程中存在大量幻觉。", "method": "提出一个两步程序：首先，通过比较基于正则表达式的经典方法和使用多语言模型（text-embedding-ada-002/MiniLM-L12-v2）对n-token块进行向量化以进行语义搜索的方法来分割文档，识别最相关的片段。然后，将大型语言模型（LLaMA-2 7b, 70b, LLaMA-3 8b和GPT-4 Turbo）应用于选定的片段进行实体提取，其中LLaMA模型使用LoRA进行微调。", "result": "微调后，LLaMA模型中的幻觉显著减少。基于片段向量化和LLM的方法显著优于经典方法（准确率为39.5%）。在开源模型中，经过微调的LLaMA-2 70B实现了最高准确率（79.4%），超过其基础版本（61.7%）。基础LLaMA-3 8B模型（76.6%）表现已与微调后的LLaMA-2 70B模型相当。GPT-4 Turbo实现了最高准确率（86.1%）。", "conclusion": "微调能够显著减少LLaMA模型在法律文档命名实体提取中的幻觉，并大幅提高其性能。基于片段向量化和LLM的提取方法优于传统方法，且新型基础模型（如LLaMA-3 8B）的性能进步迅速。", "translation": "从法律文件中提取交通事故信息对于量化保险公司成本至关重要。提取物理和/或心理残疾百分比以及相关赔偿金额等实体是一个具有挑战性的过程，即使对于专家来说也是如此，因为法庭判决中存在细微的论证和推理。本文提出一个两步程序：首先，分割文档以识别最相关的片段；然后，提取实体。对于文本分割，比较了两种方法：一种是基于正则表达式的经典方法，另一种是将文档分成n-token块，然后使用多语言模型（text-embedding-ada-002/MiniLM-L12-v2）进行向量化以进行语义搜索的方法。随后，将大型语言模型（LLaMA-2 7b、70b、LLaMA-3 8b和GPT-4 Turbo）通过提示应用于选定的片段进行实体提取。对于LLaMA模型，使用LoRA进行微调。LLaMA-2 7b即使在零温度下，在提取中也显示出大量的幻觉，这是命名实体提取的一个重要争议点。这项工作表明，在对模型进行微调后，这些幻觉显著减少。基于片段向量化和随后使用LLM的方法的性能显著超过了经典方法，后者实现了39.5%的准确率。在开源模型中，经过微调的LLaMA-2 70B实现了最高的准确率79.4%，超过其基础版本61.7%。值得注意的是，基础LLaMA-3 8B模型已经表现出与经过微调的LLaMA-2 70B模型相当的性能，达到了76.6%，这突显了模型开发的快速进展。同时，GPT-4 Turbo实现了86.1%的最高准确率。", "summary": "本文研究了LLaMA模型微调对法律文档中命名实体提取中幻觉的影响。研究提出了一种两步提取方法，包括文档分割和基于LLM的实体提取，并比较了多种LLaMA模型（包括微调版本）与GPT-4 Turbo的性能。结果显示，微调显著减少了LLaMA模型的幻觉，并大幅提高了提取准确性，优于传统方法。特别是，微调后的LLaMA-2 70B表现出色，而基础LLaMA-3 8B也展现出与高性能微调模型相当的能力，同时GPT-4 Turbo取得了最佳性能。", "keywords": "命名实体提取, LLaMA, 微调, 幻觉, 法律文档", "comments": "这项研究通过实证证明了LLaMA模型在特定领域（法律文档）进行微调对减少幻觉和提高命名实体提取准确性的重要性。它提供了一个实用的两步流程，结合了语义分割和LLM提取。论文还突出了开源模型（LLaMA-3）的快速进步，其基础版本已能与经过微调的旧模型相媲美，这对于资源有限的研究者和开发者而言具有重要意义。同时，也确认了闭源模型（GPT-4 Turbo）在当前阶段的领先地位。"}}
{"id": "2506.08423", "title": "Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy", "authors": ["Utkarsh Pratiush", "Austin Houston", "Kamyar Barakati", "Aditya Raghavan", "Dasol Yoon", "Harikrishnan KP", "Zhaslan Baraissov", "Desheng Ma", "Samuel S. Welborn", "Mikolaj Jakowski", "Shawn-Patrick Barhorst", "Alexander J. Pattison", "Panayotis Manganaris", "Sita Sirisha Madugula", "Sai Venkata Gayathri Ayyagari", "Vishal Kennedy", "Ralph Bulanadi", "Michelle Wang", "Kieran J. Pang", "Ian Addison-Smith", "Willy Menacho", "Horacio V. Guzman", "Alexander Kiefer", "Nicholas Furth", "Nikola L. Kolev", "Mikhail Petrov", "Viktoriia Liu", "Sergey Ilyev", "Srikar Rairao", "Tommaso Rodani", "Ivan Pinto-Huguet", "Xuli Chen", "Josep Cruañes", "Marta Torrens", "Jovan Pomar", "Fanzhi Su", "Pawan Vedanti", "Zhiheng Lyu", "Xingzhi Wang", "Lehan Yao", "Amir Taqieddin", "Forrest Laskowski", "Xiangyu Yin", "Yu-Tsun Shao", "Benjamin Fein-Ashley", "Yi Jiang", "Vineet Kumar", "Himanshu Mishra", "Yogesh Paul", "Adib Bazgir", "Rama chandra Praneeth Madugula", "Yuwen Zhang", "Pravan Omprakash", "Jian Huang", "Eric Montufar-Morales", "Vivek Chawla", "Harshit Sethi", "Jie Huang", "Lauri Kurki", "Grace Guinan", "Addison Salvador", "Arman Ter-Petrosyan", "Madeline Van Winkle", "Steven R. Spurgeon", "Ganesh Narasimha", "Zijie Wu", "Richard Liu", "Yongtao Liu", "Boris Slautin", "Andrew R Lupini", "Rama Vasudevan", "Gerd Duscher", "Sergei V. Kalinin"], "summary": "Microscopy is a primary source of information on materials structure and\nfunctionality at nanometer and atomic scales. The data generated is often\nwell-structured, enriched with metadata and sample histories, though not always\nconsistent in detail or format. The adoption of Data Management Plans (DMPs) by\nmajor funding agencies promotes preservation and access. However, deriving\ninsights remains difficult due to the lack of standardized code ecosystems,\nbenchmarks, and integration strategies. As a result, data usage is inefficient\nand analysis time is extensive. In addition to post-acquisition analysis, new\nAPIs from major microscope manufacturers enable real-time, ML-based analytics\nfor automated decision-making and ML-agent-controlled microscope operation.\nYet, a gap remains between the ML and microscopy communities, limiting the\nimpact of these methods on physics, materials discovery, and optimization.\nHackathons help bridge this divide by fostering collaboration between ML\nresearchers and microscopy experts. They encourage the development of novel\nsolutions that apply ML to microscopy, while preparing a future workforce for\ninstrumentation, materials science, and applied ML. This hackathon produced\nbenchmark datasets and digital twins of microscopes to support community growth\nand standardized workflows. All related code is available at GitHub:\nhttps://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.08423v1", "AI": {"title_translation": "Mic-hackathon 2024：机器学习在电子和扫描探针显微镜中的应用黑客马拉松", "tldr": "Mic-hackathon 2024旨在通过举办黑客马拉松，弥合机器学习和显微镜领域之间的鸿沟，以提高显微镜数据分析效率并促进ML在显微镜操作中的应用，并生成了基准数据集和显微镜数字孪生。", "motivation": "尽管显微镜数据丰富，但由于缺乏标准化代码生态系统、基准和集成策略，导致数据洞察提取困难，数据使用效率低下，分析时间长。此外，机器学习和显微镜社区之间存在鸿沟，限制了ML方法在物理、材料发现和优化中的影响。", "method": "通过举办“Mic-hackathon 2024”黑客马拉松，促进机器学习研究人员与显微镜专家之间的协作，鼓励开发将机器学习应用于显微镜的新颖解决方案。", "result": "本次黑客马拉松产生了基准数据集和显微镜数字孪生，以支持社区发展和标准化工作流程。", "conclusion": "黑客马拉松是弥合机器学习和显微镜社区之间鸿沟的有效方式，它能促进新颖解决方案的开发，培养未来人才，并最终推动显微镜数据分析和操作的标准化与效率提升。", "translation": "显微镜是纳米和原子尺度材料结构和功能信息的主要来源。生成的数据通常结构良好，富含元数据和样品历史记录，尽管细节或格式并非总是保持一致。主要资助机构采纳数据管理计划 (DMP) 促进了数据保存和访问。然而，由于缺乏标准化的代码生态系统、基准和集成策略，从中获取洞察仍然困难。因此，数据使用效率低下，分析时间冗长。除了后采集分析之外，主要显微镜制造商提供的新 API 使得实时、基于机器学习的分析能够实现自动化决策和机器学习智能体控制的显微镜操作。然而，机器学习和显微镜社区之间仍然存在鸿沟，限制了这些方法对物理学、材料发现和优化的影响。黑客马拉松通过促进机器学习研究人员和显微镜专家之间的协作来帮助弥合这一鸿沟。它们鼓励开发将机器学习应用于显微镜的新颖解决方案，同时为仪器、材料科学和应用机器学习培养未来的劳动力。本次黑客马拉松产生了基准数据集和显微镜数字孪生，以支持社区发展和标准化工作流程。所有相关代码均可在 GitHub 上获取：https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1。", "summary": "Mic-hackathon 2024旨在解决显微镜数据分析中存在的标准化不足、效率低下以及机器学习与显微镜社区之间的鸿沟问题。通过举办黑客马拉松，该活动成功促进了ML研究人员和显微镜专家之间的协作，鼓励将ML应用于显微镜数据分析和操作，并产出了基准数据集和显微镜数字孪生，以期推动社区发展和标准化工作流程，培养未来人才。", "keywords": "机器学习, 显微镜, 黑客马拉松, 数据分析, 数字孪生", "comments": "该黑客马拉松通过促进跨学科合作，有效解决了显微镜数据分析中机器学习应用面临的实际挑战。其产出的基准数据集和数字孪生对于推动显微镜数据处理和机器学习自动化具有重要意义，有助于加速材料科学和物理领域的研究进展。"}}
{"id": "2506.08428", "title": "Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings", "authors": ["Evan Markou", "Thalaiyasingam Ajanthan", "Stephen Gould"], "summary": "Many high-dimensional optimisation problems exhibit rich geometric structures\nin their set of minimisers, often forming smooth manifolds due to\nover-parametrisation or symmetries. When this structure is known, at least\nlocally, it can be exploited through reduction mappings that reparametrise part\nof the parameter space to lie on the solution manifold. These reductions\nnaturally arise from inner optimisation problems and effectively remove\nredundant directions, yielding a lower-dimensional objective. In this work, we\nintroduce a general framework to understand how such reductions influence the\noptimisation landscape. We show that well-designed reduction mappings improve\ncurvature properties of the objective, leading to better-conditioned problems\nand theoretically faster convergence for gradient-based methods. Our analysis\nunifies a range of scenarios where structural information at optimality is\nleveraged to accelerate convergence, offering a principled explanation for the\nempirical gains observed in such optimisation algorithms.", "comment": "37 pages, 5 figures", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.08428v1", "AI": {"title_translation": "通过降维映射实现非凸优化的更快速收敛", "tldr": "本文引入了一个通用框架，解释了降维映射如何改善优化问题的曲率特性，从而使基于梯度的算法收敛更快，并为利用最优性结构加速收敛的经验增益提供了理论解释。", "motivation": "许多高维优化问题在最小化器集合中表现出丰富的几何结构（如平滑流形），当这种结构已知时，可以通过降维映射来利用它，从而移除冗余方向，降低目标维度。本文旨在理解这些降维映射如何影响优化格局，并利用它们来加速收敛。", "method": "本文引入了一个通用框架来理解降维映射如何影响优化格局。研究表明，精心设计的降维映射可以改善目标函数的曲率特性，从而产生条件更好的问题。", "result": "结果表明，降维映射能改善目标函数的曲率特性，使问题条件更好，并理论上加速了基于梯度方法的收敛。", "conclusion": "本研究的分析统一了利用最优性结构加速收敛的各种场景，为在优化算法中观察到的经验增益提供了原理性解释。", "translation": "许多高维优化问题在它们的最小化器集合中表现出丰富的几何结构，通常由于过参数化或对称性而形成平滑流形。当这种结构已知时（至少在局部），可以通过降维映射来利用它，这些映射重新参数化了部分参数空间以使其位于解流形上。这些降维自然地产生于内部优化问题，并有效地移除了冗余方向，从而产生一个更低维的目标。在这项工作中，我们引入了一个通用框架来理解这种降维如何影响优化格局。我们表明，精心设计的降维映射改善了目标函数的曲率特性，从而产生条件更好的问题，并理论上加速了基于梯度方法的收敛。我们的分析统一了利用最优性结构加速收敛的各种场景，为在这些优化算法中观察到的经验增益提供了原理性解释。", "summary": "本文提出了一个通用框架，用于理解降维映射在高维非凸优化中的作用。研究发现，利用问题解的几何结构设计的降维映射能够改善目标函数的曲率特性，从而使优化问题条件更好，并显著加速基于梯度的算法的收敛速度。该工作为利用结构信息加速优化提供了统一的理论解释。", "keywords": "非凸优化, 收敛速率, 降维映射, 几何结构, 曲率特性", "comments": "本文的创新之处在于提出了一个通用框架来理论化降维映射如何改善非凸优化问题的条件性和收敛性。它为之前经验上观察到的优化加速提供了原理性解释，对于理解和设计更高效的优化算法具有重要意义。"}}
{"id": "2506.08436", "title": "Olica: Efficient Structured Pruning of Large Language Models without Retraining", "authors": ["Jiujun He", "Huazhen Lin"], "summary": "Most existing structured pruning methods for Large Language Models (LLMs)\nrequire substantial computational and data resources for retraining to\nreestablish the corrupted correlations, making them prohibitively expensive. To\naddress this, we propose a pruning framework for LLMs called Orthogonal\ndecomposition and Linear Calibration (Olica), which eliminates the need for\nretraining. A key observation is that the multi-head attention (MHA) layer\ndepends on two types of matrix products. By treating these matrix products as\nunified entities and applying principal component analysis (PCA), we extract\nthe most important information to compress LLMs without sacrificing accuracy or\ndisrupting their original structure. Consequently, retraining becomes\nunnecessary. A fast decomposition method is devised, reducing the complexity of\nPCA by a factor of the square of the number of attention heads. Additionally,\nto mitigate error accumulation problem caused by pruning the feed-forward\nnetwork (FFN) layer, we introduce a linear calibration method to reconstruct\nthe residual errors of pruned layers using low-rank matrices. By leveraging\nsingular value decomposition (SVD) on the solution of the least-squares\nproblem, these matrices are obtained without requiring retraining. Extensive\nexperiments show that the proposed Olica is efficient in terms of data usage,\nGPU memory, and running time, while delivering superior performance across\nmultiple benchmarks.", "comment": "Accepted to ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08436v1", "AI": {"title_translation": "Olica：无需再训练的大语言模型高效结构化剪枝", "tldr": "Olica是一种无需再训练的大语言模型高效结构化剪枝框架，通过正交分解和线性校准实现，显著降低了计算和数据资源需求。", "motivation": "大多数现有的大语言模型（LLM）结构化剪枝方法需要大量的计算和数据资源进行再训练以恢复损坏的相关性，这使得它们成本过高。", "method": "本文提出了一种名为Olica的LLM剪枝框架，该框架无需再训练。它通过将多头注意力（MHA）层的矩阵乘积视为统一实体并应用主成分分析（PCA）来提取最重要的信息以压缩LLM，同时不牺牲准确性或破坏原始结构。此外，还设计了一种快速分解方法，将PCA的复杂度降低了注意力头数量的平方倍。为了减轻剪枝前馈网络（FFN）层引起的误差累积问题，Olica引入了一种线性校准方法，使用低秩矩阵重建剪枝层的残差误差，这些矩阵通过对最小二乘问题的解进行奇异值分解（SVD）获得，无需再训练。", "result": "大量的实验表明，所提出的Olica在数据使用、GPU内存和运行时间方面表现高效，同时在多个基准测试中提供卓越的性能。", "conclusion": "Olica是一种无需再训练的、高效且高性能的LLM结构化剪枝框架，有效解决了现有方法计算成本高昂的问题。", "translation": "大多数现有的大语言模型（LLM）结构化剪枝方法需要大量的计算和数据资源进行再训练以恢复损坏的相关性，这使得它们成本过高。为了解决这个问题，我们提出了一种名为正交分解和线性校准（Olica）的LLM剪枝框架，该框架无需再训练。一个关键的观察是多头注意力（MHA）层依赖于两种类型的矩阵乘积。通过将这些矩阵乘积视为统一实体并应用主成分分析（PCA），我们提取了最重要的信息来压缩LLM，而无需牺牲准确性或破坏其原始结构。因此，再训练变得不必要。我们设计了一种快速分解方法，将PCA的复杂度降低了注意力头数量的平方倍。此外，为了减轻剪枝前馈网络（FFN）层引起的误差累积问题，我们引入了一种线性校准方法，使用低秩矩阵重建剪枝层的残差误差。通过对最小二乘问题的解进行奇异值分解（SVD），无需再训练即可获得这些矩阵。大量的实验表明，所提出的Olica在数据使用、GPU内存和运行时间方面表现高效，同时在多个基准测试中提供卓越的性能。", "summary": "本文提出了一种名为Olica的大语言模型结构化剪枝框架，其核心优势在于无需进行耗时的再训练过程。Olica通过对多头注意力（MHA）层应用基于PCA的正交分解来提取关键信息进行压缩，并引入了一种快速分解方法以提高效率。同时，针对前馈网络（FFN）层的剪枝，Olica采用线性校准方法，利用低秩矩阵和SVD来重建残差误差，从而有效避免了误差累积问题。实验结果证明，Olica在资源效率和性能方面均优于现有方法。", "keywords": "大语言模型, 结构化剪枝, 无需再训练, PCA, 线性校准", "comments": "Olica的创新之处在于其无需再训练的剪枝范式，这显著降低了LLM剪枝的计算成本和时间。通过结合PCA进行MHA层压缩和线性校准解决FFN层误差累积，该方法提供了一个全面且高效的解决方案，对于资源受限的LLM部署具有重要意义。其对PCA复杂度的优化也体现了方法设计的精巧性。"}}
{"id": "2506.08448", "title": "Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions", "authors": ["Hyakka Nakada", "Shu Tanaka"], "summary": "Quantum Annealing (QA) can efficiently solve combinatorial optimization\nproblems whose objective functions are represented by Quadratic Unconstrained\nBinary Optimization (QUBO) formulations. For broader applicability of QA,\nquadratization methods are used to transform higher-order problems into QUBOs.\nHowever, quadratization methods for complex problems involving Machine Learning\n(ML) remain largely unknown. In these problems, strong nonlinearity and dense\ninteractions prevent conventional methods from being applied. Therefore, we\nmodel target functions by the sum of rectified linear unit bases, which not\nonly have the ability of universal approximation, but also have an equivalent\nquadratic-polynomial representation. In this study, the proof of concept is\nverified both numerically and analytically. In addition, by combining QA with\nthe proposed quadratization, we design a new black-box optimization scheme, in\nwhich ML surrogate regressors are inputted to QA after the quadratization\nprocess.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.08448v1", "AI": {"title_translation": "高阶和密集交互的二次无约束二元优化形式的系统高效构建", "tldr": "本文提出一种系统高效的方法，通过使用ReLU基函数将高阶和密集交互问题转化为QUBO形式，从而使量子退火能解决复杂的机器学习优化问题。", "motivation": "量子退火（QA）能高效解决QUBO形式的组合优化问题，但对于涉及机器学习的复杂问题，由于强非线性和密集交互，传统的二次化方法无法应用，限制了QA的广泛适用性。", "method": "作者通过使用整流线性单元（ReLU）基函数的总和来建模目标函数，这种方法不仅具有通用逼近能力，还具有等效的二次多项式表示。此外，将QA与所提出的二次化方法结合，设计了一种新的黑盒优化方案，其中ML替代回归器在二次化后输入到QA。", "result": "概念验证得到了数值和分析上的验证。成功设计了一种新的黑盒优化方案。", "conclusion": "通过引入基于ReLU基函数的二次化方法，能够系统高效地将高阶和密集交互问题转化为QUBO形式，从而扩展了量子退火在解决复杂机器学习优化问题上的应用范围。", "translation": "量子退火（QA）可以有效地解决目标函数表示为二次无约束二元优化（QUBO）形式的组合优化问题。为了扩大QA的适用性，二次化方法被用于将高阶问题转换为QUBO。然而，针对涉及机器学习（ML）的复杂问题的二次化方法在很大程度上仍然未知。在这些问题中，强非线性和密集交互阻碍了传统方法的应用。因此，我们通过整流线性单元（ReLU）基函数的总和来建模目标函数，这不仅具有通用逼近能力，而且具有等效的二次多项式表示。在本研究中，概念验证得到了数值和分析上的验证。此外，通过将QA与所提出的二次化方法结合，我们设计了一种新的黑盒优化方案，其中ML替代回归器在二次化过程后输入到QA。", "summary": "本文提出一种系统高效的方法，用于构建高阶和密集交互的二次无约束二元优化（QUBO）形式，以扩展量子退火（QA）在解决复杂组合优化问题中的应用。针对传统二次化方法无法处理机器学习中强非线性和密集交互的挑战，作者通过使用具有通用逼近能力和等效二次多项式表示的整流线性单元（ReLU）基函数来建模目标函数。研究通过数值和分析验证了概念，并结合QA与所提出的二次化方法，设计了一种新的黑盒优化方案。", "keywords": "量子退火, 二次无约束二元优化, 二次化, 整流线性单元, 机器学习", "comments": "本文的创新点在于提出了基于ReLU基函数的二次化方法，解决了传统方法难以处理机器学习中高阶和密集交互问题的痛点，显著扩展了量子退火的应用范围。这种方法提供了一种将复杂非线性问题转化为QA可解QUBO形式的通用途径。"}}
{"id": "2506.08897", "title": "PlantBert: An Open Source Language Model for Plant Science", "authors": ["Hiba Khey", "Amine Lakhder", "Salma Rouichi", "Imane El Ghabi", "Kamal Hejjaoui", "Younes En-nahli", "Fahd Kalloubi", "Moez Amri"], "summary": "The rapid advancement of transformer-based language models has catalyzed\nbreakthroughs in biomedical and clinical natural language processing; however,\nplant science remains markedly underserved by such domain-adapted tools. In\nthis work, we present PlantBert, a high-performance, open-source language model\nspecifically tailored for extracting structured knowledge from plant\nstress-response literature. Built upon the DeBERTa architecture-known for its\ndisentangled attention and robust contextual encoding-PlantBert is fine-tuned\non a meticulously curated corpus of expert-annotated abstracts, with a primary\nfocus on lentil (Lens culinaris) responses to diverse abiotic and biotic\nstressors. Our methodology combines transformer-based modeling with\nrule-enhanced linguistic post-processing and ontology-grounded entity\nnormalization, enabling PlantBert to capture biologically meaningful\nrelationships with precision and semantic fidelity. The underlying corpus is\nannotated using a hierarchical schema aligned with the Crop Ontology,\nencompassing molecular, physiological, biochemical, and agronomic dimensions of\nplant adaptation. PlantBert exhibits strong generalization capabilities across\nentity types and demonstrates the feasibility of robust domain adaptation in\nlow-resource scientific fields. By providing a scalable and reproducible\nframework for high-resolution entity recognition, PlantBert bridges a critical\ngap in agricultural NLP and paves the way for intelligent, data-driven systems\nin plant genomics, phenomics, and agronomic knowledge discovery. Our model is\npublicly released to promote transparency and accelerate cross-disciplinary\ninnovation in computational plant science.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08897v1", "AI": {"title_translation": "PlantBert：一个用于植物科学的开源语言模型", "tldr": "PlantBert是一个基于DeBERTa架构的开源语言模型，专门为植物科学领域定制，用于从植物逆境响应文献中提取结构化知识，并通过在低资源科学领域的强大泛化能力弥补了农业NLP的关键空白。", "motivation": "当前的Transformer语言模型在生物医学和临床自然语言处理领域取得了突破，但植物科学领域仍缺乏此类领域适应性工具，因此需要一个专门的模型来从植物科学文献中提取结构化知识。", "method": "PlantBert基于DeBERTa架构构建，并在一个精心策划的专家标注摘要语料库上进行了微调，主要关注扁豆对非生物和生物胁迫的响应。该方法结合了Transformer模型、规则增强的语言后处理和本体论实体规范化，以精确捕获生物学上有意义的关系。底层语料库使用与作物本体论对齐的分层模式进行标注。", "result": "PlantBert在不同实体类型上表现出强大的泛化能力，并证明了在低资源科学领域进行鲁棒领域适应的可行性。它为高分辨率实体识别提供了一个可扩展且可重复的框架。", "conclusion": "PlantBert弥补了农业NLP中的一个关键空白，并为植物基因组学、表型组学和农学知识发现中的智能、数据驱动系统铺平了道路。该模型已公开发布，以促进计算植物科学领域的透明度和跨学科创新。", "translation": "Transformer语言模型的快速发展推动了生物医学和临床自然语言处理领域的突破；然而，植物科学领域仍明显缺乏此类领域适应性工具。在这项工作中，我们提出了PlantBert，一个高性能、开源的语言模型，专门用于从植物逆境响应文献中提取结构化知识。PlantBert基于DeBERTa架构（以其解耦注意力和强大的上下文编码而闻名），在一个精心策划的专家标注摘要语料库上进行了微调，主要关注扁豆（Lens culinaris）对各种非生物和生物胁迫的响应。我们的方法将基于Transformer的建模与规则增强的语言后处理和本体论实体规范化相结合，使PlantBert能够精确且语义忠实地捕获生物学上有意义的关系。底层语料库使用与作物本体论对齐的分层模式进行标注，涵盖了植物适应的分子、生理、生化和农学维度。PlantBert在实体类型上表现出强大的泛化能力，并证明了在低资源科学领域进行鲁棒领域适应的可行性。通过为高分辨率实体识别提供一个可扩展且可重复的框架，PlantBert弥补了农业NLP中的一个关键空白，并为植物基因组学、表型组学和农学知识发现中的智能、数据驱动系统铺平了道路。我们的模型已公开发布，以促进计算植物科学领域的透明度和加速跨学科创新。", "summary": "PlantBert是一个专门为植物科学领域设计的开源语言模型，旨在解决该领域缺乏领域适应性工具的问题。它基于DeBERTa架构，通过在专家标注的植物逆境响应文献语料库上进行微调，并结合规则增强的语言后处理和本体论实体规范化，实现了从文献中精确提取结构化知识的能力。PlantBert在低资源科学领域展现出强大的泛化能力，并为农业NLP中的高分辨率实体识别提供了可扩展的框架，从而推动植物基因组学、表型组学和农学知识发现。", "keywords": "PlantBert, 语言模型, 植物科学, 自然语言处理, 开源", "comments": "PlantBert的创新性在于其在低资源科学领域（如植物科学）成功实现了领域适应性语言模型，填补了农业NLP的空白。其开源性质促进了透明度和跨学科合作，对于推动计算植物科学领域的数据驱动研究具有重要意义。该模型结合了先进的Transformer架构与领域特定的标注和后处理，使其能够精确捕捉生物学关系。"}}
{"id": "2506.08917", "title": "Quantum Adiabatic Generation of Human-Like Passwords", "authors": ["Sascha Mücke", "Raoul Heese", "Thore Gerlach", "David Biesner", "Loong Kuan Lee", "Nico Piatkowski"], "summary": "Generative Artificial Intelligence (GenAI) for Natural Language Processing\n(NLP) is the predominant AI technology to date. An important perspective for\nQuantum Computing (QC) is the question whether QC has the potential to reduce\nthe vast resource requirements for training and operating GenAI models. While\nlarge-scale generative NLP tasks are currently out of reach for practical\nquantum computers, the generation of short semantic structures such as\npasswords is not. Generating passwords that mimic real user behavior has many\napplications, for example to test an authentication system against realistic\nthreat models. Classical password generation via deep learning have recently\nbeen investigated with significant progress in their ability to generate novel,\nrealistic password candidates. In the present work we investigate the utility\nof adiabatic quantum computers for this task. More precisely, we study\ndifferent encodings of token strings and propose novel approaches based on the\nQuadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum\nIndependent Set (UD-MIS) problems. Our approach allows us to estimate the token\ndistribution from data and adiabatically prepare a quantum state from which we\neventually sample the generated passwords via measurements. Our results show\nthat relatively small samples of 128 passwords, generated on the QuEra Aquila\n256-qubit neutral atom quantum computer, contain human-like passwords such as\n\"Tunas200992\" or \"teedem28iglove\".", "comment": "9 pages, 4 figures", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.08917v1", "AI": {"title_translation": "量子绝热生成类人密码", "tldr": "本文探讨了使用绝热量子计算机生成类人密码的可能性，提出了基于QUBO和UD-MIS的新方法，并在量子计算机上进行了演示。", "motivation": "生成式人工智能（GenAI）对资源需求巨大，而量子计算（QC）可能有助于减少这些需求。尽管大规模生成式NLP任务目前超出实际量子计算机的能力，但生成短语义结构（如密码）则不然。生成模仿真实用户行为的密码对于测试认证系统以对抗现实威胁模型具有重要应用价值。本文旨在探索绝热量子计算机在此任务中的效用。", "method": "作者研究了绝热量子计算机在生成类人密码方面的应用。他们研究了令牌字符串的不同编码方式，并提出了基于二次无约束二元优化（QUBO）和单位圆最大独立集（UD-MIS）问题的新方法。该方法通过从数据中估计令牌分布，然后绝热地准备一个量子态，最终通过测量从中采样生成密码。实验在QuEra Aquila 256量子比特中性原子量子计算机上进行。", "result": "研究结果表明，在QuEra Aquila 256量子比特中性原子量子计算机上生成的128个相对较小的密码样本中，包含有类似“Tunas200992”或“teedem28iglove”的类人密码。", "conclusion": "绝热量子计算机可以有效地生成类人密码，这表明量子计算在特定生成式AI任务中具有潜在作用，即使大规模NLP目前仍遥不可及。本研究的结果证明了其所提出的量子方法的实用性。", "translation": "迄今为止，用于自然语言处理（NLP）的生成式人工智能（GenAI）是主流的AI技术。对于量子计算（QC）来说，一个重要的视角是，QC是否有可能减少训练和运行GenAI模型所需的巨大资源。虽然大规模的生成式NLP任务目前超出了实际量子计算机的能力范围，但生成短语义结构（如密码）则不然。生成模仿真实用户行为的密码有许多应用，例如用于测试认证系统以对抗真实的威胁模型。最近，通过深度学习进行经典密码生成的研究取得了显著进展，在生成新颖、逼真的密码候选方面表现出色。在当前的工作中，我们研究了绝热量子计算机在此任务中的效用。更准确地说，我们研究了令牌字符串的不同编码，并提出了基于二次无约束二元优化（QUBO）和单位圆最大独立集（UD-MIS）问题的新方法。我们的方法允许我们从数据中估计令牌分布，并绝热地准备一个量子态，最终通过测量从中采样生成的密码。我们的结果表明，在QuEra Aquila 256量子比特中性原子量子计算机上生成的128个相对较小的密码样本中，包含有类似“Tunas200992”或“teedem28iglove”的类人密码。", "summary": "本文探讨了将绝热量子计算机应用于生成类人密码的可行性，这是一项对认证系统测试有用的任务。作者提出了基于QUBO和UD-MIS问题的新颖方法来编码令牌字符串。他们证明了通过从数据中估计令牌分布并绝热准备量子态进行采样的途径，可以在一台256量子比特的量子计算机上生成逼真的密码。", "keywords": "量子绝热计算, 密码生成, 生成式AI, QUBO, UD-MIS", "comments": "本文通过将绝热量子计算应用于实际的生成式AI任务（密码生成），展现了创新性，指出了量子计算机在当前限制下可能发挥优势或实现可行性的特定领域。它突显了量子计算在资源密集型AI中的潜力，即使大规模NLP仍遥不可及。文中利用QUBO和UD-MIS进行编码是重要的技术贡献。"}}
{"id": "2506.08920", "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs", "authors": ["Zeyu Leo Liu", "Greg Durrett", "Eunsol Choi"], "summary": "Knowledge editing techniques for large language models (LLMs) can inject\nknowledge that is later reproducible verbatim, but they fall short on\npropagating that knowledge: models cannot answer questions that require\nreasoning with the injected knowledge. We present a hypernetwork-based approach\nfor knowledge propagation, named PropMEND, where we meta-learn how to modify\ngradients of a language modeling loss to encourage injected information to\npropagate. Our approach extends the meta-objective of MEND [29] so that\ngradient updates on knowledge are transformed to enable answering multi-hop\nquestions involving that knowledge. We show improved performance on the\nRippleEdit dataset, showing almost 2x accuracy on challenging multi-hop\nquestions whose answers are not explicitly stated in the injected fact. We\nfurther introduce a new dataset, Controlled RippleEdit, to evaluate the\ngeneralization of our hypernetwork, testing knowledge propagation along\nrelations and entities unseen during hypernetwork training. PropMEND still\noutperforms existing approaches in unseen entity-relation pairs, yet the\nperformance gap decreases substantially, suggesting future work in propagating\nknowledge to a wide range of relations.", "comment": "Under review", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08920v1", "AI": {"title_translation": "PropMEND：用于LLM中知识传播的超网络", "tldr": "PropMEND是一个基于超网络的方法，通过元学习修改梯度来促进LLM中注入知识的传播，解决了现有知识编辑技术在知识推理方面的不足。", "motivation": "现有的大型语言模型（LLMs）知识编辑技术可以注入可复现的知识，但在知识传播方面存在不足，模型无法回答需要对注入知识进行推理的问题。", "method": "本文提出了一种名为PropMEND的基于超网络的方法，该方法元学习如何修改语言建模损失的梯度，以鼓励注入信息进行传播。该方法扩展了MEND的元目标，使得知识上的梯度更新能够回答涉及该知识的多跳问题。", "result": "PropMEND在RippleEdit数据集上表现出改进的性能，在答案未明确说明的多跳问题上准确率几乎提高了2倍。在新的Controlled RippleEdit数据集上，PropMEND在未见过的实体-关系对中仍然优于现有方法，但性能差距显著缩小。", "conclusion": "PropMEND通过超网络实现了LLM中注入知识的有效传播，尤其是在多跳推理问题上表现出色。然而，在将知识传播到更广泛的关系方面仍有待进一步研究。", "translation": "大型语言模型（LLMs）的知识编辑技术可以注入可复现的知识，但它们在传播该知识方面存在不足：模型无法回答需要对注入知识进行推理的问题。我们提出了一种基于超网络的知识传播方法，名为PropMEND，我们元学习如何修改语言建模损失的梯度，以鼓励注入信息进行传播。我们的方法扩展了MEND [29] 的元目标，以便知识上的梯度更新能够回答涉及该知识的多跳问题。我们在RippleEdit数据集上展示了改进的性能，在答案未在注入事实中明确说明的具有挑战性的多跳问题上，准确率几乎提高了2倍。我们进一步引入了一个新数据集Controlled RippleEdit，以评估我们超网络的泛化能力，测试在超网络训练期间未见过的关系和实体上的知识传播。PropMEND在未见过的实体-关系对中仍然优于现有方法，但性能差距显著缩小，这表明未来在将知识传播到更广泛的关系方面的工作。", "summary": "PropMEND是一种利用超网络来解决大型语言模型中注入知识传播不足问题的方法。它通过元学习修改语言模型损失的梯度，以促进知识的传播，从而使模型能够回答需要推理的多跳问题。该方法在RippleEdit数据集上显著提高了多跳问题的准确性，并在新数据集Controlled RippleEdit上展现了对未见实体和关系的泛化能力，尽管在后者上的性能提升有所减小，提示未来研究方向。", "keywords": "知识传播, 大型语言模型, 超网络, 元学习, 知识编辑", "comments": "PropMEND通过引入超网络和元学习来解决LLM知识编辑中的关键挑战——知识传播和推理能力。其创新点在于通过修改梯度来促进知识在模型内部的扩散，而非仅仅是简单的事实注入。在多跳推理上的显著性能提升证明了其有效性。然而，在处理未见过的关系时性能下降，指出了该领域未来泛化能力提升的重要性。"}}
{"id": "2506.08935", "title": "Can A Gamer Train A Mathematical Reasoning Model?", "authors": ["Andrew Shin"], "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08935v1", "AI": {"title_translation": "游戏玩家能训练数学推理模型吗？", "tldr": "本文展示了通过结合强化学习和内存优化技术，单个普通游戏GPU也能训练出性能媲美大型模型的数学推理模型，打破了高性能AI研究需要大规模基础设施的范式。", "motivation": "大型语言模型（LLMs）在数学推理等任务中表现出色，但其开发通常需要高昂的计算资源，即使是最新方法也依赖于高端硬件集群。本文旨在挑战这种范式，证明在资源受限环境下也能训练出强大的数学推理模型。", "method": "通过整合强化学习和内存优化技术，在配备16GB内存的RTX 3080 Ti显卡上训练了一个1.5B参数的数学推理模型。", "result": "在资源受限的环境下，该模型在数学推理基准测试中取得了与参数量是其数倍的模型相当或更优的性能。", "conclusion": "研究结果挑战了最先进的数学推理需要大规模基础设施的范式，从而使更多人能够进行高性能AI研究。", "translation": "尽管大型语言模型（LLMs）在包括数学推理在内的各种任务中取得了卓越的性能，但它们的开发通常需要高昂的计算资源。最近的进展降低了训练有能力的模型的成本，但即使是这些方法也依赖于高端硬件集群。在本文中，我们通过整合强化学习和内存优化技术，证明了单个普通游戏GPU可以训练出强大的数学推理模型。具体来说，我们在拥有16GB内存的RTX 3080 Ti显卡上训练了一个1.5B参数的数学推理模型，该模型在资源受限的环境下，在数学推理基准测试中取得了与参数量是其数倍的模型相当或更优的性能。我们的结果挑战了最先进的数学推理需要大规模基础设施的范式，从而使高性能AI研究得以普及。https://github.com/shinandrew/YouronMath.", "summary": "本文旨在解决大型语言模型在数学推理方面对高昂计算资源的依赖问题。研究人员通过整合强化学习和内存优化技术，成功地在单个普通游戏GPU（RTX 3080 Ti，16GB内存）上训练了一个1.5B参数的数学推理模型。实验结果表明，该模型在资源受限的环境下，在数学推理基准测试中取得了与参数量是其数倍的模型相当或更优的性能，从而挑战了高性能AI研究需要大规模基础设施的传统观念，促进了AI研究的普及化。", "keywords": "数学推理, 强化学习, 内存优化, 游戏GPU, 资源受限AI", "comments": "本文的创新之处在于，它打破了高性能数学推理模型训练需要大规模计算资源的传统认知。通过巧妙地结合强化学习和内存优化技术，研究人员证明了个人游戏GPU也能训练出具有竞争力的模型，这对于AI研究的普及化和降低研究门槛具有重要意义。"}}
{"id": "2506.08548", "title": "Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification", "authors": ["Moria Mayala", "Erwan Scornet", "Charles Tillier", "Olivier Wintenberger"], "summary": "Many classification tasks involve imbalanced data, in which a class is\nlargely underrepresented. Several techniques consists in creating a rebalanced\ndataset on which a classifier is trained. In this paper, we study theoretically\nsuch a procedure, when the classifier is a Centered Random Forests (CRF). We\nestablish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates\nand exact constant. We then prove that the CRF trained on the rebalanced\ndataset exhibits a bias, which can be removed with appropriate techniques.\nBased on an importance sampling (IS) approach, the resulting debiased\nestimator, called IS-ICRF, satisfies a CLT centered at the prediction function\nvalue. For high imbalance settings, we prove that the IS-ICRF estimator enjoys\na variance reduction compared to the ICRF trained on the original data.\nTherefore, our theoretical analysis highlights the benefits of training random\nforests on a rebalanced dataset (followed by a debiasing procedure) compared to\nusing the original data. Our theoretical results, especially the variance rates\nand the variance reduction, appear to be valid for Breiman's random forests in\nour experiments.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.08548v1", "AI": {"title_translation": "无限中心随机森林的渐近正态性——在不平衡分类中的应用", "tldr": "本文理论研究了在不平衡数据上训练中心随机森林（CRF）的去偏估计器，并证明了其渐近正态性和方差降低优势。", "motivation": "许多分类任务涉及不平衡数据，其中一个类别代表性严重不足。现有技术通过创建再平衡数据集来解决此问题，但其理论基础有待深入研究，尤其是在分类器为中心随机森林时。", "method": "本文理论研究了当分类器是中心随机森林（CRF）时，在再平衡数据集上训练分类器的过程。建立了无限CRF的中心极限定理（CLT），并给出了明确的速率和精确常数。证明了在再平衡数据集上训练的CRF存在偏差，并提出通过重要性采样（IS）方法去除偏差，得到去偏估计器IS-ICRF。", "result": "建立了无限CRF的中心极限定理（CLT），具有明确的速率和精确常数。证明了在再平衡数据集上训练的CRF存在偏差。基于重要性采样（IS）的去偏估计器IS-ICRF满足以预测函数值为中心的CLT。在高不平衡设置下，IS-ICRF估计器与在原始数据上训练的ICRF相比，方差有所降低。实验表明，理论结果（特别是方差率和方差降低）对Breiman随机森林也有效。", "conclusion": "理论分析强调了在再平衡数据集（并进行去偏处理）上训练随机森林相比使用原始数据的优势，尤其是在方差降低方面。", "translation": "许多分类任务涉及不平衡数据，其中一个类别代表性严重不足。几种技术包括创建再平衡数据集，在此数据集上训练分类器。在本文中，我们理论研究了当分类器是中心随机森林（CRF）时，这种过程。我们建立了无限CRF的中心极限定理（CLT），具有明确的速率和精确常数。然后我们证明了在再平衡数据集上训练的CRF表现出偏差，这可以通过适当的技术去除。基于重要性采样（IS）方法，得到的去偏估计器，称为IS-ICRF，满足以预测函数值为中心的CLT。对于高度不平衡的设置，我们证明了IS-ICRF估计器与在原始数据上训练的ICRF相比，方差有所降低。因此，我们的理论分析强调了在再平衡数据集（并进行去偏过程）上训练随机森林相比使用原始数据的好处。我们的理论结果，特别是方差率和方差降低，在我们的实验中似乎对Breiman的随机森林也有效。", "summary": "本文理论研究了在不平衡数据上使用中心随机森林（CRF）进行分类的再平衡和去偏过程。作者建立了无限CRF的中心极限定理，并证明了在再平衡数据集上训练的CRF存在偏差。通过引入基于重要性采样的去偏估计器IS-ICRF，作者证明了其满足中心极限定理，并且在高不平衡情况下相比在原始数据上训练的ICRF能显著降低方差。研究结果强调了对不平衡数据进行再平衡和去偏处理在随机森林分类中的优势。", "keywords": "不平衡分类, 中心随机森林, 渐近正态性, 中心极限定理, 方差降低", "comments": "这篇论文通过严谨的理论分析，为不平衡数据分类中随机森林的应用提供了坚实的基础。它不仅揭示了再平衡数据集训练CRF的偏差问题，更提出了有效的去偏方法并证明了其在方差降低方面的优势，对于实际应用具有重要的指导意义。"}}
{"id": "2506.08952", "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions", "authors": ["Clara Lachenmaier", "Judith Sieker", "Sina Zarrieß"], "summary": "Communication among humans relies on conversational grounding, allowing\ninterlocutors to reach mutual understanding even when they do not have perfect\nknowledge and must resolve discrepancies in each other's beliefs. This paper\ninvestigates how large language models (LLMs) manage common ground in cases\nwhere they (don't) possess knowledge, focusing on facts in the political domain\nwhere the risk of misinformation and grounding failure is high. We examine the\nability of LLMs to answer direct knowledge questions and loaded questions that\npresuppose misinformation. We evaluate whether loaded questions lead LLMs to\nengage in active grounding and correct false user beliefs, in connection to\ntheir level of knowledge and their political bias. Our findings highlight\nsignificant challenges in LLMs' ability to engage in grounding and reject false\nuser beliefs, raising concerns about their role in mitigating misinformation in\npolitical discourse.", "comment": "Preprint accepted at ACL Main Conference 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08952v1", "AI": {"title_translation": "LLMs在“知”与“不知”中能否实现语境对齐：一项关于直接和带有预设的政治问题的研究", "tldr": "本研究探讨了大型语言模型（LLMs）在处理政治领域知识和虚假信息时，能否像人类一样进行语境对齐（grounding），结果发现LLMs在拒绝错误用户信念方面存在显著挑战。", "motivation": "人类交流依赖于语境对齐来达成相互理解，即使知识不完美。本研究旨在调查大型语言模型（LLMs）在处理政治领域知识和虚假信息时，如何管理共同语境，因为该领域误报和语境对齐失败的风险很高。", "method": "研究检查了LLMs回答直接知识问题和预设虚假信息的带有预设的问题的能力。评估了带有预设的问题是否导致LLMs主动进行语境对齐并纠正错误的用户信念，同时考虑了它们的知识水平和政治偏见。", "result": "研究结果强调了LLMs在进行语境对齐和拒绝错误用户信念方面存在显著挑战。", "conclusion": "LLMs在减轻政治话语中的虚假信息方面存在令人担忧的问题。", "translation": "人类之间的交流依赖于语境对齐，即使在知识不完善且必须解决彼此信念差异的情况下，对话者也能达成相互理解。本文研究了大型语言模型（LLMs）在（不）拥有知识的情况下如何管理共同语境，重点关注政治领域的实事，因为该领域虚假信息和语境对齐失败的风险很高。我们检验了LLMs回答直接知识问题和预设虚假信息的带有预设问题（loaded questions）的能力。我们评估了带有预设的问题是否导致LLMs主动进行语境对齐并纠正错误的用户信念，这与其知识水平和政治偏见有关。我们的研究结果强调了LLMs在进行语境对齐和拒绝错误用户信念方面存在显著挑战，这引发了对其在政治话语中减轻虚假信息作用的担忧。", "summary": "本文研究了大型语言模型（LLMs）在处理政治领域知识和虚假信息时，能否有效地进行语境对齐和纠正错误用户信念。通过测试LLMs对直接问题和带有预设问题的回答能力，研究发现LLMs在拒绝虚假用户信念方面存在显著困难，这对其在政治话语中对抗虚假信息的能力提出了担忧。", "keywords": "LLMs, 语境对齐, 虚假信息, 政治问题, 带有预设的问题", "comments": "这项研究揭示了当前LLMs在处理复杂语境对齐和识别/纠正虚假信息方面的局限性，尤其是在政治这种敏感领域。其重要性在于指出了LLMs在作为信息传播者和误信息缓解者角色时可能面临的风险，为未来LLMs在事实核查和批判性思维方面的改进提供了明确方向。"}}
{"id": "2506.08558", "title": "Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees", "authors": ["William de Vazelhes", "Xiao-Tong Yuan", "Bin Gu"], "summary": "In sparse optimization, enforcing hard constraints using the $\\ell_0$\npseudo-norm offers advantages like controlled sparsity compared to convex\nrelaxations. However, many real-world applications demand not only sparsity\nconstraints but also some extra constraints. While prior algorithms have been\ndeveloped to address this complex scenario with mixed combinatorial and convex\nconstraints, they typically require the closed form projection onto the mixed\nconstraints which might not exist, and/or only provide local guarantees of\nconvergence which is different from the global guarantees commonly sought in\nsparse optimization. To fill this gap, in this paper, we study the problem of\nsparse optimization with extra \\qw{\\textit{support-preserving}} constraints\ncommonly encountered in the literature. We present a new variant of iterative\nhard-thresholding algorithm equipped with a two-step consecutive projection\noperator customized for these mixed constraints, serving as a simple\nalternative to the Euclidean projection onto the mixed constraint. By\nintroducing a novel trade-off between sparsity relaxation and sub-optimality,\nwe provide global guarantees in objective value for the output of our\nalgorithm, in the deterministic, stochastic, and zeroth-order settings, under\nthe conventional restricted strong-convexity/smoothness assumptions. As a\nfundamental contribution in proof techniques, we develop a novel extension of\nthe classic three-point lemma to the considered two-step non-convex projection\noperator, which allows us to analyze the convergence in objective value in an\nelegant way that has not been possible with existing techniques. In the\nzeroth-order case, such technique also improves upon the state-of-the-art\nresult from de Vazelhes et. al. (2022), even in the case without additional\nconstraints, by allowing us to remove a non-vanishing system error present in\ntheir work.", "comment": "Accepted for publication at ICML 2025", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.08558v1", "AI": {"title_translation": "稀疏保支持集上的优化：具有全局最优性保证的两步投影", "tldr": "本文提出了一种新的迭代硬阈值算法变体，该算法采用定制的两步投影操作，用于解决稀疏优化中带有额外保支持约束的问题，并在确定性、随机和零阶设置下提供了目标值的全局最优性保证。", "motivation": "在稀疏优化中，许多实际应用不仅需要稀疏性约束，还需要额外的约束。现有算法通常需要对混合约束进行闭合形式投影（可能不存在），并且/或只提供局部收敛保证，这与稀疏优化中通常寻求的全局保证不同。本文旨在填补这一空白，研究带有额外“保支持”约束的稀疏优化问题。", "method": "本文提出了一种新的迭代硬阈值算法变体，配备了定制的两步连续投影算子，用于处理混合约束。通过引入稀疏性松弛和次优性之间的新权衡，该算法在传统受限强凸性/平滑度假设下，在确定性、随机和零阶设置中，为其输出的目标值提供了全局保证。作为证明技术的一项基本贡献，开发了经典三点引理对所考虑的两步非凸投影算子的新扩展。", "result": "该算法在确定性、随机和零阶设置下，提供了目标值的全局最优性保证。在零阶情况下，这项技术甚至在没有额外约束的情况下，也改进了de Vazelhes等人（2022）的最新结果，因为它消除了他们工作中存在的非消失系统误差。", "conclusion": "本文成功提出了一种新的迭代硬阈值算法，通过两步投影操作和新颖的证明技术，解决了带有保支持约束的稀疏优化问题，并在多种设置下提供了全局最优性保证，同时在零阶情况下超越了现有技术水平。", "translation": "在稀疏优化中，使用 $\\ell_0$ 伪范数强制执行硬约束相比凸松弛具有可控稀疏性等优点。然而，许多现实世界的应用不仅需要稀疏性约束，还需要一些额外的约束。虽然已有算法旨在解决这种混合组合和凸约束的复杂场景，但它们通常需要对混合约束进行闭合形式投影（这可能不存在），并且/或只提供局部收敛保证，这与稀疏优化中通常寻求的全局保证不同。为了填补这一空白，本文研究了文献中常见的带有额外“保支持”约束的稀疏优化问题。我们提出了一种新的迭代硬阈值算法变体，该算法配备了专门为这些混合约束定制的两步连续投影算子，作为对混合约束进行欧几里得投影的简单替代方案。通过引入稀疏性松弛和次优性之间的新权衡，我们在传统受限强凸性/平滑度假设下，在确定性、随机和零阶设置中，为我们算法的输出目标值提供了全局保证。作为证明技术的一项基本贡献，我们开发了经典三点引理对所考虑的两步非凸投影算子的新扩展，这使我们能够以现有技术无法实现的方式优雅地分析目标值的收敛性。在零阶情况下，这项技术甚至在没有额外约束的情况下，也改进了de Vazelhes等人（2022）的最新结果，因为它消除了他们工作中存在的非消失系统误差。", "summary": "本文研究了带有额外保支持约束的稀疏优化问题，并提出了一种新的迭代硬阈值算法变体。该算法采用定制的两步连续投影算子，作为欧几里得投影的替代方案，并引入了稀疏性松弛和次优性之间的新权衡。在传统假设下，该方法在确定性、随机和零阶设置中提供了目标值的全局最优性保证。此外，本文还开发了经典三点引理对两步非凸投影算子的新扩展，从而能够优雅地分析目标值的收敛性，并在零阶情况下改进了现有技术水平。", "keywords": "稀疏优化, 两步投影, 全局最优性, 迭代硬阈值, 保支持约束", "comments": "这项工作在稀疏优化领域具有重要意义，因为它解决了现有方法在处理混合约束时缺乏全局最优性保证和对闭合形式投影的依赖性问题。提出的两步投影和新颖的证明技术（三点引理的扩展）是其核心创新点，使得算法在多种设置下都能提供强有力的理论保证，特别是在零阶优化方面取得了显著进步。这为实际应用中更复杂的稀疏优化问题提供了更可靠的解决方案。"}}
{"id": "2506.08592", "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings", "authors": ["Liyan Xu", "Zhenlin Su", "Mo Yu", "Jiangnan Li", "Fandong Meng", "Jie Zhou"], "summary": "This work focuses on an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within the semantics,\nresulting in failed dense retrieval on even simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset in Chinese, named\nCapRetrieval, whose passages are image captions, and queries are phrases\ninquiring entities or events in various forms. Zero-shot evaluation suggests\nthat encoders may fail on these fine-grained matching, regardless of training\nsources or model sizes. Aiming for enhancement, we proceed to finetune encoders\nwith our proposed data generation strategies, which obtains the best\nperformance on CapRetrieval. Within this process, we further identify an issue\nof granularity dilemma, a challenge for embeddings to express fine-grained\nsalience while aligning with overall semantics. Our dataset, code and models in\nthis work are publicly released at https://github.com/lxucs/CapRetrieval.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08592v1", "AI": {"title_translation": "密集检索器在简单查询上可能失败：揭示嵌入的粒度困境", "tldr": "文本编码器在识别细粒度实体或事件时存在局限性，导致密集检索即使在简单查询上也会失败。本文提出了一个中文评估数据集CapRetrieval，并发现编码器在细粒度匹配上表现不佳，同时提出了数据生成策略来提升性能，并揭示了粒度困境。", "motivation": "本文旨在解决文本编码器的一个局限性：即嵌入可能无法识别语义中的细粒度实体或事件，这导致密集检索即使在简单情况下也会失败。", "method": "为检验文本编码器的上述行为，本文首先引入了一个新的中文评估数据集CapRetrieval，其段落为图像标题，查询是询问各种形式的实体或事件的短语。随后，进行零样本评估。为提升性能，研究者使用所提出的数据生成策略对编码器进行微调。", "result": "零样本评估表明，无论训练来源或模型大小如何，编码器在细粒度匹配上都可能失败。通过所提出的数据生成策略对编码器进行微调后，在CapRetrieval数据集上取得了最佳性能。在此过程中，本文进一步识别出“粒度困境”问题，即嵌入在表达细粒度显著性同时与整体语义对齐的挑战。", "conclusion": "本文发现文本编码器在细粒度匹配上存在“粒度困境”，即难以同时表达细粒度显著性并与整体语义对齐，这导致密集检索在简单查询上也会失败。通过提出的微调策略可以提升性能。", "translation": "这项工作关注文本编码器的一个已知局限性：嵌入可能无法识别语义中的细粒度实体或事件，导致即使在简单情况下密集检索也会失败。为了检验这种行为，我们首先引入了一个新的中文评估数据集，名为CapRetrieval，其段落是图像标题，查询是询问各种形式的实体或事件的短语。零样本评估表明，无论训练来源或模型大小如何，编码器在这些细粒度匹配上都可能失败。为了增强性能，我们接着使用我们提出的数据生成策略对编码器进行微调，这在CapRetrieval上获得了最佳性能。在此过程中，我们进一步识别出粒度困境问题，即嵌入在表达细粒度显著性同时与整体语义对齐的挑战。这项工作中的数据集、代码和模型已在https://github.com/lxucs/CapRetrieval公开。", "summary": "本研究关注文本编码器在识别细粒度实体或事件方面的局限性，这导致密集检索在简单查询上也会失败。为探究此现象，我们引入了一个新的中文评估数据集CapRetrieval，其包含图像标题和实体/事件查询。零样本评估揭示了编码器在细粒度匹配上的普遍性失败。为改进，我们提出了数据生成策略来微调编码器，并在CapRetrieval上取得了最佳表现。此过程揭示了“粒度困境”，即嵌入难以同时表达细粒度信息并保持整体语义对齐。数据集、代码和模型均已公开。", "keywords": "密集检索, 文本嵌入, 细粒度匹配, 粒度困境, CapRetrieval", "comments": "本论文的创新之处在于提出了一个新的中文细粒度评估数据集CapRetrieval，并明确指出了文本嵌入在细粒度匹配中存在的“粒度困境”，即在表达细粒度信息和保持整体语义之间存在挑战。此外，论文还提出了有效的数据生成策略来提升密集检索在细粒度任务上的性能，为未来的研究提供了有价值的资源和方向。"}}
{"id": "2506.08616", "title": "Generalizing while preserving monotonicity in comparison-based preference learning models", "authors": ["Julien Fageot", "Peva Blanchard", "Gilles Bareilles", "Lê-Nguyên Hoang"], "summary": "If you tell a learning model that you prefer an alternative $a$ over another\nalternative $b$, then you probably expect the model to be monotone, that is,\nthe valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps\nsurprisingly, many widely deployed comparison-based preference learning models,\nincluding large language models, fail to have this guarantee. Until now, the\nonly comparison-based preference learning algorithms that were proved to be\nmonotone are the Generalized Bradley-Terry models. Yet, these models are unable\nto generalize to uncompared data. In this paper, we advance the understanding\nof the set of models with generalization ability that are monotone. Namely, we\npropose a new class of Linear Generalized Bradley-Terry models with Diffusion\nPriors, and identify sufficient conditions on alternatives' embeddings that\nguarantee monotonicity. Our experiments show that this monotonicity is far from\nbeing a general guarantee, and that our new class of generalizing models\nimproves accuracy, especially when the dataset is limited.", "comment": null, "cate": "math.ST", "url": "http://arxiv.org/abs/2506.08616v1", "AI": {"title_translation": "在基于比较的偏好学习模型中实现泛化同时保持单调性", "tldr": "许多基于比较的偏好学习模型缺乏单调性保证，而现有单调模型无法泛化。本文提出了一种新的模型类别，即带有扩散先验的线性广义Bradley-Terry模型，该模型在保持单调性的同时提高了泛化能力和准确性，尤其是在数据有限的情况下。", "motivation": "许多广泛部署的基于比较的偏好学习模型（包括大型语言模型）未能保证单调性，即当偏好a优于b时，a的估值增加而b的估值减少。虽然广义Bradley-Terry模型被证明是单调的，但它们无法泛化到未比较的数据。因此，需要开发既能泛化又能保持单调性的模型。", "method": "本文提出了一种新的模型类别：带有扩散先验的线性广义Bradley-Terry模型（Linear Generalized Bradley-Terry models with Diffusion Priors）。此外，论文还确定了替代方案嵌入的充分条件以保证单调性。", "result": "实验表明，单调性远非一般性保证。提出的新型泛化模型提高了准确性，尤其是在数据集有限的情况下。", "conclusion": "本文推进了对具有泛化能力的单调模型集合的理解。通过引入带有扩散先验的线性广义Bradley-Terry模型，研究解决了现有模型在单调性和泛化能力方面的局限性，并证明了其在准确性上的改进。", "translation": "如果你告诉一个学习模型你偏爱替代方案a而不是替代方案b，那么你可能会期望模型是单调的，也就是说，a的估值增加，b的估值减少。然而，或许令人惊讶的是，许多广泛部署的基于比较的偏好学习模型，包括大型语言模型，都未能提供这种保证。到目前为止，唯一被证明是单调的基于比较的偏好学习算法是广义Bradley-Terry模型。然而，这些模型无法泛化到未比较的数据。在本文中，我们推进了对具有泛化能力的单调模型集合的理解。具体来说，我们提出了一种新的带有扩散先验的线性广义Bradley-Terry模型类别，并确定了替代方案嵌入的充分条件以保证单调性。我们的实验表明，这种单调性远非一般性保证，并且我们新的泛化模型类别提高了准确性，尤其是在数据集有限的情况下。", "summary": "本文探讨了基于比较的偏好学习模型中单调性与泛化能力并存的问题。现有模型常缺乏单调性保证，而能保证单调性的广义Bradley-Terry模型又无法泛化。为此，研究提出了一种新的模型类别——带有扩散先验的线性广义Bradley-Terry模型，并识别了保证单调性的充分条件。实验结果表明，单调性并非普遍特性，而新提出的泛化模型显著提高了准确性，尤其是在数据量有限的情况下。", "keywords": "偏好学习, 单调性, 泛化, Bradley-Terry模型, 扩散先验", "comments": "本文的创新之处在于提出了一个能同时解决基于比较的偏好学习模型中单调性和泛化能力问题的框架。它填补了现有模型在这两个关键特性上的空白，特别是通过引入扩散先验的线性广义Bradley-Terry模型，为在有限数据下提高模型性能提供了有效途径。这对于需要高可靠性和准确性的偏好学习应用具有重要意义，例如在推荐系统或人机交互中。"}}
{"id": "2506.08646", "title": "TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning", "authors": ["Mingyu Zheng", "Zhifan Feng", "Jia Wang", "Lanrui Wang", "Zheng Lin", "Yang Hao", "Weiping Wang"], "summary": "Despite the commendable progress of recent LLM-based data synthesis methods,\nthey face two limitations in generating table instruction tuning data. First,\nthey can not thoroughly explore the vast input space of table understanding\ntasks, leading to limited data diversity. Second, they ignore the weaknesses in\ntable understanding ability of the target LLM and blindly pursue the increase\nof data quantity, resulting in suboptimal data efficiency. In this paper, we\nintroduce a progressive and weakness-guided data synthesis framework tailored\nfor table instruction tuning, named TableDreamer, to mitigate the above issues.\nSpecifically, we first synthesize diverse tables and related instructions as\nseed data, and then perform an iterative exploration of the input space under\nthe guidance of the newly identified weakness data, which eventually serve as\nthe final training data for fine-tuning the target LLM. Extensive experiments\non 10 tabular benchmarks demonstrate the effectiveness of the proposed\nframework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%\n(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms\nstate-of-the-art data synthesis baselines which use more training data. The\ncode and data is available at https://github.com/SpursGoZmy/TableDreamer", "comment": "27 pages, 19 figures, Findings of ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08646v1", "AI": {"title_translation": "TableDreamer：面向表格指令微调的渐进式弱点引导数据从零合成", "tldr": "TableDreamer是一个渐进式且弱点引导的数据合成框架，用于表格指令微调，它通过迭代探索和关注LLM的弱点来提高数据多样性和效率，显著提升了Llama3.1-8B-instruct在表格基准上的准确率。", "motivation": "现有的基于LLM的数据合成方法在生成表格指令微调数据时存在两个局限性：一是无法彻底探索表格理解任务的巨大输入空间，导致数据多样性有限；二是忽视目标LLM在表格理解能力上的弱点，盲目追求数据量增加，导致数据效率低下。", "method": "本文引入了一个名为TableDreamer的渐进式弱点引导数据合成框架。具体而言，首先合成多样化的表格和相关指令作为种子数据，然后在新识别的弱点数据指导下对输入空间进行迭代探索，最终将这些数据用作目标LLM的微调训练数据。", "result": "在10个表格基准上的广泛实验表明，所提出的框架是有效的，它使用2.7万条GPT-4o合成数据，将Llama3.1-8B-instruct的平均准确率提高了11.62%（从49.07%到60.69%），并优于使用更多训练数据的最先进数据合成基线。", "conclusion": "TableDreamer框架通过其渐进式和弱点引导的数据合成方法，有效解决了现有表格指令微调数据合成的局限性，显著提升了大型语言模型在表格理解任务上的性能。", "translation": "尽管最近基于LLM的数据合成方法取得了值得称赞的进展，但它们在生成表格指令微调数据方面面临两个限制。首先，它们无法彻底探索表格理解任务的巨大输入空间，导致数据多样性有限。其次，它们忽视了目标LLM在表格理解能力上的弱点，盲目追求数据量的增加，导致数据效率低下。在本文中，我们引入了一个名为TableDreamer的渐进式弱点引导数据合成框架，专门用于表格指令微调，以缓解上述问题。具体而言，我们首先合成多样化的表格和相关指令作为种子数据，然后在新识别的弱点数据指导下对输入空间进行迭代探索，这些数据最终将作为微调目标LLM的最终训练数据。在10个表格基准上的广泛实验证明了所提出框架的有效性，它使用2.7万条GPT-4o合成数据，将Llama3.1-8B-instruct的平均准确率提高了11.62%（从49.07%到60.69%），并优于使用更多训练数据的最先进数据合成基线。代码和数据可在https://github.com/SpursGoZmy/TableDreamer获取。", "summary": "本文提出了TableDreamer，一个针对表格指令微调的渐进式弱点引导数据合成框架。它旨在解决现有数据合成方法在数据多样性不足和数据效率低下等问题。TableDreamer首先合成种子数据，然后根据目标LLM的弱点迭代探索输入空间生成训练数据。实验结果显示，该框架显著提升了Llama3.1-8B-instruct在多个表格基准上的准确率，并超越了其他先进的数据合成方法。", "keywords": "表格指令微调, 数据合成, LLM, 弱点引导, TableDreamer", "comments": "TableDreamer的创新之处在于其“渐进式”和“弱点引导”的数据合成范式。它不仅关注数据多样性，更重要的是，它能够针对目标LLM的薄弱环节生成针对性数据，从而以更少的数据量实现更好的微调效果，这对于节省计算资源和提高模型性能具有重要意义。其超越使用更多数据的SOTA基线也证明了其数据效率的优势。"}}
{"id": "2506.08654", "title": "A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck", "authors": ["Ciro Benito Raggio", "Paolo Zaffino", "Maria Francesca Spadea"], "summary": "Shortened Abstract\n  Cone-beam computed tomography (CBCT) has become a widely adopted modality for\nimage-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,\nlimited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield\nunit values and hindering direct dose calculation. Synthetic CT (sCT)\ngeneration from CBCT addresses these issues, especially using deep learning\n(DL) methods. Existing approaches are limited by institutional heterogeneity,\nscanner-dependent variations, and data privacy regulations that prevent\nmulti-center data sharing.\n  To overcome these challenges, we propose a cross-silo horizontal federated\nlearning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,\nextending our FedSynthCT framework. A conditional generative adversarial\nnetwork was collaboratively trained on data from three European medical centers\nin the public SynthRAD2025 challenge dataset.\n  The federated model demonstrated effective generalization across centers,\nwith mean absolute error (MAE) ranging from $64.38\\pm13.63$ to $85.90\\pm7.10$\nHU, structural similarity index (SSIM) from $0.882\\pm0.022$ to $0.922\\pm0.039$,\nand peak signal-to-noise ratio (PSNR) from $32.86\\pm0.94$ to $34.91\\pm1.04$ dB.\nNotably, on an external validation dataset of 60 patients, comparable\nperformance was achieved (MAE: $75.22\\pm11.81$ HU, SSIM: $0.904\\pm0.034$, PSNR:\n$33.52\\pm2.06$ dB) without additional training, confirming robust\ngeneralization despite protocol, scanner differences and registration errors.\n  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT\nsynthesis while preserving data privacy and offer a collaborative solution for\ndeveloping generalizable models across institutions without centralized data\nsharing or site-specific fine-tuning.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.08654v1", "AI": {"title_translation": "一种用于头颈部可泛化CBCT到合成CT转换的隐私保护联邦学习框架", "tldr": "该研究提出了一种隐私保护的联邦学习框架，用于将头颈部锥形束CT（CBCT）图像转换为合成CT（sCT），并在多中心数据下实现了良好的泛化性能，同时保护了数据隐私。", "motivation": "锥形束CT（CBCT）在图像引导放射治疗（IGRT）中广泛应用，但存在噪声大、软组织对比度有限和伪影等问题，导致Hounsfield单位值不可靠并阻碍直接剂量计算。深度学习方法生成的合成CT（sCT）可以解决这些问题。然而，现有方法受到机构异质性、扫描仪差异和数据隐私法规的限制，无法进行多中心数据共享。", "method": "本研究提出了一种跨筒仓的水平联邦学习（FL）方法，用于头颈部区域的CBCT到sCT合成，该方法是FedSynthCT框架的扩展。研究人员在一个公共SynthRAD2025挑战数据集上，利用来自三个欧洲医疗中心的数据协同训练了一个条件生成对抗网络。", "result": "联邦模型在不同中心之间表现出有效的泛化能力，平均绝对误差（MAE）范围为64.38±13.63至85.90±7.10 HU，结构相似性指数（SSIM）范围为0.882±0.022至0.922±0.039，峰值信噪比（PSNR）范围为32.86±0.94至34.91±1.04 dB。值得注意的是，在包含60名患者的外部验证数据集上，无需额外训练即实现了可比的性能（MAE：75.22±11.81 HU，SSIM：0.904±0.034，PSNR：33.52±2.06 dB），证实了尽管存在协议、扫描仪差异和配准误差，模型仍具有鲁棒的泛化能力。", "conclusion": "这些研究结果证明了联邦学习在CBCT到sCT合成中实现技术可行性，同时保护了数据隐私，并为在不进行集中数据共享或特定站点微调的情况下，跨机构开发可泛化模型提供了一种协作解决方案。", "translation": "锥形束计算机断层扫描（CBCT）已成为图像引导放射治疗（IGRT）中广泛采用的模式。然而，CBCT存在噪声增加、软组织对比度有限和伪影等问题，导致Hounsfield单位值不可靠并阻碍直接剂量计算。从CBCT生成合成CT（sCT）解决了这些问题，特别是使用深度学习（DL）方法。现有方法受到机构异质性、扫描仪依赖性变异和数据隐私法规的限制，这些限制阻止了多中心数据共享。\n为了克服这些挑战，我们提出了一种用于头颈部CBCT到sCT合成的跨筒仓水平联邦学习（FL）方法，扩展了我们的FedSynthCT框架。一个条件生成对抗网络在公共SynthRAD2025挑战数据集中来自三个欧洲医疗中心的数据上进行了协同训练。\n联邦模型在不同中心之间表现出有效的泛化能力，平均绝对误差（MAE）范围为64.38±13.63至85.90±7.10 HU，结构相似性指数（SSIM）范围为0.882±0.022至0.922±0.039，峰值信噪比（PSNR）范围为32.86±0.94至34.91±1.04 dB。值得注意的是，在包含60名患者的外部验证数据集上，无需额外训练即实现了可比的性能（MAE：75.22±11.81 HU，SSIM：0.904±0.034，PSNR：33.52±2.06 dB），证实了尽管存在协议、扫描仪差异和配准误差，模型仍具有鲁棒的泛化能力。\n这些研究结果证明了联邦学习在CBCT到sCT合成中实现技术可行性，同时保护了数据隐私，并为在不进行集中数据共享或特定站点微调的情况下，跨机构开发可泛化模型提供了一种协作解决方案。", "summary": "本研究针对CBCT图像在IGRT中存在的缺陷以及多中心数据共享的隐私限制，提出了一种基于联邦学习的CBCT到sCT转换框架。通过协同训练一个条件生成对抗网络，该框架在三个欧洲医疗中心的数据上实现了良好的泛化性能，并在外部验证集上表现出鲁棒性。这为在保护数据隐私的前提下，开发可泛化的医疗图像转换模型提供了可行的解决方案。", "keywords": "联邦学习, CBCT, 合成CT, 隐私保护, 图像转换", "comments": "该论文的创新点在于将联邦学习应用于医疗图像转换领域，特别是CBCT到sCT的生成，有效解决了医疗数据隐私保护和多中心数据异质性带来的挑战。其重要性在于提供了一种无需集中数据共享即可开发泛化性强模型的途径，对于推动医疗AI在临床实践中的应用具有重要意义。该方法有望加速高质量合成CT的生成，从而提高放射治疗的准确性和效率。"}}
{"id": "2506.08999", "title": "Employing self-supervised learning models for cross-linguistic child speech maturity classification", "authors": ["Theo Zhang", "Madurya Suresh", "Anne S. Warlaumont", "Kasia Hitczenko", "Alejandrina Cristia", "Margaret Cychosz"], "summary": "Speech technology systems struggle with many downstream tasks for child\nspeech due to small training corpora and the difficulties that child speech\npose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer\nmodels to address a fundamental classification task: identifying child\nvocalizations. Unlike previous corpora, our dataset captures maximally\necologically-valid child vocalizations across an unprecedented sample,\ncomprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,\nPapua New Guinea, Solomon Islands, and France. The dataset contains 242,004\nlabeled vocalizations, magnitudes larger than previous work. Models were\ntrained to distinguish between cry, laughter, mature (consonant+vowel), and\nimmature speech (just consonant or vowel). Models trained on the dataset\noutperform state-of-the-art models trained on previous datasets, achieved\nclassification accuracy comparable to humans, and were robust across rural and\nurban settings.", "comment": "To be published in Interspeech 2025. 5 pages, 2 figures. For\n  associated Github repository, see\n  https://github.com/spoglab-stanford/w2v2-pro-sm/tree/main/speechbrain/recipes/W2V2-LL4300-Pro-SM", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08999v1", "AI": {"title_translation": "采用自监督学习模型进行跨语言儿童语音成熟度分类", "tldr": "本文通过引入一个大规模、跨语言的儿童语音数据集SpeechMaturity，并结合最先进的Transformer模型，显著提升了儿童语音成熟度分类的准确性和鲁棒性，超越了现有方法并达到人类水平。", "motivation": "语音技术系统在处理儿童语音时面临挑战，主要原因在于训练语料库规模小以及儿童语音本身的复杂性。", "method": "研究人员开发了一个名为SpeechMaturity的新型数据集，该数据集包含来自美国、玻利维亚、瓦努阿图、巴布亚新几内亚、所罗门群岛和法国等25种以上语言背景的儿童的242,004个带标签发声。他们将此数据集应用于最先进的Transformer模型，以执行识别儿童发声的分类任务，具体区分哭声、笑声、成熟语音（辅音+元音）和不成熟语音（仅辅音或元音）。", "result": "基于SpeechMaturity数据集训练的模型在分类准确性上优于使用现有数据集训练的最先进模型，达到了与人类相当的分类准确率，并且在城乡环境中均表现出良好的鲁棒性。", "conclusion": "通过引入大规模、生态学有效的新数据集和应用先进的Transformer模型，本研究显著提升了跨语言儿童语音成熟度分类的性能和泛化能力。", "translation": "语音技术系统在儿童语音的许多下游任务中面临困难，原因在于训练语料库小以及儿童语音本身带来的挑战。我们应用一个新的数据集SpeechMaturity到最先进的Transformer模型，以解决一个基本的分类任务：识别儿童发声。与以往的语料库不同，我们的数据集捕捉了前所未有的样本中生态学上最有效的儿童发声，包括在美国、玻利维亚、瓦努阿图、巴布亚新几内亚、所罗门群岛和法国习得25种以上语言的儿童。该数据集包含242,004个带标签的发声，规模比以前的工作大几个数量级。模型被训练用于区分哭声、笑声、成熟语音（辅音+元音）和不成熟语音（仅辅音或元音）。在该数据集上训练的模型优于在以前数据集上训练的最先进模型，达到了与人类相当的分类准确率，并且在农村和城市环境中都表现出鲁棒性。", "summary": "本文旨在解决儿童语音处理中面临的挑战，通过引入一个大规模、跨语言的生态学有效数据集SpeechMaturity，该数据集包含24万余个带标签的儿童发声。研究人员将此数据集应用于最先进的Transformer模型，以分类儿童发声为哭声、笑声、成熟或不成熟语音。结果显示，新模型在分类准确性上超越了现有技术，达到人类水平，并对不同地域环境表现出鲁棒性。", "keywords": "儿童语音, 语音成熟度分类, 自监督学习, 跨语言, Transformer模型", "comments": "这项研究通过构建一个前所未有的大规模、跨语言、生态学有效的儿童语音数据集，显著推动了儿童语音分析领域的发展。其创新之处在于数据集的规模和多样性，以及将先进的Transformer模型应用于儿童语音成熟度分类。这项工作有望为未来的儿童语言发展评估和相关技术应用奠定坚实基础。"}}
{"id": "2506.09033", "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "authors": ["Haozhen Zhang", "Tao Feng", "Jiaxuan You"], "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To guide learning, we employ a lightweight rule-based\nreward comprising format rewards, final outcome rewards, and a novel cost\nreward for performance and cost trade-off optimization, opening a pathway\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\nonly on simple model descriptors such as pricing, latency, and example\nperformance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms over several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.Code is available at\nhttps://github.com/ulab-uiuc/Router-R1.", "comment": "Code is available at https://github.com/ulab-uiuc/Router-R1", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09033v1", "AI": {"title_translation": "Router-R1：通过强化学习教授大型语言模型多轮路由与聚合", "tldr": "Router-R1是一个基于强化学习的框架，旨在通过将路由器实例化为一个LLM，并允许其进行多轮路由和聚合，以解决现有LLM路由器在处理复杂任务时单轮映射的局限性，从而实现性能、泛化能力和成本管理的提升。", "motivation": "现有的大型语言模型（LLM）路由器通常执行单轮、一对一的映射，即将每个查询单独分配给单个模型，这限制了它们处理需要多个LLM互补优势的复杂任务的能力。", "method": "Router-R1将多LLM路由和聚合表述为一个顺序决策过程，并将其本身实例化为一个有能力的LLM。它利用LLM的推理能力来交错“思考”动作（内部审议）和“路由”动作（动态模型调用），并将每个响应整合到其不断演进的上下文中。为了指导学习，Router-R1采用了一种轻量级的基于规则的奖励机制，包括格式奖励、最终结果奖励以及一种新颖的成本奖励，用于优化性能和成本之间的权衡。此外，Router-R1仅依赖于简单的模型描述符（如定价、延迟和示例性能）进行条件设置，从而实现对未见模型选择的强大泛化。", "result": "在七个通用和多跳问答基准测试中，Router-R1的表现优于多个强大的基线模型，取得了卓越的性能，同时保持了强大的泛化能力和成本管理。", "conclusion": "Router-R1通过强化学习框架有效地解决了现有LLM路由器在处理复杂任务时的局限性，实现了多轮路由和聚合，并在性能、泛化能力和成本管理方面表现出色，为优化性能-成本权衡提供了新途径。", "translation": "各种大型语言模型（LLMs）的迅速出现推动了LLM路由器的发展，这些路由器将用户查询分配给最合适的模型。然而，现有的LLM路由器通常执行单轮、一对一的映射（即，将每个查询单独分配给单个模型），这限制了它们处理需要多个LLM互补优势的复杂任务的能力。在本文中，我们提出了Router-R1，一个基于强化学习（RL）的框架，将多LLM路由和聚合表述为一个顺序决策过程。Router-R1将路由器本身实例化为一个有能力的LLM，利用其推理能力交错“思考”动作（内部审议）和“路由”动作（动态模型调用），并将每个响应整合到其不断演进的上下文中。为了指导学习，我们采用了一种轻量级的基于规则的奖励机制，包括格式奖励、最终结果奖励以及一种新颖的成本奖励，用于性能和成本权衡优化，为通过RL优化性能-成本权衡开辟了道路。Router-R1还仅依赖于简单的模型描述符，如定价、延迟和示例性能，从而实现对未见模型选择的强大泛化。在七个通用和多跳问答基准测试中的实验表明，Router-R1优于多个强大的基线模型，取得了卓越的性能，同时保持了强大的泛化能力和成本管理。代码可在https://github.com/ulab-uiuc/Router-R1获取。", "summary": "Router-R1是一个基于强化学习的框架，旨在通过将路由器实例化为一个大型语言模型（LLM），以解决现有LLM路由器在处理需要多个LLM协同工作的复杂任务时的局限性。它将多LLM路由和聚合建模为顺序决策过程，并允许LLM在“思考”和“路由”动作之间交错进行。该框架采用轻量级规则奖励机制，包括性能和成本权衡优化，并仅依赖简单的模型描述符，从而实现对新模型的强大泛化。实验结果表明，Router-R1在多个问答基准测试中优于现有基线，展现出卓越的性能、强大的泛化能力和有效的成本管理。", "keywords": "大型语言模型路由, 强化学习, 多轮聚合, 性能-成本权衡, 泛化能力", "comments": "Router-R1的创新之处在于将LLM本身作为路由器，并利用强化学习实现多轮路由和聚合，这显著提升了LLM路由器处理复杂任务的能力。其引入的成本奖励机制为优化性能-成本权衡提供了新的思路，具有重要的实践意义。此外，仅依赖简单模型描述符的泛化能力也值得关注。"}}
{"id": "2506.08734", "title": "Flexible and Efficient Drift Detection without Labels", "authors": ["Nelvin Tan", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "summary": "Machine learning models are being increasingly used to automate decisions in\nalmost every domain, and ensuring the performance of these models is crucial\nfor ensuring high quality machine learning enabled services. Ensuring concept\ndrift is detected early is thus of the highest importance. A lot of research on\nconcept drift has focused on the supervised case that assumes the true labels\nof supervised tasks are available immediately after making predictions.\nControlling for false positives while monitoring the performance of predictive\nmodels used to make inference from extremely large datasets periodically, where\nthe true labels are not instantly available, becomes extremely challenging. We\npropose a flexible and efficient concept drift detection algorithm that uses\nclassical statistical process control in a label-less setting to accurately\ndetect concept drifts. We shown empirically that under computational\nconstraints, our approach has better statistical power than previous known\nmethods. Furthermore, we introduce a new drift detection framework to model the\nscenario of detecting drift (without labels) given prior detections, and show\nour how our drift detection algorithm can be incorporated effectively into this\nframework. We demonstrate promising performance via numerical simulations.", "comment": "9 pages, 4 figures", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.08734v1", "AI": {"title_translation": "灵活高效的无标签漂移检测", "tldr": "提出了一种在无标签环境下，使用经典统计过程控制进行概念漂移检测的灵活高效算法，并证明其在计算约束下比现有方法具有更好的统计功效。", "motivation": "机器学习模型在自动化决策中的应用日益广泛，确保模型性能至关重要。概念漂移的早期检测非常重要，尤其是在真实标签不能立即获得的无标签设置中，控制误报率同时监测模型性能极具挑战性。", "method": "提出了一种在无标签环境下使用经典统计过程控制的灵活高效的概念漂移检测算法。此外，还引入了一个新的漂移检测框架，用于在给定先前检测的情况下（无标签）建模漂移检测场景，并展示了该算法如何有效地融入此框架。", "result": "经验性地表明，在计算约束下，该方法比以前已知的方法具有更好的统计功效。通过数值模拟展示了有前景的性能。", "conclusion": "该研究提出了一种在无标签设置下进行概念漂移检测的有效方法，并通过实验和数值模拟验证了其优越的性能和灵活性。", "translation": "机器学习模型正越来越多地被用于几乎所有领域的自动化决策，而确保这些模型的性能对于提供高质量的机器学习服务至关重要。因此，尽早检测概念漂移是重中之重。关于概念漂移的大量研究都集中在监督学习情况，即假设在做出预测后可以立即获得监督任务的真实标签。然而，在真实标签无法立即获得的情况下，定期监测用于从海量数据集中进行推断的预测模型的性能，同时控制误报，变得极具挑战性。我们提出了一种灵活高效的概念漂移检测算法，该算法在无标签设置下使用经典统计过程控制来准确检测概念漂移。我们通过经验证明，在计算约束下，我们的方法比以前已知的方法具有更好的统计功效。此外，我们引入了一个新的漂移检测框架，用于在给定先前检测的情况下（无标签）建模漂移检测场景，并展示了我们的漂移检测算法如何有效地融入此框架。我们通过数值模拟展示了有前景的性能。", "summary": "这篇论文提出了一种在无标签环境下进行概念漂移检测的灵活高效算法。该算法利用经典统计过程控制，解决了真实标签不可立即获取时模型性能监测的挑战。研究表明，在计算受限条件下，该方法比现有方法具有更高的统计功效。此外，论文还引入了一个新的漂移检测框架，并展示了该算法如何有效地集成其中，数值模拟也验证了其良好性能。", "keywords": "概念漂移检测, 无标签, 统计过程控制, 机器学习, 模型性能", "comments": "该论文的创新点在于提出了在无标签环境下进行概念漂移检测的方法，这对于实际应用中真实标签获取困难的场景非常重要。通过结合经典统计过程控制，提升了检测的统计功效，并在计算约束下展现出优越性，具有很高的实用价值。引入新的检测框架也增强了方法的普适性。"}}
{"id": "2506.08746", "title": "Towards Secure and Private Language Models for Nuclear Power Plants", "authors": ["Muhammad Anwar", "Mishca de Costa", "Issam Hammad", "Daniel Lau"], "summary": "This paper introduces a domain-specific Large Language Model for nuclear\napplications, built from the publicly accessible Essential CANDU textbook.\nDrawing on a compact Transformer-based architecture, the model is trained on a\nsingle GPU to protect the sensitive data inherent in nuclear operations.\nDespite relying on a relatively small dataset, it shows encouraging signs of\ncapturing specialized nuclear vocabulary, though the generated text sometimes\nlacks syntactic coherence. By focusing exclusively on nuclear content, this\napproach demonstrates the feasibility of in-house LLM solutions that align with\nrigorous cybersecurity and data confidentiality standards. Early successes in\ntext generation underscore the model's utility for specialized tasks, while\nalso revealing the need for richer corpora, more sophisticated preprocessing,\nand instruction fine-tuning to enhance domain accuracy. Future directions\ninclude extending the dataset to cover diverse nuclear subtopics, refining\ntokenization to reduce noise, and systematically evaluating the model's\nreadiness for real-world applications in nuclear domain.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08746v1", "AI": {"title_translation": "核电厂安全和私有语言模型的探索", "tldr": "本文介绍了一种基于小型数据集和Transformer架构的核领域专用LLM，旨在保护敏感数据，并展示了其捕获专业词汇的潜力，但需改进文本连贯性和数据丰富性。", "motivation": "保护核操作中的敏感数据，探索在满足严格网络安全和数据保密标准下，开发核领域专用LLM的可行性。", "method": "构建了一个基于Transformer架构的核领域专用大型语言模型，该模型使用公开的“Essential CANDU”教材进行训练，并在一块GPU上完成训练，以保护敏感数据。", "result": "模型在捕获专业核词汇方面显示出令人鼓舞的迹象，尽管生成的文本有时缺乏句法连贯性。它证明了开发符合严格网络安全和数据保密标准的内部LLM解决方案的可行性。", "conclusion": "内部LLM解决方案在核领域专用任务中具有实用性，但需要更丰富的数据集、更复杂的预处理和指令微调来提高领域准确性。", "translation": "本文介绍了一种用于核应用的领域专用大型语言模型，该模型基于可公开获取的“Essential CANDU”教科书构建。该模型采用紧凑的基于Transformer的架构，在单块GPU上进行训练，以保护核操作中固有的敏感数据。尽管依赖相对较小的数据集，但它显示出捕获专业核词汇的令人鼓舞的迹象，尽管生成的文本有时缺乏句法连贯性。通过专注于核内容，这种方法证明了符合严格网络安全和数据保密标准的内部LLM解决方案的可行性。文本生成方面的早期成功突显了该模型在专业任务中的实用性，同时也揭示了需要更丰富语料库、更复杂的预处理和指令微调来提高领域准确性。未来的方向包括扩展数据集以涵盖不同的核子主题，完善分词以减少噪音，并系统地评估该模型在核领域实际应用中的准备情况。", "summary": "本文介绍了一种基于公开教材“Essential CANDU”构建的核领域专用大型语言模型。该模型采用紧凑的Transformer架构，在单GPU上训练，旨在保护核操作的敏感数据。尽管数据集较小，模型展现了学习专业核词汇的潜力，并验证了内部LLM解决方案在满足网络安全和数据保密标准方面的可行性。研究指出，未来需通过扩充语料、优化预处理和指令微调来提升模型性能。", "keywords": "领域专用语言模型, 核电厂, 数据安全, Transformer, 私有LLM", "comments": "这项研究的创新之处在于探索了在敏感领域（如核电厂）开发内部、安全且私有的LLM的可行性，这对于数据安全和主权至关重要。其重要性在于证明了即使在资源有限（单GPU、小数据集）的情况下，也能构建出具有专业领域知识的模型。局限性在于当前模型生成的文本连贯性不足，且需要更丰富的数据和更精细的训练方法来提高准确性。"}}
{"id": "2506.08757", "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL", "authors": ["Mishca de Costa", "Muhammad Anwar", "Dave Mercier", "Mark Randall", "Issam Hammad"], "summary": "Retrieving operational data from nuclear power plants requires exceptional\naccuracy and transparency due to the criticality of the decisions it supports.\nTraditionally, natural language to SQL (NL-to-SQL) approaches have been\nexplored for querying such data. While NL-to-SQL promises ease of use, it poses\nsignificant risks: end-users cannot easily validate generated SQL queries, and\nlegacy nuclear plant databases -- often complex and poorly structured --\ncomplicate query generation due to decades of incremental modifications. These\nchallenges increase the likelihood of inaccuracies and reduce trust in the\napproach. In this work, we propose an alternative paradigm: leveraging\nfunction-calling large language models (LLMs) to address these challenges.\nInstead of directly generating SQL queries, we define a set of pre-approved,\npurpose-specific functions representing common use cases. Queries are processed\nby invoking these functions, which encapsulate validated SQL logic. This hybrid\napproach mitigates the risks associated with direct NL-to-SQL translations by\nensuring that SQL queries are reviewed and optimized by experts before\ndeployment. While this strategy introduces the upfront cost of developing and\nmaintaining the function library, we demonstrate how NL-to-SQL tools can assist\nin the initial generation of function code, allowing experts to focus on\nvalidation rather than creation. Our study includes a performance comparison\nbetween direct NL-to-SQL generation and the proposed function-based approach,\nhighlighting improvements in accuracy and maintainability. This work\nunderscores the importance of balancing user accessibility with operational\nsafety and provides a novel, actionable framework for robust data retrieval in\ncritical systems.", "comment": "44th Annual CNS Conference and the 49th Annual CNS/CNA Student\n  Conference, Westin Harbour Castle Hotel, Toronto, ON, Canada, June 8-11, 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08757v1", "AI": {"title_translation": "提高核电厂数据检索的准确性和可维护性：一种超越NL-to-SQL的函数调用LLM方法", "tldr": "针对核电厂数据检索，本研究提出使用函数调用LLM而非NL-to-SQL，通过预定义函数封装SQL逻辑，以提高准确性和可维护性，降低风险。", "motivation": "核电厂运行数据检索对准确性和透明度要求极高。传统的自然语言到SQL (NL-to-SQL) 方法存在显著风险：最终用户难以验证生成的SQL查询，且复杂的遗留数据库使查询生成困难，易导致不准确性并降低信任度。", "method": "本研究提出一种替代范式，利用函数调用大型语言模型（LLMs）。该方法不直接生成SQL查询，而是定义一套预先批准的、特定用途的函数来代表常见用例。查询通过调用这些封装了经过验证的SQL逻辑的函数来处理。这种混合方法通过确保SQL查询在部署前经过专家审查和优化，从而减轻了直接NL-to-SQL翻译的风险。同时，探索了利用NL-to-SQL工具辅助函数代码的初始生成。", "result": "研究通过性能比较表明，所提出的基于函数的方法相对于直接NL-to-SQL生成，在数据检索的准确性和可维护性方面有所改进。", "conclusion": "这项工作强调了平衡用户可访问性与操作安全的重要性，并为关键系统中稳健的数据检索提供了一个新颖、可操作的框架。", "translation": "从核电厂检索运行数据需要极高的准确性和透明度，因为这支撑着关键决策。传统上，人们探索了自然语言到SQL (NL-to-SQL) 的方法来查询此类数据。虽然NL-to-SQL承诺易用性，但它带来了重大风险：最终用户无法轻易验证生成的SQL查询，而且核电厂的遗留数据库——通常复杂且结构不良——由于数十年的增量修改而使查询生成变得复杂。这些挑战增加了不准确的可能性并降低了对该方法的信任。在这项工作中，我们提出了一种替代范式：利用函数调用大型语言模型 (LLM) 来解决这些挑战。我们不直接生成SQL查询，而是定义一套预先批准的、特定用途的函数，代表常见的用例。通过调用这些封装了经过验证的SQL逻辑的函数来处理查询。这种混合方法通过确保SQL查询在部署前经过专家审查和优化，从而减轻了与直接NL-to-SQL翻译相关的风险。虽然这种策略引入了开发和维护函数库的前期成本，但我们展示了NL-to-SQL工具如何协助函数代码的初始生成，使专家能够专注于验证而不是创建。我们的研究包括了直接NL-to-SQL生成与所提出的基于函数的方法之间的性能比较，突出了在准确性和可维护性方面的改进。这项工作强调了平衡用户可访问性与操作安全的重要性，并为关键系统中稳健的数据检索提供了一个新颖、可操作的框架。", "summary": "鉴于核电厂数据检索对准确性和透明度的极高要求，本研究针对传统NL-to-SQL方法在复杂遗留数据库中存在的验证困难和不准确性风险，提出了一种基于函数调用大型语言模型（LLM）的新范式。该方法通过定义和调用封装了已验证SQL逻辑的预批准函数，避免了直接生成SQL，从而显著提高了数据检索的准确性和可维护性，同时兼顾了用户便利性和操作安全性，为关键系统提供了一种新的数据检索框架。", "keywords": "函数调用LLM, 核电厂数据检索, NL-to-SQL, 准确性, 可维护性", "comments": "该论文提出了一种创新的混合方法，将LLM的自然语言理解能力与预先验证的函数逻辑相结合，有效解决了在核电厂这类高风险、复杂遗留数据库环境中，传统NL-to-SQL方案所面临的准确性和可信度问题。其核心创新在于将SQL生成过程“黑箱化”为专家验证过的函数调用，从而大幅提升了安全性与可维护性。同时，论文也考虑到了函数库开发的前期成本，并提出了利用NL-to-SQL工具辅助函数代码生成的实用策略，这增强了其方案的可行性。"}}
{"id": "2506.08885", "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)", "authors": ["Danush Khanna", "Krishna Kumar", "Basab Ghosh", "Vinija Jain", "Vasu Sharma", "Aman Chadha", "Amitava Das"], "summary": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08885v1", "AI": {"title_translation": "对抗性攻击安全对齐 (ALKALI)：通过 GRACE（几何表示感知对比增强）保护大型语言模型——引入对抗性脆弱性质量指数 (AVQI)", "tldr": "当前LLM防御机制在对抗性攻击面前存在几何盲点。本文引入ALKALI基准测试揭示了高攻击成功率，并提出了GRACE框架通过潜在空间正则化降低攻击成功率，同时引入AVQI度量来量化潜在对齐失败。", "motivation": "大型语言模型（LLMs）面临的对抗性威胁日益加剧，而现有防御机制无法有效应对。研究发现，LLM的对齐存在一个关键的几何盲点，即对抗性提示能够伪装成安全表示，从而规避表面层面的防御，如直接偏好优化（DPO）。", "method": "本文引入了ALKALI，一个包含9,000个提示的对抗性基准测试，涵盖三个宏观类别、六个子类型和十五个攻击家族。为缓解潜在伪装漏洞，提出了GRACE（几何表示感知对比增强）对齐框架，该框架将偏好学习与潜在空间正则化相结合。GRACE强制执行两个约束：安全和对抗性完成之间的潜在分离，以及不安全和越狱行为之间的对抗性内聚。此外，引入了AVQI，一个几何感知度量，通过聚类分离和紧凑性量化潜在对齐失败。", "result": "对21个领先LLM的评估显示，无论是开源还是闭源模型，攻击成功率（ASR）都异常高，暴露了潜在伪装这一结构性盲点。GRACE框架在不修改基础模型的情况下，通过重塑内部几何，实现了高达39%的ASR降低。AVQI揭示了不安全完成何时模仿安全完成的几何结构，为模型内部编码安全的方式提供了原则性视角。", "conclusion": "LLMs在对抗性攻击面前存在潜在几何盲点，导致现有防御失效。ALKALI基准测试揭示了LLMs普遍存在的高攻击成功率。GRACE框架通过潜在空间正则化有效降低了攻击成功率，而AVQI度量则为理解和量化LLM内部安全编码提供了新工具。", "translation": "大型语言模型（LLMs）面临的对抗性威胁升级速度快于当前防御机制的适应能力。我们揭示了对齐中的一个关键几何盲点：对抗性提示利用潜在伪装，危险地接近安全表示流形，同时编码不安全意图，从而规避像直接偏好优化（DPO）等表面层防御，这些防御对潜在几何结构视而不见。我们引入了ALKALI，这是第一个经过严格策划的对抗性基准测试，也是迄今为止最全面的，涵盖了9,000个提示，分为三个宏观类别、六个子类型和十五个攻击家族。对21个领先LLM的评估显示，无论是开源还是闭源模型，攻击成功率（ASR）都异常高，暴露了我们称之为潜在伪装的潜在漏洞，这是一个结构性盲点，其中对抗性完成模仿安全完成的潜在几何结构。为了缓解这一漏洞，我们引入了GRACE——几何表示感知对比增强，这是一个将偏好学习与潜在空间正则化相结合的对齐框架。GRACE强制执行两个约束：安全和对抗性完成之间的潜在分离，以及不安全和越狱行为之间的对抗性内聚。这些操作通过学习到的注意力配置文件引导的层级池化嵌入进行，在不修改基础模型的情况下重塑内部几何结构，并实现了高达39%的ASR降低。此外，我们引入了AVQI，一个几何感知度量，通过聚类分离和紧凑性量化潜在对齐失败。AVQI揭示了不安全完成何时模仿安全完成的几何结构，为模型内部编码安全的方式提供了原则性视角。我们已将代码公开在https://anonymous.4open.science/r/alkali-B416/README.md。", "summary": "本文探讨了大型语言模型（LLMs）在对抗性攻击下存在的几何盲点，即对抗性提示通过潜在伪装规避现有防御。研究引入了迄今最全面的对抗性基准测试ALKALI，发现21个主流LLM的攻击成功率（ASR）普遍偏高。为解决这一问题，论文提出了GRACE框架，通过潜在空间正则化实现了高达39%的ASR降低。同时，引入了AVQI度量，用于量化和理解LLM内部的安全对齐失败。", "keywords": "LLM安全, 对抗性攻击, 几何盲点, 对齐, GRACE, AVQI", "comments": "本文揭示了LLM对齐中一个重要的“几何盲点”，指出了现有防御机制的不足，具有重要的理论和实践意义。ALKALI基准测试的构建非常全面，为后续研究提供了宝贵资源。GRACE框架通过不修改基础模型的方式进行内部几何重塑，是一种新颖且高效的防御策略。AVQI作为新的评估指标，为深入理解LLM内部安全编码提供了独特的视角，有助于推动LLM安全对齐研究的发展。"}}
{"id": "2506.08893", "title": "Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning", "authors": ["Kai Zhou", "Youbiao He", "Chong Zhong", "Yifu Wu"], "summary": "Despite high reliability, modern power systems with growing renewable\npenetration face an increasing risk of cascading outages. Real-time cascade\nmitigation requires fast, complex operational decisions under uncertainty. In\nthis work, we extend the influence graph into a Markov decision process model\n(MDP) for real-time mitigation of cascading outages in power transmission\nsystems, accounting for uncertainties in generation, load, and initial\ncontingencies. The MDP includes a do-nothing action to allow for conservative\ndecision-making and is solved using reinforcement learning. We present a policy\ngradient learning algorithm initialized with a policy corresponding to the\nunmitigated case and designed to handle invalid actions. The proposed learning\nmethod converges faster than the conventional algorithm. Through careful reward\ndesign, we learn a policy that takes conservative actions without deteriorating\nsystem conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus\nsystems. The results show that proactive line disconnections can effectively\nreduce cascading risk, and certain lines consistently emerge as critical in\nmitigating cascade propagation.", "comment": null, "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.08893v1", "AI": {"title_translation": "采用强化学习改进的影响图在电力系统中实时级联缓解", "tldr": "本文通过将影响图扩展为马尔可夫决策过程（MDP）模型，并利用强化学习来解决，从而实现了电力系统中的实时级联停电缓解，通过学习主动和保守的线路断开策略。", "motivation": "尽管可靠性高，但随着可再生能源渗透率的增长，现代电力系统面临着日益增加的级联停电风险。在不确定性下，实时级联缓解需要快速、复杂的运行决策。", "method": "本研究将影响图扩展为一个马尔可夫决策过程（MDP）模型，用于电力传输系统中的实时级联停电缓解，其中考虑了发电、负荷和初始偶发事件的不确定性。该MDP模型包含一个“不采取任何行动”的动作，以实现保守决策，并使用强化学习来求解。研究提出了一种策略梯度学习算法，该算法以未缓解情况下的策略进行初始化，并旨在处理无效动作。模型在IEEE 14总线和IEEE 118总线系统上进行了验证。", "result": "所提出的学习方法比传统算法收敛更快。通过精心设计的奖励机制，学到了一种在不恶化系统条件下采取保守行动的策略。结果表明，主动的线路断开可以有效降低级联风险，并且某些线路在缓解级联传播中始终表现出关键作用。", "conclusion": "主动的线路断开可以有效降低电力系统的级联风险，并且某些线路在缓解级联传播中始终是关键的。", "translation": "尽管可靠性高，但随着可再生能源渗透率的增长，现代电力系统面临着日益增加的级联停电风险。在不确定性下，实时级联缓解需要快速、复杂的运行决策。在这项工作中，我们将影响图扩展为马尔可夫决策过程模型（MDP），用于电力传输系统中的实时级联停电缓解，其中考虑了发电、负荷和初始偶发事件的不确定性。该MDP包含一个“不采取任何行动”的动作，以实现保守决策，并使用强化学习来求解。我们提出了一种策略梯度学习算法，该算法以未缓解情况下的策略进行初始化，并旨在处理无效动作。所提出的学习方法比传统算法收敛更快。通过精心设计的奖励，我们学习了一种在不恶化系统条件下采取保守行动的策略。该模型在IEEE 14总线和IEEE 118总线系统上进行了验证。结果表明，主动的线路断开可以有效降低级联风险，并且某些线路在缓解级联传播中始终表现出关键作用。", "summary": "本文提出了一种用于电力系统实时级联缓解的策略，通过将影响图扩展为马尔可夫决策过程（MDP）模型，并利用强化学习策略梯度算法进行求解。该方法考虑了不确定性，并学习了保守的行动策略。在IEEE总线系统上的验证表明，该方法收敛更快，通过主动断开线路有效降低了风险，并识别出级联缓解中的关键线路。", "keywords": "级联停电, 强化学习, 马尔可夫决策过程, 电力系统, 实时缓解", "comments": "本文创新性地将强化学习应用于电力系统可靠性中的一个关键问题。通过将影响图与MDP相结合，并使用精心设计的策略梯度算法，它解决了不确定性下实时决策的挑战。对保守行动和更快收敛的关注突出了其实用性。识别关键线路对电网运营商来说是宝贵的见解。"}}
{"id": "2506.08954", "title": "Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction", "authors": ["Ruben Weitzman", "Peter Mørch Groth", "Lood Van Niekerk", "Aoi Otani", "Yarin Gal", "Debora Marks", "Pascal Notin"], "summary": "Retrieving homologous protein sequences is essential for a broad range of\nprotein modeling tasks such as fitness prediction, protein design, structure\nmodeling, and protein-protein interactions. Traditional workflows have relied\non a two-step process: first retrieving homologs via Multiple Sequence\nAlignments (MSA), then training models on one or more of these alignments.\nHowever, MSA-based retrieval is computationally expensive, struggles with\nhighly divergent sequences or complex insertions & deletions patterns, and\noperates independently of the downstream modeling objective. We introduce\nProtriever, an end-to-end differentiable framework that learns to retrieve\nrelevant homologs while simultaneously training for the target task. When\napplied to protein fitness prediction, Protriever achieves state-of-the-art\nperformance compared to sequence-based models that rely on MSA-based homolog\nretrieval, while being two orders of magnitude faster through efficient vector\nsearch. Protriever is both architecture- and task-agnostic, and can flexibly\nadapt to different retrieval strategies and protein databases at inference time\n-- offering a scalable alternative to alignment-centric approaches.", "comment": "Accepted at ICML 2025", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.08954v1", "AI": {"title_translation": "Protriever：用于适应度预测的端到端可微分蛋白质同源性搜索", "tldr": "Protriever是一种用于蛋白质同源性搜索的端到端可微分框架，它克服了传统MSA方法的局限性，在蛋白质适应度预测方面实现了最先进的性能，并且速度快两个数量级。", "motivation": "传统的蛋白质同源序列检索方法（基于多序列比对MSA）计算成本高昂，难以处理高度分化的序列或复杂的插入/缺失模式，并且与下游建模目标独立，这限制了蛋白质建模任务（如适应度预测、蛋白质设计和结构建模）的效率和效果。", "method": "本文提出了Protriever，一个端到端可微分的框架，它能够在学习检索相关同源序列的同时，同步训练目标任务（例如蛋白质适应度预测）。该方法通过高效的向量搜索实现。", "result": "Protriever在蛋白质适应度预测方面达到了最先进的性能，优于依赖MSA同源序列检索的基于序列的模型。它通过高效的向量搜索，速度比现有方法快两个数量级。Protriever是架构和任务无关的，并且在推理时可以灵活适应不同的检索策略和蛋白质数据库。", "conclusion": "Protriever为蛋白质同源性搜索提供了一种可扩展的替代方案，克服了传统比对中心方法的局限性。它通过端到端可微分的学习方式，在蛋白质适应度预测上取得了最先进的性能，并显著提高了计算效率。", "translation": "检索同源蛋白质序列对于广泛的蛋白质建模任务至关重要，例如适应度预测、蛋白质设计、结构建模和蛋白质-蛋白质相互作用。传统的工作流程依赖于两步过程：首先通过多序列比对（MSA）检索同源序列，然后在一个或多个比对上训练模型。然而，基于MSA的检索计算成本高昂，难以处理高度分化的序列或复杂的插入和缺失模式，并且独立于下游建模目标。我们引入了Protriever，一个端到端可微分的框架，它在学习检索相关同源序列的同时，同步训练目标任务。当应用于蛋白质适应度预测时，与依赖基于MSA的同源序列检索的基于序列的模型相比，Protriever实现了最先进的性能，并通过高效的向量搜索，速度快两个数量级。Protriever是架构和任务无关的，并且在推理时可以灵活适应不同的检索策略和蛋白质数据库——为以比对为中心的方法提供了一种可扩展的替代方案。", "summary": "Protriever是一个创新的端到端可微分框架，旨在通过将同源性检索与目标任务训练相结合，解决传统多序列比对（MSA）方法在蛋白质同源性搜索中的效率和局限性问题。该框架在蛋白质适应度预测任务中表现出色，不仅实现了最先进的性能，而且通过高效的向量搜索，速度比现有方法快两个数量级，为蛋白质建模提供了一种可扩展且灵活的替代方案。", "keywords": "蛋白质同源性搜索, 适应度预测, 端到端可微分, 多序列比对, 向量搜索", "comments": "Protriever的创新之处在于其端到端可微分的设计，将同源性检索与下游任务（如适应度预测）的训练紧密结合，从而克服了传统MSA方法计算昂贵且与任务分离的弊端。其在性能上达到SOTA并实现两个数量级的速度提升，预示着其在蛋白质工程和计算生物学领域具有巨大的应用潜力，为更高效的蛋白质建模提供了新的范式。"}}
