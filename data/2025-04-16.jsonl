{"id": "2504.10504", "pdf": "https://arxiv.org/pdf/2504.10504", "abs": "https://arxiv.org/abs/2504.10504", "authors": ["Rita Sevastjanova", "Robin Gerling", "Thilo Spinner", "Mennatallah El-Assady"], "title": "LayerFlow: Layer-wise Exploration of LLM Embeddings using Uncertainty-aware Interlinked Projections", "categories": ["cs.CL", "cs.GR"], "comment": null, "summary": "Large language models (LLMs) represent words through contextual word\nembeddings encoding different language properties like semantics and syntax.\nUnderstanding these properties is crucial, especially for researchers\ninvestigating language model capabilities, employing embeddings for tasks\nrelated to text similarity, or evaluating the reasons behind token importance\nas measured through attribution methods. Applications for embedding exploration\nfrequently involve dimensionality reduction techniques, which reduce\nhigh-dimensional vectors to two dimensions used as coordinates in a\nscatterplot. This data transformation step introduces uncertainty that can be\npropagated to the visual representation and influence users' interpretation of\nthe data. To communicate such uncertainties, we present LayerFlow - a visual\nanalytics workspace that displays embeddings in an interlinked projection\ndesign and communicates the transformation, representation, and interpretation\nuncertainty. In particular, to hint at potential data distortions and\nuncertainties, the workspace includes several visual components, such as convex\nhulls showing 2D and HD clusters, data point pairwise distances, cluster\nsummaries, and projection quality metrics. We show the usability of the\npresented workspace through replication and expert case studies that highlight\nthe need to communicate uncertainty through multiple visual components and\ndifferent data perspectives."}
{"id": "2504.10615", "pdf": "https://arxiv.org/pdf/2504.10615", "abs": "https://arxiv.org/abs/2504.10615", "authors": ["Thilo Hagendorff", "Sarah Fabi"], "title": "Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can perform reasoning computations both\ninternally within their latent space and externally by generating explicit\ntoken sequences like chains of thought. Significant progress in enhancing\nreasoning abilities has been made by scaling test-time compute. However,\nunderstanding and quantifying model-internal reasoning abilities - the\ninferential \"leaps\" models make between individual token predictions - remains\ncrucial. This study introduces a benchmark (n = 4,000 items) designed to\nquantify model-internal reasoning in different domains. We achieve this by\nhaving LLMs indicate the correct solution to reasoning problems not through\ndescriptive text, but by selecting a specific language of their initial\nresponse token that is different from English, the benchmark language. This not\nonly requires models to reason beyond their context window, but also to\noverrise their default tendency to respond in the same language as the prompt,\nthereby posing an additional cognitive strain. We evaluate a set of 18 LLMs,\nshowing significant performance variations, with GPT-4.5 achieving the highest\naccuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B\n(65.6%). Control experiments and difficulty scaling analyses suggest that while\nLLMs engage in internal reasoning, we cannot rule out heuristic exploitations\nunder certain conditions, marking an area for future investigation. Our\nexperiments demonstrate that LLMs can \"think\" via latent-space computations,\nrevealing model-internal inference strategies that need further understanding,\nespecially regarding safety-related concerns such as covert planning,\ngoal-seeking, or deception emerging without explicit token traces."}
{"id": "2504.10637", "pdf": "https://arxiv.org/pdf/2504.10637", "abs": "https://arxiv.org/abs/2504.10637", "authors": ["Afra Amini", "Tim Vieira", "Ryan Cotterell"], "title": "Better Estimation of the KL Divergence Between Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Estimating the Kullback--Leibler (KL) divergence between language models has\nmany applications, e.g., reinforcement learning from human feedback (RLHF),\ninterpretability, and knowledge distillation. However, computing the exact KL\ndivergence between two arbitrary language models is intractable. Thus,\npractitioners often resort to the use of sampling-based estimators. While it is\neasy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased\nestimate of the KL divergence between language models, this estimator\nnotoriously suffers from high variance, and can even result in a negative\nestimate of the KL divergence, a non-negative quantity. In this paper, we\nintroduce a Rao--Blackwellized estimator that is also unbiased and provably has\nvariance less than or equal to that of the standard Monte Carlo estimator. In\nan empirical study on sentiment-controlled fine-tuning, we show that our\nestimator provides more stable KL estimates and reduces variance substantially\nin practice. Additionally, we derive an analogous Rao--Blackwellized estimator\nof the gradient of the KL divergence, which leads to more stable training and\nproduces models that more frequently appear on the Pareto frontier of reward\nvs. KL compared to the ones trained with the MC estimator of the gradient."}
{"id": "2504.10646", "pdf": "https://arxiv.org/pdf/2504.10646", "abs": "https://arxiv.org/abs/2504.10646", "authors": ["Saif Punjwani", "Larry Heck"], "title": "Weight-of-Thought Reasoning: Exploring Neural Network Weights for Enhanced LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities when prompted with strategies such as Chain-of-Thought (CoT).\nHowever, these approaches focus on token-level output without considering\ninternal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a\nnovel approach that examines neural network weights before inference to\nidentify reasoning pathways. Unlike existing methods, WoT explores the weight\nspace through graph-based message passing, multi-step reasoning processes, and\nattention mechanisms. Our implementation creates an interconnected graph of\nreasoning nodes. Experiments on diverse reasoning tasks (syllogistic,\nmathematical, algebraic, combinatorial, and geometric) demonstrate that WoT\nachieves superior performance compared to traditional methods, particularly for\ncomplex problems. This approach leads to both improved performance and greater\ninterpretability of the reasoning process, offering a promising direction for\nenhancing LLM reasoning capabilities."}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514", "abs": "https://arxiv.org/abs/2504.10514", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI."}
{"id": "2504.10647", "pdf": "https://arxiv.org/pdf/2504.10647", "abs": "https://arxiv.org/abs/2504.10647", "authors": ["Nafis Sadeq", "Xin Xu", "Zhouhang Xie", "Julian McAuley", "Byungkyu Kang", "Prarit Lamba", "Xiang Gao"], "title": "Improving In-Context Learning with Reasoning Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Language models rely on semantic priors to perform in-context learning, which\nleads to poor performance on tasks involving inductive reasoning.\nInstruction-tuning methods based on imitation learning can superficially\nenhance the in-context learning performance of language models, but they often\nfail to improve the model's understanding of the underlying rules that connect\ninputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning\ndistillation technique designed to improve the inductive reasoning capabilities\nof language models. Through a careful combination of data augmentation,\nfiltering, supervised fine-tuning, and alignment, ReDis achieves significant\nperformance improvements across a diverse range of tasks, including 1D-ARC,\nList Function, ACRE, and MiniSCAN. Experiments on three language model\nbackbones show that ReDis outperforms equivalent few-shot prompting baselines\nacross all tasks and even surpasses the teacher model, GPT-4o, in some cases.\nReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%,\n2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within\na similar hypothesis search space. The code, dataset, and model checkpoints\nwill be made available at\nhttps://github.com/NafisSadeq/reasoning-distillation.git."}
{"id": "2504.10558", "pdf": "https://arxiv.org/pdf/2504.10558", "abs": "https://arxiv.org/abs/2504.10558", "authors": ["Hu Gao", "Depeng Dang"], "title": "Enhancing Image Restoration through Learning Context-Rich and Detail-Accurate Features", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.08950", "summary": "Image restoration involves recovering high-quality images from their\ncorrupted versions, requiring a nuanced balance between spatial details and\ncontextual information. While certain methods address this balance, they\npredominantly emphasize spatial aspects, neglecting frequency variation\ncomprehension. In this paper, we present a multi-scale design that optimally\nbalances these competing objectives, seamlessly integrating spatial and\nfrequency domain knowledge to selectively recover the most informative\ninformation. Specifically, we develop a hybrid scale frequency selection block\n(HSFSBlock), which not only captures multi-scale information from the spatial\ndomain, but also selects the most informative components for image restoration\nin the frequency domain. Furthermore, to mitigate the inherent noise introduced\nby skip connections employing only addition or concatenation, we introduce a\nskip connection attention mechanism (SCAM) to selectively determines the\ninformation that should propagate through skip connections. The resulting\ntightly interlinked architecture, named as LCDNet. Extensive experiments\nconducted across diverse image restoration tasks showcase that our model\nattains performance levels that are either superior or comparable to those of\nstate-of-the-art algorithms."}
{"id": "2504.10660", "pdf": "https://arxiv.org/pdf/2504.10660", "abs": "https://arxiv.org/abs/2504.10660", "authors": ["Paul Rosu"], "title": "LITERA: An LLM Based Approach to Latin-to-English Translation", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL Findings", "summary": "This paper introduces an LLM-based Latin-to-English translation platform\ndesigned to address the challenges of translating Latin texts. We named the\nmodel LITERA, which stands for Latin Interpretation and Translations into\nEnglish for Research Assistance. Through a multi-layered translation process\nutilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an\nunprecedented level of accuracy, showcased by greatly improved BLEU scores,\nparticularly in classical Latin, along with improved BLEURT scores. The\ndevelopment of LITERA involved close collaboration with Duke University's\nClassical Studies Department, which was instrumental in creating a small,\nhigh-quality parallel Latin-English dataset. This paper details the\narchitecture, fine-tuning methodology, and prompting strategies used in LITERA,\nemphasizing its ability to produce literal translations."}
{"id": "2504.10563", "pdf": "https://arxiv.org/pdf/2504.10563", "abs": "https://arxiv.org/abs/2504.10563", "authors": ["Qikai Yang", "Cheng Ji", "Huaiying Luo", "Panfeng Li", "Zhicheng Ding"], "title": "Data Augmentation Through Random Style Replacement", "categories": ["cs.CV"], "comment": "Accepted by 2025 6th International Conference on Computer Vision,\n  Image and Deep Learning", "summary": "In this paper, we introduce a novel data augmentation technique that combines\nthe advantages of style augmentation and random erasing by selectively\nreplacing image subregions with style-transferred patches. Our approach first\napplies a random style transfer to training images, then randomly substitutes\nselected areas of these images with patches derived from the style-transferred\nversions. This method is able to seamlessly accommodate a wide range of\nexisting style transfer algorithms and can be readily integrated into diverse\ndata augmentation pipelines. By incorporating our strategy, the training\nprocess becomes more robust and less prone to overfitting. Comparative\nexperiments demonstrate that, relative to previous style augmentation methods,\nour technique achieves superior performance and faster convergence."}
{"id": "2504.10663", "pdf": "https://arxiv.org/pdf/2504.10663", "abs": "https://arxiv.org/abs/2504.10663", "authors": ["Mykola Trokhymovych", "Oleksandr Kosovan", "Nathan Forrester", "Pablo Aragón", "Diego Saez-Trumper", "Ricardo Baeza-Yates"], "title": "Characterizing Knowledge Manipulation in a Russian Wikipedia Fork", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Wikipedia is powered by MediaWiki, a free and open-source software that is\nalso the infrastructure for many other wiki-based online encyclopedias. These\ninclude the recently launched website Ruwiki, which has copied and modified the\noriginal Russian Wikipedia content to conform to Russian law. To identify\npractices and narratives that could be associated with different forms of\nknowledge manipulation, this article presents an in-depth analysis of this\nRussian Wikipedia fork. We propose a methodology to characterize the main\nchanges with respect to the original version. The foundation of this study is a\ncomprehensive comparative analysis of more than 1.9M articles from Russian\nWikipedia and its fork. Using meta-information and geographical, temporal,\ncategorical, and textual features, we explore the changes made by Ruwiki\neditors. Furthermore, we present a classification of the main topics of\nknowledge manipulation in this fork, including a numerical estimation of their\nscope. This research not only sheds light on significant changes within Ruwiki,\nbut also provides a methodology that could be applied to analyze other\nWikipedia forks and similar collaborative projects."}
{"id": "2504.10567", "pdf": "https://arxiv.org/pdf/2504.10567", "abs": "https://arxiv.org/abs/2504.10567", "authors": ["Yushu Wu", "Yanyu Li", "Ivan Skorokhodov", "Anil Kag", "Willi Menapace", "Sharath Girish", "Aliaksandr Siarohin", "Yanzhi Wang", "Sergey Tulyakov"], "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "8 pages, 4 figures, 6 tables", "summary": "Autoencoder (AE) is the key to the success of latent diffusion models for\nimage and video generation, reducing the denoising resolution and improving\nefficiency. However, the power of AE has long been underexplored in terms of\nnetwork design, compression ratio, and training strategy. In this work, we\nsystematically examine the architecture design choices and optimize the\ncomputation distribution to obtain a series of efficient and high-compression\nvideo AEs that can decode in real time on mobile devices. We also unify the\ndesign of plain Autoencoder and image-conditioned I2V VAE, achieving\nmultifunctionality in a single network. In addition, we find that the widely\nadopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no\nsignificant improvements when training AEs at scale. We propose a novel latent\nconsistency loss that does not require complicated discriminator design or\nhyperparameter tuning, but provides stable improvements in reconstruction\nquality. Our AE achieves an ultra-high compression ratio and real-time decoding\nspeed on mobile while outperforming prior art in terms of reconstruction\nmetrics by a large margin. We finally validate our AE by training a DiT on its\nlatent space and demonstrate fast, high-quality text-to-video generation\ncapability."}
{"id": "2504.10679", "pdf": "https://arxiv.org/pdf/2504.10679", "abs": "https://arxiv.org/abs/2504.10679", "authors": ["F. A. Rizvi", "T. Navojith", "A. M. N. H. Adhikari", "W. P. U. Senevirathna", "Dharshana Kasthurirathna", "Lakmini Abeywardhana"], "title": "Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 Pages, 2 figures, 7 Tables", "summary": "Brand reputation in the banking sector is maintained through insightful\nanalysis of customer opinion on code-mixed and multilingual content.\nConventional NLP models misclassify or ignore code-mixed text, when mix with\nlow resource languages such as Sinhala-English and fail to capture\ndomain-specific knowledge. This study introduces a hybrid NLP method to improve\nkeyword extraction, content filtering, and aspect-based classification of\nbanking content. Keyword extraction in English is performed with a hybrid\napproach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT\nembeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%.\nCode-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa\nmodel integrated with a domain-specific Sinhala financial vocabulary, and it\nresults in an accuracy of 87.4%. To ensure data quality, irrelevant comment\nfiltering was performed using several models, with the BERT-base-uncased model\nachieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better\nthan GPT-4o, SVM, and keyword-based filtering. Aspect classification followed\nthe same pattern, with the BERT-base-uncased model achieving 87.4% for English\nand XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based\napproaches. These findings confirm that fine-tuned transformer models\noutperform traditional methods in multilingual financial text analysis. The\npresent framework offers an accurate and scalable solution for brand reputation\nmonitoring in code-mixed and low-resource banking environments."}
{"id": "2504.10568", "pdf": "https://arxiv.org/pdf/2504.10568", "abs": "https://arxiv.org/abs/2504.10568", "authors": ["Aruna Gauba", "Irene Pi", "Yunze Man", "Ziqi Pang", "Vikram S. Adve", "Yu-Xiong Wang"], "title": "AgMMU: A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark", "categories": ["cs.CV"], "comment": "Project Website: https://agmmu.github.io/ Huggingface:\n  https://huggingface.co/datasets/AgMMU/AgMMU_v1/", "summary": "We curate a dataset AgMMU for evaluating and developing vision-language\nmodels (VLMs) to produce factually accurate answers for knowledge-intensive\nexpert domains. Our AgMMU concentrates on one of the most socially beneficial\ndomains, agriculture, which requires connecting detailed visual observation\nwith precise knowledge to diagnose, e.g., pest identification, management\ninstructions, etc. As a core uniqueness of our dataset, all facts, questions,\nand answers are extracted from 116,231 conversations between real-world users\nand authorized agricultural experts. After a three-step dataset curation\npipeline with GPT-4o, LLaMA models, and human verification, AgMMU features an\nevaluation set of 5,460 multiple-choice questions (MCQs) and open-ended\nquestions (OEQs). We also provide a development set that contains 205,399\npieces of agricultural knowledge information, including disease identification,\nsymptoms descriptions, management instructions, insect and pest identification,\nand species identification. As a multimodal factual dataset, it reveals that\nexisting VLMs face significant challenges with questions requiring both\ndetailed perception and factual knowledge. Moreover, open-source VLMs still\ndemonstrate a substantial performance gap compared to proprietary ones. To\nadvance knowledge-intensive VLMs, we conduct fine-tuning experiments using our\ndevelopment set, which improves LLaVA-1.5 evaluation accuracy by up to 3.1%. We\nhope that AgMMU can serve both as an evaluation benchmark dedicated to\nagriculture and a development suite for incorporating knowledge-intensive\nexpertise into general-purpose VLMs."}
{"id": "2504.10681", "pdf": "https://arxiv.org/pdf/2504.10681", "abs": "https://arxiv.org/abs/2504.10681", "authors": ["Soham Shah", "Kumar Shridhar", "Surojit Chatterjee", "Souvik Sen"], "title": "EMAFusion: A Self-Optimizing System for Seamless LLM Selection and Integration", "categories": ["cs.CL"], "comment": null, "summary": "While recent advances in large language models (LLMs) have significantly\nenhanced performance across diverse natural language tasks, the high\ncomputational and financial costs associated with their deployment remain\nsubstantial barriers. Existing routing strategies partially alleviate this\nchallenge by assigning queries to cheaper or specialized models, but they\nfrequently rely on extensive labeled data or fragile task-specific heuristics.\nConversely, fusion techniques aggregate multiple LLM outputs to boost accuracy\nand robustness, yet they often exacerbate cost and may reinforce shared biases.\n  We introduce EMAFusion, a new framework that self-optimizes for seamless LLM\nselection and reliable execution for a given query. Specifically, EMAFusion\nintegrates a taxonomy-based router for familiar query types, a learned router\nfor ambiguous inputs, and a cascading approach that progressively escalates\nfrom cheaper to more expensive models based on multi-judge confidence\nevaluations. Through extensive evaluations, we find EMAFusion outperforms the\nbest individual models by over 2.6 percentage points (94.3% vs. 91.7%), while\nbeing 4X cheaper than the average cost. EMAFusion further achieves a remarkable\n17.1 percentage point improvement over models like GPT-4 at less than 1/20th\nthe cost. Our combined routing approach delivers 94.3% accuracy compared to\ntaxonomy-based (88.1%) and learned model predictor-based (91.7%) methods alone,\ndemonstrating the effectiveness of our unified strategy. Finally, EMAFusion\nsupports flexible cost-accuracy trade-offs, allowing users to balance their\nbudgetary constraints and performance needs."}
{"id": "2504.10635", "pdf": "https://arxiv.org/pdf/2504.10635", "abs": "https://arxiv.org/abs/2504.10635", "authors": ["Chunzhuo Wang", "Zhewen Xue", "T. Sunil Kumar", "Guido Camps", "Hans Hallez", "Bart Vanrumste"], "title": "Skeleton-Based Intake Gesture Detection With Spatial-Temporal Graph Convolutional Networks", "categories": ["cs.CV"], "comment": "The manuscript has been accepted in 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (IEEE EMBC\n  2025)", "summary": "Overweight and obesity have emerged as widespread societal challenges,\nfrequently linked to unhealthy eating patterns. A promising approach to enhance\ndietary monitoring in everyday life involves automated detection of food intake\ngestures. This study introduces a skeleton based approach using a model that\ncombines a dilated spatial-temporal graph convolutional network (ST-GCN) with a\nbidirectional long-short-term memory (BiLSTM) framework, as called\nST-GCN-BiLSTM, to detect intake gestures. The skeleton-based method provides\nkey benefits, including environmental robustness, reduced data dependency, and\nenhanced privacy preservation. Two datasets were employed for model validation.\nThe OREBA dataset, which consists of laboratory-recorded videos, achieved\nsegmental F1-scores of 86.18% and 74.84% for identifying eating and drinking\ngestures. Additionally, a self-collected dataset using smartphone recordings in\nmore adaptable experimental conditions was evaluated with the model trained on\nOREBA, yielding F1-scores of 85.40% and 67.80% for detecting eating and\ndrinking gestures. The results not only confirm the feasibility of utilizing\nskeleton data for intake gesture detection but also highlight the robustness of\nthe proposed approach in cross-dataset validation."}
{"id": "2504.10724", "pdf": "https://arxiv.org/pdf/2504.10724", "abs": "https://arxiv.org/abs/2504.10724", "authors": ["Avinash Kumar", "Shashank Nag", "Jason Clemons", "Lizy John", "Poulami Das"], "title": "HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Deploying large language models (LLMs) presents critical challenges due to\nthe inherent trade-offs associated with key performance metrics, such as\nlatency, accuracy, and throughput. Typically, gains in one metric is\naccompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently\nnavigate this trade-off space by skipping some of the later model layers when\nit confidently finds an output token early, thus reducing latency without\nimpacting accuracy. However, as the early exits taken depend on the task and\nare unknown apriori to request processing, EE-LLMs conservatively load the\nentire model, limiting resource savings and throughput. Also, current\nframeworks statically select a model for a user task, limiting our ability to\nadapt to changing nature of the input queries.\n  We propose HELIOS to address these challenges. First, HELIOS shortlists a set\nof candidate LLMs, evaluates them using a subset of prompts, gathering\ntelemetry data in real-time. Second, HELIOS uses the early exit data from these\nevaluations to greedily load the selected model only up to a limited number of\nlayers. This approach yields memory savings which enables us to process more\nrequests at the same time, thereby improving throughput. Third, HELIOS monitors\nand periodically reassesses the performance of the candidate LLMs and if\nneeded, switches to another model that can service incoming queries more\nefficiently (such as using fewer layers without lowering accuracy). Our\nevaluations show that HELIOS achieves 1.48$\\times$ throughput, 1.10$\\times$\nenergy-efficiency, 1.39$\\times$ lower response time, and 3.7$\\times$\nimprovements in inference batch sizes compared to the baseline, when optimizing\nfor the respective service level objectives."}
{"id": "2504.10642", "pdf": "https://arxiv.org/pdf/2504.10642", "abs": "https://arxiv.org/abs/2504.10642", "authors": ["Tan-Hanh Pham", "Chris Ngo", "Trong-Duong Bui", "Minh Luu Quang", "Tan-Huong Pham", "Truong-Son Hy"], "title": "SilVar-Med: A Speech-Driven Visual Language Model for Explainable Abnormality Detection in Medical Imaging", "categories": ["cs.CV"], "comment": "CVPR Multimodal Algorithmic Reasoning Workshop 2025 - SilVarMed", "summary": "Medical Visual Language Models have shown great potential in various\nhealthcare applications, including medical image captioning and diagnostic\nassistance. However, most existing models rely on text-based instructions,\nlimiting their usability in real-world clinical environments especially in\nscenarios such as surgery, text-based interaction is often impractical for\nphysicians. In addition, current medical image analysis models typically lack\ncomprehensive reasoning behind their predictions, which reduces their\nreliability for clinical decision-making. Given that medical diagnosis errors\ncan have life-changing consequences, there is a critical need for interpretable\nand rational medical assistance. To address these challenges, we introduce an\nend-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image\nassistant that integrates speech interaction with VLMs, pioneering the task of\nvoice-based communication for medical image analysis. In addition, we focus on\nthe interpretation of the reasoning behind each prediction of medical\nabnormalities with a proposed reasoning dataset. Through extensive experiments,\nwe demonstrate a proof-of-concept study for reasoning-driven medical image\ninterpretation with end-to-end speech interaction. We believe this work will\nadvance the field of medical AI by fostering more transparent, interactive, and\nclinically viable diagnostic support systems. Our code and dataset are publicly\navailable at SiVar-Med."}
{"id": "2504.10768", "pdf": "https://arxiv.org/pdf/2504.10768", "abs": "https://arxiv.org/abs/2504.10768", "authors": ["Ralf Schmälzle", "Sue Lim", "Yuetong Du", "Gary Bente"], "title": "The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "This paper examines the thin-slicing approach - the ability to make accurate\njudgments based on minimal information - in the context of scientific\npresentations. Drawing on research from nonverbal communication and personality\npsychology, we show that brief excerpts (thin slices) reliably predict overall\npresentation quality. Using a novel corpus of over one hundred real-life\nscience talks, we employ Large Language Models (LLMs) to evaluate transcripts\nof full presentations and their thin slices. By correlating LLM-based\nevaluations of short excerpts with full-talk assessments, we determine how much\ninformation is needed for accurate predictions. Our results demonstrate that\nLLM-based evaluations align closely with human ratings, proving their validity,\nreliability, and efficiency. Critically, even very short excerpts (less than 10\npercent of a talk) strongly predict overall evaluations. This suggests that the\nfirst moments of a presentation convey relevant information that is used in\nquality evaluations and can shape lasting impressions. The findings are robust\nacross different LLMs and prompting strategies. This work extends thin-slicing\nresearch to public speaking and connects theories of impression formation to\nLLMs and current research on AI communication. We discuss implications for\ncommunication and social cognition research on message reception. Lastly, we\nsuggest an LLM-based thin-slicing framework as a scalable feedback tool to\nenhance human communication."}
{"id": "2504.10659", "pdf": "https://arxiv.org/pdf/2504.10659", "abs": "https://arxiv.org/abs/2504.10659", "authors": ["Zi-Han Jiang", "Chien-Wei Lin", "Wei-Hua Li", "Hsuan-Tung Liu", "Yi-Ren Yeh", "Chu-Song Chen"], "title": "Relation-Rich Visual Document Generator for Visual Information Extraction", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Despite advances in Large Language Models (LLMs) and Multimodal LLMs (MLLMs)\nfor visual document understanding (VDU), visual information extraction (VIE)\nfrom relation-rich documents remains challenging due to the layout diversity\nand limited training data. While existing synthetic document generators attempt\nto address data scarcity, they either rely on manually designed layouts and\ntemplates, or adopt rule-based approaches that limit layout diversity. Besides,\ncurrent layout generation methods focus solely on topological patterns without\nconsidering textual content, making them impractical for generating documents\nwith complex associations between the contents and layouts. In this paper, we\npropose a Relation-rIch visual Document GEnerator (RIDGE) that addresses these\nlimitations through a two-stage approach: (1) Content Generation, which\nleverages LLMs to generate document content using a carefully designed\nHierarchical Structure Text format which captures entity categories and\nrelationships, and (2) Content-driven Layout Generation, which learns to create\ndiverse, plausible document layouts solely from easily available Optical\nCharacter Recognition (OCR) results, requiring no human labeling or annotations\nefforts. Experimental results have demonstrated that our method significantly\nenhances the performance of document understanding models on various VIE\nbenchmarks. The code and model will be available at\nhttps://github.com/AI-Application-and-Integration-Lab/RIDGE ."}
{"id": "2504.10792", "pdf": "https://arxiv.org/pdf/2504.10792", "abs": "https://arxiv.org/abs/2504.10792", "authors": ["Jessica Lin", "Amir Zeldes"], "title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction."}
{"id": "2504.10669", "pdf": "https://arxiv.org/pdf/2504.10669", "abs": "https://arxiv.org/abs/2504.10669", "authors": ["Gokul Raju Govinda Raju", "Nikola Zubić", "Marco Cannici", "Davide Scaramuzza"], "title": "Perturbed State Space Feature Encoders for Optical Flow with Event Cameras", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 4 figures, 4 tables. Equal contribution by Gokul Raju\n  Govinda Raju and Nikola Zubi\\'c", "summary": "With their motion-responsive nature, event-based cameras offer significant\nadvantages over traditional cameras for optical flow estimation. While deep\nlearning has improved upon traditional methods, current neural networks adopted\nfor event-based optical flow still face temporal and spatial reasoning\nlimitations. We propose Perturbed State Space Feature Encoders (P-SSE) for\nmulti-frame optical flow with event cameras to address these challenges. P-SSE\nadaptively processes spatiotemporal features with a large receptive field akin\nto Transformer-based methods, while maintaining the linear computational\ncomplexity characteristic of SSMs. However, the key innovation that enables the\nstate-of-the-art performance of our model lies in our perturbation technique\napplied to the state dynamics matrix governing the SSM system. This approach\nsignificantly improves the stability and performance of our model. We integrate\nP-SSE into a framework that leverages bi-directional flows and recurrent\nconnections, expanding the temporal context of flow prediction. Evaluations on\nDSEC-Flow and MVSEC datasets showcase P-SSE's superiority, with 8.48% and\n11.86% improvements in EPE performance, respectively."}
{"id": "2504.10797", "pdf": "https://arxiv.org/pdf/2504.10797", "abs": "https://arxiv.org/abs/2504.10797", "authors": ["Annabella Sakunkoo", "Jonathan Sakunkoo"], "title": "Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies", "categories": ["cs.CL", "cs.AI", "cs.HC", "H.5; J.4"], "comment": null, "summary": "Across cultures, names tell a lot about their bearers as they carry deep\npersonal and cultural significance. Names also serve as powerful signals of\ngender, race, and status in the social hierarchy - a pecking order in which\nindividual positions shape others' expectations on their perceived competence\nand worth. With the widespread adoption of LLMs and as names are often an input\nfor LLMs, it is crucial to evaluate whether LLMs may sort people into status\npositions based on first and last names and, if so, whether it is in an unfair,\nbiased fashion. While prior work has primarily investigated biases in first\nnames, little attention has been paid to last names and even less to the\ncombined effects of first and last names. In this study, we conduct a\nlarge-scale analysis of name variations across 5 ethnicities to examine how AI\nexhibits name biases. Our study investigates three key characteristics of\ninequality and finds that LLMs reflect and reinforce status hierarchies based\non names that signal gender and ethnicity as they encode differential\nexpectations of competence, leadership, and economic potential. Contrary to the\ncommon assumption that AI tends to favor Whites, we show that East and, in some\ncontexts, South Asian names receive higher rankings. We also disaggregate\nAsians, a population projected to be the largest immigrant group in the U.S. by\n2055. Our results challenge the monolithic Asian model minority assumption,\nillustrating a more complex and stratified model of bias. Gender moderates\nbiases, with girls facing unfair disadvantages in certain racial groups.\nAdditionally, spanning cultural categories by adopting Western first names\nimproves AI-perceived status for East and Southeast Asian students,\nparticularly for girls. Our findings underscore the importance of\nintersectional and more nuanced understandings of race, gender, and mixed\nidentities in the evaluation of LLMs."}
{"id": "2504.10676", "pdf": "https://arxiv.org/pdf/2504.10676", "abs": "https://arxiv.org/abs/2504.10676", "authors": ["Zhanbo Huang", "Xiaoming Liu", "Yu Kong"], "title": "H-MoRe: Learning Human-centric Motion Representation for Action Analysis", "categories": ["cs.CV"], "comment": "15 pages, 14 figures, 7 tables, accepted to CVPR 2025 (Highlight)", "summary": "In this paper, we propose H-MoRe, a novel pipeline for learning precise\nhuman-centric motion representation. Our approach dynamically preserves\nrelevant human motion while filtering out background movement. Notably, unlike\nprevious methods relying on fully supervised learning from synthetic data,\nH-MoRe learns directly from real-world scenarios in a self-supervised manner,\nincorporating both human pose and body shape information. Inspired by\nkinematics, H-MoRe represents absolute and relative movements of each body\npoint in a matrix format that captures nuanced motion details, termed\nworld-local flows. H-MoRe offers refined insights into human motion, which can\nbe integrated seamlessly into various action-related applications. Experimental\nresults demonstrate that H-MoRe brings substantial improvements across various\ndownstream tasks, including gait recognition(CL@R1: +16.01%), action\nrecognition(Acc@1: +8.92%), and video generation(FVD: -67.07%). Additionally,\nH-MoRe exhibits high inference efficiency (34 fps), making it suitable for most\nreal-time scenarios. Models and code will be released upon publication."}
{"id": "2504.10823", "pdf": "https://arxiv.org/pdf/2504.10823", "abs": "https://arxiv.org/abs/2504.10823", "authors": ["Ayoung Lee", "Ryan Sungmo Kwon", "Peter Railton", "Lu Wang"], "title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Navigating high-stakes dilemmas involving conflicting values is challenging\neven for humans, let alone for AI. Yet prior work in evaluating the reasoning\ncapabilities of large language models (LLMs) in such situations has been\nlimited to everyday scenarios. To close this gap, this work first introduces\nCLASH (Character perspective-based LLM Assessments in Situations with\nHigh-stakes), a meticulously curated dataset consisting of 345 high-impact\ndilemmas along with 3,795 individual perspectives of diverse values. In\nparticular, we design CLASH in a way to support the study of critical aspects\nof value-based decision-making processes which are missing from prior work,\nincluding understanding decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in characters' perspectives. By\nbenchmarking 10 open and closed frontier models, we uncover several key\nfindings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,\nachieve less than 50% accuracy in identifying situations where the decision\nshould be ambivalent, while they perform significantly better in clear-cut\nscenarios. (2) While LLMs reasonably predict psychological discomfort as marked\nby human, they inadequately comprehend perspectives involving value shifts,\nindicating a need for LLMs to reason over complex values. (3) Our experiments\nalso reveal a significant correlation between LLMs' value preferences and their\nsteerability towards a given value. (4) Finally, LLMs exhibit greater\nsteerability when engaged in value reasoning from a third-party perspective,\ncompared to a first-person setup, though certain value pairs benefit uniquely\nfrom the first-person framing."}
{"id": "2504.10685", "pdf": "https://arxiv.org/pdf/2504.10685", "abs": "https://arxiv.org/abs/2504.10685", "authors": ["Yuqian Fu", "Xingyu Qiu", "Bin Ren", "Yanwei Fu", "Radu Timofte", "Nicu Sebe", "Ming-Hsuan Yang", "Luc Van Gool", "Kaijin Zhang", "Qingpeng Nong", "Xiugang Dong", "Hong Gao", "Xiangsheng Zhou", "Jiancheng Pan", "Yanxing Liu", "Xiao He", "Jiahao Li", "Yuze Sun", "Xiaomeng Huang", "Zhenyu Zhang", "Ran Ma", "Yuhan Liu", "Zijian Zhuang", "Shuai Yi", "Yixiong Zou", "Lingyi Hong", "Mingxi Chen", "Runze Li", "Xingdong Sheng", "Wenqiang Zhang", "Weisen Chen", "Yongxin Yan", "Xinguo Chen", "Yuanjie Shao", "Zhengrong Zuo", "Nong Sang", "Hao Wu", "Haoran Sun", "Shuming Hu", "Yan Zhang", "Zhiguang Shi", "Yu Zhang", "Chao Chen", "Tao Wang", "Da Feng", "Linhai Zhuo", "Ziming Lin", "Yali Huang", "Jie Me", "Yiming Yang", "Mi Guo", "Mingyuan Jiu", "Mingliang Xu", "Maomao Xiong", "Qunshu Zhang", "Xinyu Cao", "Yuqing Yang", "Dianmo Sheng", "Xuanpu Zhao", "Zhiyu Li", "Xuyang Ding", "Wenqian Li"], "title": "NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by CVPRW 25 @ NTIRE", "summary": "Cross-Domain Few-Shot Object Detection (CD-FSOD) poses significant challenges\nto existing object detection and few-shot detection models when applied across\ndomains. In conjunction with NTIRE 2025, we organized the 1st CD-FSOD\nChallenge, aiming to advance the performance of current object detectors on\nentirely novel target domains with only limited labeled data. The challenge\nattracted 152 registered participants, received submissions from 42 teams, and\nconcluded with 13 teams making valid final submissions. Participants approached\nthe task from diverse perspectives, proposing novel models that achieved new\nstate-of-the-art (SOTA) results under both open-source and closed-source\nsettings. In this report, we present an overview of the 1st NTIRE 2025 CD-FSOD\nChallenge, highlighting the proposed solutions and summarizing the results\nsubmitted by the participants."}
{"id": "2504.10845", "pdf": "https://arxiv.org/pdf/2504.10845", "abs": "https://arxiv.org/abs/2504.10845", "authors": ["Phill Kyu Rhee"], "title": "Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive Language Generators", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "Large Language Models (LLMs), powered by Transformers, have demonstrated\nhuman-like intelligence capabilities, yet their underlying mechanisms remain\npoorly understood. This paper presents a novel framework for interpreting LLMs\nas probabilistic left context-sensitive languages (CSLs) generators. We\nhypothesize that Transformers can be effectively decomposed into three\nfundamental components: context windows, attention mechanisms, and\nautoregressive generation frameworks. This decomposition allows for the\ndevelopment of more flexible and interpretable computational models, moving\nbeyond the traditional view of attention and autoregression as inseparable\nprocesses. We argue that next-token predictions can be understood as\nprobabilistic, dynamic approximations of left CSL production rules, providing\nan intuitive explanation for how simple token predictions can yield human-like\nintelligence outputs. Given that all CSLs are left context-sensitive\n(Penttonen, 1974), we conclude that Transformers stochastically approximate\nCSLs, which are widely recognized as models of human-like intelligence. This\ninterpretation bridges the gap between Formal Language Theory and the observed\ngenerative power of Transformers, laying a foundation for future advancements\nin generative AI theory and applications. Our novel perspective on Transformer\narchitectures will foster a deeper understanding of LLMs and their future\npotentials."}
{"id": "2504.10686", "pdf": "https://arxiv.org/pdf/2504.10686", "abs": "https://arxiv.org/abs/2504.10686", "authors": ["Bin Ren", "Hang Guo", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yawei Li", "Yao Zhang", "Xinning Chai", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Li Song", "Hongyuan Yu", "Pufan Xu", "Cheng Wan", "Zhijuan Huang", "Peng Guo", "Shuyuan Cui", "Chenjun Li", "Xuehai Hu", "Pan Pan", "Xin Zhang", "Heng Zhang", "Qing Luo", "Linyan Jiang", "Haibo Lei", "Qifang Gao", "Yaqing Li", "Weihua Luo", "Tsing Li", "Qing Wang", "Yi Liu", "Yang Wang", "Hongyu An", "Liou Zhang", "Shijie Zhao", "Lianhong Song", "Long Sun", "Jinshan Pan", "Jiangxin Dong", "Jinhui Tang", "Jing Wei", "Mengyang Wang", "Ruilong Guo", "Qian Wang", "Qingliang Liu", "Yang Cheng", "Davinci", "Enxuan Gu", "Pinxin Liu", "Yongsheng Yu", "Hang Hua", "Yunlong Tang", "Shihao Wang", "Yukun Yang", "Zhiyu Zhang", "Yukun Yang", "Jiyu Wu", "Jiancheng Huang", "Yifan Liu", "Yi Huang", "Shifeng Chen", "Rui Chen", "Yi Feng", "Mingxi Li", "Cailu Wan", "Xiangji Wu", "Zibin Liu", "Jinyang Zhong", "Kihwan Yoon", "Ganzorig Gankhuyag", "Shengyun Zhong", "Mingyang Wu", "Renjie Li", "Yushen Zuo", "Zhengzhong Tu", "Zongang Gao", "Guannan Chen", "Yuan Tian", "Wenhui Chen", "Weijun Yuan", "Zhan Li", "Yihang Chen", "Yifan Deng", "Ruting Deng", "Yilin Zhang", "Huan Zheng", "Yanyan Wei", "Wenxuan Zhao", "Suiyi Zhao", "Fei Wang", "Kun Li", "Yinggan Tang", "Mengjie Su", "Jae-hyeon Lee", "Dong-Hyeop Son", "Ui-Jin Choi", "Tiancheng Shao", "Yuqing Zhang", "Mengcheng Ma", "Donggeun Ko", "Youngsang Kwak", "Jiun Lee", "Jaehwa Kwak", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Jing Hu", "Hui Deng", "Xuan Zhang", "Lin Zhu", "Qinrui Fan", "Weijian Deng", "Junnan Wu", "Wenqin Deng", "Yuquan Liu", "Zhaohong Xu", "Jameer Babu Pinjari", "Kuldeep Purohit", "Zeyu Xiao", "Zhuoyuan Li", "Surya Vashisth", "Akshay Dudhane", "Praful Hambarde", "Sachin Chaudhary", "Satya Naryan Tazi", "Prashant Patil", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Wei-Chen Shen", "I-Hsiang Chen", "Yunzhe Xu", "Chen Zhao", "Zhizhou Chen", "Akram Khatami-Rizi", "Ahmad Mahmoudi-Aznaveh", "Alejandro Merino", "Bruno Longarela", "Javier Abad", "Marcos V. Conde", "Simone Bianco", "Luca Cogo", "Gianmarco Corti"], "title": "The Tenth NTIRE 2025 Efficient Super-Resolution Challenge Report", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by CVPR2025 NTIRE Workshop, Efficient Super-Resolution\n  Challenge Report. 50 pages", "summary": "This paper presents a comprehensive review of the NTIRE 2025 Challenge on\nSingle-Image Efficient Super-Resolution (ESR). The challenge aimed to advance\nthe development of deep models that optimize key computational metrics, i.e.,\nruntime, parameters, and FLOPs, while achieving a PSNR of at least 26.90 dB on\nthe $\\operatorname{DIV2K\\_LSDIR\\_valid}$ dataset and 26.99 dB on the\n$\\operatorname{DIV2K\\_LSDIR\\_test}$ dataset. A robust participation saw\n\\textbf{244} registered entrants, with \\textbf{43} teams submitting valid\nentries. This report meticulously analyzes these methods and results,\nemphasizing groundbreaking advancements in state-of-the-art single-image ESR\ntechniques. The analysis highlights innovative approaches and establishes\nbenchmarks for future research in the field."}
{"id": "2504.10861", "pdf": "https://arxiv.org/pdf/2504.10861", "abs": "https://arxiv.org/abs/2504.10861", "authors": ["Amanpreet Singh", "Joseph Chee Chang", "Chloe Anastasiades", "Dany Haddad", "Aakanksha Naik", "Amber Tanaka", "Angele Zamarron", "Cecile Nguyen", "Jena D. Hwang", "Jason Dunkleberger", "Matt Latzke", "Smita Rao", "Jaron Lochner", "Rob Evans", "Rodney Kinney", "Daniel S. Weld", "Doug Downey", "Sergey Feldman"], "title": "Ai2 Scholar QA: Organized Literature Synthesis with Attribution", "categories": ["cs.CL"], "comment": "7 pages", "summary": "Retrieval-augmented generation is increasingly effective in answering\nscientific questions from literature, but many state-of-the-art systems are\nexpensive and closed-source. We introduce Ai2 Scholar QA, a free online\nscientific question answering application. To facilitate research, we make our\nentire pipeline public: as a customizable open-source Python package and\ninteractive web app, along with paper indexes accessible through public APIs\nand downloadable datasets. We describe our system in detail and present\nexperiments analyzing its key design decisions. In an evaluation on a recent\nscientific QA benchmark, we find that Ai2 Scholar QA outperforms competing\nsystems."}
{"id": "2504.10716", "pdf": "https://arxiv.org/pdf/2504.10716", "abs": "https://arxiv.org/abs/2504.10716", "authors": ["Stathis Galanakis", "Alexandros Lattas", "Stylianos Moschoglou", "Bernhard Kainz", "Stefanos Zafeiriou"], "title": "SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in diffusion models, generating realistic head\nportraits from novel viewpoints remains a significant challenge. Most current\napproaches are constrained to limited angular ranges, predominantly focusing on\nfrontal or near-frontal views. Moreover, although the recent emerging\nlarge-scale diffusion models have been proven robust in handling 3D scenes,\nthey underperform on facial data, given their complex structure and the uncanny\nvalley pitfalls. In this paper, we propose SpinMeRound, a diffusion-based\napproach designed to generate consistent and accurate head portraits from novel\nviewpoints. By leveraging a number of input views alongside an identity\nembedding, our method effectively synthesizes diverse viewpoints of a subject\nwhilst robustly maintaining its unique identity features. Through\nexperimentation, we showcase our model's generation capabilities in 360 head\nsynthesis, while beating current state-of-the-art multiview diffusion models."}
{"id": "2504.10903", "pdf": "https://arxiv.org/pdf/2504.10903", "abs": "https://arxiv.org/abs/2504.10903", "authors": ["Sicheng Feng", "Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Efficient Reasoning Models: A Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning models have demonstrated remarkable progress in solving complex and\nlogic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to\narriving at a final answer. Yet, the emergence of this \"slow-thinking\"\nparadigm, with numerous tokens generated in sequence, inevitably introduces\nsubstantial computational overhead. To this end, it highlights an urgent need\nfor effective acceleration. This survey aims to provide a comprehensive\noverview of recent advances in efficient reasoning. It categorizes existing\nworks into three key directions: (1) shorter - compressing lengthy CoTs into\nconcise yet effective reasoning chains; (2) smaller - developing compact\nlanguage models with strong reasoning capabilities through techniques such as\nknowledge distillation, other model compression techniques, and reinforcement\nlearning; and (3) faster - designing efficient decoding strategies to\naccelerate inference. A curated collection of papers discussed in this survey\nis available in our GitHub repository."}
{"id": "2504.10727", "pdf": "https://arxiv.org/pdf/2504.10727", "abs": "https://arxiv.org/abs/2504.10727", "authors": ["Darryl Hannan", "John Cooper", "Dylan White", "Timothy Doster", "Henry Kvinge", "Yijing Watkins"], "title": "Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization", "categories": ["cs.CV"], "comment": "26 pages, CVPR MORSE Workshop 2025", "summary": "Multimodal large language models (MLLMs) have altered the landscape of\ncomputer vision, obtaining impressive results across a wide range of tasks,\nespecially in zero-shot settings. Unfortunately, their strong performance does\nnot always transfer to out-of-distribution domains, such as earth observation\n(EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks,\nsuch as image captioning and scene understanding, while failing at tasks that\nrequire more fine-grained spatial reasoning, such as object localization.\nHowever, MLLMs are advancing rapidly and insights quickly become out-dated. In\nthis work, we analyze more recent MLLMs that have been explicitly trained to\ninclude fine-grained spatial reasoning capabilities, benchmarking them on EO\nobject localization tasks. We demonstrate that these models are performant in\ncertain settings, making them well suited for zero-shot scenarios.\nAdditionally, we provide a detailed discussion focused on prompt selection,\nground sample distance (GSD) optimization, and analyzing failure cases. We hope\nthat this work will prove valuable as others evaluate whether an MLLM is well\nsuited for a given EO localization task and how to optimize it."}
{"id": "2504.10906", "pdf": "https://arxiv.org/pdf/2504.10906", "abs": "https://arxiv.org/abs/2504.10906", "authors": ["Changjiang Gao", "Hankun Lin", "Shujian Huang", "Xin Huang", "Xue Han", "Junlan Feng", "Chao Deng", "Jiajun Chen"], "title": "Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From", "categories": ["cs.CL"], "comment": null, "summary": "The ability of cross-lingual context retrieval is a fundamental aspect of\ncross-lingual alignment of large language models (LLMs), where the model\nextracts context information in one language based on requests in another\nlanguage. Despite its importance in real-life applications, this ability has\nnot been adequately investigated for state-of-the-art models. In this paper, we\nevaluate the cross-lingual context retrieval ability of over 40 LLMs across 12\nlanguages to understand the source of this ability, using cross-lingual machine\nreading comprehension (xMRC) as a representative scenario. Our results show\nthat several small, post-trained open LLMs show strong cross-lingual context\nretrieval ability, comparable to closed-source LLMs such as GPT-4o, and their\nestimated oracle performances greatly improve after post-training. Our\ninterpretability analysis shows that the cross-lingual context retrieval\nprocess can be divided into two main phases: question encoding and answer\nretrieval, which are formed in pre-training and post-training, respectively.\nThe phasing stability correlates with xMRC performance, and the xMRC bottleneck\nlies at the last model layers in the second phase, where the effect of\npost-training can be evidently observed. Our results also indicate that\nlarger-scale pretraining cannot improve the xMRC performance. Instead, larger\nLLMs need further multilingual post-training to fully unlock their\ncross-lingual context retrieval potential. Our code and is available at\nhttps://github.com/NJUNLP/Cross-Lingual-Context-Retrieval"}
{"id": "2504.10738", "pdf": "https://arxiv.org/pdf/2504.10738", "abs": "https://arxiv.org/abs/2504.10738", "authors": ["Ankit Kumar Shaw", "Kun Jiang", "Tuopu Wen", "Chandan Kumar Sah", "Yining Shi", "Mengmeng Yang", "Diange Yang", "Xiaoli Lian"], "title": "CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "I.2.9; I.2.7; I.2.10; I.5.5; I.5.4; I.2.11"], "comment": "Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author.\n  The main paper and supplementary material are both included here, total 23\n  pages (main paper is 10 pages and supplementary material is 13 pages), total\n  17 figures (6 figures in main paper and 11 figures in supplementary\n  material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code\n  will be available at https://Ankit-Zefan.github.io/CleanMap/", "summary": "The rapid growth of intelligent connected vehicles (ICVs) and integrated\nvehicle-road-cloud systems has increased the demand for accurate, real-time HD\nmap updates. However, ensuring map reliability remains challenging due to\ninconsistencies in crowdsourced data, which suffer from motion blur, lighting\nvariations, adverse weather, and lane marking degradation. This paper\nintroduces CleanMAP, a Multimodal Large Language Model (MLLM)-based\ndistillation framework designed to filter and refine crowdsourced data for\nhigh-confidence HD map updates. CleanMAP leverages an MLLM-driven lane\nvisibility scoring model that systematically quantifies key visual parameters,\nassigning confidence scores (0-10) based on their impact on lane detection. A\nnovel dynamic piecewise confidence-scoring function adapts scores based on lane\nvisibility, ensuring strong alignment with human evaluations while effectively\nfiltering unreliable data. To further optimize map accuracy, a\nconfidence-driven local map fusion strategy ranks and selects the top-k\nhighest-scoring local maps within an optimal confidence range (best score minus\n10%), striking a balance between data quality and quantity. Experimental\nevaluations on a real-world autonomous vehicle dataset validate CleanMAP's\neffectiveness, demonstrating that fusing the top three local maps achieves the\nlowest mean map update error of 0.28m, outperforming the baseline (0.37m) and\nmeeting stringent accuracy thresholds (<= 0.32m). Further validation with\nreal-vehicle data confirms 84.88% alignment with human evaluators, reinforcing\nthe model's robustness and reliability. This work establishes CleanMAP as a\nscalable and deployable solution for crowdsourced HD map updates, ensuring more\nprecise and reliable autonomous navigation. The code will be available at\nhttps://Ankit-Zefan.github.io/CleanMap/"}
{"id": "2504.10982", "pdf": "https://arxiv.org/pdf/2504.10982", "abs": "https://arxiv.org/abs/2504.10982", "authors": ["Yingjian Chen", "Feiyang Li", "Xingyu Song", "Tianxiao Li", "Issey Sudeka", "Irene Li"], "title": "Exploring the Role of KG-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages."}
{"id": "2504.10746", "pdf": "https://arxiv.org/pdf/2504.10746", "abs": "https://arxiv.org/abs/2504.10746", "authors": ["Xiulong Liu", "Anurag Kumar", "Paul Calamia", "Sebastia V. Amengual", "Calvin Murdock", "Ishwarya Ananthabhotla", "Philip Robinson", "Eli Shlizerman", "Vamsi Krishna Ithapu", "Ruohan Gao"], "title": "Hearing Anywhere in Any Environment", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "In mixed reality applications, a realistic acoustic experience in spatial\nenvironments is as crucial as the visual experience for achieving true\nimmersion. Despite recent advances in neural approaches for Room Impulse\nResponse (RIR) estimation, most existing methods are limited to the single\nenvironment on which they are trained, lacking the ability to generalize to new\nrooms with different geometries and surface materials. We aim to develop a\nunified model capable of reconstructing the spatial acoustic experience of any\nenvironment with minimum additional measurements. To this end, we present xRIR,\na framework for cross-room RIR prediction. The core of our generalizable\napproach lies in combining a geometric feature extractor, which captures\nspatial context from panorama depth images, with a RIR encoder that extracts\ndetailed acoustic features from only a few reference RIR samples. To evaluate\nour method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity\nsimulation of over 300,000 RIRs from 260 rooms. Experiments show that our\nmethod strongly outperforms a series of baselines. Furthermore, we successfully\nperform sim-to-real transfer by evaluating our model on four real-world\nenvironments, demonstrating the generalizability of our approach and the\nrealism of our dataset."}
{"id": "2504.11001", "pdf": "https://arxiv.org/pdf/2504.11001", "abs": "https://arxiv.org/abs/2504.11001", "authors": ["Alan Dao", "Thinh Le"], "title": "ReZero: Enhancing LLM search ability by trying one-more-time", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient."}
{"id": "2504.10750", "pdf": "https://arxiv.org/pdf/2504.10750", "abs": "https://arxiv.org/abs/2504.10750", "authors": ["Michele Grimaldi", "Nouf Alkaabi", "Francesco Ruscio", "Sebastian Realpe Rua", "Rafael Garcia", "Nuno Gracias"], "title": "Real-time Seafloor Segmentation and Mapping", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Posidonia oceanica meadows are a species of seagrass highly dependent on\nrocks for their survival and conservation. In recent years, there has been a\nconcerning global decline in this species, emphasizing the critical need for\nefficient monitoring and assessment tools. While deep learning-based semantic\nsegmentation and visual automated monitoring systems have shown promise in a\nvariety of applications, their performance in underwater environments remains\nchallenging due to complex water conditions and limited datasets. This paper\nintroduces a framework that combines machine learning and computer vision\ntechniques to enable an autonomous underwater vehicle (AUV) to inspect the\nboundaries of Posidonia oceanica meadows autonomously. The framework\nincorporates an image segmentation module using an existing Mask R-CNN model\nand a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a\nnew class dedicated to rocks is introduced to enhance the existing model,\naiming to contribute to a comprehensive monitoring approach and provide a\ndeeper understanding of the intricate interactions between the meadow and its\nsurrounding environment. The image segmentation model is validated using real\nunderwater images, while the overall inspection framework is evaluated in a\nrealistic simulation environment, replicating actual monitoring scenarios with\nreal underwater images. The results demonstrate that the proposed framework\nenables the AUV to autonomously accomplish the main tasks of underwater\ninspection and segmentation of rocks. Consequently, this work holds significant\npotential for the conservation and protection of marine environments, providing\nvaluable insights into the status of Posidonia oceanica meadows and supporting\ntargeted preservation efforts"}
{"id": "2504.11004", "pdf": "https://arxiv.org/pdf/2504.11004", "abs": "https://arxiv.org/abs/2504.11004", "authors": ["Jinwu Hu", "Wei Zhang", "Yufeng Wang", "Yu Hu", "Bin Xiao", "Mingkui Tan", "Qing Du"], "title": "Dynamic Compressing Prompts for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Under review (submited in 2024.11)", "summary": "Large Language Models (LLMs) have shown outstanding performance across a\nvariety of tasks, partly due to advanced prompting techniques. However, these\ntechniques often require lengthy prompts, which increase computational costs\nand can hinder performance because of the limited context windows of LLMs.\nWhile prompt compression is a straightforward solution, existing methods\nconfront the challenges of retaining essential information, adapting to context\nchanges, and remaining effective across different tasks. To tackle these\nissues, we propose a task-agnostic method called Dynamic Compressing Prompts\n(LLM-DCP). Our method reduces the number of prompt tokens while aiming to\npreserve the performance as much as possible. We model prompt compression as a\nMarkov Decision Process (MDP), enabling the DCP-Agent to sequentially remove\nredundant tokens by adapting to dynamic contexts and retaining crucial content.\nWe develop a reward function for training the DCP-Agent that balances the\ncompression rate, the quality of the LLM output, and the retention of key\ninformation. This allows for prompt token reduction without needing an external\nblack-box LLM. Inspired by the progressive difficulty adjustment in curriculum\nlearning, we introduce a Hierarchical Prompt Compression (HPC) training\nstrategy that gradually increases the compression difficulty, enabling the\nDCP-Agent to learn an effective compression method that maintains information\nintegrity. Experiments demonstrate that our method outperforms state-of-the-art\ntechniques, especially at higher compression rates. The code for our approach\nwill be available at https://github.com/Fhujinwu/DCP."}
{"id": "2504.10757", "pdf": "https://arxiv.org/pdf/2504.10757", "abs": "https://arxiv.org/abs/2504.10757", "authors": ["Amirhosein Chahe", "Lifeng Zhou"], "title": "ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Vision-language models (VLMs) show promise for autonomous driving but often\nlack transparent reasoning capabilities that are critical for safety. We\ninvestigate whether explicitly modeling reasoning during fine-tuning enhances\nVLM performance on driving decision tasks. Using GPT-4o, we generate structured\nreasoning chains for driving scenarios from the DriveLM benchmark with\ncategory-specific prompting strategies. We compare reasoning-based fine-tuning,\nanswer-only fine-tuning, and baseline instruction-tuned models across multiple\nsmall VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results\ndemonstrate that reasoning-based fine-tuning consistently outperforms\nalternatives, with Llama3.2-11B-reason achieving the highest performance.\nModels fine-tuned with reasoning show substantial improvements in accuracy and\ntext generation quality, suggesting explicit reasoning enhances internal\nrepresentations for driving decisions. These findings highlight the importance\nof transparent decision processes in safety-critical domains and offer a\npromising direction for developing more interpretable autonomous driving\nsystems."}
{"id": "2504.11042", "pdf": "https://arxiv.org/pdf/2504.11042", "abs": "https://arxiv.org/abs/2504.11042", "authors": ["Sukannya Purkayastha", "Zhuang Li", "Anne Lauscher", "Lizhen Qu", "Iryna Gurevych"], "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews", "categories": ["cs.CL"], "comment": "29 pages, 18 Figures, 15 Tables", "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)"}
{"id": "2504.10764", "pdf": "https://arxiv.org/pdf/2504.10764", "abs": "https://arxiv.org/abs/2504.10764", "authors": ["Jostan Brown", "Cindy Grimm", "Joseph R. Davidson"], "title": "SeeTree -- A modular, open-source system for tree detection and orchard localization", "categories": ["cs.CV", "cs.RO"], "comment": "26 pages, 12 figures", "summary": "Accurate localization is an important functional requirement for precision\norchard management. However, there are few off-the-shelf commercial solutions\navailable to growers. In this paper, we present SeeTree, a modular, open source\nembedded system for tree trunk detection and orchard localization that is\ndeployable on any vehicle. Building on our prior work on vision-based in-row\nlocalization using particle filters, SeeTree includes several new capabilities.\nFirst, it provides capacity for full orchard localization including out-of-row\nheadland turning. Second, it includes the flexibility to integrate either\nvisual, GNSS, or wheel odometry in the motion model. During field experiments\nin a commercial orchard, the system converged to the correct location 99% of\nthe time over 800 trials, even when starting with large uncertainty in the\ninitial particle locations. When turning out of row, the system correctly\ntracked 99% of the turns (860 trials representing 43 unique row changes). To\nhelp support adoption and future research and development, we make our dataset,\ndesign files, and source code freely available to the community."}
{"id": "2504.11082", "pdf": "https://arxiv.org/pdf/2504.11082", "abs": "https://arxiv.org/abs/2504.11082", "authors": ["Efthymios Georgiou", "Vassilis Katsouros", "Yannis Avrithis", "Alexandros Potamianos"], "title": "DeepMLF: Multimodal language model with learnable tokens for deep fusion in sentiment analysis", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization."}
{"id": "2504.10765", "pdf": "https://arxiv.org/pdf/2504.10765", "abs": "https://arxiv.org/abs/2504.10765", "authors": ["Jeremy Klotz", "Shree K. Nayar"], "title": "Minimal Sensing for Orienting a Solar Panel", "categories": ["cs.CV"], "comment": "12 pages, 9 figures", "summary": "A solar panel harvests the most energy when pointing in the direction that\nmaximizes the total illumination (irradiance) falling on it. Given an arbitrary\norientation of a panel and an arbitrary environmental illumination, we address\nthe problem of finding the direction of maximum total irradiance. We develop a\nminimal sensing approach where measurements from just four photodetectors are\nused to iteratively vary the tilt of the panel to maximize the irradiance. Many\nenvironments produce irradiance functions with multiple local maxima. As a\nresult, simply measuring the gradient of the irradiance function and applying\ngradient ascent will not work. We show that a larger, optimized tilt between\nthe detectors and the panel is equivalent to blurring the irradiance function.\nThis has the effect of eliminating local maxima and turning the irradiance\nfunction into a unimodal one, whose maximum can be found using gradient ascent.\nWe show that there is a close relationship between our approach and scale space\ntheory. We have collected a large dataset of high-dynamic range lighting\nenvironments in New York City, called \\textit{UrbanSky}. We used this dataset\nto conduct simulations to verify the robustness of our approach. Finally, we\nhave built a portable solar panel with four compact detectors and an actuator\nto conduct experiments in various real-world settings: direct sunlight, cloudy\nsky, urban settings with occlusions and shadows, and complex indoor lighting.\nIn all cases, we show significant improvements in harvested energy compared to\nstandard approaches for controlling the orientation of a solar panel."}
{"id": "2504.11104", "pdf": "https://arxiv.org/pdf/2504.11104", "abs": "https://arxiv.org/abs/2504.11104", "authors": ["René Peinl"], "title": "Using LLMs as prompt modifier to avoid biases in AI image generators", "categories": ["cs.CL", "cs.CV", "cs.CY"], "comment": null, "summary": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/"}
{"id": "2504.10776", "pdf": "https://arxiv.org/pdf/2504.10776", "abs": "https://arxiv.org/abs/2504.10776", "authors": ["Zhenyu Yu", "Hanqing Chen", "Mohd Yamani Idna Idris", "Pei Wang"], "title": "Rainy: Unlocking Satellite Calibration for Deep Learning in Precipitation", "categories": ["cs.CV"], "comment": null, "summary": "Precipitation plays a critical role in the Earth's hydrological cycle,\ndirectly affecting ecosystems, agriculture, and water resource management.\nAccurate precipitation estimation and prediction are crucial for understanding\nclimate dynamics, disaster preparedness, and environmental monitoring. In\nrecent years, artificial intelligence (AI) has gained increasing attention in\nquantitative remote sensing (QRS), enabling more advanced data analysis and\nimproving precipitation estimation accuracy. Although traditional methods have\nbeen widely used for precipitation estimation, they face limitations due to the\ndifficulty of data acquisition and the challenge of capturing complex feature\nrelationships. Furthermore, the lack of standardized multi-source satellite\ndatasets, and in most cases, the exclusive reliance on station data,\nsignificantly hinders the effective application of advanced AI models. To\naddress these challenges, we propose the Rainy dataset, a multi-source\nspatio-temporal dataset that integrates pure satellite data with station data,\nand propose Taper Loss, designed to fill the gap in tasks where only in-situ\ndata is available without area-wide support. The Rainy dataset supports five\nmain tasks: (1) satellite calibration, (2) precipitation event prediction, (3)\nprecipitation level prediction, (4) spatiotemporal prediction, and (5)\nprecipitation downscaling. For each task, we selected benchmark models and\nevaluation metrics to provide valuable references for researchers. Using\nprecipitation as an example, the Rainy dataset and Taper Loss demonstrate the\nseamless collaboration between QRS and computer vision, offering data support\nfor AI for Science in the field of QRS and providing valuable insights for\ninterdisciplinary collaboration and integration."}
{"id": "2504.11108", "pdf": "https://arxiv.org/pdf/2504.11108", "abs": "https://arxiv.org/abs/2504.11108", "authors": ["René Peinl", "Vincent Tischler"], "title": "Benchmarking Vision Language Models on German Factual Data", "categories": ["cs.CL"], "comment": null, "summary": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages."}
{"id": "2504.10786", "pdf": "https://arxiv.org/pdf/2504.10786", "abs": "https://arxiv.org/abs/2504.10786", "authors": ["Gene Tangtartharakul", "Katherine R. Storrs"], "title": "Visual Language Models show widespread visual deficits on neuropsychological tests", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.0; I.2.10"], "comment": "31 pages, 3 figures, 1 supplementary document with 1 figure and 51\n  sample images", "summary": "Visual Language Models (VLMs) show remarkable performance in visual reasoning\ntasks, successfully tackling college-level challenges that require high-level\nunderstanding of images. However, some recent reports of VLMs struggling to\nreason about elemental visual concepts like orientation, position, continuity,\nand occlusion suggest a potential gulf between human and VLM vision. Here we\nuse the toolkit of neuropsychology to systematically assess the capabilities of\nthree state-of-the-art VLMs across visual domains. Using 51 tests drawn from\nsix clinical and experimental batteries, we characterise the visual abilities\nof leading VLMs relative to normative performance in healthy adults. While the\nmodels excel in straightforward object recognition tasks, we find widespread\ndeficits in low- and mid-level visual abilities that would be considered\nclinically significant in humans. These selective deficits, profiled through\nvalidated test batteries, suggest that an artificial system can achieve complex\nobject recognition without developing foundational visual concepts that in\nhumans require no explicit training."}
{"id": "2504.11169", "pdf": "https://arxiv.org/pdf/2504.11169", "abs": "https://arxiv.org/abs/2504.11169", "authors": ["Laura De Grazia", "Pol Pastells", "Mauro Vázquez Chas", "Desmond Elliott", "Danae Sánchez Villegas", "Mireia Farrús", "Mariona Taulé"], "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context."}
{"id": "2504.10795", "pdf": "https://arxiv.org/pdf/2504.10795", "abs": "https://arxiv.org/abs/2504.10795", "authors": ["Guandong Li", "Mengxia Ye"], "title": "3D Wavelet Convolutions with Extended Receptive Fields for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2504.04463", "summary": "Deep neural networks face numerous challenges in hyperspectral image\nclassification, including high-dimensional data, sparse ground object\ndistributions, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To better adapt to ground\nobject distributions while expanding receptive fields without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nWCNet, an improved 3D-DenseNet model integrated with wavelet transforms. We\nintroduce wavelet transforms to effectively extend convolutional receptive\nfields and guide CNNs to better respond to low frequencies through cascading,\ntermed wavelet convolution. Each convolution focuses on different frequency\nbands of the input signal with gradually increasing effective ranges. This\nprocess enables greater emphasis on low-frequency components while adding only\na small number of trainable parameters. This dynamic approach allows the model\nto flexibly focus on critical spatial structures when processing different\nregions, rather than relying on fixed receptive fields of single static\nkernels. The Wavelet Conv module enhances model representation capability by\nexpanding receptive fields through 3D wavelet transforms without increasing\nnetwork depth or width. Experimental results demonstrate superior performance\non the IN, UP, and KSC datasets, outperforming mainstream hyperspectral image\nclassification methods."}
{"id": "2504.11183", "pdf": "https://arxiv.org/pdf/2504.11183", "abs": "https://arxiv.org/abs/2504.11183", "authors": ["Ej Zhou", "Weiming Lu"], "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting", "categories": ["cs.CL"], "comment": null, "summary": "Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English (\\textsc{eng}), Chinese (\\textsc{zho}),\nRussian (\\textsc{rus}), Indonesian (\\textsc{ind}) and Thai (\\textsc{tha}), and\nanalyzed four bias dimensions: \\textit{gender}, \\textit{religion},\n\\textit{nationality}, and \\textit{race-color}. By constructing multilingual\nbias evaluation datasets, this study allows fair comparisons between models\nacross languages. We have further investigated three debiasing\nmethods-\\texttt{CDA}, \\texttt{Dropout}, \\texttt{SenDeb}-and demonstrated that\ndebiasing methods from high-resource languages can be effectively transferred\nto low-resource ones, providing actionable insights for fairness research in\nmultilingual NLP."}
{"id": "2504.10804", "pdf": "https://arxiv.org/pdf/2504.10804", "abs": "https://arxiv.org/abs/2504.10804", "authors": ["Jiani Liu", "Zhiyuan Wang", "Zeliang Zhang", "Chao Huang", "Susan Liang", "Yunlong Tang", "Chenliang Xu"], "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability", "categories": ["cs.CV"], "comment": "Work in progress. 10 pages. 4 figures", "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nrange of applications, including many safety-critical tasks. However, their\nunique architectural properties raise new challenges and opportunities in\nadversarial robustness. In particular, we observe that adversarial examples\ncrafted on ViTs exhibit higher transferability compared to those crafted on\nCNNs, suggesting that ViTs contain structural characteristics favorable for\ntransferable attacks. In this work, we investigate the role of computational\nredundancy in ViTs and its impact on adversarial transferability. Unlike prior\nstudies that aim to reduce computation for efficiency, we propose to exploit\nthis redundancy to improve the quality and transferability of adversarial\nexamples. Through a detailed analysis, we identify two forms of redundancy,\nincluding the data-level and model-level, that can be harnessed to amplify\nattack effectiveness. Building on this insight, we design a suite of\ntechniques, including attention sparsity manipulation, attention head\npermutation, clean token regularization, ghost MoE diversification, and\ntest-time adversarial training. Extensive experiments on the ImageNet-1k\ndataset validate the effectiveness of our approach, showing that our methods\nsignificantly outperform existing baselines in both transferability and\ngenerality across diverse model architectures."}
{"id": "2504.11186", "pdf": "https://arxiv.org/pdf/2504.11186", "abs": "https://arxiv.org/abs/2504.11186", "authors": ["Minjie Zou", "Sahana Srinivasan", "Thaddaeus Wai Soon Lo", "Ke Zou", "Gabriel Dawei Yang", "Xuguang Ai", "Hyunjae Kim", "Maxwell Singer", "Fares Antaki", "Kelvin Li", "Robert Chang", "Marcus Tan", "David Ziyou Chen", "Dianbo Liu", "Qingyu Chen", "Yih Chung Tham"], "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items", "categories": ["cs.CL", "cs.AI"], "comment": "83 pages, 6 figures, 3 tables, 9 supplementary figures, 7\n  supplementary tables", "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications."}
{"id": "2504.10808", "pdf": "https://arxiv.org/pdf/2504.10808", "abs": "https://arxiv.org/abs/2504.10808", "authors": ["Md Rakibul Hasan", "Shafin Rahman", "Md Zakir Hossain", "Aneesh Krishna", "Tom Gedeon"], "title": "Tabular foundation model to detect empathy from visual cues", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Detecting empathy from video interactions is an emerging area of research.\nVideo datasets, however, are often released as extracted features (i.e.,\ntabular data) rather than raw footage due to privacy and ethical concerns.\nPrior research on such tabular datasets established tree-based classical\nmachine learning approaches as the best-performing models. Motivated by the\nrecent success of textual foundation models (i.e., large language models), we\nexplore the use of tabular foundation models in empathy detection from tabular\nvisual features. We experiment with two recent tabular foundation models $-$\nTabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups.\nOur experiments on a public human-robot interaction benchmark demonstrate a\nsignificant boost in cross-subject empathy detection accuracy over several\nstrong baselines (accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow\n0.669$). In addition to performance improvement, we contribute novel insights\nand an evaluation setup to ensure generalisation on unseen subjects in this\npublic benchmark. As the practice of releasing video features as tabular\ndatasets is likely to persist due to privacy constraints, our findings will be\nwidely applicable to future empathy detection video datasets as well."}
{"id": "2504.11277", "pdf": "https://arxiv.org/pdf/2504.11277", "abs": "https://arxiv.org/abs/2504.11277", "authors": ["Guocong Li", "Weize Liu", "Yihang Wu", "Ping Wang", "Shuaihan Huang", "Hongxia Xu", "Jian Wu"], "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance."}
{"id": "2504.10809", "pdf": "https://arxiv.org/pdf/2504.10809", "abs": "https://arxiv.org/abs/2504.10809", "authors": ["Christophe Bolduc", "Yannick Hold-Geoffroy", "Zhixin Shu", "Jean-François Lalonde"], "title": "GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR", "categories": ["cs.CV"], "comment": null, "summary": "We present GaSLight, a method that generates spatially-varying lighting from\nregular images. Our method proposes using HDR Gaussian Splats as light source\nrepresentation, marking the first time regular images can serve as light\nsources in a 3D renderer. Our two-stage process first enhances the dynamic\nrange of images plausibly and accurately by leveraging the priors embedded in\ndiffusion models. Next, we employ Gaussian Splats to model 3D lighting,\nachieving spatially variant lighting. Our approach yields state-of-the-art\nresults on HDR estimations and their applications in illuminating virtual\nobjects and scenes. To facilitate the benchmarking of images as light sources,\nwe introduce a novel dataset of calibrated and unsaturated HDR to evaluate\nimages as light sources. We assess our method using a combination of this novel\ndataset and an existing dataset from the literature. The code to reproduce our\nmethod will be available upon acceptance."}
{"id": "2504.11290", "pdf": "https://arxiv.org/pdf/2504.11290", "abs": "https://arxiv.org/abs/2504.11290", "authors": ["Joshua Otten", "Antonios Anastasopoulos", "Kevin Moran"], "title": "Automated Python Translation", "categories": ["cs.CL"], "comment": "15 pages, 4 figures, 17 tables", "summary": "Python is one of the most commonly used programming languages in industry and\neducation. Its English keywords and built-in functions/modules allow it to come\nclose to pseudo-code in terms of its readability and ease of writing. However,\nthose who do not speak English may not experience these advantages. In fact,\nthey may even be hindered in their ability to understand Python code, as the\nEnglish nature of its terms creates an additional layer of overhead. To that\nend, we introduce the task of automatically translating Python's natural\nmodality (keywords, error types, identifiers, etc.) into other human languages.\nThis presents a unique challenge, considering the abbreviated nature of these\nforms, as well as potential untranslatability of advanced\nmathematical/programming concepts across languages. We therefore create an\nautomated pipeline to translate Python into other human languages, comparing\nstrategies using machine translation and large language models. We then use\nthis pipeline to acquire translations from five common Python libraries\n(pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a\nquality test on a subset of these terms in French, Greek, and Bengali. We hope\nthis will provide a clearer path forward towards creating a universal Python,\naccessible to anyone regardless of nationality or language background."}
{"id": "2504.10810", "pdf": "https://arxiv.org/pdf/2504.10810", "abs": "https://arxiv.org/abs/2504.10810", "authors": ["Anmol Singhal Navya Singhal"], "title": "PatrolVision: Automated License Plate Recognition in the wild", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in IEEE Southeast Con 2025. To be published in IEEEXplore", "summary": "Adoption of AI driven techniques in public services remains low due to\nchallenges related to accuracy and speed of information at population scale.\nComputer vision techniques for traffic monitoring have not gained much\npopularity despite their relative strength in areas such as autonomous driving.\nDespite large number of academic methods for Automatic License Plate\nRecognition (ALPR) systems, very few provide an end to end solution for\npatrolling in the city. This paper presents a novel prototype for a low power\nGPU based patrolling system to be deployed in an urban environment on\nsurveillance vehicles for automated vehicle detection, recognition and\ntracking. In this work, we propose a complete ALPR system for Singapore license\nplates having both single and double line creating our own YOLO based network.\nWe focus on unconstrained capture scenarios as would be the case in real world\napplication, where the license plate (LP) might be considerably distorted due\nto oblique views. In this work, we first detect the license plate from the full\nimage using RFB-Net and rectify multiple distorted license plates in a single\nimage. After that, the detected license plate image is fed to our network for\ncharacter recognition. We evaluate the performance of our proposed system on a\nnewly built dataset covering more than 16,000 images. The system was able to\ncorrectly detect license plates with 86\\% precision and recognize characters of\na license plate in 67\\% of the test set, and 89\\% accuracy with one incorrect\ncharacter (partial match). We also test latency of our system and achieve 64FPS\non Tesla P4 GPU"}
{"id": "2504.11331", "pdf": "https://arxiv.org/pdf/2504.11331", "abs": "https://arxiv.org/abs/2504.11331", "authors": ["Hao Liu", "Lijun He", "Jiaxi Liang", "Zhihan Ren", "Fan Li"], "title": "Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL", "cs.MM"], "comment": "submitted to ACM MM2025", "summary": "Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+3.1\\% F1 and +5.4\\% precision on\nTwitter2015)."}
{"id": "2504.10822", "pdf": "https://arxiv.org/pdf/2504.10822", "abs": "https://arxiv.org/abs/2504.10822", "authors": ["Janna Bruner", "Amit Moryossef", "Lior Wolf"], "title": "IlluSign: Illustrating Sign Language Videos by Leveraging the Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Sign languages are dynamic visual languages that involve hand gestures, in\ncombination with non manual elements such as facial expressions. While video\nrecordings of sign language are commonly used for education and documentation,\nthe dynamic nature of signs can make it challenging to study them in detail,\nespecially for new learners and educators. This work aims to convert sign\nlanguage video footage into static illustrations, which serve as an additional\neducational resource to complement video content. This process is usually done\nby an artist, and is therefore quite costly. We propose a method that\nillustrates sign language videos by leveraging generative models' ability to\nunderstand both the semantic and geometric aspects of images. Our approach\nfocuses on transferring a sketch like illustration style to video footage of\nsign language, combining the start and end frames of a sign into a single\nillustration, and using arrows to highlight the hand's direction and motion.\nWhile many style transfer methods address domain adaptation at varying levels\nof abstraction, applying a sketch like style to sign languages, especially for\nhand gestures and facial expressions, poses a significant challenge. To tackle\nthis, we intervene in the denoising process of a diffusion model, injecting\nstyle as keys and values into high resolution attention layers, and fusing\ngeometric information from the image and edges as queries. For the final\nillustration, we use the attention mechanism to combine the attention weights\nfrom both the start and end illustrations, resulting in a soft combination. Our\nmethod offers a cost effective solution for generating sign language\nillustrations at inference time, addressing the lack of such resources in\neducational materials."}
{"id": "2504.11337", "pdf": "https://arxiv.org/pdf/2504.11337", "abs": "https://arxiv.org/abs/2504.11337", "authors": ["Zhihao Xu", "Yongqi Tong", "Xin Zhang", "Jun Zhou", "Xiting Wang"], "title": "REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Multi-objective preference alignment in language models often encounters a\nchallenging trade-off: optimizing for one human preference (e.g., helpfulness)\nfrequently compromises others (e.g., harmlessness) due to the inherent\nconflicts between competing objectives. While prior work mainly focuses on\nalgorithmic solutions, we explore a novel data-driven approach to uncover the\ntypes of data that can effectively mitigate these conflicts. Specifically, we\npropose the concept of Reward Consistency (RC), which identifies samples that\nalign with multiple preference objectives, thereby reducing conflicts during\ntraining. Through gradient-based analysis, we demonstrate that RC-compliant\nsamples inherently constrain performance degradation during multi-objective\noptimization. Building on these insights, we further develop Reward Consistency\nSampling, a framework that automatically constructs preference datasets that\neffectively mitigate conflicts during multi-objective alignment. Our generated\ndata achieves an average improvement of 13.37% in both the harmless rate and\nhelpfulness win rate when optimizing harmlessness and helpfulness, and can\nconsistently resolve conflicts in varying multi-objective scenarios."}
{"id": "2504.10825", "pdf": "https://arxiv.org/pdf/2504.10825", "abs": "https://arxiv.org/abs/2504.10825", "authors": ["Dianbing Xi", "Jiepeng Wang", "Yuanzhi Liang", "Xi Qiu", "Yuchi Huo", "Rui Wang", "Chi Zhang", "Xuelong Li"], "title": "OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding", "categories": ["cs.CV"], "comment": "Our project page: https://tele-ai.github.io/OmniVDiff/", "summary": "In this paper, we propose a novel framework for controllable video diffusion,\nOmniVDiff, aiming to synthesize and comprehend multiple video visual content in\na single diffusion model. To achieve this, OmniVDiff treats all video visual\nmodalities in the color space to learn a joint distribution, while employing an\nadaptive control strategy that dynamically adjusts the role of each visual\nmodality during the diffusion process, either as a generation modality or a\nconditioning modality. This allows flexible manipulation of each modality's\nrole, enabling support for a wide range of tasks. Consequently, our model\nsupports three key functionalities: (1) Text-conditioned video generation:\nmulti-modal visual video sequences (i.e., rgb, depth, canny, segmentaion) are\ngenerated based on the text conditions in one diffusion process; (2) Video\nunderstanding: OmniVDiff can estimate the depth, canny map, and semantic\nsegmentation across the input rgb frames while ensuring coherence with the rgb\ninput; and (3) X-conditioned video generation: OmniVDiff generates videos\nconditioned on fine-grained attributes (e.g., depth maps or segmentation maps).\nBy integrating these diverse tasks into a unified video diffusion framework,\nOmniVDiff enhances the flexibility and scalability for controllable video\ndiffusion, making it an effective tool for a variety of downstream\napplications, such as video-to-video translation. Extensive experiments\ndemonstrate the effectiveness of our approach, highlighting its potential for\nvarious video-related applications."}
{"id": "2504.11369", "pdf": "https://arxiv.org/pdf/2504.11369", "abs": "https://arxiv.org/abs/2504.11369", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "title": "OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": "Under review with ARR", "summary": "Open Large Language Models (OLLMs) are increasingly leveraged in generative\nAI applications, posing new challenges for detecting their outputs. We propose\nOpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate\nmachine-generated text detectors on the Turing Test and Authorship Attribution\nproblems. OpenTuringBench focuses on a representative set of OLLMs, and\nfeatures a number of challenging evaluation tasks, including\nhuman/machine-manipulated texts, out-of-domain texts, and texts from previously\nunseen models. We also provide OTBDetector, a contrastive learning framework to\ndetect and attribute OLLM-based machine-generated texts. Results highlight the\nrelevance and varying degrees of difficulty of the OpenTuringBench tasks, with\nour detector achieving remarkable capabilities across the various tasks and\noutperforming most existing detectors. Resources are available on the\nOpenTuringBench Hugging Face repository at\nhttps://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench"}
{"id": "2504.10829", "pdf": "https://arxiv.org/pdf/2504.10829", "abs": "https://arxiv.org/abs/2504.10829", "authors": ["Hengyu Shi", "Junhao Su", "Huansheng Ning", "Xiaoming Wei", "Jialin Gao"], "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation", "categories": ["cs.CV"], "comment": null, "summary": "Conditional layout generation aims to automatically generate visually\nappealing and semantically coherent layouts from user-defined constraints.\nWhile recent methods based on generative models have shown promising results,\nthey typically require substantial amounts of training data or extensive\nfine-tuning, limiting their versatility and practical applicability.\nAlternatively, some training-free approaches leveraging in-context learning\nwith Large Language Models (LLMs) have emerged, but they often suffer from\nlimited reasoning capabilities and overly simplistic ranking mechanisms, which\nrestrict their ability to generate consistently high-quality layouts. To this\nend, we propose LayoutCoT, a novel approach that leverages the reasoning\ncapabilities of LLMs through a combination of Retrieval-Augmented Generation\n(RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms\nlayout representations into a standardized serialized format suitable for\nprocessing by LLMs. A Layout-aware RAG is used to facilitate effective\nretrieval and generate a coarse layout by LLMs. This preliminary layout,\ntogether with the selected exemplars, is then fed into a specially designed CoT\nreasoning module for iterative refinement, significantly enhancing both\nsemantic coherence and visual quality. We conduct extensive experiments on five\npublic datasets spanning three conditional layout generation tasks.\nExperimental results demonstrate that LayoutCoT achieves state-of-the-art\nperformance without requiring training or fine-tuning. Notably, our CoT\nreasoning module enables standard LLMs, even those without explicit deep\nreasoning abilities, to outperform specialized deep-reasoning models such as\ndeepseek-R1, highlighting the potential of our approach in unleashing the deep\nreasoning capabilities of LLMs for layout generation tasks."}
{"id": "2504.11373", "pdf": "https://arxiv.org/pdf/2504.11373", "abs": "https://arxiv.org/abs/2504.11373", "authors": ["Wang Bill Zhu", "Tianqi Chen", "Ching Ying Lin", "Jade Law", "Mazen Jizzini", "Jorge J. Nieva", "Ruishan Liu", "Robin Jia"], "title": "Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False Presuppositions", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems."}
{"id": "2504.10834", "pdf": "https://arxiv.org/pdf/2504.10834", "abs": "https://arxiv.org/abs/2504.10834", "authors": ["Sihang Chen", "Lijun Yun", "Ze Liu", "JianFeng Zhu", "Jie Chen", "Hui Wang", "Yueping Nie"], "title": "LightFormer: A lightweight and efficient decoder for remote sensing image segmentation", "categories": ["cs.CV"], "comment": "26 pages, 69 figures", "summary": "Deep learning techniques have achieved remarkable success in the semantic\nsegmentation of remote sensing images and in land-use change detection.\nNevertheless, their real-time deployment on edge platforms remains constrained\nby decoder complexity. Herein, we introduce LightFormer, a lightweight decoder\nfor time-critical tasks that involve unstructured targets, such as disaster\nassessment, unmanned aerial vehicle search-and-rescue, and cultural heritage\nmonitoring. LightFormer employs a feature-fusion and refinement module built on\nchannel processing and a learnable gating mechanism to aggregate multi-scale,\nmulti-range information efficiently, which drastically curtails model\ncomplexity. Furthermore, we propose a spatial information selection module\n(SISM) that integrates long-range attention with a detail preservation branch\nto capture spatial dependencies across multiple scales, thereby substantially\nimproving the recognition of unstructured targets in complex scenes. On the\nISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9%\nvs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters,\nthus achieving an excellent accuracy-efficiency trade-off. Consistent results\non LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its\nrobustness and superior perception of unstructured objects. These findings\nhighlight LightFormer as a practical solution for remote sensing applications\nwhere both computational economy and high-precision segmentation are\nimperative."}
{"id": "2504.11381", "pdf": "https://arxiv.org/pdf/2504.11381", "abs": "https://arxiv.org/abs/2504.11381", "authors": ["Juan Diego Rodriguez", "Wenxuan Ding", "Katrin Erk", "Greg Durrett"], "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items."}
{"id": "2504.10842", "pdf": "https://arxiv.org/pdf/2504.10842", "abs": "https://arxiv.org/abs/2504.10842", "authors": ["Shuai Yuan", "Xiangan Liang", "Tianwu Lin", "Shuang Chen", "Rui Liu", "Jie Wang", "Hongsheng Zhang", "Peng Gong"], "title": "A comprehensive review of remote sensing in wetland classification and mapping", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Wetlands constitute critical ecosystems that support both biodiversity and\nhuman well-being; however, they have experienced a significant decline since\nthe 20th century. Back in the 1970s, researchers began to employ remote sensing\ntechnologies for wetland classification and mapping to elucidate the extent and\nvariations of wetlands. Although some review articles summarized the\ndevelopment of this field, there is a lack of a thorough and in-depth\nunderstanding of wetland classification and mapping: (1) the scientific\nimportance of wetlands, (2) major data, methods used in wetland classification\nand mapping, (3) driving factors of wetland changes, (4) current research\nparadigm and limitations, (5) challenges and opportunities in wetland\nclassification and mapping under the context of technological innovation and\nglobal environmental change. In this review, we aim to provide a comprehensive\nperspective and new insights into wetland classification and mapping for\nreaders to answer these questions. First, we conduct a meta-analysis of over\n1,200 papers, encompassing wetland types, methods, sensor types, and study\nsites, examining prevailing trends in wetland classification and mapping. Next,\nwe review and synthesize the wetland features and existing data and methods in\nwetland classification and mapping. We also summarize typical wetland mapping\nproducts and explore the intrinsic driving factors of wetland changes across\nmultiple spatial and temporal scales. Finally, we discuss current limitations\nand propose future directions in response to global environmental change and\ntechnological innovation. This review consolidates our understanding of wetland\nremote sensing and offers scientific recommendations that foster transformative\nprogress in wetland science."}
{"id": "2504.11409", "pdf": "https://arxiv.org/pdf/2504.11409", "abs": "https://arxiv.org/abs/2504.11409", "authors": ["Ali Taghibakhshi", "Sharath Turuvekere Sreenivas", "Saurav Muralidharan", "Marcin Chochowski", "Yashaswi Karnati", "Raviraj Joshi", "Ameya Sunil Mahabaleshwarkar", "Zijia Chen", "Yoshi Suhara", "Oluwatobi Olabiyi", "Daniel Korzekwa", "Mostofa Patwary", "Mohammad Shoeybi", "Jan Kautz", "Bryan Catanzaro", "Ashwath Aithal", "Nima Tajbakhsh", "Pavlo Molchanov"], "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning", "categories": ["cs.CL"], "comment": null, "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier."}
{"id": "2504.10852", "pdf": "https://arxiv.org/pdf/2504.10852", "abs": "https://arxiv.org/abs/2504.10852", "authors": ["Pengxiao Han", "Changkun Ye", "Jinguang Tong", "Cuicui Jiang", "Jie Hong", "Li Fang", "Xuesong Li"], "title": "Enhancing Features in Long-tailed Data Using Large Vision Mode", "categories": ["cs.CV"], "comment": null, "summary": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018."}
{"id": "2504.11420", "pdf": "https://arxiv.org/pdf/2504.11420", "abs": "https://arxiv.org/abs/2504.11420", "authors": ["Quanyu Long", "Jianda Chen", "Zhengyuan Liu", "Nancy F. Chen", "Wenya Wang", "Sinno Jialin Pan"], "title": "Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts", "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet they often rely on external context to handle complex\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\ntop-ranked documents in a single pass, many real-world scenarios demand\ncompositional retrieval, where multiple sources must be combined in a\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\nthat models this process as a Markov Decision Process (MDP), decomposing the\nprobability of retrieving a set of elements into a sequence of conditional\nprobabilities and allowing each retrieval step to be conditioned on previously\nselected examples. We train the retriever in two stages: first, we efficiently\nconstruct supervised sequential data for initial policy training; we then\nrefine the policy to align with the LLM's preferences using a reward grounded\nin the structural correspondence of generated programs. Experimental results\nshow that our method consistently and significantly outperforms baselines,\nunderscoring the importance of explicitly modeling inter-example dependencies.\nThese findings highlight the potential of compositional retrieval for tasks\nrequiring multiple pieces of evidence or examples."}
{"id": "2504.10854", "pdf": "https://arxiv.org/pdf/2504.10854", "abs": "https://arxiv.org/abs/2504.10854", "authors": ["Hanning Chen", "Yang Ni", "Wenjun Huang", "Hyunwoo Oh", "Yezi Liu", "Tamoghno Das", "Mohsen Imani"], "title": "LVLM_CSP: Accelerating Large Vision Language Models via Clustering, Scattering, and Pruning for Reasoning Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision Language Models (LVLMs) have been widely adopted to guide vision\nfoundation models in performing reasoning segmentation tasks, achieving\nimpressive performance. However, the substantial computational overhead\nassociated with LVLMs presents a new challenge. The primary source of this\ncomputational cost arises from processing hundreds of image tokens. Therefore,\nan effective strategy to mitigate such overhead is to reduce the number of\nimage tokens, a process known as image token pruning. Previous studies on image\ntoken pruning for LVLMs have primarily focused on high level visual\nunderstanding tasks, such as visual question answering and image captioning. In\ncontrast, guiding vision foundation models to generate accurate visual masks\nbased on textual queries demands precise semantic and spatial reasoning\ncapabilities. Consequently, pruning methods must carefully control individual\nimage tokens throughout the LVLM reasoning process. Our empirical analysis\nreveals that existing methods struggle to adequately balance reductions in\ncomputational overhead with the necessity to maintain high segmentation\naccuracy. In this work, we propose LVLM_CSP, a novel training free visual token\npruning method specifically designed for LVLM based reasoning segmentation\ntasks. LVLM_CSP consists of three stages: clustering, scattering, and pruning.\nInitially, the LVLM performs coarse-grained visual reasoning using a subset of\nselected image tokens. Next, fine grained reasoning is conducted, and finally,\nmost visual tokens are pruned in the last stage. Extensive experiments\ndemonstrate that LVLM_CSP achieves a 65% reduction in image token inference\nFLOPs with virtually no accuracy degradation, and a 70% reduction with only a\nminor 1% drop in accuracy on the 7B LVLM."}
{"id": "2504.11426", "pdf": "https://arxiv.org/pdf/2504.11426", "abs": "https://arxiv.org/abs/2504.11426", "authors": ["Xue Zhang", "Songming Zhang", "Yunlong Liang", "Fandong Meng", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 9 figures, 11 tables, under review. Code is available at:\n  https://github.com/songmzhang/DSKDv2. arXiv admin note: text overlap with\n  arXiv:2406.17328", "summary": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies."}
{"id": "2504.10871", "pdf": "https://arxiv.org/pdf/2504.10871", "abs": "https://arxiv.org/abs/2504.10871", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui", "Yuxin Jing", "Yuhan Lyu"], "title": "DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and Visible Images Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Existing infrared and visible image fusion(IVIF) algorithms often prioritize\nhigh-quality images, neglecting image degradation such as low light and noise,\nwhich limits the practical potential. This paper propose Degradation-Aware\nAdaptive image Fusion (DAAF), which achieves unified modeling of adaptive\ndegradation optimization and image fusion. Specifically, DAAF comprises an\nauxiliary Adaptive Degradation Optimization Network (ADON) and a Feature\nInteractive Local-Global Fusion (FILGF) Network. Firstly, ADON includes\ninfrared and visible-light branches. Within the infrared branch,\nfrequency-domain feature decomposition and extraction are employed to isolate\nGaussian and stripe noise. In the visible-light branch, Retinex decomposition\nis applied to extract illumination and reflectance components, enabling\ncomplementary enhancement of detail and illumination distribution.\nSubsequently, FILGF performs interactive multi-scale local-global feature\nfusion. Local feature fusion consists of intra-inter model feature complement,\nwhile global feature fusion is achieved through a interactive cross-model\nattention. Extensive experiments have shown that DAAF outperforms current IVIF\nalgorithms in normal and complex degradation scenarios."}
{"id": "2504.11431", "pdf": "https://arxiv.org/pdf/2504.11431", "abs": "https://arxiv.org/abs/2504.11431", "authors": ["Maria Teleki", "Xiangjue Dong", "Haoran Liu", "James Caverlee"], "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": "To appear in ICWSM 2025", "summary": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault."}
{"id": "2504.10873", "pdf": "https://arxiv.org/pdf/2504.10873", "abs": "https://arxiv.org/abs/2504.10873", "authors": ["Tonko E. W. Bossen", "Andreas Møgelmose", "Ross Greer"], "title": "Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "In autonomous driving, it is crucial to correctly interpret traffic gestures\n(TGs), such as those of an authority figure providing orders or instructions,\nor a pedestrian signaling the driver, to ensure a safe and pleasant traffic\nenvironment for all road users. This study investigates the capabilities of\nstate-of-the-art vision-language models (VLMs) in zero-shot interpretation,\nfocusing on their ability to caption and classify human gestures in traffic\ncontexts. We create and publicly share two custom datasets with varying formal\nand informal TGs, such as 'Stop', 'Reverse', 'Hail', etc. The datasets are\n\"Acted TG (ATG)\" and \"Instructive TG In-The-Wild (ITGI)\". They are annotated\nwith natural language, describing the pedestrian's body position and gesture.\nWe evaluate models using three methods utilizing expert-generated captions as\nbaseline and control: (1) caption similarity, (2) gesture classification, and\n(3) pose sequence reconstruction similarity. Results show that current VLMs\nstruggle with gesture understanding: sentence similarity averages below 0.59,\nand classification F1 scores reach only 0.14-0.39, well below the expert\nbaseline of 0.70. While pose reconstruction shows potential, it requires more\ndata and refined metrics to be reliable. Our findings reveal that although some\nSOTA VLMs can interpret zero-shot human traffic gestures, none are accurate and\nrobust enough to be trustworthy, emphasizing the need for further research in\nthis domain."}
{"id": "2504.11442", "pdf": "https://arxiv.org/pdf/2504.11442", "abs": "https://arxiv.org/abs/2504.11442", "authors": ["Leon Guertler", "Bobby Cheng", "Simon Yu", "Bo Liu", "Leshem Choshen", "Cheston Tan"], "title": "TextArena", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "work in progress; 5 pages, 3 figures", "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/."}
{"id": "2504.10877", "pdf": "https://arxiv.org/pdf/2504.10877", "abs": "https://arxiv.org/abs/2504.10877", "authors": ["Soheil Gharatappeh", "Salimeh Sekeh", "Vikas Dhiman"], "title": "Weather-Aware Object Detection Transformer for Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "RT-DETRs have shown strong performance across various computer vision tasks\nbut are known to degrade under challenging weather conditions such as fog. In\nthis work, we investigate three novel approaches to enhance RT-DETR robustness\nin foggy environments: (1) Domain Adaptation via Perceptual Loss, which\ndistills domain-invariant features from a teacher network to a student using\nperceptual supervision; (2) Weather Adaptive Attention, which augments the\nattention mechanism with fog-sensitive scaling by introducing an auxiliary\nfoggy image stream; and (3) Weather Fusion Encoder, which integrates a\ndual-stream encoder architecture that fuses clear and foggy image features via\nmulti-head self and cross-attention. Despite the architectural innovations,\nnone of the proposed methods consistently outperform the baseline RT-DETR. We\nanalyze the limitations and potential causes, offering insights for future\nresearch in weather-aware object detection."}
{"id": "2504.11456", "pdf": "https://arxiv.org/pdf/2504.11456", "abs": "https://arxiv.org/abs/2504.11456", "authors": ["Zhiwei He", "Tian Liang", "Jiahao Xu", "Qiuzhi Liu", "Xingyu Chen", "Yue Wang", "Linfeng Song", "Dian Yu", "Zhenwen Liang", "Wenxuan Wang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath."}
{"id": "2504.10878", "pdf": "https://arxiv.org/pdf/2504.10878", "abs": "https://arxiv.org/abs/2504.10878", "authors": ["Yilang Peng", "Sijia Qian", "Yingdan Lu", "Cuihua Shen"], "title": "Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4.9; J.4"], "comment": "26 pages", "summary": "In today's visually dominated social media landscape, predicting the\nperceived credibility of visual content and understanding what drives human\njudgment are crucial for countering misinformation. However, these tasks are\nchallenging due to the diversity and richness of visual features. We introduce\na Large Language Model (LLM)-informed feature discovery framework that\nleverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and\nexplain its reasoning. We extract and quantify interpretable features using\ntargeted prompts and integrate them into machine learning models to improve\ncredibility predictions. We tested this approach on 4,191 visual social media\nposts across eight topics in science, health, and politics, using credibility\nratings from 5,355 crowdsourced workers. Our method outperformed zero-shot\nGPT-based predictions by 13 percent in R2, and revealed key features like\ninformation concreteness and image format. We discuss the implications for\nmisinformation mitigation, visual credibility, and the role of LLMs in social\nscience."}
{"id": "2504.10490", "pdf": "https://arxiv.org/pdf/2504.10490", "abs": "https://arxiv.org/abs/2504.10490", "authors": ["Gabriel Bo", "Marc Bernardino", "Justin Gu"], "title": "GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA", "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 11 figures. This submission cites arXiv:2404.19756.\n  Supplementary materials and additional information are available at\n  arXiv:2404.19756", "summary": "We explore the potential of integrating learnable and interpretable\nmodules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based\nrepresentations--within a pre-trained GPT-2 model to enhance multi-task\nlearning accuracy. Motivated by the recent surge in using KAN and graph\nattention (GAT) architectures in chain-of-thought (CoT) models and debates over\ntheir benefits compared to simpler architectures like MLPs, we begin by\nenhancing a standard self-attention transformer using Low-Rank Adaptation\n(LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This\napproach yields significant improvements. To further boost interpretability and\nricher representations, we develop two variants that attempt to improve the\nstandard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However,\nsystematic evaluations reveal that neither variant outperforms the optimized\nLoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set,\n99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On\nsonnet generation, we get a CHRF score of 42.097. These findings highlight that\nefficient parameter adaptation via LoRA remains the most effective strategy for\nour tasks: sentiment analysis, paraphrase detection, and sonnet generation."}
{"id": "2504.10880", "pdf": "https://arxiv.org/pdf/2504.10880", "abs": "https://arxiv.org/abs/2504.10880", "authors": ["Aviral Chharia", "Tianyu Ren", "Tomotake Furuhata", "Kenji Shimada"], "title": "Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task", "categories": ["cs.CV"], "comment": "CVPR Workshop 2025; Project Website:\n  https://Safe-Construct.github.io/Safe-Construct", "summary": "Recognizing safety violations in construction environments is critical yet\nremains underexplored in computer vision. Existing models predominantly rely on\n2D object detection, which fails to capture the complexities of real-world\nviolations due to: (i) an oversimplified task formulation treating violation\nrecognition merely as object detection, (ii) inadequate validation under\nrealistic conditions, (iii) absence of standardized baselines, and (iv) limited\nscalability from the unavailability of synthetic dataset generators for diverse\nconstruction scenarios. To address these challenges, we introduce\nSafe-Construct, the first framework that reformulates violation recognition as\na 3D multi-view engagement task, leveraging scene-level worker-object context\nand 3D spatial understanding. We also propose the Synthetic Indoor Construction\nSite Generator (SICSG) to create diverse, scalable training data, overcoming\ndata limitations. Safe-Construct achieves a 7.6% improvement over\nstate-of-the-art methods across four violation types. We rigorously evaluate\nour approach in near-realistic settings, incorporating four violations, four\nworkers, 14 objects, and challenging conditions like occlusions (worker-object,\nworker-worker) and variable illumination (back-lighting, overexposure,\nsunlight). By integrating 3D multi-view spatial understanding and synthetic\ndata generation, Safe-Construct sets a new benchmark for scalable and robust\nsafety monitoring in high-risk industries. Project Website:\nhttps://Safe-Construct.github.io/Safe-Construct"}
{"id": "2504.10496", "pdf": "https://arxiv.org/pdf/2504.10496", "abs": "https://arxiv.org/abs/2504.10496", "authors": ["Ning Li", "Jingran Zhang", "Justin Cui"], "title": "ArxivBench: Can LLMs Assist Researchers in Conducting Research?", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable effectiveness in\ncompleting various tasks such as reasoning, translation, and question\nanswering. However the issue of factual incorrect content in LLM-generated\nresponses remains a persistent challenge. In this study, we evaluate both\nproprietary and open-source LLMs on their ability to respond with relevant\nresearch papers and accurate links to articles hosted on the arXiv platform,\nbased on high level prompts. To facilitate this evaluation, we introduce\narXivBench, a benchmark specifically designed to assess LLM performance across\neight major subject categories on arXiv and five subfields within computer\nscience, one of the most popular categories among them. Our findings reveal a\nconcerning accuracy of LLM-generated responses depending on the subject, with\nsome subjects experiencing significantly lower accuracy than others. Notably,\nClaude-3.5-Sonnet exhibits a substantial advantage in generating both relevant\nand accurate responses. And interestingly, most LLMs achieve a much higher\naccuracy in the Artificial Intelligence sub-field than other sub-fields. This\nbenchmark provides a standardized tool for evaluating the reliability of\nLLM-generated scientific responses, promoting more dependable use of LLMs in\nacademic and research environments. Our code is open-sourced at\nhttps://github.com/arxivBenchLLM/arXivBench and our dataset is available on\nhuggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench."}
{"id": "2504.10883", "pdf": "https://arxiv.org/pdf/2504.10883", "abs": "https://arxiv.org/abs/2504.10883", "authors": ["Karan Jain", "Mohammad Nayeem Teli"], "title": "Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently gained state of the art performance on many\nimage generation tasks. However, most models require significant computational\nresources to achieve this. This becomes apparent in the application of medical\nimage synthesis due to the 3D nature of medical datasets like CT-scans, MRIs,\nelectron microscope, etc. In this paper we propose a novel architecture for a\nsingle GPU memory-efficient training for diffusion models for high dimensional\nmedical datasets. The proposed model is built by using an invertible UNet\narchitecture with invertible attention modules. This leads to the following two\ncontributions: 1. denoising diffusion models and thus enabling memory usage to\nbe independent of the dimensionality of the dataset, and 2. reducing the energy\nusage during training. While this new model can be applied to a multitude of\nimage generation tasks, we showcase its memory-efficiency on the 3D BraTS2020\ndataset leading to up to 15\\% decrease in peak memory consumption during\ntraining with comparable results to SOTA while maintaining the image quality."}
{"id": "2504.10499", "pdf": "https://arxiv.org/pdf/2504.10499", "abs": "https://arxiv.org/abs/2504.10499", "authors": ["Zulun Zhu", "Tiancheng Huang", "Kai Wang", "Junda Ye", "Xinghe Chen", "Siqiang Luo"], "title": "Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey", "categories": ["cs.IR", "cs.CL", "Information storage and retrieval of data, Natural language\n  processing", "H.3.3; I.2.7"], "comment": null, "summary": "Large language models (LLMs) struggle with the factual error during inference\ndue to the lack of sufficient training data and the most updated knowledge,\nleading to the hallucination problem. Retrieval-Augmented Generation (RAG) has\ngained attention as a promising solution to address the limitation of LLMs, by\nretrieving relevant information from external source to generate more accurate\nanswers to the questions. Given the pervasive presence of structured knowledge\nin the external source, considerable strides in RAG have been made to employ\nthe techniques related to graphs and achieve more complex reasoning based on\nthe topological information between knowledge entities. However, there is\ncurrently neither unified review examining the diverse roles of graphs in RAG,\nnor a comprehensive resource to help researchers navigate and contribute to\nthis evolving field. This survey offers a novel perspective on the\nfunctionality of graphs within RAG and their impact on enhancing performance\nacross a wide range of graph-structured data. It provides a detailed breakdown\nof the roles that graphs play in RAG, covering database construction,\nalgorithms, pipelines, and tasks. Finally, it identifies current challenges and\noutline future research directions, aiming to inspire further developments in\nthis field. Our graph-centered analysis highlights the commonalities and\ndifferences in existing methods, setting the stage for future researchers in\nareas such as graph learning, database systems, and natural language\nprocessing."}
{"id": "2504.10885", "pdf": "https://arxiv.org/pdf/2504.10885", "abs": "https://arxiv.org/abs/2504.10885", "authors": ["Zeyu Zhang", "Zijian Chen", "Zicheng Zhang", "Yuze Sun", "Yuan Tian", "Ziheng Jia", "Chunyi Li", "Xiaohong Liu", "Xiongkuo Min", "Guangtao Zhai"], "title": "PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities\nacross a wide range of multimodal tasks, achieving ever-increasing performance\non various evaluation benchmarks. However, existing benchmarks are typically\nstatic and often overlap with pre-training datasets, leading to fixed\ncomplexity constraints and substantial data contamination issues. Meanwhile,\nmanually annotated datasets are labor-intensive, time-consuming, and subject to\nhuman bias and inconsistency, leading to reliability and reproducibility\nissues. To address these problems, we propose a fully dynamic multimodal\nevaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which\naims to generate fresh, diverse, and verifiable evaluation data automatically\nin puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw\nmaterial sampling module, a visual content generation module, and a puzzle rule\ndesign module, which ensures that each evaluation instance is primitive, highly\nrandomized, and uniquely solvable, enabling continual adaptation to the\nevolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a\ndynamic and scalable benchmark comprising 11,840 VQA samples. It features six\ncarefully designed puzzle tasks targeting three core LMM competencies, visual\nrecognition, logical reasoning, and context understanding. PuzzleBench differs\nfrom static benchmarks that quickly become outdated. It enables ongoing dataset\nrefreshing through OVPG and a rich set of open-ended puzzle designs, allowing\nseamless adaptation to the evolving capabilities of LMMs."}
{"id": "2504.10501", "pdf": "https://arxiv.org/pdf/2504.10501", "abs": "https://arxiv.org/abs/2504.10501", "authors": ["Shravika Mittal", "Darshi Shah", "Shin Won Do", "Mai ElSherief", "Tanushree Mitra", "Munmun De Choudhury"], "title": "Exposure to Content Written by Large Language Models Can Reduce Stigma Around Opioid Use Disorder in Online Communities", "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Widespread stigma, both in the offline and online spaces, acts as a barrier\nto harm reduction efforts in the context of opioid use disorder (OUD). This\nstigma is prominently directed towards clinically approved medications for\naddiction treatment (MAT), people with the condition, and the condition itself.\nGiven the potential of artificial intelligence based technologies in promoting\nhealth equity, and facilitating empathic conversations, this work examines\nwhether large language models (LLMs) can help abate OUD-related stigma in\nonline communities. To answer this, we conducted a series of pre-registered\nrandomized controlled experiments, where participants read LLM-generated,\nhuman-written, or no responses to help seeking OUD-related content in online\ncommunities. The experiment was conducted under two setups, i.e., participants\nread the responses either once (N = 2,141), or repeatedly for 14 days (N =\n107). We found that participants reported the least stigmatized attitudes\ntoward MAT after consuming LLM-generated responses under both the setups. This\nstudy offers insights into strategies that can foster inclusive online\ndiscourse on OUD, e.g., based on our findings LLMs can be used as an\neducation-based intervention to promote positive attitudes and increase\npeople's propensity toward MAT."}
{"id": "2504.10888", "pdf": "https://arxiv.org/pdf/2504.10888", "abs": "https://arxiv.org/abs/2504.10888", "authors": ["Jiahuan Long", "Wen Yao", "Tingsong Jiang", "Chao Ma"], "title": "CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial patches are widely used to evaluate the robustness of object\ndetection systems in real-world scenarios. These patches were initially\ndesigned to deceive single-modal detectors (e.g., visible or infrared) and have\nrecently been extended to target visible-infrared dual-modal detectors.\nHowever, existing dual-modal adversarial patch attacks have limited attack\neffectiveness across diverse physical scenarios. To address this, we propose\nCDUPatch, a universal cross-modal patch attack against visible-infrared object\ndetectors across scales, views, and scenarios. Specifically, we observe that\ncolor variations lead to different levels of thermal absorption, resulting in\ntemperature differences in infrared imaging. Leveraging this property, we\npropose an RGB-to-infrared adapter that maps RGB patches to infrared patches,\nenabling unified optimization of cross-modal patches. By learning an optimal\ncolor distribution on the adversarial patch, we can manipulate its thermal\nresponse and generate an adversarial infrared texture. Additionally, we\nintroduce a multi-scale clipping strategy and construct a new visible-infrared\ndataset, MSDrone, which contains aerial vehicle images in varying scales and\nperspectives. These data augmentation strategies enhance the robustness of our\npatch in real-world conditions. Experiments on four benchmark datasets (e.g.,\nDroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms\nexisting patch attacks in the digital domain. Extensive physical tests further\nconfirm strong transferability across scales, views, and scenarios."}
{"id": "2504.10512", "pdf": "https://arxiv.org/pdf/2504.10512", "abs": "https://arxiv.org/abs/2504.10512", "authors": ["Minh-Anh Nguyen", "Dung D. Le"], "title": "JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios."}
{"id": "2504.10889", "pdf": "https://arxiv.org/pdf/2504.10889", "abs": "https://arxiv.org/abs/2504.10889", "authors": ["Shripad Pate", "Aiman Farooq", "Suvrankar Dutta", "Musadiq Aadil Sheikh", "Atin Kumar", "Deepak Mishra"], "title": "Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A Detailed Annotation Framework and Multi-Label Classification Model", "categories": ["cs.CV"], "comment": null, "summary": "Accurate rib fracture identification and classification are essential for\ntreatment planning. However, existing datasets often lack fine-grained\nannotations, particularly regarding rib fracture characterization, type, and\nprecise anatomical location on individual ribs. To address this, we introduce a\nnovel rib fracture annotation protocol tailored for fracture classification.\nFurther, we enhance fracture classification by leveraging cross-modal\nembeddings that bridge radiological images and clinical descriptions. Our\napproach employs hyperbolic embeddings to capture the hierarchical nature of\nfracture, mapping visual features and textual descriptions into a shared\nnon-Euclidean manifold. This framework enables more nuanced similarity\ncomputations between imaging characteristics and clinical descriptions,\naccounting for the inherent hierarchical relationships in fracture taxonomy.\nExperimental results demonstrate that our approach outperforms existing methods\nacross multiple classification tasks, with average recall improvements of 6% on\nthe AirRib dataset and 17.5% on the public RibFrac dataset."}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514", "abs": "https://arxiv.org/abs/2504.10514", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI."}
{"id": "2504.10905", "pdf": "https://arxiv.org/pdf/2504.10905", "abs": "https://arxiv.org/abs/2504.10905", "authors": ["Yukang Lin", "Yan Hong", "Zunnan Xu", "Xindi Li", "Chao Xu", "Chuanbiao Song", "Ronghui Li", "Haoxing Chen", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang", "Xiu Li"], "title": "InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation", "categories": ["cs.CV", "cs.HC"], "comment": "under preview", "summary": "Recent video generation research has focused heavily on isolated actions,\nleaving interactive motions-such as hand-face interactions-largely unexamined.\nThese interactions are essential for emerging biometric authentication systems,\nwhich rely on interactive motion-based anti-spoofing approaches. From a\nsecurity perspective, there is a growing need for large-scale, high-quality\ninteractive videos to train and strengthen authentication models. In this work,\nwe introduce a novel paradigm for animating realistic hand-face interactions.\nOur approach simultaneously learns spatio-temporal contact dynamics and\nbiomechanically plausible deformation effects, enabling natural interactions\nwhere hand movements induce anatomically accurate facial deformations while\nmaintaining collision-free contact. To facilitate this research, we present\nInterHF, a large-scale hand-face interaction dataset featuring 18 interaction\npatterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a\nregion-aware diffusion model designed specifically for interaction animation.\nInterAnimate leverages learnable spatial and temporal latents to effectively\ncapture dynamic interaction priors and integrates a region-aware interaction\nmechanism that injects these priors into the denoising process. To the best of\nour knowledge, this work represents the first large-scale effort to\nsystematically study human hand-face interactions. Qualitative and quantitative\nresults show InterAnimate produces highly realistic animations, setting a new\nbenchmark. Code and data will be made public to advance research."}
{"id": "2504.10519", "pdf": "https://arxiv.org/pdf/2504.10519", "abs": "https://arxiv.org/abs/2504.10519", "authors": ["Yuhang Yao", "Haixin Wang", "Yibo Chen", "Jiawen Wang", "Min Chang Jordan Ren", "Bosheng Ding", "Salman Avestimehr", "Chaoyang He"], "title": "Toward Super Agent System with Hybrid AI Routers", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis paper presents a design of the Super Agent System. Upon receiving a user\nprompt, the system first detects the intent of the user, then routes the\nrequest to specialized task agents with the necessary tools or automatically\ngenerates agentic workflows. In practice, most applications directly serve as\nAI assistants on edge devices such as phones and robots. As different language\nmodels vary in capability and cloud-based models often entail high\ncomputational costs, latency, and privacy concerns, we then explore the hybrid\nmode where the router dynamically selects between local and cloud models based\non task complexity. Finally, we introduce the blueprint of an on-device super\nagent enhanced with cloud. With advances in multi-modality models and edge\nhardware, we envision that most computations can be handled locally, with cloud\ncollaboration only as needed. Such architecture paves the way for super agents\nto be seamlessly integrated into everyday life in the near future."}
{"id": "2504.10920", "pdf": "https://arxiv.org/pdf/2504.10920", "abs": "https://arxiv.org/abs/2504.10920", "authors": ["Peipei Song", "Long Zhang", "Long Lan", "Weidong Chen", "Dan Guo", "Xun Yang", "Meng Wang"], "title": "Towards Efficient Partially Relevant Video Retrieval with Active Moment Discovering", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM) on January 19,\n  2025. The code is available at https://github.com/songpipi/AMDNet", "summary": "Partially relevant video retrieval (PRVR) is a practical yet challenging task\nin text-to-video retrieval, where videos are untrimmed and contain much\nbackground content. The pursuit here is of both effective and efficient\nsolutions to capture the partial correspondence between text queries and\nuntrimmed videos. Existing PRVR methods, which typically focus on modeling\nmulti-scale clip representations, however, suffer from content independence and\ninformation redundancy, impairing retrieval performance. To overcome these\nlimitations, we propose a simple yet effective approach with active moment\ndiscovering (AMDNet). We are committed to discovering video moments that are\nsemantically consistent with their queries. By using learnable span anchors to\ncapture distinct moments and applying masked multi-moment attention to\nemphasize salient moments while suppressing redundant backgrounds, we achieve\nmore compact and informative video representations. To further enhance moment\nmodeling, we introduce a moment diversity loss to encourage different moments\nof distinct regions and a moment relevance loss to promote semantically\nquery-relevant moments, which cooperate with a partially relevant retrieval\nloss for end-to-end optimization. Extensive experiments on two large-scale\nvideo datasets (\\ie, TVR and ActivityNet Captions) demonstrate the superiority\nand efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller\n(\\#parameters) while 6.0 points higher (SumR) than the up-to-date method\nGMMFormer on TVR."}
{"id": "2504.10650", "pdf": "https://arxiv.org/pdf/2504.10650", "abs": "https://arxiv.org/abs/2504.10650", "authors": ["Éva Székely", "Jūra Miniota", "Míša", "Hejná"], "title": "Will AI shape the way we speak? The emerging sociolinguistic influence of synthetic voices", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "eess.AS", "I.2.7; K.4.2; H.5.2"], "comment": "5 pages, 0 figures, International Workshop on Spoken Dialogue Systems\n  Technology (IWSDS) 2025", "summary": "The growing prevalence of conversational voice interfaces, powered by\ndevelopments in both speech and language technologies, raises important\nquestions about their influence on human communication. While written\ncommunication can signal identity through lexical and stylistic choices,\nvoice-based interactions inherently amplify socioindexical elements - such as\naccent, intonation, and speech style - which more prominently convey social\nidentity and group affiliation. There is evidence that even passive media such\nas television is likely to influence the audience's linguistic patterns. Unlike\npassive media, conversational AI is interactive, creating a more immersive and\nreciprocal dynamic that holds a greater potential to impact how individuals\nspeak in everyday interactions. Such heightened influence can be expected to\narise from phenomena such as acoustic-prosodic entrainment and linguistic\naccommodation, which occur naturally during interaction and enable users to\nadapt their speech patterns in response to the system. While this phenomenon is\nstill emerging, its potential societal impact could provide organisations,\nmovements, and brands with a subtle yet powerful avenue for shaping and\ncontrolling public perception and social identity. We argue that the\nsocioindexical influence of AI-generated speech warrants attention and should\nbecome a focus of interdisciplinary research, leveraging new and existing\nmethodologies and technologies to better understand its implications."}
{"id": "2504.10929", "pdf": "https://arxiv.org/pdf/2504.10929", "abs": "https://arxiv.org/abs/2504.10929", "authors": ["Chang Yu", "Yisi Luo", "Kai Ye", "Xile Zhao", "Deyu Meng"], "title": "Cross-Frequency Implicit Neural Representation with Self-Evolving Parameters", "categories": ["cs.CV"], "comment": null, "summary": "Implicit neural representation (INR) has emerged as a powerful paradigm for\nvisual data representation. However, classical INR methods represent data in\nthe original space mixed with different frequency components, and several\nfeature encoding parameters (e.g., the frequency parameter $\\omega$ or the rank\n$R$) need manual configurations. In this work, we propose a self-evolving\ncross-frequency INR using the Haar wavelet transform (termed CF-INR), which\ndecouples data into four frequency components and employs INRs in the wavelet\nspace. CF-INR allows the characterization of different frequency components\nseparately, thus enabling higher accuracy for data representation. To more\nprecisely characterize cross-frequency components, we propose a cross-frequency\ntensor decomposition paradigm for CF-INR with self-evolving parameters, which\nautomatically updates the rank parameter $R$ and the frequency parameter\n$\\omega$ for each frequency component through self-evolving optimization. This\nself-evolution paradigm eliminates the laborious manual tuning of these\nparameters, and learns a customized cross-frequency feature encoding\nconfiguration for each dataset. We evaluate CF-INR on a variety of visual data\nrepresentation and recovery tasks, including image regression, inpainting,\ndenoising, and cloud removal. Extensive experiments demonstrate that CF-INR\noutperforms state-of-the-art methods in each case."}
{"id": "2504.10738", "pdf": "https://arxiv.org/pdf/2504.10738", "abs": "https://arxiv.org/abs/2504.10738", "authors": ["Ankit Kumar Shaw", "Kun Jiang", "Tuopu Wen", "Chandan Kumar Sah", "Yining Shi", "Mengmeng Yang", "Diange Yang", "Xiaoli Lian"], "title": "CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "I.2.9; I.2.7; I.2.10; I.5.5; I.5.4; I.2.11"], "comment": "Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author.\n  The main paper and supplementary material are both included here, total 23\n  pages (main paper is 10 pages and supplementary material is 13 pages), total\n  17 figures (6 figures in main paper and 11 figures in supplementary\n  material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code\n  will be available at https://Ankit-Zefan.github.io/CleanMap/", "summary": "The rapid growth of intelligent connected vehicles (ICVs) and integrated\nvehicle-road-cloud systems has increased the demand for accurate, real-time HD\nmap updates. However, ensuring map reliability remains challenging due to\ninconsistencies in crowdsourced data, which suffer from motion blur, lighting\nvariations, adverse weather, and lane marking degradation. This paper\nintroduces CleanMAP, a Multimodal Large Language Model (MLLM)-based\ndistillation framework designed to filter and refine crowdsourced data for\nhigh-confidence HD map updates. CleanMAP leverages an MLLM-driven lane\nvisibility scoring model that systematically quantifies key visual parameters,\nassigning confidence scores (0-10) based on their impact on lane detection. A\nnovel dynamic piecewise confidence-scoring function adapts scores based on lane\nvisibility, ensuring strong alignment with human evaluations while effectively\nfiltering unreliable data. To further optimize map accuracy, a\nconfidence-driven local map fusion strategy ranks and selects the top-k\nhighest-scoring local maps within an optimal confidence range (best score minus\n10%), striking a balance between data quality and quantity. Experimental\nevaluations on a real-world autonomous vehicle dataset validate CleanMAP's\neffectiveness, demonstrating that fusing the top three local maps achieves the\nlowest mean map update error of 0.28m, outperforming the baseline (0.37m) and\nmeeting stringent accuracy thresholds (<= 0.32m). Further validation with\nreal-vehicle data confirms 84.88% alignment with human evaluators, reinforcing\nthe model's robustness and reliability. This work establishes CleanMAP as a\nscalable and deployable solution for crowdsourced HD map updates, ensuring more\nprecise and reliable autonomous navigation. The code will be available at\nhttps://Ankit-Zefan.github.io/CleanMap/"}
{"id": "2504.10958", "pdf": "https://arxiv.org/pdf/2504.10958", "abs": "https://arxiv.org/abs/2504.10958", "authors": ["Alexander Köhler", "Michael Breuß"], "title": "Recognition of Geometrical Shapes by Dictionary Learning", "categories": ["cs.CV", "cs.LG", "00A69, 68U05, 68T05"], "comment": "6 pages, 4 figures, ACDSA 2025 conference", "summary": "Dictionary learning is a versatile method to produce an overcomplete set of\nvectors, called atoms, to represent a given input with only a few atoms. In the\nliterature, it has been used primarily for tasks that explore its powerful\nrepresentation capabilities, such as for image reconstruction. In this work, we\npresent a first approach to make dictionary learning work for shape\nrecognition, considering specifically geometrical shapes. As we demonstrate,\nthe choice of the underlying optimization method has a significant impact on\nrecognition quality. Experimental results confirm that dictionary learning may\nbe an interesting method for shape recognition tasks."}
{"id": "2504.10766", "pdf": "https://arxiv.org/pdf/2504.10766", "abs": "https://arxiv.org/abs/2504.10766", "authors": ["Ming Li", "Yanhong Li", "Ziyue Li", "Tianyi Zhou"], "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training."}
{"id": "2504.10967", "pdf": "https://arxiv.org/pdf/2504.10967", "abs": "https://arxiv.org/abs/2504.10967", "authors": ["Yubin Gu", "Yuan Meng", "Kaihang Zheng", "Xiaoshuai Sun", "Jiayi Ji", "Weijian Ruan", "Liujuan Cao", "Rongrong Ji"], "title": "An Efficient and Mixed Heterogeneous Model for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration~(IR), as a fundamental multimedia data processing task, has\na significant impact on downstream visual applications. In recent years,\nresearchers have focused on developing general-purpose IR models capable of\nhandling diverse degradation types, thereby reducing the cost and complexity of\nmodel development. Current mainstream approaches are based on three\narchitectural paradigms: CNNs, Transformers, and Mambas. CNNs excel in\nefficient inference, whereas Transformers and Mamba excel at capturing\nlong-range dependencies and modeling global contexts. While each architecture\nhas demonstrated success in specialized, single-task settings, limited efforts\nhave been made to effectively integrate heterogeneous architectures to jointly\naddress diverse IR challenges. To bridge this gap, we propose RestorMixer, an\nefficient and general-purpose IR model based on mixed-architecture fusion.\nRestorMixer adopts a three-stage encoder-decoder structure, where each stage is\ntailored to the resolution and feature characteristics of the input. In the\ninitial high-resolution stage, CNN-based blocks are employed to rapidly extract\nshallow local features. In the subsequent stages, we integrate a refined\nmulti-directional scanning Mamba module with a multi-scale window-based\nself-attention mechanism. This hierarchical and adaptive design enables the\nmodel to leverage the strengths of CNNs in local feature extraction, Mamba in\nglobal context modeling, and attention mechanisms in dynamic feature\nrefinement. Extensive experimental results demonstrate that RestorMixer\nachieves leading performance across multiple IR tasks while maintaining high\ninference efficiency. The official code can be accessed at\nhttps://github.com/ClimBin/RestorMixer."}
{"id": "2504.10816", "pdf": "https://arxiv.org/pdf/2504.10816", "abs": "https://arxiv.org/abs/2504.10816", "authors": ["Zhichao Xu", "Aosong Feng", "Yijun Tian", "Haibo Ding", "Lin Leee Cheong"], "title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "In recent years, dense retrieval has been the focus of information retrieval\n(IR) research. While effective, dense retrieval produces uninterpretable dense\nvectors, and suffers from the drawback of large index size. Learned sparse\nretrieval (LSR) has emerged as promising alternative, achieving competitive\nretrieval performance while also being able to leverage the classical inverted\nindex data structure for efficient retrieval. However, limited works have\nexplored scaling LSR beyond BERT scale. In this work, we identify two\nchallenges in training large language models (LLM) for LSR: (1) training\ninstability during the early stage of contrastive training; (2) suboptimal\nperformance due to pre-trained LLM's unidirectional attention. To address these\nchallenges, we propose two corresponding techniques: (1) a lightweight\nadaptation training phase to eliminate training instability; (2) two model\nvariants to enable bidirectional information. With these techniques, we are\nable to train LSR models with 8B scale LLM, and achieve competitive retrieval\nperformance with reduced index size. Furthermore, we are among the first to\nanalyze the performance-efficiency tradeoff of LLM-based LSR model through the\nlens of model quantization. Our findings provide insights into adapting LLMs\nfor efficient retrieval modeling."}
{"id": "2504.10972", "pdf": "https://arxiv.org/pdf/2504.10972", "abs": "https://arxiv.org/abs/2504.10972", "authors": ["Yihang Liu", "Lianghua He", "Ying Wen", "Longzhen Yang", "Hongzhou Chen"], "title": "AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images", "categories": ["cs.CV"], "comment": null, "summary": "Current self-supervised methods, such as contrastive learning, predominantly\nfocus on global discrimination, neglecting the critical fine-grained anatomical\ndetails required for accurate radiographic analysis. To address this challenge,\nwe propose an Anatomy-driven self-supervised framework for enhancing\nFine-grained Representation in radiographic image analysis (AFiRe). The core\nidea of AFiRe is to align the anatomical consistency with the unique\ntoken-processing characteristics of Vision Transformer. Specifically, AFiRe\nsynergistically performs two self-supervised schemes: (i) Token-wise\nanatomy-guided contrastive learning, which aligns image tokens based on\nstructural and categorical consistency, thereby enhancing fine-grained\nspatial-anatomical discrimination; (ii) Pixel-level anomaly-removal\nrestoration, which particularly focuses on local anomalies, thereby refining\nthe learned discrimination with detailed geometrical information. Additionally,\nwe propose Synthetic Lesion Mask to enhance anatomical diversity while\npreserving intra-consistency, which is typically corrupted by traditional data\naugmentations, such as Cropping and Affine transformations. Experimental\nresults show that AFiRe: (i) provides robust anatomical discrimination,\nachieving more cohesive feature clusters compared to state-of-the-art\ncontrastive learning methods; (ii) demonstrates superior generalization,\nsurpassing 7 radiography-specific self-supervised methods in multi-label\nclassification tasks with limited labeling; and (iii) integrates fine-grained\ninformation, enabling precise anomaly detection using only image-level\nannotations."}
{"id": "2504.10886", "pdf": "https://arxiv.org/pdf/2504.10886", "abs": "https://arxiv.org/abs/2504.10886", "authors": ["Jiseon Kim", "Jea Kwon", "Luiz Felipe Vecchietti", "Alice Oh", "Meeyoung Cha"], "title": "Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Accepted to ICLR 2025 Workshop - BiAlign (Bidirectional Human-AI\n  Alignment)", "summary": "Deploying large language models (LLMs) with agency in real-world applications\nraises critical questions about how these models will behave. In particular,\nhow will their decisions align with humans when faced with moral dilemmas? This\nstudy examines the alignment between LLM-driven decisions and human judgment in\nvarious contexts of the moral machine experiment, including personas reflecting\ndifferent sociodemographics. We find that the moral decisions of LLMs vary\nsubstantially by persona, showing greater shifts in moral decisions for\ncritical tasks than humans. Our data also indicate an interesting partisan\nsorting phenomenon, where political persona predominates the direction and\ndegree of LLM decisions. We discuss the ethical implications and risks\nassociated with deploying these models in applications that involve moral\ndecisions."}
{"id": "2504.10974", "pdf": "https://arxiv.org/pdf/2504.10974", "abs": "https://arxiv.org/abs/2504.10974", "authors": ["Zhisheng Zhang", "Peng Zhang", "Fengxiang Wang", "Liangli Ma", "Fuchun Sun"], "title": "Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Enhancing forward-looking sonar images is critical for accurate underwater\ntarget detection. Current deep learning methods mainly rely on supervised\ntraining with simulated data, but the difficulty in obtaining high-quality\nreal-world paired data limits their practical use and generalization. Although\nself-supervised approaches from remote sensing partially alleviate data\nshortages, they neglect the cross-modal degradation gap between sonar and\nremote sensing images. Directly transferring pretrained weights often leads to\noverly smooth sonar images, detail loss, and insufficient brightness. To\naddress this, we propose a feature-space transformation that maps sonar images\nfrom the pixel domain to a robust feature domain, effectively bridging the\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\nleverages complementary inter-frame information to naturally remove speckle\nnoise and enhance target-region brightness. Experiments on three self-collected\nreal-world forward-looking sonar datasets show that our method significantly\noutperforms existing approaches, effectively suppressing noise, preserving\ndetailed edges, and substantially improving brightness, demonstrating strong\npotential for underwater target detection applications."}
{"id": "2504.10893", "pdf": "https://arxiv.org/pdf/2504.10893", "abs": "https://arxiv.org/abs/2504.10893", "authors": ["Yize Zhang", "Tianshu Wang", "Sirui Chen", "Kun Wang", "Xingyu Zeng", "Hongyu Lin", "Xianpei Han", "Le Sun", "Chaochao Lu"], "title": "ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search", "categories": ["cs.AI", "cs.CL"], "comment": "Project homepage: https://opencausalab.github.io/ARise", "summary": "Large language models (LLMs) have demonstrated impressive capabilities and\nare receiving increasing attention to enhance their reasoning through scaling\ntest--time compute. However, their application in open--ended,\nknowledge--intensive, complex reasoning scenarios is still limited.\nReasoning--oriented methods struggle to generalize to open--ended scenarios due\nto implicit assumptions of complete world knowledge. Meanwhile,\nknowledge--augmented reasoning (KAR) methods fail to address two core\nchallenges: 1) error propagation, where errors in early steps cascade through\nthe chain, and 2) verification bottleneck, where the explore--exploit tradeoff\narises in multi--branch decision processes. To overcome these limitations, we\nintroduce ARise, a novel framework that integrates risk assessment of\nintermediate reasoning states with dynamic retrieval--augmented generation\n(RAG) within a Monte Carlo tree search paradigm. This approach enables\neffective construction and optimization of reasoning plans across multiple\nmaintained hypothesis branches. Experimental results show that ARise\nsignificantly outperforms the state--of--the--art KAR methods by up to 23.10%,\nand the latest RAG-equipped large reasoning models by up to 25.37%."}
{"id": "2504.10976", "pdf": "https://arxiv.org/pdf/2504.10976", "abs": "https://arxiv.org/abs/2504.10976", "authors": ["Linhao Li", "Yongzhang Tan", "Siyuan Yang", "Hao Cheng", "Yongfeng Dong", "Liang Yang"], "title": "Adaptive Decision Boundary for Few-Shot Class-Incremental Learning", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses from a limited set of training samples without forgetting knowledge of\npreviously learned classes. Conventional FSCIL methods typically build a robust\nfeature extractor during the base training session with abundant training\nsamples and subsequently freeze this extractor, only fine-tuning the classifier\nin subsequent incremental phases. However, current strategies primarily focus\non preventing catastrophic forgetting, considering only the relationship\nbetween novel and base classes, without paying attention to the specific\ndecision spaces of each class. To address this challenge, we propose a\nplug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible\nwith most FSCIL methods. Specifically, we assign a specific decision boundary\nto each class and adaptively adjust these boundaries during training to\noptimally refine the decision spaces for the classes in each session.\nFurthermore, to amplify the distinctiveness between classes, we employ a novel\ninter-class constraint loss that optimizes the decision boundaries and\nprototypes for each class. Extensive experiments on three benchmarks, namely\nCIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS\nmethod with existing FSCIL techniques significantly improves performance,\nachieving overall state-of-the-art results."}
{"id": "2504.11190", "pdf": "https://arxiv.org/pdf/2504.11190", "abs": "https://arxiv.org/abs/2504.11190", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "title": "Enhancing multimodal analogical reasoning with Logic Augmented Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods."}
{"id": "2504.10979", "pdf": "https://arxiv.org/pdf/2504.10979", "abs": "https://arxiv.org/abs/2504.10979", "authors": ["Pancheng Zhao", "Deng-Ping Fan", "Shupeng Cheng", "Salman Khan", "Fahad Shahbaz Khan", "David Clifton", "Peng Xu", "Jufeng Yang"], "title": "Deep Learning in Concealed Dense Prediction", "categories": ["cs.CV"], "comment": "Technique Report", "summary": "Deep learning is developing rapidly and handling common computer vision tasks\nwell. It is time to pay attention to more complex vision tasks, as model size,\nknowledge, and reasoning capabilities continue to improve. In this paper, we\nintroduce and review a family of complex tasks, termed Concealed Dense\nPrediction (CDP), which has great value in agriculture, industry, etc. CDP's\nintrinsic trait is that the targets are concealed in their surroundings, thus\nfully perceiving them requires fine-grained representations, prior knowledge,\nauxiliary reasoning, etc. The contributions of this review are three-fold: (i)\nWe introduce the scope, characteristics, and challenges specific to CDP tasks\nand emphasize their essential differences from generic vision tasks. (ii) We\ndevelop a taxonomy based on concealment counteracting to summarize deep\nlearning efforts in CDP through experiments on three tasks. We compare 25\nstate-of-the-art methods across 12 widely used concealed datasets. (iii) We\ndiscuss the potential applications of CDP in the large model era and summarize\n6 potential research directions. We offer perspectives for the future\ndevelopment of CDP by constructing a large-scale multimodal instruction\nfine-tuning dataset, CvpINST, and a concealed visual perception agent,\nCvpAgent."}
{"id": "2504.11239", "pdf": "https://arxiv.org/pdf/2504.11239", "abs": "https://arxiv.org/abs/2504.11239", "authors": ["Chang Yang", "Ruiyu Wang", "Junzhe Jiang", "Qi Jiang", "Qinggang Zhang", "Yanchen Deng", "Shuxin Li", "Shuyue Hu", "Bo Li", "Florian T. Pokorny", "Xiao Huang", "Xinrun Wang"], "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Preliminary work, 10 pages for main text", "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI)."}
{"id": "2504.10984", "pdf": "https://arxiv.org/pdf/2504.10984", "abs": "https://arxiv.org/abs/2504.10984", "authors": ["Sami Arja", "Nimrod Kruger", "Alexandre Marcireau", "Nicholas Owen Ralph", "Saeed Afshar", "Gregory Cohen"], "title": "Seeing like a Cephalopod: Colour Vision with a Monochrome Event Camera", "categories": ["cs.CV"], "comment": "15 pages, 14 figures, 1 table. Accepted at CVPR 2025 (Workshop on\n  Event-based Vision)", "summary": "Cephalopods exhibit unique colour discrimination capabilities despite having\none type of photoreceptor, relying instead on chromatic aberration induced by\ntheir ocular optics and pupil shapes to perceive spectral information. We took\ninspiration from this biological mechanism to design a spectral imaging system\nthat combines a ball lens with an event-based camera. Our approach relies on a\nmotorised system that shifts the focal position, mirroring the adaptive lens\nmotion in cephalopods. This approach has enabled us to achieve\nwavelength-dependent focusing across the visible light and near-infrared\nspectrum, making the event a spectral sensor. We characterise chromatic\naberration effects, using both event-based and conventional frame-based\nsensors, validating the effectiveness of bio-inspired spectral discrimination\nboth in simulation and in a real setup as well as assessing the spectral\ndiscrimination performance. Our proposed approach provides a robust spectral\nsensing capability without conventional colour filters or computational\ndemosaicing. This approach opens new pathways toward new spectral sensing\nsystems inspired by nature's evolutionary solutions. Code and analysis are\navailable at: https://samiarja.github.io/neuromorphic_octopus_eye/"}
{"id": "2504.11243", "pdf": "https://arxiv.org/pdf/2504.11243", "abs": "https://arxiv.org/abs/2504.11243", "authors": ["Balahari Vignesh Balu", "Florian Geissler", "Francesco Carella", "Joao-Vitor Zacchi", "Josef Jiru", "Nuria Mata", "Reinhard Stolle"], "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods."}
{"id": "2504.10985", "pdf": "https://arxiv.org/pdf/2504.10985", "abs": "https://arxiv.org/abs/2504.10985", "authors": ["Minghui Lin", "Shu Wang", "Xiang Wang", "Jianhua Tang", "Longbin Fu", "Zhengrong Zuo", "Nong Sang"], "title": "DMPT: Decoupled Modality-aware Prompt Tuning for Multi-modal Object Re-identification", "categories": ["cs.CV"], "comment": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision\n  (WACV)", "summary": "Current multi-modal object re-identification approaches based on large-scale\npre-trained backbones (i.e., ViT) have displayed remarkable progress and\nachieved excellent performance. However, these methods usually adopt the\nstandard full fine-tuning paradigm, which requires the optimization of\nconsiderable backbone parameters, causing extensive computational and storage\nrequirements. In this work, we propose an efficient prompt-tuning framework\ntailored for multi-modal object re-identification, dubbed DMPT, which freezes\nthe main backbone and only optimizes several newly added decoupled\nmodality-aware parameters. Specifically, we explicitly decouple the visual\nprompts into modality-specific prompts which leverage prior modality knowledge\nfrom a powerful text encoder and modality-independent semantic prompts which\nextract semantic information from multi-modal inputs, such as visible,\nnear-infrared, and thermal-infrared. Built upon the extracted features, we\nfurther design a Prompt Inverse Bind (PromptIBind) strategy that employs bind\nprompts as a medium to connect the semantic prompt tokens of different\nmodalities and facilitates the exchange of complementary multi-modal\ninformation, boosting final re-identification results. Experimental results on\nmultiple common benchmarks demonstrate that our DMPT can achieve competitive\nresults to existing state-of-the-art methods while requiring only 6.5%\nfine-tuning of the backbone parameters."}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257", "abs": "https://arxiv.org/abs/2504.11257", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/"}
{"id": "2504.10986", "pdf": "https://arxiv.org/pdf/2504.10986", "abs": "https://arxiv.org/abs/2504.10986", "authors": ["Bo-Cheng Hu", "Ge-Peng Ji", "Dian Shao", "Deng-Ping Fan"], "title": "PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Technical report (4 tables 3 figures 8 pages)", "summary": "Accurate medical image segmentation is essential for effective diagnosis and\ntreatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by\nintroducing a reverse attention (RA) module that utilizes background\ninformation. However, PraNet-V1 struggles with multi-class segmentation tasks.\nTo address this limitation, we propose PraNet-V2, which, compared to PraNet-V1,\neffectively performs a broader range of tasks including multi-class\nsegmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention\n(DSRA) module, which incorporates explicit background supervision, independent\nbackground modeling, and semantically enriched attention fusion. Our PraNet-V2\nframework demonstrates strong performance on four polyp segmentation datasets.\nAdditionally, by integrating DSRA to iteratively enhance foreground\nsegmentation results in three state-of-the-art semantic segmentation models, we\nachieve up to a 1.36% improvement in mean Dice score. Code is available at:\nhttps://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor."}
{"id": "2504.11281", "pdf": "https://arxiv.org/pdf/2504.11281", "abs": "https://arxiv.org/abs/2504.11281", "authors": ["Chaoran Chen", "Zhiping Zhang", "Bingcan Guo", "Shang Ma", "Ibrahim Khalilov", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "title": "The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to Fine-Print Injections", "categories": ["cs.HC", "cs.CL", "cs.CR"], "comment": null, "summary": "A Large Language Model (LLM) powered GUI agent is a specialized autonomous\nsystem that performs tasks on the user's behalf according to high-level\ninstructions. It does so by perceiving and interpreting the graphical user\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\nsequences of actions, and then interacting with GUIs by executing the actions\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\nfilling forms or booking services, GUI agents often need to process and act on\nsensitive user data. However, this autonomy introduces new privacy and security\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\nbehaviors or induces unintended disclosures of private information. These\nattacks often exploit the discrepancy between visual saliency for agents and\nhuman users, or the agent's limited ability to detect violations of contextual\nintegrity in task automation. In this paper, we characterized six types of such\nattacks, and conducted an experimental study to test these attacks with six\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\nparticularly to contextually embedded threats. Moreover, human users are also\nsusceptible to many of these attacks, indicating that simple human oversight\nmay not reliably prevent failures. This misalignment highlights the need for\nprivacy-aware agent design. We propose practical defense strategies to inform\nthe development of safer and more reliable GUI agents."}
{"id": "2504.10995", "pdf": "https://arxiv.org/pdf/2504.10995", "abs": "https://arxiv.org/abs/2504.10995", "authors": ["Chaoyang Wang", "Zeyu Zhang", "Long Teng", "Zijun Li", "Shichao Kan"], "title": "TMCIR: Token Merge Benefits Composed Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2310.05473 by other authors", "summary": "Composed Image Retrieval (CIR) retrieves target images using a multi-modal\nquery that combines a reference image with text describing desired\nmodifications. The primary challenge is effectively fusing this visual and\ntextual information. Current cross-modal feature fusion approaches for CIR\nexhibit an inherent bias in intention interpretation. These methods tend to\ndisproportionately emphasize either the reference image features\n(visual-dominant fusion) or the textual modification intent (text-dominant\nfusion through image-to-text conversion). Such an imbalanced representation\noften fails to accurately capture and reflect the actual search intent of the\nuser in the retrieval results. To address this challenge, we propose TMCIR, a\nnovel framework that advances composed image retrieval through two key\ninnovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP\nencoders contrastively using intent-reflecting pseudo-target images,\nsynthesized from reference images and textual descriptions via a diffusion\nmodel. This step enhances the encoder ability of text to capture nuanced\nintents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune\nall encoders contrastively by comparing adaptive token-fusion features with the\ntarget image. This mechanism dynamically balances visual and textual\nrepresentations within the contrastive learning pipeline, optimizing the\ncomposed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR\ndatasets demonstrate that TMCIR significantly outperforms state-of-the-art\nmethods, particularly in capturing nuanced user intent."}
{"id": "2504.11336", "pdf": "https://arxiv.org/pdf/2504.11336", "abs": "https://arxiv.org/abs/2504.11336", "authors": ["Abitha Thankaraj", "Yiding Jiang", "J. Zico Kolter", "Yonatan Bisk"], "title": "Looking beyond the next token", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm."}
{"id": "2504.11008", "pdf": "https://arxiv.org/pdf/2504.11008", "abs": "https://arxiv.org/abs/2504.11008", "authors": ["Qinyue Tong", "Ziqian Lu", "Jun Liu", "Yangming Zheng", "Zheming Lu"], "title": "MediSee: Reasoning-based Pixel-level Perception in Medical Images", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Despite remarkable advancements in pixel-level medical image perception,\nexisting methods are either limited to specific tasks or heavily rely on\naccurate bounding boxes or text labels as input prompts. However, the medical\nknowledge required for input is a huge obstacle for general public, which\ngreatly reduces the universality of these methods. Compared with these\ndomain-specialized auxiliary information, general users tend to rely on oral\nqueries that require logical reasoning. In this paper, we introduce a novel\nmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),\nwhich aims to comprehend implicit queries about medical images and generate the\ncorresponding segmentation mask and bounding box for the target object. To\naccomplish this task, we first introduce a Multi-perspective, Logic-driven\nMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, which\nencompasses a substantial collection of medical entity targets along with their\ncorresponding reasoning. Furthermore, we propose MediSee, an effective baseline\nmodel designed for medical reasoning segmentation and detection. The\nexperimental results indicate that the proposed method can effectively address\nMedSD with implicit colloquial queries and outperform traditional medical\nreferring segmentation methods."}
{"id": "2504.11343", "pdf": "https://arxiv.org/pdf/2504.11343", "abs": "https://arxiv.org/abs/2504.11343", "authors": ["Wei Xiong", "Jiarui Yao", "Yuhui Xu", "Bo Pang", "Lei Wang", "Doyen Sahoo", "Junnan Li", "Nan Jiang", "Tong Zhang", "Caiming Xiong", "Hanze Dong"], "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "12 pages, 4 figures", "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training."}
{"id": "2504.11014", "pdf": "https://arxiv.org/pdf/2504.11014", "abs": "https://arxiv.org/abs/2504.11014", "authors": ["Eunsoo Im", "Jung Kwon Lee", "Changhyun Jee"], "title": "GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*", "categories": ["cs.CV", "cs.AI"], "comment": "9pages, 1 supple", "summary": "The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/"}
{"id": "2504.11364", "pdf": "https://arxiv.org/pdf/2504.11364", "abs": "https://arxiv.org/abs/2504.11364", "authors": ["Tianwei Ni", "Allen Nie", "Sapana Chaudhary", "Yao Liu", "Huzefa Rangwala", "Rasool Fakoor"], "title": "Teaching Large Language Models to Reason through Learning and Forgetting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$."}
{"id": "2504.11015", "pdf": "https://arxiv.org/pdf/2504.11015", "abs": "https://arxiv.org/abs/2504.11015", "authors": ["Chenyang Zhu", "Xing Zhang", "Yuyang Sun", "Ching-Chun Chang", "Isao Echizen"], "title": "AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in image generation, particularly diffusion models, have\nsignificantly lowered the barrier for creating sophisticated forgeries, making\nimage manipulation detection and localization (IMDL) increasingly challenging.\nWhile prior work in IMDL has focused largely on natural images, the anime\ndomain remains underexplored-despite its growing vulnerability to AI-generated\nforgeries. Misrepresentations of AI-generated images as hand-drawn artwork,\ncopyright violations, and inappropriate content modifications pose serious\nthreats to the anime community and industry. To address this gap, we propose\nAnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive\nannotations. It comprises over two million images including real, partially\nmanipulated, and fully AI-generated samples. Experiments indicate that models\ntrained on existing IMDL datasets of natural images perform poorly when applied\nto anime images, highlighting a clear domain gap between anime and natural\nimages. To better handle IMDL tasks in anime domain, we further propose\nAniXplore, a novel model tailored to the visual characteristics of anime\nimagery. Extensive evaluations demonstrate that AniXplore achieves superior\nperformance compared to existing methods. Dataset and code can be found in\nhttps://flytweety.github.io/AnimeDL2M/."}
{"id": "2504.11367", "pdf": "https://arxiv.org/pdf/2504.11367", "abs": "https://arxiv.org/abs/2504.11367", "authors": ["Rui Tang", "Ziyun Yong", "Shuyu Jiang", "Xingshu Chen", "Yaofang Liu", "Yi-Cheng Zhang", "Gui-Quan Sun", "Wei Wang"], "title": "Network Alignment", "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "Complex networks are frequently employed to model physical or virtual complex\nsystems. When certain entities exist across multiple systems simultaneously,\nunveiling their corresponding relationships across the networks becomes\ncrucial. This problem, known as network alignment, holds significant\nimportance. It enhances our understanding of complex system structures and\nbehaviours, facilitates the validation and extension of theoretical physics\nresearch about studying complex systems, and fosters diverse practical\napplications across various fields. However, due to variations in the\nstructure, characteristics, and properties of complex networks across different\nfields, the study of network alignment is often isolated within each domain,\nwith even the terminologies and concepts lacking uniformity. This review\ncomprehensively summarizes the latest advancements in network alignment\nresearch, focusing on analyzing network alignment characteristics and progress\nin various domains such as social network analysis, bioinformatics,\ncomputational linguistics and privacy protection. It provides a detailed\nanalysis of various methods' implementation principles, processes, and\nperformance differences, including structure consistency-based methods, network\nembedding-based methods, and graph neural network-based (GNN-based) methods.\nAdditionally, the methods for network alignment under different conditions,\nsuch as in attributed networks, heterogeneous networks, directed networks, and\ndynamic networks, are presented. Furthermore, the challenges and the open\nissues for future studies are also discussed."}
{"id": "2504.11019", "pdf": "https://arxiv.org/pdf/2504.11019", "abs": "https://arxiv.org/abs/2504.11019", "authors": ["Hyejin Lee", "Seokjun Hong", "Jeonghoon Song", "Haechan Cho", "Zhixiong Jin", "Byeonghun Kim", "Joobin Jin", "Jaegyun Im", "Byeongjoon Noh", "Hwasoo Yeo"], "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen", "categories": ["cs.CV", "I.2.10; I.4.8; H.2.8; J.7"], "comment": "30 pages, 15 figures", "summary": "Reliable traffic data are essential for understanding urban mobility and\ndeveloping effective traffic management strategies. This study introduces the\nDRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale\nurban traffic dataset collected systematically from synchronized drone videos\nat approximately 250 meters altitude, covering nine interconnected\nintersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle\ntrajectories that include directional information, processed through video\nsynchronization and orthomap alignment, resulting in a comprehensive dataset of\n81,699 vehicle trajectories. Through our DRIFT dataset, researchers can\nsimultaneously analyze traffic at multiple scales - from individual vehicle\nmaneuvers like lane-changes and safety metrics such as time-to-collision to\naggregate network flow dynamics across interconnected urban intersections. The\nDRIFT dataset is structured to enable immediate use without additional\npreprocessing, complemented by open-source models for object detection and\ntrajectory extraction, as well as associated analytical tools. DRIFT is\nexpected to significantly contribute to academic research and practical\napplications, such as traffic flow analysis and simulation studies. The dataset\nand related resources are publicly accessible at\nhttps://github.com/AIxMobility/The-DRIFT."}
{"id": "2504.11393", "pdf": "https://arxiv.org/pdf/2504.11393", "abs": "https://arxiv.org/abs/2504.11393", "authors": ["Ian Magnusson", "Nguyen Tai", "Ben Bogin", "David Heineman", "Jena D. Hwang", "Luca Soldaini", "Akshita Bhagia", "Jiacheng Liu", "Dirk Groeneveld", "Oyvind Tafjord", "Noah A. Smith", "Pang Wei Koh", "Jesse Dodge"], "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute."}
{"id": "2504.11024", "pdf": "https://arxiv.org/pdf/2504.11024", "abs": "https://arxiv.org/abs/2504.11024", "authors": ["Andrea Simonelli", "Norman Müller", "Peter Kontschieder"], "title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The increasing availability of digital 3D environments, whether through\nimage-based 3D reconstruction, generation, or scans obtained by robots, is\ndriving innovation across various applications. These come with a significant\ndemand for 3D interaction, such as 3D Interactive Segmentation, which is useful\nfor tasks like object selection and manipulation. Additionally, there is a\npersistent need for solutions that are efficient, precise, and performing well\nacross diverse settings, particularly in unseen environments and with\nunfamiliar objects. In this work, we introduce a 3D interactive segmentation\nmethod that consistently surpasses previous state-of-the-art techniques on both\nin-domain and out-of-domain datasets. Our simple approach integrates a\nvoxel-based sparse encoder with a lightweight transformer-based decoder that\nimplements implicit click fusion, achieving superior performance and maximizing\nefficiency. Our method demonstrates substantial improvements on benchmark\ndatasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on\nunseen geometric distributions such as the ones obtained by Gaussian Splatting.\nThe project web-page is available at https://simonelli-andrea.github.io/easy3d."}
{"id": "2504.11441", "pdf": "https://arxiv.org/pdf/2504.11441", "abs": "https://arxiv.org/abs/2504.11441", "authors": ["Elizabeth Fons", "Rachneet Kaur", "Zhen Zeng", "Soham Palande", "Tucker Balch", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "TADACap: Time-series Adaptive Domain-Aware Captioning", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICAIF 2024", "summary": "While image captioning has gained significant attention, the potential of\ncaptioning time-series images, prevalent in areas like finance and healthcare,\nremains largely untapped. Existing time-series captioning methods typically\noffer generic, domain-agnostic descriptions of time-series shapes and struggle\nto adapt to new domains without substantial retraining. To address these\nlimitations, we introduce TADACap, a retrieval-based framework to generate\ndomain-aware captions for time-series images, capable of adapting to new\ndomains without retraining. Building on TADACap, we propose a novel retrieval\nstrategy that retrieves diverse image-caption pairs from a target domain\ndatabase, namely TADACap-diverse. We benchmarked TADACap-diverse against\nstate-of-the-art methods and ablation variants. TADACap-diverse demonstrates\ncomparable semantic accuracy while requiring significantly less annotation\neffort."}
{"id": "2504.11034", "pdf": "https://arxiv.org/pdf/2504.11034", "abs": "https://arxiv.org/abs/2504.11034", "authors": ["Fatemeh Amerehi", "Patrick Healy"], "title": "Defending Against Frequency-Based Attacks with Diffusion Models", "categories": ["cs.CV"], "comment": "Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW), 5th Workshop on Adversarial Machine Learning in Computer Vision:\n  Foundation Models + X", "summary": "Adversarial training is a common strategy for enhancing model robustness\nagainst adversarial attacks. However, it is typically tailored to the specific\nattack types it is trained on, limiting its ability to generalize to unseen\nthreat models. Adversarial purification offers an alternative by leveraging a\ngenerative model to remove perturbations before classification. Since the\npurifier is trained independently of both the classifier and the threat models,\nit is better equipped to handle previously unseen attack scenarios. Diffusion\nmodels have proven highly effective for noise purification, not only in\ncountering pixel-wise adversarial perturbations but also in addressing\nnon-adversarial data shifts. In this study, we broaden the focus beyond\npixel-wise robustness to explore the extent to which purification can mitigate\nboth spectral and spatial adversarial attacks. Our findings highlight its\neffectiveness in handling diverse distortion patterns across low- to\nhigh-frequency regions."}
{"id": "2504.11038", "pdf": "https://arxiv.org/pdf/2504.11038", "abs": "https://arxiv.org/abs/2504.11038", "authors": ["Yudong Zhang", "Ruobing Xie", "Jiansheng Chen", "Xingwu Sun", "Zhanhui Kang", "Yu Wang"], "title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by NAACL 2025 main", "summary": "In typical multimodal tasks, such as Visual Question Answering (VQA),\nadversarial attacks targeting a specific image and question can lead large\nvision-language models (LVLMs) to provide incorrect answers. However, it is\ncommon for a single image to be associated with multiple questions, and LVLMs\nmay still answer other questions correctly even for an adversarial image\nattacked by a specific question. To address this, we introduce the\nquery-agnostic visual attack (QAVA), which aims to create robust adversarial\nexamples that generate incorrect responses to unspecified and unknown\nquestions. Compared to traditional adversarial attacks focused on specific\nimages and questions, QAVA significantly enhances the effectiveness and\nefficiency of attacks on images when the question is unknown, achieving\nperformance comparable to attacks on known target questions. Our research\nbroadens the scope of visual adversarial attacks on LVLMs in practical\nsettings, uncovering previously overlooked vulnerabilities, particularly in the\ncontext of visual adversarial threats. The code is available at\nhttps://github.com/btzyd/qava."}
{"id": "2504.11050", "pdf": "https://arxiv.org/pdf/2504.11050", "abs": "https://arxiv.org/abs/2504.11050", "authors": ["Yunshuang Yuan", "Monika Sester"], "title": "Leveraging LLMs and attention-mechanism for automatic annotation of historical maps", "categories": ["cs.CV"], "comment": null, "summary": "Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis."}
{"id": "2504.11055", "pdf": "https://arxiv.org/pdf/2504.11055", "abs": "https://arxiv.org/abs/2504.11055", "authors": ["Alireza Salehi", "Mohammadreza Salehi", "Reshad Hosseini", "Cees G. M. Snoek", "Makoto Yamada", "Mohammad Sabokrou"], "title": "Crane: Context-Guided Prompt Learning and Attention Refinement for Zero-Shot Anomaly Detections", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly Detection (AD) involves identifying deviations from normal data\ndistributions and is critical in fields such as medical diagnostics and\nindustrial defect detection. Traditional AD methods typically require the\navailability of normal training samples; however, this assumption is not always\nfeasible, as collecting such data can be impractical. Additionally, these\nmethods often struggle to generalize across different domains. Recent\nadvancements, such as AnomalyCLIP and AdaCLIP, utilize the zero-shot\ngeneralization capabilities of CLIP but still face a performance gap between\nimage-level and pixel-level anomaly detection. To address this gap, we propose\na novel approach that conditions the prompts of the text encoder based on image\ncontext extracted from the vision encoder. Also, to capture fine-grained\nvariations more effectively, we have modified the CLIP vision encoder and\naltered the extraction of dense features. These changes ensure that the\nfeatures retain richer spatial and structural information for both normal and\nanomalous prompts. Our method achieves state-of-the-art performance, improving\nperformance by 2% to 29% across different metrics on 14 datasets. This\ndemonstrates its effectiveness in both image-level and pixel-level anomaly\ndetection."}
{"id": "2504.11063", "pdf": "https://arxiv.org/pdf/2504.11063", "abs": "https://arxiv.org/abs/2504.11063", "authors": ["Pedro Diaz-Garcia", "Felix Escalona", "Miguel Cazorla"], "title": "UKDM: Underwater keypoint detection and matching using underwater image enhancement techniques", "categories": ["cs.CV"], "comment": null, "summary": "The purpose of this paper is to explore the use of underwater image\nenhancement techniques to improve keypoint detection and matching. By applying\nadvanced deep learning models, including generative adversarial networks and\nconvolutional neural networks, we aim to find the best method which improves\nthe accuracy of keypoint detection and the robustness of matching algorithms.\nWe evaluate the performance of these techniques on various underwater datasets,\ndemonstrating significant improvements over traditional methods."}
{"id": "2504.11066", "pdf": "https://arxiv.org/pdf/2504.11066", "abs": "https://arxiv.org/abs/2504.11066", "authors": ["Marco Micheletto", "Giulia Orrù", "Luca Ghiani", "Gian Luca Marcialis"], "title": "Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Presentation Attack Detection (PAD) systems are usually designed\nindependently of the fingerprint verification system. While this can be\nacceptable for use cases where specific user templates are not predetermined,\nit represents a missed opportunity to enhance security in scenarios where\nintegrating PAD with the fingerprint verification system could significantly\nleverage users' templates, which are the real target of a potential\npresentation attack. This does not mean that a PAD should be specifically\ndesigned for such users; that would imply the availability of many enrolled\nusers' PAI and, consequently, complexity, time, and cost increase. On the\ncontrary, we propose to equip a basic PAD, designed according to the state of\nthe art, with an innovative add-on module called the Closeness Binary Code (CC)\nmodule. The term \"closeness\" refers to a peculiar property of the bona\nfide-related features: in an Euclidean feature space, genuine fingerprints tend\nto cluster in a specific pattern. First, samples from the same finger are close\nto each other, then samples from other fingers of the same user and finally,\nsamples from fingers of other users. This property is statistically verified in\nour previous publication, and further confirmed in this paper. It is\nindependent of the user population and the feature set class, which can be\nhandcrafted or deep network-based (embeddings). Therefore, the add-on can be\ndesigned without the need for the targeted user samples; moreover, it exploits\nher/his samples' \"closeness\" property during the verification stage. Extensive\nexperiments on benchmark datasets and state-of-the-art PAD methods confirm the\nbenefits of the proposed add-on, which can be easily coupled with the main PAD\nmodule integrated into the fingerprint verification system."}
{"id": "2504.11080", "pdf": "https://arxiv.org/pdf/2504.11080", "abs": "https://arxiv.org/abs/2504.11080", "authors": ["Elman Ghazaei", "Erchan Aptoula"], "title": "Change State Space Models for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite their frequent use for change detection, both ConvNets and Vision\ntransformers (ViT) exhibit well-known limitations, namely the former struggle\nto model long-range dependencies while the latter are computationally\ninefficient, rendering them challenging to train on large-scale datasets.\nVision Mamba, an architecture based on State Space Models has emerged as an\nalternative addressing the aforementioned deficiencies and has been already\napplied to remote sensing change detection, though mostly as a feature\nextracting backbone. In this article the Change State Space Model is\nintroduced, that has been specifically designed for change detection by\nfocusing on the relevant changes between bi-temporal images, effectively\nfiltering out irrelevant information. By concentrating solely on the changed\nfeatures, the number of network parameters is reduced, enhancing significantly\ncomputational efficiency while maintaining high detection performance and\nrobustness against input degradation. The proposed model has been evaluated via\nthree benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based\ncounterparts at a fraction of their computational complexity. The\nimplementation will be made available at https://github.com/Elman295/CSSM upon\nacceptance."}
{"id": "2504.11092", "pdf": "https://arxiv.org/pdf/2504.11092", "abs": "https://arxiv.org/abs/2504.11092", "authors": ["Jiaxin Huang", "Sheng Miao", "BangBnag Yang", "Yuewen Ma", "Yiyi Liao"], "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion."}
{"id": "2504.11101", "pdf": "https://arxiv.org/pdf/2504.11101", "abs": "https://arxiv.org/abs/2504.11101", "authors": ["Yulong Zhang", "Tianyi Liang", "Xinyue Huang", "Erfei Cui", "Xu Guo", "Pei Chu", "Chenhui Li", "Ru Zhang", "Wenhai Wang", "Gongshen Liu"], "title": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2\\% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0\\% accuracy gains on\nmathematical calculation tasks, and requiring rephrasing only 7.3\\% of inputs\nwhile maintaining overall performance. Notably, the entire process requires\nneither training nor supervision while maintaining plug-and-play functionality\nthroughout."}
{"id": "2504.11106", "pdf": "https://arxiv.org/pdf/2504.11106", "abs": "https://arxiv.org/abs/2504.11106", "authors": ["Jiangtao Liu", "Zhaoxin Wang", "Handing Wang", "Cong Tian", "Yaochu Jin"], "title": "Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Recent advancements in Text-to-Image (T2I) generation have significantly\nenhanced the realism and creativity of generated images. However, such powerful\ngenerative capabilities pose risks related to the production of inappropriate\nor harmful content. Existing defense mechanisms, including prompt checkers and\npost-hoc image checkers, are vulnerable to sophisticated adversarial attacks.\nIn this work, we propose TCBS-Attack, a novel query-based black-box jailbreak\nattack that searches for tokens located near the decision boundaries defined by\ntext and image checkers. By iteratively optimizing tokens near these\nboundaries, TCBS-Attack generates semantically coherent adversarial prompts\ncapable of bypassing multiple defensive layers in T2I models. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art jailbreak attacks across various T2I models, including\nsecurely trained open-source models and commercial online services like DALL-E\n3. TCBS-Attack achieves an ASR-4 of 45\\% and an ASR-1 of 21\\% on jailbreaking\nfull-chain T2I models, significantly surpassing baseline methods."}
{"id": "2504.11111", "pdf": "https://arxiv.org/pdf/2504.11111", "abs": "https://arxiv.org/abs/2504.11111", "authors": ["Yu Lin", "Jianghang Lin", "Kai Ye", "You Shen", "Yan Zhang", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "S$^2$Teacher: Step-by-step Teacher for Sparsely Annotated Oriented Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Although fully-supervised oriented object detection has made significant\nprogress in multimodal remote sensing image understanding, it comes at the cost\nof labor-intensive annotation. Recent studies have explored weakly and\nsemi-supervised learning to alleviate this burden. However, these methods\noverlook the difficulties posed by dense annotations in complex remote sensing\nscenes. In this paper, we introduce a novel setting called sparsely annotated\noriented object detection (SAOOD), which only labels partial instances, and\npropose a solution to address its challenges. Specifically, we focus on two key\nissues in the setting: (1) sparse labeling leading to overfitting on limited\nforeground representations, and (2) unlabeled objects (false negatives)\nconfusing feature learning. To this end, we propose the S$^2$Teacher, a novel\nmethod that progressively mines pseudo-labels for unlabeled objects, from easy\nto hard, to enhance foreground representations. Additionally, it reweights the\nloss of unlabeled objects to mitigate their impact during training. Extensive\nexperiments demonstrate that S$^2$Teacher not only significantly improves\ndetector performance across different sparse annotation levels but also\nachieves near-fully-supervised performance on the DOTA dataset with only 10%\nannotation instances, effectively balancing detection accuracy with annotation\nefficiency. The code will be public."}
{"id": "2504.11112", "pdf": "https://arxiv.org/pdf/2504.11112", "abs": "https://arxiv.org/abs/2504.11112", "authors": ["Leonardo M. Joao", "Jancarlo F. Gomes", "Silvio J. F. Guimaraes", "Ewa Kijak", "Alexandre X. Falcao"], "title": "Flyweight FLIM Networks for Salient Object Detection in Biomedical Images", "categories": ["cs.CV"], "comment": null, "summary": "Salient Object Detection (SOD) with deep learning often requires substantial\ncomputational resources and large annotated datasets, making it impractical for\nresource-constrained applications. Lightweight models address computational\ndemands but typically strive in complex and scarce labeled-data scenarios.\nFeature Learning from Image Markers (FLIM) learns an encoder's convolutional\nkernels among image patches extracted from discriminative regions marked on a\nfew representative images, dismissing large annotated datasets, pretraining,\nand backpropagation. Such a methodology exploits information redundancy\ncommonly found in biomedical image applications. This study presents methods to\nlearn dilated-separable convolutional kernels and multi-dilation layers without\nbackpropagation for FLIM networks. It also proposes a novel network\nsimplification method to reduce kernel redundancy and encoder size. By\ncombining a FLIM encoder with an adaptive decoder, a concept recently\nintroduced to estimate a pointwise convolution per image, this study presents\nvery efficient (named flyweight) SOD models for biomedical images. Experimental\nresults in challenging datasets demonstrate superior efficiency and\neffectiveness to lightweight models. By requiring significantly fewer\nparameters and floating-point operations, the results show competitive\neffectiveness to heavyweight models. These advances highlight the potential of\nFLIM networks for data-limited and resource-constrained applications with\ninformation redundancy."}
{"id": "2504.11128", "pdf": "https://arxiv.org/pdf/2504.11128", "abs": "https://arxiv.org/abs/2504.11128", "authors": ["P. Tomkiewicz", "J. Jaworski", "P. Zielonka", "A. Wilinski"], "title": "K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.4.6, I.4.7, I.4.3"], "comment": "16 pages, 6 figures", "summary": "This paper presents a novel computational approach for evaluating urban\nmetrics through density gradient analysis using multi-modal satellite imagery,\nwith applications including public transport and other urban systems. By\ncombining optical and Synthetic Aperture Radar (SAR) data, we develop a method\nto segment urban areas, identify urban centers, and quantify density gradients.\nOur approach calculates two key metrics: the density gradient coefficient\n($\\alpha$) and the minimum effective distance (LD) at which density reaches a\ntarget threshold. We further employ machine learning techniques, specifically\nK-means clustering, to objectively identify uniform and high-variability\nregions within density gradient plots. We demonstrate that these metrics\nprovide an effective screening tool for public transport analyses by revealing\nthe underlying urban structure. Through comparative analysis of two\nrepresentative cities with contrasting urban morphologies (monocentric vs\npolycentric), we establish relationships between density gradient\ncharacteristics and public transport network topologies. Cities with clear\ndensity peaks in their gradient plots indicate distinct urban centers requiring\ndifferent transport strategies than those with more uniform density\ndistributions. This methodology offers urban planners a cost-effective,\nglobally applicable approach to preliminary public transport assessment using\nfreely available satellite data. The complete implementation, with additional\nexamples and documentation, is available in an open-source repository under the\nMIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis."}
{"id": "2504.11134", "pdf": "https://arxiv.org/pdf/2504.11134", "abs": "https://arxiv.org/abs/2504.11134", "authors": ["Gustav Hanning", "Gabrielle Flood", "Viktor Larsson"], "title": "Visual Re-Ranking with Non-Visual Side Information", "categories": ["cs.CV"], "comment": "Accepted at Scandinavian Conference on Image Analysis (SCIA) 2025", "summary": "The standard approach for visual place recognition is to use global image\ndescriptors to retrieve the most similar database images for a given query\nimage. The results can then be further improved with re-ranking methods that\nre-order the top scoring images. However, existing methods focus on re-ranking\nbased on the same image descriptors that were used for the initial retrieval,\nwhich we argue provides limited additional signal.\n  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),\nwhich is a graph neural network-based re-ranking method that, in addition to\nthe visual descriptors, can leverage other types of available side information.\nThis can for example be other sensor data (such as signal strength of nearby\nWiFi or BlueTooth endpoints) or geometric properties such as camera poses for\ndatabase images. In many applications this information is already present or\ncan be acquired with low effort. Our architecture leverages the concept of\naffinity vectors to allow for a shared encoding of the heterogeneous\nmulti-modal input. Two large-scale datasets, covering both outdoor and indoor\nlocalization scenarios, are utilized for training and evaluation. In\nexperiments we show significant improvement not only on image retrieval\nmetrics, but also for the downstream visual localization task."}
{"id": "2504.11143", "pdf": "https://arxiv.org/pdf/2504.11143", "abs": "https://arxiv.org/abs/2504.11143", "authors": ["Xiang Wang", "Shiwei Zhang", "Hangjie Yuan", "Yujie Wei", "Yingya Zhang", "Changxin Gao", "Yuehuan Wang", "Nong Sang"], "title": "Taming Consistency Distillation for Accelerated Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in human image animation have been propelled by video\ndiffusion models, yet their reliance on numerous iterative denoising steps\nresults in high inference costs and slow speeds. An intuitive solution involves\nadopting consistency models, which serve as an effective acceleration paradigm\nthrough consistency distillation. However, simply employing this strategy in\nhuman image animation often leads to quality decline, including visual\nblurring, motion degradation, and facial distortion, particularly in dynamic\nregions. In this paper, we propose the DanceLCM approach complemented by\nseveral enhancements to improve visual quality and motion continuity at\nlow-step regime: (1) segmented consistency distillation with an auxiliary\nlight-weight head to incorporate supervision from real video latents,\nmitigating cumulative errors resulting from single full-trajectory generation;\n(2) a motion-focused loss to centre on motion regions, and explicit injection\nof facial fidelity features to improve face authenticity. Extensive qualitative\nand quantitative experiments demonstrate that DanceLCM achieves results\ncomparable to state-of-the-art video diffusion models with a mere 2-4 inference\nsteps, significantly reducing the inference burden without compromising video\nquality. The code and models will be made publicly available."}
{"id": "2504.11150", "pdf": "https://arxiv.org/pdf/2504.11150", "abs": "https://arxiv.org/abs/2504.11150", "authors": ["Mahir Gulzar", "Yar Muhammad", "Naveed Muhammad"], "title": "GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Predicting future trajectories of surrounding vehicles heavily relies on what\ncontextual information is given to a motion prediction model. The context\nitself can be static (lanes, regulatory elements, etc) or dynamic (traffic\nparticipants). This paper presents a lane graph-based motion prediction model\nthat first predicts graph-based goal proposals and later fuses them with cross\nattention over multiple contextual elements. We follow the famous\nencoder-interactor-decoder architecture where the encoder encodes scene context\nusing lightweight Gated Recurrent Units, the interactor applies cross-context\nattention over encoded scene features and graph goal proposals, and the decoder\nregresses multimodal trajectories via Laplacian Mixture Density Network from\nthe aggregated encodings. Using cross-attention over graph-based goal proposals\ngives robust trajectory estimates since the model learns to attend to future\ngoal-relevant scene elements for the intended agent. We evaluate our work on\nnuScenes motion prediction dataset, achieving state-of-the-art results."}
{"id": "2504.11154", "pdf": "https://arxiv.org/pdf/2504.11154", "abs": "https://arxiv.org/abs/2504.11154", "authors": ["Kaan Aydin", "Joelle Hanna", "Damian Borth"], "title": "SAR-to-RGB Translation with Latent Diffusion for Earth Observation", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 3 figures", "summary": "Earth observation satellites like Sentinel-1 (S1) and Sentinel-2 (S2) provide\ncomplementary remote sensing (RS) data, but S2 images are often unavailable due\nto cloud cover or data gaps. To address this, we propose a diffusion model\n(DM)-based approach for SAR-to-RGB translation, generating synthetic optical\nimages from SAR inputs. We explore three different setups: two using Standard\nDiffusion, which reconstruct S2 images by adding and removing noise (one\nwithout and one with class conditioning), and one using Cold Diffusion, which\nblends S2 with S1 before removing the SAR signal. We evaluate the generated\nimages in downstream tasks, including land cover classification and cloud\nremoval. While generated images may not perfectly replicate real S2 data, they\nstill provide valuable information. Our results show that class conditioning\nimproves classification accuracy, while cloud removal performance remains\ncompetitive despite our approach not being optimized for it. Interestingly,\ndespite exhibiting lower perceptual quality, the Cold Diffusion setup performs\nwell in land cover classification, suggesting that traditional quantitative\nevaluation metrics may not fully reflect the practical utility of generated\nimages. Our findings highlight the potential of DMs for SAR-to-RGB translation\nin RS applications where RGB images are missing."}
{"id": "2504.11160", "pdf": "https://arxiv.org/pdf/2504.11160", "abs": "https://arxiv.org/abs/2504.11160", "authors": ["Haohan Chen", "Hongjia Liu", "Shiyong Lan", "Wenwu Wang", "Yixin Qiao", "Yao Li", "Guonan Deng"], "title": "DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gaze estimation, which predicts gaze direction, commonly faces the challenge\nof interference from complex gaze-irrelevant information in face images. In\nthis work, we propose DMAGaze, a novel gaze estimation framework that exploits\ninformation from facial images in three aspects: gaze-relevant global features\n(disentangled from facial image), local eye features (extracted from cropped\neye patch), and head pose estimation features, to improve overall performance.\nFirstly, we design a new continuous mask-based Disentangler to accurately\ndisentangle gaze-relevant and gaze-irrelevant information in facial images by\nachieving the dual-branch disentanglement goal through separately\nreconstructing the eye and non-eye regions. Furthermore, we introduce a new\ncascaded attention module named Multi-Scale Global Local Attention Module\n(MS-GLAM). Through a customized cascaded attention structure, it effectively\nfocuses on global and local information at multiple scales, further enhancing\nthe information from the Disentangler. Finally, the global gaze-relevant\nfeatures disentangled by the upper face branch, combined with head pose and\nlocal eye features, are passed through the detection head for high-precision\ngaze estimation. Our proposed DMAGaze has been extensively validated on two\nmainstream public datasets, achieving state-of-the-art performance."}
{"id": "2504.11164", "pdf": "https://arxiv.org/pdf/2504.11164", "abs": "https://arxiv.org/abs/2504.11164", "authors": ["Chenming Li", "Chengxu Liu", "Yuanting Fan", "Xiao Jin", "Xingsong Hou", "Xueming Qian"], "title": "TSAL: Few-shot Text Segmentation Based on Attribute Learning", "categories": ["cs.CV"], "comment": null, "summary": "Recently supervised learning rapidly develops in scene text segmentation.\nHowever, the lack of high-quality datasets and the high cost of pixel\nannotation greatly limit the development of them. Considering the\nwell-performed few-shot learning methods for downstream tasks, we investigate\nthe application of the few-shot learning method to scene text segmentation. We\npropose TSAL, which leverages CLIP's prior knowledge to learn text attributes\nfor segmentation. To fully utilize the semantic and texture information in the\nimage, a visual-guided branch is proposed to separately extract text and\nbackground features. To reduce data dependency and improve text detection\naccuracy, the adaptive prompt-guided branch employs effective adaptive prompt\ntemplates to capture various text attributes. To enable adaptive prompts\ncapture distinctive text features and complex background distribution, we\npropose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of\ndifferent attributes with visual features and prompt prototypes, AFA enables\nadaptive prompts to capture both general and distinctive attribute information.\nTSAL can capture the unique attributes of text and achieve precise segmentation\nusing only few images. Experiments demonstrate that our method achieves SOTA\nperformance on multiple text segmentation datasets under few-shot settings and\nshow great potential in text-related domains."}
{"id": "2504.11165", "pdf": "https://arxiv.org/pdf/2504.11165", "abs": "https://arxiv.org/abs/2504.11165", "authors": ["Linlin Xiao", "Zhang Tiancong", "Yutong Jia", "Xinyu Nie", "Mengyao Wang", "Xiaohang Shao"], "title": "YOLO-RS: Remote Sensing Enhanced Crop Detection Methods", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of remote sensing technology, crop classification\nand health detection based on deep learning have gradually become a research\nhotspot. However, the existing target detection methods show poor performance\nwhen dealing with small targets in remote sensing images, especially in the\ncase of complex background and image mixing, which is difficult to meet the\npractical application requirementsite. To address this problem, a novel target\ndetection model YOLO-RS is proposed in this paper. The model is based on the\nlatest Yolov11 which significantly enhances the detection of small targets by\nintroducing the Context Anchor Attention (CAA) mechanism and an efficient\nmulti-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional\nfeature fusion strategy in the feature fusion process, which effectively\nenhances the model's performance in the detection of small targets. Small\ntarget detection. Meanwhile, the ACmix module at the end of the model backbone\nnetwork solves the category imbalance problem by adaptively adjusting the\ncontrast and sample mixing, thus enhancing the detection accuracy in complex\nscenes. In the experiments on the PDT remote sensing crop health detection\ndataset and the CWC crop classification dataset, YOLO-RS improves both the\nrecall and the mean average precision (mAP) by about 2-3\\% or so compared with\nthe existing state-of-the-art methods, while the F1-score is also significantly\nimproved. Moreover, the computational complexity of the model only increases by\nabout 5.2 GFLOPs, indicating its significant advantages in both performance and\nefficiency. The experimental results validate the effectiveness and application\npotential of YOLO-RS in the task of detecting small targets in remote sensing\nimages."}
{"id": "2504.11171", "pdf": "https://arxiv.org/pdf/2504.11171", "abs": "https://arxiv.org/abs/2504.11171", "authors": ["Johannes Jakubik", "Felix Yang", "Benedikt Blumenstiel", "Erik Scheurer", "Rocco Sedona", "Stefano Maurogiovanni", "Jente Bosmans", "Nikolaos Dionelis", "Valerio Marsocci", "Niklas Kopp", "Rahul Ramachandran", "Paolo Fraccaro", "Thomas Brunschwiler", "Gabriele Cavallaro", "Juan Bernabe-Moreno", "Nicolas Longépé"], "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license."}
{"id": "2504.11172", "pdf": "https://arxiv.org/pdf/2504.11172", "abs": "https://arxiv.org/abs/2504.11172", "authors": ["Benedikt Blumenstiel", "Paolo Fraccaro", "Valerio Marsocci", "Johannes Jakubik", "Stefano Maurogiovanni", "Mikolaj Czerkawski", "Rocco Sedona", "Gabriele Cavallaro", "Thomas Brunschwiler", "Juan Bernabe-Moreno", "Nicolas Longépé"], "title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale foundation models in Earth Observation can learn versatile,\nlabel-efficient representations by leveraging massive amounts of unlabeled\ndata. However, existing public datasets are often limited in scale, geographic\ncoverage, or sensor variety. We introduce TerraMesh, a new globally diverse,\nmultimodal dataset combining optical, synthetic aperture radar, elevation, and\nland-cover modalities in an Analysis-Ready Data format. TerraMesh includes over\n9 million samples with eight spatiotemporal aligned modalities, enabling\nlarge-scale pre-training and fostering robust cross-modal correlation learning.\nWe provide detailed data processing steps, comprehensive statistics, and\nempirical evidence demonstrating improved model performance when pre-trained on\nTerraMesh. The dataset will be made publicly available with a permissive\nlicense."}
{"id": "2504.11199", "pdf": "https://arxiv.org/pdf/2504.11199", "abs": "https://arxiv.org/abs/2504.11199", "authors": ["Min Jung Lee", "Dayoung Gong", "Minsu Cho"], "title": "Video Summarization with Large Language Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content."}
{"id": "2504.11202", "pdf": "https://arxiv.org/pdf/2504.11202", "abs": "https://arxiv.org/abs/2504.11202", "authors": ["Junjie Luo", "John Mamish", "Alan Fu", "Thomas Concannon", "Josiah Hester", "Emma Alexander", "Qi Guo"], "title": "Focal Split: Untethered Snapshot Depth from Differential Defocus", "categories": ["cs.CV", "eess.IV", "eess.SP", "68U10", "I.4.8"], "comment": "CVPR 2025, 8 pages, 7 figures", "summary": "We introduce Focal Split, a handheld, snapshot depth camera with fully\nonboard power and computing based on depth-from-differential-defocus (DfDD).\nFocal Split is passive, avoiding power consumption of light sources. Its\nachromatic optical system simultaneously forms two differentially defocused\nimages of the scene, which can be independently captured using two photosensors\nin a snapshot. The data processing is based on the DfDD theory, which\nefficiently computes a depth and a confidence value for each pixel with only\n500 floating point operations (FLOPs) per pixel from the camera measurements.\nWe demonstrate a Focal Split prototype, which comprises a handheld custom\ncamera system connected to a Raspberry Pi 5 for real-time data processing. The\nsystem consumes 4.9 W and is powered on a 5 V, 10,000 mAh battery. The\nprototype can measure objects with distances from 0.4 m to 1.2 m, outputting\n480$\\times$360 sparse depth maps at 2.1 frames per second (FPS) using\nunoptimized Python scripts. Focal Split is DIY friendly. A comprehensive guide\nto building your own Focal Split depth camera, code, and additional data can be\nfound at https://focal-split.qiguo.org."}
{"id": "2504.11218", "pdf": "https://arxiv.org/pdf/2504.11218", "abs": "https://arxiv.org/abs/2504.11218", "authors": ["Zeming wei", "Junyi Lin", "Yang Liu", "Weixing Chen", "Jingzhou Luo", "Guanbin Li", "Liang Lin"], "title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians", "categories": ["cs.CV"], "comment": "The first large-scale 3D Gaussians Affordance Reasoning Benchmark", "summary": "3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities."}
{"id": "2504.11230", "pdf": "https://arxiv.org/pdf/2504.11230", "abs": "https://arxiv.org/abs/2504.11230", "authors": ["Jingshun Huang", "Haitao Lin", "Tianyu Wang", "Yanwei Fu", "Xiangyang Xue", "Yi Zhu"], "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image", "categories": ["cs.CV", "cs.RO"], "comment": "To appear in CVPR 2025 (Highlight)", "summary": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page."}
{"id": "2504.11232", "pdf": "https://arxiv.org/pdf/2504.11232", "abs": "https://arxiv.org/abs/2504.11232", "authors": ["Elisa Ancarani", "Julie Tores", "Lucile Sassatelli", "Rémy Sun", "Hui-Yin Wu", "Frédéric Precioso"], "title": "Leveraging multimodal explanatory annotations for video interpretation with Modality Specific Dataset", "categories": ["cs.CV", "cs.MM"], "comment": "6 pages, 8 Figures", "summary": "We examine the impact of concept-informed supervision on multimodal video\ninterpretation models using MOByGaze, a dataset containing human-annotated\nexplanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs),\nwhich consist of data subsets categorized by the modality (visual, textual, or\naudio) of annotated concepts. Models trained on CMSDs outperform those using\ntraditional legacy training in both early and late fusion approaches. Notably,\nthis approach enables late fusion models to achieve performance close to that\nof early fusion models. These findings underscore the importance of\nmodality-specific annotations in developing robust, self-explainable video\nmodels and contribute to advancing interpretable multimodal learning in complex\nvideo analysis."}
{"id": "2504.11262", "pdf": "https://arxiv.org/pdf/2504.11262", "abs": "https://arxiv.org/abs/2504.11262", "authors": ["Xiaoxiao Ma", "Junxiong Tong"], "title": "Enhanced Small Target Detection via Multi-Modal Fusion and Attention Mechanisms: A YOLOv5 Approach", "categories": ["cs.CV"], "comment": "Accepted by ATC 2024", "summary": "With the rapid development of information technology, modern warfare\nincreasingly relies on intelligence, making small target detection critical in\nmilitary applications. The growing demand for efficient, real-time detection\nhas created challenges in identifying small targets in complex environments due\nto interference. To address this, we propose a small target detection method\nbased on multi-modal image fusion and attention mechanisms. This method\nleverages YOLOv5, integrating infrared and visible light data along with a\nconvolutional attention module to enhance detection performance. The process\nbegins with multi-modal dataset registration using feature point matching,\nensuring accurate network training. By combining infrared and visible light\nfeatures with attention mechanisms, the model improves detection accuracy and\nrobustness. Experimental results on anti-UAV and Visdrone datasets demonstrate\nthe effectiveness and practicality of our approach, achieving superior\ndetection results for small and dim targets."}
{"id": "2504.11268", "pdf": "https://arxiv.org/pdf/2504.11268", "abs": "https://arxiv.org/abs/2504.11268", "authors": ["Juan Garcia Giraldo", "Nikolaos Dimitriadis", "Ke Wang", "Pascal Frossard"], "title": "Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 6 figures", "summary": "Model merging is a flexible and computationally tractable approach to merge\nsingle-task checkpoints into a multi-task model. Prior work has solely focused\non constrained multi-task settings where there is a one-to-one mapping between\na sample and a task, overlooking the paradigm where multiple tasks may operate\non the same sample, e.g., scene understanding. In this paper, we focus on the\nmulti-task setting with single-input-multiple-outputs (SIMO) and show that it\nqualitatively differs from the single-input-single-output model merging\nsettings studied in the literature due to the existence of task-specific\ndecoders and diverse loss objectives. We identify that existing model merging\nmethods lead to significant performance degradation, primarily due to\nrepresentation misalignment between the merged encoder and task-specific\ndecoders. We propose two simple and efficient fixes for the SIMO setting to\nre-align the feature representation after merging. Compared to joint\nfine-tuning, our approach is computationally effective and flexible, and sheds\nlight into identifying task relationships in an offline manner. Experiments on\nNYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task\narithmetic suffices to enable multi-task capabilities; however, the\nrepresentations generated by the merged encoder has to be re-aligned with the\ntask-specific heads; (2) the proposed architecture rivals traditional\nmulti-task learning in performance but requires fewer samples and training\nsteps by leveraging the existence of task-specific models."}
{"id": "2504.11271", "pdf": "https://arxiv.org/pdf/2504.11271", "abs": "https://arxiv.org/abs/2504.11271", "authors": ["Xinning Chai", "Yao Zhang", "Yuxuan Zhang", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Li Song"], "title": "Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) have been widely used in efficient image\nsuper-resolution. However, for CNN-based methods, performance gains often\nrequire deeper networks and larger feature maps, which increase complexity and\ninference costs. Inspired by LoRA's success in fine-tuning large language\nmodels, we explore its application to lightweight models and propose\nDistillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which\nimproves model performance without increasing architectural complexity or\ninference costs. Specifically, we integrate ConvLoRA into the efficient SR\nnetwork SPAN by replacing the SPAB module with the proposed SConvLB module and\nincorporating ConvLoRA layers into both the pixel shuffle block and its\npreceding convolutional layer. DSCLoRA leverages low-rank decomposition for\nparameter updates and employs a spatial feature affinity-based knowledge\ndistillation strategy to transfer second-order statistical information from\nteacher models (pre-trained SPAN) to student models (ours). This method\npreserves the core knowledge of lightweight models and facilitates optimal\nsolution discovery under certain conditions. Experiments on benchmark datasets\nshow that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its\nefficiency and competitive image quality. Notably, DSCLoRA ranked first in the\nOverall Performance Track of the NTIRE 2025 Efficient Super-Resolution\nChallenge. Our code and models are made publicly available at\nhttps://github.com/Yaozzz666/DSCF-SR."}
{"id": "2504.11289", "pdf": "https://arxiv.org/pdf/2504.11289", "abs": "https://arxiv.org/abs/2504.11289", "authors": ["Xiang Wang", "Shiwei Zhang", "Longxiang Tang", "Yingya Zhang", "Changxin Gao", "Yuehuan Wang", "Nong Sang"], "title": "UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer", "categories": ["cs.CV"], "comment": "The training and inference code (based on Wan2.1) is available at\n  https://github.com/ali-vilab/UniAnimate-DiT", "summary": "This report presents UniAnimate-DiT, an advanced project that leverages the\ncutting-edge and powerful capabilities of the open-source Wan2.1 model for\nconsistent human image animation. Specifically, to preserve the robust\ngenerative capabilities of the original Wan2.1 model, we implement Low-Rank\nAdaptation (LoRA) technique to fine-tune a minimal set of parameters,\nsignificantly reducing training memory overhead. A lightweight pose encoder\nconsisting of multiple stacked 3D convolutional layers is designed to encode\nmotion information of driving poses. Furthermore, we adopt a simple\nconcatenation operation to integrate the reference appearance into the model\nand incorporate the pose information of the reference image for enhanced pose\nalignment. Experimental results show that our approach achieves visually\nappearing and temporally consistent high-fidelity animations. Trained on 480p\n(832x480) videos, UniAnimate-DiT demonstrates strong generalization\ncapabilities to seamlessly upscale to 720P (1280x720) during inference. The\ntraining and inference code is publicly available at\nhttps://github.com/ali-vilab/UniAnimate-DiT."}
{"id": "2504.11295", "pdf": "https://arxiv.org/pdf/2504.11295", "abs": "https://arxiv.org/abs/2504.11295", "authors": ["Yeongmin Kim", "Sotiris Anagnostidis", "Yuming Du", "Edgar Schönfeld", "Jonas Kohler", "Markos Georgopoulos", "Albert Pumarola", "Ali Thabet", "Artsiom Sanakoyeu"], "title": "Autoregressive Distillation of Diffusion Transformers", "categories": ["cs.CV"], "comment": "CVPR 2025 Oral", "summary": "Diffusion models with transformer architectures have demonstrated promising\ncapabilities in generating high-fidelity images and scalability for high\nresolution. However, iterative sampling process required for synthesis is very\nresource-intensive. A line of work has focused on distilling solutions to\nprobability flow ODEs into few-step student models. Nevertheless, existing\nmethods have been limited by their reliance on the most recent denoised samples\nas input, rendering them susceptible to exposure bias. To address this\nlimitation, we propose AutoRegressive Distillation (ARD), a novel approach that\nleverages the historical trajectory of the ODE to predict future steps. ARD\noffers two key benefits: 1) it mitigates exposure bias by utilizing a predicted\nhistorical trajectory that is less susceptible to accumulated errors, and 2) it\nleverages the previous history of the ODE trajectory as a more effective source\nof coarse-grained information. ARD modifies the teacher transformer\narchitecture by adding token-wise time embedding to mark each input from the\ntrajectory history and employs a block-wise causal attention mask for training.\nFurthermore, incorporating historical inputs only in lower transformer layers\nenhances performance and efficiency. We validate the effectiveness of ARD in a\nclass-conditioned generation on ImageNet and T2I synthesis. Our model achieves\na $5\\times$ reduction in FID degradation compared to the baseline methods while\nrequiring only 1.1\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of\n1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available\n1024p text-to-image distilled models in prompt adherence score with a minimal\ndrop in FID compared to the teacher. Project page:\nhttps://github.com/alsdudrla10/ARD."}
{"id": "2504.11305", "pdf": "https://arxiv.org/pdf/2504.11305", "abs": "https://arxiv.org/abs/2504.11305", "authors": ["Jincheng Kang", "Yi Cen", "Yigang Cen", "Ke Wang", "Yuhan Liu"], "title": "CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 11 figures", "summary": "Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments."}
{"id": "2504.11306", "pdf": "https://arxiv.org/pdf/2504.11306", "abs": "https://arxiv.org/abs/2504.11306", "authors": ["Trinnhallen Brisley", "Aryan Gandhi", "Joseph Magen"], "title": "Context-Aware Palmprint Recognition via a Relative Similarity Metric", "categories": ["cs.CV"], "comment": null, "summary": "We propose a new approach to matching mechanism for palmprint recognition by\nintroducing a Relative Similarity Metric (RSM) that enhances the robustness and\ndiscriminability of existing matching frameworks. While conventional systems\nrely on direct pairwise similarity measures, such as cosine or Euclidean\ndistances, these metrics fail to capture how a pairwise similarity compares\nwithin the context of the entire dataset. Our method addresses this by\nevaluating the relative consistency of similarity scores across up to all\nidentities, allowing for better suppression of false positives and negatives.\nApplied atop the CCNet architecture, our method achieves a new state-of-the-art\n0.000036% Equal Error Rate (EER) on the Tongji dataset, outperforming previous\nmethods and demonstrating the efficacy of incorporating relational structure\ninto the palmprint matching process."}
{"id": "2504.11307", "pdf": "https://arxiv.org/pdf/2504.11307", "abs": "https://arxiv.org/abs/2504.11307", "authors": ["Sonia Laguna", "Lin Zhang", "Can Deniz Bezek", "Monika Farkas", "Dieter Schweizer", "Rahel A. Kubik-Huch", "Orcun Goksel"], "title": "Uncertainty Estimation for Trust Attribution to Speed-of-Sound Reconstruction with Variational Networks", "categories": ["cs.CV"], "comment": "Published at the International Journal of Computer Assisted Radiology\n  and Surgery. Presented at the 16th International Conference on Information\n  Processing in Computer-Assisted Interventions 2025", "summary": "Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its\nimaging can provide a promising biomarker for diagnosis. Reconstructing SoS\nimages from ultrasound acquisitions can be cast as a limited-angle\ncomputed-tomography problem, with Variational Networks being a promising\nmodel-based deep learning solution. Some acquired data frames may, however, get\ncorrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows,\nwhich in turn negatively affects the resulting SoS reconstructions. We propose\nto use the uncertainty in SoS reconstructions to attribute trust to each\nindividual acquired frame. Given multiple acquisitions, we then use an\nuncertainty based automatic selection among these retrospectively, to improve\ndiagnostic decisions. We investigate uncertainty estimation based on Monte\nCarlo Dropout and Bayesian Variational Inference. We assess our automatic frame\nselection method for differential diagnosis of breast cancer, distinguishing\nbetween benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions\nclassified as BI-RADS~4, which represents suspicious cases for probable\nmalignancy. The most trustworthy frame among four acquisitions of each lesion\nwas identified using uncertainty based criteria. Selecting a frame informed by\nuncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout\nand Bayesian Variational Inference, respectively, superior to any\nuncertainty-uninformed baselines with the best one achieving 64%. A novel use\nof uncertainty estimation is proposed for selecting one of multiple data\nacquisitions for further processing and decision making."}
{"id": "2504.11309", "pdf": "https://arxiv.org/pdf/2504.11309", "abs": "https://arxiv.org/abs/2504.11309", "authors": ["Hongbo Li", "Shangchao Yang", "Ruiyang Xia", "Lin Yuan", "Xinbo Gao"], "title": "Big Brother is Watching: Proactive Deepfake Detection via Learnable Hidden Face", "categories": ["cs.CV"], "comment": null, "summary": "As deepfake technologies continue to advance, passive detection methods\nstruggle to generalize with various forgery manipulations and datasets.\nProactive defense techniques have been actively studied with the primary aim of\npreventing deepfake operation effectively working. In this paper, we aim to\nbridge the gap between passive detection and proactive defense, and seek to\nsolve the detection problem utilizing a proactive methodology. Inspired by\nseveral watermarking-based forensic methods, we explore a novel detection\nframework based on the concept of ``hiding a learnable face within a face''.\nSpecifically, relying on a semi-fragile invertible steganography network, a\nsecret template image is embedded into a host image imperceptibly, acting as an\nindicator monitoring for any malicious image forgery when being restored by the\ninverse steganography process. Instead of being manually specified, the secret\ntemplate is optimized during training to resemble a neutral facial appearance,\njust like a ``big brother'' hidden in the image to be protected. By\nincorporating a self-blending mechanism and robustness learning strategy with a\nsimulative transmission channel, a robust detector is built to accurately\ndistinguish if the steganographic image is maliciously tampered or benignly\nprocessed. Finally, extensive experiments conducted on multiple datasets\ndemonstrate the superiority of the proposed approach over competing passive and\nproactive detection methods."}
{"id": "2504.11310", "pdf": "https://arxiv.org/pdf/2504.11310", "abs": "https://arxiv.org/abs/2504.11310", "authors": ["Dayong Liu", "Qingrui Zhang", "Zeyang Meng"], "title": "Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection", "categories": ["cs.CV"], "comment": "in Chinese language", "summary": "In multi-target tracking and detection tasks, it is necessary to continuously\ntrack multiple targets, such as vehicles, pedestrians, etc. To achieve this\ngoal, the system must be able to continuously acquire and process image frames\ncontaining these targets. These consecutive frame images enable the algorithm\nto update the position and state of the target in real-time in each frame of\nthe image. How to accurately associate the detected target with the target in\nthe previous or next frame to form a stable trajectory is a complex problem.\nTherefore, a multi object tracking and detection method for intelligent driving\nvehicles based on YOLOv5 and point cloud 3D projection is proposed. Using\nRetinex algorithm to enhance the image of the environment in front of the\nvehicle, remove light interference in the image, and build an intelligent\ndetection model based on YOLOv5 network structure. The enhanced image is input\ninto the model, and multiple targets in front of the vehicle are identified\nthrough feature extraction and target localization. By combining point cloud 3D\nprojection technology, the correlation between the position changes of adjacent\nframe images in the projection coordinate system can be inferred. By\nsequentially projecting the multi-target recognition results of multiple\nconsecutive frame images into the 3D laser point cloud environment, effective\ntracking of the motion trajectories of all targets in front of the vehicle can\nbe achieved. The experimental results show that the application of this method\nfor intelligent driving vehicle front multi-target tracking and detection\nyields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its\nsuperior tracking and detection performance."}
{"id": "2504.11326", "pdf": "https://arxiv.org/pdf/2504.11326", "abs": "https://arxiv.org/abs/2504.11326", "authors": ["Henghui Ding", "Chang Liu", "Nikhila Ravi", "Shuting He", "Yunchao Wei", "Song Bai", "Philip Torr", "Kehuan Song", "Xinglin Xie", "Kexin Zhang", "Licheng Jiao", "Lingling Li", "Shuyuan Yang", "Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Mengjiao Wang", "Junpei Zhang", "Xu Liu", "Yuting Yang", "Mengru Ma", "Hao Fang", "Runmin Cong", "Xiankai Lu", "Zhiyang Che", "Wei Zhan", "Tianming Liang", "Haichao Jiang", "Wei-Shi Zheng", "Jian-Fang Hu", "Haobo Yuan", "Xiangtai Li", "Tao Zhang", "Lu Qi", "Ming-Hsuan Yang"], "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild", "categories": ["cs.CV"], "comment": "Workshop Page: https://pvuw.github.io/. arXiv admin note: text\n  overlap with arXiv:2504.00476, arXiv:2504.05178", "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/."}
{"id": "2504.11346", "pdf": "https://arxiv.org/pdf/2504.11346", "abs": "https://arxiv.org/abs/2504.11346", "authors": ["Yu Gao", "Lixue Gong", "Qiushan Guo", "Xiaoxia Hou", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xuanda Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Xin Xia", "Xuefeng Xiao", "Zhonghua Zhai", "Xinyu Zhang", "Qi Zhang", "Yuwei Zhang", "Shijia Zhao", "Jianchao Yang", "Weilin Huang"], "title": "Seedream 3.0 Technical Report", "categories": ["cs.CV"], "comment": "Seedream 3.0 Technical Report", "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality."}
{"id": "2504.11347", "pdf": "https://arxiv.org/pdf/2504.11347", "abs": "https://arxiv.org/abs/2504.11347", "authors": ["Soyoung Yoo", "Namwoo Kang"], "title": "DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation", "categories": ["cs.CV", "physics.app-ph", "68T07"], "comment": "28 pages, 18 figures. Not yet submitted to a journal or conference", "summary": "Data-driven design is emerging as a powerful strategy to accelerate\nengineering innovation. However, its application to vehicle wheel design\nremains limited due to the lack of large-scale, high-quality datasets that\ninclude 3D geometry and physical performance metrics. To address this gap, this\nstudy proposes a synthetic design-performance dataset generation framework\nusing generative AI. The proposed framework first generates 2D rendered images\nusing Stable Diffusion, and then reconstructs the 3D geometry through 2.5D\ndepth estimation. Structural simulations are subsequently performed to extract\nengineering performance data. To further expand the design and performance\nspace, topology optimization is applied, enabling the generation of a more\ndiverse set of wheel designs. The final dataset, named DeepWheel, consists of\nover 6,000 photo-realistic images and 900 structurally analyzed 3D models. This\nmulti-modal dataset serves as a valuable resource for surrogate model training,\ndata-driven inverse design, and design space exploration. The proposed\nmethodology is also applicable to other complex design domains. The dataset is\nreleased under the Creative Commons Attribution-NonCommercial 4.0\nInternational(CC BY-NC 4.0) and is available on the\nhttps://www.smartdesignlab.org/datasets"}
{"id": "2504.11349", "pdf": "https://arxiv.org/pdf/2504.11349", "abs": "https://arxiv.org/abs/2504.11349", "authors": ["Yuezhe Yang", "Boyu Yang", "Yaqian Wang", "Yang He", "Xingbo Dong", "Zhe Jin"], "title": "Explicit and Implicit Representations in AI-based 3D Reconstruction for Radiology: A systematic literature review", "categories": ["cs.CV", "cs.AI", "cs.GR", "68T45", "I.4.5"], "comment": "43 pages, 5 figures, submit to Medical Image Analysis", "summary": "The demand for high-quality medical imaging in clinical practice and assisted\ndiagnosis has made 3D reconstruction in radiological imaging a key research\nfocus. Artificial intelligence (AI) has emerged as a promising approach to\nenhancing reconstruction accuracy while reducing acquisition and processing\ntime, thereby minimizing patient radiation exposure and discomfort and\nultimately benefiting clinical diagnosis. This review explores state-of-the-art\nAI-based 3D reconstruction algorithms in radiological imaging, categorizing\nthem into explicit and implicit approaches based on their underlying\nprinciples. Explicit methods include point-based, volume-based, and Gaussian\nrepresentations, while implicit methods encompass implicit prior embedding and\nneural radiance fields. Additionally, we examine commonly used evaluation\nmetrics and benchmark datasets. Finally, we discuss the current state of\ndevelopment, key challenges, and future research directions in this evolving\nfield. Our project available on: https://github.com/Bean-Young/AI4Med."}
{"id": "2504.11366", "pdf": "https://arxiv.org/pdf/2504.11366", "abs": "https://arxiv.org/abs/2504.11366", "authors": ["Hasan Wehbi", "Hasan Nasrallah", "Mohamad Hasan Zahweh", "Zeinab Takach", "Veera Ganesh Yalla", "Ali J. Ghandour"], "title": "A Decade of Wheat Mapping for Lebanon", "categories": ["cs.CV"], "comment": null, "summary": "Wheat accounts for approximately 20% of the world's caloric intake, making it\na vital component of global food security. Given this importance, mapping wheat\nfields plays a crucial role in enabling various stakeholders, including policy\nmakers, researchers, and agricultural organizations, to make informed decisions\nregarding food security, supply chain management, and resource allocation. In\nthis paper, we tackle the problem of accurately mapping wheat fields out of\nsatellite images by introducing an improved pipeline for winter wheat\nsegmentation, as well as presenting a case study on a decade-long analysis of\nwheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer\n(TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing\npipeline based on the Fields of The World (FTW) framework. Our proposed\npipeline addresses key challenges encountered in existing approaches, such as\nthe clustering of small agricultural parcels in a single large field. By\nmerging wheat segmentation with precise field boundary extraction, our method\nproduces geometrically coherent and semantically rich maps that enable us to\nperform in-depth analysis such as tracking crop rotation pattern over years.\nExtensive evaluations demonstrate improved boundary delineation and field-level\nprecision, establishing the potential of the proposed framework in operational\nagricultural monitoring and historical trend analysis. By allowing for accurate\nmapping of wheat fields, this work lays the foundation for a range of critical\nstudies and future advances, including crop monitoring and yield estimation."}
{"id": "2504.11368", "pdf": "https://arxiv.org/pdf/2504.11368", "abs": "https://arxiv.org/abs/2504.11368", "authors": ["Jingkun Chen", "Haoran Duan", "Xiao Zhang", "Boyan Gao", "Tao Tan", "Vicente Grau", "Jungong Han"], "title": "From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation", "categories": ["cs.CV", "68T45", "I.2.10; I.4.8"], "comment": "10 pages, 5 figures", "summary": "Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git."}
{"id": "2504.11379", "pdf": "https://arxiv.org/pdf/2504.11379", "abs": "https://arxiv.org/abs/2504.11379", "authors": ["Liu Yang", "Huiyu Duan", "Yucheng Zhu", "Xiaohong Liu", "Lu Liu", "Zitong Xu", "Guangji Ma", "Xiongkuo Min", "Guangtao Zhai", "Patrick Le Callet"], "title": "Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model", "categories": ["cs.CV"], "comment": "10 pages", "summary": "$360^{\\circ}$ omnidirectional images (ODIs) have gained considerable\nattention recently, and are widely used in various virtual reality (VR) and\naugmented reality (AR) applications. However, capturing such images is\nexpensive and requires specialized equipment, making ODI synthesis increasingly\nimportant. While common 2D image generation and editing methods are rapidly\nadvancing, these models struggle to deliver satisfactory results when\ngenerating or editing ODIs due to the unique format and broad 360$^{\\circ}$\nField-of-View (FoV) of ODIs. To bridge this gap, we construct\n\\textbf{\\textit{Any2Omni}}, the first comprehensive ODI generation-editing\ndataset comprises 60,000+ training data covering diverse input conditions and\nup to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an\n\\textbf{\\underline{Omni}} model for \\textbf{\\underline{Omni}}-directional image\ngeneration and editing (\\textbf{\\textit{Omni$^2$}}), with the capability of\nhandling various ODI generation and editing tasks under diverse input\nconditions using one model. Extensive experiments demonstrate the superiority\nand effectiveness of the proposed Omni$^2$ model for both the ODI generation\nand editing tasks."}
{"id": "2504.11406", "pdf": "https://arxiv.org/pdf/2504.11406", "abs": "https://arxiv.org/abs/2504.11406", "authors": ["Felipe Crispim Salvagnini", "Jancarlo F. Gomes", "Cid A. N. Santos", "Silvio Jamil F. Guimarães", "Alexandre X. Falcão"], "title": "Multi-level Cellular Automata for FLIM networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The necessity of abundant annotated data and complex network architectures\npresents a significant challenge in deep-learning Salient Object Detection\n(deep SOD) and across the broader deep-learning landscape. This challenge is\nparticularly acute in medical applications in developing countries with limited\ncomputational resources. Combining modern and classical techniques offers a\npath to maintaining competitive performance while enabling practical\napplications. Feature Learning from Image Markers (FLIM) methodology empowers\nexperts to design convolutional encoders through user-drawn markers, with\nfilters learned directly from these annotations. Recent findings demonstrate\nthat coupling a FLIM encoder with an adaptive decoder creates a flyweight\nnetwork suitable for SOD, requiring significantly fewer parameters than\nlightweight models and eliminating the need for backpropagation. Cellular\nAutomata (CA) methods have proven successful in data-scarce scenarios but\nrequire proper initialization -- typically through user input, priors, or\nrandomness. We propose a practical intersection of these approaches: using FLIM\nnetworks to initialize CA states with expert knowledge without requiring user\ninteraction for each image. By decoding features from each level of a FLIM\nnetwork, we can initialize multiple CAs simultaneously, creating a multi-level\nframework. Our method leverages the hierarchical knowledge encoded across\ndifferent network layers, merging multiple saliency maps into a high-quality\nfinal output that functions as a CA ensemble. Benchmarks across two challenging\nmedical datasets demonstrate the competitiveness of our multi-level CA approach\ncompared to established models in the deep SOD literature."}
{"id": "2504.11415", "pdf": "https://arxiv.org/pdf/2504.11415", "abs": "https://arxiv.org/abs/2504.11415", "authors": ["Nikolette Pedersen", "Regitze Sydendal", "Andreas Wulff", "Ralf Raumanns", "Eike Petersen", "Veronika Cheplygina"], "title": "Robustness and sex differences in skin cancer detection: logistic regression vs CNNs", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages (excluding appendix), 2 figures (excluding appendix),\n  submitted to MIUA 2025 conference (response pending)", "summary": "Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github."}
{"id": "2504.11416", "pdf": "https://arxiv.org/pdf/2504.11416", "abs": "https://arxiv.org/abs/2504.11416", "authors": ["Panagiotis Agrafiotis", "Begüm Demir"], "title": "Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted for publication in ISPRS Journal of Photogrammetry and\n  Remote Sensing", "summary": "Accurate, detailed, and high-frequent bathymetry is crucial for shallow\nseabed areas facing intense climatological and anthropogenic pressures. Current\nmethods utilizing airborne or satellite optical imagery to derive bathymetry\nprimarily rely on either SfM-MVS with refraction correction or Spectrally\nDerived Bathymetry (SDB). However, SDB methods often require extensive manual\nfieldwork or costly reference data, while SfM-MVS approaches face challenges\neven after refraction correction. These include depth data gaps and noise in\nenvironments with homogeneous visual textures, which hinder the creation of\naccurate and complete Digital Surface Models (DSMs) of the seabed. To address\nthese challenges, this work introduces a methodology that combines the\nhigh-fidelity 3D reconstruction capabilities of the SfM-MVS methods with\nstate-of-the-art refraction correction techniques, along with the spectral\nanalysis capabilities of a new deep learning-based method for bathymetry\nprediction. This integration enables a synergistic approach where SfM-MVS\nderived DSMs with data gaps are used as training data to generate complete\nbathymetric maps. In this context, we propose Swin-BathyUNet that combines\nU-Net with Swin Transformer self-attention layers and a cross-attention\nmechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve\nbathymetric accuracy by capturing long-range spatial relationships and can also\nfunction as a standalone solution for standard SDB with various training depth\ndata, independent of the SfM-MVS output. Experimental results in two completely\ndifferent test sites in the Mediterranean and Baltic Seas demonstrate the\neffectiveness of the proposed approach through extensive experiments that\ndemonstrate improvements in bathymetric accuracy, detail, coverage, and noise\nreduction in the predicted DSM. The code is available at\nhttps://github.com/pagraf/Swin-BathyUNet."}
{"id": "2504.11418", "pdf": "https://arxiv.org/pdf/2504.11418", "abs": "https://arxiv.org/abs/2504.11418", "authors": ["Tibor Kubík", "Oldřich Kodym", "Petr Šilling", "Kateřina Trávníčková", "Tomáš Mojžiš", "Jan Matula"], "title": "Leveraging Point Transformers for Detecting Anatomical Landmarks in Digital Dentistry", "categories": ["cs.CV"], "comment": "10 pages + references, 3 figures, MICCAI2024 3DTeethland Challenge\n  submission", "summary": "The increasing availability of intraoral scanning devices has heightened\ntheir importance in modern clinical orthodontics. Clinicians utilize advanced\nComputer-Aided Design techniques to create patient-specific treatment plans\nthat include laboriously identifying crucial landmarks such as cusps,\nmesial-distal locations, facial axis points, and tooth-gingiva boundaries.\nDetecting such landmarks automatically presents challenges, including limited\ndataset sizes, significant anatomical variability among subjects, and the\ngeometric nature of the data. We present our experiments from the 3DTeethLand\nGrand Challenge at MICCAI 2024. Our method leverages recent advancements in\npoint cloud learning through transformer architectures. We designed a Point\nTransformer v3 inspired module to capture meaningful geometric and anatomical\nfeatures, which are processed by a lightweight decoder to predict per-point\ndistances, further processed by graph-based non-minima suppression. We report\npromising results and discuss insights on learned feature interpretability."}
{"id": "2504.11423", "pdf": "https://arxiv.org/pdf/2504.11423", "abs": "https://arxiv.org/abs/2504.11423", "authors": ["Dazhong Shen", "Guanglu Song", "Yi Zhang", "Bingqi Ma", "Lujundong Li", "Dongzhi Jiang", "Zhuofan Zong", "Yu Liu"], "title": "ADT: Tuning Diffusion Models with Adversarial Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality."}
{"id": "2504.11427", "pdf": "https://arxiv.org/pdf/2504.11427", "abs": "https://arxiv.org/abs/2504.11427", "authors": ["Yanrui Bin", "Wenbo Hu", "Haoyuan Wang", "Xinya Chen", "Bing Wang"], "title": "NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, Project Page: https://normalcrafter.github.io/", "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos."}
{"id": "2504.11434", "pdf": "https://arxiv.org/pdf/2504.11434", "abs": "https://arxiv.org/abs/2504.11434", "authors": ["Yifan Ding", "Xixi Liu", "Jonas Unger", "Gabriel Eilertsen"], "title": "Enhancing Out-of-Distribution Detection with Extended Logit Normalization", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for the safe deployment of\nmachine learning models. Recent advances have explored improved classification\nlosses and representation learning strategies to enhance OOD detection.\nHowever, these methods are often tailored to specific post-hoc detection\ntechniques, limiting their generalizability. In this work, we identify a\ncritical issue in Logit Normalization (LogitNorm), which inhibits its\neffectiveness in improving certain post-hoc OOD detection methods. To address\nthis, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel\nhyperparameter-free formulation that significantly benefits a wide range of\npost-hoc detection methods. By incorporating feature distance-awareness to\nLogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and\nin-distribution (ID) confidence calibration than its predecessor. Extensive\nexperiments across standard benchmarks demonstrate that our approach\noutperforms state-of-the-art training-time methods in OOD detection while\nmaintaining strong ID classification accuracy."}
{"id": "2504.11441", "pdf": "https://arxiv.org/pdf/2504.11441", "abs": "https://arxiv.org/abs/2504.11441", "authors": ["Elizabeth Fons", "Rachneet Kaur", "Zhen Zeng", "Soham Palande", "Tucker Balch", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "TADACap: Time-series Adaptive Domain-Aware Captioning", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICAIF 2024", "summary": "While image captioning has gained significant attention, the potential of\ncaptioning time-series images, prevalent in areas like finance and healthcare,\nremains largely untapped. Existing time-series captioning methods typically\noffer generic, domain-agnostic descriptions of time-series shapes and struggle\nto adapt to new domains without substantial retraining. To address these\nlimitations, we introduce TADACap, a retrieval-based framework to generate\ndomain-aware captions for time-series images, capable of adapting to new\ndomains without retraining. Building on TADACap, we propose a novel retrieval\nstrategy that retrieves diverse image-caption pairs from a target domain\ndatabase, namely TADACap-diverse. We benchmarked TADACap-diverse against\nstate-of-the-art methods and ablation variants. TADACap-diverse demonstrates\ncomparable semantic accuracy while requiring significantly less annotation\neffort."}
{"id": "2504.11447", "pdf": "https://arxiv.org/pdf/2504.11447", "abs": "https://arxiv.org/abs/2504.11447", "authors": ["An Zhaol", "Shengyuan Zhang", "Ling Yang", "Zejian Li", "Jiale Wu", "Haoran Xu", "AnYang Wei", "Perry Pengyun GU Lingyun Sun"], "title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion", "categories": ["cs.CV"], "comment": "Our code is public available on\n  https://github.com/happyw1nd/DistillationDPO", "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO."}
{"id": "2504.11451", "pdf": "https://arxiv.org/pdf/2504.11451", "abs": "https://arxiv.org/abs/2504.11451", "authors": ["Minghua Liu", "Mikaela Angelina Uy", "Donglai Xiang", "Hao Su", "Sanja Fidler", "Nicholas Sharp", "Jun Gao"], "title": "PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond", "categories": ["cs.CV"], "comment": "https://research.nvidia.com/labs/toronto-ai/partfield-release/", "summary": "We propose PartField, a feedforward approach for learning part-based 3D\nfeatures, which captures the general concept of parts and their hierarchy\nwithout relying on predefined templates or text-based names, and can be applied\nto open-world 3D shapes across various modalities. PartField requires only a 3D\nfeedforward pass at inference time, significantly improving runtime and\nrobustness compared to prior approaches. Our model is trained by distilling 2D\nand 3D part proposals from a mix of labeled datasets and image segmentations on\nlarge unsupervised datasets, via a contrastive learning formulation. It\nproduces a continuous feature field which can be clustered to yield a\nhierarchical part decomposition. Comparisons show that PartField is up to 20%\nmore accurate and often orders of magnitude faster than other recent\nclass-agnostic part-segmentation methods. Beyond single-shape part\ndecomposition, consistency in the learned field emerges across shapes, enabling\ntasks such as co-segmentation and correspondence, which we demonstrate in\nseveral applications of these general-purpose, hierarchical, and consistent 3D\nfeature fields. Check our Webpage!\nhttps://research.nvidia.com/labs/toronto-ai/partfield-release/"}
{"id": "2504.11455", "pdf": "https://arxiv.org/pdf/2504.11455", "abs": "https://arxiv.org/abs/2504.11455", "authors": ["Junke Wang", "Zhi Tian", "Xun Wang", "Xinyu Zhang", "Weilin Huang", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL", "categories": ["cs.CV"], "comment": "technical report, work in progress", "summary": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR."}
{"id": "2504.11457", "pdf": "https://arxiv.org/pdf/2504.11457", "abs": "https://arxiv.org/abs/2504.11457", "authors": ["Ziqi Pang", "Xin Xu", "Yu-Xiong Wang"], "title": "Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "With the success of image generation, generative diffusion models are\nincreasingly adopted for discriminative tasks, as pixel generation provides a\nunified perception interface. However, directly repurposing the generative\ndenoising process for discriminative objectives reveals critical gaps rarely\naddressed previously. Generative models tolerate intermediate sampling errors\nif the final distribution remains plausible, but discriminative tasks require\nrigorous accuracy throughout, as evidenced in challenging multi-modal tasks\nlike referring image segmentation. Motivated by this gap, we analyze and\nenhance alignment between generative diffusion processes and perception tasks,\nfocusing on how perception quality evolves during denoising. We find: (1)\nearlier denoising steps contribute disproportionately to perception quality,\nprompting us to propose tailored learning objectives reflecting varying\ntimestep contributions; (2) later denoising steps show unexpected perception\ndegradation, highlighting sensitivity to training-denoising distribution\nshifts, addressed by our diffusion-tailored data augmentation; and (3)\ngenerative processes uniquely enable interactivity, serving as controllable\nuser interfaces adaptable to correctional prompts in multi-round interactions.\nOur insights significantly improve diffusion-based perception models without\narchitectural changes, achieving state-of-the-art performance on depth\nestimation, referring image segmentation, and generalist perception tasks. Code\navailable at https://github.com/ziqipang/ADDP."}
{"id": "2410.10291", "pdf": "https://arxiv.org/pdf/2410.10291", "abs": "https://arxiv.org/abs/2410.10291", "authors": ["Xiangru Zhu", "Penglei Sun", "Yaoxian Song", "Yanghua Xiao", "Zhixu Li", "Chengyu Wang", "Jun Huang", "Bei Yang", "Xiaoxiao Xu"], "title": "Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted by ICLR 2025", "summary": "Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench ."}
{"id": "2504.10493", "pdf": "https://arxiv.org/pdf/2504.10493", "abs": "https://arxiv.org/abs/2504.10493", "authors": ["K. A. Muthukumar", "Dhruva Nandi", "Priya Ranjan", "Krithika Ramachandran", "Shiny PJ", "Anirban Ghosh", "Ashwini M", "Aiswaryah Radhakrishnan", "V. E. Dhandapani", "Rajiv Janardhanan"], "title": "Integrating electrocardiogram and fundus images for early detection of cardiovascular diseases", "categories": ["eess.IV", "cs.CV"], "comment": "EMD, Fundus image, CNN, CVD prediction", "summary": "Cardiovascular diseases (CVD) are a predominant health concern globally,\nemphasizing the need for advanced diagnostic techniques. In our research, we\npresent an avant-garde methodology that synergistically integrates ECG readings\nand retinal fundus images to facilitate the early disease tagging as well as\ntriaging of the CVDs in the order of disease priority. Recognizing the\nintricate vascular network of the retina as a reflection of the cardiovascular\nsystem, alongwith the dynamic cardiac insights from ECG, we sought to provide a\nholistic diagnostic perspective. Initially, a Fast Fourier Transform (FFT) was\napplied to both the ECG and fundus images, transforming the data into the\nfrequency domain. Subsequently, the Earth Mover's Distance (EMD) was computed\nfor the frequency-domain features of both modalities. These EMD values were\nthen concatenated, forming a comprehensive feature set that was fed into a\nNeural Network classifier. This approach, leveraging the FFT's spectral\ninsights and EMD's capability to capture nuanced data differences, offers a\nrobust representation for CVD classification. Preliminary tests yielded a\ncommendable accuracy of 84 percent, underscoring the potential of this combined\ndiagnostic strategy. As we continue our research, we anticipate refining and\nvalidating the model further to enhance its clinical applicability in resource\nlimited healthcare ecosystems prevalent across the Indian sub-continent and\nalso the world at large."}
{"id": "2504.10502", "pdf": "https://arxiv.org/pdf/2504.10502", "abs": "https://arxiv.org/abs/2504.10502", "authors": ["Abraham Itzhak Weinberg"], "title": "Human-Oriented Image Retrieval System (HORSE): A Neuro-Symbolic Approach to Optimizing Retrieval of Previewed Images", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "Image retrieval remains a challenging task due to the complex interaction\nbetween human visual perception, memory, and computational processes. Current\nimage search engines often struggle to efficiently retrieve images based on\nnatural language descriptions, as they rely on time-consuming preprocessing,\ntagging, and machine learning pipelines. This paper introduces the\nHuman-Oriented Retrieval Search Engine for Images (HORSE), a novel approach\nthat leverages neuro-symbolic indexing to improve image retrieval by focusing\non human-oriented indexing. By integrating cognitive science insights with\nadvanced computational techniques, HORSE enhances the retrieval process, making\nit more aligned with how humans perceive, store, and recall visual information.\nThe neuro-symbolic framework combines the strengths of neural networks and\nsymbolic reasoning, mitigating their individual limitations. The proposed\nsystem optimizes image retrieval, offering a more intuitive and efficient\nsolution for users. We discuss the design and implementation of HORSE,\nhighlight its potential applications in fields such as design error detection\nand knowledge management, and suggest future directions for research to further\nrefine the system's metrics and capabilities."}
{"id": "2504.10526", "pdf": "https://arxiv.org/pdf/2504.10526", "abs": "https://arxiv.org/abs/2504.10526", "authors": ["Mingyang Zhu", "Yinting Liu", "Mingyu Li", "Jiacheng Wang"], "title": "PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with SAM2", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Current methods for pathology image segmentation typically treat 2D slices\nindependently, ignoring valuable cross-slice information. We present\nPathSeqSAM, a novel approach that treats 2D pathology slices as sequential\nvideo frames using SAM2's memory mechanisms. Our method introduces a\ndistance-aware attention mechanism that accounts for variable physical\ndistances between slices and employs LoRA for domain adaptation. Evaluated on\nthe KPI Challenge 2024 dataset for glomeruli segmentation, PathSeqSAM\ndemonstrates improved segmentation quality, particularly in challenging cases\nthat benefit from cross-slice context. We have publicly released our code at\nhttps://github.com/JackyyyWang/PathSeqSAM."}
{"id": "2504.10552", "pdf": "https://arxiv.org/pdf/2504.10552", "abs": "https://arxiv.org/abs/2504.10552", "authors": ["Arash Torabi Goodarzi", "Roman Kochnev", "Waleed Khalid", "Furui Qin", "Tolgay Atinc Uzun", "Yashkumar Sanjaybhai Dhameliya", "Yash Kanubhai Kathiriya", "Zofia Antonina Bentyn", "Dmitry Ignatov", "Radu Timofte"], "title": "LEMUR Neural Network Dataset: Towards Seamless AutoML", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DL"], "comment": null, "summary": "Neural networks are fundamental in artificial intelligence, driving progress\nin computer vision and natural language processing. High-quality datasets are\ncrucial for their development, and there is growing interest in datasets\ncomposed of neural networks themselves to support benchmarking, automated\nmachine learning (AutoML), and model analysis. We introduce LEMUR, an open\nsource dataset of neural network models with well-structured code for diverse\narchitectures across tasks such as object detection, image classification,\nsegmentation, and natural language processing. LEMUR is primarily designed to\nenable fine-tuning of large language models (LLMs) for AutoML tasks, providing\na rich source of structured model representations and associated performance\ndata. Leveraging Python and PyTorch, LEMUR enables seamless extension to new\ndatasets and models while maintaining consistency. It integrates an\nOptuna-powered framework for evaluation, hyperparameter optimization,\nstatistical analysis, and graphical insights. LEMUR provides an extension that\nenables models to run efficiently on edge devices, facilitating deployment in\nresource-constrained environments. Providing tools for model evaluation,\npreprocessing, and database management, LEMUR supports researchers and\npractitioners in developing, testing, and analyzing neural networks.\nAdditionally, it offers an API that delivers comprehensive information about\nneural network models and their complete performance statistics with a single\nrequest, which can be used in experiments with code-generating large language\nmodels. The LEMUR will be released as an open source project under the MIT\nlicense upon acceptance of the paper."}
{"id": "2504.10584", "pdf": "https://arxiv.org/pdf/2504.10584", "abs": "https://arxiv.org/abs/2504.10584", "authors": ["Roni H. Goldshmid", "John O. Dabiri", "John E. Sader"], "title": "Visual anemometry of natural vegetation from their leaf motion", "categories": ["physics.flu-dyn", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "High-resolution, near-ground wind-speed data are critical for improving the\naccuracy of weather predictions and climate models,$^{1-3}$ supporting wildfire\ncontrol efforts,$^{4-7}$ and ensuring the safe passage of airplanes during\ntakeoff and landing maneouvers.$^{8,9}$ Quantitative wind speed anemometry\ngenerally employs on-site instrumentation for accurate single-position data or\nsophisticated remote techniques such as Doppler radar for quantitative field\nmeasurements. It is widely recognized that the wind-induced motion of\nvegetation depends in a complex manner on their structure and mechanical\nproperties, obviating their use in quantitative anemometry.$^{10-14}$ We\nanalyze measurements on a host of different vegetation showing that leaf motion\ncan be decoupled from the leaf's branch and support structure, at\nlow-to-moderate wind speed, $U_{wind}$. This wind speed range is characterized\nby a leaf Reynolds number, enabling the development of a remote, quantitative\nanemometry method based on the formula,\n$U_{wind}\\approx740\\sqrt{{\\mu}U_{leaf}/{\\rho}D}$, that relies only on the leaf\nsize $D$, its measured fluctuating (RMS) speed $U_{leaf}$, the air viscosity\n$\\mu$, and its mass density $\\rho$. This formula is corroborated by a\nfirst-principles model and validated using a host of laboratory and field tests\non diverse vegetation types, ranging from oak, olive, and magnolia trees\nthrough to camphor and bullgrass. The findings of this study open the door to a\nnew paradigm in anemometry, using natural vegetation to enable remote and rapid\nquantitative field measurements at global locations with minimal cost."}
{"id": "2504.10745", "pdf": "https://arxiv.org/pdf/2504.10745", "abs": "https://arxiv.org/abs/2504.10745", "authors": ["Indu Panigrahi", "Sunnie S. Y. Kim", "Amna Liaqat", "Rohan Jinturkar", "Olga Russakovsky", "Ruth Fong", "Parastoo Abtahi"], "title": "Interactivity x Explainability: Toward Understanding How Interactivity Can Improve Computer Vision Explanations", "categories": ["cs.HC", "cs.CV"], "comment": "To appear in Extended Abstracts of the CHI Conference on Human\n  Factors in Computing Systems (CHI EA '25)", "summary": "Explanations for computer vision models are important tools for interpreting\nhow the underlying models work. However, they are often presented in static\nformats, which pose challenges for users, including information overload, a gap\nbetween semantic and pixel-level information, and limited opportunities for\nexploration. We investigate interactivity as a mechanism for tackling these\nissues in three common explanation types: heatmap-based, concept-based, and\nprototype-based explanations. We conducted a study (N=24), using a bird\nidentification task, involving participants with diverse technical and domain\nexpertise. We found that while interactivity enhances user control, facilitates\nrapid convergence to relevant information, and allows users to expand their\nunderstanding of the model and explanation, it also introduces new challenges.\nTo address these, we provide design recommendations for interactive computer\nvision explanations, including carefully selected default views, independent\ninput controls, and constrained output spaces."}
{"id": "2504.10807", "pdf": "https://arxiv.org/pdf/2504.10807", "abs": "https://arxiv.org/abs/2504.10807", "authors": ["Huseyin Tuna Erdinc", "Yunlin Zeng", "Abhinav Prakash Gahlot", "Felix J. Herrmann"], "title": "Power-scaled Bayesian Inference with Score-based Generative mModels", "categories": ["cs.LG", "cs.CV", "physics.geo-ph"], "comment": "8 pages, 4 figures", "summary": "We propose a score-based generative algorithm for sampling from power-scaled\npriors and likelihoods within the Bayesian inference framework. Our algorithm\nenables flexible control over prior-likelihood influence without requiring\nretraining for different power-scaling configurations. Specifically, we focus\non synthesizing seismic velocity models conditioned on imaged seismic. Our\nmethod enables sensitivity analysis by sampling from intermediate power\nposteriors, allowing us to assess the relative influence of the prior and\nlikelihood on samples of the posterior distribution. Through a comprehensive\nset of experiments, we evaluate the effects of varying the power parameter in\ndifferent settings: applying it solely to the prior, to the likelihood of a\nBayesian formulation, and to both simultaneously. The results show that\nincreasing the power of the likelihood up to a certain threshold improves the\nfidelity of posterior samples to the conditioning data (e.g., seismic images),\nwhile decreasing the prior power promotes greater structural diversity among\nsamples. Moreover, we find that moderate scaling of the likelihood leads to a\nreduced shot data residual, confirming its utility in posterior refinement."}
{"id": "2504.10820", "pdf": "https://arxiv.org/pdf/2504.10820", "abs": "https://arxiv.org/abs/2504.10820", "authors": ["Kelum Gajamannage", "Dilhani I. Jayathilake", "Maria Vasilyeva"], "title": "Efficient and Robust Remote Sensing Image Denoising Using Randomized Approximation of Geodesics' Gramian on the Manifold Underlying the Patch Space", "categories": ["eess.IV", "cs.CV", "68U10, 94A08, 68T10", "I.4.3; I.4.5"], "comment": "21 pages, 5 figures, and submitted to the International Journal of\n  Remote Sensing", "summary": "Remote sensing images are widely utilized in many disciplines such as feature\nrecognition and scene semantic segmentation. However, due to environmental\nfactors and the issues of the imaging system, the image quality is often\ndegraded which may impair subsequent visual tasks. Even though denoising remote\nsensing images plays an essential role before applications, the current\ndenoising algorithms fail to attain optimum performance since these images\npossess complex features in the texture. Denoising frameworks based on\nartificial neural networks have shown better performance; however, they require\nexhaustive training with heterogeneous samples that extensively consume\nresources like power, memory, computation, and latency. Thus, here we present a\ncomputationally efficient and robust remote sensing image denoising method that\ndoesn't require additional training samples. This method partitions patches of\na remote-sensing image in which a low-rank manifold, representing the\nnoise-free version of the image, underlies the patch space. An efficient and\nrobust approach to revealing this manifold is a randomized approximation of the\nsingular value spectrum of the geodesics' Gramian matrix of the patch space.\nThe method asserts a unique emphasis on each color channel during denoising so\nthe three denoised channels are merged to produce the final image."}
{"id": "2504.10833", "pdf": "https://arxiv.org/pdf/2504.10833", "abs": "https://arxiv.org/abs/2504.10833", "authors": ["Shubham Kumar", "Dwip Dalal", "Narendra Ahuja"], "title": "Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a\npromising tool for generating semantic explanations of the decision-making\nprocesses in deep neural networks, having applications in both model\nimprovement and understanding. It is vital that the explanation is accurate, or\nfaithful, to the model, yet we identify several limitations of prior\nfaithfulness metrics that inhibit an accurate evaluation; most notably, prior\nmetrics involve only the set of concepts present, ignoring how they may be\nspatially distributed. We address these limitations with Surrogate Faithfulness\n(SF), an evaluation method that introduces a spatially-aware surrogate and two\nnovel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)\nexplanations, where concepts are found that maximize faithfulness. Our\nexperiments show that (1) adding spatial-awareness to prior U-CBEMs increases\nfaithfulness in all cases; (2) OF produces significantly more faithful\nexplanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's\nlearned concepts generalize well to out-of-domain data and are more robust to\nadversarial examples, where prior U-CBEMs struggle."}
{"id": "2504.10857", "pdf": "https://arxiv.org/pdf/2504.10857", "abs": "https://arxiv.org/abs/2504.10857", "authors": ["Shun Iwase", "Zubair Irshad", "Katherine Liu", "Vitor Guizilini", "Robert Lee", "Takuya Ikeda", "Ayako Amma", "Koichi Nishiwaki", "Kris Kitani", "Rares Ambrus", "Sergey Zakharov"], "title": "ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping", "categories": ["cs.RO", "cs.CV"], "comment": "Published at CVPR 2025, Webpage: https://sh8.io/#/zerograsp", "summary": "Robotic grasping is a cornerstone capability of embodied systems. Many\nmethods directly output grasps from partial information without modeling the\ngeometry of the scene, leading to suboptimal motion and even collisions. To\naddress these issues, we introduce ZeroGrasp, a novel framework that\nsimultaneously performs 3D reconstruction and grasp pose prediction in near\nreal-time. A key insight of our method is that occlusion reasoning and modeling\nthe spatial relationships between objects is beneficial for both accurate\nreconstruction and grasping. We couple our method with a novel large-scale\nsynthetic dataset, which comprises 1M photo-realistic images, high-resolution\n3D reconstructions and 11.3B physically-valid grasp pose annotations for 12K\nobjects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the\nGraspNet-1B benchmark as well as through real-world robot experiments.\nZeroGrasp achieves state-of-the-art performance and generalizes to novel\nreal-world objects by leveraging synthetic data."}
{"id": "2504.10916", "pdf": "https://arxiv.org/pdf/2504.10916", "abs": "https://arxiv.org/abs/2504.10916", "authors": ["Zhenyu Yang", "Haiming Zhu", "Rihui Zhang", "Haipeng Zhang", "Jianliang Wang", "Chunhao Wang", "Minbin Chen", "Fang-Fang Yin"], "title": "Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification", "categories": ["physics.med-ph", "cs.CV"], "comment": "27 pages, 3 figures", "summary": "Background: Deep learning has significantly advanced medical image analysis,\nwith Vision Transformers (ViTs) offering a powerful alternative to\nconvolutional models by modeling long-range dependencies through\nself-attention. However, ViTs are inherently data-intensive and lack\ndomain-specific inductive biases, limiting their applicability in medical\nimaging. In contrast, radiomics provides interpretable, handcrafted descriptors\nof tissue heterogeneity but suffers from limited scalability and integration\ninto end-to-end learning frameworks. In this work, we propose the\nRadiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features\nwith data-driven visual embeddings within a ViT backbone.\n  Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and\npatch-wise ViT embeddings through early fusion, enhancing robustness and\nperformance in medical image classification.\n  Methods: Following the standard ViT pipeline, images were divided into\npatches. For each patch, handcrafted radiomic features were extracted and fused\nwith linearly projected pixel embeddings. The fused representations were\nnormalized, positionally encoded, and passed to the ViT encoder. A learnable\n[CLS] token aggregated patch-level information for classification. We evaluated\nRE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal\nOCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was\nbenchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.\n  Results: RE-ViT achieved state-of-the-art results: on BUSI,\nAUC=0.950+/-0.011; on ChestXray2017, AUC=0.989+/-0.004; on Retinal OCT,\nAUC=0.986+/-0.001, which outperforms other comparison models.\n  Conclusions: The RE-ViT framework effectively integrates radiomics with ViT\narchitectures, demonstrating improved performance and generalizability across\nmultimodal medical image classification tasks."}
{"id": "2504.10978", "pdf": "https://arxiv.org/pdf/2504.10978", "abs": "https://arxiv.org/abs/2504.10978", "authors": ["Pu Wang", "Zhihua Zhang", "Dianjie Lu", "Guijuan Zhang", "Youshan Zhang", "Zhuoran Zheng"], "title": "AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Since human and environmental factors interfere, captured polyp images\nusually suffer from issues such as dim lighting, blur, and overexposure, which\npose challenges for downstream polyp segmentation tasks. To address the\nchallenges of noise-induced degradation in polyp images, we present AgentPolyp,\na novel framework integrating CLIP-based semantic guidance and dynamic image\nenhancement with a lightweight neural network for segmentation. The agent first\nevaluates image quality using CLIP-driven semantic analysis (e.g., identifying\n``low-contrast polyps with vascular textures\") and adapts reinforcement\nlearning strategies to dynamically apply multi-modal enhancement operations\n(e.g., denoising, contrast adjustment). A quality assessment feedback loop\noptimizes pixel-level enhancement and segmentation focus in a collaborative\nmanner, ensuring robust preprocessing before neural network segmentation. This\nmodular architecture supports plug-and-play extensions for various enhancement\nalgorithms and segmentation networks, meeting deployment requirements for\nendoscopic devices."}
{"id": "2504.11022", "pdf": "https://arxiv.org/pdf/2504.11022", "abs": "https://arxiv.org/abs/2504.11022", "authors": ["Joana Reuss", "Jan Macdonald", "Simon Becker", "Konrad Schultka", "Lorenz Richter", "Marco Körner"], "title": "Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset", "categories": ["cs.LG", "cs.CV"], "comment": "19 pages, 7 figures, 12 tables", "summary": "Spatial imbalances in crop type data pose significant challenges for accurate\nclassification in remote sensing applications. Algorithms aiming at\ntransferring knowledge from data-rich to data-scarce tasks have thus surged in\npopularity. However, despite their effectiveness in previous evaluations, their\nperformance in challenging real-world applications is unclear and needs to be\nevaluated. This study benchmarks transfer learning and several meta-learning\nalgorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML),\nAlmost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the\nreal-world EuroCropsML time series dataset, which combines farmer-reported crop\ndata with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal.\nOur findings indicate that MAML-based meta-learning algorithms achieve slightly\nhigher accuracy compared to simpler transfer learning methods when applied to\ncrop type classification tasks in Estonia after pre-training on data from\nLatvia. However, this improvement comes at the cost of increased computational\ndemands and training time. Moreover, we find that the transfer of knowledge\nbetween geographically disparate regions, such as Estonia and Portugal, poses\nsignificant challenges to all investigated algorithms. These insights\nunderscore the trade-offs between accuracy and computational resource\nrequirements in selecting machine learning methods for real-world crop type\nclassification tasks and highlight the difficulties of transferring knowledge\nbetween different regions of the Earth. To facilitate future research in this\ndomain, we present the first comprehensive benchmark for evaluating transfer\nand meta-learning methods for crop type classification under real-world\nconditions. The corresponding code is publicly available at\nhttps://github.com/dida-do/eurocrops-meta-learning."}
{"id": "2504.11031", "pdf": "https://arxiv.org/pdf/2504.11031", "abs": "https://arxiv.org/abs/2504.11031", "authors": ["Timm Linder", "Kadir Yilmaz", "David B. Adrian", "Bastian Leibe"], "title": "Acquisition of high-quality images for camera calibration in robotics applications via speech prompts", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 6 figures", "summary": "Accurate intrinsic and extrinsic camera calibration can be an important\nprerequisite for robotic applications that rely on vision as input. While there\nis ongoing research on enabling camera calibration using natural images, many\nsystems in practice still rely on using designated calibration targets with\ne.g. checkerboard patterns or April tag grids. Once calibration images from\ndifferent perspectives have been acquired and feature descriptors detected,\nthose are typically used in an optimization process to minimize the geometric\nreprojection error. For this optimization to converge, input images need to be\nof sufficient quality and particularly sharpness; they should neither contain\nmotion blur nor rolling-shutter artifacts that can arise when the calibration\nboard was not static during image capture. In this work, we present a novel\ncalibration image acquisition technique controlled via voice commands recorded\nwith a clip-on microphone, that can be more robust and user-friendly than e.g.\ntriggering capture with a remote control, or filtering out blurry frames from a\nvideo sequence in postprocessing. To achieve this, we use a state-of-the-art\nspeech-to-text transcription model with accurate per-word timestamping to\ncapture trigger words with precise temporal alignment. Our experiments show\nthat the proposed method improves user experience by being fast and efficient,\nallowing us to successfully calibrate complex multi-camera setups."}
{"id": "2504.11089", "pdf": "https://arxiv.org/pdf/2504.11089", "abs": "https://arxiv.org/abs/2504.11089", "authors": ["Fuyin Lai", "Edith Heiter", "Guillaume Bied", "Jefrey Lijffijt"], "title": "InfoClus: Informative Clustering of High-dimensional Data Embeddings", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 9 figures", "summary": "Developing an understanding of high-dimensional data can be facilitated by\nvisualizing that data using dimensionality reduction. However, the\nlow-dimensional embeddings are often difficult to interpret. To facilitate the\nexploration and interpretation of low-dimensional embeddings, we introduce a\nnew concept named partitioning with explanations. The idea is to partition the\ndata shown through the embedding into groups, each of which is given a sparse\nexplanation using the original high-dimensional attributes. We introduce an\nobjective function that quantifies how much we can learn through observing the\nexplanations of the data partitioning, using information theory, and also how\ncomplex the explanations are. Through parameterization of the complexity, we\ncan tune the solutions towards the desired granularity. We propose InfoClus,\nwhich optimizes the partitioning and explanations jointly, through greedy\nsearch constrained over a hierarchical clustering. We conduct a qualitative and\nquantitative analysis of InfoClus on three data sets. We contrast the results\non the Cytometry data with published manual analysis results, and compare with\ntwo other recent methods for explaining embeddings (RVX and VERA). These\ncomparisons highlight that InfoClus has distinct advantages over existing\nprocedures and methods. We find that InfoClus can automatically create good\nstarting points for the analysis of dimensionality-reduction-based scatter\nplots."}
{"id": "2504.11104", "pdf": "https://arxiv.org/pdf/2504.11104", "abs": "https://arxiv.org/abs/2504.11104", "authors": ["René Peinl"], "title": "Using LLMs as prompt modifier to avoid biases in AI image generators", "categories": ["cs.CL", "cs.CV", "cs.CY"], "comment": null, "summary": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/"}
{"id": "2504.11118", "pdf": "https://arxiv.org/pdf/2504.11118", "abs": "https://arxiv.org/abs/2504.11118", "authors": ["Henrik Krauss", "Takehisa Yairi"], "title": "Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This study introduces a novel method for revealing human covert attention\npatterns using gameplay data alone, utilizing offline attention techniques from\nreinforcement learning (RL). We propose the contextualized, task-relevant (CTR)\nattention network, which generates attention maps from both human and RL agent\ngameplay in Atari environments. These maps are sparse yet retain the necessary\ninformation for the current player's decision making. We compare the\nCTR-derived attention maps with a temporally integrated overt attention (TIOA)\nmodel based on eye-tracking data, serving as a point of comparison and\ndiscussion. Visual inspection reveals distinct attention patterns: human CTR\nmaps focus on the player and rather nearby opponents, occasionally shifting\nbetween stronger focus and broader views - sometimes even attending to empty\nspace ahead. In contrast, agent maps maintain a consistent broad focus on most\nobjects, including distant ones and the player. Quantitative analysis further\ndemonstrates that human CTR maps align more closely with TIOA than agent maps\ndo. Our findings indicate that the CTR attention network can effectively reveal\nhuman covert attention patterns from gameplay alone, without the need for\nadditional data like brain activity recordings. This work contributes to\nunderstanding human-agent attention differences and enables the development of\nRL agents augmented with human covert attention."}
{"id": "2504.11195", "pdf": "https://arxiv.org/pdf/2504.11195", "abs": "https://arxiv.org/abs/2504.11195", "authors": ["Lijun Sheng", "Jian Liang", "Zilei Wang", "Ran He"], "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "CVPR 2025", "summary": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT."}
{"id": "2504.11247", "pdf": "https://arxiv.org/pdf/2504.11247", "abs": "https://arxiv.org/abs/2504.11247", "authors": ["Fikrican Özgür", "René Zurbrügg", "Suryansh Kumar"], "title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "10 pages, 9 figures, 6 tables", "summary": "Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art\nalgorithm for achieving sample-efficient multi-goal reinforcement learning (RL)\nin robotic manipulation tasks with binary rewards. HER facilitates learning\nfrom failed attempts by replaying trajectories with redefined goals. However,\nit relies on a heuristic-based replay method that lacks a principled framework.\nTo address this limitation, we introduce a novel replay strategy,\n\"Next-Future\", which focuses on rewarding single-step transitions. This\napproach significantly enhances sample efficiency and accuracy in learning\nmulti-goal Markov decision processes (MDPs), particularly under stringent\naccuracy requirements -- a critical aspect for performing complex and precise\nrobotic-arm tasks. We demonstrate the efficacy of our method by highlighting\nhow single-step learning enables improved value approximation within the\nmulti-goal RL framework. The performance of the proposed replay strategy is\nevaluated across eight challenging robotic manipulation tasks, using ten random\nseeds for training. Our results indicate substantial improvements in sample\nefficiency for seven out of eight tasks and higher success rates in six tasks.\nFurthermore, real-world experiments validate the practical feasibility of the\nlearned policies, demonstrating the potential of \"Next-Future\" in solving\ncomplex robotic-arm tasks."}
{"id": "2504.11249", "pdf": "https://arxiv.org/pdf/2504.11249", "abs": "https://arxiv.org/abs/2504.11249", "authors": ["Luke Evans", "Octavian-Vlad Murad", "Lars Dingeldein", "Pilar Cossio", "Roberto Covino", "Marina Meila"], "title": "Cryo-em images are intrinsically low dimensional", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "q-bio.BM", "stat.ML"], "comment": null, "summary": "Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry."}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257", "abs": "https://arxiv.org/abs/2504.11257", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/"}
{"id": "2504.11286", "pdf": "https://arxiv.org/pdf/2504.11286", "abs": "https://arxiv.org/abs/2504.11286", "authors": ["Pengcheng Zheng", "Kecheng Chen", "Jiaxin Huang", "Bohao Chen", "Ju Liu", "Yazhou Ren", "Xiaorong Pu"], "title": "Efficient Medical Image Restoration via Reliability Guided Learning in Frequency Domain", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency."}
{"id": "2504.11389", "pdf": "https://arxiv.org/pdf/2504.11389", "abs": "https://arxiv.org/abs/2504.11389", "authors": ["Kevin Xie", "Amirmojtaba Sabour", "Jiahui Huang", "Despoina Paschalidou", "Greg Klar", "Umar Iqbal", "Sanja Fidler", "Xiaohui Zeng"], "title": "VideoPanda: Video Panoramic Diffusion with Multi-view Attention", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project website at\n  https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/", "summary": "High resolution panoramic video content is paramount for immersive\nexperiences in Virtual Reality, but is non-trivial to collect as it requires\nspecialized equipment and intricate camera setups. In this work, we introduce\nVideoPanda, a novel approach for synthesizing 360$^\\circ$ videos conditioned on\ntext or single-view video data. VideoPanda leverages multi-view attention\nlayers to augment a video diffusion model, enabling it to generate consistent\nmulti-view videos that can be combined into immersive panoramic content.\nVideoPanda is trained jointly using two conditions: text-only and single-view\nvideo, and supports autoregressive generation of long-videos. To overcome the\ncomputational burden of multi-view video generation, we randomly subsample the\nduration and camera views used during training and show that the model is able\nto gracefully generalize to generating more frames during inference. Extensive\nevaluations on both real-world and synthetic video datasets demonstrate that\nVideoPanda generates more realistic and coherent 360$^\\circ$ panoramas across\nall input conditions compared to existing methods. Visit the project website at\nhttps://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results."}
{"id": "2504.11438", "pdf": "https://arxiv.org/pdf/2504.11438", "abs": "https://arxiv.org/abs/2504.11438", "authors": ["Lewis Clifton", "Xin Tian", "Duangdao Palasuwan", "Phandee Watanaboonyongcharoen", "Ponlapat Rojnuckarin", "Nantheera Anantrasirichai"], "title": "Mamba-Based Ensemble learning for White Blood Cell Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification."}
