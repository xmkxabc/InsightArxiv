{"id": "2507.23229", "title": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation", "authors": ["Yufei Chen", "Yao Wang", "Haibin Zhang", "Tao Gu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23229v1", "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23229v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23453", "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems", "authors": ["Lijia Liu", "Takumi Kondo", "Kyohei Atarashi", "Koh Takeuchi", "Jiyi Li", "Shigeru Saito", "Hisashi Kashima"], "categories": ["cs.CR", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23453v1", "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23453v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23611", "title": "LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora", "authors": ["Estelle Ruellan", "Eric Clay", "Nicholas Ascoli"], "categories": ["cs.CR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23611v1", "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23611v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23641", "title": "Polynomial Lattices for the BIKE Cryptosystem", "authors": ["Michael Schaller"], "categories": ["cs.CR", "11T71, 94A60"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23641v1", "summary": "In this paper we introduce a rank $2$ lattice over a polynomial ring arising\nfrom the public key of the BIKE cryptosystem \\cite{aragon2022bike}. The secret\nkey is a sparse vector in this lattice. We study properties of this lattice and\ngeneralize the recovery of weak keys from \\cite{BardetDLO16}. In particular, we\nshow that they implicitly solved a shortest vector problem in the lattice we\nconstructed. Rather than finding only a shortest vector, we obtain a reduced\nbasis of the lattice which makes it possible to check for more weak keys.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23641v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23002", "title": "Noise-Coded Illumination for Forensic and Photometric Video Analysis", "authors": ["Peter F. Michael", "Zekun Hao", "Serge Belongie", "Abe Davis"], "categories": ["cs.GR", "cs.CR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      ACM Transactions on Graphics (2025), presented at SIGGRAPH 2025", "url": "http://arxiv.org/abs/2507.23002v1", "summary": "The proliferation of advanced tools for manipulating video has led to an arms\nrace, pitting those who wish to sow disinformation against those who want to\ndetect and expose it. Unfortunately, time favors the ill-intentioned in this\nrace, with fake videos growing increasingly difficult to distinguish from real\nones. At the root of this trend is a fundamental advantage held by those\nmanipulating media: equal access to a distribution of what we consider\nauthentic (i.e., \"natural\") video. In this paper, we show how coding very\nsubtle, noise-like modulations into the illumination of a scene can help combat\nthis advantage by creating an information asymmetry that favors verification.\nOur approach effectively adds a temporal watermark to any video recorded under\ncoded illumination. However, rather than encoding a specific message, this\nwatermark encodes an image of the unmanipulated scene as it would appear lit\nonly by the coded illumination. We show that even when an adversary knows that\nour technique is being used, creating a plausible coded fake video amounts to\nsolving a second, more difficult version of the original adversarial content\ncreation problem at an information disadvantage. This is a promising avenue for\nprotecting high-stakes settings like public events and interviews, where the\ncontent on display is a likely target for manipulation, and while the\nillumination can be controlled, the cameras capturing video cannot.", "comment": "ACM Transactions on Graphics (2025), presented at SIGGRAPH 2025", "pdf_url": "http://arxiv.org/pdf/2507.23002v1", "cate": "cs.GR", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23432", "title": "Scalable contribution bounding to achieve privacy", "authors": ["Vincent Cohen-Addad", "Alessandro Epasto", "Jason Lee", "Morteza Zadimoghaddam"], "categories": ["cs.DS", "cs.CR", "cs.DC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23432v1", "summary": "In modern datasets, where single records can have multiple owners, enforcing\nuser-level differential privacy requires capping each user's total\ncontribution. This \"contribution bounding\" becomes a significant combinatorial\nchallenge. Existing sequential algorithms for this task are computationally\nintensive and do not scale to the massive datasets prevalent today. To address\nthis scalability bottleneck, we propose a novel and efficient distributed\nalgorithm. Our approach models the complex ownership structure as a hypergraph,\nwhere users are vertices and records are hyperedges. The algorithm proceeds in\nrounds, allowing users to propose records in parallel. A record is added to the\nfinal dataset only if all its owners unanimously agree, thereby ensuring that\nno user's predefined contribution limit is violated. This method aims to\nmaximize the size of the resulting dataset for high utility while providing a\npractical, scalable solution for implementing user-level privacy in large,\nreal-world systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23432v1", "cate": "cs.DS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23608", "title": "Medical Image De-Identification Benchmark Challenge", "authors": ["Linmin Pei", "Granger Sutton", "Michael Rutherford", "Ulrike Wagner", "Tracy Nolan", "Kirk Smith", "Phillip Farmer", "Peter Gu", "Ambar Rana", "Kailing Chen", "Thomas Ferleman", "Brian Park", "Ye Wu", "Jordan Kojouharov", "Gargi Singh", "Jon Lemon", "Tyler Willis", "Milos Vukadinovic", "Grant Duffy", "Bryan He", "David Ouyang", "Marco Pereanez", "Daniel Samber", "Derek A. Smith", "Christopher Cannistraci", "Zahi Fayad", "David S. Mendelson", "Michele Bufano", "Elmar Kotter", "Hamideh Haghiri", "Rajesh Baidya", "Stefan Dvoretskii", "Klaus H. Maier-Hein", "Marco Nolden", "Christopher Ablett", "Silvia Siggillino", "Sandeep Kaushik", "Hongzhu Jiang", "Sihan Xie", "Zhiyu Wan", "Alex Michie", "Simon J Doran", "Angeline Aurelia Waly", "Felix A. Nathaniel Liang", "Humam Arshad Mustagfirin", "Michelle Grace Felicia", "Kuo Po Chih", "Rahul Krish", "Ghulam Rasool", "Nidhal Bouaynaya", "Nikolas Koutsoubis", "Kyle Naddeo", "Kartik Pandit", "Tony O'Sullivan", "Raj Krish", "Qinyan Pan", "Scott Gustafson", "Benjamin Kopchick", "Laura Opsahl-Ong", "Andrea Olvera-Morales", "Jonathan Pinney", "Kathryn Johnson", "Theresa Do", "Juergen Klenk", "Maria Diaz", "Arti Singh", "Rong Chai", "David A. Clunie", "Fred Prior", "Keyvan Farahani"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19 pages", "url": "http://arxiv.org/abs/2507.23608v1", "summary": "The de-identification (deID) of protected health information (PHI) and\npersonally identifiable information (PII) is a fundamental requirement for\nsharing medical images, particularly through public repositories, to ensure\ncompliance with patient privacy laws. In addition, preservation of non-PHI\nmetadata to inform and enable downstream development of imaging artificial\nintelligence (AI) is an important consideration in biomedical research. The\ngoal of MIDI-B was to provide a standardized platform for benchmarking of DICOM\nimage deID tools based on a set of rules conformant to the HIPAA Safe Harbor\nregulation, the DICOM Attribute Confidentiality Profiles, and best practices in\npreservation of research-critical metadata, as defined by The Cancer Imaging\nArchive (TCIA). The challenge employed a large, diverse, multi-center, and\nmulti-modality set of real de-identified radiology images with synthetic\nPHI/PII inserted.\n  The MIDI-B Challenge consisted of three phases: training, validation, and\ntest. Eighty individuals registered for the challenge. In the training phase,\nwe encouraged participants to tune their algorithms using their in-house or\npublic data. The validation and test phases utilized the DICOM images\ncontaining synthetic identifiers (of 216 and 322 subjects, respectively). Ten\nteams successfully completed the test phase of the challenge. To measure\nsuccess of a rule-based approach to image deID, scores were computed as the\npercentage of correct actions from the total number of required actions. The\nscores ranged from 97.91% to 99.93%. Participants employed a variety of\nopen-source and proprietary tools with customized configurations, large\nlanguage models, and optical character recognition (OCR). In this paper we\nprovide a comprehensive report on the MIDI-B Challenge's design,\nimplementation, results, and lessons learned.", "comment": "19 pages", "pdf_url": "http://arxiv.org/pdf/2507.23608v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2405.05846", "title": "An Inversion-based Measure of Memorization for Diffusion Models", "authors": ["Zhe Ma", "Qingming Li", "Xuhong Zhang", "Tianyu Du", "Ruixiao Lin", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "categories": ["cs.CR", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2405.05846v3", "summary": "The past few years have witnessed substantial advances in image generation\npowered by diffusion models. However, it was shown that diffusion models are\nsusceptible to training data memorization, raising significant concerns\nregarding copyright infringement and privacy invasion. This study delves into a\nrigorous analysis of memorization in diffusion models. We introduce InvMM, an\ninversion-based measure of memorization, which is based on inverting a\nsensitive latent noise distribution accounting for the replication of an image.\nFor accurate estimation of the measure, we propose an adaptive algorithm that\nbalances the normality and sensitivity of the noise distribution. Comprehensive\nexperiments across four datasets, conducted on both unconditional and\ntext-guided diffusion models, demonstrate that InvMM provides a reliable and\ncomplete quantification of memorization. Notably, InvMM is commensurable\nbetween samples, reveals the true extent of memorization from an adversarial\nstandpoint and implies how memorization differs from membership. In practice,\nit serves as an auditing tool for developers to reliably assess the risk of\nmemorization, thereby contributing to the enhancement of trustworthiness and\nprivacy-preserving capabilities of diffusion models.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2405.05846v3", "cate": "cs.CR", "date": "2024-05-09", "updated": "2025-07-31"}
{"id": "2405.13156", "title": "A Privacy-Preserving DAO Model Using NFT Authentication for the Punishment not Reward Blockchain Architecture", "authors": ["Talgar Bayan", "Richard Banach"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      This paper was accepted and presented at the International Conference on Blockchain Research and Applications (BCRA 2024), Hangzhou, China, July 26-27, 2024. An extended version has been submitted to the journal Blockchain: Research and Applications (Elsevier) for publication consideration. This arXiv version corresponds to the conference-accepted manuscript", "url": "http://arxiv.org/abs/2405.13156v2", "summary": "This paper presents a decentralised autonomous organisation (DAO) model that\nuses non-fungible tokens (NFTs) for identity management and privacy-preserving\ninteractions within a Punishment not Reward (PnR) blockchain mechanism. The\nproposed model introduces a dual NFT architecture deployed on Layer 2 networks:\nMembership NFTs (\\(NFT_{auth}\\)) for authentication and access control and\ninteraction NFTs (\\(NFT_{priv}\\)) for private interactions among participants.\nOur Layer 2 implementation achieves 97\\% gas cost reduction while maintaining\nsecurity through cross-chain mechanisms. The identity management system\nincorporates decentralised KYC processes and Sybil attack resistance using\nsoulbound token characteristics. Governance operates through smart contracts\nthat manage reputation and administer punitive measures, including conditional\nidentity disclosure for forensic purposes. Governance operates through smart\ncontracts that manage reputation and administer punitive measures, including\nconditional identity disclosure when misconduct is detected.", "comment": "This paper was accepted and presented at the International Conference\n  on Blockchain Research and Applications (BCRA 2024), Hangzhou, China, July\n  26-27, 2024. An extended version has been submitted to the journal\n  Blockchain: Research and Applications (Elsevier) for publication\n  consideration. This arXiv version corresponds to the conference-accepted\n  manuscript", "pdf_url": "http://arxiv.org/pdf/2405.13156v2", "cate": "cs.CR", "date": "2024-05-21", "updated": "2025-07-31"}
{"id": "2408.05997", "title": "On the Formalization of Cryptographic Migration", "authors": ["Daniel Loebenberger", "Stefan-Lukas Gazdag", "Daniel Herzinger", "Eduard Hirsch", "Christian Näther", "Jan-Philipp Steghöfer"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.05997v4", "summary": "We present a novel approach to gaining insight into the structure of\ncryptographic migration problems which are classic problems in applied\ncryptography. We use a formal model to capture the inherent dependencies and\ncomplexities of such transitions. Using classical mathematical results from\ncombinatorics, probability theory, and combinatorial analysis, we evaluate the\nchallenges of migrating large cryptographic IT infrastructures and prove that -\nin a suitable sense - cryptographic migration exhibits a certain expected\ncomplexity. We also provide numerical data for selected parameter sets.\nFurthermore, we analyze the proposed model in terms of real-world patterns and\nits practical applicability. Additionally, we discuss the challenges of\nmodeling real-world migration projects. As concrete examples we examine the\ntransition to post-quantum cryptography of the CI/CD system GitLab and the\nmulti-level technological transition of distribution power grids. This work\npaves the way for future advancements in both the theoretical understanding and\npractical implementation of cryptographic migration strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.05997v4", "cate": "cs.CR", "date": "2024-08-12", "updated": "2025-07-31"}
{"id": "2502.11641", "title": "A Zero-Knowledge Proof for the Syndrome Decoding Problem in the Lee Metric", "authors": ["Mladen Kovačević", "Tatjana Grbić", "Darko Čapko", "Nemanja Nedić", "Srdjan Vukmirović"], "categories": ["cs.CR", "cs.IT", "math.IT", "94A60, 68P25"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11641v3", "summary": "The syndrome decoding problem is one of the NP-complete problems lying at the\nfoundation of code-based cryptography. The variant thereof where the distance\nbetween vectors is measured with respect to the Lee metric, rather than the\nmore commonly used Hamming metric, has been analyzed recently in several works\ndue to its potential relevance for building more efficient code-based\ncryptosystems. The purpose of this article is to present a zero-knowledge proof\nof knowledge for this variant of the problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11641v3", "cate": "cs.CR", "date": "2025-02-17", "updated": "2025-07-31"}
{"id": "2503.06989", "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application", "authors": ["Wenzhuo Xu", "Zhipeng Wei", "Xiongtao Sun", "Zonghao Ying", "Deyue Zhang", "Dongdong Yang", "Xiangzheng Zhang", "Quanchen Zou"], "categories": ["cs.CR", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06989v2", "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06989v2", "cate": "cs.CR", "date": "2025-03-10", "updated": "2025-07-31"}
{"id": "2504.13201", "title": "CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via Subspace Concept Rotation", "authors": ["Jirui Yang", "Zheyu Lin", "Zhihui Lu", "Yinggui Wang", "Lei Wang", "Tao Wei", "Xin Du", "Shuhan Yang"], "categories": ["cs.CR", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13201v2", "summary": "Large Language Models (LLMs) are increasingly becoming the cognitive core of\nEmbodied Intelligence (EI) systems, such as robots and autonomous vehicles.\nHowever, this integration also exposes them to serious jailbreak risks, where\nmalicious instructions can be transformed into dangerous physical actions.\nExisting defense mechanisms suffer from notable drawbacks--including high\ntraining costs, significant inference delays, and complex hyperparameter\ntuning--which limit their practical applicability. To address these challenges,\nwe propose a novel and efficient inference-time defense framework: Concept\nEnhancement Engineering (CEE). CEE enhances the model's inherent safety\nmechanisms by directly manipulating its internal representations, requiring\nneither additional training nor external modules, thereby improving defense\nefficiency. Furthermore, CEE introduces a rotation-based control mechanism that\nenables stable and linearly tunable behavioral control of the model. This\ndesign eliminates the need for tedious manual tuning and avoids the output\ndegradation issues commonly observed in other representation engineering\nmethods. Extensive experiments across multiple EI safety benchmarks and diverse\nattack scenarios demonstrate that CEE significantly improves the defense\nsuccess rates of various multimodal LLMs. It effectively mitigates safety risks\nwhile preserving high-quality generation and inference efficiency, offering a\npromising solution for deploying safer embodied intelligence systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13201v2", "cate": "cs.CR", "date": "2025-04-15", "updated": "2025-07-31"}
{"id": "2507.01694", "title": "Graph Representation-based Model Poisoning on Federated Large Language Models", "authors": ["Hanlin Cai", "Haofan Dong", "Houtianfu Wang", "Kai Li", "Ozgur B. Akan"], "categories": ["cs.CR", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures (Submitted to IEEE Communication Magazine)", "url": "http://arxiv.org/abs/2507.01694v2", "summary": "Federated large language models (FedLLMs) enable powerful generative\ncapabilities within wireless networks while preserving data privacy.\nNonetheless, FedLLMs remain vulnerable to model poisoning attacks. This article\nfirst reviews recent advancements in model poisoning techniques and existing\ndefense mechanisms for FedLLMs, underscoring critical limitations, especially\nwhen dealing with non-IID textual data distributions. Current defense\nstrategies predominantly employ distance or similarity-based outlier detection\nmechanisms, relying on the assumption that malicious updates markedly differ\nfrom benign statistical patterns. However, this assumption becomes inadequate\nagainst adaptive adversaries targeting billion-parameter LLMs. The article\nfurther investigates graph representation-based model poisoning (GRMP), an\nemerging attack paradigm that exploits higher-order correlations among benign\nclient gradients to craft malicious updates indistinguishable from legitimate\nones. GRMP can effectively circumvent advanced defense systems, causing\nsubstantial degradation in model accuracy and overall performance. Moreover,\nthe article outlines a forward-looking research roadmap that emphasizes the\nnecessity of graph-aware secure aggregation methods, specialized vulnerability\nmetrics tailored for FedLLMs, and evaluation frameworks to enhance the\nrobustness of federated language model deployments.", "comment": "7 pages, 5 figures (Submitted to IEEE Communication Magazine)", "pdf_url": "http://arxiv.org/pdf/2507.01694v2", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-31"}
{"id": "2507.08540", "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection", "authors": ["Ioannis Lamprou", "Alexander Shevtsov", "Ioannis Arapakis", "Sotiris Ioannidis"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08540v2", "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08540v2", "cate": "cs.CR", "date": "2025-07-11", "updated": "2025-07-31"}
{"id": "2507.19060", "title": "PurpCode: Reasoning for Safer Code Generation", "authors": ["Jiawei Liu", "Nirav Diwan", "Zhe Wang", "Haoyu Zhai", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Muntasir Wahed", "Yinlin Deng", "Hadjer Benkraouda", "Yuxiang Wei", "Lingming Zhang", "Ismini Lourentzou", "Gang Wang"], "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19060v2", "summary": "We introduce PurpCode, the first post-training recipe for training safe code\nreasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule\nLearning, which explicitly teaches the model to reference cybersafety rules to\ngenerate vulnerability-free code and to avoid facilitating malicious\ncyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms.\nTo empower the training pipelines with comprehensive cybersafety data, we\nconduct internal red-teaming to synthesize comprehensive and high-coverage\nprompts based on real-world tasks for inducing unsafe cyberactivities in the\nmodel. Based on PurpCode, we develop a reasoning-based coding model, namely\nPurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming\nvarious frontier models. Meanwhile, our alignment method decreases the model\noverrefusal rates in both general and cybersafety-specific scenarios, while\npreserving model utility in both code generation and common security knowledge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19060v2", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2507.23080", "title": "Causal-Inspired Multi-Agent Decision-Making via Graph Reinforcement Learning", "authors": ["Jing Wang", "Yan Jin", "Fei Ding", "Chongfeng Wei"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23080v1", "summary": "Since the advent of autonomous driving technology, it has experienced\nremarkable progress over the last decade. However, most existing research still\nstruggles to address the challenges posed by environments where multiple\nvehicles have to interact seamlessly. This study aims to integrate causal\nlearning with reinforcement learning-based methods by leveraging causal\ndisentanglement representation learning (CDRL) to identify and extract causal\nfeatures that influence optimal decision-making in autonomous vehicles. These\nfeatures are then incorporated into graph neural network-based reinforcement\nlearning algorithms to enhance decision-making in complex traffic scenarios. By\nusing causal features as inputs, the proposed approach enables the optimization\nof vehicle behavior at an unsignalized intersection. Experimental results\ndemonstrate that our proposed method achieves the highest average reward during\ntraining and our approach significantly outperforms other learning-based\nmethods in several key metrics such as collision rate and average cumulative\nreward during testing. This study provides a promising direction for advancing\nmulti-agent autonomous driving systems and make autonomous vehicles' navigation\nsafer and more efficient in complex traffic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23080v1", "cate": "cs.MA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.05727", "title": "Toward Integrated Solutions: A Systematic Interdisciplinary Review of Cybergrooming Research", "authors": ["Heajun An", "Marcos Silva", "Qi Zhang", "Arav Singh", "Minqian Liu", "Xinyi Zhang", "Sarvech Qadir", "Sang Won Lee", "Lifu Huang", "Pamela J. Wisniewski", "Jin-Hee Cho"], "categories": ["cs.CY", "cs.CR"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05727v2", "summary": "Cybergrooming exploits minors through online trust-building, yet research\nremains fragmented, limiting holistic prevention. Social sciences focus on\nbehavioral insights, while computational methods emphasize detection, but their\nintegration remains insufficient. This review systematically synthesizes both\nfields using the PRISMA framework to enhance clarity, reproducibility, and\ncross-disciplinary collaboration. Findings show that qualitative methods offer\ndeep insights but are resource-intensive, machine learning models depend on\ndata quality, and standard metrics struggle with imbalance and cultural\nnuances. By bridging these gaps, this review advances interdisciplinary\ncybergrooming research, guiding future efforts toward more effective prevention\nand detection strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05727v2", "cate": "cs.CY", "date": "2025-02-18", "updated": "2025-07-31"}
{"id": "2507.23644", "title": "Barriers to Healthcare: Agent-Based Modeling to Mitigate Inequity", "authors": ["Alba Aguilera", "Georgina Curto", "Nardine Osman"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23644v1", "summary": "Agent-based simulations have an enormous potential as tools to evaluate\nsocial policies in a non-invasive way, before these are implemented to\nreal-world populations. However, the recommendations that these computational\napproaches may offer to tackle urgent human development challenges can vary\nsubstantially depending on how we model agents' (people) behaviour and the\ncriteria that we use to measure inequity. In this paper, we integrate the\nconceptual framework of the capability approach (CA), which is explicitly\ndesigned to promote and assess human well-being, to guide the simulation and\nevaluate the effectiveness of policies. We define a reinforcement learning\nenvironment where agents behave to restore their capabilities under the\nconstraints of a specific policy. Working in collaboration with local\nstakeholders, non-profits and domain experts, we apply our model in a case\nstudy to mitigate health inequity among the population experiencing\nhomelessness (PEH) in Barcelona. By doing so, we present the first proof of\nconcept simulation, aligned with the CA for human development, to assess the\nimpact of policies under parliamentary discussion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23644v1", "cate": "cs.MA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23694", "title": "A survey of multi-agent geosimulation methodologies: from ABM to LLM", "authors": ["Virginia Padilla", "Jacinto Dávila"], "categories": ["cs.MA", "cs.AI", "68T42", "I.2.11"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      20 pages, 1 table", "url": "http://arxiv.org/abs/2507.23694v1", "summary": "We provide a comprehensive examination of agent-based approaches that codify\nthe principles and linkages underlying multi-agent systems, simulations, and\ninformation systems. Based on two decades of study, this paper confirms a\nframework intended as a formal specification for geosimulation platforms. Our\nfindings show that large language models (LLMs) can be effectively incorporated\nas agent components if they follow a structured architecture specific to\nfundamental agent activities such as perception, memory, planning, and action.\nThis integration is precisely consistent with the architecture that we\nformalize, providing a solid platform for next-generation geosimulation\nsystems.", "comment": "20 pages, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.23694v1", "cate": "cs.MA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22902", "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22902v1", "summary": "Background: Globally we face a projected shortage of 11 million healthcare\npractitioners by 2030, and administrative burden consumes 50% of clinical time.\nArtificial intelligence (AI) has the potential to help alleviate these\nproblems. However, no end-to-end autonomous large language model (LLM)-based AI\nsystem has been rigorously evaluated in real-world clinical practice. In this\nstudy, we evaluated whether a multi-agent LLM-based AI framework can function\nautonomously as an AI doctor in a virtual urgent care setting. Methods: We\nretrospectively compared the performance of the multi-agent AI system Doctronic\nand board-certified clinicians across 500 consecutive urgent-care telehealth\nencounters. The primary end points: diagnostic concordance, treatment plan\nconsistency, and safety metrics, were assessed by blinded LLM-based\nadjudication and expert human review. Results: The top diagnosis of Doctronic\nand clinician matched in 81% of cases, and the treatment plan aligned in 99.2%\nof cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not\nsupported by clinical findings). In an expert review of discordant cases, AI\nperformance was superior in 36.1%, and human performance was superior in 9.3%;\nthe diagnoses were equivalent in the remaining cases. Conclusions: In this\nfirst large-scale validation of an autonomous AI doctor, we demonstrated strong\ndiagnostic and treatment plan concordance with human clinicians, with AI\nperformance matching and in some cases exceeding that of practicing clinicians.\nThese findings indicate that multi-agent AI systems achieve comparable clinical\ndecision-making to human providers and offer a potential solution to healthcare\nworkforce shortages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22902v1", "cate": "cs.HC", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.22951", "title": "Unifying Post-hoc Explanations of Knowledge Graph Completions", "authors": ["Alessandro Lonardi", "Samy Badreddine", "Tarek R. Besold", "Pablo Sanchez Martin"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22951v1", "summary": "Post-hoc explainability for Knowledge Graph Completion (KGC) lacks\nformalization and consistent evaluations, hindering reproducibility and\ncross-study comparisons. This paper argues for a unified approach to post-hoc\nexplainability in KGC. First, we propose a general framework to characterize\npost-hoc explanations via multi-objective optimization, balancing their\neffectiveness and conciseness. This unifies existing post-hoc explainability\nalgorithms in KGC and the explanations they produce. Next, we suggest and\nempirically support improved evaluation protocols using popular metrics like\nMean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of\ninterpretability as the ability of explanations to address queries meaningful\nto end-users. By unifying methods and refining evaluation standards, this work\naims to make research in KGC explainability more reproducible and impactful.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22951v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23015", "title": "Learning to Prune Branches in Modern Tree-Fruit Orchards", "authors": ["Abhinav Jain", "Cindy Grimm", "Stefan Lee"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23015v1", "summary": "Dormant tree pruning is labor-intensive but essential to maintaining modern\nhighly-productive fruit orchards. In this work we present a closed-loop\nvisuomotor controller for robotic pruning. The controller guides the cutter\nthrough a cluttered tree environment to reach a specified cut point and ensures\nthe cutters are perpendicular to the branch. We train the controller using a\nnovel orchard simulation that captures the geometric distribution of branches\nin a target apple orchard configuration. Unlike traditional methods requiring\nfull 3D reconstruction, our controller uses just optical flow images from a\nwrist-mounted camera. We deploy our learned policy in simulation and the\nreal-world for an example V-Trellis envy tree with zero-shot transfer,\nachieving a 30% success rate -- approximately half the performance of an oracle\nplanner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23015v1", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22929", "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow", "authors": ["Xiaoyu Pan", "Yang Bai", "Ke Zou", "Yang Zhou", "Jun Zhou", "Huazhu Fu", "Yih-Chung Tham", "Yong Liu"], "categories": ["cs.CL", "cs.CV", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 figures, 5 tables. submit/6621751", "url": "http://arxiv.org/abs/2507.22929v1", "summary": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic\ndiagnosis, holding significant potential to address vision-threatening\ndiseases. However, their accuracy is constrained by hallucinations stemming\nfrom limited ophthalmic knowledge, insufficient visual localization and\nreasoning capabilities, and a scarcity of multimodal ophthalmic data, which\ncollectively impede precise lesion detection and disease diagnosis.\nFurthermore, existing medical benchmarks fail to effectively evaluate various\ntypes of hallucinations or provide actionable solutions to mitigate them. To\naddress the above challenges, we introduce EH-Benchmark, a novel ophthalmology\nbenchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'\nhallucinations based on specific tasks and error types into two primary\nclasses: Visual Understanding and Logical Composition, each comprising multiple\nsubclasses. Given that MLLMs predominantly rely on language-based reasoning\nrather than visual processing, we propose an agent-centric, three-phase\nframework, including the Knowledge-Level Retrieval stage, the Task-Level Case\nStudies stage, and the Result-Level Validation stage. Experimental results show\nthat our multi-agent framework significantly mitigates both types of\nhallucinations, enhancing accuracy, interpretability, and reliability. Our\nproject is available at https://github.com/ppxy1/EH-Benchmark.", "comment": "9 figures, 5 tables. submit/6621751", "pdf_url": "http://arxiv.org/pdf/2507.22929v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.23018", "title": "Data Readiness for Scientific AI at Scale", "authors": ["Wesley Brewer", "Patrick Widener", "Valentine Anantharaj", "Feiyi Wang", "Tom Beck", "Arjun Shankar", "Sarp Oral"], "categories": ["cs.AI", "cs.CE", "cs.DC", "cs.LG", "I.2.6"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 1 figure, 2 tables", "url": "http://arxiv.org/abs/2507.23018v1", "summary": "This paper examines how Data Readiness for AI (DRAI) principles apply to\nleadership-scale scientific datasets used to train foundation models. We\nanalyze archetypal workflows across four representative domains - climate,\nnuclear fusion, bio/health, and materials - to identify common preprocessing\npatterns and domain-specific constraints. We introduce a two-dimensional\nreadiness framework composed of Data Readiness Levels (raw to AI-ready) and\nData Processing Stages (ingest to shard), both tailored to high performance\ncomputing (HPC) environments. This framework outlines key challenges in\ntransforming scientific data for scalable AI training, emphasizing\ntransformer-based generative models. Together, these dimensions form a\nconceptual maturity matrix that characterizes scientific data readiness and\nguides infrastructure development toward standardized, cross-domain support for\nscalable and reproducible AI for science.", "comment": "10 pages, 1 figure, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.23018v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23045", "title": "A Certifably Correct Algorithm for Generalized Robot-World and Hand-Eye Calibration", "authors": ["Emmett Wise", "Pushyami Kaveti", "Qilong Chen", "Wenhao Wang", "Hanumant Singh", "Jonathan Kelly", "David M. Rosen", "Matthew Giamou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      25 pages, 10 figures, submitted to the International Journal of Robotics Research", "url": "http://arxiv.org/abs/2507.23045v1", "summary": "Automatic extrinsic sensor calibration is a fundamental problem for\nmulti-sensor platforms. Reliable and general-purpose solutions should be\ncomputationally efficient, require few assumptions about the structure of the\nsensing environment, and demand little effort from human operators. Since the\nengineering effort required to obtain accurate calibration parameters increases\nwith the number of sensors deployed, robotics researchers have pursued methods\nrequiring few assumptions about the sensing environment and minimal effort from\nhuman operators. In this work, we introduce a fast and certifiably globally\noptimal algorithm for solving a generalized formulation of the\n$\\textit{robot-world and hand-eye calibration}$ (RWHEC) problem. The\nformulation of RWHEC presented is \"generalized\" in that it supports the\nsimultaneous estimation of multiple sensor and target poses, and permits the\nuse of monocular cameras that, alone, are unable to measure the scale of their\nenvironments. In addition to demonstrating our method's superior performance\nover existing solutions, we derive novel identifiability criteria and establish\n$\\textit{a priori}$ guarantees of global optimality for problem instances with\nbounded measurement errors. We also introduce a complementary Lie-algebraic\nlocal solver for RWHEC and compare its performance with our global method and\nprior art. Finally, we provide a free and open-source implementation of our\nalgorithms and experiments.", "comment": "25 pages, 10 figures, submitted to the International Journal of\n  Robotics Research", "pdf_url": "http://arxiv.org/pdf/2507.23045v1", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23167", "title": "LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration", "authors": ["Jizhou Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23167v1", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, with different models excelling in distinct domains and specific\nabilities. Effectively combining the predictions of multiple LLMs is crucial\nfor enhancing system robustness and performance. However, existing ensemble\nmethods often rely on simple techniques like voting or logits ensembling, which\noverlook the varying confidence and reliability of models in different\ncontexts. In this work, we propose LENS (Learning ENsemble confidence from\nNeural States), a novel approach that learns to estimate model confidence by\nanalyzing internal representations. For each LLM, we train a lightweight linear\nconfidence predictor that leverages layer-wise hidden states and normalized\nprobabilities as inputs. This allows for more nuanced weighting of model\npredictions based on their context-dependent reliability. Our method does not\nrequire modifying the model parameters and requires negligible additional\ncomputation. Experimental results on multiple-choice and boolean\nquestion-answering tasks demonstrate that LENS outperforms traditional ensemble\nmethods by a substantial margin. Our findings suggest that internal\nrepresentations provide valuable signals for determining model confidence and\ncan be effectively leveraged for ensemble learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23167v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23067", "title": "FairReason: Balancing Reasoning and Social Bias in MLLMs", "authors": ["Zhenyu Pan", "Yutong Zhang", "Jianshu Zhang", "Haoran Lu", "Haozheng Luo", "Yuwei Han", "Philip S. Yu", "Manling Li", "Han Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23067v1", "summary": "Multimodal Large Language Models (MLLMs) already achieve state-of-the-art\nresults across a wide range of tasks and modalities. To push their reasoning\nability further, recent studies explore advanced prompting schemes and\npost-training fine-tuning. Although these techniques improve logical accuracy,\nthey frequently leave the models' outputs burdened with pronounced social\nbiases. Clarifying how reasoning gains interact with bias mitigation-and\nwhether the two objectives inherently trade off-therefore remains an open and\npressing research problem. Our study begins by benchmarking three\nbias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation\n(KD), and rule-based reinforcement learning (RL)-under identical conditions,\nestablishing their baseline strengths and weaknesses. Building on these\nresults, we vary the proportion of debias-focused and reasoning-centric samples\nwithin each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps\nreveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement\nlearning cuts stereotype scores by 10% while retaining 88% of the model's\noriginal reasoning accuracy, offering concrete guidance for balancing fairness\nand capability in MLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23067v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23053", "title": "In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion", "authors": ["Yuanhao Chen", "Liu Zhao", "Ji Ma", "Peng Lu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23053v1", "summary": "Quadruped robots face persistent challenges in achieving versatile locomotion\ndue to limitations in reference motion data diversity. To address these\nchallenges, this approach introduces an in-between motion generation based\nmulti-style quadruped robot locomotion framework, integrating synergistic\nadvances in motion generation and imitation learning. Our approach establishes\na unified pipeline addressing two fundamental aspects: First, we propose a CVAE\nbased motion generator, synthesizing multi-style dynamically feasible\nlocomotion sequences between arbitrary start and end states. By embedding\nphysical constraints and leveraging joint poses based phase manifold\ncontinuity, this component produces physically plausible motions spanning\nmultiple gait modalities while ensuring kinematic compatibility with robotic\nmorphologies. Second, we adopt the adversarial motion priors algorithm. We\nvalidate the effectiveness of generated motion data in enhancing controller\nstability and improving velocity tracking performance. The proposed framework\ndemonstrates significant improvements in velocity tracking and deployment\nstability. We successfully deploy the framework on a real-world quadruped\nrobot, and the experimental validation confirms the framework's capability to\ngenerate and execute complex motion profiles, including gallop, tripod,\ntrotting and pacing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23053v1", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23190", "title": "Accessibility Scout: Personalized Accessibility Scans of Built Environments", "authors": ["William Huang", "Xia Su", "Jon E. Froehlich", "Yang Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      18 pages, 16 figures. Presented at ACM UIST 2025", "url": "http://arxiv.org/abs/2507.23190v1", "summary": "Assessing the accessibility of unfamiliar built environments is critical for\npeople with disabilities. However, manual assessments, performed by users or\ntheir personal health professionals, are laborious and unscalable, while\nautomatic machine learning methods often neglect an individual user's unique\nneeds. Recent advances in Large Language Models (LLMs) enable novel approaches\nto this problem, balancing personalization with scalability to enable more\nadaptive and context-aware assessments of accessibility. We present\nAccessibility Scout, an LLM-based accessibility scanning system that identifies\naccessibility concerns from photos of built environments. With use,\nAccessibility Scout becomes an increasingly capable \"accessibility scout\",\ntailoring accessibility scans to an individual's mobility level, preferences,\nand specific environmental interests through collaborative Human-AI\nassessments. We present findings from three studies: a formative study with six\nparticipants to inform the design of Accessibility Scout, a technical\nevaluation of 500 images of built environments, and a user study with 10\nparticipants of varying mobility. Results from our technical evaluation and\nuser study show that Accessibility Scout can generate personalized\naccessibility scans that extend beyond traditional ADA considerations. Finally,\nwe conclude with a discussion on the implications of our work and future steps\nfor building more scalable and personalized accessibility assessments of the\nphysical world.", "comment": "18 pages, 16 figures. Presented at ACM UIST 2025", "pdf_url": "http://arxiv.org/pdf/2507.23190v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23091", "title": "Moravec's Paradox: Towards an Auditory Turing Test", "authors": ["David Noever", "Forrest McKee"], "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23091v1", "summary": "This research work demonstrates that current AI systems fail catastrophically\non auditory tasks that humans perform effortlessly. Drawing inspiration from\nMoravec's paradox (i.e., tasks simple for humans often prove difficult for\nmachines, and vice versa), we introduce an auditory Turing test comprising 917\nchallenges across seven categories: overlapping speech, speech in noise,\ntemporal distortion, spatial audio, coffee-shop noise, phone distortion, and\nperceptual illusions. Our evaluation of state-of-the-art audio models including\nGPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate\nexceeding 93%, with even the best-performing model achieving only 6.9% accuracy\non tasks that humans solved at 7.5 times higher success (52%). These results\nexpose focusing failures in how AI systems process complex auditory scenes,\nparticularly in selective attention, noise robustness, and contextual\nadaptation. Our benchmark not only quantifies the human-machine auditory gap\nbut also provides insights into why these failures occur, suggesting that\ncurrent architectures lack fundamental mechanisms for human-like auditory scene\nanalysis. The traditional design of audio CAPTCHAs highlights common filters\nthat humans evolved but machines fail to select in multimodal language models.\nThis work establishes a diagnostic framework for measuring progress toward\nhuman-level machine listening and highlights the need for novel approaches\nintegrating selective attention, physics-based audio understanding, and\ncontext-aware perception into multimodal AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23091v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23088", "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance", "authors": ["Lalithkumar Seenivasan", "Jiru Xu", "Roger D. Soberanis Mukul", "Hao Ding", "Grayson Byrd", "Yu-Chun Ku", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23088v1", "summary": "Emerging surgical data science and robotics solutions, especially those\ndesigned to provide assistance in situ, require natural human-machine\ninterfaces to fully unlock their potential in providing adaptive and intuitive\naid. Contemporary AI-driven solutions remain inherently rigid, offering limited\nflexibility and restricting natural human-machine interaction in dynamic\nsurgical environments. These solutions rely heavily on extensive task-specific\npre-training, fixed object categories, and explicit manual-prompting. This work\nintroduces a novel Perception Agent that leverages speech-integrated\nprompt-engineered large language models (LLMs), segment anything model (SAM),\nand any-point tracking foundation models to enable a more natural human-machine\ninteraction in real-time intraoperative surgical assistance. Incorporating a\nmemory repository and two novel mechanisms for segmenting unseen elements,\nPerception Agent offers the flexibility to segment both known and unseen\nelements in the surgical scene through intuitive interaction. Incorporating the\nability to memorize novel elements for use in future surgeries, this work takes\na marked step towards human-machine symbiosis in surgical procedures. Through\nquantitative analysis on a public dataset, we show that the performance of our\nagent is on par with considerably more labor-intensive manual-prompting\nstrategies. Qualitatively, we show the flexibility of our agent in segmenting\nnovel elements (instruments, phantom grafts, and gauze) in a custom-curated\ndataset. By offering natural human-machine interaction and overcoming rigidity,\nour Perception Agent potentially brings AI-based real-time assistance in\ndynamic surgical environments closer to reality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23088v1", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23261", "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System", "authors": ["Hui Yi Leong", "Yuqing Wu"], "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23261v1", "summary": "Current multi-agent systems (MAS) frameworks often rely on manually designed\nand static collaboration graph structures, limiting adaptability and\nperformance. To address these limitations, we propose DynaSwarm, a dynamic\nframework that enhances LLM-based MAS through two key innovations: (1) an\nactor-critic reinforcement learning (A2C) mechanism to optimize graph\nstructures with improved stability over prior RL methods, and (2) a dynamic\ngraph selector that adaptively chooses the optimal graph structure for each\ninput sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the\nneed for rigid, one-fits-all graph architectures, instead leveraging\nsample-specific idiosyncrasies to dynamically route queries through specialized\nagent networks. (c) We propose to fine-tune the demonstration retriever to\nfully exploit the power of in-context learning (ICL). Extensive experiments on\nquestion answering, mathematical reasoning, and coding tasks demonstrate that\nDynaSwarm consistently outperforms state-of-the-art single-agent and MAS\nbaselines across multiple LLM backbones. Our findings highlight the importance\nof sample-aware structural flexibility in LLM MAS designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23261v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23163", "title": "Argumentatively Coherent Judgmental Forecasting", "authors": ["Deniz Gorur", "Antonio Rago", "Francesca Toni"], "categories": ["cs.AI", "I.2.7"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      17 pages, 18 figures, ECAI 2025", "url": "http://arxiv.org/abs/2507.23163v1", "summary": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions.", "comment": "17 pages, 18 figures, ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.23163v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23172", "title": "Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks", "authors": ["Vira Joshi", "Zifan Xu", "Bo Liu", "Peter Stone", "Amy Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      RLC 2025", "url": "http://arxiv.org/abs/2507.23172v1", "summary": "Multi-task Reinforcement Learning (MTRL) has emerged as a critical training\nparadigm for applying reinforcement learning (RL) to a set of complex\nreal-world robotic tasks, which demands a generalizable and robust policy. At\nthe same time, \\emph{massively parallelized training} has gained popularity,\nnot only for significantly accelerating data collection through GPU-accelerated\nsimulation but also for enabling diverse data collection across multiple tasks\nby simulating heterogeneous scenes in parallel. However, existing MTRL research\nhas largely been limited to off-policy methods like SAC in the\nlow-parallelization regime. MTRL could capitalize on the higher asymptotic\nperformance of on-policy algorithms, whose batches require data from the\ncurrent policy, and as a result, take advantage of massive parallelization\noffered by GPU-accelerated simulation. To bridge this gap, we introduce a\nmassively parallelized $\\textbf{M}$ulti-$\\textbf{T}$ask $\\textbf{Bench}$mark\nfor robotics (MTBench), an open-sourced benchmark featuring a broad\ndistribution of 50 manipulation tasks and 20 locomotion tasks, implemented\nusing the GPU-accelerated simulator IsaacGym. MTBench also includes four base\nRL algorithms combined with seven state-of-the-art MTRL algorithms and\narchitectures, providing a unified framework for evaluating their performance.\nOur extensive experiments highlight the superior speed of evaluating MTRL\napproaches using MTBench, while also uncovering unique challenges that arise\nfrom combining massive parallelism with MTRL. Code is available at\n$\\href{https://github.com/Viraj-Joshi/MTBench}{\nhttps://github.com/Viraj-Joshi/MTBench}$", "comment": "RLC 2025", "pdf_url": "http://arxiv.org/pdf/2507.23172v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22954", "title": "Neural Autoregressive Modeling of Brain Aging", "authors": ["Ridvan Yesiloglu", "Wei Peng", "Md Tauhidul Islam", "Ehsan Adeli"], "categories": ["cs.LG", "eess.IV", "q-bio.NC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at Deep Generative Models Workshop @ MICCAI 2025", "url": "http://arxiv.org/abs/2507.22954v1", "summary": "Brain aging synthesis is a critical task with broad applications in clinical\nand computational neuroscience. The ability to predict the future structural\nevolution of a subject's brain from an earlier MRI scan provides valuable\ninsights into aging trajectories. Yet, the high-dimensionality of data, subtle\nchanges of structure across ages, and subject-specific patterns constitute\nchallenges in the synthesis of the aging brain. To overcome these challenges,\nwe propose NeuroAR, a novel brain aging simulation model based on generative\nautoregressive transformers. NeuroAR synthesizes the aging brain by\nautoregressively estimating the discrete token maps of a future scan from a\nconvenient space of concatenated token embeddings of a previous and future\nscan. To guide the generation, it concatenates into each scale the subject's\nprevious scan, and uses its acquisition age and the target age at each block\nvia cross-attention. We evaluate our approach on both the elderly population\nand adolescent subjects, demonstrating superior performance over\nstate-of-the-art generative models, including latent diffusion models (LDM) and\ngenerative adversarial networks, in terms of image fidelity. Furthermore, we\nemploy a pre-trained age predictor to further validate the consistency and\nrealism of the synthesized images with respect to expected aging patterns.\nNeuroAR significantly outperforms key models, including LDM, demonstrating its\nability to model subject-specific brain aging trajectories with high fidelity.", "comment": "Accepted at Deep Generative Models Workshop @ MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.22954v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23269", "title": "XABPs: Towards eXplainable Autonomous Business Processes", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "categories": ["cs.SE", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23269v1", "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23269v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23191", "title": "Tractable Responsibility Measures for Ontology-Mediated Query Answering", "authors": ["Meghyn Bienvenu", "Diego Figueira", "Pierre Lafourcade"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Long version of a paper to appear at KR 2025, which contains further proof details in the appendix", "url": "http://arxiv.org/abs/2507.23191v1", "summary": "Recent work on quantitative approaches to explaining query answers employs\nresponsibility measures to assign scores to facts in order to quantify their\nrespective contributions to obtaining a given answer. In this paper, we study\nthe complexity of computing such responsibility scores in the setting of\nontology-mediated query answering, focusing on a very recently introduced\nfamily of Shapley-value-based responsibility measures defined in terms of\nweighted sums of minimal supports (WSMS). By exploiting results from the\ndatabase setting, we can show that such measures enjoy polynomial data\ncomplexity for classes of ontology-mediated queries that are\nfirst-order-rewritable, whereas the problem becomes \"shP\"-hard when the\nontology language can encode reachability queries (via axioms like $\\exists R.\nA \\sqsubseteq A$). To better understand the tractability frontier, we next\nexplore the combined complexity of WSMS computation. We prove that\nintractability applies already to atomic queries if the ontology language\nsupports conjunction, as well as to unions of `well-behaved' conjunctive\nqueries, even in the absence of an ontology. By contrast, our study yields\npositive results for common DL-Lite dialects: by means of careful analysis, we\nidentify classes of structurally restricted conjunctive queries (which\nintuitively disallow undesirable interactions between query atoms) that admit\ntractable WSMS computation.", "comment": "Long version of a paper to appear at KR 2025, which contains further\n  proof details in the appendix", "pdf_url": "http://arxiv.org/pdf/2507.23191v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23203", "title": "Quadratic Programming-Based Posture Manipulation and Thrust-vectoring for Agile Dynamic Walking on Narrow Pathways", "authors": ["Chenghao Wang", "Eric Sihite", "Kaushik Venkatesh Krishnamurthy", "Shreyansh Pitroda", "Adarsh Salagame", "Alireza Ramezani", "Morteza Gharib"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23203v1", "summary": "There has been significant advancement in legged robot's agility where they\ncan show impressive acrobatic maneuvers, such as parkour. These maneuvers rely\nheavily on posture manipulation. To expand the stability and locomotion\nplasticity, we use the multi-modal ability in our legged-aerial platform, the\nHusky Beta, to perform thruster-assisted walking. This robot has thrusters on\neach of its sagittal knee joints which can be used to stabilize its frontal\ndynamic as it walks. In this work, we perform a simulation study of quadruped\nnarrow-path walking with Husky $\\beta$, where the robot will utilize its\nthrusters to stably walk on a narrow path. The controller is designed based on\na centroidal dynamics model with thruster and foot ground contact forces as\ninputs. These inputs are regulated using a QP solver to be used in a model\npredictive control framework. In addition to narrow-path walking, we also\nperform a lateral push-recovery simulation to study how the thrusters can be\nused to stabilize the frontal dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23203v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22956", "title": "LLM-Assisted Cheating Detection in Korean Language via Keystrokes", "authors": ["Dong Hyun Roh", "Rajesh Kumar", "An Ngo"], "categories": ["cs.LG", "cs.HC", "K.3.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This paper has 11 pages, 6 figures, 2 tables, and has been accepted for publication at IEEE-IJCB 2025", "url": "http://arxiv.org/abs/2507.22956v1", "summary": "This paper presents a keystroke-based framework for detecting LLM-assisted\ncheating in Korean, addressing key gaps in prior research regarding language\ncoverage, cognitive context, and the granularity of LLM involvement. Our\nproposed dataset includes 69 participants who completed writing tasks under\nthree conditions: Bona fide writing, paraphrasing ChatGPT responses, and\ntranscribing ChatGPT responses. Each task spans six cognitive processes defined\nin Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and\ncreate). We extract interpretable temporal and rhythmic features and evaluate\nmultiple classifiers under both Cognition-Aware and Cognition-Unaware settings.\nTemporal features perform well under Cognition-Aware evaluation scenarios,\nwhile rhythmic features generalize better under cross-cognition scenarios.\nMoreover, detecting bona fide and transcribed responses was easier than\nparaphrased ones for both the proposed models and human evaluators, with the\nmodels significantly outperforming the humans. Our findings affirm that\nkeystroke dynamics facilitate reliable detection of LLM-assisted writing across\nvarying cognitive demands and writing strategies, including paraphrasing and\ntranscribing LLM-generated responses.", "comment": "This paper has 11 pages, 6 figures, 2 tables, and has been accepted\n  for publication at IEEE-IJCB 2025", "pdf_url": "http://arxiv.org/pdf/2507.22956v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23336", "title": "DSBC : Data Science task Benchmarking with Context engineering", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Giulio Martini", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2507.23336v1", "summary": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents.", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2507.23336v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23197", "title": "Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification", "authors": ["Yuke Liao", "Blaise Genest", "Kuldeep Meel", "Shaan Aryaman"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23197v1", "summary": "To handle complex instances, we revisit a divide-and-conquer approach to\nbreak down the complexity: instead of few complex BaB calls, we rely on many\nsmall {\\em partial} MILP calls. The crucial step is to select very few but very\nimportant ReLUs to treat using (costly) binary variables. The previous attempts\nwere suboptimal in that respect. To select these important ReLU variables, we\npropose a novel {\\em solution-aware} ReLU scoring ({\\sf SAS}), as well as adapt\nthe BaB-SR and BaB-FSB branching functions as {\\em global} ReLU scoring ({\\sf\nGS}) functions. We compare them theoretically as well as experimentally, and\n{\\sf SAS} is more efficient at selecting a set of variables to open using\nbinary variables. Compared with previous attempts, SAS reduces the number of\nbinary variables by around 6 times, while maintaining the same level of\naccuracy. Implemented in {\\em Hybrid MILP}, calling first $\\alpha,\\beta$-CROWN\nwith a short time-out to solve easier instances, and then partial MILP,\nproduces a very accurate yet efficient verifier, reducing by up to $40\\%$ the\nnumber of undecided instances to low levels ($8-15\\%$), while keeping a\nreasonable runtime ($46s-417s$ on average per instance), even for fairly large\nCNNs with 2 million parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23197v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23270", "title": "Simulation-based planning of Motion Sequences for Automated Procedure Optimization in Multi-Robot Assembly Cells", "authors": ["Loris Schneider", "Marc Ungen", "Elias Huber", "Jan-Felix Klein"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23270v1", "summary": "Reconfigurable multi-robot cells offer a promising approach to meet\nfluctuating assembly demands. However, the recurrent planning of their\nconfigurations introduces new challenges, particularly in generating optimized,\ncoordinated multi-robot motion sequences that minimize the assembly duration.\nThis work presents a simulation-based method for generating such optimized\nsequences. The approach separates assembly steps into task-related core\noperations and connecting traverse operations. While core operations are\nconstrained and predetermined, traverse operations offer substantial\noptimization potential. Scheduling the core operations is formulated as an\noptimization problem, requiring feasible traverse operations to be integrated\nusing a decomposition-based motion planning strategy. Several solution\ntechniques are explored, including a sampling heuristic, tree-based search and\ngradient-free optimization. For motion planning, a decomposition method is\nproposed that identifies specific areas in the schedule, which can be solved\nindependently with modified centralized path planning algorithms. The proposed\nmethod generates efficient and collision-free multi-robot assembly procedures\nthat outperform a baseline relying on decentralized, robot-individual motion\nplanning. Its effectiveness is demonstrated through simulation experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23270v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22959", "title": "Scientific Machine Learning with Kolmogorov-Arnold Networks", "authors": ["Salah A. Faroughi", "Farinaz Mostajeran", "Amin Hamed Mashhadzadeh", "Shirko Faroughi"], "categories": ["cs.LG", "cs.CE", "math-ph", "math.MP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22959v1", "summary": "The field of scientific machine learning, which originally utilized\nmultilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold\nNetworks (KANs) for data encoding. This shift is driven by the limitations of\nMLPs, including poor interpretability, fixed activation functions, and\ndifficulty capturing localized or high-frequency features. KANs address these\nissues with enhanced interpretability and flexibility, enabling more efficient\nmodeling of complex nonlinear interactions and effectively overcoming the\nconstraints associated with conventional MLP architectures. This review\ncategorizes recent progress in KAN-based models across three distinct\nperspectives: (i) data-driven learning, (ii) physics-informed modeling, and\n(iii) deep operator learning. Each perspective is examined through the lens of\narchitectural design, training strategies, application efficacy, and\ncomparative evaluation against MLP-based counterparts. By benchmarking KANs\nagainst MLPs, we highlight consistent improvements in accuracy, convergence,\nand spectral representation, clarifying KANs' advantages in capturing complex\ndynamics while learning more effectively. Finally, this review identifies\ncritical challenges and open research questions in KAN development,\nparticularly regarding computational efficiency, theoretical guarantees,\nhyperparameter tuning, and algorithm complexity. We also outline future\nresearch directions aimed at improving the robustness, scalability, and\nphysical consistency of KAN-based frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22959v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23344", "title": "Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation", "authors": ["Tatsuya Mitomi", "Fumiyasu Makinoshima", "Fumiya Makihara", "Eigo Segawa"], "categories": ["cs.LG", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23344v1", "summary": "Bike-sharing systems are emerging in various cities as a new ecofriendly\ntransportation system. In these systems, spatiotemporally varying user demands\nlead to imbalanced inventory at bicycle stations, resulting in additional\nrelocation costs. Therefore, it is essential to manage user demand through\noptimal dynamic pricing for the system. However, optimal pricing design for\nsuch a system is challenging because the system involves users with diverse\nbackgrounds and their probabilistic choices. To address this problem, we\ndevelop a differentiable agent-based simulation to rapidly design dynamic\npricing in bike-sharing systems, achieving balanced bicycle inventory despite\nspatiotemporally heterogeneous trips and probabilistic user decisions. We first\nvalidate our approach against conventional methods through numerical\nexperiments involving 25 bicycle stations and five time slots, yielding 100\nparameters. Compared to the conventional methods, our approach obtains a more\naccurate solution with a 73% to 78% reduction in loss while achieving more than\na 100-fold increase in convergence speed. We further validate our approach on a\nlarge-scale urban bike-sharing system scenario involving 289 bicycle stations,\nresulting in a total of 1156 parameters. Through simulations using the obtained\npricing policies, we confirm that these policies can naturally induce balanced\ninventory without any manual relocation. Additionally, we find that the cost of\ndiscounts to induce the balanced inventory can be minimized by setting\nappropriate initial conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23344v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23276", "title": "How Far Are AI Scientists from Changing the World?", "authors": ["Qiujie Xie", "Yixuan Weng", "Minjun Zhu", "Fuchen Shen", "Shulin Huang", "Zhen Lin", "Jiahui Zhou", "Zilan Mao", "Zijie Yang", "Linyi Yang", "Jian Wu", "Yue Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23276v1", "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23276v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23273", "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting", "authors": ["Jaeseok Park", "Chanoh Park", "Minsu Kim", "Soohwan Kim"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23273v1", "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,\nconventional approaches based on camera sensor, even RGB-D, suffer from\nfundamental limitations such as high computational load, failure in\nenvironments with poor texture or illumination, and short operational ranges.\nLiDAR emerges as a robust alternative, but its integration with 3DGS introduces\nnew challenges, such as the need for exceptional global alignment for\nphotorealistic quality and prolonged optimization times caused by sparse data.\nTo address these challenges, we propose GSFusion, an online\nLiDAR-Inertial-Visual mapping system that ensures high-precision map\nconsistency through a surfel-to-surfel constraint in the global pose-graph\noptimization. To handle sparse data, our system employs a pixel-aware Gaussian\ninitialization strategy for efficient representation and a bounded sigmoid\nconstraint to prevent uncontrolled Gaussian growth. Experiments on public and\nour datasets demonstrate our system outperforms existing 3DGS SLAM systems in\nterms of rendering quality and map-building efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23273v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22962", "title": "Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations", "authors": ["Boyuan Zheng", "Victor W. Chu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Pre-print v0.8 2025-07-30", "url": "http://arxiv.org/abs/2507.22962v1", "summary": "Climate extremes present escalating risks to agriculture intensifying the\nneed for reliable multi-hazard early warning systems (EWS). The situation is\nevolving due to climate change and hence such systems should have the\nintelligent to continue to learn from recent climate behaviours. However,\ntraditional single-hazard forecasting methods fall short in capturing complex\ninteractions among concurrent climatic events. To address this deficiency, in\nthis paper, we combine sequential deep learning models and advanced Explainable\nArtificial Intelligence (XAI) techniques to introduce a multi-hazard\nforecasting framework for agriculture. In our experiments, we utilize\nmeteorological data from four prominent agricultural regions in the United\nStates (between 2010 and 2023) to validate the predictive accuracy of our\nframework on multiple severe event types, which are extreme cold, floods,\nfrost, hail, heatwaves, and heavy rainfall, with tailored models for each area.\nThe framework uniquely integrates attention mechanisms with TimeSHAP (a\nrecurrent XAI explainer for time series) to provide comprehensive temporal\nexplanations revealing not only which climatic features are influential but\nprecisely when their impacts occur. Our results demonstrate strong predictive\naccuracy, particularly with the BiLSTM architecture, and highlight the system's\ncapacity to inform nuanced, proactive risk management strategies. This research\nsignificantly advances the explainability and applicability of multi-hazard\nEWS, fostering interdisciplinary trust and effective decision-making process\nfor climate risk management in the agricultural industry.", "comment": "Pre-print v0.8 2025-07-30", "pdf_url": "http://arxiv.org/pdf/2507.22962v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23429", "title": "Chatting with your ERP: A Recipe", "authors": ["Jorge Ruiz Gómez", "Lidia Andrés Susinos", "Jorge Alamo Olivé", "Sonia Rey Osorno", "Manuel Luis Gonzalez Hernández"], "categories": ["cs.AI", "cs.DB", "cs.ET", "cs.HC", "cs.MA", "68T50, 68P20", "I.2.7; H.2.5; H.2.8; H.5.m"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      11 pages, includes 3 tables summarizing schema and model performance. Submitted on July 31, 2025. Targets integration of LLM agents with ERP systems using open-weight models and Ollama deployment", "url": "http://arxiv.org/abs/2507.23429v1", "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.", "comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment", "pdf_url": "http://arxiv.org/pdf/2507.23429v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23330", "title": "AI Must not be Fully Autonomous", "authors": ["Tosin Adewumi", "Lama Alkhaled", "Florent Imbert", "Hui Han", "Nudrat Habib", "Karl Löwenmark"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      11 pages, 1 figure", "url": "http://arxiv.org/abs/2507.23330v1", "summary": "Autonomous Artificial Intelligence (AI) has many benefits. It also has many\nrisks. In this work, we identify the 3 levels of autonomous AI. We are of the\nposition that AI must not be fully autonomous because of the many risks,\nespecially as artificial superintelligence (ASI) is speculated to be just\ndecades away. Fully autonomous AI, which can develop its own objectives, is at\nlevel 3 and without responsible human oversight. However, responsible human\noversight is crucial for mitigating the risks. To ague for our position, we\ndiscuss theories of autonomy, AI and agents. Then, we offer 12 distinct\narguments and 6 counterarguments with rebuttals to the counterarguments. We\nalso present 15 pieces of recent evidence of AI misaligned values and other\nrisks in the appendix.", "comment": "11 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.23330v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23305", "title": "Whisker-based Active Tactile Perception for Contour Reconstruction", "authors": ["Yixuan Dang", "Qinyang Xu", "Yu Zhang", "Xiangtong Yao", "Liding Zhang", "Zhenshan Bing", "Florian Roehrbein", "Alois Knoll"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23305v1", "summary": "Perception using whisker-inspired tactile sensors currently faces a major\nchallenge: the lack of active control in robots based on direct contact\ninformation from the whisker. To accurately reconstruct object contours, it is\ncrucial for the whisker sensor to continuously follow and maintain an\nappropriate relative touch pose on the surface. This is especially important\nfor localization based on tip contact, which has a low tolerance for sharp\nsurfaces and must avoid slipping into tangential contact. In this paper, we\nfirst construct a magnetically transduced whisker sensor featuring a compact\nand robust suspension system composed of three flexible spiral arms. We develop\na method that leverages a characterized whisker deflection profile to directly\nextract the tip contact position using gradient descent, with a Bayesian filter\napplied to reduce fluctuations. We then propose an active motion control policy\nto maintain the optimal relative pose of the whisker sensor against the object\nsurface. A B-Spline curve is employed to predict the local surface curvature\nand determine the sensor orientation. Results demonstrate that our algorithm\ncan effectively track objects and reconstruct contours with sub-millimeter\naccuracy. Finally, we validate the method in simulations and real-world\nexperiments where a robot arm drives the whisker sensor to follow the surfaces\nof three different objects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23305v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22963", "title": "FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization", "authors": ["Abdelrhman Gaber", "Hassan Abd-Eltawab", "John Elgallab", "Youssif Abuzied", "Dineo Mpanya", "Turgay Celik", "Swarun Kumar", "Tamer ElBatt"], "categories": ["cs.LG", "q-bio.OT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22963v1", "summary": "Cardiovascular diseases (CVD) cause over 17 million deaths annually\nworldwide, highlighting the urgent need for privacy-preserving predictive\nsystems. We introduce FedCVD++, an enhanced federated learning (FL) framework\nthat integrates both parametric models (logistic regression, SVM, neural\nnetworks) and non-parametric models (Random Forest, XGBoost) for coronary heart\ndisease risk prediction. To address key FL challenges, we propose: (1)\ntree-subset sampling that reduces Random Forest communication overhead by 70%,\n(2) XGBoost-based feature extraction enabling lightweight federated ensembles,\nand (3) federated SMOTE synchronization for resolving cross-institutional class\nimbalance.\n  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves\nstate-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its\ncentralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81)\nmatches non-federated performance. Additionally, our communication-efficient\nstrategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.\n  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher\nF1-scores and superior scalability for multi-institutional deployment. This\nwork represents the first practical integration of non-parametric models into\nfederated healthcare systems, providing a privacy-preserving solution validated\nunder real-world clinical constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22963v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23735", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Yvan R. Petillot"], "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23735v1", "summary": "Achieving robust cognitive autonomy in robots navigating complex,\nunpredictable environments remains a fundamental challenge in robotics. This\npaper presents Underwater Robot Self-Organizing Autonomy (UROSA), a\ngroundbreaking architecture leveraging distributed Large Language Model AI\nagents integrated within the Robot Operating System 2 (ROS 2) framework to\nenable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA\ndecentralises cognition into specialised AI agents responsible for multimodal\nperception, adaptive reasoning, dynamic mission planning, and real-time\ndecision-making. Central innovations include flexible agents dynamically\nadapting their roles, retrieval-augmented generation utilising vector databases\nfor efficient knowledge management, reinforcement learning-driven behavioural\noptimisation, and autonomous on-the-fly ROS 2 node generation for runtime\nfunctional extensibility. Extensive empirical validation demonstrates UROSA's\npromising adaptability and reliability through realistic underwater missions in\nsimulation and real-world deployments, showing significant advantages over\ntraditional rule-based architectures in handling unforeseen scenarios,\nenvironmental uncertainties, and novel mission objectives. This work not only\nadvances underwater autonomy but also establishes a scalable, safe, and\nversatile cognitive robotics framework capable of generalising to a diverse\narray of real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23735v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23377", "title": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform", "authors": ["Zhuo Li", "Xianghuai Deng", "Chiwei Feng", "Hanmeng Li", "Shenjie Wang", "Haichao Zhang", "Teng Jia", "Conlin Chen", "Louis Linchun Wu", "Jia Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23377v1", "summary": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23377v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23324", "title": "Assessing the Alignment of Automated Vehicle Decisions with Human Reasons", "authors": ["Lucas Elbert Suryana", "Saeed Rahmani", "Simeon Craig Calvert", "Arkady Zgonnikov", "Bart van Arem"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This version incorporates revisions based on peer-review feedback from a prior submission. The work has not yet been accepted and is being prepared for resubmission", "url": "http://arxiv.org/abs/2507.23324v1", "summary": "A key challenge in deploying automated vehicles (AVs) is ensuring they make\nappropriate decisions in ethically challenging everyday driving situations.\nWhile much attention has been paid to rare, high-stakes dilemmas such as\ntrolley problems, similar tensions also arise in routine scenarios, such as\nnavigating empty intersections, where multiple human considerations, including\nlegality and comfort, often conflict. Current AV planning systems typically\nrely on rigid rules, which struggle to balance these competing considerations\nand can lead to behaviour that misaligns with human expectations. This paper\nproposes a novel reasons-based trajectory evaluation framework that\noperationalises the tracking condition of Meaningful Human Control (MHC). The\nframework models the reasons of human agents, such as regulatory compliance, as\nquantifiable functions and evaluates how well candidate AV trajectories align\nwith these reasons. By assigning adjustable weights to agent priorities and\nintegrating a balance function to discourage the exclusion of any agent, the\nframework supports interpretable decision evaluation. Through a\nreal-world-inspired overtaking scenario, we show how this approach reveals\ntensions, for instance between regulatory compliance, efficiency, and comfort.\nThe framework functions as a modular evaluation layer over existing planning\nalgorithms. It offers a transparent tool for assessing ethical alignment in\neveryday scenarios and provides a practical step toward implementing MHC in\nreal-world AV deployment.", "comment": "This version incorporates revisions based on peer-review feedback\n  from a prior submission. The work has not yet been accepted and is being\n  prepared for resubmission", "pdf_url": "http://arxiv.org/pdf/2507.23324v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23000", "title": "Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation", "authors": ["Shengao Yi", "Xiaojiang Li", "Wei Tu", "Tianhong Zhao"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23000v1", "summary": "As extreme heat events intensify due to climate change and urbanization,\ncities face increasing challenges in mitigating outdoor heat stress. While\ntraditional physical models such as SOLWEIG and ENVI-met provide detailed\nassessments of human-perceived heat exposure, their computational demands limit\nscalability for city-wide planning. In this study, we propose GSM-UTCI, a\nmultimodal deep learning framework designed to predict daytime average\nUniversal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The\nmodel fuses surface morphology (nDSM), high-resolution land cover data, and\nhourly meteorological conditions using a feature-wise linear modulation (FiLM)\narchitecture that dynamically conditions spatial features on atmospheric\ncontext. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical\naccuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\\deg}C,\nwhile reducing inference time from hours to under five minutes for an entire\ncity. To demonstrate its planning relevance, we apply GSM-UTCI to simulate\nsystematic landscape transformation scenarios in Philadelphia, replacing bare\nearth, grass, and impervious surfaces with tree canopy. Results show spatially\nheterogeneous but consistently strong cooling effects, with impervious-to-tree\nconversion producing the highest aggregated benefit (-4.18{\\deg}C average\nchange in UTCI across 270.7 km2). Tract-level bivariate analysis further\nreveals strong alignment between thermal reduction potential and land cover\nproportions. These findings underscore the utility of GSM-UTCI as a scalable,\nfine-grained decision support tool for urban climate adaptation, enabling\nscenario-based evaluation of greening strategies across diverse urban\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23000v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22958", "title": "CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam", "authors": ["Ruslan Khrulev"], "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 97D50", "I.2.7; I.4; K.3.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures, 10 tables. Code is available at: this https URL", "url": "http://arxiv.org/abs/2507.22958v1", "summary": "This paper introduces a novel benchmark, EGE-Math Solutions Assessment\nBenchmark, for evaluating Vision-Language Models (VLMs) on their ability to\nassess hand-written mathematical solutions. Unlike existing benchmarks that\nfocus on problem solving, our approach centres on understanding student\nsolutions, identifying mistakes, and assigning grades according to fixed\ncriteria. We compile 122 scanned solutions from the Russian Unified State Exam\n(EGE) together with official expert grades, and evaluate seven modern VLMs from\nGoogle, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The\nresults reveal current limitations in mathematical reasoning and human-rubric\nalignment, opening new research avenues in AI-assisted assessment. You can find\ncode in https://github.com/Karifannaa/Auto-check-EGE-math", "comment": "15 pages, 3 figures, 10 tables. Code is available at:\n  https://github.com/Karifannaa/Auto-check-EGE-math", "pdf_url": "http://arxiv.org/pdf/2507.22958v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.22889", "title": "Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls", "authors": ["Tom Sheffer", "Alon Miron", "Yaniv Dover", "Ariel Goldstein"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22889v1", "summary": "Conversations transform individual knowledge into collective insight,\nallowing groups of humans and increasingly groups of artificial intelligence\n(AI) agents to collaboratively solve complex problems. Whether interactions\nbetween AI agents can replicate the synergy observed in human discussions\nremains an open question. To investigate this, we systematically compared four\nconversational configurations: pairs of large language models (LLM-LLM), trios\nof LLMs, trios of humans, and mixed human-LLM pairs. After agents answered\nquestions individually, they engaged in open-ended discussions and then\nreconsidered their initial answers. Interactions involving humans consistently\nled to accuracy improvements after the conversations, benefiting both stronger\nand weaker participants. By contrast, purely LLM-based pairs and trios\nexhibited declines in accuracy, demonstrating limited conversational synergy.\nAnalysis of participants' confidence and answer-switching behavior revealed\nthat knowledge diversity is a critical factor enabling collaborative\nimprovement. Crucially, the lack of gains in LLM-LLM interactions did not stem\nfrom a fundamental limitation of the models' ability to collaborate, but from\nhighly similar knowledge states that left little room for productive exchange.\nOur findings argue for a paradigm shift in AI development: rather than\noptimizing individual models solely for standalone performance, explicitly\ncultivating diversity across agents, even at the cost of slightly lower\nindividual accuracy, may yield AI collaborators that are more effective in\ngroup settings with humans or other AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22889v1", "cate": "cs.HC", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2504.06684", "title": "SDHN: Skewness-Driven Hypergraph Networks for Enhanced Localized Multi-Robot Coordination", "authors": ["Delin Zhao", "Yanbo Shan", "Chang Liu", "Shenghang Lin", "Yingxin Shou", "Bin Xu"], "categories": ["cs.RO", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06684v2", "summary": "Multi-Agent Reinforcement Learning is widely used for multi-robot\ncoordination, where simple graphs typically model pairwise interactions.\nHowever, such representations fail to capture higher-order collaborations,\nlimiting effectiveness in complex tasks. While hypergraph-based approaches\nenhance cooperation, existing methods often generate arbitrary hypergraph\nstructures and lack adaptability to environmental uncertainties. To address\nthese challenges, we propose the Skewness-Driven Hypergraph Network (SDHN),\nwhich employs stochastic Bernoulli hyperedges to explicitly model higher-order\nmulti-robot interactions. By introducing a skewness loss, SDHN promotes an\nefficient structure with Small-Hyperedge Dominant Hypergraph, allowing robots\nto prioritize localized synchronization while still adhering to the overall\ninformation, similar to human coordination. Extensive experiments on Moving\nAgents in Formation and Robotic Warehouse tasks validate SDHN's effectiveness,\ndemonstrating superior performance over state-of-the-art baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06684v2", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-07-31"}
{"id": "2507.23440", "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation", "authors": ["Mingzhe Li", "Xin Lu", "Yanyan Zhao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by Findings of ACL 2025", "url": "http://arxiv.org/abs/2507.23440v1", "summary": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate", "comment": "Accepted by Findings of ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.23440v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23339", "title": "Learning to Drift with Individual Wheel Drive: Maneuvering Autonomous Vehicle at the Handling Limits", "authors": ["Yihan Zhou", "Yiwen Lu", "Bo Yang", "Jiayun Li", "Yilin Mo"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23339v1", "summary": "Drifting, characterized by controlled vehicle motion at high sideslip angles,\nis crucial for safely handling emergency scenarios at the friction limits.\nWhile recent reinforcement learning approaches show promise for drifting\ncontrol, they struggle with the significant simulation-to-reality gap, as\npolicies that perform well in simulation often fail when transferred to\nphysical systems. In this paper, we present a reinforcement learning framework\nwith GPU-accelerated parallel simulation and systematic domain randomization\nthat effectively bridges the gap. The proposed approach is validated on both\nsimulation and a custom-designed and open-sourced 1/10 scale Individual Wheel\nDrive (IWD) RC car platform featuring independent wheel speed control.\nExperiments across various scenarios from steady-state circular drifting to\ndirection transitions and variable-curvature path following demonstrate that\nour approach achieves precise trajectory tracking while maintaining controlled\nsideslip angles throughout complex maneuvers in both simulated and real-world\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23339v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23009", "title": "Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead", "authors": ["Tom Sühr", "Florian E. Dorner", "Olawale Salaudeen", "Augustin Kelava", "Samira Samadi"], "categories": ["cs.LG", "cs.AI", "91E45", "I.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23009v1", "summary": "Large Language Models (LLMs) have achieved remarkable results on a range of\nstandardized tests originally designed to assess human cognitive and\npsychological traits, such as intelligence and personality. While these results\nare often interpreted as strong evidence of human-like characteristics in LLMs,\nthis paper argues that such interpretations constitute an ontological error.\nHuman psychological and educational tests are theory-driven measurement\ninstruments, calibrated to a specific human population. Applying these tests to\nnon-human subjects without empirical validation, risks mischaracterizing what\nis being measured. Furthermore, a growing trend frames AI performance on\nbenchmarks as measurements of traits such as ``intelligence'', despite known\nissues with validity, data contamination, cultural bias and sensitivity to\nsuperficial prompt changes. We argue that interpreting benchmark performance as\nmeasurements of human-like traits, lacks sufficient theoretical and empirical\njustification. This leads to our position: Stop Evaluating AI with Human Tests,\nDevelop Principled, AI-specific Tests instead. We call for the development of\nprincipled, AI-specific evaluation frameworks tailored to AI systems. Such\nframeworks might build on existing frameworks for constructing and validating\npsychometrics tests, or could be created entirely from scratch to fit the\nunique context of AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23009v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23006", "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction", "authors": ["Zhensheng Yuan", "Haozhi Huang", "Zhen Xiong", "Di Wang", "Guanghua Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23006v1", "summary": "We present a framework that enables fast reconstruction and real-time\nrendering of urban-scale scenes while maintaining robustness against appearance\nvariations across multi-view captures. Our approach begins with scene\npartitioning for parallel training, employing a visibility-based image\nselection strategy to optimize training efficiency. A controllable\nlevel-of-detail (LOD) strategy explicitly regulates Gaussian density under a\nuser-defined budget, enabling efficient training and rendering while\nmaintaining high visual fidelity. The appearance transformation module\nmitigates the negative effects of appearance inconsistencies across images\nwhile enabling flexible adjustments. Additionally, we utilize enhancement\nmodules, such as depth regularization, scale regularization, and antialiasing,\nto improve reconstruction fidelity. Experimental results demonstrate that our\nmethod effectively reconstructs urban-scale scenes and outperforms previous\napproaches in both efficiency and quality. The source code is available at:\nhttps://yzslab.github.io/REUrbanGS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23006v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22890", "title": "Evaluating LLMs for Visualization Generation and Understanding", "authors": ["Saadiq Rauf Khan", "Vinit Chandak", "Sougata Mukherjea"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22890v1", "summary": "Information Visualization has been utilized to gain insights from complex\ndata. In recent times, Large Language models (LLMs) have performed very well in\nmany tasks. In this paper, we showcase the capabilities of different popular\nLLMs to generate code for visualization based on simple prompts. We also\nanalyze the power of LLMs to understand some common visualizations by answering\nquestions. Our study shows that LLMs could generate code for some simpler\nvisualizations such as bar and pie charts. Moreover, they could answer simple\nquestions about visualizations. However, LLMs also have several limitations.\nFor example, some of them had difficulty generating complex visualizations,\nsuch as violin plot. LLMs also made errors in answering some questions about\nvisualizations, for example, identifying relationships between close boundaries\nand determining lengths of shapes. We believe that our insights can be used to\nimprove both LLMs and Information Visualization systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22890v1", "cate": "cs.HC", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.23419", "title": "WiRM: Wireless Respiration Monitoring Using Conjugate Multiple Channel State Information and Fast Iterative Filtering in Wi-Fi Systems", "authors": ["James Rhodes", "Lawrence Ong", "Duy T. Ngo"], "categories": ["cs.ET", "eess.SP"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23419v1", "summary": "Monitoring respiratory health with the use of channel state information (CSI)\nhas shown promising results. Many existing methods focus on monitoring only the\nrespiratory rate, while others focus on monitoring the motion of the chest as a\npatient breathes, which is referred to as the respiratory waveform. This paper\npresents WiRM, a two-staged approach to contactless respiration monitoring. In\nthe first stage, WiRM improves upon existing respiratory rate estimation\ntechniques by using conjugate multiplication for phase sanitisation and\nadaptive multi-trace carving (AMTC) for tracing how the respiratory rate\nchanges over time. When compared against three state-of-the-art methods, WiRM\nhas achieved an average reduction of $38\\%$ in respiratory rate root mean\nsquared error (RMSE). In the second stage, WiRM uses this improved respiratory\nrate estimate to inform the decomposition and selection of the respiratory\nwaveform from the CSI data. Remarkably, WiRM delivers a $178.3\\%$ improvement\nin average absolute correlation with the ground truth respiratory waveform.\nWithin the literature, it is difficult to compare the robustness of existing\nalgorithms in noisy environments. In this paper, we develop a purpose-built\nsimulation toolkit to evaluate the robustness of respiration monitoring\nsolutions under various noise conditions, including thermal, multiplicative,\nand phase noise. Our results show that WiRM demonstrates improved or comparable\nresilience to these common noise sources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23419v1", "cate": "cs.ET", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.19219", "title": "Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding", "authors": ["Shiyue Wang", "Haozheng Xu", "Yuhan Zhang", "Jingran Lin", "Changhong Lu", "Xiangfeng Wang", "Wenhao Li"], "categories": ["cs.AI", "cs.LG", "cs.MA", "math.CO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      112 pages, 21 figures, 20 tables. The project website is: this https URL", "url": "http://arxiv.org/abs/2505.19219v2", "summary": "Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial\nintelligence and robotics, requiring the computation of collision-free paths\nfor multiple agents navigating from their start locations to designated goals.\nAs autonomous systems become increasingly prevalent in warehouses, urban\ntransportation, and other complex environments, MAPF has evolved from a\ntheoretical challenge to a critical enabler of real-world multi-robot\ncoordination. This comprehensive survey bridges the long-standing divide\nbetween classical algorithmic approaches and emerging learning-based methods in\nMAPF research. We present a unified framework that encompasses search-based\nmethods (including Conflict-Based Search, Priority-Based Search, and Large\nNeighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP\nformulations), and data-driven techniques (reinforcement learning, supervised\nlearning, and hybrid strategies). Through systematic analysis of experimental\npractices across 200+ papers, we uncover significant disparities in evaluation\nmethodologies, with classical methods typically tested on larger-scale\ninstances (up to 200 by 200 grids with 1000+ agents) compared to learning-based\napproaches (predominantly 10-100 agents). We provide a comprehensive taxonomy\nof evaluation metrics, environment types, and baseline selections, highlighting\nthe need for standardized benchmarking protocols. Finally, we outline promising\nfuture directions including mixed-motive MAPF with game-theoretic\nconsiderations, language-grounded planning with large language models, and\nneural solver architectures that combine the rigor of classical methods with\nthe flexibility of deep learning. This survey serves as both a comprehensive\nreference for researchers and a practical guide for deploying MAPF solutions in\nincreasingly complex real-world applications.", "comment": "112 pages, 21 figures, 20 tables. The project website is:\n  https://wangsh1yue.github.io/Where-Paths-Collide", "pdf_url": "http://arxiv.org/pdf/2505.19219v2", "cate": "cs.AI", "date": "2025-05-25", "updated": "2025-07-31"}
{"id": "2507.23488", "title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery", "authors": ["Kacper Kadziolka", "Saber Salehkaleybar"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23488v1", "summary": "Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23488v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23350", "title": "Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications", "authors": ["Mahmoud Ghorab", "Matthias Lorenzen"], "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.23350v1", "summary": "There is a growing demand for autonomous mobile robots capable of navigating\nunstructured agricultural environments. Tasks such as weed control in meadows\nrequire efficient path planning through an unordered set of coordinates while\nminimizing travel distance and adhering to curvature constraints to prevent\nsoil damage and protect vegetation. This paper presents an integrated\nnavigation framework combining a global path planner based on the Dubins\nTraveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control\n(NMPC) strategy for local path planning and control. The DTSP generates a\nminimum-length, curvature-constrained path that efficiently visits all targets,\nwhile the NMPC leverages this path to compute control signals to accurately\nreach each waypoint. The system's performance was validated through comparative\nsimulation analysis on real-world field datasets, demonstrating that the\ncoupled DTSP-based planner produced smoother and shorter paths, with a\nreduction of about 16% in the provided scenario, compared to decoupled methods.\nBased thereon, the NMPC controller effectively steered the robot to the desired\nwaypoints, while locally optimizing the trajectory and ensuring adherence to\nconstraints. These findings demonstrate the potential of the proposed framework\nfor efficient autonomous navigation in agricultural environments.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.23350v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23010", "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods", "authors": ["Siwoo Park"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23010v1", "summary": "This paper investigates the inverse capabilities and broader utility of\nmultimodal latent spaces within task-specific AI (Artificial Intelligence)\nmodels. While these models excel at their designed forward tasks (e.g.,\ntext-to-image generation, audio-to-text transcription), their potential for\ninverse mappings remains largely unexplored. We propose an optimization-based\nframework to infer input characteristics from desired outputs, applying it\nbidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio\n(Whisper-Large-V3, Chatterbox-TTS) modalities.\n  Our central hypothesis posits that while optimization can guide models\ntowards inverse tasks, their multimodal latent spaces will not consistently\nsupport semantically meaningful and perceptually coherent inverse mappings.\nExperimental results consistently validate this hypothesis. We demonstrate that\nwhile optimization can force models to produce outputs that align textually\nwith targets (e.g., a text-to-image model generating an image that an image\ncaptioning model describes correctly, or an ASR model transcribing optimized\naudio accurately), the perceptual quality of these inversions is chaotic and\nincoherent. Furthermore, when attempting to infer the original semantic input\nfrom generative models, the reconstructed latent space embeddings frequently\nlack semantic interpretability, aligning with nonsensical vocabulary tokens.\n  These findings highlight a critical limitation. multimodal latent spaces,\nprimarily optimized for specific forward tasks, do not inherently possess the\nstructure required for robust and interpretable inverse mappings. Our work\nunderscores the need for further research into developing truly semantically\nrich and invertible multimodal latent spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23010v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23021", "title": "Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction", "authors": ["Giuseppe Cartella", "Vittorio Cuculo", "Alessandro D'Amelio", "Marcella Cornia", "Giuseppe Boccignone", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2507.23021v1", "summary": "Predicting human gaze scanpaths is crucial for understanding visual\nattention, with applications in human-computer interaction, autonomous systems,\nand cognitive robotics. While deep learning models have advanced scanpath\nprediction, most existing approaches generate averaged behaviors, failing to\ncapture the variability of human visual exploration. In this work, we present\nScanDiff, a novel architecture that combines diffusion models with Vision\nTransformers to generate diverse and realistic scanpaths. Our method explicitly\nmodels scanpath variability by leveraging the stochastic nature of diffusion\nmodels, producing a wide range of plausible gaze trajectories. Additionally, we\nintroduce textual conditioning to enable task-driven scanpath generation,\nallowing the model to adapt to different visual search objectives. Experiments\non benchmark datasets show that ScanDiff surpasses state-of-the-art methods in\nboth free-viewing and task-driven scenarios, producing more diverse and\naccurate scanpaths. These results highlight its ability to better capture the\ncomplexity of human visual behavior, pushing forward gaze prediction research.\nSource code and models are publicly available at\nhttps://aimagelab.github.io/ScanDiff.", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2507.23021v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22891", "title": "Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter", "authors": ["Jérôme Ferrari", "Benoit Delinchant", "Frédéric Wurtz", "Olga Rouchouze"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Cired 2025, Jun 2025, Gen{è}ve (CH), Switzerland", "url": "http://arxiv.org/abs/2507.22891v1", "summary": "As part of the energy transition and the rise in energy prices, the number of\ncollective self-consumption operations in France is steadily increasing.\nHowever, energy flow monitoring currently relies on historical ''day+1'' data\nprovided by Linky meters, which does not offer real time feedback to help\nparticipants adapt their energy consumption behaviors. This article introduces\na new open-source infrastructure for real-time monitoring based on Linky meter\ndata, enabling participants to make informed decisions and take timely actions.\nIt includes a description of the xKy device, applied to a collective\nself-consumption operation involving nine participants, supported by the Energy\nTransition Observatory (OTE). The project encompasses the implementation of\ngateways in participants' homes and the development and operation of real-time\nmonitoring website, aimed at increasing participants' self-consumption rate.", "comment": "Cired 2025, Jun 2025, Gen{\\`e}ve (CH), Switzerland", "pdf_url": "http://arxiv.org/pdf/2507.22891v1", "cate": "cs.HC", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.23618", "title": "SOME: Symmetric One-Hot Matching Elector -- A Lightweight Microsecond Decoder for Quantum Error Correction", "authors": ["Xinyi Guo", "Geguang Miao", "Shinichi Nishizawa", "Hiromitsu Awano", "Shinji Kimura", "Takashi Sato"], "categories": ["cs.ET", "quant-ph"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23618v1", "summary": "Conventional quantum error correction (QEC) decoders such as Minimum-Weight\nPerfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast\ndecoding, respectively, but both suffer from high topological complexity. In\ncontrast, Ising model-based decoders reduce topological complexity but demand\nconsiderable decoding time. We propose the Symmetric One-Hot Matching Elector\n(SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic\nUnconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO\n(OHQ). Each variable in the QUBO represents whether a given pair of flipped\nsyndromes is matched, while the error probabilities between the pair are\nencoded as interaction coefficients (weight). Constraints ensure that each\nflipped syndrome is matched exactly once. Valid solutions of OHQ correspond to\nself-inverse permutation matrices, characterized by symmetric one-hot encoding.\nTo solve the OHQ efficiently, SOME reformulates the decoding task as the\nconstruction of permutation matrices that minimize the total weight. It\ninitializes each candidate matrix from one of the minimum-weight syndrome\npairs, then iteratively appends additional pairs in ascending order of weight,\nand finally selects the permutation matrix with the lowest total energy. SOME\nachieves up to a 99.9x reduction in variable count and reduces decoding times\nfrom milliseconds to microseconds on a single-threaded commodity CPU. OHQ also\nmaintains performance up to a 10.5% physical error rate, surpassing the highest\nknown threshold of MWPM@.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23618v1", "cate": "cs.ET", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21035", "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis", "authors": ["Haoyang Liu", "Yijiang Li", "Haohan Wang"], "categories": ["cs.AI", "cs.LG", "cs.MA", "q-bio.GN"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      51 pages (13 pages for the main text, 9 pages for references, and 29 pages for the appendix)", "url": "http://arxiv.org/abs/2507.21035v2", "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.", "comment": "51 pages (13 pages for the main text, 9 pages for references, and 29\n  pages for the appendix)", "pdf_url": "http://arxiv.org/pdf/2507.21035v2", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2507.23497", "title": "Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification", "authors": ["David A Kelly", "Hana Chockler"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      13 pages, 13 figures, appendix included", "url": "http://arxiv.org/abs/2507.23497v1", "summary": "Existing algorithms for explaining the outputs of image classifiers are based\non a variety of approaches and produce explanations that lack formal rigor. On\nthe other hand, logic-based explanations are formally and rigorously defined\nbut their computability relies on strict assumptions about the model that do\nnot hold on image classifiers.\n  In this paper, we show that causal explanations, in addition to being\nformally and rigorously defined, enjoy the same formal properties as\nlogic-based ones, while still lending themselves to black-box algorithms and\nbeing a natural fit for image classifiers. We prove formal properties of causal\nexplanations and introduce contrastive causal explanations for image\nclassifiers. Moreover, we augment the definition of explanation with confidence\nawareness and introduce complete causal explanations: explanations that are\nclassified with exactly the same confidence as the original image.\n  We implement our definitions, and our experimental results demonstrate that\ndifferent models have different patterns of sufficiency, contrastiveness, and\ncompleteness. Our algorithms are efficiently computable, taking on average 6s\nper image on a ResNet50 model to compute all types of explanations, and are\ntotally black-box, needing no knowledge of the model, no access to model\ninternals, no access to gradient, nor requiring any properties, such as\nmonotonicity, of the model.", "comment": "13 pages, 13 figures, appendix included", "pdf_url": "http://arxiv.org/pdf/2507.23497v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23445", "title": "Quantifying and Visualizing Sim-to-Real Gaps: Physics-Guided Regularization for Reproducibility", "authors": ["Yuta Kawachi"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23445v1", "summary": "Simulation-to-real transfer using domain randomization for robot control\noften relies on low-gear-ratio, backdrivable actuators, but these approaches\nbreak down when the sim-to-real gap widens. Inspired by the traditional PID\ncontroller, we reinterpret its gains as surrogates for complex, unmodeled plant\ndynamics. We then introduce a physics-guided gain regularization scheme that\nmeasures a robot's effective proportional gains via simple real-world\nexperiments. Then, we penalize any deviation of a neural controller's local\ninput-output sensitivities from these values during training. To avoid the\noverly conservative bias of naive domain randomization, we also condition the\ncontroller on the current plant parameters. On an off-the-shelf two-wheeled\nbalancing robot with a 110:1 gearbox, our gain-regularized,\nparameter-conditioned RNN achieves angular settling times in hardware that\nclosely match simulation. At the same time, a purely domain-randomized policy\nexhibits persistent oscillations and a substantial sim-to-real gap. These\nresults demonstrate a lightweight, reproducible framework for closing\nsim-to-real gaps on affordable robotic hardware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23445v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23035", "title": "KLLM: Fast LLM Inference with K-Means Quantization", "authors": ["Xueying Wu", "Baijun Zhou", "Zhihui Gao", "Yuzhe Fu", "Qilin Zheng", "Yintao He", "Hai Li"], "categories": ["cs.LG", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23035v1", "summary": "Large language model (LLM) inference poses significant challenges due to its\nintensive memory and computation demands. Weight and activation quantization\n(WAQ) offers a promising solution by reducing both memory footprint and\narithmetic complexity. However, two key challenges remain in the existing WAQ\ndesigns. (1) Traditional WAQ designs rely on uniform integer-based quantization\nfor hardware efficiency, but this often results in significant accuracy\ndegradation at low precision. K-Means-based quantization, a non-uniform\nquantization technique, achieves higher accuracy by matching the Gaussian-like\ndistributions of weights and activations in LLMs. However, its non-uniform\nnature prevents direct execution on low-precision compute units, requiring\ndequantization and floating-point matrix multiplications (MatMuls) during\ninference. (2) Activation outliers further hinder effective low-precision WAQ.\nOffline thresholding methods for outlier detection can lead to significant\nmodel performance degradation, while existing online detection techniques\nintroduce substantial runtime overhead.\n  To address the aforementioned challenges and fully unleash the potential of\nWAQ with K-Means quantization for LLM inference, in this paper, we propose\nKLLM, a hardware-software co-design framework. KLLM features an index-based\ncomputation scheme for efficient execution of MatMuls and nonlinear operations\non K-Means-quantized data, which avoids most of the dequantization and\nfull-precision computations. Moreover, KLLM incorporates a novel outlier\ndetection engine, Orizuru, that efficiently identifies the top-$k$ largest and\nsmallest elements in the activation data stream during online inference.\n  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,\n7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the\nA100 GPU and Atom, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23035v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23027", "title": "Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging", "authors": ["Krishan Agyakari Raja Babu", "Om Prabhu", "Annu", "Mohanasankar Sivaprakasam"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the MICCAI Workshop on \"Medical Image Computing in Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025", "url": "http://arxiv.org/abs/2507.23027v1", "summary": "Automated cardiac interpretation in resource-constrained settings (RCS) is\noften hindered by poor-quality echocardiographic imaging, limiting the\neffectiveness of downstream diagnostic models. While super-resolution (SR)\ntechniques have shown promise in enhancing magnetic resonance imaging (MRI) and\ncomputed tomography (CT) scans, their application to echocardiography-a widely\naccessible but noise-prone modality-remains underexplored. In this work, we\ninvestigate the potential of deep learning-based SR to improve classification\naccuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS\ndataset, we stratify samples by image quality and evaluate two clinically\nrelevant tasks of varying complexity: a relatively simple Two-Chamber vs.\nFour-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole\nvs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR\nmodels-Super-Resolution Generative Adversarial Network (SRGAN) and\nSuper-Resolution Residual Network (SRResNet), to enhance poor-quality images\nand observe significant gains in performance metric-particularly with SRResNet,\nwhich also offers computational efficiency. Our findings demonstrate that SR\ncan effectively recover diagnostic value in degraded echo scans, making it a\nviable tool for AI-assisted care in RCS, achieving more with less.", "comment": "Accepted at the MICCAI Workshop on \"Medical Image Computing in\n  Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025", "pdf_url": "http://arxiv.org/pdf/2507.23027v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22892", "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation", "authors": ["Ismail Hossain", "Mridul Banik"], "categories": ["cs.HC", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22892v1", "summary": "Conventional augmentative and alternative communication (AAC) systems and\nlanguage-learning platforms often fail to adapt in real time to the user's\ncognitive and linguistic needs, especially in neurological conditions such as\npost-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in\nnoninvasive electroencephalography (EEG)--based brain-computer interfaces\n(BCIs) and transformer--based large language models (LLMs) offer complementary\nstrengths: BCIs capture users' neural intent with low fatigue, while LLMs\ngenerate contextually tailored language content. We propose and evaluate a\nnovel hybrid framework that leverages real-time EEG signals to drive an\nLLM-powered language rehabilitation assistant. This system aims to: (1) enable\nusers with severe speech or motor impairments to navigate language-learning\nmodules via mental commands; (2) dynamically personalize vocabulary,\nsentence-construction exercises, and corrective feedback; and (3) monitor\nneural markers of cognitive effort to adjust task difficulty on the fly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22892v1", "cate": "cs.HC", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.22972", "title": "Complexity-energy trade-off in programmable unitary interferometers", "authors": ["Nikita A. Nemkov", "Stanislav S. Straupe"], "categories": ["physics.optics", "cs.ET"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.22972v1", "summary": "Coherent multiport interferometers are a promising approach to realize matrix\nmultiplication in integrated photonics. However, most known architectures -\nsuch as MZI and beamsplitter meshes, as well as more general interferometers -\nsuffer from complicated procedures for mapping the matrix elements of the\ndesired transformation to specific phaseshifts in the device. We point out that\nthe high programming complexity is intrinsic, rather than accidental. At the\nsame time, we argue that interferometers admitting efficient programming\nalgorithms in general yield a much lower useful output energy, which ultimately\nlimits their accuracy and energy efficiency.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.22972v1", "cate": "physics.optics", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23554", "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23554v1", "summary": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23554v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23523", "title": "H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation", "authors": ["Hongzhe Bi", "Lingxuan Wu", "Tianwei Lin", "Hengkai Tan", "Zhizhong Su", "Hang Su", "Jun Zhu"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23523v1", "summary": "Imitation learning for robotic manipulation faces a fundamental challenge:\nthe scarcity of large-scale, high-quality robot demonstration data. Recent\nrobotic foundation models often pre-train on cross-embodiment robot datasets to\nincrease data scale, while they face significant limitations as the diverse\nmorphologies and action spaces across different robot embodiments make unified\ntraining challenging. In this paper, we present H-RDT (Human to Robotics\nDiffusion Transformer), a novel approach that leverages human manipulation data\nto enhance robot manipulation capabilities. Our key insight is that large-scale\negocentric human manipulation videos with paired 3D hand pose annotations\nprovide rich behavioral priors that capture natural manipulation strategies and\ncan benefit robotic policy learning. We introduce a two-stage training\nparadigm: (1) pre-training on large-scale egocentric human manipulation data,\nand (2) cross-embodiment fine-tuning on robot-specific data with modular action\nencoders and decoders. Built on a diffusion transformer architecture with 2B\nparameters, H-RDT uses flow matching to model complex action distributions.\nExtensive evaluations encompassing both simulation and real-world experiments,\nsingle-task and multitask scenarios, as well as few-shot learning and\nrobustness assessments, demonstrate that H-RDT outperforms training from\nscratch and existing state-of-the-art methods, including Pi0 and RDT, achieving\nsignificant improvements of 13.9% and 40.5% over training from scratch in\nsimulation and real-world experiments, respectively. The results validate our\ncore hypothesis that human manipulation data can serve as a powerful foundation\nfor learning bimanual robotic manipulation policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23523v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23037", "title": "Linking Actor Behavior to Process Performance Over Time", "authors": ["Aurélie Leribaux", "Rafael Oyamada", "Johannes De Smedt", "Zahra Dasht Bozorgi", "Artem Polyvyanyy", "Jochen De Weerdt"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at the 5th Workshop on Change, Drift, and Dynamics of Organizational Processes (ProDy), BPM 2025", "url": "http://arxiv.org/abs/2507.23037v1", "summary": "Understanding how actor behavior influences process outcomes is a critical\naspect of process mining. Traditional approaches often use aggregate and static\nprocess data, overlooking the temporal and causal dynamics that arise from\nindividual actor behavior. This limits the ability to accurately capture the\ncomplexity of real-world processes, where individual actor behavior and\ninteractions between actors significantly shape performance. In this work, we\naddress this gap by integrating actor behavior analysis with Granger causality\nto identify correlating links in time series data. We apply this approach to\nrealworld event logs, constructing time series for actor interactions, i.e.\ncontinuation, interruption, and handovers, and process outcomes. Using Group\nLasso for lag selection, we identify a small but consistently influential set\nof lags that capture the majority of causal influence, revealing that actor\nbehavior has direct and measurable impacts on process performance, particularly\nthroughput time. These findings demonstrate the potential of actor-centric,\ntime series-based methods for uncovering the temporal dependencies that drive\nprocess outcomes, offering a more nuanced understanding of how individual\nbehaviors impact overall process efficiency.", "comment": "Accepted for presentation at the 5th Workshop on Change, Drift, and\n  Dynamics of Organizational Processes (ProDy), BPM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23037v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23033", "title": "Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields", "authors": ["Ranxi Lin", "Canming Yao", "Jiayi Li", "Weihang Liu", "Xin Lou", "Pingqiang Zhou"], "categories": ["cs.CV", "cs.NE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23033v1", "summary": "Neural Radiance Fields (NeRF)-based models have achieved remarkable success\nin 3D reconstruction and rendering tasks. However, during both training and\ninference, these models rely heavily on dense point sampling along rays from\nmultiple viewpoints, resulting in a surge in floating-point operations and\nseverely limiting their use in resource-constrained scenarios like edge\ncomputing. Spiking Neural Networks (SNNs), which communicate via binary spikes\nover discrete time steps, offer a promising alternative due to their\nenergy-efficient nature. Given the inherent variability in scene scale and\ntexture complexity in neural rendering and the prevailing practice of training\nseparate models per scene, we propose a spike-based NeRF framework with a\ndynamic time step training strategy, termed Pretrain-Adaptive Time-step\nAdjustment (PATA). This approach automatically explores the trade-off between\nrendering quality and time step length during training. Consequently, it\nenables scene-adaptive inference with variable time steps and reduces the\nadditional consumption of computational resources in the inference process.\nAnchoring to the established Instant-NGP architecture, we evaluate our method\nacross diverse datasets. The experimental results show that PATA can preserve\nrendering fidelity while reducing inference time steps by 64\\% and running\npower by 61.55\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23033v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22893", "title": "Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure", "authors": ["Giuseppe Riva"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22893v1", "summary": "Contemporary human-AI interaction research overlooks how AI systems\nfundamentally reshape human cognition pre-consciously, a critical blind spot\nfor understanding distributed cognition. This paper introduces \"Cognitive\nInfrastructure Studies\" (CIS) as a new interdisciplinary domain to\nreconceptualize AI as \"cognitive infrastructures\": foundational, often\ninvisible systems conditioning what is knowable and actionable in digital\nsocieties. These semantic infrastructures transport meaning, operate through\nanticipatory personalization, and exhibit adaptive invisibility, making their\ninfluence difficult to detect. Critically, they automate \"relevance judgment,\"\nshifting the \"locus of epistemic agency\" to non-human systems. Through\nnarrative scenarios spanning individual (cognitive dependency), collective\n(democratic deliberation), and societal (governance) scales, we describe how\ncognitive infrastructures reshape human cognition, public reasoning, and social\nepistemologies. CIS aims to address how AI preprocessing reshapes distributed\ncognition across individual, collective, and cultural scales, requiring\nunprecedented integration of diverse disciplinary methods. The framework also\naddresses critical gaps across disciplines: cognitive science lacks\npopulation-scale preprocessing analysis capabilities, digital sociology cannot\naccess individual cognitive mechanisms, and computational approaches miss\ncultural transmission dynamics. To achieve this goal CIS also provides\nmethodological innovations for studying invisible algorithmic influence:\n\"infrastructure breakdown methodologies\", experimental approaches that reveal\ncognitive dependencies by systematically withdrawing AI preprocessing after\nperiods of habituation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22893v1", "cate": "cs.HC", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.23342", "title": "FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies", "authors": ["Laura Acosta García", "Juan Aznar Poveda", "Fabian Margreiter", "Antonio-Javier García Sánchez", "Joan García Haro", "Thomas Fahringer", "José Lorente López", "José-Víctor Rodríguez"], "categories": ["cs.NI", "cs.ET"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23342v1", "summary": "The Internet of Things (IoT) has transformed many industries, and LoRaWAN\n(Long Range Wide Area Network), built on LoRa (Long Range) technology, has\nbecome a crucial solution for enabling scalable, low-cost, and energy-efficient\ncommunication in wide-area networks. Simulation tools are essential for\noptimizing the transmission parameters and, therefore, the energy efficiency\nand performance of LoRaWAN networks. While existing simulation frameworks\naccurately replicate real-world scenarios by including multiple layers of\ncommunication protocols, they often imply significant computational overhead\nand simulation times. To address this issue, this paper introduces FAST-LoRa, a\nnovel simulation framework designed to enable fast and efficient evaluation of\nLoRaWAN networks and selection of transmission parameters. FAST-LoRa\nstreamlines computation by relying on analytical models without complex\npacket-level simulations and implementing gateway reception using efficient\nmatrix operations. Rather than aiming to replace discrete-event simulators,\nFAST-LoRa is intended as a lightweight and accurate approximation tool for\nevaluating transmission parameter strategies in scenarios with stable traffic\npatterns and uplink-focused communications. In our evaluation, we compare\nFAST-LoRa with a well-established simulator using multiple network\nconfigurations with varying numbers of end devices and gateways. The results\nshow that FAST-LoRa achieves similar accuracy in estimating key network\nmetrics, even in complex scenarios with interference and multi-gateway\nreception, with a Mean Absolute Error (MAE) of 0.940 $\\times 10^{-2}$ for the\nPacket Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while\nsignificantly reducing computational time by up to three orders of magnitude.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23342v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23565", "title": "Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI", "authors": ["Botao Zhu", "Xianbin Wang", "Dusit Niyato"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23565v1", "summary": "In collaborative systems, the effective completion of tasks hinges on\ntask-specific trust evaluations of potential devices for distributed\ncollaboration. However, the complexity of tasks, the spatiotemporal dynamism of\ndistributed device resources, and the inevitable assessment overhead\ndramatically increase the complexity and resource consumption of the trust\nevaluation process. As a result, ill-timed or overly frequent trust evaluations\ncan reduce utilization rate of constrained resources, negatively affecting\ncollaborative task execution. To address this challenge, this paper proposes an\nautonomous trust orchestration method based on a new concept of semantic\nchain-of-trust. Our technique employs agentic AI and hypergraph to establish\nand maintain trust relationships among devices. By leveraging its strengths in\nautonomous perception, task decomposition, and semantic reasoning, we propose\nagentic AI to perceive device states and autonomously perform trust evaluations\nof collaborators based on historical performance data only during device idle\nperiods, thereby enabling efficient utilization of distributed resources. In\naddition, agentic AI performs task-specific trust evaluations on collaborator\nresources by analyzing the alignment between resource capabilities and task\nrequirements. Moreover, by maintaining a trust hypergraph embedded with trust\nsemantics for each device, agentic AI enables hierarchical management of\ncollaborators and identifies collaborators requiring trust evaluation based on\ntrust semantics, thereby achieving a balance between overhead and trust\naccuracy. Furthermore, local trust hypergraphs from multiple devices can be\nchained together to support multi-hop collaboration, enabling efficient\ncoordination in large-scale systems. Experimental results demonstrate that the\nproposed method achieves resource-efficient trust evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23565v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23540", "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving", "authors": ["Yi Zhang", "Erik Leo Haß", "Kuo-Yi Chao", "Nenad Petrovic", "Yinglei Song", "Chengdong Wu", "Alois Knoll"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23540v1", "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23540v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23043", "title": "Prediction of Significant Creatinine Elevation in First ICU Stays with Vancomycin Use: A retrospective study through Catboost", "authors": ["Junyi Fan", "Li Sun", "Shuheng Chen", "Yong Si", "Minoo Ahmadi", "Greg Placencia", "Elham Pishgar", "Kamiar Alaei", "Maryam Pishgar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23043v1", "summary": "Background: Vancomycin, a key antibiotic for severe Gram-positive infections\nin ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in\ncritically ill patients is challenging. This study aimed to develop a machine\nlearning model to predict vancomycin-related creatinine elevation using routine\nICU data.\n  Methods: We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV\ndatabase who received vancomycin. Kidney injury was defined by KDIGO criteria\n(creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were\nselected via SelectKBest (top 30) and Random Forest ranking (final 15). Six\nalgorithms were tested with 5-fold cross-validation. Interpretability was\nevaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior\nsampling.\n  Results: Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation.\nCatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800,\nspecificity 0.681, negative predictive value 0.900). Key predictors were\nphosphate, total bilirubin, magnesium, Charlson index, and APSIII. SHAP\nconfirmed phosphate as a major risk factor. ALE showed dose-response patterns.\nBayesian analysis estimated mean risk 60.5% (95% credible interval: 16.8-89.4%)\nin high-risk cases.\n  Conclusions: This machine learning model predicts vancomycin-associated\ncreatinine elevation from routine ICU data with strong accuracy and\ninterpretability, enabling early risk detection and supporting timely\ninterventions in critical care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23043v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23042", "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO", "I.2.6; I.2.9; I.2.10; C.3.3"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.23042v1", "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.23042v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22894", "title": "When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research", "authors": ["Monique Munarini"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Presented at HHAI25:The 4th International Conference Series on Hybrid Human-Artificial Intelligence, workshop Mind the AI-GAP 2025:Co-Designing Socio-Technical Systems. (June 9-13, 2025 in Pisa, Italy)", "url": "http://arxiv.org/abs/2507.22894v1", "summary": "This reflective paper explores often-unspoken challenges of designing and\nfacilitating co-design and participatory workshops, offering practical\nstrategies for early career researchers (ECRs) navigating these methods.\nDrawing from personal experience conducting a series of workshops titled: How\nto Think About Equity in the AI Ecosystem. It follows the full arc of the\nworkshop experience, from conceptualization and activity planning to\nparticipant recruitment and facilitation, offering a grounded account of what\nhappens when participation does not go as expected. The paper examines the\nmethodological challenges of engaging non-expert participants, particularly\nwhen operating without institutional support, financial incentives, or\nintegration into larger events. Despite initial difficulties such as low\nattendance, the workshop fostered rich discussions among a demographically\ndiverse group and ultimately led to one participant volunteering to\nco-facilitate a subsequent session. This transition from participant to\nco-facilitator exemplifies the redistribution of epistemic authority,\npositioning lived experience as central to research and engagement practices.\nBy reframing perceived failure as a productive site of learning, the paper\noffers practical strategies for ECRs working across disciplines who often\nnavigate unfamiliar methodological terrains, contributing to broader\nconversations on the realities of doing interdisciplinary, participatory work\nin practice.", "comment": "Presented at HHAI25:The 4th International Conference Series on Hybrid\n  Human-Artificial Intelligence, workshop Mind the AI-GAP 2025:Co-Designing\n  Socio-Technical Systems. (June 9-13, 2025 in Pisa, Italy)", "pdf_url": "http://arxiv.org/pdf/2507.22894v1", "cate": "cs.HC", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2507.23454", "title": "Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary", "authors": ["Marta Bieńkiewicz", "Julia Ayache", "Panayiotis Charalambous", "Cristina Becchio", "Marco Corragio", "Bertram Taetz", "Francesco De Lellis", "Antonio Grotta", "Anna Server", "Daniel Rammer", "Richard Kulpa", "Franck Multon", "Azucena Garcia-Palacios", "Jessica Sutherland", "Kathleen Bryson", "Stéphane Donikian", "Didier Stricker", "Benoît Bardy"], "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.GR", "q-bio.NC", "I.3.0; I.2; J.4; K.4"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      pre-print", "url": "http://arxiv.org/abs/2507.23454v1", "summary": "This article explores a critical gap in Mixed Reality (MR) technology: while\nadvances have been made, MR still struggles to authentically replicate human\nembodiment and socio-motor interaction. For MR to enable truly meaningful\nsocial experiences, it needs to incorporate multi-modal data streams and\nmulti-agent interaction capabilities. To address this challenge, we present a\ncomprehensive glossary covering key topics such as Virtual Characters and\nAutonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges\nof Social MR within Neuroscience, Embodiment, and Technology. Our aim is to\ndrive the transformative evolution of MR technologies that prioritize\nhuman-centric innovation, fostering richer digital connections. We advocate for\nMR systems that enhance social interaction and collaboration between humans and\nvirtual autonomous agents, ensuring inclusivity, ethical design and\npsychological safety in the process.", "comment": "pre-print", "pdf_url": "http://arxiv.org/pdf/2507.23454v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23087", "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at the Workshop on Distributed Ledger Technologies in Business Process Management, At the International Conference for Business Process Management (BPM), 2025", "url": "http://arxiv.org/abs/2507.23087v1", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "pdf_url": "http://arxiv.org/pdf/2507.23087v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23633", "title": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying", "authors": ["Qian Zhao", "Zhuo Sun", "Bin Guo", "Zhiwen Yu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23633v1", "summary": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23633v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23544", "title": "User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals", "authors": ["Ryo Miyoshi", "Yuki Okafuji", "Takuya Iwamoto", "Junya Nakanishi", "Jun Baba"], "categories": ["cs.RO", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for presentation at IEEE/RSJ International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "url": "http://arxiv.org/abs/2507.23544v1", "summary": "In recent years, the demand for social robots has grown, requiring them to\nadapt their behaviors based on users' states. Accurately assessing user\nexperience (UX) in human-robot interaction (HRI) is crucial for achieving this\nadaptability. UX is a multi-faceted measure encompassing aspects such as\nsentiment and engagement, yet existing methods often focus on these\nindividually. This study proposes a UX estimation method for HRI by leveraging\nmultimodal social signals. We construct a UX dataset and develop a\nTransformer-based model that utilizes facial expressions and voice for\nestimation. Unlike conventional models that rely on momentary observations, our\napproach captures both short- and long-term interaction patterns using a\nmulti-instance learning framework. This enables the model to capture temporal\ndynamics in UX, providing a more holistic representation. Experimental results\ndemonstrate that our method outperforms third-party human evaluators in UX\nestimation.", "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23544v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23073", "title": "Locally Differentially Private Thresholding Bandits", "authors": ["Annalisa Barbara", "Joseph Lazzaro", "Ciara Pike-Burke"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      18th European Workshop on Reinforcement Learning (EWRL 2025)", "url": "http://arxiv.org/abs/2507.23073v1", "summary": "This work investigates the impact of ensuring local differential privacy in\nthe thresholding bandit problem. We consider both the fixed budget and fixed\nconfidence settings. We propose methods that utilize private responses,\nobtained through a Bernoulli-based differentially private mechanism, to\nidentify arms with expected rewards exceeding a predefined threshold. We show\nthat this procedure provides strong privacy guarantees and derive theoretical\nperformance bounds on the proposed algorithms. Additionally, we present general\nlower bounds that characterize the additional loss incurred by any\ndifferentially private mechanism, and show that the presented algorithms match\nthese lower bounds up to poly-logarithmic factors. Our results provide valuable\ninsights into privacy-preserving decision-making frameworks in bandit problems.", "comment": "18th European Workshop on Reinforcement Learning (EWRL 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23073v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23058", "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation", "authors": ["Alexandru Buburuzan"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      A dissertation submitted to The University of Manchester for the degree of Bachelor of Science in Artificial Intelligence", "url": "http://arxiv.org/abs/2507.23058v1", "summary": "Safety-critical applications, such as autonomous driving and medical image\nanalysis, require extensive multimodal data for rigorous testing. Synthetic\ndata methods are gaining prominence due to the cost and complexity of gathering\nreal-world data, but they demand a high degree of realism and controllability\nto be useful. This work introduces two novel methods for synthetic data\ngeneration in autonomous driving and medical image analysis, namely MObI and\nAnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal\nObject Inpainting that leverages a diffusion model to produce realistic and\ncontrollable object inpaintings across perceptual modalities, demonstrated\nsimultaneously for camera and lidar. Given a single reference RGB image, MObI\nenables seamless object insertion into existing multimodal scenes at a\nspecified 3D location, guided by a bounding box, while maintaining semantic\nconsistency and multimodal coherence. Unlike traditional inpainting methods\nthat rely solely on edit masks, this approach uses 3D bounding box conditioning\nto ensure accurate spatial positioning and realistic scaling. AnydoorMed\nextends this paradigm to the medical imaging domain, focusing on\nreference-guided inpainting for mammography scans. It leverages a\ndiffusion-based model to inpaint anomalies with impressive detail preservation,\nmaintaining the reference anomaly's structural integrity while semantically\nblending it with the surrounding tissue. Together, these methods demonstrate\nthat foundation models for reference-guided inpainting in natural images can be\nreadily adapted to diverse perceptual modalities, paving the way for the next\ngeneration of systems capable of constructing highly realistic, controllable\nand multimodal counterfactual scenarios.", "comment": "A dissertation submitted to The University of Manchester for the\n  degree of Bachelor of Science in Artificial Intelligence", "pdf_url": "http://arxiv.org/pdf/2507.23058v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22895", "title": "Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface", "authors": ["Ye Sun", "Bowei Zhao", "Dezhong Yao", "Rui Zhang", "Bohan Zhang", "Xiaoyuan Li", "Jing Wang", "Mingxuan Qu", "Gang Liu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      18 pages, 9 figures", "url": "http://arxiv.org/abs/2507.22895v1", "summary": "Brain-computer interfaces (BCIs) enable real-time interaction between the\nbrain and external devices by decoding neural signals. However, existing\nmotor-based BCI paradigms, like motor imagery BCI, face challenges with\nimprecise labeling in real-world use. This mismatch between EEG signals and\ntrue behavioral intentions leads to pseudo-labels, undermining decoding\naccuracy and system robustness. To overcome this bottleneck, this paper first\nproposes a novel motor intention extraction framework based on a non-invasive\nbrain-muscle interface (BMuI)($\\text{BCI} =\n\\frac{\\text{Brain}}{\\text{Computer}} \\text{ Interface} =\n\\frac{\\text{Brain}}{\\not\\text{Muscle}}\\! \\text{ (BMuI)} \\times\n\\!\\frac{\\not\\text{Muscle}}{\\text{Computer}}\\! \\text{ Interface}$). This method\nsimulates the neural pathway from the brain to the muscles in order to capture\nand enhance the weak motor intention signals originating in the brain. It then\nuses EMG as a high-fidelity relay medium to achieve more accurate intention\nrecognition and transmission. To systematically validate the feasibility and\neffectiveness of this approach, we conducted both offline experiments (to\nrepeatedly verify feasibility) and online experiments (to construct a real-time\ninteractive system and evaluate its performance). The results show that BMuI is\nfeasible, achieving a prediction accuracy of 0.8314; in the online experiment,\nall participants are able to successfully control the Unity virtual arm.", "comment": "18 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.22895v1", "cate": "cs.HC", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "categories": ["cs.NE", "cs.AR", "cs.ET"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, Accepted for presentation in IEEE MetroXRAINE 2025 conference", "url": "http://arxiv.org/abs/2506.05588v3", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 5 figures, Accepted for presentation in IEEE MetroXRAINE\n  2025 conference", "pdf_url": "http://arxiv.org/pdf/2506.05588v3", "cate": "cs.NE", "date": "2025-06-05", "updated": "2025-07-31"}
{"id": "2507.23118", "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23118v1", "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23118v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23664", "title": "Personalized Education with Ranking Alignment Recommendation", "authors": ["Haipeng Liu", "Yuxuan Liu", "Ting Long"], "categories": ["cs.AI", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23664v1", "summary": "Personalized question recommendation aims to guide individual students\nthrough questions to enhance their mastery of learning targets. Most previous\nmethods model this task as a Markov Decision Process and use reinforcement\nlearning to solve, but they struggle with efficient exploration, failing to\nidentify the best questions for each student during training. To address this,\nwe propose Ranking Alignment Recommendation (RAR), which incorporates\ncollaborative ideas into the exploration mechanism, enabling more efficient\nexploration within limited training episodes. Experiments show that RAR\neffectively improves recommendation performance, and our framework can be\napplied to any RL-based question recommender. Our code is available in\nhttps://github.com/wuming29/RAR.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23664v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23589", "title": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study", "authors": ["Kai Goebel", "Patrik Zips"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23589v1", "summary": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23589v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23077", "title": "A Foundation Model for Material Fracture Prediction", "authors": ["Agnese Marcato", "Aleksandra Pachalieva", "Ryley G. Hill", "Kai Gao", "Xiaoyu Wang", "Esteban Rougier", "Zhou Lei", "Vinamra Agrawal", "Janel Chua", "Qinjun Kang", "Jeffrey D. Hyman", "Abigail Hunter", "Nathan DeBardeleben", "Earl Lawrence", "Hari Viswanathan", "Daniel O'Malley", "Javier E. Santos"], "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.geo-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23077v1", "summary": "Accurately predicting when and how materials fail is critical to designing\nsafe, reliable structures, mechanical systems, and engineered components that\noperate under stress. Yet, fracture behavior remains difficult to model across\nthe diversity of materials, geometries, and loading conditions in real-world\napplications. While machine learning (ML) methods show promise, most models are\ntrained on narrow datasets, lack robustness, and struggle to generalize.\nMeanwhile, physics-based simulators offer high-fidelity predictions but are\nfragmented across specialized methods and require substantial high-performance\ncomputing resources to explore the input space. To address these limitations,\nwe present a data-driven foundation model for fracture prediction, a\ntransformer-based architecture that operates across simulators, a wide range of\nmaterials (including plastic-bonded explosives, steel, aluminum, shale, and\ntungsten), and diverse loading conditions. The model supports both structured\nand unstructured meshes, combining them with large language model embeddings of\ntextual input decks specifying material properties, boundary conditions, and\nsolver settings. This multimodal input design enables flexible adaptation\nacross simulation scenarios without changes to the model architecture. The\ntrained model can be fine-tuned with minimal data on diverse downstream tasks,\nincluding time-to-failure estimation, modeling fracture evolution, and adapting\nto combined finite-discrete element method simulations. It also generalizes to\nunseen materials such as titanium and concrete, requiring as few as a single\nsample, dramatically reducing data needs compared to standard ML. Our results\nshow that fracture prediction can be unified under a single model architecture,\noffering a scalable, extensible alternative to simulator-specific workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23077v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23064", "title": "Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints", "authors": ["Santosh Patapati", "Trisanth Srinivasan", "Murari Ambati"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "I.4.8; I.2.10; I.2.6; C.3.3; I.4.9"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.23064v1", "summary": "Autonomous cars need geometric accuracy and semantic understanding to\nnavigate complex environments, yet most stacks handle them separately. We\npresent XYZ-Drive, a single vision-language model that reads a front-camera\nframe, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs\nsteering and speed. A lightweight goal-centered cross-attention layer lets\nwaypoint tokens highlight relevant image and map patches, supporting both\naction and textual explanations, before the fused tokens enter a partially\nfine-tuned LLaMA-3.2 11B model.\n  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and\n0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and\nhalving collisions, all while significantly improving efficiency by using only\na single branch. Sixteen ablations explain the gains. Removing any modality\n(vision, waypoint, map) drops success by up to 11%, confirming their\ncomplementary roles and rich connections. Replacing goal-centered attention\nwith simple concatenation cuts 3% in performance, showing query-based fusion\ninjects map knowledge more effectively. Keeping the transformer frozen loses\n5%, showing the importance of fine-tuning when applying VLMs for specific tasks\nsuch as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs\nlane edges and raises crash rate.\n  Overall, these results demonstrate that early, token-level fusion of intent\nand map layout enables accurate, transparent, real-time driving.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.23064v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22896", "title": "iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement", "authors": ["Kohou Wang", "ZhaoXiang Liu", "Lin Bai", "Kun Fan", "Xiang Liu", "Huan Hu", "Kai Wang", "Shiguo Lian"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      17 pages, 12 figures", "url": "http://arxiv.org/abs/2507.22896v1", "summary": "It is crucial that robots' performance can be improved after deployment, as\nthey are inherently likely to encounter novel scenarios never seen before. This\npaper presents an innovative solution: an interactive learning-based robot\nsystem powered by a Multi-modal Large Language Model(MLLM). A key feature of\nour system is its ability to learn from natural dialogues with non-expert\nusers. We also propose chain of question to clarify the exact intent of the\nquestion before providing an answer and dual-modality retrieval modules to\nleverage these interaction events to avoid repeating same mistakes, ensuring a\nseamless user experience before model updates, which is in contrast to current\nmainstream MLLM-based robotic systems. Our system marks a novel approach in\nrobotics by integrating interactive learning, paving the way for superior\nadaptability and performance in diverse environments. We demonstrate the\neffectiveness and improvement of our method through experiments, both\nquantitively and qualitatively.", "comment": "17 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.22896v1", "cate": "cs.HC", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.22810", "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen Guo"], "categories": ["cs.HC", "cs.ET", "cs.SE"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22810v2", "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22810v2", "cate": "cs.HC", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23120", "title": "Vibe Modeling: Challenges and Opportunities", "authors": ["Jordi Cabot"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23120v1", "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23120v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23701", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23701v1", "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23701v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23592", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "authors": ["Haiyun Zhang", "Stefano Dalla Gasperina", "Saad N. Yousaf", "Toshimitsu Tsuboi", "Tetsuya Narita", "Ashish D. Deshpande"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 10 figures, submitted to RA-L", "url": "http://arxiv.org/abs/2507.23592v1", "summary": "Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications.", "comment": "8 pages, 10 figures, submitted to RA-L", "pdf_url": "http://arxiv.org/pdf/2507.23592v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23093", "title": "On the Sustainability of AI Inferences in the Edge", "authors": ["Ghazal Sobhani", "Md. Monzurul Amin Ifath", "Tushar Sharma", "Israat Haque"], "categories": ["cs.LG", "cs.AI", "cs.PF"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, 8 figures, 6 tables, in preparation for journal submission", "url": "http://arxiv.org/abs/2507.23093v1", "summary": "The proliferation of the Internet of Things (IoT) and its cutting-edge\nAI-enabled applications (e.g., autonomous vehicles and smart industries)\ncombine two paradigms: data-driven systems and their deployment on the edge.\nUsually, edge devices perform inferences to support latency-critical\napplications. In addition to the performance of these resource-constrained edge\ndevices, their energy usage is a critical factor in adopting and deploying edge\napplications. Examples of such devices include Raspberry Pi (RPi), Intel Neural\nCompute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU).\nDespite their adoption in edge deployment for AI inferences, there is no study\non their performance and energy usage for informed decision-making on the\ndevice and model selection to meet the demands of applications. This study\nfills the gap by rigorously characterizing the performance of traditional,\nneural networks, and large language models on the above-edge devices.\nSpecifically, we analyze trade-offs among model F1 score, inference time,\ninference power, and memory usage. Hardware and framework optimization, along\nwith external parameter tuning of AI models, can balance between model\nperformance and resource usage to realize practical edge AI deployments.", "comment": "14 pages, 8 figures, 6 tables, in preparation for journal submission", "pdf_url": "http://arxiv.org/pdf/2507.23093v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23070", "title": "Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model", "authors": ["Dmitry Demidov", "Zaigham Zaheer", "Omkar Thawakar", "Salman Khan", "Fahad Shahbaz Khan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.23070v1", "summary": "Fine-grained image classification, the task of distinguishing between\nvisually similar subcategories within a broader category (e.g., bird species,\ncar models, flower types), is a challenging computer vision problem.\nTraditional approaches rely heavily on fixed vocabularies and closed-set\nclassification paradigms, limiting their scalability and adaptability in\nreal-world settings where novel classes frequently emerge. Recent research has\ndemonstrated that combining large language models (LLMs) with vision-language\nmodels (VLMs) makes open-set recognition possible without the need for\npredefined class labels. However, the existing methods are often limited in\nharnessing the power of LLMs at the classification phase, and also rely heavily\non the guessed class names provided by an LLM without thorough analysis and\nrefinement. To address these bottlenecks, we propose our training-free method,\nEnriched-FineR (or E-FineR for short), which demonstrates state-of-the-art\nresults in fine-grained visual recognition while also offering greater\ninterpretability, highlighting its strong potential in real-world scenarios and\nnew domains where expert annotations are difficult to obtain. Additionally, we\ndemonstrate the application of our proposed approach to zero-shot and few-shot\nclassification, where it demonstrated performance on par with the existing SOTA\nwhile being training-free and not requiring human interventions. Overall, our\nvocabulary-free framework supports the shift in image classification from rigid\nlabel prediction to flexible, language-driven understanding, enabling scalable\nand generalizable systems for real-world applications. Well-documented code is\navailable on https://github.com/demidovd98/e-finer.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23070v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22897", "title": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems", "authors": ["Luyu Chen", "Quanyu Dai", "Zeyu Zhang", "Xueyang Feng", "Mingyu Zhang", "Pengcheng Tang", "Xu Chen", "Yue Zhu", "Zhenhua Dong"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted by TheWebConf'25 Industry Track", "url": "http://arxiv.org/abs/2507.22897v1", "summary": "Conversational recommender systems (CRS) enhance user experience through\nmulti-turn interactions, yet evaluating CRS remains challenging. User\nsimulators can provide comprehensive evaluations through interactions with CRS,\nbut building realistic and diverse simulators is difficult. While recent work\nleverages large language models (LLMs) to simulate user interactions, they\nstill fall short in emulating individual real users across diverse scenarios\nand lack explicit rating mechanisms for quantitative evaluation. To address\nthese gaps, we propose RecUserSim, an LLM agent-based user simulator with\nenhanced simulation realism and diversity while providing explicit scores.\nRecUserSim features several key modules: a profile module for defining\nrealistic and diverse user personas, a memory module for tracking interaction\nhistory and discovering unknown preferences, and a core action module inspired\nby Bounded Rationality theory that enables nuanced decision-making while\ngenerating more fine-grained actions and personalized responses. To further\nenhance output control, a refinement module is designed to fine-tune final\nresponses. Experiments demonstrate that RecUserSim generates diverse,\ncontrollable outputs and produces realistic, high-quality dialogues, even with\nsmaller base LLMs. The ratings generated by RecUserSim show high consistency\nacross different base LLMs, highlighting its effectiveness for CRS evaluation.", "comment": "Accepted by TheWebConf'25 Industry Track", "pdf_url": "http://arxiv.org/pdf/2507.22897v1", "cate": "cs.HC", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.23168", "title": "Extension Decisions in Open Source Software Ecosystem", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Paper published in JSS journal", "url": "http://arxiv.org/abs/2507.23168v1", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "comment": "Paper published in JSS journal", "pdf_url": "http://arxiv.org/pdf/2507.23168v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22955", "title": "LLMs Between the Nodes: Community Discovery Beyond Vectors", "authors": ["Ekta Gujral", "Apurva Sinha"], "categories": ["cs.SI", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22955v1", "summary": "Community detection in social network graphs plays a vital role in uncovering\ngroup dynamics, influence pathways, and the spread of information. Traditional\nmethods focus primarily on graph structural properties, but recent advancements\nin Large Language Models (LLMs) open up new avenues for integrating semantic\nand contextual information into this task. In this paper, we present a detailed\ninvestigation into how various LLM-based approaches perform in identifying\ncommunities within social graphs. We introduce a two-step framework called\nCommLLM, which leverages the GPT-4o model along with prompt-based reasoning to\nfuse language model outputs with graph structure. Evaluations are conducted on\nsix real-world social network datasets, measuring performance using key metrics\nsuch as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI),\nVariation of Information (VOI), and cluster purity. Our findings reveal that\nLLMs, particularly when guided by graph-aware strategies, can be successfully\napplied to community detection tasks in small to medium-sized graphs. We\nobserve that the integration of instruction-tuned models and carefully\nengineered prompts significantly improves the accuracy and coherence of\ndetected communities. These insights not only highlight the potential of LLMs\nin graph-based research but also underscore the importance of tailoring model\ninteractions to the specific structure of graph data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22955v1", "cate": "cs.SI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23726", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "authors": ["Luoxin Chen", "Jinming Gu", "Liankai Huang", "Wenhao Huang", "Zhicheng Jiang", "Allan Jie", "Xiaoran Jin", "Xing Jin", "Chenggang Li", "Kaijing Ma", "Cheng Ren", "Jiawei Shen", "Wenlei Shi", "Tong Sun", "He Sun", "Jiahui Wang", "Siran Wang", "Zhihong Wang", "Chenrui Wei", "Shufa Wei", "Yonghui Wu", "Yuchen Wu", "Yihang Xia", "Huajian Xin", "Fan Yang", "Huaiyuan Ying", "Hongyi Yuan", "Zheng Yuan", "Tianyang Zhan", "Chi Zhang", "Yue Zhang", "Ge Zhang", "Tianyun Zhao", "Jianqiu Zhao", "Yichi Zhou", "Thomas Hanwen Zhu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23726v1", "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23726v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23629", "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "authors": ["Yewei Huang", "John McConnell", "Xi Lin", "Brendan Englot"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23629v1", "summary": "We present DRACo-SLAM2, a distributed SLAM framework for underwater robot\nteams equipped with multibeam imaging sonar. This framework improves upon the\noriginal DRACo-SLAM by introducing a novel representation of sonar maps as\nobject graphs and utilizing object graph matching to achieve time-efficient\ninter-robot loop closure detection without relying on prior geometric\ninformation. To better-accommodate the needs and characteristics of underwater\nscan matching, we propose incremental Group-wise Consistent Measurement Set\nMaximization (GCM), a modification of Pairwise Consistent Measurement Set\nMaximization (PCM), which effectively handles scenarios where nearby\ninter-robot loop closures share similar registration errors. The proposed\napproach is validated through extensive comparative analyses on simulated and\nreal-world datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23629v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23111", "title": "Scalable Generative Modeling of Weighted Graphs", "authors": ["Richard Williams", "Eric Nalisnick", "Andrew Holbrook"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 pages, 5 figures, included appendix. code at this https URL", "url": "http://arxiv.org/abs/2507.23111v1", "summary": "Weighted graphs are ubiquitous throughout biology, chemistry, and the social\nsciences, motivating the development of generative models for abstract weighted\ngraph data using deep neural networks. However, most current deep generative\nmodels are either designed for unweighted graphs and are not easily extended to\nweighted topologies or incorporate edge weights without consideration of a\njoint distribution with topology. Furthermore, learning a distribution over\nweighted graphs must account for complex nonlocal dependencies between both the\nedges of the graph and corresponding weights of each edge. We develop an\nautoregressive model BiGG-E, a nontrivial extension of the BiGG model, that\nlearns a joint distribution over weighted graphs while still exploiting\nsparsity to generate a weighted graph with $n$ nodes and $m$ edges in $O((n +\nm)\\log n)$ time. Simulation studies and experiments on a variety of benchmark\ndatasets demonstrate that BiGG-E best captures distributions over weighted\ngraphs while remaining scalable and computationally efficient.", "comment": "25 pages, 5 figures, included appendix. code at\n  https://github.com/rlwilliams34/BiGG-E", "pdf_url": "http://arxiv.org/pdf/2507.23111v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23110", "title": "Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation", "authors": ["Zheyuan Zhang", "Linkai Peng", "Wanying Dou", "Cuiling Sun", "Halil Ertugrul Aktas", "Andrea M. Bejar", "Elif Keles", "Gorkem Durak", "Ulas Bagci"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23110v1", "summary": "Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences\nwhose appearance differs more than the acquisition sites that produce them.\nExisting domain-generalization benchmarks focus almost on cross-center shifts\nand overlook this dominant source of variability. Pancreas segmentation remains\na major challenge in abdominal imaging: the gland is small, irregularly,\nsurrounded by organs and fat, and often suffers from low T1 contrast.\nState-of-the-art deep networks that already achieve >90% Dice on the liver or\nkidneys still miss 20-30% of the pancreas. The organ is also systematically\nunder-represented in public cross-domain benchmarks, despite its clinical\nimportance in early cancer detection, surgery, and diabetes research. To close\nthis gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas\nsegmentation dataset for investigating domain generalization in medical\nimaging. The dataset comprises 563 MRI scans from six institutions, spanning\nboth venous phase and out-of-phase sequences, enabling study of both\ncross-center and cross-sequence variations with pixel-accurate pancreas masks\ncreated by a double-blind, two-pass protocol. Through comprehensive analysis,\nwe reveal three insights: (i) limited sampling introduces significant variance\nthat may be mistaken for distribution shifts, (ii) cross-center performance\ncorrelates with source domain performance for identical sequences, and (iii)\ncross-sequence shifts require specialized solutions. We also propose a\nsemi-supervised approach that leverages anatomical invariances, significantly\noutperforming state-of-the-art domain generalization techniques with 61.63%\nDice score improvements and 87.00% on two test centers for cross-sequence\nsegmentation. PancreasDG sets a new benchmark for domain generalization in\nmedical imaging. Dataset, code, and models will be available at\nhttps://pancreasdg.netlify.app.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23110v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22898", "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "categories": ["cs.HC", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22898v1", "summary": "We developed a voice-driven artificial intelligence (AI) system that guides\nanyone - from paramedics to family members - through expert-level stroke\nevaluations using natural conversation, while also enabling smartphone video\ncapture of key examination components for documentation and potential expert\nreview. This addresses a critical gap in emergency care: current stroke\nrecognition by first responders is inconsistent and often inaccurate, with\nsensitivity for stroke detection as low as 58%, causing life-threatening delays\nin treatment. Three non-medical volunteers used our AI system to assess ten\nsimulated stroke patients, including cases with likely large vessel occlusion\n(LVO) strokes and stroke-like conditions, while we measured diagnostic\naccuracy, completion times, user confidence, and expert physician review of the\nAI-generated reports. The AI system correctly identified 84% of individual\nstroke signs and detected 75% of likely LVOs, completing evaluations in just\nover 6 minutes. Users reported high confidence (median 4.5/5) and ease of use\n(mean 4.67/5). The system successfully identified 86% of actual strokes but\nalso incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert\nphysician reviewed the AI reports with videos, they identified the correct\ndiagnosis in 100% of cases, but felt confident enough to make preliminary\ntreatment decisions in only 40% of cases due to observed AI errors including\nincorrect scoring and false information. While the current system's limitations\nnecessitate human oversight, ongoing rapid advancements in speech-to-speech AI\nmodels suggest that future versions are poised to enable highly accurate\nassessments. Achieving human-level voice interaction could transform emergency\nmedical care, putting expert-informed assessment capabilities in everyone's\nhands.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22898v1", "cate": "cs.HC", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.23178", "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "categories": ["cs.SE", "cs.AI", "I.2.5"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      14 pages, 12 figures, under review", "url": "http://arxiv.org/abs/2507.23178v1", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "comment": "14 pages, 12 figures, under review", "pdf_url": "http://arxiv.org/pdf/2507.23178v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23025", "title": "Constructing and Sampling Directed Graphs with Linearly Rescaled Degree Matrices", "authors": ["Yunxiang Yan", "Meng Jiang"], "categories": ["cs.SI", "cs.DM"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      SIGKDD 2022", "url": "http://arxiv.org/abs/2507.23025v1", "summary": "In recent years, many large directed networks such as online social networks\nare collected with the help of powerful data engineering and data storage\ntechniques. Analyses of such networks attract significant attention from both\nthe academics and industries. However, analyses of large directed networks are\noften time-consuming and expensive because the complexities of a lot of graph\nalgorithms are often polynomial with the size of the graph. Hence, sampling\nalgorithms that can generate graphs preserving properties of original graph are\nof great importance because they can speed up the analysis process. We propose\na promising framework to sample directed graphs: Construct a sample graph with\nlinearly rescaled Joint Degree Matrix (JDM) and Degree Correlation Matrix\n(DCM). Previous work shows that graphs with the same JDM and DCM will have a\nrange of very similar graph properties. We also conduct experiments on\nreal-world datasets to show that the numbers of non-zero entries in JDM and DCM\nare quite small compared to the number of edges and nodes. Adopting this\nframework, we propose a novel graph sampling algorithm that can provably\npreserves in-degree and out-degree distributions, which are two most\nfundamental properties of a graph. We also prove the upper bound for deviations\nin the joint degree distribution and degree correlation distribution, which\ncorrespond to JDM and DCM. Besides, we prove that the deviations in these\ndistributions are negatively correlated with the sparsity of the JDM and DCM.\nConsidering that these two matrices are always quite sparse, we believe that\nproposed algorithm will have a better-than-theory performance on real-world\nlarge directed networks.", "comment": "SIGKDD 2022", "pdf_url": "http://arxiv.org/pdf/2507.23025v1", "cate": "cs.SI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23751", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "authors": ["Ping Yu", "Jack Lanchantin", "Tianlu Wang", "Weizhe Yuan", "Olga Golovneva", "Ilia Kulikov", "Sainbayar Sukhbaatar", "Jason Weston", "Jing Xu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23751v1", "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23751v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23660", "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "authors": ["Haoxuan Jiang", "Peicong Qian", "Yusen Xie", "Xiaocong Li", "Ming Liu", "Jun Ma"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23660v1", "summary": "LiDAR-based localization serves as a critical component in autonomous\nsystems, yet existing approaches face persistent challenges in balancing\nrepeatability, accuracy, and environmental adaptability. Traditional point\ncloud registration methods relying solely on offline maps often exhibit limited\nrobustness against long-term environmental changes, leading to localization\ndrift and reliability degradation in dynamic real-world scenarios. To address\nthese challenges, this paper proposes DuLoc, a robust and accurate localization\nmethod that tightly couples LiDAR-inertial odometry with offline map-based\nlocalization, incorporating a constant-velocity motion model to mitigate\noutlier noise in real-world scenarios. Specifically, we develop a LiDAR-based\nlocalization framework that seamlessly integrates a prior global map with\ndynamic real-time local maps, enabling robust localization in unbounded and\nchanging environments. Extensive real-world experiments in ultra unbounded port\nthat involve 2,856 hours of operational data across 32 Intelligent Guided\nVehicles (IGVs) are conducted and reported in this study. The results attained\ndemonstrate that our system outperforms other state-of-the-art LiDAR\nlocalization systems in large-scale changing outdoor environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23660v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23115", "title": "FLOSS: Federated Learning with Opt-Out and Straggler Support", "authors": ["David J Goetze", "Dahlia J Felten", "Jeannie R Albrecht", "Rohit Bhattacharya"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.23115v1", "summary": "Previous work on data privacy in federated learning systems focuses on\nprivacy-preserving operations for data from users who have agreed to share\ntheir data for training. However, modern data privacy agreements also empower\nusers to use the system while opting out of sharing their data as desired. When\ncombined with stragglers that arise from heterogeneous device capabilities, the\nresult is missing data from a variety of sources that introduces bias and\ndegrades model performance. In this paper, we present FLOSS, a system that\nmitigates the impacts of such missing data on federated learning in the\npresence of stragglers and user opt-out, and empirically demonstrate its\nperformance in simulations.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.23115v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23134", "title": "Details Matter for Indoor Open-vocabulary 3D Instance Segmentation", "authors": ["Sanghun Jung", "Jingjing Zheng", "Ke Zhang", "Nan Qiao", "Albert Y. C. Chen", "Lu Xia", "Chi Liu", "Yuyin Sun", "Xiao Zeng", "Hsiang-Wei Huang", "Byron Boots", "Min Sun", "Cheng-Hao Kuo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.23134v1", "summary": "Unlike closed-vocabulary 3D instance segmentation that is often trained\nend-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages\nvision-language models (VLMs) to generate 3D instance proposals and classify\nthem. While various concepts have been proposed from existing research, we\nobserve that these individual concepts are not mutually exclusive but\ncomplementary. In this paper, we propose a new state-of-the-art solution for\nOV-3DIS by carefully designing a recipe to combine the concepts together and\nrefining them to address key challenges. Our solution follows the two-stage\nscheme: 3D proposal generation and instance classification. We employ robust 3D\ntracking-based proposal aggregation to generate 3D proposals and remove\noverlapped or partial proposals by iterative merging/removal. For the\nclassification stage, we replace the standard CLIP model with Alpha-CLIP, which\nincorporates object masks as an alpha channel to reduce background noise and\nobtain object-centric representation. Additionally, we introduce the\nstandardized maximum similarity (SMS) score to normalize text-to-proposal\nsimilarity, effectively filtering out false positives and boosting precision.\nOur framework achieves state-of-the-art performance on ScanNet200 and S3DIS\nacross all AP and AR metrics, even surpassing an end-to-end closed-vocabulary\nmethod.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23134v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22899", "title": "A visual analytics tool for taxonomy-based trajectory data exploration", "authors": ["Ivan A. Hanono Cozzetti", "Ahmad Abdou"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      71 pages, 92 figures", "url": "http://arxiv.org/abs/2507.22899v1", "summary": "The analysis of spatio-temporal data presents significant challenges due to\nthe complexity and heterogeneity of movement patterns. This project proposes a\ndata analytics tool that combines data visualization and statistical\ncomputation to facilitate spatio-temporal data analysis through a multi-level\napproach. The tool categorizes moving objects into distinct taxonomies using\nMachine Learning models, adding meaningful structure to the analysis. Two case\nstudies demonstrate the methodology's effectiveness. The first analyzed Arctic\nfox trajectories, successfully identifying and labeling foxes with Geometric or\nKinematic-based behaviors, further categorized into Curvature and Acceleration\ngroups. Statistical indicators revealed that foxes with Acceleration-based\nbehavior showed constant, steady acceleration, while those with Curvature-based\nbehavior exhibited acceleration peaks and sudden deceleration. The second case\nstudy examined tropical cyclone data, labeling trajectories with Speed,\nCurvature, and hybrid Geometric-based behaviors through unique statistical\nvariables. Analysis of hybrid Geometric behavior (Curvature and Indentation\ncombined) identified specific angles with the highest impact on hurricane shape\nand geometry. The proposed method and tool demonstrate that spatio-temporal\ndata, despite inherent complexity, can be analyzed and explained in detail,\nproviding a theoretical and practical blueprint applicable to multiple domains.", "comment": "71 pages, 92 figures", "pdf_url": "http://arxiv.org/pdf/2507.22899v1", "cate": "cs.HC", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.23348", "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Our code and data are available at this https URL", "url": "http://arxiv.org/abs/2507.23348v1", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "pdf_url": "http://arxiv.org/pdf/2507.23348v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23148", "title": "Countering the Forgetting of Novel Health Information with 'Social Boosting'", "authors": ["Vaibhav Krishna", "Nicholas A. Christakis"], "categories": ["cs.SI", "stat.AP"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures, 3 tables", "url": "http://arxiv.org/abs/2507.23148v1", "summary": "To mitigate the adverse effects of low-quality or false information, studies\nhave shown the effectiveness of various intervention techniques through\ndebunking or so-called pre-bunking. However, the effectiveness of such\ninterventions can decay. Here, we investigate the role of the detailed social\nstructure of the local villages within which the intervened individuals live,\nwhich provides opportunities for the targeted individuals to discuss and\ninternalize new knowledge. We evaluated this with respect to a critically\nimportant topic, information about maternal and child health care, delivered\nvia a 22-month in-home intervention. Specifically, we examined the effect of\nhaving friendship ties on the retention of knowledge interventions among\ntargeted individuals in 110 isolated Honduran villages. We hypothesize that\nindividuals who receive specific knowledge can internalize and consolidate this\ninformation by engaging in social interactions where, for instance, they have\nan opportunity to discuss it with others in the process. The opportunity to\nexplain information to others (knowledge sharing) promotes deeper cognitive\nprocessing and elaborative encoding, which ultimately enhances memory\nretention. We found that well-connected individuals within a social network\nexperience an enhanced effectiveness of knowledge interventions. These\nindividuals may be more likely to internalize and retain the information and\nreinforce it in others, due to increased opportunities for social interaction\nwhere they teach others or learn from them, a mechanism we refer to as \"social\nboosting\". These findings underscore the role of social interactions in\nreinforcing health knowledge interventions over the long term. We believe these\nfindings would be of interest to the health policy, the global health\nworkforce, and healthcare professionals focusing on disadvantaged populations\nand UN missions on infodemics.", "comment": "15 pages, 3 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.23148v1", "cate": "cs.SI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23012", "title": "PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers", "authors": ["Ashkan Sobhani", "Sogand Sadrhaghighi", "Xingjun Chu"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23012v1", "summary": "Large-scale distributed training in production data centers place significant\ndemands on network infrastructure. In particular, significant load balancing\nchallenges arise when processing AI/ML workloads, consisting of low-entropy,\nbursty and long-lived flows. Existing solutions designed for Ethernet, such as\nEqual-Cost Multi-Path (ECMP) struggle to maintain high network utilization.\nWhile major industry players (e.g., Ultra Ethernet Consortium) and parts of\nacademia have proposed packet spraying to enhance AI/ML workload performance,\nwe argue that existing packet spraying solutions lead to buffer inflation over\ntime, negatively affecting network performance. Specifically, when ACK\ncoalescing is used, these solutions lead to stale information, degrading\nnetwork performance. Additionally, in asymmetric network conditions- such as\nmix of ordered an unordered traffic, or link degradation and failures- existing\npacket spraying solutions often lead to increased tail latency. In this paper,\nwe present the design and evaluation of PRIME, a pseudo-randomized round-robin\napproach to packet spraying that considers the network topology to optimize\nload distribution and performance. PRIME uses congestion as an indicator to\nre-balance the load. To this extent, PRIME takes into account various\ncongestion signals, accounting for congestion severity, and their decay times\nto avoid network hotspots. We extensively evaluated PRIME using large-scale\nproduction-level simulator. Our results indicate that, compared to existing\nsolutions, PRIME leads to up to 15% improvement for permutation traffic and up\nto 27% improvement in network degradation scenarios", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23012v1", "cate": "cs.NI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23773", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "authors": ["Mingkai Deng", "Jinyu Hou", "Yilin Shen", "Hongxia Jin", "Graham Neubig", "Zhiting Hu", "Eric Xing"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23773v1", "summary": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23773v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23677", "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "authors": ["Xiaohan Li", "Ziren Gong", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia", "Dong Liu", "Jun Wu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23677v1", "summary": "3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM\napplications due to its fast rendering and high-fidelity representation.\nHowever, existing 3DGS-SLAM systems have predominantly focused on indoor\nenvironments and relied on active depth sensors, leaving a gap for large-scale\noutdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian\nSplatting SLAM system designed for outdoor scenarios. Our approach uses only\nRGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages\ndepth estimates from pre-trained deep stereo networks to guide 3D Gaussian\noptimization with a multi-loss strategy enhancing both geometric consistency\nand visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM\nachieves superior tracking accuracy and mapping performance compared to other\n3DGS-based solutions in complex outdoor environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23677v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23128", "title": "Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts", "authors": ["Anaïs Baranger", "Lucas Maison"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to ICASSP 2026", "url": "http://arxiv.org/abs/2507.23128v1", "summary": "Although prior work in computer vision has shown strong correlations between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, such\nrelationships remain underexplored in audio-based models. In this study, we\ninvestigate how training conditions and input features affect the robustness\nand generalization abilities of spoken keyword classifiers under OOD\nconditions. We benchmark several neural architectures across a variety of\nevaluation sets. To quantify the impact of noise on generalization, we make use\nof two metrics: Fairness (F), which measures overall accuracy gains compared to\na baseline model, and Robustness (R), which assesses the convergence between ID\nand OOD performance. Our results suggest that noise-aware training improves\nrobustness in some configurations. These findings shed new light on the\nbenefits and limitations of noise-based augmentation for generalization in\nspeech models.", "comment": "Submitted to ICASSP 2026", "pdf_url": "http://arxiv.org/pdf/2507.23128v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23143", "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention", "authors": ["Xiaochen Zhao", "Hongyi Xu", "Guoxian Song", "You Xie", "Chenxu Zhang", "Xiu Li", "Linjie Luo", "Jinli Suo", "Yebin Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICLR 2025, code is available at this https URL", "url": "http://arxiv.org/abs/2507.23143v1", "summary": "We propose X-NeMo, a novel zero-shot diffusion-based portrait animation\npipeline that animates a static portrait using facial movements from a driving\nvideo of a different individual. Our work first identifies the root causes of\nthe key issues in prior approaches, such as identity leakage and difficulty in\ncapturing subtle and extreme expressions. To address these challenges, we\nintroduce a fully end-to-end training framework that distills a 1D\nidentity-agnostic latent motion descriptor from driving image, effectively\ncontrolling motion through cross-attention during image generation. Our\nimplicit motion descriptor captures expressive facial motion in fine detail,\nlearned end-to-end from a diverse video dataset without reliance on pretrained\nmotion detectors. We further enhance expressiveness and disentangle motion\nlatents from identity cues by supervising their learning with a dual GAN\ndecoder, alongside spatial and color augmentations. By embedding the driving\nmotion into a 1D latent vector and controlling motion via cross-attention\nrather than additive spatial guidance, our design eliminates the transmission\nof spatial-aligned structural clues from the driving condition to the diffusion\nbackbone, substantially mitigating identity leakage. Extensive experiments\ndemonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly\nexpressive animations with superior identity resemblance. Our code and models\nare available for research.", "comment": "ICLR 2025, code is available at\n  https://github.com/bytedance/x-nemo-inference", "pdf_url": "http://arxiv.org/pdf/2507.23143v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22900", "title": "Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants", "authors": ["Sergio Rojas-Galeano"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22900v1", "summary": "This exploratory study examines how AI code assistants shape novice\nprogrammers' experiences during a two-part exam in an introductory programming\ncourse. In the first part, students completed a programming task with access to\nAI support; in the second, they extended their solutions without AI. We\ncollected Likert-scale and open-ended responses from 20 students to evaluate\ntheir perceptions and challenges. Findings suggest that AI tools were perceived\nas helpful for understanding code and increasing confidence, particularly\nduring initial development. However, students reported difficulties\ntransferring knowledge to unaided tasks, revealing possible overreliance and\ngaps in conceptual understanding. These insights highlight the need for\npedagogical strategies that integrate AI meaningfully while reinforcing\nfoundational programming skills.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22900v1", "cate": "cs.HC", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.23356", "title": "Quality Evaluation of COBOL to Java Code Transformation", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Submitted to ASE 2025", "url": "http://arxiv.org/abs/2507.23356v1", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "comment": "Submitted to ASE 2025", "pdf_url": "http://arxiv.org/pdf/2507.23356v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23546", "title": "Empirical cross-system meta-analysis of long-term transmission grid evolution", "authors": ["Bálint Hartmann", "Michelle T. Cirunay"], "categories": ["cs.SI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      26 pages", "url": "http://arxiv.org/abs/2507.23546v1", "summary": "The potential of grid-side flexibility, the latent ability to reconfigure\ntransmission network topology remains under-used partly because of the lack of\nempirical studies on how real-world grids evolve.", "comment": "26 pages", "pdf_url": "http://arxiv.org/pdf/2507.23546v1", "cate": "cs.SI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23177", "title": "InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps", "authors": ["Neagin Neasamoni Santhi", "Davide Villa", "Michele Polese", "Tommaso Melodia"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      10 pages, 12 figures, 3 tables", "url": "http://arxiv.org/abs/2507.23177v1", "summary": "Ultra-dense fifth generation (5G) and beyond networks leverage spectrum\nsharing and frequency reuse to enhance throughput, but face unpredictable\nin-band uplink (UL) interference challenges that significantly degrade Signal\nto Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases\n(gNBs). This is particularly problematic at cell edges, where overlapping\nregions force User Equipments (UEs) to increase transmit power, and in\ndirectional millimeter wave systems, where beamforming sidelobes can create\nunexpected interference. The resulting signal degradation disrupts protocol\noperations, including scheduling and resource allocation, by distorting quality\nindicators like Reference Signal Received Power (RSRP) and Received Signal\nStrength Indicator (RSSI), and can compromise critical functions such as\nchannel state reporting and Hybrid Automatic Repeat Request (HARQ)\nacknowledgments. To address this problem, this article introduces InterfO-RAN,\na real-time programmable solution that leverages a Convolutional Neural Network\n(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical\nlayer, detecting in-band interference with accuracy exceeding 91% in under 650\nus. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics\nProcessing Unit (GPU), coexisting with the 5G NR physical layer processing of\nNVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial\nRadio Units (RUs) and smartphones, our solution was trained and tested on more\nthan 7 million NR UL slots collected from real-world environments,\ndemonstrating robust interference detection capabilities essential for\nmaintaining network performance in dense deployments.", "comment": "10 pages, 12 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.23177v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.21813", "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "authors": ["Zhangcheng Qiang", "Kerry Taylor", "Weiqing Wang", "Jing Jiang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      14 pages, 4 figures, 4 tables, 2 prompt templates", "url": "http://arxiv.org/abs/2503.21813v3", "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.", "comment": "14 pages, 4 figures, 4 tables, 2 prompt templates", "pdf_url": "http://arxiv.org/pdf/2503.21813v3", "cate": "cs.CL", "date": "2025-03-25", "updated": "2025-05-14"}
{"id": "2507.23682", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "authors": ["Xiaoyu Chen", "Hangxing Wei", "Pushi Zhang", "Chuheng Zhang", "Kaixin Wang", "Yanjiang Guo", "Rushuai Yang", "Yucen Wang", "Xinquan Xiao", "Li Zhao", "Jianyu Chen", "Jiang Bian"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.23682v1", "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.", "comment": "Project page: https://aka.ms/villa-x", "pdf_url": "http://arxiv.org/pdf/2507.23682v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23136", "title": "Observational Multiplicity", "authors": ["Erin George", "Deanna Needell", "Berk Ustun"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23136v1", "summary": "Many prediction tasks can admit multiple models that can perform almost\nequally well. This phenomenon can can undermine interpretability and safety\nwhen competing models assign conflicting predictions to individuals. In this\nwork, we study how arbitrariness can arise in probabilistic classification\ntasks as a result of an effect that we call \\emph{observational multiplicity}.\nWe discuss how this effect arises in a broad class of practical applications\nwhere we learn a classifier to predict probabilities $p_i \\in [0,1]$ but are\ngiven a dataset of observations $y_i \\in \\{0,1\\}$. We propose to evaluate the\narbitrariness of individual probability predictions through the lens of\n\\emph{regret}. We introduce a measure of regret for probabilistic\nclassification tasks, which measures how the predictions of a model could\nchange as a result of different training labels change. We present a\ngeneral-purpose method to estimate the regret in a probabilistic classification\ntask. We use our measure to show that regret is higher for certain groups in\nthe dataset and discuss potential applications of regret. We demonstrate how\nestimating regret promote safety in real-world applications by abstention and\ndata collection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23136v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23150", "title": "Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery", "authors": ["Philip Wootaek Shin", "Vishal Gaur", "Rahul Ramachandran", "Manil Maskey", "Jack Sampson", "Vijaykrishnan Narayanan", "Sujit Roy"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23150v1", "summary": "High-resolution satellite imagery is essential for geospatial analysis, yet\ndifferences in spatial resolution across satellite sensors present challenges\nfor data fusion and downstream applications. Super-resolution techniques can\nhelp bridge this gap, but existing methods rely on artificially downscaled\nimages rather than real sensor data and are not well suited for heterogeneous\nsatellite sensors with differing spectral, temporal characteristics. In this\nwork, we develop a preliminary framework to align and Harmonized Landsat\nSentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a\nreference from the HLS dataset. Our approach aims to bridge the resolution gap\nbetween these sensors and improve the quality of super-resolved Landsat\nimagery. Quantitative and qualitative evaluations demonstrate the effectiveness\nof our method, showing its potential for enhancing satellite-based sensing\napplications. This study provides insights into the feasibility of\nheterogeneous satellite image super-resolution and highlights key\nconsiderations for future advancements in the field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23150v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22901", "title": "Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images", "authors": ["Shingo Hattori", "Takefumi Hiraki"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Presented at ACM SIGGRAPH Asia 2022 Posters", "url": "http://arxiv.org/abs/2507.22901v1", "summary": "Large, high-resolution displays are installed throughout the city as public\ndisplays. By superimposing invisible information on the images of these\ndisplays, large numbers of devices with cameras and sensors can communicate\nwith the displays without prior pairing. Several applications have been\nproposed, such as operating robots or communicating information to users by\ndisplaying 2D codes on images. However, the display of 2D codes has the problem\nof compromising the appearance of displayed content.\n  Abe et al. proposed a method of communicating with devices by superimposing\ninvisible information using color vibration on images displayed on\noff-the-shelf liquid-crystal displays (LCD). Using this method, we can embed\nthe information for devices in images without interfering with the displayed\ncontent. Abe et al. uses a simple serial loop operation to search for color\npairs comprising a color vibration, which requires a very long processing time\ndue to the huge search space.\n  In this paper, we propose an accelerated and optimized search method for\ncolor pairs that constitute the imperceptible color vibration for embedding\ninformation on LCD images. To achieve fast color pair search, we parallelized\nthe search process, which is previously done individually, by using arrays\nrepresenting the amount of movement and an operation to extract elements from\nthe array that satisfy the conditions. In addition, we investigate the amount\nof information that can be superimposed on nine color images using the\nimperceptible color vibration and clarify the applicability of embedding\ninformation into images using the color vibration.", "comment": "Presented at ACM SIGGRAPH Asia 2022 Posters", "pdf_url": "http://arxiv.org/pdf/2507.22901v1", "cate": "cs.HC", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.23361", "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Our code and data are available at this https URL", "url": "http://arxiv.org/abs/2507.23361v1", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "pdf_url": "http://arxiv.org/pdf/2507.23361v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23699", "title": "Exploring Left-Wing Extremism on the Decentralized Web: An Analysis of Lemmygrad.ml", "authors": ["Utkucan Balci", "Michael Sirivianos", "Jeremy Blackburn"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23699v1", "summary": "This study investigates the presence of left-wing extremism on the\nLemmygrad.ml instance of the decentralized social media platform Lemmy, from\nits launch in 2019 up to a month after the bans of the subreddits r/GenZedong\nand r/GenZhou. We conduct a temporal analysis on Lemmygrad.ml's user activity,\nwith also measuring the degree of highly abusive or hateful content.\nFurthermore, we explore the content of their posts using a transformer-based\ntopic modeling approach. Our findings reveal a substantial increase in user\nactivity and toxicity levels following the migration of these subreddits to\nLemmygrad.ml. We also identify posts that support authoritarian regimes,\nendorse the Russian invasion of Ukraine, and feature anti-Zionist and\nantisemitic content. Overall, our findings contribute to a more nuanced\nunderstanding of political extremism within decentralized social networks and\nemphasize the necessity of analyzing both ends of the political spectrum in\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23699v1", "cate": "cs.SI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23286", "title": "Optimal Packetization Towards Low Latency in Random Access Networks (extended version)", "authors": ["Zihong Li", "Anshan Yuan", "Xinghua Sun"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This article is an extended version of a paper to be presented at the IEEE 25th International Conference on Communication Technology (ICCT), Shenyang, China, October 2025", "url": "http://arxiv.org/abs/2507.23286v1", "summary": "As the demand for low-latency services grows, ensuring the delay performance\nof random access (RA) networks has become a priority. Existing studies on the\nqueueing delay performance of the Aloha model universally treat packets as\natomic transmission units, focusing primarily on delay measured in time slots.\nHowever, the impact of packetization on queueing delay has been consistently\noverlooked, particularly for the mean queueing delay measured in seconds, which\nserves as a more precise and practically relevant performance metric than its\nslot-based counterpart. Here, packetization refers to the process of\ndetermining the number of bits assembled into a packet. To optimize queueing\ndelay from the perspective of packetization, this paper establishes the\nmathematical relationship between packetization and mean queueing delay in\nseconds for both connection-free and connection-based Aloha schemes, and\nexplores the optimal packetization strategy to minimize this delay. We identify\nthe optimal mean queueing delay and its corresponding packet size via numerical\nmethods, and further analyze the influence of various network parameters. We\nfurther use simulations to investigate the similar impact of packetization on\njitter of queueing delay. We then apply our analysis to re-evaluate the complex\ntrade-off between the connection-free and connection-based schemes through the\nnew perspective of packetization. Furthermore, recognizing that an analysis of\nthe queueing delay performance for RA-SDT in NTN scenarios, especially from a\npacketization perspective, also remains an unexplored area, we apply the\nanalysis to this scenario as a case study.", "comment": "This article is an extended version of a paper to be presented at the\n  IEEE 25th International Conference on Communication Technology (ICCT),\n  Shenyang, China, October 2025", "pdf_url": "http://arxiv.org/pdf/2507.23286v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22904", "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches", "authors": ["Ehsan Latif", "Zirak Khan", "Xiaoming Zhai"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Submitted to NeurIPS2025", "url": "http://arxiv.org/abs/2507.22904v1", "summary": "Scientific sketches (e.g., models) offer a powerful lens into students'\nconceptual understanding, yet AI-powered automated assessment of such\nfree-form, visually diverse artifacts remains a critical challenge. Existing\nsolutions often treat sketch evaluation as either an image classification task\nor monolithic vision-language models, which lack interpretability, pedagogical\nalignment, and adaptability across cognitive levels. To address these\nlimitations, we present SketchMind, a cognitively grounded, multi-agent\nframework for evaluating and improving student-drawn scientific sketches.\nSketchMind comprises modular agents responsible for rubric parsing, sketch\nperception, cognitive alignment, and iterative feedback with sketch\nmodification, enabling personalized and transparent evaluation. We evaluate\nSketchMind on a curated dataset of 3,575 student-generated sketches across six\nscience assessment items with different highest order of Bloom's level that\nrequire students to draw models to explain phenomena. Compared to baseline\nGPT-4o performance without SRG (average accuracy: 55.6%), and with SRG\nintegration achieves 77.1% average accuracy (+21.4% average absolute gain). We\nalso demonstrate that multi-agent orchestration with SRG enhances SketchMind\nperformance, for example, GPT-4.1 gains an average 8.9% increase in sketch\nprediction accuracy, outperforming single-agent pipelines across all items.\nHuman evaluators rated the feedback and co-created sketches generated by\n\\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,\nsignificantly higher than those of baseline models (e.g., 2.3 for GPT-4o).\nExperts noted the system's potential to meaningfully support conceptual growth\nthrough guided revision. Our code and (pending approval) dataset will be\nreleased to support reproducibility and future research in AI-driven education.", "comment": "Submitted to NeurIPS2025", "pdf_url": "http://arxiv.org/pdf/2507.22904v1", "cate": "cs.HC", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.23698", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "authors": ["Shaofei Cai", "Zhancun Mu", "Haiwen Xia", "Bowei Zhang", "Anji Liu", "Yitao Liang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23698v1", "summary": "While Reinforcement Learning (RL) has achieved remarkable success in language\nmodeling, its triumph hasn't yet fully translated to visuomotor agents. A\nprimary challenge in RL models is their tendency to overfit specific tasks or\nenvironments, thereby hindering the acquisition of generalizable behaviors\nacross diverse settings. This paper provides a preliminary answer to this\nchallenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can\nachieve zero-shot generalization to unseen worlds. Specifically, we explore\nRL's potential to enhance generalizable spatial reasoning and interaction\ncapabilities in 3D worlds. To address challenges in multi-task RL\nrepresentation, we analyze and establish cross-view goal specification as a\nunified multi-task goal space for visuomotor policies. Furthermore, to overcome\nthe significant bottleneck of manual task design, we propose automated task\nsynthesis within the highly customizable Minecraft environment for large-scale\nmulti-task RL training, and we construct an efficient distributed RL framework\nto support this. Experimental results show RL significantly boosts interaction\nsuccess rates by $4\\times$ and enables zero-shot generalization of spatial\nreasoning across diverse environments, including real-world settings. Our\nfindings underscore the immense potential of RL training in 3D simulated\nenvironments, especially those amenable to large-scale task generation, for\nsignificantly advancing visuomotor agents' spatial reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23698v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23141", "title": "AI paradigm for solving differential equations: first-principles data generation and scale-dilation operator AI solver", "authors": ["Xiangshu Gong", "Zhiqiang Xie", "Xiaowei Jin", "Chen Wang", "Yanling Qu", "Wangmeng Zuo", "Hui Li"], "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23141v1", "summary": "Many problems are governed by differential equations (DEs). Artificial\nintelligence (AI) is a new path for solving DEs. However, data is very scarce\nand existing AI solvers struggle with approximation of high frequency\ncomponents (AHFC). We propose an AI paradigm for solving diverse DEs, including\nDE-ruled first-principles data generation methodology and scale-dilation\noperator (SDO) AI solver. Using either prior knowledge or random fields, we\ngenerate solutions and then substitute them into the DEs to derive the sources\nand initial/boundary conditions through balancing DEs, thus producing\narbitrarily vast amount of, first-principles-consistent training datasets at\nextremely low computational cost. We introduce a reversible SDO that leverages\nthe Fourier transform of the multiscale solutions to fix AHFC, and design a\nspatiotemporally coupled, attention-based Transformer AI solver of DEs with\nSDO. An upper bound on the Hessian condition number of the loss function is\nproven to be proportional to the squared 2-norm of the solution gradient,\nrevealing that SDO yields a smoother loss landscape, consequently fixing AHFC\nwith efficient training. Extensive tests on diverse DEs demonstrate that our AI\nparadigm achieves consistently superior accuracy over state-of-the-art methods.\nThis work makes AI solver of DEs to be truly usable in broad nature and\nengineering fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23141v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23162", "title": "Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues", "authors": ["Xu Cao", "Takafumi Taketomi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.23162v1", "summary": "We propose a neural inverse rendering approach that jointly reconstructs\ngeometry, spatially varying reflectance, and lighting conditions from\nmulti-view images captured under varying directional lighting. Unlike prior\nmulti-view photometric stereo methods that require light calibration or\nintermediate cues such as per-view normal maps, our method jointly optimizes\nall scene parameters from raw images in a single stage. We represent both\ngeometry and reflectance as neural implicit fields and apply shadow-aware\nvolume rendering. A spatial network first predicts the signed distance and a\nreflectance latent code for each scene point. A reflectance network then\nestimates reflectance values conditioned on the latent code and angularly\nencoded surface normal, view, and light directions. The proposed method\noutperforms state-of-the-art normal-guided approaches in shape and lighting\nestimation accuracy, generalizes to view-unaligned multi-light images, and\nhandles objects with challenging geometry and reflectance.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23162v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22903", "title": "A blessing or a burden? Exploring worker perspectives of using a social robot in a church", "authors": ["Andrew Blair", "Peggy Gregory", "Mary Ellen Foster"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted by the 2025 34th IEEE International Conference on Robot and Human Interactive Communication (ROMAN)", "url": "http://arxiv.org/abs/2507.22903v1", "summary": "Recent technological advances have allowed robots to assist in the service\nsector, and consequently accelerate job and sector transformation. Less\nattention has been paid to the use of robots in real-world organisations where\nsocial benefits, as opposed to profits, are the primary motivator. To explore\nthese opportunities, we have partnered with a working church and visitor\nattraction. We conducted interviews with 15 participants from a range of\nstakeholder groups within the church to understand worker perspectives of\nintroducing a social robot to the church and analysed the results using\nreflexive thematic analysis. Findings indicate mixed responses to the use of a\nrobot, with participants highlighting the empathetic responsibility the church\nhas towards people and the potential for unintended consequences. However,\ninformation provision and alleviation of menial or mundane tasks were\nidentified as potential use cases. This highlights the need to consider not\nonly the financial aspects of robot introduction, but also how social and\nintangible values shape what roles a robot should take on within an\norganisation.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (ROMAN)", "pdf_url": "http://arxiv.org/pdf/2507.22903v1", "cate": "cs.HC", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.23370", "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Pengfei Gao and Zhao Tian contributed equally to this technical report", "url": "http://arxiv.org/abs/2507.23370v1", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "pdf_url": "http://arxiv.org/pdf/2507.23370v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22922", "title": "Predicting stock prices with ChatGPT-annotated Reddit sentiment", "authors": ["Mateusz Kmak", "Kamil Chmurzyński", "Kamil Matejuk", "Paweł Kotzbach", "Jan Kocoń"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      International Conference on Computational Science 2025", "url": "http://arxiv.org/abs/2507.22922v1", "summary": "The surge of retail investor activity on social media, exemplified by the\n2021 GameStop short squeeze, raised questions about the influence of online\nsentiment on stock prices. This paper explores whether sentiment derived from\nsocial media discussions can meaningfully predict stock market movements. We\nfocus on Reddit's r/wallstreetbets and analyze sentiment related to two\ncompanies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's\nrole, we employ two existing text-based sentiment analysis methods and\nintroduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model\ndesigned to better interpret the informal language and emojis prevalent in\nsocial media discussions. We use correlation and causality metrics to determine\nthese models' predictive power. Surprisingly, our findings suggest that social\nmedia sentiment has only a weak correlation with stock prices. At the same\ntime, simpler metrics, such as the volume of comments and Google search trends,\nexhibit stronger predictive signals. These results highlight the complexity of\nretail investor behavior and suggest that traditional sentiment analysis may\nnot fully capture the nuances of market-moving online discussions.", "comment": "International Conference on Computational Science 2025", "pdf_url": "http://arxiv.org/pdf/2507.22922v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.23421", "title": "Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication", "authors": ["Sara Cavallero", "Fabio Saggese", "Junya Shiraishi", "Israel Leyva-Mayorga", "Shashi Raj Pandey", "Chiara Buratti", "Petar Popovski"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Communications, Copyright might be transferred without notice", "url": "http://arxiv.org/abs/2507.23421v1", "summary": "This paper introduces a dual-mode communication framework for wireless\ndevices that integrates query-driven (pull) and event-driven (push)\ntransmissions within a unified time-frame structure. Devices typically respond\nto information requests in pull mode, but if an anomaly is detected, they\npreempt the regular response to report the critical condition. Additionally,\npush-based communication is used to proactively send critical data without\nwaiting for a request. This adaptive approach ensures timely, context-aware,\nand efficient data delivery across different network conditions. To achieve\nhigh energy efficiency, we incorporate a wake-up radio mechanism and we design\na tailored medium access control (MAC) protocol that supports data traffic\nbelonging to the different communication classes. A comprehensive system-level\nanalysis is conducted, accounting for the wake-up control operation and\nevaluating three key performance metrics: the success probability of anomaly\nreports (push traffic), the success probability of query responses (pull\ntraffic) and the total energy consumption. Numerical results characterize the\nsystem's behavior and highlight the inherent trade-off in success probabilities\nbetween push- and pull-based traffic as a function of allocated communication\nresources. Our analysis demonstrates that the proposed approach reduces energy\nconsumption by up to 30% compared to a traditional approach, while maintaining\nreliable support for both communication paradigms.", "comment": "Submitted to IEEE Transactions on Communications, Copyright might be\n  transferred without notice", "pdf_url": "http://arxiv.org/pdf/2507.23421v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22906", "title": "DNN-based Methods of Jointly Sensing Number and Directions of Targets via a Green Massive H2AD MIMO Receiver", "authors": ["Bin Deng", "Jiatong Bai", "Feilong Zhao", "Zuming Xie", "Maolin Li", "Yan Wang", "Feng Shu"], "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22906v1", "summary": "As a green MIMO structure, the heterogeneous hybrid analog-digital H2AD MIMO\narchitecture has been shown to own a great potential to replace the massive or\nextremely large-scale fully-digital MIMO in the future wireless networks to\naddress the three challenging problems faced by the latter: high energy\nconsumption, high circuit cost, and high complexity. However, how to\nintelligently sense the number and direction of multi-emitters via such a\nstructure is still an open hard problem. To address this, we propose a\ntwo-stage sensing framework that jointly estimates the number and direction\nvalues of multiple targets. Specifically, three target number sensing methods\nare designed: an improved eigen-domain clustering (EDC) framework, an enhanced\ndeep neural network (DNN) based on five key statistical features, and an\nimproved one-dimensional convolutional neural network (1D-CNN) utilizing full\neigenvalues. Subsequently, a low-complexity and high-accuracy DOA estimation is\nachieved via the introduced online micro-clustering (OMC-DOA) method.\nFurthermore, we derive the Cram\\'er-Rao lower bound (CRLB) for the H2AD under\nmultiple-source conditions as a theoretical performance benchmark. Simulation\nresults show that the developed three methods achieve 100\\% number of targets\nsensing at moderate-to-high SNRs, while the improved 1D-CNN exhibits superior\nunder extremely-low SNR conditions. The introduced OMC-DOA outperforms existing\nclustering and fusion-based DOA methods in multi-source environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22906v1", "cate": "eess.SP", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.23719", "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "authors": ["Parker McDonnell", "Lingsheng Meng", "Hari Krishna Hariprasad", "Alexander Hedrick", "Eduardo Miscles", "Samuel Gilinsky", "Jean-Michel Mongeau", "Kaushik Jayaram"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23719v1", "summary": "The American cockroach (Periplaneta americana) uses its soft antennae to\nguide decision making by extracting rich tactile information from tens of\nthousands of distributed mechanosensors. Although tactile sensors enable\nrobust, autonomous perception and navigation in natural systems, replicating\nthese capabilities in insect-scale robots remains challenging due to stringent\nsize, weight, and power constraints that limit existing sensor technologies. To\novercome these limitations, we introduce CITRAS (Cockroach Inspired Tactile\nRobotic Antenna Sensor), a bioinspired, multi-segmented, compliant laminate\nsensor with embedded capacitive angle sensors. CITRAS is compact (73.7x15.6x2.1\nmm), lightweight (491 mg), and low-power (32 mW), enabling seamless integration\nwith miniature robotic platforms. The segmented compliant structure passively\nbends in response to environmental stimuli, achieving accurate hinge angle\nmeasurements with maximum errors of just 0.79 degree (quasistatic bending) and\n3.58 degree (dynamic bending). Experimental evaluations demonstrate CITRAS'\nmultifunctional tactile perception capabilities: predicting base-to-tip\ndistances with 7.75 % error, estimating environmental gap widths with 6.73 %\nerror, and distinguishing surface textures through differential sensor\nresponse. The future integration of this bioinspired tactile antenna in\ninsect-scale robots addresses critical sensing gaps, promising enhanced\nautonomous exploration, obstacle avoidance, and environmental mapping in\ncomplex, confined environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23719v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23154", "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in the 2025 International Conference on Machine Intelligence for GeoAnalytics and Remote Sensing (MIGARS)", "url": "http://arxiv.org/abs/2507.23154v1", "summary": "Urban heatwaves, droughts, and land degradation are pressing and growing\nchallenges in the context of climate change. A valuable approach to studying\nthem requires accurate spatio-temporal information on land surface conditions.\nOne of the most important variables for assessing and understanding these\nphenomena is Land Surface Temperature (LST), which is derived from satellites\nand provides essential information about the thermal state of the Earth's\nsurface. However, satellite platforms inherently face a trade-off between\nspatial and temporal resolutions. To bridge this gap, we propose FuseTen, a\nnovel generative framework that produces daily LST observations at a fine 10 m\nspatial resolution by fusing spatio-temporal observations derived from\nSentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative\narchitecture trained using an averaging-based supervision strategy grounded in\nphysical principles. It incorporates attention and normalization modules within\nthe fusion process and uses a PatchGAN discriminator to enforce realism.\nExperiments across multiple dates show that FuseTen outperforms linear\nbaselines, with an average 32.06% improvement in quantitative metrics and\n31.42% in visual fidelity. To the best of our knowledge, this is the first\nnon-linear method to generate daily LST estimates at such fine spatial\nresolution.", "comment": "Accepted in the 2025 International Conference on Machine Intelligence\n  for GeoAnalytics and Remote Sensing (MIGARS)", "pdf_url": "http://arxiv.org/pdf/2507.23154v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23174", "title": "CNN-based solution for mango classification in agricultural environments", "authors": ["Beatriz Díaz Peón", "Jorge Torres Gómez", "Ariel Fajardo Márquez"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23174v1", "summary": "This article exemplifies the design of a fruit detection and classification\nsystem using Convolutional\n  Neural Networks (CNN). The goal is to develop a system that automatically\nassesses fruit quality for\n  farm inventory management. Specifically, a method for mango fruit\nclassification was developed using\n  image processing, ensuring both accuracy and efficiency. Resnet-18 was\nselected as the preliminary\n  architecture for classification, while a cascade detector was used for\ndetection, balancing execution speed\n  and computational resource consumption. Detection and classification results\nwere displayed through a\n  graphical interface developed in MatLab App Designer, streamlining system\ninteraction. The integration\n  of convolutional neural networks and cascade detectors proffers a reliable\nsolution for fruit classification\n  and detection, with potential applications in agricultural quality control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23174v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22905", "title": "Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22905v1", "summary": "As large language models (LLMs) become increasingly integrated into robotic\nsystems, their potential to generate socially and culturally appropriate\naffective touch remains largely unexplored. This study investigates whether\nLLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive\ntactile behaviours to convey emotions in human-robot interaction. We produced\ntext based touch descriptions for 12 distinct emotions across three cultural\ncontexts (Chinese, Belgian, and unspecified), and examined their\ninterpretability in both robot-to-human and human-to-robot scenarios. A total\nof 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)\nevaluated these LLM-generated tactile behaviours for emotional decoding and\nperceived appropriateness. Results reveal that: (1) under matched cultural\nconditions, participants successfully decoded six out of twelve emotions-mainly\nsocially oriented emotions such as love and Ekman emotions such as anger,\nhowever, self-focused emotions like pride and embarrassment were more difficult\nto interpret; (2) tactile behaviours were perceived as more appropriate when\ndirected from human to robot than from robot to human, revealing an asymmetry\nin social expectations based on interaction roles; (3) behaviours interpreted\nas aggressive (e.g., anger), overly intimate (e.g., love), or emotionally\nambiguous (i.e., not clearly decodable) were significantly more likely to be\nrated as inappropriate; and (4) cultural mismatches reduced decoding accuracy\nand increased the likelihood of behaviours being judged as inappropriate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22905v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02"}
{"id": "2507.23425", "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "authors": ["Daphné Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      9 pages, 9 figures", "url": "http://arxiv.org/abs/2507.23425v1", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "comment": "9 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.23425v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22930", "title": "Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection", "authors": ["Shalini Jangra", "Suparna De", "Nishanth Sastry", "Saeed Fadaei"], "categories": ["cs.CL", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages, 4 Figures, Accepted in \"The 17th International Conference on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "url": "http://arxiv.org/abs/2507.22930v1", "summary": "Social platforms such as Reddit have a network of communities of shared\ninterests, with a prevalence of posts and comments from which one can infer\nusers' Personal Information Identifiers (PIIs). While such self-disclosures can\nlead to rewarding social interactions, they pose privacy risks and the threat\nof online harms. Research into the identification and retrieval of such risky\nself-disclosures of PIIs is hampered by the lack of open-source labeled\ndatasets. To foster reproducible research into PII-revealing text detection, we\ndevelop a novel methodology to create synthetic equivalents of PII-revealing\ndata that can be safely shared. Our contributions include creating a taxonomy\nof 19 PII-revealing categories for vulnerable populations and the creation and\nrelease of a synthetic PII-labeled multi-text span dataset generated from 3\ntext generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and\nzephyr-7b-beta, with sequential instruction prompting to resemble the original\nReddit posts. The utility of our methodology to generate this synthetic dataset\nis evaluated with three metrics: First, we require reproducibility equivalence,\ni.e., results from training a model on the synthetic data should be comparable\nto those obtained by training the same models on the original posts. Second, we\nrequire that the synthetic data be unlinkable to the original users, through\ncommon mechanisms such as Google Search. Third, we wish to ensure that the\nsynthetic data be indistinguishable from the original, i.e., trained humans\nshould not be able to tell them apart. We release our dataset and code at\nhttps://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster\nreproducible research into PII privacy risks in online social media.", "comment": "15 pages, 4 Figures, Accepted in \"The 17th International Conference\n  on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "pdf_url": "http://arxiv.org/pdf/2507.22930v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.23433", "title": "From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks", "authors": ["Erfan Delfani", "Nikolaos Pappas"], "categories": ["cs.NI", "cs.IT", "math.IT"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23433v1", "summary": "Timely and informative data dissemination in communication networks is\nessential for enhancing system performance and energy efficiency, as it reduces\nthe transmission of outdated or redundant data. Timeliness metrics, such as Age\nof Information (AoI), effectively quantify data freshness; however, these\nmetrics fail to account for the intrinsic informativeness of the content\nitself. To address this limitation, content-based metrics have been proposed\nthat combine both timeliness and informativeness. Nevertheless, existing\nstudies have predominantly focused on evaluating average metric values, leaving\nthe complete distribution-particularly in multi-hop network scenarios-largely\nunexplored. In this paper, we provide a comprehensive analysis of the\nstationary distribution of the Version Age of Information (VAoI), a\ncontent-based metric, under various scheduling policies, including randomized\nstationary, uniform, and threshold-based policies, with transmission\nconstraints in single-hop and multi-hop networks. We derive closed-form\nexpressions for the stationary distribution and average VAoI under these\nscheduling approaches. Furthermore, for threshold-based scheduling, we\nanalytically determine the optimal threshold value that minimizes VAoI and\nderive the corresponding optimal VAoI in closed form. Numerical evaluations\nverify our analytical findings, providing valuable insights into leveraging\nVAoI in the design of efficient communication networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23433v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23029", "title": "A CPFSK Transceiver with Hybrid CSS-DSSS Spreading for LPWAN PHY Communication", "authors": ["Wenkun Wen", "Ruiqi Zhang", "Peiran Wu", "Tierui Min", "Minghua Xia"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      15 pages, 12 figures, and 4 tables. To appear in IEEE Internet of Things Journal", "url": "http://arxiv.org/abs/2507.23029v1", "summary": "Traditional low-power wide-area network (LPWAN) transceivers typically\ncompromise data rates to achieve deep coverage. This paper presents a novel\ntransceiver that achieves high receiver sensitivity and low computational\ncomplexity. At the transmitter, we replace the conventional direct sequence\nspread spectrum (DSSS) preamble with a chirp spread spectrum (CSS) preamble,\nconsisting of a pair of down-chirp and up-chirp signals that are conjugate to\neach other, simplifying packet synchronization. For enhanced coverage, the\npayload incorporates continuous phase frequency shift keying (CPFSK) to\nmaintain a constant envelope and phase continuity, in conjunction with DSSS to\nachieve a high spreading gain. At the receiver, we develop a double-peak\ndetection method to improve synchronization and a non-coherent joint\ndespreading and demodulation scheme that increases receiver sensitivity while\nmaintaining simplicity in implementation. Furthermore, we optimize the preamble\ndetection threshold and spreading sequences for maximum non-coherent receiver\nperformance. The software-defined radio (SDR) prototype, developed using GNU\nRadio and USRP, along with operational snapshots, showcases its practical\nengineering applications. Extensive Monte Carlo simulations and field-test\ntrials demonstrate that our transceiver outperforms traditional ones in terms\nof receiver sensitivity, while also being low in complexity and cost-effective\nfor LPWAN requirements.", "comment": "15 pages, 12 figures, and 4 tables. To appear in IEEE Internet of\n  Things Journal", "pdf_url": "http://arxiv.org/pdf/2507.23029v1", "cate": "cs.IT", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22908", "title": "A Privacy-Preserving Federated Framework with Hybrid Quantum-Enhanced Learning for Financial Fraud Detection", "authors": ["Abhishek Sawaika", "Swetang Krishna", "Tushar Tomar", "Durga Pritam Suggisetti", "Aditi Lal", "Tanmaya Shrivastav", "Nouhaila Innan", "Muhammad Shafique"], "categories": ["q-fin.CP", "cs.AI", "cs.LG", "I.2"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      To be published in proceedings of IEEE International Conference on Quantum Computing and Engineering (QCE) 2025", "url": "http://arxiv.org/abs/2507.22908v1", "summary": "Rapid growth of digital transactions has led to a surge in fraudulent\nactivities, challenging traditional detection methods in the financial sector.\nTo tackle this problem, we introduce a specialised federated learning framework\nthat uniquely combines a quantum-enhanced Long Short-Term Memory (LSTM) model\nwith advanced privacy preserving techniques. By integrating quantum layers into\nthe LSTM architecture, our approach adeptly captures complex\ncross-transactional patters, resulting in an approximate 5% performance\nimprovement across key evaluation metrics compared to conventional models.\nCentral to our framework is \"FedRansel\", a novel method designed to defend\nagainst poisoning and inference attacks, thereby reducing model degradation and\ninference accuracy by 4-8%, compared to standard differential privacy\nmechanisms. This pseudo-centralised setup with a Quantum LSTM model, enhances\nfraud detection accuracy and reinforces the security and confidentiality of\nsensitive financial data.", "comment": "To be published in proceedings of IEEE International Conference on\n  Quantum Computing and Engineering (QCE) 2025", "pdf_url": "http://arxiv.org/pdf/2507.22908v1", "cate": "q-fin.CP", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.23078", "title": "Experimentally-Driven Analysis of Stability in Connected Vehicle Platooning: Insights and Control Strategies", "authors": ["Niladri Dutta", "Elham Abolfazli", "Themistoklis Charalambous"], "categories": ["eess.SY", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23078v1", "summary": "This paper presents the development of a tangible platform for demonstrating\nthe practical implementation of cooperative adaptive cruise control (CACC)\nsystems, an enhancement to the standard adaptive cruise control (ACC) concept\nby means of Vehicle-to-Everything (V2X) communication. It involves a detailed\nexamination of existing longitudinal controllers and their performance in\nhomogeneous vehicle platoons. Moreover, extensive tests are conducted using\nmultiple autonomous experimental vehicle platform topologies to verify the\neffectiveness of the controller. The outcomes from both simulations and field\ntests affirm the substantial benefits of the proposed CACC platooning approach\nin longitudinal vehicle platooning scenarios. This research is crucial due to a\nnotable gap in the existing literature; while numerous studies focus on\nsimulated vehicle platooning systems, there is lack of research demonstrating\nthese controllers on physical vehicle systems or robot platforms. This paper\nseeks to fill this gap by providing a practical demonstration of CACC systems\nin action, showcasing their potential for real-world application in intelligent\ntransportation systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23078v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23170", "title": "BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning", "authors": ["Jinan Zhou", "Rajat Ghosh", "Vaishnavi Bhargava", "Debojyoti Dutta", "Aryan Singhal"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23170v1", "summary": "When designing LLM services, practitioners care about three key properties:\ninference-time budget, factual authenticity, and reasoning capacity. However,\nour analysis shows that no model can simultaneously optimize for all three. We\nformally prove this trade-off and propose a principled framework named The BAR\nTheorem for LLM-application design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23170v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23185", "title": "Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network", "authors": ["Jongwook Si", "Sungyoung Kim"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2507.23185v1", "summary": "The problem of single-image rain streak removal goes beyond simple noise\nsuppression, requiring the simultaneous preservation of fine structural details\nand overall visual quality. In this study, we propose a novel image restoration\nnetwork that effectively constrains the restoration process by introducing a\nCorner Loss, which prevents the loss of object boundaries and detailed texture\ninformation during restoration. Furthermore, we propose a Residual\nConvolutional Block Attention Module (R-CBAM) Block into the encoder and\ndecoder to dynamically adjust the importance of features in both spatial and\nchannel dimensions, enabling the network to focus more effectively on regions\nheavily affected by rain streaks. Quantitative evaluations conducted on the\nRain100L and Rain100H datasets demonstrate that the proposed method\nsignificantly outperforms previous approaches, achieving a PSNR of 33.29 dB on\nRain100L and 26.16 dB on Rain100H.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2507.23185v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22952", "title": "Automated Label Placement on Maps via Large Language Models", "authors": ["Harry Shomer", "Jiejun Xu"], "categories": ["cs.HC", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Workshop on AI for Data Editing (AI4DE) at KDD 2025", "url": "http://arxiv.org/abs/2507.22952v1", "summary": "Label placement is a critical aspect of map design, serving as a form of\nspatial annotation that directly impacts clarity and interpretability. Despite\nits importance, label placement remains largely manual and difficult to scale,\nas existing automated systems struggle to integrate cartographic conventions,\nadapt to context, or interpret labeling instructions. In this work, we\nintroduce a new paradigm for automatic label placement (ALP) that formulates\nthe task as a data editing problem and leverages large language models (LLMs)\nfor context-aware spatial annotation. To support this direction, we curate\nMAPLE, the first known benchmarking dataset for evaluating ALP on real-world\nmaps, encompassing diverse landmark types and label placement annotations from\nopen-source data. Our method retrieves labeling guidelines relevant to each\nlandmark type leveraging retrieval-augmented generation (RAG), integrates them\ninto prompts, and employs instruction-tuned LLMs to generate ideal label\ncoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall\nperformance and generalization across different types of landmarks. This\nincludes both zero-shot and instruction-tuned performance. Our results\ndemonstrate that LLMs, when guided by structured prompts and domain-specific\nretrieval, can learn to perform accurate spatial edits, aligning the generated\noutputs with expert cartographic standards. Overall, our work presents a\nscalable framework for AI-assisted map finishing and demonstrates the potential\nof foundation models in structured data editing tasks. The code and data can be\nfound at https://github.com/HarryShomer/MAPLE.", "comment": "Workshop on AI for Data Editing (AI4DE) at KDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.22952v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23640", "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23640v1", "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23640v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.14422", "title": "MindVote: When AI Meets the Wild West of Social Media Opinion", "authors": ["Xutao Mao", "Ezra Xuanru Tao", "Leyao Wang"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.14422v3", "summary": "Large Language Models (LLMs) are increasingly used as scalable tools for\npilot testing, predicting public opinion distributions before deploying costly\nsurveys. To serve as effective pilot testing tools, the performance of these\nLLMs is typically benchmarked against their ability to reproduce the outcomes\nof past structured surveys. This evaluation paradigm, however, is misaligned\nwith the dynamic, context-rich social media environments where public opinion\nis increasingly formed and expressed. By design, surveys strip away the social,\ncultural, and temporal context that shapes public opinion, and LLM benchmarks\nbuilt on this paradigm inherit these critical limitations. To bridge this gap,\nwe introduce MindVote, the first benchmark for public opinion distribution\nprediction grounded in authentic social media discourse. MindVote is\nconstructed from 3,918 naturalistic polls sourced from Reddit and Weibo,\nspanning 23 topics and enriched with detailed annotations for platform,\ntopical, and temporal context. Using this benchmark, we conduct a comprehensive\nevaluation of 15 LLMs. MindVote provides a robust, ecologically valid framework\nto move beyond survey-based evaluations and advance the development of more\nsocially intelligent AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.14422v3", "cate": "cs.SI", "date": "2025-05-20", "updated": "2025-07-31"}
{"id": "2507.23556", "title": "Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching", "authors": ["Botao Zhu", "Xianbin Wang"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23556v1", "summary": "Due to the diverse physical attributes of computing resources and tasks,\ndeveloping effective mechanisms to facilitate task and resource matching in\ncomplex connected systems for value-oriented task completion has become\nincreasingly challenging. To address the challenge, this paper proposes a\nnetworked physical computing system that integrates the physical attributes of\ncomputing resources and tasks as well as task-specific trust relationships\namong devices to enable value-driven task completion. Specifically, we propose\na state-of-the-art hypergraph-aided trusted task-resource matching\n(TTR-matching) framework to achieve the envisioned physical computing. First, a\ntask-specific trusted physical resource hypergraph is defined, which integrates\ntask-specific trust, the physical attributes of resources, and task types. This\nenables accurate modeling of device collaboration dependencies under specific\ntask types. Next, a task hypergraph is generated to associate the task\ninitiator with the physical attributes of the corresponding tasks. Based on\nthese two hypergraphs, a hypergraph matching algorithm is designed to\nfacilitate task-specific trusted collaborator selection and accurate\ntask-resource matching for value-maximizing task completion. Extensive\nexperimental results demonstrate that the proposed TTR-matching framework\noutperforms comparison algorithms in identifying task-specific trustworthy\ncollaborators and maximizing the average value of task completion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23556v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23175", "title": "Optimal compressed sensing for mixing stochastic processes", "authors": ["Yonatan Gutman", "Adam Śpiewak"], "categories": ["cs.IT", "math.DS", "math.IT", "math.PR", "68P30, 94A29, 31E05, 37A35, 60G10"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23175v1", "summary": "Jalali and Poor introduced an asymptotic framework for compressed sensing of\nstochastic processes, demonstrating that any rate strictly greater than the\nmean information dimension serves as an upper bound on the number of random\nlinear measurements required for (universal) almost lossless recovery of\n$\\psi^*$-mixing processes, as measured in the normalized $L^2$ norm. In this\nwork, we show that if the normalized number of random linear measurements is\nstrictly less than the mean information dimension, then almost lossless\nrecovery of a $\\psi^*$-mixing process is impossible by any sequence of\ndecompressors. This establishes the mean information dimension as the\nfundamental limit for compressed sensing in this setting (and, in fact, the\nprecise threshold for the problem). To this end, we introduce a new quantity,\nrelated to techniques from geometric measure theory: the correlation dimension\nrate, which is shown to be a lower bound for compressed sensing of arbitrary\nstationary stochastic processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23175v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22910", "title": "Large Language Models in the Travel Domain: An Industrial Experience", "authors": ["Sergio Di Meglio", "Aniello Somma", "Luigi Libero Lucio Starace", "Fabio Scippacercola", "Giancarlo Sperlì", "Sergio Di Martino"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Manuscript accepted to the International Conference on Software Engineering and Knowledge Engineering (SEKE) 2025", "url": "http://arxiv.org/abs/2507.22910v1", "summary": "Online property booking platforms are widely used and rely heavily on\nconsistent, up-to-date information about accommodation facilities, often\nsourced from third-party providers. However, these external data sources are\nfrequently affected by incomplete or inconsistent details, which can frustrate\nusers and result in a loss of market. In response to these challenges, we\npresent an industrial case study involving the integration of Large Language\nModels (LLMs) into CALEIDOHOTELS, a property reservation platform developed by\nFERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,\nfine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.\nBoth models were assessed based on their ability to generate consistent and\nhomogeneous descriptions while minimizing hallucinations. Mixtral 8x7B\noutperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision\n(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet\nmore concise content (249 vs. 277 words on average). However, this came at a\nsignificantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB\nand $0.16/hour for Mistral 7B. Our findings provide practical insights into the\ntrade-offs between model quality and resource efficiency, offering guidance for\ndeploying LLMs in production environments and demonstrating their effectiveness\nin enhancing the consistency and reliability of accommodation data.", "comment": "Manuscript accepted to the International Conference on Software\n  Engineering and Knowledge Engineering (SEKE) 2025", "pdf_url": "http://arxiv.org/pdf/2507.22910v1", "cate": "cs.CL", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.23391", "title": "Policy Learning from Large Vision-Language Model Feedback without Reward Modeling", "authors": ["Tung M. Luu", "Donghoon Lee", "Younghwan Lee", "Chang D. Yoo"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025", "url": "http://arxiv.org/abs/2507.23391v1", "summary": "Offline reinforcement learning (RL) provides a powerful framework for\ntraining robotic agents using pre-collected, suboptimal datasets, eliminating\nthe need for costly, time-consuming, and potentially hazardous online\ninteractions. This is particularly useful in safety-critical real-world\napplications, where online data collection is expensive and impractical.\nHowever, existing offline RL algorithms typically require reward labeled data,\nwhich introduces an additional bottleneck: reward function design is itself\ncostly, labor-intensive, and requires significant domain expertise. In this\npaper, we introduce PLARE, a novel approach that leverages large\nvision-language models (VLMs) to provide guidance signals for agent training.\nInstead of relying on manually designed reward functions, PLARE queries a VLM\nfor preference labels on pairs of visual trajectory segments based on a\nlanguage task description. The policy is then trained directly from these\npreference labels using a supervised contrastive preference learning objective,\nbypassing the need to learn explicit reward models. Through extensive\nexperiments on robotic manipulation tasks from the MetaWorld, PLARE achieves\nperformance on par with or surpassing existing state-of-the-art VLM-based\nreward generation methods. Furthermore, we demonstrate the effectiveness of\nPLARE in real-world manipulation tasks with a physical robot, further\nvalidating its practical applicability.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.23391v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23186", "title": "NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions", "authors": ["Peter Sharpe"], "categories": ["cs.LG", "cs.PL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23186v1", "summary": "Sparsity detection in black-box functions enables significant computational\nspeedups in gradient-based optimization through Jacobian compression, but\nexisting finite-difference methods suffer from false negatives due to\ncoincidental zero gradients. These false negatives can silently corrupt\ngradient calculations, leading to difficult-to-diagnose errors. We introduce\nNaN-propagation, which exploits the universal contamination property of IEEE\n754 Not-a-Number floating-point values to trace input-output dependencies\nthrough floating-point numerical computations. By systematically contaminating\ninputs with NaN and observing which outputs become NaN, the method reconstructs\nconservative sparsity patterns that eliminate false negatives. We demonstrate\nthe approach on an aerospace wing weight model, achieving a 1.52x speedup while\ndetecting dozens of dependencies missed by conventional methods -- a\nsignificant improvement since gradient computation is the bottleneck in many\noptimization workflows. The technique leverages IEEE 754 compliance to work\nacross programming languages and math libraries without modifying existing\nblack-box codes. Advanced strategies including NaN payload encoding enable\nfaster-than-linear time complexity, improving upon existing black-box sparsity\ndetection methods. Practical algorithms are also proposed to mitigate\nchallenges from branching code execution common in engineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23186v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23188", "title": "Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space", "authors": ["Shiyao Yu", "Zi-An Wang", "Kangning Yin", "Zheng Tian", "Mingyuan Zhang", "Weixin Si", "Shihao Zou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE TMM 2025", "url": "http://arxiv.org/abs/2507.23188v1", "summary": "Motion retrieval is crucial for motion acquisition, offering superior\nprecision, realism, controllability, and editability compared to motion\ngeneration. Existing approaches leverage contrastive learning to construct a\nunified embedding space for motion retrieval from text or visual modality.\nHowever, these methods lack a more intuitive and user-friendly interaction mode\nand often overlook the sequential representation of most modalities for\nimproved retrieval performance. To address these limitations, we propose a\nframework that aligns four modalities -- text, audio, video, and motion --\nwithin a fine-grained joint embedding space, incorporating audio for the first\ntime in motion retrieval to enhance user immersion and convenience. This\nfine-grained space is achieved through a sequence-level contrastive learning\napproach, which captures critical details across modalities for better\nalignment. To evaluate our framework, we augment existing text-motion datasets\nwith synthetic but diverse audio recordings, creating two multi-modal motion\nretrieval datasets. Experimental results demonstrate superior performance over\nstate-of-the-art methods across multiple sub-tasks, including an 10.16%\nimprovement in R@10 for text-to-motion retrieval and a 25.43% improvement in\nR@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our\nresults show that our 4-modal framework significantly outperforms its 3-modal\ncounterpart, underscoring the potential of multi-modal motion retrieval for\nadvancing motion acquisition.", "comment": "Accepted by IEEE TMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23188v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23096", "title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23096v1", "summary": "Large language models (LLMs) are rapidly increasing in capability, but they\nstill struggle with highly specialized programming tasks such as scientific\nvisualization. We present an LLM assistant, ChatVis, that aids the LLM to\ngenerate Python code for ParaView scientific visualization tasks, without the\nneed for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought\nprompt simplification, retrieval-augmented prompt generation using a vector\ndatabase of documentation and code examples, and error checking with iterative\nprompt feedback to correct errors until a visualization is produced. An\nintegral part of our approach is a benchmark suite of canonical visualization\ntasks, ParaView regression tests, and scientific use cases that includes\ncomprehensive evaluation metrics. We evaluate our visualization assistant by\ncomparing results with a variety of top-performing unassisted LLMs. We find\nthat all the metrics are significantly improved with ChatVis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23096v1", "cate": "cs.HC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "1908.01212", "title": "Typing Tensor Calculus in 2-Categories (I)", "authors": ["Fatimah Rita Ahmadi"], "categories": ["math.CT", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Category Theory (math.CT)", "pdf_link": null, "comments": "Comments:      28 pages; extended introduction, more explanation", "url": "http://arxiv.org/abs/1908.01212v4", "summary": "To formalize calculations in linear algebra for the development of efficient\nalgorithms and a framework suitable for functional programming languages and\nfaster parallelized computations, we adopt an approach that treats elements of\nlinear algebra, such as matrices, as morphisms in the category of matrices,\n$\\mathbf{Mat_{k}}$. This framework is further extended by generalizing the\nresults to arbitrary monoidal semiadditive categories. To enrich this\nperspective and accommodate higher-rank matrices (tensors), we define\nsemiadditive 2-categories, where matrices $T_{ij}$ are represented as\n1-morphisms, and tensors with four indices $T_{ijkl}$ as 2-morphisms. This\nformalization provides an index-free, typed linear algebra framework that\nincludes matrices and tensors with up to four indices. Furthermore, we extend\nthe framework to monoidal semiadditive 2-categories and demonstrate detailed\noperations and vectorization within the 2-category of 2Vec introduced by\nKapranov and Voevodsky.", "comment": "28 pages; extended introduction, more explanation", "pdf_url": "http://arxiv.org/pdf/1908.01212v4", "cate": "math.CT", "date": "2019-08-03", "updated": "2025-01-09"}
{"id": "2505.20980", "title": "Identifying Super Spreaders in Multilayer Networks", "authors": ["Michał Czuba", "Mateusz Stolarski", "Adam Piróg", "Piotr Bielak", "Piotr Bródka"], "categories": ["cs.SI", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20980v2", "summary": "Identifying super-spreaders can be framed as a subtask of the influence\nmaximisation problem. It seeks to pinpoint agents within a network that, if\nselected as single diffusion seeds, disseminate information most effectively.\nMultilayer networks, a specific class of heterogeneous graphs, can capture\ndiverse types of interactions (e.g., physical-virtual or professional-social),\nand thus offer a more accurate representation of complex relational structures.\nIn this work, we introduce a novel approach to identifying super-spreaders in\nsuch networks by leveraging graph neural networks. To this end, we construct a\ndataset by simulating information diffusion across hundreds of networks - to\nthe best of our knowledge, the first of its kind tailored specifically to\nmultilayer networks. We further formulate the task as a variation of the\nranking prediction problem based on a four-dimensional vector that quantifies\neach agent's spreading potential: (i) the number of activations; (ii) the\nduration of the diffusion process; (iii) the peak number of activations; and\n(iv) the simulation step at which this peak occurs. Our model,\nTopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom\naggregation layer. This design enables generalisation to previously unseen data\nand adapts to varying graph sizes. In an extensive evaluation, we compare our\nmodel against classic centrality-based heuristics and competitive deep learning\nmethods. The results, obtained across a broad spectrum of real-world and\nsynthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves\nsuperior performance in identifying high-impact nodes, while also offering\nimproved interpretability through its structured output.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20980v2", "cate": "cs.SI", "date": "2025-05-27", "updated": "2025-07-31"}
{"id": "2402.03158", "title": "Optimal and Near-Optimal Adaptive Vector Quantization", "authors": ["Ran Ben-Basat", "Yaniv Ben-Itzhak", "Michael Mitzenmacher", "Shay Vargaftik"], "categories": ["cs.LG", "cs.DS", "cs.IT", "cs.NI", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.03158v2", "summary": "Quantization is a fundamental optimization for many machine-learning use\ncases, including compressing gradients, model weights and activations, and\ndatasets. The most accurate form of quantization is \\emph{adaptive}, where the\nerror is minimized with respect to a given input, rather than optimizing for\nthe worst case. However, optimal adaptive quantization methods are considered\ninfeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present\nalgorithms that find optimal solutions with asymptotically improved time and\nspace complexity. We also present an even faster near-optimal algorithm for\nlarge inputs. Our experiments show our algorithms may open the door to using\nAVQ more extensively in a variety of machine learning applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.03158v2", "cate": "cs.LG", "date": "2024-02-05", "updated": "2025-07-31"}
{"id": "2507.23180", "title": "The Construction of Near-optimal Universal Coding of Integers", "authors": ["Wei Yan", "Yunghsiang S. Han"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23180v1", "summary": "Universal Coding of Integers (UCI) is suitable for discrete memoryless\nsources with unknown probability distributions and infinitely countable\nalphabet sizes. The UCI is a class of prefix codes, such that the ratio of the\naverage codeword length to $\\max\\{1, H(P)\\}$ is within a constant expansion\nfactor $K_{\\mathcal{C}}$ for any decreasing probability distribution $P$, where\n$H(P)$ is the entropy of $P$. For any UCI code $\\mathcal{C}$, define \\emph{the\nminimum expansion factor} $K_{\\mathcal{C}}^{*}$ to represent the infimum of the\nset of extension factors of $\\mathcal{C}$. Each $\\mathcal{C}$ has a unique\ncorresponding $K_{\\mathcal{C}}^{*}$, and the smaller $K_{\\mathcal{C}}^{*}$ is,\nthe better the compression performance of $\\mathcal{C}$ is. A class of UCI\n$\\mathcal{C}$ (or family $\\{\\mathcal{C}_i\\}_{i=1}^{\\infty}$) achieving the\nsmallest $K_{\\mathcal{C}}^{*}$ is defined as the \\emph{optimal UCI}. The best\nresult currently is that the range of $C_{\\mathcal{C}}^{*}$ for the optimal UCI\nis $2\\leq C_{\\mathcal{C}}^{*}\\leq 2.5$. In this paper, we prove that there\nexists a class of near-optimal UCIs, called $\\nu$ code, to achieve\n$K_\\nu=2.0386$. This narrows the range of the minimum expansion factor for\noptimal UCI to $2\\leq C_{\\mathcal{C}}^{*}\\leq 2.0386$. Another new class of\nUCI, called $\\Delta\\delta$ code, is specifically constructed. We show that the\n$\\Delta\\delta$ code and $\\nu$ code are currently optimal in terms of minimum\nexpansion factor. In addition, we propose a new proof that shows the minimum\nexpansion factor of the optimal UCI is lower bounded by $2$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23180v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22911", "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing", "authors": ["Jinzhi Wang", "Qingke Peng", "Haozhou Li", "Zeyuan Zeng", "Qinfeng Song", "Kaixuan Yang", "Jiangbo Zhang", "Yaoying Wang", "Ruimeng Li", "Biyi Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22911v1", "summary": "Electric power marketing customer service plays a critical role in addressing\ninquiries, complaints, and service requests. However, current systems, such as\nChina's 95598 hotline, often struggle with slow response times, inflexible\nprocedures, and limited accuracy in domain-specific tasks. While large language\nmodels (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,\nthey lack the domain expertise and empathy required in this field. To bridge\nthis gap, we introduce ElectriQ, the first benchmark designed to evaluate and\nenhance LLMs in electric power marketing scenarios. ElectriQ consists of a\ndialogue dataset covering six key service categories and introduces four\nevaluation metrics: professionalism, popularity, readability, and\nuser-friendliness. We further incorporate a domain-specific knowledge base and\npropose a knowledge augmentation method to boost model performance. Experiments\non 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and\naugmented, can surpass GPT-4o in terms of professionalism and\nuser-friendliness. ElectriQ establishes a comprehensive foundation for\ndeveloping LLMs tailored to the needs of power marketing services.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22911v1", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.23487", "title": "Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions", "authors": ["Jinshan Zhen", "Yuanyue Ge", "Tianxiao Zhu", "Hui Zhao", "Ya Xiong"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2507.23487v1", "summary": "Accurate mass estimation of table-top grown strawberries under field\nconditions remains challenging due to frequent occlusions and pose variations.\nThis study proposes a vision-based pipeline integrating RGB-D sensing and deep\nlearning to enable non-destructive, real-time and online mass estimation. The\nmethod employed YOLOv8-Seg for instance segmentation, Cycle-consistent\ngenerative adversarial network (CycleGAN) for occluded region completion, and\ntilt-angle correction to refine frontal projection area calculations. A\npolynomial regression model then mapped the geometric features to mass.\nExperiments demonstrated mean mass estimation errors of 8.11% for isolated\nstrawberries and 10.47% for occluded cases. CycleGAN outperformed large mask\ninpainting (LaMa) model in occlusion recovery, achieving superior pixel area\nratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU)\nscores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical\nlimitations of traditional methods, offering a robust solution for automated\nharvesting and yield monitoring with complex occlusion patterns.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.23487v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23217", "title": "Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation", "authors": ["Hyeon Seong Jeong", "Sangwoo Jo", "Byeong Hyun Yoon", "Yoonseok Heo", "Haedong Jeong", "Taehoon Kim"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23217v1", "summary": "Understanding complex multimodal documents remains challenging due to their\nstructural inconsistencies and limited training data availability. We introduce\n\\textit{DocsRay}, a training-free document understanding system that integrates\npseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented\nGeneration (RAG). Our approach leverages multimodal Large Language Models'\n(LLMs) native capabilities to seamlessly process documents containing diverse\nelements such as text, images, charts, and tables without requiring specialized\nmodels or additional training. DocsRay's framework synergistically combines\nthree key techniques: (1) a semantic structuring module using prompt-based LLM\ninteractions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal\nanalysis that converts diverse document elements into unified, text-centric\nrepresentations using the inherent capabilities of multimodal LLMs, and (3) an\nefficient two-stage hierarchical retrieval system that reduces retrieval\ncomplexity from $O(N)$ to $O(S + k_1 \\cdot N_s)$. Evaluated on documents\naveraging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency\nfrom 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the\nMMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%,\nsubstantially surpassing previous state-of-the-art results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23217v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23193", "title": "A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery", "authors": ["Youngsun Jang", "Dongyoun Kim", "Chulwoo Pack", "Kwanghee Won"], "categories": ["cs.CV", "I.4.6; I.2.10; I.5.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures. Presented at ACM RACS 2024 (Pompei, Italy, Nov 5-8, 2024)", "url": "http://arxiv.org/abs/2507.23193v1", "summary": "This study introduces a novel dataset for segmenting flooded areas in\nsatellite images. After reviewing 77 existing benchmarks utilizing satellite\nimagery, we identified a shortage of suitable datasets for this specific task.\nTo fill this gap, we collected satellite imagery of the 2019 Midwestern USA\nfloods from Planet Explorer by Planet Labs (Image \\c{opyright} 2024 Planet Labs\nPBC). The dataset consists of 10 satellite images per location, each containing\nboth flooded and non-flooded areas. We selected ten locations from each of the\nfive states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset\nensures uniform resolution and resizing during data processing. For evaluating\nsemantic segmentation performance, we tested state-of-the-art models in\ncomputer vision and remote sensing on our dataset. Additionally, we conducted\nan ablation study varying window sizes to capture temporal characteristics.\nOverall, the models demonstrated modest results, suggesting a requirement for\nfuture multimodal and temporal learning strategies. The dataset will be\npublicly available on\n<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>.", "comment": "8 pages, 2 figures. Presented at ACM RACS 2024 (Pompei, Italy, Nov\n  5-8, 2024)", "pdf_url": "http://arxiv.org/pdf/2507.23193v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23215", "title": "Silent Impact: Tracking Tennis Shots from the Passive Arm", "authors": ["Junyong Park", "Saelyne Yang", "Sungho Jo"], "categories": ["cs.HC", "H.5.2; I.5.4"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      15 pages, 9 figures,", "url": "http://arxiv.org/abs/2507.23215v1", "summary": "Wearable technology has transformed sports analytics, offering new dimensions\nin enhancing player experience. Yet, many solutions involve cumbersome setups\nthat inhibit natural motion. In tennis, existing products require sensors on\nthe racket or dominant arm, causing distractions and discomfort. We propose\nSilent Impact, a novel and user-friendly system that analyzes tennis shots\nusing a sensor placed on the passive arm. Collecting Inertial Measurement Unit\nsensor data from 20 recreational tennis players, we developed neural networks\nthat exclusively utilize passive arm data to detect and classify six shots,\nachieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,\ncomparable to the dominant arm. These models were then incorporated into an\nend-to-end prototype, which records passive arm motion through a smartwatch and\ndisplays a summary of shots on a mobile app. User study (N=10) showed that\nparticipants felt less burdened physically and mentally using Silent Impact on\nthe passive arm. Overall, our research establishes the passive arm as an\neffective, comfortable alternative for tennis shot analysis, advancing\nuser-friendly sports analytics.", "comment": "15 pages, 9 figures,", "pdf_url": "http://arxiv.org/pdf/2507.23215v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23205", "title": "Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks", "authors": ["Hebi Li", "Forrest Sheng Bao", "Qi Xiao", "Jin Tian"], "categories": ["cs.PL", "cs.SE"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23205v1", "summary": "Foreign Function Interfaces (FFIs) are essential for enabling\ninteroperability between programming languages, yet existing FFI solutions are\nill-suited for the dynamic, interactive workflows prevalent in modern notebook\nenvironments such as Jupyter. Current approaches require extensive manual\nconfiguration, introduce significant boilerplate, and often lack support for\nrecursive calls and object-oriented programming (OOP) constructs-features\ncritical for productive, multi-language development.\n  We present Kernel-FFI, a transparent, language-agnostic framework that\nenables seamless cross-language function calls and object manipulation within\ninteractive notebooks. Kernel-FFI employs source-level transformation to\nautomatically rewrite cross-language invocations, eliminating the need for\nmanual bindings or boilerplate. Kernel-FFI provides robust support for OOP by\nenabling foreign object referencing and automatic resource management across\nlanguage boundaries. Furthermore, to address the blocking nature of Jupyter\nkernels and support recursive and asynchronous foreign calls, we introduce a\nnovel side-channel communication mechanism. Our tool will be open-sourced and\navailable at https://codepod.io/docs/kernel-ffi", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23205v1", "cate": "cs.PL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21903", "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation", "authors": ["Tiviatis Sim", "Kaiwen Yang", "Shen Xin", "Kenji Kawaguchi"], "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21903v2", "summary": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21903v2", "cate": "cs.SI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2504.10403", "title": "Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks", "authors": ["Yan Zhu", "Jingyang Zhu", "Ting Wang", "Yuanming Shi", "Chunxiao Jiang", "Khaled Ben Letaief"], "categories": ["cs.LG", "cs.DC", "cs.NI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10403v3", "summary": "Advancements in artificial intelligence (AI) and low-earth orbit (LEO)\nsatellites have promoted the application of large remote sensing foundation\nmodels for various downstream tasks. However, direct downloading of these\nmodels for fine-tuning on the ground is impeded by privacy concerns and limited\nbandwidth. Satellite federated learning (FL) offers a solution by enabling\nmodel fine-tuning directly on-board satellites and aggregating model updates\nwithout data downloading. Nevertheless, for large foundation models, the\ncomputational capacity of satellites is insufficient to support effective\non-board fine-tuning in traditional satellite FL frameworks. To address these\nchallenges, we propose a satellite-ground collaborative federated fine-tuning\nframework. The key of the framework lies in how to reasonably decompose and\nallocate model components to alleviate insufficient on-board computation\ncapabilities. During fine-tuning, satellites exchange intermediate results with\nground stations or other satellites for forward propagation and back\npropagation, which brings communication challenges due to the special\ncommunication topology of space transmission networks, such as intermittent\nsatellite-ground communication, short duration of satellite-ground\ncommunication windows, and unstable inter-orbit inter-satellite links (ISLs).\nTo reduce transmission delays, we further introduce tailored communication\nstrategies that integrate both communication and computing resources.\nSpecifically, we propose a parallel intra-orbit communication strategy, a\ntopology-aware satellite-ground communication strategy, and a\nlatency-minimalization inter-orbit communication strategy to reduce space\ncommunication costs. Simulation results demonstrate significant reductions in\ntraining time with improvements of approximately 33%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10403v3", "cate": "cs.LG", "date": "2025-04-14", "updated": "2025-07-31"}
{"id": "2507.23200", "title": "Efficient DFT of Zadoff-Chu Sequences using lmFH Pattern", "authors": ["Fanping Du"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2507.23200v1", "summary": "Having established that Zadoff-Chu (ZC) sequences are inherently linear\nmicro-frequency hopping (lmFH) symbols, this paper first presents an intuitive\nand visual exposition of the computation of the DFT and IDFT of ZC sequences\nusing the lmFH pattern. This yields interesting results. Subsequently, an\nalternative form for computing the cumulative sum of ZC sequences using the\nGeneralized Quadratic Gauss Sum is introduced. Furthermore, building on the\nmicro-frequency hopping (mFH) concept, this paper shows that the DFT of ZC\nsequences can be transformed into an lmFH symbol with frequency shift and phase\noffset. Therefore, the DFT of ZC sequences can be computed via cumulative\nfrequency points, similar to the computation of normal mFH symbols.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.23200v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22912", "title": "A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms", "authors": ["Navid Yazdanjue", "Morteza Rakhshaninejad", "Hossein Yazdanjouei", "Mohammad Sadegh Khorshidi", "Mikko S. Niemela", "Fang Chen", "Amir H. Gandomi"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figures, 9 tables", "url": "http://arxiv.org/abs/2507.22912v1", "summary": "Illegal marketplaces have increasingly shifted to concealed parts of the\ninternet, including the deep and dark web, as well as platforms such as\nTelegram, Reddit, and Pastebin. These channels enable the anonymous trade of\nillicit goods including drugs, weapons, and stolen credentials. Detecting and\ncategorizing such content remains challenging due to limited labeled data, the\nevolving nature of illicit language, and the structural heterogeneity of online\nsources. This paper presents a hierarchical classification framework that\ncombines fine-tuned language models with a semi-supervised ensemble learning\nstrategy to detect and classify illicit marketplace content across diverse\nplatforms. We extract semantic representations using ModernBERT, a transformer\nmodel for long documents, finetuned on domain-specific data from deep and dark\nweb pages, Telegram channels, Subreddits, and Pastebin pastes to capture\nspecialized jargon and ambiguous linguistic patterns. In addition, we\nincorporate manually engineered features such as document structure, embedded\npatterns including Bitcoin addresses, emails, and IPs, and metadata, which\ncomplement language model embeddings. The classification pipeline operates in\ntwo stages. The first stage uses a semi-supervised ensemble of XGBoost, Random\nForest, and SVM with entropy-based weighted voting to detect sales-related\ndocuments. The second stage further classifies these into drug, weapon, or\ncredential sales. Experiments on three datasets, including our multi-source\ncorpus, DUTA, and CoDA, show that our model outperforms several baselines,\nincluding BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The\nmodel achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of\n0.95388, demonstrating strong generalization, robustness under limited\nsupervision, and effectiveness in real-world illicit content detection.", "comment": "16 pages, 5 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.22912v1", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.23579", "title": "Impact of a Lower Limb Exosuit Anchor Points on Energetics and Biomechanics", "authors": ["Chiara Lambranzi", "Giulia Oberti", "Christian Di Natali", "Darwin G. Caldwell", "Manuela Galli", "Elena De Momi", "Jesùs Ortiz"], "categories": ["physics.med-ph", "cs.RO", "eess.SP"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 10 figures", "url": "http://arxiv.org/abs/2507.23579v1", "summary": "Anchor point placement is a crucial yet often overlooked aspect of exosuit\ndesign since it determines how forces interact with the human body. This work\nanalyzes the impact of different anchor point positions on gait kinematics,\nmuscular activation and energetic consumption. A total of six experiments were\nconducted with 11 subjects wearing the XoSoft exosuit, which assists hip\nflexion in five configurations. Subjects were instrumented with an IMU-based\nmotion tracking system, EMG sensors, and a mask to measure metabolic\nconsumption. The results show that positioning the knee anchor point on the\nposterior side while keeping the hip anchor on the anterior part can reduce\nmuscle activation in the hip flexors by up to 10.21\\% and metabolic expenditure\nby up to 18.45\\%. Even if the only assisted joint was the hip, all the\nconfigurations introduced changes also in the knee and ankle kinematics.\nOverall, no single configuration was optimal across all subjects, suggesting\nthat a personalized approach is necessary to transmit the assistance forces\noptimally. These findings emphasize that anchor point position does indeed have\na significant impact on exoskeleton effectiveness and efficiency. However,\nthese optimal positions are subject-specific to the exosuit design, and there\nis a strong need for future work to tailor musculoskeletal models to individual\ncharacteristics and validate these results in clinical populations.", "comment": "12 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.23579v1", "cate": "physics.med-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23221", "title": "A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations", "authors": ["Charles O'Neill", "Slava Chalnev", "Chi Chi Zhao", "Max Kirkby", "Mudith Jayasekara"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23221v1", "summary": "Contextual hallucinations -- statements unsupported by given context --\nremain a significant challenge in AI. We demonstrate a practical\ninterpretability insight: a generator-agnostic observer model detects\nhallucinations via a single forward pass and a linear probe on its residual\nstream. This probe isolates a single, transferable linear direction separating\nhallucinated from faithful text, outperforming baselines by 5-27 points and\nshowing robust mid-layer performance across Gemma-2 models (2B to 27B).\nGradient-times-activation localises this signal to sparse, late-layer MLP\nactivity. Critically, manipulating this direction causally steers generator\nhallucination rates, proving its actionability. Our results offer novel\nevidence of internal, low-dimensional hallucination tracking linked to specific\nMLP sub-circuits, exploitable for detection and mitigation. We release the\n2000-example ContraTales benchmark for realistic assessment of such solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23221v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23202", "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks", "authors": ["Chengwei Xia", "Fan Ma", "Ruijie Quan", "Kun Zhan", "Yi Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23202v1", "summary": "This paper addresses the challenge of generating adversarial image using a\ndiffusion model to deceive multimodal large language models (MLLMs) into\ngenerating the targeted responses, while avoiding significant distortion of the\nclean image. To address the above challenges, we propose an adversarial-guided\ndiffusion (AGD) approach for adversarial attack MLLMs. We introduce\nadversarial-guided noise to ensure attack efficacy. A key observation in our\ndesign is that, unlike most traditional adversarial attacks which embed\nhigh-frequency perturbations directly into the clean image, AGD injects target\nsemantics into the noise component of the reverse diffusion. Since the added\nnoise in a diffusion model spans the entire frequency spectrum, the adversarial\nsignal embedded within it also inherits this full-spectrum property.\nImportantly, during reverse diffusion, the adversarial image is formed as a\nlinear combination of the clean image and the noise. Thus, when applying\ndefenses such as a simple low-pass filtering, which act independently on each\ncomponent, the adversarial image within the noise component is less likely to\nbe suppressed, as it is not confined to the high-frequency band. This makes AGD\ninherently robust to variety defenses. Extensive experiments demonstrate that\nour AGD outperforms state-of-the-art methods in attack performance as well as\nin model robustness to some defenses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23202v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23298", "title": "Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System", "authors": ["Kazushi Kato", "Koji Inoue", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted by 27th ACM International Conference on Multimodal Interaction (ICMI '25), Long paper", "url": "http://arxiv.org/abs/2507.23298v1", "summary": "In human dialogue, nonverbal information such as nodding and facial\nexpressions is as crucial as verbal information, and spoken dialogue systems\nare also expected to express such nonverbal behaviors. We focus on nodding,\nwhich is critical in an attentive listening system, and propose a model that\npredicts both its timing and type in real time. The proposed model builds on\nthe voice activity projection (VAP) model, which predicts voice activity from\nboth listener and speaker audio. We extend it to prediction of various types of\nnodding in a continuous and real-time manner unlike conventional models. In\naddition, the proposed model incorporates multi-task learning with verbal\nbackchannel prediction and pretraining on general dialogue data. In the timing\nand type prediction task, the effectiveness of multi-task learning was\nsignificantly demonstrated. We confirmed that reducing the processing rate\nenables real-time operation without a substantial drop in accuracy, and\nintegrated the model into an avatar attentive listening system. Subjective\nevaluations showed that it outperformed the conventional method, which always\ndoes nodding in sync with verbal backchannel. The code and trained models are\navailable at https://github.com/MaAI-Kyoto/MaAI.", "comment": "Accepted by 27th ACM International Conference on Multimodal\n  Interaction (ICMI '25), Long paper", "pdf_url": "http://arxiv.org/pdf/2507.23298v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23292", "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy", "authors": ["RJ Skerry-Ryan", "Julian Salazar", "Soroosh Mariooryad", "David Kao", "Daisy Stanton", "Eric Battenberg", "Matt Shannon", "Ron J. Weiss", "Robin Scheibler", "Jonas Rothfuss", "Tom Bagby"], "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23292v1", "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23292v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2406.17552", "title": "Long-Time and Short-Time Dynamics in a Weighted-Median Opinion Model on Networks", "authors": ["Lasse Mohr", "Poul G. Hjorth", "Mason A. Porter"], "categories": ["physics.soc-ph", "cs.SI", "math.DS", "nlin.AO"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      30 pages, 13 figures, Submitted to SIAM Journal on Applied Dynamical Systems. Version 2 of this manuscript had a mistake in the arxiv title. The manuscript, figure, and appendix of this version is identical to version 2; the only change is the arxiv title which has been changed to align with the title of the manuscript", "url": "http://arxiv.org/abs/2406.17552v3", "summary": "Social interactions influence people's opinions. In some situations, these\ninteractions eventually yield a consensus opinion; in others, they can lead to\nopinion fragmentation and the formation of different opinion groups in the form\nof ``echo chambers''. Consider a social network of individuals with\ncontinuous-valued scalar opinions, and suppose that they can change their\nopinions when they interact with each other. In many models of the opinion\ndynamics of individuals in a network, it is common for opinion updates to\ndepend on the mean opinion of interacting individuals. As an alternative, which\nmay be more realistic in some situations, we study an opinion model with an\nopinion-update rule that depends on the weighted median of the opinions of\ninteracting individuals. Through numerical simulations of our median-update\nopinion model, we investigate how the final opinion distribution depends on\nnetwork structure. For configuration-model networks, we also derive a\nmean-field approximation for the asymptotic dynamics of the opinion\ndistribution when there are infinitely many individuals. We numerically\ninvestigate its accuracy for short-time opinion dynamics on various networks.", "comment": "30 pages, 13 figures, Submitted to SIAM Journal on Applied Dynamical\n  Systems. Version 2 of this manuscript had a mistake in the arxiv title. The\n  manuscript, figure, and appendix of this version is identical to version 2;\n  the only change is the arxiv title which has been changed to align with the\n  title of the manuscript", "pdf_url": "http://arxiv.org/pdf/2406.17552v3", "cate": "physics.soc-ph", "date": "2024-06-25", "updated": "2025-07-31"}
{"id": "2506.11298", "title": "Jelly: a Fast and Convenient RDF Serialization Format", "authors": ["Piotr Sowinski", "Karolina Bogacka", "Anastasiya Danilenka", "Nikita Kozlov"], "categories": ["cs.DB", "cs.NI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      Developers Workshop, co-located with SEMANTiCS'25: International Conference on Semantic Systems, September 3-5, 2025, Vienna, Austria", "url": "http://arxiv.org/abs/2506.11298v2", "summary": "Existing RDF serialization formats such as Turtle, N-Quads, and JSON-LD are\nwidely used for communication and storage in knowledge graph and Semantic Web\napplications. However, they suffer from limitations in performance, compression\nratio, and lack of native support for RDF streams. To address these\nshortcomings, we introduce Jelly, a fast and convenient binary serialization\nformat for RDF data that supports both batch and streaming use cases. Jelly is\ndesigned to maximize serialization throughput, reduce file size with\nlightweight streaming compression, and minimize compute resource usage. Built\non Protocol Buffers, Jelly is easy to integrate with modern programming\nlanguages and RDF libraries. To maximize reusability, Jelly has an open\nprotocol specification, open-source implementations in Java and Python\nintegrated with popular RDF libraries, and a versatile command-line tool. To\nillustrate its usefulness, we outline concrete use cases where Jelly can\nprovide tangible benefits. We consider that by combining practical usability\nwith state-of-the-art efficiency, Jelly is an important contribution to the\nSemantic Web tool stack.", "comment": "Developers Workshop, co-located with SEMANTiCS'25: International\n  Conference on Semantic Systems, September 3-5, 2025, Vienna, Austria", "pdf_url": "http://arxiv.org/pdf/2506.11298v2", "cate": "cs.DB", "date": "2025-06-12", "updated": "2025-07-31"}
{"id": "2507.23234", "title": "Secure Integrated Sensing and Communication Networks: Stochastic Performance Analysis", "authors": ["Marziyeh Soltani", "Mahtab Mirmohseni", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23234v1", "summary": "This paper analyzes the stochastic security performance of a multiple-input\nmultiple-output (MIMO) integrated sensing and communication (ISAC) system in a\ndownlink scenario. A base station (BS) transmits a multi-functional signal to\nsimultaneously communicate with a user, sense a target's angular location, and\ncounteract eavesdropping threats. The attack model considers a passive\nsingle-antenna communication eavesdropper intercepting communication data, as\nwell as a multi-antenna sensing eavesdropper attempting to infer the target's\nlocation. We also consider a malicious target scenario where the target plays\nthe role of the communication eavesdropper. The BS-user and BS-eavesdroppers\nchannels follow Rayleigh fading, while the target's azimuth angle is uniformly\ndistributed. To evaluate the performance in this random network, we derive the\nergodic secrecy rate (ESR) and the ergodic Cramer-Rao lower bound (CRB), for\ntarget localization, at both the BS and the sensing eavesdropper. This involves\ncomputing the probability density functions (PDFs) of the signal-to-noise ratio\n(SNR) and CRB, leveraging the central limit theorem for tractability. We\ncharacterize the boundary of the CRB-secrecy rate region, and interpret the\nperformance tradeoffs between communication and sensing while guaranteeing a\nlevel of security and privacy in the random ISAC networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23234v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22913", "title": "A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models", "authors": ["Jinyu Liu", "Xiaoying Song", "Diana Zhang", "Jason Thomale", "Daqing He", "Lingzi Hong"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures, accepted by ASIST 2025", "url": "http://arxiv.org/abs/2507.22913v1", "summary": "Providing subject access to information resources is an essential function of\nany library management system. Large language models (LLMs) have been widely\nused in classification and summarization tasks, but their capability to perform\nsubject analysis is underexplored. Multi-label classification with traditional\nmachine learning (ML) models has been used for subject analysis but struggles\nwith unseen cases. LLMs offer an alternative but often over-generate and\nhallucinate. Therefore, we propose a hybrid framework that integrates\nembedding-based ML models with LLMs. This approach uses ML models to (1)\npredict the optimal number of LCSH labels to guide LLM predictions and (2)\npost-edit the predicted terms with actual LCSH terms to mitigate\nhallucinations. We experimented with LLMs and the hybrid framework to predict\nthe subject terms of books using the Library of Congress Subject Headings\n(LCSH). Experiment results show that providing initial predictions to guide LLM\ngenerations and imposing post-edits result in more controlled and\nvocabulary-aligned outputs.", "comment": "13 pages, 2 figures, accepted by ASIST 2025", "pdf_url": "http://arxiv.org/pdf/2507.22913v1", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.23734", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "authors": ["Dongming Wu", "Yanping Fu", "Saike Huang", "Yingfei Liu", "Fan Jia", "Nian Liu", "Feng Dai", "Tiancai Wang", "Rao Muhammad Anwer", "Fahad Shahbaz Khan", "Jianbing Shen"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. The code is at this https URL", "url": "http://arxiv.org/abs/2507.23734v1", "summary": "General robotic grasping systems require accurate object affordance\nperception in diverse open-world scenarios following human instructions.\nHowever, current studies suffer from the problem of lacking reasoning-based\nlarge-scale affordance prediction data, leading to considerable concern about\nopen-world effectiveness. To address this limitation, we build a large-scale\ngrasping-oriented affordance segmentation benchmark with human-like\ninstructions, named RAGNet. It contains 273k images, 180 categories, and 26k\nreasoning instructions. The images cover diverse embodied data domains, such as\nwild, robot, ego-centric, and even simulation data. They are carefully\nannotated with an affordance map, while the difficulty of language instructions\nis largely increased by removing their category name and only providing\nfunctional descriptions. Furthermore, we propose a comprehensive\naffordance-based grasping framework, named AffordanceNet, which consists of a\nVLM pre-trained on our massive affordance data and a grasping network that\nconditions an affordance map to grasp the target. Extensive experiments on\naffordance segmentation benchmarks and real-robot manipulation tasks show that\nour model has a powerful open-world generalization ability. Our data and code\nis available at https://github.com/wudongming97/AffordanceNet.", "comment": "Accepted by ICCV 2025. The code is at\n  https://github.com/wudongming97/AffordanceNet", "pdf_url": "http://arxiv.org/pdf/2507.23734v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23257", "title": "Efficient Machine Unlearning via Influence Approximation", "authors": ["Jiawei Liu", "Chenwang Wu", "Defu Lian", "Enhong Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures", "url": "http://arxiv.org/abs/2507.23257v1", "summary": "Due to growing privacy concerns, machine unlearning, which aims at enabling\nmachine learning models to ``forget\" specific training data, has received\nincreasing attention. Among existing methods, influence-based unlearning has\nemerged as a prominent approach due to its ability to estimate the impact of\nindividual training samples on model parameters without retraining. However,\nthis approach suffers from prohibitive computational overhead arising from the\nnecessity to compute the Hessian matrix and its inverse across all training\nsamples and parameters, rendering it impractical for large-scale models and\nscenarios involving frequent data deletion requests. This highlights the\ndifficulty of forgetting. Inspired by cognitive science, which suggests that\nmemorizing is easier than forgetting, this paper establishes a theoretical link\nbetween memorizing (incremental learning) and forgetting (unlearning). This\nconnection allows machine unlearning to be addressed from the perspective of\nincremental learning. Unlike the time-consuming Hessian computations in\nunlearning (forgetting), incremental learning (memorizing) typically relies on\nmore efficient gradient optimization, which supports the aforementioned\ncognitive theory. Based on this connection, we introduce the Influence\nApproximation Unlearning (IAU) algorithm for efficient machine unlearning from\nthe incremental perspective. Extensive empirical evaluations demonstrate that\nIAU achieves a superior balance among removal guarantee, unlearning efficiency,\nand comparable model utility, while outperforming state-of-the-art methods\nacross diverse datasets and model architectures. Our code is available at\nhttps://github.com/Lolo1222/IAU.", "comment": "12 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.23257v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23206", "title": "Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images", "authors": ["Xiaoyu Ji", "Ali Shakouri", "Fengqing Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23206v1", "summary": "Food crystal agglomeration is a phenomenon occurs during crystallization\nwhich traps water between crystals and affects food product quality. Manual\nannotation of agglomeration in 2D microscopic images is particularly difficult\ndue to the transparency of water bonding and the limited perspective focusing\non a single slide of the imaged sample. To address this challenge, we first\npropose a supervised baseline model to generate segmentation pseudo-labels for\nthe coarsely labeled classification dataset. Next, an instance classification\nmodel that simultaneously performs pixel-wise segmentation is trained. Both\nmodels are used in the inference stage to combine their respective strengths in\nclassification and segmentation. To preserve crystal properties, a post\nprocessing module is designed and included to both steps. Our method improves\ntrue positive agglomeration classification accuracy and size distribution\npredictions compared to other existing methods. Given the variability in\nconfidence levels of manual annotations, our proposed method is evaluated under\ntwo confidence levels and successfully classifies potential agglomerated\ninstances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23206v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23470", "title": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models", "authors": ["Sebastian Gürtl", "Gloria Schimetta", "David Kerschbaumer", "Michael Liut", "Alexander Steinmaurer"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Learnersourcing: Student-generated Content @ Scale Workshop at L@S 2025", "url": "http://arxiv.org/abs/2507.23470v1", "summary": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation.", "comment": "Learnersourcing: Student-generated Content @ Scale Workshop at L@S\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.23470v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23335", "title": "Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions", "authors": ["Qilin Zhou", "Haipeng Wang", "Zhengyuan Wei", "W. K. Chan"], "categories": ["cs.LG", "cs.SE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      accepted by QRS 2025", "url": "http://arxiv.org/abs/2507.23335v1", "summary": "Patch robustness certification is an emerging verification approach for\ndefending against adversarial patch attacks with provable guarantees for deep\nlearning systems. Certified recovery techniques guarantee the prediction of the\nsole true label of a certified sample. However, existing techniques, if\napplicable to top-k predictions, commonly conduct pairwise comparisons on those\nvotes between labels, failing to certify the sole true label within the top k\nprediction labels precisely due to the inflation on the number of votes\ncontrolled by the attacker (i.e., attack budget); yet enumerating all\ncombinations of vote allocation suffers from the combinatorial explosion\nproblem. We propose CostCert, a novel, scalable, and precise voting-based\ncertified recovery defender. CostCert verifies the true label of a sample\nwithin the top k predictions without pairwise comparisons and combinatorial\nexplosion through a novel design: whether the attack budget on the sample is\ninfeasible to cover the smallest total additional votes on top of the votes\nuncontrollable by the attacker to exclude the true labels from the top k\nprediction labels. Experiments show that CostCert significantly outperforms the\ncurrent state-of-the-art defender PatchGuard, such as retaining up to 57.3% in\ncertified accuracy when the patch size is 96, whereas PatchGuard has already\ndropped to zero.", "comment": "accepted by QRS 2025", "pdf_url": "http://arxiv.org/pdf/2507.23335v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.10373", "title": "Reproducing the first and second moment of empirical degree distributions", "authors": ["Mattia Marzi", "Francesca Giuffrida", "Diego Garlaschelli", "Tiziano Squartini"], "categories": ["physics.soc-ph", "cs.SI", "physics.data-an", "q-fin.ST"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      17 pages, 10 figures", "url": "http://arxiv.org/abs/2505.10373v2", "summary": "The study of probabilistic models for the analysis of complex networks\nrepresents a flourishing research field. Among the former, Exponential Random\nGraphs (ERGs) have gained increasing attention over the years. So far, only\nlinear ERGs have been extensively employed to gain insight into the structural\norganisation of real-world complex networks. None, however, is capable of\naccounting for the variance of the empirical degree distribution. To this aim,\nnon-linear ERGs must be considered. After showing that the usual mean-field\napproximation forces the degree-corrected version of the two-star model to\ndegenerate, we define a fitness-induced variant of it. Such a `softened' model\nis capable of reproducing the sample variance, while retaining the explanatory\npower of its linear counterpart, within a purely canonical framework.", "comment": "17 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2505.10373v2", "cate": "physics.soc-ph", "date": "2025-05-15", "updated": "2025-07-31"}
{"id": "2507.23296", "title": "Exploiting Movable Elements of Intelligent Reflecting Surface for Enhancement of Integrated Sensing and Communication", "authors": ["Xingyu Peng", "Qin Tao", "Yong Liang Guan", "Xiaoming Chen"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      16 pages, 13 figures", "url": "http://arxiv.org/abs/2507.23296v1", "summary": "In this paper, we propose to exploit movable elements of intelligent\nreflecting surface (IRS) to enhance the overall performance of integrated\nsensing and communication (ISAC) systems. Firstly, focusing on a single-user\nscenario, we reveal the function of movable elements by performance analysis,\nand then design a joint beamforming and element position optimization scheme.\nFurther, we extend it to a general multi-user scenario, and also propose an\nelement position optimization scheme according to the derived performance\nexpressions. Finally, simulation results confirm that the movement of IRS\nelements can improve the communication rate and the sensing accuracy, and\nespecially broaden the coverage of ISAC.", "comment": "16 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.23296v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22915", "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models", "authors": ["Esmail Gumaan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.22915v1", "summary": "Hallucination in Large Language Models (LLMs) refers to the generation of\ncontent that is not faithful to the input or the real-world facts. This paper\nprovides a rigorous treatment of hallucination in LLMs, including formal\ndefinitions and theoretical analyses. We distinguish between intrinsic and\nextrinsic hallucinations, and define a \\textit{hallucination risk} for models.\nWe derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes\nand Rademacher complexity). We then survey detection strategies for\nhallucinations, such as token-level uncertainty estimation, confidence\ncalibration, and attention alignment checks. On the mitigation side, we discuss\napproaches including retrieval-augmented generation, hallucination-aware\nfine-tuning, logit calibration, and the incorporation of fact-verification\nmodules. We propose a unified detection and mitigation workflow, illustrated\nwith a diagram, to integrate these strategies. Finally, we outline evaluation\nprotocols for hallucination, recommending datasets, metrics, and experimental\nsetups to quantify and reduce hallucinations. Our work lays a theoretical\nfoundation and practical guidelines for addressing the crucial challenge of\nhallucination in LLMs.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.22915v1", "cate": "cs.CL", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2309.14792", "title": "Exploiting Local Observations for Robust Robot Learning", "authors": ["Wenshuai Zhao", "Eetu-Aleksi Rantala", "Sahar Salimpour", "Zhiyuan Li", "Joni Pajarinen", "Jorge Peña Queralta"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures", "url": "http://arxiv.org/abs/2309.14792v4", "summary": "While many robotic tasks can be addressed using either centralized\nsingle-agent control with full state observation or decentralized multi-agent\ncontrol, clear criteria for choosing between these approaches remain\nunderexplored. This paper systematically investigates how multi-agent\nreinforcement learning (MARL) with local observations can improve robustness in\ncomplex robotic systems compared to traditional centralized control. Through\ntheoretical analysis and empirical validation, we show that in certain tasks,\ndecentralized MARL can achieve performance comparable to centralized methods\nwhile exhibiting greater resilience to perturbations and agent failures. By\nanalytically demonstrating the equivalence of single-agent reinforcement\nlearning (SARL) and MARL under full observability, we identify observability as\nthe critical factor distinguishing the two paradigms. We further derive bounds\nquantifying performance degradation under external perturbations for locally\nobservable policies. Empirical results on standard MARL benchmarks confirm that\nMARL with limited observations can maintain competitive performance. Finally,\nreal-world experiments with a mobile manipulator demonstrate that decentralized\nMARL controllers achieve markedly improved robustness to agent malfunctions and\nenvironmental disturbances relative to centralized baselines. Together, these\nfindings highlight MARL with local observations as a robust and practical\nalternative to conventional centralized control in complex robotic systems.", "comment": "8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2309.14792v4", "cate": "cs.RO", "date": "2023-09-26", "updated": "2025-07-31"}
{"id": "2507.23291", "title": "Evaluating the Dynamics of Membership Privacy in Deep Learning", "authors": ["Yuetian Chen", "Zhiqi Wang", "Nathalie Baracaldo", "Swanand Ravindra Kadhe", "Lei Yu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23291v1", "summary": "Membership inference attacks (MIAs) pose a critical threat to the privacy of\ntraining data in deep learning. Despite significant progress in attack\nmethodologies, our understanding of when and how models encode membership\ninformation during training remains limited. This paper presents a dynamic\nanalytical framework for dissecting and quantifying privacy leakage dynamics at\nthe individual sample level. By tracking per-sample vulnerabilities on an\nFPR-TPR plane throughout training, our framework systematically measures how\nfactors such as dataset complexity, model architecture, and optimizer choice\ninfluence the rate and severity at which samples become vulnerable. Crucially,\nwe discover a robust correlation between a sample's intrinsic learning\ndifficulty, and find that the privacy risk of samples highly vulnerable in the\nfinal trained model is largely determined early during training. Our results\nthus provide a deeper understanding of how privacy risks dynamically emerge\nduring training, laying the groundwork for proactive, privacy-aware model\ntraining strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23291v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23225", "title": "YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection", "authors": ["Zicheng Lin", "Weichao Pan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23225v1", "summary": "Road damage detection is a critical task for ensuring traffic safety and\nmaintaining infrastructure integrity. While deep learning-based detection\nmethods are now widely adopted, they still face two core challenges: first, the\ninadequate multi-scale feature extraction capabilities of existing networks for\ndiverse targets like cracks and potholes, leading to high miss rates for\nsmall-scale damage; and second, the substantial parameter counts and\ncomputational demands of mainstream models, which hinder their deployment for\nefficient, real-time detection in practical applications. To address these\nissues, this paper proposes a high-precision and lightweight model, YOLO - Road\nOrthogonal Compact (YOLO-ROC). We designed a Bidirectional Multi-scale Spatial\nPyramid Pooling Fast (BMS-SPPF) module to enhance multi-scale feature\nextraction and implemented a hierarchical channel compression strategy to\nreduce computational complexity. The BMS-SPPF module leverages a bidirectional\nspatial-channel attention mechanism to improve the detection of small targets.\nConcurrently, the channel compression strategy reduces the parameter count from\n3.01M to 0.89M and GFLOPs from 8.1 to 2.6. Experiments on the\nRDD2022_China_Drone dataset demonstrate that YOLO-ROC achieves a mAP50 of\n67.6%, surpassing the baseline YOLOv8n by 2.11%. Notably, the mAP50 for the\nsmall-target D40 category improved by 16.8%, and the final model size is only\n2.0 MB. Furthermore, the model exhibits excellent generalization performance on\nthe RDD2022_China_Motorbike dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23225v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23492", "title": "Digital literacy interventions can boost humans in discerning deepfakes", "authors": ["Dominique Geissler", "Claire Robertson", "Stefan Feuerriegel"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23492v1", "summary": "Deepfakes, i.e., images generated by artificial intelligence (AI), can erode\ntrust in institutions and compromise election outcomes, as people often\nstruggle to discern real images from deepfakes. Improving digital literacy can\nhelp address these challenges, yet scalable and effective approaches remain\nlargely unexplored. Here, we compare the efficacy of five digital literacy\ninterventions to boost people's ability to discern deepfakes: (1) textual\nguidance on common indicators of deepfakes; (2) visual demonstrations of these\nindicators; (3) a gamified exercise for identifying deepfakes; (4) implicit\nlearning through repeated exposure and feedback; and (5) explanations of how\ndeepfakes are generated with the help of AI. We conducted an experiment with\nN=1,200 participants from the United States to test the immediate and long-term\neffectiveness of our interventions. Our results show that our interventions can\nboost deepfake discernment by up to 13 percentage points while maintaining\ntrust in real images. Altogether, our approach is scalable, suitable for\ndiverse populations, and highly effective for boosting deepfake detection while\nmaintaining trust in truthful information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23492v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2301.02410", "title": "CodePod: A Language-Agnostic Hierarchical Scoping System for Interactive Development", "authors": ["Hebi Li", "Forrest Sheng Bao", "Qi Xiao", "Jin Tian"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2301.02410v2", "summary": "Interactive development environments like Jupyter Notebooks enable\nincremental coding through cells with immediate feedback, but their linear\nstructure and global namespace limit scalability for large software projects.\nWe present CodePod, a hierarchical extension of Jupyter that introduces a novel\nscoped execution model with formal semantics. Our key contribution is a\nlanguage-agnostic runtime system that performs source-level transformations to\nimplement hierarchical scoping rules, enabling true incremental evaluation\nacross nested modules without requiring language-specific kernel modifications.\nWe formalize the scoping semantics as a mathematical framework with precise\nvisibility relations and prove key properties including uniqueness of symbol\nresolution and correctness of the resolution algorithm. A qualitative user\nstudy with seven senior developers demonstrates that CodePod enables\nsignificant improvements in project scalability compared to Jupyter, with\nnotable reductions in navigation effort. We validate the system's effectiveness\non large-scale projects with thousands of lines of code, demonstrating its\napplicability beyond traditional notebook boundaries. Our tool is open-source\nand available at https://codepod.io", "comment": null, "pdf_url": "http://arxiv.org/pdf/2301.02410v2", "cate": "cs.SE", "date": "2023-01-06", "updated": "2025-07-31"}
{"id": "2507.23528", "title": "Hybrid Generative Semantic and Bit Communications in Satellite Networks: Trade-offs in Latency, Generation Quality, and Computation", "authors": ["Chong Huang", "Gaojie Chen", "Jing Zhu", "Qu Luo", "Pei Xiao", "Wei Huang", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, accepted for pulication in IEEE Globecom 2025", "url": "http://arxiv.org/abs/2507.23528v1", "summary": "As satellite communications play an increasingly important role in future\nwireless networks, the issue of limited link budget in satellite systems has\nattracted significant attention in current research. Although semantic\ncommunications emerge as a promising solution to address these constraints, it\nintroduces the challenge of increased computational resource consumption in\nwireless communications. To address these challenges, we propose a multi-layer\nhybrid bit and generative semantic communication framework which can adapt to\nthe dynamic satellite communication networks. Furthermore, to balance the\nsemantic communication efficiency and performance in satellite-to-ground\ntransmissions, we introduce a novel semantic communication efficiency metric\n(SEM) that evaluates the trade-offs among latency, computational consumption,\nand semantic reconstruction quality in the proposed framework. Moreover, we\nutilize a novel deep reinforcement learning (DRL) algorithm group relative\npolicy optimization (GRPO) to optimize the resource allocation in the proposed\nnetwork. Simulation results demonstrate the flexibility of our proposed\ntransmission framework and the effectiveness of the proposed metric SEM,\nillustrate the relationships among various semantic communication metrics.", "comment": "6 pages, accepted for pulication in IEEE Globecom 2025", "pdf_url": "http://arxiv.org/pdf/2507.23528v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22916", "title": "From Propagator to Oscillator: The Dual Role of Symmetric Differential Equations in Neural Systems", "authors": ["Kun Jiang"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2507.22916v1", "summary": "In our previous work, we proposed a novel neuron model based on symmetric\ndifferential equations and demonstrated its potential as an efficient signal\npropagator. Building upon that foundation, the present study delves deeper into\nthe intrinsic dynamics and functional diversity of this model. By\nsystematically exploring the parameter space and employing a range of\nmathematical analysis tools, we theoretically reveal the system 's core\nproperty of functional duality. Specifically, the model exhibits two distinct\ntrajectory behaviors: one is asymptotically stable, corresponding to a reliable\nsignal propagator; the other is Lyapunov stable, characterized by sustained\nself-excited oscillations, functioning as a signal generator. To enable\neffective monitoring and prediction of system states during simulations, we\nintroduce a novel intermediate-state metric termed on-road energy. Simulation\nresults confirm that transitions between the two functional modes can be\ninduced through parameter adjustments or modifications to the connection\nstructure. Moreover, we show that oscillations can be effectively suppressed by\nintroducing external signals. These findings draw a compelling parallel to the\ndual roles of biological neurons in both information transmission and rhythm\ngeneration, thereby establishing a solid theoretical basis and a clear\nfunctional roadmap for the broader application of this model in neuromorphic\nengineering.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.22916v1", "cate": "cs.NE", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2404.13458", "title": "Generalizable Motion Policies through Keypoint Parameterization and Transportation Maps", "authors": ["Giovanni Franzese", "Ravi Prakash", "Cosimo Della Santina", "Jens Kober"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This article was accepted at IEEE Transactions on Robotics (T-RO)", "url": "http://arxiv.org/abs/2404.13458v2", "summary": "Learning from Interactive Demonstrations has revolutionized the way\nnon-expert humans teach robots. It is enough to kinesthetically move the robot\naround to teach pick-and-place, dressing, or cleaning policies. However, the\nmain challenge is correctly generalizing to novel situations, e.g., different\nsurfaces to clean or different arm postures to dress. This article proposes a\nnovel task parameterization and generalization to transport the original robot\npolicy, i.e., position, velocity, orientation, and stiffness. Unlike the state\nof the art, only a set of keypoints is tracked during the demonstration and the\nexecution, e.g., a point cloud of the surface to clean. We then propose to fit\na nonlinear transformation that would deform the space and then the original\npolicy using the paired source and target point sets. The use of function\napproximators like Gaussian Processes allows us to generalize, or transport,\nthe policy from every space location while estimating the uncertainty of the\nresulting policy due to the limited task keypoints and the reduced number of\ndemonstrations. We compare the algorithm's performance with state-of-the-art\ntask parameterization alternatives and analyze the effect of different function\napproximators. We also validated the algorithm on robot manipulation tasks,\ni.e., different posture arm dressing, different location product reshelving,\nand different shape surface cleaning.", "comment": "This article was accepted at IEEE Transactions on Robotics (T-RO)", "pdf_url": "http://arxiv.org/pdf/2404.13458v2", "cate": "cs.RO", "date": "2024-04-20", "updated": "2025-07-31"}
{"id": "2507.23303", "title": "An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items", "authors": ["Luca Corbucci", "Javier Alejandro Borges Legrottaglie", "Francesco Spinnato", "Anna Monreale", "Riccardo Guidotti"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23303v1", "summary": "Accurately identifying items forgotten during a supermarket visit and\nproviding clear, interpretable explanations for recommending them remains an\nunderexplored problem within the Next Basket Prediction (NBP) domain. Existing\nNBP approaches typically only focus on forecasting future purchases, without\nexplicitly addressing the detection of unintentionally omitted items. This gap\nis partly due to the scarcity of real-world datasets that allow for the\nreliable estimation of forgotten items. Furthermore, most current NBP methods\nrely on black-box models, which lack transparency and limit the ability to\njustify recommendations to end users. In this paper, we formally introduce the\nforgotten item prediction task and propose two novel interpretable-by-design\nalgorithms. These methods are tailored to identify forgotten items while\noffering intuitive, human-understandable explanations. Experiments on a\nreal-world retail dataset show our algorithms outperform state-of-the-art NBP\nbaselines by 10-15% across multiple evaluation metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23303v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23226", "title": "Toward Safe, Trustworthy and Realistic Augmented Reality User Experience", "authors": ["Yanming Xiu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      2 pages, 4 figures", "url": "http://arxiv.org/abs/2507.23226v1", "summary": "As augmented reality (AR) becomes increasingly integrated into everyday life,\nensuring the safety and trustworthiness of its virtual content is critical. Our\nresearch addresses the risks of task-detrimental AR content, particularly that\nwhich obstructs critical information or subtly manipulates user perception. We\ndeveloped two systems, ViDDAR and VIM-Sense, to detect such attacks using\nvision-language models (VLMs) and multimodal reasoning modules. Building on\nthis foundation, we propose three future directions: automated, perceptually\naligned quality assessment of virtual content; detection of multimodal attacks;\nand adaptation of VLMs for efficient and user-centered deployment on AR\ndevices. Overall, our work aims to establish a scalable, human-aligned\nframework for safeguarding AR experiences and seeks feedback on perceptual\nmodeling, multimodal AR content implementation, and lightweight model\nadaptation.", "comment": "2 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.23226v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23585", "title": "Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web", "authors": ["Sophia Liu", "Shm Garanganao Almeda"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      To appear in: Adjunct Proceedings of the 36th ACM Conference on Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "url": "http://arxiv.org/abs/2507.23585v1", "summary": "Today's algorithm-driven interfaces, from recommendation feeds to GenAI\ntools, often prioritize engagement and efficiency at the expense of user\nagency. As systems take on more decision-making, users have less control over\nwhat they see and how meaning or relationships between content are constructed.\nThis paper introduces \"Hypertextual Friction,\" a conceptual design stance that\nrepositions classical hypertext principles--friction, traceability, and\nstructure--as actionable values for reclaiming agency in algorithmically\nmediated environments. Through a comparative analysis of real-world\ninterfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image\ntools--we examine how different systems structure user experience, navigation,\nand authorship. We show that hypertext systems emphasize provenance,\nassociative thinking, and user-driven meaning-making, while algorithmic systems\ntend to obscure process and flatten participation. We contribute: (1) a\ncomparative analysis of how interface structures shape agency in user-driven\nversus agent-driven systems, and (2) a conceptual stance that offers\nhypertextual values as design commitments for reclaiming agency in an\nincreasingly algorithmic web.", "comment": "To appear in: Adjunct Proceedings of the 36th ACM Conference on\n  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "pdf_url": "http://arxiv.org/pdf/2507.23585v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2405.08788", "title": "Using weakest application conditions to rank graph transformations for graph repair", "authors": ["Lars Fritsche", "Alexander Lauer", "Maximilian Kratz", "Andy Schürr", "Gabriele Taentzer"], "categories": ["cs.SE", "68R10", "D.2.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      51 pages, 25 Figures, new, more efficient method for constructing application conditions, theoretical comparison to other concepts of consistency, extended evaluation", "url": "http://arxiv.org/abs/2405.08788v3", "summary": "When using graphs and graph transformations to model systems, consistency is\nan important concern. While consistency has primarily been viewed as a binary\nproperty, i.e., a graph is consistent or inconsistent with respect to a set of\nconstraints, recent work has presented an approach to consistency as a\ngraduated property. This allows living with inconsistencies for a while and\nrepairing them when necessary. For repairing inconsistencies in a graph, we use\ngraph transformation rules with so-called {\\em impairment-indicating and\nrepair-indicating application conditions} to understand how much repair gain\ncertain rule applications would bring. Both types of conditions can be derived\nfrom given graph constraints. Our main theorem shows that the difference\nbetween the number of actual constraint violations before and after a graph\ntransformation step can be characterized by the difference between the numbers\nof violated impairment-indicating and repair-indicating application conditions.\nThis theory forms the basis for algorithms with look-ahead that rank graph\ntransformations according to their potential for graph repair. An evaluation\nshows that graph repair can be well supported by rules with these new types of\napplication conditions in terms of effectiveness and scalability.", "comment": "51 pages, 25 Figures, new, more efficient method for constructing\n  application conditions, theoretical comparison to other concepts of\n  consistency, extended evaluation", "pdf_url": "http://arxiv.org/pdf/2405.08788v3", "cate": "cs.SE", "date": "2024-05-14", "updated": "2025-07-31"}
{"id": "2507.23686", "title": "From Link Diversity to Cross-Band Feedback Collaboration: A New Perspective on Hybrid Optical-RF Systems", "authors": ["Menghan Li", "Yulin Shao", "Runxin Zhang", "Lu Lu"], "categories": ["cs.IT", "cs.SY", "eess.SY", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23686v1", "summary": "We suggest a re-examination of the conventional view that hybrid\noptical-radio frequency (O-RF) systems are primarily diversity-driven networks\nthat switch between RF and optical links for robustness. Instead, we uncover a\nnew architectural opportunity: repurposing the optical downlink to enable\nreal-time feedback channel coding over the RF uplink, where structured decoder\nfeedback is delivered from the access point to guide the transmitter's coding\nstrategy. This insight marks a conceptual paradigm shift from passive link\ndiversity to active cross-band collaboration, where the wideband,\ninterference-free optical wireless communication (OWC) is no longer merely a\ndownlink backup but a functional enabler of uplink reliability. To realize this\nvision, we propose a novel architecture, O-RF with Cross-Band Feedback\n(O-RF-CBF), that exploits the optical downlink feedback to facilitate adaptive\nRF uplink coding. Numerical results reveal that O-RF-CBF achieves significant\nuplink throughput gains over traditional O-RF systems. Our findings highlight\nthat inter-band synergy, not redundancy, is the key to unlocking the full\npotential of hybrid wireless networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23686v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22917", "title": "Reading Between the Timelines: RAG for Answering Diachronic Questions", "authors": ["Kwun Hang Lau", "Ruiyuan Zhang", "Weijie Shi", "Xiaofang Zhou", "Xiaojun Cheng"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22917v1", "summary": "While Retrieval-Augmented Generation (RAG) excels at injecting static,\nfactual knowledge into Large Language Models (LLMs), it exhibits a critical\ndeficit in handling longitudinal queries that require tracking entities and\nphenomena across time. This blind spot arises because conventional,\nsemantically-driven retrieval methods are not equipped to gather evidence that\nis both topically relevant and temporally coherent for a specified duration. We\naddress this challenge by proposing a new framework that fundamentally\nredesigns the RAG pipeline to infuse temporal logic. Our methodology begins by\ndisentangling a user's query into its core subject and its temporal window. It\nthen employs a specialized retriever that calibrates semantic matching against\ntemporal relevance, ensuring the collection of a contiguous evidence set that\nspans the entire queried period. To enable rigorous evaluation of this\ncapability, we also introduce the Analytical Diachronic Question Answering\nBenchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus\nof real and synthetic financial news. Empirical results on ADQAB show that our\napproach yields substantial gains in answer accuracy, surpassing standard RAG\nimplementations by 13% to 27%. This work provides a validated pathway toward\nRAG systems capable of performing the nuanced, evolutionary analysis required\nfor complex, real-world questions. The dataset and code for this study are\npublicly available at https://github.com/kwunhang/TA-RAG.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22917v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2404.16705", "title": "SHINE: Social Homology Identification for Navigation in Crowded Environments", "authors": ["Diego Martinez-Baselga", "Oscar de Groot", "Luzia Knoedler", "Luis Riazuelo", "Javier Alonso-Mora", "Luis Montano"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication at The International Journal of Robotics Research. Please, when citing the paper, refer to the official manuscript with the following DOI: https://doi.org/10.1177/02783649251344639", "url": "http://arxiv.org/abs/2404.16705v3", "summary": "Navigating mobile robots in social environments remains a challenging task\ndue to the intricacies of human-robot interactions. Most of the motion planners\ndesigned for crowded and dynamic environments focus on choosing the best\nvelocity to reach the goal while avoiding collisions, but do not explicitly\nconsider the high-level navigation behavior (avoiding through the left or right\nside, letting others pass or passing before others, etc.). In this work, we\npresent a novel motion planner that incorporates topology distinct paths\nrepresenting diverse navigation strategies around humans. The planner selects\nthe topology class that imitates human behavior the best using a deep neural\nnetwork model trained on real-world human motion data, ensuring socially\nintelligent and contextually aware navigation. Our system refines the chosen\npath through an optimization-based local planner in real time, ensuring\nseamless adherence to desired social behaviors. In this way, we decouple\nperception and local planning from the decision-making process. We evaluate the\nprediction accuracy of the network with real-world data. In addition, we assess\nthe navigation capabilities in both simulation and a real-world platform,\ncomparing it with other state-of-the-art planners. We demonstrate that our\nplanner exhibits socially desirable behaviors and shows a smooth and remarkable\nperformance.", "comment": "This paper has been accepted for publication at The International\n  Journal of Robotics Research. Please, when citing the paper, refer to the\n  official manuscript with the following DOI: 10.1177/02783649251344639", "pdf_url": "http://arxiv.org/pdf/2404.16705v3", "cate": "cs.RO", "date": "2024-04-25", "updated": "2025-07-31"}
{"id": "2507.23317", "title": "Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner", "authors": ["Tao He", "Rongchuan Mu", "Lizi Liao", "Yixin Cao", "Ming Liu", "Bing Qin"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      33 pages, 3 figures, 19 tables", "url": "http://arxiv.org/abs/2507.23317v1", "summary": "Large reasoning models (LRMs) have recently shown promise in solving complex\nmath problems when optimized with Reinforcement Learning (RL). But conventional\napproaches rely on outcome-only rewards that provide sparse feedback, resulting\nin inefficient optimization process. In this work, we investigate the function\nof process reward models (PRMs) to accelerate the RL training for LRMs. We\npropose a novel intrinsic signal-driven generative process evaluation mechanism\noperating at the thought level to address major bottlenecks in RL-based\ntraining. Specifically, instead of requiring PRMs to know how to solve\nproblems, our method uses intrinsic signals in solutions to judge stepwise\ncorrectness and aggregate contiguous correct/incorrect steps into coherent\n'thought' units. This structured, thought-level rewards enable more reliable\ncredit assignment by reducing ambiguity in step segmentation and alleviating\nreward hacking. We further introduce a capability-adaptive reward mechanism\nthat dynamically balances exploration and exploitation based on the LRM's\ncurrent proficiency, guiding learning without stifling creative\ntrial-and-error. These innovations are integrated into a new off-policy RL\nalgorithm, TP-GRPO, which extends grouped proximal optimization with\nprocess-based rewards and improves training efficiency. Experiments on 1.5B and\n7B parameter LRMs demonstrate that our method achieves higher problem-solving\naccuracy with significantly fewer training samples than outcome-only reward\nbaselines. The results validate that well-structured process rewards can\nsubstantially accelerate LRM optimization in math reasoning tasks. Code is\navailable at https://github.com/cs-holder/tp_grpo.", "comment": "33 pages, 3 figures, 19 tables", "pdf_url": "http://arxiv.org/pdf/2507.23317v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23237", "title": "Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning", "authors": ["Fan Lyu", "Linglan Zhao", "Chengyan Liu", "Yinying Mei", "Zhang Zhang", "Jian Zhang", "Fuyuan Hu", "Liang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures", "url": "http://arxiv.org/abs/2507.23237v1", "summary": "Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new\nconcepts from limited data while retaining knowledge of previous classes.\nRecently, many studies have started to leverage unlabeled samples to assist\nmodels in learning from few-shot samples, giving rise to the field of\nSemi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However,\nthese studies often assume that the source of unlabeled data is only confined\nto novel classes of the current session, which presents a narrow perspective\nand cannot align well with practical scenarios. To better reflect real-world\nscenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by\nincorporating both base and all the ever-seen novel classes in the unlabeled\nset. This change in the composition of unlabeled samples poses a new challenge\nfor existing methods, as they struggle to distinguish between unlabeled samples\nfrom base and novel classes. To address this issue, we propose an\nAmbiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC\ndynamically uses abundant base samples to correct biased feature distributions\nfor few-shot novel classes. Experiments on three benchmark datasets show that\nour method outperforms existing works, setting new state-of-the-art results.", "comment": "6 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23237v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22936", "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "authors": ["Md Talha Mohsin"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      22 Pages, 6 Tables, 7 Figures", "url": "http://arxiv.org/abs/2507.22936v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used.", "comment": "22 Pages, 6 Tables, 7 Figures", "pdf_url": "http://arxiv.org/pdf/2507.22936v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2406.10375", "title": "Mokav: Execution-driven Differential Testing with LLMs", "authors": ["Khashayar Etemadi", "Bardia Mohammadi", "Zhendong Su", "Martin Monperrus"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.10375v2", "summary": "It is essential to detect functional differences between programs in various\nsoftware engineering tasks, such as automated program repair, mutation testing,\nand code refactoring. The problem of detecting functional differences between\ntwo programs can be reduced to searching for a difference exposing test (DET):\na test input that results in different outputs on the subject programs. In this\npaper, we propose Mokav, a novel execution-driven tool that leverages LLMs to\ngenerate DETs. Mokav takes two versions of a program (P and Q) and an example\ntest input. When successful, Mokav generates a valid DET, a test input that\nleads to provably different outputs on P and Q. Mokav iteratively prompts an\nLLM with a specialized prompt to generate new test inputs. At each iteration,\nMokav provides execution-based feedback from previously generated tests until\nthe LLM produces a DET. We evaluate Mokav on 1535 pairs of Python programs\ncollected from the Codeforces competition platform and 32 pairs of programs\nfrom the QuixBugs dataset. Our experiments show that Mokav outperforms the\nstate-of-the-art, Pynguin and Differential Prompting, by a large margin. Mokav\ncan generate DETs for 81.7% (1,255/1535) of the program pairs in our benchmark\n(versus 4.9% for Pynguin and 37.3% for Differential Prompting). We demonstrate\nthat the iterative and execution-driven feedback components of the system\ncontribute to its high effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.10375v2", "cate": "cs.SE", "date": "2024-06-14", "updated": "2025-07-31"}
{"id": "2507.23702", "title": "Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces", "authors": ["Duc Thien Hua", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23702v1", "summary": "We investigate the integration of beyond diagonal reconfigurable intelligent\nsurfaces (BDRISs) into cell free massive multiple input multiple output\n(CFmMIMO) systems to enhance simultaneous wireless information and power\ntransfer (SWIPT). To simultaneously support two groups of users energy\nreceivers (ERs) and information receivers (IRs) without sacrificing time\nfrequency resources, a subset of access points (APs) is dedicated to serving\nERs with the aid of a BDRIS, while the remaining APs focus on supporting IRs. A\nprotective partial zero forcing precoding technique is implemented at the APs\nto manage the non coherent interference between the ERs and IRs. Subsequently,\nclosed form expressions for the spectral efficiency of the IRs and the average\nsum of harvested energy at the ERs are leveraged to formulate a comprehensive\noptimization problem. This problem jointly optimizes the AP selection, AP power\ncontrol, and scattering matrix design at the BDRIS, all based on long term\nstatistical channel state information. This challenging problem is then\neffectively transformed into more tractable forms. To solve these sub problems,\nefficient algorithms are proposed, including a heuristic search for the\nscattering matrix design, as well as successive convex approximation and deep\nreinforcement learning methods for the joint AP mode selection and power\ncontrol design. Numerical results show that a BDRIS with a group or fully\nconnected architecture achieves significant energy harvesting gains over the\nconventional diagonal RIS, especially delivering up to a seven fold increase in\nthe average sum of harvested energy when a heuristic based scattering matrix\ndesign is employed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23702v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22919", "title": "A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations", "authors": ["Qixuan Hu", "Xumou Zhang", "Jinman Kim", "Florence Bourgeois", "Adam G. Dunn"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22919v1", "summary": "Objectives: With accurate estimates of expected safety results, clinical\ntrials could be designed to avoid terminations and limit exposing participants\nto unnecessary risks. We evaluated methods for predicting serious adverse event\n(SAE) results in clinical trials using information only from their\nregistrations prior to the trial. Material and Methods: We analysed 22,107\ntwo-arm parallel interventional clinical trials from ClinicalTrials.gov with\nstructured summary results. Two prediction models were developed: a classifier\npredicting will experimental arm have higher SAE rates (area under the receiver\noperating characteristic curve; AUC) than control arm, and a regression model\nto predict the proportion of SAEs in control arms (root mean squared error;\nRMSE). A transfer learning approach using pretrained language models (e.g.,\nClinicalT5, BioBERT) was used for feature extraction, combined with downstream\nmodel for prediction. To maintain semantic representation in long trial texts\nexceeding localised language model input limits, a sliding window method was\ndeveloped for embedding extraction. Results: The best model\n(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a\nhigher proportion of patients with SAEs. When predicting proportion of\nparticipants experiencing SAE in the control arm, the same model achieved RMSE\nof 18.6%. The sliding window approach consistently outperformed methods without\nit. Across 12 classifiers, the average absolute AUC increase was 2.00%; across\n12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:\nSummary results data available at ClinicalTrials.gov remains underutilised. The\npotential to estimate results of trials before they start is an opportunity to\nimprove trial design and flag discrepancies between expected and reported\nsafety results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22919v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2406.15304", "title": "Learning Object Compliance via Young's Modulus from Single Grasps using Camera-Based Tactile Sensors", "authors": ["Michael Burgess", "Jialiang Zhao", "Laurence Willemet"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.15304v4", "summary": "Compliance is a useful parametrization of tactile information that humans\noften utilize in manipulation tasks. It can be used to inform low-level\ncontact-rich actions or characterize objects at a high-level. In robotic\nmanipulation, existing approaches to estimate compliance have struggled to\ngeneralize across both object shape and material. Using camera-based tactile\nsensors, proprioception, and force measurements, we present a novel approach to\nestimate object compliance as Young's modulus (E) from parallel grasps. We\nevaluate our method over a novel dataset of 285 common objects, including a\nwide array of shapes and materials with Young's moduli ranging from 5.0 kPa to\n250 GPa. Combining analytical and data-driven approaches, we develop a hybrid\nsystem using a multi-tower neural network to analyze a sequence of tactile\nimages from grasping. This system is shown to estimate the Young's modulus of\nunseen objects within an order of magnitude at 74.2% accuracy across our\ndataset. This is an improvement over purely analytical and data-driven\nbaselines which exhibit 28.9% and 65.0% accuracy respectively. Importantly,\nthis estimation system performs irrespective of object geometry and\ndemonstrates increased robustness across material types. Code is available on\nGitHub and collected data is available on HuggingFace.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.15304v4", "cate": "cs.RO", "date": "2024-06-18", "updated": "2025-07-30"}
{"id": "2507.23389", "title": "Causal Explanation of Concept Drift -- A Truly Actionable Approach", "authors": ["David Komnick", "Kathrin Lammers", "Barbara Hammer", "Valerie Vaquet", "Fabian Hinder"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This manuscript is accepted to be presented at the TempXAI workshop at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD 2025)", "url": "http://arxiv.org/abs/2507.23389v1", "summary": "In a world that constantly changes, it is crucial to understand how those\nchanges impact different systems, such as industrial manufacturing or critical\ninfrastructure. Explaining critical changes, referred to as concept drift in\nthe field of machine learning, is the first step towards enabling targeted\ninterventions to avoid or correct model failures, as well as malfunctions and\nerrors in the physical world. Therefore, in this work, we extend model-based\ndrift explanations towards causal explanations, which increases the\nactionability of the provided explanations. We evaluate our explanation\nstrategy on a number of use cases, demonstrating the practical usefulness of\nour framework, which isolates the causally relevant features impacted by\nconcept drift and, thus, allows for targeted intervention.", "comment": "This manuscript is accepted to be presented at the TempXAI workshop\n  at the European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (ECMLPKDD 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23389v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23242", "title": "Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents", "authors": ["Sungguk Cha", "DongWook Kim", "Taeseung Hahn", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23242v1", "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on effective query\nformulation to unlock external knowledge, yet optimizing queries for diverse,\nunstructured real-world documents remains a challenge. We introduce\n\\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query\nrewriting that eliminates the need for human-annotated datasets and extends\napplicability to both text-only and multi-modal databases. By synthesizing\nscenario-question pairs and leveraging Generalized Reward Policy Optimization\n(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing\nretrieval performance across varied domains. Experiments on industrial in-house\ndata demonstrate significant improvements, with\n$\\text{RL-QR}_{\\text{multi-modal}}$ achieving an 11\\% relative gain in NDCG@3\nfor multi-modal RAG and $\\text{RL-QR}_{\\text{lexical}}$ yielding a 9\\% gain for\nlexical retrievers. However, challenges persist with semantic and hybrid\nretrievers, where rewriters failed to improve performance, likely due to\ntraining misalignments. Our findings highlight RL-QR's potential to\nrevolutionize query optimization for RAG systems, offering a scalable,\nannotation-free solution for real-world retrieval tasks, while identifying\navenues for further refinement in semantic retrieval contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23242v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23756", "title": "Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System", "authors": ["Diana Mortagua"], "categories": ["cs.LG", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23756v1", "summary": "This study centers on overcoming the challenge of selecting the best\nannotators for each query in Active Learning (AL), with the objective of\nminimizing misclassifications. AL recognizes the challenges related to cost and\ntime when acquiring labeled data, and decreases the number of labeled data\nneeded. Nevertheless, there is still the necessity to reduce annotation errors,\naiming to be as efficient as possible, to achieve the expected accuracy faster.\nMost strategies for query-annotator pairs do not consider internal factors that\naffect productivity, such as mood, attention, motivation, and fatigue levels.\nThis work addresses this gap in the existing literature, by not only\nconsidering how the internal factors influence annotators (mood and fatigue\nlevels) but also presenting a new query-annotator pair strategy, using a\nKnowledge-Based Recommendation System (RS). The RS ranks the available\nannotators, allowing to choose one or more to label the queried instance using\ntheir past accuracy values, and their mood and fatigue levels, as well as\ninformation about the instance queried. This work bases itself on existing\nliterature on mood and fatigue influence on human performance, simulating\nannotators in a realistic manner, and predicting their performance with the RS.\nThe results show that considering past accuracy values, as well as mood and\nfatigue levels reduces the number of annotation errors made by the annotators,\nand the uncertainty of the model through its training, when compared to not\nusing internal factors. Accuracy and F1-score values were also better in the\nproposed approach, despite not being as substantial as the aforementioned. The\nmethodologies and findings presented in this study begin to explore the open\nchallenge of human cognitive factors affecting AL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23756v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2407.05028", "title": "Testing Compositionality", "authors": ["Gijs van Cuyck", "Lars van Arragon", "Jan Tretmans"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This is a preprint of an extended version of this https URL Formal Aspects of Component Software. FACS 2024. Lecture Notes in Computer Science, vol 15189. pp 39-56. This extended version adds new sections about combining the different introduced algorithms and how to apply them in practice, as well as several new examples in earlier sections", "url": "http://arxiv.org/abs/2407.05028v3", "summary": "Compositionality supports the manipulation of large systems by working on\ntheir components. For model-based testing, this means that large systems can be\ntested by modelling and testing their components: passing tests for all\ncomponents implies passing tests for the whole system. In previous work, we\ndefined mutual acceptance for specification models and proved that this\nproperty is a sufficient condition for compositionality in model-based testing.\nIn this paper, we present three main algorithms for using mutual acceptance in\npractice. First, we can verify mutual acceptance on specifications, proving\ncompositionality for all valid implementations. Second, we give a sound and\nexhaustive model-based testing procedure which checks mutual acceptance on a\nspecific black-box implementation. The result is that testing the correctness\nof large systems can be decomposed into testing the component implementations\nfor uioco conformance to their specifications, and testing for environmental\nconformance to the specifications of their environment. Finally, we optimise\nthis procedure further by utilizing the constraints imposed by multiple\nspecifications at the same time. These three algorithms together allow picking\nthe most suitable approach for a given situation, trading in more generalizable\nresults for faster runtime by optimising for a specific context as desired.", "comment": "This is a preprint of an extended version of\n  https://doi.org/10.1007/978-3-031-71261-6_3 Formal Aspects of Component\n  Software. FACS 2024. Lecture Notes in Computer Science, vol 15189. pp 39-56.\n  This extended version adds new sections about combining the different\n  introduced algorithms and how to apply them in practice, as well as several\n  new examples in earlier sections", "pdf_url": "http://arxiv.org/pdf/2407.05028v3", "cate": "cs.SE", "date": "2024-07-06", "updated": "2025-07-31"}
{"id": "2507.22992", "title": "Improved Simulation of Asynchronous Entanglement Distribution in Noisy Quantum Networks", "authors": ["Emma Hughes", "William Munizzi", "Prineha Narang"], "categories": ["quant-ph", "cs.IT", "math.IT", "physics.optics"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      26 pages, 2 figures, 1 computational package", "url": "http://arxiv.org/abs/2507.22992v1", "summary": "This work introduces a lightweight simulation framework for evaluating\nasynchronous entanglement distribution protocols under realistic error models.\nWe focus on two contemporary protocols: sequential, where entanglement is\nestablished one node at a time, and parallel, where all nodes attempt to\ngenerate entanglement simultaneously. We evaluate the performance of each\nprotocol using two key metrics: the fidelity of distributed entangled states,\nand the hashing rate, a measure of entanglement efficiency. These metrics are\ncompared between both protocols across a range of network sizes and noise\nparameters. We demonstrate that the parallel protocol consistently outperforms\nthe sequential, particularly in the hashing rate metric due to reduced runtime,\nsuggesting that parallel protocols are a strong candidate for a realizable\nquantum Internet. Our framework offers an accessible and scalable tool for\nevaluating entanglement distribution strategies, by reducing the simulation of\ncomplex quantum processes to simple memory time calculations.", "comment": "26 pages, 2 figures, 1 computational package", "pdf_url": "http://arxiv.org/pdf/2507.22992v1", "cate": "quant-ph", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22920", "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey", "authors": ["Jindong Li", "Yali Fu", "Jiahong Liu", "Linxiao Cao", "Wei Ji", "Menglin Yang", "Irwin King", "Ming-Hsuan Yang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22920v1", "summary": "The rapid advancement of large language models (LLMs) has intensified the\nneed for effective mechanisms to transform continuous multimodal data into\ndiscrete representations suitable for language-based processing. Discrete\ntokenization, with vector quantization (VQ) as a central approach, offers both\ncomputational efficiency and compatibility with LLM architectures. Despite its\ngrowing importance, there is a lack of a comprehensive survey that\nsystematically examines VQ techniques in the context of LLM-based systems. This\nwork fills this gap by presenting the first structured taxonomy and analysis of\ndiscrete tokenization methods designed for LLMs. We categorize 8 representative\nVQ variants that span classical and modern paradigms and analyze their\nalgorithmic principles, training dynamics, and integration challenges with LLM\npipelines. Beyond algorithm-level investigation, we discuss existing research\nin terms of classical applications without LLMs, LLM-based single-modality\nsystems, and LLM-based multimodal systems, highlighting how quantization\nstrategies influence alignment, reasoning, and generation performance. In\naddition, we identify key challenges including codebook collapse, unstable\ngradient estimation, and modality-specific encoding constraints. Finally, we\ndiscuss emerging research directions such as dynamic and task-adaptive\nquantization, unified tokenization frameworks, and biologically inspired\ncodebook learning. This survey bridges the gap between traditional vector\nquantization and modern LLM applications, serving as a foundational reference\nfor the development of efficient and generalizable multimodal systems. A\ncontinuously updated version is available at:\nhttps://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22920v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2407.08722", "title": "Controlling diverse robots by inferring Jacobian fields with deep networks", "authors": ["Sizhe Lester Li", "Annan Zhang", "Boyuan Chen", "Hanna Matusik", "Chao Liu", "Daniela Rus", "Vincent Sitzmann"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2407.08722v2", "summary": "Mirroring the complex structures and diverse functions of natural organisms\nis a long-standing challenge in robotics. Modern fabrication techniques have\ngreatly expanded the feasible hardware, but using these systems requires\ncontrol software to translate the desired motions into actuator commands.\nConventional robots can easily be modeled as rigid links connected by joints,\nbut it remains an open challenge to model and control biologically inspired\nrobots that are often soft or made of several materials, lack sensing\ncapabilities, and may change their material properties with use. Here, we\nintroduce a method that uses deep neural networks to map a video stream of a\nrobot to its visuomotor Jacobian field (the sensitivity of all 3D points to the\nrobot's actuators). Our method enables the control of robots from only a single\ncamera, makes no assumptions about the robots' materials, actuation, or\nsensing, and is trained without expert intervention by observing the execution\nof random commands. We demonstrate our method on a diverse set of robot\nmanipulators that vary in actuation, materials, fabrication, and cost. Our\napproach achieves accurate closed-loop control and recovers the causal dynamic\nstructure of each robot. Because it enables robot control using a generic\ncamera as the only sensor, we anticipate that our work will broaden the design\nspace of robotic systems and serve as a starting point for lowering the barrier\nto robotic automation.", "comment": "Project Page:\n  https://sizhe-li.github.io/publication/neural_jacobian_field", "pdf_url": "http://arxiv.org/pdf/2407.08722v2", "cate": "cs.RO", "date": "2024-07-11", "updated": "2025-07-30"}
{"id": "2507.23412", "title": "A Machine Learning Approach for Honey Adulteration Detection using Mineral Element Profiles", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23412v1", "summary": "This paper aims to develop a Machine Learning (ML)-based system for detecting\nhoney adulteration utilizing honey mineral element profiles. The proposed\nsystem comprises two phases: preprocessing and classification. The\npreprocessing phase involves the treatment of missing-value attributes and\nnormalization. In the classifica-tion phase, we use three supervised ML models:\nlogistic regression, decision tree, and random forest, to dis-criminate between\nauthentic and adulterated honey. To evaluate the performance of the ML models,\nwe use a public dataset comprising measurements of mineral element content of\nauthentic honey, sugar syrups, and adul-terated honey. Experimental findings\nshow that mineral element content in honey provides robust discriminative\ninformation for detecting honey adulteration. Results also demonstrate that the\nrandom forest-based classifier outperforms other classifiers on this dataset,\nachieving the highest cross-validation accuracy of 98.37%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23412v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23245", "title": "Automated Mapping the Pathways of Cranial Nerve II, III, V, and VII/VIII: A Multi-Parametric Multi-Stage Diffusion Tractography Atlas", "authors": ["Lei Xie", "Jiahao Huang", "Jiawei Zhang", "Jianzhong He", "Yiang Pan", "Guoqiang Xie", "Mengjun Li", "Qingrun Zeng", "Mingchu Li", "Yuanjing Feng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23245v1", "summary": "Cranial nerves (CNs) play a crucial role in various essential functions of\nthe human brain, and mapping their pathways from diffusion MRI (dMRI) provides\nvaluable preoperative insights into the spatial relationships between\nindividual CNs and key tissues. However, mapping a comprehensive and detailed\nCN atlas is challenging because of the unique anatomical structures of each CN\npair and the complexity of the skull base environment.In this work, we present\nwhat we believe to be the first study to develop a comprehensive diffusion\ntractography atlas for automated mapping of CN pathways in the human brain. The\nCN atlas is generated by fiber clustering by using the streamlines generated by\nmulti-parametric fiber tractography for each pair of CNs. Instead of disposable\nclustering, we explore a new strategy of multi-stage fiber clustering for\nmultiple analysis of approximately 1,000,000 streamlines generated from the 50\nsubjects from the Human Connectome Project (HCP). Quantitative and visual\nexperiments demonstrate that our CN atlas achieves high spatial correspondence\nwith expert manual annotations on multiple acquisition sites, including the HCP\ndataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of\npituitary adenoma patients. The proposed CN atlas can automatically identify 8\nfiber bundles associated with 5 pairs of CNs, including the optic nerve CN II,\noculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear\nnerve CN VII/VIII, and its robustness is demonstrated experimentally. This work\ncontributes to the field of diffusion imaging by facilitating more efficient\nand automated mapping the pathways of multiple pairs of CNs, thereby enhancing\nthe analysis and understanding of complex brain structures through\nvisualization of their spatial relationships with nearby anatomy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23245v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2309.12365", "title": "An Efficient Intelligent Semi-Automated Warehouse Inventory Stocktaking System", "authors": ["Chunan Tong"], "categories": ["cs.HC", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.12365v3", "summary": "In the context of evolving supply chain management, the significance of\nefficient inventory management has grown substantially for businesses. However,\nconventional manual and experience-based approaches often struggle to meet the\ncomplexities of modern market demands. This research introduces an intelligent\ninventory management system to address challenges related to inaccurate data,\ndelayed monitoring, and overreliance on subjective experience in forecasting.\nThe proposed system integrates bar code and distributed flutter application\ntechnologies for intelligent perception, alongside comprehensive big data\nanalytics to enable data-driven decision-making. Through meticulous analysis,\nsystem design, critical technology exploration, and simulation validation, the\neffectiveness of the proposed system is successfully demonstrated. The\nintelligent system facilitates second-level monitoring, high-frequency checks,\nand artificial intelligence-driven forecasting, consequently enhancing the\nautomation, precision, and intelligence of inventory management. This system\ncontributes to cost reduction and optimized inventory sizes through accurate\npredictions and informed decisions, ultimately achieving a mutually beneficial\nscenario. The outcomes of this research offer", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.12365v3", "cate": "cs.HC", "date": "2023-09-13", "updated": "2025-07-31"}
{"id": "2410.12547", "title": "REST API Testing in DevOps: A Study on an Evolving Healthcare IoT Application", "authors": ["Hassan Sartaj", "Shaukat Ali", "Julie Marie Gjøby"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.12547v2", "summary": "Healthcare Internet of Things (IoT) applications often integrate various\nthird-party healthcare applications and medical devices through REST APIs,\nresulting in complex and interdependent networks of REST APIs. Oslo City's\nhealthcare department collaborates with various industry partners to develop\nsuch healthcare IoT applications enriched with a diverse set of REST APIs.\nFollowing the DevOps process, these REST APIs continuously evolve to\naccommodate evolving needs such as new features, services, and devices. Oslo\nCity's primary goal is to utilize automated solutions for continuous testing of\nthese REST APIs at each evolution stage, thereby ensuring their dependability.\nAlthough the literature offers various automated REST API testing tools, their\neffectiveness in regression testing of the evolving REST APIs of healthcare IoT\napplications within a DevOps context remains undetermined. This paper evaluates\nstate-of-the-art and well-established REST API testing tools, specifically,\nRESTest, EvoMaster, Schemathesis, RESTler, and RestTestGen, for the regression\ntesting of a real-world healthcare IoT application, considering failures,\nfaults, coverage, regressions, and cost. We conducted experiments using all\naccessible REST APIs (17 APIs with 120 endpoints), and 14 releases evolved\nduring DevOps. Overall, all tools generated tests leading to several failures,\n18 potential faults, up to 84% coverage, and 23 regressions. Over 70% of tests\ngenerated by all tools fail to detect failures, resulting in significant\noverhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.12547v2", "cate": "cs.SE", "date": "2024-10-16", "updated": "2025-07-31"}
{"id": "2507.23179", "title": "Cyclotomy, cyclotomic cosets and arimetic propeties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$", "authors": ["Juncheng Zhou", "Hongfeng Wu"], "categories": ["math.NT", "cs.IT", "math.IT"], "primary_category": "Subjects:       Number Theory (math.NT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23179v1", "summary": "Arithmetic properties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle\nx^{p^sq^t}-1\\rangle}$ are obtained by using the cyclotomic classes of order 2\nwith respect to $n=p^sq^t$, where $p\\equiv3 \\mathrm{mod} 4$,\n$\\gcd(\\phi(p^s),\\phi(q^t))=2$, $l$ is a primitive root modulo $q^t$ and\n$\\mathrm{ord}_{p^s}(l)=\\phi(p^s)/2$. The form of these cyclotomic classes\nenables us to further generalize the results obtained in \\cite{ref1}. The\nexplicit expressions of primitive idempotents of minimal ideals in\n$\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$ are also obtained.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23179v1", "cate": "math.NT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22921", "title": "Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers", "authors": ["Lee Harris"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22921v1", "summary": "Language models can capture complex relationships in given text, but these\nare notorious for being costly and for producing information that does not\nexist (i.e., hallucinations). Furthermore, the resources invested into\nproducing this information would be wasted if it were incorrect. We address\nthese issues by proposing, implementing, and applying the Language Model Chain\n(LMC) algorithm. In this, a language model's response to a given prompt about\ngiven text is only correct if it exists in the collection of possible (i.e.,\ncandidate) answers, and text corresponding to incorrect responses is fed into a\nmore predictive (but slower) language model. This process is repeated for a\ncollection of language models, or until all predictions about the text are\ncorrect. We used the LMC algorithm to extract patient dates of birth from\nmedical documents, and combining a collection of language models in a\nmulti-stage cascade significantly increased prediction speed and accuracy over\nindividual language models, while greatly reducing the number of corresponding\nhallucinations. We believe that the novel LMC algorithm significantly\ncontributes to the knowledge extraction field, and that this should be explored\nmuch further in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22921v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2409.15135", "title": "Controllable Traffic Simulation through LLM-Guided Hierarchical Reasoning and Refinement", "authors": ["Zhiyuan Liu", "Leheng Li", "Yuning Wang", "Haotian Lin", "Hao Cheng", "Zhizhe Liu", "Lei He", "Jianqiang Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2409.15135v2", "summary": "Evaluating autonomous driving systems in complex and diverse traffic\nscenarios through controllable simulation is essential to ensure their safety\nand reliability. However, existing traffic simulation methods face challenges\nin their controllability. To address this, we propose a novel diffusion-based\nand LLM-enhanced traffic simulation framework. Our approach incorporates a\nhigh-level understanding module and a low-level refinement module, which\nsystematically examines the hierarchical structure of traffic elements, guides\nLLMs to thoroughly analyze traffic scenario descriptions step by step, and\nrefines the generation by self-reflection, enhancing their understanding of\ncomplex situations. Furthermore, we propose a Frenet-frame-based cost function\nframework that provides LLMs with geometrically meaningful quantities,\nimproving their grasp of spatial relationships in a scenario and enabling more\naccurate cost function generation. Experiments on the Waymo Open Motion Dataset\n(WOMD) demonstrate that our method can handle more intricate descriptions and\ngenerate a broader range of scenarios in a controllable manner.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2409.15135v2", "cate": "cs.RO", "date": "2024-09-23", "updated": "2025-07-31"}
{"id": "2507.23418", "title": "Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23418v1", "summary": "In this paper, we propose a system for detecting adulteration in coconut\nmilk, utilizing infrared spectroscopy. The machine learning-based proposed\nsystem comprises three phases: preprocessing, feature extraction, and\nclassification. The first phase involves removing irrelevant data from coconut\nmilk spectral signals. In the second phase, we employ the Linear Discriminant\nAnalysis (LDA) algorithm for extracting the most discriminating features. In\nthe third phase, we use the K-Nearest Neighbor (KNN) model to classify coconut\nmilk samples into authentic or adulterated. We evaluate the performance of the\nproposed system using a public dataset comprising Fourier Transform Infrared\n(FTIR) spectral information of pure and contaminated coconut milk samples.\nFindings show that the proposed method successfully detects adulteration with a\ncross-validation accuracy of 93.33%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23418v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23251", "title": "A Deep Dive into Generic Object Tracking: A Survey", "authors": ["Fereshteh Aghaee Meibodi", "Shadi Alijani", "Homayoun Najjaran"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      55 pages, 29 figures, 9 tables", "url": "http://arxiv.org/abs/2507.23251v1", "summary": "Generic object tracking remains an important yet challenging task in computer\nvision due to complex spatio-temporal dynamics, especially in the presence of\nocclusions, similar distractors, and appearance variations. Over the past two\ndecades, a wide range of tracking paradigms, including Siamese-based trackers,\ndiscriminative trackers, and, more recently, prominent transformer-based\napproaches, have been introduced to address these challenges. While a few\nexisting survey papers in this field have either concentrated on a single\ncategory or widely covered multiple ones to capture progress, our paper\npresents a comprehensive review of all three categories, with particular\nemphasis on the rapidly evolving transformer-based methods. We analyze the core\ndesign principles, innovations, and limitations of each approach through both\nqualitative and quantitative comparisons. Our study introduces a novel\ncategorization and offers a unified visual and tabular comparison of\nrepresentative methods. Additionally, we organize existing trackers from\nmultiple perspectives and summarize the major evaluation benchmarks,\nhighlighting the fast-paced advancements in transformer-based tracking driven\nby their robust spatio-temporal modeling capabilities.", "comment": "55 pages, 29 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.23251v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2408.09186", "title": "EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition", "authors": ["Qile Liu", "Weishan Ye", "Lingli Zhang", "Zhen Liang"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      18 pages, 10 figures, 14 tables. Accepted in ACMMM 2025", "url": "http://arxiv.org/abs/2408.09186v2", "summary": "Emotion recognition using electroencephalography (EEG) signals has attracted\nincreasing attention in recent years. However, existing methods often lack\ngeneralization in cross-corpus settings, where a model trained on one dataset\nis directly applied to another without retraining, due to differences in data\ndistribution and recording conditions. To tackle the challenge of cross-corpus\nEEG-based emotion recognition, we propose a novel framework termed Soft\nContrastive Masked Modeling (SCMM). Grounded in the theory of emotional\ncontinuity, SCMM integrates soft contrastive learning with a hybrid masking\nstrategy to effectively capture emotion dynamics (refer to short-term\ncontinuity). Specifically, in the self-supervised learning stage, we propose a\nsoft weighting mechanism that assigns similarity scores to sample pairs,\nenabling fine-grained modeling of emotional transitions and capturing the\ntemporal continuity of human emotions. To further enhance representation\nlearning, we design a similarity-aware aggregator that fuses complementary\ninformation from semantically related samples based on pairwise similarities,\nthereby improving feature expressiveness and reconstruction quality. This dual\ndesign contributes to a more discriminative and transferable representation,\nwhich is crucial for robust cross-corpus generalization. Extensive experiments\non the SEED, SEED-IV, and DEAP datasets show that SCMM achieves\nstate-of-the-art (SOTA) performance, outperforming the second-best method by an\naverage accuracy of 4.26% under both same-class and different-class\ncross-corpus settings. The source code is available at\nhttps://github.com/Kyler-RL/SCMM.", "comment": "18 pages, 10 figures, 14 tables. Accepted in ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2408.09186v2", "cate": "cs.HC", "date": "2024-08-17", "updated": "2025-07-31"}
{"id": "2412.15441", "title": "Insights into resource utilization of code small language models serving with runtime engines and execution providers", "authors": ["Francisco Durán", "Matias Martinez", "Patricia Lago", "Silverio Martínez-Fernández"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted in Journal of Systems and Software (JSS). For its published version refer to the Journal of JSS", "url": "http://arxiv.org/abs/2412.15441v2", "summary": "The rapid growth of language models, particularly in code generation,\nrequires substantial computational resources, raising concerns about energy\nconsumption and environmental impact. Optimizing language models inference\nresource utilization is crucial, and Small Language Models (SLMs) offer a\npromising solution to reduce resource demands. Our goal is to analyze the\nimpact of deep learning serving configurations, defined as combinations of\nruntime engines and execution providers, on resource utilization, in terms of\nenergy consumption, execution time, and computing-resource utilization from the\npoint of view of software engineers conducting inference in the context of code\ngeneration SLMs. We conducted a technology-oriented, multi-stage experimental\npipeline using twelve code generation SLMs to investigate energy consumption,\nexecution time, and computing-resource utilization across the configurations.\nSignificant differences emerged across configurations. CUDA execution provider\nconfigurations outperformed CPU execution provider configurations in both\nenergy consumption and execution time. Among the configurations, TORCH paired\nwith CUDA demonstrated the greatest energy efficiency, achieving energy savings\nfrom 37.99% up to 89.16% compared to other serving configurations. Similarly,\noptimized runtime engines like ONNX with the CPU execution provider achieved\nfrom 8.98% up to 72.04% energy savings within CPU-based configurations. Also,\nTORCH paired with CUDA exhibited efficient computing-resource utilization.\nServing configuration choice significantly impacts resource utilization. While\nfurther research is needed, we recommend the above configurations best suited\nto software engineers' requirements for enhancing serving resource utilization\nefficiency.", "comment": "Accepted in Journal of Systems and Software (JSS). For its published\n  version refer to the Journal of JSS", "pdf_url": "http://arxiv.org/pdf/2412.15441v2", "cate": "cs.SE", "date": "2024-12-19", "updated": "2025-07-30"}
{"id": "2507.23526", "title": "Channel Estimation for 6G Near-Field Wireless Communications: A Comprehensive Survey", "authors": ["Wen-Xuan Long", "Shengyu Ye", "Marco Moretti", "Michele Morelli", "Luca Sanguinetti", "Rui Chen", "Cheng-Xiang Wang"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23526v1", "summary": "The sixth-generation (6G) wireless systems are expected to adopt extremely\nlarge aperture arrays (ELAAs), novel antenna architectures, and operate in\nextremely high-frequency bands to meet growing data demands. ELAAs\nsignificantly increase the number of antennas, enabling finer spatial\nresolution and improved beamforming. At high frequencies, ELAAs shift\ncommunication from the conventional far-field to near-field regime, where\nspherical wavefronts dominate and the channel response depends on both angle\nand distance, increasing channel dimensionality. Conventional far-field channel\nestimation methods, which rely on angular information, struggle in near-field\nscenarios due to increased pilot overhead and computational complexity. This\npaper presents a comprehensive survey of recent advances in near-field channel\nestimation. It first defines the near- and far-field boundary from an\nelectromagnetic perspective and discusses key propagation differences,\nalongside a brief review of ELAA developments. Then, it introduces mainstream\nnear-field channel models and compares them with far-field models. Major\nestimation techniques are reviewed under different configurations\n(single/multi-user, single/multi-carrier), including both direct estimation and\nRIS-assisted cascaded estimation. These techniques reveal trade-offs among\nestimation accuracy, complexity, and overhead. This survey aims to provide\ninsights and foundations for efficient and scalable near-field channel\nestimation in 6G systems, while identifying key challenges and future research\ndirections.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23526v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22923", "title": "How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting", "authors": ["Aman Gupta", "Yingying Zhuang", "Zhou Yu", "Ziji Zhang", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at Prompt Optimization KDD '25", "url": "http://arxiv.org/abs/2507.22923v1", "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs), their performance varies substantially across different languages and\ntasks. In multilingual retrieval-augmented generation (RAG)-based systems,\nknowledge bases (KB) are often shared from high-resource languages (such as\nEnglish) to low-resource ones, resulting in retrieved information from the KB\nbeing in a different language than the rest of the context. In such scenarios,\ntwo common practices are pre-translation to create a mono-lingual prompt and\ncross-lingual prompting for direct inference. However, the impact of these\nchoices remains unclear. In this paper, we systematically evaluate the impact\nof different prompt translation strategies for classification tasks with\nRAG-enhanced LLMs in multilingual systems. Experimental results show that an\noptimized prompting strategy can significantly improve knowledge sharing across\nlanguages, therefore improve the performance on the downstream classification\ntask. The findings advocate for a broader utilization of multilingual resource\nsharing and cross-lingual prompt optimization for non-English languages,\nespecially the low-resource ones.", "comment": "Accepted at Prompt Optimization KDD '25", "pdf_url": "http://arxiv.org/pdf/2507.22923v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2409.19490", "title": "KineDepth: Utilizing Robot Kinematics for Online Metric Depth Estimation", "authors": ["Soofiyan Atar", "Yuheng Zhi", "Florian Richter", "Michael Yip"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures", "url": "http://arxiv.org/abs/2409.19490v2", "summary": "Depth perception is essential for a robot's spatial and geometric\nunderstanding of its environment, with many tasks traditionally relying on\nhardware-based depth sensors like RGB-D or stereo cameras. However, these\nsensors face practical limitations, including issues with transparent and\nreflective objects, high costs, calibration complexity, spatial and energy\nconstraints, and increased failure rates in compound systems. While monocular\ndepth estimation methods offer a cost-effective and simpler alternative, their\nadoption in robotics is limited due to their output of relative rather than\nmetric depth, which is crucial for robotics applications. In this paper, we\npropose a method that utilizes a single calibrated camera, enabling the robot\nto act as a \"measuring stick\" to convert relative depth estimates into metric\ndepth in real-time as tasks are performed. Our approach employs an LSTM-based\nmetric depth regressor, trained online and refined through probabilistic\nfiltering, to accurately restore the metric depth across the monocular depth\nmap, particularly in areas proximal to the robot's motion. Experiments with\nreal robots demonstrate that our method significantly outperforms current\nstate-of-the-art monocular metric depth estimation techniques, achieving a\n22.1% reduction in depth error and a 52% increase in success rate for a\ndownstream task.", "comment": "8 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2409.19490v2", "cate": "cs.RO", "date": "2024-09-29", "updated": "2025-07-31"}
{"id": "2507.23428", "title": "Merging Memory and Space: A Spatiotemporal State Space Neural Operator", "authors": ["Nodens F. Koren", "Samuel Lanthaler"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23428v1", "summary": "We propose the Spatiotemporal State Space Neural Operator (ST-SSM), a compact\narchitecture for learning solution operators of time-dependent partial\ndifferential equations (PDEs). ST-SSM introduces a novel factorization of the\nspatial and temporal dimensions, using structured state-space models to\nindependently model temporal evolution and spatial interactions. This design\nenables parameter efficiency and flexible modeling of long-range spatiotemporal\ndynamics. A theoretical connection is established between SSMs and neural\noperators, and a unified universality theorem is proved for the resulting class\nof architectures. Empirically, we demonstrate that our factorized formulation\noutperforms alternative schemes such as zigzag scanning and parallel\nindependent processing on several PDE benchmarks, including 1D Burgers'\nequation, 1D Kuramoto-Sivashinsky equation, and 2D Navier-Stokes equations\nunder varying physical conditions. Our model performs competitively with\nexisting baselines while using significantly fewer parameters. In addition, our\nresults reinforce previous findings on the benefits of temporal memory by\nshowing improved performance under partial observability. Our results highlight\nthe advantages of dimensionally factorized operator learning for efficient and\ngeneralizable PDE modeling, and put this approach on a firm theoretical\nfooting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23428v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23253", "title": "Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality", "authors": ["Mingyang Yu", "Xiahui Guo", "Peng chen", "Zhenkai Li", "Yang Shu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23253v1", "summary": "Time Series forecasting is critical in diverse domains such as weather\nforecasting, financial investment, and traffic management. While traditional\nnumerical metrics like mean squared error (MSE) can quantify point-wise\naccuracy, they fail to evaluate the geometric structure of time series data,\nwhich is essential to understand temporal dynamics. To address this issue, we\npropose the time series Geometric Structure Index (TGSI), a novel evaluation\nmetric that transforms time series into images to leverage their inherent\ntwo-dimensional geometric representations. However, since the image\ntransformation process is non-differentiable, TGSI cannot be directly\nintegrated as a training loss. We further introduce the Shape-Aware Temporal\nLoss (SATL), a multi-component loss function operating in the time series\nmodality to bridge this gap and enhance structure modeling during training.\nSATL combines three components: a first-order difference loss that measures\nstructural consistency through the MSE between first-order differences, a\nfrequency domain loss that captures essential periodic patterns using the Fast\nFourier Transform while minimizing noise, and a perceptual feature loss that\nmeasures geometric structure difference in time-series by aligning temporal\nfeatures with geometric structure features through a pre-trained temporal\nfeature extractor and time-series image autoencoder. Experiments across\nmultiple datasets demonstrate that models trained with SATL achieve superior\nperformance in both MSE and the proposed TGSI metrics compared to baseline\nmethods, without additional computational cost during inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23253v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2409.11911", "title": "AI vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok", "authors": ["Jiajun Wang", "Xiangzhe Yuan", "Siying Hu", "Zhicong Lu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Published online in International Journal of Human Computer Interaction", "url": "http://arxiv.org/abs/2409.11911v2", "summary": "With the development of generative AI technology, a vast array of\nAI-generated paintings (AIGP) have gone viral on social media like TikTok.\nHowever, some negative news about AIGP has also emerged. For example, in 2022,\nnumerous painters worldwide organized a large-scale anti-AI movement because of\nthe infringement in generative AI model training. This event reflected a social\nissue that, with the development and application of generative AI, public\nfeedback and feelings towards it may have been overlooked. Therefore, to\ninvestigate public interactions and perceptions towards AIGP on social media,\nwe analyzed user engagement level and comment sentiment scores of AIGP using\nhuman painting videos as a baseline. In analyzing user engagement, we also\nconsidered the possible moderating effect of the aesthetic quality of\nPaintings. Utilizing topic modeling, we identified seven reasons, including\nhyperrealistic quality, ambivalent reactions, perceived theft of art, etc.,\nleading to negative public perceptions of AIGP. Our work may provide\ninstructive suggestions for future generative AI technology development and\navoid potential crises in human-AI collaboration.", "comment": "Published online in International Journal of Human Computer\n  Interaction", "pdf_url": "http://arxiv.org/pdf/2409.11911v2", "cate": "cs.HC", "date": "2024-09-18", "updated": "2025-07-31"}
{"id": "2503.22688", "title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large Language Models in Interactive Code Generation", "authors": ["Peiding Wang", "Li Zhang", "Fang Liu", "Lin Shi", "Minxiao Li", "Bo Shen", "An Fu"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.22688v3", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions.\nThey offer limited insight into LLMs' abilities to generate code that strictly\nfollows users' instructions in multi-turn interaction scenarios. In this paper,\nwe introduce CodeIF-Bench, a benchmark for evaluating the instruction-following\ncapabilities of LLMs in interactive code generation. Specifically, CodeIF-Bench\nincorporates nine types of verifiable instructions aligned with the real-world\nsoftware development requirements, which can be independently and objectively\nvalidated through specified test cases, facilitating the evaluation of\ninstruction-following capability in multi-turn interactions. In both\n\\textit{Static Conversation} and \\textit{Dynamic Conversation} settings, we\nevaluate the performance of 7 state-of-the-art LLMs and summarize the important\nfactors influencing the instruction-following ability of LLMs in multi-turn\ninteractions, as well as potential directions for improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.22688v3", "cate": "cs.SE", "date": "2025-03-05", "updated": "2025-07-31"}
{"id": "2507.23646", "title": "Information geometry of Lévy processes and financial models", "authors": ["Jaehyung Choi"], "categories": ["stat.TH", "cs.IT", "math.DG", "math.IT", "math.PR", "q-fin.MF"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2507.23646v1", "summary": "We explore the information geometry of L\\'evy processes. As a starting point,\nwe derive the $\\alpha$-divergence between two L\\'evy processes. Subsequently,\nthe Fisher information matrix and the $\\alpha$-connection associated with the\ngeometry of L\\'evy processes are computed from the $\\alpha$-divergence. In\naddition, we discuss statistical applications of this information geometry. As\nillustrative examples, we investigate the differential-geometric structures of\nvarious L\\'evy processes relevant to financial modeling, including tempered\nstable processes, the CGMY model, and variance gamma processes.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2507.23646v1", "cate": "stat.TH", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23387", "title": "H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with Precision Recovery and Cache-Aware Optimization", "authors": ["Weicheng Xue", "Baisong Xu", "Kai Yang", "Yongxiang Liu", "Dengdeng Fan", "Pengxiang Xu", "Yonghong Tian"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23387v1", "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose H2SGEMM,\na high-performance algorithm for emulating FP32 general matrix-matrix\nmultiplication (GEMM) using only FP16 computation units on a representative AI\naccelerator. The method decomposes each FP32 operand into two FP16 values and\ncompensates for numerical errors through a tunable scaling strategy. A detailed\nanalysis of numerical errors, including underflow conditions and precision\nloss, guides the selection of scaling parameters to preserve up to 22 bits of\nmantissa accuracy. We further investigate the effect of computation order on\naccuracy and demonstrate that a term-wise accumulation scheme improves\nnumerical stability over conventional FP32 GEMM in low-exponent regimes.\nFinally, a cache-aware blocking strategy and double-buffered pipeline are\nintroduced to overlap memory transfers with computation, enabling H2SGEMM to\nachieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend\n910A NPU lacking native FP32 support. Extensive numerical experiments confirm\nthat our method not only recovers the accuracy of native FP32 GEMM but also\nexhibits superior numerical stability under certain conditions, due to its\nstructured and error-aware computation order.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23387v1", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22925", "title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22925v1", "summary": "Long-term memory is one of the key factors influencing the reasoning\ncapabilities of Large Language Model Agents (LLM Agents). Incorporating a\nmemory mechanism that effectively integrates past interactions can\nsignificantly enhance decision-making and contextual coherence of LLM Agents.\nWhile recent works have made progress in memory storage and retrieval, such as\nencoding memory into dense vectors for similarity-based search or organizing\nknowledge in the form of graph, these approaches often fall short in structured\nmemory organization and efficient retrieval. To address these limitations, we\npropose a Hierarchical Memory (H-MEM) architecture for LLM Agents that\norganizes and updates memory in a multi-level fashion based on the degree of\nsemantic abstraction. Each memory vector is embedded with a positional index\nencoding pointing to its semantically related sub-memories in the next layer.\nDuring the reasoning phase, an index-based routing mechanism enables efficient,\nlayer-by-layer retrieval without performing exhaustive similarity computations.\nWe evaluate our method on five task settings from the LoCoMo dataset.\nExperimental results show that our approach consistently outperforms five\nbaseline methods, demonstrating its effectiveness in long-term dialogue\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22925v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2410.09213", "title": "iFANnpp: Nuclear Power Plant Digital Twin for Robots and Autonomous Intelligence", "authors": ["Youndo Do", "Marc Zebrowitz", "Jackson Stahl", "Fan Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.09213v3", "summary": "Robotics has gained attention in the nuclear industry due to its precision\nand ability to automate tasks. However, there is a critical need for advanced\nsimulation and control methods to predict robot behavior and optimize plant\nperformance, motivating the use of digital twins. Most existing digital twins\ndo not offer a total design of a nuclear power plant. Moreover, they are\ndesigned for specific algorithms or tasks, making them unsuitable for broader\nresearch applications. In response, this work proposes a comprehensive nuclear\npower plant digital twin designed to improve real-time monitoring, operational\nefficiency, and predictive maintenance. A full nuclear power plant is modeled\nin Unreal Engine 5 and integrated with a high-fidelity Generic Pressurized\nWater Reactor Simulator to create a realistic model of a nuclear power plant\nand a real-time updated virtual environment. The virtual environment provides\nvarious features for researchers to easily test custom robot algorithms and\nframeworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.09213v3", "cate": "cs.RO", "date": "2024-10-11", "updated": "2025-07-31"}
{"id": "2507.23437", "title": "Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design", "authors": ["Yinhui Ma", "Tomomasa Yamasaki", "Zhehui Wang", "Tao Luo", "Bo Wang"], "categories": ["cs.LG", "I.2.6; C.1.3; C.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to ICCAD 2025 (camera-ready); 9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.23437v1", "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach\nto automatically co-optimizing neural network performance and hardware energy\nefficiency, making it particularly useful for the development of Deep Neural\nNetwork accelerators on the edge. However, the extensive search space and high\ncomputational cost pose significant challenges to its practical adoption. To\naddress these limitations, we propose Coflex, a novel HW-NAS framework that\nintegrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian\noptimization. By leveraging sparse inducing points, Coflex reduces the GP\nkernel complexity from cubic to near-linear with respect to the number of\ntraining samples, without compromising optimization performance. This enables\nscalable approximation of large-scale search space, substantially decreasing\ncomputational overhead while preserving high predictive accuracy. We evaluate\nthe efficacy of Coflex across various benchmarks, focusing on\naccelerator-specific architecture. Our experi- mental results show that Coflex\noutperforms state-of-the-art methods in terms of network accuracy and\nEnergy-Delay-Product, while achieving a computational speed-up ranging from\n1.9x to 9.5x.", "comment": "Accepted to ICCAD 2025 (camera-ready); 9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23437v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23263", "title": "Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels", "authors": ["Haoxian Ruan", "Zhihua Xu", "Zhijing Yang", "Guang Ma", "Jieming Xie", "Changxiang Fan", "Tianshui Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 13 figures, publish to ESWA (Expert Systems With Applications)", "url": "http://arxiv.org/abs/2507.23263v1", "summary": "Multi-label image recognition with partial labels (MLR-PL) is designed to\ntrain models using a mix of known and unknown labels. Traditional methods rely\non semantic or feature correlations to create pseudo-labels for unidentified\nlabels using pre-set thresholds. This approach often overlooks the varying\nscore distributions across categories, resulting in inaccurate and incomplete\npseudo-labels, thereby affecting performance. In our study, we introduce the\nSemantic-Aware Threshold Learning (SATL) algorithm. This innovative approach\ncalculates the score distribution for both positive and negative samples within\neach category and determines category-specific thresholds based on these\ndistributions. These distributions and thresholds are dynamically updated\nthroughout the learning process. Additionally, we implement a differential\nranking loss to establish a significant gap between the score distributions of\npositive and negative samples, enhancing the discrimination of the thresholds.\nComprehensive experiments and analysis on large-scale multi-label datasets,\nsuch as Microsoft COCO and VG-200, demonstrate that our method significantly\nimproves performance in scenarios with limited labels.", "comment": "15 pages, 13 figures, publish to ESWA (Expert Systems With\n  Applications)", "pdf_url": "http://arxiv.org/pdf/2507.23263v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.07486", "title": "Visual Story-Writing: Writing by Manipulating Visual Representations of Stories", "authors": ["Damien Masson", "Zixin Zhao", "Fanny Chevalier"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      In Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology (UIST '25)", "url": "http://arxiv.org/abs/2410.07486v2", "summary": "We define \"visual story-writing\" as using visual representations of story\nelements to support writing and revising narrative texts. To demonstrate this\napproach, we developed a text editor that automatically visualizes a graph of\nentity interactions, movement between locations, and a timeline of story\nevents. Interacting with these visualizations results in suggested text edits:\nfor example, connecting two characters in the graph creates an interaction\nbetween them, moving an entity updates their described location, and\nrearranging events on the timeline reorganizes the narrative sequence. Through\ntwo user studies on narrative text editing and writing, we found that visuals\nsupported participants in planning high-level revisions, tracking story\nelements, and exploring story variations in ways that encourage creativity.\nBroadly, our work lays the foundation for writing support, not just through\nwords, but also visuals.", "comment": "In Proceedings of the 38th Annual ACM Symposium on User Interface\n  Software and Technology (UIST '25)", "pdf_url": "http://arxiv.org/pdf/2410.07486v2", "cate": "cs.HC", "date": "2024-10-09", "updated": "2025-07-31"}
{"id": "2504.19105", "title": "Blended PC Peer Review Model: Process and Reflection", "authors": ["Chakkrit Tantithamthavorn", "Nicole Novielli", "Ayushi Rastogi", "Olga Baysal", "Bram Adams"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Published at ACM SIGSOFT Software Engineering Notes", "url": "http://arxiv.org/abs/2504.19105v2", "summary": "The academic peer review system is under increasing pressure due to a growing\nvolume of submissions and a limited pool of available reviewers, resulting in\ndelayed decisions and an uneven distribution of reviewing responsibilities.\nBuilding upon the International Conference on Mining Software Repositories\n(MSR) community's earlier experience with a Shadow PC (2021 and 2022) and\nJunior PC (2023 and 2024), MSR 2025 experimented with a Blended Program\nCommittee (PC) peer review model for its Technical Track. This new model pairs\nup one Junior PC member with two regular PC members as part of the core review\nteam of a given paper, instead of adding them as an extra reviewer. This paper\npresents the rationale, implementation, and reflections on the model, including\nempirical insights from a post-review author survey evaluating the quality and\nusefulness of reviews. Our findings highlight the potential of a Blended PC to\nalleviate reviewer shortages, foster inclusivity, and sustain a high-quality\npeer review process. We offer lessons learned and recommendations to guide\nfuture adoption and refinement of the model.", "comment": "Published at ACM SIGSOFT Software Engineering Notes", "pdf_url": "http://arxiv.org/pdf/2504.19105v2", "cate": "cs.SE", "date": "2025-04-27", "updated": "2025-07-31"}
{"id": "2507.23707", "title": "Cellular, Cell-less, and Everything in Between: A Unified Framework for Utility Region Analysis in Wireless Networks", "authors": ["Renato Luis Garrido Cavalcante", "Tomasz Piotrowski", "Slawomir Stanczak"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23707v1", "summary": "We introduce a unified framework for analyzing utility regions of wireless\nnetworks, with a focus on the signal-to-interference-noise-ratio (SINR) and\nachievable rate regions. The framework provides valuable insights into\ninterference patterns of modern network architectures, such as cell-less and\nextremely large MIMO networks, and it generalizes existing characterizations of\nthe weak Pareto boundary. A central contribution is the derivation of\nsufficient conditions that guarantee convexity of the utility regions.\nConvexity is an important property because it ensures that time sharing (or\nuser grouping) cannot simultaneously increase the utility of all users when the\nnetwork operates on the weak Pareto boundary. These sufficient conditions also\nhave two key implications. First, they identify a family of (weighted) sum-rate\nmaximization problems that are inherently convex without any variable\ntransformations, thus paving the way for the development of efficient, provably\noptimal solvers for this family. Second, they provide a rigorous justification\nfor formulating sum-rate maximization problems directly in terms of achievable\nrates, rather than SINR levels. Our theoretical insights also motivate an\nalternative to the concept of favorable propagation in the massive MIMO\nliterature -- one that explicitly accounts for self-interference and the\nbeamforming strategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23707v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23431", "title": "Towards a Testbed for Scalable FaaS Platforms", "authors": ["Trever Schirmer", "David Bermbach"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted for Publication at the 13th IEEE International Conference on Cloud Engineering (IC2E 2025)", "url": "http://arxiv.org/abs/2507.23431v1", "summary": "Most cloud platforms have a Function-as-a-Service (FaaS) offering that\nenables users to easily write highly scalable applications. To better\nunderstand how the platform's architecture impacts its performance, we present\na research-focused testbed that can be adapted to quickly evaluate the impact\nof different architectures and technologies on the characteristics of\nscalability-focused FaaS platforms.", "comment": "Accepted for Publication at the 13th IEEE International Conference on\n  Cloud Engineering (IC2E 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23431v1", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22928", "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding", "authors": ["Xi Chen", "Aske Plaat", "Niki van Stein"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22928v1", "summary": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on\nmulti-step tasks, yet whether the generated \"thoughts\" reflect the true\ninternal reasoning process is unresolved. We present the first feature-level\ncausal study of CoT faithfulness. Combining sparse autoencoders with activation\npatching, we extract monosemantic features from Pythia-70M and Pythia-2.8B\nwhile they tackle GSM8K math problems under CoT and plain (noCoT) prompting.\nSwapping a small set of CoT-reasoning features into a noCoT run raises answer\nlog-probabilities significantly in the 2.8B model, but has no reliable effect\nin 70M, revealing a clear scale threshold. CoT also leads to significantly\nhigher activation sparsity and feature interpretability scores in the larger\nmodel, signalling more modular internal computation. For example, the model's\nconfidence in generating correct answers improves from 1.2 to 4.3. We introduce\npatch-curves and random-feature patching baselines, showing that useful CoT\ninformation is not only present in the top-K patches but widely distributed.\nOverall, our results indicate that CoT can induce more interpretable internal\nstructures in high-capacity LLMs, validating its role as a structured prompting\nmethod.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22928v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2412.16107", "title": "Allocation for Omnidirectional Aerial Robots: Incorporating Power Dynamics", "authors": ["Eugenio Cuniato", "Mike Allenspach", "Thomas Stastny", "Helen Oleynikova", "Roland Siegwart", "Michael Pantic"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.16107v2", "summary": "Tilt-rotor aerial robots are more dynamic and versatile than fixed-rotor\nplatforms, since the thrust vector and body orientation are decoupled. However,\nthe coordination of servos and propellers (the allocation problem) is not\ntrivial, especially accounting for overactuation and actuator dynamics. We\nincrementally build and present three novel allocation methods for tiltrotor\naerial robots, comparing them to state-of-the-art methods on a real system\nperforming dynamic maneuvers. We extend the state-of-the-art geometric\nallocation into a differential allocation, which uses the platform's redundancy\nand does not suffer from singularities. We expand it by incorporating actuator\ndynamics and propeller power dynamics. These allow us to model dynamic\npropeller acceleration limits, bringing two main advantages: balancing\npropeller speed without the need of nullspace goals and allowing the platform\nto selectively turn-off propellers during flight, opening the door to new\nmanipulation possibilities. We also use actuator dynamics and limits to\nnormalize the allocation problem, making it easier to tune and allowing it to\ntrack 70% faster trajectories than a geometric allocation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.16107v2", "cate": "cs.RO", "date": "2024-12-20", "updated": "2025-07-31"}
{"id": "2507.23449", "title": "Manifold-regularised Signature Kernel Large-Margin $\\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection", "authors": ["Shervin Rahimzadeh Arashloo"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23449v1", "summary": "We generalise the recently introduced large-margin $\\ell_p$-SVDD approach to\nexploit the geometry of data distribution via manifold regularising and a\nsignature kernel representation for time series anomaly detection.\nSpecifically, we formulate a manifold-regularised variant of the $\\ell_p$-SVDD\nmethod to encourage label smoothness on the underlying manifold to capture\nstructural information for improved detection performance. Drawing on an\nexisting Representer theorem, we then provide an effective optimisation\ntechnique for the proposed method and show that it can benefit from the\nsignature kernel to capture time series complexities for anomaly detection.\n  We theoretically study the proposed approach using Rademacher complexities to\nanalyse its generalisation performance and also provide an experimental\nassessment of the proposed method across various data sets to compare its\nperformance against other methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23449v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23268", "title": "PixNerd: Pixel Neural Field Diffusion", "authors": ["Shuai Wang", "Ziteng Gao", "Chenhui Zhu", "Weilin Huang", "Limin Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      a single-scale, single-stage, efficient, end-to-end pixel space diffusion model", "url": "http://arxiv.org/abs/2507.23268v1", "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID\non ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.", "comment": "a single-scale, single-stage, efficient, end-to-end pixel space\n  diffusion model", "pdf_url": "http://arxiv.org/pdf/2507.23268v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.19101", "title": "Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems", "authors": ["Vaishali Dhanoa", "Anton Wolter", "Gabriela Molina León", "Hans-Jörg Schulz", "Niklas Elmqvist"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19101v2", "summary": "Autonomous agents powered by Large Language Models are transforming AI,\ncreating an imperative for the visualization field to embrace agentic\nframeworks. However, our field's focus on a human in the sensemaking loop\nraises critical questions about autonomy, delegation, and coordination for such\n\\textit{agentic visualization} that preserve human agency while amplifying\nanalytical capabilities. This paper addresses these questions by reinterpreting\nexisting visualization systems with semi-automated or fully automatic AI\ncomponents through an agentic lens. Based on this analysis, we extract a\ncollection of design patterns for agentic visualization, including agentic\nroles, communication and coordination. These patterns provide a foundation for\nfuture agentic visualization systems that effectively harness AI agents while\nmaintaining human insight and control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19101v2", "cate": "cs.HC", "date": "2025-05-25", "updated": "2025-07-31"}
{"id": "2505.04834", "title": "The Design Space of Lockfiles Across Package Managers", "authors": ["Yogya Gamage", "Deepika Tiwari", "Martin Monperrus", "Benoit Baudry"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.04834v2", "summary": "Software developers reuse third-party packages that are hosted in package\nregistries. At build time, a package manager resolves and fetches the direct\nand indirect dependencies of a project. Most package managers also generate a\nlockfile, which records the exact set of resolved dependency versions.\nLockfiles are used to reduce build times; to verify the integrity of resolved\npackages; and to support build reproducibility across environments and time.\nDespite these beneficial features, developers often struggle with their\nmaintenance, usage, and interpretation. In this study, we unveil the major\nchallenges related to lockfiles, such that future researchers and engineers can\naddress them. We perform the first comprehensive study of lockfiles across 7\npopular package managers, npm, pnpm, Cargo, Poetry, Pipenv, Gradle, and Go.\nFirst, we highlight the wide variety of design decisions that package managers\nmake, regarding the generation process as well as the content of lockfiles.\nNext, we conduct a qualitative analysis based on semi-structured interviews\nwith 15 developers. We capture first-hand insights about the benefits that\ndevelopers perceive in lockfiles, as well as the challenges they face to manage\nthese files. Following these observations, we make 5 recommendations to further\nimprove lockfiles, for a better developer experience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.04834v2", "cate": "cs.SE", "date": "2025-05-07", "updated": "2025-07-30"}
{"id": "2405.04261", "title": "Graph Reconstruction from Noisy Random Subgraphs", "authors": ["Andrew McGregor", "Rik Sengupta"], "categories": ["cs.IT", "cs.DS", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, to appear in ISIT 2024", "url": "http://arxiv.org/abs/2405.04261v2", "summary": "We consider the problem of reconstructing an undirected graph $G$ on $n$\nvertices given multiple random noisy subgraphs or \"traces\". Specifically, a\ntrace is generated by sampling each vertex with probability $p_v$, then taking\nthe resulting induced subgraph on the sampled vertices, and then adding noise\nin the form of either (a) deleting each edge in the subgraph with probability\n$1-p_e$, or (b) deleting each edge with probability $f_e$ and transforming a\nnon-edge into an edge with probability $f_e$. We show that, under mild\nassumptions on $p_v$, $p_e$ and $f_e$, if $G$ is selected uniformly at random,\nthen $O(p_e^{-1} p_v^{-2} \\log n)$ or $O((f_e-1/2)^{-2} p_v^{-2} \\log n)$\ntraces suffice to reconstruct $G$ with high probability. In contrast, if $G$ is\narbitrary, then $\\exp(\\Omega(n))$ traces are necessary even when $p_v=1,\np_e=1/2$.", "comment": "6 pages, to appear in ISIT 2024", "pdf_url": "http://arxiv.org/pdf/2405.04261v2", "cate": "cs.IT", "date": "2024-05-07", "updated": "2025-07-31"}
{"id": "2507.23533", "title": "Threshold-Driven Streaming Graph: Expansion and Rumor Spreading", "authors": ["Flora Angileri", "Andrea Clementi", "Emanuele Natale", "Michele Salvi", "Isabella Ziccardi"], "categories": ["cs.DC", "math.PR"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23533v1", "summary": "A randomized distributed algorithm called RAES was introduced in [Becchetti\net al., SODA 2020] to extract a bounded-degree expander from a dense $n$-vertex\nexpander graph $G = (V, E)$. The algorithm relies on a simple threshold-based\nprocedure. A key assumption in [Becchetti et al., SODA 2020] is that the input\ngraph $G$ is static - i.e., both its vertex set $V$ and edge set $E$ remain\nunchanged throughout the process - while the analysis of RAES in dynamic models\nis left as a major open question.\n  In this work, we investigate the behavior of RAES under a dynamic graph model\ninduced by a streaming node-churn process (also known as the sliding window\nmodel), where, at each discrete round, a new node joins the graph and the\noldest node departs. This process yields a bounded-degree dynamic graph\n$\\mathcal{G} =\\{ G_t = (V_t, E_t) : t \\in \\mathbb{N}\\}$ that captures essential\ncharacteristics of peer-to-peer networks -- specifically, node churn and\nthreshold on the number of connections each node can manage. We prove that\nevery snapshot $G_t$ in the dynamic graph sequence has good expansion\nproperties with high probability. Furthermore, we leverage this property to\nestablish a logarithmic upper bound on the completion time of the well-known\nPUSH and PULL rumor spreading protocols over the dynamic graph $\\mathcal{G}$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23533v1", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22931", "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "authors": ["Shuyu Guo", "Zhaochun Ren"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22931v1", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22931v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2412.20451", "title": "CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance", "authors": ["Jinming Li", "Yichen Zhu", "Zhibin Tang", "Junjie Wen", "Minjie Zhu", "Xiaoyu Liu", "Chengmeng Li", "Ran Cheng", "Yaxin Peng", "Yan Peng", "Feifei Feng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project webpage is available at this https URL", "url": "http://arxiv.org/abs/2412.20451v2", "summary": "Robot foundation models, particularly Vision-Language-Action (VLA) models,\nhave garnered significant attention for their ability to enhance robot policy\nlearning, greatly improving robot's generalization and robustness. OpenAI's\nrecent model, O1, showcased impressive capabilities in solving complex problems\nby utilizing extensive reasoning chains. This prompts an important question:\ncan robot models achieve better performance in multi-task , complex\nenvironments by reviewing prior observations and then providing task-specific\nreasoning to guide action prediction? In this paper, we introduce\nChain-of-Affordance (CoA-VLA) , a novel approach to scaling robot models by\nincorporating reasoning in the format of sequential robot affordances to\nfacilitate task completion. Specifically, we prompt the model to consider the\nfollowing four types of affordances before taking action: (1) object affordance\n- what object to manipulate and where it is ; (2) grasp affordance - the\nspecific object part to grasp ; (3) spatial affordance - the optimal space to\nplace the object ; and (4) movement affordance-the collision - free path for\nmovement. We further transform each affordance into two prompting formats:\nvisual affordance and textual affordance. We introduce a novel vision-language\nco-injection module that integrates this knowledge into the policy network.\nThis allows the robot to leverage essential contextual information during\naction inference, resulting in improved precision and robustness. Our\nexperiments demonstrate that CoA-VLA outperforms state-of-the-art robot\nfoundation models, including OpenVLA and Octo, on a variety of tasks.\nFurthermore, CoA-VLA exhibits strong generalization capabilities, including\nrecognizing unseen object poses, identifying free space, and avoiding obstacles\nin novel environments.", "comment": "Project webpage is available at https://chain-of-affordance.github.io", "pdf_url": "http://arxiv.org/pdf/2412.20451v2", "cate": "cs.RO", "date": "2024-12-29", "updated": "2025-07-31"}
{"id": "2507.23491", "title": "Explainable artificial intelligence model predicting the risk of all-cause mortality in patients with type 2 diabetes mellitus", "authors": ["Olga Vershinina", "Jacopo Sabbatinelli", "Anna Rita Bonfigli", "Dalila Colombaretti", "Angelica Giuliani", "Mikhail Krivonosov", "Arseniy Trukhanov", "Claudio Franceschi", "Mikhail Ivanchenko", "Fabiola Olivieri"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23491v1", "summary": "Objective. Type 2 diabetes mellitus (T2DM) is a highly prevalent\nnon-communicable chronic disease that substantially reduces life expectancy.\nAccurate estimation of all-cause mortality risk in T2DM patients is crucial for\npersonalizing and optimizing treatment strategies. Research Design and Methods.\nThis study analyzed a cohort of 554 patients (aged 40-87 years) with diagnosed\nT2DM over a maximum follow-up period of 16.8 years, during which 202 patients\n(36%) died. Key survival-associated features were identified, and multiple\nmachine learning (ML) models were trained and validated to predict all-cause\nmortality risk. To improve model interpretability, Shapley additive\nexplanations (SHAP) was applied to the best-performing model. Results. The\nextra survival trees (EST) model, incorporating ten key features, demonstrated\nthe best predictive performance. The model achieved a C-statistic of 0.776,\nwith the area under the receiver operating characteristic curve (AUC) values of\n0.86, 0.80, 0.841, and 0.826 for 5-, 10-, 15-, and 16.8-year all-cause\nmortality predictions, respectively. The SHAP approach was employed to\ninterpret the model's individual decision-making processes. Conclusions. The\ndeveloped model exhibited strong predictive performance for mortality risk\nassessment. Its clinically interpretable outputs enable potential bedside\napplication, improving the identification of high-risk patients and supporting\ntimely treatment optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23491v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23272", "title": "Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2", "authors": ["Solha Kang", "Eugene Kim", "Joris Vankerschaver", "Utku Ozbulak"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in Breast Care (DeepBreath), 2025", "url": "http://arxiv.org/abs/2507.23272v1", "summary": "Breast MRI provides high-resolution volumetric imaging critical for tumor\nassessment and treatment planning, yet manual interpretation of 3D scans\nremains labor-intensive and subjective. While AI-powered tools hold promise for\naccelerating medical image analysis, adoption of commercial medical AI products\nremains limited in low- and middle-income countries due to high license costs,\nproprietary software, and infrastructure demands. In this work, we investigate\nwhether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,\nminimal-input 3D tumor segmentation in breast MRI. Using a single bounding box\nannotation on one slice, we propagate segmentation predictions across the 3D\nvolume using three different slice-wise tracking strategies: top-to-bottom,\nbottom-to-top, and center-outward. We evaluate these strategies across a large\ncohort of patients and find that center-outward propagation yields the most\nconsistent and accurate segmentations. Despite being a zero-shot model not\ntrained for volumetric medical data, SAM2 achieves strong segmentation\nperformance under minimal supervision. We further analyze how segmentation\nperformance relates to tumor size, location, and shape, identifying key failure\nmodes. Our results suggest that general-purpose foundation models such as SAM2\ncan support 3D medical image analysis with minimal supervision, offering an\naccessible and affordable alternative for resource-constrained settings.", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep\n  Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in\n  Breast Care (DeepBreath), 2025", "pdf_url": "http://arxiv.org/pdf/2507.23272v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.06460", "title": "Ragged Blocks: Rendering Structured Text with Style", "authors": ["Sam Cohen", "Ravi Chugh"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      UIST 2025 Paper + Appendices", "url": "http://arxiv.org/abs/2507.06460v2", "summary": "Whether it be source code in a programming language, prose in natural\nlanguage, or otherwise, text is highly structured. Currently, text\nvisualizations are confined either to _flat, line-based_ decorations, which can\nconvey only limited information about textual structure, or _nested boxes_,\nwhich convey structure but often destroy the typographic layout of the\nunderlying text. We hypothesize that the lack of rich styling options limits\nthe kinds of information that are displayed alongside text, wherever it may be\ndisplayed.\n  In this paper, we show that it is possible to achieve arbitrarily nested\ndecorations while minimally disturbing the underlying typographic layout.\nSpecifically, we present a layout algorithm that generates _ragged blocks_, or\n_rocks_, which are rectilinear polygons that allow nested text to be compactly\nrendered even when styled with borders and padding. Our layout algorithm is\nevaluated on a benchmark suite comprising representative source code files in\nmultiple programming languages. The (ragged block) layouts produced by our\nalgorithm are substantially more compact than the (rectangular block) layouts\nproduced by conventional techniques, when uniformly styling every element in\nthe syntax tree with borders and padding.", "comment": "UIST 2025 Paper + Appendices", "pdf_url": "http://arxiv.org/pdf/2507.06460v2", "cate": "cs.HC", "date": "2025-07-09", "updated": "2025-07-31"}
{"id": "2507.19115", "title": "Automated Code Review Using Large Language Models at Ericsson: An Experience Report", "authors": ["Shweta Ramesh", "Joy Bose", "Hamender Singh", "A K Raghavan", "Sujoy Roychowdhury", "Giriprasad Sridhara", "Nishrith Saini", "Ricardo Britto"], "categories": ["cs.SE", "cs.AI", "D.2.7"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, 1 table. Accepted in ICSME 2025 conference in Auckland", "url": "http://arxiv.org/abs/2507.19115v2", "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results.", "comment": "6 pages, 4 figures, 1 table. Accepted in ICSME 2025 conference in\n  Auckland", "pdf_url": "http://arxiv.org/pdf/2507.19115v2", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2409.14148", "title": "A New Upper Bound for Distributed Hypothesis Testing Using the Auxiliary Receiver Approach", "authors": ["Zhenduo Wen", "Amin Gohari"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.14148v4", "summary": "This paper employs the add-and-subtract technique of the auxiliary receiver\napproach to establish a new upper bound for the distributed hypothesis testing\nproblem. This new bound has fewer assumptions than the upper bound proposed by\nRahman and Wagner, is at least as tight as the bound by Rahman and Wagner, and\ncan outperform it in certain Gaussian settings. Conceptually speaking, unlike\nRahman and Wagner, who view their additional receiver as side information, we\nview it as an auxiliary receiver and use a different manipulation for\nsingle-letterization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.14148v4", "cate": "cs.IT", "date": "2024-09-21", "updated": "2025-07-31"}
{"id": "2507.23700", "title": "The ArborX library: version 2.0", "authors": ["Andrey Prokopenko", "Daniel Arndt", "Damien Lebrun-Grandié", "Bruno Turcksin"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23700v1", "summary": "This paper provides an overview of the 2.0 release of the ArborX library, a\nperformance portable geometric search library based on Kokkos. We describe the\nmajor changes in ArborX 2.0 including a new interface for the library to\nsupport a wider range of user problems, new search data structures (brute\nforce, distributed), support for user functions to be executed on the results\n(callbacks), and an expanded set of the supported algorithms (ray tracing,\nclustering).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23700v1", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23020", "title": "Axioms for Model Fidelity Evaluation", "authors": ["Evan Taylor", "Edward Louis", "Gregory Mocko"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      This is the authors' preprint (submitted version) of a paper accepted for ASME IDETC/CIE 2025, edited only to note preprint status. Posted in compliance with ASME 's preprint and copyright policy. The final version is copyrighted by ASME and will appear in the ASME Digital Collection. The DOI will be added once published", "url": "http://arxiv.org/abs/2507.23020v1", "summary": "Digital engineering has transformed the design and development process.\nHowever, the utility of digital engineering is fundamentally dependent on the\nassumption that a simulation provides information consistent with reality. This\nrelationship is described as model fidelity. Despite the widespread use of the\nterm, existing definitions of model fidelity often lack formal rigor in\npractical application, which leaves ambiguity in how this similarity should be\nevaluated. This paper presents seven fundamental axioms to aid the development\nof future fidelity evaluation frameworks. An example of a ground vehicle model\nis used under an existing fidelity evaluation framework to observe the\napplicability of these axioms. In addition, these axioms are used as a\nreference point for considering future opportunities in future work related to\nmodel fidelity.", "comment": "This is the authors' preprint (submitted version) of a paper accepted\n  for ASME IDETC/CIE 2025, edited only to note preprint status. Posted in\n  compliance with ASME 's preprint and copyright policy. The final version is\n  copyrighted by ASME and will appear in the ASME Digital Collection. The DOI\n  will be added once published", "pdf_url": "http://arxiv.org/pdf/2507.23020v1", "cate": "cs.CE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22933", "title": "Augmented Vision-Language Models: A Systematic Review", "authors": ["Anthony C Davis", "Burhan Sadiq", "Tianmin Shu", "Chien-Ming Huang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22933v1", "summary": "Recent advances in visual-language machine learning models have demonstrated\nexceptional ability to use natural language and understand visual scenes by\ntraining on large, unstructured datasets. However, this training paradigm\ncannot produce interpretable explanations for its outputs, requires retraining\nto integrate new information, is highly resource-intensive, and struggles with\ncertain forms of logical reasoning. One promising solution involves integrating\nneural networks with external symbolic information systems, forming neural\nsymbolic systems that can enhance reasoning and memory abilities. These neural\nsymbolic systems provide more interpretable explanations to their outputs and\nthe capacity to assimilate new information without extensive retraining.\nUtilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural\ncomponent, augmented by external systems, offers a pragmatic approach to\nrealizing the benefits of neural-symbolic integration. This systematic\nliterature review aims to categorize techniques through which visual-language\nunderstanding can be improved by interacting with external symbolic information\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22933v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2501.02184", "title": "Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments", "authors": ["Ahmed A. Elgohary", "Sameh A. Eisa", "Shivam Bajpai"], "categories": ["cs.RO", "math.OC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.02184v4", "summary": "Many autonomous robots aimed at source-seeking are studied, and their\ncontrols designed, using unicycle modeling and formulation. This is true not\nonly for model-based controllers, but also for model-free, real-time control\nmethods such as extremum seeking control (ESC). In this paper, we propose a\nunicycle-based ESC design applicable to differential wheeled robots that: (1)\nis very simple design, based on one simple control-affine law, and without\nstate integrators; (2) attenuates oscillations known to persist in ESC designs\n(i.e., fully stop at the source); and (3) operates in a model-free, real-time\nsetting, tolerating environmental/sensor noise. We provide simulation and\nreal-world robotic experimental results for fixed and moving light source\nseeking by a differential wheeled robot using our proposed design. Results\nindicate clear advantages of our proposed design when compared to the\nliterature, including attenuation of undesired oscillations, improved\nconvergence speed, and better handling of noise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.02184v4", "cate": "cs.RO", "date": "2025-01-04", "updated": "2025-07-31"}
{"id": "2507.23495", "title": "Incorporating structural uncertainty in causal decision making", "authors": ["Maurits Kaptein"], "categories": ["cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This work is under review at the Journal of Causal Inference", "url": "http://arxiv.org/abs/2507.23495v1", "summary": "Practitioners making decisions based on causal effects typically ignore\nstructural uncertainty. We analyze when this uncertainty is consequential\nenough to warrant methodological solutions (Bayesian model averaging over\ncompeting causal structures). Focusing on bivariate relationships ($X\n\\rightarrow Y$ vs. $X \\leftarrow Y$), we establish that model averaging is\nbeneficial when: (1) structural uncertainty is moderate to high, (2) causal\neffects differ substantially between structures, and (3) loss functions are\nsufficiently sensitive to the size of the causal effect. We prove optimality\nresults of our suggested methodological solution under regularity conditions\nand demonstrate through simulations that modern causal discovery methods can\nprovide, within limits, the necessary quantification. Our framework complements\nexisting robust causal inference approaches by addressing a distinct source of\nuncertainty typically overlooked in practice.", "comment": "This work is under review at the Journal of Causal Inference", "pdf_url": "http://arxiv.org/pdf/2507.23495v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23277", "title": "iLRM: An Iterative Large 3D Reconstruction Model", "authors": ["Gyeongjin Kang", "Seungtae Nam", "Xiangyu Sun", "Sameh Khamis", "Abdelrahman Mohamed", "Eunbyung Park"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.23277v1", "summary": "Feed-forward 3D modeling has emerged as a promising approach for rapid and\nhigh-quality 3D reconstruction. In particular, directly generating explicit 3D\nrepresentations, such as 3D Gaussian splatting, has attracted significant\nattention due to its fast and high-quality rendering, as well as numerous\napplications. However, many state-of-the-art methods, primarily based on\ntransformer architectures, suffer from severe scalability issues because they\nrely on full attention across image tokens from multiple input views, resulting\nin prohibitive computational costs as the number of views or image resolution\nincreases. Toward a scalable and efficient feed-forward 3D reconstruction, we\nintroduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D\nGaussian representations through an iterative refinement mechanism, guided by\nthree core principles: (1) decoupling the scene representation from input-view\nimages to enable compact 3D representations; (2) decomposing fully-attentional\nmulti-view interactions into a two-stage attention scheme to reduce\ncomputational costs; and (3) injecting high-resolution information at every\nlayer to achieve high-fidelity reconstruction. Experimental results on widely\nused datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms\nexisting methods in both reconstruction quality and speed. Notably, iLRM\nexhibits superior scalability, delivering significantly higher reconstruction\nquality under comparable computational cost by efficiently leveraging a larger\nnumber of input views.", "comment": "Project page: https://gynjn.github.io/iLRM/", "pdf_url": "http://arxiv.org/pdf/2507.23277v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22252", "title": "Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving", "authors": ["Kexin Liang", "Jan Luca Kästle", "Bani Anvari", "Simeon C. Calvert", "J. W. C. van Lint"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22252v2", "summary": "When automated driving systems encounter complex situations beyond their\noperational capabilities, they issue takeover requests, prompting drivers to\nresume vehicle control and return to the driving loop as a critical safety\nbackup. However, this control transition places significant demands on drivers,\nrequiring them to promptly respond to takeover requests while executing\nhigh-quality interventions. To ensure safe and comfortable control transitions,\nit is essential to develop a deep understanding of the key factors influencing\nvarious takeover performance aspects. This study evaluates drivers' takeover\nperformance across three dimensions: response efficiency, user experience, and\ndriving safety - using a driving simulator experiment. EXtreme Gradient\nBoosting (XGBoost) models are used to investigate the contributions of two\ncritical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in\npredicting various takeover performance metrics by comparing the predictive\nresults to the baseline models that rely solely on basic Driver Characteristics\n(DC). The results reveal that (i) higher SA enables drivers to respond to\ntakeover requests more quickly, particularly for reflexive responses; and (ii)\nSC shows a greater overall impact on takeover quality than SA, where higher SC\ngenerally leads to enhanced subjective rating scores and objective execution\ntrajectories. These findings highlight the distinct yet complementary roles of\nSA and SC in shaping performance components, offering valuable insights for\noptimizing human-vehicle interactions and enhancing automated driving system\ndesign.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22252v2", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2501.11842", "title": "Harnessing Rydberg Atomic Receivers: From Quantum Physics to Wireless Communications", "authors": ["Yuanbin Chen", "Xufeng Guo", "Chau Yuen", "Yufei Zhao", "Yong Liang Guan", "Chong Meng Samson See", "Merouane Débbah", "Lajos Hanzo"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This revised manuscript has been submitted to IEEE journal, 16 pages, 10 figures", "url": "http://arxiv.org/abs/2501.11842v2", "summary": "The intrinsic integration of Rydberg atomic receivers into wireless\ncommunication systems is proposed, by harnessing the principles of quantum\nphysics in wireless communications. More particularly, we conceive a pair of\nRydberg atomic receivers, one incorporates a local oscillator (LO), referred to\nas an LO-dressed receiver, while the other operates without an LO and is termed\nan LO-free receiver. The appropriate wireless model is developed for each\nconfiguration, elaborating on the receiver's responses to the radio frequency\n(RF) signal, on the potential noise sources, and on the signal-to-noise ratio\n(SNR) performance. The developed wireless model conforms to the classical RF\nframework, facilitating compatibility with established signal processing\nmethodologies. Next, we investigate the associated distortion effects that\nmight occur, specifically identifying the conditions under which distortion\narises and demonstrating the boundaries of linear dynamic ranges. This provides\ncritical insights into its practical implementations in wireless systems.\nFinally, extensive simulation results are provided for characterizing the\nperformance of wireless systems, harnessing this pair of Rydberg atomic\nreceivers. Our results demonstrate that LO-dressed systems achieve a\nsignificant SNR gain of approximately 40~50 dB over conventional RF receivers\nin the standard quantum limit regime. This SNR head-room translates into\nreduced symbol error rates, enabling efficient and reliable transmission with\nhigher-order constellations.", "comment": "This revised manuscript has been submitted to IEEE journal, 16 pages,\n  10 figures", "pdf_url": "http://arxiv.org/pdf/2501.11842v2", "cate": "cs.IT", "date": "2025-01-21", "updated": "2025-07-31"}
{"id": "2507.23609", "title": "Consistent Point Matching", "authors": ["Halid Ziya Yerebakan", "Gerardo Hermosillo Valadez"], "categories": ["cs.CV", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23609v1", "summary": "This study demonstrates that incorporating a consistency heuristic into the\npoint-matching algorithm \\cite{yerebakan2023hierarchical} improves robustness\nin matching anatomical locations across pairs of medical images. We validated\nour approach on diverse longitudinal internal and public datasets spanning CT\nand MRI modalities. Notably, it surpasses state-of-the-art results on the Deep\nLesion Tracking dataset. Additionally, we show that the method effectively\naddresses landmark localization. The algorithm operates efficiently on standard\nCPU hardware and allows configurable trade-offs between speed and robustness.\nThe method enables high-precision navigation between medical images without\nrequiring a machine learning model or training data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23609v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23218", "title": "An Information Bottleneck Asset Pricing Model", "authors": ["Che Sun"], "categories": ["cs.CE", "cs.AI"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23218v1", "summary": "Deep neural networks (DNNs) have garnered significant attention in financial\nasset pricing, due to their strong capacity for modeling complex nonlinear\nrelationships within financial data. However, sophisticated models are prone to\nover-fitting to the noise information in financial data, resulting in inferior\nperformance. To address this issue, we propose an information bottleneck asset\npricing model that compresses data with low signal-to-noise ratios to eliminate\nredundant information and retain the critical information for asset pricing.\nOur model imposes constraints of mutual information during the nonlinear\nmapping process. Specifically, we progressively reduce the mutual information\nbetween the input data and the compressed representation while increasing the\nmutual information between the compressed representation and the output\nprediction. The design ensures that irrelevant information, which is\nessentially the noise in the data, is forgotten during the modeling of\nfinancial nonlinear relationships without affecting the final asset pricing. By\nleveraging the constraints of the Information bottleneck, our model not only\nharnesses the nonlinear modeling capabilities of deep networks to capture the\nintricate relationships within financial data but also ensures that noise\ninformation is filtered out during the information compression process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23218v1", "cate": "cs.CE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22934", "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey", "authors": ["Jingwei Zhao", "Yuhua Wen", "Qifei Li", "Minchi Hu", "Yingying Zhou", "Jingyao Xue", "Junyang Wu", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Submitted to ACM Computing Surveys", "url": "http://arxiv.org/abs/2507.22934v1", "summary": "Intent recognition aims to identify users' underlying intentions,\ntraditionally focusing on text in natural language processing. With growing\ndemands for natural human-computer interaction, the field has evolved through\ndeep learning and multimodal approaches, incorporating data from audio, vision,\nand physiological signals. Recently, the introduction of Transformer-based\nmodels has led to notable breakthroughs in this domain. This article surveys\ndeep learning methods for intent recognition, covering the shift from unimodal\nto multimodal techniques, relevant datasets, methodologies, applications, and\ncurrent challenges. It provides researchers with insights into the latest\ndevelopments in multimodal intent recognition (MIR) and directions for future\nresearch.", "comment": "Submitted to ACM Computing Surveys", "pdf_url": "http://arxiv.org/pdf/2507.22934v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2503.03449", "title": "Tiny LiDARs for Manipulator Self-Awareness: Sensor Characterization and Initial Localization Experiments", "authors": ["Giammarco Caroleo", "Alessandro Albini", "Daniele De Martini", "Timothy D. Barfoot", "Perla Maiolino"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, 3 tables, IEEE/RSJ International Conference on Intelligent Robots and Systems 2025 accepted paper", "url": "http://arxiv.org/abs/2503.03449v2", "summary": "For several tasks, ranging from manipulation to inspection, it is beneficial\nfor robots to localize a target object in their surroundings. In this paper, we\npropose an approach that utilizes coarse point clouds obtained from\nminiaturized VL53L5CX Time-of-Flight (ToF) sensors (tiny LiDARs) to localize a\ntarget object in the robot's workspace. We first conduct an experimental\ncampaign to calibrate the dependency of sensor readings on relative range and\norientation to targets. We then propose a probabilistic sensor model, which we\nvalidate in an object pose estimation task using a Particle Filter (PF). The\nresults show that the proposed sensor model improves the performance of the\nlocalization of the target object with respect to two baselines: one that\nassumes measurements are free from uncertainty and one in which the confidence\nis provided by the sensor datasheet.", "comment": "7 pages, 6 figures, 3 tables, IEEE/RSJ International Conference on\n  Intelligent Robots and Systems 2025 accepted paper", "pdf_url": "http://arxiv.org/pdf/2503.03449v2", "cate": "cs.RO", "date": "2025-03-05", "updated": "2025-07-31"}
{"id": "2507.23501", "title": "Directional Ensemble Aggregation for Actor-Critics", "authors": ["Nicklas Werge", "Yi-Shan Wu", "Bahareh Tasdighi", "Melih Kandemir"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23501v1", "summary": "Off-policy reinforcement learning in continuous control tasks depends\ncritically on accurate $Q$-value estimates. Conservative aggregation over\nensembles, such as taking the minimum, is commonly used to mitigate\noverestimation bias. However, these static rules are coarse, discard valuable\ninformation from the ensemble, and cannot adapt to task-specific needs or\ndifferent learning regimes. We propose Directional Ensemble Aggregation (DEA),\nan aggregation method that adaptively combines $Q$-value estimates in\nactor-critic frameworks. DEA introduces two fully learnable directional\nparameters: one that modulates critic-side conservatism and another that guides\nactor-side policy exploration. Both parameters are learned using ensemble\ndisagreement-weighted Bellman errors, which weight each sample solely by the\ndirection of its Bellman error. This directional learning mechanism allows DEA\nto adjust conservatism and exploration in a data-driven way, adapting\naggregation to both uncertainty levels and the phase of training. We evaluate\nDEA across continuous control benchmarks and learning regimes - from\ninteractive to sample-efficient - and demonstrate its effectiveness over static\nensemble strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23501v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23278", "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing", "authors": ["Hao Tang", "Chenwei Xie", "Xiaoyi Bao", "Tingyu Weng", "Pandeng Li", "Yun Zheng", "Liwei Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23278v1", "summary": "In this paper, we propose UniLIP, which extends CLIP to reconstruction,\ngeneration and editing, thereby building a unified tokenizer upon its\nexceptional comprehension capabilities. Previous CLIP-based unified methods\noften require additional diffusion decoders or quantization to support\nreconstruction and generation tasks, leading to inconsistent reconstruction or\ndegradation of original comprehension performance.In contrast, we introduce a\ntwo-stage training scheme and a self-distillation strategy that progressively\nintegrates reconstruction capabilities into CLIP, allowing it to maintain\noriginal comprehension performance while achieving effective image\nreconstruction. Furthermore, we propose a dual-condition architecture to\nconnect the MLLM and diffusion transformer, using both learnable queries and\nthe last layer multimodal hidden states as joint conditions. This method not\nonly enables the utilization of the MLLM's strong reasoning capabilities in\ngeneration tasks, but also maximizes the exploitation of the rich information\nin UniLIP features during editing tasks. In text-to-image generation tasks,\nUniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark\nrespectively, surpassing all previous unified models of similar scale. In image\nediting, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,\nsurpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP\neffectively expand the application scope of CLIP, enabling continuous CLIP\nfeatures to not only serve as the optimal choice for understanding tasks but\nalso achieve highly competitive performance in generation and editing tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23278v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2401.13481", "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment", "authors": ["Joshua Ashkinaze", "Julia Mendelsohn", "Li Qiwei", "Ceren Budak", "Eric Gilbert"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted at ACM Collective Intelligence 2025. Originally posted 2024", "url": "http://arxiv.org/abs/2401.13481v3", "summary": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity.", "comment": "Accepted at ACM Collective Intelligence 2025. Originally posted 2024", "pdf_url": "http://arxiv.org/pdf/2401.13481v3", "cate": "cs.CY", "date": "2024-01-24", "updated": "2025-07-31"}
{"id": "2504.05326", "title": "Totally Disjoint 3-Digit Decimal Check Digit Codes", "authors": ["Larry A. Dunning"], "categories": ["cs.IT", "math.CO", "math.IT", "68P30, 94B25, 05B15, 05B40, 20N15", "H.1.1; G.2.1; F.2.1"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05326v2", "summary": "In 1969 J. Verhoeff provided the first examples of a decimal error detecting\ncode using a single check digit to provide protection against all single,\ntransposition and adjacent twin errors. The three versions of such a code that\nhe presented are length 3-digit codes with 2 information digits. Existence of a\n4-digit code would imply the existence of 10 such disjoint 3-digit codes. This\npaper presents 3 pairwise disjoint 3-digit codes. The codes developed herein,\nhave the property that the knowledge of the multiset of digits included in a\nword is sufficient to determine the entire codeword even though their positions\nwere unknown. Thus the codes are permutation-free, and this fulfills Verhoeff's\ndesire to eliminate \"cyclic errors\". Phonetic errors, where 2 digit pairs of\nthe forms X0 and 1X are interchanged, are also eliminated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05326v2", "cate": "cs.IT", "date": "2025-03-26", "updated": "2025-07-31"}
{"id": "2505.12928", "title": "Minos: Exploiting Cloud Performance Variation with Function-as-a-Service Instance Selection", "authors": ["Trever Schirmer", "Valentin Carl", "Nils Höller", "Tobias Pfandzelter", "David Bermbach"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted for Publication at the 13th IEEE International Conference on Cloud Engineering (IC2E 2025)", "url": "http://arxiv.org/abs/2505.12928v2", "summary": "Serverless Function-as-a-Service (FaaS) is a popular cloud paradigm to\nquickly and cheaply implement complex applications. Because the function\ninstances cloud providers start to execute user code run on shared\ninfrastructure, their performance can vary. From a user perspective, slower\ninstances not only take longer to complete, but also increase cost due to the\npay-per-use model of FaaS services where execution duration is billed with\nmicrosecond accuracy. In this paper, we present Minos, a system to take\nadvantage of this performance variation by intentionally terminating instances\nthat are slow. Fast instances are not terminated, so that they can be re-used\nfor subsequent invocations. One use case for this are data processing and\nmachine learning workflows, which often download files as a first step, during\nwhich Minos can run a short benchmark. Only if the benchmark passes, the main\npart of the function is actually executed. Otherwise, the request is re-queued\nand the instance crashes itself, so that the platform has to assign the request\nto another (potentially faster) instance. In our experiments, this leads to a\nspeedup of up to 13% in the resource intensive part of a data processing\nworkflow, resulting in up to 4% faster overall performance (and consequently 4%\ncheaper prices). Longer and complex workflows lead to increased savings, as the\npool of fast instances is re-used more often. For platforms exhibiting this\nbehavior, users get better performance and save money by wasting more of the\nplatforms resources.", "comment": "Accepted for Publication at the 13th IEEE International Conference on\n  Cloud Engineering (IC2E 2025)", "pdf_url": "http://arxiv.org/pdf/2505.12928v2", "cate": "cs.DC", "date": "2025-05-19", "updated": "2025-07-31"}
{"id": "2507.23443", "title": "Adjoint-Based Aerodynamic Shape Optimization with a Manifold Constraint Learned by Diffusion Models", "authors": ["Long Chen", "Emre Oezkaya", "Jan Rottmayer", "Nicolas R. Gauger", "Zebang Shen", "Yinyu Ye"], "categories": ["cs.CE", "cs.LG", "math.OC"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23443v1", "summary": "We introduce an adjoint-based aerodynamic shape optimization framework that\nintegrates a diffusion model trained on existing designs to learn a smooth\nmanifold of aerodynamically viable shapes. This manifold is enforced as an\nequality constraint to the shape optimization problem. Central to our method is\nthe computation of adjoint gradients of the design objectives (e.g., drag and\nlift) with respect to the manifold space. These gradients are derived by first\ncomputing shape derivatives with respect to conventional shape design\nparameters (e.g., Hicks-Henne parameters) and then backpropagating them through\nthe diffusion model to its latent space via automatic differentiation. Our\nframework preserves mathematical rigor and can be integrated into existing\nadjoint-based design workflows with minimal modification. Demonstrated on\nextensive transonic RANS airfoil design cases using off-the-shelf and\ngeneral-purpose nonlinear optimizers, our approach eliminates ad hoc parameter\ntuning and variable scaling, maintains robustness across initialization and\noptimizer choices, and achieves superior aerodynamic performance compared to\nconventional approaches. This work establishes how AI generated priors\nintegrates effectively with adjoint methods to enable robust, high-fidelity\naerodynamic shape optimization through automatic differentiation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23443v1", "cate": "cs.CE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22946", "title": "SmartCourse: A Contextual AI-Powered Course Advising System for Undergraduates", "authors": ["Yixuan Mi", "Yiduo Yu", "Yiyi Zhao"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, 1 table. *Corresponding author: Yixuan Mi. Code: this https URL", "url": "http://arxiv.org/abs/2507.22946v1", "summary": "We present SmartCourse, an integrated course management and AI-driven\nadvising system for undergraduate students (specifically tailored to the\nComputer Science (CPS) major). SmartCourse addresses the limitations of\ntraditional advising tools by integrating transcript and plan information for\nstudent-specific context. The system combines a command-line interface (CLI)\nand a Gradio web GUI for instructors and students, manages user accounts,\ncourse enrollment, grading, and four-year degree plans, and integrates a\nlocally hosted large language model (via Ollama) for personalized course\nrecommendations. It leverages transcript and major plan to offer contextual\nadvice (e.g., prioritizing requirements or retakes). We evaluated the system on\n25 representative advising queries and introduced custom metrics: PlanScore,\nPersonalScore, Lift, and Recall to assess recommendation quality across\ndifferent context conditions. Experiments show that using full context yields\nsubstantially more relevant recommendations than context-omitted modes,\nconfirming the necessity of transcript and plan information for personalized\nacademic advising. SmartCourse thus demonstrates how transcript-aware AI can\nenhance academic planning.", "comment": "7 pages, 6 figures, 1 table. *Corresponding author: Yixuan Mi. Code:\n  https://github.com/EthanYixuanMi/Smartcourse-Contextual-Advising", "pdf_url": "http://arxiv.org/pdf/2507.22946v1", "cate": "cs.CY", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.22935", "title": "Trusted Knowledge Extraction for Operations and Maintenance Intelligence", "authors": ["Kathleen Mealey", "Jonathan A. Karr Jr.", "Priscila Saboia Moreira", "Paul R. Brenner", "Charles F. Vardeman II"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22935v1", "summary": "Deriving operational intelligence from organizational data repositories is a\nkey challenge due to the dichotomy of data confidentiality vs data integration\nobjectives, as well as the limitations of Natural Language Processing (NLP)\ntools relative to the specific knowledge structure of domains such as\noperations and maintenance. In this work, we discuss Knowledge Graph\nconstruction and break down the Knowledge Extraction process into its Named\nEntity Recognition, Coreference Resolution, Named Entity Linking, and Relation\nExtraction functional components. We then evaluate sixteen NLP tools in concert\nwith or in comparison to the rapidly advancing capabilities of Large Language\nModels (LLMs). We focus on the operational and maintenance intelligence use\ncase for trusted applications in the aircraft industry. A baseline dataset is\nderived from a rich public domain US Federal Aviation Administration dataset\nfocused on equipment failures or maintenance requirements. We assess the\nzero-shot performance of NLP and LLM tools that can be operated within a\ncontrolled, confidential environment (no data is sent to third parties). Based\non our observation of significant performance limitations, we discuss the\nchallenges related to trusted NLP and LLM tools as well as their Technical\nReadiness Level for wider use in mission-critical industries such as aviation.\nWe conclude with recommendations to enhance trust and provide our open-source\ncurated dataset to support further baseline testing and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22935v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2503.05911", "title": "Generalizable Image Repair for Robust Visual Control", "authors": ["Carson Sobolewski", "Zhenjiang Mao", "Kshitij Maruti Vejre", "Ivan Ruchkin"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures, 2 tables, 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2503.05911v2", "summary": "Vision-based control relies on accurate perception to achieve robustness.\nHowever, image distribution changes caused by sensor noise, adverse weather,\nand dynamic lighting can degrade perception, leading to suboptimal control\ndecisions. Existing approaches, including domain adaptation and adversarial\ntraining, improve robustness but struggle to generalize to unseen corruptions\nwhile introducing computational overhead. To address this challenge, we propose\na real-time image repair module that restores corrupted images before they are\nused by the controller. Our method leverages generative adversarial models,\nspecifically CycleGAN and pix2pix, for image repair. CycleGAN enables unpaired\nimage-to-image translation to adapt to novel corruptions, while pix2pix\nexploits paired image data when available to improve the quality. To ensure\nalignment with control performance, we introduce a control-focused loss\nfunction that prioritizes perceptual consistency in repaired images. We\nevaluated our method in a simulated autonomous racing environment with various\nvisual corruptions. The results show that our approach significantly improves\nperformance compared to baselines, mitigating distribution shift and enhancing\ncontroller reliability.", "comment": "8 pages, 4 figures, 2 tables, 2025 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2503.05911v2", "cate": "cs.RO", "date": "2025-03-07", "updated": "2025-07-31"}
{"id": "2507.23504", "title": "A Verifier Hierarchy", "authors": ["Maurits Kaptein"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This paper is primarily relevant to cs.CC, but submitted under this http URL due to lack of endorsement. The paper is under review at \"Information and Communication\"", "url": "http://arxiv.org/abs/2507.23504v1", "summary": "We investigate the trade-off between certificate length and verifier runtime.\nWe prove a Verifier Trade-off Theorem showing that reducing the inherent\nverification time of a language from \\(f(n)\\) to \\(g(n)\\), where \\(f(n) \\ge\ng(n)\\), requires certificates of length at least \\(\\Omega(\\log(f(n) / g(n)))\\).\nThis theorem induces a natural hierarchy based on certificate complexity. We\ndemonstrate its applicability to analyzing conjectured separations between\ncomplexity classes (e.g., \\(\\np\\) and \\(\\exptime\\)) and to studying natural\nproblems such as string periodicity and rotation detection. Additionally, we\nprovide perspectives on the \\(\\p\\) vs. \\(\\np\\) problem by relating it to the\nexistence of sub-linear certificates.", "comment": "This paper is primarily relevant to cs.CC, but submitted under cs.ML\n  due to lack of endorsement. The paper is under review at \"Information and\n  Communication\"", "pdf_url": "http://arxiv.org/pdf/2507.23504v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23284", "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval", "authors": ["Dohwan Ko", "Ji Soo Lee", "Minhyuk Choi", "Zihang Meng", "Hyunwoo J. Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Highlight", "url": "http://arxiv.org/abs/2507.23284v1", "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.", "comment": "ICCV 2025 Highlight", "pdf_url": "http://arxiv.org/pdf/2507.23284v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.14928", "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Paper URL: this https URL ;Presentation Video: this https URL", "url": "http://arxiv.org/abs/2504.14928v3", "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.", "comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576 ;Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0", "pdf_url": "http://arxiv.org/pdf/2504.14928v3", "cate": "cs.AI", "date": "2025-04-21", "updated": "2025-07-31"}
{"id": "2404.18154", "title": "Explaining vague language", "authors": ["Paul Égré", "Benjamin Spector"], "categories": ["cs.CL", "cs.GT", "cs.IT", "math.IT", "91A86", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.18154v2", "summary": "Why is language vague? Vagueness may be explained and rationalized if it can\nbe shown that vague language is more useful to speaker and hearer than precise\nlanguage. In a well-known paper, Lipman proposes a game-theoretic account of\nvagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot\nbe strictly better than precision at equilibrium. More recently, \\'Egr\\'e,\nSpector, Mortier and Verheyen have put forward a Bayesian account of vagueness\nestablishing that using vague words can be strictly more informative than using\nprecise words. This paper proposes to compare both results and to explain why\nthey are not in contradiction. Lipman's definition of vagueness relies\nexclusively on a property of signaling strategies, without making any\nassumptions about the lexicon, whereas \\'Egr\\'e et al.'s involves a layer of\nsemantic content. We argue that the semantic account of vagueness is needed,\nand more adequate and explanatory of vagueness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.18154v2", "cate": "cs.CL", "date": "2024-04-28", "updated": "2025-07-31"}
{"id": "2507.15230", "title": "GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis", "authors": ["Guoxi Liu", "Thomas Randall", "Rong Ge", "Federico Iuricich"], "categories": ["cs.DC", "cs.GR"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE VIS 2025", "url": "http://arxiv.org/abs/2507.15230v3", "summary": "Unstructured meshes present challenges in scientific data analysis due to\nirregular distribution and complex connectivity. Computing and storing\nconnectivity information is a major bottleneck for visualization algorithms,\naffecting both time and memory performance. Recent task-parallel data\nstructures address this by precomputing connectivity information at runtime\nwhile the analysis algorithm executes, effectively hiding computation costs and\nimproving performance. However, existing approaches are CPU-bound, forcing the\ndata structure and analysis algorithm to compete for the same computational\nresources, limiting potential speedups. To overcome this limitation, we\nintroduce a novel task-parallel approach optimized for heterogeneous CPU-GPU\nsystems. Specifically, we offload the computation of mesh connectivity\ninformation to GPU threads, enabling CPU threads to focus on executing the\nvisualization algorithm. Following this paradigm, we propose GALE (GPU-Aided\nLocalized data structurE), the first open-source CUDA-based data structure\ndesigned for heterogeneous task parallelism. Experiments on two 20-core CPUs\nand an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over\nstate-of-the-art localized data structures while maintaining memory efficiency.", "comment": "Accepted at IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2507.15230v3", "cate": "cs.DC", "date": "2025-07-21", "updated": "2025-07-30"}
{"id": "2507.23600", "title": "EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution", "authors": ["Yu-Tang Chang", "Shih-Fang Chen"], "categories": ["cs.LG", "cs.CE", "G.1.6; G.3; G.4; I.6.5"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23600v1", "summary": "Signal unmixing analysis decomposes data into basic patterns and is widely\napplied in chemical and biological research. Multivariate curve resolution\n(MCR), a branch of signal unmixing, separates mixed chemical signals into base\npatterns (components) and their concentrations, playing a key role in\nunderstanding composition. Classical MCR is typically framed as matrix\nfactorization (MF) and requires a user-specified component count, usually\nunknown in real data. As dataset size or component count increases, the\nscalability and reliability of MF-based MCR face significant challenges. This\nstudy reformulates MCR as a generative process (gMCR), and introduces an\nenergy-based deep learning solver, EB-gMCR, that automatically discovers the\nsmallest component set able to reconstruct the data faithfully. EB-gMCR starts\nfrom a large candidate pool (e.g., 1024 spectra) and employs a differentiable\ngating network to retain only active components while estimating their\nconcentrations. On noisy synthetic datasets containing up to 256 latent\nsources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count\nwithin 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near\nexact component estimation. Additional chemical priors, such as non-negativity\nor nonlinear mixing, enter as simple plug-in functions, enabling adaptation to\nother instruments or domains without altering the core learning process. By\nuniting high-capacity generative modeling and hard component selection, EB-gMCR\noffers a practical route to large-scale signal unmixing analysis, including\nchemical library-driven scenarios. The source code is available at\nhttps://github.com/b05611038/ebgmcr_solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23600v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22947", "title": "ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios", "authors": ["Shou'ang Wei", "Xinyun Wang", "Shuzhen Bi", "Jian Chen", "Ruijia Li", "Bo Jiang", "Xin Lin", "Min Zhang", "Yu Song", "BingDong Li", "Aimin Zhou", "Hao Hao"], "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22947v1", "summary": "The emergence of Large Language Models (LLMs) presents transformative\nopportunities for education, generating numerous novel application scenarios.\nHowever, significant challenges remain: evaluation metrics vary substantially\nacross different educational scenarios, while many emerging scenarios lack\nappropriate assessment metrics. Current benchmarks predominantly measure\ngeneral intelligence rather than pedagogical capabilities. To address this gap,\nwe introduce ELMES, an open-source automated evaluation framework specifically\ndesigned for assessing LLMs in educational settings. ELMES features a modular\narchitecture that enables researchers to create dynamic, multi-agent dialogues\nthrough simple configuration files, facilitating flexible scenario design\nwithout requiring extensive programming expertise. The framework incorporates a\nhybrid evaluation engine that objectively quantifies traditionally subjective\npedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic\nbenchmarking of state-of-the-art LLMs across four critical educational\nscenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching,\nInterdisciplinary Lesson Plan Generation, and Contextualized Question\nGeneration, employing fine-grained metrics developed in collaboration with\neducation specialists. Our results demonstrate distinct capability\ndistributions among models, revealing context-specific strengths and\nlimitations. ELMES provides educators and researchers with an accessible\nevaluation framework that significantly reduces adaptation barriers for diverse\neducational applications while advancing the practical implementation of LLMs\nin pedagogy. The framework is publicly available at\n\\emph{https://github.com/sii-research/elmes.git}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22947v1", "cate": "cs.CY", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.22937", "title": "CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering", "authors": ["Jinkun Zhao", "Yuanshuai Wang", "Xingjian Zhang", "Ruibo Chen", "Xingchuang Liao", "Junle Wang", "Lei Huang", "Kui Zhang", "Wenjun Wu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22937v1", "summary": "With the rapid evolution of artificial intelligence, AIOps has emerged as a\nprominent paradigm in DevOps. Lots of work has been proposed to improve the\nperformance of different AIOps phases. However, constrained by domain-specific\nknowledge, a single model can only handle the operation requirement of a\nspecific task,such as log parser,root cause analysis. Meanwhile, combining\nmultiple models can achieve more efficient results, which have been proved in\nboth previous ensemble learning and the recent LLM training domain. Inspired by\nthese works,to address the similar challenges in AIOPS, this paper first\nproposes a collaboration-of-expert framework(CoE-Ops) incorporating a\ngeneral-purpose large language model task classifier. A retrieval-augmented\ngeneration mechanism is introduced to improve the framework's capability in\nhandling both Question-Answering tasks with high-level(Code,build,Test,etc.)\nand low-level(fault analysis,anomaly detection,etc.). Finally, the proposed\nmethod is implemented in the AIOps domain, and extensive experiments are\nconducted on the DevOps-EVAL dataset. Experimental results demonstrate that\nCoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps\ntasks compared to existing CoE methods, delivers up to 8% accuracy enhancement\nover single AIOps models in DevOps problem resolution, and outperforms\nlarger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22937v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2503.06891", "title": "AKF-LIO: LiDAR-Inertial Odometry with Gaussian Map by Adaptive Kalman Filter", "authors": ["Xupeng Xie", "Ruoyu Geng", "Jun Ma", "Boyu Zhou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to IROS 2025 Conference, this https URL", "url": "http://arxiv.org/abs/2503.06891v2", "summary": "Existing LiDAR-Inertial Odometry (LIO) systems typically use sensor-specific\nor environment-dependent measurement covariances during state estimation,\nleading to laborious parameter tuning and suboptimal performance in challenging\nconditions (e.g., sensor degeneracy and noisy observations). Therefore, we\npropose an Adaptive Kalman Filter (AKF) framework that dynamically estimates\ntime-varying noise covariances of LiDAR and Inertial Measurement Unit (IMU)\nmeasurements, enabling context-aware confidence weighting between sensors.\nDuring LiDAR degeneracy, the system prioritizes IMU data while suppressing\ncontributions from unreliable inputs like moving objects or noisy point clouds.\nFurthermore, a compact Gaussian-based map representation is introduced to model\nenvironmental planarity and spatial noise. A correlated registration strategy\nensures accurate plane normal estimation via pseudo-merge, even in unstructured\nenvironments like forests. Extensive experiments validate the robustness of the\nproposed system across diverse environments, including dynamic scenes and\ngeometrically degraded scenarios. Our method achieves reliable localization\nresults across all MARS-LVIG sequences and ranks 8th on the KITTI Odometry\nBenchmark. The code will be released at https://github.com/xpxie/AKF-LIO.git.", "comment": "Submitted to IROS 2025 Conference,\n  https://github.com/xpxie/AKF-LIO.git", "pdf_url": "http://arxiv.org/pdf/2503.06891v2", "cate": "cs.RO", "date": "2025-03-10", "updated": "2025-07-31"}
{"id": "2507.23512", "title": "Differentially Private Clipped-SGD: High-Probability Convergence with Arbitrary Clipping Level", "authors": ["Saleh Vatan Khah", "Savelii Chezhegov", "Shahrokh Farahmand", "Samuel Horváth", "Eduard Gorbunov"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      60 pages", "url": "http://arxiv.org/abs/2507.23512v1", "summary": "Gradient clipping is a fundamental tool in Deep Learning, improving the\nhigh-probability convergence of stochastic first-order methods like SGD,\nAdaGrad, and Adam under heavy-tailed noise, which is common in training large\nlanguage models. It is also a crucial component of Differential Privacy (DP)\nmechanisms. However, existing high-probability convergence analyses typically\nrequire the clipping threshold to increase with the number of optimization\nsteps, which is incompatible with standard DP mechanisms like the Gaussian\nmechanism. In this work, we close this gap by providing the first\nhigh-probability convergence analysis for DP-Clipped-SGD with a fixed clipping\nlevel, applicable to both convex and non-convex smooth optimization under\nheavy-tailed noise, characterized by a bounded central $\\alpha$-th moment\nassumption, $\\alpha \\in (1,2]$. Our results show that, with a fixed clipping\nlevel, the method converges to a neighborhood of the optimal solution with a\nfaster rate than the existing ones. The neighborhood can be balanced against\nthe noise introduced by DP, providing a refined trade-off between convergence\nspeed and privacy guarantees.", "comment": "60 pages", "pdf_url": "http://arxiv.org/pdf/2507.23512v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23295", "title": "LED Benchmark: Diagnosing Structural Layout Errors for Document Layout Analysis", "authors": ["Inbum Heo", "Taewook Hwang", "Jeesu Jung", "Sangkeun Jung"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23295v1", "summary": "Recent advancements in Document Layout Analysis through Large Language Models\nand Multimodal Models have significantly improved layout detection. However,\ndespite these improvements, challenges remain in addressing critical structural\nerrors, such as region merging, splitting, and missing content. Conventional\nevaluation metrics like IoU and mAP, which focus primarily on spatial overlap,\nare insufficient for detecting these errors. To address this limitation, we\npropose Layout Error Detection (LED), a novel benchmark designed to evaluate\nthe structural robustness of document layout predictions. LED defines eight\nstandardized error types, and formulates three complementary tasks: error\nexistence detection, error type classification, and element-wise error type\nclassification. Furthermore, we construct LED-Dataset, a synthetic dataset\ngenerated by injecting realistic structural errors based on empirical\ndistributions from DLA models. Experimental results across a range of LMMs\nreveal that LED effectively differentiates structural understanding\ncapabilities, exposing modality biases and performance trade-offs not visible\nthrough traditional metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23295v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2506.18199", "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Research is incomplete", "url": "http://arxiv.org/abs/2506.18199v2", "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "comment": "Research is incomplete", "pdf_url": "http://arxiv.org/pdf/2506.18199v2", "cate": "cs.CL", "date": "2025-06-22", "updated": "2025-07-30"}
{"id": "2412.19792", "title": "InfAlign: Inference-aware language model alignment", "authors": ["Ananth Balashankar", "Ziteng Sun", "Jonathan Berant", "Jacob Eisenstein", "Michael Collins", "Adrian Hutter", "Jong Lee", "Chirag Nagpal", "Flavien Prost", "Aradhana Sinha", "Ananda Theertha Suresh", "Ahmad Beirami"], "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.19792v4", "summary": "Language model alignment is a critical step in training modern generative\nlanguage models. Alignment targets to improve win rate of a sample from the\naligned model against the base model. Today, we are increasingly using\ninference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)\nto decode from language models rather than standard sampling. We show that this\ntrain/test mismatch makes standard RLHF framework sub-optimal in view of such\ninference-time methods. To this end, we propose a framework for inference-aware\nalignment (InfAlign), which aims to optimize inference-time win rate of the\naligned policy against the base model. We prove that for any inference-time\ndecoding procedure, the optimal aligned policy is the solution to the standard\nRLHF problem with a transformation of the reward. This motivates us to provide\nthe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,\nwhich involves a reward calibration step and a KL-regularized reward\nmaximization step with a transformation of the calibrated reward. For best-of-N\nsampling and best-of-N jailbreaking, we propose specific transformations\noffering up to 3-8% improvement on inference-time win rates. Finally, we also\nshow that our proposed reward calibration method is a strong baseline for\noptimizing standard win rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.19792v4", "cate": "cs.LG", "date": "2024-12-27", "updated": "2025-07-31"}
{"id": "2307.02968", "title": "A Simple $(1-ε)$-Approximation Semi-Streaming Algorithm for Maximum (Weighted) Matching", "authors": ["Sepehr Assadi"], "categories": ["cs.DS", "cs.DC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      25 pages. This is the TheoretiCS journal version", "url": "http://arxiv.org/abs/2307.02968v4", "summary": "We present a simple semi-streaming algorithm for $(1-\\epsilon)$-approximation\nof bipartite matching in $O(\\log{\\!(n)}/\\epsilon)$ passes. This matches the\nperformance of state-of-the-art \"$\\epsilon$-efficient\" algorithms -- the ones\nwith much better dependence on $\\epsilon$ albeit with some mild dependence on\n$n$ -- while being considerably simpler.\n  The algorithm relies on a direct application of the multiplicative weight\nupdate method with a self-contained primal-dual analysis that can be of\nindependent interest. To show case this, we use the same ideas, alongside\nstandard tools from matching theory, to present an equally simple\nsemi-streaming algorithm for $(1-\\epsilon)$-approximation of weighted matchings\nin general (not necessarily bipartite) graphs, again in\n$O(\\log{\\!(n)}/\\epsilon)$ passes.", "comment": "25 pages. This is the TheoretiCS journal version", "pdf_url": "http://arxiv.org/pdf/2307.02968v4", "cate": "cs.DS", "date": "2023-07-06", "updated": "2025-07-31"}
{"id": "2507.22838", "title": "Modelling and simulation of electro-mechanically coupled dielectric elastomers and myocardial tissue using smoothed finite element methods", "authors": ["Tan Tran", "Denisa Martonova", "Sigrid Leyendecker"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22838v2", "summary": "Computational modelling offers a cost-effective and time-efficient\nalternative to experimental studies in biomedical engineering. In cardiac\nelectro-mechanics, finite element method (FEM)-based simulations provide\nvaluable insights into diseased tissue behaviour and the development of\nassistive systems such as di-electric elastomer actuators. However, the use of\nautomatically generated tetrahedral meshes, commonly applied due to geometric\ncomplexity, often leads to numerical issues including overly stiff responses\nand volume locking, particularly in incompressible materials. Smoothed finite\nelement methods (S-FEMs) offer a promising alternative by softening the\nstiffness matrix through gradient smoothing over defined smoothing domains.\nThis work extends S-FEM formulations to electro-mechanically coupled problems\nand compares their performance against standard linear FEM. We implement and\nevaluate four approaches in the Abaqus environment via custom user elements:\nstandard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and\nthe hybrid face/node-based S-FEM (FSNS-FEM). Two benchmark problems are\nstudied: the electrically induced contraction of a compressible dielectric\nelastomer and an incompressible, orthotropic myocardial tissue sample.\nReference solutions are obtained using a mesh consisting of higher-order\nelements. Our results demonstrate that FSNS-FEM provides the best balance\nbetween accuracy and computational efficiency, closely matching reference data.\nNS-FEM produces softer results, which leads to an overestimation of the true\ndeformation. FS-FEM and standard FEM consistently exhibit overly stiff\nbehaviour, with pronounced volume locking in the myocardial case. These\nfindings support the potential of S-FEMs, in particular FSNS-FEM, for accurate\nsimulation of coupled electro-mechanical behaviour in complex biomedical\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22838v2", "cate": "cs.CE", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23434", "title": "Future Illiteracies -- Architectural Epistemology and Artificial Intelligence", "authors": ["Mustapha El Moussaoui"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures. MDPI - Architecture 2025", "url": "http://arxiv.org/abs/2507.23434v1", "summary": "In the age of artificial intelligence, architectural practice faces a paradox\nof immense potential and creeping standardization. As humans are increasingly\nrelying on AI-generated outputs, architecture risks becoming a spectacle of\nrepetition- a shuffling of data that neither truly innovates nor progresses\nvertically in creative depth. This paper explores the critical role of data in\nAI systems, scrutinizing the training datasets that form the basis of AI's\ngenerative capabilities and the implications for architectural practice. We\nargue that when architects approach AI passively, without actively engaging\ntheir own creative and critical faculties, they risk becoming passive users\nlocked in an endless loop of horizontal expansion without meaningful vertical\ngrowth. By examining the epistemology of architecture in the AI age, this paper\ncalls for a paradigm where AI serves as a tool for vertical and horizontal\ngrowth, contingent on human creativity and agency. Only by mastering this\ndynamic relationship can architects avoid the trap of passive, standardized\ndesign and unlock the true potential of AI.", "comment": "14 pages, 7 figures. MDPI - Architecture 2025", "pdf_url": "http://arxiv.org/pdf/2507.23434v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.08575", "title": "Prophecies all the Way: Game-based Model-Checking for HyperQPTL beyond $\\forall^*\\exists^*$", "authors": ["Sarah Winter", "Martin Zimmermann"], "categories": ["cs.LO", "cs.FL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.08575v4", "summary": "Model-checking HyperLTL, a temporal logic expressing properties of sets of\ntraces with applications to information-flow based security and privacy, has a\ndecidable, but TOWER-complete, model-checking problem. While the classical\nmodel-checking algorithm for full HyperLTL is automata-theoretic, more\nrecently, a game-based alternative for the $\\forall^*\\exists^*$-fragment has\nbeen presented.\n  Here, we employ imperfect information-games to extend the game-based approach\nto full HyperQPTL, which features arbitrary quantifier prefixes and\nquantification over propositions and can express every $\\omega$-regular\nhyperproperty. As a byproduct of our game-based algorithm, we obtain\nfinite-state implementations of Skolem functions via transducers with lookahead\nthat explain satisfaction or violation of HyperQPTL properties.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.08575v4", "cate": "cs.LO", "date": "2025-04-11", "updated": "2025-07-31"}
{"id": "2507.22938", "title": "A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents", "authors": ["Sumit Soman", "H. G. Ranjani", "Sujoy Roychowdhury", "Venkata Dharma Surya Narayana Sastry", "Akshat Jain", "Pranav Gangrade", "Ayaaz Khan"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the KDD 2025 Workshop on Structured Knowledge for Large Language Models", "url": "http://arxiv.org/abs/2507.22938v1", "summary": "Question-Answering (QA) from technical documents often involves questions\nwhose answers are present in figures, such as flowcharts or flow diagrams.\nText-based Retrieval Augmented Generation (RAG) systems may fail to answer such\nquestions. We leverage graph representations of flowcharts obtained from Visual\nlarge Language Models (VLMs) and incorporate them in a text-based RAG system to\nshow that this approach can enable image retrieval for QA in the telecom\ndomain. We present the end-to-end approach from processing technical documents,\nclassifying image types, building graph representations, and incorporating them\nwith the text embedding pipeline for efficient retrieval. We benchmark the same\non a QA dataset created based on proprietary telecom product information\ndocuments. Results show that the graph representations obtained using a\nfine-tuned VLM model have lower edit distance with respect to the ground truth,\nwhich illustrate the robustness of these representations for flowchart images.\nFurther, the approach for QA using these representations gives good retrieval\nperformance using text-based embedding models, including a telecom-domain\nadapted one. Our approach also alleviates the need for a VLM in inference,\nwhich is an important cost benefit for deployed QA systems.", "comment": "Accepted for publication at the KDD 2025 Workshop on Structured\n  Knowledge for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2507.22938v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2503.12725", "title": "Humanoids in Hospitals: A Technical Study of Humanoid Robot Surrogates for Dexterous Medical Interventions", "authors": ["Soofiyan Atar", "Xiao Liang", "Calvin Joyce", "Florian Richter", "Wood Ricardo", "Charles Goldberg", "Preetham Suresh", "Michael Yip"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2503.12725v2", "summary": "The increasing demand for healthcare workers, driven by aging populations and\nlabor shortages, presents a significant challenge for hospitals. Humanoid\nrobots have the potential to alleviate these pressures by leveraging their\nhuman-like dexterity and adaptability to assist in medical procedures. This\nwork conducted an exploratory study on the feasibility of humanoid robots\nperforming direct clinical tasks through teleoperation. A bimanual\nteleoperation system was developed for the Unitree G1 Humanoid Robot,\nintegrating high-fidelity pose tracking, custom grasping configurations, and an\nimpedance controller to safely and precisely manipulate medical tools. The\nsystem is evaluated in seven diverse medical procedures, including physical\nexaminations, emergency interventions, and precision needle tasks. Our results\ndemonstrate that humanoid robots can successfully replicate critical aspects of\nhuman medical assessments and interventions, with promising quantitative\nperformance in ventilation and ultrasound-guided tasks. However, challenges\nremain, including limitations in force output for procedures requiring high\nstrength and sensor sensitivity issues affecting clinical accuracy. This study\nhighlights the potential and current limitations of humanoid robots in hospital\nsettings and lays the groundwork for future research on robotic healthcare\nintegration.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2503.12725v2", "cate": "cs.RO", "date": "2025-03-17", "updated": "2025-07-31"}
{"id": "2507.23534", "title": "Continual Learning with Synthetic Boundary Experience Blending", "authors": ["Chih-Fan Hsu", "Ming-Ching Chang", "Wei-Chao Chen"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23534v1", "summary": "Continual learning (CL) aims to address catastrophic forgetting in models\ntrained sequentially on multiple tasks. While experience replay has shown\npromise, its effectiveness is often limited by the sparse distribution of\nstored key samples, leading to overly simplified decision boundaries. We\nhypothesize that introducing synthetic data near the decision boundary\n(Synthetic Boundary Data, or SBD) during training serves as an implicit\nregularizer, improving boundary stability and mitigating forgetting. To\nvalidate this hypothesis, we propose a novel training framework, {\\bf\nExperience Blending}, which integrates knowledge from both stored key samples\nand synthetic, boundary-adjacent data. Experience blending consists of two core\ncomponents: (1) a multivariate Differential Privacy (DP) noise mechanism that\ninjects batch-wise noise into low-dimensional feature representations,\ngenerating SBD; and (2) an end-to-end training strategy that jointly leverages\nboth stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100,\nand Tiny ImageNet demonstrate that our method outperforms nine CL baselines,\nachieving accuracy improvements of 10%, 6%, and 13%, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23534v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23300", "title": "Training-free Geometric Image Editing on Diffusion Models", "authors": ["Hanshen Zhu", "Zhen Zhu", "Kaile Zhang", "Yiming Gong", "Yuliang Liu", "Xiang Bai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      24 pages, 22 figures, ICCV", "url": "http://arxiv.org/abs/2507.23300v1", "summary": "We tackle the task of geometric image editing, where an object within an\nimage is repositioned, reoriented, or reshaped while preserving overall scene\ncoherence. Previous diffusion-based editing methods often attempt to handle all\nrelevant subtasks in a single step, proving difficult when transformations\nbecome large or structurally complex. We address this by proposing a decoupled\npipeline that separates object transformation, source region inpainting, and\ntarget region refinement. Both inpainting and refinement are implemented using\na training-free diffusion approach, FreeFine. In experiments on our new\nGeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine\noutperforms state-of-the-art alternatives in image fidelity, and edit\nprecision, especially under demanding transformations. Code and benchmark are\navailable at: https://github.com/CIawevy/FreeFine", "comment": "24 pages, 22 figures, ICCV", "pdf_url": "http://arxiv.org/pdf/2507.23300v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.10933", "title": "Cross-layer Integrated Sensing and Communication: A Joint Industrial and Academic Perspective", "authors": ["Henk Wymeersch", "Nuutti Tervo", "Stefan Wänstedt", "Sharief Saleh", "Joerg Ahlendorf", "Ozgur Akgul", "Vasileios Tsekenis", "Sokratis Barmpounakis", "Liping Bai", "Martin Beale", "Rafael Berkvens", "Nabeel Nisar Bhat", "Hui Chen", "Shrayan Das", "Claude Desset", "Antonio de la Oliva", "Prajnamaya Dass", "Jeroen Famaey", "Hamed Farhadi", "Gerhard P. Fettweis", "Yu Ge", "Hao Guo", "Rreze Halili", "Katsuyuki Haneda", "Abdur Rahman Mohamed Ismail", "Akshay Jain", "Sylvaine Kerboeuf", "Musa Furkan Keskin", "Emad Ibrahim", "Bilal Khan", "Siddhartha Kumar", "Stefan Köpsell", "Apostolos Kousaridas", "Pekka Kyösti", "Simon Lindberg", "Mohammad Hossein Moghaddam", "Ahmad Nimr", "Victor Pettersson", "Aarno Pärssinen", "Basuki Priyanto", "Athanasios Stavridis", "Tommy Svensson", "Sonika Ujjwal"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10933v2", "summary": "Integrated sensing and communication (ISAC) enables radio systems to\nsimultaneously sense and communicate with their environment. This paper,\ndeveloped within the Hexa-X-II project funded by the European Union, presents a\ncomprehensive cross-layer vision for ISAC in 6G networks, integrating insights\nfrom physical-layer design, hardware architectures, AI-driven intelligence, and\nprotocol-level innovations. We begin by revisiting the foundational principles\nof ISAC, highlighting synergies and trade-offs between sensing and\ncommunication across different integration levels. Enabling technologies (such\nas multiband operation, massive and distributed MIMO, non-terrestrial networks,\nreconfigurable intelligent surfaces, and machine learning) are analyzed in\nconjunction with hardware considerations including waveform design,\nsynchronization, and full-duplex operation. To bridge implementation and\nsystem-level evaluation, we introduce a quantitative cross-layer framework\nlinking design parameters to key performance and value indicators. By\nsynthesizing perspectives from both academia and industry, this paper outlines\nhow deeply integrated ISAC can transform 6G into a programmable and\ncontext-aware platform supporting applications from reliable wireless access to\nautonomous mobility and digital twinning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10933v2", "cate": "eess.SP", "date": "2025-05-16", "updated": "2025-07-31"}
{"id": "2407.15738", "title": "Parallel Split Learning with Global Sampling", "authors": ["Mohammad Kohankhaki", "Ahmad Ayad", "Mahdi Barhoush", "Anke Schmeink"], "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.15738v4", "summary": "Distributed deep learning in resource-constrained environments faces\nscalability and generalization challenges due to large effective batch sizes\nand non-identically distributed client data. We introduce a server-driven\nsampling strategy that maintains a fixed global batch size by dynamically\nadjusting client-side batch sizes. This decouples the effective batch size from\nthe number of participating devices and ensures that global batches better\nreflect the overall data distribution. Using standard concentration bounds, we\nestablish tighter deviation guarantees compared to existing approaches.\nEmpirical results on a benchmark dataset confirm that the proposed method\nimproves model accuracy, training efficiency, and convergence stability,\noffering a scalable solution for learning at the network edge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.15738v4", "cate": "cs.LG", "date": "2024-07-22", "updated": "2025-07-31"}
{"id": "2507.07426", "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "authors": ["Zerui Yang", "Yuwei Wan", "Siyu Yan", "Yudai Matsuda", "Tong Xie", "Bram Hoex", "Linqi Song"], "categories": ["cs.AI", "cs.CE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07426v3", "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug repositioning. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repositioning. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Extensive experiments on the\nDrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially\nhigher recall and robustness compared to both general-purpose LLMs and deep\nlearning baselines. Our results highlight the importance of structured\nreasoning, agent-based collaboration, and feedback-driven search mechanisms in\nadvancing LLM applications for drug repositioning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07426v3", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2507.23653", "title": "Architectural practice process and artificial intelligence -- an evolving practice", "authors": ["Mustapha El Moussaoui"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      15 pages, 7 figures. De Gruyter Brill - Open Engineering 2025", "url": "http://arxiv.org/abs/2507.23653v1", "summary": "In an era of exponential technological advancement, artificial intelligence\n(AI) has emerged as a transformative force in architecture, reshaping\ntraditional design and construction practices. This article explores the\nmultifaceted roles of AI in the architectural process, emphasizing its\npotential to enhance creativity and efficiency while addressing its limitations\nin capturing multisensory and experiential dimensions of space. Historically,\narchitectural innovation has paralleled technological progress, from basic\ntools to advanced computer-aided design systems. However, the integration of AI\npresents unique challenges, requiring architects to critically evaluate its\nrole in design. A narrative review methodology was adopted, focusing on\nacademic sources selected for their relevance, recency, and credibility. The\nfindings reveal that AI is increasingly integrated across various stages of the\narchitectural process, from early conceptualization and site analysis to\ngenerative design and construction detailing. AI tools excel at automating\nrepetitive tasks and generating innovative design solutions, freeing architects\nto focus on creativity and problem-solving. Additionally, AI's (text- toimage)\nvisual representation strength challenges the ocularcentric approaches in\narchitecture, which should push future architects to address the holistic\nsensory and experiential qualities of space or the critical thinking inherent\nto architectural design. While AI offers transformative potential, architects\nmust view it as a collaborative partner rather than a passive tool.", "comment": "15 pages, 7 figures. De Gruyter Brill - Open Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2507.23653v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22939", "title": "PARROT: An Open Multilingual Radiology Reports Dataset", "authors": ["Bastien Le Guellec", "Kokou Adambounou", "Lisa C Adams", "Thibault Agripnidis", "Sung Soo Ahn", "Radhia Ait Chalal", "Tugba Akinci D Antonoli", "Philippe Amouyel", "Henrik Andersson", "Raphael Bentegeac", "Claudio Benzoni", "Antonino Andrea Blandino", "Felix Busch", "Elif Can", "Riccardo Cau", "Armando Ugo Cavallo", "Christelle Chavihot", "Erwin Chiquete", "Renato Cuocolo", "Eugen Divjak", "Gordana Ivanac", "Barbara Dziadkowiec Macek", "Armel Elogne", "Salvatore Claudio Fanni", "Carlos Ferrarotti", "Claudia Fossataro", "Federica Fossataro", "Katarzyna Fulek", "Michal Fulek", "Pawel Gac", "Martyna Gachowska", "Ignacio Garcia Juarez", "Marco Gatti", "Natalia Gorelik", "Alexia Maria Goulianou", "Aghiles Hamroun", "Nicolas Herinirina", "Krzysztof Kraik", "Dominik Krupka", "Quentin Holay", "Felipe Kitamura", "Michail E Klontzas", "Anna Kompanowska", "Rafal Kompanowski", "Alexandre Lefevre", "Tristan Lemke", "Maximilian Lindholz", "Lukas Muller", "Piotr Macek", "Marcus Makowski", "Luigi Mannacio", "Aymen Meddeb", "Antonio Natale", "Beatrice Nguema Edzang", "Adriana Ojeda", "Yae Won Park", "Federica Piccione", "Andrea Ponsiglione", "Malgorzata Poreba", "Rafal Poreba", "Philipp Prucker", "Jean Pierre Pruvo", "Rosa Alba Pugliesi", "Feno Hasina Rabemanorintsoa", "Vasileios Rafailidis", "Katarzyna Resler", "Jan Rotkegel", "Luca Saba", "Ezann Siebert", "Arnaldo Stanzione", "Ali Fuat Tekin", "Liz Toapanta Yanchapaxi", "Matthaios Triantafyllou", "Ekaterini Tsaoulia", "Evangelia Vassalou", "Federica Vernuccio", "Johan Wasselius", "Weilang Wang", "Szymon Urban", "Adrian Wlodarczak", "Szymon Wlodarczak", "Andrzej Wysocki", "Lina Xu", "Tomasz Zatonski", "Shuhang Zhang", "Sebastian Ziegelmayer", "Gregory Kuchcinski", "Keno K Bressem"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22939v1", "summary": "Rationale and Objectives: To develop and validate PARROT (Polyglottal\nAnnotated Radiology Reports for Open Testing), a large, multicentric,\nopen-access dataset of fictional radiology reports spanning multiple languages\nfor testing natural language processing applications in radiology. Materials\nand Methods: From May to September 2024, radiologists were invited to\ncontribute fictional radiology reports following their standard reporting\npractices. Contributors provided at least 20 reports with associated metadata\nincluding anatomical region, imaging modality, clinical context, and for\nnon-English reports, English translations. All reports were assigned ICD-10\ncodes. A human vs. AI report differentiation study was conducted with 154\nparticipants (radiologists, healthcare professionals, and non-healthcare\nprofessionals) assessing whether reports were human-authored or AI-generated.\nResults: The dataset comprises 2,658 radiology reports from 76 authors across\n21 countries and 13 languages. Reports cover multiple imaging modalities (CT:\n36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical\nregions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)\nbeing most prevalent. In the differentiation study, participants achieved 53.9%\naccuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated\nreports, with radiologists performing significantly better (56.9%, 95% CI:\n53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the\nlargest open multilingual radiology report dataset, enabling development and\nvalidation of natural language processing applications across linguistic,\ngeographic, and clinical boundaries without privacy constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22939v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2504.02439", "title": "Estimating Scene Flow in Robot Surroundings with Distributed Miniaturized Time-of-Flight Sensors", "authors": ["Jack Sander", "Giammarco Caroleo", "Alessandro Albini", "Perla Maiolino"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures, 2 tables, 1 algorithm, IEEE RO-MAN 2025 accepted paper", "url": "http://arxiv.org/abs/2504.02439v2", "summary": "Tracking motions of humans or objects in the surroundings of the robot is\nessential to improve safe robot motions and reactions. In this work, we present\nan approach for scene flow estimation from low-density and noisy point clouds\nacquired from miniaturized Time of Flight (ToF) sensors distributed on the\nrobot body. The proposed method clusters points from consecutive frames and\napplies Iterative Closest Point (ICP) to estimate a dense motion flow, with\nadditional steps introduced to mitigate the impact of sensor noise and\nlow-density data points. Specifically, we employ a fitness-based classification\nto distinguish between stationary and moving points and an inlier removal\nstrategy to refine geometric correspondences. The proposed approach is\nvalidated in an experimental setup where 24 ToF are used to estimate the\nvelocity of an object moving at different controlled speeds. Experimental\nresults show that the method consistently approximates the direction of the\nmotion and its magnitude with an error which is in line with sensor noise.", "comment": "7 pages, 5 figures, 2 tables, 1 algorithm, IEEE RO-MAN 2025 accepted\n  paper", "pdf_url": "http://arxiv.org/pdf/2504.02439v2", "cate": "cs.RO", "date": "2025-04-03", "updated": "2025-07-31"}
{"id": "2507.23535", "title": "Transparent AI: The Case for Interpretability and Explainability", "authors": ["Dhanesh Ramachandram", "Himanshu Joshi", "Judy Zhu", "Dhari Gandhi", "Lucas Hartman", "Ananya Raval"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23535v1", "summary": "As artificial intelligence systems increasingly inform high-stakes decisions\nacross sectors, transparency has become foundational to responsible and\ntrustworthy AI implementation. Leveraging our role as a leading institute in\nadvancing AI research and enabling industry adoption, we present key insights\nand lessons learned from practical interpretability applications across diverse\ndomains. This paper offers actionable strategies and implementation guidance\ntailored to organizations at varying stages of AI maturity, emphasizing the\nintegration of interpretability as a core design principle rather than a\nretrospective add-on.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23535v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23307", "title": "ST-SAM: SAM-Driven Self-Training Framework for Semi-Supervised Camouflaged Object Detection", "authors": ["Xihang Hu", "Fuming Sun", "Jiazhe Liu", "Feilong Xu", "Xiaoli Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures, ACM MM 2025", "url": "http://arxiv.org/abs/2507.23307v1", "summary": "Semi-supervised Camouflaged Object Detection (SSCOD) aims to reduce reliance\non costly pixel-level annotations by leveraging limited annotated data and\nabundant unlabeled data. However, existing SSCOD methods based on\nTeacher-Student frameworks suffer from severe prediction bias and error\npropagation under scarce supervision, while their multi-network architectures\nincur high computational overhead and limited scalability. To overcome these\nlimitations, we propose ST-SAM, a highly annotation-efficient yet concise\nframework that breaks away from conventional SSCOD constraints. Specifically,\nST-SAM employs Self-Training strategy that dynamically filters and expands\nhigh-confidence pseudo-labels to enhance a single-model architecture, thereby\nfundamentally circumventing inter-model prediction bias. Furthermore, by\ntransforming pseudo-labels into hybrid prompts containing domain-specific\nknowledge, ST-SAM effectively harnesses the Segment Anything Model's potential\nfor specialized tasks to mitigate error accumulation in self-training.\nExperiments on COD benchmark datasets demonstrate that ST-SAM achieves\nstate-of-the-art performance with only 1\\% labeled data, outperforming existing\nSSCOD methods and even matching fully supervised methods. Remarkably, ST-SAM\nrequires training only a single network, without relying on specific models or\nloss functions. This work establishes a new paradigm for annotation-efficient\nSSCOD. Codes will be available at https://github.com/hu-xh/ST-SAM.", "comment": "10 pages, 6 figures, ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23307v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.06005", "title": "Towards Serverless Processing of Spatiotemporal Big Data Queries", "authors": ["Diana Baumann", "Tim C. Rese", "David Bermbach"], "categories": ["cs.DB", "cs.DC"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      Accepted for publication in 13th IEEE International Conference on Cloud Engineering (IC2E 2025)", "url": "http://arxiv.org/abs/2507.06005v2", "summary": "Spatiotemporal data are being produced in continuously growing volumes by a\nvariety of data sources and a variety of application fields rely on rapid\nanalysis of such data. Existing systems such as PostGIS or MobilityDB usually\nbuild on relational database systems, thus, inheriting their scale-out\ncharacteristics. As a consequence, big spatiotemporal data scenarios still have\nlimited support even though many query types can easily be parallelized. In\nthis paper, we propose our vision of a native serverless data processing\napproach for spatiotemporal data: We break down queries into small subqueries\nwhich then leverage the near-instant scaling of Function-as-a-Service platforms\nto execute them in parallel. With this, we partially solve the scalability\nneeds of big spatiotemporal data processing.", "comment": "Accepted for publication in 13th IEEE International Conference on\n  Cloud Engineering (IC2E 2025)", "pdf_url": "http://arxiv.org/pdf/2507.06005v2", "cate": "cs.DB", "date": "2025-07-08", "updated": "2025-07-31"}
{"id": "2507.23669", "title": "Automating AI Failure Tracking: Semantic Association of Reports in AI Incident Database", "authors": ["Diego Russo", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "categories": ["cs.CY", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted at the 28th European Conference on Artificial Intelligence (ECAI 2025)", "url": "http://arxiv.org/abs/2507.23669v1", "summary": "Artificial Intelligence (AI) systems are transforming critical sectors such\nas healthcare, finance, and transportation, enhancing operational efficiency\nand decision-making processes. However, their deployment in high-stakes domains\nhas exposed vulnerabilities that can result in significant societal harm. To\nsystematically study and mitigate these risk, initiatives like the AI Incident\nDatabase (AIID) have emerged, cataloging over 3,000 real-world AI failure\nreports. Currently, associating a new report with the appropriate AI Incident\nrelies on manual expert intervention, limiting scalability and delaying the\nidentification of emerging failure patterns.\n  To address this limitation, we propose a retrieval-based framework that\nautomates the association of new reports with existing AI Incidents through\nsemantic similarity modeling. We formalize the task as a ranking problem, where\neach report-comprising a title and a full textual description-is compared to\npreviously documented AI Incidents based on embedding cosine similarity.\nBenchmarking traditional lexical methods, cross-encoder architectures, and\ntransformer-based sentence embedding models, we find that the latter\nconsistently achieve superior performance. Our analysis further shows that\ncombining titles and descriptions yields substantial improvements in ranking\naccuracy compared to using titles alone. Moreover, retrieval performance\nremains stable across variations in description length, highlighting the\nrobustness of the framework. Finally, we find that retrieval performance\nconsistently improves as the training set expands. Our approach provides a\nscalable and efficient solution for supporting the maintenance of the AIID.", "comment": "Accepted at the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23669v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22940", "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes", "authors": ["Rui Jiao", "Yue Zhang", "Jinku Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22940v1", "summary": "We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy\nfor Confidence Enhancement), a novel framework addressing a critical\nvulnerability in Large Language Models (LLMs): the prevalence of factual\ninaccuracies within intermediate reasoning steps despite correct final answers.\nThis phenomenon poses substantial risks in high-stakes domains including\nhealthcare, legal analysis, and scientific research, where erroneous yet\nconfidently presented reasoning can mislead users into dangerous decisions. Our\nframework integrates three core components: (1) a specialized fact-checking\nclassifier trained on counterfactually augmented data to detect subtle factual\ninconsistencies within reasoning chains; (2) a Group Relative Policy\nOptimization (GRPO) reinforcement learning approach that balances factuality,\ncoherence, and structural correctness through multi-dimensional rewards; and\n(3) a mechanistic interpretability module examining how factuality improvements\nmanifest in model activations during reasoning processes. Extensive evaluation\nacross ten state-of-the-art models reveals concerning patterns: even leading\nmodels like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of\nonly 81.93% and 82.57% respectively. RELIANCE significantly enhances factual\nrobustness (up to 49.90% improvement) while maintaining or improving\nperformance on challenging benchmarks including Math-500, AIME-2024, and GPQA.\nFurthermore, our activation-level analysis provides actionable insights into\nhow factual enhancements reshape reasoning trajectories within model\narchitectures, establishing foundations for future training methodologies that\nexplicitly target factual robustness through activation-guided optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22940v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2505.09771", "title": "Grasp EveryThing (GET): 1-DoF, 3-Fingered Gripper with Tactile Sensing for Robust Grasping", "authors": ["Michael Burgess", "Edward H. Adelson"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.09771v3", "summary": "We introduce the Grasp EveryThing (GET) gripper, a novel 1-DoF, 3-finger\ndesign for securely grasping objects of many shapes and sizes. Mounted on a\nstandard parallel jaw actuator, the design features three narrow, tapered\nfingers arranged in a two-against-one configuration, where the two fingers\nconverge into a V-shape. The GET gripper is more capable of conforming to\nobject geometries and forming secure grasps than traditional designs with two\nflat fingers. Inspired by the principle of self-similarity, these V-shaped\nfingers enable secure grasping across a wide range of object sizes. Further to\nthis end, fingers are parametrically designed for convenient resizing and\ninterchangeability across robotic embodiments with a parallel jaw gripper.\nAdditionally, we incorporate a rigid fingernail for ease in manipulating small\nobjects. Tactile sensing can be integrated into the standalone finger via an\nexternally-mounted camera. A neural network was trained to estimate normal\nforce from tactile images with an average validation error of 1.3 N across a\ndiverse set of geometries. In grasping 15 objects and performing 3 tasks via\nteleoperation, the GET fingers consistently outperformed standard flat fingers.\nAll finger designs, compatible with multiple robotic embodiments, both\nincorporating and lacking tactile sensing, are available on GitHub.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.09771v3", "cate": "cs.RO", "date": "2025-05-14", "updated": "2025-07-30"}
{"id": "2507.23536", "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices", "authors": ["Georg Slamanig", "Francesco Corti", "Olga Saukh"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23536v1", "summary": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23536v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23309", "title": "PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving", "authors": ["Xuewei Tang", "Mengmeng Yang", "Tuopu Wen", "Peijin Jia", "Le Cui", "Mingshang Luo", "Kehua Sheng", "Bo Zhang", "Diange Yang", "Kun Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23309v1", "summary": "With the growing interest in autonomous driving, there is an increasing\ndemand for accurate and reliable road perception technologies. In complex\nenvironments without high-definition map support, autonomous vehicles must\nindependently interpret their surroundings to ensure safe and robust\ndecision-making. However, these scenarios pose significant challenges due to\nthe large number, complex geometries, and frequent occlusions of road elements.\nA key limitation of existing approaches lies in their insufficient exploitation\nof the structured priors inherently present in road elements, resulting in\nirregular, inaccurate predictions. To address this, we propose PriorFusion, a\nunified framework that effectively integrates semantic, geometric, and\ngenerative priors to enhance road element perception. We introduce an\ninstance-aware attention mechanism guided by shape-prior features, then\nconstruct a data-driven shape template space that encodes low-dimensional\nrepresentations of road elements, enabling clustering to generate anchor points\nas reference priors. We design a diffusion-based framework that leverages these\nprior anchors to generate accurate and complete predictions. Experiments on\nlarge-scale autonomous driving datasets demonstrate that our method\nsignificantly improves perception accuracy, particularly under challenging\nconditions. Visualization results further confirm that our approach produces\nmore accurate, regular, and coherent predictions of road elements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23309v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23718", "title": "Informing AI Risk Assessment with News Media: Analyzing National and Political Variation in the Coverage of AI Risks", "authors": ["Mowafak Allaham", "Kimon Kieslich", "Nicholas Diakopoulos"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted to 8th AAAI/ACM Conference on AI, Ethics, and Society (2025)", "url": "http://arxiv.org/abs/2507.23718v1", "summary": "Risk-based approaches to AI governance often center the technological\nartifact as the primary focus of risk assessments, overlooking systemic risks\nthat emerge from the complex interaction between AI systems and society. One\npotential source to incorporate more societal context into these approaches is\nthe news media, as it embeds and reflects complex interactions between AI\nsystems, human stakeholders, and the larger society. News media is influential\nin terms of which AI risks are emphasized and discussed in the public sphere,\nand thus which risks are deemed important. Yet, variations in the news media\nbetween countries and across different value systems (e.g. political\norientations) may differentially shape the prioritization of risks through the\nmedia's agenda setting and framing processes. To better understand these\nvariations, this work presents a comparative analysis of a cross-national\nsample of news media spanning 6 countries (the U.S., the U.K., India,\nAustralia, Israel, and South Africa). Our findings show that AI risks are\nprioritized differently across nations and shed light on how left vs. right\nleaning U.S. based outlets not only differ in the prioritization of AI risks in\ntheir coverage, but also use politicized language in the reporting of these\nrisks. These findings can inform risk assessors and policy-makers about the\nnuances they should account for when considering news media as a supplementary\nsource for risk-based governance approaches.", "comment": "Accepted to 8th AAAI/ACM Conference on AI, Ethics, and Society (2025)", "pdf_url": "http://arxiv.org/pdf/2507.23718v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23013", "title": "Stabilization of Age-Structured Competing Populations", "authors": ["Carina Veil", "Miroslav Krstić", "Patrick McNamee", "Oliver Sawodny"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      submitted to IFAC Automatica", "url": "http://arxiv.org/abs/2507.23013v1", "summary": "Age-structured models represent the dynamic behaviors of populations over\ntime and result in integro-partial differential equations (IPDEs). Such\nprocesses arise in biotechnology, economics, demography, and other domains.\nCoupled age-structured IPDE population dynamics with two or more species occur\nin epidemiology and ecology, but have received little attention thus far. This\nwork considers an exponentially unstable model of two competing predator\npopulations, formally referred to in the literature as ''competition''\ndynamics. If one were to apply an input that simultaneously harvests both\npredator species, one would have control over only the product of the densities\nof the species, not over their ratio. Therefore, it is necessary to design a\ncontrol input that directly harvests only one of the two predator species,\nwhile indirectly influencing the other via a backstepping approach. The model\nis transformed into a system of two coupled ordinary differential equations\n(ODEs), of which only one is actuated, and two autonomous, exponentially stable\nintegral delay equations (IDEs) which enter the ODEs as nonlinear disturbances.\nThe ODEs are globally stabilized with backstepping and an estimate of the\nregion of attraction of the asymptotically stabilized equilibrium of the full\nIPDE system is provided, under a positivity restriction on control. These\ngeneralizations open exciting possibilities for future research directions,\nsuch as investigating population systems with more than two species.", "comment": "submitted to IFAC Automatica", "pdf_url": "http://arxiv.org/pdf/2507.23013v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22944", "title": "Opacity as Authority: Arbitrariness and the Preclusion of Contestation", "authors": ["Naomi Omeonga wa Kayembe"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22944v1", "summary": "This article redefines arbitrariness not as a normative flaw or a symptom of\ndomination, but as a foundational functional mechanism structuring human\nsystems and interactions. Diverging from critical traditions that conflate\narbitrariness with injustice, it posits arbitrariness as a semiotic trait: a\nproperty enabling systems - linguistic, legal, or social - to operate\neffectively while withholding their internal rationale. Building on Ferdinand\nde Saussure's concept of l'arbitraire du signe, the analysis extends this\nprinciple beyond language to demonstrate its cross-domain applicability,\nparticularly in law and social dynamics. The paper introduces the \"Motivation\n-> Constatability -> Contestability\" chain, arguing that motivation functions\nas a crucial interface rendering an act's logic vulnerable to intersubjective\ncontestation. When this chain is broken through mechanisms like\n\"immotivization\" or \"Conflict Lateralization\" (exemplified by \"the blur of the\nwolf drowned in the fish\"), acts produce binding effects without exposing their\nrationale, thus precluding justiciability. This structural opacity, while\nappearing illogical, is a deliberate design protecting authority from\naccountability. Drawing on Shannon's entropy model, the paper formalizes\narbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern\ntheory of arbitrariness as a neutral operator central to control as well as\ncare, an overlooked dimension of interpersonal relations. While primarily\ndeveloped through human social systems, this framework also illuminates a new\npathway for analyzing explainability in advanced artificial intelligence\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22944v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2506.02169", "title": "LoL-NMPC: Low-Level Dynamics Integration in Nonlinear Model Predictive Control for Unmanned Aerial Vehicles", "authors": ["Parakh M. Gupta", "Ondřej Procházka", "Jan Hřebec", "Matej Novosad", "Robert Pěnička", "Martin Saska"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025", "url": "http://arxiv.org/abs/2506.02169v2", "summary": "[Accepted to IROS 2025] In this paper, we address the problem of tracking\nhigh-speed agile trajectories for Unmanned Aerial Vehicles(UAVs), where model\ninaccuracies can lead to large tracking errors. Existing Nonlinear Model\nPredictive Controller(NMPC) methods typically neglect the dynamics of the\nlow-level flight controllers such as underlying PID controller present in many\nflight stacks, and this results in sub-optimal tracking performance at high\nspeeds and accelerations. To this end, we propose a novel NMPC formulation,\nLoL-NMPC, which explicitly incorporates low-level controller dynamics and motor\ndynamics in order to minimize trajectory tracking errors while maintaining\ncomputational efficiency. By leveraging linear constraints inside low-level\ndynamics, our approach inherently accounts for actuator constraints without\nrequiring additional reallocation strategies. The proposed method is validated\nin both simulation and real-world experiments, demonstrating improved tracking\naccuracy and robustness at speeds up to 98.57 km/h and accelerations of 3.5 g.\nOur results show an average 21.97 % reduction in trajectory tracking error over\nstandard NMPC formulation, with LoL-NMPC maintaining real-time feasibility at\n100 Hz on an embedded ARM-based flight computer.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.02169v2", "cate": "cs.RO", "date": "2025-06-02", "updated": "2025-07-31"}
{"id": "2507.23539", "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions", "authors": ["Piotr Indyk", "Michael Kapralov", "Kshiteej Sheth", "Tal Wagner"], "categories": ["cs.LG", "cs.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in ICLR 2025", "url": "http://arxiv.org/abs/2507.23539v1", "summary": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.", "comment": "Published in ICLR 2025", "pdf_url": "http://arxiv.org/pdf/2507.23539v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23311", "title": "Forgetting of task-specific knowledge in model merging-based continual learning", "authors": ["Timm Hess", "Gido M van de Ven", "Tinne Tuytelaars"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23311v1", "summary": "This paper investigates the linear merging of models in the context of\ncontinual learning (CL). Using controlled visual cues in computer vision\nexperiments, we demonstrate that merging largely preserves or enhances shared\nknowledge, while unshared task-specific knowledge rapidly degrades. We further\nfind that merging models from an incremental training process consistently\noutperforms merging models trained in parallel.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23311v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22941", "title": "SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology", "authors": ["Paul Minchella", "Loïc Verlingue", "Stéphane Chrétien", "Rémi Vaucher", "Guillaume Metzler"], "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.AP"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures, accepted for ECML PKDD 2025", "url": "http://arxiv.org/abs/2507.22941v1", "summary": "Electronic medical reports (EHR) contain a vast amount of information that\ncan be leveraged for machine learning applications in healthcare. However,\nexisting survival analysis methods often struggle to effectively handle the\ncomplexity of textual data, particularly in its sequential form. Here, we\npropose SigBERT, an innovative temporal survival analysis framework designed to\nefficiently process a large number of clinical reports per patient. SigBERT\nprocesses timestamped medical reports by extracting and averaging word\nembeddings into sentence embeddings. To capture temporal dynamics from the time\nseries of sentence embedding coordinates, we apply signature extraction from\nrough path theory to derive geometric features for each patient, which\nsignificantly enhance survival model performance by capturing complex temporal\ndynamics. These features are then integrated into a LASSO-penalized Cox model\nto estimate patient-specific risk scores. The model was trained and evaluated\non a real-world oncology dataset from the L\\'eon B\\'erard Center corpus, with a\nC-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT\nintegrates sequential medical data to enhance risk estimation, advancing\nnarrative-based survival analysis.", "comment": "12 pages, 2 figures, accepted for ECML PKDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.22941v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.23076", "title": "Terahertz for Radar applications and Wireless Communication", "authors": ["Sofiane Latreche", "Hocine Bellahsene", "Abdelmalik Taleb-Ahmed"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23076v1", "summary": "Technological advancements in the design of electronic and optical materials\nhave opened up the possibility of utilizing the latest available Radio\nFrequency spectrum the Terahertz (THz) band. This band holds great promise for\nnext-generation wireless systems, which are poised to seamlessly integrate a\nwide array of data-intensive and time-sensitive applications. In this article,\nwe delve into the Terahertz band, providing insights into its properties and\nshowcasing examples of its applications. We begin by exploring the specific\ncharacteristics of wireless communications and radar systems operating in the\nTHz band. Subsequently, we analyze various effects and parameters unique to\neach of these applications.so we scrutinize the application of Terahertz (THz)\nwireless and radar systems, delving into the modeling of various facets of\nradio frequency propagation within this domain. The interpretation of our\nfindings will be presented at the conclusion of this study.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23076v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22968", "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations", "authors": ["Chengqian Ma", "Wei Tao", "Yiwen Guo"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22968v1", "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22968v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.02708", "title": "Optimizing Start Locations in Ergodic Search for Disaster Response", "authors": ["Ananya Rao", "Alyssa Hargis", "David Wettergreen", "Howie Choset"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02708v3", "summary": "In disaster response scenarios, deploying robotic teams effectively is\ncrucial for improving situational awareness and enhancing search and rescue\noperations. The use of robots in search and rescue has been studied but the\nquestion of where to start robot deployments has not been addressed. This work\naddresses the problem of optimally selecting starting locations for robots with\nheterogeneous capabilities by formulating a joint optimization problem. To\ndetermine start locations, this work adds a constraint to the ergodic\noptimization framework whose minimum assigns robots to start locations. This\nbecomes a little more challenging when the robots are heterogeneous (equipped\nwith different sensing and motion modalities) because not all robots start at\nthe same location, and a more complex adaptation of the aforementioned\nconstraint is applied. Our method assumes access to potential starting\nlocations, which can be obtained from expert knowledge or aerial imagery. We\nexperimentally evaluate the efficacy of our joint optimization approach by\ncomparing it to baseline methods that use fixed starting locations for all\nrobots. Our experimental results show significant gains in coverage\nperformance, with average improvements of 35.98% on synthetic data and 31.91%\non real-world data for homogeneous and heterogeneous teams, in terms of the\nergodic metric.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02708v3", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-30"}
{"id": "2507.23562", "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform", "authors": ["Sirine Arfa", "Bernhard Vogginger", "Christian Mayr"], "categories": ["cs.LG", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, 3 tables", "url": "http://arxiv.org/abs/2507.23562v1", "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.", "comment": "8 pages, 5 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.23562v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23313", "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models", "authors": ["Alfio Ferrara", "Sergio Picascia", "Elisabetta Rocchetti"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      to be published in: Applications of AI in the Analysis of Cultural and Artistic Heritage, organized within the 35th IEEE International Workshop on Machine Learning for Signal Processing (MLSP) 2025", "url": "http://arxiv.org/abs/2507.23313v1", "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ngenerating artistic content by learning from billions of images, including\npopular artworks. However, the fundamental question of how these models\ninternally represent concepts, such as content and style in paintings, remains\nunexplored. Traditional computer vision assumes content and style are\northogonal, but diffusion models receive no explicit guidance about this\ndistinction during training. In this work, we investigate how transformer-based\ntext-to-image diffusion models encode content and style concepts when\ngenerating artworks. We leverage cross-attention heatmaps to attribute pixels\nin generated images to specific prompt tokens, enabling us to isolate image\nregions influenced by content-describing versus style-describing tokens. Our\nfindings reveal that diffusion models demonstrate varying degrees of\ncontent-style separation depending on the specific artistic prompt and style\nrequested. In many cases, content tokens primarily influence object-related\nregions while style tokens affect background and texture areas, suggesting an\nemergent understanding of the content-style distinction. These insights\ncontribute to our understanding of how large-scale generative models internally\nrepresent complex artistic concepts without explicit supervision. We share the\ncode and dataset, together with an exploratory tool for visualizing attention\nmaps at https://github.com/umilISLab/artistic-prompt-interpretation.", "comment": "to be published in: Applications of AI in the Analysis of Cultural\n  and Artistic Heritage, organized within the 35th IEEE International Workshop\n  on Machine Learning for Signal Processing (MLSP) 2025", "pdf_url": "http://arxiv.org/pdf/2507.23313v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23399", "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators", "authors": ["Peter Sandrini"], "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23399v1", "summary": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23399v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23139", "title": "Robust Control Design and Analysis for Nonlinear Systems with Uncertain Initial Conditions Based on Lifting Linearization", "authors": ["Sourav Sinha", "Mazen Farhood"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      24 pages, 13 figures", "url": "http://arxiv.org/abs/2507.23139v1", "summary": "This paper presents a robust control synthesis and analysis framework for\nnonlinear systems with uncertain initial conditions. First, a deep\nlearning-based lifting approach is proposed to approximate nonlinear dynamical\nsystems with linear parameter-varying (LPV) state-space models in\nhigher-dimensional spaces while simultaneously characterizing the uncertain\ninitial states within the lifted state space. Then, convex synthesis conditions\nare provided to generate full-state feedback nonstationary LPV (NSLPV)\ncontrollers for the lifted LPV system. A performance measure similar to the\nl2-induced norm is used to provide robust performance guarantees in the\npresence of exogenous disturbances and uncertain initial conditions. The paper\nalso includes results for synthesizing full-state feedback LTI controllers and\noutput feedback NSLPV controllers. Additionally, a robustness analysis approach\nbased on integral quadratic constraint (IQC) theory is developed to analyze and\ntune the synthesized controllers while accounting for noise associated with\nstate measurements. This analysis approach characterizes model parameters and\ndisturbance inputs using IQCs to reduce conservatism. Finally, the\neffectiveness of the proposed framework is demonstrated through two\nillustrative examples.", "comment": "24 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.23139v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23084", "title": "AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads", "authors": ["Taiyi Wang", "Eiko Yoneki"], "categories": ["cs.DB", "cs.AI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.23084v1", "summary": "Efficiently selecting indexes is fundamental to database performance\noptimization, particularly for systems handling large-scale analytical\nworkloads. While deep reinforcement learning (DRL) has shown promise in\nautomating index selection through its ability to learn from experience, few\nworks address how these RL-based index advisors can adapt to scaling workloads\ndue to exponentially growing action spaces and heavy trial and error. To\naddress these challenges, we introduce AutoIndexer, a framework that combines\nworkload compression, query optimization, and specialized RL models to scale\nindex selection effectively. By operating on compressed workloads, AutoIndexer\nsubstantially lowers search complexity without sacrificing much index quality.\nExtensive evaluations show that it reduces end-to-end query execution time by\nup to 95% versus non-indexed baselines. On average, it outperforms\nstate-of-the-art RL-based index advisors by approximately 20% in workload cost\nsavings while cutting tuning time by over 50%. These results affirm\nAutoIndexer's practicality for large and diverse workloads.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.23084v1", "cate": "cs.DB", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.12194", "title": "UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization", "authors": ["Hongming Shen", "Xun Chen", "Yulin Hui", "Zhenyu Wu", "Wei Wang", "Qiyang Lyu", "Tianchen Deng", "Danwei Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12194v2", "summary": "Existing LGL methods typically consider only partial information (e.g.,\ngeometric features) from LiDAR observations or are designed for homogeneous\nLiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL\nmethod is proposed, termed UniLGL, which simultaneously achieves spatial and\nmaterial uniformity, as well as sensor-type uniformity. The key idea of the\nproposed method is to encode the complete point cloud, which contains both\ngeometric and material information, into a pair of BEV images (i.e., a spatial\nBEV image and an intensity BEV image). An end-to-end multi-BEV fusion network\nis designed to extract uniform features, equipping UniLGL with spatial and\nmaterial uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a\nviewpoint invariance hypothesis is introduced, which replaces the conventional\ntranslation equivariance assumption commonly used in existing LPR networks and\nsupervises UniLGL to achieve sensor-type uniformity in both global descriptors\nand local feature representations. Finally, based on the mapping between local\nfeatures on the 2D BEV image and the point cloud, a robust global pose\nestimator is derived that determines the global minimum of the global pose on\nSE(3) without requiring additional registration. To validate the effectiveness\nof the proposed uniform LGL, extensive benchmarks are conducted in real-world\nenvironments, and the results show that the proposed UniLGL is demonstratively\ncompetitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL\nhas been deployed on diverse platforms, including full-size trucks and agile\nMicro Aerial Vehicles (MAVs), to enable high-precision localization and mapping\nas well as multi-MAV collaborative exploration in port and forest environments,\ndemonstrating the applicability of UniLGL in industrial and field scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12194v2", "cate": "cs.RO", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2507.23568", "title": "Optimised Feature Subset Selection via Simulated Annealing", "authors": ["Fernando Martínez-García", "Álvaro Rubio-García", "Samuel Fernández-Lorenzo", "Juan José García-Ripoll", "Diego Porras"], "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures", "url": "http://arxiv.org/abs/2507.23568v1", "summary": "We introduce SA-FDR, a novel algorithm for $\\ell_0$-norm feature selection\nthat considers this task as a combinatorial optimisation problem and solves it\nby using simulated annealing to perform a global search over the space of\nfeature subsets. The optimisation is guided by the Fisher discriminant ratio,\nwhich we use as a computationally efficient proxy for model quality in\nclassification tasks. Our experiments, conducted on datasets with up to\nhundreds of thousands of samples and hundreds of features, demonstrate that\nSA-FDR consistently selects more compact feature subsets while achieving a high\npredictive accuracy. This ability to recover informative yet minimal sets of\nfeatures stems from its capacity to capture inter-feature dependencies often\nmissed by greedy optimisation approaches. As a result, SA-FDR provides a\nflexible and effective solution for designing interpretable models in\nhigh-dimensional settings, particularly when model sparsity, interpretability,\nand performance are crucial.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.23568v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23315", "title": "Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification", "authors": ["Vineet Kumar Rakesh", "Soumya Mazumdar", "Tapas Samanta", "Sarbajit Pal", "Amitabha Das"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures, 4 tables. Includes ablation study and evaluation on 7 lightweight deep learning models. Code and logs available at this https URL", "url": "http://arxiv.org/abs/2507.23315v1", "summary": "Lightweight convolutional and transformer-based models have become vital for\nreal-time image classification in resource-constrained applications, such as\nembedded systems and edge devices. This work analyzes the influence of\nhyperparameter adjustment on the accuracy and convergence behavior of seven\nefficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT\nv2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are\ntrained on the ImageNet-1K dataset under consistent training settings, with an\nemphasis on real-time practicality. An comprehensive ablation study is\nundertaken to separate the effect of critical hyperparameters, including\nlearning rate schedules, batch sizes, input resolution, data augmentation,\nregularization approaches, and optimizer choice. To assess appropriateness for\nreal-time applications, each model is assessed not only in terms of Top-1 and\nTop-5 classification accuracy, but also in terms of inference time, parameter\ncount, model size, and frames-per-second (FPS) on a GPU-accelerated edge\ndeployment simulation. Results demonstrate that cosine learning rate decay and\nadjustable batch size may greatly boost both accuracy and convergence speed,\nwhile keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80%\nTop-1 accuracy with efficient inference performance, offering a compelling\nbalance between accuracy and deployment cost for VGG-style models. The results\ngive practical guidance for constructing resource-efficient deep learning\nmodels appropriate for real-time image processing pipelines. All code and\ntraining logs are publicly accessible at\nhttps://github.com/VineetKumarRakesh/lcnn-opt.", "comment": "13 pages, 4 figures, 4 tables. Includes ablation study and evaluation\n  on 7 lightweight deep learning models. Code and logs available at\n  https://github.com/VineetKumarRakesh/lcnn-opt", "pdf_url": "http://arxiv.org/pdf/2507.23315v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.12892", "title": "\"I will never pay for this\" Perception of fairness and factors affecting behaviour on 'pay-or-ok' models", "authors": ["Victor Morel", "Farzaneh Karegar", "Cristiana Santos"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted for publication at APF2025", "url": "http://arxiv.org/abs/2505.12892v4", "summary": "The rise of cookie paywalls ('pay-or-ok' models) has prompted growing debates\naround the right to privacy and data protection, monetisation, and the\nlegitimacy of user consent. Despite their increasing use across sectors,\nlimited research has explored how users perceive these models or what shapes\ntheir decisions to either consent to tracking or pay. To address this gap, we\nconducted four focus groups (n= 14) to examine users' perceptions of cookie\npaywalls, their judgments of fairness, and the conditions under which they\nmight consider paying, alongside a legal analysis within the EU data protection\nlegal framework.\n  Participants primarily viewed cookie paywalls as profit-driven, with fairness\nperceptions varying depending on factors such as the presence of a third option\nbeyond consent or payment, transparency of data practices, and the authenticity\nor exclusivity of the paid content. Participants voiced expectations for\ngreater transparency, meaningful control over data collection, and less\ncoercive alternatives, such as contextual advertising or \"reject all\" buttons.\nAlthough some conditions, including trusted providers, exclusive content, and\nreasonable pricing, could make participants consider paying, most expressed\nreluctance or unwillingness to do so.\n  Crucially, our findings raise concerns about economic exclusion, where\nprivacy and data protection might end up becoming a privilege rather than\nfundamental rights. Consent given under financial pressure may not meet the\nstandard of being freely given, as required by the GDPR. To address these\nconcerns, we recommend user-centred approaches that enhance transparency,\nreduce coercion, ensure the value of paid content, and explore inclusive\nalternatives. These measures are essential for supporting fairness, meaningful\nchoice, and user autonomy in consent-driven digital environments.", "comment": "Accepted for publication at APF2025", "pdf_url": "http://arxiv.org/pdf/2505.12892v4", "cate": "cs.CY", "date": "2025-05-19", "updated": "2025-07-31"}
{"id": "2507.23147", "title": "Foundation Models for Clean Energy Forecasting: A Comprehensive Review", "authors": ["Md Meftahul Ferdaus", "Tanmoy Dam", "Md Rasel Sarkar", "Moslem Uddin", "Sreenatha G. Anavatti"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      This paper is currently under review at the journal", "url": "http://arxiv.org/abs/2507.23147v1", "summary": "As global energy systems transit to clean energy, accurate renewable\ngeneration and renewable demand forecasting is imperative for effective grid\nmanagement. Foundation Models (FMs) can help improve forecasting of renewable\ngeneration and demand because FMs can rapidly process complex, high-dimensional\ntime-series data. This review paper focuses on FMs in the realm of renewable\nenergy forecasting, primarily focusing on wind and solar. We present an\noverview of the architectures, pretraining strategies, finetuning methods, and\ntypes of data used in the context of renewable energy forecasting. We emphasize\nthe role of models that are trained at a large scale, domain specific\nTransformer architectures, where attention is paid to spatial temporal\ncorrelations, the embedding of domain knowledge, and also the brief and\nintermittent nature of renewable generation. We assess recent FM based\nadvancements in forecast accuracy such as reconciling predictions over multiple\ntime scales and quantifying uncertainty in renewable energy forecasting. We\nalso review existing challenges and areas of improvement in long-term and\nmultivariate time series forecasting. In this survey, a distinction between\ntheory and practice is established regarding the use of FMs in the clean energy\nforecasting domain. Additionally, it critically assesses the strengths and\nweaknesses of FMs while advancing future research direction in this new and\nexciting area of forecasting.", "comment": "This paper is currently under review at the journal", "pdf_url": "http://arxiv.org/pdf/2507.23147v1", "cate": "eess.SY", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23095", "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity", "authors": ["Ishani Mondal", "Meera Bharadwaj", "Ayush Roy", "Aparna Garimella", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under Submission", "url": "http://arxiv.org/abs/2507.23095v1", "summary": "We present SMART-Editor, a framework for compositional layout and content\nediting across structured (posters, websites) and unstructured (natural images)\ndomains. Unlike prior models that perform local edits, SMART-Editor preserves\nglobal coherence through two strategies: Reward-Refine, an inference-time\nrewardguided refinement method, and RewardDPO, a training-time preference\noptimization approach using reward-aligned layout pairs. To evaluate model\nperformance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,\ncascading edit scenarios. SMART-Editor outperforms strong baselines like\nInstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in\nstructured settings and Reward-Refine showing advantages on natural images.\nAutomatic and human evaluations confirm the value of reward-guided planning in\nproducing semantically consistent and visually aligned edits.", "comment": "Under Submission", "pdf_url": "http://arxiv.org/pdf/2507.23095v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.12911", "title": "LaViPlan : Language-Guided Visual Path Planning with RLVR", "authors": ["Hayeon Oh"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been withdrawn due to an internal institutional policy that prohibits preprint submissions to arXiv", "url": "http://arxiv.org/abs/2507.12911v3", "summary": "Out-of-distribution (OOD) scenarios in autonomous driving refer to situations\nthat deviate from the training domain, often leading to unexpected and\npotentially hazardous behavior from planners that lack prior exposure to such\ncases. Recently, Vision-Language Models (VLMs) have been introduced into\nautonomous driving research for their promising generalization capabilities in\nOOD settings. Early studies demonstrated that VLMs could recognize OOD\nscenarios and generate user-level decisions such as \"go straight\" or \"turn\nright.\" However, a new challenge has emerged due to the misalignment between\nthe VLM's high-level decisions or visual reasoning expressed in language, and\nthe low-level predicted trajectories interpreted as actions. In this paper, we\npropose LaViPlan, a framework that leverages Reinforcement Learning with\nVerifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.\nThis approach addresses the vision-language-action misalignment observed in\nexisting VLMs fine-tuned via supervised learning, which can recognize driving\nscenarios but often produce context-unaware decisions. Experimental results\ndemonstrate that our method improves situational awareness and decision-making\nunder OOD conditions, highlighting its potential to mitigate the misalignment\nissue. This work introduces a promising post-training paradigm for VLM agents\nin the context of autonomous driving.", "comment": "This paper has been withdrawn due to an internal institutional policy\n  that prohibits preprint submissions to arXiv", "pdf_url": "http://arxiv.org/pdf/2507.12911v3", "cate": "cs.RO", "date": "2025-07-17", "updated": "2025-07-23"}
{"id": "2507.23581", "title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning", "authors": ["Chuanyue Yu", "Kuo Zhao", "Yuhan Li", "Heng Chang", "Mingjian Feng", "Xiangzhe Jiang", "Yufei Sun", "Jia Li", "Yuzhi Zhang", "Jianxin Li", "Ziwei Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23581v1", "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23581v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23318", "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning", "authors": ["Jiajun Cao", "Qizhe Zhang", "Peidong Jia", "Xuhui Zhao", "Bo Lan", "Xiaoan Zhang", "Xiaobao Wei", "Sixiang Chen", "Zhuo Li", "Yang Wang", "Liyun Li", "Xianming Liu", "Ming Lu", "Shanghang Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.23318v1", "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential\nin complex scene understanding and action reasoning, leading to their\nincreasing adoption in end-to-end autonomous driving systems. However, the long\nvisual tokens of VLA models greatly increase computational costs. Current\nvisual token pruning methods in Vision-Language Models (VLM) rely on either\nvisual token similarity or visual-text attention, but both have shown poor\nperformance in autonomous driving scenarios. Given that human drivers\nconcentrate on relevant foreground areas while driving, we assert that\nretaining visual tokens containing this foreground information is essential for\neffective decision-making. Inspired by this, we propose FastDriveVLA, a novel\nreconstruction-based vision token pruning framework designed specifically for\nautonomous driving. FastDriveVLA includes a plug-and-play visual token pruner\ncalled ReconPruner, which prioritizes foreground information through MAE-style\npixel reconstruction. A novel adversarial foreground-background reconstruction\nstrategy is designed to train ReconPruner for the visual encoder of VLA models.\nOnce trained, ReconPruner can be seamlessly applied to different VLA models\nwith the same visual encoder without retraining. To train ReconPruner, we also\nintroduce a large-scale dataset called nuScenes-FG, consisting of 241K\nimage-mask pairs with annotated foreground regions. Our approach achieves\nstate-of-the-art results on the nuScenes closed-loop planning benchmark across\ndifferent pruning ratios.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23318v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2206.03234", "title": "Disparate Conditional Prediction in Multiclass Classifiers", "authors": ["Sivan Sabato", "Eran Treister", "Elad Yom-Tov"], "categories": ["cs.LG", "cs.CY", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published at ICML 2025", "url": "http://arxiv.org/abs/2206.03234v3", "summary": "We propose methods for auditing multiclass classifiers for fairness under\nmulticlass equalized odds,by estimating the deviation from equalized odds when\nthe classifier is not completely fair. We generalize to multiclass classifiers\nthe measure of Disparate Conditional Prediction (DCP), originally suggested by\nSabato & Yom-Tov (2020) for binary classifiers. DCP is defined as the fraction\nof the population for which the classifier predicts with conditional prediction\nprobabilities that differ from the closest common baseline. We provide new\nlocal-optimization methods for estimating the multiclass DCPunder two different\nregimes,one in which the conditional confusion matrices for each protected\nsub-population are known, and one in which these cannot be estimated, for\ninstance, because the classifier is inaccessible or because good-quality\nindividual-level data is not available. These methods can be used to detect\nclassifiers that likely treat a significant fraction of the population\nunfairly. Experiments demonstrate the accuracy of the methods. Code is provided\nat https://github.com/sivansabato/ DCPmulticlass.", "comment": "Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2206.03234v3", "cate": "cs.LG", "date": "2022-06-07", "updated": "2025-07-31"}
{"id": "2507.23280", "title": "Data-Driven Stochastic Control via Non-i.i.d. Trajectories: Foundations and Guarantees", "authors": ["Abolfazl Lavaei"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23280v1", "summary": "This work establishes a crucial step toward advancing data-driven\ntrajectory-based methods for stochastic systems with unknown mathematical\ndynamics. In contrast to scenario-based approaches that rely on independent and\nidentically distributed (i.i.d.) trajectories, this work develops a data-driven\nframework where each trajectory is gathered over a finite horizon and exhibits\ntemporal dependence-referred to as a non-i.i.d. trajectory. To ensure safety of\ndynamical systems using such trajectories, the current body of literature\nprimarily considers dynamics subject to unknown-but-bounded disturbances, which\nfacilitates robust analysis. While promising, such bounds may be violated in\npractice and the resulting worst-case robust analysis tends to be overly\nconservative. To overcome these fundamental challenges, this paper considers\nstochastic systems with unknown mathematical dynamics, influenced by process\nnoise with unknown distributions. In the proposed framework, data is collected\nfrom stochastic systems under multiple realizations within a finite-horizon\nexperiment, where each realization generates a non-i.i.d. trajectory.\nLeveraging the concept of stochastic control barrier certificates constructed\nfrom data, this work quantifies probabilistic safety guarantees with a\ncertified confidence level. To achieve this, the proposed conditions are\nformulated as sum-of-squares (SOS) optimization problems, relying solely on\nempirical average of the collected trajectories and statistical features of the\nprocess noise. The efficacy of the approach has been validated on three\nstochastic benchmarks with both unknown models and noise distributions. In one\ncase study, it is shown that while no safety controller exists for the robust\nanalysis of the system under bounded disturbances, the proposed stochastic\nframework offers a safety controller with guaranteed probabilistic\nsatisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23280v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22909", "title": "Rydberg Atomic Receivers for Wireless Communications: Fundamentals, Potential, Applications, and Challenges", "authors": ["Yin Zhang", "Jiayi Zhang", "Bokai Xu", "Yuanbin Chen", "Zhilong Liu", "Jiakang Zheng", "Enyu Shi", "Ziheng Liu", "Tierui Gong", "Wei E. I. Sha", "Chau Yuen", "Shi Jin", "Bo Ai"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22909v1", "summary": "Rydberg atomic receivers (RARs) leverage the quantum coherence of highly\nexcited atoms to overcome the intrinsic physical limitations of conventional\nradio frequency receivers (RFRs), particularly in sensitivity, and bandwidth.\nThis innovative technology represents a paradigm shift in wireless\ncommunication systems. This paper systematically explains the fundamental\nsensing mechanisms of RARs, contrasts their differences from RFRs in working\nprinciples and architectures. We explore their advantages in emerging wireless\ncommunication scenarios, such as integrated sensing and communications, quantum\nRydberg radar, and quantum space communications. Practical challenges, such as\nlimited instantaneous bandwidth and nonlinear distortion, are identified. To\naddress these issues, mitigation strategies and future research directions are\nalso outlined, supporting the advancement of RAR-aided wireless systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22909v1", "cate": "eess.SP", "date": "2025-07-16", "updated": "2025-07-16"}
{"id": "2507.23104", "title": "RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL", "authors": ["Jeffrey Eben", "Aitzaz Ahmad", "Stephen Lau"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23104v1", "summary": "Despite advances in large language model (LLM)-based natural language\ninterfaces for databases, scaling to enterprise-level data catalogs remains an\nunder-explored challenge. Prior works addressing this challenge rely on\ndomain-specific fine-tuning - complicating deployment - and fail to leverage\nimportant semantic context contained within database metadata. To address these\nlimitations, we introduce a component-based retrieval architecture that\ndecomposes database schemas and metadata into discrete semantic units, each\nseparately indexed for targeted retrieval. Our approach prioritizes effective\ntable identification while leveraging column-level information, ensuring the\ntotal number of retrieved tables remains within a manageable context budget.\nExperiments demonstrate that our method maintains high recall and accuracy,\nwith our system outperforming baselines over massive databases with varying\nstructure and available metadata. Our solution enables practical text-to-SQL\nsystems deployable across diverse enterprise settings without specialized\nfine-tuning, addressing a critical scalability gap in natural language database\ninterfaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23104v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.14820", "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning", "authors": ["Bingran Chen", "Baorun Li", "Jian Yang", "Yong Liu", "Guangyao Zhai"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14820v2", "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14820v2", "cate": "cs.RO", "date": "2025-07-20", "updated": "2025-07-31"}
{"id": "2507.23604", "title": "Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning", "authors": ["Tommaso Marzi", "Cesare Alippi", "Andrea Cini"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23604v1", "summary": "Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for\nlearning scalable multi-agent policies, but suffer from partial observability\nand induced non-stationarity. These challenges can be addressed by introducing\nmechanisms that facilitate coordination and high-level planning. Specifically,\ncoordination and temporal abstraction can be achieved through communication\n(e.g., message passing) and Hierarchical Reinforcement Learning (HRL)\napproaches to decision-making. However, optimization issues limit the\napplicability of hierarchical policies to multi-agent systems. As such, the\ncombination of these approaches has not been fully explored. To fill this void,\nwe propose a novel and effective methodology for learning multi-agent\nhierarchies of message-passing policies. We adopt the feudal HRL framework and\nrely on a hierarchical graph structure for planning and coordination among\nagents. Agents at lower levels in the hierarchy receive goals from the upper\nlevels and exchange messages with neighboring agents at the same level. To\nlearn hierarchical multi-agent policies, we design a novel reward-assignment\nmethod based on training the lower-level policies to maximize the advantage\nfunction associated with the upper levels. Results on relevant benchmarks show\nthat our method performs favorably compared to the state of the art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23604v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23325", "title": "FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models", "authors": ["Yiming Yang", "Hongbin Lin", "Yueru Luo", "Suzhong Fu", "Chao Zheng", "Xinrui Yan", "Shuqi Mei", "Kun Tang", "Shuguang Cui", "Zhen Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23325v1", "summary": "Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)\nroad scene understanding, which can serve as a key perception module in\nplanning-oriented end-to-end autonomous driving systems. Existing lane topology\nreasoning methods often fall short in effectively leveraging temporal\ninformation to enhance detection and reasoning performance. Recently,\nstream-based temporal propagation method has demonstrated promising results by\nincorporating temporal cues at both the query and BEV levels. However, it\nremains limited by over-reliance on historical queries, vulnerability to pose\nestimation failures, and insufficient temporal propagation. To overcome these\nlimitations, we propose FASTopoWM, a novel fast-slow lane segment topology\nreasoning framework augmented with latent world models. To reduce the impact of\npose estimation failures, this unified framework enables parallel supervision\nof both historical and newly initialized queries, facilitating mutual\nreinforcement between the fast and slow systems. Furthermore, we introduce\nlatent query and BEV world models conditioned on the action latent to propagate\nthe state representations from past observations to the current timestep. This\ndesign substantially improves the performance of temporal perception within the\nslow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate\nthat FASTopoWM outperforms state-of-the-art methods in both lane segment\ndetection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%\non OLS).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23325v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.02851", "title": "Leveraging LLMs to Create Content Corpora for Niche Domains", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages (main content), 5 figures. Supplementary materials can be found at this https URL", "url": "http://arxiv.org/abs/2505.02851v2", "summary": "Constructing specialized content corpora from vast, unstructured web sources\nfor domain-specific applications poses substantial data curation challenges. In\nthis paper, we introduce a streamlined approach for generating high-quality,\ndomain-specific corpora by efficiently acquiring, filtering, structuring, and\ncleaning web-based data. We showcase how Large Language Models (LLMs) can be\nleveraged to address complex data curation at scale, and propose a strategical\nframework incorporating LLM-enhanced techniques for structured content\nextraction and semantic deduplication. We validate our approach in the behavior\neducation domain through its integration into 30 Day Me, a habit formation\napplication. Our data pipeline, named 30DayGen, enabled the extraction and\nsynthesis of 3,531 unique 30-day challenges from over 15K webpages. A user\nsurvey reports a satisfaction score of 4.3 out of 5, with 91% of respondents\nindicating willingness to use the curated content for their habit-formation\ngoals.", "comment": "9 pages (main content), 5 figures. Supplementary materials can be\n  found at https://github.com/pigfyy/30DayGen-Supplementary-Materials", "pdf_url": "http://arxiv.org/pdf/2505.02851v2", "cate": "cs.CL", "date": "2025-05-02", "updated": "2025-07-31"}
{"id": "2507.23308", "title": "A Framework for Ethical Decision-Making in Automated Vehicles through Human Reasons-based Supervision", "authors": ["Lucas Elbert Suryana", "Saeed Rahmani", "Simeon Craig Calvert", "Arkady Zgonnikov", "Bart van Arem"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures", "url": "http://arxiv.org/abs/2507.23308v1", "summary": "Ethical dilemmas are a common challenge in everyday driving, requiring human\ndrivers to balance competing priorities such as safety, efficiency, and rule\ncompliance. However, much of the existing research in automated vehicles (AVs)\nhas focused on high-stakes \"trolley problems,\" which involve extreme and rare\nsituations. Such scenarios, though rich in ethical implications, are rarely\napplicable in real-world AV decision-making. In practice, when AVs confront\neveryday ethical dilemmas, they often appear to prioritise strict adherence to\ntraffic rules. By contrast, human drivers may bend the rules in\ncontext-specific situations, using judgement informed by practical concerns\nsuch as safety and efficiency. According to the concept of meaningful human\ncontrol, AVs should respond to human reasons, including those of drivers,\nvulnerable road users, and policymakers. This work introduces a novel human\nreasons-based supervision framework that detects when AV behaviour misaligns\nwith expected human reasons to trigger trajectory reconsideration. The\nframework integrates with motion planning and control systems to support\nreal-time adaptation, enabling decisions that better reflect safety,\nefficiency, and regulatory considerations. Simulation results demonstrate that\nthis approach could help AVs respond more effectively to ethical challenges in\ndynamic driving environments by prompting replanning when the current\ntrajectory fails to align with human reasons. These findings suggest that our\napproach offers a path toward more adaptable, human-centered decision-making in\nAVs.", "comment": "7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.23308v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23057", "title": "Neural Energy Landscapes Predict Working Memory Decline After Brain Tumor Resection", "authors": ["Triet M. Tran", "Sina Khanmohammadi"], "categories": ["eess.SP", "q-bio.NC"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23057v1", "summary": "Surgical resection is the primary treatment option for brain tumor patients,\nbut it carries the risk of postoperative cognitive dysfunction. This study\ninvestigates how tumor-induced alterations in presurgical neural dynamics\nrelate to postoperative working memory decline. We analyzed functional magnetic\nresonance imaging (fMRI) of brain tumor patients before surgery and extracted\nenergy landscapes of high-order brain interactions. We then examined the\nrelation between these energy features and postoperative working memory\nperformance using statistical and machine learning (random forest) models.\nPatients with lower postoperative working memory scores exhibited fewer but\nmore extreme transitions between local energy minima and maxima, whereas\npatients with higher scores showed more frequent but less extreme shifts.\nFurthermore, the presurgical high-order energy features were able to accurately\npredict postoperative working memory decline with a mean accuracy of 90\\%, F1\nscore of 87.5\\%, and an AUC of 0.95. Our study suggests that the brain\ntumor-induced disruptions in high-order neural dynamics before surgery are\npredictive of postoperative working memory decline. Our findings pave the path\nfor personalized surgical planning and targeted interventions to mitigate\ncognitive risks associated with brain tumor resection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23057v1", "cate": "eess.SP", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23121", "title": "Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity", "authors": ["Xinwei Wu", "Haojie Li", "Hongyu Liu", "Xinyu Ji", "Ruohan Li", "Yule Chen", "Yigeng Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)", "url": "http://arxiv.org/abs/2507.23121v1", "summary": "In this work, we study a critical research problem regarding the\ntrustworthiness of large language models (LLMs): how LLMs behave when\nencountering ambiguous narrative text, with a particular focus on Chinese\ntextual ambiguity. We created a benchmark dataset by collecting and generating\nambiguous sentences with context and their corresponding disambiguated pairs,\nrepresenting multiple possible interpretations. These annotated examples are\nsystematically categorized into 3 main categories and 9 subcategories. Through\nexperiments, we discovered significant fragility in LLMs when handling\nambiguity, revealing behavior that differs substantially from humans.\nSpecifically, LLMs cannot reliably distinguish ambiguous text from unambiguous\ntext, show overconfidence in interpreting ambiguous text as having a single\nmeaning rather than multiple meanings, and exhibit overthinking when attempting\nto understand the various possible meanings. Our findings highlight a\nfundamental limitation in current LLMs that has significant implications for\ntheir deployment in real-world applications where linguistic ambiguity is\ncommon, calling for improved approaches to handle uncertainty in language\nunderstanding. The dataset and code are publicly available at this GitHub\nrepository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.", "comment": "Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic\n  and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)", "pdf_url": "http://arxiv.org/pdf/2507.23121v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.19079", "title": "SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research", "authors": ["Feng Zhu", "Zihang Zhang", "Kangcheng Teng", "Abduhelil Yakup", "Xiaohong Zhang"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19079v2", "summary": "High-precision navigation and positioning systems are critical for\napplications in autonomous vehicles and mobile mapping, where robust and\ncontinuous localization is essential. To test and enhance the performance of\nalgorithms, some research institutions and companies have successively\nconstructed and publicly released datasets. However, existing datasets still\nsuffer from limitations in sensor diversity and environmental coverage. To\naddress these shortcomings and advance development in related fields, the\nSmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset\nhas been developed. This dataset integrates data from multiple sensors,\nincluding Global Navigation Satellite Systems (GNSS), Inertial Measurement\nUnits (IMU), optical cameras, and LiDAR, to provide a rich and versatile\nresource for research in multi-sensor fusion and high-precision navigation. The\ndataset construction process is thoroughly documented, encompassing sensor\nconfigurations, coordinate system definitions, and calibration procedures for\nboth cameras and LiDAR. A standardized framework for data collection and\nprocessing ensures consistency and scalability, enabling large-scale analysis.\nValidation using state-of-the-art Simultaneous Localization and Mapping (SLAM)\nalgorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's\napplicability for advanced navigation research. Covering a wide range of\nreal-world scenarios, including urban areas, campuses, tunnels, and suburban\nenvironments, the dataset offers a valuable tool for advancing navigation\ntechnologies and addressing challenges in complex environments. By providing a\npublicly accessible, high-quality dataset, this work aims to bridge gaps in\nsensor diversity, data accessibility, and environmental representation,\nfostering further innovation in the field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19079v2", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2507.23607", "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates", "authors": ["Tien Huu Do", "Antoine Masquelier", "Nae Eoun Lee", "Jonathan Crowther"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23607v1", "summary": "Clinical trials are a systematic endeavor to assess the safety and efficacy\nof new drugs or treatments. Conducting such trials typically demands\nsignificant financial investment and meticulous planning, highlighting the need\nfor accurate predictions of trial outcomes. Accurately predicting patient\nenrollment, a key factor in trial success, is one of the primary challenges\nduring the planning phase. In this work, we propose a novel deep learning-based\nmethod to address this critical challenge. Our method, implemented as a neural\nnetwork model, leverages pre-trained language models (PLMs) to capture the\ncomplexities and nuances of clinical documents, transforming them into\nexpressive representations. These representations are then combined with\nencoded tabular features via an attention mechanism. To account for\nuncertainties in enrollment prediction, we enhance the model with a\nprobabilistic layer based on the Gamma distribution, which enables range\nestimation. We apply the proposed model to predict clinical trial duration,\nassuming site-level enrollment follows a Poisson-Gamma process. We carry out\nextensive experiments on real-world clinical trial data, and show that the\nproposed method can effectively predict the number of patients enrolled at a\nnumber of sites for a given clinical trial, outperforming established baseline\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23607v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23326", "title": "Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation", "authors": ["Yingkai Wang", "Yaoyao Zhu", "Xiuding Cai", "Yuhao Xiao", "Haotian Wu", "Yu Yao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23326v1", "summary": "Medical image segmentation plays a crucial role in clinical workflows, but\ndomain shift often leads to performance degradation when models are applied to\nunseen clinical domains. This challenge arises due to variations in imaging\nconditions, scanner types, and acquisition protocols, limiting the practical\ndeployment of segmentation models. Unlike natural images, medical images\ntypically exhibit consistent anatomical structures across patients, with\ndomain-specific variations mainly caused by imaging conditions. This unique\ncharacteristic makes medical image segmentation particularly challenging.\n  To address this challenge, we propose a domain generalization framework\ntailored for medical image segmentation. Our approach improves robustness to\ndomain-specific variations by introducing implicit feature perturbations guided\nby domain statistics. Specifically, we employ a learnable semantic direction\nselector and a covariance-based semantic intensity sampler to modulate\ndomain-variant features while preserving task-relevant anatomical consistency.\nFurthermore, we design an adaptive consistency constraint that is selectively\napplied only when feature adjustment leads to degraded segmentation\nperformance. This constraint encourages the adjusted features to align with the\noriginal predictions, thereby stabilizing feature selection and improving the\nreliability of the segmentation.\n  Extensive experiments on two public multi-center benchmarks show that our\nframework consistently outperforms existing domain generalization approaches,\nachieving robust and generalizable segmentation performance across diverse\nclinical domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23326v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.12103", "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7pages, 4 figures", "url": "http://arxiv.org/abs/2507.12103v3", "summary": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "comment": "7pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.12103v3", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-30"}
{"id": "2507.23396", "title": "Energy management and flexibility quantification in a discrete event distribution grid simulation", "authors": ["Sebastian Peter", "Daniel Feismann", "Johannes Bao", "Thomas Oberließen", "Christian Rehtanz"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, part of PowerTech conference proceedings", "url": "http://arxiv.org/abs/2507.23396v1", "summary": "Distribution grid operation faces new challenges caused by a rising share of\nrenewable energy sources and the introduction of additional types of loads to\nthe grid. With the increasing adoption of distributed generation and emerging\nprosumer households, Energy Management Systems, which manage and apply\nflexibility of connected devices, are gaining popularity. While potentially\nbeneficial to grid capacity, strategic energy management also adds to the\ncomplexity of distribution grid operation and planning processes. Novel\napproaches of time-series-based planning likewise face increasingly complex\nsimulation scenarios and rising computational cost. Discrete event modelling\nhelps facilitating simulations of such scenarios by restraining computation to\nthe most relevant points in simulation time. We provide an enhancement of a\ndiscrete event distribution grid simulation software that offers fast\nimplementation and testing of energy management algorithms, embedded into a\nfeature-rich simulation environment. Physical models are specified using the\nDiscrete Event System Specification. Furthermore, we contribute a communication\nprotocol that makes use of the discrete event paradigm by only computing\nflexibility potential when necessary.", "comment": "6 pages, 5 figures, part of PowerTech conference proceedings", "pdf_url": "http://arxiv.org/pdf/2507.23396v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23235", "title": "In-Orbit Cosmo-SkyMed antenna pattern estimation by a narrowband sweeper receiver", "authors": ["Mohammad Roueinfar", "Masoud Ardini"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23235v1", "summary": "This paper introduces a novel method for antenna pattern estimation in\nsatellites equipped with Synthetic Aperture Radar (SAR), utilizing a Narrowband\nSweeper Receiver (NSR). By accurately measuring power across individual\nfrequencies within SAR's inherently broadband spectrum, the NSR significantly\nenhances antenna pattern extraction accuracy. Analytical models and practical\nexperiments conducted using the Cosmo-SkyMed satellite validate the receiver's\nperformance, demonstrating superior signal-to-noise ratio (SNR) compared to\nconventional receivers. This research represents a key advancement in SAR\ntechnology, offering a robust framework for future satellite calibration and\nverification methodologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23235v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23194", "title": "Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks", "authors": ["Jianghui Wang", "Vinay Joshi", "Saptarshi Majumder", "Xu Chao", "Bin Ding", "Ziqiong Liu", "Pratik Prabhanjan Brahma", "Dong Li", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23194v1", "summary": "The demand for AI-generated GPU kernels is rapidly growing, influenced by the\nneed for scalable, hardware-optimized solutions in both industry and academia.\nAs deep learning workloads grow in complexity and diversity, it is imperative\nto automate low-level kernel development to meet performance and productivity\ndemands. Major cloud providers, semiconductor companies, and research\ninstitutions are now investing heavily in AI-driven code generation for GPUs,\naiming to reduce manual optimization efforts while achieving near-expert\nperformance on hardware like AMD MI300X. The Triton language, a Python-based\nDSL for GPU programming, has emerged as a popular target for such AI-generated\nkernels due to its balance of performance and ease-of-coding. In this work, we\npresent an evaluation suite for Triton-based GPU kernels and GEAK (Generating\nEfficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs\nto generate performant Triton code specifically for AMD GPUs, including the AMD\nMI300X and MI250. GEAK leverages inference-time compute scaling to produce\nTriton-based GPU kernels using a reasoning loop adapted from Reflexion-style\nfeedback mechanisms. On two evaluation benchmarks, GEAK significantly\noutperformed the baselines of directly prompting frontier LLMs as well as\nReflexion-based generation pipelines by achieving correctness up to $63$% and\nexecution speed up of up to $2.59$X. These results highlight the promise of\nGEAK-like agentic code generation for accelerating the adoption of diverse\nhardware platforms and democratizing access to expert-level kernel performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23194v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.20293", "title": "Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance with Model Predictive Path Integral", "authors": ["Stepan Dergachev", "Konstantin Yakovlev"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This is a pre-print of the paper accepted to IROS2025. The manuscript includes 8 pages, 4 figures, and 1 table. A supplementary video is available at this https URL Updated version: added link to source code in the abstract; updated experimental results description in Section VI.A; updated author affiliation and funding information; minor typo corrections", "url": "http://arxiv.org/abs/2507.20293v2", "summary": "Decentralized multi-agent navigation under uncertainty is a complex task that\narises in numerous robotic applications. It requires collision avoidance\nstrategies that account for both kinematic constraints, sensing and action\nexecution noise. In this paper, we propose a novel approach that integrates the\nModel Predictive Path Integral (MPPI) with a probabilistic adaptation of\nOptimal Reciprocal Collision Avoidance. Our method ensures safe and efficient\nmulti-agent navigation by incorporating probabilistic safety constraints\ndirectly into the MPPI sampling process via a Second-Order Cone Programming\nformulation. This approach enables agents to operate independently using local\nnoisy observations while maintaining safety guarantees. We validate our\nalgorithm through extensive simulations with differential-drive robots and\nbenchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.\nResults demonstrate that our approach outperforms them while achieving high\nsuccess rates, even in densely populated environments. Additionally, validation\nin the Gazebo simulator confirms its practical applicability to robotic\nplatforms. A source code is available at\nhttp://github.com/PathPlanning/MPPI-Collision-Avoidance.", "comment": "This is a pre-print of the paper accepted to IROS2025. The manuscript\n  includes 8 pages, 4 figures, and 1 table. A supplementary video is available\n  at https://youtu.be/_D4zDYJ4KCk Updated version: added link to source code in\n  the abstract; updated experimental results description in Section VI.A;\n  updated author affiliation and funding information; minor typo corrections", "pdf_url": "http://arxiv.org/pdf/2507.20293v2", "cate": "cs.RO", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2507.23615", "title": "L-GTA: Latent Generative Modeling for Time Series Augmentation", "authors": ["Luis Roque", "Carlos Soares", "Vitor Cerqueira", "Luis Torgo"], "categories": ["cs.LG", "cs.AI", "68T01", "I.5.1; G.3; H.2.8; I.2.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23615v1", "summary": "Data augmentation is gaining importance across various aspects of time series\nanalysis, from forecasting to classification and anomaly detection tasks. We\nintroduce the Latent Generative Transformer Augmentation (L-GTA) model, a\ngenerative approach using a transformer-based variational recurrent\nautoencoder. This model uses controlled transformations within the latent space\nof the model to generate new time series that preserve the intrinsic properties\nof the original dataset. L-GTA enables the application of diverse\ntransformations, ranging from simple jittering to magnitude warping, and\ncombining these basic transformations to generate more complex synthetic time\nseries datasets. Our evaluation of several real-world datasets demonstrates the\nability of L-GTA to produce more reliable, consistent, and controllable\naugmented data. This translates into significant improvements in predictive\naccuracy and similarity measures compared to direct transformation methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23615v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23331", "title": "Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision", "authors": ["Qiang Lu", "Waikit Xiu", "Xiying Li", "Shenyu Hu", "Shengbo Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11pages, 5 figures", "url": "http://arxiv.org/abs/2507.23331v1", "summary": "Traffic sign recognition, as a core component of autonomous driving\nperception systems, directly influences vehicle environmental awareness and\ndriving safety. Current technologies face two significant challenges: first,\nthe traffic sign dataset exhibits a pronounced long-tail distribution,\nresulting in a substantial decline in recognition performance of traditional\nconvolutional networks when processing low-frequency and out-of-distribution\nclasses; second, traffic signs in real-world scenarios are predominantly small\ntargets with significant scale variations, making it difficult to extract\nmulti-scale features.To overcome these issues, we propose a novel two-stage\nframework combining open-vocabulary detection and cross-modal learning. For\ntraffic sign detection, our NanoVerse YOLO model integrates a reparameterizable\nvision-language path aggregation network (RepVL-PAN) and an SPD-Conv module to\nspecifically enhance feature extraction for small, multi-scale targets. For\ntraffic sign classification, we designed a Traffic Sign Recognition Multimodal\nContrastive Learning model (TSR-MCL). By contrasting visual features from a\nVision Transformer with semantic features from a rule-based BERT, TSR-MCL\nlearns robust, frequency-independent representations, effectively mitigating\nclass confusion caused by data imbalance. On the TT100K dataset, our method\nachieves a state-of-the-art 78.4% mAP in the long-tail detection task for\nall-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,\nsignificantly outperforming mainstream algorithms and demonstrating superior\naccuracy and generalization in complex, open-world scenarios.", "comment": "11pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23331v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23401", "title": "Advancing Standard Load Profiles with Data-Driven Techniques and Recent Datasets", "authors": ["Jawana Gabrielski", "Ulf Häger"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      6 pages, 11 figures, part of 2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm) proceedings", "url": "http://arxiv.org/abs/2507.23401v1", "summary": "Estimating electricity consumption accurately is essential for the planning\nand operation of energy systems, as well as for billing processes. Standard\nLoad Profiles (SLP) are widely used to estimate consumption patterns of\ndifferent user groups. However, in Germany these SLP were formulated using\nhistorical data from over 20 years ago and have not been adjusted since.\nChanging electricity consumption behaviour, which leads to increasing\ndeviations between load patterns and SLP, results in a need for a revision\ntaking into account new data. The growing number of smart meters provides a\nlarge measurement database, which enables more accurate load modelling. This\npaper creates updated SLP using recent data. In addition, the assumptions of\nthe SLP method are validated and improvements are proposed, taking into account\nthe ease of applicability. Furthermore, a Fourier Series-based model is\nproposed as an alternative SLP model. The different models are compared and\nevaluated.", "comment": "6 pages, 11 figures, part of 2024 IEEE International Conference on\n  Communications, Control, and Computing Technologies for Smart Grids\n  (SmartGridComm) proceedings", "pdf_url": "http://arxiv.org/pdf/2507.23401v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23236", "title": "BS-1-to-N: Diffusion-Based Environment-Aware Cross-BS Channel Knowledge Map Generation for Cell-Free Networks", "authors": ["Zhuoyin Dai", "Di Wu", "Yong Zeng", "Xiaoli Xu", "Xinyi Wang", "Zesong Fei"], "categories": ["eess.SP", "eess.IV"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23236v1", "summary": "Channel knowledge map (CKM) inference across base stations (BSs) is the key\nto achieving efficient environmentaware communications. This paper proposes an\nenvironmentaware cross-BS CKM inference method called BS-1-to-N based on the\ngenerative diffusion model. To this end, we first design the BS location\nembedding (BSLE) method tailored for cross-BS CKM inference to embed BS\nlocation information in the feature vector of CKM. Further, we utilize the\ncross- and self-attention mechanism for the proposed BS-1-to-N model to\nrespectively learn the relationships between source and target BSs, as well as\nthat among target BSs. Therefore, given the locations of the source and target\nBSs, together with the source CKMs as control conditions, cross-BS CKM\ninference can be performed for an arbitrary number of source and target BSs.\nSpecifically, in architectures with massive distributed nodes like cell-free\nnetworks, traditional methods of sequentially traversing each BS for CKM\nconstruction are prohibitively costly. By contrast, the proposed BS-1-to-N\nmodel is able to achieve efficient CKM inference for a target BS at any\npotential location based on the CKMs of source BSs. This is achieved by\nexploiting the fact that within a given area, different BSs share the same\nwireless environment that leads to their respective CKMs. Therefore, similar to\nmulti-view synthesis, CKMs of different BSs are representations of the same\nwireless environment from different BS locations. By mining the implicit\ncorrelation between CKM and BS location based on the wireless environment, the\nproposed BS-1-to-N method achieves efficient CKM inference across BSs. We\nprovide extensive comparisons of CKM inference between the proposed BS-1-to-N\ngenerative model versus benchmarking schemes, and provide one use case study to\ndemonstrate its practical application for the optimization of BS deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23236v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23334", "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures", "url": "http://arxiv.org/abs/2507.23334v1", "summary": "Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency.", "comment": "8 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.23334v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22653", "title": "UniLegs: Universal Multi-Legged Robot Control through Morphology-Agnostic Policy Distillation", "authors": ["Weijie Xi", "Zhanxiang Cao", "Chenlin Ming", "Jianying Zheng", "Guyue Zhou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, IROS 2025", "url": "http://arxiv.org/abs/2507.22653v2", "summary": "Developing controllers that generalize across diverse robot morphologies\nremains a significant challenge in legged locomotion. Traditional approaches\neither create specialized controllers for each morphology or compromise\nperformance for generality. This paper introduces a two-stage teacher-student\nframework that bridges this gap through policy distillation. First, we train\nspecialized teacher policies optimized for individual morphologies, capturing\nthe unique optimal control strategies for each robot design. Then, we distill\nthis specialized expertise into a single Transformer-based student policy\ncapable of controlling robots with varying leg configurations. Our experiments\nacross five distinct legged morphologies demonstrate that our approach\npreserves morphology-specific optimal behaviors, with the Transformer\narchitecture achieving 94.47% of teacher performance on training morphologies\nand 72.64% on unseen robot designs. Comparative analysis reveals that\nTransformer-based architectures consistently outperform MLP baselines by\nleveraging attention mechanisms to effectively model joint relationships across\ndifferent kinematic structures. We validate our approach through successful\ndeployment on a physical quadruped robot, demonstrating the practical viability\nof our morphology-agnostic control framework. This work presents a scalable\nsolution for developing universal legged robot controllers that maintain\nnear-optimal performance while generalizing across diverse morphologies.", "comment": "6 pages, 3 figures, IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.22653v2", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23632", "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective", "authors": ["Gabriel Mongaras", "Eric C. Larson"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23632v1", "summary": "Since its introduction, softmax attention has become the backbone of modern\ntransformer architectures due to its expressiveness and scalability across a\nwide range of tasks. However, the main drawback of softmax attention is the\nquadratic memory requirement and computational complexity with respect to the\nsequence length. By replacing the softmax nonlinearity, linear attention and\nsimilar methods have been introduced to avoid the quadratic bottleneck of\nsoftmax attention. Despite these linear forms of attention being derived from\nthe original softmax formulation, they typically lag in terms of downstream\naccuracy. While strong intuition of the softmax nonlinearity on the query and\nkey inner product suggests that it has desirable properties compared to other\nnonlinearities, the question of why this discrepancy exists still remains\nunanswered. This work demonstrates that linear attention is an approximation of\nsoftmax attention by deriving the recurrent form of softmax attention. Using\nthis form, each part of softmax attention can be described in the language of\nrecurrent neural networks (RNNs). Describing softmax attention as an RNN allows\nfor the ablation of the components of softmax attention to understand the\nimportance of each part and how they interact. In this way, our work helps\nexplain why softmax attention is more expressive than its counterparts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23632v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23340", "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting", "authors": ["Xingyue Peng", "Yuandong Lyu", "Lang Zhang", "Jian Zhu", "Songtao Wang", "Jiaxin Deng", "Songxin Lu", "Weiliang Ma", "Dangen She", "Peng Jia", "XianPeng Lang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23340v1", "summary": "Road surface reconstruction is essential for autonomous driving, supporting\ncentimeter-accurate lane perception and high-definition mapping in complex\nurban environments.While recent methods based on mesh rendering or 3D Gaussian\nsplatting (3DGS) achieve promising results under clean and static conditions,\nthey remain vulnerable to occlusions from dynamic agents, visual clutter from\nstatic obstacles, and appearance degradation caused by lighting and weather\nchanges. We present a robust reconstruction framework that integrates\nocclusion-aware 2D Gaussian surfels with semantic-guided color enhancement to\nrecover clean, consistent road surfaces. Our method leverages a planar-adapted\nGaussian representation for efficient large-scale modeling, employs\nsegmentation-guided video inpainting to remove both dynamic and static\nforeground objects, and enhances color coherence via semantic-aware correction\nin HSV space. Extensive experiments on urban-scale datasets demonstrate that\nour framework produces visually coherent and geometrically faithful\nreconstructions, significantly outperforming prior methods under real-world\nconditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23340v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23489", "title": "Distributionally Robust Cascading Risk Quantification in Multi-Agent Rendezvous: Effects of Time Delay and Network Connectivity", "authors": ["Vivek Pandey", "Nader Motee"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23489v1", "summary": "Achieving safety in autonomous multi-agent systems, particularly in\ntime-critical tasks like rendezvous, is a critical challenge. In this paper, we\npropose a distributionally robust risk framework for analyzing cascading\nfailures in multi-agent rendezvous. To capture the complex interactions between\nnetwork connectivity, system dynamics, and communication delays, we use a\ntime-delayed network model as a benchmark. We introduce a conditional\ndistributionally robust functional to quantify cascading effects between\nagents, utilizing a bi-variate normal distribution. Our approach yields\nclosed-form risk expressions that reveal the impact of time delay, noise\nstatistics, communication topology, and failure modes on rendezvous risk. The\ninsights derived inform the design of resilient networks that mitigate the risk\nof cascading failures. We validate our theoretical results through extensive\nsimulations, demonstrating the effectiveness of our framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23489v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23381", "title": "A Secure Full-Duplex Wireless Circulator enabled by Non-Reciprocal Beyond-Diagonal RIS", "authors": ["Ziang Liu", "Bruno Clerckx"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Submitted for IEEE journal", "url": "http://arxiv.org/abs/2507.23381v1", "summary": "Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has arisen as a\npromising technology for enhancing wireless communication systems by enabling\nflexible and intelligent wave manipulation. This is achieved through the\ninterconnections among the ports of the impedance network, enabling wave\nreconfiguration when they flow through the surface. Thus, the output wave at\none port depends on waves impinging on neighboring ports, allowing non-local\ncontrol of both phase and magnitude. Non-reciprocal (NR)-BD-RIS further\nenhances this capability by breaking circuit reciprocity and, consequently,\nchannel reciprocity. This feature potentially benefits communication among\nnon-aligned transceivers. This paper introduces a novel application of\nNR-BD-RIS in full-duplex (FD) wireless circulators, where multiple FD devices\ncommunicate via an NR-BD-RIS. This system is particularly beneficial for secure\ntransmission, as it enforces one-way communication among FD devices, suppresses\nsignal from all other users, and thus prevents eavesdropping. In addition, a\nphysics-compliant system model is considered by incorporating structural\nscattering, also known as specular reflection. By accounting for this effect,\nthe advantages of NR-BD-RIS are further validated. Specifically, we formulate\nan all-user sum-rate maximization problem and propose an iterative optimization\nalgorithm that employs block coordinate descent (BCD) and penalty dual\ndecomposition (PDD) methods. Numerical evaluations illustrate that NR-BD-RIS\nconsistently outperforms reciprocal (R)-BD-RIS and conventional diagonal\n(D)-RIS in terms of sum-rate performance, particularly when more than two\nimpinging and reflection directions need to be supported. By analyzing the\npower of signals from all other users and the beampatterns, we show that secure\ntransmission can be achieved.", "comment": "Submitted for IEEE journal", "pdf_url": "http://arxiv.org/pdf/2507.23381v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22953", "title": "CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body Anatomy in Computed Tomography", "authors": ["Murong Xu", "Tamaz Amiranashvili", "Fernando Navarro", "Maksym Fritsak", "Ibrahim Ethem Hamamci", "Suprosanna Shit", "Bastian Wittmann", "Sezgin Er", "Sebastian M. Christ", "Ezequiel de la Rosa", "Julian Deseoe", "Robert Graf", "Hendrik Möller", "Anjany Sekuboyina", "Jan C. Peeken", "Sven Becker", "Giulia Baldini", "Johannes Haubold", "Felix Nensa", "René Hosch", "Nikhil Mirajkar", "Saad Khalid", "Stefan Zachow", "Marc-André Weber", "Georg Langs", "Jakob Wasserthal", "Mehmet Kemal Ozdemir", "Andrey Fedorov", "Ron Kikinis", "Stephanie Tanadini-Lang", "Jan S. Kirschke", "Stephanie E. Combs", "Bjoern Menze"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22953v1", "summary": "Accurate delineation of anatomical structures in volumetric CT scans is\ncrucial for diagnosis and treatment planning. While AI has advanced automated\nsegmentation, current approaches typically target individual structures,\ncreating a fragmented landscape of incompatible models with varying performance\nand disparate evaluation protocols. Foundational segmentation models address\nthese limitations by providing a holistic anatomical view through a single\nmodel. Yet, robust clinical deployment demands comprehensive training data,\nwhich is lacking in existing whole-body approaches, both in terms of data\nheterogeneity and, more importantly, anatomical coverage. In this work, rather\nthan pursuing incremental optimizations in model architecture, we present CADS,\nan open-source framework that prioritizes the systematic integration,\nstandardization, and labeling of heterogeneous data sources for whole-body CT\nsegmentation. At its core is a large-scale dataset of 22,022 CT volumes with\ncomplete annotations for 167 anatomical structures, representing a significant\nadvancement in both scale and coverage, with 18 times more scans than existing\ncollections and 60% more distinct anatomical targets. Building on this diverse\ndataset, we develop the CADS-model using established architectures for\naccessible and automated full-body CT segmentation. Through comprehensive\nevaluation across 18 public datasets and an independent real-world hospital\ncohort, we demonstrate advantages over SoTA approaches. Notably, thorough\ntesting of the model's performance in segmentation tasks from radiation\noncology validates its direct utility for clinical interventions. By making our\nlarge-scale dataset, our segmentation models, and our clinical software tool\npublicly available, we aim to advance robust AI solutions in radiology and make\ncomprehensive anatomical analysis accessible to clinicians and researchers\nalike.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22953v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23358", "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction", "authors": ["Renato Vukovic", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Hsien-Chin Lin", "Shutong Feng", "Nurul Lubis", "Milica Gasic"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23358v1", "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23358v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.09486", "title": "ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning", "authors": ["Yarden As", "Bhavya Sukhija", "Lenart Treven", "Carmelo Sferrazza", "Stelian Coros", "Andreas Krause"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.09486v3", "summary": "Reinforcement learning (RL) is ubiquitous in the development of modern AI\nsystems. However, state-of-the-art RL agents require extensive, and potentially\nunsafe, interactions with their environments to learn effectively. These\nlimitations confine RL agents to simulated environments, hindering their\nability to learn directly in real-world settings. In this work, we present\nActSafe, a novel model-based RL algorithm for safe and efficient exploration.\nActSafe learns a well-calibrated probabilistic model of the system and plans\noptimistically w.r.t. the epistemic uncertainty about the unknown dynamics,\nwhile enforcing pessimism w.r.t. the safety constraints. Under regularity\nassumptions on the constraints and dynamics, we show that ActSafe guarantees\nsafety during learning while also obtaining a near-optimal policy in finite\ntime. In addition, we propose a practical variant of ActSafe that builds on\nlatest model-based RL advancements and enables safe exploration even in\nhigh-dimensional settings such as visual control. We empirically show that\nActSafe obtains state-of-the-art performance in difficult exploration tasks on\nstandard safe deep RL benchmarks while ensuring safety during learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.09486v3", "cate": "cs.LG", "date": "2024-10-12", "updated": "2025-07-31"}
{"id": "2507.23638", "title": "OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting", "authors": ["Mohammad Karami", "Fatemeh Ghassemi", "Hamed Kebriaei", "Hamid Azadegan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23638v1", "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed medical institutions while preserving patient privacy, but remains\nvulnerable to Byzantine attacks and statistical heterogeneity. We present\nOptiGradTrust, a comprehensive defense framework that evaluates gradient\nupdates through a novel six-dimensional fingerprint including VAE\nreconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency\nratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module\nfor adaptive trust scoring. To address convergence challenges under data\nheterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch\nNormalization with proximal regularization for optimal accuracy-convergence\ntrade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI\ndatasets under various Byzantine attack scenarios demonstrates significant\nimprovements over state-of-the-art defenses, achieving up to +1.6 percentage\npoints over FLGuard under non-IID conditions while maintaining robust\nperformance against diverse attack patterns through our adaptive learning\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23638v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23341", "title": "The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models", "authors": ["Ahmet Can Ömercikoğlu", "Mustafa Mansur Yönügül", "Pakize Erdoğmuş"], "categories": ["cs.CV", "68T45, 68T07", "I.4.8; I.4.9; I.5.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, 4 tables", "url": "http://arxiv.org/abs/2507.23341v1", "summary": "Face detection is a crucial component in many AI-driven applications such as\nsurveillance, biometric authentication, and human-computer interaction.\nHowever, real-world conditions like low-resolution imagery present significant\nchallenges that degrade detection performance. In this study, we systematically\ninvestigate the impact of input resolution on the accuracy and robustness of\nthree prominent deep learning-based face detectors: YOLOv11, YOLOv12, and\nMTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across\nmultiple image resolutions (160x160, 320x320, and 640x640) and assess each\nmodel's performance using metrics such as precision, recall, mAP50, mAP50-95,\nand inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN\nin terms of detection accuracy, especially at higher resolutions, while YOLOv12\nexhibits slightly better recall. MTCNN, although competitive in landmark\nlocalization, lags in real-time inference speed. Our findings provide\nactionable insights for selecting resolution-aware face detection models\nsuitable for varying operational constraints.", "comment": "6 pages, 5 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.23341v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23571", "title": "Asynchronous Grid Connections Providing Fast-Frequency Response: System Integration Study", "authors": ["Felix Wald", "Amir Sajadi", "Barry Mather", "Giovanni De Carne"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23571v1", "summary": "This paper presents an integration study for a power electronic-based\nfast-frequency response technology, an asynchronous grid connection operating\nas an aggregator for behindthe-meter resources and distributed generators. Both\ntechnical feasibility and techno-economic viability studies are presented. The\ndynamic performance of the fast-frequency response enabled by the asynchronous\ngrid connection is validated with Power Hardware-in-the-Loop experiments and\ntransferred to an IEEE 9-bus system in DigSilent PowerFactory for dynamic\nstability analysis. We demonstrate that droop-based control enhancements to the\nlocal distributed generators could allow their aggregation to provide\ngrid-supporting functionalities and participate in the market for ancillary\nservices. To this end, we performed a long-term simulation embedding the system\nwithin the ancillary service market framework of PJM. The fast-frequency\nresponse regulation is subsequently used to calculate the potential revenue and\nproject the results on a 15-year investment horizon. Finally, the\ntechno-economic analysis concludes with recommendations for enhancements to\naccess the full potential of distributed generators on a technical and\nregulatory level.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23571v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23518", "title": "EVMx: An FPGA-Based Smart Contract Processing Unit", "authors": ["Joel Poncha Lemayian", "Hachem Bensalem", "Ghyslain Gagnon", "Kaiwen Zhang", "Pascal Giard"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.23518v1", "summary": "Ethereum blockchain uses smart contracts (SCs) to implement decentralized\napplications (dApps). SCs are executed by the Ethereum virtual machine (EVM)\nrunning within an Ethereum client. Moreover, the EVM has been widely adopted by\nother blockchain platforms, including Solana, Cardano, Avalanche, Polkadot, and\nmore. However, the EVM performance is limited by the constraints of the\ngeneral-purpose computer it operates on. This work proposes offloading SC\nexecution onto a dedicated hardware-based EVM. Specifically, EVMx is an\nFPGA-based SC execution engine that benefits from the inherent parallelism and\nhigh-speed processing capabilities of a hardware architecture. Synthesis\nresults demonstrate a reduction in execution time of 61% to 99% for commonly\nused operation codes compared to CPU-based SC execution environments. Moreover,\nthe execution time of Ethereum blocks on EVMx is up to 6x faster compared to\nanalogous works in the literature. These results highlight the potential of the\nproposed architecture to accelerate SC execution and enhance the performance of\nEVM-compatible blockchains.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.23518v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23001", "title": "LesionGen: A Concept-Guided Diffusion Model for Dermatology Image Synthesis", "authors": ["Jamil Fayyad", "Nourhan Bayasi", "Ziyang Yu", "Homayoun Najjaran"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at the MICCAI 2025 ISIC Workshop", "url": "http://arxiv.org/abs/2507.23001v1", "summary": "Deep learning models for skin disease classification require large, diverse,\nand well-annotated datasets. However, such resources are often limited due to\nprivacy concerns, high annotation costs, and insufficient demographic\nrepresentation. While text-to-image diffusion probabilistic models (T2I-DPMs)\noffer promise for medical data synthesis, their use in dermatology remains\nunderexplored, largely due to the scarcity of rich textual descriptions in\nexisting skin image datasets. In this work, we introduce LesionGen, a\nclinically informed T2I-DPM framework for dermatology image synthesis. Unlike\nprior methods that rely on simplistic disease labels, LesionGen is trained on\nstructured, concept-rich dermatological captions derived from expert\nannotations and pseudo-generated, concept-guided reports. By fine-tuning a\npretrained diffusion model on these high-quality image-caption pairs, we enable\nthe generation of realistic and diverse skin lesion images conditioned on\nmeaningful dermatological descriptions. Our results demonstrate that models\ntrained solely on our synthetic dataset achieve classification accuracy\ncomparable to those trained on real images, with notable gains in worst-case\nsubgroup performance. Code and data are available here.", "comment": "Accepted at the MICCAI 2025 ISIC Workshop", "pdf_url": "http://arxiv.org/pdf/2507.23001v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23365", "title": "\"I made this (sort of)\": Negotiating authorship, confronting fraudulence, and exploring new musical spaces with prompt-based AI music generation", "authors": ["Bob L. T. Sturm"], "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2; J.5"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23365v1", "summary": "I reflect on my experience creating two music albums centered on\nstate-of-the-art prompt-based AI music generation platforms. The first album\nexplicitly poses the question: What happens when I collide my junk mail with\nthese platforms? The second album is a direct response to the first, and toys\nwith the inability of state-of-the-art prompt-based AI music generation\nplatforms to generate music that is not ``practiced'', ``polished'', and\n``produced''. I seed a large language model (LLM) with information about these\nalbums and have it interview me, which results in the exploration of several\ndeeper questions: To what extent am I the author? Where am I in the resulting\nmusic? How is my musical identity changing as I am faced with machines that are\nin some ways far more talented than I? What new musical spaces does my work\nopen, for me or anyone/thing else? I conclude by reflecting on my reflections,\nas well as LLM-mediated self-reflection as method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23365v1", "cate": "cs.SD", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.12098", "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization", "authors": ["Bhavya Sukhija", "Stelian Coros", "Andreas Krause", "Pieter Abbeel", "Carmelo Sferrazza"], "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.12098v2", "summary": "Reinforcement learning (RL) algorithms aim to balance exploiting the current\nbest strategy with exploring new options that could lead to higher rewards.\nMost common RL algorithms use undirected exploration, i.e., select random\nsequences of actions. Exploration can also be directed using intrinsic rewards,\nsuch as curiosity or model epistemic uncertainty. However, effectively\nbalancing task and intrinsic rewards is challenging and often task-dependent.\nIn this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and\nextrinsic exploration. MaxInfoRL steers exploration towards informative\ntransitions, by maximizing intrinsic rewards such as the information gain about\nthe underlying task. When combined with Boltzmann exploration, this approach\nnaturally trades off maximization of the value function with that of the\nentropy over states, rewards, and actions. We show that our approach achieves\nsublinear regret in the simplified setting of multi-armed bandits. We then\napply this general formulation to a variety of off-policy model-free RL methods\nfor continuous state-action spaces, yielding novel algorithms that achieve\nsuperior performance across hard exploration problems and complex scenarios\nsuch as visual control tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.12098v2", "cate": "cs.LG", "date": "2024-12-16", "updated": "2025-07-31"}
{"id": "2507.23665", "title": "SHAP-Guided Regularization in Machine Learning Models", "authors": ["Amal Saadallah"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23665v1", "summary": "Feature attribution methods such as SHapley Additive exPlanations (SHAP) have\nbecome instrumental in understanding machine learning models, but their role in\nguiding model optimization remains underexplored. In this paper, we propose a\nSHAP-guided regularization framework that incorporates feature importance\nconstraints into model training to enhance both predictive performance and\ninterpretability. Our approach applies entropy-based penalties to encourage\nsparse, concentrated feature attributions while promoting stability across\nsamples. The framework is applicable to both regression and classification\ntasks. Our first exploration started with investigating a tree-based model\nregularization using TreeSHAP. Through extensive experiments on benchmark\nregression and classification datasets, we demonstrate that our method improves\ngeneralization performance while ensuring robust and interpretable feature\nattributions. The proposed technique offers a novel, explainability-driven\nregularization approach, making machine learning models both more accurate and\nmore reliable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23665v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23343", "title": "Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads", "authors": ["Yingjie Zhou", "Jiezhang Cao", "Zicheng Zhang", "Farong Wen", "Yanwei Jiang", "Jun Jia", "Xiaohong Liu", "Xiongkuo Min", "Guangtao Zhai"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23343v1", "summary": "Speech-driven methods for portraits are figuratively known as \"Talkers\"\nbecause of their capability to synthesize speaking mouth shapes and facial\nmovements. Especially with the rapid development of the Text-to-Image (T2I)\nmodels, AI-Generated Talking Heads (AGTHs) have gradually become an emerging\ndigital human media. However, challenges persist regarding the quality of these\ntalkers and AGTHs they generate, and comprehensive studies addressing these\nissues remain limited. To address this gap, this paper presents the largest\nAGTH quality assessment dataset THQA-10K to date, which selects 12 prominent\nT2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After\nexcluding instances where AGTH generation is unsuccessful, the THQA-10K dataset\ncontains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the\nAGTHs and give the corresponding distortion categories. In our analysis for\nsubjective experimental results, we evaluate the performance of talkers in\nterms of generalizability and quality, and also expose the distortions of\nexisting AGTHs. Finally, an objective quality assessment method based on the\nfirst frame, Y-T slice and tone-lip consistency is proposed. Experimental\nresults show that this method can achieve state-of-the-art (SOTA) performance\nin AGTH quality assessment. The work is released at\nhttps://github.com/zyj-2000/Talker.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23343v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23591", "title": "Tensor-based reduction of linear parameter-varying state-space models", "authors": ["Bogoljub Terzin", "E. Javier Olucha", "Amritam Das", "Siep Weiland", "Roland Tóth"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23591v1", "summary": "The Linear Parameter-Varying (LPV) framework is a powerful tool for\ncontrolling nonlinear and complex systems, but the conversion of nonlinear\nmodels into LPV forms often results in high-dimensional and overly conservative\nLPV models. To be able to apply control strategies, there is often a need for\nmodel reduction in order to reduce computational needs. This paper presents the\nfirst systematic approach for the joint reduction of state order and scheduling\nsignal dimension of LPV state space models. The existing methods typically\naddress these reductions separately. By formulating a tensorial form of LPV\nmodels with an affine dependency on the scheduling variables, we leverage\ntensor decomposition to find the dominant components of state and scheduling\nsubspaces. We extend the common Petrov-Galerkin projection approach to LPV\nframework by adding a scheduling projection. This extension enables the joint\nreduction. To find suitable subspaces for the extended Petrov-Galerkin\nprojection, we have developed two different methods: tensor-based LPV moment\nmatching, and an approach through Proper Orthogonal Decomposition. Advantages\nof the proposed methods are demonstrated on two different series-interconnected\nmass-spring-damper systems with nonlinear springs: one primarily used for\ncomparison with other methods and a more elaborate higher-order model designed\nto assess scalability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23591v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23570", "title": "Multiple-Parameter Graph Fractional Fourier Transform: Theory and Applications", "authors": ["Manjun Cui", "Zhichao Zhang", "Wei Yao"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23570v1", "summary": "The graph fractional Fourier transform (GFRFT) applies a single global\nfractional order to all graph frequencies, which restricts its adaptability to\ndiverse signal characteristics across the spectral domain. To address this\nlimitation, in this paper, we propose two types of multiple-parameter GFRFTs\n(MPGFRFTs) and establish their corresponding theoretical frameworks. We design\na spectral compression strategy tailored for ultra-low compression ratios,\neffectively preserving essential information even under extreme dimensionality\nreduction. To enhance flexibility, we introduce a learnable order vector scheme\nthat enables adaptive compression and denoising, demonstrating strong\nperformance on both graph signals and images. We explore the application of\nMPGFRFTs to image encryption and decryption. Experimental results validate the\nversatility and superior performance of the proposed MPGFRFT framework across\nvarious graph signal processing tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23570v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23065", "title": "Diffusion model for gradient preconditioning in hyperspectral imaging inverse problems", "authors": ["Jonathan Monsalve", "Kumar Vijay Mishra"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23065v1", "summary": "Recovering high-dimensional statistical structure from limited measurements\nis a fundamental challenge in hyperspectral imaging, where capturing\nfull-resolution data is often infeasible due to sensor, bandwidth, or\nacquisition constraints. A common workaround is to partition measurements and\nestimate local statistics-such as the covariance matrix-using only partial\nobservations. However, this strategy introduces noise in the optimization\ngradients, especially when each partition contains few samples. In this work,\nwe reinterpret this accumulation of gradient noise as a diffusion process,\nwhere successive partitions inject increasing uncertainty into the learning\nsignal. Building on this insight, we propose a novel framework that leverages\ndenoising diffusion models to learn a reverse process in gradient space. The\nmodel is trained to map noisy gradient estimates toward clean, well-conditioned\nupdates, effectively preconditioning the optimization. Our approach bridges\ngenerative modeling and inverse problem solving, improving convergence and\nreconstruction quality under aggressive sampling regimes. We validate our\nmethod on hyperspectral recovery tasks, demonstrating significant gains in\naccuracy and stability over traditional optimization pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23065v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23382", "title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models", "authors": ["Yiyan Ji", "Haoran Chen", "Qiguang Chen", "Chengyue Wu", "Libo Qin", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.8; I.2.10"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.23382v1", "summary": "Multimodal planning capabilities refer to the ability to predict, reason, and\ndesign steps for task execution with multimodal context, which is essential for\ncomplex reasoning and decision-making across multiple steps. However, current\nbenchmarks face two key challenges: (1) they cannot directly assess multimodal\nreal-world planning capabilities, and (2) they lack constraints or implicit\nconstraints across modalities. To address these issues, we introduce Multimodal\nPlanning with Complex Constraints (MPCC), the first benchmark to systematically\nevaluate MLLMs' ability to handle multimodal constraints in planning. To\naddress the first challenge, MPCC focuses on three real-world tasks: Flight\nPlanning, Calendar Planning, and Meeting Planning. To solve the second\nchallenge, we introduce complex constraints (e.g. budget, temporal, and\nspatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to\nseparate constraint complexity from search space expansion. Experiments on 13\nadvanced MLLMs reveal significant challenges: closed-source models achieve only\n21.3% feasible plans, while open-source models average below 11%. Additionally,\nwe observe that MLLMs are highly sensitive to constraint complexity and that\ntraditional multimodal prompting strategies fail in multi-constraint scenarios.\nOur work formalizes multimodal constraints in planning, provides a rigorous\nevaluation framework, and highlights the need for advancements in\nconstraint-aware reasoning for real-world MLLM applications.", "comment": "Accepted to ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.23382v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.05422", "title": "EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations", "authors": ["Yue Yao", "Mohamed-Khalil Bouzidi", "Daniel Goehring", "Joerg Reichardt"], "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05422v3", "summary": "As the prediction horizon increases, predicting the future evolution of\ntraffic scenes becomes increasingly difficult due to the multi-modal nature of\nagent motion. Most state-of-the-art (SotA) prediction models primarily focus on\nforecasting the most likely future. However, for the safe operation of\nautonomous vehicles, it is equally important to cover the distribution for\nplausible motion alternatives. To address this, we introduce EP-Diffuser, a\nnovel parameter-efficient diffusion-based generative model designed to capture\nthe distribution of possible traffic scene evolutions. Conditioned on road\nlayout and agent history, our model acts as a predictor and generates diverse,\nplausible scene continuations. We benchmark EP-Diffuser against two SotA models\nin terms of accuracy and plausibility of predictions on the Argoverse 2\ndataset. Despite its significantly smaller model size, our approach achieves\nboth highly accurate and plausible traffic scene predictions. We further\nevaluate model generalization ability in an out-of-distribution (OoD) test\nsetting using Waymo Open dataset and show superior robustness of our approach.\nThe code and model checkpoints are available at:\nhttps://github.com/continental/EP-Diffuser.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05422v3", "cate": "cs.CV", "date": "2025-04-07", "updated": "2025-07-31"}
{"id": "2507.23674", "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses", "authors": ["Muhammad Taha Cheema", "Abeer Aamir", "Khawaja Gul Muhammad", "Naveed Anwar Bhatti", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 9 figures", "url": "http://arxiv.org/abs/2507.23674v1", "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.", "comment": "13 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.23674v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23357", "title": "IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025", "authors": ["Radu-Andrei Bourceanu", "Neil De La Fuente", "Jan Grimm", "Andrei Jardan", "Andriy Manucharyan", "Cornelius Weiss", "Roman Pflugfelder"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23357v1", "summary": "This report analyzes the evolution of key design patterns in computer vision\nby examining six influential papers. The analy- sis begins with foundational\narchitectures for image recognition. We review ResNet, which introduced\nresidual connections to overcome the vanishing gradient problem and enable\neffective training of significantly deeper convolutional networks.\nSubsequently, we examine the Vision Transformer (ViT), which established a new\nparadigm by applying the Transformer ar- chitecture to sequences of image\npatches, demonstrating the efficacy of attention-based models for large-scale\nimage recogni- tion. Building on these visual representation backbones, we\ninvestigate generative models. Generative Adversarial Networks (GANs) are\nanalyzed for their novel adversarial training process, which challenges a\ngenerator against a discriminator to learn complex data distributions. Then,\nLatent Diffusion Models (LDMs) are covered, which improve upon prior generative\nmethods by performing a sequential denoising process in a perceptually\ncompressed latent space. LDMs achieve high-fidelity synthesis with greater\ncomputational efficiency, representing the current state-of-the-art for image\ngeneration. Finally, we explore self-supervised learning techniques that reduce\ndependency on labeled data. DINO is a self-distillation framework in which a\nstudent network learns to match the output of a momentum-updated teacher,\nyielding features with strong k-NN classification performance. We conclude with\nMasked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design\nto reconstruct heavily masked inputs, providing a highly scalable and effective\nmethod for pre-training large-scale vision models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23357v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23149", "title": "Learning with Episodic Hypothesis Testing in General Games: A Framework for Equilibrium Selection", "authors": ["Ruifan Yang", "Manxi Wu"], "categories": ["cs.GT", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23149v1", "summary": "We introduce a new hypothesis testing-based learning dynamics in which\nplayers update their strategies by combining hypothesis testing with\nutility-driven exploration. In this dynamics, each player forms beliefs about\nopponents' strategies and episodically tests these beliefs using empirical\nobservations. Beliefs are resampled either when the hypothesis test is rejected\nor through exploration, where the probability of exploration decreases with the\nplayer's (transformed) utility. In general finite normal-form games, we show\nthat the learning process converges to a set of approximate Nash equilibria\nand, more importantly, to a refinement that selects equilibria maximizing the\nminimum (transformed) utility across all players. Our result establishes\nconvergence to equilibrium in general finite games and reveals a novel\nmechanism for equilibrium selection induced by the structure of the learning\ndynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23149v1", "cate": "cs.GT", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23695", "title": "On the Achievable Rate of Satellite Quantum Communication Channel using Deep Autoencoder Gaussian Mixture Model", "authors": ["Mouli Chakraborty", "Subhash Chandra", "Avishek Nag", "Anshu Mukherjee"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23695v1", "summary": "We present a comparative study of the Gaussian mixture model (GMM) and the\nDeep Autoencoder Gaussian Mixture Model (DAGMM) for estimating satellite\nquantum channel capacity, considering hybrid quantum noise (HQN) and\ntransmission constraints. While GMM is simple and interpretable, DAGMM better\ncaptures non-linear variations and noise distributions. Simulations show that\nDAGMM provides tighter capacity bounds and improved clustering. This introduces\nthe Deep Cluster Gaussian Mixture Model (DCGMM) for high-dimensional quantum\ndata analysis in quantum satellite communication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23695v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23129", "title": "MRpro - open PyTorch-based MR reconstruction and processing package", "authors": ["Felix Frederik Zimmermann", "Patrick Schuenke", "Christoph S. Aigner", "Bill A. Bernhardt", "Mara Guastini", "Johannes Hammacher", "Noah Jaitner", "Andreas Kofler", "Leonid Lunin", "Stefan Martin", "Catarina Redshaw Kranich", "Jakob Schattenfroh", "David Schote", "Yanglei Wu", "Christoph Kolbitsch"], "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Submitted to Magnetic Resonance in Medicine", "url": "http://arxiv.org/abs/2507.23129v1", "summary": "We introduce MRpro, an open-source image reconstruction package built upon\nPyTorch and open data formats. The framework comprises three main areas. First,\nit provides unified data structures for the consistent manipulation of MR\ndatasets and their associated metadata (e.g., k-space trajectories). Second, it\noffers a library of composable operators, proximable functionals, and\noptimization algorithms, including a unified Fourier operator for all common\ntrajectories and an extended phase graph simulation for quantitative MR. These\ncomponents are used to create ready-to-use implementations of key\nreconstruction algorithms. Third, for deep learning, MRpro includes essential\nbuilding blocks such as data consistency layers, differentiable optimization\nlayers, and state-of-the-art backbone networks and integrates public datasets\nto facilitate reproducibility. MRpro is developed as a collaborative project\nsupported by automated quality control. We demonstrate the versatility of MRpro\nacross multiple applications, including Cartesian, radial, and spiral\nacquisitions; motion-corrected reconstruction; cardiac MR fingerprinting;\nlearned spatially adaptive regularization weights; model-based learned image\nreconstruction and quantitative parameter estimation. MRpro offers an\nextensible framework for MR image reconstruction. With reproducibility and\nmaintainability at its core, it facilitates collaborative development and\nprovides a foundation for future MR imaging research.", "comment": "Submitted to Magnetic Resonance in Medicine", "pdf_url": "http://arxiv.org/pdf/2507.23129v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23386", "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models", "authors": ["Ailiang Lin", "Zhuoyun Li", "Kotaro Funakoshi"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23386v1", "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23386v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.08278", "title": "Line-Search Filter Differential Dynamic Programming for Optimal Control with Nonlinear Equality Constraints", "authors": ["Ming Xu", "Stephen Gould", "Iman Shames"], "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.08278v4", "summary": "We present FilterDDP, a differential dynamic programming algorithm for\nsolving discrete-time, optimal control problems (OCPs) with nonlinear equality\nconstraints. Unlike prior methods based on merit functions or the augmented\nLagrangian class of algorithms, FilterDDP uses a step filter in conjunction\nwith a line search to handle equality constraints. We identify two important\ndesign choices for the step filter criteria which lead to robust numerical\nperformance: 1) we use the Lagrangian instead of the cost as one of the filter\ncriterion and, 2) for the stopping criteria and backward pass Hessians, we\nreplace the value function gradient with an estimated dual variable of the\ndynamics constraints. Both choices are rigorously justified, for 2) in\nparticular by a formal proof of local quadratic convergence. We validate\nFilterDDP on three contact implicit trajectory optimisation problems which\narise in robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.08278v4", "cate": "math.OC", "date": "2025-04-11", "updated": "2025-07-31"}
{"id": "2507.23675", "title": "One-Step Flow Policy Mirror Descent", "authors": ["Tianyi Chen", "Haitong Ma", "Na Li", "Kai Wang", "Bo Dai"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23675v1", "summary": "Diffusion policies have achieved great success in online reinforcement\nlearning (RL) due to their strong expressive capacity. However, the inference\nof diffusion policy models relies on a slow iterative sampling process, which\nlimits their responsiveness. To overcome this limitation, we propose Flow\nPolicy Mirror Descent (FPMD), an online RL algorithm that enables 1-step\nsampling during policy inference. Our approach exploits a theoretical\nconnection between the distribution variance and the discretization error of\nsingle-step sampling in straight interpolation flow matching models, and\nrequires no extra distillation or consistency training. We present two\nalgorithm variants based on flow policy and MeanFlow policy parametrizations,\nrespectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate\nthat our algorithms show strong performance comparable to diffusion policy\nbaselines while requiring hundreds of times fewer function evaluations during\ninference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23675v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23362", "title": "Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers", "authors": ["Ji Ma", "Wei Suo", "Peng Wang", "Yanning Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted By ACM MM 25", "url": "http://arxiv.org/abs/2507.23362v1", "summary": "Although large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in multi-modal understanding and reasoning, their practical\napplications are still limited by massive model parameters and high\ncomputational costs. Recent efforts from natural language processing (NLP) have\nshown the effectiveness of layer pruning, offering a plausible training-free\ncompression solution. However, due to the modality divergence between vision\nand language, it is unclear whether these NLP techniques are still effective in\nLVLMs. In this paper, we empirically prove that directly applying these layer\npruning methods to LVLMs is ineffective. Through extensive experiments, we find\nthat non-essential vision-language (VL) tokens and inter-layer feature gaps\npose critical challenges to pruning layers in LVLMs. Based on these insights,\nwe propose a novel framework Short-LVLM (SVL) that can utilize important VL\ntokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only\nachieves a superior trade-off between performance and efficiency but also\nexhibits several potential advantages, i.e., training-free, model-agnostic, and\nhighly compatible. The code for this work is publicly available at\nhttps://github.com/ASGO-MM/Short-LVLM.", "comment": "Accepted By ACM MM 25", "pdf_url": "http://arxiv.org/pdf/2507.23362v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2502.15544", "title": "Learning-based model predictive control for passenger-oriented train rescheduling with flexible train composition", "authors": ["Xiaoyu Liu", "Caio Fabio Oliveira da Silva", "Azita Dabiri", "Yihui Wang", "Bart De Schutter"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      14 pages, 14 figures, submitted to journal", "url": "http://arxiv.org/abs/2502.15544v2", "summary": "This paper focuses on passenger-oriented real-time train rescheduling,\nconsidering flexible train composition and rolling stock circulation, by\nintegrating learning-based and optimization-based approaches. A learning-based\nmodel predictive control (MPC) approach is developed for real-time train\nrescheduling with flexible train composition and rolling stock circulation to\naddress time-varying passenger demands. In the proposed approach, the values of\nthe integer variables are obtained by pre-trained long short-term memory (LSTM)\nnetworks, while the continuous variables are determined through nonlinear\nconstrained optimization. The learning-based MPC approach enables us to jointly\nconsider efficiency and constraint satisfaction by combining learning-based and\noptimization-based approaches. In order to reduce the number of integer\nvariables, four presolve techniques are developed to prune a subset of integer\ndecision variables. Numerical simulations based on real-life data from the\nBeijing urban rail transit system are conducted to illustrate the effectiveness\nof the developed learning-based MPC approach.", "comment": "14 pages, 14 figures, submitted to journal", "pdf_url": "http://arxiv.org/pdf/2502.15544v2", "cate": "eess.SY", "date": "2025-02-21", "updated": "2025-07-31"}
{"id": "2507.23746", "title": "Real-Time Transmission of Uncompressed High-Definition Video Via A VCSEL-Based Optical Wireless Link With Ultra-Low Latency", "authors": ["Hossein Kazemi", "Isaac N. O. Osahon", "Tiankuo Jiao", "David Butler", "Nikolay Ledentsov Jr.", "Ilya Titkov", "Nikolay Ledentsov", "Harald Haas"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2507.23746v1", "summary": "Real-time transmission of high-resolution video signals in an uncompressed\nand unencrypted format requires an ultra-reliable and low-latency\ncommunications (URLLC) medium with high bandwidth to maintain the quality of\nexperience (QoE) for users. We put forward the design and experimental\ndemonstration of a high-performance laser-based optical wireless communication\n(OWC) system that enables high-definition (HD) video transmission with\nsubmillisecond latencies. The serial digital interface (SDI) output of a camera\nis used to transmit the live video stream over an optical wireless link by\ndirectly modulating the SDI signal on the intensity of a 940 nm vertical cavity\nsurface emitting laser (VCSEL). The proposed SDI over light fidelity (LiFi)\nsystem corroborates error-free transmission of full HD (FHD) and 4K\nultra-high-definition (UHD) resolutions at data rates of 2.97 Gb/s and 5.94\nGb/s, respectively, with a measured end-to-end latency of under 35 ns. Since\nSDI standards support various video formats and VCSELs are high-bandwidth and\nlow-power devices, this presents a scalable and inexpensive solution for\nwireless connectivity between professional broadcast equipment using\noff-the-shelf SDI components.", "comment": "8 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.23746v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23219", "title": "Learning Arbitrary-Scale RAW Image Downscaling with Wavelet-based Recurrent Reconstruction", "authors": ["Yang Ren", "Hai Jiang", "Wei Li", "Menglong Yang", "Heng Zhang", "Zehua Sheng", "Qingsheng Ye", "Shuaicheng Liu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.23219v1", "summary": "Image downscaling is critical for efficient storage and transmission of\nhigh-resolution (HR) images. Existing learning-based methods focus on\nperforming downscaling within the sRGB domain, which typically suffers from\nblurred details and unexpected artifacts. RAW images, with their unprocessed\nphotonic information, offer greater flexibility but lack specialized\ndownscaling frameworks. In this paper, we propose a wavelet-based recurrent\nreconstruction framework that leverages the information lossless attribute of\nwavelet transformation to fulfill the arbitrary-scale RAW image downscaling in\na coarse-to-fine manner, in which the Low-Frequency Arbitrary-Scale Downscaling\nModule (LASDM) and the High-Frequency Prediction Module (HFPM) are proposed to\npreserve structural and textural integrity of the reconstructed low-resolution\n(LR) RAW images, alongside an energy-maximization loss to align high-frequency\nenergy between HR and LR domain. Furthermore, we introduce the Realistic\nNon-Integer RAW Downscaling (Real-NIRD) dataset, featuring a non-integer\ndownscaling factor of 1.3$\\times$, and incorporate it with publicly available\ndatasets with integer factors (2$\\times$, 3$\\times$, 4$\\times$) for\ncomprehensive benchmarking arbitrary-scale image downscaling purposes.\nExtensive experiments demonstrate that our method outperforms existing\nstate-of-the-art competitors both quantitatively and visually. The code and\ndataset will be released at https://github.com/RenYangSCU/ASRD.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23219v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22964", "title": "Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR", "authors": ["Sotheara Leang", "Éric Castelli", "Dominique Vaufreydaz", "Sethserey Sam"], "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22964v1", "summary": "The dynamic characteristics of speech signal provides temporal information\nand play an important role in enhancing Automatic Speech Recognition (ASR). In\nthis work, we characterized the acoustic transitions in a ratio plane of\nSpectral Subband Centroid Frequencies (SSCFs) using polar parameters to capture\nthe dynamic characteristics of the speech and minimize spectral variation.\nThese dynamic parameters were combined with Mel-Frequency Cepstral Coefficients\n(MFCCs) in Vietnamese ASR to capture more detailed spectral information. The\nSSCF0 was used as a pseudo-feature for the fundamental frequency (F0) to\ndescribe the tonal information robustly. The findings showed that the proposed\nparameters significantly reduce word error rates and exhibit greater gender\nindependence than the baseline MFCCs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22964v1", "cate": "eess.AS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23402", "title": "AGA: An adaptive group alignment framework for structured medical cross-modal representation learning", "authors": ["Wei Li", "Xun Gong", "Jiao Li", "Xiaobin Sun"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23402v1", "summary": "Learning medical visual representations from paired images and reports is a\npromising direction in representation learning. However, current\nvision-language pretraining methods in the medical domain often simplify\nclinical reports into single entities or fragmented tokens, ignoring their\ninherent structure. In addition, contrastive learning frameworks typically\ndepend on large quantities of hard negative samples, which is impractical for\nsmall-scale medical datasets. To tackle these challenges, we propose Adaptive\nGrouped Alignment (AGA), a new framework that captures structured semantics\nfrom paired medical images and reports. AGA introduces a bidirectional grouping\nmechanism based on a sparse similarity matrix. For each image-report pair, we\ncompute fine-grained similarities between text tokens and image patches. Each\ntoken selects its top-matching patches to form a visual group, and each patch\nselects its most related tokens to form a language group. To enable adaptive\ngrouping, we design two threshold gating modules, called Language Grouped\nThreshold Gate and Vision Grouped Threshold Gate, which learn grouping\nthresholds dynamically. Group representations are computed as weighted averages\nbased on similarity scores. To align each token with its group representation,\nwe introduce an Instance Aware Group Alignment loss that operates within each\nimage-text pair, removing the need for external negatives. Finally, a\nBidirectional Cross-modal Grouped Alignment module is applied to enhance\nfine-grained alignment between visual and linguistic group representations.\nExtensive experiments on public and private datasets show that our method\nachieves strong performance on image-text retrieval and classification tasks\nunder both fine-tuning and zero-shot settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23402v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.15857", "title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "authors": ["Mihir Prabhudesai", "Mengning Wu", "Amir Zadeh", "Katerina Fragkiadaki", "Deepak Pathak"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Project Webpage: this https URL", "url": "http://arxiv.org/abs/2507.15857v4", "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "comment": "Project Webpage: https://diffusion-scaling.github.io", "pdf_url": "http://arxiv.org/pdf/2507.15857v4", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-31"}
{"id": "2507.23676", "title": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data", "authors": ["Rabeya Tus Sadia", "Qiang Cheng"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23676v1", "summary": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23676v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23371", "title": "VMatcher: State-Space Semi-Dense Local Feature Matching", "authors": ["Ali Youssef"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23371v1", "summary": "This paper introduces VMatcher, a hybrid Mamba-Transformer network for\nsemi-dense feature matching between image pairs. Learning-based feature\nmatching methods, whether detector-based or detector-free, achieve\nstate-of-the-art performance but depend heavily on the Transformer's attention\nmechanism, which, while effective, incurs high computational costs due to its\nquadratic complexity. In contrast, Mamba introduces a Selective State-Space\nModel (SSM) that achieves comparable or superior performance with linear\ncomplexity, offering significant efficiency gains. VMatcher leverages a hybrid\napproach, integrating Mamba's highly efficient long-sequence processing with\nthe Transformer's attention mechanism. Multiple VMatcher configurations are\nproposed, including hierarchical architectures, demonstrating their\neffectiveness in setting new benchmarks efficiently while ensuring robustness\nand practicality for real-time applications where rapid inference is crucial.\nSource Code is available at: https://github.com/ayoussf/VMatcher", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23371v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2506.14662", "title": "PGLib-CO2: A Power Grid Library for Computing and Optimizing Carbon Emissions", "authors": ["Young-ho Cho", "Min-Seung Ko", "Hao Zhu"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.14662v3", "summary": "A sustainable electricity infrastructure requires the explicit integration of\ncarbon emissions into power system modeling and optimization paradigms.\nHowever, existing open-source datasets for power system R&D lack\ngenerator-level carbon emission profiling, limiting the ability to benchmark\nand compare various carbon-aware grid operational strategies. To address this\ngap, this work introduces PGLib-CO2, an open-source extension to the widely\nadopted PGLib-OPF test case library. PGLib-CO2 enriches standard network cases\nwith CO2 and CO2-equivalent emission intensity factors by expanding the\nfuel-type categorization used by PGLib-OPF, attaining a realistic\ngenerator-level carbon profiling. It is also packaged for both Python's\npandapower and Julia's PowerModels.jl, for a seamless, user-friendly\nintegration of emission modeling into grid computation and optimization tasks.\nThe dataset produced by PGLib-CO2 can support grid-based carbon accounting,\nemission metric evaluation, and integration into AC optimal power flow (OPF)\nand optimal load shifting (OLS) formulations. We demonstrate PGLib-CO2's\nutility through case studies that quantify cost-emission trade-offs and\noptimize a carbon-aware objective function. By standardizing carbon-enhanced\ntest cases, PGLib-CO2 provides an open-source, reproducible foundation for\nbenchmarking carbon-aware computation, facilitating future research in\nsustainable power system operation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.14662v3", "cate": "eess.SY", "date": "2025-06-17", "updated": "2025-07-31"}
{"id": "2507.23224", "title": "EMORe: Motion-Robust 5D MRI Reconstruction via Expectation-Maximization-Guided Binning Correction and Outlier Rejection", "authors": ["Syed M. Arshad", "Lee C. Potter", "Yingmin Liu", "Christopher Crabtree", "Matthew S. Tong", "Rizwan Ahmad"], "categories": ["eess.IV", "eess.SP"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23224v1", "summary": "We propose EMORe, an adaptive reconstruction method designed to enhance\nmotion robustness in free-running, free-breathing self-gated 5D cardiac\nmagnetic resonance imaging (MRI). Traditional self-gating-based motion binning\nfor 5D MRI often results in residual motion artifacts due to inaccuracies in\ncardiac and respiratory signal extraction and sporadic bulk motion,\ncompromising clinical utility. EMORe addresses these issues by integrating\nadaptive inter-bin correction and explicit outlier rejection within an\nexpectation-maximization (EM) framework, whereby the E-step and M-step are\nexecuted alternately until convergence. In the E-step, probabilistic (soft) bin\nassignments are refined by correcting misassignment of valid data and rejecting\nmotion-corrupted data to a dedicated outlier bin. In the M-step, the image\nestimate is improved using the refined soft bin assignments. Validation in a\nsimulated 5D MRXCAT phantom demonstrated EMORe's superior performance compared\nto standard compressed sensing reconstruction, showing significant improvements\nin peak signal-to-noise ratio, structural similarity index, edge sharpness, and\nbin assignment accuracy across varying levels of simulated bulk motion. In vivo\nvalidation in 13 volunteers further confirmed EMORe's robustness, significantly\nenhancing blood-myocardium edge sharpness and reducing motion artifacts\ncompared to compressed sensing, particularly in scenarios with controlled\ncoughing-induced motion. Although EMORe incurs a modest increase in\ncomputational complexity, its adaptability and robust handling of bulk motion\nartifacts significantly enhance the clinical applicability and diagnostic\nconfidence of 5D cardiac MRI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23224v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23256", "title": "EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan Africa using MedNeXt V2 with Deep Supervision", "authors": ["Ahmed Jaheen", "Abdelrahman Elsayed", "Damir Kim", "Daniil Tikhonov", "Matheus Scatolin", "Mohor Banerjee", "Qiankun Ji", "Mostafa Salem", "Hu Wang", "Sarim Hashmi", "Mohammad Yaqub"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)", "url": "http://arxiv.org/abs/2507.23256v1", "summary": "Brain cancer affects millions worldwide, and in nearly every clinical\nsetting, doctors rely on magnetic resonance imaging (MRI) to diagnose and\nmonitor gliomas. However, the current standard for tumor quantification through\nmanual segmentation of multi-parametric MRI is time-consuming, requires expert\nradiologists, and is often infeasible in under-resourced healthcare systems.\nThis problem is especially pronounced in low-income regions, where MRI scanners\nare of lower quality and radiology expertise is scarce, leading to incorrect\nsegmentation and quantification. In addition, the number of acquired MRI scans\nin Africa is typically small. To address these challenges, the BraTS-Lighthouse\n2025 Challenge focuses on robust tumor segmentation in sub-Saharan Africa\n(SSA), where resource constraints and image quality degradation introduce\nsignificant shifts. In this study, we present EMedNeXt -- an enhanced brain\ntumor segmentation framework based on MedNeXt V2 with deep supervision and\noptimized post-processing pipelines tailored for SSA. EMedNeXt introduces three\nkey contributions: a larger region of interest, an improved nnU-Net v2-based\narchitectural skeleton, and a robust model ensembling system. Evaluated on the\nhidden validation set, our solution achieved an average LesionWise DSC of 0.897\nwith an average LesionWise NSD of 0.541 and 0.84 at a tolerance of 0.5 mm and\n1.0 mm, respectively.", "comment": "Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23256v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23159", "title": "Full-Duplex-Bench v1.5: Evaluating Overlap Handling for Full-Duplex Speech Models", "authors": ["Guan-Ting Lin", "Shih-Yun Shan Kuan", "Qirui Wang", "Jiachen Lian", "Tingle Li", "Hung-yi Lee"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Work in Progress", "url": "http://arxiv.org/abs/2507.23159v1", "summary": "While full-duplex speech agents promise natural, low-latency human--machine\ninteraction by concurrently processing input and output speech, overlap\nmanagement remains under-evaluated. We introduce Full-Duplex-Bench v1.5, a\nmodular, fully automated benchmark that simulates four overlap scenarios: user\ninterruption, listener backchannel, side conversation, and ambient speech. Our\nframework supports both open-sourced and commercial models, offering a\ncomprehensive, extensible metric suite -- categorical dialogue behaviors, stop\nand response latency, prosodic adaptation, and perceived speech quality -- that\ncan be tailored to application-specific criteria. Benchmarking five\nstate-of-the-art agents reveals two principal strategies: repair-first rapid\nyielding versus continuity-first sustained flow, and highlights\nscenario-dependent performance trends. The open-sourced design enables seamless\nextension with new audio assets, languages, and deployment contexts, empowering\npractitioners to customize and accelerate the evaluation of robust full-duplex\nspeech systems.", "comment": "Work in Progress", "pdf_url": "http://arxiv.org/pdf/2507.23159v1", "cate": "eess.AS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23455", "title": "Machine learning and machine learned prediction in chest X-ray images", "authors": ["Shereiff Garrett", "Abhinav Adhikari", "Sarina Gautam", "DaShawn Marquis Morris", "Chandra Mani Adhikari"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2507.23455v1", "summary": "Machine learning and artificial intelligence are fast-growing fields of\nresearch in which data is used to train algorithms, learn patterns, and make\npredictions. This approach helps to solve seemingly intricate problems with\nsignificant accuracy without explicit programming by recognizing complex\nrelationships in data. Taking an example of 5824 chest X-ray images, we\nimplement two machine learning algorithms, namely, a baseline convolutional\nneural network (CNN) and a DenseNet-121, and present our analysis in making\nmachine-learned predictions in predicting patients with ailments. Both baseline\nCNN and DenseNet-121 perform very well in the binary classification problem\npresented in this work. Gradient-weighted class activation mapping shows that\nDenseNet-121 correctly focuses on essential parts of the input chest X-ray\nimages in its decision-making more than the baseline CNN.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.23455v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23712", "title": "Anomalous Samples for Few-Shot Anomaly Detection", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23712v1", "summary": "Several anomaly detection and classification methods rely on large amounts of\nnon-anomalous or \"normal\" samples under the assump- tion that anomalous data is\ntypically harder to acquire. This hypothesis becomes questionable in Few-Shot\nsettings, where as little as one anno- tated sample can make a significant\ndifference. In this paper, we tackle the question of utilizing anomalous\nsamples in training a model for bi- nary anomaly classification. We propose a\nmethodology that incorporates anomalous samples in a multi-score anomaly\ndetection score leveraging recent Zero-Shot and memory-based techniques. We\ncompare the utility of anomalous samples to that of regular samples and study\nthe benefits and limitations of each. In addition, we propose an\naugmentation-based validation technique to optimize the aggregation of the\ndifferent anomaly scores and demonstrate its effectiveness on popular\nindustrial anomaly detection datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23712v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23372", "title": "UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries", "authors": ["Yijie Zhu", "Lingsen Zhang", "Zitong Yu", "Rui Shao", "Tao Tan", "Liqiang Nie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23372v1", "summary": "Emotional understanding and generation are often treated as separate tasks,\nyet they are inherently complementary and can mutually enhance each other. In\nthis paper, we propose the UniEmo, a unified framework that seamlessly\nintegrates these two tasks. The key challenge lies in the abstract nature of\nemotions, necessitating the extraction of visual representations beneficial for\nboth tasks. To address this, we propose a hierarchical emotional understanding\nchain with learnable expert queries that progressively extracts multi-scale\nemotional features, thereby serving as a foundational step for unification.\nSimultaneously, we fuse these expert queries and emotional representations to\nguide the diffusion model in generating emotion-evoking images. To enhance the\ndiversity and fidelity of the generated emotional images, we further introduce\nthe emotional correlation coefficient and emotional condition loss into the\nfusion process. This step facilitates fusion and alignment for emotional\ngeneration guided by the understanding. In turn, we demonstrate that joint\ntraining allows the generation component to provide implicit feedback to the\nunderstanding part. Furthermore, we propose a novel data filtering algorithm to\nselect high-quality and diverse emotional images generated by the well-trained\nmodel, which explicitly feedback into the understanding part. Together, these\ngeneration-driven dual feedback processes enhance the model's understanding\ncapacity. Extensive experiments show that UniEmo significantly outperforms\nstate-of-the-art methods in both emotional understanding and generation tasks.\nThe code for the proposed method is available at\nhttps://github.com/JiuTian-VL/UniEmo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23372v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.18493", "title": "Global Observer Design for a Class of Linear Observed Systems on Groups", "authors": ["Changwu Liu", "Yuan Shen"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      16 pages, 1 figure", "url": "http://arxiv.org/abs/2507.18493v2", "summary": "Linear observed systems on groups encode the geometry of a variety of\npractical state estimation problems. In this paper, we propose a unified\nobserver framework for a class of linear observed systems by restricting a\nbi-invariant system on a Lie group to its normal subgroup. This structural\nproperty powerfully enables a system immersion of the original system into a\nlinear time-varying system. Leveraging the immersion, an observer is\nconstructed by first designing a Kalman-like observer for the immersed system\nand then reconstructing the group-valued state via optimization. Under a rank\ncondition, global exponential stability (GES) is achieved provided one global\noptimum of the reconstruction optimization is found, reflecting the topological\ndifficulties inherent to the non-Euclidean state space. Semi-global stability\nis guaranteed when input biases are jointly estimated. The theory is applied to\nthe GES observer design for two-frame systems, capable of modeling a family of\nnavigation problems. Two non-trivial examples are provided to illustrate\nimplementation details.", "comment": "16 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.18493v2", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2408.04210", "title": "Adaptive Cohen's Class Time-Frequency Distribution", "authors": ["Manjun Cui", "Zhichao Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.04210v2", "summary": "Inspired by the use of adaptive kernel-based Cohen's class time-frequency\ndistributions (CCTFDs) for cross-term suppression, this paper aims to explore\nnovel adaptive kernel functions for denoising. We integrate Wiener filter\nprinciple and the time-frequency filtering mechanism of CCTFD to design the\nleast-squares adaptive filter method in the Wigner-Ville distribution (WVD)\ndomain, giving birth to the least-squares adaptive filter-based CCTFD whose\nkernel function can be adjusted with the input signal automatically to achieve\nthe minimum mean-square error denoising in the WVD domain. Some examples are\nalso carried out to demonstrate that the proposed adaptive CCTFD outperforms\nsome state-of-the-arts in noise suppression.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.04210v2", "cate": "eess.SP", "date": "2024-08-08", "updated": "2025-07-31"}
{"id": "2507.23359", "title": "Pixel Embedding Method for Tubular Neurite Segmentation", "authors": ["Huayu Fu", "Jiamin Li", "Haozhi Qu", "Xiaolin Hu", "Zengcai Guo"], "categories": ["eess.IV", "cs.CV", "q-bio.NC"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23359v1", "summary": "Automatic segmentation of neuronal topology is critical for handling large\nscale neuroimaging data, as it can greatly accelerate neuron annotation and\nanalysis. However, the intricate morphology of neuronal branches and the\nocclusions among fibers pose significant challenges for deep learning based\nsegmentation. To address these issues, we propose an improved framework: First,\nwe introduce a deep network that outputs pixel level embedding vectors and\ndesign a corresponding loss function, enabling the learned features to\neffectively distinguish different neuronal connections within occluded regions.\nSecond, building on this model, we develop an end to end pipeline that directly\nmaps raw neuronal images to SWC formatted neuron structure trees. Finally,\nrecognizing that existing evaluation metrics fail to fully capture segmentation\naccuracy, we propose a novel topological assessment metric to more\nappropriately quantify the quality of neuron segmentation and reconstruction.\nExperiments on our fMOST imaging dataset demonstrate that, compared to several\nclassical methods, our approach significantly reduces the error rate in\nneuronal topology reconstruction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23359v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23223", "title": "Feature Importance across Domains for Improving Non-Intrusive Speech Intelligibility Prediction in Hearing Aids", "authors": ["Ryandhimas E. Zezario", "Sabato M. Siniscalchi", "Fei Chen", "Hsin-Min Wang", "Yu Tsao"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to Interspeech 2025", "url": "http://arxiv.org/abs/2507.23223v1", "summary": "Given the critical role of non-intrusive speech intelligibility assessment in\nhearing aids (HA), this paper enhances its performance by introducing Feature\nImportance across Domains (FiDo). We estimate feature importance on spectral\nand time-domain acoustic features as well as latent representations of Whisper.\nImportance weights are calculated per frame, and based on these weights,\nfeatures are projected into new spaces, allowing the model to focus on\nimportant areas early. Next, feature concatenation is performed to combine the\nfeatures before the assessment module processes them. Experimental results show\nthat when FiDo is incorporated into the improved multi-branched speech\nintelligibility model MBI-Net+, RMSE can be reduced by 7.62% (from 26.10 to\n24.11). MBI-Net+ with FiDo also achieves a relative RMSE reduction of 3.98%\ncompared to the best system in the 2023 Clarity Prediction Challenge. These\nresults validate FiDo's effectiveness in enhancing neural speech assessment in\nHA.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.23223v1", "cate": "eess.AS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23459", "title": "KLAN: Kuaishou Landing-page Adaptive Navigator", "authors": ["Fan Li", "Chang Meng", "Jiaqi Fu", "Shuchang Liu", "Jiashuo Zhang", "Tianke Zhang", "Xueliang Wang", "Xiaoqiang Feng"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      We propose PLPM, a new task for selecting optimal landing pages upon user entry. Our solution, KLAN, models static and dynamic user interests and is successfully deployed on Kuaishou, improving DAU and user lifetime", "url": "http://arxiv.org/abs/2507.23459v1", "summary": "Modern online platforms configure multiple pages to accommodate diverse user\nneeds. This multi-page architecture inherently establishes a two-stage\ninteraction paradigm between the user and the platform: (1) Stage I: page\nnavigation, navigating users to a specific page and (2) Stage II: in-page\ninteraction, where users engage with customized content within the specific\npage. While the majority of research has been focusing on the sequential\nrecommendation task that improves users' feedback in Stage II, there has been\nlittle investigation on how to achieve better page navigation in Stage I. To\nfill this gap, we formally define the task of Personalized Landing Page\nModeling (PLPM) into the field of recommender systems: Given a user upon app\nentry, the goal of PLPM is to proactively select the most suitable landing page\nfrom a set of candidates (e.g., functional tabs, content channels, or\naggregation pages) to optimize the short-term PDR metric and the long-term user\nengagement and satisfaction metrics, while adhering to industrial constraints.\nAdditionally, we propose KLAN (Kuaishou Landing-page Adaptive Navigator), a\nhierarchical solution framework designed to provide personalized landing pages\nunder the formulation of PLPM. KLAN comprises three key components: (1)\nKLAN-ISP captures inter-day static page preference; (2) KLAN-IIT captures\nintra-day dynamic interest transitions and (3) KLAN-AM adaptively integrates\nboth components for optimal navigation decisions. Extensive online experiments\nconducted on the Kuaishou platform demonstrate the effectiveness of KLAN,\nobtaining +0.205% and +0.192% improvements on in Daily Active Users (DAU) and\nuser Lifetime (LT). Our KLAN is ultimately deployed on the online platform at\nfull traffic, serving hundreds of millions of users. To promote further\nresearch in this important area, we will release our dataset and code upon\npaper acceptance.", "comment": "We propose PLPM, a new task for selecting optimal landing pages upon\n  user entry. Our solution, KLAN, models static and dynamic user interests and\n  is successfully deployed on Kuaishou, improving DAU and user lifetime", "pdf_url": "http://arxiv.org/pdf/2507.23459v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23771", "title": "Consensus-Driven Active Model Selection", "authors": ["Justin Kay", "Grant Van Horn", "Subhransu Maji", "Daniel Sheldon", "Sara Beery"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Highlight. 16 pages, 8 figures", "url": "http://arxiv.org/abs/2507.23771v1", "summary": "The widespread availability of off-the-shelf machine learning models poses a\nchallenge: which model, of the many available candidates, should be chosen for\na given data analysis task? This question of model selection is traditionally\nanswered by collecting and annotating a validation dataset -- a costly and\ntime-intensive process. We propose a method for active model selection, using\npredictions from candidate models to prioritize the labeling of test data\npoints that efficiently differentiate the best candidate. Our method, CODA,\nperforms consensus-driven active model selection by modeling relationships\nbetween classifiers, categories, and data points within a probabilistic\nframework. The framework uses the consensus and disagreement between models in\nthe candidate pool to guide the label acquisition process, and Bayesian\ninference to update beliefs about which model is best as more information is\ncollected. We validate our approach by curating a collection of 26 benchmark\ntasks capturing a range of model selection scenarios. CODA outperforms existing\nmethods for active model selection significantly, reducing the annotation\neffort required to discover the best model by upwards of 70% compared to the\nprevious state-of-the-art. Code and data are available at\nhttps://github.com/justinkay/coda.", "comment": "ICCV 2025 Highlight. 16 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.23771v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23373", "title": "Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation", "authors": ["Haoran Chen", "Zexiao Wang", "Haidong Cao", "Zuxuan Wu", "Yu-Gang Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23373v1", "summary": "Large Vision-Language Models like CLIP have become a powerful foundation for\nUnsupervised Domain Adaptation due to their strong zero-shot generalization.\nState-of-the-art methods typically leverage CLIP to generate pseudo-labels for\nthe target domain, then fine-tune the model to learn domain-invariant features.\nHowever, these methods attempt to align source and target domains using all\npseudo-labeled data simultaneously. This one-shot alignment struggles with\nnoisy, hard-to-classify samples, leading to error propagation and suboptimal\nfeature learning. The problem is even more amplified in the multi-source\nscenario, where diverse domain gaps and varying noise levels across multiple\nsource domains further destabilize the alignment process. To address this\nissue, in this work, we propose a progressive alignment strategy for adapting\nCLIP to unlabeled downstream task. Our method begins by training the model on a\nhigh-confidence subset of target samples, allowing it to first learn a\nwell-aligned representation from the most reliable data. As training\nprogresses, it gradually incorporates more challenging samples, guiding the\nmodel to refine its understanding without being overwhelmed by initial label\nnoise. This progressive approach effectively mitigates confirmation bias and\npromotes a more robust convergence, allowing for the learning of genuinely\ndomain-invariant features. We name our approach MP^2A and test it on three\npopular UDA benchmarks, namely ImageCLEF, Office-Home, and the most challenging\nDomainNet. Experiments showcase that MP^2A achieves state-of-the-art\nperformance when compared with most recent CLIP-based MS-UDA approaches,\ndemonstrating the effectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23373v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2301.04943", "title": "Robust Nonlinear Optimal Control via System Level Synthesis", "authors": ["Antoine P. Leeman", "Johannes Köhler", "Andrea Zanelli", "Samir Bennani", "Melanie N. Zeilinger"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Published in IEEE Transactions on Automatic Control (TAC). Code: this https URL", "url": "http://arxiv.org/abs/2301.04943v3", "summary": "This paper addresses the problem of finite horizon constrained robust optimal\ncontrol for nonlinear systems subject to norm-bounded disturbances. To this\nend, the underlying uncertain nonlinear system is decomposed based on a\nfirst-order Taylor series expansion into a nominal system and an error\n(deviation) described as an uncertain linear time-varying system. This\ndecomposition allows us to leverage system level synthesis to jointly optimize\nan affine error feedback, a nominal nonlinear trajectory, and, most\nimportantly, a dynamic linearization error over-bound used to ensure robust\nconstraint satisfaction for the nonlinear system. The proposed approach thereby\nresults in less conservative planning compared with state-of-the-art\ntechniques. We demonstrate the benefits of the proposed approach to control the\nrotational motion of a rigid body subject to state and input constraints.", "comment": "Published in IEEE Transactions on Automatic Control (TAC). Code:\n  https://github.com/antoineleeman/nonlinear-system-level-synthesis", "pdf_url": "http://arxiv.org/pdf/2301.04943v3", "cate": "math.OC", "date": "2023-01-12", "updated": "2025-07-31"}
{"id": "2410.16593", "title": "Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs", "authors": ["Haolin Li", "Haoyu Wang", "Luana Ruiz"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.16593v4", "summary": "Graph Neural Networks (GNNs) excel in many graph machine learning tasks but\nface challenges when scaling to large networks. GNN transferability allows\ntraining on smaller graphs and applying the model to larger ones, but existing\nmethods often rely on random subsampling, leading to disconnected subgraphs and\nreduced model expressivity. We propose a novel graph sampling algorithm that\nleverages feature homophily to preserve graph structure. By minimizing the\ntrace of the data correlation matrix, our method better preserves the graph\nLaplacian trace -- a proxy for the graph connectivity -- than random sampling,\nwhile achieving lower complexity than spectral methods. Experiments on citation\nnetworks show improved performance in preserving Laplacian trace and GNN\ntransferability compared to random sampling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.16593v4", "cate": "eess.SP", "date": "2024-10-22", "updated": "2025-07-30"}
{"id": "2507.23398", "title": "Smart Video Capsule Endoscopy: Raw Image-Based Localization for Enhanced GI Tract Investigation", "authors": ["Oliver Bause", "Julia Werner", "Paul Palomero Bernardo", "Oliver Bringmann"], "categories": ["eess.IV", "cs.AR", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at the 32nd International Conference on Neural Information Processing - ICONIP 2025", "url": "http://arxiv.org/abs/2507.23398v1", "summary": "For many real-world applications involving low-power sensor edge devices deep\nneural networks used for image classification might not be suitable. This is\ndue to their typically large model size and require- ment of operations often\nexceeding the capabilities of such resource lim- ited devices. Furthermore,\ncamera sensors usually capture images with a Bayer color filter applied, which\nare subsequently converted to RGB images that are commonly used for neural\nnetwork training. However, on resource-constrained devices, such conversions\ndemands their share of energy and optimally should be skipped if possible. This\nwork ad- dresses the need for hardware-suitable AI targeting sensor edge\ndevices by means of the Video Capsule Endoscopy, an important medical proce-\ndure for the investigation of the small intestine, which is strongly limited by\nits battery lifetime. Accurate organ classification is performed with a final\naccuracy of 93.06% evaluated directly on Bayer images involv- ing a CNN with\nonly 63,000 parameters and time-series analysis in the form of Viterbi\ndecoding. Finally, the process of capturing images with a camera and raw image\nprocessing is demonstrated with a customized PULPissimo System-on-Chip with a\nRISC-V core and an ultra-low power hardware accelerator providing an\nenergy-efficient AI-based image clas- sification approach requiring just 5.31\n{\\mu}J per image. As a result, it is possible to save an average of 89.9% of\nenergy before entering the small intestine compared to classic video capsules.", "comment": "Accepted at the 32nd International Conference on Neural Information\n  Processing - ICONIP 2025", "pdf_url": "http://arxiv.org/pdf/2507.23398v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23266", "title": "CUHK-EE Systems for the vTAD Challenge at NCMMSC 2025", "authors": ["Aemon Yat Fei Chiu", "Jingyu Li", "Yusheng Tian", "Guangyan Zhang", "Tan Lee"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.23266v1", "summary": "This paper presents the Voice Timbre Attribute Detection (vTAD) systems\ndeveloped by the Digital Signal Processing & Speech Technology Laboratory\n(DSP&STL) of the Department of Electronic Engineering (EE) at The Chinese\nUniversity of Hong Kong (CUHK) for the 20th National Conference on\nHuman-Computer Speech Communication (NCMMSC 2025) vTAD Challenge. The proposed\nsystems leverage WavLM-Large embeddings with attentive statistical pooling to\nextract robust speaker representations, followed by two variants of Diff-Net,\ni.e., Feed-Forward Neural Network (FFN) and Squeeze-and-Excitation-enhanced\nResidual FFN (SE-ResFFN), to compare timbre attribute intensities between\nutterance pairs. Experimental results demonstrate that the WavLM-Large+FFN\nsystem generalises better to unseen speakers, achieving 77.96% accuracy and\n21.79% EER, while the WavLM-Large+SE-ResFFN model excels in the 'Seen' setting\nwith 94.42% accuracy and 5.49% EER. These findings highlight a trade-off\nbetween model complexity and generalisation, and underscore the importance of\narchitectural choices in fine-grained speaker modelling. Our analysis also\nreveals the impact of speaker identity, annotation subjectivity, and data\nimbalance on system performance, pointing to future directions for improving\nrobustness and fairness in timbre attribute detection.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.23266v1", "cate": "eess.AS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23461", "title": "Mitigating Resolution-Drift in Federated Learning: Case of Keypoint Detection", "authors": ["Taeheon Lim", "Joohyung Lee", "Kyungjae Lee", "Jungchan Cho"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23461v1", "summary": "The Federated Learning (FL) approach enables effective learning across\ndistributed systems, while preserving user data privacy. To date, research has\nprimarily focused on addressing statistical heterogeneity and communication\nefficiency, through which FL has achieved success in classification tasks.\nHowever, its application to non-classification tasks, such as human pose\nestimation, remains underexplored. This paper identifies and investigates a\ncritical issue termed ``resolution-drift,'' where performance degrades\nsignificantly due to resolution variability across clients. Unlike class-level\nheterogeneity, resolution drift highlights the importance of resolution as\nanother axis of not independent or identically distributed (non-IID) data. To\naddress this issue, we present resolution-adaptive federated learning (RAF), a\nmethod that leverages heatmap-based knowledge distillation. Through\nmulti-resolution knowledge distillation between higher-resolution outputs\n(teachers) and lower-resolution outputs (students), our approach enhances\nresolution robustness without overfitting. Extensive experiments and\ntheoretical analysis demonstrate that RAF not only effectively mitigates\nresolution drift and achieves significant performance improvements, but also\ncan be integrated seamlessly into existing FL frameworks. Furthermore, although\nthis paper focuses on human pose estimation, our t-SNE analysis reveals\ndistinct characteristics between classification and high-resolution\nrepresentation tasks, supporting the generalizability of RAF to other tasks\nthat rely on preserving spatial detail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23461v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22918", "title": "Semantic Convergence: Investigating Shared Representations Across Scaled LLMs", "authors": ["Daniel Son", "Sanjana Rathore", "Andrew Rufail", "Adrian Simon", "Daniel Zhang", "Soham Dave", "Cole Blondin", "Kevin Zhu", "Sean O'Brien"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Submitted to ACL 2025 Student Research Workshop (poster)", "url": "http://arxiv.org/abs/2507.22918v1", "summary": "We investigate feature universality in Gemma-2 language models (Gemma-2-2B\nand Gemma-2-9B), asking whether models with a four-fold difference in scale\nstill converge on comparable internal concepts. Using the Sparse Autoencoder\n(SAE) dictionary-learning pipeline, we utilize SAEs on each model's\nresidual-stream activations, align the resulting monosemantic features via\nactivation correlation, and compare the matched feature spaces with SVCCA and\nRSA. Middle layers yield the strongest overlap, while early and late layers\nshow far less similarity. Preliminary experiments extend the analysis from\nsingle tokens to multi-token subspaces, showing that semantically similar\nsubspaces interact similarly with language models. These results strengthen the\ncase that large language models carve the world into broadly similar,\ninterpretable features despite size differences, reinforcing universality as a\nfoundation for cross-model interpretability.", "comment": "Submitted to ACL 2025 Student Research Workshop (poster)", "pdf_url": "http://arxiv.org/pdf/2507.22918v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.23374", "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting", "authors": ["Shuangkang Fang", "I-Chao Shen", "Takeo Igarashi", "Yufeng Wang", "ZeSheng Wang", "Yi Yang", "Wenrui Ding", "Shuchang Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV", "url": "http://arxiv.org/abs/2507.23374v1", "summary": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework\nleverages the inherent continuous spatial representation of NeRF to mitigate\nseveral limitations of 3DGS, including sensitivity to Gaussian initialization,\nlimited spatial awareness, and weak inter-Gaussian correlations, thereby\nenhancing its performance. In NeRF-GS, we revisit the design of 3DGS and\nprogressively align its spatial features with NeRF, enabling both\nrepresentations to be optimized within the same scene through shared 3D spatial\ninformation. We further address the formal distinctions between the two\napproaches by optimizing residual vectors for both implicit features and\nGaussian positions to enhance the personalized capabilities of 3DGS.\nExperimental results on benchmark datasets show that NeRF-GS surpasses existing\nmethods and achieves state-of-the-art performance. This outcome confirms that\nNeRF and 3DGS are complementary rather than competing, offering new insights\ninto hybrid approaches that combine 3DGS and NeRF for efficient 3D scene\nrepresentation.", "comment": "Accepted by ICCV", "pdf_url": "http://arxiv.org/pdf/2507.23374v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2409.03146", "title": "Optimal Placement and Coordinated Scheduling of Distributed Space-Based Lasers for Orbital Debris Remediation", "authors": ["David O. Williams Rogers", "Matthew C. Fox", "Paul R. Stysley", "Hang Woon Lee"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      42 pages, Advances in Space Research (accepted), Copyright 2025. This manuscript version is made available under the CC-BY-NC-ND 4.0 license", "url": "http://arxiv.org/abs/2409.03146v3", "summary": "The significant expansion of the orbital debris population poses a serious\nthreat to the safety and sustainability of space operations. This paper\ninvestigates orbital debris remediation through a constellation of\ncollaborative space-based lasers, leveraging the principle of momentum transfer\nonto debris via laser ablation. A novel delta-v vector analysis framework\nquantifies the cumulative effects of multiple concurrent laser-to-debris (L2D)\nengagements by utilizing the vector composition of the imparted delta-v\nvectors. The paper formulates the Concurrent Location-Scheduling Optimization\nProblem (CLSP) to optimize the placement of laser platforms and the scheduling\nof L2D engagements, aiming to maximize debris remediation capacity. Given the\ncomputational intractability of the CLSP, a decomposition strategy is employed,\nyielding two sequential subproblems: (1) determining optimal laser platform\nlocations via the Maximal Covering Location Problem, and (2) scheduling L2D\nengagements using a novel integer linear programming approach to maximize\ndebris remediation capacity. Computational experiments evaluate the efficacy of\nthe proposed framework across diverse mission scenarios, demonstrating critical\nconstellation functions such as collaborative and controlled nudging,\ndeorbiting, and just-in-time collision avoidance. A sensitivity analysis\nfurther explores the impact of varying the number and distribution of laser\nplatforms on debris remediation capacity, offering insights into optimizing the\nperformance of space-based laser constellations.", "comment": "42 pages, Advances in Space Research (accepted), Copyright 2025. This\n  manuscript version is made available under the CC-BY-NC-ND 4.0 license", "pdf_url": "http://arxiv.org/pdf/2409.03146v3", "cate": "math.OC", "date": "2024-09-05", "updated": "2025-07-30"}
{"id": "2502.17482", "title": "MVCNet: Multi-View Contrastive Network for Motor Imagery Classification", "authors": ["Ziwei Wang", "Siyang Li", "Xiaoqing Chen", "Dongrui Wu"], "categories": ["eess.SP", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      12 pages, 9 figures", "url": "http://arxiv.org/abs/2502.17482v4", "summary": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) enable\nneural interaction by decoding brain activity for external communication. Motor\nimagery (MI) decoding has received significant attention due to its intuitive\nmechanism. However, most existing models rely on single-stream architectures\nand overlook the multi-view nature of EEG signals, leading to limited\nperformance and generalization. We propose a multi-view contrastive network\n(MVCNet), a dual-branch architecture that parallelly integrates CNN and\nTransformer blocks to capture both local spatial-temporal features and global\ntemporal dependencies. To enhance the informativeness of training data, MVCNet\nincorporates a unified augmentation pipeline across time, frequency, and\nspatial domains. Two contrastive modules are further introduced: a cross-view\ncontrastive module that enforces consistency of original and augmented views,\nand a cross-model contrastive module that aligns features extracted from both\nbranches. Final representations are fused and jointly optimized by contrastive\nand classification losses. Experiments on five public MI datasets across three\nscenarios demonstrate that MVCNet consistently outperforms nine\nstate-of-the-art MI decoding networks, highlighting its effectiveness and\ngeneralization ability. MVCNet provides a robust solution for MI decoding by\nintegrating multi-view information and dual-branch modeling, contributing to\nthe development of more reliable BCI systems.", "comment": "12 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2502.17482v4", "cate": "eess.SP", "date": "2025-02-18", "updated": "2025-07-31"}
{"id": "2507.23521", "title": "JPEG Processing Neural Operator for Backward-Compatible Coding", "authors": ["Woo Kyoung Han", "Yongjun Lee", "Byeonghun Lee", "Sang Hyun Park", "Sunghoon Im", "Kyong Hwan Jin"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23521v1", "summary": "Despite significant advances in learning-based lossy compression algorithms,\nstandardizing codecs remains a critical challenge. In this paper, we present\nthe JPEG Processing Neural Operator (JPNeO), a next-generation JPEG algorithm\nthat maintains full backward compatibility with the current JPEG format. Our\nJPNeO improves chroma component preservation and enhances reconstruction\nfidelity compared to existing artifact removal methods by incorporating neural\noperators in both the encoding and decoding stages. JPNeO achieves practical\nbenefits in terms of reduced memory usage and parameter count. We further\nvalidate our hypothesis about the existence of a space with high mutual\ninformation through empirical evidence. In summary, the JPNeO functions as a\nhigh-performance out-of-the-box image compression pipeline without changing\nsource coding's protocol. Our source code is available at\nhttps://github.com/WooKyoungHan/JPNeO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23521v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23511", "title": "MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks", "authors": ["Yadong Niu", "Tianzi Wang", "Heinrich Dinkel", "Xingwei Sun", "Jiahao Zhou", "Gang Li", "Jizhong Liu", "Xunying Liu", "Junbo Zhang", "Jian Luan"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      9 main pages, 5 figures, 3 tables, and 14 appendix pages", "url": "http://arxiv.org/abs/2507.23511v1", "summary": "While large audio-language models have advanced open-ended audio\nunderstanding, they still fall short of nuanced human-level comprehension. This\ngap persists largely because current benchmarks, limited by data annotations\nand evaluation metrics, fail to reliably distinguish between generic and highly\ndetailed model outputs. To this end, this work introduces MECAT, a Multi-Expert\nConstructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via\na pipeline that integrates analysis from specialized expert models with\nChain-of-Thought large language model reasoning, MECAT provides\nmulti-perspective, fine-grained captions and open-set question-answering pairs.\nThe benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced\nAudio Text Evaluation). This metric penalizes generic terms and rewards\ndetailed descriptions by combining single-sample semantic similarity with\ncross-sample discriminability. A comprehensive evaluation of state-of-the-art\naudio models is also presented, providing new insights into their current\ncapabilities and limitations. The data and code are available at\nhttps://github.com/xiaomi-research/mecat", "comment": "9 main pages, 5 figures, 3 tables, and 14 appendix pages", "pdf_url": "http://arxiv.org/pdf/2507.23511v1", "cate": "eess.AS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23465", "title": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations", "authors": ["Saeed Almheiri", "Yerulan Kongrat", "Adrian Santosh", "Ruslan Tasmukhanov", "Josemaria Vera", "Muhammad Dehan Al Kautsar", "Fajri Koto"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23465v1", "summary": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23465v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23017", "title": "A Smoothing Newton Method for Rank-one Matrix Recovery", "authors": ["Tyler Maunu", "Gabriel Abreu"], "categories": ["stat.ML", "cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures", "url": "http://arxiv.org/abs/2507.23017v1", "summary": "We consider the phase retrieval problem, which involves recovering a rank-one\npositive semidefinite matrix from rank-one measurements. A recently proposed\nalgorithm based on Bures-Wasserstein gradient descent (BWGD) exhibits\nsuperlinear convergence, but it is unstable, and existing theory can only prove\nlocal linear convergence for higher rank matrix recovery. We resolve this gap\nby revealing that BWGD implements Newton's method with a nonsmooth and\nnonconvex objective. We develop a smoothing framework that regularizes the\nobjective, enabling a stable method with rigorous superlinear convergence\nguarantees. Experiments on synthetic data demonstrate this superior stability\nwhile maintaining fast convergence.", "comment": "12 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.23017v1", "cate": "stat.ML", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23411", "title": "Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories", "authors": ["Lemar Abdi", "Francisco Caetano", "Amaan Valiuddin", "Christiaan Viviers", "Hamdi Joudeh", "Fons van der Sommen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, MICCAI 2025", "url": "http://arxiv.org/abs/2507.23411v1", "summary": "In medical imaging, unsupervised out-of-distribution (OOD) detection offers\nan attractive approach for identifying pathological cases with extremely low\nincidence rates. In contrast to supervised methods, OOD-based approaches\nfunction without labels and are inherently robust to data imbalances. Current\ngenerative approaches often rely on likelihood estimation or reconstruction\nerror, but these methods can be computationally expensive, unreliable, and\nrequire retraining if the inlier data changes. These limitations hinder their\nability to distinguish nominal from anomalous inputs efficiently, consistently,\nand robustly. We propose a reconstruction-free OOD detection method that\nleverages the forward diffusion trajectories of a Stein score-based denoising\ndiffusion model (SBDDM). By capturing trajectory curvature via the estimated\nStein score, our approach enables accurate anomaly scoring with only five\ndiffusion steps. A single SBDDM pre-trained on a large, semantically aligned\nmedical dataset generalizes effectively across multiple Near-OOD and Far-OOD\nbenchmarks, achieving state-of-the-art performance while drastically reducing\ncomputational cost during inference. Compared to existing methods, SBDDM\nachieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and\nFar-OOD detection, making it a practical building block for real-time, reliable\ncomputer-aided diagnosis.", "comment": "Accepted at Uncertainty for Safe Utilization of Machine Learning in\n  Medical Imaging, MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.23411v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.04502", "title": "Physics-informed Gaussian Processes as Linear Model Predictive Controller", "authors": ["Jörn Tebbe", "Andreas Besginow", "Markus Lange-Hegermann"], "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Accepted at L4DC 2025", "url": "http://arxiv.org/abs/2412.04502v2", "summary": "We introduce a novel algorithm for controlling linear time invariant systems\nin a tracking problem. The controller is based on a Gaussian Process (GP) whose\nrealizations satisfy a system of linear ordinary differential equations with\nconstant coefficients. Control inputs for tracking are determined by\nconditioning the prior GP on the setpoints, i.e. control as inference. The\nresulting Model Predictive Control scheme incorporates pointwise soft\nconstraints by introducing virtual setpoints to the posterior Gaussian process.\nWe show theoretically that our controller satisfies open-loop stability for the\noptimal control problem by leveraging general results from Bayesian inference\nand demonstrate this result in a numerical example.", "comment": "Accepted at L4DC 2025", "pdf_url": "http://arxiv.org/pdf/2412.04502v2", "cate": "math.OC", "date": "2024-12-02", "updated": "2025-07-31"}
{"id": "2503.06981", "title": "Graph Chirp Signal and Graph Fractional Vertex-Frequency Energy Distribution", "authors": ["Manjun Cui", "Zhichao Zhang", "Wei Yao"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06981v2", "summary": "Graph signal processing (GSP) has emerged as a powerful framework for\nanalyzing data on irregular domains. In recent years, many classical techniques\nin signal processing (SP) have been successfully extended to GSP. Among them,\nchirp signals play a crucial role in various SP applications. However, graph\nchirp signals have not been formally defined despite their importance. Here, we\ndefine graph chirp signals and establish a comprehensive theoretical framework\nfor their analysis. We propose the graph fractional vertex--frequency energy\ndistribution (GFED), which provides a powerful tool for processing and\nanalyzing graph chirp signals. We introduce the general fractional graph\ndistribution (GFGD), a generalized vertex--frequency distribution, and the\nreduced interference GFED, which can suppress cross-term interference and\nenhance signal clarity. Furthermore, we propose a novel method for detecting\ngraph signals through GFED domain filtering, facilitating robust detection and\nanalysis of graph chirp signals in noisy environments. Moreover, this method\ncan be applied to real-world data for denoising more effective than some\nstate-of-the-arts, further demonstrating its practical significance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06981v2", "cate": "eess.SP", "date": "2025-03-10", "updated": "2025-07-31"}
{"id": "2507.23648", "title": "Towards Field-Ready AI-based Malaria Diagnosis: A Continual Learning Approach", "authors": ["Louise Guillon", "Soheib Biga", "Yendoube E. Kantchire", "Mouhamadou Lamine Sane", "Grégoire Pasquier", "Kossi Yakpa", "Stéphane E. Sossou", "Marc Thellier", "Laurent Bonnardot", "Laurence Lachaud", "Renaud Piarroux", "Ameyo M. Dorkenoo"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      MICCAI 2025 AMAI Workshop, Accepted, Submitted Manuscript Version", "url": "http://arxiv.org/abs/2507.23648v1", "summary": "Malaria remains a major global health challenge, particularly in low-resource\nsettings where access to expert microscopy may be limited. Deep learning-based\ncomputer-aided diagnosis (CAD) systems have been developed and demonstrate\npromising performance on thin blood smear images. However, their clinical\ndeployment may be hindered by limited generalization across sites with varying\nconditions. Yet very few practical solutions have been proposed. In this work,\nwe investigate continual learning (CL) as a strategy to enhance the robustness\nof malaria CAD models to domain shifts. We frame the problem as a\ndomain-incremental learning scenario, where a YOLO-based object detector must\nadapt to new acquisition sites while retaining performance on previously seen\ndomains. We evaluate four CL strategies, two rehearsal-based and two\nregularization-based methods, on real-life conditions thanks to a multi-site\nclinical dataset of thin blood smear images. Our results suggest that CL, and\nrehearsal-based methods in particular, can significantly improve performance.\nThese findings highlight the potential of continual learning to support the\ndevelopment of deployable, field-ready CAD tools for malaria.", "comment": "MICCAI 2025 AMAI Workshop, Accepted, Submitted Manuscript Version", "pdf_url": "http://arxiv.org/pdf/2507.23648v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23590", "title": "Identifying Hearing Difficulty Moments in Conversational Audio", "authors": ["Jack Collins", "Adrian Buzea", "Chris Collier", "Alejandro Ballesta Rosen", "Julian Maclaren", "Richard F. Lyon", "Simon Carlile"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23590v1", "summary": "Individuals regularly experience Hearing Difficulty Moments in everyday\nconversation. Identifying these moments of hearing difficulty has particular\nsignificance in the field of hearing assistive technology where timely\ninterventions are key for realtime hearing assistance. In this paper, we\npropose and compare machine learning solutions for continuously detecting\nutterances that identify these specific moments in conversational audio. We\nshow that audio language models, through their multimodal reasoning\ncapabilities, excel at this task, significantly outperforming a simple ASR\nhotword heuristic and a more conventional fine-tuning approach with Wav2Vec, an\naudio-only input architecture that is state-of-the-art for automatic speech\nrecognition (ASR).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23590v1", "cate": "cs.SD", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23509", "title": "I Am Big, You Are Little; I Am Right, You Are Wrong", "authors": ["David A. Kelly", "Akchunya Chanchal", "Nathan Blake"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, International Conference on Computer Vision, ICCV 2025", "url": "http://arxiv.org/abs/2507.23509v1", "summary": "Machine learning for image classification is an active and rapidly developing\nfield. With the proliferation of classifiers of different sizes and different\narchitectures, the problem of choosing the right model becomes more and more\nimportant.\n  While we can assess a model's classification accuracy statistically, our\nunderstanding of the way these models work is unfortunately limited. In order\nto gain insight into the decision-making process of different vision models, we\npropose using minimal sufficient pixels sets to gauge a model's\n`concentration': the pixels that capture the essence of an image through the\nlens of the model. By comparing position, overlap, and size of sets of pixels,\nwe identify that different architectures have statistically different\nconcentration, in both size and position. In particular, ConvNext and EVA\nmodels differ markedly from the others. We also identify that images which are\nmisclassified are associated with larger pixels sets than correct\nclassifications.", "comment": "10 pages, International Conference on Computer Vision, ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23509v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23155", "title": "On the Complexity of Finding Stationary Points in Nonconvex Simple Bilevel Optimization", "authors": ["Jincheng Cao", "Ruichen Jiang", "Erfan Yazdandoost Hamedani", "Aryan Mokhtari"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23155v1", "summary": "In this paper, we study the problem of solving a simple bilevel optimization\nproblem, where the upper-level objective is minimized over the solution set of\nthe lower-level problem. We focus on the general setting in which both the\nupper- and lower-level objectives are smooth but potentially nonconvex. Due to\nthe absence of additional structural assumptions for the lower-level\nobjective-such as convexity or the Polyak-{\\L}ojasiewicz (PL)\ncondition-guaranteeing global optimality is generally intractable. Instead, we\nintroduce a suitable notion of stationarity for this class of problems and aim\nto design a first-order algorithm that finds such stationary points in\npolynomial time. Intuitively, stationarity in this setting means the\nupper-level objective cannot be substantially improved locally without causing\na larger deterioration in the lower-level objective. To this end, we show that\na simple and implementable variant of the dynamic barrier gradient descent\n(DBGD) framework can effectively solve the considered nonconvex simple bilevel\nproblems up to stationarity. Specifically, to reach an $(\\epsilon_f,\n\\epsilon_g)$-stationary point-where $\\epsilon_f$ and $\\epsilon_g$ denote the\ntarget stationarity accuracies for the upper- and lower-level objectives,\nrespectively-the considered method achieves a complexity of\n$\\mathcal{O}\\left(\\max\\left(\\epsilon_f^{-\\frac{3+p}{1+p}},\n\\epsilon_g^{-\\frac{3+p}{2}}\\right)\\right)$, where $p \\geq 0$ is an arbitrary\nconstant balancing the terms. To the best of our knowledge, this is the first\ncomplexity result for a discrete-time algorithm that guarantees joint\nstationarity for both levels in general nonconvex simple bilevel problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23155v1", "cate": "math.OC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23416", "title": "Honey Adulteration Detection using Hyperspectral Imaging and Machine Learning", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23416v1", "summary": "This paper aims to develop a machine learning-based system for automatically\ndetecting honey adulteration with sugar syrup, based on honey hyperspectral\nimaging data. First, the floral source of a honey sample is classified by a\nbotanical origin identification subsystem. Then, the sugar syrup adulteration\nis identified, and its concentration is quantified by an adulteration detection\nsubsystem. Both subsystems consist of two steps. The first step involves\nextracting relevant features from the honey sample using Linear Discriminant\nAnalysis (LDA). In the second step, we utilize the K-Nearest Neighbors (KNN)\nmodel to classify the honey botanical origin in the first subsystem and\nidentify the adulteration level in the second subsystem. We assess the proposed\nsystem performance on a public honey hyperspectral image dataset. The result\nindicates that the proposed system can detect adulteration in honey with an\noverall cross-validation accuracy of 96.39%, making it an appropriate\nalternative to the current chemical-based detection methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23416v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2502.15276", "title": "Categorical Lyapunov Theory I: Stability of Flows", "authors": ["Aaron D. Ames", "Joe Moeller", "Paulo Tabuada"], "categories": ["math.DS", "cs.SY", "eess.SY", "math.CT", "18M35, 93D05, 93D30, 37B25, 37C75"], "primary_category": "Subjects:       Dynamical Systems (math.DS)", "pdf_link": null, "comments": "Comments:      31 pages", "url": "http://arxiv.org/abs/2502.15276v2", "summary": "Lyapunov's theorem provides a fundamental characterization of the stability\nof dynamical systems. This paper presents a categorical framework for Lyapunov\ntheory, generalizing stability analysis with Lyapunov functions categorically.\nCore to our approach is the set of axioms underlying a setting for stability,\nwhich give the necessary ingredients for ``doing Lyapunov theory'' in a\ncategory of interest. With these minimal assumptions, we define the stability\nof equilibria, formulate Lyapunov morphisms, and demonstrate that the existence\nof Lyapunov morphisms is necessary and sufficient for establishing the\nstability of flows. To illustrate these constructions, we show how classical\nnotions of stability, e.g., for continuous and discrete time dynamical systems,\nare captured by this categorical framework for Lyapunov theory. Finally, to\ndemonstrate the extensibility of our framework, we illustrate how enriched\ncategories, e.g., Lawvere metric spaces, yield settings for stability enabling\none to ``do Lyapunov theory'' in enriched categories.", "comment": "31 pages", "pdf_url": "http://arxiv.org/pdf/2502.15276v2", "cate": "math.DS", "date": "2025-02-21", "updated": "2025-07-30"}
{"id": "2504.16710", "title": "On the Asymptotic MSE-Optimality of Parametric Bayesian Channel Estimation in mmWave Systems", "authors": ["Franz Weißer", "Wolfgang Utschick"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.16710v2", "summary": "The mean square error (MSE)-optimal estimator is known to be the conditional\nmean estimator (CME). This paper introduces a parametric channel estimation\ntechnique based on Bayesian estimation. This technique uses the estimated\nchannel parameters to parameterize the well-known LMMSE channel estimator. We\nfirst derive an asymptotic CME formulation that holds for a wide range of\npriors on the channel parameters. Based on this, we show that parametric\nBayesian channel estimation is MSE-optimal for high signal-to-noise ratio (SNR)\nand/or long coherence intervals, i.e., many noisy observations provided within\none coherence interval. Numerical simulations validate the derived\nformulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.16710v2", "cate": "eess.SP", "date": "2025-04-23", "updated": "2025-07-31"}
{"id": "2507.23763", "title": "Topology Optimization in Medical Image Segmentation with Fast Euler Characteristic", "authors": ["Liu Li", "Qiang Ma", "Cheng Ouyang", "Johannes C. Paetzold", "Daniel Rueckert", "Bernhard Kainz"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23763v1", "summary": "Deep learning-based medical image segmentation techniques have shown\npromising results when evaluated based on conventional metrics such as the Dice\nscore or Intersection-over-Union. However, these fully automatic methods often\nfail to meet clinically acceptable accuracy, especially when topological\nconstraints should be observed, e.g., continuous boundaries or closed surfaces.\nIn medical image segmentation, the correctness of a segmentation in terms of\nthe required topological genus sometimes is even more important than the\npixel-wise accuracy. Existing topology-aware approaches commonly estimate and\nconstrain the topological structure via the concept of persistent homology\n(PH). However, these methods are difficult to implement for high dimensional\ndata due to their polynomial computational complexity. To overcome this\nproblem, we propose a novel and fast approach for topology-aware segmentation\nbased on the Euler Characteristic ($\\chi$). First, we propose a fast\nformulation for $\\chi$ computation in both 2D and 3D. The scalar $\\chi$ error\nbetween the prediction and ground-truth serves as the topological evaluation\nmetric. Then we estimate the spatial topology correctness of any segmentation\nnetwork via a so-called topological violation map, i.e., a detailed map that\nhighlights regions with $\\chi$ errors. Finally, the segmentation results from\nthe arbitrary network are refined based on the topological violation maps by a\ntopology-aware correction network. Our experiments are conducted on both 2D and\n3D datasets and show that our method can significantly improve topological\ncorrectness while preserving pixel-wise segmentation accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23763v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2401.08052", "title": "Multi-Input Multi-Output Target-Speaker Voice Activity Detection For Unified, Flexible, and Robust Audio-Visual Speaker Diarization", "authors": ["Ming Cheng", "Ming Li"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Transactions on Audio, Speech, and Language Processing", "url": "http://arxiv.org/abs/2401.08052v3", "summary": "Audio-visual learning has demonstrated promising results in many classical\nspeech tasks (e.g., speech separation, automatic speech recognition, wake-word\nspotting). We believe that introducing visual modality will also benefit\nspeaker diarization. To date, Target-Speaker Voice Activity Detection (TS-VAD)\nplays an important role in highly accurate speaker diarization. However,\nprevious TS-VAD models take audio features and utilize the speaker's acoustic\nfootprint to distinguish his or her personal speech activities, which is easily\naffected by overlapped speech in multi-speaker scenarios. Although visual\ninformation naturally tolerates overlapped speech, it suffers from spatial\nocclusion, low resolution, etc. The potential modality-missing problem blocks\nTS-VAD towards an audio-visual approach. This paper proposes a novel\nMulti-Input Multi-Output Target-Speaker Voice Activity Detection (MIMO-TSVAD)\nframework for speaker diarization. The proposed method can take audio-visual\ninput and leverage the speaker's acoustic footprint or lip track to flexibly\nconduct audio-based, video-based, and audio-visual speaker diarization in a\nunified sequence-to-sequence framework. Experimental results show that the\nMIMO-TSVAD framework demonstrates state-of-the-art performance on the\nVoxConverse, DIHARD-III, and MISP 2022 datasets under corresponding evaluation\nmetrics, obtaining the Diarization Error Rates (DERs) of 4.18%, 10.10%, and\n8.15%, respectively. In addition, it can perform robustly in heavy lip-missing\nscenarios.", "comment": "Accepted by IEEE Transactions on Audio, Speech, and Language\n  Processing", "pdf_url": "http://arxiv.org/pdf/2401.08052v3", "cate": "eess.AS", "date": "2024-01-16", "updated": "2025-07-31"}
{"id": "2507.23047", "title": "Competitive Bundle Trading", "authors": ["Yossi Azar", "Niv Buchbinder", "Roie Levin", "Or Vardi"], "categories": ["cs.DS", "cs.GT"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23047v1", "summary": "A retailer is purchasing goods in bundles from suppliers and then selling\nthese goods in bundles to customers; her goal is to maximize profit, which is\nthe revenue obtained from selling goods minus the cost of purchasing those\ngoods. In this paper, we study this general trading problem from the retailer's\nperspective, where both suppliers and customers arrive online. The retailer has\ninventory constraints on the number of goods from each type that she can store,\nand she must decide upon arrival of each supplier/customer which goods to\nbuy/sell in order to maximize profit.\n  We design an algorithm with logarithmic competitive ratio compared to an\noptimal offline solution. We achieve this via an exponential-weight-update\ndynamic pricing scheme, and our analysis dual fits the retailer's profit with\nrespect to a linear programming formulation upper bounding the optimal offline\nprofit. We prove (almost) matching lower bounds, and we also extend our result\nto an incentive compatible mechanism. Prior to our work, algorithms for trading\nbundles were known only for the special case of selling an initial inventory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23047v1", "cate": "cs.DS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23543", "title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction", "authors": ["Gopika Sudhakaran", "Hikaru Shindo", "Patrick Schramowski", "Simone Schaub-Meyer", "Kristian Kersting", "Stefan Roth"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in ICCV 2025", "url": "http://arxiv.org/abs/2507.23543v1", "summary": "Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes.", "comment": "Accepted for publication in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23543v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23160", "title": "Extended Factorization Machine Annealing for Rapid Discovery of Transparent Conducting Materials", "authors": ["Daisuke Makino", "Tatsuya Goto", "Yoshinori Suga"], "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "Comments:      12pages, 6figures", "url": "http://arxiv.org/abs/2507.23160v1", "summary": "The development of novel transparent conducting materials (TCMs) is essential\nfor enhancing the performance and reducing the cost of next-generation devices\nsuch as solar cells and displays. In this research, we focus on the\n(Al$_x$Ga$_y$In$_z$)$_2$O$_3$ system and extend the FMA framework, which\ncombines a Factorization Machine (FM) and annealing, to search for optimal\ncompositions and crystal structures with high accuracy and low cost. The\nproposed method introduces (i) the binarization of continuous variables, (ii)\nthe utilization of good solutions using a Hopfield network, (iii) the\nactivation of global search through adaptive random flips, and (iv) fine-tuning\nvia a bit-string local search. Validation using the\n(Al$_x$Ga$_y$In$_z$)$_2$O$_3$ data from the Kaggle \"Nomad2018 Predicting\nTransparent Conductors\" competition demonstrated that our method achieves\nfaster and more accurate searches than Bayesian optimization and genetic\nalgorithms. Furthermore, its application to multi-objective optimization showed\nits capability in designing materials by simultaneously considering both the\nband gap and formation energy. These results suggest that applying our method\nto larger, more complex search problems and diverse material designs that\nreflect realistic experimental conditions is expected to contribute to the\nfurther advancement of materials informatics.", "comment": "12pages, 6figures", "pdf_url": "http://arxiv.org/pdf/2507.23160v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23436", "title": "Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification", "authors": ["Abdellah Zakaria Sellam", "Salah Eddine Bekhouche", "Cosimo Distante", "Abdelmalik Taleb-Ahmed"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23436v1", "summary": "Art style classification remains a formidable challenge in computational\naesthetics due to the scarcity of expertly labeled datasets and the intricate,\noften nonlinear interplay of stylistic elements. While recent dual-teacher\nself-supervised frameworks reduce reliance on labeled data, their linear\nprojection layers and localized focus struggle to model global compositional\ncontext and complex style-feature interactions. We enhance the dual-teacher\nknowledge distillation framework to address these limitations by replacing\nconventional MLP projection and prediction heads with Kolmogorov-Arnold\nNetworks (KANs). Our approach retains complementary guidance from two teacher\nnetworks, one emphasizing localized texture and brushstroke patterns, the other\ncapturing broader stylistic hierarchies while leveraging KANs' spline-based\nactivations to model nonlinear feature correlations with mathematical\nprecision. Experiments on WikiArt and Pandora18k demonstrate that our approach\noutperforms the base dual teacher architecture in Top-1 accuracy. Our findings\nhighlight the importance of KANs in disentangling complex style manifolds,\nleading to better linear probe accuracy than MLP projections.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23436v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.17696", "title": "Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory", "authors": ["Sota Yoshihara", "Ryosuke Yamamoto", "Hiroyuki Kusumoto", "Masanari Shimura"], "categories": ["cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures. Appendix: 17 pages. First three listed authors have equal contributions", "url": "http://arxiv.org/abs/2505.17696v3", "summary": "This paper proposes a novel theoretical framework for guaranteeing and\nevaluating the resilience of long short-term memory (LSTM) networks in control\nsystems. We introduce \"recovery time\" as a new metric of resilience in order to\nquantify the time required for an LSTM to return to its normal state after\nanomalous inputs. By mathematically refining incremental input-to-state\nstability ($\\delta$ISS) theory for LSTM, we derive a practical data-independent\nupper bound on recovery time. This upper bound gives us resilience-aware\ntraining. Experimental validation on simple models demonstrates the\neffectiveness of our resilience estimation and control methods, enhancing a\nfoundation for rigorous quality assurance in safety-critical AI applications.", "comment": "9 pages, 6 figures. Appendix: 17 pages. First three listed authors\n  have equal contributions", "pdf_url": "http://arxiv.org/pdf/2505.17696v3", "cate": "cs.AI", "date": "2025-05-23", "updated": "2025-07-31"}
{"id": "2505.05003", "title": "Experimental Study on Reference-Path-Aided System Calibration for mmWave Bistatic ISAC Systems", "authors": ["Chenhao Luo", "Chongrui Wang", "Aimin Tang", "Fei Gao", "Chaojun Xu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 8 figures. Accepted by IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2505.05003v2", "summary": "Integrated sensing and communications (ISAC) has been regarded as a key\nenabling technology for next-generation wireless networks. Compared to\nmonostatic ISAC, bistatic ISAC can eliminate the critical challenge of\nself-interference cancellation and is well compatible with the existing network\ninfrastructures. However, the synchronization between the transmitter and the\nsensing receiver becomes a crucial problem. The extracted channel state\ninformation (CSI) for sensing under communication synchronization contains\ndifferent types of system errors, such as the sampling time offset (STO),\ncarrier frequency offset (CFO), and random phase shift, which can severely\ndegrade sensing performance or even render sensing infeasible. To address this\nproblem, a reference-path-aided system calibration scheme is designed for\nmmWave bistatic ISAC systems, where the line-of-sight (LoS) path can be\nblocked. By exploiting the delay-angle sparsity feature in mmWave ISAC systems,\nthe reference path, which can be either a LoS or a non-LoS (NLoS) path, is\nfirst identified. By leveraging the fact that all the paths suffer the same\nsystem errors, the channel parameter extracted from the reference path is\nutilized to compensate for the system errors in all other paths. A mmWave ISAC\nsystem is developed to validate our design. Experimental results demonstrate\nthat the proposed scheme can support precise estimation of Doppler shift and\ndelay, maintaining time-synchronization errors within 1 nanosecond.", "comment": "6 pages, 8 figures. Accepted by IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2505.05003v2", "cate": "eess.SP", "date": "2025-05-08", "updated": "2025-07-31"}
{"id": "2407.07720", "title": "Exploiting Scale-Variant Attention for Segmenting Small Medical Objects", "authors": ["Wei Dai", "Rui Liu", "Zixuan Wu", "Tianyi Wu", "Min Wang", "Junxian Zhou", "Yixuan Yuan", "Jun Liu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      14 pages, 9 figures, under review", "url": "http://arxiv.org/abs/2407.07720v5", "summary": "Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. Identifying mild syndrome with small pathological regions serves as\nan ominous warning and is fundamental in the early diagnosis of diseases. While\ndeep learning algorithms, particularly convolutional neural networks (CNNs),\nhave shown promise in segmenting medical objects, analyzing small areas in\nmedical images remains challenging. This difficulty arises due to information\nlosses and compression defects from convolution and pooling operations in CNNs,\nwhich become more pronounced as the network deepens, especially for small\nmedical objects. To address these challenges, we propose a novel scale-variant\nattention-based network (SvANet) for accurately segmenting small-scale objects\nin medical images. The SvANet consists of scale-variant attention, cross-scale\nguidance, Monte Carlo attention, and vision transformer, which incorporates\ncross-scale features and alleviates compression artifacts for enhancing the\ndiscrimination of small medical objects. Quantitative experimental results\ndemonstrate the superior performance of SvANet, achieving 96.12%, 96.11%,\n89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice coefficient for\nsegmenting kidney tumors, skin lesions, hepatic tumors, polyps, surgical\nexcision cells, retinal vasculatures, and sperms, which occupy less than 1% of\nthe image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet, FIVES, and\nSpermHealth datasets, respectively.", "comment": "14 pages, 9 figures, under review", "pdf_url": "http://arxiv.org/pdf/2407.07720v5", "cate": "eess.IV", "date": "2024-07-10", "updated": "2025-07-31"}
{"id": "2507.14534", "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14534v3", "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14534v3", "cate": "eess.AS", "date": "2025-07-19", "updated": "2025-07-30"}
{"id": "2507.23216", "title": "Efficient algorithm for linear diophantine equations in two variables", "authors": ["Mayank Deora", "Pinakpani Pal"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23216v1", "summary": "Solving linear diophantine equations in two variables have applications in\ncomputer science and mathematics. In this paper, we revisit an algorithm for\nsolving linear diophantine equations in two variables, which we refer as DEA-R\nalgorithm. The DEA-R algorithm always incurs equal or less number of recursions\nor recursive calls as compared to extended euclidean algorithm. With the\nobjective of taking advantage of the less number of recursive calls , we\npropose an optimized version of the DEA-R algorithm as DEA-OPTD. In the\nrecursive function calls in DEA-OPTD, we propose a sequence of more efficient\ncomputations. We do a theoretical comparison of the execution times of DEA-OPTD\nalgorithm and DEA-R algorithm to find any possible bound on the value of $c$\nfor DEA-OPTD being better than DEA-R. We implement and compare an iterative\nversion of DEA-OPTD (DEA-OPTDI) with two versions of a widely used algorithm on\nan specific input setting. In this comparison, we find out that our algorithm\noutperforms on the other algorithm against atleast 96% of the inputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23216v1", "cate": "cs.DS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23642", "title": "Efficient Masked Attention Transformer for Few-Shot Classification and Segmentation", "authors": ["Dustin Carrión-Ojeda", "Stefan Roth", "Simone Schaub-Meyer"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for GCPR 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.23642v1", "summary": "Few-shot classification and segmentation (FS-CS) focuses on jointly\nperforming multi-label classification and multi-class segmentation using few\nannotated examples. Although the current state of the art (SOTA) achieves high\naccuracy in both tasks, it struggles with small objects. To overcome this, we\npropose the Efficient Masked Attention Transformer (EMAT), which improves\nclassification and segmentation accuracy, especially for small objects. EMAT\nintroduces three modifications: a novel memory-efficient masked attention\nmechanism, a learnable downscaling strategy, and parameter-efficiency\nenhancements. EMAT outperforms all FS-CS methods on the PASCAL-5$^i$ and\nCOCO-20$^i$ datasets, using at least four times fewer trainable parameters.\nMoreover, as the current FS-CS evaluation setting discards available\nannotations, despite their costly collection, we introduce two novel evaluation\nsettings that consider these annotations to better reflect practical scenarios.", "comment": "Accepted for GCPR 2025. Project page: https://visinf.github.io/emat", "pdf_url": "http://arxiv.org/pdf/2507.23642v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23208", "title": "Are Recommenders Self-Aware? Label-Free Recommendation Performance Estimation via Model Uncertainty", "authors": ["Jiayu Li", "Ziyi Ye", "Guohao Jian", "Zhiqiang Guo", "Weizhi Ma", "Qingyao Ai", "Min Zhang"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23208v1", "summary": "Can a recommendation model be self-aware? This paper investigates the\nrecommender's self-awareness by quantifying its uncertainty, which provides a\nlabel-free estimation of its performance. Such self-assessment can enable more\ninformed understanding and decision-making before the recommender engages with\nany users. To this end, we propose an intuitive and effective method,\nprobability-based List Distribution uncertainty (LiDu). LiDu measures\nuncertainty by determining the probability that a recommender will generate a\ncertain ranking list based on the prediction distributions of individual items.\nWe validate LiDu's ability to represent model self-awareness in two settings:\n(1) with a matrix factorization model on a synthetic dataset, and (2) with\npopular recommendation algorithms on real-world datasets. Experimental results\nshow that LiDu is more correlated with recommendation performance than a series\nof label-free performance estimators. Additionally, LiDu provides valuable\ninsights into the dynamic inner states of models throughout training and\ninference. This work establishes an empirical connection between recommendation\nuncertainty and performance, framing it as a step towards more transparent and\nself-evaluating recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23208v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23447", "title": "Adjustable Spatio-Spectral Hyperspectral Image Compression Network", "authors": ["Martin Hermann Paul Fuchs", "Behnood Rasti", "Begüm Demir"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23447v1", "summary": "With the rapid growth of hyperspectral data archives in remote sensing (RS),\nthe need for efficient storage has become essential, driving significant\nattention toward learning-based hyperspectral image (HSI) compression. However,\na comprehensive investigation of the individual and joint effects of spectral\nand spatial compression on learning-based HSI compression has not been\nthoroughly examined yet. Conducting such an analysis is crucial for\nunderstanding how the exploitation of spectral, spatial, and joint\nspatio-spectral redundancies affects HSI compression. To address this issue, we\npropose Adjustable Spatio-Spectral Hyperspectral Image Compression Network\n(HyCASS), a learning-based model designed for adjustable HSI compression in\nboth spectral and spatial dimensions. HyCASS consists of six main modules: 1)\nspectral encoder; 2) spatial encoder; 3) compression ratio (CR) adapter\nencoder; 4) CR adapter decoder; 5) spatial decoder; and 6) spectral decoder\nmodule. The modules employ convolutional layers and transformer blocks to\ncapture both short-range and long-range redundancies. Experimental results on\ntwo HSI benchmark datasets demonstrate the effectiveness of our proposed\nadjustable model compared to existing learning-based compression models. Based\non our results, we establish a guideline for effectively balancing spectral and\nspatial compression across different CRs, taking into account the spatial\nresolution of the HSIs. Our code and pre-trained model weights are publicly\navailable at https://git.tu-berlin.de/rsim/hycass .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23447v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.07953", "title": "Incremental Collision Laws Based on the Bouc-Wen Model: External Forces and Corner Cases", "authors": ["Mihails Milehins", "Dan B. Marghitu"], "categories": ["physics.class-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Classical Physics (physics.class-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 3 figures, see this https URL ; (v2-v4) various amendments; arXiv admin note: text overlap with arXiv:2410.08147", "url": "http://arxiv.org/abs/2507.07953v4", "summary": "In the article titled \"The Bouc-Wen Model for Binary Direct Collinear\nCollisions of Convex Viscoplastic Bodies\" and published in the Journal of\nComputational and Nonlinear Dynamics (Volume 20, Issue 6, June 2025), the\nauthors studied mathematical models of binary direct collinear collisions of\nconvex viscoplastic bodies that employed two incremental collision laws based\non the Bouc-Wen differential model of hysteresis. It was shown that the models\npossess favorable analytical properties, and several model parameter\nidentification studies were conducted, demonstrating that the models can\naccurately capture the nature of a variety of collision phenomena. In this\narticle, the aforementioned models are augmented by modeling the effects of\nexternal forces as time-dependent inputs that belong to a certain function\nspace. Furthermore, the range of the parameters under which the models possess\nfavorable analytical properties is extended to several corner cases that were\nnot considered in the prior publication. Finally, the previously conducted\nmodel parameter identification studies are extended, and an additional model\nparameter identification study is provided in an attempt to validate the\nability of the augmented models to represent the effects of external forces.", "comment": "12 pages, 3 figures, see https://gitlab.com/user9716869/EBWCM ;\n  (v2-v4) various amendments; arXiv admin note: text overlap with\n  arXiv:2410.08147", "pdf_url": "http://arxiv.org/pdf/2507.07953v4", "cate": "physics.class-ph", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2506.00497", "title": "Second-Order Characterization of Micro Doppler Radar Signatures of Drone Swarms", "authors": ["Anders Malthe Westerkam", "Alba Spliid Damkjær", "Rasmus Erik Villadsen", "Magnus Ørum Bastrup Poulsen", "Troels Pedersen"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00497v3", "summary": "We investigate the second-order characteristics of the radar return signal\nfrom a swarm of rotor drones. We consider the case of a swarm of identical\ndrones, with each a number of rotors comprised of a number of rotor blades. By\nconsidering the orientation and speed of each rotor as stochastic variables, we\nderive expressions for the autocorrelation function (ACF) and power spectral\ndensity (PSD). The ACF and PSD are in the form of an infinite series with\ncoefficients that drop to zero at a predictable limit. Thus in practical\napplications, the series may be truncated. As a special case, we show that for\ndeterministic rotor speed, the ACF can be expressed in closed form. We further\ninvestigate how system parameters (Blade length, Rotor speed, number of blades,\nand number of drones) influence the derived expressions for the ACF and PSD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00497v3", "cate": "eess.SP", "date": "2025-05-31", "updated": "2025-07-31"}
{"id": "2503.17564", "title": "ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology", "authors": ["Vishwesh Ramanathan", "Tony Xu", "Pushpak Pati", "Faruk Ahmed", "Maged Goubran", "Anne L. Martel"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17564v2", "summary": "Prediction tasks in digital pathology are challenging due to the massive size\nof whole-slide images (WSIs) and the weak nature of training signals. Advances\nin computing, data availability, and self-supervised learning (SSL) have paved\nthe way for slide-level foundation models (SLFMs) that can improve prediction\ntasks in low-data regimes. However, current methods under-utilize shared\ninformation between tasks and modalities. To overcome this challenge, we\npropose ModalTune, a novel fine-tuning framework which introduces the Modal\nAdapter to integrate new modalities without modifying SLFM weights.\nAdditionally, we use large-language models (LLMs) to encode labels as text,\ncapturing semantic relationships across multiple tasks and cancer types in a\nsingle training recipe. ModalTune achieves state-of-the-art (SOTA) results\nagainst both uni-modal and multi-modal models across four cancer types, jointly\nimproving survival and cancer subtype prediction while remaining competitive in\npan-cancer settings. Additionally, we show ModalTune is generalizable to two\nout-of-distribution (OOD) datasets. To our knowledge, this is the first unified\nfine-tuning framework for multi-modal, multi-task, and pan-cancer modeling in\ndigital pathology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17564v2", "cate": "eess.IV", "date": "2025-03-21", "updated": "2025-07-30"}
{"id": "2505.14874", "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages", "authors": ["Chin-Jou Li", "Eunjung Yeo", "Kwanghee Choi", "Paula Andrea Pérez-Toro", "Masao Someki", "Rohan Kumar Das", "Zhengjun Yue", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "David R. Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      5 pages, 1 figure, Accepted to Interspeech 2025", "url": "http://arxiv.org/abs/2505.14874v4", "summary": "Automatic speech recognition (ASR) for dysarthric speech remains challenging\ndue to data scarcity, particularly in non-English languages. To address this,\nwe fine-tune a voice conversion model on English dysarthric speech (UASpeech)\nto encode both speaker characteristics and prosodic distortions, then apply it\nto convert healthy non-English speech (FLEURS) into non-English dysarthric-like\nspeech. The generated data is then used to fine-tune a multilingual ASR model,\nMassively Multilingual Speech (MMS), for improved dysarthric speech\nrecognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE\n(Tamil) demonstrates that VC with both speaker and prosody conversion\nsignificantly outperforms the off-the-shelf MMS performance and conventional\naugmentation techniques such as speed and tempo perturbation. Objective and\nsubjective analyses of the generated data further confirm that the generated\nspeech simulates dysarthric characteristics.", "comment": "5 pages, 1 figure, Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2505.14874v4", "cate": "cs.CL", "date": "2025-05-20", "updated": "2025-07-31"}
{"id": "2507.23659", "title": "Nyldon Factorization of Thue-Morse Words and Fibonacci Words", "authors": ["Kaisei Kishi", "Kazuki Kai", "Yuto Nakashima", "Shunsuke Inenaga", "Hideo Bannai"], "categories": ["cs.DS", "cs.DM"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      A full version of our conference paper accepted for SPIRE 2025", "url": "http://arxiv.org/abs/2507.23659v1", "summary": "The Nyldon factorization is a string factorization that is a non-decreasing\nproduct of Nyldon words. Nyldon words and Nyldon factorizations are recently\ndefined combinatorial objects inspired by the well-known Lyndon words and\nLyndon factorizations. In this paper, we investigate the Nyldon factorization\nof several words. First, we fully characterize the Nyldon factorizations of the\n(finite) Fibonacci and the (finite) Thue-Morse words. Moreover, we show that\nthere exists a non-decreasing product of Nyldon words that is a factorization\nof the infinite Thue-Morse word.", "comment": "A full version of our conference paper accepted for SPIRE 2025", "pdf_url": "http://arxiv.org/pdf/2507.23659v1", "cate": "cs.DS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23704", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "authors": ["Zhenyang Li", "Xiaoyang Bai", "Tongchen Zhang", "Pengfei Shen", "Weiwei Xu", "Yifan Peng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 8 figures", "url": "http://arxiv.org/abs/2507.23704v1", "summary": "High-fidelity 3D video reconstruction is essential for enabling real-time\nrendering of dynamic scenes with realistic motion in virtual and augmented\nreality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has\nachieved near-photorealistic results in video reconstruction due to the great\nrepresentation capability of deep deformation networks. However, in videos with\ncomplex motion and significant scale variations, deformation networks often\noverfit to irregular Gaussian trajectories, leading to suboptimal visual\nquality. Moreover, the gradient-based densification strategy designed for\nstatic scene reconstruction proves inadequate to address the absence of dynamic\ncontent. In light of these challenges, we propose a flow-empowered velocity\nfield modeling scheme tailored for Gaussian video reconstruction, dubbed\nFlowGaussian-VR. It consists of two core components: a velocity field rendering\n(VFR) pipeline which enables optical flow-based optimization, and a\nflow-assisted adaptive densification (FAD) strategy that adjusts the number and\nsize of Gaussians in dynamic regions. We validate our model's effectiveness on\nmulti-view dynamic reconstruction and novel view synthesis with multiple\nreal-world datasets containing challenging motion scenarios, demonstrating not\nonly notable visual improvements (over 2.5 dB gain in PSNR) and less blurry\nartifacts in dynamic textures, but also regularized and trackable per-Gaussian\ntrajectories.", "comment": "17 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.23704v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23209", "title": "Not Just What, But When: Integrating Irregular Intervals to LLM for Sequential Recommendation", "authors": ["Wei-Wei Du", "Takuma Udagawa", "Kei Tateno"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by RecSys 2025 short paper track", "url": "http://arxiv.org/abs/2507.23209v1", "summary": "Time intervals between purchasing items are a crucial factor in sequential\nrecommendation tasks, whereas existing approaches focus on item sequences and\noften overlook by assuming the intervals between items are static. However,\ndynamic intervals serve as a dimension that describes user profiling on not\nonly the history within a user but also different users with the same item\nhistory. In this work, we propose IntervalLLM, a novel framework that\nintegrates interval information into LLM and incorporates the novel\ninterval-infused attention to jointly consider information of items and\nintervals. Furthermore, unlike prior studies that address the cold-start\nscenario only from the perspectives of users and items, we introduce a new\nviewpoint: the interval perspective to serve as an additional metric for\nevaluating recommendation methods on the warm and cold scenarios. Extensive\nexperiments on 3 benchmarks with both traditional- and LLM-based baselines\ndemonstrate that our IntervalLLM achieves not only 4.4% improvements in average\nbut also the best-performing warm and cold scenarios across all users, items,\nand the proposed interval perspectives. In addition, we observe that the cold\nscenario from the interval perspective experiences the most significant\nperformance drop among all recommendation methods. This finding underscores the\nnecessity of further research on interval-based cold challenges and our\nintegration of interval information in the realm of sequential recommendation\ntasks. Our code is available here:\nhttps://github.com/sony/ds-research-code/tree/master/recsys25-IntervalLLM.", "comment": "Accepted by RecSys 2025 short paper track", "pdf_url": "http://arxiv.org/pdf/2507.23209v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23473", "title": "CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes", "authors": ["Bin Xie", "Congxuan Zhang", "Fagan Wang", "Peng Liu", "Feng Lu", "Zhen Chen", "Weiming Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCVW2025", "url": "http://arxiv.org/abs/2507.23473v1", "summary": "The widespread application of Unmanned Aerial Vehicles (UAVs) has raised\nserious public safety and privacy concerns, making UAV perception crucial for\nanti-UAV tasks. However, existing UAV tracking datasets predominantly feature\nconspicuous objects and lack diversity in scene complexity and attribute\nrepresentation, limiting their applicability to real-world scenarios. To\novercome these limitations, we present the CST Anti-UAV, a new thermal infrared\ndataset specifically designed for Single Object Tracking (SOT) in Complex\nScenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k\nhigh-quality bounding box annotations, highlighting two key properties: a\nsignificant number of tiny-sized UAV targets and the diverse and complex\nscenes. To the best of our knowledge, CST Anti-UAV is the first dataset to\nincorporate complete manual frame-level attribute annotations, enabling precise\nevaluations under varied challenges. To conduct an in-depth performance\nanalysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed\ndataset. Experimental results demonstrate that tracking tiny UAVs in complex\nenvironments remains a challenge, as the state-of-the-art method achieves only\n35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410\ndataset. These findings underscore the limitations of existing benchmarks and\nthe need for further advancements in UAV tracking research. The CST Anti-UAV\nbenchmark is about to be publicly released, which not only fosters the\ndevelopment of more robust SOT methods but also drives innovation in anti-UAV\nsystems.", "comment": "Accepted by ICCVW2025", "pdf_url": "http://arxiv.org/pdf/2507.23473v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2506.05898", "title": "On Level Crossings and Fade Durations in von Mises-Fisher Scattering Channels", "authors": ["Kenan Turbic", "Slawomir Stanczak"], "categories": ["eess.SP", "stat.AP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at WSA 2025 (Track 2)", "url": "http://arxiv.org/abs/2506.05898v2", "summary": "This paper investigates the second-order statistics of multipath fading\nchannels with von Mises-Fisher (vMF) distributed scatters. Simple closed-form\nexpressions for the mean Doppler shift and Doppler spread are derived as the\nkey spectral moments that capture the impact of mobility and scattering\ncharacteristics on level crossings and fade durations. These expressions are\nthen used to analyze the influence of vMF parameters on the Level-Crossing Rate\n(LCR) and Average Fade Duration (AFD). The results show that isotropic\nscattering yields the highest LCR and the lowest AFD, while fading dynamics\nreduce with the decreasing angular spread of scatterers. Moreover, mobile\nantenna motion parallel to the mean scattering direction results in a lower LCR\nthan the perpendicular motion, with the difference between the two cases\nincreasing with the higher concentration of scatterers.", "comment": "Accepted for presentation at WSA 2025 (Track 2)", "pdf_url": "http://arxiv.org/pdf/2506.05898v2", "cate": "eess.SP", "date": "2025-06-06", "updated": "2025-07-31"}
{"id": "2404.17484", "title": "Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention", "authors": ["Zhenghong Li", "Jiaxiang Ren", "Wensheng Cheng", "Yanzuo Liu", "Congwu Du", "Yingtian Pan", "Haibin Ling"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      MICCAI25, 10 pages, 3 figures", "url": "http://arxiv.org/abs/2404.17484v3", "summary": "Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging\ntechnique. The fundamental unit of ODT is the 1D depth-resolved trace named raw\nA-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a\ncross-sectional flow image via Doppler phase-subtraction of raw A-scans along\nB-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required\ncurrently, leading to prolonged scanning time and increased storage demands.\nAddressing this issue, we propose a novel sparse ODT reconstruction framework\nwith an Alternative State Space Attention Network (ASSAN) that effectively\nreduces raw A-scans needed. Inspired by the distinct distributions of\ninformation along A-line and B-line, ASSAN applies 1D State Space Model (SSM)\nto each A-line to learn the intra-A-scan representation, while using 1D gated\nself-attention along B-line to capture the inter-A-scan features. In addition,\nan effective feedforward network based on sequential 1D convolutions along\ndifferent axes is employed to enhance the local feature. In validation\nexperiments on real animal data, ASSAN shows clear effectiveness in the\nreconstruction in comparison with state-of-the-art reconstruction methods.", "comment": "MICCAI25, 10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2404.17484v3", "cate": "cs.CV", "date": "2024-04-26", "updated": "2025-07-30"}
{"id": "2507.07526", "title": "DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel Spectrogram Reconstruction", "authors": ["Cunhang Fan", "Sheng Zhang", "Jingjing Zhang", "Enrui Liu", "Xinhui Li", "Minggang Zhao", "Zhao Lv"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.07526v2", "summary": "Decoding speech from brain signals is a challenging research problem.\nAlthough existing technologies have made progress in reconstructing the mel\nspectrograms of auditory stimuli at the word or letter level, there remain core\nchallenges in the precise reconstruction of minute-level continuous imagined\nspeech: traditional models struggle to balance the efficiency of temporal\ndependency modeling and information retention in long-sequence decoding. To\naddress this issue, this paper proposes the Dynamic Multiscale Fusion Network\n(DMF2Mel), which consists of four core components: the Dynamic Contrastive\nFeature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided\nMulti-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the\nbidirectional state space module (convMamba). Specifically, the DC-FAM\nseparates speech-related \"foreground features\" from noisy \"background features\"\nthrough local convolution and global attention mechanisms, effectively\nsuppressing interference and enhancing the representation of transient signals.\nHAMS-Net, based on the U-Net framework,achieves cross-scale fusion of\nhigh-level semantics and low-level details. The SplineMap attention mechanism\nintegrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine\nglobal context modeling with spline-based local fitting. The convMamba captures\nlong-range temporal dependencies with linear complexity and enhances nonlinear\ndynamic modeling capabilities. Results on the SparrKULee dataset show that\nDMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram\nreconstruction for known subjects (a 48% improvement over the baseline) and\n0.048 for unknown subjects (a 35% improvement over the baseline).Code is\navailable at: https://github.com/fchest/DMF2Mel.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.07526v2", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2507.23105", "title": "The Squishy Grid Problem", "authors": ["Zixi Cai", "Kuowen Chen", "Shengquan Du", "Arnold Filtser", "Seth Pettie", "Daniel Skora"], "categories": ["cs.CG", "cs.DM", "cs.DS", "math.CO", "math.PR"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23105v1", "summary": "In this paper we consider the problem of approximating Euclidean distances by\nthe infinite integer grid graph. Although the topology of the graph is fixed,\nwe have control over the edge-weight assignment $w:E\\to \\mathbb{R}_{\\ge 0}$,\nand hope to have grid distances be asymptotically isometric to Euclidean\ndistances, that is, for all grid points $u,v$, $\\mathrm{dist}_w(u,v) = (1\\pm\no(1))\\|u-v\\|_2$. We give three methods for solving this problem, each\nattractive in its own way.\n  * Our first construction is based on an embedding of the recursive,\nnon-periodic pinwheel tiling of Radin and Conway into the integer grid.\nDistances in the pinwheel graph are asymptotically isometric to Euclidean\ndistances, but no explicit bound on the rate of convergence was known. We prove\nthat the multiplicative distortion of the pinwheel graph is\n$(1+1/\\Theta(\\log^\\xi \\log D))$, where $D$ is the Euclidean distance and\n$\\xi=\\Theta(1)$. The pinwheel tiling approach is conceptually simple, but can\nbe improved quantitatively.\n  * Our second construction is based on a hierarchical arrangement of\n\"highways.\" It is simple, achieving stretch $(1 + 1/\\Theta(D^{1/9}))$, which\nconverges doubly exponentially faster than the pinwheel tiling approach.\n  * The first two methods are deterministic. An even simpler approach is to\nsample the edge weights independently from a common distribution $\\mathscr{D}$.\nWhether there exists a distribution $\\mathscr{D}^*$ that makes grid distances\nEuclidean, asymptotically and in expectation, is major open problem in the\ntheory of first passage percolation. Previous experiments show that when\n$\\mathscr{D}$ is a Fisher distribution, grid distances are within 1\\% of\nEuclidean. We demonstrate experimentally that this level of accuracy can be\nachieved by a simple 2-point distribution that assigns weights 0.41 or 4.75\nwith probability 44\\% and 56\\%, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23105v1", "cate": "cs.CG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23740", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "authors": ["Nasim Shirvani-Mahdavi", "Devin Wingfield", "Amin Ghasemi", "Chengkai Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23740v1", "summary": "Knowledge graphs (KGs) often contain sufficient information to support the\ninference of new facts. Identifying logical rules not only improves the\ncompleteness of a knowledge graph but also enables the detection of potential\nerrors, reveals subtle data patterns, and enhances the overall capacity for\nreasoning and interpretation. However, the complexity of such rules, combined\nwith the unique labeling conventions of each KG, can make them difficult for\nhumans to understand. In this paper, we explore the potential of large language\nmodels to generate natural language explanations for logical rules.\nSpecifically, we extract logical rules using the AMIE 3.5.1 rule discovery\nalgorithm from the benchmark dataset FB15k-237 and two large-scale datasets,\nFB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including\nzero- and few-shot prompting, including variable entity types, and\nchain-of-thought reasoning. We conduct a comprehensive human evaluation of the\ngenerated explanations based on correctness, clarity, and hallucination, and\nalso assess the use of large language models as automatic judges. Our results\ndemonstrate promising performance in terms of explanation correctness and\nclarity, although several challenges remain for future research. All scripts\nand data used in this study are publicly available at\nhttps://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23740v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23220", "title": "Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders", "authors": ["Carolina Zheng", "Nicolas Beltran-Velez", "Sweta Karlekar", "Claudia Shi", "Achille Nazaret", "Asif Mallik", "Amir Feder", "David M. Blei"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23220v1", "summary": "Traditional topic models are effective at uncovering latent themes in large\ntext collections. However, due to their reliance on bag-of-words\nrepresentations, they struggle to capture semantically abstract features. While\nsome neural variants use richer representations, they are similarly constrained\nby expressing topics as word lists, which limits their ability to articulate\ncomplex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic\nmodels that operate on interpretable features learned by sparse autoencoders\n(SAEs). By defining topics over this semantically rich space, MTMs can reveal\ndeeper conceptual themes with expressive feature descriptions. Moreover,\nuniquely among topic models, MTMs enable controllable text generation using\ntopic-based steering vectors. To properly evaluate MTM topics against\nword-list-based approaches, we propose \\textit{topic judge}, an LLM-based\npairwise comparison evaluation framework. Across five datasets, MTMs match or\nexceed traditional and neural baselines on coherence metrics, are consistently\npreferred by topic judge, and enable effective steering of LLM outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23220v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23478", "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding", "authors": ["Ting Huang", "Zeyu Zhang", "Hao Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23478v1", "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23478v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21886", "title": "Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2507.21881 , arXiv:2507.21875", "url": "http://arxiv.org/abs/2507.21886v3", "summary": "Pain is a complex condition affecting a large portion of the population.\nAccurate and consistent evaluation is essential for individuals experiencing\npain, and it supports the development of effective and advanced management\nstrategies. Automatic pain assessment systems provide continuous monitoring and\nsupport clinical decision-making, aiming to reduce distress and prevent\nfunctional decline. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed method introduces a pipeline that leverages respiration as the input\nsignal and incorporates a highly efficient cross-attention transformer\nalongside a multi-windowing strategy. Extensive experiments demonstrate that\nrespiration is a valuable physiological modality for pain assessment. Moreover,\nexperiments revealed that compact and efficient models, when properly\noptimized, can achieve strong performance, often surpassing larger\ncounterparts. The proposed multi-window approach effectively captures both\nshort-term and long-term features, as well as global characteristics, thereby\nenhancing the model's representational capacity.", "comment": "arXiv admin note: text overlap with arXiv:2507.21881,\n  arXiv:2507.21875", "pdf_url": "http://arxiv.org/pdf/2507.21886v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2502.02171", "title": "DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging", "authors": ["Mohamed Youssef", "Jian Peng", "Oliver Bimber"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.02171v3", "summary": "Access to below-canopy volumetric vegetation data is crucial for\nunderstanding ecosystem dynamics. We address the long-standing limitation of\nremote sensing to penetrate deep into dense canopy layers. LiDAR and radar are\ncurrently considered the primary options for measuring 3D vegetation\nstructures, while cameras can only extract the reflectance and depth of top\nlayers. Using conventional, high-resolution aerial images, our approach allows\nsensing deep into self-occluding vegetation volumes, such as forests. It is\nsimilar in spirit to the imaging process of wide-field microscopy, but can\nhandle much larger scales and strong occlusion. We scan focal stacks by\nsynthetic-aperture imaging with drones and reduce out-of-focus signal\ncontributions using pre-trained 3D convolutional neural networks with mean\nsquared error (MSE) as the loss function. The resulting volumetric reflectance\nstacks contain low-frequency representations of the vegetation volume.\nCombining multiple reflectance stacks from various spectral channels provides\ninsights into plant health, growth, and environmental conditions throughout the\nentire vegetation volume. Compared with simulated ground truth, our correction\nleads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of\n220 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of\n0.05 when comparing with the top-vegetation layer that was measured with\nclassical multispectral aerial imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.02171v3", "cate": "cs.CV", "date": "2025-02-04", "updated": "2025-07-31"}
{"id": "2507.23500", "title": "Online Combinatorial Allocation with Interdependent Values", "authors": ["Michal Feldman", "Simon Mauras", "Divyarthi Mohan", "Rebecca Reiffenhäuser"], "categories": ["cs.GT", "cs.DS"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23500v1", "summary": "We study online combinatorial allocation problems in the secretary setting,\nunder interdependent values. In the interdependent model, introduced by Milgrom\nand Weber (1982), each agent possesses a private signal that captures her\ninformation about an item for sale, and the value of every agent depends on the\nsignals held by all agents. Mauras, Mohan, and Reiffenh\\\"auser (2024) were the\nfirst to study interdependent values in online settings, providing\nconstant-approximation guarantees for secretary settings, where agents arrive\nonline along with their signals and values, and the goal is to select the agent\nwith the highest value.\n  In this work, we extend this framework to {\\em combinatorial} secretary\nproblems, where agents have interdependent valuations over {\\em bundles} of\nitems, introducing additional challenges due to both combinatorial structure\nand interdependence. We provide $2e$-competitive algorithms for a broad class\nof valuation functions, including submodular and XOS functions, matching the\napproximation guarantees in the single-choice secretary setting. Furthermore,\nour results cover the same range of valuation classes for which constant-factor\nalgorithms exist in classical (non-interdependent) secretary settings, while\nincurring only an additional factor of $2$ due to interdependence. Finally, we\nextend our study to strategic settings, and provide a $4e$-competitive truthful\nmechanism for online bipartite matching with interdependent valuations, again\nmeeting the frontier of what is known, even without interdependence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23500v1", "cate": "cs.GT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22914", "title": "Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs", "authors": ["Victor Eiti Yamamoto", "Hideaki Takeda"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22914v1", "summary": "Knowledge graphs (KGs) are powerful tools for representing and reasoning over\nstructured information. Their main components include schema, identity, and\ncontext. While schema and identity matching are well-established in ontology\nand entity matching research, context matching remains largely unexplored. This\nis particularly important because real-world KGs often vary significantly in\nsource, size, and information density - factors not typically represented in\nthe datasets on which current entity matching methods are evaluated. As a\nresult, existing approaches may fall short in scenarios where diverse and\ncomplex contexts need to be integrated.\n  To address this gap, we propose a novel KG integration method consisting of\nlabel matching and triple matching. We use string manipulation, fuzzy matching,\nand vector similarity techniques to align entity and predicate labels. Next, we\nidentify mappings between triples that convey comparable information, using\nthese mappings to improve entity-matching accuracy. Our approach demonstrates\ncompetitive performance compared to leading systems in the OAEI competition and\nagainst supervised methods, achieving high accuracy across diverse test cases.\nAdditionally, we introduce a new dataset derived from the benchmark dataset to\nevaluate the triple-matching step more comprehensively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22914v1", "cate": "cs.CL", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2507.23777", "title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding", "authors": ["Dian Chen", "Yansong Qu", "Xinyang Li", "Ming Li", "Shengchuan Zhang"], "categories": ["cs.GR", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23777v1", "summary": "Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23777v1", "cate": "cs.GR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23779", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "authors": ["Miaosen Zhang", "Ziqiang Xu", "Jialiang Zhu", "Qi Dai", "Kai Qiu", "Yifan Yang", "Chong Luo", "Tianyi Chen", "Justin Wagle", "Tim Franklin", "Baining Guo"], "categories": ["cs.CV", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23779v1", "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23779v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23227", "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs", "authors": ["Sophie Kearney", "Shu Yang", "Zixuan Wen", "Bojian Hou", "Duy Duong-Tran", "Tianlong Chen", "Jason Moore", "Marylyn Ritchie", "Li Shen"], "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23227v1", "summary": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex\nneurodegenerative disorder, requires analysis of heterogeneous biomarkers\n(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal\nfluid proteins) typically represented in a tabular format. With flexible\nfew-shot reasoning, multimodal integration, and natural-language-based\ninterpretability, large language models (LLMs) offer unprecedented\nopportunities for prediction with structured biomedical data. We propose a\nnovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts\nTableGPT2, a multimodal tabular-specialized LLM originally developed for\nbusiness intelligence tasks, for AD diagnosis using structured biomarker data\nwith small sample sizes. Our approach constructs few-shot tabular prompts using\nin-context learning examples from structured biomedical data and finetunes\nTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary\nclassification task of AD or cognitively normal (CN). The TAP-GPT framework\nharnesses the powerful tabular understanding ability of TableGPT2 and the\nencoded prior knowledge of LLMs to outperform more advanced general-purpose\nLLMs and a tabular foundation model (TFM) developed for prediction tasks. To\nour knowledge, this is the first application of LLMs to the prediction task\nusing tabular biomarker data, paving the way for future LLM-driven multi-agent\nframeworks in biomedical informatics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23227v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23479", "title": "Seeing More with Less: Video Capsule Endoscopy with Multi-Task Learning", "authors": ["Julia Werner", "Oliver Bause", "Julius Oexle", "Maxime Le Floch", "Franz Brinkmann", "Jochen Hampe", "Oliver Bringmann"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at Applications of Medical AI (AMAI workshop) at MICCAI 2025 (submitted version)", "url": "http://arxiv.org/abs/2507.23479v1", "summary": "Video capsule endoscopy has become increasingly important for investigating\nthe small intestine within the gastrointestinal tract. However, a persistent\nchallenge remains the short battery lifetime of such compact sensor edge\ndevices. Integrating artificial intelligence can help overcome this limitation\nby enabling intelligent real-time decision- making, thereby reducing the energy\nconsumption and prolonging the battery life. However, this remains challenging\ndue to data sparsity and the limited resources of the device restricting the\noverall model size. In this work, we introduce a multi-task neural network that\ncombines the functionalities of precise self-localization within the\ngastrointestinal tract with the ability to detect anomalies in the small\nintestine within a single model. Throughout the development process, we\nconsistently restricted the total number of parameters to ensure the\nfeasibility to deploy such model in a small capsule. We report the first\nmulti-task results using the recently published Galar dataset, integrating\nestablished multi-task methods and Viterbi decoding for subsequent time-series\nanalysis. This outperforms current single-task models and represents a\nsignificant ad- vance in AI-based approaches in this field. Our model achieves\nan accu- racy of 93.63% on the localization task and an accuracy of 87.48% on\nthe anomaly detection task. The approach requires only 1 million parameters\nwhile surpassing the current baselines.", "comment": "Accepted at Applications of Medical AI (AMAI workshop) at MICCAI 2025\n  (submitted version)", "pdf_url": "http://arxiv.org/pdf/2507.23479v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23787", "title": "Amplitude amplification and estimation require inverses", "authors": ["Ewin Tang", "John Wright"], "categories": ["quant-ph", "cs.CC", "cs.DS"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.23787v1", "summary": "We prove that the generic quantum speedups for brute-force search and\ncounting only hold when the process we apply them to can be efficiently\ninverted. The algorithms speeding up these problems, amplitude amplification\nand amplitude estimation, assume the ability to apply a state preparation\nunitary $U$ and its inverse $U^\\dagger$; we give problem instances based on\ntrace estimation where no algorithm which uses only $U$ beats the naive,\nquadratically slower approach. Our proof of this is simple and goes through the\ncompressed oracle method introduced by Zhandry. Since these two subroutines are\nresponsible for the ubiquity of the quadratic \"Grover\" speedup in quantum\nalgorithms, our result explains why such speedups are far harder to come by in\nthe settings of quantum learning, metrology, and sensing. In these settings,\n$U$ models the evolution of an experimental system, so implementing $U^\\dagger$\ncan be much harder -- tantamount to reversing time within the system. Our\nresult suggests a dichotomy: without inverse access, quantum speedups are\nscarce; with it, quantum speedups abound.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.23787v1", "cate": "quant-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22924", "title": "Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers", "authors": ["Brittney Exline", "Melanie Duffin", "Brittany Harbison", "Chrissa da Gomez", "David Joyner"], "categories": ["cs.CL", "I.2.7; K.3.1"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22924v1", "summary": "Graduate-level CS programs in the U.S. increasingly enroll international\nstudents, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.\nstudents. Many of these students take online courses, where peer feedback is\nused to engage students and improve pedagogy in a scalable manner. Since these\ncourses are conducted in English, many students study in a language other than\ntheir first. This paper examines how native versus non-native English speaker\nstatus affects three metrics of peer feedback experience in online U.S.-based\ncomputing courses. Using the Twitter-roBERTa-based model, we analyze the\nsentiment of peer reviews written by and to a random sample of 500 students. We\nthen relate sentiment scores and peer feedback ratings to students' language\nbackground. Results show that native English speakers rate feedback less\nfavorably, while non-native speakers write more positively but receive less\npositive sentiment in return. When controlling for sex and age, significant\ninteractions emerge, suggesting that language background plays a modest but\ncomplex role in shaping peer feedback experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22924v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.23485", "title": "Rational complex Bezier curves", "authors": ["A. Canton", "L. Fernandez-Jambrina", "M. J. Vazquez-Gallo"], "categories": ["math.NA", "cs.GR", "cs.NA", "65D17, 68U07"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures", "url": "http://arxiv.org/abs/2507.23485v1", "summary": "In this paper we develop the formalism of rational complex Bezier curves.\nThis framework is a simple extension of the CAD paradigm, since it describes\narc of curves in terms of control polygons and weights, which are extended to\ncomplex values. One of the major advantages of this extension is that we may\nmake use of two different groups of projective transformations. Besides the\ngroup of projective transformations of the real plane, we have the group of\ncomplex projective transformations. This allows us to apply useful\ntransformations like the geometric inversion to curves in design. In addition\nto this, the use of the complex formulation allows to lower the degree of the\ncurves in some cases. This can be checked using the resultant of two\npolynomials and provides a simple formula for determining whether a rational\ncubic curve is a conic or not. Examples of application of the formalism to\nclassical curves are included.", "comment": "9 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.23485v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23784", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "authors": ["Jessica Bader", "Leander Girrbach", "Stephan Alaniz", "Zeynep Akata"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.23784v1", "summary": "Concept Bottleneck Models (CBMs) and other concept-based interpretable models\nshow great promise for making AI applications more transparent, which is\nessential in fields like medicine. Despite their success, we demonstrate that\nCBMs struggle to reliably identify the correct concepts under distribution\nshifts. To assess the robustness of CBMs to concept variations, we introduce\nSUB: a fine-grained image and concept benchmark containing 38,400 synthetic\nimages based on the CUB dataset. To create SUB, we select a CUB subset of 33\nbird classes and 45 concepts to generate images which substitute a specific\nconcept, such as wing color or belly pattern. We introduce a novel Tied\nDiffusion Guidance (TDG) method to precisely control generated images, where\nnoise sharing for two parallel denoising processes ensures that both the\ncorrect bird class and the correct attribute are generated. This novel\nbenchmark enables rigorous evaluation of CBMs and similar interpretable models,\ncontributing to the development of more robust methods. Our code is available\nat https://github.com/ExplainableML/sub and the dataset at\nhttp://huggingface.co/datasets/Jessica-bader/SUB.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23784v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23248", "title": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis", "authors": ["Shimanto Bhowmik", "Tawsif Tashwar Dipto", "Md Sazzad Islam", "Sheryl Hsu", "Tahsin Reasat"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23248v1", "summary": "Bengali is an underrepresented language in NLP research. However, it remains\na challenge due to its unique linguistic structure and computational\nconstraints. In this work, we systematically investigate the challenges that\nhinder Bengali NLP performance by focusing on the absence of standardized\nevaluation benchmarks. We then evaluated 10 recent open source Large Language\nModels (LLMs) in 8 of the translated datasets and performed a comprehensive\nerror analysis to pinpoint their primary failure modes. Our findings reveal\nconsistent performance gaps for Bengali compared to English, particularly for\nsmaller models and specific model families like Mistral. We also identified\npromising robustness in certain architectures, such as DeepSeek, that maintain\nmore stable performance across languages. Our analysis reveals an inverse\nrelationship between tokenization efficiency and LLM accuracy where models tend\nto perform worse when inputs are excessively tokenized, whereas more efficient\n\\& concise tokenization results in improved performance. These findings\nhighlight critical areas where current models fall short and underscore the\nneed for improved dataset quality and evaluation methodologies tailored to\nmultilingual contexts. This work will catalyze further research on NLP for\nunderrepresented languages, helping to democratize access to advanced language\ntechnologies worldwide. The code and dataset used in this research is publicly\navailable at https://github.com/BengaliAI/bn-llm-benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23248v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23480", "title": "FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction", "authors": ["Donghyun Lee", "Dawoon Jeong", "Jae W. Lee", "Hongil Yoon"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.23480v1", "summary": "Deep neural networks have revolutionized 3D point cloud processing, yet\nefficiently handling large and irregular point clouds remains challenging. To\ntackle this problem, we introduce FastPoint, a novel software-based\nacceleration technique that leverages the predictable distance trend between\nsampled points during farthest point sampling. By predicting the distance\ncurve, we can efficiently identify subsequent sample points without\nexhaustively computing all pairwise distances. Our proposal substantially\naccelerates farthest point sampling and neighbor search operations while\npreserving sampling quality and model performance. By integrating FastPoint\ninto state-of-the-art 3D point cloud models, we achieve 2.55x end-to-end\nspeedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23480v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.17170", "title": "Advancing Quantum State Preparation Using Decision Diagram with Local Invertible Maps", "authors": ["Xin Hong", "Aochu Dai", "Chenjian Li", "Sanjiang Li", "Shenggang Ying", "Mingsheng Ying"], "categories": ["cs.DS", "quant-ph"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2507.14496", "url": "http://arxiv.org/abs/2507.17170v2", "summary": "Quantum state preparation (QSP) is a fundamental task in quantum computing\nand quantum information processing. It is critical to the execution of many\nquantum algorithms, including those in quantum machine learning. In this paper,\nwe propose a family of efficient QSP algorithms tailored to different numbers\nof available ancilla qubits - ranging from no ancilla qubits, to a single\nancilla qubit, to a sufficiently large number of ancilla qubits. Our approach\nexploits the power of Local Invertible Map Tensor Decision Diagrams (LimTDDs) -\na highly compact representation of quantum states that combines tensor networks\nand decision diagrams to reduce quantum circuit complexity. Extensive\nexperiments demonstrate that our methods significantly outperform existing\napproaches and exhibit better scalability for large-scale quantum states, both\nin terms of runtime and gate complexity. Furthermore, our method shows\nexponential improvement in best-case scenarios.", "comment": "arXiv admin note: text overlap with arXiv:2507.14496", "pdf_url": "http://arxiv.org/pdf/2507.17170v2", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-31"}
{"id": "2507.22926", "title": "Multi-Relation Extraction in Entity Pairs using Global Context", "authors": ["Nilesh", "Atul Gupta", "Avinash C Panday"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 9 figures", "url": "http://arxiv.org/abs/2507.22926v1", "summary": "In document-level relation extraction, entities may appear multiple times in\na document, and their relationships can shift from one context to another.\nAccurate prediction of the relationship between two entities across an entire\ndocument requires building a global context spanning all relevant sentences.\nPrevious approaches have focused only on the sentences where entities are\nmentioned, which fails to capture the complete document context necessary for\naccurate relation extraction. Therefore, this paper introduces a novel input\nembedding approach to capture the positions of mentioned entities throughout\nthe document rather than focusing solely on the span where they appear. The\nproposed input encoding approach leverages global relationships and\nmulti-sentence reasoning by representing entities as standalone segments,\nindependent of their positions within the document. The performance of the\nproposed method has been tested on three benchmark relation extraction\ndatasets, namely DocRED, Re-DocRED, and REBEL. The experimental results\ndemonstrated that the proposed method accurately predicts relationships between\nentities in a document-level setting. The proposed research also has\ntheoretical and practical implications. Theoretically, it advances global\ncontext modeling and multi-sentence reasoning in document-level relation\nextraction. Practically, it enhances relationship detection, enabling improved\nperformance in real-world NLP applications requiring comprehensive entity-level\ninsights and interpretability.", "comment": "11 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.22926v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2401.13639", "title": "Winding Clearness for Differentiable Point Cloud Optimization", "authors": ["Dong Xiao", "Yueji Ma", "Zuoqiang Shi", "Shiqing Xin", "Wenping Wang", "Bailin Deng", "Bin Wang"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Accepted by Computer-Aided Design through SPM 2025", "url": "http://arxiv.org/abs/2401.13639v2", "summary": "We propose to explore the properties of raw point clouds through the\n\\emph{winding clearness}, a concept we first introduce for measuring the\nclarity of the interior/exterior relationships represented by the winding\nnumber field of the point cloud. In geometric modeling, the winding number is a\npowerful tool for distinguishing the interior and exterior of a given surface\n$\\partial \\Omega$, and it has been previously used for point normal orientation\nand surface reconstruction. In this work, we introduce a novel approach to\nevaluate and optimize the quality of point clouds based on the winding\nclearness. We observe that point clouds with less noise generally exhibit\nbetter winding clearness. Accordingly, we propose an objective function that\nquantifies the error in winding clearness, solely utilizing the coordinates of\nthe point clouds. Moreover, we demonstrate that the winding clearness error is\ndifferentiable and can serve as a loss function in point cloud processing. We\npresent this observation from two aspects: 1) We update the coordinates of the\npoints by back-propagating the loss function for individual point clouds,\nresulting in an overall improvement without involving a neural network. 2) We\nincorporate winding clearness as a geometric constraint in the diffusion-based\n3D generative model and update the network parameters to generate point clouds\nwith less noise. Experimental results demonstrate the effectiveness of\noptimizing the winding clearness in enhancing the point cloud quality. Notably,\nour method exhibits superior performance in handling noisy point clouds with\nthin structures, highlighting the benefits of the global perspective enabled by\nthe winding number.", "comment": "Accepted by Computer-Aided Design through SPM 2025", "pdf_url": "http://arxiv.org/pdf/2401.13639v2", "cate": "cs.GR", "date": "2024-01-24", "updated": "2025-07-31"}
{"id": "2402.11461", "title": "FGeo-HyperGNet: Geometric Problem Solving Integrating FormalGeo Symbolic System and Hypergraph Neural Network", "authors": ["Xiaokai Zhang", "Yang Li", "Na Zhu", "Cheng Qin", "Zhenbing Zeng", "Tuo Leng"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by IJCAI 2025", "url": "http://arxiv.org/abs/2402.11461v3", "summary": "Geometric problem solving has always been a long-standing challenge in the\nfields of mathematical reasoning and artificial intelligence. We built a\nneural-symbolic system, called FGeo-HyperGNet, to automatically perform\nhuman-like geometric problem solving. The symbolic component is a formal system\nbuilt on FormalGeo, which can automatically perform geometric relational\nreasoning and algebraic calculations and organize the solution into a\nhypergraph with conditions as hypernodes and theorems as hyperedges. The neural\ncomponent, called HyperGNet, is a hypergraph neural network based on the\nattention mechanism, including an encoder to encode the structural and semantic\ninformation of the hypergraph and a theorem predictor to provide guidance in\nsolving problems. The neural component predicts theorems according to the\nhypergraph, and the symbolic component applies theorems and updates the\nhypergraph, thus forming a predict-apply cycle to ultimately achieve readable\nand traceable automatic solving of geometric problems. Experiments demonstrate\nthe effectiveness of this neural-symbolic architecture. We achieved\nstate-of-the-art results with a TPA of 93.50% and a PSSR of 88.36% on the\nFormalGeo7K dataset. The code is available at\nhttps://github.com/BitSecret/HyperGNet.", "comment": "Accepted by IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2402.11461v3", "cate": "cs.AI", "date": "2024-02-18", "updated": "2025-07-31"}
{"id": "2507.23297", "title": "Simulation-based inference for Precision Neutrino Physics through Neural Monte Carlo tuning", "authors": ["A. Gavrikov", "A. Serafini", "D. Dolzhikov", "A. Garfagnini", "M. Gonchar", "M. Grassi", "R. Brugnera", "V. Cerrone", "L. V. D'Auria", "R. M. Guizzetti", "L. Lastrucci", "G. Andronico", "V. Antonelli", "A. Barresi", "D. Basilico", "M. Beretta", "A. Bergnoli", "M. Borghesi", "A. Brigatti", "R. Bruno", "A. Budano", "B. Caccianiga", "A. Cammi", "R. Caruso", "D. Chiesa", "C. Clementi", "C. Coletta", "S. Dusini", "A. Fabbri", "G. Felici", "G. Ferrante", "M. G. Giammarchi", "N. Giudice", "N. Guardone", "F. Houria", "C. Landini", "I. Lippi", "L. Loi", "P. Lombardi", "F. Mantovani", "S. M. Mari", "A. Martini", "L. Miramonti", "M. Montuschi", "M. Nastasi", "D. Orestano", "F. Ortica", "A. Paoloni", "L. Pelicci", "E. Percalli", "F. Petrucci", "E. Previtali", "G. Ranucci", "A. C. Re", "B. Ricci", "A. Romani", "C. Sirignano", "M. Sisti", "L. Stanco", "E. Stanescu Farilla", "V. Strati", "M. D. C Torri", "C. Tuvè", "C. Venettacci", "G. Verde", "L. Votano"], "categories": ["physics.data-an", "cs.LG", "hep-ex", "hep-ph", "physics.ins-det"], "primary_category": "Subjects:       Data Analysis, Statistics and Probability (physics.data-an)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23297v1", "summary": "Precise modeling of detector energy response is crucial for next-generation\nneutrino experiments which present computational challenges due to lack of\nanalytical likelihoods. We propose a solution using neural likelihood\nestimation within the simulation-based inference framework. We develop two\ncomplementary neural density estimators that model likelihoods of calibration\ndata: conditional normalizing flows and a transformer-based regressor. We adopt\nJUNO - a large neutrino experiment - as a case study. The energy response of\nJUNO depends on several parameters, all of which should be tuned, given their\nnon-linear behavior and strong correlations in the calibration data. To this\nend, we integrate the modeled likelihoods with Bayesian nested sampling for\nparameter inference, achieving uncertainties limited only by statistics with\nnear-zero systematic biases. The normalizing flows model enables unbinned\nlikelihood analysis, while the transformer provides an efficient binned\nalternative. By providing both options, our framework offers flexibility to\nchoose the most appropriate method for specific needs. Finally, our approach\nestablishes a template for similar applications across experimental neutrino\nand broader particle physics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23297v1", "cate": "physics.data-an", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23483", "title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion", "authors": ["Mutian Xu", "Chongjie Ye", "Haolin Liu", "Yushuang Wu", "Jiahao Chang", "Xiaoguang Han"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight). Project page: this https URL", "url": "http://arxiv.org/abs/2507.23483v1", "summary": "3D data simulation aims to bridge the gap between simulated and real-captured\n3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D\ndata simulation methods inject predefined physical priors but struggle to\ncapture the full complexity of real data. An optimal approach involves learning\nan implicit mapping from synthetic to realistic data in a data-driven manner,\nbut progress in this solution has met stagnation in recent studies. This work\nexplores a new solution path of data-driven 3D simulation, called\nStable-Sim2Real, based on a novel two-stage depth diffusion model. The initial\nstage finetunes Stable-Diffusion to generate the residual between the real and\nsynthetic paired depth, producing a stable but coarse depth, where some local\nregions may deviate from realistic patterns. To enhance this, both the\nsynthetic and initial output depth are fed into a second-stage diffusion, where\ndiffusion loss is adjusted to prioritize these distinct areas identified by a\n3D discriminator. We provide a new benchmark scheme to evaluate 3D data\nsimulation methods. Extensive experiments show that training the network with\nthe 3D simulated data derived from our method significantly enhances\nperformance in real-world 3D visual tasks. Moreover, the evaluation\ndemonstrates the high similarity between our 3D simulated data and\nreal-captured patterns. Project page:\nhttps://mutianxu.github.io/stable-sim2real/.", "comment": "ICCV 2025 (Highlight). Project page:\n  https://mutianxu.github.io/stable-sim2real/", "pdf_url": "http://arxiv.org/pdf/2507.23483v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22450", "title": "Settling Weighted Token Swapping up to Algorithmic Barriers", "authors": ["Nicole Wein", "Guanyu Tony Zhang"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22450v2", "summary": "We study the weighted token swapping problem, in which we are given a graph\non $n$ vertices, $n$ weighted tokens, an initial assignment of one token to\neach vertex, and a final assignment of one token to each vertex. The goal is to\nfind a minimum-cost sequence of swaps of adjacent tokens to reach the final\nassignment from the initial assignment, where the cost is the sum over all\nswaps of the sum of the weights of the two swapped tokens. Unweighted token\nswapping has been extensively studied: it is NP-hard to approximate to a factor\nbetter than $14/13$, and there is a polynomial-time 4-approximation, along with\na tight \"barrier\" result showing that the class of locally optimal algorithms\ncannot achieve a ratio better than 4. For trees, the problem remains NP-hard to\nsolve exactly, and there is a polynomial-time 2-approximation, along with a\ntight barrier result showing that the class of $\\ell$-straying algorithms\ncannot achieve a ratio better than 2. Weighted token swapping with $\\{0,1\\}$\nweights is much harder to approximation: it is NP-hard to approximate even to a\nfactor of $(1-\\varepsilon) \\cdot \\ln n$ for any constant $\\varepsilon>0$.\nRestricting to positive weights, no approximation algorithms are known, and the\nonly known lower bounds are those inherited directly from the unweighted\nversion. We provide the first approximation algorithms for weighted token\nswapping on both trees and general graphs, along with tight barrier results.\nLetting $w$ and $W$ be the minimum and maximum token weights, our approximation\nratio is $2+2W/w$ for general graphs and $1+W/w$ for trees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22450v2", "cate": "cs.DS", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.22927", "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation", "authors": ["Zhehao Tan", "Yihan Jiao", "Dan Yang", "Lei Liu", "Jie Feng", "Duolin Sun", "Yue Shen", "Jian Wang", "Peng Wei", "Jinjie Gu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22927v1", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge, where the LLM's ability to generate responses\nbased on the combination of a given query and retrieved documents is crucial.\nHowever, most benchmarks focus on overall RAG system performance, rarely\nassessing LLM-specific capabilities. Current benchmarks emphasize broad aspects\nsuch as noise robustness, but lack a systematic and granular evaluation\nframework on document utilization. To this end, we introduce\n\\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,\nemphasizing the following progressive dimensions: (1) multi-level filtering\nabilities, (2) combination abilities, and (3) reference reasoning. To provide a\nmore nuanced understanding of LLMs' roles in RAG systems, we formulate an\ninnovative placeholder-based approach to decouple the contributions of the\nLLM's parametric knowledge and the external knowledge. Experiments demonstrate\nthe limitations of representative LLMs in the RAG system's generation\ncapabilities, particularly in error resilience and context faithfulness. Our\nbenchmark provides a reproducible framework for developing more reliable and\nefficient RAG systems. Our code is available in\nhttps://github.com/Alipay-Med/PRGB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22927v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.21288", "title": "Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties", "authors": ["Guanxiong Chen", "Shashwat Suri", "Yuhao Wu", "Etienne Voulga", "David I. W. Levin", "Dinesh K. Pai"], "categories": ["cs.GR", "cs.AI"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Added middle name of Prof. Pai", "url": "http://arxiv.org/abs/2507.21288v2", "summary": "Materials used in real clothing exhibit remarkable complexity and spatial\nvariation due to common processes such as stitching, hemming, dyeing, printing,\npadding, and bonding. Simulating these materials, for instance using finite\nelement methods, is often computationally demanding and slow. Worse, such\nmethods can suffer from numerical artifacts called ``membrane locking'' that\nmakes cloth appear artificially stiff. Here we propose a general framework,\ncalled Mass-Spring Net, for learning a simple yet efficient surrogate model\nthat captures the effects of these complex materials using only motion\nobservations. The cloth is discretized into a mass-spring network with unknown\nmaterial parameters that are learned directly from the motion data, using a\nnovel force-and-impulse loss function. Our approach demonstrates the ability to\naccurately model spatially varying material properties from a variety of data\nsources, and immunity to membrane locking which plagues FEM-based simulations.\nCompared to graph-based networks and neural ODE-based architectures, our method\nachieves significantly faster training times, higher reconstruction accuracy,\nand improved generalization to novel dynamic scenarios.", "comment": "Added middle name of Prof. Pai", "pdf_url": "http://arxiv.org/pdf/2507.21288v2", "cate": "cs.GR", "date": "2025-07-28", "updated": "2025-07-30"}
{"id": "2408.01254", "title": "TrIM, Triangular Input Movement Systolic Array for Convolutional Neural Networks: Dataflow and Analytical Modelling", "authors": ["Cristian Sestito", "Shady Agwa", "Themis Prodromakis"], "categories": ["cs.AI", "cs.AR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This work has been accepted by IEEE TCASAI for publication", "url": "http://arxiv.org/abs/2408.01254v3", "summary": "In order to follow the ever-growing computational complexity and data\nintensity of state-of-the-art AI models, new computing paradigms are being\nproposed. These paradigms aim at achieving high energy efficiency by mitigating\nthe Von Neumann bottleneck that relates to the energy cost of moving data\nbetween the processing cores and the memory. Convolutional Neural Networks\n(CNNs) are susceptible to this bottleneck, given the massive data they have to\nmanage. Systolic arrays (SAs) are promising architectures to mitigate data\ntransmission cost, thanks to high data utilization of Processing Elements\n(PEs). These PEs continuously exchange and process data locally based on\nspecific dataflows (such as weight stationary and row stationary), in turn\nreducing the number of memory accesses to the main memory. In SAs, convolutions\nare managed either as matrix multiplications or exploiting the raster-order\nscan of sliding windows. However, data redundancy is a primary concern\naffecting area, power, and energy. In this paper, we propose TrIM: a novel\ndataflow for SAs based on a Triangular Input Movement and compatible with CNN\ncomputing. TrIM maximizes the local input utilization, minimizes the weight\ndata movement, and solves the data redundancy problem. Furthermore, TrIM does\nnot incur the significant on-chip memory penalty introduced by the row\nstationary dataflow. When compared to state-of-the-art SA dataflows, the high\ndata utilization offered by TrIM guarantees ~10X less memory access.\nFurthermore, considering that PEs continuously overlap multiplications and\naccumulations, TrIM achieves high throughput (up to 81.8% higher than row\nstationary), other than requiring a limited number of registers (up to 15.6X\nfewer registers than row stationary).", "comment": "This work has been accepted by IEEE TCASAI for publication", "pdf_url": "http://arxiv.org/pdf/2408.01254v3", "cate": "cs.AI", "date": "2024-08-02", "updated": "2025-07-31"}
{"id": "2507.23349", "title": "Optimal Transport Learning: Balancing Value Optimization and Fairness in Individualized Treatment Rules", "authors": ["Wenhai Cui", "Xiaoting Ji", "Wen Su", "Xiaodong Yan", "Xingqiu Zhao"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23349v1", "summary": "Individualized treatment rules (ITRs) have gained significant attention due\nto their wide-ranging applications in fields such as precision medicine,\nridesharing, and advertising recommendations. However, when ITRs are influenced\nby sensitive attributes such as race, gender, or age, they can lead to outcomes\nwhere certain groups are unfairly advantaged or disadvantaged. To address this\ngap, we propose a flexible approach based on the optimal transport theory,\nwhich is capable of transforming any optimal ITR into a fair ITR that ensures\ndemographic parity. Recognizing the potential loss of value under fairness\nconstraints, we introduce an ``improved trade-off ITR,\" designed to balance\nvalue optimization and fairness while accommodating varying levels of fairness\nthrough parameter adjustment. To maximize the value of the improved trade-off\nITR under specific fairness levels, we propose a smoothed fairness constraint\nfor estimating the adjustable parameter. Additionally, we establish a\ntheoretical upper bound on the value loss for the improved trade-off ITR. We\ndemonstrate performance of the proposed method through extensive simulation\nstudies and application to the Next 36 entrepreneurial program dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23349v1", "cate": "stat.ML", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23508", "title": "Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion", "authors": ["Timing Li", "Bing Cao", "Jiahe Feng", "Haifang Cao", "Qinghau Hu", "Pengfei Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23508v1", "summary": "Image fusion synthesizes complementary information from multiple sources,\nmitigating the inherent limitations of unimodal imaging systems. Accurate image\nregistration is essential for effective multi-source data fusion. However,\nexisting registration methods, often based on image translation in Euclidean\nspace, fail to handle cross-modal misalignment effectively, resulting in\nsuboptimal alignment and fusion quality. To overcome this limitation, we\nexplore image alignment in non-Euclidean space and propose a Hyperbolic Cycle\nAlignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign\nis the first image registration method based on hyperbolic space. It introduces\na dual-path cross-modal cyclic registration framework, in which a forward\nregistration network aligns cross-modal inputs, while a backward registration\nnetwork reconstructs the original image, forming a closed-loop registration\nstructure with geometric consistency. Additionally, we design a Hyperbolic\nHierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into\nhyperbolic space and imposes registration constraints, effectively reducing\ninterference caused by modality discrepancies. We further analyze image\nregistration in both Euclidean and hyperbolic spaces, demonstrating that\nhyperbolic space enables more sensitive and effective multi-modal image\nregistration. Extensive experiments on misaligned multi-modal images\ndemonstrate that our method significantly outperforms existing approaches in\nboth image alignment and fusion. Our code will be publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23508v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.00059", "title": "Computational Verification of the Buratti--Horak--Rosa Conjecture for Small Integers and Inductive Approaches", "authors": ["Ranjan N Naik"], "categories": ["cs.DM", "cs.DS", "math.CO"], "primary_category": "Subjects:       Discrete Mathematics (cs.DM)", "pdf_link": null, "comments": "Comments:      This result supports the results by Mariusz Meszka for all primes up to 23 (included) with the aid of a computer. Additional results on Coprime BHR Conjecture verifications for p < 31 and Inductive Approaches are included in this revision", "url": "http://arxiv.org/abs/2507.00059v4", "summary": "This paper presents a comprehensive computational approach to verify and\ninductively construct Hamiltonian paths for the Buratti--Horak--Rosa (BHR)\nConjecture. The conjecture posits that for any multiset $L$ of $p-1$ positive\nintegers not exceeding $\\lfloor p/2 \\rfloor$, there exists a Hamiltonian path\nin the complete graph $K_p$ with vertex-set $\\{0, 1, \\dots, p-1\\}$ whose edge\nlengths (under the cyclic metric) match $L$, if and only if for every divisor\n$d$ of $p$, the number of multiples of $d$ appearing in $L$ is at most $p - d$.\n  Building upon prior computational work by Mariusz Meszka, which verified the\nconjecture for all primes up to $p=23$, our Python program extends this\nverification significantly. We approach the problem by systematically\ngenerating frequency partitions (FPs) of edge lengths and employing a recursive\nbacktracking algorithm. We report successful computational verification for all\nfrequency partitions for integers $p < 32$, specifically presenting results for\n$p=31$ and a composite $p=26$. For the composite number $p=30$, the Python code\ntook approximately 11 hours to verify on a Lenovo laptop. For $p=16$, $167,898$\nvalid multisets were processed, taking around 20 hours on Google Colab Pro+.\n  Furthermore, we introduce and implement two constructive, inductive\nstrategies for building Hamiltonian paths: (1) increasing the multiplicity of\nan existing edge length, and (2) adding a new edge length. These methods,\nsupported by a reuse-insertion heuristic and backtracking search, demonstrate\nsuccessful constructions for evolving FPs up to $p=40$. Through these empirical\ntests and performance metrics, we provide strong computational evidence for the\nvalidity of the BHR conjecture within the scope tested, and outline the\nscalability of our approach for higher integer values.", "comment": "This result supports the results by Mariusz Meszka for all primes up\n  to 23 (included) with the aid of a computer. Additional results on Coprime\n  BHR Conjecture verifications for p < 31 and Inductive Approaches are included\n  in this revision", "pdf_url": "http://arxiv.org/pdf/2507.00059v4", "cate": "cs.DM", "date": "2025-06-26", "updated": "2025-07-31"}
{"id": "2507.22932", "title": "FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification", "authors": ["Baptiste Lefort", "Eric Benhamou", "Beatrice Guez", "Jean-Jacques Ohana", "Ethan Setrouk", "Alban Etienne"], "categories": ["cs.CL", "q-fin.GN"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.22932v1", "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.22932v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.01631", "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D Vision Across Altitudes). Our code will be made public after the conference at this https URL", "url": "http://arxiv.org/abs/2507.01631v2", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Our code will be made public after the conference\n  at https://github.com/Ellimac0/Snake-NeRF", "pdf_url": "http://arxiv.org/pdf/2507.01631v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-31"}
{"id": "2507.23267", "title": "Your Spending Needs Attention: Modeling Financial Habits with Transformers", "authors": ["D. T. Braithwaite", "Misael Cavalcanti", "R. Austin McEver", "Hiroto Udagawa", "Daniel Silva", "Rohan Ramanath", "Felipe Meneses", "Arissa Yoshida", "Evan Wingert", "Matheus Ramos", "Brian Zanfelice", "Aman Gupta"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23267v1", "summary": "Predictive models play a crucial role in the financial industry, enabling\nrisk prediction, fraud detection, and personalized recommendations, where\nslight changes in core model performance can result in billions of dollars in\nrevenue or losses. While financial institutions have access to enormous amounts\nof user data (e.g., bank transactions, in-app events, and customer support\nlogs), leveraging this data effectively remains challenging due to its\ncomplexity and scale. Thus, in many financial institutions, most production\nmodels follow traditional machine learning (ML) approaches by converting\nunstructured data into manually engineered tabular features. Conversely, other\ndomains (e.g., natural language processing) have effectively utilized\nself-supervised learning (SSL) to learn rich representations from raw data,\nremoving the need for manual feature extraction. In this paper, we investigate\nusing transformer-based representation learning models for transaction data,\nhypothesizing that these models, trained on massive data, can provide a novel\nand powerful approach to understanding customer behavior. We propose a new\nmethod enabling the use of SSL with transaction data by adapting\ntransformer-based models to handle both textual and structured attributes. Our\napproach, denoted nuFormer, includes an end-to-end fine-tuning method that\nintegrates user embeddings with existing tabular features. Our experiments\ndemonstrate improvements for large-scale recommendation problems at Nubank.\nNotably, these gains are achieved solely through enhanced representation\nlearning rather than incorporating new data sources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23267v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.02508", "title": "When Words Smile: Generating Diverse Emotional Facial Expressions from Text", "authors": ["Haidong Xu", "Meishan Zhang", "Hao Ju", "Zhedong Zheng", "Erik Cambria", "Min Zhang", "Hao Fei"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      19 pages. Resources: this https URL", "url": "http://arxiv.org/abs/2412.02508v3", "summary": "Enabling digital humans to express rich emotions has significant applications\nin dialogue systems, gaming, and other interactive scenarios. While recent\nadvances in talking head synthesis have achieved impressive results in lip\nsynchronization, they tend to overlook the rich and dynamic nature of facial\nexpressions. To fill this critical gap, we introduce an end-to-end\ntext-to-expression model that explicitly focuses on emotional dynamics. Our\nmodel learns expressive facial variations in a continuous latent space and\ngenerates expressions that are diverse, fluid, and emotionally coherent. To\nsupport this task, we introduce EmoAva, a large-scale and high-quality dataset\ncontaining 15,000 text-3D expression pairs. Extensive experiments on both\nexisting datasets and EmoAva demonstrate that our method significantly\noutperforms baselines across multiple evaluation metrics, marking a significant\nadvancement in the field.", "comment": "19 pages. Resources: https://github.com/WalkerMitty/EmoAva", "pdf_url": "http://arxiv.org/pdf/2412.02508v3", "cate": "cs.AI", "date": "2024-12-03", "updated": "2025-07-31"}
{"id": "2507.23620", "title": "DivControl: Knowledge Diversion for Controllable Image Generation", "authors": ["Yucheng Xie", "Fu Feng", "Ruixiao Shi", "Jing Wang", "Yong Rui", "Xin Geng"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23620v1", "summary": "Diffusion models have advanced from text-to-image (T2I) to image-to-image\n(I2I) generation by incorporating structured inputs such as depth maps,\nenabling fine-grained spatial control. However, existing methods either train\nseparate models for each condition or rely on unified architectures with\nentangled representations, resulting in poor generalization and high adaptation\ncosts for novel conditions. To this end, we propose DivControl, a decomposable\npretraining framework for unified controllable generation and efficient\nadaptation. DivControl factorizes ControlNet via SVD into basic\ncomponents-pairs of singular vectors-which are disentangled into\ncondition-agnostic learngenes and condition-specific tailors through knowledge\ndiversion during multi-condition training. Knowledge diversion is implemented\nvia a dynamic gate that performs soft routing over tailors based on the\nsemantics of condition instructions, enabling zero-shot generalization and\nparameter-efficient adaptation to novel conditions. To further improve\ncondition fidelity and training efficiency, we introduce a representation\nalignment loss that aligns condition embeddings with early diffusion features.\nExtensive experiments demonstrate that DivControl achieves state-of-the-art\ncontrollability with 36.4$\\times$ less training cost, while simultaneously\nimproving average performance on basic conditions. It also delivers strong\nzero-shot and few-shot performance on unseen conditions, demonstrating superior\nscalability, modularity, and transferability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23620v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23567", "title": "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection", "authors": ["Yung-Hsu Yang", "Luigi Piccinelli", "Mattia Segu", "Siyuan Li", "Rui Huang", "Yuqian Fu", "Marc Pollefeys", "Hermann Blum", "Zuria Bauer"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.23567v1", "summary": "Monocular 3D object detection is valuable for various applications such as\nrobotics and AR/VR. Existing methods are confined to closed-set settings, where\nthe training and testing sets consist of the same scenes and/or object\ncategories. However, real-world applications often introduce new environments\nand novel object categories, posing a challenge to these methods. In this\npaper, we address monocular 3D object detection in an open-set setting and\nintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).\nWe propose to lift the open-set 2D detection into 3D space through our designed\n3D bounding box head, enabling end-to-end joint training for both 2D and 3D\ntasks to yield better overall performance. We condition the object queries with\ngeometry prior and overcome the generalization for 3D estimation across diverse\nscenes. To further improve performance, we design the canonical image space for\nmore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set\nsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and\nachieve new state-of-the-art results. Code and models are available at\nroyyang0714.github.io/3D-MOOD.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23567v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22943", "title": "A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies", "authors": ["Shirley V Wang", "Georg Hahn", "Sushama Kattinakere Sreedhara", "Mufaddal Mahesri", "Haritha S. Pillai", "Rajendra Aldis", "Joyce Lii", "Sarah K. Dutcher", "Rhoda Eniafe", "Jamal T. Jones", "Keewan Kim", "Jiwei He", "Hana Lee", "Sengwee Toh", "Rishi J Desai", "Jie Yang"], "categories": ["cs.CL", "stat.ME"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22943v1", "summary": "Background: One of the ways to enhance analyses conducted with large claims\ndatabases is by validating the measurement characteristics of code-based\nalgorithms used to identify health outcomes or other key study parameters of\ninterest. These metrics can be used in quantitative bias analyses to assess the\nrobustness of results for an inferential study given potential bias from\noutcome misclassification. However, extensive time and resource allocation are\ntypically re-quired to create reference-standard labels through manual chart\nreview of free-text notes from linked electronic health records. Methods: We\ndescribe an expedited process that introduces efficiency in a validation study\nus-ing two distinct mechanisms: 1) use of natural language processing (NLP) to\nreduce time spent by human reviewers to review each chart, and 2) a multi-wave\nadaptive sampling approach with pre-defined criteria to stop the validation\nstudy once performance characteristics are identified with sufficient\nprecision. We illustrate this process in a case study that validates the\nperformance of a claims-based outcome algorithm for intentional self-harm in\npatients with obesity. Results: We empirically demonstrate that the\nNLP-assisted annotation process reduced the time spent on review per chart by\n40% and use of the pre-defined stopping rule with multi-wave samples would have\nprevented review of 77% of patient charts with limited compromise to precision\nin derived measurement characteristics. Conclusion: This approach could\nfacilitate more routine validation of code-based algorithms used to define key\nstudy parameters, ultimately enhancing understanding of the reliability of\nfind-ings derived from database studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22943v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.23364", "title": "Holistic Evaluations of Topic Models", "authors": ["Thomas Compton"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      10 pages, 6 tables", "url": "http://arxiv.org/abs/2507.23364v1", "summary": "Topic models are gaining increasing commercial and academic interest for\ntheir ability to summarize large volumes of unstructured text. As unsupervised\nmachine learning methods, they enable researchers to explore data and help\ngeneral users understand key themes in large text collections. However, they\nrisk becoming a 'black box', where users input data and accept the output as an\naccurate summary without scrutiny. This article evaluates topic models from a\ndatabase perspective, drawing insights from 1140 BERTopic model runs. The goal\nis to identify trade-offs in optimizing model parameters and to reflect on what\nthese findings mean for the interpretation and responsible use of topic models", "comment": "10 pages, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.23364v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.18666", "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "url": "http://arxiv.org/abs/2503.18666v3", "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identify 87.26%\nof the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.", "comment": "Accepted by the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "pdf_url": "http://arxiv.org/pdf/2503.18666v3", "cate": "cs.AI", "date": "2025-03-24", "updated": "2025-07-31"}
{"id": "2507.23673", "title": "SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S. Elson", "Stamatia Giannarou"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23673v1", "summary": "Hyperspectral imaging (HSI) provides rich spectral information for medical\nimaging, yet encounters significant challenges due to data limitations and\nhardware variations. We introduce SAMSA, a novel interactive segmentation\nframework that combines an RGB foundation model with spectral analysis. SAMSA\nefficiently utilizes user clicks to guide both RGB segmentation and spectral\nsimilarity computations. The method addresses key limitations in HSI\nsegmentation through a unique spectral feature fusion strategy that operates\nindependently of spectral band count and resolution. Performance evaluation on\npublicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on a\nneurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperative\nporcine hyperspectral dataset. Experimental results demonstrate SAMSA's\neffectiveness in few-shot and zero-shot learning scenarios and using minimal\ntraining examples. Our approach enables seamless integration of datasets with\ndifferent spectral characteristics, providing a flexible framework for\nhyperspectral medical image analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23673v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23569", "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization", "authors": ["Maxime Pietrantoni", "Gabriela Csurka", "Torsten Sattler"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      CVPR 2025", "url": "http://arxiv.org/abs/2507.23569v1", "summary": "Visual localization is the task of estimating a camera pose in a known\nenvironment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based\nrepresentations for accurate and privacy-preserving visual localization. We\npropose Gaussian Splatting Feature Fields (GSFFs), a scene representation for\nvisual localization that combines an explicit geometry model (3DGS) with an\nimplicit feature field. We leverage the dense geometric information and\ndifferentiable rasterization algorithm from 3DGS to learn robust feature\nrepresentations grounded in 3D. In particular, we align a 3D scale-aware\nfeature field and a 2D feature encoder in a common embedding space through a\ncontrastive framework. Using a 3D structure-informed clustering procedure, we\nfurther regularize the representation learning and seamlessly convert the\nfeatures to segmentations, which can be used for privacy-preserving visual\nlocalization. Pose refinement, which involves aligning either feature maps or\nsegmentations from a query image with those rendered from the GSFFs scene\nrepresentation, is used to achieve localization. The resulting privacy- and\nnon-privacy-preserving localization pipelines, evaluated on multiple real-world\ndatasets, show state-of-the-art performances.", "comment": "CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.23569v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23063", "title": "Math Natural Language Inference: this should be easy!", "authors": ["Valeria de Paiva", "Qiyue Gao", "Hai Hu", "Pavel Kovalev", "Yikang Liu", "Lawrence S. Moss", "Zhiheng Qian"], "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages plus appendices", "url": "http://arxiv.org/abs/2507.23063v1", "summary": "We ask whether contemporary LLMs are able to perform natural language\ninference (NLI) tasks on mathematical texts. We call this the Math NLI problem.\nWe construct a corpus of Math NLI pairs whose premises are from extant\nmathematical text and whose hypotheses and gold labels were provided by people\nwith experience in both research-level mathematics and also in the NLI field.\nWe also investigate the quality of corpora using the same premises but whose\nhypotheses are provided by LLMs themselves. We not only investigate the\nperformance but also the inter-group consistency of the diverse group of LLMs.\nWe have both positive and negative findings. Among our positive findings: in\nsome settings, using a majority vote of LLMs is approximately equivalent to\nusing human-labeled data in the Math NLI area. On the negative side: LLMs still\nstruggle with mathematical language. They occasionally fail at even basic\ninferences. Current models are not as prone to hypothesis-only \"inference\" in\nour data the way the previous generation had been. In addition to our findings,\nwe also provide our corpora as data to support future work on Math NLI.", "comment": "9 pages plus appendices", "pdf_url": "http://arxiv.org/pdf/2507.23063v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23410", "title": "Towards LLM-Enhanced Product Line Scoping", "authors": ["Alexander Felfernig", "Damian Garber", "Viet-Man Le", "Sebastian Lubos", "Thi Ngoc Trang Tran"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23410v1", "summary": "The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23410v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22960", "title": "Hybrid Particle Swarm Optimization for Fast and Reliable Parameter Extraction in Thermoreflectance", "authors": ["Bingjia Xiao", "Tao Chen", "Wenbin Zhang", "Xin Qian", "Puqing Jiang"], "categories": ["cs.NE", "cond-mat.other"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      28 pages, 8 figures", "url": "http://arxiv.org/abs/2507.22960v1", "summary": "Frequency-domain thermoreflectance (FDTR) is a widely used technique for\ncharacterizing thermal properties of multilayer thin films. However, extracting\nmultiple parameters from FDTR measurements presents a nonlinear inverse problem\ndue to its high dimensionality and multimodal, non-convex solution space. This\nstudy evaluates four popular global optimization algorithms: Genetic Algorithm\n(GA), Quantum Genetic Algorithm (QGA), Particle Swarm Optimization (PSO), and\nFireworks Algorithm (FWA), for extracting parameters from FDTR measurements of\na GaN/Si heterostructure. However, none achieve reliable convergence within 60\nseconds. To improve convergence speed and accuracy, we propose an AI-driven\nhybrid optimization framework that combines each global algorithm with a\nQuasi-Newton local refinement method, resulting in four hybrid variants: HGA,\nHQGA, HPSO, and HFWA. Among these, HPSO outperforms all other methods, with 80%\nof trials reaching the target fitness value within 60 seconds, showing greater\nrobustness and a lower risk of premature convergence. In contrast, only 30% of\nHGA and HQGA trials and 20% of HFWA trials achieve this threshold. We then\nevaluate the worst-case performance across 100 independent trials for each\nalgorithm when the time is extended to 1000 seconds. Only HPSO, PSO, and HGA\nconsistently reach the target accuracy, with HPSO converging five times faster\nthan the others. HPSO provides a general-purpose solution for inverse problems\nin thermal metrology and can be readily extended to other model-fitting\ntechniques.", "comment": "28 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.22960v1", "cate": "cs.NE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2505.11122", "title": "Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining", "authors": ["Yu Shi", "Yitong Duan", "Jian Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11122v2", "summary": "Alpha factor mining is pivotal in quantitative investment for identifying\npredictive signals from complex financial data. While traditional formulaic\nalpha mining relies on human expertise, contemporary automated methods, such as\nthose based on genetic programming or reinforcement learning, often struggle\nwith search inefficiency or yield alpha factors that are difficult to\ninterpret. This paper introduces a novel framework that integrates Large\nLanguage Models (LLMs) with Monte Carlo Tree Search (MCTS) to overcome these\nlimitations. Our framework leverages the LLM's instruction-following and\nreasoning capability to iteratively generate and refine symbolic alpha formulas\nwithin an MCTS-driven exploration. A key innovation is the guidance of MCTS\nexploration by rich, quantitative feedback from financial backtesting of each\ncandidate factor, enabling efficient navigation of the vast search space.\nFurthermore, a frequent subtree avoidance mechanism is introduced to enhance\nsearch diversity and prevent formulaic homogenization, further improving\nperformance. Experimental results on real-world stock market data demonstrate\nthat our LLM-based framework outperforms existing methods by mining alphas with\nsuperior predictive accuracy and trading performance. The resulting formulas\nare also more amenable to human interpretation, establishing a more effective\nand efficient paradigm for formulaic alpha mining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11122v2", "cate": "cs.AI", "date": "2025-05-16", "updated": "2025-07-31"}
{"id": "2507.23736", "title": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for Scalable, Uncertainty-Aware Redaction", "authors": ["Kyle Naddeo", "Nikolas Koutsoubis", "Rahul Krish", "Ghulam Rasool", "Nidhal Bouaynaya", "Tony OSullivan", "Raj Krish"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures,", "url": "http://arxiv.org/abs/2507.23736v1", "summary": "Access to medical imaging and associated text data has the potential to drive\nmajor advances in healthcare research and patient outcomes. However, the\npresence of Protected Health Information (PHI) and Personally Identifiable\nInformation (PII) in Digital Imaging and Communications in Medicine (DICOM)\nfiles presents a significant barrier to the ethical and secure sharing of\nimaging datasets. This paper presents a hybrid de-identification framework\ndeveloped by Impact Business Information Solutions (IBIS) that combines\nrule-based and AI-driven techniques, and rigorous uncertainty quantification\nfor comprehensive PHI/PII removal from both metadata and pixel data.\n  Our approach begins with a two-tiered rule-based system targeting explicit\nand inferred metadata elements, further augmented by a large language model\n(LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of\nsynthetic datasets simulating realistic clinical PHI/PII. For pixel data, we\nemploy an uncertainty-aware Faster R-CNN model to localize embedded text,\nextract candidate PHI via Optical Character Recognition (OCR), and apply the\nNER pipeline for final redaction. Crucially, uncertainty quantification\nprovides confidence measures for AI-based detections to enhance automation\nreliability and enable informed human-in-the-loop verification to manage\nresidual risks.\n  This uncertainty-aware deidentification framework achieves robust performance\nacross benchmark datasets and regulatory standards, including DICOM, HIPAA, and\nTCIA compliance metrics. By combining scalable automation, uncertainty\nquantification, and rigorous quality assurance, our solution addresses critical\nchallenges in medical data de-identification and supports the secure, ethical,\nand trustworthy release of imaging data for research.", "comment": "15 pages, 6 figures,", "pdf_url": "http://arxiv.org/pdf/2507.23736v1", "cate": "stat.ML", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23575", "title": "Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language Translation", "authors": ["Sobhan Asasi", "Mohamed Ilyas Lakhal", "Ozge Mercanoglu Sincan", "Richard Bowden"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at BMVC 2025", "url": "http://arxiv.org/abs/2507.23575v1", "summary": "Sign Language Translation (SLT) is a challenging task that requires bridging\nthe modality gap between visual and linguistic information while capturing\nsubtle variations in hand shapes and movements. To address these challenges, we\nintroduce \\textbf{BeyondGloss}, a novel gloss-free SLT framework that leverages\nthe spatio-temporal reasoning capabilities of Video Large Language Models\n(VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail,\nwe propose a novel approach to generate fine-grained, temporally-aware textual\ndescriptions of hand motion. A contrastive alignment module aligns these\ndescriptions with video features during pre-training, encouraging the model to\nfocus on hand-centric temporal dynamics and distinguish signs more effectively.\nTo further enrich hand-specific representations, we distill fine-grained\nfeatures from HaMeR. Additionally, we apply a contrastive loss between sign\nvideo representations and target language embeddings to reduce the modality gap\nin pre-training. \\textbf{BeyondGloss} achieves state-of-the-art performance on\nthe Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of the\nproposed framework. We will release the code upon acceptance of the paper.", "comment": "Accepted at BMVC 2025", "pdf_url": "http://arxiv.org/pdf/2507.23575v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23082", "title": "Exploring In-Context Learning for Frame-Semantic Parsing", "authors": ["Diego Garat", "Guillermo Moncecchi", "Dina Wonsever"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23082v1", "summary": "Frame Semantic Parsing (FSP) entails identifying predicates and labeling\ntheir arguments according to Frame Semantics. This paper investigates the use\nof In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP\nwithout model fine-tuning. We propose a method that automatically generates\ntask-specific prompts for the Frame Identification (FI) and Frame Semantic Role\nLabeling (FSRL) subtasks, relying solely on the FrameNet database. These\nprompts, constructed from frame definitions and annotated examples, are used to\nguide six different LLMs. Experiments are conducted on a subset of frames\nrelated to violent events. The method achieves competitive results, with F1\nscores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers\na practical and effective alternative to traditional fine-tuning for\ndomain-specific FSP tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23082v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23400", "title": "MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization", "authors": ["Yongbing Zhang", "Fang Nan", "Shengxiang Gao", "Yuxin Huang", "Kaiwen Tan", "Zhengtao Yu"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23400v1", "summary": "The core challenge faced by multi-document summarization is the complexity of\nrelationships among documents and the presence of information redundancy. Graph\nclustering is an effective paradigm for addressing this issue, as it models the\ncomplex relationships among documents using graph structures and reduces\ninformation redundancy through clustering, achieving significant research\nprogress. However, existing methods often only consider single-relational\ngraphs and require a predefined number of clusters, which hinders their ability\nto fully represent rich relational information and adaptively partition\nsentence groups to reduce redundancy. To overcome these limitations, we propose\nMRGSEM-Sum, an unsupervised multi-document summarization framework based on\nmulti-relational graphs and structural entropy minimization. Specifically, we\nconstruct a multi-relational graph that integrates semantic and discourse\nrelations between sentences, comprehensively modeling the intricate and dynamic\nconnections among sentences across documents. We then apply a two-dimensional\nstructural entropy minimization algorithm for clustering, automatically\ndetermining the optimal number of clusters and effectively organizing sentences\ninto coherent groups. Finally, we introduce a position-aware compression\nmechanism to distill each cluster, generating concise and informative\nsummaries. Extensive experiments on four benchmark datasets (Multi-News,\nDUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently\noutperforms previous unsupervised methods and, in several cases, achieves\nperformance comparable to supervised models and large language models. Human\nevaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high\nconsistency and coverage, approaching human-level quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23400v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23474", "title": "Finger Force Decoding from Motor Units Activity on Neuromorphic Hardware", "authors": ["Farah Baracat", "Giacomo Indiveri", "Elisa Donati"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23474v1", "summary": "Accurate finger force estimation is critical for next-generation\nhuman-machine interfaces. Traditional electromyography (EMG)-based decoding\nmethods using deep learning require large datasets and high computational\nresources, limiting their use in real-time, embedded systems. Here, we propose\na novel approach that performs finger force regression using spike trains from\nindividual motor neurons, extracted from high-density EMG. These biologically\ngrounded signals drive a spiking neural network implemented on a mixed-signal\nneuromorphic processor. Unlike prior work that encodes EMG into events, our\nmethod exploits spike timing on motor units to perform low-power, real-time\ninference. This is the first demonstration of motor neuron-based continuous\nregression computed directly on neuromorphic hardware. Our results confirm\naccurate finger-specific force prediction with minimal energy use, opening new\npossibilities for embedded decoding in prosthetics and wearable\nneurotechnology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23474v1", "cate": "cs.NE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2506.07528", "title": "Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification", "authors": ["Qisheng Hu", "Quanyu Long", "Wenya Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2506.07528v2", "summary": "Multi-hop claim verification is inherently challenging, requiring multi-step\nreasoning to construct verification chains while iteratively searching for\ninformation to uncover hidden bridging facts. This process is fundamentally\ninterleaved, as effective reasoning relies on dynamically retrieved evidence,\nwhile effective search demands reasoning to refine queries based on partial\ninformation. To achieve this, we propose Hierarchical Agent Reasoning and\nInformation Search (HARIS), explicitly modeling the coordinated process of\nreasoning-driven searching and search-informed reasoning. HARIS consists of a\nhigh-level reasoning agent that focuses on constructing the main verification\nchain, generating factual questions when more information is needed, and a\nlow-level search agent that iteratively retrieves more information, refining\nits search based on intermediate findings. This design allows each agent to\nspecialize in its respective task, enhancing verification accuracy and\ninterpretability. HARIS is trained using reinforcement learning with\noutcome-based rewards. Experimental results on the EX-FEVER and HOVER\nbenchmarks demonstrate that HARIS achieves strong performance, greatly\nadvancing multi-hop claim verification.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.07528v2", "cate": "cs.AI", "date": "2025-06-09", "updated": "2025-07-31"}
{"id": "2507.23767", "title": "Scaled Beta Models and Feature Dilution for Dynamic Ticket Pricing", "authors": ["Jonathan R. Landers"], "categories": ["stat.ML", "cs.LG", "68T05, 62H30, 62F10, 68Q32", "F.2.2; I.2.6; I.5.2; G.3"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      27 pages, 11 figures, 3 tables", "url": "http://arxiv.org/abs/2507.23767v1", "summary": "A novel approach is presented for identifying distinct signatures of\nperforming acts in the secondary ticket resale market by analyzing dynamic\npricing distributions. Using a newly curated, time series dataset from the\nSeatGeek API, we model ticket pricing distributions as scaled Beta\ndistributions. This enables accurate parameter estimation from incomplete\nstatistical data using a hybrid of quantile matching and the method of moments.\nIncorporating the estimated $\\alpha$ and $\\beta$ parameters into Random Forest\nclassifiers significantly improves pairwise artist classification accuracy,\ndemonstrating the unique economic signatures in event pricing data.\nAdditionally, we provide theoretical and empirical evidence that incorporating\nzero-variance (constant-value) features into Random Forest models acts as an\nimplicit regularizer, enhancing feature variety and robustness. This\nregularization promotes deeper, more varied trees in the ensemble, improving\nthe bias-variance tradeoff and mitigating overfitting to dominant features.\nThese findings are validated on both the new ticket pricing dataset and the\nstandard UCI ML handwritten digits dataset.", "comment": "27 pages, 11 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.23767v1", "cate": "stat.ML", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23595", "title": "MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model", "authors": ["Yaoye Zhu", "Zhe Wang", "Yan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV25 poster", "url": "http://arxiv.org/abs/2507.23595v1", "summary": "As cooperative systems that leverage roadside cameras to assist autonomous\nvehicle perception become increasingly widespread, large-scale precise\ncalibration of infrastructure cameras has become a critical issue. Traditional\nmanual calibration methods are often time-consuming, labor-intensive, and may\nrequire road closures. This paper proposes MamV2XCalib, the first V2X-based\ninfrastructure camera calibration method with the assistance of vehicle-side\nLiDAR. MamV2XCalib only requires autonomous vehicles equipped with LiDAR to\ndrive near the cameras to be calibrated in the infrastructure, without the need\nfor specific reference objects or manual intervention. We also introduce a new\ntargetless LiDAR-camera calibration method, which combines multi-scale features\nand a 4D correlation volume to estimate the correlation between vehicle-side\npoint clouds and roadside images. We model the temporal information and\nestimate the rotation angles with Mamba, effectively addressing calibration\nfailures in V2X scenarios caused by defects in the vehicle-side data (such as\nocclusions) and large differences in viewpoint. We evaluate MamV2XCalib on the\nV2X-Seq and TUMTraf-V2X real-world datasets, demonstrating the effectiveness\nand robustness of our V2X-based automatic calibration approach. Compared to\nprevious LiDAR-camera methods designed for calibration on one car, our approach\nachieves better and more stable calibration performance in V2X scenarios with\nfewer parameters. The code is available at\nhttps://github.com/zhuyaoye/MamV2XCalib.", "comment": "ICCV25 poster", "pdf_url": "http://arxiv.org/pdf/2507.23595v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23083", "title": "Context-aware Rotary Position Embedding", "authors": ["Ali Veisi", "Delaram Fartoot", "Hamidreza Amirzadeh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      4 pages, 1 table", "url": "http://arxiv.org/abs/2507.23083v1", "summary": "Positional encoding is a vital component of Transformer architectures,\nenabling models to incorporate sequence order into self-attention mechanisms.\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\nto their compatibility with relative position encoding and computational\nefficiency. However, RoPE relies on static, input-independent sinusoidal\nfrequency patterns, limiting its ability to model context-sensitive\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\nEmbedding), a novel generalization of RoPE that dynamically generates\nhead-specific frequency patterns conditioned on token embeddings. This design\nintroduces token- and context-sensitive positional representations while\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\ninput-dependent phase shifts using a bounded transformation of token embeddings\nand integrates them into the rotary mechanism across attention heads. We\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\nnext-token prediction tasks. Experimental results show that CARoPE consistently\noutperforms RoPE and other common positional encoding baselines, achieving\nsignificantly lower perplexity, even at longer context lengths. Additionally,\nCARoPE enables faster training throughput without sacrificing model stability.\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\nefficient upgrade to existing positional encoding strategies in Transformer\nmodels.", "comment": "4 pages, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.23083v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2402.01124", "title": "TransFR: Transferable Federated Recommendation with Adapter Tuning on Pre-trained Language Models", "authors": ["Honglei Zhang", "Zhiwei Li", "Haoxuan Li", "Xin Zhou", "Jie Zhang", "Yidong Li"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.01124v2", "summary": "Federated recommendations (FRs), facilitating multiple local clients to\ncollectively learn a global model without disclosing user private data, have\nemerged as a prevalent on-device service. In conventional FRs, a dominant\nparadigm is to utilize discrete identities to represent clients and items,\nwhich are then mapped to domain-specific embeddings to participate in model\ntraining. Despite considerable performance, we reveal three inherent\nlimitations that can not be ignored in federated settings, i.e.,\nnon-transferability across domains, ineffectiveness in cold-start settings, and\npotential privacy violations during federated training. To this end, we propose\na transferable federated recommendation model, TransFR, which delicately\nincorporates the general capabilities empowered by pre-trained models and the\npersonalized abilities by fine-tuning local private data. Specifically, it\nfirst learns domain-agnostic representations of items by exploiting pre-trained\nmodels with public textual corpora. To tailor for FR tasks, we further\nintroduce efficient federated adapter-tuning and test-time adaptation\nmechanisms, which facilitate personalized local adapters for each client by\nfitting their private data distributions. We theoretically prove the advantages\nof incorporating adapter tuning in FRs regarding both effectiveness and\nprivacy. Through extensive experiments, we show that our TransFR model\nsurpasses several state-of-the-art FRs on transferability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.01124v2", "cate": "cs.IR", "date": "2024-02-02", "updated": "2025-07-31"}
{"id": "2506.17114", "title": "Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models", "authors": ["Dadi Guo", "Jiayu Liu", "Zhiyuan Fan", "Zhitao He", "Haoran Li", "Yumeng Wang", "Yi R. Fung"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17114v3", "summary": "Large reasoning models (e.g., R1, o3) have demonstrated remarkable\nmathematical problem-solving abilities. However, the high reported accuracy of\nthese advanced models on popular datasets, reliance on purely numerical\nevaluation and potential benchmark leakage, often masks their true reasoning\nshortcomings. To address this, we propose leveraging the inherent rigor and\nmethodological complexity of mathematical proofs as a diagnostic tool to expose\nthese hidden failures. Specifically, we introduce the RFMDataset (Reveal\nFailure Modes), a collection of 200 diverse mathematical proof problems, and\nthoroughly evaluate advanced models' performance on it. Our in-depth analysis\nof their failures uncovers 10 fine-grained error types, which shows fundamental\nlimitations in current large reasoning models: 1) large reasoning models\ngrapple profoundly with mathematical proofs, with some generating entirely\ncorrect proofs for less than 20% of problems and failing even on basic ones; 2)\nmodels exhibit a diverse spectrum of reasoning failures, prominently\ndemonstrating the lack of guarantees for the correctness and rigor of\nsingle-step reasoning; and 3) models show hallucination and incompleteness\nduring the reasoning process. Our findings reveal that models' self-reflection\nis insufficient to resolve the current logical dilemmas, necessitating\nformalized and fine-grained logical training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17114v3", "cate": "cs.AI", "date": "2025-06-20", "updated": "2025-07-31"}
{"id": "2507.23768", "title": "Formal Bayesian Transfer Learning via the Total Risk Prior", "authors": ["Nathan Wycoff", "Ali Arab", "Lisa O. Singh"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23768v1", "summary": "In analyses with severe data-limitations, augmenting the target dataset with\ninformation from ancillary datasets in the application domain, called source\ndatasets, can lead to significantly improved statistical procedures. However,\nexisting methods for this transfer learning struggle to deal with situations\nwhere the source datasets are also limited and not guaranteed to be\nwell-aligned with the target dataset. A typical strategy is to use the\nempirical loss minimizer on the source data as a prior mean for the target\nparameters, which places the estimation of source parameters outside of the\nBayesian formalism. Our key conceptual contribution is to use a risk minimizer\nconditional on source parameters instead. This allows us to construct a single\njoint prior distribution for all parameters from the source datasets as well as\nthe target dataset. As a consequence, we benefit from full Bayesian uncertainty\nquantification and can perform model averaging via Gibbs sampling over\nindicator variables governing the inclusion of each source dataset. We show how\na particular instantiation of our prior leads to a Bayesian Lasso in a\ntransformed coordinate system and discuss computational techniques to scale our\napproach to moderately sized datasets. We also demonstrate that recently\nproposed minimax-frequentist transfer learning techniques may be viewed as an\napproximate Maximum a Posteriori approach to our model. Finally, we demonstrate\nsuperior predictive performance relative to the frequentist baseline on a\ngenetics application, especially when the source data are limited.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23768v1", "cate": "stat.ML", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23597", "title": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction", "authors": ["Zijian Dong", "Longteng Duan", "Jie Song", "Michael J. Black", "Andreas Geiger"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight), Project Page: this https URL", "url": "http://arxiv.org/abs/2507.23597v1", "summary": "We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian\navatars from a single-view image. The main challenge lies in inferring unseen\nappearance and geometric details while ensuring 3D consistency and realism.\nMost previous methods rely on 2D diffusion models to synthesize unseen views;\nhowever, these generated views are sparse and inconsistent, resulting in\nunrealistic 3D artifacts and blurred appearance. To address these limitations,\nwe leverage a generative avatar model, that can generate diverse 3D avatars by\nsampling deformed Gaussians from a learned prior distribution. Due to the\nlimited amount of 3D training data such a 3D model alone cannot capture all\nimage details of unseen identities. Consequently, we integrate it as a prior,\nensuring 3D consistency by projecting input images into its latent space and\nenforcing additional 3D appearance and geometric constraints. Our novel\napproach formulates Gaussian avatar creation as a model inversion process by\nfitting the generative avatar to synthetic views from 2D diffusion models. The\ngenerative avatar provides a meaningful initialization for model fitting,\nenforces 3D regularization, and helps in refining pose estimation. Experiments\nshow that our method surpasses state-of-the-art techniques and generalizes well\nto real-world scenarios. Our Gaussian avatars are also inherently animatable", "comment": "ICCV 2025 (Highlight), Project Page: https://zj-dong.github.io/MoGA/", "pdf_url": "http://arxiv.org/pdf/2507.23597v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23135", "title": "ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans", "authors": ["Ananya Sadana", "Yash Kumar Lal", "Jiawei Zhou"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23135v1", "summary": "Understanding causal relationships across modalities is a core challenge for\nmultimodal models operating in real-world environments. We introduce ISO-Bench,\na benchmark for evaluating whether models can infer causal dependencies between\nvisual observations and procedural text. Each example presents an image of a\ntask step and a text snippet from a plan, with the goal of deciding whether the\nvisual step occurs before or after the referenced text step. Evaluation results\non ten frontier vision-language models show underwhelming performance: the best\nzero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest\ngains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further\nhighlights concrete directions for improving causal understanding in multimodal\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23135v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2411.06254", "title": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking with Large Language Models", "authors": ["Minghan Li", "Eric Gaussier", "Juntao Li", "Guodong Zhou"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06254v2", "summary": "The emergence of large language models (LLMs) such as Llama has significantly\nadvanced neural information retrieval (IR). However, applying LLMs to long\ndocument reranking remains computationally expensive and may be ineffective.\nMoreover, the internal behavior of LLMs during document relevance judgment is\nstill underexplored. In this paper, we begin with an in-depth analysis of\ndecoder-only LLM attention patterns and find that several attention heads\nconsistently align with relevance signals, yet this alignment deteriorates as\nirrelevant content increases. Motivated by this observation, we revisit and\nextend the block selection paradigm, introducing KeyB2, a scalable reranking\nframework that combines block pre-selection with powerful decoder-only LLMs.\nKeyB2 generalizes the selection stage to support BM25, cross-encoder, and\nbi-encoder, and adapts LLM to compute fine-grained relevance scores. We further\nintroduce a new bi-encoder strategy that performs strongly and efficiently.\nExtensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh\ndemonstrate that KeyB2 outperforms baselines including RankLLaMA,\nRankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on\nTREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking\nlatency compared with RankLLaMA by over 83% and memory usage by over 74%,\npositioning it as a practical and effective solution for long document ranking\nwith LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06254v2", "cate": "cs.IR", "date": "2024-11-09", "updated": "2025-07-31"}
{"id": "2507.07820", "title": "AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift", "authors": ["Eunsu Baek", "Keondo Park", "Jeonggil Ko", "Min-hwan Oh", "Taesik Gong", "Hyung-Sin Kim"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07820v2", "summary": "Current AI advances largely rely on scaling neural models and expanding\ntraining datasets to achieve generalization and robustness. Despite notable\nsuccesses, this paradigm incurs significant environmental, economic, and\nethical costs, limiting sustainability and equitable access. Inspired by\nbiological sensory systems, where adaptation occurs dynamically at the input\n(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive\nsensing as a necessary and foundational shift. Adaptive sensing proactively\nmodulates sensor parameters (e.g., exposure, sensitivity, multimodal\nconfigurations) at the input level, significantly mitigating covariate shifts\nand improving efficiency. Empirical evidence from recent studies demonstrates\nthat adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass\nsubstantially larger models (e.g., OpenCLIP-H) trained with significantly more\ndata and compute. We (i) outline a roadmap for broadly integrating adaptive\nsensing into real-world applications spanning humanoid, healthcare, autonomous\nsystems, agriculture, and environmental monitoring, (ii) critically assess\ntechnical and ethical integration challenges, and (iii) propose targeted\nresearch directions, such as standardized benchmarks, real-time adaptive\nalgorithms, multimodal integration, and privacy-preserving methods.\nCollectively, these efforts aim to transition the AI community toward\nsustainable, robust, and equitable artificial intelligence systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07820v2", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2306.01654", "title": "Insights into Closed-form IPM-GAN Discriminator Guidance for Diffusion Modeling", "authors": ["Aadithya Srikanth", "Siddarth Asokan", "Nishanth Shetty", "Chandra Sekhar Seelamantula"], "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.01654v2", "summary": "Diffusion models are a state-of-the-art generative modeling framework that\ntransform noise to images via Langevin sampling, guided by the score, which is\nthe gradient of the logarithm of the data distribution. Recent works have shown\nempirically that the generation quality can be improved when guided by\nclassifier network, which is typically the discriminator trained in a\ngenerative adversarial network (GAN) setting. In this paper, we propose a\ntheoretical framework to analyze the effect of the GAN discriminator on\nLangevin-based sampling, and show that the IPM-GAN optimization can be seen as\none of smoothed score-matching, wherein the scores of the data and the\ngenerator distributions are convolved with the kernel function associated with\nthe IPM. The proposed approach serves to unify score-based training and\noptimization of IPM-GANs. Based on these insights, we demonstrate that\nclosed-form kernel-based discriminator guidance, results in improvements (in\nterms of CLIP-FID and KID metrics) when applied atop baseline diffusion models.\nWe demonstrate these results on the denoising diffusion implicit model (DDIM)\nand latent diffusion model (LDM) settings on various standard datasets. We also\nshow that the proposed approach can be combined with existing\naccelerated-diffusion techniques to improve latent-space image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.01654v2", "cate": "cs.LG", "date": "2023-06-02", "updated": "2025-07-31"}
{"id": "2507.23599", "title": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for Geometric Structure Preservation", "authors": ["Yuchen Zhou", "Yan Luo", "Xiangang Wang", "Xingjian Gu", "Mingzhou Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23599v1", "summary": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23599v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23158", "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal", "authors": ["Yuhan Liu", "Michael J. Q. Zhang", "Eunsol Choi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Earlier version of this paper was presented at 2nd Workshop on Models of Human Feedback for AI Alignment (MoFA), ICML 2025", "url": "http://arxiv.org/abs/2507.23158v1", "summary": "Once language models (LMs) are deployed, they can interact with users\nlong-term, ideally evolving continuously based on their feedback. Asking for\ndirect user feedback can be disruptive; thus, we study harvesting user feedback\nfrom user-LM interaction logs. We study implicit user feedback in two user-LM\ninteraction datasets (WildChat and LMSYS). First, we analyze user feedback in\nthe user-LLM conversation trajectory, providing insights into when and why such\nfeedback occurs. Second, we study harvesting learning signals from such\nimplicit user feedback. We find that the contents of user feedback (e.g., user\nwanted clarification), not just the polarity (e.g., users were unhappy with the\nprevious model response), can improve model performance in short human-designed\nquestions (MTBench) but not on longer and more complex questions (WildBench).\nWe also find that the usefulness of user feedback is largely tied to the\nquality of the user's initial prompt. Together, we provide an in-depth study of\nimplicit user feedback, showing its potential and limitations.", "comment": "Earlier version of this paper was presented at 2nd Workshop on Models\n  of Human Feedback for AI Alignment (MoFA), ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.23158v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.12311", "title": "An Ecosystem for Ontology Interoperability", "authors": ["Zhangcheng Qiang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      5 pages, 8 figures", "url": "http://arxiv.org/abs/2507.12311v3", "summary": "Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nand data integration in real-world applications. A case study of sensor\nobservation in the building domain validates the usefulness of the proposed\necosystem.", "comment": "5 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.12311v3", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2507.22949", "title": "Convergence analysis of a second-order SAV-ZEC scheme for the Cahn-Hilliard-Navier-Stokes system", "authors": ["Jingwei Sun", "Zeyu Xia", "Wei Zhang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22949v1", "summary": "Incorporating the scalar auxiliary variable (SAV) method and the zero energy\ncontribution (ZEC) technique, we analyze a linear and fully decoupled numerical\nscheme for the Cahn-Hilliard-Naiver-Stokes (CHNS) system. More precisely, the\nfully discrete scheme combines the marker-and-cell (MAC) finite difference\nspatial approximation and BDF2 temporal discretization, as well as the\nAdams-Bashforth extrapolation for the nonlinear terms, based on the SAV-ZEC\nreformulation. A pressure correction approach is applied to decouple the Stokes\nequation. Only constant-coefficient Poisson-like solvers are needed in the\nimplementation for the resulting numerical system. The numerical scheme is\nunconditionally stable with respect to a rewritten total energy functional,\nrepresented in terms of one auxiliary variable in the double-well potential,\nanother auxiliary variable to balance all the nonlinear and coupled terms, the\nsurface energy in the original phase variable, combined with the kinematic\nenergy part. Specifically, the error estimate for the phase variable in the\n$\\ell^{\\infty}(0,T;H_h^1)\\cap\\ell^2(0,T;H_h^3)$ norm, the velocity variable in\nthe $\\ell^{\\infty}(0,T;\\ell^2)\\cap\\ell^2(0,T;H_h^1)$ norm, is derived with\noptimal convergence rates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22949v1", "cate": "math.NA", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2507.21872", "title": "MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors", "authors": ["Shouyi Lu", "Zihan Lin", "Chao Lu", "Huanran Wang", "Guirong Zhuo", "Lianqing Zheng"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21872v3", "summary": "Autonomous driving systems rely heavily on multimodal perception data to\nunderstand complex environments. However, the long-tailed distribution of\nreal-world data hinders generalization, especially for rare but safety-critical\nvehicle categories. To address this challenge, we propose MultiEditor, a\ndual-branch latent diffusion framework designed to edit images and LiDAR point\nclouds in driving scenarios jointly. At the core of our approach is introducing\n3D Gaussian Splatting (3DGS) as a structural and appearance prior for target\nobjects. Leveraging this prior, we design a multi-level appearance control\nmechanism--comprising pixel-level pasting, semantic-level guidance, and\nmulti-branch refinement--to achieve high-fidelity reconstruction across\nmodalities. We further propose a depth-guided deformable cross-modality\ncondition module that adaptively enables mutual guidance between modalities\nusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.\nExtensive experiments demonstrate that MultiEditor achieves superior\nperformance in visual and geometric fidelity, editing controllability, and\ncross-modality consistency. Furthermore, generating rare-category vehicle data\nwith MultiEditor substantially enhances the detection accuracy of perception\nmodels on underrepresented classes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21872v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2406.13265", "title": "Molecule Graph Networks with Many-body Equivariant Interactions", "authors": ["Zetian Mao", "Chuan-Shen Hu", "Jiawen Li", "Chen Liang", "Diptesh Das", "Masato Sumita", "Kelin Xia", "Koji Tsuda"], "categories": ["cs.LG", "cond-mat.mtrl-sci"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.13265v3", "summary": "Message passing neural networks have demonstrated significant efficacy in\npredicting molecular interactions. Introducing equivariant vectorial\nrepresentations augments expressivity by capturing geometric data symmetries,\nthereby improving model accuracy. However, two-body bond vectors in opposition\nmay cancel each other out during message passing, leading to the loss of\ndirectional information on their shared node. In this study, we develop\nEquivariant N-body Interaction Networks (ENINet) that explicitly integrates l =\n1 equivariant many-body interactions to enhance directional symmetric\ninformation in the message passing scheme. We provided a mathematical analysis\ndemonstrating the necessity of incorporating many-body equivariant interactions\nand generalized the formulation to $N$-body interactions. Experiments indicate\nthat integrating many-body equivariant representations enhances prediction\naccuracy across diverse scalar and tensorial quantum chemical properties.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.13265v3", "cate": "cs.LG", "date": "2024-06-19", "updated": "2025-07-31"}
{"id": "2507.23601", "title": "Mamba-based Efficient Spatio-Frequency Motion Perception for Video Camouflaged Object Detection", "authors": ["Xin Li", "Keren Fu", "Qijun Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 11 figures", "url": "http://arxiv.org/abs/2507.23601v1", "summary": "Existing video camouflaged object detection (VCOD) methods primarily rely on\nspatial appearance features to perceive motion cues for breaking camouflage.\nHowever, the high similarity between foreground and background in VCOD results\nin limited discriminability of spatial appearance features (e.g., color and\ntexture), restricting detection accuracy and completeness. Recent studies\ndemonstrate that frequency features can not only enhance feature representation\nto compensate for appearance limitations but also perceive motion through\ndynamic variations in frequency energy. Furthermore, the emerging state space\nmodel called Mamba, enables efficient perception of motion cues in frame\nsequences due to its linear-time long-sequence modeling capability. Motivated\nby this, we propose a novel visual camouflage Mamba (Vcamba) based on\nspatio-frequency motion perception that integrates frequency and spatial\nfeatures for efficient and accurate VCOD. Specifically, we propose a receptive\nfield visual state space (RFVSS) module to extract multi-scale spatial features\nafter sequence modeling. For frequency learning, we introduce an adaptive\nfrequency component enhancement (AFE) module with a novel frequency-domain\nsequential scanning strategy to maintain semantic consistency. Then we propose\na space-based long-range motion perception (SLMP) module and a frequency-based\nlong-range motion perception (FLMP) module to model spatio-temporal and\nfrequency-temporal sequences in spatial and frequency phase domains. Finally,\nthe space and frequency motion fusion module (SFMF) integrates dual-domain\nfeatures for unified motion representation. Experimental results show that our\nVcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2\ndatasets with lower computation cost, confirming the superiority of Vcamba. Our\ncode is available at: https://github.com/BoydeLi/Vcamba.", "comment": "11 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.23601v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23211", "title": "Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Zhe Cui"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23211v1", "summary": "Large Language Models exhibit powerful few-shot in-context learning (ICL)\ncapabilities, but the performance is highly sensitive to provided examples.\n  Recent research has focused on retrieving corresponding examples for each\ninput query, not only enhancing the efficiency and scalability of the learning\nprocess but also mitigating inherent biases in manual example selection.\n  However, these studies have primarily emphasized leveraging Positive samples\nwhile overlooking the additional information within Negative samples for\ncontextual learning.\n  We propose a novel method that utilizes Negative samples to better select\nPositive sample examples, thereby enhancing the performance of few-shot ICL.\nInitially, we construct Positive and Negative sample corpora based on\nZero-Shot-Cot. Then, during inference, we employ a semantic similarity-based\napproach to select the most similar examples from both the Positive and\nNegative corpora for a given query. Subsequently, we further retrieve Positive\nexamples from the Positive sample corpus based on semantic similarity to the\nNegative examples, then concatenating them with the previously selected\nPositive examples to serve as ICL demonstrations. Experimental results\ndemonstrate that our approach surpasses methods solely relying on the most\nsimilar positive examples for context, validating that the additional\ninformation in negative samples aids in enhancing ICL performance through\nimproved Positive sample selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23211v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.18518", "title": "Transform Before You Query: A Privacy-Preserving Approach for Vector Retrieval with Embedding Space Alignment", "authors": ["Ruiqi He", "Zekun Fei", "Jiaqi Li", "Xinyuan Zhu", "Biao Yi", "Siyi Lv", "Weijie Liu", "Zheli Liu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18518v2", "summary": "Vector Database (VDB) can efficiently index and search high-dimensional\nvector embeddings from unstructured data, crucially enabling fast semantic\nsimilarity search essential for modern AI applications like generative AI and\nrecommendation systems. Since current VDB service providers predominantly use\nproprietary black-box models, users are forced to expose raw query text to them\nvia API in exchange for the vector retrieval services. Consequently, if query\ntext involves confidential records from finance or healthcare domains, this\nmechanism inevitably leads to critical leakage of user's sensitive information.\nTo address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed\n\\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector\nretrieval framework that leverages the alignment relationship between the\nsemantic spaces of different embedding models to derive approximate embeddings\nfor the query text. STEER performs the retrieval using the approximate\nembeddings within the original VDB and requires no modifications to the server\nside. Our theoretical and experimental analyses demonstrate that STEER\neffectively safeguards query text privacy while maintaining the retrieval\naccuracy. Even though approximate embeddings are approximations of the\nembeddings from proprietary models, they still prevent the providers from\nrecovering the query text through Embedding Inversion Attacks (EIAs). Extensive\nexperimental results show that Recall@100 of STEER can basically achieve a\ndecrease of less than 5\\%. Furthermore, even when searching within a text\ncorpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher\nthan current baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18518v2", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2507.23099", "title": "Hybrid Shifted Gegenbauer Integral-Pseudospectral Method for Solving Time-Fractional Benjamin-Bona-Mahony-Burgers Equation", "authors": ["Kareem T. Elgindy"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23099v1", "summary": "This paper presents a high-order hybrid shifted Gegenbauer\nintegral-pseudospectral (HSG-IPS) method for solving the time-fractional\nBenjamin-Bona-Mahony-Burgers (FBBMB) equation. A key innovation of our approach\nis the transformation of the original equation into a fractional\npartial-integro differential form that contains only a first-order derivative,\nwhich can be accurately approximated using a first-order shifted Gegenbauer\ndifferentiation matrix (SGDM), while all other terms in the transformed\nequation are resolved using highly accurate quadrature rules. The method\ncombines several advanced numerical techniques including the shifted Gegenbauer\npseudospectral (SGPS) method, Gegenbauer-based fractional approximation (GBFA),\nshifted Gegenbauer integration matrix (SGIM), shifted Gegenbauer integration\nrow vector (SGIRV), and SGDM to achieve spectral accuracy. Numerical\nexperiments demonstrate that the HSG-IPS method outperforms existing numerical\napproaches, achieving significantly lower average absolute errors (AAEs) with\ncomputational times as low as 0.04-0.05 seconds. The method's robustness is\nvalidated across various fractional orders, showing excellent agreement with\nanalytical solutions. The transformation strategy effectively circumvents the\nnumerical instability associated with direct approximation of high-order\nderivatives in the original equation, while the use of shifted Gegenbauer (SG)\npolynomials and barycentric representations ensures numerical stability and\nefficiency. This work provides a powerful computational framework for modeling\nwave propagation, dispersion, and nonlinearity in fractional calculus\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23099v1", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.21875", "title": "Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21875v3", "summary": "Pain is a complex and pervasive condition that affects a significant portion\nof the population. Accurate and consistent assessment is essential for\nindividuals suffering from pain, as well as for developing effective management\nstrategies in a healthcare system. Automatic pain assessment systems enable\ncontinuous monitoring, support clinical decision-making, and help minimize\npatient distress while mitigating the risk of functional deterioration.\nLeveraging physiological signals offers objective and precise insights into a\nperson's state, and their integration in a multimodal framework can further\nenhance system performance. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed approach introduces \\textit{Tiny-BioMoE}, a lightweight pretrained\nembedding model for biosignal analysis. Trained on $4.4$ million biosignal\nimage representations and consisting of only $7.3$ million parameters, it\nserves as an effective tool for extracting high-quality embeddings for\ndownstream tasks. Extensive experiments involving electrodermal activity, blood\nvolume pulse, respiratory signals, peripheral oxygen saturation, and their\ncombinations highlight the model's effectiveness across diverse modalities in\nautomatic pain recognition tasks. \\textit{\\textcolor{blue}{The model's\narchitecture (code) and weights are available at\nhttps://github.com/GkikasStefanos/Tiny-BioMoE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21875v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2407.01621", "title": "Deciphering interventional dynamical causality from non-intervention complex systems", "authors": ["Jifan Shi", "Yang Li", "Juan Zhao", "Siyang Leng", "Rui Bao", "Kazuyuki Aihara", "Luonan Chen", "Wei Lin"], "categories": ["cs.LG", "q-bio.QM", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.01621v2", "summary": "Detecting and quantifying causality is a focal topic in the fields of\nscience, engineering, and interdisciplinary studies. However, causal studies on\nnon-intervention systems attract much attention but remain extremely\nchallenging. Delay-embedding technique provides a promising approach. In this\nstudy, we propose a framework named Interventional Dynamical Causality (IntDC)\nin contrast to the traditional Constructive Dynamical Causality (ConDC). ConDC,\nincluding Granger causality, transfer entropy and convergence of cross-mapping,\nmeasures the causality by constructing a dynamical model without considering\ninterventions. A computational criterion, Interventional Embedding Entropy\n(IEE), is proposed to measure causal strengths in an interventional manner. IEE\nis an intervened causal information flow but in the delay-embedding space.\nFurther, the IEE theoretically and numerically enables the deciphering of IntDC\nsolely from observational (non-interventional) time-series data, without\nrequiring any knowledge of dynamical models or real interventions in the\nconsidered system. In particular, IEE can be applied to rank causal effects\naccording to their importance and construct causal networks from data. We\nconducted numerical experiments to demonstrate that IEE can find causal edges\naccurately, eliminate effects of confounding, and quantify causal strength\nrobustly over traditional indices. We also applied IEE to real-world tasks. IEE\nperformed as an accurate and robust tool for causal analyses solely from the\nobservational data. The IntDC framework and IEE algorithm provide an efficient\napproach to the study of causality from time series in diverse non-intervention\ncomplex systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.01621v2", "cate": "cs.LG", "date": "2024-06-29", "updated": "2025-07-30"}
{"id": "2507.23643", "title": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free Training Framework for Spiking Neural Networks", "authors": ["Changqing Xu", "Ziqiang Yang", "Yi Liu", "Xinfang Liao", "Guiqi Mo", "Hao Zeng", "Yintang Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23643v1", "summary": "Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23643v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23247", "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication", "authors": ["Sneha Oram", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23247v1", "summary": "There has been an increase in recent advancements in the explainability and\ndevelopment of personalized chatbots for mental health. However, the reasoning\naspects for explainability and dialogue discourse have not been explored\npreviously for mental health. Hence, we are investigating the pragmatic\nreasoning capability of large language models (LLMs) in this domain. We\nintroduce P-ReMe dataset, and propose a modified definition for the pragmatic\nphenomena of implicature (implied meaning) and presupposition (implicit\nassumption) in mental health. Following the definition, we formulate two tasks\nin implicature and one task in presupposition. To benchmark the dataset and the\npresented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and\nQwen. The results of the experiments suggest that Mistral and Qwen show\nsubstantial reasoning capabilities in the domain. In addition, we also propose\nStiPRompts to study the stigma around mental health with the state-of-the-art\nLLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings\nshow that Claude-3.5-haiku deals with the stigma more responsibly compared to\nthe other two LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23247v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22879", "title": "RecGPT Technical Report", "authors": ["Chao Yi", "Dian Chen", "Gaoyang Guo", "Jiakai Tang", "Jian Wu", "Jing Yu", "Mao Zhang", "Sunhao Dai", "Wen Chen", "Wenjun Yang", "Yuning Jiang", "Zhujin Gao", "Bo Zheng", "Chi Li", "Dimin Wang", "Dixuan Wang", "Fan Li", "Fan Zhang", "Haibin Chen", "Haozhuang Liu", "Jialin Zhu", "Jiamang Wang", "Jiawei Wu", "Jin Cui", "Ju Huang", "Kai Zhang", "Kan Liu", "Lang Tian", "Liang Rao", "Longbin Li", "Lulu Zhao", "Na He", "Peiyang Wang", "Qiqi Huang", "Tao Luo", "Wenbo Su", "Xiaoxiao He", "Xin Tong", "Xu Chen", "Xunke Xi", "Yang Li", "Yaxuan Wu", "Yeqiu Yang", "Yi Hu", "Yinnan Song", "Yuchen Li", "Yujie Luo", "Yujin Yuan", "Yuliang Yan", "Zhengyang Wang", "Zhibo Xiao", "Zhixin Ma", "Zile Zhou", "Ziqi Zhang"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22879v2", "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22879v2", "cate": "cs.IR", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23195", "title": "$hp$-adaptive finite element simulation of a static anti-plane shear crack in a nonlinear strain-limiting elastic solid", "authors": ["S. M. Mallikarjunaiah", "Pavithra Venkatachalapthy"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23195v1", "summary": "An $hp$-adaptive continuous Galerkin finite element method is developed to\nanalyze a static anti-plane shear crack embedded in a nonlinear,\nstrain-limiting elastic body. The geometrically linear material is described by\na constitutive law relating stress and strain that is algebraically nonlinear.\nIn this investigation, the constitutive relation utilized is \\textit{uniformly\nbounded}, \\textit{monotone}, \\textit{coercive}, and \\textit{Lipschitz\ncontinuous}, ensuring the well-posedness of the mathematical model. The\ngoverning equation, derived from the balance of linear momentum coupled with\nthe nonlinear constitutive relationship, is formulated as a second-order\nquasi-linear elliptic partial differential equation. For a body with an edge\ncrack, this governing equation is augmented with a classical traction-free\nboundary condition on the crack faces. An $hp$-adaptive finite element scheme\nis proposed for the numerical approximation of the resulting boundary value\nproblem. The adaptive strategy is driven by a dual-component error estimation\nscheme: mesh refinement ($h$-adaptivity) is guided by a residual-based a\nposteriori error indicator of the \\textit{Kelly type}, while the local\npolynomial degree ($p$-adaptivity) is adjusted based on an estimator of the\nlocal solution regularity. The performance, accuracy, and convergence\ncharacteristics of the proposed method are demonstrated through numerical\nexperiments. The structure of the regularized crack-tip fields is examined for\nvarious modeling parameters. Furthermore, the presented framework establishes a\nrobust foundation for extension to more complex and computationally demanding\nproblems, including quasi-static and dynamic crack propagation in brittle\nmaterials.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23195v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21881", "title": "Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2507.21875", "url": "http://arxiv.org/abs/2507.21881v3", "summary": "Pain is a multifaceted phenomenon that affects a substantial portion of the\npopulation. Reliable and consistent evaluation benefits those experiencing pain\nand underpins the development of effective and advanced management strategies.\nAutomatic pain-assessment systems deliver continuous monitoring, inform\nclinical decision-making, and aim to reduce distress while preventing\nfunctional decline. By incorporating physiological signals, these systems\nprovide objective, accurate insights into an individual's condition. This study\nhas been submitted to the \\textit{Second Multimodal Sensing Grand Challenge for\nNext-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline\nthat leverages electrodermal activity signals as input modality. Multiple\nrepresentations of the signal are created and visualized as waveforms, and they\nare jointly visualized within a single multi-representation diagram. Extensive\nexperiments incorporating various processing and filtering techniques, along\nwith multiple representation combinations, demonstrate the effectiveness of the\nproposed approach. It consistently yields comparable, and in several cases\nsuperior, results to traditional fusion methods, establishing it as a robust\nalternative for integrating different signal representations or modalities.", "comment": "arXiv admin note: text overlap with arXiv:2507.21875", "pdf_url": "http://arxiv.org/pdf/2507.21881v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2407.03080", "title": "Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios", "authors": ["Patricia A. Apellániz", "Ana Jiménez", "Borja Arroyo Galende", "Juan Parras", "Santiago Zazo"], "categories": ["cs.LG", "cs.AI", "I.2.0"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 6 Figures", "url": "http://arxiv.org/abs/2407.03080v2", "summary": "While synthetic tabular data generation using Deep Generative Models (DGMs)\noffers a compelling solution to data scarcity and privacy concerns, their\neffectiveness relies on the availability of substantial training data, often\nlacking in real-world scenarios. To overcome this limitation, we propose a\nnovel methodology that explicitly integrates artificial inductive biases into\nthe generative process to improve data quality in low-data regimes. Our\nframework leverages transfer learning and meta-learning techniques to construct\nand inject informative inductive biases into DGMs. We evaluate four approaches\n(pre-training, model averaging, Model-Agnostic Meta-Learning (MAML), and Domain\nRandomized Search (DRS)) and analyze their impact on the quality of the\ngenerated text. Experimental results show that incorporating inductive bias\nsubstantially improves performance, with transfer learning methods\noutperforming meta-learning, achieving up to 60\\% gains in Jensen-Shannon\ndivergence. The methodology is model-agnostic and especially relevant in\ndomains such as healthcare and finance, where high-quality synthetic data are\nessential, and data availability is often limited.", "comment": "19 pages, 6 Figures", "pdf_url": "http://arxiv.org/pdf/2407.03080v2", "cate": "cs.LG", "date": "2024-07-03", "updated": "2025-07-31"}
{"id": "2507.23652", "title": "Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis", "authors": ["Kunpeng Qiu", "Zhiying Zhou", "Yongxin Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by MICCAI2025", "url": "http://arxiv.org/abs/2507.23652v1", "summary": "Medical image annotation is constrained by privacy concerns and\nlabor-intensive labeling, significantly limiting the performance and\ngeneralization of segmentation models. While mask-controllable diffusion models\nexcel in synthesis, they struggle with precise lesion-mask alignment. We\npropose \\textbf{Adaptively Distilled ControlNet}, a task-agnostic framework\nthat accelerates training and optimization through dual-model distillation.\nSpecifically, during training, a teacher model, conditioned on mask-image\npairs, regularizes a mask-only student model via predicted noise alignment in\nparameter space, further enhanced by adaptive regularization based on\nlesion-background ratios. During sampling, only the student model is used,\nenabling privacy-preserving medical image generation. Comprehensive evaluations\non two distinct medical datasets demonstrate state-of-the-art performance:\nTransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves\n2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code\nis available at GitHub.", "comment": "Accepted by MICCAI2025", "pdf_url": "http://arxiv.org/pdf/2507.23652v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23279", "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models", "authors": ["Zunhai Su", "Qingyuan Li", "Hao Zhang", "YuLei Qian", "Yuchen Xie", "Kehong Yuan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23279v1", "summary": "Sparsely activated Mixture-of-Experts (MoE) models have shown promise in\nenhancing the learning capacity of large language models (LLMs). Leveraging the\nintrinsic importance differences among experts, recent research has explored\nexpert-level compression techniques to improve the efficiency of MoE LLMs.\nHowever, existing approaches often rely on empirical criteria to identify\ncritical experts, lacking a deeper exploration and understanding of the\nheterogeneous importance of experts. In this study, we present the first\ndiscovery and investigation of a distinct subset of experts that play a crucial\nrole in the underlying mechanisms during the model's forward inference. These\nexperts are prevalent in open-source MoE LLMs, and despite their limited\nnumber, pruning them leads to a significant decline in model performance (e.g.,\npruning three causes Qwen3-30B-A3B to produce repetitive and uninformative\noutputs). We refer to these experts as Super Experts (SEs). Our comprehensive\nanalysis provides progressively deeper insights into SEs. (i) SEs are\ncharacterized by rare but extreme activation outliers in the output of the\ndown_proj, which give rise to massive activations in the hidden states between\ndecoder layers. Moreover, the distribution of SEs remains model-specific and is\nunaffected by post-training processes. (ii) By pruning SEs, we assess their\nsignificance across a variety of tasks, revealing their considerable impact on\nthe model's overall performance, particularly in mathematical reasoning. (iii)\nWe further enhance our understanding of the influence of SEs compression. Our\nfindings confirm that MoE LLMs rely on SEs to induce attention sinks, which are\ncrucial for the distribution of attention scores but are significantly\ndisrupted by SE pruning. The code is available at\nhttps://github.com/ZunhaiSu/Super-Experts-Profilling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23279v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.16725", "title": "RAVine: Reality-Aligned Evaluation for Agentic Search", "authors": ["Yilong Xu", "Xiang Long", "Zhi Zheng", "Jinhua Gao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16725v2", "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16725v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-31"}
{"id": "2507.23199", "title": "Error analysis of the projected PO method with additive inflation for the partially observed Lorenz 96 model", "authors": ["Kota Takeda"], "categories": ["math.NA", "cs.NA", "math.DS", "62M20, 62F15, 35R30, 93C55, 65C05"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23199v1", "summary": "We consider the filtering problem with the partially observed Lorenz 96\nmodel. Although the accuracy of the 3DVar filter applied to this problem has\nbeen established, that of the EnKF has not yet been. This study aims to\nestablish the error bound of a variant of the EnKF, known as the PO method. By\nintroducing the additive inflation and a projection of the background\ncovariance to the observation space, we establish the error bound of the PO\nmethod. A numerical example validates theoretical findings and shows the\npotential to extend the analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23199v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22995", "title": "Balancing Information Preservation and Disentanglement in Self-Supervised Music Representation Learning", "authors": ["Julia Wilkins", "Sivan Ding", "Magdalena Fuentes", "Juan Pablo Bello"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      In proceedings of WASPAA 2025. 4 pages, 4 figures, 1 table", "url": "http://arxiv.org/abs/2507.22995v1", "summary": "Recent advances in self-supervised learning (SSL) methods offer a range of\nstrategies for capturing useful representations from music audio without the\nneed for labeled data. While some techniques focus on preserving comprehensive\ndetails through reconstruction, others favor semantic structure via contrastive\nobjectives. Few works examine the interaction between these paradigms in a\nunified SSL framework. In this work, we propose a multi-view SSL framework for\ndisentangling music audio representations that combines contrastive and\nreconstructive objectives. The architecture is designed to promote both\ninformation fidelity and structured semantics of factors in disentangled\nsubspaces. We perform an extensive evaluation on the design choices of\ncontrastive strategies using music audio representations in a controlled\nsetting. We find that while reconstruction and contrastive strategies exhibit\nconsistent trade-offs, when combined effectively, they complement each other;\nthis enables the disentanglement of music attributes without compromising\ninformation integrity.", "comment": "In proceedings of WASPAA 2025. 4 pages, 4 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.22995v1", "cate": "cs.SD", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22359", "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models", "authors": ["Qianhong Guo", "Wei Xie", "Xiaofang Cai", "Enze Wang", "Shuoyoucheng Ma", "Kai Chen", "Xiaofeng Wang", "Baosheng Wang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22359v2", "summary": "Although large language models (LLMs) demonstrate remarkable capabilities\nacross various tasks, evaluating their capabilities remains a challenging task.\nExisting evaluation methods suffer from issues such as data contamination,\nblack-box operation, and subjective preference. These issues make it difficult\nto evaluate the LLMs' true capabilities comprehensively. To tackle these\nchallenges, we propose a novel benchmark-free evaluation paradigm,\nLLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,\nand evaluate mutually. This method integrates four key evaluation criteria:\ndynamic, transparent, objective, and professional, which existing evaluation\nmethods cannot satisfy simultaneously. Experiments on eight mainstream LLMs\nacross mathematics and programming verify the advantages of our method in\ndistinguishing LLM performance. Furthermore, our study reveals several novel\nfindings that are difficult for traditional methods to detect, including but\nnot limited to: (1) Gemini demonstrates the highest original and professional\nquestion-design capabilities among others; (2) Some LLMs exhibit\n''memorization-based answering'' by misrecognizing questions as familiar ones\nwith a similar structure; (3) LLM evaluation results demonstrate high\nconsistency (robustness).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22359v2", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2408.05177", "title": "Coarse Graining with Neural Operators for Simulating Chaotic Systems", "authors": ["Chuwei Wang", "Julius Berner", "Zongyi Li", "Di Zhou", "Jiayun Wang", "Jane Bae", "Anima Anandkumar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.05177v5", "summary": "Accurately predicting the long-term behavior of chaotic systems is crucial\nfor various applications such as climate modeling. However, achieving such\npredictions typically requires iterative computations over a dense\nspatiotemporal grid to account for the unstable nature of chaotic systems,\nwhich is expensive and impractical in many real-world situations. An\nalternative approach to such a full-resolved simulation is using a coarse grid\nand then correcting its errors through a \\textit{closure model}, which\napproximates the overall information from fine scales not captured in the\ncoarse-grid simulation. Recently, ML approaches have been used for closure\nmodeling, but they typically require a large number of training samples from\nexpensive fully-resolved simulations (FRS). In this work, we prove an even more\nfundamental limitation, i.e., the standard approach to learning closure models\nsuffers from a large approximation error for generic problems, no matter how\nlarge the model is, and it stems from the non-uniqueness of the mapping. We\npropose an alternative end-to-end learning approach using a physics-informed\nneural operator (PINO) that overcomes this limitation by not using a closure\nmodel or a coarse-grid solver. We first train the PINO model on data from a\ncoarse-grid solver and then fine-tune it with (a small amount of) FRS and\nphysics-based losses on a fine grid. The discretization-free nature of neural\noperators means that they do not suffer from the restriction of a coarse grid\nthat closure models face, and they can provably approximate the long-term\nstatistics of chaotic systems. In our experiments, our PINO model achieves a\n330x speedup compared to FRS with a relative error $\\sim 10\\%$. In contrast,\nthe closure model coupled with a coarse-grid solver is $60$x slower than PINO\nwhile having a much higher error $\\sim186\\%$ when the closure model is trained\non the same FRS dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.05177v5", "cate": "cs.LG", "date": "2024-08-09", "updated": "2025-07-30"}
{"id": "2507.23657", "title": "OmniTraj: Pre-Training on Heterogeneous Data for Adaptive and Zero-Shot Human Trajectory Prediction", "authors": ["Yang Gao", "Po-Chien Luan", "Kaouther Messaoud", "Lan Feng", "Alexandre Alahi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23657v1", "summary": "While large-scale pre-training has advanced human trajectory prediction, a\ncritical challenge remains: zero-shot transfer to unseen dataset with varying\ntemporal dynamics. State-of-the-art pre-trained models often require\nfine-tuning to adapt to new datasets with different frame rates or observation\nhorizons, limiting their scalability and practical utility. In this work, we\nsystematically investigate this limitation and propose a robust solution. We\nfirst demonstrate that existing data-aware discrete models struggle when\ntransferred to new scenarios with shifted temporal setups. We then isolate the\ntemporal generalization from dataset shift, revealing that a simple, explicit\nconditioning mechanism for temporal metadata is a highly effective solution.\nBased on this insight, we present OmniTraj, a Transformer-based model\npre-trained on a large-scale, heterogeneous dataset. Our experiments show that\nexplicitly conditioning on the frame rate enables OmniTraj to achieve\nstate-of-the-art zero-shot transfer performance, reducing prediction error by\nover 70\\% in challenging cross-setup scenarios. After fine-tuning, OmniTraj\nachieves state-of-the-art results on four datasets, including NBA, JTA,\nWorldPose, and ETH-UCY. The code is publicly available:\nhttps://github.com/vita-epfl/omnitraj", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23657v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23319", "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content", "authors": ["Alfio Ferrara", "Sergio Picascia", "Laura Pinnavaia", "Vojimir Ranitovic", "Elisabetta Rocchetti", "Alice Tuveri"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23319v1", "summary": "Proprietary Large Language Models (LLMs) have shown tendencies toward\npoliteness, formality, and implicit content moderation. While previous research\nhas primarily focused on explicitly training models to moderate and detoxify\nsensitive content, there has been limited exploration of whether LLMs\nimplicitly sanitize language without explicit instructions. This study\nempirically analyzes the implicit moderation behavior of GPT-4o-mini when\nparaphrasing sensitive content and evaluates the extent of sensitivity shifts.\nOur experiments indicate that GPT-4o-mini systematically moderates content\ntoward less sensitive classes, with substantial reductions in derogatory and\ntaboo language. Also, we evaluate the zero-shot capabilities of LLMs in\nclassifying sentence sensitivity, comparing their performances against\ntraditional methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23319v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23207", "title": "Improved Analysis of Khatri-Rao Random Projections and Applications", "authors": ["Arvind K. Saibaba", "Bhisham Dev Verma", "Grey Ballard"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      27 pages and 4 figures", "url": "http://arxiv.org/abs/2507.23207v1", "summary": "Randomization has emerged as a powerful set of tools for large-scale matrix\nand tensor decompositions. Randomized algorithms involve computing sketches\nwith random matrices. A prevalent approach is to take the random matrix as a\nstandard Gaussian random matrix, for which the theory is well developed.\nHowever, this approach has the drawback that the cost of generating and\nmultiplying by the random matrix can be prohibitively expensive. Khatri-Rao\nrandom projections (KRPs), obtained by sketching with Khatri-Rao products of\nrandom matrices, offer a viable alternative and are much cheaper to generate.\nHowever, the theoretical guarantees of using KRPs are much more pessimistic\ncompared to their accuracy observed in practice. We attempt to close this gap\nby obtaining improved analysis of the use of KRPs in matrix and tensor low-rank\ndecompositions. We propose and analyze a new algorithm for low-rank\napproximations of block-structured matrices (e.g., block Hankel) using KRPs. We\nalso develop new algorithms to accelerate tensor computations in the Tucker\nformat using KRPs, and give theoretical guarantees of the resulting low-rank\napproximations. Numerical experiments on synthetic and real-world tensors show\nthe computational benefits of the proposed methods.", "comment": "27 pages and 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.23207v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22782", "title": "Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies", "authors": ["Hugo Garrido-Lestache", "Jeremy Kedziora"], "categories": ["cs.AI", "cs.LG", "I.2.0; I.2.8"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.22782v2", "summary": "This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement\nlearning algorithm designed to enhance multi-agent collaboration in cooperative\nenvironments. TAAC employs a Centralized Training/Centralized Execution scheme\nincorporating multi-headed attention mechanisms in both the actor and critic.\nThis design facilitates dynamic, inter-agent communication, allowing agents to\nexplicitly query teammates, thereby efficiently managing the exponential growth\nof joint-action spaces while ensuring a high degree of collaboration. We\nfurther introduce a penalized loss function which promotes diverse yet\ncomplementary roles among agents. We evaluate TAAC in a simulated soccer\nenvironment against benchmark algorithms representing other multi-agent\nparadigms, including Proximal Policy Optimization and Multi-Agent\nActor-Attention-Critic. We find that TAAC exhibits superior performance and\nenhanced collaborative behaviors across a variety of metrics (win rates, goal\ndifferentials, Elo ratings, inter-agent connectivity, balanced spatial\ndistributions, and frequent tactical interactions such as ball possession\nswaps).", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.22782v2", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2408.10610", "title": "On the Approximation of Stationary Processes using the ARMA Model", "authors": ["Anand Ganesh", "Babhrubahan Bose", "Anand Rajagopalan"], "categories": ["cs.LG", "math.PR", "stat.ME", "60G10", "G.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 1 figure", "url": "http://arxiv.org/abs/2408.10610v4", "summary": "We look at a problem related to Autoregressive Moving Average (ARMA) models,\non quantifying the approximation error between a true stationary process $X_t$\nand an ARMA model $Y_t$. We take the transfer function representation $x(L)$ of\na stationary process $X_t$ and show that the $L^{\\infty}$ norm of $x$ acts as a\nvalid norm on $X_t$ that controls the $\\ell^2$ norm of its Wold coefficients.\nWe then show that a certain subspace of stationary processes, which includes\nARMA models, forms a Banach algebra under the $L^{\\infty}$ norm that respects\nthe multiplicative structure of $H^{\\infty}$ transfer functions and thus\nimproves on the structural properties of the cepstral norm for ARMA models. The\nnatural definition of invertibility in this algebra is consistent with the\noriginal definition of ARMA invertibility, and generalizes better to non-ARMA\nprocesses than Wiener's $\\ell^1$ condition. Finally, we calculate some explicit\napproximation bounds in the simpler context of continuous transfer functions,\nand critique some heuristic ideas on Pad\\'e approximations and parsimonious\nmodels.", "comment": "11 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2408.10610v4", "cate": "cs.LG", "date": "2024-08-20", "updated": "2025-07-31"}
{"id": "2507.23683", "title": "I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation", "authors": ["Jialei Chen", "Wuhao Xu", "Sipeng He", "Baoru Huang", "Dongchun Ren"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23683v1", "summary": "Vast and high-quality data are essential for end-to-end autonomous driving\nsystems. However, current driving data is mainly collected by vehicles, which\nis expensive and inefficient. A potential solution lies in synthesizing data\nfrom real-world images. Recent advancements in 3D reconstruction demonstrate\nphotorealistic novel view synthesis, highlighting the potential of generating\ndriving data from images captured on the road. This paper introduces a novel\nmethod, I2V-GS, to transfer the Infrastructure view To the Vehicle view with\nGaussian Splatting. Reconstruction from sparse infrastructure viewpoints and\nrendering under large view transformations is a challenging problem. We adopt\nthe adaptive depth warp to generate dense training views. To further expand the\nrange of views, we employ a cascade strategy to inpaint warped images, which\nalso ensures inpainting content is consistent across views. To further ensure\nthe reliability of the diffusion model, we utilize the cross-view information\nto perform a confidenceguided optimization. Moreover, we introduce RoadSight, a\nmulti-modality, multi-view dataset from real scenarios in infrastructure views.\nTo our knowledge, I2V-GS is the first framework to generate autonomous driving\ndatasets with infrastructure-vehicle view transformation. Experimental results\ndemonstrate that I2V-GS significantly improves synthesis quality under vehicle\nview, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%,\n34.2%, and 14.9%, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23683v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23404", "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring", "authors": ["Salah Eddine Bekhouche", "Azeddine Benlamoudi", "Yazid Bounab", "Fadi Dornaika", "Abdenour Hadid"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23404v1", "summary": "Arabic poses a particular challenge for natural language processing (NLP) and\ninformation retrieval (IR) due to its complex morphology, optional diacritics\nand the coexistence of Modern Standard Arabic (MSA) and various dialects.\nDespite the growing global significance of Arabic, it is still underrepresented\nin NLP research and benchmark resources. In this paper, we present an enhanced\nDense Passage Retrieval (DPR) framework developed specifically for Arabic. At\nthe core of our approach is a novel Attentive Relevance Scoring (ARS) that\nreplaces standard interaction mechanisms with an adaptive scoring function that\nmore effectively models the semantic relevance between questions and passages.\nOur method integrates pre-trained Arabic language models and architectural\nrefinements to improve retrieval performance and significantly increase ranking\naccuracy when answering Arabic questions. The code is made publicly available\nat \\href{https://github.com/Bekhouche/APR}{GitHub}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23404v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23408", "title": "An optimal preconditioner for high-order scheme arising from multi-dimensional Riesz space fractional diffusion equations with variable coefficients", "authors": ["Yuan-Yuan Huang", "Wei Qu", "Sean Y. Hon", "Siu-Long Lei"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23408v1", "summary": "In this paper, we propose an efficient method for solving multi-dimensional\nRiesz space fractional diffusion equations with variable coefficients. The\nCrank-Nicolson (CN) method is used for temporal discretization, while the\nfourth-order fractional centered difference (4FCD) method is employed for\nspatial discretization. Using a novel technique, we show that the CN-4FCD\nscheme for the multi-dimensional case is unconditionally stable and convergent,\nachieving second-order accuracy in time and fourth-order accuracy in space with\nrespect to the discrete L2-norm. Moreover, leveraging the symmetric multi-level\nToeplitz-like structure of the coefficient matrix in the discrete linear\nsystems, we enhance the computational efficiency of the proposed scheme with a\nsine transform-based preconditioner, ensuring a mesh-size-independent\nconvergence rate for the conjugate gradient method. Finally, two numerical\nexamples validate the theoretical analysis and demonstrate the superior\nperformance of the proposed preconditioner compared to existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23408v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2304.01430", "title": "Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots", "authors": ["Dong Lao", "Zhengyang Hu", "Francesco Locatello", "Yanchao Yang", "Stefano Soatto"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2304.01430v3", "summary": "We investigate the emergence of objects in visual perception in the absence\nof any semantic annotation. The resulting model has received no supervision,\ndoes not use any pre-trained features, and yet it can segment the domain of an\nimage into multiple independently moving regions. The resulting motion\nsegmentation method can handle an unknown and varying number of objects in\nreal-time. The core multi-modal conditional encoder-decoder architecture has\none modality (optical flow) feed the encoder to produce a collection of latent\ncodes (slots), and the other modality (color image) conditions the decoder to\ngenerate the first modality (flow) from the slots. The training criterion is\ndesigned to foster 'information separation' among the slots, while the\narchitecture explicitly allocates activations to individual slots, leading to a\nmethod we call Divided Attention (DivA). At test time, DivA handles a different\nnumber of objects and different image resolution than seen at training, and is\ninvariant to permutations of the slots. DivA achieves state-of-the-art\nperformance while tripling the runtime speed of comparable methods, up to 104\nFPS, and reduces the performance gap from supervised methods to 12% or less.\nObjects bootstrapped by DivA can then be used to prime static classifiers via\ncontrastive learning. On fewer than 5,000 video clips, training DINO on DivA's\nobject proposals narrows the performance gap to ImageNet-based training by up\nto 30.2% compared to training directly on the video frames.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2304.01430v3", "cate": "cs.CV", "date": "2023-04-04", "updated": "2025-07-31"}
{"id": "2409.17092", "title": "Accumulator-Aware Post-Training Quantization for Large Language Models", "authors": ["Ian Colbert", "Giuseppe Franco", "Fabian Grob", "Jinjie Zhang", "Rayan Saab"], "categories": ["cs.LG", "cs.AI", "cs.DM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.17092v2", "summary": "When quantizing weights and activations to increasingly narrower\nrepresentations, the cost of additions begins to dominate that of\nmultiplications in multiply-accumulate (MAC) units. Recent studies show that\nreducing addition costs via low-precision accumulation improves throughput,\npower, and area across inference platforms, albeit with an increased risk of\noverflow. Accumulator-aware quantization research has so far only considered\nthe quantization-aware training (QAT) paradigm, in which models are fine-tuned\nor trained from scratch with quantization in the loop. As models and datasets\ncontinue to grow in size, QAT techniques become increasingly more expensive,\nwhich has motivated the recent surge in post-training quantization (PTQ)\nresearch. To bridge this gap, we introduce AXE, the first accumulator-aware\nquantization framework explicitly designed to endow overflow avoidance\nguarantees to PTQ algorithms. We present theoretical motivation for AXE and\ndemonstrate its flexibility by implementing it on top of two existing\nalgorithms: GPFQ and OPTQ. We design AXE to support multi-stage accumulation,\nopening the door to full datapath optimization for the first time. We evaluate\nAXE using recent language generation models; when quantizing Llama3 8B for a\n16-bit multi-stage accumulation datapath, AXE maintains up to 98% of the FP16\nperplexity, surpassing naive bit width manipulation by up to 15%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.17092v2", "cate": "cs.LG", "date": "2024-09-25", "updated": "2025-07-31"}
{"id": "2507.23685", "title": "UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image Restoration", "authors": ["Zihan Cheng", "Liangtai Zhou", "Dian Chen", "Ni Tang", "Xiaotong Luo", "Yanyun Qu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23685v1", "summary": "All-in-One Image Restoration (AiOIR) has emerged as a promising yet\nchallenging research direction. To address its core challenges, we propose a\nnovel unified image restoration framework based on latent diffusion models\n(LDMs). Our approach structurally integrates low-quality visual priors into the\ndiffusion process, unlocking the powerful generative capacity of diffusion\nmodels for diverse degradations. Specifically, we design a Degradation-Aware\nFeature Fusion (DAFF) module to enable adaptive handling of diverse degradation\ntypes. Furthermore, to mitigate detail loss caused by the high compression and\niterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in\nthe decoder to enhance texture and fine-structure recovery. Extensive\nexperiments across multi-task and mixed degradation settings demonstrate that\nour method consistently achieves state-of-the-art performance, highlighting the\npractical potential of diffusion priors for unified image restoration. Our code\nwill be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23685v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23407", "title": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration", "authors": ["Ante Wang", "Yujie Lin", "Jingyao Liu", "Suhang Wu", "Hao Liu", "Xinyan Xiao", "Jinsong Su"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23407v1", "summary": "Critical thinking is essential for building robust AI systems, preventing\nthem from blindly accepting flawed data or biased reasoning. However, prior\nwork has primarily focused on passive critical thinking, where models simply\nreject problematic queries without taking constructive steps to address user\nrequests. In this work, we introduce proactive critical thinking, a paradigm\nwhere models actively seek missing or clarifying information from users to\nresolve their queries better. To evaluate this capability, we present GSM-MC\nand GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical\nreasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math\nproblems with a key variable deliberately removed, requiring models to identify\nand request the missing information. GSM-MCE further increases the difficulty\nby introducing irrelevant details to test robustness against distractions.\nExperiments on Qwen3 and Llama series models show that, while these models\nexcel in traditional reasoning tasks due to extensive post-training and\ninference-time scaling, they struggle with proactive critical thinking,\nespecially smaller ones. However, we demonstrate that reinforcement learning\n(RL) can significantly improve this ability. Using our enhanced RL algorithm,\nwe achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to\n73.98% on GSM-MC. We hope this work advances models that collaborate more\neffectively with users in problem-solving through proactive critical thinking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23407v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23450", "title": "The Effect of Prior Parameters on Standardized Kalman Filter-Based EEG Source Localization", "authors": ["Dilshanie Prasikala", "Joonas Lahtinen", "Alexandra Koulouri", "Sampsa Pursiainen"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23450v1", "summary": "EEG Source localization is a critical tool in neuroscience, with applications\nranging from epilepsy diagnosis to cognitive research. It involves solving an\nill-posed inverse problem that lacks a unique solution unless constrained by\nprior knowledge. The Bayesian framework enables the incorporation of such\nknowledge, typically encoded through prior models. Various algorithms have been\nproposed for source localization, and they differ significantly in how prior\nknowledge is incorporated. Some approaches rely on anatomical or functional\nconstraints, while others use statistical distributions or sampling-based\ntechniques. In this landscape, the Standardized Kalman Filter (SKF) represents\na dynamic Bayesian approach that integrates temporal modeling with a Gaussian\nprior structure. It addresses the depth bias, a common limitation in source\nlocalization, through a post-hoc standardization step that equalizes\nsensitivity across cortical depths and makes deep activity detection feasible.\n  This study focuses on the development and optimization of Gaussian prior\nmodels within the SKF framework for simultaneous cortical and sub-cortical\nactivity detection. Synthetic data similar to the P20 / N20 component of the\nsomatosensory evoked potentials (SEP) was used to identify effective prior\nparameter configurations for reconstructing both deep and superficial sources\nunder different noise levels. We also investigated the role of RTS smoothing in\nenhancing source separability. Our results indicate that raising the\nstandardization exponent to 1.25, along with smoothing, significantly improves\ndepth localization accuracy at low noise levels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23450v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2406.14313", "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability", "authors": ["Riya Sawhney", "Samrat Yadav", "Indrajit Bhattacharya", "Mausam"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.14313v3", "summary": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.14313v3", "cate": "cs.CL", "date": "2024-06-20", "updated": "2025-07-31"}
{"id": "2412.00123", "title": "Electricity Price Prediction Using Multi-Kernel Gaussian Process Regression Combined with Kernel-Based Support Vector Regression", "authors": ["Abhinav Das", "Stephan Schlüter", "Lorenz Schneider"], "categories": ["cs.LG", "math.PR", "62M10(Primary), 62M20, 60G15, 62J05(Secondary)"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.00123v4", "summary": "This paper presents a new hybrid model for predicting German electricity\nprices. The algorithm is based on a combination of Gaussian Process Regression\n(GPR) and Support Vector Regression (SVR). Although GPR is a competent model\nfor learning stochastic patterns within data and for interpolation, its\nperformance for out-of-sample data is not very promising. By choosing a\nsuitable data-dependent covariance function, we can enhance the performance of\nGPR for the German hourly power prices being tested. However, since the\nout-of-sample prediction is dependent on the training data, the prediction is\nvulnerable to noise and outliers. To overcome this issue, a separate prediction\nis calculated using SVR, which applies margin-based optimization. This method\nis advantageous when dealing with non-linear processes and outliers, since only\ncertain necessary points (support vectors) in the training data are responsible\nfor regression. The individual predictions are then linearly combined using\nuniform weights. When tested on historic German power prices, this approach\noutperforms the publicly available benchmarks, namely the LASSO estimated\nautoregressive regression model, deep neural network provided in the recent\nresearch by [1].", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.00123v4", "cate": "cs.LG", "date": "2024-11-28", "updated": "2025-07-31"}
{"id": "2507.23709", "title": "Explainable Image Classification with Reduced Overconfidence for Tissue Characterisation", "authors": ["Alfie Roddan", "Chi Xu", "Serine Ajlouni", "Irini Kakaletri", "Patra Charalampaki", "Stamatia Giannarou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23709v1", "summary": "The deployment of Machine Learning models intraoperatively for tissue\ncharacterisation can assist decision making and guide safe tumour resections.\nFor image classification models, pixel attribution methods are popular to infer\nexplainability. However, overconfidence in deep learning model's predictions\ntranslates to overconfidence in pixel attribution. In this paper, we propose\nthe first approach which incorporates risk estimation into a pixel attribution\nmethod for improved image classification explainability. The proposed method\niteratively applies a classification model with a pixel attribution method to\ncreate a volume of PA maps. This volume is used for the first time, to generate\na pixel-wise distribution of PA values. We introduce a method to generate an\nenhanced PA map by estimating the expectation values of the pixel-wise\ndistributions. In addition, the coefficient of variation (CV) is used to\nestimate pixel-wise risk of this enhanced PA map. Hence, the proposed method\nnot only provides an improved PA map but also produces an estimation of risk on\nthe output PA values. Performance evaluation on probe-based Confocal Laser\nEndomicroscopy (pCLE) data and ImageNet verifies that our improved\nexplainability method outperforms the state-of-the-art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23709v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23486", "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains", "authors": ["Shirui Wang", "Zhihui Tang", "Huaxia Yang", "Qiuhong Gong", "Tiantian Gu", "Hongyang Ma", "Yongxin Wang", "Wubin Sun", "Zeliang Lian", "Kehang Mao", "Yinan Jiang", "Zhicheng Huang", "Lingyun Ma", "Wenjie Shen", "Yajie Ji", "Yunhui Tan", "Chunbo Wang", "Yunlu Gao", "Qianling Ye", "Rui Lin", "Mingyu Chen", "Lijuan Niu", "Zhihao Wang", "Peng Yu", "Mengran Lang", "Yue Liu", "Huimin Zhang", "Haitao Shen", "Long Chen", "Qiguang Zhao", "Si-Xuan Liu", "Lina Zhou", "Hua Gao", "Dongqiang Ye", "Lingmin Meng", "Youtao Yu", "Naixin Liang", "Jianxiong Wu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23486v1", "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23486v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23547", "title": "Quantum simulation of Helmholtz equations via Schr{ö}dingerization", "authors": ["Anjiao Gu", "Shi Jin", "Chuwen Ma"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23547v1", "summary": "The Helmholtz equation is a prototypical model for time-harmonic wave\npropagation. Numerical solutions become increasingly challenging as the wave\nnumber $k$ grows, due to the equation's elliptic yet noncoercive character and\nthe highly oscillatory nature of its solutions, with wavelengths scaling as\n$1/k$. These features lead to strong indefiniteness and large system sizes.\n  We present a quantum algorithm for solving such indefinite problems, built\nupon the Schr\\\"odingerization framework. This approach reformulates linear\ndifferential equations into Schr\\\"odinger-type systems by capturing the steady\nstate of damped dynamics. A warped phase transformation lifts the original\nproblem to a higher-dimensional formulation, making it compatible with quantum\ncomputation. To suppress numerical pollution, the algorithm incorporates\nasymptotic dispersion correction. It achieves a query complexity of\n$\\mathcal{O}(\\kappa^2\\text{polylog}\\varepsilon^{-1})$, where $\\kappa$ is the\ncondition number and $\\varepsilon$ the desired accuracy. For the Helmholtz\nequation, a simple preconditioner further reduces the complexity to\n$\\mathcal{O}(\\kappa\\text{polylog}\\varepsilon^{-1})$. Our constructive extension\nto the quantum setting is broadly applicable to all indefinite problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23547v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.02744", "title": "Neutral Residues: Revisiting Adapters for Model Extension", "authors": ["Franck Signe Talla", "Edouard Grave", "Hervé Jégou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025", "url": "http://arxiv.org/abs/2410.02744v3", "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.", "comment": "Accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2410.02744v3", "cate": "cs.CL", "date": "2024-10-03", "updated": "2025-07-31"}
{"id": "2501.08727", "title": "Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models", "authors": ["Zerui Tao", "Yuhta Takida", "Naoki Murata", "Qibin Zhao", "Yuki Mitsufuji"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2501.08727v2", "summary": "Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an\nincreasingly popular technique with many applications. Among the various PEFT\nmethods, Low-Rank Adaptation (LoRA) and its variants have gained significant\nattention due to their effectiveness, enabling users to fine-tune models with\nlimited computational resources. However, the approximation gap between the\nlow-rank assumption and desired fine-tuning weights prevents the simultaneous\nacquisition of ultra-parameter-efficiency and better performance. To reduce\nthis gap and further improve the power of LoRA, we propose a new PEFT method\nthat combines two classes of adaptations, namely, transform and residual\nadaptations. In specific, we first apply a full-rank and dense transform to the\npre-trained weight. This learnable transform is expected to align the\npre-trained weight as closely as possible to the desired weight, thereby\nreducing the rank of the residual weight. Then, the residual part can be\neffectively approximated by more compact and parameter-efficient structures,\nwith a smaller approximation error. To achieve ultra-parameter-efficiency in\npractice, we design highly flexible and effective tensor decompositions for\nboth the transform and residual adaptations. Additionally, popular PEFT methods\nsuch as DoRA can be summarized under this transform plus residual adaptation\nscheme. Experiments are conducted on fine-tuning Stable Diffusion models in\nsubject-driven and controllable generation. The results manifest that our\nmethod can achieve better performances and parameter efficiency compared to\nLoRA and several baselines.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2501.08727v2", "cate": "cs.LG", "date": "2025-01-15", "updated": "2025-07-31"}
{"id": "2507.23715", "title": "DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching", "authors": ["Emery Pierson", "Lei Li", "Angela Dai", "Maks Ovsjanikov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Presented at ICCV 2025", "url": "http://arxiv.org/abs/2507.23715v1", "summary": "Deep functional maps have recently emerged as a powerful tool for solving\nnon-rigid shape correspondence tasks. Methods that use this approach combine\nthe power and flexibility of the functional map framework, with data-driven\nlearning for improved accuracy and generality. However, most existing methods\nin this area restrict the learning aspect only to the feature functions and\nstill rely on axiomatic modeling for formulating the training loss or for\nfunctional map regularization inside the networks. This limits both the\naccuracy and the applicability of the resulting approaches only to scenarios\nwhere assumptions of the axiomatic models hold. In this work, we show, for the\nfirst time, that both in-network regularization and functional map training can\nbe replaced with data-driven methods. For this, we first train a generative\nmodel of functional maps in the spectral domain using score-based generative\nmodeling, built from a large collection of high-quality maps. We then exploit\nthe resulting model to promote the structural properties of ground truth\nfunctional maps on new shape collections. Remarkably, we demonstrate that the\nlearned models are category-agnostic, and can fully replace commonly used\nstrategies such as enforcing Laplacian commutativity or orthogonality of\nfunctional maps. Our key technical contribution is a novel distillation\nstrategy from diffusion models in the spectral domain. Experiments demonstrate\nthat our learned regularization leads to better results than axiomatic\napproaches for zero-shot non-rigid shape matching. Our code is available at:\nhttps://github.com/daidedou/diffumatch/", "comment": "Presented at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23715v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23541", "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning", "authors": ["Keer Lu", "Zheng Liang", "Youquan Li", "Jiejun Tan", "Da Pan", "Shusen Zhang", "Guosheng Dong", "Huang Leng"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23541v1", "summary": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23541v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23586", "title": "Fitted norm preconditioners for the Hodge Laplacian in mixed form", "authors": ["Wietse M. Boon", "Johannes Kraus", "Tomáš Luber", "Maria Lymbery"], "categories": ["math.NA", "cs.NA", "65N22, 65F08, 35J05, 58J10, 65N30, 58A14"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23586v1", "summary": "We use the practical framework for abstract perturbed saddle point problems\nrecently introduced by Hong et al. to analyze the mixed formulation of the\nHodge Laplace problem. We compose two parameter-dependent norms in which the\nuniform continuity and stability of the problem follow. This not only\nguarantees the well-posedness of the corresponding variational formulation on\nthe continuous level, but also of related compatible discrete models.\n  We further simplify the obtained norms and, in both cases, arrive at the same\nnorm-equivalent preconditioner that is easily implementable. The efficiency and\nuniformity of the preconditioner are demonstrated numerically by the fast\nconvergence and uniformly bounded number of preconditioned MINRES iterations\nrequired to solve various instances of Hodge Laplace problems in two and three\nspace dimensions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23586v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.05343", "title": "EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts", "authors": ["Yuto Haneji", "Taichi Nishimura", "Hirotaka Kameko", "Keisuke Shirai", "Tomoya Yoshida", "Keiya Kajimura", "Koki Yamamoto", "Taiyu Cui", "Tomohiro Nishimoto", "Shinsuke Mori"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Main 8 pages, supplementary 6 pages", "url": "http://arxiv.org/abs/2410.05343v3", "summary": "Mistake action detection is crucial for developing intelligent archives that\ndetect workers' errors and provide feedback. Existing studies have focused on\nvisually apparent mistakes in free-style activities, resulting in video-only\napproaches to mistake detection. However, in text-following activities, models\ncannot determine the correctness of some actions without referring to the\ntexts. Additionally, current mistake datasets rarely use procedural texts for\nvideo recording except for cooking. To fill these gaps, this paper proposes the\nEgoOops dataset, where egocentric videos record erroneous activities when\nfollowing procedural texts across diverse domains. It features three types of\nannotations: video-text alignment, mistake labels, and descriptions for\nmistakes. We also propose a mistake detection approach, combining video-text\nalignment and mistake label classification to leverage the texts. Our\nexperimental results show that incorporating procedural texts is essential for\nmistake detection. Data is available through\nhttps://y-haneji.github.io/EgoOops-project-page/.", "comment": "Main 8 pages, supplementary 6 pages", "pdf_url": "http://arxiv.org/pdf/2410.05343v3", "cate": "cs.CV", "date": "2024-10-07", "updated": "2025-07-31"}
{"id": "2501.15544", "title": "Advancing Generative Artificial Intelligence and Large Language Models for Demand Side Management with Internet of Electric Vehicles", "authors": ["Hanwen Zhang", "Ruichen Zhang", "Wei Zhang", "Dusit Niyato", "Yonggang Wen", "Chunyan Miao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 Pages", "url": "http://arxiv.org/abs/2501.15544v4", "summary": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions.", "comment": "11 Pages", "pdf_url": "http://arxiv.org/pdf/2501.15544v4", "cate": "cs.LG", "date": "2025-01-26", "updated": "2025-07-31"}
{"id": "2507.23755", "title": "Slot Attention with Re-Initialization and Self-Distillation", "authors": ["Rongzhen Zhao", "Yi Zhao", "Juho Kannala", "Joni Pajarinen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.23755v1", "summary": "Unlike popular solutions based on dense feature maps, Object-Centric Learning\n(OCL) represents visual scenes as sub-symbolic object-level feature vectors,\ntermed slots, which are highly versatile for tasks involving visual modalities.\nOCL typically aggregates object superpixels into slots by iteratively applying\ncompetitive cross attention, known as Slot Attention, with the slots as the\nquery. However, once initialized, these slots are reused naively, causing\nredundant slots to compete with informative ones for representing objects. This\noften results in objects being erroneously segmented into parts. Additionally,\nmainstream methods derive supervision signals solely from decoding slots into\nthe input's reconstruction, overlooking potential supervision based on internal\ninformation. To address these issues, we propose Slot Attention with\nre-Initialization and self-Distillation (DIAS): $\\emph{i)}$ We reduce\nredundancy in the aggregated slots and re-initialize extra aggregation to\nupdate the remaining slots; $\\emph{ii)}$ We drive the bad attention map at the\nfirst aggregation iteration to approximate the good at the last iteration to\nenable self-distillation. Experiments demonstrate that DIAS achieves\nstate-of-the-art on OCL tasks like object discovery and recognition, while also\nimproving advanced visual prediction and reasoning. Our code is available on\nhttps://github.com/Genera1Z/DIAS.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23755v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23577", "title": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text", "authors": ["Alva West", "Luodan Zhang", "Liuliu Zhang", "Minjun Zhu", "Yixuan Weng", "Yue Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23577v1", "summary": "The proliferation of sophisticated text generation models necessitates the\ndevelopment of robust detection methods capable of identifying\nmachine-generated content, particularly text designed to evade detection\nthrough adversarial perturbations. Existing zero-shot detectors often rely on\nstatistical measures that implicitly assume Gaussian distributions, a premise\nthat falters when confronted with the heavy-tailed statistical artifacts\ncharacteristic of adversarial or non-native English texts. This paper\nintroduces T-Detect, a novel detection method that fundamentally redesigns the\nstatistical core of curvature-based detectors. Our primary innovation is the\nreplacement of standard Gaussian normalization with a heavy-tailed discrepancy\nscore derived from the Student's t-distribution. This approach is theoretically\ngrounded in the empirical observation that adversarial texts exhibit\nsignificant leptokurtosis, rendering traditional statistical assumptions\ninadequate. T-Detect computes a detection score by normalizing the\nlog-likelihood of a passage against the expected moments of a t-distribution,\nproviding superior resilience to statistical outliers. We validate our approach\non the challenging RAID benchmark for adversarial text and the comprehensive\nHART dataset. Experiments show that T-Detect provides a consistent performance\nuplift over strong baselines, improving AUROC by up to 3.9\\% in targeted\ndomains. When integrated into a two-dimensional detection framework (CT), our\nmethod achieves state-of-the-art performance, with an AUROC of 0.926 on the\nBooks domain of RAID. Our contributions are a new, theoretically-justified\nstatistical foundation for text detection, an ablation-validated method that\ndemonstrates superior robustness, and a comprehensive analysis of its\nperformance under adversarial conditions. Ours code are released at\nhttps://github.com/ResearAI/t-detect.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23577v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23602", "title": "Efficient Numerical Strategies for Entropy-Regularized Semi-Discrete Optimal Transport", "authors": ["Moaad Khamlich", "Francesco Romor", "Gianluigi Rozza"], "categories": ["math.NA", "cs.NA", "65K10, 49Q22, 65M60, 90C25, 65Y20", "G.1.6; I.3.5; G.4"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23602v1", "summary": "Semi-discrete optimal transport (SOT), which maps a continuous probability\nmeasure to a discrete one, is a fundamental problem with wide-ranging\napplications. Entropic regularization is often employed to solve the SOT\nproblem, leading to a regularized (RSOT) formulation that can be solved\nefficiently via its convex dual. However, a significant computational challenge\nemerges when the continuous source measure is discretized via the finite\nelement (FE) method to handle complex geometries or densities, such as those\narising from solutions to Partial Differential Equations (PDEs). The evaluation\nof the dual objective function requires dense interactions between the numerous\nsource quadrature points and all target points, creating a severe bottleneck\nfor large-scale problems. This paper presents a cohesive framework of numerical\nstrategies to overcome this challenge. We accelerate the dual objective and\ngradient evaluations by combining distance-based truncation with fast spatial\nqueries using R-trees. For overall convergence, we integrate multilevel\ntechniques based on hierarchies of both the FE source mesh and the discrete\ntarget measure, alongside a robust scheduling strategy for the regularization\nparameter. When unified, these methods drastically reduce the computational\ncost of RSOT, enabling its practical application to complex, large-scale\nscenarios. We provide an open-source C++ implementation of this framework,\nbuilt upon the deal.II finite element library, available at\nhttps://github.com/SemiDiscreteOT/SemiDiscreteOT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23602v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23557", "title": "Tree-indexed sums of Catalan numbers", "authors": ["Alin Bostan", "Valentin Féray", "Paul Thévenin"], "categories": ["math.CO", "cs.SC", "math.PR"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      62 pages, 8 figures", "url": "http://arxiv.org/abs/2507.23557v1", "summary": "We consider a family of infinite sums of products of Catalan numbers, indexed\nby trees. We show that these sums are polynomials in $1/\\pi$ with rational\ncoefficients; the proof is effective and provides an algorithm to explicitly\ncompute these sums. Along the way we introduce parametric liftings of our sums,\nand show that they are polynomials in the complete elliptic integrals of the\nfirst and second kind. Moreover, the degrees of these polynomials are at most\nhalf of the number of vertices of the tree. The computation of these\ntree-indexed sums is motivated by the study of large meandric systems, which\nare non-crossing configurations of loops in the plane.", "comment": "62 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.23557v1", "cate": "math.CO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2411.06409", "title": "Automated Strategy Invention for Confluence of Term Rewrite Systems", "authors": ["Liao Zhang", "Fabian Mitterwallner", "Jan Jakubuv", "Cezary Kaliszyk"], "categories": ["cs.LO", "cs.AI", "F.4.2; I.2.8"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06409v2", "summary": "Term rewriting plays a crucial role in software verification and compiler\noptimization. With dozens of highly parameterizable techniques developed to\nprove various system properties, automatic term rewriting tools work in an\nextensive parameter space. This complexity exceeds human capacity for parameter\nselection, motivating an investigation into automated strategy invention. In\nthis paper, we focus on confluence, an important property of term rewrite\nsystems, and apply machine learning to develop the first learning-guided\nautomatic confluence prover. Moreover, we randomly generate a large dataset to\nanalyze confluence for term rewrite systems. Our results focus on improving the\nstate-of-the-art automatic confluence prover CSI: When equipped with our\ninvented strategies, it surpasses its human-designed strategies both on the\naugmented dataset and on the original human-created benchmark dataset Cops,\nproving/disproving the confluence of several term rewrite systems for which no\nautomated proofs were known before.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06409v2", "cate": "cs.LO", "date": "2024-11-10", "updated": "2025-07-31"}
{"id": "2501.16325", "title": "Tailored Forecasting from Short Time Series via Meta-learning", "authors": ["Declan A. Norton", "Edward Ott", "Andrew Pomerance", "Brian Hunt", "Michelle Girvan"], "categories": ["cs.LG", "nlin.CD", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      23 pages, 12 figures", "url": "http://arxiv.org/abs/2501.16325v2", "summary": "Machine learning models can effectively forecast dynamical systems from\ntime-series data, but they typically require large amounts of past data, making\nforecasting particularly challenging for systems with limited history. To\novercome this, we introduce Meta-learning for Tailored Forecasting using\nRelated Time Series (METAFORS), which generalizes knowledge across systems to\nenable forecasting in data-limited scenarios. By learning from a library of\nmodels trained on longer time series from potentially related systems, METAFORS\nbuilds and initializes a model tailored to short time-series data from the\nsystem of interest. Using a reservoir computing implementation and testing on\nsimulated chaotic systems, we demonstrate that METAFORS can reliably predict\nboth short-term dynamics and long-term statistics without requiring contextual\nlabels. We see this even when test and related systems exhibit substantially\ndifferent behaviors, highlighting METAFORS' strengths in data-limited\nscenarios.", "comment": "23 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2501.16325v2", "cate": "cs.LG", "date": "2025-01-27", "updated": "2025-07-31"}
{"id": "2507.23772", "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting", "authors": ["Di Li", "Jie Feng", "Jiahao Chen", "Weisheng Dong", "Guanbin Li", "Yuhui Zheng", "Mingtao Feng", "Guangming Shi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23772v1", "summary": "3D affordance reasoning, the task of associating human instructions with the\nfunctional regions of 3D objects, is a critical capability for embodied agents.\nCurrent methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited\nto single-object, single-step interactions, a paradigm that falls short of\naddressing the long-horizon, multi-object tasks required for complex real-world\napplications. To bridge this gap, we introduce the novel task of Sequential 3D\nGaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale\nbenchmark featuring 1800+ scenes to support research on long-horizon affordance\nunderstanding in complex 3DGS environments. We then propose SeqSplatNet, an\nend-to-end framework that directly maps an instruction to a sequence of 3D\naffordance masks. SeqSplatNet employs a large language model that\nautoregressively generates text interleaved with special segmentation tokens,\nguiding a conditional decoder to produce the corresponding 3D mask. To handle\ncomplex scene geometry, we introduce a pre-training strategy, Conditional\nGeometric Reconstruction, where the model learns to reconstruct complete\naffordance region masks from known geometric observations, thereby building a\nrobust geometric prior. Furthermore, to resolve semantic ambiguities, we design\na feature injection mechanism that lifts rich semantic features from 2D Vision\nFoundation Models (VFM) and fuses them into the 3D decoder at multiple scales.\nExtensive experiments demonstrate that our method sets a new state-of-the-art\non our challenging benchmark, effectively advancing affordance reasoning from\nsingle-step interactions to complex, sequential tasks at the scene level.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23772v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23588", "title": "DiffLoRA: Differential Low-Rank Adapters for Large Language Models", "authors": ["Alexandre Misrahi", "Nadezhda Chirkova", "Maxime Louis", "Vassilina Nikoulina"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23588v1", "summary": "Differential Transformer has recently been proposed to improve performance in\nTransformer models by canceling out noise through a denoiser attention\nmechanism. In this work, we introduce DiffLoRA, a parameter-efficient\nadaptation of the differential attention mechanism, with low-rank adapters on\nboth positive and negative attention terms. This approach retains the\nefficiency of LoRA while aiming to benefit from the performance gains of\ndifferential attention. We evaluate DiffLoRA across a broad range of NLP tasks,\nincluding general benchmarks, many-shot in-context learning, RAG, and\nlong-context tests. We observe that, although DiffLoRA falls short of other\nparameter-efficient fine-tuning methods in most evaluation tasks, it shows\ninteresting results in certain domains (+11 pts on LoRA for HumanEval). We\nanalyze the attention patterns post-finetuning to identify the reasons for this\nbehavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23588v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23613", "title": "A Multi-Frequency Helmholtz Solver Based on the WaveHoltz Algorithm", "authors": ["Daniel Appelö", "Francis Appiah", "Jeffrey W. Banks", "Cassandra Carrick", "William D. Henshaw", "Donald W. Schwendeman"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23613v1", "summary": "We develop and analyze a new approach for simultaneously computing multiple\nsolutions to the Helmholtz equation for different frequencies and different\nforcing functions. The new Multi-Frequency WaveHoltz (MFWH) algorithm is an\nextension of the original WaveHoltz method and both are based on time-filtering\nsolutions to an associated wave equation. With MFWH, the different Helmholtz\nsolutions are computed simultaneously by solving a single wave equation\ncombined with multiple time filters. The MFWH algorithm defines a fixed-point\niteration which can be accelerated with Krylov methods such as GMRES. The\nsolution of the wave equation can be efficiently solved with either explicit\ntime-stepping or implicit time-stepping using as few as five time-steps per\nperiod. When combined with an $O(N)$ solver for the implicit equations, such a\nmultigrid, the scheme has an $O(N)$ solution cost when the frequencies are\nfixed and the number of grid points $N$ increases. High-order accurate\napproximations in space are used together with second-order accurate\napproximations in time. We show how to remove time discretization errors so\nthat the MFWH solutions converge to the corresponding solutions to the\ndiscretized Helmholtz problems. Numerical results are given using second-order\naccurate and fourth-accurate discretizations to confirm the convergence theory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23613v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2411.18659", "title": "DHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models", "authors": ["Yudong Zhang", "Ruobing Xie", "Xingwu Sun", "Yiqing Huang", "Jiansheng Chen", "Zhanhui Kang", "Di Wang", "Yu Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia 2025", "url": "http://arxiv.org/abs/2411.18659v2", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\nperformance on complex multimodal tasks. However, they continue to suffer from\nsignificant hallucination issues, including object, attribute, and relational\nhallucinations. To accurately detect these hallucinations, we investigated the\nvariations in cross-modal attention patterns between hallucination and\nnon-hallucination states. Leveraging these distinctions, we developed a\nlightweight detector capable of identifying hallucinations. Our proposed\nmethod, Detecting Hallucinations by Cross-modal Attention Patterns (DHCP), is\nstraightforward and does not require additional LVLM training or extra LVLM\ninference steps. Experimental results show that DHCP achieves remarkable\nperformance in hallucination detection. By offering novel insights into the\nidentification and analysis of hallucinations in LVLMs, DHCP contributes to\nadvancing the reliability and trustworthiness of these models. The code is\navailable at https://github.com/btzyd/DHCP.", "comment": "Accepted by ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2411.18659v2", "cate": "cs.CV", "date": "2024-11-27", "updated": "2025-07-31"}
{"id": "2502.06210", "title": "Achieving Deep Continual Learning via Evolution", "authors": ["Aojun Lu", "Junchao Ke", "Chunhui Ding", "Jiahao Fan", "Jiancheng Lv", "Yanan Sun"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.06210v2", "summary": "Deep neural networks, despite their remarkable success, remain fundamentally\nlimited in their ability to perform Continual Learning (CL). While most current\nmethods aim to enhance the capabilities of a single model, Inspired by the\ncollective learning mechanisms of human populations, we introduce Evolving\nContinual Learning (ECL), a framework that maintains and evolves a diverse\npopulation of neural network models. ECL continually searches for an optimal\narchitecture for each introduced incremental task. This tailored model is\ntrained on the corresponding task and archived as a specialized expert,\ncontributing to a growing collection of skills. This approach inherently\nresolves the core CL challenges: stability is achieved through the isolation of\nexpert models, while plasticity is greatly enhanced by evolving unique,\ntask-specific architectures. Experimental results demonstrate that ECL\nsignificantly outperforms state-of-the-art individual-level CL methods. By\nshifting the focus from individual adaptation to collective evolution, ECL\npresents a novel path toward AI systems capable of CL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.06210v2", "cate": "cs.LG", "date": "2025-02-10", "updated": "2025-07-31"}
{"id": "2507.23778", "title": "Half-Physics: Enabling Kinematic 3D Human Model with Physical Interactions", "authors": ["Li Siyao", "Yao Feng", "Omid Tehari", "Chen Change Loy", "Michael J. Black"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23778v1", "summary": "While current general-purpose 3D human models (e.g., SMPL-X) efficiently\nrepresent accurate human shape and pose, they lacks the ability to physically\ninteract with the environment due to the kinematic nature. As a result,\nkinematic-based interaction models often suffer from issues such as\ninterpenetration and unrealistic object dynamics. To address this limitation,\nwe introduce a novel approach that embeds SMPL-X into a tangible entity capable\nof dynamic physical interactions with its surroundings. Specifically, we\npropose a \"half-physics\" mechanism that transforms 3D kinematic motion into a\nphysics simulation. Our approach maintains kinematic control over inherent\nSMPL-X poses while ensuring physically plausible interactions with scenes and\nobjects, effectively eliminating penetration and unrealistic object dynamics.\nUnlike reinforcement learning-based methods, which demand extensive and complex\ntraining, our half-physics method is learning-free and generalizes to any body\nshape and motion; meanwhile, it operates in real time. Moreover, it preserves\nthe fidelity of the original kinematic motion while seamlessly integrating\nphysical interactions", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23778v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23661", "title": "Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning", "authors": ["Salam Thabet Doghmash", "Motaz Saad"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      23 pages, 5 figures", "url": "http://arxiv.org/abs/2507.23661v1", "summary": "Hate speech identification in social media has become an increasingly\nimportant issue in recent years. In this research, we address two problems: 1)\nto detect hate speech in Arabic text, 2) to clean a given text from hate\nspeech. The meaning of cleaning here is replacing each bad word with stars\nbased on the number of letters for each word. Regarding the first problem, we\nconduct several experiments using deep learning models and transformers to\ndetermine the best model in terms of the F1 score. Regarding second problem, we\nconsider it as a machine translation task, where the input is a sentence\ncontaining dirty text and the output is the same sentence with masking the\ndirty text. The presented methods achieve the best model in hate speech\ndetection with a 92\\% Macro F1 score and 95\\% accuracy. Regarding the text\ncleaning experiment, the best result in the hate speech masking model reached\n0.3 in BLEU score with 1-gram, which is a good result compared with the state\nof the art machine translation systems.", "comment": "23 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23661v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23651", "title": "Regularization of Inverse Problems by Filtered Diagonal Frame Decomposition under general source", "authors": ["Dang Duc Trong", "Nguyen Dang Minh", "Luu Xuan Thang", "Luu Dang Khoa"], "categories": ["math.NA", "cs.NA", "math.AP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23651v1", "summary": "Let $X$ and $Y$ be Hilbert spaces, and $\\mathbf{K}: \\text{dom} \\mathbf{K}\n\\subset X \\to Y$ a bounded linear operator. This paper addresses the inverse\nproblem $\\mathbf{K}x = y$, where exact data $y$ is replaced by noisy data\n$y^\\delta$ satisfying $\\|y^\\delta - y\\|_Y \\leq \\delta$. Due to the\nill-posedness of such problems, we employ regularization methods to stabilize\nsolutions. While singular value decomposition (SVD) provides a classical\napproach, its computation can be costly and impractical for certain operators.\nWe explore alternatives via Diagonal Frame Decomposition (DFD), generalizing\nSVD-based techniques, and introduce a regularized solution $x^\\delta_\\alpha =\n\\sum_{\\lambda \\in \\Lambda} \\kappa_\\lambda g_\\alpha(\\kappa_\\lambda^2) \\langle\ny^\\delta, v_\\lambda \\rangle \\overline{u}_\\lambda$. Convergence rates and\noptimality are analyzed under a generalized source condition\n$\\mathbf{M}_{\\varphi, E} = \\{ x \\in \\text{dom} \\mathbf{K} : \\sum_{\\lambda \\in\n\\Lambda} [\\varphi(\\kappa_\\lambda^2)]^{-1} |\\langle x, u_\\lambda \\rangle|^2 \\leq\nE^2 \\}$. Key questions include constructing DFD systems, relating DFD and SVD\nsingular values, and extending source conditions. We present theoretical\nresults, including modulus of continuity bounds and convergence rates for a\npriori and a posteriori parameter choices, with applications to polynomial and\nexponentially ill-posed problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23651v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2501.09112", "title": "Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision Networks for Photometric Redshift Estimation", "authors": ["Andrew Engel", "Nell Byler", "Adam Tsou", "Gautham Narayan", "Emmanuel Bonilla", "Ian Smith"], "categories": ["astro-ph.IM", "cs.AI"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      Accepted at ApJ", "url": "http://arxiv.org/abs/2501.09112v2", "summary": "We present Mantis Shrimp, a multi-survey deep learning model for photometric\nredshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and\ninfrared (UnWISE) imagery. Machine learning is now an established approach for\nphotometric redshift estimation, with generally acknowledged higher performance\nin areas with a high density of spectroscopically identified galaxies over\ntemplate-based methods. Multiple works have shown that image-based\nconvolutional neural networks can outperform tabular-based color/magnitude\nmodels. In comparison to tabular models, image models have additional design\ncomplexities: it is largely unknown how to fuse inputs from different\ninstruments which have different resolutions or noise properties. The Mantis\nShrimp model estimates the conditional density estimate of redshift using\ncutout images. The density estimates are well calibrated and the point\nestimates perform well in the distribution of available spectroscopically\nconfirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and\ncatastrophic outlier rate ($\\eta$=17.53$\\%$). We find that early fusion\napproaches (e.g., resampling and stacking images from different instruments)\nmatch the performance of late fusion approaches (e.g., concatenating latent\nspace representations), so that the design choice ultimately is left to the\nuser. Finally, we study how the models learn to use information across bands,\nfinding evidence that our models successfully incorporates information from all\nsurveys. The applicability of our model to the analysis of large populations of\ngalaxies is limited by the speed of downloading cutouts from external servers;\nhowever, our model could be useful in smaller studies such as generating priors\nover redshift for stellar population synthesis.", "comment": "Accepted at ApJ", "pdf_url": "http://arxiv.org/pdf/2501.09112v2", "cate": "astro-ph.IM", "date": "2025-01-15", "updated": "2025-07-31"}
{"id": "2502.17264", "title": "Kandinsky Conformal Prediction: Beyond Class- and Covariate-Conditional Coverage", "authors": ["Konstantina Bairaktari", "Jiayun Wu", "Zhiwei Steven Wu"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.17264v2", "summary": "Conformal prediction is a powerful distribution-free framework for\nconstructing prediction sets with coverage guarantees. Classical methods, such\nas split conformal prediction, provide marginal coverage, ensuring that the\nprediction set contains the label of a random test point with a target\nprobability. However, these guarantees may not hold uniformly across different\nsubpopulations, leading to disparities in coverage. Prior work has explored\ncoverage guarantees conditioned on events related to the covariates and label\nof the test point. We present Kandinsky conformal prediction, a framework that\nsignificantly expands the scope of conditional coverage guarantees. In contrast\nto Mondrian conformal prediction, which restricts its coverage guarantees to\ndisjoint groups -- reminiscent of the rigid, structured grids of Piet\nMondrian's art -- our framework flexibly handles overlapping and fractional\ngroup memberships defined jointly on covariates and labels, reflecting the\nlayered, intersecting forms in Wassily Kandinsky's compositions. Our algorithm\nunifies and extends existing methods, encompassing covariate-based group\nconditional, class conditional, and Mondrian conformal prediction as special\ncases, while achieving a minimax-optimal high-probability conditional coverage\nbound. Finally, we demonstrate the practicality of our approach through\nempirical evaluation on real-world datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.17264v2", "cate": "cs.LG", "date": "2025-02-24", "updated": "2025-07-31"}
{"id": "2507.23782", "title": "MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion", "authors": ["Zihan Wang", "Jeff Tan", "Tarasha Khurana", "Neehar Peri", "Deva Ramanan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.23782v1", "summary": "We address the problem of dynamic scene reconstruction from sparse-view\nvideos. Prior work often requires dense multi-view captures with hundreds of\ncalibrated cameras (e.g. Panoptic Studio). Such multi-view setups are\nprohibitively expensive to build and cannot capture diverse scenes in-the-wild.\nIn contrast, we aim to reconstruct dynamic human behaviors, such as repairing a\nbike or dancing, from a small set of sparse-view cameras with complete scene\ncoverage (e.g. four equidistant inward-facing static cameras). We find that\ndense multi-view reconstruction methods struggle to adapt to this sparse-view\nsetup due to limited overlap between viewpoints. To address these limitations,\nwe carefully align independent monocular reconstructions of each camera to\nproduce time- and view-consistent dynamic scene reconstructions. Extensive\nexperiments on PanopticStudio and Ego-Exo4D demonstrate that our method\nachieves higher quality reconstructions than prior art, particularly when\nrendering novel views. Code, data, and data-processing scripts are available on\nhttps://github.com/ImNotPrepared/MonoFusion.", "comment": "ICCV 2025. Project Page:\n  https://imnotprepared.github.io/research/25_DSR/", "pdf_url": "http://arxiv.org/pdf/2507.23782v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23776", "title": "Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities", "authors": ["Yunxiang Yan", "Tomohiro Sawada", "Kartik Goyal"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.23776v1", "summary": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.23776v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2311.02490", "title": "Improved Convergence Factor of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations", "authors": ["Casey Garner", "Gilad Lerman", "Teng Zhang"], "categories": ["math.NA", "cs.NA", "math.OC", "stat.ML", "65F10, 65H10, 68W40"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      40 pages, 10 figures", "url": "http://arxiv.org/abs/2311.02490v3", "summary": "This paper studies the commonly utilized windowed Anderson acceleration (AA)\nalgorithm for fixed-point methods, $x^{(k+1)}=q(x^{(k)})$. It provides the\nfirst proof that when the operator $q$ is linear and symmetric the windowed AA,\nwhich uses a sliding window of prior iterates, improves the root-linear\nconvergence factor over the fixed-point iterations. When $q$ is nonlinear, yet\nhas a symmetric Jacobian at a fixed point, a slightly modified AA algorithm is\nproved to have an analogous root-linear convergence factor improvement over\nfixed-point iterations. Simulations verify our observations. Furthermore,\nexperiments with different data models demonstrate AA is significantly superior\nto the standard fixed-point methods for Tyler's M-estimation.", "comment": "40 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2311.02490v3", "cate": "math.NA", "date": "2023-11-04", "updated": "2025-07-31"}
{"id": "2502.20632", "title": "Lattice Protein Folding with Variational Annealing", "authors": ["Shoummo Ahsan Khandoker", "Estelle M. Inack", "Mohamed Hibat-Allah"], "categories": ["cond-mat.dis-nn", "cs.AI", "cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20632v2", "summary": "Understanding the principles of protein folding is a cornerstone of\ncomputational biology, with implications for drug design, bioengineering, and\nthe understanding of fundamental biological processes. Lattice protein folding\nmodels offer a simplified yet powerful framework for studying the complexities\nof protein folding, enabling the exploration of energetically optimal folds\nunder constrained conditions. However, finding these optimal folds is a\ncomputationally challenging combinatorial optimization problem. In this work,\nwe introduce a novel upper-bound training scheme that employs masking to\nidentify the lowest-energy folds in two-dimensional Hydrophobic-Polar (HP)\nlattice protein folding. By leveraging Dilated Recurrent Neural Networks (RNNs)\nintegrated with an annealing process driven by temperature-like fluctuations,\nour method accurately predicts optimal folds for benchmark systems of up to 60\nbeads. Our approach also effectively masks invalid folds from being sampled\nwithout compromising the autoregressive sampling properties of RNNs. This\nscheme is generalizable to three spatial dimensions and can be extended to\nlattice protein models with larger alphabets. Our findings emphasize the\npotential of advanced machine learning techniques in tackling complex protein\nfolding problems and a broader class of constrained combinatorial optimization\nchallenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20632v2", "cate": "cond-mat.dis-nn", "date": "2025-02-28", "updated": "2025-07-30"}
{"id": "2503.13544", "title": "Decision by Supervised Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization", "authors": ["Juhyeong Kim", "Sungyoon Choi", "Youngbin Lee", "Yejin Kim", "Yongmin Choi", "Yongjae Lee"], "categories": ["cs.LG", "q-fin.CP", "q-fin.PM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2503.13544v4", "summary": "We propose Decision by Supervised Learning (DSL), a practical framework for\nrobust portfolio optimization. DSL reframes portfolio construction as a\nsupervised learning problem: models are trained to predict optimal portfolio\nweights, using cross-entropy loss and portfolios constructed by maximizing the\nSharpe or Sortino ratio. To further enhance stability and reliability, DSL\nemploys Deep Ensemble methods, substantially reducing variance in portfolio\nallocations. Through comprehensive backtesting across diverse market universes\nand neural architectures, shows superior performance compared to both\ntraditional strategies and leading machine learning-based methods, including\nPrediction-Focused Learning and End-to-End Learning. We show that increasing\nthe ensemble size leads to higher median returns and more stable risk-adjusted\nperformance. The code is available at https://github.com/DSLwDE/DSLwDE.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2503.13544v4", "cate": "cs.LG", "date": "2025-03-16", "updated": "2025-07-30"}
{"id": "2507.23785", "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis", "authors": ["Bowen Zhang", "Sicheng Xu", "Chuxin Wang", "Jiaolong Yang", "Feng Zhao", "Dong Chen", "Baining Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.23785v1", "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.", "comment": "ICCV 2025. Project page: https://gvfdiffusion.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.23785v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2404.12829", "title": "LiMe: a Latin Corpus of Late Medieval Criminal Sentences", "authors": ["Alessandra Bassani", "Beatrice Del Bo", "Alfio Ferrara", "Marta Mangini", "Sergio Picascia", "Ambra Stefanello"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.12829v2", "summary": "The Latin language has received attention from the computational linguistics\nresearch community, which has built, over the years, several valuable\nresources, ranging from detailed annotated corpora to sophisticated tools for\nlinguistic analysis. With the recent advent of large language models,\nresearchers have also started developing models capable of generating vector\nrepresentations of Latin texts. The performances of such models remain behind\nthe ones for modern languages, given the disparity in available data. In this\npaper, we present the LiMe dataset, a corpus of 325 documents extracted from a\nseries of medieval manuscripts called Libri sententiarum potestatis Mediolani,\nand thoroughly annotated by experts, in order to be employed for masked\nlanguage model, as well as supervised natural language processing tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.12829v2", "cate": "cs.CL", "date": "2024-04-19", "updated": "2025-07-31"}
{"id": "2312.14057", "title": "Weighted least-squares approximation with determinantal point processes and generalized volume sampling", "authors": ["Anthony Nouy", "Bertrand Michel"], "categories": ["math.NA", "cs.LG", "cs.NA", "math.ST", "stat.TH"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Compared with the first version, conjectures (13) on DPP and (16) on volume sampling have been modified, including a convexity requirement. Proofs of propositions 5.4 and 5.13 have been modified accordingly. Remarks 5.5 and 5.6 have been added to discuss alternatives to conjecture (13) on DPP", "url": "http://arxiv.org/abs/2312.14057v4", "summary": "We consider the problem of approximating a function from $L^2$ by an element\nof a given $m$-dimensional space $V_m$, associated with some feature map\n$\\boldsymbol{\\varphi}$, using evaluations of the function at random points\n$x_1, \\dots,x_n$. After recalling some results on optimal weighted\nleast-squares using independent and identically distributed points, we consider\nweighted least-squares using projection determinantal point processes (DPP) or\nvolume sampling. These distributions introduce dependence between the points\nthat promotes diversity in the selected features $\\boldsymbol{\\varphi}(x_i)$.\nWe first provide a generalized version of volume-rescaled sampling yielding\nquasi-optimality results in expectation with a number of samples $n =\nO(m\\log(m))$, that means that the expected $L^2$ error is bounded by a constant\ntimes the best approximation error in $L^2$. Also, further assuming that the\nfunction is in some normed vector space $H$ continuously embedded in $L^2$, we\nfurther prove that the approximation error in $L^2$ is almost surely bounded by\nthe best approximation error measured in the $H$-norm. This includes the cases\nof functions from $L^\\infty$ or reproducing kernel Hilbert spaces. Finally, we\npresent an alternative strategy consisting in using independent repetitions of\nprojection DPP (or volume sampling), yielding similar error bounds as with\ni.i.d. or volume sampling, but in practice with a much lower number of samples.\nNumerical experiments illustrate the performance of the different strategies.", "comment": "Compared with the first version, conjectures (13) on DPP and (16) on\n  volume sampling have been modified, including a convexity requirement. Proofs\n  of propositions 5.4 and 5.13 have been modified accordingly. Remarks 5.5 and\n  5.6 have been added to discuss alternatives to conjecture (13) on DPP", "pdf_url": "http://arxiv.org/pdf/2312.14057v4", "cate": "math.NA", "date": "2023-12-21", "updated": "2025-07-31"}
{"id": "2502.20934", "title": "Revisiting the Evaluation Bias Introduced by Frame Sampling Strategies in Surgical Video Segmentation Using SAM2", "authors": ["Utku Ozbulak", "Seyed Amir Mousavi", "Francesca Tozzi", "Niki Rashidian", "Wouter Willaert", "Wesley De Neve", "Joris Vankerschaver"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop on Fairness of AI in Medical Imaging (FAIMI), 2025", "url": "http://arxiv.org/abs/2502.20934v3", "summary": "Real-time video segmentation is a promising opportunity for AI-assisted\nsurgery, offering intraoperative guidance by identifying tools and anatomical\nstructures. Despite growing interest in surgical video segmentation, annotation\nprotocols vary widely across datasets -- some provide dense, frame-by-frame\nlabels, while others rely on sparse annotations sampled at low frame rates such\nas 1 FPS. In this study, we investigate how such inconsistencies in annotation\ndensity and frame rate sampling influence the evaluation of zero-shot\nsegmentation models, using SAM2 as a case study for cholecystectomy procedures.\nSurprisingly, we find that under conventional sparse evaluation settings, lower\nframe rates can appear to outperform higher ones due to a smoothing effect that\nconceals temporal inconsistencies. However, when assessed under real-time\nstreaming conditions, higher frame rates yield superior segmentation stability,\nparticularly for dynamic objects like surgical graspers. To understand how\nthese differences align with human perception, we conducted a survey among\nsurgeons, nurses, and machine learning engineers and found that participants\nconsistently preferred high-FPS segmentation overlays, reinforcing the\nimportance of evaluating every frame in real-time applications rather than\nrelying on sparse sampling strategies. Our findings highlight the risk of\nevaluation bias that is introduced by inconsistent dataset protocols and bring\nattention to the need for temporally fair benchmarking in surgical video AI.", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop\n  on Fairness of AI in Medical Imaging (FAIMI), 2025", "pdf_url": "http://arxiv.org/pdf/2502.20934v3", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-07-31"}
{"id": "2504.16283", "title": "Affect Models Have Weak Generalizability to Atypical Speech", "authors": ["Jaya Narain", "Amrit Romana", "Vikramjit Mitra", "Colin Lea", "Shirley Ren"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2504.16283v2", "summary": "Speech and voice conditions can alter the acoustic properties of speech,\nwhich could impact the performance of paralinguistic models for affect for\npeople with atypical speech. We evaluate publicly available models for\nrecognizing categorical and dimensional affect from speech on a dataset of\natypical speech, comparing results to datasets of typical speech. We\ninvestigate three dimensions of speech atypicality: intelligibility, which is\nrelated to pronounciation; monopitch, which is related to prosody, and\nharshness, which is related to voice quality. We look at (1) distributional\ntrends of categorical affect predictions within the dataset, (2) distributional\ncomparisons of categorical affect predictions to similar datasets of typical\nspeech, and (3) correlation strengths between text and speech predictions for\nspontaneous speech for valence and arousal. We find that the output of affect\nmodels is significantly impacted by the presence and degree of speech\natypicalities. For instance, the percentage of speech predicted as sad is\nsignificantly higher for all types and grades of atypical speech when compared\nto similar typical speech datasets. In a preliminary investigation on improving\nrobustness for atypical speech, we find that fine-tuning models on\npseudo-labeled atypical speech data improves performance on atypical speech\nwithout impacting performance on typical speech. Our results emphasize the need\nfor broader training and evaluation datasets for speech emotion models, and for\nmodeling approaches that are robust to voice and speech differences.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2504.16283v2", "cate": "cs.LG", "date": "2025-04-22", "updated": "2025-07-30"}
{"id": "2311.18266", "title": "Prompt-Based Exemplar Super-Compression and Regeneration for Class-Incremental Learning", "authors": ["Ruxiao Duan", "Jieneng Chen", "Adam Kortylewski", "Alan Yuille", "Yaoyao Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      BMVC 2025. Code: this https URL", "url": "http://arxiv.org/abs/2311.18266v3", "summary": "Replay-based methods in class-incremental learning (CIL) have attained\nremarkable success. Despite their effectiveness, the inherent memory\nrestriction results in saving a limited number of exemplars with poor\ndiversity. In this paper, we introduce PESCR, a novel approach that\nsubstantially increases the quantity and enhances the diversity of exemplars\nbased on a pre-trained general-purpose diffusion model, without fine-tuning it\non target datasets or storing it in the memory buffer. Images are compressed\ninto visual and textual prompts, which are saved instead of the original\nimages, decreasing memory consumption by a factor of 24. In subsequent phases,\ndiverse exemplars are regenerated by the diffusion model. We further propose\npartial compression and diffusion-based data augmentation to minimize the\ndomain gap between generated exemplars and real images. PESCR significantly\nimproves CIL performance across multiple benchmarks, e.g., 3.2% above the\nprevious state-of-the-art on ImageNet-100.", "comment": "BMVC 2025. Code: https://github.com/KerryDRX/PESCR", "pdf_url": "http://arxiv.org/pdf/2311.18266v3", "cate": "cs.CV", "date": "2023-11-30", "updated": "2025-07-31"}
{"id": "2406.15444", "title": "Cutting Through the Noise: Boosting LLM Performance on Math Word Problems", "authors": ["Ujjwala Anantheswaran", "Himanshu Gupta", "Kevin Scaria", "Shreyas Verma", "Chitta Baral", "Swaroop Mishra"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "url": "http://arxiv.org/abs/2406.15444v4", "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%.", "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "pdf_url": "http://arxiv.org/pdf/2406.15444v4", "cate": "cs.CL", "date": "2024-05-30", "updated": "2025-07-31"}
{"id": "2406.07108", "title": "On the power of adaption and randomization", "authors": ["David Krieg", "Erich Novak", "Mario Ullrich"], "categories": ["math.NA", "cs.CC", "cs.NA", "math.FA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.07108v2", "summary": "We present bounds on the maximal gain of adaptive and randomized algorithms\nover non-adaptive, deterministic ones for approximating linear operators on\nconvex sets. If the sets are additionally symmetric, then our results are\noptimal. For non-symmetric sets, we unify some notions of $n$-widths and\ns-numbers, and show their connection to minimal errors. We also discuss\nextensions to non-linear widths and approximation based on function values, and\nconclude with a list of open problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.07108v2", "cate": "math.NA", "date": "2024-06-11", "updated": "2025-07-31"}
{"id": "2503.09215", "title": "Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space", "authors": ["Jian Zhu", "Zhengyu Jia", "Tian Gao", "Jiaxin Deng", "Shidi Li", "Lang Zhang", "Fu Liu", "Peng Jia", "Xianpeng Lang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2503.09215v3", "summary": "Advanced end-to-end autonomous driving systems predict other vehicles'\nmotions and plan ego vehicle's trajectory. The world model that can foresee the\noutcome of the trajectory has been used to evaluate the autonomous driving\nsystem. However, existing world models predominantly emphasize the trajectory\nof the ego vehicle and leave other vehicles uncontrollable. This limitation\nhinders their ability to realistically simulate the interaction between the ego\nvehicle and the driving scenario. In this paper, we propose a driving World\nModel named EOT-WM, unifying Ego-Other vehicle Trajectories in videos for\ndriving simulation. Specifically, it remains a challenge to match multiple\ntrajectories in the BEV space with each vehicle in the video to control the\nvideo generation. We first project ego-other vehicle trajectories in the BEV\nspace into the image coordinate for vehicle-trajectory match via pixel\npositions. Then, trajectory videos are encoded by the Spatial-Temporal\nVariational Auto Encoder to align with driving video latents spatially and\ntemporally in the unified visual space. A trajectory-injected diffusion\nTransformer is further designed to denoise the noisy video latents for video\ngeneration with the guidance of ego-other vehicle trajectories. In addition, we\npropose a metric based on control latent similarity to evaluate the\ncontrollability of trajectories. Extensive experiments are conducted on the\nnuScenes dataset, and the proposed model outperforms the state-of-the-art\nmethod by 30% in FID and 55% in FVD. The model can also predict unseen driving\nscenes with self-produced trajectories.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2503.09215v3", "cate": "cs.CV", "date": "2025-03-12", "updated": "2025-07-31"}
{"id": "2505.00830", "title": "Intersectional Divergence: Measuring Fairness in Regression", "authors": ["Joe Germino", "Nuno Moniz", "Nitesh V. Chawla"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00830v2", "summary": "Fairness in machine learning research is commonly framed in the context of\nclassification tasks, leaving critical gaps in regression. In this paper, we\npropose a novel approach to measure intersectional fairness in regression\ntasks, going beyond the focus on single protected attributes from existing work\nto consider combinations of all protected attributes. Furthermore, we contend\nthat it is insufficient to measure the average error of groups without regard\nfor imbalanced domain preferences. Accordingly, we propose Intersectional\nDivergence (ID) as the first fairness measure for regression tasks that 1)\ndescribes fair model behavior across multiple protected attributes and 2)\ndifferentiates the impact of predictions in target ranges most relevant to\nusers. We extend our proposal demonstrating how ID can be adapted into a loss\nfunction, IDLoss, that satisfies convergence guarantees and has piecewise\nsmooth properties that enable practical optimization. Through an extensive\nexperimental evaluation, we demonstrate how ID allows unique insights into\nmodel behavior and fairness, and how incorporating IDLoss into optimization can\nconsiderably improve single-attribute and intersectional model fairness while\nmaintaining a competitive balance in predictive performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00830v2", "cate": "cs.LG", "date": "2025-05-01", "updated": "2025-07-31"}
{"id": "2408.02123", "title": "FovEx: Human-Inspired Explanations for Vision Transformers and Convolutional Neural Networks", "authors": ["Mahadev Prasad Panda", "Matteo Tiezzi", "Martina Vilas", "Gemma Roig", "Bjoern M. Eskofier", "Dario Zanca"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in the International Journal of Computer Vision (Springer Nature)", "url": "http://arxiv.org/abs/2408.02123v3", "summary": "Explainability in artificial intelligence (XAI) remains a crucial aspect for\nfostering trust and understanding in machine learning models. Current visual\nexplanation techniques, such as gradient-based or class-activation-based\nmethods, often exhibit a strong dependence on specific model architectures.\nConversely, perturbation-based methods, despite being model-agnostic, are\ncomputationally expensive as they require evaluating models on a large number\nof forward passes. In this work, we introduce Foveation-based Explanations\n(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly\nintegrates biologically inspired perturbations by iteratively creating foveated\nrenderings of the image and combines them with gradient-based visual\nexplorations to determine locations of interest efficiently. These locations\nare selected to maximize the performance of the model to be explained with\nrespect to the downstream task and then combined to generate an attribution\nmap. We provide a thorough evaluation with qualitative and quantitative\nassessments on established benchmarks. Our method achieves state-of-the-art\nperformance on both transformers (on 4 out of 5 metrics) and convolutional\nmodels (on 3 out of 5 metrics), demonstrating its versatility among various\narchitectures. Furthermore, we show the alignment between the explanation map\nproduced by FovEx and human gaze patterns (+14\\% in NSS compared to RISE,\n+203\\% in NSS compared to GradCAM). This comparison enhances our confidence in\nFovEx's ability to close the interpretation gap between humans and machines.", "comment": "Accepted in the International Journal of Computer Vision (Springer\n  Nature)", "pdf_url": "http://arxiv.org/pdf/2408.02123v3", "cate": "cs.CV", "date": "2024-08-04", "updated": "2025-07-31"}
{"id": "2411.18337", "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation", "authors": ["T. G. D. K. Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages,6 tables, 1 figure, Proceedings of the 1st International Conference on NLP & AI for Cyber Security", "url": "http://arxiv.org/abs/2411.18337v4", "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.", "comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security", "pdf_url": "http://arxiv.org/pdf/2411.18337v4", "cate": "cs.CL", "date": "2024-11-27", "updated": "2025-07-31"}
{"id": "2408.04503", "title": "Row-aware Randomized SVD with applications", "authors": ["Davide Palitta", "Sascha Portaro"], "categories": ["math.NA", "cs.NA", "65F55, 68W20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      28 pages, 6 figures", "url": "http://arxiv.org/abs/2408.04503v4", "summary": "The randomized singular value decomposition proposed in [27] has certainly\nbecome one of the most well-established randomization-based algorithms in\nnumerical linear algebra. The key ingredient of the entire procedure is the\ncomputation of a subspace which is close to the column space of the target\nmatrix $\\mathbf{A}$ up to a certain probabilistic confidence. In this paper we\nemploy a modification to the standard randomized SVD procedure which leads, in\ngeneral, to better approximations to $\\text{Range}(\\mathbf{A})$ at the same\ncomputational cost. To this end, we explicitly construct information from the\nrow space of $\\mathbf{A}$ enhancing the quality of the approximation. We derive\nnovel error bounds which improve over existing results for $\\mathbf{A}$ having\nimportant gaps in its singular values. We also observe that very few pieces of\ninformation from $\\text{Range}(\\mathbf{A}^T)$ may be necessary. We thus design\na variant of this algorithm equipped with a subsampling step which largely\nincreases the efficiency of the procedure while often attaining competitive\naccuracy records. Our findings are supported by both theoretical analysis and\nnumerical results.", "comment": "28 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2408.04503v4", "cate": "math.NA", "date": "2024-08-08", "updated": "2025-07-31"}
{"id": "2503.15621", "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "authors": ["Federico Cocchi", "Nicholas Moratelli", "Davide Caffagni", "Sara Sarto", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshop on What is Next in Multimodal Foundation Models", "url": "http://arxiv.org/abs/2503.15621v2", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.", "comment": "ICCV 2025 Workshop on What is Next in Multimodal Foundation Models", "pdf_url": "http://arxiv.org/pdf/2503.15621v2", "cate": "cs.CV", "date": "2025-03-19", "updated": "2025-07-31"}
{"id": "2505.05702", "title": "Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning", "authors": ["Seongjin Choi", "Gahee Kim", "Yong-Geun Oh"], "categories": ["cs.LG", "math.AT", "05C65, 55U10, 68T07"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in IEEE Access", "url": "http://arxiv.org/abs/2505.05702v3", "summary": "The absence of intrinsic adjacency relations and orientation systems in\nhypergraphs creates fundamental challenges for constructing sheaf Laplacians of\narbitrary degrees. We resolve these limitations through symmetric simplicial\nsets derived directly from hypergraphs, called symmetric simplicial lifting,\nwhich encode all possible oriented subrelations within each hyperedge as\nordered tuples. This construction canonically defines adjacency via facet maps\nwhile inherently preserving hyperedge provenance. We establish that the\nnormalized degree zero sheaf Laplacian on our symmetric simplicial lifting\nreduces exactly to the traditional graph normalized sheaf Laplacian when\nrestricted to graphs, validating its mathematical consistency with prior\ngraph-based sheaf theory. Furthermore, the induced structure preserves all\nstructural information from the original hypergraph, ensuring that every\nmulti-way relational detail is faithfully retained. Leveraging this framework,\nwe introduce Hypergraph Neural Sheaf Diffusion (HNSD), the first principled\nextension of neural sheaf diffusion to hypergraphs. HNSD operates via\nnormalized degree zero sheaf Laplacian over symmetric simplicial lifting,\nresolving orientation ambiguity and adjacency sparsity inherent to hypergraph\nlearning. Experimental evaluations demonstrate HNSDs competitive performance\nacross established benchmarks.", "comment": "Published in IEEE Access", "pdf_url": "http://arxiv.org/pdf/2505.05702v3", "cate": "cs.LG", "date": "2025-05-09", "updated": "2025-07-30"}
{"id": "2409.16178", "title": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single Image", "authors": ["Dimitrije Antić", "Georgios Paschalidis", "Shashank Tripathi", "Theo Gevers", "Sai Kumar Dwivedi", "Dimitrios Tzionas"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV'25 Camera Ready; 12 pages, 11 figures, 5 tables", "url": "http://arxiv.org/abs/2409.16178v3", "summary": "Recovering 3D object pose and shape from a single image is a challenging and\nill-posed problem. This is due to strong (self-)occlusions, depth ambiguities,\nthe vast intra- and inter-class shape variance, and the lack of 3D ground truth\nfor natural images. Existing deep-network methods are trained on synthetic\ndatasets to predict 3D shapes, so they often struggle generalizing to\nreal-world images. Moreover, they lack an explicit feedback loop for refining\nnoisy estimates, and primarily focus on geometry without directly considering\npixel alignment. To tackle these limitations, we develop a novel\nrender-and-compare optimization framework, called SDFit. This has three key\ninnovations: First, it uses a learned category-specific and morphable\nsigned-distance-function (mSDF) model, and fits this to an image by iteratively\nrefining both 3D pose and shape. The mSDF robustifies inference by constraining\nthe search on the manifold of valid shapes, while allowing for arbitrary shape\ntopologies. Second, SDFit retrieves an initial 3D shape that likely matches the\nimage, by exploiting foundational models for efficient look-up into 3D shape\ndatabases. Third, SDFit initializes pose by establishing rich 2D-3D\ncorrespondences between the image and the mSDF through foundational features.\nWe evaluate SDFit on three image datasets, i.e., Pix3D, Pascal3D+, and COMIC.\nSDFit performs on par with SotA feed-forward networks for unoccluded images and\ncommon poses, but is uniquely robust to occlusions and uncommon poses.\nMoreover, it requires no retraining for unseen images. Thus, SDFit contributes\nnew insights for generalizing in the wild. Code is available at\nhttps://anticdimi.github.io/sdfit.", "comment": "ICCV'25 Camera Ready; 12 pages, 11 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2409.16178v3", "cate": "cs.CV", "date": "2024-09-24", "updated": "2025-07-31"}
{"id": "2412.11167", "title": "Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette", "authors": ["Jiahao Yuan", "Zixiang Di", "Shangzixin Zhao", "Zhiqing Cui", "Hanqing Wang", "Guisong Yang", "Usman Naseem"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures", "url": "http://arxiv.org/abs/2412.11167v3", "summary": "Large language models (LLMs) face challenges in aligning with diverse\ncultural values despite their remarkable performance in generation, which stems\nfrom inherent monocultural biases and difficulties in capturing nuanced\ncultural semantics. Existing methods struggle to adapt to unknown culture after\nfine-tuning. Inspired by cultural geography across five continents, we propose\nCultural Palette, a multi-agent framework that redefines cultural alignment as\nan adaptive \"color-blending\" process for country-specific adaptation. Our\napproach harnesses cultural geography across five continents (Africa, America,\nAsia, Europe, Oceania) through three key steps: First, we synthesize the\nPentachromatic Cultural Palette Dataset using GPT-4o, refining\ncontinental-level dialogues with Hofstede's cultural dimensions to establish\nfoundational cultural representations. Second, five continent-level alignment\nagents form specialized cultural communities that generate region-specific\ndraft responses. Third, a Meta Agent employs Cultural MoErges to dynamically\nblend these cultural \"colors\" through attention-gated parameter merging, akin\nto mixing pigments on a palette, resolving conflicts while preserving cultural\nnuances to produce the final culturally-aligned response. Extensive experiments\nacross various countries demonstrate that Cultural Palette surpasses existing\nbaselines in cultural alignment.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2412.11167v3", "cate": "cs.CL", "date": "2024-12-15", "updated": "2025-07-31"}
{"id": "2411.19610", "title": "Unified discontinuous Galerkin analysis of a thermo/poro-viscoelasticity model", "authors": ["Stefano Bonetti", "Mattia Corti"], "categories": ["math.NA", "cs.NA", "65N30, 65N22, 65N12, 76S05"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2303.09481", "url": "http://arxiv.org/abs/2411.19610v2", "summary": "We present and analyze a discontinuous Galerkin method for the numerical\nmodeling of a Kelvin-Voigt thermo/poro-viscoelastic problem. We present the\nderivation of the model and we develop a stability analysis in the continuous\nsetting that holds both for the full inertial and quasi-static problems and\nthat is robust with respect to most of the physical parameters of the problem.\nFor spatial discretization, we propose an arbitrary-order weighted symmetric\ninterior penalty scheme that supports general polytopal grids and is robust\nwith respect to strong heterogeneities in the model coefficients. For the\nsemi-discrete problem, we prove the extension of the stability result\ndemonstrated in the continuous setting and we provide an a-priori error\nestimate. A wide set of numerical simulations is presented to assess the\nconvergence and robustness properties of the proposed method. Moreover, we test\nthe scheme with literature and physically sound test cases for proof-of-concept\napplications in the geophysical context.", "comment": "arXiv admin note: text overlap with arXiv:2303.09481", "pdf_url": "http://arxiv.org/pdf/2411.19610v2", "cate": "math.NA", "date": "2024-11-29", "updated": "2025-07-31"}
{"id": "2503.15768", "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer", "authors": ["Alexandra DeLucia", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.15768v2", "summary": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.15768v2", "cate": "cs.CL", "date": "2025-03-20", "updated": "2025-07-30"}
{"id": "2505.06275", "title": "SinBasis Networks: Matrix-Equivalent Feature Extraction for Wave-Like Optical Spectrograms", "authors": ["Yuzhou Zhu", "Zheng Zhang", "Ruyi Zhang", "Liang Zhou"], "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.optics"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.06275v2", "summary": "Wave-like images-from attosecond streaking spectrograms to optical spectra,\naudio mel-spectrograms and periodic video frames-encode critical harmonic\nstructures that elude conventional feature extractors. We propose a unified,\nmatrix-equivalent framework that reinterprets convolution and attention as\nlinear transforms on flattened inputs, revealing filter weights as basis\nvectors spanning latent feature subspaces. To infuse spectral priors we apply\nelementwise $\\sin(\\cdot)$ mappings to each weight matrix. Embedding these\ntransforms into CNN, ViT and Capsule architectures yields Sin-Basis Networks\nwith heightened sensitivity to periodic motifs and built-in invariance to\nspatial shifts. Experiments on a diverse collection of wave-like image\ndatasets-including 80,000 synthetic attosecond streaking spectrograms,\nthousands of Raman, photoluminescence and FTIR spectra, mel-spectrograms from\nAudioSet and cycle-pattern frames from Kinetics-demonstrate substantial gains\nin reconstruction accuracy, translational robustness and zero-shot cross-domain\ntransfer. Theoretical analysis via matrix isomorphism and Mercer-kernel\ntruncation quantifies how sinusoidal reparametrization enriches expressivity\nwhile preserving stability in data-scarce regimes. Sin-Basis Networks thus\noffer a lightweight, physics-informed approach to deep learning across all\nwave-form imaging modalities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.06275v2", "cate": "cs.LG", "date": "2025-05-06", "updated": "2025-07-31"}
{"id": "2410.02630", "title": "Understanding implementation pitfalls of distance-based metrics for image segmentation", "authors": ["Gasper Podobnik", "Tomaz Vrtovec"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.02630v2", "summary": "Distance-based metrics, such as the Hausdorff distance (HD), are widely used\nto validate segmentation performance in (bio)medical imaging. However, their\nimplementation is complex, and critical differences across open-source tools\nremain largely unrecognized by the community. These discrepancies undermine\nbenchmarking efforts, introduce bias in biomarker calculations, and potentially\ndistort medical device development and clinical commissioning. In this study,\nwe systematically dissect 11 open-source tools that implement distance-based\nmetric computation by performing both a conceptual analysis of their\ncomputational steps and an empirical analysis on representative two- and\nthree-dimensional image datasets. Alarmingly, we observed deviations in HD\nexceeding 100 mm and identified multiple statistically significant differences\nbetween tools - demonstrating that statistically significant improvements on\nthe same set of segmentations can be achieved simply by selecting a particular\nimplementation. These findings cast doubts on the validity of prior comparisons\nof results across studies without accounting for the differences in metric\nimplementations. To address this, we provide practical recommendations for tool\nselection; additionally, our conceptual analysis informs about the future\nevolution of implementing open-source tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.02630v2", "cate": "cs.CV", "date": "2024-10-03", "updated": "2025-07-31"}
{"id": "2503.15299", "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpektor", "Jonathan Herzig", "Roi Reichart"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to COLM 2025", "url": "http://arxiv.org/abs/2503.15299v3", "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.", "comment": "Accepted to COLM 2025", "pdf_url": "http://arxiv.org/pdf/2503.15299v3", "cate": "cs.CL", "date": "2025-03-19", "updated": "2025-07-31"}
{"id": "2501.08855", "title": "A simple-to-implement nonlinear preconditioning of Newton's method for solving the steady Navier-Stokes equations", "authors": ["Muhammad Mohebujjaman", "Mengying Xiao", "Cheng Zhang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.08855v2", "summary": "The Newton's method for solving stationary Navier-Stokes equations (NSE) is\nknown to convergent fast, however, may fail due to a bad initial guess. This\nwork presents a simple-to-implement nonlinear preconditioning of Newton's\niteration, that remains the quadratic convergence and enlarges the domain of\nconvergence. The proposed AAPicard-Newton method adds the Anderson accelerated\nPicard step at each iteration of Newton's method for solving NSE, which has\nbeen shown globally stable for the relaxation parameter $\\beta_{k+1}\\equiv1$ in\nthe Anderson acceleration optimization step, convergent quadratically, and\nconverges faster with a smaller convergence rate for large Reynolds number.\nSeveral benchmark numerical tests have been tested and are well-aligned with\nthe theoretical results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.08855v2", "cate": "math.NA", "date": "2025-01-15", "updated": "2025-07-31"}
{"id": "2503.17788", "title": "Learning to Align and Refine: A Foundation-to-Diffusion Framework for Occlusion-Robust Two-Hand Reconstruction", "authors": ["Gaoge Han", "Yongkang Cheng", "Zhe Chen", "Shaoli Huang", "Tongliang Liu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17788v2", "summary": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a dual-stage\nFoundation-to-Diffusion framework that precisely align 2D prior guidance from\nvision foundation models and diffusion-based generative 3D interaction\nrefinement to achieve occlusion-robust two-hand reconstruction. First, we\nintroduce a lightweight fusion alignment encoder that aligns fused multimodal\n2D priors like key points, segmentation maps, and depth cues from vision\nfoundation models during training. This provides robust structured guidance,\nfurther enabling efficient inference without heavy foundation model encoders at\ntest time while maintaining high reconstruction accuracy. Second, we implement\na two-hand diffusion model explicitly trained to convert interpenetrated 3D\nposes into plausible, penetration-free counterparts. Through collision\ngradient-guided denoising, the model rectifies artifacts while preserving\nnatural spatial relationships between hands. Extensive evaluations demonstrate\nthat our method achieves state-of-the-art performance on InterHand2.6M, HIC,\nand FreiHAND datasets, significantly advancing occlusion handling and\ninteraction robustness. Our code will be publicly released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17788v2", "cate": "cs.CV", "date": "2025-03-22", "updated": "2025-07-31"}
{"id": "2505.07797", "title": "A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values", "authors": ["Daniel Beechey", "Thomas M. S. Smith", "Özgür Şimşek"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.07797v2", "summary": "Reinforcement learning agents can achieve super-human performance in complex\ndecision-making tasks, but their behaviour is often difficult to understand and\nexplain. This lack of explanation limits deployment, especially in\nsafety-critical settings where understanding and trust are essential. We\nidentify three core explanatory targets that together provide a comprehensive\nview of reinforcement learning agents: behaviour, outcomes, and predictions. We\ndevelop a unified theoretical framework for explaining these three elements of\nreinforcement learning agents through the influence of individual features that\nthe agent observes in its environment. We derive feature influences by using\nShapley values, which collectively and uniquely satisfy a set of well-motivated\naxioms for fair and consistent credit assignment. The proposed approach,\nShapley Values for Explaining Reinforcement Learning (SVERL), provides a single\ntheoretical framework to comprehensively and meaningfully explain reinforcement\nlearning agents. It yields explanations with precise semantics that are not\nonly interpretable but also mathematically justified, enabling us to identify\nand correct conceptual issues in prior explanations. Through illustrative\nexamples, we show how SVERL produces useful, intuitive explanations of agent\nbehaviour, outcomes, and predictions, which are not apparent from observing\nagent behaviour alone.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.07797v2", "cate": "cs.LG", "date": "2025-05-12", "updated": "2025-07-31"}
{"id": "2411.04351", "title": "LidaRefer: Context-aware Outdoor 3D Visual Grounding for Autonomous Driving", "authors": ["Yeong-Seung Baek", "Heung-Seon Oh"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 5 figures", "url": "http://arxiv.org/abs/2411.04351v2", "summary": "3D visual grounding (VG) aims to locate objects or regions within 3D scenes\nguided by natural language descriptions. While indoor 3D VG has advanced,\noutdoor 3D VG remains underexplored due to two challenges: (1) large-scale\noutdoor LiDAR scenes are dominated by background points and contain limited\nforeground information, making cross-modal alignment and contextual\nunderstanding more difficult; and (2) most outdoor datasets lack spatial\nannotations for referential non-target objects, which hinders explicit learning\nof referential context. To this end, we propose LidaRefer, a context-aware 3D\nVG framework for outdoor scenes. LidaRefer incorporates an object-centric\nfeature selection strategy to focus on semantically relevant visual features\nwhile reducing computational overhead. Then, its transformer-based\nencoder-decoder architecture excels at establishing fine-grained cross-modal\nalignment between refined visual features and word-level text features, and\ncapturing comprehensive global context. Additionally, we present\nDiscriminative-Supportive Collaborative localization (DiSCo), a novel\nsupervision strategy that explicitly models spatial relationships between\ntarget, contextual, and ambiguous objects for accurate target identification.\nTo enable this without manual labeling, we introduce a pseudo-labeling approach\nthat retrieves 3D localization labels for referential non-target objects.\nLidaRefer achieves state-of-the-art performance on Talk2Car-3D dataset under\nvarious evaluation settings.", "comment": "18 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2411.04351v2", "cate": "cs.CV", "date": "2024-11-07", "updated": "2025-07-31"}
{"id": "2504.04640", "title": "Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation", "authors": ["Eylon Caplan", "Tania Chakraborty", "Dan Goldwasser"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint, under review", "url": "http://arxiv.org/abs/2504.04640v2", "summary": "Variation in language use, shaped by speakers' sociocultural background and\nspecific context of use, offers a rich lens into cultural perspectives, values,\nand opinions. However, the computational study of these Sociocultural\nLinguistic Phenomena (SLP) has often been limited to bespoke analyses of\nspecific groups or topics, hindering the pace of scientific discovery. To\naddress this, we introduce Splits!, a 9.7 million-post dataset from Reddit\ndesigned for systematic and flexible research. The dataset contains posts from\nover 53,000 users across 6 demographic groups, organized into 89 discussion\ntopics to enable comparative analysis. We validate Splits! via\nself-identification and by successfully replicating several known SLPs from\nexisting literature. We complement this dataset with a framework that leverages\nefficient retrieval methods to rapidly validate potential SLPs (PSLPs) by\nautomatically evaluating whether a given hypothesis is supported by our data.\nCrucially, to distinguish between novel and obvious insights, the framework\nincorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We\ndemonstrate that the two-stage process reduces the number of statistically\nsignificant findings requiring manual inspection by a factor of 1.5-1.8x,\nstreamlining the discovery of promising phenomena for further investigation.", "comment": "Preprint, under review", "pdf_url": "http://arxiv.org/pdf/2504.04640v2", "cate": "cs.CL", "date": "2025-04-06", "updated": "2025-07-31"}
{"id": "2505.08884", "title": "Jacobian-Free Newton-Krylov with a globalization method for solving groundwater flow models of multi-layer aquifer systems", "authors": ["Raghav Singhal", "Emin Can Dogrul", "Zhaojun Bai"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08884v2", "summary": "A Jacobian free Newton Krylov (JFNK) method with a globalization scheme is\nintroduced to solve large and complex nonlinear systems of equations that arise\nin groundwater flow models of multi-layer aquifer systems. We explore the\nadvantages of the JFNK method relative to the Newton-Krylov (NK) method and\nidentify the circumstances in which the JFNK method demonstrates computing\nefficiency. We perform the validation and efficiency of the JFNK method on\nvarious test cases involving an unconfined single-layer aquifer and a two-layer\naquifer with both confined and unconfined conditions. The results are validated\nby the NK method. The JFNK method is incorporated in Integrated Water Flow\nModel (IWFM), an integrated hydrologic model developed and maintained by\nCalifornia Department of Water Resources. We examine the determinacy of the\nJFNK's adaptability on practical models such as the California Central Valley\nGroundwater-Surface Water Simulation Model (C2VSim).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08884v2", "cate": "math.NA", "date": "2025-05-13", "updated": "2025-07-30"}
{"id": "2504.09753", "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Siddhant Gupta", "Drishti Sharma", "Jebish Purbey", "Kanwal Mehreen", "Muhammad Arham", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      24 pages, 18 figures", "url": "http://arxiv.org/abs/2504.09753v3", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages.", "comment": "24 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2504.09753v3", "cate": "cs.CL", "date": "2025-04-13", "updated": "2025-07-31"}
{"id": "2505.18102", "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?", "authors": ["Takashi Ishida", "Thanawat Lodkaew", "Ikko Yamane"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Extended version of the paper presented as an Oral at the ICML 2025 Workshop on the Impact of Memorization on Trustworthy Foundation Models", "url": "http://arxiv.org/abs/2505.18102v2", "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.", "comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models", "pdf_url": "http://arxiv.org/pdf/2505.18102v2", "cate": "cs.LG", "date": "2025-05-23", "updated": "2025-07-31"}
{"id": "2411.11098", "title": "MolParser: End-to-end Visual Recognition of Molecule Structures in the Wild", "authors": ["Xi Fang", "Jiankun Wang", "Xiaochen Cai", "Shangqian Chen", "Shuwen Yang", "Haoyi Tao", "Nan Wang", "Lin Yao", "Linfeng Zhang", "Guolin Ke"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.11098v3", "summary": "In recent decades, chemistry publications and patents have increased rapidly.\nA significant portion of key information is embedded in molecular structure\nfigures, complicating large-scale literature searches and limiting the\napplication of large language models in fields such as biology, chemistry, and\npharmaceuticals. The automatic extraction of precise chemical structures is of\ncritical importance. However, the presence of numerous Markush structures in\nreal-world documents, along with variations in molecular image quality, drawing\nstyles, and noise, significantly limits the performance of existing optical\nchemical structure recognition (OCSR) methods. We present MolParser, a novel\nend-to-end OCSR method that efficiently and accurately recognizes chemical\nstructures from real-world documents, including difficult Markush structure. We\nuse a extended SMILES encoding rule to annotate our training dataset. Under\nthis rule, we build MolParser-7M, the largest annotated molecular image dataset\nto our knowledge. While utilizing a large amount of synthetic data, we employed\nactive learning methods to incorporate substantial in-the-wild data,\nspecifically samples cropped from real patents and scientific literature, into\nthe training process. We trained an end-to-end molecular image captioning\nmodel, MolParser, using a curriculum learning approach. MolParser significantly\noutperforms classical and learning-based methods across most scenarios, with\npotential for broader downstream applications. The dataset is publicly\navailable in huggingface.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.11098v3", "cate": "cs.CV", "date": "2024-11-17", "updated": "2025-07-31"}
{"id": "2504.11952", "title": "Robust and Fine-Grained Detection of AI Generated Texts", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      18 pages, 6 figures", "url": "http://arxiv.org/abs/2504.11952v3", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.", "comment": "18 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2504.11952v3", "cate": "cs.CL", "date": "2025-04-16", "updated": "2025-07-31"}
{"id": "2404.09363", "title": "Momentum-based gradient descent methods for Lie groups", "authors": ["Cédric M. Campos", "David Martín de Diego", "José Torrente"], "categories": ["math.OC", "cs.LG", "cs.NA", "math.DG", "math.NA", "65K10 (Primary) 70G45, 22E99 (Secondary)"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      22 pages, 2 algorithms, 6 figures", "url": "http://arxiv.org/abs/2404.09363v2", "summary": "Polyak's Heavy Ball (PHB; Polyak, 1964), a.k.a. Classical Momentum, and\nNesterov's Accelerated Gradient (NAG; Nesterov, 1983) are well-established\nmomentum-descent methods for optimization. Although the latter generally\noutperforms the former, primarily, generalizations of PHB-like methods to\nnonlinear spaces have not been sufficiently explored in the literature. In this\npaper, we propose a generalization of NAG-like methods for Lie group\noptimization. This generalization is based on the variational one-to-one\ncorrespondence between classical and accelerated momentum methods (Campos et\nal., 2023). We provide numerical experiments for chosen retractions on the\ngroup of rotations based on the Frobenius norm and the Rosenbrock function to\ndemonstrate the effectiveness of our proposed methods, and that align with\nresults of the Euclidean case, that is, a faster convergence rate for NAG.", "comment": "22 pages, 2 algorithms, 6 figures", "pdf_url": "http://arxiv.org/pdf/2404.09363v2", "cate": "math.OC", "date": "2024-04-14", "updated": "2025-07-31"}
{"id": "2505.23628", "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora", "authors": ["Jiaxin Bai", "Wei Fan", "Qi Hu", "Qing Zong", "Chunyang Li", "Hong Ting Tsang", "Hongyu Luo", "Yauwai Yim", "Haoyu Huang", "Xiao Zhou", "Feng Qin", "Tianshi Zheng", "Xi Peng", "Xin Yao", "Huiwen Yang", "Leijie Wu", "Yi Ji", "Gong Zhang", "Renhai Chen", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages, preprint, code: this https URL", "url": "http://arxiv.org/abs/2505.23628v2", "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.", "comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG", "pdf_url": "http://arxiv.org/pdf/2505.23628v2", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-07-31"}
{"id": "2505.20553", "title": "A ZeNN architecture to avoid the Gaussian trap", "authors": ["Luís Carvalho", "João L. Costa", "José Mourão", "Gonçalo Oliveira"], "categories": ["cs.LG", "math.PR", "68T07, 68T01", "I.2.0; G.0"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      New experiments involving PiNNs for solving Schrödinger and Bessel-type equations", "url": "http://arxiv.org/abs/2505.20553v2", "summary": "We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order\nto overcome several shortcomings of standard multi-layer perceptrons (MLPs).\nNamely, in the large width limit, MLPs are non-parametric, they do not have a\nwell-defined pointwise limit, they lose non-Gaussian attributes and become\nunable to perform feature learning; moreover, finite width MLPs perform poorly\nin learning high frequencies. The new ZeNN architecture is inspired by three\nsimple principles from harmonic analysis:\n  i) Enumerate the perceptons and introduce a non-learnable weight to enforce\nconvergence;\n  ii) Introduce a scaling (or frequency) factor;\n  iii) Choose activation functions that lead to near orthogonal systems.\n  We will show that these ideas allow us to fix the referred shortcomings of\nMLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they\nexhibit a rich asymptotic structure beyond Gaussianity, and perform feature\nlearning. Moreover, when appropriate activation functions are chosen, (finite\nwidth) ZeNNs excel at learning high-frequency features of functions with low\ndimensional domains.", "comment": "New experiments involving PiNNs for solving Schr\\\"odinger and\n  Bessel-type equations", "pdf_url": "http://arxiv.org/pdf/2505.20553v2", "cate": "cs.LG", "date": "2025-05-26", "updated": "2025-07-31"}
{"id": "2411.18823", "title": "Multi-Task Label Discovery via Hierarchical Task Tokens for Partially Annotated Dense Predictions", "authors": ["Jingdong Zhang", "Hanrong Ye", "Xin Li", "Wenping Wang", "Dan Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.18823v2", "summary": "In recent years, simultaneous learning of multiple dense prediction tasks\nwith partially annotated label data has emerged as an important research area.\nPrevious works primarily focus on leveraging cross-task relations or conducting\nadversarial training for extra regularization, which achieve promising\nperformance improvements, while still suffering from the lack of direct\npixel-wise supervision and extra training of heavy mapping networks. To\neffectively tackle this challenge, we propose a novel approach to optimize a\nset of compact learnable hierarchical task tokens, including global and\nfine-grained ones, to discover consistent pixel-wise supervision signals in\nboth feature and prediction levels. Specifically, the global task tokens are\ndesigned for effective cross-task feature interactions in a global context.\nThen, a group of fine-grained task-specific spatial tokens for each task is\nlearned from the corresponding global task tokens. It is embedded to have dense\ninteractions with each task-specific feature map. The learned global and local\nfine-grained task tokens are further used to discover pseudo task-specific\ndense labels at different levels of granularity, and they can be utilized to\ndirectly supervise the learning of the multi-task dense prediction framework.\nExtensive experimental results on challenging NYUD-v2, Cityscapes, and PASCAL\nContext datasets demonstrate significant improvements over existing\nstate-of-the-art methods for partially annotated multi-task dense prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.18823v2", "cate": "cs.CV", "date": "2024-11-27", "updated": "2025-07-31"}
{"id": "2504.16060", "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      COLM 2025 & CVinW @ CVPR 2025 (Spotlight). Homepage: this https URL", "url": "http://arxiv.org/abs/2504.16060v3", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication.", "comment": "COLM 2025 & CVinW @ CVPR 2025 (Spotlight). Homepage:\n  https://vlm-reg.github.io/", "pdf_url": "http://arxiv.org/pdf/2504.16060v3", "cate": "cs.CL", "date": "2025-04-22", "updated": "2025-07-31"}
{"id": "2412.12180", "title": "Fully stochastic trust-region methods with Barzilai-Borwein steplengths", "authors": ["Stefania Bellavia", "Benedetta Morini", "Mahsa Yousefi"], "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.12180v2", "summary": "We investigate stochastic gradient methods and stochastic counterparts of the\nBarzilai-Borwein steplengths and their application to finite-sum minimization\nproblems. Our proposal is based on the Trust-Region-ish (TRish) framework\nintroduced in [F. E. Curtis, K. Scheinberg, R. Shi, {\\it A stochastic trust\nregion algorithm based on careful step normalization}, Informs Journal on\nOptimization, 1, 2019]. The new framework, named TRishBB, aims to enhance the\nperformance of TRish and at reducing the computational cost of the second-order\nTRish variant. We propose three different methods belonging to the TRishBB\nframework and present the convergence analysis for possibly nonconvex objective\nfunctions, considering biased and unbiased gradient approximations. Our\nanalysis requires neither diminishing step-sizes nor full gradient evaluation.\nThe numerical experiments in machine learning applications demonstrate the\neffectiveness of applying the Barzilai-Borwein steplength with stochastic\ngradients and show improved testing accuracy compared to the TRish method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.12180v2", "cate": "math.OC", "date": "2024-12-13", "updated": "2025-07-31"}
{"id": "2506.00068", "title": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2506.00068v2", "summary": "Large Language Models (LLMs) increasingly shape public discourse, yet most\nevaluations of political and economic bias have focused on high-resource,\nWestern languages and contexts. This leaves critical blind spots in\nlow-resource, multilingual regions such as Pakistan, where linguistic identity\nis closely tied to political, religious, and regional ideologies. We present a\nsystematic evaluation of political bias in 13 state-of-the-art LLMs across five\nPakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework\nintegrates a culturally adapted Political Compass Test (PCT) with multi-level\nframing analysis, capturing both ideological stance (economic/social axes) and\nstylistic framing (content, tone, emphasis). Prompts are aligned with 11\nsocio-political themes specific to the Pakistani context. Results show that\nwhile LLMs predominantly reflect liberal-left orientations consistent with\nWestern training data, they exhibit more authoritarian framing in regional\nlanguages, highlighting language-conditioned ideological modulation. We also\nidentify consistent model-specific bias patterns across languages. These\nfindings show the need for culturally grounded, multilingual bias auditing\nframeworks in global NLP.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2506.00068v2", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-07-31"}
{"id": "2506.03956", "title": "Adapt before Continual Learning", "authors": ["Aojun Lu", "Tao Feng", "Hangjie Yuan", "Chunhui Ding", "Yanan Sun"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.03956v3", "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). Although pre-trained models (PTMs) have provided a strong\nfoundation for CL, existing approaches face a fundamental challenge in\nbalancing these two competing objectives. Current methods typically address\nstability by freezing the PTM backbone, which severely limits the model's\nplasticity, particularly when incoming data distribution diverges largely from\nthe pre-training data. Alternatively, sequentially fine-tuning the entire PTM\ncan adapt to new knowledge but often leads to catastrophic forgetting,\nhighlighting the critical stability-plasticity trade-off in PTM-based CL. To\naddress this limitation, we propose Adapting PTMs before the core CL} process\n(ACL), a novel framework that introduces a plug-and-play adaptation phase prior\nto learning each new task. During this phase, ACL refines the PTM backbone by\naligning embeddings with their original class prototypes while distancing them\nfrom irrelevant classes. This mechanism theoretically and empirically\ndemonstrates desirable balance between stability and plasticity, significantly\nimproving CL performance across benchmarks and integrated methods. Code is\navailable at https://github.com/byyx666/ACL_code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.03956v3", "cate": "cs.LG", "date": "2025-06-04", "updated": "2025-07-31"}
{"id": "2412.06458", "title": "Pruning All-Rounder: Rethinking and Improving Inference Efficiency for Large Vision Language Models", "authors": ["Wei Suo", "Ji Ma", "Mengyang Sun", "Lin Yuanbo Wu", "Peng Wang", "Yanning Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 25", "url": "http://arxiv.org/abs/2412.06458v2", "summary": "Although Large Vision-Language Models (LVLMs) have achieved impressive\nresults, their high computational costs pose a significant barrier to wide\napplication. To enhance inference efficiency, most existing approaches can be\ncategorized as parameter-dependent or token-dependent strategies to reduce\ncomputational demands. However, parameter-dependent methods require retraining\nLVLMs to recover performance while token-dependent strategies struggle to\nconsistently select the most relevant tokens. In this paper, we systematically\nanalyze the above challenges and provide a series of valuable insights for\ninference acceleration. Based on these findings, we propose a novel framework,\nthe Pruning All-Rounder (PAR). Different from previous works, PAR develops a\nmeta-router to adaptively organize pruning flows across both tokens and layers.\nWith a self-supervised learning manner, our method achieves a superior balance\nbetween performance and efficiency. Notably, PAR is highly flexible, offering\nmultiple pruning versions to address a range of acceleration scenarios. The\ncode for this work is publicly available at\nhttps://github.com/ASGO-MM/Pruning-All-Rounder.", "comment": "Accepted by ICCV 25", "pdf_url": "http://arxiv.org/pdf/2412.06458v2", "cate": "cs.CV", "date": "2024-12-09", "updated": "2025-07-31"}
{"id": "2505.18497", "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.18497v2", "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.18497v2", "cate": "cs.CL", "date": "2025-05-24", "updated": "2025-07-31"}
{"id": "2505.05085", "title": "Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation", "authors": ["Gary Froyland", "Kevin Kühl"], "categories": ["math.DS", "cs.LG", "cs.NA", "math.NA", "47A15, 37C30, 47A58, 68T07"], "primary_category": "Subjects:       Dynamical Systems (math.DS)", "pdf_link": null, "comments": "Comments:      23 pages, 13 figures", "url": "http://arxiv.org/abs/2505.05085v2", "summary": "Transfer and Koopman operator methods offer a framework for representing\ncomplex, nonlinear dynamical systems via linear transformations, enabling a\ndeeper understanding of the underlying dynamics. The spectra of these operators\nprovide important insights into system predictability and emergent behaviour,\nalthough efficiently estimating them from data can be challenging. We approach\nthis issue through the lens of general operator and representational learning,\nin which we approximate these linear operators using efficient\nfinite-dimensional representations. Specifically, we machine-learn orthonormal\nbasis functions that are dynamically tailored to the system. This learned basis\nprovides a particularly accurate approximation of the operator's action as well\nas a nearly invariant finite-dimensional subspace. We illustrate our approach\nwith examples that showcase the retrieval of spectral properties from the\nestimated operator, and emphasise the dynamically adaptive quality of the\nmachine-learned basis.", "comment": "23 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2505.05085v2", "cate": "math.DS", "date": "2025-05-08", "updated": "2025-07-30"}
{"id": "2506.07106", "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 KnowFM", "url": "http://arxiv.org/abs/2506.07106v2", "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.", "comment": "ACL 2025 KnowFM", "pdf_url": "http://arxiv.org/pdf/2506.07106v2", "cate": "cs.CL", "date": "2025-06-08", "updated": "2025-07-31"}
{"id": "2506.12284", "title": "GrokAlign: Geometric Characterisation and Acceleration of Grokking", "authors": ["Thomas Walker", "Ahmed Imtiaz Humayun", "Randall Balestriero", "Richard Baraniuk"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      23 pages, 11 figures, 3 tables", "url": "http://arxiv.org/abs/2506.12284v2", "summary": "A key challenge for the machine learning community is to understand and\naccelerate the training dynamics of deep networks that lead to delayed\ngeneralisation and emergent robustness to input perturbations, also known as\ngrokking. Prior work has associated phenomena like delayed generalisation with\nthe transition of a deep network from a linear to a feature learning regime,\nand emergent robustness with changes to the network's functional geometry, in\nparticular the arrangement of the so-called linear regions in deep networks\nemploying continuous piecewise affine nonlinearities. Here, we explain how\ngrokking is realised in the Jacobian of a deep network and demonstrate that\naligning a network's Jacobians with the training data (in the sense of cosine\nsimilarity) ensures grokking under a low-rank Jacobian assumption. Our results\nprovide a strong theoretical motivation for the use of Jacobian regularisation\nin optimizing deep networks -- a method we introduce as GrokAlign -- which we\nshow empirically to induce grokking much sooner than more conventional\nregularizers like weight decay. Moreover, we introduce centroid alignment as a\ntractable and interpretable simplification of Jacobian alignment that\neffectively identifies and tracks the stages of deep network training dynamics.\nAccompanying webpage (https://thomaswalker1.github.io/blog/grokalign.html) and\ncode (https://github.com/ThomasWalker1/grokalign).", "comment": "23 pages, 11 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.12284v2", "cate": "cs.LG", "date": "2025-06-14", "updated": "2025-07-31"}
{"id": "2501.02201", "title": "Acknowledging Focus Ambiguity in Visual Questions", "authors": ["Chongyan Chen", "Yu-Yun Tseng", "Zhuoheng Li", "Anush Venkatesh", "Danna Gurari"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.02201v2", "summary": "No published work on visual question answering (VQA) accounts for ambiguity\nregarding where the content described in the question is located in the image.\nTo fill this gap, we introduce VQ-FocusAmbiguity, the first VQA dataset that\nvisually grounds each plausible image region a question could refer to when\narriving at valid answers. We next analyze and compare our dataset to existing\ndatasets to reveal its unique properties. Finally, we benchmark modern models\nfor two novel tasks related to acknowledging focus ambiguity: recognizing\nwhether a visual question has focus ambiguity and locating all plausible focus\nregions within the image. Results show that the dataset is challenging for\nmodern models. To facilitate future progress on these tasks, we publicly share\nthe dataset with an evaluation server at\nhttps://vizwiz.org/tasks-and-datasets/focus-ambiguity-in-visual-questions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.02201v2", "cate": "cs.CV", "date": "2025-01-04", "updated": "2025-07-31"}
{"id": "2506.08184", "title": "Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 Workshop on Long Context Foundation Models (ICFM). Code: this https URL", "url": "http://arxiv.org/abs/2506.08184v3", "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.", "comment": "Accepted at ICML 2025 Workshop on Long Context Foundation Models\n  (ICFM). Code: https://github.com/zhuangziGiantfish/Unable-to-Forget", "pdf_url": "http://arxiv.org/pdf/2506.08184v3", "cate": "cs.CL", "date": "2025-06-09", "updated": "2025-07-31"}
{"id": "2507.13492", "title": "On the time integration for phase field modeling of grain growth in additive manufacturing", "authors": ["Chaoqian Yuan", "Chinnapat Panwisawas", "Ye Lu"], "categories": ["physics.comp-ph", "cs.NA", "math.NA"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13492v3", "summary": "Phase field simulations play a key role in the understanding of\nmicrostructure evolution in additive manufacturing. However, they have been\nfound extremely computationally expensive. One of the reasons is the small time\nstep requirement to resolve the complex microstructure evolution during the\nrapid solidification process. This paper investigates the possibility of using\na class of stabilized time integration algorithms to accelerate such phase\nfield simulations by increasing the time steps. The specific time integration\nformulation and theoretical analysis on energy stability were developed, based\non a phase field model dedicated to simulating rapid solidification in additive\nmanufacturing. The numerical results confirmed that the proposed method can\nensure the numerical stability and a decreasing energy requirement for the\nphase field simulations with at least two orders-of-magnitude larger time steps\nover conventional explicit methods. 2D and 3D phase field simulations have been\nconducted with relevant physical and kinetic parameters for 316L stainless\nsteels. This work provides a numerical framework for efficient phase field\nsimulations and open numerous opportunities for large scale phase field\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13492v3", "cate": "physics.comp-ph", "date": "2025-07-17", "updated": "2025-07-30"}
{"id": "2506.10006", "title": "HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction", "authors": ["Jie Qin", "Wei Yang", "Yan Su", "Yiran Zhu", "Weizhen Li", "Yunyue Pan", "Chengchang Pan", "Honggang Qi"], "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      8 pages,6 figures,3 tables,accepted by the 33rd ACM International Conference on Multimedia(ACM MM 2025)", "url": "http://arxiv.org/abs/2506.10006v2", "summary": "In breast cancer HER2 assessment, clinical evaluation relies on combined H&E\nand IHC images, yet acquiring both modalities is often hindered by clinical\nconstraints and cost. We propose an adaptive bimodal prediction framework that\nflexibly supports single- or dual-modality inputs through two core innovations:\na dynamic branch selector activating modality completion or joint inference\nbased on input availability, and a cross-modal GAN (CM-GAN) enabling\nfeature-space reconstruction of missing modalities. This design dramatically\nimproves H&E-only accuracy from 71.44% to 94.25%, achieves 95.09% with full\ndual-modality inputs, and maintains 90.28% reliability under single-modality\nconditions. The \"dual-modality preferred, single-modality compatible\"\narchitecture delivers near-dual-modality accuracy without mandatory\nsynchronized acquisition, offering a cost-effective solution for\nresource-limited regions and significantly improving HER2 assessment\naccessibility.", "comment": "8 pages,6 figures,3 tables,accepted by the 33rd ACM International\n  Conference on Multimedia(ACM MM 2025)", "pdf_url": "http://arxiv.org/pdf/2506.10006v2", "cate": "cs.MM", "date": "2025-04-12", "updated": "2025-07-31"}
{"id": "2506.14781", "title": "Two-dimensional Parallel Tempering for Constrained Optimization", "authors": ["Corentin Delacour", "M Mahmudul Hasan Sajeeb", "Joao P. Hespanha", "Kerem Y. Camsari"], "categories": ["cs.LG", "cond-mat.stat-mech", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Added references in Introduction", "url": "http://arxiv.org/abs/2506.14781v2", "summary": "Sampling Boltzmann probability distributions plays a key role in machine\nlearning and optimization, motivating the design of hardware accelerators such\nas Ising machines. While the Ising model can in principle encode arbitrary\noptimization problems, practical implementations are often hindered by soft\nconstraints that either slow down mixing when too strong, or fail to enforce\nfeasibility when too weak. We introduce a two-dimensional extension of the\npowerful parallel tempering algorithm (PT) that addresses this challenge by\nadding a second dimension of replicas interpolating the penalty strengths. This\nscheme ensures constraint satisfaction in the final replicas, analogous to\nlow-energy states at low temperature. The resulting two-dimensional parallel\ntempering algorithm (2D-PT) improves mixing in heavily constrained replicas and\neliminates the need to explicitly tune the penalty strength. In a\nrepresentative example of graph sparsification with copy constraints, 2D-PT\nachieves near-ideal mixing, with Kullback-Leibler divergence decaying as\nO(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of\nmagnitude speedup over conventional PT with the same number of replicas. The\nmethod applies broadly to constrained Ising problems and can be deployed on\nexisting Ising machines.", "comment": "Added references in Introduction", "pdf_url": "http://arxiv.org/pdf/2506.14781v2", "cate": "cs.LG", "date": "2025-05-24", "updated": "2025-07-30"}
{"id": "2502.16421", "title": "Learning from Rendering: Realistic and Controllable Extreme Rainy Image Synthesis for Autonomous Driving Simulation", "authors": ["Kaibin Zhou", "Kaifeng Huang", "Hao Deng", "Zelin Tao", "Ziniu Liu", "Lin Zhang", "Shengjie Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16421v2", "summary": "Autonomous driving simulators provide an effective and low-cost alternative\nfor evaluating or enhancing visual perception models. However, the reliability\nof evaluation depends on the diversity and realism of the generated scenes.\nExtreme weather conditions, particularly extreme rainfalls, are rare and costly\nto capture in real-world settings. While simulated environments can help\naddress this limitation, existing rainy image synthesizers often suffer from\npoor controllability over illumination and limited realism, which significantly\nundermines the effectiveness of the model evaluation. To that end, we propose a\nlearning-from-rendering rainy image synthesizer, which combines the benefits of\nthe realism of rendering-based methods and the controllability of\nlearning-based methods. To validate the effectiveness of our extreme rainy\nimage synthesizer on semantic segmentation task, we require a continuous set of\nwell-labeled extreme rainy images. By integrating the proposed synthesizer with\nthe CARLA driving simulator, we develop CARLARain an extreme rainy street scene\nsimulator which can obtain paired rainy-clean images and labels under complex\nillumination conditions. Qualitative and quantitative experiments validate that\nCARLARain can effectively improve the accuracy of semantic segmentation models\nin extreme rainy scenes, with the models' accuracy (mIoU) improved by 5% - 8%\non the synthetic dataset and significantly enhanced in real extreme rainy\nscenarios under complex illuminations. Our source code and datasets are\navailable at https://github.com/kb824999404/CARLARain/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16421v2", "cate": "cs.CV", "date": "2025-02-23", "updated": "2025-07-31"}
{"id": "2506.12365", "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics", "authors": ["Asifullah Khan", "Muhammad Zaeem Khan", "Saleha Jamshed", "Sadia Ahmad", "Aleesha Zainab", "Kaynat Khatib", "Faria Bibi", "Abdul Rehman"], "categories": ["cs.CL", "cs.DB"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12365v2", "summary": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), including enhancements to their reasoning skills,\nadaptability to various tasks, increased computational efficiency, and the\nability to make ethical decisions. The techniques that have been most effective\nin bridging the gap between human and machine communications include the\nChain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from\nHuman Feedback. The improvements in multimodal learning and few-shot or\nzero-shot techniques have further empowered LLMs to handle complex jobs with\nminor input. A significant focus is placed on efficiency, detailing scaling\nstrategies, optimization techniques, and the influential Mixture-of-Experts\n(MoE) architecture, which strategically routes inputs to specialized\nsubnetworks to boost predictive accuracy, while optimizing resource allocation.\nThis survey also offers a broader perspective on recent advancements in LLMs,\ngoing beyond isolated aspects such as model architecture or ethical concerns.\nAdditionally, it explores the role of LLMs in Agentic AI and their use as\nAutonomous Decision-Making Systems, and categorizes emerging methods that\nenhance LLM reasoning, efficiency, and ethical alignment. The survey also\nidentifies underexplored areas such as interpretability, cross-modal\nintegration, and sustainability. While significant advancements have been made\nin LLMs, challenges such as high computational costs, biases, and ethical risks\nremain. Overcoming these requires a focus on bias mitigation, transparent\ndecision-making, and explicit ethical guidelines. Future research will\ngenerally focus on enhancing the model's ability to handle multiple inputs,\nthereby making it more intelligent, safe, and reliable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12365v2", "cate": "cs.CL", "date": "2025-06-14", "updated": "2025-07-31"}
{"id": "2506.17247", "title": "Recursive Learning-Based Virtual Buffering for Analytical Global Placement", "authors": ["Andrew B. Kahng", "Yiting Liu", "Zhiang Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17247v2", "summary": "Due to the skewed scaling of interconnect versus cell delay in modern\ntechnology nodes, placement with buffer porosity (i.e., cell density) awareness\nis essential for timing closure in physical synthesis flows. However, existing\napproaches face two key challenges: (i) traditional van Ginneken-Lillis-style\nbuffering approaches are computationally expensive during global placement; and\n(ii) machine learning-based approaches, such as BufFormer, lack a thorough\nconsideration of Electrical Rule Check (ERC) violations and fail to \"close the\nloop\" back into the physical design flow. In this work, we propose\nMLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware\nanalytical global placement framework, built on top of the OpenROAD\ninfrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based\ngenerative buffering approach to predict buffer types and locations, addressing\nERC violations during global placement. We compare MLBuf-RePlAce against the\ndefault virtual buffering-based timing-driven global placer in OpenROAD, using\nopen-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts\nrepositories. Without degradation of post-route power, MLBuf-RePlAce achieves\n(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)\nwithin the open-source OpenROAD flow. When evaluated by completion in a\ncommercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of\n(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17247v2", "cate": "cs.LG", "date": "2025-06-07", "updated": "2025-07-30"}
{"id": "2506.15692", "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement", "authors": ["Jaehyun Nam", "Jinsung Yoon", "Jiefeng Chen", "Jinwoo Shin", "Sercan Ö. Arık", "Tomas Pfister"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15692v2", "summary": "Agents based on large language models (LLMs) for machine learning engineering\n(MLE) can automatically implement ML models via code generation. However,\nexisting approaches to build such agents often rely heavily on inherent LLM\nknowledge and employ coarse exploration strategies that modify the entire code\nstructure at once. This limits their ability to select effective task-specific\nmodels and perform deep exploration within specific components, such as\nexperimenting extensively with feature engineering options. To overcome these,\nwe propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first\nleverages external knowledge by using a search engine to retrieve effective\nmodels from the web, forming an initial solution, then iteratively refines it\nby exploring various strategies targeting specific ML components. This\nexploration is guided by ablation studies analyzing the impact of individual\ncode blocks. Furthermore, we introduce a novel ensembling method using an\neffective strategy suggested by MLE-STAR. Our experimental results show that\nMLE-STAR achieves medals in 64% of the Kaggle competitions on the MLE-bench\nLite, significantly outperforming the best alternative.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15692v2", "cate": "cs.LG", "date": "2025-05-27", "updated": "2025-07-30"}
{"id": "2502.20263", "title": "Vector-Quantized Vision Foundation Models for Object-Centric Learning", "authors": ["Rongzhen Zhao", "Vivienne Wang", "Juho Kannala", "Joni Pajarinen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2502.20263v4", "summary": "Perceiving visual scenes as objects and background--like humans\ndo--Object-Centric Learning (OCL) aggregates image or video feature maps into\nobject-level feature vectors, termed \\textit{slots}. OCL's self-supervision of\nreconstructing the input from these aggregated slots struggles with complex\nobject textures, thus Vision Foundation Model (VFM) representations are used as\nthe aggregation input and reconstruction target. However, existing methods\nleverage VFM representations in diverse ways and often fail to fully exploit\ntheir potential. In response, we propose a clean architecture--Vector-Quantized\nVFMs for OCL (VQ-VFM-OCL, or VVO)--that unifies mainstream OCL methods. The key\nto our unification is simple yet effective, just shared quantizing the same VFM\nrepresentation as the reconstruction target. Through mathematical modeling and\nstatistical verification, we further analyze why VFM representations facilitate\nOCL aggregation and how their shared quantization as reconstruction targets\nstrengthens OCL supervision. Experiments show that across different VFMs,\naggregators and decoders, our VVO consistently outperforms baselines in object\ndiscovery and recognition, as well as downstream visual prediction and\nreasoning. The implementation and model checkpoints are available on\nhttps://github.com/Genera1Z/VQ-VFM-OCL.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2502.20263v4", "cate": "cs.CV", "date": "2025-02-27", "updated": "2025-07-31"}
{"id": "2506.21875", "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation", "authors": ["Jian Zhang", "Linhao Zhang", "Bokai Lei", "Chuhan Wu", "Wei Jia", "Xiao Zhou"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21875v2", "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21875v2", "cate": "cs.CL", "date": "2025-06-27", "updated": "2025-07-31"}
{"id": "2507.07695", "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities", "authors": ["Hruday Markondapatnaikuni", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      21 pages, 14 figures", "url": "http://arxiv.org/abs/2507.07695v2", "summary": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study.", "comment": "21 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.07695v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2507.07675", "title": "Some Theoretical Results on Layerwise Effective Dimension Oscillations in Finite Width ReLU Networks", "authors": ["Darshan Makwana"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Incomplete citations", "url": "http://arxiv.org/abs/2507.07675v2", "summary": "We analyze the layerwise effective dimension (rank of the feature matrix) in\nfully-connected ReLU networks of finite width. Specifically, for a fixed batch\nof $m$ inputs and random Gaussian weights, we derive closed-form expressions\nfor the expected rank of the \\$m\\times n\\$ hidden activation matrices. Our main\nresult shows that $\\mathbb{E}[EDim(\\ell)]=m[1-(1-2/\\pi)^\\ell]+O(e^{-c m})$ so\nthat the rank deficit decays geometrically with ratio $1-2 / \\pi \\approx\n0.3634$. We also prove a sub-Gaussian concentration bound, and identify the\n\"revival\" depths at which the expected rank attains local maxima. In\nparticular, these peaks occur at depths\n$\\ell_k^*\\approx(k+1/2)\\pi/\\log(1/\\rho)$ with height $\\approx (1-e^{-\\pi/2}) m\n\\approx 0.79m$. We further show that this oscillatory rank behavior is a\nfinite-width phenomenon: under orthogonal weight initialization or strong\nnegative-slope leaky-ReLU, the rank remains (nearly) full. These results\nprovide a precise characterization of how random ReLU layers alternately\ncollapse and partially revive the subspace of input variations, adding nuance\nto prior work on expressivity of deep networks.", "comment": "Incomplete citations", "pdf_url": "http://arxiv.org/pdf/2507.07675v2", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2502.20760", "title": "VRM: Knowledge Distillation via Virtual Relation Matching", "authors": ["Weijia Zhang", "Fei Xie", "Weidong Cai", "Chao Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2502.20760v3", "summary": "Knowledge distillation (KD) aims to transfer the knowledge of a more capable\nyet cumbersome teacher model to a lightweight student model. In recent years,\nrelation-based KD methods have fallen behind, as their instance-matching\ncounterparts dominate in performance. In this paper, we revive relational KD by\nidentifying and tackling several key issues in relation-based methods,\nincluding their susceptibility to overfitting and spurious responses.\nSpecifically, we transfer novelly constructed affinity graphs that compactly\nencapsulate a wealth of beneficial inter-sample, inter-class, and inter-view\ncorrelations by exploiting virtual views and relations as a new kind of\nknowledge. As a result, the student has access to richer guidance signals and\nstronger regularisation throughout the distillation process. To further\nmitigate the adverse impact of spurious responses, we prune the affinity graphs\nby dynamically detaching redundant and unreliable edges. Extensive experiments\non CIFAR-100, ImageNet, and MS-COCO datasets demonstrate the superior\nperformance of the proposed virtual relation matching (VRM) method, where it\nconsistently sets new state-of-the-art records over a range of models,\narchitectures, tasks, and set-ups. For instance, VRM for the first time hits\n74.0% accuracy for ResNet50-to-MobileNetV2 distillation on ImageNet, and\nimproves DeiT-T by 14.44% on CIFAR-100 with a ResNet56 teacher.", "comment": "Accepted by ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2502.20760v3", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-07-31"}
{"id": "2507.06448", "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06448v3", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06448v3", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-31"}
{"id": "2507.10073", "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "authors": ["Simon Münker"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15pages, 1 figure, 2 tables", "url": "http://arxiv.org/abs/2507.10073v2", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "comment": "15pages, 1 figure, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.10073v2", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-31"}
{"id": "2507.09252", "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding", "authors": ["Shukai Gong", "Yiyang Fu", "Fengyuan Ran", "Quyu Kong", "Feng Zhou"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09252v2", "summary": "We propose TPP-SD, a novel approach that accelerates Transformer temporal\npoint process (TPP) sampling by adapting speculative decoding (SD) techniques\nfrom language models. By identifying the structural similarities between\nthinning algorithms for TPPs and speculative decoding for language models, we\ndevelop an efficient sampling framework that leverages a smaller draft model to\ngenerate multiple candidate events, which are then verified by the larger\ntarget model in parallel. TPP-SD maintains the same output distribution as\nautoregressive sampling while achieving significant acceleration. Experiments\non both synthetic and real datasets demonstrate that our approach produces\nsamples from identical distributions as standard methods, but with 2-6$\\times$\nspeedup. Our ablation studies analyze the impact of hyperparameters such as\ndraft length and draft model size on sampling efficiency. TPP-SD bridges the\ngap between powerful Transformer TPP models and the practical need for rapid\nsequence sampling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09252v2", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-31"}
{"id": "2503.01199", "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via System and Algorithm Codesign", "authors": ["Kaimin Liao", "Hua Wang", "Zhi Chen", "Luchao Wang", "Yaohua Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01199v2", "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01199v2", "cate": "cs.CV", "date": "2025-03-03", "updated": "2025-07-31"}
{"id": "2507.08606", "title": "DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures", "authors": ["Benno Uthayasooriyar", "Antoine Ly", "Franck Vermet", "Caio Corro"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08606v3", "summary": "We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08606v3", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-31"}
{"id": "2507.17745", "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention", "authors": ["Yiwen Chen", "Zhihao Li", "Yikai Wang", "Hu Zhang", "Qin Li", "Chi Zhang", "Guosheng Lin"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.17745v3", "summary": "Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.", "comment": "Project Page: https://buaacyw.github.io/ultra3d/", "pdf_url": "http://arxiv.org/pdf/2507.17745v3", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-31"}
{"id": "2507.13762", "title": "MolPIF: A Parameter Interpolation Flow Model for Molecule Generation", "authors": ["Yaowei Jin", "Junjie Wang", "Wenkai Xiang", "Duanhua Cao", "Dan Teng", "Zhehuan Fan", "Jiacheng Xiong", "Xia Sheng", "Chuanlong Zeng", "Duo An", "Mingyue Zheng", "Shuangjia Zheng", "Qian Shi"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13762v3", "summary": "Advances in deep learning for molecular generation show promise in\naccelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown\nimpressive performance across diverse chemical tasks, with their success often\nascribed to the paradigm of modeling in a low-variance parameter space.\nHowever, the Bayesian inference-based strategy imposes limitations on designing\nmore flexible distribution transformation pathways, making it challenging to\nadapt to diverse data distributions and varied task requirements. Furthermore,\nthe potential for simpler, more efficient parameter-space-based models is\nunexplored. To address this, we propose a novel Parameter Interpolation Flow\nmodel (named PIF) with detailed theoretical foundation, training, and inference\nprocedures. We then develop MolPIF for structure-based drug design,\ndemonstrating its superior performance across diverse metrics compared to\nbaselines. This work validates the effectiveness of parameter-space-based\ngenerative modeling paradigm for molecules and offers new perspectives for\nmodel design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13762v3", "cate": "cs.LG", "date": "2025-07-18", "updated": "2025-07-31"}
{"id": "2503.03222", "title": "Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D Pretraining", "authors": ["Zhumei Wang", "Zechen Hu", "Ruoxi Guo", "Huaijin Pi", "Ziyong Feng", "Sida Peng", "Xiaowei Zhou", "Mingtao Pei", "Siyuan Huang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2503.03222v5", "summary": "Recovering absolute human motion from monocular inputs is challenging due to\ntwo main issues. First, existing methods depend on 3D training data collected\nfrom limited environments, constraining out-of-distribution generalization. The\nsecond issue is the difficulty of estimating metric-scale poses from monocular\ninput. To address these challenges, we introduce Mocap-2-to-3, a novel\nframework that performs multi-view lifting from monocular input by leveraging\n2D data pre-training, enabling the reconstruction of metrically accurate 3D\nmotions with absolute positions. To leverage abundant 2D data, we decompose\ncomplex 3D motion into multi-view syntheses. We first pretrain a single-view\ndiffusion model on extensive 2D datasets, then fine-tune a multi-view model\nusing public 3D data to enable view-consistent motion generation from monocular\ninput, allowing the model to acquire action priors and diversity through 2D\ndata. Furthermore, to recover absolute poses, we propose a novel human motion\nrepresentation that decouples the learning of local pose and global movements,\nwhile encoding geometric priors of the ground to accelerate convergence. This\nenables progressive recovery of motion in absolute space during inference.\nExperimental results on in-the-wild benchmarks demonstrate that our method\nsurpasses state-of-the-art approaches in both camera-space motion realism and\nworld-grounded human positioning, while exhibiting superior generalization\ncapability. Our code will be made publicly available.", "comment": "Project page: https://wangzhumei.github.io/mocap-2-to-3/", "pdf_url": "http://arxiv.org/pdf/2503.03222v5", "cate": "cs.CV", "date": "2025-03-05", "updated": "2025-07-31"}
{"id": "2507.11832", "title": "ILID: Native Script Language Identification for Indian Languages", "authors": ["Yash Ingle", "Pruthwik Mishra"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 1 figure, 6 tables, Paper accepted in RANLP 2025", "url": "http://arxiv.org/abs/2507.11832v2", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script, making the task even more challenging.\nTaking all these challenges into account, we develop and release a dataset of\n250K sentences consisting of 23 languages including English and all 22 official\nIndian languages labeled with their language identifiers, where data in most\nlanguages are newly created. We also develop and release baseline models using\nstate-of-the-art approaches in machine learning and fine-tuning pre-trained\ntransformer models. Our models outperforms the state-of-the-art pre-trained\ntransformer models for the language identification task. The dataset and the\ncodes are available at https://yashingle-ai.github.io/ILID/ and in Huggingface\nopen source libraries.", "comment": "10 pages, 1 figure, 6 tables, Paper accepted in RANLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.11832v2", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2507.19119", "title": "PatchTraj: Unified Time-Frequency Representation Learning via Dynamic Patches for Trajectory Prediction", "authors": ["Yanghong Liu", "Xingping Dong", "Ming Li", "Weixing Zhang", "Yidong Lou"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19119v3", "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two main\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representations lack interaction with their frequency components in\njointly modeling trajectory sequences. To address these challenges, we propose\nPatchTraj, a dynamic patch-based framework that integrates time-frequency joint\nmodeling for trajectory prediction. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, and employ dynamic patch\npartitioning to perform multi-scale segmentation, capturing hierarchical motion\npatterns. Each patch undergoes adaptive embedding with scale-aware feature\nextraction, followed by hierarchical feature aggregation to model both\nfine-grained and long-range dependencies. The outputs of the two branches are\nfurther enhanced via cross-modal attention, facilitating complementary fusion\nof temporal and spectral cues. The resulting enhanced embeddings exhibit strong\nexpressive power, enabling accurate predictions even when using a vanilla\nTransformer architecture. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB\ndatasets demonstrate that our method achieves state-of-the-art performance.\nNotably, on the egocentric JRDB dataset, PatchTraj attains significant relative\nimprovements of 26.7% in ADE and 17.4% in FDE, underscoring its substantial\npotential in embodied intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19119v3", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2507.18376", "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges", "authors": ["Xing Hu", "Haodong Chen", "Qianqian Duan", "Danfeng Hong", "Ruijiao Li", "Huiliang Shang", "Linghua Jiang", "Haima Yang", "Dawei Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18376v2", "summary": "With the global population growing and arable land resources becoming\nincreasingly scarce,smart agriculture and precision agriculture have emerged as\nkey directions for the future ofagricultural development.Artificial\nintelligence (AI) technologies, particularly deep learning models, have found\nwidespread applications in areas such as crop monitoring and pest detection. As\nan emerging generative model, diffusion models have shown significant promise\nin tasks like agricultural image processing, data augmentation, and remote\nsensing. Compared to traditional generative adversarial networks (GANs),\ndiffusion models offer superior training stability and generation quality,\neffectively addressing challenges such as limited agricultural data and\nimbalanced image samples. This paper reviews the latest advancements in the\napplication of diffusion models in agriculture, focusing on their potential in\ncrop pest and disease detection, remote sensing image enhancement, crop growth\nprediction, and agricultural resource management. Experimental results\ndemonstrate that diffusion models significantly improve model accuracy and\nrobustness in data augmentation, image generation, and denoising, especially in\ncomplex environments. Despite challenges related to computational efficiency\nand generalization capabilities, diffusion models are expected to play an\nincreasingly important role in smart and precision agriculture as technology\nadvances, providing substantial support for the sustainable development of\nglobal agriculture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18376v2", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2503.04165", "title": "WeakSupCon: Weakly Supervised Contrastive Learning for Encoder Pre-training", "authors": ["Bodong Zhang", "Hamid Manoochehri", "Xiwen Li", "Beatrice S. Knudsen", "Tolga Tasdizen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Medical Image Computing and Computer Assisted Intervention (MICCAI) 2025 workshop on Efficient Medical AI", "url": "http://arxiv.org/abs/2503.04165v2", "summary": "Weakly supervised multiple instance learning (MIL) is a challenging task\ngiven that only bag-level labels are provided, while each bag typically\ncontains multiple instances. This topic has been extensively studied in\nhistopathological image analysis, where labels are usually available only at\nthe whole slide image (WSI) level, while each WSI could be divided into\nthousands of small image patches for training. The dominant MIL approaches\nfocus on feature aggregation and take fixed patch features as inputs. However,\nweakly supervised feature representation learning in MIL settings is always\nneglected. Those features used to be generated by self-supervised learning\nmethods that do not utilize weak labels, or by foundation encoders pre-trained\non other large datasets. In this paper, we propose a novel weakly supervised\nfeature representation learning method called Weakly Supervised Contrastive\nLearning (WeakSupCon) that utilizes bag-level labels. In our method, we employ\nmulti-task learning and define distinct contrastive losses for samples with\ndifferent bag labels. Our experiments demonstrate that the features generated\nusing WeakSupCon with limited computing resources significantly enhance MIL\nclassification performance compared to self-supervised approaches across three\ndatasets. Our WeakSupCon code is available at\ngithub.com/BzhangURU/Paper_WeakSupCon", "comment": "Medical Image Computing and Computer Assisted Intervention (MICCAI)\n  2025 workshop on Efficient Medical AI", "pdf_url": "http://arxiv.org/pdf/2503.04165v2", "cate": "cs.CV", "date": "2025-03-06", "updated": "2025-07-30"}
{"id": "2507.17186", "title": "FinGAIA: A Chinese Benchmark for AI Agents in Real-World Financial Domain", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17186v2", "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17186v2", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-31"}
{"id": "2507.21004", "title": "Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability", "authors": ["Fang Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The project has been open sourced at Github ( this https URL )", "url": "http://arxiv.org/abs/2507.21004v2", "summary": "Deep Neural Networks (DNNs) deliver impressive performance but their\nblack-box nature limits deployment in high-stakes domains requiring\ntransparency. We introduce Compositional Function Networks (CFNs), a novel\nframework that builds inherently interpretable models by composing elementary\nmathematical functions with clear semantics. Unlike existing interpretable\napproaches that are limited to simple additive structures, CFNs support diverse\ncompositional patterns -- sequential, parallel, and conditional -- enabling\ncomplex feature interactions while maintaining transparency. A key innovation\nis that CFNs are fully differentiable, allowing efficient training through\nstandard gradient descent. We demonstrate CFNs' versatility across multiple\ndomains, from symbolic regression to image classification with deep\nhierarchical networks. Our empirical evaluation shows CFNs achieve competitive\nperformance against black-box models (96.24% accuracy on CIFAR-10) while\noutperforming state-of-the-art interpretable models like Explainable Boosting\nMachines. By combining the hierarchical expressiveness and efficient training\nof deep learning with the intrinsic interpretability of well-defined\nmathematical functions, CFNs offer a powerful framework for applications where\nboth performance and accountability are paramount.", "comment": "The project has been open sourced at Github\n  (https://github.com/fanglioc/Compositional_Function_Networks)", "pdf_url": "http://arxiv.org/pdf/2507.21004v2", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2507.19095", "title": "GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network", "authors": ["Binxiong Li", "Xu Xiang", "Xue Li", "Quanzhou Lou", "Binyu Zhao", "Yujie Liu", "Huijie Tang", "Benhan Yang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The source code for this study is available at this https URL", "url": "http://arxiv.org/abs/2507.19095v2", "summary": "Attributed graph clustering holds significant importance in modern data\nanalysis. However, due to the complexity of graph data and the heterogeneity of\nnode attributes, leveraging graph information for clustering remains\nchallenging. To address this, we propose a novel deep graph clustering model,\nGCL-GCN, specifically designed to address the limitations of existing models in\ncapturing local dependencies and complex structures when dealing with sparse\nand heterogeneous graph data. GCL-GCN introduces an innovative Graphormer\nmodule that combines centrality encoding and spatial relationships, effectively\ncapturing both global and local information between nodes, thereby enhancing\nthe quality of node representations. Additionally, we propose a novel\ncontrastive learning module that significantly enhances the discriminative\npower of feature representations. In the pre-training phase, this module\nincreases feature distinction through contrastive learning on the original\nfeature matrix, ensuring more identifiable initial representations for\nsubsequent graph convolution and clustering tasks. Extensive experimental\nresults on six datasets demonstrate that GCL-GCN outperforms 14 advanced\nmethods in terms of clustering quality and robustness. Specifically, on the\nCora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,\nrespectively, compared to the primary comparison method MBN.", "comment": "The source code for this study is available at\n  https://github.com/YF-W/GCL-GCN", "pdf_url": "http://arxiv.org/pdf/2507.19095v2", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2503.04351", "title": "PLMP -- Point-Line Minimal Problems for Projective SfM", "authors": ["Kim Kiehn", "Albin Ahlbäck", "Kathlén Kohn"], "categories": ["cs.CV", "math.AG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04351v2", "summary": "We completely classify all minimal problems for Structure-from-Motion (SfM)\nwhere arrangements of points and lines are fully observed by multiple\nuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have\nunique solutions and can thus be solved linearly. Two of the linear problems\nallow an arbitrary number of views, while all other minimal problems have at\nmost 9 cameras. All minimal problems have at most 7 points and at most 12\nlines. We compute the number of solutions of each minimal problem, as this\ngives a measurement of the problem's intrinsic difficulty, and find that these\nnumber are relatively low (e.g., when comparing with minimal problems for\ncalibrated cameras). Finally, by exploring stabilizer subgroups of\nsubarrangements, we develop a geometric and systematic way to 1) factorize\nminimal problems into smaller problems, 2) identify minimal problems in\nunderconstrained problems, and 3) formally prove non-minimality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04351v2", "cate": "cs.CV", "date": "2025-03-06", "updated": "2025-07-31"}
{"id": "2507.21568", "title": "Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages", "authors": ["Aarón Galiano-Jiménez", "Juan Antonio Pérez-Ortiz", "Felipe Sánchez-Martínez", "Víctor M. Sánchez-Cartagena"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      17 pages, 12 figures", "url": "http://arxiv.org/abs/2507.21568v2", "summary": "This paper explores sequence-level knowledge distillation (KD) of\nmultilingual pre-trained encoder-decoder translation models. We argue that the\nteacher model's output distribution holds valuable insights for the student,\nbeyond the approximated mode obtained through beam search (the standard\ndecoding method), and present Multi-Hypothesis Distillation (MHD), a\nsequence-level KD method that generates multiple translations for each source\nsentence. This provides a larger representation of the teacher model\ndistribution and exposes the student model to a wider range of target-side\nprefixes. We leverage $n$-best lists from beam search to guide the student's\nlearning and examine alternative decoding methods to address issues like low\nvariability and the under-representation of infrequent tokens. For low-resource\nlanguages, our research shows that while sampling methods may slightly\ncompromise translation quality compared to beam search based approaches, they\nenhance the generated corpora with greater variability and lexical richness.\nThis ultimately improves student model performance and mitigates the gender\nbias amplification often associated with KD.", "comment": "17 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.21568v2", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.21433", "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse", "authors": ["Kaiwen Chen", "Xin Tan", "Minchen Yu", "Hong Xu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures", "url": "http://arxiv.org/abs/2507.21433v2", "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.", "comment": "11 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.21433v2", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.21197", "title": "AdaptHetero: Machine Learning Interpretation-Driven Subgroup Adaptation for EHR-Based Clinical Prediction", "authors": ["Ling Liao", "Eva Aagaard"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures", "url": "http://arxiv.org/abs/2507.21197v2", "summary": "Machine learning interpretation (MLI) has primarily been leveraged to build\nclinician trust and uncover actionable insights in EHRs. However, the intrinsic\ncomplexity and heterogeneity of EHR data limit its effectiveness in guiding\nsubgroup-specific modeling. We propose AdaptHetero, a novel MLI-driven\nframework that transforms interpretability insights into actionable guidance\nfor tailoring model training and evaluation across subpopulations within\nindividual hospital systems. Evaluated on three large-scale EHR datasets:\nGOSSIS-1-eICU, WiDS, and MIMIC-IV, AdaptHetero consistently identifies\nheterogeneous model behaviors in predicting ICU mortality, in-hospital death,\nand hidden hypoxemia. By integrating SHAP-based interpretation and unsupervised\nclustering, the framework enhances the identification of clinically meaningful\nsubgroup-specific characteristics, leading to improved predictive performance\nand optimized clinical deployment.", "comment": "12 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.21197v2", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-30"}
{"id": "2503.07047", "title": "Recovering Partially Corrupted Objects via Sketch-Guided Bidirectional Feature Interaction", "authors": ["Yongle Zhang", "Yimin Liu", "Yan Huang", "Qiang Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2503.07047v2", "summary": "Text-guided diffusion models have achieved remarkable success in object\ninpainting by providing high-level semantic guidance through text prompts.\nHowever, they often lack precise pixel-level spatial control, especially in\nscenarios involving partially corrupted objects where critical uncorrupted cues\nremain. To overcome this limitation, sketch-guided methods have been\nintroduced, using either indirect gradient modulation or direct sketch\ninjection to improve structural control. Yet, existing approaches typically\nestablish a one-way mapping from the sketch to the masked regions only,\nneglecting the contextual information from unmasked object areas. This leads to\na disconnection between the sketch and the uncorrupted content, thereby causing\nsketch-guided inconsistency and structural mismatch. To tackle this challenge,\nwe propose a sketch-guided bidirectional feature interaction framework built\nupon a pretrained Stable Diffusion model. Our bidirectional interaction\nfeatures two complementary directions, context-to-sketch and\nsketch-to-inpainting, that enable fine-grained spatial control for partially\ncorrupted object inpainting. In the context-to-sketch direction, multi-scale\nlatents from uncorrupted object regions are propagated to the sketch branch to\ngenerate a visual mask that adapts the sketch features to the visible context\nand denoising progress. In the sketch-to-inpainting direction, a\nsketch-conditional affine transformation modulates the influence of sketch\nguidance based on the learned visual mask, ensuring consistency with\nuncorrupted object content. This interaction is applied at multiple scales\nwithin the encoder of the diffusion U-Net, enabling the model to restore object\nstructures with enhanced spatial fidelity. Extensive experiments on two newly\nconstructed benchmark datasets demonstrate that our approach outperforms\nstate-of-the-art methods.", "comment": "13 pages. This work has been submitted to the IEEE for possible\n  publication", "pdf_url": "http://arxiv.org/pdf/2503.07047v2", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-07-31"}
{"id": "2507.22581", "title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "authors": ["Inaya Rahmanisa", "Lyzander Marciano Andrylie", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Our code and dataset are made available at this https URL", "url": "http://arxiv.org/abs/2507.22581v2", "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.", "comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron", "pdf_url": "http://arxiv.org/pdf/2507.22581v2", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.22069", "title": "A Compute-Matched Re-Evaluation of TroVE on MATH", "authors": ["Tobias Sesterhenn", "Ian Berlot-Attwell", "Janis Zenkner", "Christian Bartelt"], "categories": ["cs.PL", "cs.AI"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22069v2", "summary": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22069v2", "cate": "cs.PL", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2507.22174", "title": "Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic", "authors": ["Molly Wang", "Kin. K Leung"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22174v2", "summary": "Reinforcement Learning (RL) has been widely used for packet routing in\ncommunication networks, but traditional RL methods rely on the Markov\nassumption that the current state contains all necessary information for\ndecision-making. In reality, internet traffic is non-Markovian, and past states\ndo influence routing performance. Moreover, common deep RL approaches use\nfunction approximators, such as neural networks, that do not model the spatial\nstructure in network topologies. To address these shortcomings, we design a\nnetwork environment with non-Markovian traffic and introduce a spatial-temporal\nRL (STRL) framework for packet routing. Our approach outperforms traditional\nbaselines by more than 19% during training and 7% for inference despite a\nchange in network topology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22174v2", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2503.10410", "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground Simulation", "authors": ["Yuwen Du", "Anning Hu", "Zichen Chao", "Yifan Lu", "Junhao Ge", "Genjia Liu", "Weitao Wu", "Lanjun Wang", "Siheng Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10410v3", "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10410v3", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-07-31"}
{"id": "2507.05903", "title": "AI-Reporter: A Path to a New Genre of Scientific Communication", "authors": ["Gerd Graßhoff"], "categories": ["cs.DL", "cs.CL"], "primary_category": "Subjects:       Digital Libraries (cs.DL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05903v2", "summary": "The AI-Reporter represents a paradigmatic shift in scientific publication\npractice. This document demonstrates through a concrete case study how our\nsystem transforms academic presentations into publication-ready chapters -- in\nless than three minutes. Using Arno Simons' lecture on Large Language Models\nfrom the ``Large Language Models for the History, Philosophy, and Sociology of\nScience'' workshop (NEPI) as an example, we show how technological innovation\nbridges the gap between ephemeral presentation and permanent scientific\ndocumentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05903v2", "cate": "cs.DL", "date": "2025-07-08", "updated": "2025-07-31"}
{"id": "2507.22477", "title": "LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks", "authors": ["Hui Liu", "Chen Jia", "Fan Shi", "Xu Cheng", "Mengfei Shi", "Xia Xie", "Shengyong Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.22477v2", "summary": "Achieving pixel-level segmentation with low computational cost using\nmultimodal data remains a key challenge in crack segmentation tasks. Existing\nmethods lack the capability for adaptive perception and efficient interactive\nfusion of cross-modal features. To address these challenges, we propose a\nLightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently\nperceives and integrates morphological and textural cues from different\nmodalities under multimodal crack scenarios, generating clear pixel-level crack\nsegmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive\nCue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain\nDynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack\ncues through the proposed mask-guided Efficient Dynamic Guided Scanning\nStrategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain\nPerceptron (AFDP) and a dual-pooling fusion strategy to effectively capture\nspatial and frequency-domain cues across modalities. Moreover, we design a\nLightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive\ncomplex morphological structures with minimal computational overhead, replacing\nmost convolutional operations in LIDAR. Experiments on three datasets\ndemonstrate that our method outperforms other state-of-the-art (SOTA) methods.\nOn the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465\nin mIoU with only 5.35M parameters. Code and datasets are available at\nhttps://github.com/Karl1109/LIDAR-Mamba.", "comment": "This paper has been accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.22477v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.22303", "title": "CS-SHRED: Enhancing SHRED for Robust Recovery of Spatiotemporal Dynamics", "authors": ["Romulo B. da Silva", "Diego Passos", "Cássio M. Oishi", "J. Nathan Kutz"], "categories": ["cs.LG", "68T07, 35Q35, 94A12", "I.2.6; I.5.4; I.6.3; J.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      30 pages, 7 figures, 13 tables. Code: this https URL", "url": "http://arxiv.org/abs/2507.22303v2", "summary": "We present CS-SHRED, a novel deep learning architecture that integrates\nCompressed Sensing (CS) into a Shallow Recurrent Decoder (SHRED) to reconstruct\nspatiotemporal dynamics from incomplete, compressed, or corrupted data. Our\napproach introduces two key innovations. First, by incorporating CS techniques\ninto the SHRED architecture, our method leverages a batch-based forward\nframework with $\\ell_1$ regularization to robustly recover signals even in\nscenarios with sparse sensor placements, noisy measurements, and incomplete\nsensor acquisitions. Second, an adaptive loss function dynamically combines\nMean Squared Error (MSE) and Mean Absolute Error (MAE) terms with a piecewise\nSignal-to-Noise Ratio (SNR) regularization, which suppresses noise and outliers\nin low-SNR regions while preserving fine-scale features in high-SNR regions.\n  We validate CS-SHRED on challenging problems including viscoelastic fluid\nflows, maximum specific humidity fields, sea surface temperature distributions,\nand rotating turbulent flows. Compared to the traditional SHRED approach,\nCS-SHRED achieves significantly higher reconstruction fidelity -- as\ndemonstrated by improved SSIM and PSNR values, lower normalized errors, and\nenhanced LPIPS scores-thereby providing superior preservation of small-scale\nstructures and increased robustness against noise and outliers.\n  Our results underscore the advantages of the jointly trained CS and SHRED\ndesign architecture which includes an LSTM sequence model for characterizing\nthe temporal evolution with a shallow decoder network (SDN) for modeling the\nhigh-dimensional state space. The SNR-guided adaptive loss function for the\nspatiotemporal data recovery establishes CS-SHRED as a promising tool for a\nwide range of applications in environmental, climatic, and scientific data\nanalyses.", "comment": "30 pages, 7 figures, 13 tables. Code:\n  https://github.com/romulobrito/cs-shred", "pdf_url": "http://arxiv.org/pdf/2507.22303v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2503.11806", "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling", "authors": ["Christopher Xie", "Armen Avetisyan", "Henry Howard-Jenkins", "Yawar Siddiqui", "Julian Straub", "Richard Newcombe", "Vasileios Balntas", "Jakob Engel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2503.11806v2", "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout\nthat uses human feedback from an egocentric standpoint. We study this approach\nthrough introduction of a novel local correction task, where users identify\nlocal errors and prompt a model to automatically correct them. Building on\nSceneScript, a state-of-the-art framework for 3D scene layout estimation that\nleverages structured language, we propose a solution that structures this\nproblem as \"infilling\", a task studied in natural language processing. We train\na multi-task version of SceneScript that maintains performance on global\npredictions while significantly improving its local correction ability. We\nintegrate this into a human-in-the-loop system, enabling a user to iteratively\nrefine scene layout estimates via a low-friction \"one-click fix'' workflow. Our\nsystem enables the final refined layout to diverge from the training\ndistribution, allowing for more accurate modelling of complex layouts.", "comment": "Project page: https://www.projectaria.com/scenescript/", "pdf_url": "http://arxiv.org/pdf/2503.11806v2", "cate": "cs.CV", "date": "2025-03-14", "updated": "2025-07-30"}
{"id": "2507.22062", "title": "Meta CLIP 2: A Worldwide Scaling Recipe", "authors": ["Yung-Sung Chuang", "Yang Li", "Dong Wang", "Ching-Feng Yeh", "Kehan Lyu", "Ramya Raghavendra", "James Glass", "Lifei Huang", "Jason Weston", "Luke Zettlemoyer", "Xinlei Chen", "Zhuang Liu", "Saining Xie", "Wen-tau Yih", "Shang-Wen Li", "Hu Xu"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.22062v2", "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.22062v2", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-30"}
{"id": "2507.22498", "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping", "authors": ["Yuhwan Jeong", "Yunseo Yang", "Youngho Yoon", "Kuk-Jin Yoon"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ICCV25", "url": "http://arxiv.org/abs/2507.22498v2", "summary": "Adverse weather conditions cause diverse and complex degradation patterns,\ndriving the development of All-in-One (AiO) models. However, recent AiO\nsolutions still struggle to capture diverse degradations, since global\nfiltering methods like direct operations on the frequency domain fail to handle\nhighly variable and localized distortions. To address these issue, we propose\nSpectral-based Spatial Grouping Transformer (SSGformer), a novel approach that\nleverages spectral decomposition and group-wise attention for multi-weather\nimage restoration. SSGformer decomposes images into high-frequency edge\nfeatures using conventional edge detection and low-frequency information via\nSingular Value Decomposition. We utilize multi-head linear attention to\neffectively model the relationship between these features. The fused features\nare integrated with the input to generate a grouping-mask that clusters regions\nbased on the spatial similarity and image texture. To fully leverage this mask,\nwe introduce a group-wise attention mechanism, enabling robust adverse weather\nremoval and ensuring consistent performance across diverse weather conditions.\nWe also propose a Spatial Grouping Transformer Block that uses both channel\nattention and spatial attention, effectively balancing feature-wise\nrelationships and spatial dependencies. Extensive experiments show the\nsuperiority of our approach, validating its effectiveness in handling the\nvaried and intricate adverse weather degradations.", "comment": "accepted by ICCV25", "pdf_url": "http://arxiv.org/pdf/2507.22498v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.22633", "title": "H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity", "authors": ["Wei Guo", "Siyuan Lu", "Yiqi Tong", "Zhaojun Hu", "Fuzhen Zhuang", "Xiao Zhang", "Tao Fan", "Jin Dong"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22633v2", "summary": "Different from existing federated fine-tuning (FFT) methods for foundation\nmodels, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored\nscenario where clients exhibit double heterogeneity in model architectures and\ndownstream tasks. This hybrid heterogeneity introduces two significant\nchallenges: 1) heterogeneous matrix aggregation, where clients adopt different\nlarge-scale foundation models based on their task requirements and resource\nlimitations, leading to dimensional mismatches during LoRA parameter\naggregation; and 2) multi-task knowledge interference, where local shared\nparameters, trained with both task-shared and task-specific knowledge, cannot\nensure only task-shared knowledge is transferred between clients. To address\nthese challenges, we propose H2Tune, a federated foundation model fine-tuning\nwith hybrid heterogeneity. Our framework H2Tune consists of three key\ncomponents: (i) sparsified triple matrix decomposition to align hidden\ndimensions across clients through constructing rank-consistent middle matrices,\nwith adaptive sparsification based on client resources; (ii) relation-guided\nmatrix layer alignment to handle heterogeneous layer structures and\nrepresentation capabilities; and (iii) alternating task-knowledge\ndisentanglement mechanism to decouple shared and specific knowledge of local\nmodel parameters through alternating optimization. Theoretical analysis proves\na convergence rate of O(1/\\sqrt{T}). Extensive experiments show our method\nachieves up to 15.4% accuracy improvement compared to state-of-the-art\nbaselines. Our code is available at\nhttps://anonymous.4open.science/r/H2Tune-1407.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22633v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2503.14939", "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models", "authors": ["Tengjin Weng", "Jingyi Wang", "Wenhao Jiang", "Zhong Ming"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ICCV 2025", "url": "http://arxiv.org/abs/2503.14939v2", "summary": "Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. Code and dataset are available at\nhttps://wwwtttjjj.github.io/VisNumBench/.", "comment": "accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.14939v2", "cate": "cs.CV", "date": "2025-03-19", "updated": "2025-07-31"}
{"id": "2507.22607", "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 5 figures, 6 tables. Work in progress", "url": "http://arxiv.org/abs/2507.22607v2", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.", "comment": "21 pages, 5 figures, 6 tables. Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.22607v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.22530", "title": "HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors", "authors": ["Xincheng Yao", "Yijun Yang", "Kangwei Guo", "Ruiqiang Xiao", "Haipeng Zhou", "Haisu Tao", "Jian Yang", "Lei Zhu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by MICCAI 2025", "url": "http://arxiv.org/abs/2507.22530v2", "summary": "The segmentation of the hepatic vasculature in surgical videos holds\nsubstantial clinical significance in the context of hepatectomy procedures.\nHowever, owing to the dearth of an appropriate dataset and the inherently\ncomplex task characteristics, few researches have been reported in this domain.\nTo address this issue, we first introduce a high quality frame-by-frame\nannotated hepatic vasculature dataset containing 35 long hepatectomy videos and\n11442 high-resolution frames. On this basis, we propose a novel high-resolution\nvideo vasculature segmentation network, dubbed as HRVVS. We innovatively embed\na pretrained visual autoregressive modeling (VAR) model into different layers\nof the hierarchical encoder as prior information to reduce the information\ndegradation generated during the downsampling process. In addition, we designed\na dynamic memory decoder on a multi-view segmentation network to minimize the\ntransmission of redundant information while preserving more details between\nframes. Extensive experiments on surgical video datasets demonstrate that our\nproposed HRVVS significantly outperforms the state-of-the-art methods. The\nsource code and dataset will be publicly available at\n\\{https://github.com/scott-yjyang/HRVVS}.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.22530v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.22789", "title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer", "authors": ["Junyu Wu", "Weiming Chang", "Xiaotao Liu", "Guanyou He", "Haoqiang Hong", "Boqi Liu", "Hongtao Tian", "Tao Yang", "Yunsheng Shi", "Feng Lin", "Ting Yao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      I haven't received company approval yet, and I uploaded it by mistake", "url": "http://arxiv.org/abs/2507.22789v2", "summary": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels.", "comment": "I haven't received company approval yet, and I uploaded it by mistake", "pdf_url": "http://arxiv.org/pdf/2507.22789v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2503.15897", "title": "Learning 3D Scene Analogies with Neural Contextual Scene Maps", "authors": ["Junho Kim", "Gwangtak Bae", "Eun Sun Lee", "Young Min Kim"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2503.15897v2", "summary": "Understanding scene contexts is crucial for machines to perform tasks and\nadapt prior knowledge in unseen or noisy 3D environments. As data-driven\nlearning is intractable to comprehensively encapsulate diverse ranges of\nlayouts and open spaces, we propose teaching machines to identify relational\ncommonalities in 3D spaces. Instead of focusing on point-wise or object-wise\nrepresentations, we introduce 3D scene analogies, which are smooth maps between\n3D scene regions that align spatial relationships. Unlike well-studied single\ninstance-level maps, these scene-level maps smoothly link large scene regions,\npotentially enabling unique applications in trajectory transfer in AR/VR, long\ndemonstration transfer for imitation learning, and context-aware object\nrearrangement. To find 3D scene analogies, we propose neural contextual scene\nmaps, which extract descriptor fields summarizing semantic and geometric\ncontexts, and holistically align them in a coarse-to-fine manner for map\nestimation. This approach reduces reliance on individual feature points, making\nit robust to input noise or shape variations. Experiments demonstrate the\neffectiveness of our approach in identifying scene analogies and transferring\ntrajectories or object placements in diverse indoor scenes, indicating its\npotential for robotics and AR/VR applications. Project page including the code\nis available through this link:\nhttps://82magnolia.github.io/3d_scene_analogies/.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.15897v2", "cate": "cs.CV", "date": "2025-03-20", "updated": "2025-07-31"}
{"id": "2408.03351", "title": "Quantum Transfer Learning for MNIST Classification Using a Hybrid Quantum-Classical Approach", "authors": ["Soumyadip Sarkar"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.03351v2", "summary": "We implement a hybrid quantum-classical model for image classification that\ncompresses MNIST digit images into a low-dimensional feature space and then\nmaps these features onto a 5-qubit quantum state. First, an autoencoder\ncompresses each $28\\times28$ image (784 pixels) into a 64-dimensional latent\nvector, preserving salient features of the digit with minimal reconstruction\nerror. We further reduce the latent representation to 5 principal components\nusing Principal Component Analysis (PCA), to match the 5 available qubits.\nThese 5 features are encoded as rotation angles in a quantum circuit with 5\nqubits. The quantum feature map applies single-qubit rotations ($R_y$ gates)\nproportional to the feature values, followed by a Hadamard gate and a cascade\nof entangling CNOT gates to produce a non-product entangled state. Measuring\nthe 5-qubit state yields a 32-dimensional probability distribution over basis\noutcomes, which serves as a quantum-enhanced feature vector for classification.\nA classical neural network with a softmax output is then trained on these\n32-dimensional quantum feature vectors to predict the digit class. We evaluate\nthe hybrid model on the MNIST dataset and compare it to a purely classical\nbaseline that uses the 64-dimensional autoencoder latent features for\nclassification. The results show that the hybrid model can successfully\nclassify digits, demonstrating the feasibility of integrating quantum computing\nin the classification pipeline, although its accuracy (about 75\\% on test data)\ncurrently falls below the classical baseline (about 98\\% on the same compressed\ndata).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.03351v2", "cate": "quant-ph", "date": "2024-08-05", "updated": "2025-07-31"}
{"id": "2503.17526", "title": "Beyond the Encoder: Joint Encoder-Decoder Contrastive Pre-Training Improves Dense Prediction", "authors": ["Sébastien Quetin", "Tapotosh Ghosh", "Farhad Maleki"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17526v2", "summary": "Contrastive learning methods in self-supervised settings have primarily\nfocused on pre-training encoders, while decoders are typically introduced and\ntrained separately for downstream dense prediction tasks. However, this\nconventional approach overlooks the potential benefits of jointly pre-training\nboth encoder and decoder. In this paper, we propose DeCon, an efficient\nencoder-decoder self-supervised learning (SSL) framework that supports joint\ncontrastive pre-training. We first extend existing SSL architectures to\naccommodate diverse decoders and their corresponding contrastive losses. Then,\nwe introduce a weighted encoder-decoder contrastive loss with non-competing\nobjectives to enable the joint pre-training of encoder-decoder architectures.\nBy adapting an established contrastive SSL framework for dense prediction\ntasks, DeCon achieves new state-of-the-art results: on COCO object detection\nand instance segmentation when pre-trained on COCO dataset; across almost all\ndense downstream benchmark tasks when pre-trained on COCO+ and ImageNet-1K. Our\nresults demonstrate that joint pre-training enhances the representation power\nof the encoder and improves performance in dense prediction tasks. This gain\npersists across heterogeneous decoder architectures, various encoder\narchitectures, and in out-of-domain limited-data scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17526v2", "cate": "cs.CV", "date": "2025-03-21", "updated": "2025-07-31"}
{"id": "2408.12319", "title": "Neural-ANOVA: Analytical Model Decomposition using Automatic Integration", "authors": ["Steffen Limmer", "Steffen Udluft", "Clemens Otte"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, 3 tables, accepted for publication at MLSP 2025", "url": "http://arxiv.org/abs/2408.12319v2", "summary": "The analysis of variance (ANOVA) decomposition offers a systematic method to\nunderstand the interaction effects that contribute to a specific decision\noutput. In this paper we introduce Neural-ANOVA, an approach to decompose\nneural networks into the sum of lower-order models using the functional ANOVA\ndecomposition. Our approach formulates a learning problem, which enables fast\nanalytical evaluation of integrals over subspaces that appear in the\ncalculation of the ANOVA decomposition. Finally, we conduct numerical\nexperiments to provide insights into the approximation properties compared to\nother regression approaches from the literature.", "comment": "6 pages, 3 figures, 3 tables, accepted for publication at MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2408.12319v2", "cate": "stat.ML", "date": "2024-08-22", "updated": "2025-07-31"}
{"id": "2503.17856", "title": "ClaraVid: A Holistic Scene Reconstruction Benchmark From Aerial Perspective With Delentropy-Based Complexity Profiling", "authors": ["Radu Beche", "Sergiu Nedevschi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted ICCV 2025", "url": "http://arxiv.org/abs/2503.17856v2", "summary": "The development of aerial holistic scene understanding algorithms is hindered\nby the scarcity of comprehensive datasets that enable both semantic and\ngeometric reconstruction. While synthetic datasets offer an alternative,\nexisting options exhibit task-specific limitations, unrealistic scene\ncompositions, and rendering artifacts that compromise real-world applicability.\nWe introduce ClaraVid, a synthetic aerial dataset specifically designed to\novercome these limitations. Comprising 16,917 high-resolution images captured\nat 4032x3024 from multiple viewpoints across diverse landscapes, ClaraVid\nprovides dense depth maps, panoptic segmentation, sparse point clouds, and\ndynamic object masks, while mitigating common rendering artifacts. To further\nadvance neural reconstruction, we introduce the Delentropic Scene Profile\n(DSP), a novel complexity metric derived from differential entropy analysis,\ndesigned to quantitatively assess scene difficulty and inform reconstruction\ntasks. Utilizing DSP, we systematically benchmark neural reconstruction\nmethods, uncovering a consistent, measurable correlation between scene\ncomplexity and reconstruction accuracy. Empirical results indicate that higher\ndelentropy strongly correlates with increased reconstruction errors, validating\nDSP as a reliable complexity prior. The data and code are available on the\nproject page at https://rdbch.github.io/claravid/", "comment": "Accepted ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.17856v2", "cate": "cs.CV", "date": "2025-03-22", "updated": "2025-07-31"}
{"id": "2410.03094", "title": "Entanglement-induced provable and robust quantum learning advantages", "authors": ["Haimeng Zhao", "Dong-Ling Deng"], "categories": ["quant-ph", "cs.CC", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      7 pages, 2 figures + 13-page supplementary materials", "url": "http://arxiv.org/abs/2410.03094v2", "summary": "Quantum computing holds unparalleled potentials to enhance machine learning.\nHowever, a demonstration of quantum learning advantage has not been achieved so\nfar. We make a step forward by rigorously establishing a noise-robust,\nunconditional quantum learning advantage in expressivity, inference speed, and\ntraining efficiency, compared to commonly-used classical models. Our proof is\ninformation-theoretic and pinpoints the origin of this advantage: entanglement\ncan be used to reduce the communication required by non-local tasks. In\nparticular, we design a task that can be solved with certainty by quantum\nmodels with a constant number of parameters using entanglement, whereas\ncommonly-used classical models must scale linearly to achieve a\nlarger-than-exponentially-small accuracy. We show that the quantum model is\ntrainable with constant resources and robust against constant noise. Through\nnumerical and trapped-ion experiments on IonQ Aria, we demonstrate the desired\nadvantage. Our results provide valuable guidance for demonstrating quantum\nlearning advantages with current noisy intermediate-scale devices.", "comment": "7 pages, 2 figures + 13-page supplementary materials", "pdf_url": "http://arxiv.org/pdf/2410.03094v2", "cate": "quant-ph", "date": "2024-10-04", "updated": "2025-07-31"}
{"id": "2503.18711", "title": "Accenture-NVS1: A Novel View Synthesis Dataset", "authors": ["Thomas Sugg", "Kyle O'Brien", "Lekh Poudel", "Alex Dumouchelle", "Michelle Jou", "Marc Bosch", "Deva Ramanan", "Srinivasa Narasimhan", "Shubham Tulsiani"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures", "url": "http://arxiv.org/abs/2503.18711v2", "summary": "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.", "comment": "6 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2503.18711v2", "cate": "cs.CV", "date": "2025-03-24", "updated": "2025-07-30"}
{"id": "2412.06946", "title": "A Deep Learning Powered Numerical Relativity Surrogate for Binary Black Hole Waveforms", "authors": ["Osvaldo Gramaxo Freitas", "Anastasios Theodoropoulos", "Nino Villanueva", "Tiago Fernandes", "Solange Nunes", "José A. Font", "Antonio Onofre", "Alejandro Torres-Forné", "José D. Martin-Guerrero"], "categories": ["gr-qc", "astro-ph.HE", "astro-ph.IM", "cs.LG"], "primary_category": "Subjects:       General Relativity and Quantum Cosmology (gr-qc)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.06946v3", "summary": "Gravitational-wave approximants are essential for gravitational-wave\nastronomy, allowing the coverage binary black hole parameter space for\ninference or match filtering without costly numerical relativity (NR)\nsimulations, but generally trading some accuracy for computational efficiency.\nTo reduce this trade-off, NR surrogate models can be constructed using\ninterpolation within NR waveform space. We present a 2-stage training approach\nfor neural network-based NR surrogate models. Initially trained on\napproximant-generated waveforms and then fine-tuned with NR data, these\ndual-stage artificial neural surrogate (\\texttt{DANSur}) models offer rapid and\ncompetitively accurate waveform generation, generating millions in under 20ms\non a GPU while keeping mean mismatches with NR around $10^{-4}$. Implemented in\nthe \\textsc{bilby} framework, we show they can be used for parameter estimation\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.06946v3", "cate": "gr-qc", "date": "2024-12-09", "updated": "2025-07-31"}
{"id": "2503.19480", "title": "GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers", "authors": ["Shijie Ma", "Yuying Ge", "Teng Wang", "Yuxin Guo", "Yixiao Ge", "Ying Shan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project released at: this https URL", "url": "http://arxiv.org/abs/2503.19480v3", "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.", "comment": "ICCV 2025. Project released at:\n  https://mashijie1028.github.io/GenHancer/", "pdf_url": "http://arxiv.org/pdf/2503.19480v3", "cate": "cs.CV", "date": "2025-03-25", "updated": "2025-07-31"}
{"id": "2502.15215", "title": "Tensor Product Neural Networks for Functional ANOVA Model", "authors": ["Seokhun Park", "Insung Kong", "Yongchan Choi", "Chanmoo Park", "Yongdai Kim"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      45 pages", "url": "http://arxiv.org/abs/2502.15215v5", "summary": "Interpretability for machine learning models is becoming more and more\nimportant as machine learning models become more complex. The functional ANOVA\nmodel, which decomposes a high-dimensional function into a sum of lower\ndimensional functions (commonly referred to as components), is one of the most\npopular tools for interpretable AI, and recently, various neural networks have\nbeen developed for estimating each component in the functional ANOVA model.\nHowever, such neural networks are highly unstable when estimating each\ncomponent since the components themselves are not uniquely defined. That is,\nthere are multiple functional ANOVA decompositions for a given function. In\nthis paper, we propose a novel neural network which guarantees a unique\nfunctional ANOVA decomposition and thus is able to estimate each component\nstably and accurately. We call our proposed neural network ANOVA Tensor Product\nNeural Network (ANOVA-TPNN) since it is motivated by the tensor product basis\nexpansion. Theoretically, we prove that ANOVA-TPNN can approximate any smooth\nfunction well. Empirically, we show that ANOVA-TPNN provide much more stable\nestimation of each component and thus much more stable interpretation when\ntraining data and initial values of the model parameters vary than existing\nneural networks do.", "comment": "45 pages", "pdf_url": "http://arxiv.org/pdf/2502.15215v5", "cate": "stat.ML", "date": "2025-02-21", "updated": "2025-07-31"}
{"id": "2503.22351", "title": "One Look is Enough: Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation on High-Resolution Images", "authors": ["Byeongjun Kwon", "Munchurl Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (camera-ready version). [Project page]( this https URL )", "url": "http://arxiv.org/abs/2503.22351v3", "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches, resulting in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrate that our PRO can be seamlessly integrated into existing depth\nestimation models.", "comment": "ICCV 2025 (camera-ready version). [Project\n  page](https://kaist-viclab.github.io/One-Look-is-Enough_site)", "pdf_url": "http://arxiv.org/pdf/2503.22351v3", "cate": "cs.CV", "date": "2025-03-28", "updated": "2025-07-31"}
{"id": "2505.21567", "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models", "authors": ["Feng Jiang", "Zihao Zheng", "Xiuping Cui", "Maoliang Li", "JIayu Chen", "Xiang Chen"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      There is an error in this paper, and as the author, I request retraction", "url": "http://arxiv.org/abs/2505.21567v2", "summary": "With the development of Embodied Artificial intelligence, the end-to-end\ncontrol policy such as Vision-Language-Action (VLA) model has become the\nmainstream. Existing VLA models faces expensive computing/storage cost, which\nneed to be optimized. Quantization is considered as the most effective method\nwhich can not only reduce the memory cost but also achieve computation\nacceleration. However, we find the token alignment of VLA models hinders the\napplication of existing quantization methods. To address this, we proposed an\noptimized framework called EaqVLA, which apply encoding-aligned quantization to\nVLA models. Specifically, we propose an complete analysis method to find the\nmisalignment in various granularity. Based on the analysis results, we propose\na mixed precision quantization with the awareness of encoding alignment.\nExperiments shows that the porposed EaqVLA achieves better quantization\nperformance (with the minimal quantization loss for end-to-end action control\nand xxx times acceleration) than existing quantization methods.", "comment": "There is an error in this paper, and as the author, I request\n  retraction", "pdf_url": "http://arxiv.org/pdf/2505.21567v2", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-07-31"}
{"id": "2504.05164", "title": "Balancing Task-invariant Interaction and Task-specific Adaptation for Unified Image Fusion", "authors": ["Xingyu Hu", "Junjun Jiang", "Chenyang Wang", "Kui Jiang", "Xianming Liu", "Jiayi Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2504.05164v2", "summary": "Unified image fusion aims to integrate complementary information from\nmulti-source images, enhancing image quality through a unified framework\napplicable to diverse fusion tasks. While treating all fusion tasks as a\nunified problem facilitates task-invariant knowledge sharing, it often\noverlooks task-specific characteristics, thereby limiting the overall\nperformance. Existing general image fusion methods incorporate explicit task\nidentification to enable adaptation to different fusion tasks. However, this\ndependence during inference restricts the model's generalization to unseen\nfusion tasks. To address these issues, we propose a novel unified image fusion\nframework named \"TITA\", which dynamically balances both Task-invariant\nInteraction and Task-specific Adaptation. For task-invariant interaction, we\nintroduce the Interaction-enhanced Pixel Attention (IPA) module to enhance\npixel-wise interactions for better multi-source complementary information\nextraction. For task-specific adaptation, the Operation-based Adaptive Fusion\n(OAF) module dynamically adjusts operation weights based on task properties.\nAdditionally, we incorporate the Fast Adaptive Multitask Optimization (FAMO)\nstrategy to mitigate the impact of gradient conflicts across tasks during joint\ntraining. Extensive experiments demonstrate that TITA not only achieves\ncompetitive performance compared to specialized methods across three image\nfusion scenarios but also exhibits strong generalization to unseen fusion\ntasks. The source codes are released at https://github.com/huxingyuabc/TITA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2504.05164v2", "cate": "cs.CV", "date": "2025-04-07", "updated": "2025-07-31"}
{"id": "2507.18675", "title": "Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks", "authors": ["Utkarsh Shandilya", "Marsha Mariya Kappan", "Sanyam Jain", "Vijeta Sharma"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18675v2", "summary": "Human action recognition plays a critical role in healthcare and medicine,\nsupporting applications such as patient behavior monitoring, fall detection,\nsurgical robot supervision, and procedural skill assessment. While traditional\nmodels like CNNs and RNNs have achieved moderate success, they often struggle\nto generalize across diverse and complex actions. Recent advancements in\nvision-language models, especially the transformer-based CLIP model, offer\npromising capabilities for generalizing action recognition from video data. In\nthis work, we evaluate CLIP on the UCF-101 dataset and systematically analyze\nits performance under three masking strategies: (1) percentage-based and\nshape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to\nsuppress bias-inducing elements, and (3) isolation masking that retains only\nclass-specific regions. Our results reveal that CLIP exhibits inconsistent\nbehavior and frequent misclassifications, particularly when essential visual\ncues are obscured. To overcome these limitations, we propose incorporating\nclass-specific noise, learned via a custom loss function, to reinforce\nattention to class-defining features. This enhancement improves classification\naccuracy and model confidence while reducing bias. We conclude with a\ndiscussion on the challenges of applying such models in clinical domains and\noutline directions for future work to improve generalizability across\ndomain-independent healthcare scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18675v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-30"}
{"id": "2504.13593", "title": "KAN or MLP? Point Cloud Shows the Way Forward", "authors": ["Yan Shi", "Qingdong He", "Yijun Liu", "Xiaoyu Liu", "Jingyong Su"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13593v2", "summary": "Multi-Layer Perceptrons (MLPs) have become one of the fundamental\narchitectural component in point cloud analysis due to its effective feature\nlearning mechanism. However, when processing complex geometric structures in\npoint clouds, MLPs' fixed activation functions struggle to efficiently capture\nlocal geometric features, while suffering from poor parameter efficiency and\nhigh model redundancy. In this paper, we propose PointKAN, which applies\nKolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate\ntheir efficacy in hierarchical feature representation. First, we introduce a\nGeometric Affine Module (GAM) to transform local features, improving the\nmodel's robustness to geometric variations. Next, in the Local Feature\nProcessing (LFP), a parallel structure extracts both group-level features and\nglobal context, providing a rich representation of both fine details and\noverall structure. Finally, these features are combined and processed in the\nGlobal Feature Processing (GFP). By repeating these operations, the receptive\nfield gradually expands, enabling the model to capture complete geometric\ninformation of the point cloud. To overcome the high parameter counts and\ncomputational inefficiency of standard KANs, we develop Efficient-KANs in the\nPointKAN-elite variant, which significantly reduces parameters while\nmaintaining accuracy. Experimental results demonstrate that PointKAN\noutperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN,\nand ShapeNetPart, with particularly strong performance in Few-shot Learning\ntask. Additionally, PointKAN achieves substantial reductions in parameter\ncounts and computational complexity (FLOPs). This work highlights the potential\nof KANs-based architectures in 3D vision and opens new avenues for research in\npoint cloud understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13593v2", "cate": "cs.CV", "date": "2025-04-18", "updated": "2025-07-31"}
{"id": "2507.19747", "title": "TokenBlowUp: Resolving Representational Singularities in LLM Token Spaces via Monoidal Transformations", "authors": ["Dongfang Zhao"], "categories": ["math.AG", "cs.LG"], "primary_category": "Subjects:       Algebraic Geometry (math.AG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19747v2", "summary": "Recent work has provided compelling evidence challenging the foundational\nmanifold hypothesis for the token embedding spaces of Large Language Models\n(LLMs). These findings reveal the presence of geometric singularities around\npolysemous tokens, which can lead to representational instability. Existing\nmethodologies, which presuppose a smooth data manifold, are ill-equipped to\naddress such intrinsic structural flaws. In this paper, we formalize this\nproblem in the language of scheme theory and propose a rigorous resolution by\napplying the scheme-theoretic blow-up at each singular point. This procedure\nreplaces a singular point in the ambient affine scheme with its exceptional\ndivisor, which we identify as a canonical geometric space -- a projective space\nof directions -- that houses the disambiguated semantic meanings of the token.\nThis process of ``representational desingularization'' constructs a new\ngeometric landscape for embeddings. We prove a formal theorem guaranteeing the\ngeometric regularization of this new space, showing that the original\npathologies are resolved. Finally, we outline the architectural implications of\nour framework, arguing for a paradigm shift from static look-ups to dynamic,\ngeometrically-grounded computation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19747v2", "cate": "math.AG", "date": "2025-07-26", "updated": "2025-07-30"}
{"id": "2504.17761", "title": "Step1X-Edit: A Practical Framework for General Image Editing", "authors": ["Shiyu Liu", "Yucheng Han", "Peng Xing", "Fukun Yin", "Rui Wang", "Wei Cheng", "Jiaqi Liao", "Yingming Wang", "Honghao Fu", "Chunrui Han", "Guopeng Li", "Yuang Peng", "Quan Sun", "Jingwei Wu", "Yan Cai", "Zheng Ge", "Ranchen Ming", "Lei Xia", "Xianfang Zeng", "Yibo Zhu", "Binxing Jiao", "Xiangyu Zhang", "Gang Yu", "Daxin Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      code: this https URL", "url": "http://arxiv.org/abs/2504.17761v5", "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "comment": "code: https://github.com/stepfun-ai/Step1X-Edit", "pdf_url": "http://arxiv.org/pdf/2504.17761v5", "cate": "cs.CV", "date": "2025-04-24", "updated": "2025-07-31"}
{"id": "2505.02178", "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery", "authors": ["Shubhendu Jena", "Amine Ouasfi", "Mae Younes", "Adnane Boukhayma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page : this https URL", "url": "http://arxiv.org/abs/2505.02178v4", "summary": "We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets.", "comment": "ICCV 2025. Project page :\n  https://shubhendu-jena.github.io/Sparfels-web/", "pdf_url": "http://arxiv.org/pdf/2505.02178v4", "cate": "cs.CV", "date": "2025-05-04", "updated": "2025-07-31"}
{"id": "2505.12620", "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation", "authors": ["Haiquan Wen", "Yiwei He", "Zhenglin Huang", "Tianxiao Li", "Zihan Yu", "Xingru Huang", "Lu Qi", "Baoyuan Wu", "Xiangtai Li", "Guangliang Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.12620v4", "summary": "Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.12620v4", "cate": "cs.CV", "date": "2025-05-19", "updated": "2025-07-31"}
{"id": "2505.14729", "title": "Uncovering Cultural Representation Disparities in Vision-Language Models", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Srishti Yadav", "Suman Debnath", "Alejandro Salamanca", "Desmond Elliott"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      28 pages, 36 figures", "url": "http://arxiv.org/abs/2505.14729v3", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities\nacross a range of tasks, yet concerns about their potential biases exist. This\nwork investigates the extent to which prominent VLMs exhibit cultural biases by\nevaluating their performance on an image-based country identification task at a\ncountry level. Utilizing the geographically diverse Country211 dataset, we\nprobe several large vision language models (VLMs) under various prompting\nstrategies: open-ended questions, multiple-choice questions (MCQs) including\nchallenging setups like multilingual and adversarial settings. Our analysis\naims to uncover disparities in model accuracy across different countries and\nquestion formats, providing insights into how training data distribution and\nevaluation methodologies might influence cultural biases in VLMs. The findings\nhighlight significant variations in performance, suggesting that while VLMs\npossess considerable visual understanding, they inherit biases from their\npre-training data and scale that impact their ability to generalize uniformly\nacross diverse global contexts.", "comment": "28 pages, 36 figures", "pdf_url": "http://arxiv.org/pdf/2505.14729v3", "cate": "cs.CV", "date": "2025-05-20", "updated": "2025-07-31"}
{"id": "2505.20884", "title": "YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation", "authors": ["Weichao Pan", "Bohan Xu", "Xu Wang", "Chengze Lv", "Shuoyang Wang", "Zhenke Duan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      2025 International Conference on Intelligent Computing (ICIC 2025)", "url": "http://arxiv.org/abs/2505.20884v2", "summary": "Fire detection in dynamic environments faces continuous challenges, including\nthe interference of illumination changes, many false detections or missed\ndetections, and it is difficult to achieve both efficiency and accuracy. To\naddress the problem of feature extraction limitation and information loss in\nthe existing YOLO-based models, this study propose You Only Look Once for Fire\nDetection with Attention-guided Inverted Residual and Dual-pooling Downscale\nFusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted\nResidual Block (AIR) integrates hybrid channel-spatial attention with inverted\nresiduals to adaptively enhance fire features and suppress environmental noise;\n(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns\nthrough learnable fusion of max-average pooling outputs, mitigating small-fire\ndetection failures. Extensive evaluation on two public datasets shows the\nefficient performance of our model. Our proposed model keeps the sum amount of\nparameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),\nand mAP75 is higher than the mainstream real-time object detection models\nYOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants\n1.3-5.5%. For more details, please visit our repository:\nhttps://github.com/JEFfersusu/YOLO-FireAD", "comment": "2025 International Conference on Intelligent Computing (ICIC 2025)", "pdf_url": "http://arxiv.org/pdf/2505.20884v2", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-07-31"}
{"id": "2505.24329", "title": "DisTime: Distribution-based Time Representation for Video Large Language Models", "authors": ["Yingsen Zeng", "Zepeng Huang", "Yujie Zhong", "Chengjian Feng", "Jie Hu", "Lin Ma", "Yang Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2505.24329v2", "summary": "Despite advances in general video understanding, Video Large Language Models\n(Video-LLMs) face challenges in precise temporal localization due to discrete\ntime representations and limited temporally aware datasets. Existing methods\nfor temporal expression either conflate time with text-based numerical values,\nadd a series of dedicated temporal tokens, or regress time using specialized\ntemporal grounding heads. To address these issues, we introduce DisTime, a\nlightweight framework designed to enhance temporal comprehension in Video-LLMs.\nDisTime employs a learnable token to create a continuous temporal embedding\nspace and incorporates a Distribution-based Time Decoder that generates\ntemporal probability distributions, effectively mitigating boundary ambiguities\nand maintaining temporal continuity. Additionally, the Distribution-based Time\nEncoder re-encodes timestamps to provide time markers for Video-LLMs. To\novercome temporal granularity limitations in existing datasets, we propose an\nautomated annotation paradigm that combines the captioning capabilities of\nVideo-LLMs with the localization expertise of dedicated temporal models. This\nleads to the creation of InternVid-TG, a substantial dataset with 1.25M\ntemporally grounded events across 179k videos, surpassing ActivityNet-Caption\nby 55 times. Extensive experiments demonstrate that DisTime achieves\nstate-of-the-art performance across benchmarks in three time-sensitive tasks\nwhile maintaining competitive performance in Video QA tasks. Code and data are\nreleased at https://github.com/josephzpng/DisTime.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2505.24329v2", "cate": "cs.CV", "date": "2025-05-30", "updated": "2025-07-31"}
{"id": "2506.00956", "title": "Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection", "authors": ["Geonu Lee", "Yujeong Oh", "Geonhui Jang", "Soyoung Lee", "Jeonghyo Song", "Sungmin Cha", "YoungJoon Yoo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00956v2", "summary": "In this paper, we introduce a new benchmark for continual learning in anomaly\ndetection, aimed at better reflecting real-world deployment scenarios. Our\nbenchmark, Continual-MEGA, includes a large and diverse dataset that\nsignificantly expands existing evaluation settings by combining carefully\ncurated existing datasets with our newly proposed dataset, ContinualAD. In\naddition to standard continual learning with expanded quantity, we propose a\nnovel scenario that measures zero-shot generalization to unseen classes, those\nnot observed during continual adaptation. This setting poses a new problem\nsetting that continual adaptation also enhances zero-shot performance. We also\npresent a unified baseline algorithm that improves robustness in few-shot\ndetection and maintains strong generalization. Through extensive evaluations,\nwe report three key findings: (1) existing methods show substantial room for\nimprovement, particularly in pixel-level defect localization; (2) our proposed\nmethod consistently outperforms prior approaches; and (3) the newly introduced\nContinualAD dataset enhances the performance of strong anomaly detection\nmodels. We release the benchmark and code in\nhttps://github.com/Continual-Mega/Continual-Mega.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00956v2", "cate": "cs.CV", "date": "2025-06-01", "updated": "2025-07-31"}
{"id": "2506.19330", "title": "Comparative Performance of Finetuned ImageNet Pre-trained Models for Electronic Component Classification", "authors": ["Yidi Shao", "Longfei Zhou", "Fangshuo Tang", "Xinyi Shi", "Dalang Chen", "Shengtao Xia"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Due to issues related to author order and some problems in the current version regarding methodology, we would like to withdraw the preprint to avoid potential conflicts", "url": "http://arxiv.org/abs/2506.19330v2", "summary": "Electronic component classification and detection are crucial in\nmanufacturing industries, significantly reducing labor costs and promoting\ntechnological and industrial development. Pre-trained models, especially those\ntrained on ImageNet, are highly effective in image classification, allowing\nresearchers to achieve excellent results even with limited data. This paper\ncompares the performance of twelve ImageNet pre-trained models in classifying\nelectronic components. Our findings show that all models tested delivered\nrespectable accuracies. MobileNet-V2 recorded the highest at 99.95%, while\nEfficientNet-B0 had the lowest at 92.26%. These results underscore the\nsubstantial benefits of using ImageNet pre-trained models in image\nclassification tasks and confirm the practical applicability of these methods\nin the electronics manufacturing sector.", "comment": "Due to issues related to author order and some problems in the\n  current version regarding methodology, we would like to withdraw the preprint\n  to avoid potential conflicts", "pdf_url": "http://arxiv.org/pdf/2506.19330v2", "cate": "cs.CV", "date": "2025-06-24", "updated": "2025-07-31"}
{"id": "2506.19955", "title": "ZIP: Scalable Crowd Counting via Zero-Inflated Poisson Modeling", "authors": ["Yiming Ma", "Victor Sanchez", "Tanaya Guha"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 11 figures", "url": "http://arxiv.org/abs/2506.19955v3", "summary": "Most crowd counting methods directly regress blockwise density maps using\nMean Squared Error (MSE) losses. This practice has two key limitations: (1) it\nfails to account for the extreme spatial sparsity of annotations - over 95% of\n8x8 blocks are empty across standard benchmarks, so supervision signals in\ninformative regions are diluted by the predominant zeros; (2) MSE corresponds\nto a Gaussian error model that poorly matches discrete, non-negative count\ndata. To address these issues, we introduce ZIP, a scalable crowd counting\nframework that models blockwise counts with a Zero-Inflated Poisson likelihood:\na zero-inflation term learns the probability a block is structurally empty\n(handling excess zeros), while the Poisson component captures expected counts\nwhen people are present (respecting discreteness). We provide a generalization\nanalysis showing a tighter risk bound for ZIP than MSE-based losses and DMCount\nprovided that the training resolution is moderately large. To assess the\nscalability of ZIP, we instantiate it on backbones spanning over 100x in\nparameters/compute. Experiments on ShanghaiTech A & B, UCF-QNRF, and NWPU-Crowd\ndemonstrate that ZIP consistently surpasses state-of-the-art methods across all\nmodel scales.", "comment": "15 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.19955v3", "cate": "cs.CV", "date": "2025-06-24", "updated": "2025-07-31"}
{"id": "2506.21509", "title": "Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration", "authors": ["Jiahe Chen", "Jiaying He", "Qian Shao", "Qiyuan Chen", "Jiahe Ying", "Hongxia Xu", "Jintai Chen", "Jianwei Zheng", "Jian Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21509v2", "summary": "Large Vision-Language Models (LVLMs) have demonstrated significant\nadvancements in multimodal understanding, yet they are frequently hampered by\nhallucination-the generation of text that contradicts visual input. Existing\ntraining-free decoding strategies exhibit critical limitations, including the\nuse of static constraints that do not adapt to semantic drift during\ngeneration, inefficiency stemming from the need for multiple forward passes,\nand degradation of detail due to overly rigid intervention rules. To overcome\nthese challenges, this paper introduces Dynamic Logits Calibration (DLC), a\nnovel training-free decoding framework designed to dynamically align text\ngeneration with visual evidence at inference time. At the decoding phase, DLC\nstep-wise employs CLIP to assess the semantic alignment between the input image\nand the generated text sequence. Then, the Relative Visual Advantage (RVA) of\ncandidate tokens is evaluated against a dynamically updated contextual\nbaseline, adaptively adjusting output logits to favor tokens that are visually\ngrounded. Furthermore, an adaptive weighting mechanism, informed by a real-time\ncontext alignment score, carefully balances the visual guidance while ensuring\nthe overall quality of the textual output. Extensive experiments conducted\nacross diverse benchmarks and various LVLM architectures (such as LLaVA,\nInstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces\nhallucinations, outperforming current methods while maintaining high inference\nefficiency by avoiding multiple forward passes. Overall, we present an\neffective and efficient decoding-time solution to mitigate hallucinations,\nthereby enhancing the reliability of LVLMs for more practices. Code will be\nreleased on Github.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21509v2", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-31"}
{"id": "2507.13373", "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection", "authors": ["Xiaojian Lin", "Wenxin Zhang", "Yuchu Jiang", "Wangyu Wu", "Yiran Guo", "Kangxu Wang", "Zongzheng Zhang", "Guijin Wang", "Lei Jin", "Hao Zhao"], "categories": ["cs.CV", "I.4.8; I.2.10; H.5.1; I.2.6"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures. Supplementary material: 8 pages, 7 figures. Accepted at ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.13373v2", "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.", "comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.13373v2", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-31"}
{"id": "2507.14632", "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14632v2", "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14632v2", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-31"}
{"id": "2507.18371", "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image", "authors": ["DongFu Yin", "Xiaotian Chen", "Fei Richard Yu", "Xuanchen Li", "Xinhao Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18371v2", "summary": "Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18371v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2507.19621", "title": "Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond", "authors": ["Sheethal Bhat", "Bogdan Georgescu", "Adarsh Bhandary Panambur", "Mathias Zinnen", "Tri-Thien Nguyen", "Awais Mansoor", "Karim Khalifa Elbarbary", "Siming Bayer", "Florin-Cristian Ghesu", "Sasa Grbic", "Andreas Maier"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      I am asking for a withdrawal of the paper as I did not have institutional approval to release this paper right now", "url": "http://arxiv.org/abs/2507.19621v2", "summary": "Detecting abnormalities in medical images poses unique challenges due to\ndifferences in feature representations and the intricate relationship between\nanatomical structures and abnormalities. This is especially evident in\nmammography, where dense breast tissue can obscure lesions, complicating\nradiological interpretation. Despite leveraging anatomical and semantic\ncontext, existing detection methods struggle to learn effective class-specific\nfeatures, limiting their applicability across different tasks and imaging\nmodalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal\ncontrastive detector that enables feature-based detection. It employs\ncross-attention with inherently derived, intuitive class-specific exemplar\nfeatures and is trained with an iterative strategy. We achieve state-of-the-art\nperformance across three distinct imaging modalities from four public datasets.\nOn Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass\ndetection and 0.55 for calcifications, yielding an absolute improvement of 16\npercentage points. Additionally, a radiologist-supported evaluation of 100\nmammograms from an out-of-distribution Chinese cohort demonstrates a twofold\ngain in lesion detection performance. For chest X-rays and angiography, we\nachieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving\nresults by 4 and 7 percentage points, respectively. These results highlight the\npotential of our approach to advance robust and generalizable detection systems\nfor medical imaging.", "comment": "I am asking for a withdrawal of the paper as I did not have\n  institutional approval to release this paper right now", "pdf_url": "http://arxiv.org/pdf/2507.19621v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-30"}
{"id": "2507.20216", "title": "Dual-Stream Global-Local Feature Collaborative Representation Network for Scene Classification of Mining Area", "authors": ["Shuqi Fan", "Haoyi Wang", "Xianju Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IJCNN 2025", "url": "http://arxiv.org/abs/2507.20216v2", "summary": "Scene classification of mining areas provides accurate foundational data for\ngeological environment monitoring and resource development planning. This study\nfuses multi-source data to construct a multi-modal mine land cover scene\nclassification dataset. A significant challenge in mining area classification\nlies in the complex spatial layout and multi-scale characteristics. By\nextracting global and local features, it becomes possible to comprehensively\nreflect the spatial distribution, thereby enabling a more accurate capture of\nthe holistic characteristics of mining scenes. We propose a dual-branch fusion\nmodel utilizing collaborative representation to decompose global features into\na set of key semantic vectors. This model comprises three key components:(1)\nMulti-scale Global Transformer Branch: It leverages adjacent large-scale\nfeatures to generate global channel attention features for small-scale\nfeatures, effectively capturing the multi-scale feature relationships. (2)\nLocal Enhancement Collaborative Representation Branch: It refines the attention\nweights by leveraging local features and reconstructed key semantic sets,\nensuring that the local context and detailed characteristics of the mining area\nare effectively integrated. This enhances the model's sensitivity to\nfine-grained spatial variations. (3) Dual-Branch Deep Feature Fusion Module: It\nfuses the complementary features of the two branches to incorporate more scene\ninformation. This fusion strengthens the model's ability to distinguish and\nclassify complex mining landscapes. Finally, this study employs multi-loss\ncomputation to ensure a balanced integration of the modules. The overall\naccuracy of this model is 83.63%, which outperforms other comparative models.\nAdditionally, it achieves the best performance across all other evaluation\nmetrics.", "comment": "Accepted to IJCNN 2025", "pdf_url": "http://arxiv.org/pdf/2507.20216v2", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2507.20356", "title": "Detecting Visual Information Manipulation Attacks in Augmented Reality: A Multimodal Semantic Reasoning Approach", "authors": ["Yanming Xiu", "Maria Gorlatova"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures", "url": "http://arxiv.org/abs/2507.20356v2", "summary": "The virtual content in augmented reality (AR) can introduce misleading or\nharmful information, leading to semantic misunderstandings or user errors. In\nthis work, we focus on visual information manipulation (VIM) attacks in AR\nwhere virtual content changes the meaning of real-world scenes in subtle but\nimpactful ways. We introduce a taxonomy that categorizes these attacks into\nthree formats: character, phrase, and pattern manipulation, and three purposes:\ninformation replacement, information obfuscation, and extra wrong information.\nBased on the taxonomy, we construct a dataset, AR-VIM. It consists of 452\nraw-AR video pairs spanning 202 different scenes, each simulating a real-world\nAR scenario. To detect such attacks, we propose a multimodal semantic reasoning\nframework, VIM-Sense. It combines the language and visual understanding\ncapabilities of vision-language models (VLMs) with optical character\nrecognition (OCR)-based textual analysis. VIM-Sense achieves an attack\ndetection accuracy of 88.94% on AR-VIM, consistently outperforming vision-only\nand text-only baselines. The system reaches an average attack detection latency\nof 7.07 seconds in a simulated video processing framework and 7.17 seconds in a\nreal-world evaluation conducted on a mobile Android AR application.", "comment": "11 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.20356v2", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2507.20414", "title": "Indian Sign Language Detection for Real-Time Translation using Machine Learning", "authors": ["Rajat Singhal", "Jatin Gupta", "Akhil Sharma", "Anushka Gupta", "Navya Sharma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, 2 tables. Published in Proceedings of the 6th International Conference on Recent Advances in Information Technology (RAIT), 2025, IEEE", "url": "http://arxiv.org/abs/2507.20414v2", "summary": "Gestural language is used by deaf & mute communities to communicate through\nhand gestures & body movements that rely on visual-spatial patterns known as\nsign languages. Sign languages, which rely on visual-spatial patterns of hand\ngestures & body movements, are the primary mode of communication for deaf &\nmute communities worldwide. Effective communication is fundamental to human\ninteraction, yet individuals in these communities often face significant\nbarriers due to a scarcity of skilled interpreters & accessible translation\ntechnologies. This research specifically addresses these challenges within the\nIndian context by focusing on Indian Sign Language (ISL). By leveraging machine\nlearning, this study aims to bridge the critical communication gap for the deaf\n& hard-of-hearing population in India, where technological solutions for ISL\nare less developed compared to other global sign languages. We propose a\nrobust, real-time ISL detection & translation system built upon a Convolutional\nNeural Network (CNN). Our model is trained on a comprehensive ISL dataset &\ndemonstrates exceptional performance, achieving a classification accuracy of\n99.95%. This high precision underscores the model's capability to discern the\nnuanced visual features of different signs. The system's effectiveness is\nrigorously evaluated using key performance metrics, including accuracy, F1\nscore, precision & recall, ensuring its reliability for real-world\napplications. For real-time implementation, the framework integrates MediaPipe\nfor precise hand tracking & motion detection, enabling seamless translation of\ndynamic gestures. This paper provides a detailed account of the model's\narchitecture, the data preprocessing pipeline & the classification methodology.\nThe research elaborates the model architecture, preprocessing & classification\nmethodologies for enhancing communication in deaf & mute communities.", "comment": "7 pages, 6 figures, 2 tables. Published in Proceedings of the 6th\n  International Conference on Recent Advances in Information Technology (RAIT),\n  2025, IEEE", "pdf_url": "http://arxiv.org/pdf/2507.20414v2", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2507.20469", "title": "Priority-Aware Clinical Pathology Hierarchy Training for Multiple Instance Learning", "authors": ["Sungrae Hong", "Kyungeun Kim", "Juhyeon Kim", "Sol Lee", "Jisu Shin", "Chanjae Song", "Mun Yong Yi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, Accepted for oral presentation by The 2nd MICCAI Student Board (MSB) EMERGE Workshop", "url": "http://arxiv.org/abs/2507.20469v2", "summary": "Multiple Instance Learning (MIL) is increasingly being used as a support tool\nwithin clinical settings for pathological diagnosis decisions, achieving high\nperformance and removing the annotation burden. However, existing approaches\nfor clinical MIL tasks have not adequately addressed the priority issues that\nexist in relation to pathological symptoms and diagnostic classes, causing MIL\nmodels to ignore priority among classes. To overcome this clinical limitation\nof MIL, we propose a new method that addresses priority issues using two\nhierarchies: vertical inter-hierarchy and horizontal intra-hierarchy. The\nproposed method aligns MIL predictions across each hierarchical level and\nemploys an implicit feature re-usability during training to facilitate\nclinically more serious classes within the same level. Experiments with\nreal-world patient data show that the proposed method effectively reduces\nmisdiagnosis and prioritizes more important symptoms in multiclass scenarios.\nFurther analysis verifies the efficacy of the proposed components and\nqualitatively confirms the MIL predictions against challenging cases with\nmultiple symptoms.", "comment": "10 pages, 4 figures, Accepted for oral presentation by The 2nd MICCAI\n  Student Board (MSB) EMERGE Workshop", "pdf_url": "http://arxiv.org/pdf/2507.20469v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2507.21358", "title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy", "authors": ["Jicheng Yuan", "Manh Nguyen Duc", "Qian Liu", "Manfred Hauswirth", "Danh Le Phuoc"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The manuscript has been accepted by ICONIP2025", "url": "http://arxiv.org/abs/2507.21358v3", "summary": "Vision-based bird's-eye-view (BEV) 3D object detection has advanced\nsignificantly in autonomous driving by offering cost-effectiveness and rich\ncontextual information. However, existing methods often construct BEV\nrepresentations by collapsing extracted object features, neglecting intrinsic\nenvironmental contexts, such as roads and pavements. This hinders detectors\nfrom comprehensively perceiving the characteristics of the physical world. To\nalleviate this, we introduce a multi-task learning framework, Collaborative\nPerceiver (CoP), that leverages spatial occupancy as auxiliary information to\nmine consistent structural and conceptual similarities shared between 3D object\ndetection and occupancy prediction tasks, bridging gaps in spatial\nrepresentations and feature refinement. To this end, we first propose a\npipeline to generate dense occupancy ground truths incorporating local density\ninformation (LDO) for reconstructing detailed environmental information. Next,\nwe employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained\nlocal features according to distinct object properties. Furthermore, we develop\na global-local collaborative feature fusion (CFF) module that seamlessly\nintegrates complementary knowledge between both tasks, thus composing more\nrobust BEV representations. Extensive experiments on the nuScenes benchmark\ndemonstrate that CoP outperforms existing vision-based frameworks, achieving\n49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are\navailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.", "comment": "The manuscript has been accepted by ICONIP2025", "pdf_url": "http://arxiv.org/pdf/2507.21358v3", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2507.21584", "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs", "authors": ["Kejia Zhang", "Keda Tao", "Zhiming Luo", "Chang Liu", "Jiasheng Tang", "Huan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21584v2", "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21584v2", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.22136", "title": "Color as the Impetus: Transforming Few-Shot Learner", "authors": ["Chaofei Qi", "Zhitai Liu", "Jianbin Qiu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22136v2", "summary": "Humans possess innate meta-learning capabilities, partly attributable to\ntheir exceptional color perception. In this paper, we pioneer an innovative\nviewpoint on few-shot learning by simulating human color perception mechanisms.\nWe propose the ColorSense Learner, a bio-inspired meta-learning framework that\ncapitalizes on inter-channel feature extraction and interactive learning. By\nstrategically emphasizing distinct color information across different channels,\nour approach effectively filters irrelevant features while capturing\ndiscriminative characteristics. Color information represents the most intuitive\nvisual feature, yet conventional meta-learning methods have predominantly\nneglected this aspect, focusing instead on abstract feature differentiation\nacross categories. Our framework bridges the gap via synergistic color-channel\ninteractions, enabling better intra-class commonality extraction and larger\ninter-class differences. Furthermore, we introduce a meta-distiller based on\nknowledge distillation, ColorSense Distiller, which incorporates prior teacher\nknowledge to augment the student network's meta-learning capacity. We've\nconducted comprehensive coarse/fine-grained and cross-domain experiments on\neleven few-shot benchmarks for validation. Numerous experiments reveal that our\nmethods have extremely strong generalization ability, robustness, and\ntransferability, and effortless handle few-shot classification from the\nperspective of color perception.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22136v2", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.22886", "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation", "authors": ["Kaining Ying", "Henghui Ding", "Guangquan Jie", "Yu-Gang Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.22886v2", "summary": "Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovisual content. To extend the\nboundaries of RAVS and facilitate future research in this field, we propose\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\ncontaining 2,104 videos and 61,095 multimodal referring expressions. OmniAVS\nstands out with three key innovations: (1) 8 types of multimodal expressions\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\nunderstanding audio content beyond just detecting their presence; and (3) the\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\nchallenges of multimodal reasoning and fine-grained understanding of\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\nperform reasoning-based segmentation. Extensive experiments show that OISA\noutperforms existing methods on OmniAVS and achieves competitive results on\nother related tasks.", "comment": "ICCV 2025, Project Page: https://henghuiding.com/OmniAVS/", "pdf_url": "http://arxiv.org/pdf/2507.22886v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2412.13811", "title": "A Lightweight Optimization Framework for Estimating 3D Brain Tumor Infiltration", "authors": ["Jonas Weidner", "Michal Balcerak", "Ivan Ezhov", "André Datchev", "Laurin Lux", "Lucas Zimmer", "Daniel Rueckert", "Björn Menze", "Benedikt Wiestler"], "categories": ["physics.med-ph", "cs.CV"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.13811v2", "summary": "Glioblastoma, the most aggressive primary brain tumor, poses a severe\nclinical challenge due to its diffuse microscopic infiltration, which remains\nlargely undetected on standard MRI. As a result, current radiotherapy planning\nemploys a uniform 15 mm margin around the resection cavity, failing to capture\npatient-specific tumor spread. Tumor growth modeling offers a promising\napproach to reveal this hidden infiltration. However, methods based on partial\ndifferential equations or physics-informed neural networks tend to be\ncomputationally intensive or overly constrained, limiting their clinical\nadaptability to individual patients. In this work, we propose a lightweight,\nrapid, and robust optimization framework that estimates the 3D tumor\nconcentration by fitting it to MRI tumor segmentations while enforcing a smooth\nconcentration landscape. This approach achieves superior tumor recurrence\nprediction on 192 brain tumor patients across two public datasets,\noutperforming state-of-the-art baselines while reducing runtime from 30 minutes\nto less than one minute. Furthermore, we demonstrate the framework's\nversatility and adaptability by showing its ability to seamlessly integrate\nadditional imaging modalities or physical constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.13811v2", "cate": "physics.med-ph", "date": "2024-12-18", "updated": "2025-07-31"}
