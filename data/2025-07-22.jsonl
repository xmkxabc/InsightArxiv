{"id": "2507.15859", "title": "Decentralized AI-driven IoT Architecture for Privacy-Preserving and Latency-Optimized Healthcare in Pandemic and Critical Care Scenarios", "authors": ["Harsha Sammangi", "Aditya Jagatha", "Giridhar Reddy Bojja", "Jun Liu"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      10 Pages", "url": "http://arxiv.org/abs/2507.15859v1", "summary": "AI Innovations in the IoT for Real-Time Patient Monitoring On one hand, the\ncurrent traditional centralized healthcare architecture poses numerous issues,\nincluding data privacy, delay, and security. Here, we present an AI-enabled\ndecentralized IoT architecture that can address such challenges during a\npandemic and critical care settings. This work presents our architecture to\nenhance the effectiveness of the current available federated learning,\nblockchain, and edge computing approach, maximizing data privacy, minimizing\nlatency, and improving other general system metrics. Experimental results\ndemonstrate transaction latency, energy consumption, and data throughput orders\nof magnitude lower than competitive cloud solutions.", "comment": "10 Pages", "pdf_url": "http://arxiv.org/pdf/2507.15859v1", "cate": "cs.CR", "date": "2025-04-29", "updated": "2025-04-29"}
{"id": "2507.15984", "title": "BACFuzz: Exposing the Silence on Broken Access Control Vulnerabilities in Web Applications", "authors": ["I Putu Arya Dharmaadi", "Mohannad Alhanahnah", "Van-Thuan Pham", "Fadi Mohsen", "Fatih Turkmen"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Under peer-review", "url": "http://arxiv.org/abs/2507.15984v1", "summary": "Broken Access Control (BAC) remains one of the most critical and widespread\nvulnerabilities in web applications, allowing attackers to access unauthorized\nresources or perform privileged actions. Despite its severity, BAC is\nunderexplored in automated testing due to key challenges: the lack of reliable\noracles and the difficulty of generating semantically valid attack requests. We\nintroduce BACFuzz, the first gray-box fuzzing framework specifically designed\nto uncover BAC vulnerabilities, including Broken Object-Level Authorization\n(BOLA) and Broken Function-Level Authorization (BFLA) in PHP-based web\napplications. BACFuzz combines LLM-guided parameter selection with runtime\nfeedback and SQL-based oracle checking to detect silent authorization flaws. It\nemploys lightweight instrumentation to capture runtime information that guides\ntest generation, and analyzes backend SQL queries to verify whether\nunauthorized inputs flow into protected operations. Evaluated on 20 real-world\nweb applications, including 15 CVE cases and 2 known benchmarks, BACFuzz\ndetects 16 of 17 known issues and uncovers 26 previously unknown BAC\nvulnerabilities with low false positive rates. All identified issues have been\nresponsibly disclosed, and artifacts will be publicly released.", "comment": "Under peer-review", "pdf_url": "http://arxiv.org/pdf/2507.15984v1", "cate": "cs.CR", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15997", "title": "\"We Need a Standard\": Toward an Expert-Informed Privacy Label for Differential Privacy", "authors": ["Onyinye Dibia", "Mengyi Lu", "Prianka Bhattacharjee", "Joseph P. Near", "Yuanyuan Feng"], "categories": ["cs.CR", "cs.HC", "68-XX 68-XX 68-XX"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      13 pages, 5 figures", "url": "http://arxiv.org/abs/2507.15997v1", "summary": "The increasing adoption of differential privacy (DP) leads to public-facing\nDP deployments by both government agencies and companies. However, real-world\nDP deployments often do not fully disclose their privacy guarantees, which vary\ngreatly between deployments. Failure to disclose certain DP parameters can lead\nto misunderstandings about the strength of the privacy guarantee, undermining\nthe trust in DP. In this work, we seek to inform future standards for\ncommunicating the privacy guarantees of DP deployments. Based on\nsemi-structured interviews with 12 DP experts, we identify important DP\nparameters necessary to comprehensively communicate DP guarantees, and describe\nwhy and how they should be disclosed. Based on expert recommendations, we\ndesign an initial privacy label for DP to comprehensively communicate privacy\nguarantees in a standardized format.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.15997v1", "cate": "cs.CR", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16040", "title": "Blocklisted Oblivious Pseudorandom Functions", "authors": ["Xinyuan Zhang", "Anrin Chakraborti", "Michael Reiter"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16040v1", "summary": "An oblivious pseudorandom function (OPRF) is a protocol by which a client and\nserver interact to evaluate a pseudorandom function on a key provided by the\nserver and an input provided by the client, without divulging the key or input\nto the other party. We extend this notion by enabling the server to specify a\nblocklist, such that OPRF evaluation succeeds only if the client's input is not\non the blocklist. More specifically, our design gains performance by embedding\nthe client input into a metric space, where evaluation continues only if this\nembedding does not cluster with blocklist elements. Our framework exploits this\nstructure to separate the embedding and blocklist check to enable efficient\nimplementations of each, but then must stitch these phases together through\ncryptographic means. Our framework also supports subsequent evaluation of the\nOPRF on the same input more efficiently. We demonstrate the use of our design\nfor password blocklisting in augmented password-authenticated key exchange, and\nto MAC only executables that are not similar to ones on a blocklist of known\nmalware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16040v1", "cate": "cs.CR", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16060", "title": "MFAz: Historical Access Based Multi-Factor Authorization", "authors": ["Eyasu Getahun Chekole", "Howard Halim", "Jianying Zhou"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16060v1", "summary": "Unauthorized access remains one of the critical security challenges in the\nrealm of cybersecurity. With the increasing sophistication of attack\ntechniques, the threat of unauthorized access is no longer confined to the\nconventional ones, such as exploiting weak access control policies. Instead,\nadvanced exploitation strategies, such as session hijacking-based attacks, are\nbecoming increasingly prevalent, posing serious security concerns. Session\nhijacking enables attackers to take over an already established session between\nlegitimate peers in a stealthy manner, thereby gaining unauthorized access to\nprivate resources. Unfortunately, traditional access control mechanisms, such\nas static access control policies, are insufficient to prevent session\nhijacking or other advanced exploitation techniques. In this work, we propose a\nnew multi-factor authorization (MFAz) scheme that proactively mitigates\nunauthorized access attempts both conventional and advanced unauthorized access\nattacks. The proposed scheme employs fine-grained access control rules (ARs)\nand verification points (VPs) that are systematically generated from\nhistorically granted accesses as the first and second authorization factors,\nrespectively. As a proof-of-concept, we implement the scheme using different\ntechniques. We leverage bloom filter to achieve runtime and storage efficiency,\nand blockchain to make authorization decisions in a temper-proof and\ndecentralized manner. To the best of our knowledge, this is the first formal\nintroduction of a multi-factor authorization scheme, which is orthogonal to the\nmulti-factor authentication (MFA) schemes. The effectiveness of our proposed\nscheme is experimentally evaluated using a smart-city testbed involving\ndifferent devices with varying computational capacities. The experimental\nresults reveal high effectiveness of the scheme both in security and\nperformance guarantees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16060v1", "cate": "cs.CR", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16134", "title": "DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT", "authors": ["Baofu Han", "Bing Li", "Yining Qi", "Raja Jurdak", "Kaibin Huang", "Chau Yuen"], "categories": ["cs.CR", "cs.DC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16134v1", "summary": "Privacy-Preserving Federated Learning (PPFL) has emerged as a secure\ndistributed Machine Learning (ML) paradigm that aggregates locally trained\ngradients without exposing raw data. To defend against model poisoning threats,\nseveral robustness-enhanced PPFL schemes have been proposed by integrating\nanomaly detection. Nevertheless, they still face two major challenges: (1) the\nreliance on heavyweight encryption techniques results in substantial\ncommunication and computation overhead; and (2) single-strategy defense\nmechanisms often fail to provide sufficient robustness against adaptive\nadversaries. To overcome these challenges, we propose DP2Guard, a lightweight\nPPFL framework that enhances both privacy and robustness. DP2Guard leverages a\nlightweight gradient masking mechanism to replace costly cryptographic\noperations while ensuring the privacy of local gradients. A hybrid defense\nstrategy is proposed, which extracts gradient features using singular value\ndecomposition and cosine similarity, and applies a clustering algorithm to\neffectively identify malicious gradients. Additionally, DP2Guard adopts a trust\nscore-based adaptive aggregation scheme that adjusts client weights according\nto historical behavior, while blockchain records aggregated results and trust\nscores to ensure tamper-proof and auditable training. Extensive experiments\nconducted on two public datasets demonstrate that DP2Guard effectively defends\nagainst four advanced poisoning attacks while ensuring privacy with reduced\ncommunication and computation costs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16134v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16164", "title": "Attacking interpretable NLP systems", "authors": ["Eldor Abdukhamidov", "Tamer Abuhmed", "Joanna C. S. Santos", "Mohammed Abuhamad"], "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; D.4.6"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16164v1", "summary": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16164v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16203", "title": "SVAgent: AI Agent for Hardware Security Verification Assertion", "authors": ["Rui Guo", "Avinash Ayalasomayajula", "Henian Li", "Jingbo Zhou", "Sujan Kumar Saha", "Farimah Farahmandi"], "categories": ["cs.CR", "cs.AI", "cs.AR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16203v1", "summary": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16203v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16241", "title": "eX-NIDS: A Framework for Explainable Network Intrusion Detection Leveraging Large Language Models", "authors": ["Paul R. B. Houssel", "Siamak Layeghy", "Priyanka Singh", "Marius Portmann"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16241v1", "summary": "This paper introduces eX-NIDS, a framework designed to enhance\ninterpretability in flow-based Network Intrusion Detection Systems (NIDS) by\nleveraging Large Language Models (LLMs). In our proposed framework, flows\nlabelled as malicious by NIDS are initially processed through a module called\nthe Prompt Augmenter. This module extracts contextual information and Cyber\nThreat Intelligence (CTI)-related knowledge from these flows. This enriched,\ncontext-specific data is then integrated with an input prompt for an LLM,\nenabling it to generate detailed explanations and interpretations of why the\nflow was identified as malicious by NIDS. We compare the generated\ninterpretations against a Basic-Prompt Explainer baseline, which does not\nincorporate any contextual information into the LLM's input prompt. Our\nframework is quantitatively evaluated using the Llama 3 and GPT-4 models,\nemploying a novel evaluation method tailored for natural language explanations,\nfocusing on their correctness and consistency. The results demonstrate that\naugmented LLMs can produce accurate and consistent explanations, serving as\nvaluable complementary tools in NIDS to explain the classification of malicious\nflows. The use of augmented prompts enhances performance by over 20% compared\nto the Basic-Prompt Explainer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16241v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16276", "title": "From Contracts to Code: Automating Smart Contract Generation with Multi-Level Finite State Machines", "authors": ["Lambard Maxence", "Bertelle Cyrille", "Duvallet Claude"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16276v1", "summary": "In an increasingly complex contractual landscape, the demand for\ntransparency, security, and efficiency has intensified. Blockchain technology,\nwith its decentralized and immutable nature, addresses these challenges by\nreducing intermediary costs, minimizing fraud risks, and enhancing system\ncompatibility. Smart contracts, initially conceptualized by Nick Szabo and\nlater implemented on the Ethereum blockchain, automate and secure contractual\nclauses, offering a robust solution for various industries. However, their\ncomplexity and the requirement for advanced programming skills present\nsignificant barriers to widespread adoption. This study introduces a\nmulti-level finite state machine model designed to represent and track the\nexecution of smart contracts. Our model aims to simplify smart contract\ndevelopment by providing a formalized framework that abstracts underlying\ntechnical complexities, making it accessible to professionals without deep\ntechnical expertise. The hierarchical structure of the multi-level finite state\nmachine enhances contract modularity and traceability, facilitating detailed\nrepresentation and evaluation of functional properties. The paper explores the\npotential of this multi-level approach, reviewing existing methodologies and\ntools, and detailing the smart contract generation process with an emphasis on\nreusable components and modularity. We also conduct a security analysis to\nevaluate potential vulnerabilities in our model, ensuring the robustness and\nreliability of the generated smart contracts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16276v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16291", "title": "Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted by EAI ICDF2C 2025", "url": "http://arxiv.org/abs/2507.16291v1", "summary": "Voice phishing (vishing) remains a persistent threat in cybersecurity,\nexploiting human trust through persuasive speech. While machine learning\n(ML)-based classifiers have shown promise in detecting malicious call\ntranscripts, they remain vulnerable to adversarial manipulations that preserve\nsemantic content. In this study, we explore a novel attack vector where large\nlanguage models (LLMs) are leveraged to generate adversarial vishing\ntranscripts that evade detection while maintaining deceptive intent. We\nconstruct a systematic attack pipeline that employs prompt engineering and\nsemantic obfuscation to transform real-world vishing scripts using four\ncommercial LLMs. The generated transcripts are evaluated against multiple ML\nclassifiers trained on a real-world Korean vishing dataset (KorCCViD) with\nstatistical testing. Our experiments reveal that LLM-generated transcripts are\nboth practically and statistically effective against ML-based classifiers. In\nparticular, transcripts crafted by GPT-4o significantly reduce classifier\naccuracy (by up to 30.96%) while maintaining high semantic similarity, as\nmeasured by BERTScore. Moreover, these attacks are both time-efficient and\ncost-effective, with average generation times under 9 seconds and negligible\nfinancial cost per query. The results underscore the pressing need for more\nresilient vishing detection frameworks and highlight the imperative for LLM\nproviders to enforce stronger safeguards against prompt misuse in adversarial\nsocial engineering contexts.", "comment": "Accepted by EAI ICDF2C 2025", "pdf_url": "http://arxiv.org/pdf/2507.16291v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16329", "title": "DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling", "authors": ["Boheng Li", "Junjie Wang", "Yiming Li", "Zhiyang Hu", "Leyi Qi", "Jianshuo Dong", "Run Wang", "Han Qiu", "Zhan Qin", "Tianwei Zhang"], "categories": ["cs.CR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Preprint version. Under review", "url": "http://arxiv.org/abs/2507.16329v1", "summary": "Despite the integration of safety alignment and external filters,\ntext-to-image (T2I) generative models are still susceptible to producing\nharmful content, such as sexual or violent imagery. This raises serious\nconcerns about unintended exposure and potential misuse. Red teaming, which\naims to proactively identify diverse prompts that can elicit unsafe outputs\nfrom the T2I system (including the core generative model as well as potential\nexternal safety filters and other processing components), is increasingly\nrecognized as an essential method for assessing and improving safety before\nreal-world deployment. Yet, existing automated red teaming approaches often\ntreat prompt discovery as an isolated, prompt-level optimization task, which\nlimits their scalability, diversity, and overall effectiveness. To bridge this\ngap, in this paper, we propose DREAM, a scalable red teaming framework to\nautomatically uncover diverse problematic prompts from a given T2I system.\nUnlike most prior works that optimize prompts individually, DREAM directly\nmodels the probabilistic distribution of the target system's problematic\nprompts, which enables explicit optimization over both effectiveness and\ndiversity, and allows efficient large-scale sampling after training. To achieve\nthis without direct access to representative training samples, we draw\ninspiration from energy-based models and reformulate the objective into simple\nand tractable objectives. We further introduce GC-SPSA, an efficient\noptimization algorithm that provide stable gradient estimates through the long\nand potentially non-differentiable T2I pipeline. The effectiveness of DREAM is\nvalidated through extensive experiments, demonstrating that it surpasses 9\nstate-of-the-art baselines by a notable margin across a broad range of T2I\nmodels and safety filters in terms of prompt success rate and diversity.", "comment": "Preprint version. Under review", "pdf_url": "http://arxiv.org/pdf/2507.16329v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16372", "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion", "authors": ["Tian Dong", "Yan Meng", "Shaofeng Li", "Guoxing Chen", "Zhen Liu", "Haojin Zhu"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted by USENIX Security 2025. Please cite this paper as \"Tian Dong, Yan Meng, Shaofeng Li, Guoxing Chen, Zhen Liu, Haojin Zhu. Depth Gives a False Sense of Privacy: LLM Internal States Inversion. In the 34th USENIX Security Symposium (USENIX Security '25).\"", "url": "http://arxiv.org/abs/2507.16372v1", "summary": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design.", "comment": "Accepted by USENIX Security 2025. Please cite this paper as \"Tian\n  Dong, Yan Meng, Shaofeng Li, Guoxing Chen, Zhen Liu, Haojin Zhu. Depth Gives\n  a False Sense of Privacy: LLM Internal States Inversion. In the 34th USENIX\n  Security Symposium (USENIX Security '25).\"", "pdf_url": "http://arxiv.org/pdf/2507.16372v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16540", "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks", "authors": ["Radowanul Haque", "Aftab Ali", "Sally McClean", "Naveed Khan"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16540v1", "summary": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16540v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16306", "title": "COMPASS: Cooperative Multi-Agent Persistent Monitoring using Spatio-Temporal Attention Network", "authors": ["Xingjian Zhang", "Yizhuo Wang", "Guillaume Sartoretti"], "categories": ["cs.MA", "cs.RO"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16306v1", "summary": "Persistent monitoring of dynamic targets is essential in real-world\napplications such as disaster response, environmental sensing, and wildlife\nconservation, where mobile agents must continuously gather information under\nuncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL)\nframework that enables decentralized agents to persistently monitor multiple\nmoving targets efficiently. We model the environment as a graph, where nodes\nrepresent spatial locations and edges capture topological proximity, allowing\nagents to reason over structured layouts and revisit informative regions as\nneeded. Each agent independently selects actions based on a shared\nspatio-temporal attention network that we design to integrate historical\nobservations and spatial context. We model target dynamics using Gaussian\nProcesses (GPs), which support principled belief updates and enable\nuncertainty-aware planning. We train COMPASS using centralized value estimation\nand decentralized policy execution under an adaptive reward setting. Our\nextensive experiments demonstrate that COMPASS consistently outperforms strong\nbaselines in uncertainty reduction, target coverage, and coordination\nefficiency across dynamic multi-target scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16306v1", "cate": "cs.MA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16576", "title": "From Text to Actionable Intelligence: Automating STIX Entity and Relationship Extraction", "authors": ["Ahmed Lekssays", "Husrev Taha Sencar", "Ting Yu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      This paper is accepted at RAID 2025", "url": "http://arxiv.org/abs/2507.16576v1", "summary": "Sharing methods of attack and their effectiveness is a cornerstone of\nbuilding robust defensive systems. Threat analysis reports, produced by various\nindividuals and organizations, play a critical role in supporting security\noperations and combating emerging threats. To enhance the timeliness and\nautomation of threat intelligence sharing, several standards have been\nestablished, with the Structured Threat Information Expression (STIX) framework\nemerging as one of the most widely adopted. However, generating STIX-compatible\ndata from unstructured security text remains a largely manual, expert-driven\nprocess. To address this challenge, we introduce AZERG, a tool designed to\nassist security analysts in automatically generating structured STIX\nrepresentations. To achieve this, we adapt general-purpose large language\nmodels for the specific task of extracting STIX-formatted threat data. To\nmanage the complexity, the task is divided into four subtasks: entity detection\n(T1), entity type identification (T2), related pair detection (T3), and\nrelationship type identification (T4). We apply task-specific fine-tuning to\naccurately extract relevant entities and infer their relationships in\naccordance with the STIX specification. To address the lack of training data,\nwe compiled a comprehensive dataset with 4,011 entities and 2,075 relationships\nextracted from 141 full threat analysis reports, all annotated in alignment\nwith the STIX standard. Our models achieved F1-scores of 84.43% for T1, 88.49%\nfor T2, 95.47% for T3, and 84.60% for T4 in real-world scenarios. We validated\ntheir performance against a range of open- and closed-parameter models, as well\nas state-of-the-art methods, demonstrating improvements of 2-25% across tasks.", "comment": "This paper is accepted at RAID 2025", "pdf_url": "http://arxiv.org/pdf/2507.16576v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16611", "title": "Smooth Games of Configuration in the Linear-Quadratic Setting", "authors": ["Jesse Milzman", "Jeffrey Mao", "Giuseppe Loianno"], "categories": ["cs.MA", "cs.GT"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16611v1", "summary": "Dynamic game theory offers a toolbox for formalizing and solving for both\ncooperative and non-cooperative strategies in multi-agent scenarios. However,\nthe optimal configuration of such games remains largely unexplored. While there\nis existing literature on the parametrization of dynamic games, little research\nexamines this parametrization from a strategic perspective where each agent's\nconfiguration choice is influenced by the decisions of others. In this work, we\nintroduce the concept of a game of configuration, providing a framework for the\nstrategic fine-tuning of differential games. We define a game of configuration\nas a two-stage game within the setting of finite-horizon, affine-quadratic, AQ,\ndifferential games. In the first stage, each player chooses their corresponding\nconfiguration parameter, which will impact their dynamics and costs in the\nsecond stage. We provide the subgame perfect solution concept and a method for\ncomputing first stage cost gradients over the configuration space. This then\nallows us to formulate a gradient-based method for searching for local\nsolutions to the configuration game, as well as provide necessary conditions\nfor equilibrium configurations over their downstream (second stage)\ntrajectories. We conclude by demonstrating the effectiveness of our approach in\nexample AQ systems, both zero-sum and general-sum.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16611v1", "cate": "cs.MA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16585", "title": "LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models", "authors": ["Ahmed Lekssays", "Hamza Mouhcine", "Khang Tran", "Ting Yu", "Issa Khalil"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      This paper is accepted at USENIX 2025", "url": "http://arxiv.org/abs/2507.16585v1", "summary": "Software vulnerabilities present a persistent security challenge, with over\n25,000 new vulnerabilities reported in the Common Vulnerabilities and Exposures\n(CVE) database in 2024 alone. While deep learning based approaches show promise\nfor vulnerability detection, recent studies reveal critical limitations in\nterms of accuracy and robustness: accuracy drops by up to 45% on rigorously\nverified datasets, and performance degrades significantly under simple code\nmodifications. This paper presents LLMxCPG, a novel framework integrating Code\nProperty Graphs (CPG) with Large Language Models (LLM) for robust vulnerability\ndetection. Our CPG-based slice construction technique reduces code size by\n67.84 to 90.93% while preserving vulnerability-relevant context. Our approach's\nability to provide a more concise and accurate representation of code snippets\nenables the analysis of larger code segments, including entire projects. This\nconcise representation is a key factor behind the improved detection\ncapabilities of our method, as it can now identify vulnerabilities that span\nmultiple functions. Empirical evaluation demonstrates LLMxCPG's effectiveness\nacross verified datasets, achieving 15-40% improvements in F1-score over\nstate-of-the-art baselines. Moreover, LLMxCPG maintains high performance across\nfunction-level and multi-function codebases while exhibiting robust detection\nefficacy under various syntactic code modifications.", "comment": "This paper is accepted at USENIX 2025", "pdf_url": "http://arxiv.org/pdf/2507.16585v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15867", "title": "RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems", "authors": ["John Wu", "Adam Cross", "Jimeng Sun"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15867v1", "summary": "Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail\nto capture these conditions in electronic health records (EHR), leaving crucial\ninformation buried in clinical notes. Current approaches struggle with medical\nabbreviations, miss implicit disease mentions, raise privacy concerns with\ncloud processing, and lack clinical reasoning abilities. We present Rare\nDisease Mining Agents (RDMA), a framework that mirrors how medical experts\nidentify rare disease patterns in EHR. RDMA connects scattered clinical\nobservations that together suggest specific rare conditions. By handling\nclinical abbreviations, recognizing implicit disease patterns, and applying\ncontextual reasoning locally on standard hardware, RDMA reduces privacy risks\nwhile improving F1 performance by upwards of 30\\% and decreasing inferences\ncosts 10-fold. This approach helps clinicians avoid the privacy risk of using\ncloud services while accessing key rare disease information from EHR systems,\nsupporting earlier diagnosis for rare disease patients. Available at\nhttps://github.com/jhnwu3/RDMA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15867v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.16773", "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs", "authors": ["Yue Li", "Xiao Li", "Hao Wu", "Yue Zhang", "Fengyuan Xu", "Xiuzhen Cheng", "Sheng Zhong"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16773v1", "summary": "Large Language Models (LLMs) have become integral to automated code analysis,\nenabling tasks such as vulnerability detection and code comprehension. However,\ntheir integration introduces novel attack surfaces. In this paper, we identify\nand investigate a new class of prompt-based attacks, termed Copy-Guided Attacks\n(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.\nBy injecting carefully crafted triggers into external code snippets,\nadversaries can induce the model to replicate malicious content during\ninference. This behavior enables two classes of vulnerabilities: inference\nlength manipulation, where the model generates abnormally short or excessively\nlong reasoning traces; and inference result manipulation, where the model\nproduces misleading or incorrect conclusions. We formalize CGA as an\noptimization problem and propose a gradient-based approach to synthesize\neffective triggers. Empirical evaluation on state-of-the-art reasoning LLMs\nshows that CGA reliably induces infinite loops, premature termination, false\nrefusals, and semantic distortions in code analysis tasks. While highly\neffective in targeted settings, we observe challenges in generalizing CGA\nacross diverse prompts due to computational constraints, posing an open\nquestion for future research. Our findings expose a critical yet underexplored\nvulnerability in LLM-powered development pipelines and call for urgent advances\nin prompt-level defense mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16773v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15901", "title": "Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet"], "categories": ["cs.AI", "cs.CY", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15901v1", "summary": "The implementation of Artificial Intelligence (AI) in household environments,\nespecially in the form of proactive autonomous agents, brings about\npossibilities of comfort and attention as well as it comes with intra or\nextramural ethical challenges. This article analyzes agentic AI and its\napplications, focusing on its move from reactive to proactive autonomy,\nprivacy, fairness and user control. We review responsible innovation\nframeworks, human-centered design principles, and governance practices to\ndistill practical guidance for ethical smart home systems. Vulnerable user\ngroups such as elderly individuals, children, and neurodivergent who face\nhigher risks of surveillance, bias, and privacy risks were studied in detail in\ncontext of Agentic AI. Design imperatives are highlighted such as tailored\nexplainability, granular consent mechanisms, and robust override controls,\nsupported by participatory and inclusive methodologies. It was also explored\nhow data-driven insights, including social media analysis via Natural Language\nProcessing(NLP), can inform specific user needs and ethical concerns. This\nsurvey aims to provide both a conceptual foundation and suggestions for\ndeveloping transparent, inclusive, and trustworthy agentic AI in household\nautomation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15901v1", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15975", "title": "Fast Task Planning with Neuro-Symbolic Relaxation", "authors": ["Qiwei Du", "Bowen Li", "Yi Du", "Shaoshu Su", "Taimeng Fu", "Zitong Zhan", "Zhipeng Zhao", "Chen Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2507.15975v1", "summary": "Real-world task planning requires long-horizon reasoning over large sets of\nentities with complex relationships and attributes, leading to a combinatorial\nexplosion for classical symbolic planners. To prune the search space, recent\nmethods prioritize searching on a simplified task only containing a few\n\"important\" entities predicted by a neural network. However, such a simple\nneuro-symbolic (NeSy) integration risks omitting critical entities and wasting\nresources on unsolvable simplified tasks. To enable Fast and reliable planning,\nwe introduce a NeSy relaxation strategy (Flax), combining neural importance\nprediction with symbolic expansion. Specifically, we first learn a graph neural\nnetwork to predict entity importance to create a simplified task and solve it\nwith a symbolic planner. Then, we solve a rule-relaxed task to obtain a quick\nrough plan, and reintegrate all referenced entities into the simplified task to\nrecover any overlooked but essential elements. Finally, we apply complementary\nrules to refine the updated task, keeping it both reliable and compact.\nExtensive experiments are conducted on both synthetic and real-world maze\nnavigation benchmarks where a robot must traverse through a maze and interact\nwith movable objects. The results show that Flax boosts the average success\nrate by 20.82% and cuts mean wall-clock planning time by 17.65% compared with\nthe state-of-the-art NeSy baseline. We expect that Flax offers a practical path\ntoward fast, scalable, long-horizon task planning in complex environments.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.15975v1", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16788", "title": "AUTOPSY: A Framework for Tackling Privacy Challenges in the Automotive Industry", "authors": ["Sebastian Pape", "Anis Bkakria", "Maurice Heymann", "Badreddine Chah", "Abdeljalil Abbas-Turki", "Sarah Syed-Winkler", "Matthias Hiller", "Reda Yaich"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      19 pages, 4 figures", "url": "http://arxiv.org/abs/2507.16788v1", "summary": "With the General Data Protection Regulation (GDPR) in place, all domains have\nto ensure compliance with privacy legislation. However, compliance does not\nnecessarily result in a privacy-friendly system as for example getting users'\nconsent to process their data does not improve the privacy-friendliness of the\nsystem. Therefore, the goal of the AUTOPSY project was to support the privacy\nengineering process in the automotive domain by providing several building\nblocks which technically improve the privacy-friendliness of modern, i.e.,\nconnected and (partially) automated vehicles. This paper presents the results\nof the AUTOPSY project: a system model to identify relevant entities and\nlocations to apply privacy enhancing technologies (PETs); the privacy manager\naiming at more control of the data flow from the vehicle, a PET selection\napproach based on GDPR principles, and an architectural framework for\nautomotive privacy. Furthermore, we built a demonstrator for location-based\nservices to evaluate the architectural framework.", "comment": "19 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.16788v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16068", "title": "Compositional Coordination for Multi-Robot Teams with Large Language Models", "authors": ["Zhehui Huang", "Guangyao Shi", "Yuwei Wu", "Vijay Kumar", "Gaurav S. Sukhatme"], "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures", "url": "http://arxiv.org/abs/2507.16068v1", "summary": "Multi-robot coordination has traditionally relied on a task-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB directly converts natural language\nmission descriptions into executable Python code for multi-robot systems\nthrough two key components: (1) Mission Decomposition for Task Representation,\nwhich parses the mission into a task graph with dependencies, and (2) Code\nGeneration, which uses the task graph and a structured knowledge base to\ngenerate deployable robot control code. We further introduce a dataset of\nnatural language mission specifications to support development and\nbenchmarking. Experimental results in both simulation and real-world settings\nshow that LAN2CB enables effective and flexible multi-robot coordination from\nnatural language, significantly reducing the need for manual engineering while\nsupporting generalization across mission types. Website:\nhttps://sites.google.com/view/lan2cb.", "comment": "9 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.16068v1", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16000", "title": "A Comprehensive Evaluation of LiDAR Odometry Techniques", "authors": ["Easton Potokar", "Michael Kaess"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025", "url": "http://arxiv.org/abs/2507.16000v1", "summary": "Light Detection and Ranging (LiDAR) sensors have become the sensor of choice\nfor many robotic state estimation tasks. Because of this, in recent years there\nhas been significant work done to fine the most accurate method to perform\nstate estimation using these sensors. In each of these prior works, an\nexplosion of possible technique combinations has occurred, with each work\ncomparing LiDAR Odometry (LO) \"pipelines\" to prior \"pipelines\". Unfortunately,\nlittle work up to this point has performed the significant amount of ablation\nstudies comparing the various building-blocks of a LO pipeline. In this work,\nwe summarize the various techniques that go into defining a LO pipeline and\nempirically evaluate these LO components on an expansive number of datasets\nacross environments, LiDAR types, and vehicle motions. Finally, we make\nempirically-backed recommendations for the design of future LO pipelines to\nprovide the most accurate and reliable performance.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.16000v1", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16045", "title": "Chameleon Channels: Measuring YouTube Accounts Repurposed for Deception and Profit", "authors": ["Alejandro Cuevas", "Manoel Horta Ribeiro", "Nicolas Christin"], "categories": ["cs.CY", "cs.CR"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      21 pages, 12 figures, 2 tables", "url": "http://arxiv.org/abs/2507.16045v1", "summary": "Online content creators spend significant time and effort building their user\nbase through a long, often arduous process, which requires finding the right\n``niche'' to cater to. So, what incentive is there for an established content\ncreator known for cat memes to completely reinvent their page channel and start\npromoting cryptocurrency services or cover electoral news events? And, if they\ndo, do their existing subscribers not notice? We explore this problem of\n\\textit{repurposed channels}, whereby a channel changes its identity and\ncontents. We first characterize a market for ``second-hand'' social media\naccounts, which recorded sales exceeding USD~1M during our 6-month observation\nperiod. By observing YouTube channels (re)sold over these 6~months, we find\nthat a substantial number (37\\%) are used to disseminate potentially harmful\ncontent, often without facing any penalty. Even more surprisingly, these\nchannels seem to gain rather than lose subscribers. To estimate the prevalence\nof channel repurposing ``in the wild,'' we also collect two snapshots of 1.4M\nquasi-randomly sampled YouTube accounts. In a 3-month period, we estimate that\n$\\sim$0.25\\% channels -- collectively holding $\\sim$44M subscribers -- were\nrepurposed. We confirm that these repurposed channels share several\ncharacteristics with sold channels -- mainly, the fact that they had a\nsignificantly high presence of potentially problematic content. Across\nrepurposed channels, we find channels that became disinformation channels, as\nwell as channels that link to web pages with financial scams. We reason that\nabusing the residual trust placed on these channels is advantageous to\nfinancially- and ideologically-motivated adversaries. This phenomenon is not\nexclusive to YouTube and we posit that the market for cultivating organic\naudiences is set to grow, particularly if it remains unchallenged by\nmitigations, technical or otherwise.", "comment": "21 pages, 12 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.16045v1", "cate": "cs.CY", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16077", "title": "AI-driven Orchestration at Scale: Estimating Service Metrics on National-Wide Testbeds", "authors": ["Rodrigo Moreira", "Rafael Pasquini", "Joberto S. B. Martins", "Tereza C. Carvalho", "Flvio de Oliveira Silva"], "categories": ["cs.ET", "cs.AI", "cs.LG", "cs.MA", "cs.NI"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "Comments:      17 pages, 18 figures, 14 tables,", "url": "http://arxiv.org/abs/2507.16077v1", "summary": "Network Slicing (NS) realization requires AI-native orchestration\narchitectures to efficiently and intelligently handle heterogeneous user\nrequirements. To achieve this, network slicing is evolving towards a more\nuser-centric digital transformation, focusing on architectures that incorporate\nnative intelligence to enable self-managed connectivity in an integrated and\nisolated manner. However, these initiatives face the challenge of validating\ntheir results in production environments, particularly those utilizing\nML-enabled orchestration, as they are often tested in local networks or\nlaboratory simulations. This paper proposes a large-scale validation method\nusing a network slicing prediction model to forecast latency using Deep Neural\nNetworks (DNNs) and basic ML algorithms embedded within an NS architecture,\nevaluated in real large-scale production testbeds. It measures and compares the\nperformance of different DNNs and ML algorithms, considering a distributed\ndatabase application deployed as a network slice over two large-scale\nproduction testbeds. The investigation highlights how AI-based prediction\nmodels can enhance network slicing orchestration architectures and presents a\nseamless, production-ready validation method as an alternative to fully\ncontrolled simulations or laboratory setups.", "comment": "17 pages, 18 figures, 14 tables,", "pdf_url": "http://arxiv.org/pdf/2507.16077v1", "cate": "cs.ET", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16034", "title": "Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation", "authors": ["Xuying Huang", "Sicong Pan", "Olga Zatsarynna", "Juergen Gall", "Maren Bennewitz"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to RA-L", "url": "http://arxiv.org/abs/2507.16034v1", "summary": "User privacy in mobile robotics has become a critical concern. Existing\nmethods typically prioritize either the performance of downstream robotic tasks\nor privacy protection, with the latter often constraining the effectiveness of\ntask execution. To jointly address both objectives, we study semantic-based\nrobot navigation in an ultra-low-resolution setting to preserve visual privacy.\nA key challenge in such scenarios is recovering semantic segmentation from\nultra-low-resolution RGB images. In this work, we introduce a novel fully\njoint-learning method that integrates an agglomerative feature extractor and a\nsegmentation-aware discriminator to solve ultra-low-resolution semantic\nsegmentation, thereby enabling privacy-preserving, semantic object-goal\nnavigation. Our method outperforms different baselines on ultra-low-resolution\nsemantic segmentation and our improved segmentation results increase the\nsuccess rate of the semantic object-goal navigation in a real-world\nprivacy-constrained scenario.", "comment": "Submitted to RA-L", "pdf_url": "http://arxiv.org/pdf/2507.16034v1", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16181", "title": "Pulse-Level Simulation of Crosstalk Attacks on Superconducting Quantum Hardware", "authors": ["Syed Emad Uddin Shubha", "Tasnuva Farheen"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      This paper has been accepted to the Security, Privacy, and Resilience Workshop at IEEE Quantum Week (QCE 2025) and will appear in the workshop proceedings", "url": "http://arxiv.org/abs/2507.16181v1", "summary": "Hardware crosstalk in multi-tenant superconducting quantum computers poses a\nsevere security threat, allowing adversaries to induce targeted errors across\ntenant boundaries by injecting carefully engineered pulses. We present a\nsimulation-based study of active crosstalk attacks at the pulse level,\nanalyzing how adversarial control of pulse timing, shape, amplitude, and\ncoupling can disrupt a victim's computation. Our framework models the\ntime-dependent dynamics of a three-qubit system in the rotating frame,\ncapturing both always-on couplings and injected drive pulses. We examine two\nattack strategies: attacker-first (pulse before victim operation) and\nvictim-first (pulse after), and systematically identify the pulse and coupling\nconfigurations that cause the largest logical errors. Protocol-level\nexperiments on quantum coin flip and XOR classification circuits show that some\nprotocols are highly vulnerable to these attacks, while others remain robust.\nBased on these findings, we discuss practical methods for detection and\nmitigation to improve security in quantum cloud platforms.", "comment": "This paper has been accepted to the Security, Privacy, and Resilience\n  Workshop at IEEE Quantum Week (QCE 2025) and will appear in the workshop\n  proceedings", "pdf_url": "http://arxiv.org/pdf/2507.16181v1", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16221", "title": "Unbeatable imitation of a friend", "authors": ["Masahiko Ueda"], "categories": ["physics.soc-ph", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      16 pages, 2 figures", "url": "http://arxiv.org/abs/2507.16221v1", "summary": "Imitation sometimes achieves success in multi-agent situations even though it\nis very simple. In game theory, success of imitation has been characterized by\nunbeatability against other agents. Previous studies specified conditions under\nwhich imitation is unbeatable in repeated games, and clarified that the\nexistence of unbeatable imitation is strongly related to the existence of\npayoff-controlling strategies, called zero-determinant strategies. However, the\nprevious studies mainly focused on ``imitation of opponents''. It was pointed\nout that imitation of other players in the same group and imitation of other\nplayers in the same role in other groups generally result in different\noutcomes. Here, we investigate the existence condition of unbeatable imitation\nin the latter ``imitation of friends'' situations. We find that it is stronger\nthan the existence condition of unbeatable zero-determinant strategies, whereas\nboth are very limited. Our findings suggest a strong relation between them even\nin the `imitation of friends'' situations.", "comment": "16 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.16221v1", "cate": "physics.soc-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16059", "title": "Therapist-Exoskeleton-Patient Interaction: An Immersive Gait Therapy", "authors": ["Emek Bar Kktabak", "Matthew R. Short", "Lorenzo Vianello", "Daniel Ludvig", "Levi Hargrove", "Kevin Lynch", "Jose Pons"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16059v1", "summary": "Following a stroke, individuals often experience mobility and balance\nimpairments due to lower-limb weakness and loss of independent joint control.\nGait recovery is a key goal of rehabilitation, traditionally achieved through\nhigh-intensity therapist-led training. However, manual assistance can be\nphysically demanding and limits the therapist's ability to interact with\nmultiple joints simultaneously. Robotic exoskeletons offer multi-joint support,\nreduce therapist strain, and provide objective feedback, but current control\nstrategies often limit therapist involvement and adaptability.\n  We present a novel gait rehabilitation paradigm based on physical\nHuman-Robot-Human Interaction (pHRHI), where both the therapist and the\npost-stroke individual wear lower-limb exoskeletons virtually connected at the\nhips and knees via spring-damper elements. This enables bidirectional\ninteraction, allowing the therapist to guide movement and receive haptic\nfeedback. In a study with eight chronic stroke patients, pHRHI training\noutperformed conventional therapist-guided treadmill walking, leading to\nincreased joint range of motion, step metrics, muscle activation, and\nmotivation. These results highlight pHRHI's potential to combine robotic\nprecision with therapist intuition for improved rehabilitation outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16059v1", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15865", "title": "From Reasoning to Super-Intelligence: A Search-Theoretic Perspective", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15865v1", "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful tool for enhancing\nthe problem-solving capabilities of large language models (LLMs). However, the\ntheoretical foundations of learning from CoT data remain underdeveloped, and\nexisting approaches -- such as Supervised Fine-Tuning (SFT), Reinforcement\nLearning (RL), Tree-of-Thoughts (ToT), and Monte Carlo Tree Search (MCTS) --\noften fail on complex reasoning tasks. In this work, we identify core obstacles\nthat hinder effective CoT learning, including distribution drift, lack of\nembedded search, and exponential inference costs. We introduce the Diligent\nLearner, a new learning paradigm that explicitly models reasoning as a\ndepth-first search guided by a validator and supports backtracking upon\nfailure. Under two mild and realistic assumptions, we prove that the Diligent\nLearner can efficiently learn from CoT data while existing methods fail to do\nso. This framework offers a path toward building scalable and reliable\nreasoning systems trained on naturally occurring, incomplete data -- paving the\nway for the development of Large Reasoning Models (LRMs) with robust,\ninterpretable problem-solving abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15865v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.16220", "title": "LENS-DF: Deepfake Detection and Temporal Localization for Long-Form Noisy Speech", "authors": ["Xuechen Liu", "Wanying Ge", "Xin Wang", "Junichi Yamagishi"], "categories": ["cs.SD", "cs.CR"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE International Joint Conference on Biometrics (IJCB) 2025, Osaka, Japan", "url": "http://arxiv.org/abs/2507.16220v1", "summary": "This study introduces LENS-DF, a novel and comprehensive recipe for training\nand evaluating audio deepfake detection and temporal localization under\ncomplicated and realistic audio conditions. The generation part of the recipe\noutputs audios from the input dataset with several critical characteristics,\nsuch as longer duration, noisy conditions, and containing multiple speakers, in\na controllable fashion. The corresponding detection and localization protocol\nuses models. We conduct experiments based on self-supervised learning front-end\nand simple back-end. The results indicate that models trained using data\ngenerated with LENS-DF consistently outperform those trained via conventional\nrecipes, demonstrating the effectiveness and usefulness of LENS-DF for robust\naudio deepfake detection and localization. We also conduct ablation studies on\nthe variations introduced, investigating their impact on and relevance to\nrealistic challenges in the field.", "comment": "Accepted by IEEE International Joint Conference on Biometrics (IJCB)\n  2025, Osaka, Japan", "pdf_url": "http://arxiv.org/pdf/2507.16220v1", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16249", "title": "Multi-Agent Reinforcement Learning for Sample-Efficient Deep Neural Network Mapping", "authors": ["Srivatsan Krishnan", "Jason Jabbour", "Dan Zhang", "Natasha Jaques", "Aleksandra Faust", "Shayegan Omidshafiei", "Vijay Janapa Reddi"], "categories": ["cs.LG", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16249v1", "summary": "Mapping deep neural networks (DNNs) to hardware is critical for optimizing\nlatency, energy consumption, and resource utilization, making it a cornerstone\nof high-performance accelerator design. Due to the vast and complex mapping\nspace, reinforcement learning (RL) has emerged as a promising approach-but its\neffectiveness is often limited by sample inefficiency. We present a\ndecentralized multi-agent reinforcement learning (MARL) framework designed to\novercome this challenge. By distributing the search across multiple agents, our\nframework accelerates exploration. To avoid inefficiencies from training\nmultiple agents in parallel, we introduce an agent clustering algorithm that\nassigns similar mapping parameters to the same agents based on correlation\nanalysis. This enables a decentralized, parallelized learning process that\nsignificantly improves sample efficiency. Experimental results show our MARL\napproach improves sample efficiency by 30-300x over standard single-agent RL,\nachieving up to 32.61x latency reduction and 16.45x energy-delay product (EDP)\nreduction under iso-sample conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16249v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16120", "title": "FTIN: Frequency-Time Integration Network for Inertial Odometry", "authors": ["Shanshan Zhang", "Qi Zhang", "Siyue Wang", "Tianshui Wen", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16120v1", "summary": "In recent years, machine learning has achieved significant advancements in\ninertial odometry. However, most existing inertial odometry methods primarily\nrely on CNNs in the time domain. These methods often struggle to capture\nlong-term dependency in inertial measurement unit data, thereby constraining\nthe potential for further improvements in localization accuracy. To address\nthese issues, we propose a novel network architecture that integrates both\nfrequency-domain and time-domain information. Specifically, we leverage the\nglobal view and energy compaction properties of frequency-domain learning to\neffectively model long-term dependency and reduce redundancy in IMU data.\nAdditionally, we introduce a Scalar LSTM to capture sequential dependencies in\nthe time domain, enabling cross-domain information fusion and providing a\nstable and reliable reference for localization. Experimental evaluations on\nmultiple public datasets (e.g., RIDI, RoNIN, OxIOD, RNIN, TLIO, and IMUNet)\ndemonstrate the effectiveness of the proposed frequency-time domain fusion\nstrategy. Notably, on the RoNIN dataset, our method achieves a 43.0% reduction\nin absolute trajectory error and a 13.1% reduction in relative trajectory error\ncompared to RoNIN ResNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16120v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15866", "title": "Purchase and Production Optimization in a Meat Processing Plant", "authors": ["Marek Vlk", "Premysl Sucha", "Jaroslaw Rudy", "Radoslaw Idzikowski"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      25 pages, 5 figures", "url": "http://arxiv.org/abs/2507.15866v1", "summary": "The food production industry, especially the meat production sector, faces\nmany challenges that have even escalated due to the recent outbreak of the\nenergy crisis in the European Union. Therefore, efficient use of input\nmaterials is an essential aspect affecting the profit of such companies. This\npaper addresses an optimization problem concerning the purchase and subsequent\nmaterial processing we solved for a meat processing company. Unlike the\nmajority of existing papers, we do not concentrate on how this problem concerns\nsupply chain management, but we focus purely on the production stage. The\nproblem involves the concept of alternative ways of material processing, stock\nof material with different expiration dates, and extra constraints widely\nneglected in the current literature, namely, the minimum order quantity and the\nminimum percentage in alternatives. We prove that each of these two constraints\nmakes the problem \\mbox{$\\mathcal{NP}$-hard}, and hence we design a simple\niterative approach based on integer linear programming that allows us to solve\nreal-life instances even using an open-source integer linear programming\nsolver. Another advantage of this approach is that it mitigates numerical\nissues, caused by the extensive range of data values, we experienced with a\ncommercial solver. The results obtained using real data from the meat\nprocessing company showed that our algorithm can find the optimum solution in a\nfew seconds for all considered use cases.", "comment": "25 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.15866v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.16226", "title": "Distilled Large Language Model in Confidential Computing Environment for System-on-Chip Design", "authors": ["Dong Ben", "Hui Feng", "Qian Wang"], "categories": ["cs.AI", "cs.CR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures;", "url": "http://arxiv.org/abs/2507.16226v1", "summary": "Large Language Models (LLMs) are increasingly used in circuit design tasks\nand have typically undergone multiple rounds of training. Both the trained\nmodels and their associated training data are considered confidential\nintellectual property (IP) and must be protected from exposure. Confidential\nComputing offers a promising solution to protect data and models through\nTrusted Execution Environments (TEEs). However, existing TEE implementations\nare not designed to support the resource-intensive nature of LLMs efficiently.\nIn this work, we first present a comprehensive evaluation of the LLMs within a\nTEE-enabled confidential computing environment, specifically utilizing Intel\nTrust Domain Extensions (TDX). We constructed experiments on three\nenvironments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and\nevaluated their performance in terms of tokens per second.\n  Our first observation is that distilled models, i.e., DeepSeek, surpass other\nmodels in performance due to their smaller parameters, making them suitable for\nresource-constrained devices. Also, in the quantized models such as 4-bit\nquantization (Q4) and 8-bit quantization (Q8), we observed a performance gain\nof up to 3x compared to FP16 models. Our findings indicate that for fewer\nparameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms\nthe CPU version in executing computations within a secure environment. We\nfurther validate the results using a testbench designed for SoC design tasks.\nThese validations demonstrate the potential of efficiently deploying\nlightweight LLMs on resource-constrained systems for semiconductor CAD\napplications.", "comment": "7 pages, 4 figures;", "pdf_url": "http://arxiv.org/pdf/2507.16226v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16601", "title": "Low complexity convergence rate bounds for push-sum algorithms with homogeneous correlation structure", "authors": ["Balzs Gerencsr", "Mikls Kornyik"], "categories": ["math.PR", "cs.MA", "37M25 (Primary) 93D50, 93D05 (Secondary}", "G.3"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures", "url": "http://arxiv.org/abs/2507.16601v1", "summary": "The objective of this work is to establish an upper bound for the almost sure\nconvergence rate for a class of push-sum algorithms. The current work extends\nthe methods and results of the authors on a similar low-complexity bound on\npush-sum algorithms with some particular synchronous message passing schemes\nand complements the general approach of Gerencs\\'er and Gerencs\\'er from 2022\nproviding an exact, but often less accessible description. Furthermore, a\nparametric analysis is presented on the ``weight'' of the messages, which is\nfound to be convex with an explicit expression for the gradient. This allows\nthe fine-tuning of the algorithm used for improved efficiency. Numerical\nresults confirm the speedup in evaluating the computable bounds without\ndeteriorating their performance, for a graph on 120 vertices the runtime drops\nby more than 4 orders of magnitude.", "comment": "15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.16601v1", "cate": "math.PR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16121", "title": "DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion Modeling", "authors": ["Shanshan Zhang", "Qi Zhang", "Siyue Wang", "Tianshui Wen", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16121v1", "summary": "Inertial odometry (IO) directly estimates the position of a carrier from\ninertial sensor measurements and serves as a core technology for the widespread\ndeployment of consumer grade localization systems. While existing IO methods\ncan accurately reconstruct simple and near linear motion trajectories, they\noften fail to account for drift errors caused by complex motion patterns such\nas turning. This limitation significantly degrades localization accuracy and\nrestricts the applicability of IO systems in real world scenarios. To address\nthese challenges, we propose a lightweight IO framework. Specifically, inertial\ndata is projected into a high dimensional implicit nonlinear feature space\nusing the Star Operation method, enabling the extraction of complex motion\nfeatures that are typically overlooked. We further introduce a collaborative\nattention mechanism that jointly models global motion dynamics across both\nchannel and temporal dimensions. In addition, we design Multi Scale Gated\nConvolution Units to capture fine grained dynamic variations throughout the\nmotion process, thereby enhancing the model's ability to learn rich and\nexpressive motion representations. Extensive experiments demonstrate that our\nproposed method consistently outperforms SOTA baselines across six widely used\ninertial datasets. Compared to baseline models on the RoNIN dataset, it\nachieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a\nnew benchmark in the field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16121v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15874", "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM", "authors": ["Yin Wu", "Daniel Slieter", "Vivek Subramanian", "Ahmed Abouelazm", "Robin Bohn", "J. Marius Zllner"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15874v1", "summary": "The growing number of ADAS-equipped vehicles has led to a dramatic increase\nin driving data, yet most of them capture routine driving behavior. Identifying\nand understanding safety-critical corner cases within this vast dataset remains\na significant challenge. Braking events are particularly indicative of\npotentially hazardous situations, motivating the central question of our\nresearch: Why does a vehicle brake? Existing approaches primarily rely on\nrule-based heuristics to retrieve target scenarios using predefined condition\nfilters. While effective in simple environments such as highways, these methods\nlack generalization in complex urban settings. In this paper, we propose a\nnovel framework that leverages Large Language Model (LLM) for scenario\nunderstanding and reasoning. Our method bridges the gap between low-level\nnumerical signals and natural language descriptions, enabling LLM to interpret\nand classify driving scenarios. We propose a dual-path scenario retrieval that\nsupports both category-based search for known scenarios and embedding-based\nretrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate\nevaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.\nExperimental results show that our method outperforms rule-based baselines and\ngeneralizes well to OOD scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15874v1", "cate": "cs.AI", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2507.16302", "title": "Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning", "authors": ["Boheng Li", "Renjie Gu", "Junjie Wang", "Leyi Qi", "Yiming Li", "Run Wang", "Zhan Qin", "Tianwei Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint version. Under review", "url": "http://arxiv.org/abs/2507.16302v1", "summary": "Text-to-image (T2I) diffusion models have achieved impressive image\ngeneration quality and are increasingly fine-tuned for personalized\napplications. However, these models often inherit unsafe behaviors from toxic\npretraining data, raising growing safety concerns. While recent safety-driven\nunlearning methods have made promising progress in suppressing model toxicity,\nthey are identified to be fragile to downstream fine-tuning, where we reveal\nthat state-of-the-art methods largely fail to retain their effectiveness even\nwhen fine-tuned on entirely benign datasets. To mitigate this problem, in this\npaper, we propose ResAlign, a safety-driven unlearning framework with enhanced\nresilience against downstream fine-tuning. By modeling downstream fine-tuning\nas an implicit optimization problem with a Moreau Envelope-based reformulation,\nResAlign enables efficient gradient estimation to minimize the recovery of\nharmful behaviors. Additionally, a meta-learning strategy is proposed to\nsimulate a diverse distribution of fine-tuning scenarios to improve\ngeneralization. Extensive experiments across a wide range of datasets,\nfine-tuning methods, and configurations demonstrate that ResAlign consistently\noutperforms prior unlearning approaches in retaining safety after downstream\nfine-tuning while preserving benign generation capability well.", "comment": "Preprint version. Under review", "pdf_url": "http://arxiv.org/pdf/2507.16302v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2409.12330", "title": "Heterogeneous Mixed Traffic Control and Coordination", "authors": ["Iftekharul Islam", "Weizi Li", "Xuan Wang", "Shuai Li", "Kevin Heaslip"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025", "url": "http://arxiv.org/abs/2409.12330v2", "summary": "Urban intersections with diverse vehicle types, from small cars to large\nsemi-trailers, pose significant challenges for traffic control. This study\nexplores how robot vehicles (RVs) can enhance heterogeneous traffic flow,\nparticularly at unsignalized intersections where traditional methods fail\nduring power outages. Using reinforcement learning (RL) and real-world data, we\nsimulate mixed traffic at complex intersections with RV penetration rates\nranging from 10% to 90%. Results show that average waiting times drop by up to\n86% and 91% compared to signalized and unsignalized intersections,\nrespectively. We observe a \"rarity advantage,\" where less frequent vehicles\nbenefit the most (up to 87%). Although CO2 emissions and fuel consumption\nincrease with RV penetration, they remain well below those of traditional\nsignalized traffic. Decreased space headways also indicate more efficient road\nusage. These findings highlight RVs' potential to improve traffic efficiency\nand reduce environmental impact in complex, heterogeneous settings.", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2025", "pdf_url": "http://arxiv.org/pdf/2409.12330v2", "cate": "cs.MA", "date": "2024-09-18", "updated": "2025-07-22"}
{"id": "2507.16124", "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making", "authors": ["Dakota Sullivan", "Shirley Zhang", "Jennica Li", "Heather Kirkorian", "Bilge Mutlu", "Kassem Fawaz"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed equally to this work", "url": "http://arxiv.org/abs/2507.16124v1", "summary": "Social robots are embodied agents that interact with people while following\nhuman communication norms. These robots interact using verbal and non-verbal\ncues, and share the physical environments of people. While social robots have\npreviously utilized rule-based systems or probabilistic models for user\ninteraction, the rapid evolution of large language models (LLMs) presents new\nopportunities to develop LLM-empowered social robots for enhanced human-robot\ninteraction. To fully realize these capabilities, however, robots need to\ncollect data such as audio, fine-grained images, video, and locations. As a\nresult, LLMs often process sensitive personal information, particularly within\nhome environments. Given the tension between utility and privacy risks,\nevaluating how current LLMs manage sensitive data is critical. Specifically, we\naim to explore the extent to which out-of-the-box LLMs are privacy-aware in the\ncontext of household social robots. In this study, we present a set of\nprivacy-relevant scenarios crafted through the lens of Contextual Integrity\n(CI). We first survey users' privacy preferences regarding in-home social robot\nbehaviors and then examine how their privacy orientation affects their choices\nof these behaviors (N = 450). We then provide the same set of scenarios and\nquestions to state-of-the-art LLMs (N = 10) and find that the agreement between\nhumans and LLMs is low. To further investigate the capabilities of LLMs as a\npotential privacy controller, we implement four additional prompting strategies\nand compare their results. Finally, we discuss the implications and potential\nof AI privacy awareness in human-robot interaction.", "comment": "18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed\n  equally to this work", "pdf_url": "http://arxiv.org/pdf/2507.16124v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15875", "title": "Differential Multimodal Transformers", "authors": ["Jerry Li", "Timothy Oh", "Joseph Hoang", "Vardhit Veeramachaneni"], "categories": ["cs.AI", "cs.MM"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15875v1", "summary": "Small language models have gained significant popularity due to their\nefficiency and growing capabilities. However, incorporating additional\nmodalities, such as vision, can exacerbate the challenge of limited context\nwindows by introducing noise. Recent studies have highlighted that Transformer\nattention mechanisms often disproportionately focus on irrelevant contexts. In\nthis work, we extend the Differential Attention mechanism, originally designed\nfor text-only models, to the text-vision model PaliGemma. Our aim is to\nevaluate its ability to mitigate noisy information retrieval and reduce\nhallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,\nincorporating Differential Attention, and experimented with various parameter\nsettings and configurations. We demonstrate that Differential Attention can be\nadapted and integrated into the fine-tuning of existing models to enhance noisy\ninformation retrieval and question-answering capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15875v1", "cate": "cs.AI", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2507.15862", "title": "Quantifying Holistic Review: A Multi-Modal Approach to College Admissions Prediction", "authors": ["Jun-Wei Zeng", "Jerry Shen"], "categories": ["cs.LG", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15862v1", "summary": "This paper introduces the Comprehensive Applicant Profile Score (CAPS), a\nnovel multi-modal framework designed to quantitatively model and interpret\nholistic college admissions evaluations. CAPS decomposes applicant profiles\ninto three interpretable components: academic performance (Standardized\nAcademic Score, SAS), essay quality (Essay Quality Index, EQI), and\nextracurricular engagement (Extracurricular Impact Score, EIS). Leveraging\ntransformer-based semantic embeddings, LLM scoring, and XGBoost regression,\nCAPS provides transparent and explainable evaluations aligned with human\njudgment. Experiments on a synthetic but realistic dataset demonstrate strong\nperformance, achieving an EQI prediction R^2 of 0.80, classification accuracy\nover 75%, a macro F1 score of 0.69, and a weighted F1 score of 0.74. CAPS\naddresses key limitations in traditional holistic review -- particularly the\nopacity, inconsistency, and anxiety faced by applicants -- thus paving the way\nfor more equitable and data-informed admissions practices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15862v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2411.00459", "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques", "authors": ["Yulin Chen", "Haoran Li", "Zihao Zheng", "Yangqiu Song", "Dekai Wu", "Bryan Hooi"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      To Appear in ACL 2025", "url": "http://arxiv.org/abs/2411.00459v5", "summary": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results.", "comment": "To Appear in ACL 2025", "pdf_url": "http://arxiv.org/pdf/2411.00459v5", "cate": "cs.CR", "date": "2024-11-01", "updated": "2025-07-22"}
{"id": "2410.00081", "title": "From homeostasis to resource sharing: Biologically and economically aligned multi-objective multi-agent AI safety benchmarks", "authors": ["Roland Pihlakas", "Joel Pyykk"], "categories": ["cs.MA", "cs.AI"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      20 pages, 13 figures, 1 tables", "url": "http://arxiv.org/abs/2410.00081v4", "summary": "Developing safe, aligned agentic AI systems requires comprehensive empirical\ntesting, yet many existing benchmarks neglect crucial themes aligned with\nbiology and economics, both time-tested fundamental sciences describing our\nneeds and preferences. To address this gap, the present work focuses on\nintroducing biologically and economically motivated themes that have been\nneglected in current mainstream discussions on AI safety - namely a set of\nmulti-objective, multi-agent alignment benchmarks that emphasize homeostasis\nfor bounded and biological objectives, diminishing returns for unbounded,\ninstrumental, and business objectives, sustainability principle, and resource\nsharing. We implemented eight main benchmark environments on the above themes,\nto illustrate key pitfalls and challenges in agentic AI-s, such as unboundedly\nmaximizing a homeostatic objective, over-optimizing one objective at the\nexpense of others, neglecting safety constraints, or depleting shared\nresources.", "comment": "20 pages, 13 figures, 1 tables", "pdf_url": "http://arxiv.org/pdf/2410.00081v4", "cate": "cs.MA", "date": "2024-09-30", "updated": "2025-07-22"}
{"id": "2507.16139", "title": "Equivariant Goal Conditioned Contrastive Reinforcement Learning", "authors": ["Arsh Tangri", "Nichols Crawford Taylor", "Haojie Huang", "Robert Platt"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16139v1", "summary": "Contrastive Reinforcement Learning (CRL) provides a promising framework for\nextracting useful structured representations from unlabeled interactions. By\npulling together state-action pairs and their corresponding future states,\nwhile pushing apart negative pairs, CRL enables learning nontrivial policies\nwithout manually designed rewards. In this work, we propose Equivariant CRL\n(ECRL), which further structures the latent space using equivariant\nconstraints. By leveraging inherent symmetries in goal-conditioned manipulation\ntasks, our method improves both sample efficiency and spatial generalization.\nSpecifically, we formally define Goal-Conditioned Group-Invariant MDPs to\ncharacterize rotation-symmetric robotic manipulation tasks, and build on this\nby introducing a novel rotation-invariant critic representation paired with a\nrotation-equivariant actor for Contrastive RL. Our approach consistently\noutperforms strong baselines across a range of simulated tasks in both\nstate-based and image-based settings. Finally, we extend our method to the\noffline RL setting, demonstrating its effectiveness across multiple tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16139v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15876", "title": "Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A Bayesian Graphical Approach", "authors": ["Eric Benhamou", "Jean-Jacques Ohana", "Alban Etienne", "Batrice Guez", "Ethan Setrouk", "Thomas Jacquot"], "categories": ["cs.AI", "q-fin.PR", "q-fin.ST", "q-fin.TR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.15876v1", "summary": "Commodity Trading Advisors (CTAs) have historically relied on trend-following\nrules that operate on vastly different horizons from long-term breakouts that\ncapture major directional moves to short-term momentum signals that thrive in\nfast-moving markets. Despite a large body of work on trend following, the\nrelative merits and interactions of short-versus long-term trend systems remain\ncontroversial. This paper adds to the debate by (i) dynamically decomposing CTA\nreturns into short-term trend, long-term trend and market beta factors using a\nBayesian graphical model, and (ii) showing how the blend of horizons shapes the\nstrategy's risk-adjusted performance.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.15876v1", "cate": "cs.AI", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2507.15869", "title": "An open dataset of neural networks for hypernetwork research", "authors": ["David Kurtenbach", "Lior Shamir"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Electronics, published", "url": "http://arxiv.org/abs/2507.15869v1", "summary": "Despite the transformative potential of AI, the concept of neural networks\nthat can produce other neural networks by generating model weights\n(hypernetworks) has been largely understudied. One of the possible reasons is\nthe lack of available research resources that can be used for the purpose of\nhypernetwork research. Here we describe a dataset of neural networks, designed\nfor the purpose of hypernetworks research. The dataset includes $10^4$ LeNet-5\nneural networks trained for binary image classification separated into 10\nclasses, such that each class contains 1,000 different neural networks that can\nidentify a certain ImageNette V2 class from all other classes. A computing\ncluster of over $10^4$ cores was used to generate the dataset. Basic\nclassification results show that the neural networks can be classified with\naccuracy of 72.0%, indicating that the differences between the neural networks\ncan be identified by supervised machine learning algorithms. The ultimate\npurpose of the dataset is to enable hypernetworks research. The dataset and the\ncode that generates it are open and accessible to the public.", "comment": "Electronics, published", "pdf_url": "http://arxiv.org/pdf/2507.15869v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2501.16680", "title": "Differentially Private Set Representations", "authors": ["Sarvar Patel", "Giuseppe Persiano", "Joon Young Seo", "Kevin Yeo"], "categories": ["cs.CR", "cs.DS"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Appears at NeurIPS 2024", "url": "http://arxiv.org/abs/2501.16680v2", "summary": "We study the problem of differentially private (DP) mechanisms for\nrepresenting sets of size $k$ from a large universe. Our first construction\ncreates $(\\epsilon,\\delta)$-DP representations with error probability of\n$1/(e^\\epsilon + 1)$ using space at most $1.05 k \\epsilon \\cdot \\log(e)$ bits\nwhere the time to construct a representation is $O(k \\log(1/\\delta))$ while\ndecoding time is $O(\\log(1/\\delta))$. We also present a second algorithm for\npure $\\epsilon$-DP representations with the same error using space at most $k\n\\epsilon \\cdot \\log(e)$ bits, but requiring large decoding times. Our\nalgorithms match our lower bounds on privacy-utility trade-offs (including\nconstants but ignoring $\\delta$ factors) and we also present a new space lower\nbound matching our constructions up to small constant factors. To obtain our\nresults, we design a new approach embedding sets into random linear systems\ndeviating from most prior approaches that inject noise into non-private\nsolutions.", "comment": "Appears at NeurIPS 2024", "pdf_url": "http://arxiv.org/pdf/2501.16680v2", "cate": "cs.CR", "date": "2025-01-28", "updated": "2025-07-22"}
{"id": "2505.08195", "title": "Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations", "authors": ["Jinming Hu", "Hassan Nawaz", "Yuting Rui", "Lijie Chi", "Arif Ullah", "Pavlo O. Dral"], "categories": ["physics.comp-ph", "cs.AI", "cs.LG", "cs.MA", "physics.chem-ph"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08195v3", "summary": "We have developed Aitomia - a platform powered by AI to assist in performing\nAI-driven atomistic and quantum chemical (QC) simulations. This evolving\nintelligent assistant platform is equipped with chatbots and AI agents to help\nexperts and guide non-experts in setting up and running atomistic simulations,\nmonitoring their computational status, analyzing simulation results, and\nsummarizing them for the user in both textual and graphical forms. We achieve\nthese goals by exploiting large language models that leverage the versatility\nof our MLatom ecosystem, supporting AI-enhanced computational chemistry tasks\nranging from ground-state to excited-state calculations, including geometry\noptimizations, thermochemistry, and spectral calculations. The multi-agent\nimplementation enables autonomous executions of the complex computational\nworkflows, such as the computation of the reaction enthalpies. Aitomia is the\nfirst intelligent assistant publicly accessible online on a cloud computing\nplatform for atomistic simulations of broad scope (Aitomistic Hub at\nhttps://aitomistic.xyz). It may also be deployed locally as described at\nhttp://mlatom.com/aitomia. Aitomia is expected to lower the barrier to\nperforming atomistic simulations, thereby democratizing simulations and\naccelerating research and development in relevant fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08195v3", "cate": "physics.comp-ph", "date": "2025-05-13", "updated": "2025-07-22"}
{"id": "2507.16175", "title": "Scanning Bot: Efficient Scan Planning using Panoramic Cameras", "authors": ["Euijeong Lee", "Kyung Min Han", "Young J. Kim"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16175v1", "summary": "Panoramic RGB-D cameras are known for their ability to produce high quality\n3D scene reconstructions. However, operating these cameras involves manually\nselecting viewpoints and physically transporting the camera, making the\ngeneration of a 3D model time consuming and tedious. Additionally, the process\ncan be challenging for novice users due to spatial constraints, such as\nensuring sufficient feature overlap between viewpoint frames. To address these\nchallenges, we propose a fully autonomous scan planning that generates an\nefficient tour plan for environment scanning, ensuring collision-free\nnavigation and adequate overlap between viewpoints within the plan. Extensive\nexperiments conducted in both synthetic and real-world environments validate\nthe performance of our planner against state-of-the-art view planners. In\nparticular, our method achieved an average scan coverage of 99 percent in the\nreal-world experiment, with our approach being up to 3 times faster than\nstate-of-the-art planners in total scan time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16175v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15877", "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning", "authors": ["Simon Ouellette"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15877v1", "summary": "We run a controlled compositional generalization experiment in the ARC-AGI\ndomain: an open-world problem domain in which the ability to generalize\nout-of-distribution is, by design, an essential characteristic for success. We\ncompare neural program synthesis and test-time fine-tuning approaches on this\nexperiment. We find that execution-guided neural program synthesis outperforms\nall reference algorithms in its ability to compose novel solutions. Our\nempirical findings also suggest that the success of TTFT on ARC-AGI lies mainly\nin eliciting in-distribution knowledge that the LLM otherwise fails to rely on\ndirectly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15877v1", "cate": "cs.AI", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2507.15884", "title": "Prompt Smart, Pay Less: Cost-Aware APO for Real-World Applications", "authors": ["Jayesh Choudhari", "Piyush Kumar Singh", "Douglas McIlwraith", "Snehal Nair"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15884v1", "summary": "Prompt design is a critical factor in the effectiveness of Large Language\nModels (LLMs), yet remains largely heuristic, manual, and difficult to scale.\nThis paper presents the first comprehensive evaluation of Automatic Prompt\nOptimization (APO) methods for real-world, high-stakes multiclass\nclassification in a commercial setting, addressing a critical gap in the\nexisting literature where most of the APO frameworks have been validated only\non benchmark classification tasks of limited complexity.\n  We introduce APE-OPRO, a novel hybrid framework that combines the\ncomplementary strengths of APE and OPRO, achieving notably better\ncost-efficiency, around $18\\%$ improvement over OPRO, without sacrificing\nperformance. We benchmark APE-OPRO alongside both gradient-free (APE, OPRO) and\ngradient-based (ProTeGi) methods on a dataset of ~2,500 labeled products.\n  Our results highlight key trade-offs: ProTeGi offers the strongest absolute\nperformance at lower API cost but higher computational time as noted\nin~\\cite{protegi}, while APE-OPRO strikes a compelling balance between\nperformance, API efficiency, and scalability. We further conduct ablation\nstudies on depth and breadth hyperparameters, and reveal notable sensitivity to\nlabel formatting, indicating implicit sensitivity in LLM behavior. These\nfindings provide actionable insights for implementing APO in commercial\napplications and establish a foundation for future research in multi-label,\nvision, and multimodal prompt optimization scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15884v1", "cate": "cs.LG", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2502.10556", "title": "Recent Advances in Malware Detection: Graph Learning and Explainability", "authors": ["Hossein Shokouhinejad", "Roozbeh Razavi-Far", "Hesamodin Mohammadian", "Mahdi Rabbani", "Samuel Ansong", "Griffin Higgins", "Ali A Ghorbani"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.10556v2", "summary": "The rapid evolution of malware has necessitated the development of\nsophisticated detection methods that go beyond traditional signature-based\napproaches. Graph learning techniques have emerged as powerful tools for\nmodeling and analyzing the complex relationships inherent in malware behavior,\nleveraging advancements in Graph Neural Networks (GNNs) and related methods.\nThis survey provides a comprehensive exploration of recent advances in malware\ndetection, focusing on the interplay between graph learning and explainability.\nIt begins by reviewing malware analysis techniques and datasets, emphasizing\ntheir foundational role in understanding malware behavior and supporting\ndetection strategies. The survey then discusses feature engineering, graph\nreduction, and graph embedding methods, highlighting their significance in\ntransforming raw data into actionable insights, while ensuring scalability and\nefficiency. Furthermore, this survey focuses on explainability techniques and\ntheir applications in malware detection, ensuring transparency and\ntrustworthiness. By integrating these components, this survey demonstrates how\ngraph learning and explainability contribute to building robust, interpretable,\nand scalable malware detection systems. Future research directions are outlined\nto address existing challenges and unlock new opportunities in this critical\narea of cybersecurity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.10556v2", "cate": "cs.CR", "date": "2025-02-14", "updated": "2025-07-22"}
{"id": "2506.02951", "title": "Adaptive Graph Pruning for Multi-Agent Communication", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "categories": ["cs.CL", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ECAI 2025", "url": "http://arxiv.org/abs/2506.02951v2", "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "comment": "ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.02951v2", "cate": "cs.CL", "date": "2025-06-03", "updated": "2025-07-22"}
{"id": "2507.16214", "title": "Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for Safe Approaching Maneuvers", "authors": ["Batu Candan", "Simone Servadio"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16214v1", "summary": "Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16214v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15880", "title": "The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture", "authors": ["Andy E. Williams"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15880v1", "summary": "Intelligence-biological, artificial, or collective-requires structural\ncoherence across recursive reasoning processes to scale effectively. As complex\nsystems grow, coherence becomes fragile unless a higher-order structure ensures\nsemantic consistency. This paper introduces the Recursive Coherence Principle\n(RCP): a foundational constraint stating that for any reasoning system of order\nN, composed of systems operating over conceptual spaces of order N-1, semantic\ncoherence is preserved only by a recursively evaluable generalization operator\nthat spans and aligns those lower-order conceptual spaces. Crucially, this\ncoherence enables structural alignment. Without recursive coherence, no system\ncan reliably preserve goals, meanings, or reasoning consistency at scale. We\nformally define the Functional Model of Intelligence (FMI) as the only known\noperator capable of satisfying the RCP at any scale. The FMI is a minimal,\ncomposable architecture with internal functions (evaluation, modeling,\nadaptation, stability, decomposition, bridging) and external functions\n(storage, recall, System 1 and System 2 reasoning) vital for preserving\nsemantic structure across inference and coordination layers. We prove that any\nsystem lacking the FMI will experience recursive coherence breakdown as it\nscales, arguing that common AI issues like misalignment, hallucination, and\ninstability are symptoms of this structural coherence loss. Unlike other\nfoundational principles, RCP uniquely captures the internal, recursive dynamics\nneeded for coherent, alignable intelligence, modeling semantic coherence under\nrecursion. This work significantly impacts AI alignment, advocating a shift\nfrom behavioral constraints to structural coherence, and offers a pathway for\nsafely generalizable, robustly coherent AI at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15880v1", "cate": "cs.AI", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.15897", "title": "ReDi: Rectified Discrete Flow", "authors": ["Jaehoon Yoo", "Wonjung Kim", "Seunghoon Hong"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15897v1", "summary": "Discrete Flow-based Models (DFMs) are powerful generative models for\nhigh-quality discrete data but typically suffer from slow sampling speeds due\nto their reliance on iterative decoding processes. This reliance on a\nmulti-step process originates from the factorization approximation of DFMs,\nwhich is necessary for handling high-dimensional data. In this paper, we\nrigorously characterize the approximation error from factorization using\nConditional Total Correlation (TC), which depends on the coupling. To reduce\nthe Conditional TC and enable efficient few-step generation, we propose\nRectified Discrete Flow (ReDi), a novel iterative method that reduces\nfactorization error by rectifying the coupling between source and target\ndistributions. We theoretically prove that each ReDi step guarantees a\nmonotonic decreasing Conditional TC, ensuring its convergence. Empirically,\nReDi significantly reduces Conditional TC and enables few-step generation.\nMoreover, we demonstrate that the rectified couplings are well-suited for\ntraining efficient one-step models on image generation. ReDi offers a simple\nand theoretically grounded approach for tackling the few-step challenge,\nproviding a new perspective on efficient discrete data synthesis. Code is\navailable at https://github.com/Ugness/ReDi_discrete", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15897v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15981", "title": "Implications of Current Litigation on the Design of AI Systems for Healthcare Delivery", "authors": ["Gennie Mansi", "Mark Riedl"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      15 pages, 8 Figures", "url": "http://arxiv.org/abs/2507.15981v1", "summary": "Many calls for explainable AI (XAI) systems in medicine are tied to a desire\nfor AI accountability--accounting for, mitigating, and ultimately preventing\nharms from AI systems. Because XAI systems provide human-understandable\nexplanations for their output, they are often viewed as a primary path to\nprevent harms to patients. However, when harm occurs, laws, policies, and\nregulations also shape AI accountability by impacting how harmed individuals\ncan obtain recourse. Current approaches to XAI explore physicians' medical and\nrelational needs to counter harms to patients, but there is a need to\nunderstand how XAI systems should account for the legal considerations of those\nimpacted. We conduct an analysis of 31 legal cases and reported harms to\nidentify patterns around how AI systems impact patient care. Our findings\nreflect how patients' medical care relies on a complex web of\nstakeholders--physicians, state health departments, health insurers, care\nfacilities, among others--and many AI systems deployed across their healthcare\ndelivery negatively impact their care. In response, patients have had no option\nbut to seek legal recourse for harms. We shift the frame from\nphysician-centered to patient-centered accountability approaches by describing\nhow lawyers and technologists need to recognize and address where AI harms\nhappen. We present paths for preventing or countering harm (1) by changing\nliability structures to reflect the role of many stakeholders in shaping how AI\nsystems impact patient care; and (2) by designing XAI systems that can help\nadvocates, such as legal representatives, who provide critical legal expertise\nand practically support recourse for patients.", "comment": "15 pages, 8 Figures", "pdf_url": "http://arxiv.org/pdf/2507.15981v1", "cate": "cs.HC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2502.12441", "title": "Choosing Coordinate Forms for Solving ECDLP Using Shor's Algorithm", "authors": ["Yan Huang", "Fangguo Zhang", "Fei Gao", "Zijian Zhou", "Longjiang Qu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      The primary concerns lie in the limited significance and novelty. While the paper explores the use of projective coordinates, quantum resource requirements are worse than those achieved with previously studied affine coordinates, as carefully documented in the manuscript", "url": "http://arxiv.org/abs/2502.12441v2", "summary": "Shor's algorithm is well-known for its capability to address the elliptic\ncurve discrete logarithm problem (ECDLP) in polynomial time. The enhancement of\nits quantum resources continues to be a crucial focus of research.\nNevertheless, the application of projective coordinates for quantum resource\noptimization remains an unresolved issue, mainly because the representation of\nprojective coordinates lacks uniqueness without employing modular division\noperations. Our study reveals that projective coordinates do not provide the\nsame advantages as affine coordinates when utilizing Shor's method to tackle\nthe ECDLP.", "comment": "The primary concerns lie in the limited significance and novelty.\n  While the paper explores the use of projective coordinates, quantum resource\n  requirements are worse than those achieved with previously studied affine\n  coordinates, as carefully documented in the manuscript", "pdf_url": "http://arxiv.org/pdf/2502.12441v2", "cate": "cs.CR", "date": "2025-02-18", "updated": "2025-07-22"}
{"id": "2507.16233", "title": "GFM-Planner: Perception-Aware Trajectory Planning with Geometric Feature Metric", "authors": ["Yue Lin", "Xiaoxuan Zhang", "Yang Liu", "Dong Wang", "Huchuan Lu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2507.16233v1", "summary": "Like humans who rely on landmarks for orientation, autonomous robots depend\non feature-rich environments for accurate localization. In this paper, we\npropose the GFM-Planner, a perception-aware trajectory planning framework based\non the geometric feature metric, which enhances LiDAR localization accuracy by\nguiding the robot to avoid degraded areas. First, we derive the Geometric\nFeature Metric (GFM) from the fundamental LiDAR localization problem. Next, we\ndesign a 2D grid-based Metric Encoding Map (MEM) to efficiently store GFM\nvalues across the environment. A constant-time decoding algorithm is further\nproposed to retrieve GFM values for arbitrary poses from the MEM. Finally, we\ndevelop a perception-aware trajectory planning algorithm that improves LiDAR\nlocalization capabilities by guiding the robot in selecting trajectories\nthrough feature-rich areas. Both simulation and real-world experiments\ndemonstrate that our approach enables the robot to actively select trajectories\nthat significantly enhance LiDAR localization accuracy.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.16233v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15885", "title": "ADEPTS: A Capability Framework for Human-Centered Agent Design", "authors": ["Pierluca D'Oro", "Caley Drooff", "Joy Chen", "Joseph Tighe"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15885v1", "summary": "Large language models have paved the way to powerful and flexible AI agents,\nassisting humans by increasingly integrating into their daily life. This\nflexibility, potential, and growing adoption demands a holistic and\ncross-disciplinary approach to developing, monitoring and discussing the\ncapabilities required for agent-driven user experiences. However, current\nguidance on human-centered AI agent development is scattered: UX heuristics\nfocus on interface behaviors, engineering taxonomies describe internal\npipelines, and ethics checklists address high-level governance. There is no\nconcise, user-facing vocabulary that tells teams what an agent should\nfundamentally be able to do. We introduce ADEPTS, a capability framework\ndefining a set of core user-facing capabilities to provide unified guidance\naround the development of AI agents. ADEPTS is based on six principles for\nhuman-centered agent design, that express the minimal, user-facing capabilities\nan AI agent should demonstrate to be understandable, controllable and\ntrustworthy in everyday use. ADEPTS complements existing frameworks and\ntaxonomies; differently from them, it sits at the interface between technical\nand experience development. By presenting ADEPTS, we aim to condense complex\nAI-UX requirements into a compact framework that is actionable guidance for AI\nresearchers, designers, engineers, and policy reviewers alike. We believe\nADEPTS has the potential of accelerating the improvement of user-relevant agent\ncapabilities, of easing the design of experiences that take advantage of those\ncapabilities, and of providing a shared language to track and discuss progress\naround the development of AI agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15885v1", "cate": "cs.AI", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.15900", "title": "Improving the Generation of VAEs with High Dimensional Latent Spaces by the use of Hyperspherical Coordinates", "authors": ["Alejandro Ascarate", "Leo Lebrat", "Rodrigo Santa Cruz", "Clinton Fookes", "Olivier Salvado"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures, published in IJCNN25 (in press)", "url": "http://arxiv.org/abs/2507.15900v1", "summary": "Variational autoencoders (VAE) encode data into lower-dimensional latent\nvectors before decoding those vectors back to data. Once trained, decoding a\nrandom latent vector from the prior usually does not produce meaningful data,\nat least when the latent space has more than a dozen dimensions. In this paper,\nwe investigate this issue by drawing insight from high dimensional statistics:\nin these regimes, the latent vectors of a standard VAE are by construction\ndistributed uniformly on a hypersphere. We propose to formulate the latent\nvariables of a VAE using hyperspherical coordinates, which allows compressing\nthe latent vectors towards an island on the hypersphere, thereby reducing the\nlatent sparsity and we show that this improves the generation ability of the\nVAE. We propose a new parameterization of the latent space with limited\ncomputational overhead.", "comment": "8 pages, 3 figures, published in IJCNN25 (in press)", "pdf_url": "http://arxiv.org/pdf/2507.15900v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15996", "title": "Understanding the Impact of Physicians' Legal Considerations on XAI Systems", "authors": ["Gennie Mansi", "Mark Riedl"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2507.15996v1", "summary": "Physicians are--and feel--ethically, professionally, and legally responsible\nfor patient outcomes, buffering patients from harmful AI determinations from\nmedical AI systems. Many have called for explainable AI (XAI) systems to help\nphysicians incorporate medical AI recommendations into their workflows in a way\nthat reduces the potential of harms to patients. While prior work has\ndemonstrated how physicians' legal concerns impact their medical decision\nmaking, little work has explored how XAI systems should be designed in light of\nthese concerns. In this study, we conducted interviews with 10 physicians to\nunderstand where and how they anticipate errors that may occur with a medical\nAI system and how these anticipated errors connect to their legal concerns. In\nour study, physicians anticipated risks associated with using an AI system for\npatient care, but voiced unknowns around how their legal risk mitigation\nstrategies may change given a new technical system. Based on these findings, we\ndescribe the implications for designing XAI systems that can address\nphysicians' legal concerns. Specifically, we identify the need to provide AI\nrecommendations alongside contextual information that guides their risk\nmitigation strategies, including how non-legally related aspects of their\nsystems, such as medical documentation and auditing requests, might be\nincorporated into a legal case.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.15996v1", "cate": "cs.HC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2502.16580", "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?", "authors": ["Yulin Chen", "Haoran Li", "Yuan Sui", "Yufei He", "Yue Liu", "Yangqiu Song", "Bryan Hooi"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      To Appear in ACL 2025", "url": "http://arxiv.org/abs/2502.16580v3", "summary": "Prompt injection attacks manipulate large language models (LLMs) by\nmisleading them to deviate from the original input instructions and execute\nmaliciously injected instructions, because of their instruction-following\ncapabilities and inability to distinguish between the original input\ninstructions and maliciously injected instructions. To defend against such\nattacks, recent studies have developed various detection mechanisms. If we\nrestrict ourselves specifically to works which perform detection rather than\ndirect defense, most of them focus on direct prompt injection attacks, while\nthere are few works for the indirect scenario, where injected instructions are\nindirectly from external tools, such as a search engine. Moreover, current\nworks mainly investigate injection detection methods and pay less attention to\nthe post-processing method that aims to mitigate the injection after detection.\nIn this paper, we investigate the feasibility of detecting and removing\nindirect prompt injection attacks, and we construct a benchmark dataset for\nevaluation. For detection, we assess the performance of existing LLMs and\nopen-source detection models, and we further train detection models using our\ncrafted training datasets. For removal, we evaluate two intuitive methods: (1)\nthe segmentation removal method, which segments the injected document and\nremoves parts containing injected instructions, and (2) the extraction removal\nmethod, which trains an extraction model to identify and remove injected\ninstructions.", "comment": "To Appear in ACL 2025", "pdf_url": "http://arxiv.org/pdf/2502.16580v3", "cate": "cs.CR", "date": "2025-02-23", "updated": "2025-07-22"}
{"id": "2507.16305", "title": "Trajectory Planning of a Curtain Wall Installation Robot Based on Biomimetic Mechanisms", "authors": ["Xiao Liu", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16305v1", "summary": "As the robotics market rapidly evolves, energy consumption has become a\ncritical issue, particularly restricting the application of construction\nrobots. To tackle this challenge, our study innovatively draws inspiration from\nthe mechanics of human upper limb movements during weight lifting, proposing a\nbio-inspired trajectory planning framework that incorporates human energy\nconversion principles. By collecting motion trajectories and electromyography\n(EMG) signals during dumbbell curls, we construct an anthropomorphic trajectory\nplanning that integrates human force exertion patterns and energy consumption\npatterns. Utilizing the Particle Swarm Optimization (PSO) algorithm, we achieve\ndynamic load distribution for robotic arm trajectory planning based on\nhuman-like movement features. In practical application, these bio-inspired\nmovement characteristics are applied to curtain wall installation tasks,\nvalidating the correctness and superiority of our trajectory planning method.\nSimulation results demonstrate a 48.4% reduction in energy consumption through\nintelligent conversion between kinetic and potential energy. This approach\nprovides new insights and theoretical support for optimizing energy use in\ncurtain wall installation robots during actual handling tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16305v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15895", "title": "Integrating Reason-Based Moral Decision-Making in the Reinforcement Learning Architecture", "authors": ["Lisa Dargasz"], "categories": ["cs.AI", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Master's thesis, April 2025, 122 pages", "url": "http://arxiv.org/abs/2507.15895v1", "summary": "Reinforcement Learning is a machine learning methodology that has\ndemonstrated strong performance across a variety of tasks. In particular, it\nplays a central role in the development of artificial autonomous agents. As\nthese agents become increasingly capable, market readiness is rapidly\napproaching, which means those agents, for example taking the form of humanoid\nrobots or autonomous cars, are poised to transition from laboratory prototypes\nto autonomous operation in real-world environments. This transition raises\nconcerns leading to specific requirements for these systems - among them, the\nrequirement that they are designed to behave ethically. Crucially, research\ndirected toward building agents that fulfill the requirement to behave\nethically - referred to as artificial moral agents(AMAs) - has to address a\nrange of challenges at the intersection of computer science and philosophy.\nThis study explores the development of reason-based artificial moral agents\n(RBAMAs). RBAMAs are build on an extension of the reinforcement learning\narchitecture to enable moral decision-making based on sound normative\nreasoning, which is achieved by equipping the agent with the capacity to learn\na reason-theory - a theory which enables it to process morally relevant\npropositions to derive moral obligations - through case-based feedback. They\nare designed such that they adapt their behavior to ensure conformance to these\nobligations while they pursue their designated tasks. These features contribute\nto the moral justifiability of the their actions, their moral robustness, and\ntheir moral trustworthiness, which proposes the extended architecture as a\nconcrete and deployable framework for the development of AMAs that fulfills key\nethical desiderata. This study presents a first implementation of an RBAMA and\ndemonstrates the potential of RBAMAs in initial experiments.", "comment": "Master's thesis, April 2025, 122 pages", "pdf_url": "http://arxiv.org/pdf/2507.15895v1", "cate": "cs.AI", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2507.15903", "title": "Towards Mitigation of Hallucination for LLM-empowered Agents: Progressive Generalization Bound Exploration and Watchdog Monitor", "authors": ["Siyuan Liu", "Wenjing Liu", "Zhiwei Xu", "Xin Wang", "Bo Chen", "Tao Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15903v1", "summary": "Empowered by large language models (LLMs), intelligent agents have become a\npopular paradigm for interacting with open environments to facilitate AI\ndeployment. However, hallucinations generated by LLMs-where outputs are\ninconsistent with facts-pose a significant challenge, undermining the\ncredibility of intelligent agents. Only if hallucinations can be mitigated, the\nintelligent agents can be used in real-world without any catastrophic risk.\nTherefore, effective detection and mitigation of hallucinations are crucial to\nensure the dependability of agents. Unfortunately, the related approaches\neither depend on white-box access to LLMs or fail to accurately identify\nhallucinations. To address the challenge posed by hallucinations of intelligent\nagents, we present HalMit, a novel black-box watchdog framework that models the\ngeneralization bound of LLM-empowered agents and thus detect hallucinations\nwithout requiring internal knowledge of the LLM's architecture. Specifically, a\nprobabilistic fractal sampling technique is proposed to generate a sufficient\nnumber of queries to trigger the incredible responses in parallel, efficiently\nidentifying the generalization bound of the target agent. Experimental\nevaluations demonstrate that HalMit significantly outperforms existing\napproaches in hallucination monitoring. Its black-box nature and superior\nperformance make HalMit a promising solution for enhancing the dependability of\nLLM-powered systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15903v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16013", "title": "AI, Expert or Peer? -- Examining the Impact of Perceived Feedback Source on Pre-Service Teachers Feedback Perception and Uptake", "authors": ["Lucas Jasper Jacobsen", "Ute Mertens", "Thorben Jansen", "Kira Elena Weber"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      37 pages, 2 figures, 6 tables", "url": "http://arxiv.org/abs/2507.16013v1", "summary": "Feedback plays a central role in learning, yet pre-service teachers'\nengagement with feedback depends not only on its quality but also on their\nperception of the feedback content and source. Large Language Models (LLMs) are\nincreasingly used to provide educational feedback; however, negative\nperceptions may limit their practical use, and little is known about how\npre-service teachers' perceptions and behavioral responses differ by feedback\nsource. This study investigates how the perceived source of feedback - LLM,\nexpert, or peer - influences feedback perception and uptake, and whether\nrecognition accuracy and feedback quality moderate these effects. In a\nrandomized experiment with 273 pre-service teachers, participants received\nwritten feedback on a mathematics learning goal, identified its source, rated\nfeedback perceptions across five dimensions (fairness, usefulness, acceptance,\nwillingness to improve, positive and negative affect), and revised the learning\ngoal according to the feedback (i.e. feedback uptake). Results revealed that\nLLM-generated feedback received the highest ratings in fairness and usefulness,\nleading to the highest uptake (52%). Recognition accuracy significantly\nmoderated the effect of feedback source on perception, with particularly\npositive evaluations when LLM feedback was falsely ascribed to experts.\nHigher-quality feedback was consistently assigned to experts, indicating an\nexpertise heuristic in source judgments. Regression analysis showed that only\nfeedback quality significantly predicted feedback uptake. Findings highlight\nthe need to address source-related biases and promote feedback and AI literacy\nin teacher education.", "comment": "37 pages, 2 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.16013v1", "cate": "cs.HC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15860", "title": "Prediction of Alpha-Particle-Immune Gate-All-Around Field-Effect Transistors (GAA-FET) Based SRAM Design", "authors": ["Albert Lu", "Reza Arghavani", "Hiu Yung Wong"], "categories": ["cs.ET", "physics.comp-ph"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15860v1", "summary": "In this paper, using 3D Technology Computer-Aided-Design (TCAD) simulations,\nwe show that it is possible to design a static random-access memory (SRAM)\nusing gate-all-around field-effect-transistor (GAA-FET) technology so that it\nis immune to single alpha particle radiation error. In other words, with the\ndesign, there will be no single-event upset (SEU) due to alpha particles. We\nfirst use ab initio calculations in PHITS to show that there is a maximum\nlinear energy transfer (LET), LETmax, for the alpha particle in Si and\nSi$_x$Ge$_{1-x}$. Based on that, by designing a sub-7nm GAA-FET-based SRAM with\nbottom dielectric isolation (BDI), we show that the SRAM does not flip even if\nthe particle strike is in the worst-case scenario (LET > LETmax).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15860v1", "cate": "cs.ET", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2503.03108", "title": "OMNISEC: LLM-Driven Provenance-based Intrusion Detection via Retrieval-Augmented Behavior Prompting", "authors": ["Wenrui Cheng", "Tiantian Zhu", "Shunan Jing", "Jian-Ping Mei", "Mingjun Ma", "Jiaobo Jin", "Zhengqiu Weng"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.03108v4", "summary": "Recently, Provenance-based Intrusion Detection Systems (PIDSes) have been\nwidely used for endpoint threat analysis. These studies can be broadly\ncategorized into rule-based detection systems and learning-based detection\nsystems. Among these, due to the evolution of attack techniques, rules cannot\ndynamically model all the characteristics of attackers. As a result, such\nsystems often face false negatives. Learning-based detection systems are\nfurther divided into supervised learning and anomaly detection. The scarcity of\nattack samples hinders the usability and effectiveness of supervised\nlearning-based detection systems in practical applications. Anomaly-based\ndetection systems face a massive false positive problem because they cannot\ndistinguish between changes in normal behavior and real attack behavior. The\nalert results of detection systems are closely related to the manual labor\ncosts of subsequent security analysts. To reduce manual analysis time, we\npropose OMNISEC, which applies large language models (LLMs) to anomaly-based\nintrusion detection systems via retrieval-augmented behavior prompting. OMNISEC\ncan identify abnormal nodes and corresponding abnormal events by constructing\nsuspicious nodes and rare paths. By combining two external knowledge bases,\nOMNISEC uses Retrieval Augmented Generation (RAG) to enable the LLM to\ndetermine whether abnormal behavior is a real attack. Finally, OMNISEC can\nreconstruct the attack graph and restore the complete attack behavior chain of\nthe attacker's intrusion. Experimental results show that OMNISEC outperforms\nstate-of-the-art methods on public benchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.03108v4", "cate": "cs.CR", "date": "2025-03-05", "updated": "2025-07-22"}
{"id": "2507.16328", "title": "Design and Dimensional Optimization of Legged Structures for Construction Robots", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16328v1", "summary": "Faced with complex and unstructured construction environments, wheeled and\ntracked robots exhibit significant limitations in terrain adaptability and\nflexibility, making it difficult to meet the requirements of autonomous\noperation. Inspired by ants in nature, this paper proposes a leg configuration\ndesign and optimization method tailored for construction scenarios, aiming to\nenhance the autonomous mobility of construction robots. This paper analyzes the\nfull operational motion performance of the leg during both swing and stance\nphases. First, based on kinematic modeling and multi-dimensional workspace\nanalysis, the concept of an \"improved workspace\" is introduced, and graphical\nmethods are used to optimize the leg dimensions during the swing phase.\nFurthermore, a new concept of \"average manipulability\" is introduced based on\nthe velocity Jacobian matrix, and numerical solutions are applied to obtain the\nleg segment ratio that maximizes manipulability. To overcome the difficulties\nassociated with traditional analytical methods, virtual prototype simulations\nare conducted in ADAMS to explore the relationship between the robot body's\noptimal flexibility and leg segment proportions. In summary, the leg segment\nproportions with the best comprehensive motion performance are obtained. This\nstudy presents the first multi-dimensional quantitative evaluation framework\nfor leg motion performance tailored for construction environments, providing a\nstructural design foundation for legged construction robots to achieve\nautonomous mobility in complex terrains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16328v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15974", "title": "Does More Inference-Time Compute Really Help Robustness?", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Weichen Yu", "Chawin Sitawarin", "Vikash Sehwag", "Prateek Mittal"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.15974v1", "summary": "Recently, Zaremba et al. demonstrated that increasing inference-time\ncomputation improves robustness in large proprietary reasoning LLMs. In this\npaper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,\nQwen3, Phi-reasoning) can also benefit from inference-time scaling using a\nsimple budget forcing strategy. More importantly, we reveal and critically\nexamine an implicit assumption in prior work: intermediate reasoning steps are\nhidden from adversaries. By relaxing this assumption, we identify an important\nsecurity risk, intuitively motivated and empirically verified as an inverse\nscaling law: if intermediate reasoning steps become explicitly accessible,\nincreased inference-time computation consistently reduces model robustness.\nFinally, we discuss practical scenarios where models with hidden reasoning\nchains are still vulnerable to attacks, such as models with tool-integrated\nreasoning and advanced reasoning extraction attacks. Our findings collectively\ndemonstrate that the robustness benefits of inference-time scaling depend\nheavily on the adversarial setting and deployment context. We urge\npractitioners to carefully weigh these subtle trade-offs before applying\ninference-time scaling in security-sensitive, real-world applications.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.15974v1", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15904", "title": "Fast-VAT: Accelerating Cluster Tendency Visualization using Cython and Numba", "authors": ["MSR Avinash", "Ismael Lachheb"], "categories": ["cs.LG", "cs.NE", "I.5.3; D.2.8; G.4"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures, 3 tables. Code available at this https URL", "url": "http://arxiv.org/abs/2507.15904v1", "summary": "Visual Assessment of Cluster Tendency (VAT) is a widely used unsupervised\ntechnique to assess the presence of cluster structure in unlabeled datasets.\nHowever, its standard implementation suffers from significant performance\nlimitations due to its O(n^2) time complexity and inefficient memory usage. In\nthis work, we present Fast-VAT, a high-performance reimplementation of the VAT\nalgorithm in Python, augmented with Numba's Just-In-Time (JIT) compilation and\nCython's static typing and low-level memory optimizations. Our approach\nachieves up to 50x speedup over the baseline implementation, while preserving\nthe output fidelity of the original method. We validate Fast-VAT on a suite of\nreal and synthetic datasets -- including Iris, Mall Customers, and Spotify\nsubsets -- and verify cluster tendency using Hopkins statistics, PCA, and\nt-SNE. Additionally, we compare VAT's structural insights with clustering\nresults from DBSCAN and K-Means to confirm its reliability.", "comment": "10 pages, 3 figures, 3 tables. Code available at\n  https://github.com/Ashx098/VAT-Optimized", "pdf_url": "http://arxiv.org/pdf/2507.15904v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16033", "title": "\"Just a strange pic\": Evaluating 'safety' in GenAI Image safety annotation tasks from diverse annotators' perspectives", "authors": ["Ding Wang", "Mark Daz", "Charvi Rastogi", "Aida Davani", "Vinodkumar Prabhakaran", "Pushkar Mishra", "Roma Patel", "Alicia Parrish", "Zoe Ashwood", "Michela Paganini", "Tian Huey Teh", "Verena Rieser", "Lora Aroyo"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society 2025 (AIES 2025)", "url": "http://arxiv.org/abs/2507.16033v1", "summary": "Understanding what constitutes safety in AI-generated content is complex.\nWhile developers often rely on predefined taxonomies, real-world safety\njudgments also involve personal, social, and cultural perceptions of harm. This\npaper examines how annotators evaluate the safety of AI-generated images,\nfocusing on the qualitative reasoning behind their judgments. Analyzing 5,372\nopen-ended comments, we find that annotators consistently invoke moral,\nemotional, and contextual reasoning that extends beyond structured safety\ncategories. Many reflect on potential harm to others more than to themselves,\ngrounding their judgments in lived experience, collective risk, and\nsociocultural awareness. Beyond individual perceptions, we also find that the\nstructure of the task itself -- including annotation guidelines -- shapes how\nannotators interpret and express harm. Guidelines influence not only which\nimages are flagged, but also the moral judgment behind the justifications.\nAnnotators frequently cite factors such as image quality, visual distortion,\nand mismatches between prompt and output as contributing to perceived harm\ndimensions, which are often overlooked in standard evaluation frameworks. Our\nfindings reveal that existing safety pipelines miss critical forms of reasoning\nthat annotators bring to the task. We argue for evaluation designs that\nscaffold moral reflection, differentiate types of harm, and make space for\nsubjective, context-sensitive interpretations of AI-generated content.", "comment": "Accepted to AAAI/ACM Conference on Artificial Intelligence, Ethics,\n  and Society 2025 (AIES 2025)", "pdf_url": "http://arxiv.org/pdf/2507.16033v1", "cate": "cs.HC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16584", "title": "Quantum Annealing Hyperparameter Analysis for Optimal Sensor Placement in Production Environments", "authors": ["Nico Kraus", "Marvin Erdmann", "Alexander Kuzmany", "Daniel Porawski", "Jonas Stein"], "categories": ["cs.ET"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16584v1", "summary": "To increase efficiency in automotive manufacturing, newly produced vehicles\ncan move autonomously from the production line to the distribution area. This\nrequires an optimal placement of sensors to ensure full coverage while\nminimizing the number of sensors used. The underlying optimization problem\nposes a computational challenge due to its large-scale nature. Currently,\nclassical solvers rely on heuristics, often yielding non-optimal solutions for\nlarge instances, resulting in suboptimal sensor distributions and increased\noperational costs.\n  We explore quantum computing methods that may outperform classical heuristics\nin the future. We implemented quantum annealing with D-Wave, transforming the\nproblem into a quadratic unconstrained binary optimization formulation with\none-hot and binary encoding. Hyperparameters like the penalty terms and the\nannealing time are optimized and the results are compared with default\nparameter settings.\n  Our results demonstrate that quantum annealing is capable of solving\ninstances derived from real-world scenarios. Through the use of decomposition\ntechniques, we are able to scale the problem size further, bringing it closer\nto practical, industrial applicability. Through this work, we provide key\ninsights into the importance of quantum annealing parametrization,\ndemonstrating how quantum computing could contribute to cost-efficient,\nlarge-scale optimization problems once the hardware matures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16584v1", "cate": "cs.ET", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2504.07280", "title": "Conthereum: Concurrent Ethereum Optimized Transaction Scheduling for Multi-Core Execution", "authors": ["Atefeh Zareh Chahoki", "Maurice Herlihy", "Marco Roveri"], "categories": ["cs.CR", "cs.DC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      10 pages, 3 tables, 7 figures, 1 algorithms", "url": "http://arxiv.org/abs/2504.07280v3", "summary": "Conthereum is a concurrent Ethereum solution for intra-block parallel\ntransaction execution, enabling validators to utilize multi-core infrastructure\nand transform the sequential execution model of Ethereum into a parallel one.\nThis shift significantly increases throughput and transactions per second\n(TPS), while ensuring conflict-free execution in both proposer and attestor\nmodes and preserving execution order consistency in the attestor. At the heart\nof Conthereum is a novel, lightweight, high-performance scheduler inspired by\nthe Flexible Job Shop Scheduling Problem (FJSS). We propose a custom greedy\nheuristic algorithm, along with its efficient implementation, that solves this\nformulation effectively and decisively outperforms existing scheduling methods\nin finding suboptimal solutions that satisfy the constraints, achieve minimal\nmakespan, and maximize speedup in parallel execution. Additionally, Conthereum\nincludes an offline phase that equips its real-time scheduler with a conflict\nanalysis repository obtained through static analysis of smart contracts,\nidentifying potentially conflicting functions using a pessimistic approach.\nBuilding on this novel scheduler and extensive conflict data, Conthereum\noutperforms existing concurrent intra-block solutions. Empirical evaluations\nshow near-linear throughput gains with increasing computational power on\nstandard 8-core machines. Although scalability deviates from linear with higher\ncore counts and increased transaction conflicts, Conthereum still significantly\nimproves upon the current sequential execution model and outperforms existing\nconcurrent solutions under a wide range of conditions.", "comment": "10 pages, 3 tables, 7 figures, 1 algorithms", "pdf_url": "http://arxiv.org/pdf/2504.07280v3", "cate": "cs.CR", "date": "2025-04-09", "updated": "2025-07-22"}
{"id": "2507.16335", "title": "Topology Optimization of Leg Structures for Construction Robots Based on Variable Density Method", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16335v1", "summary": "In complex terrain construction environments, there are high demands for\nrobots to achieve both high payload capacity and mobility flexibility. As the\nkey load-bearing component, the optimization of robotic leg structures is of\nparticular importance. Therefore, this study focuses on the optimization of leg\nstructures for construction robots, proposing a topology optimization strategy\nbased on the SIMP (Solid Isotropic Microstructures with Penalization) variable\ndensity method along with a structural re-design approach. The design\nperformance is comprehensively validated through finite element analysis using\nANSYS. First, static and modal analyses are conducted to evaluate the\nrationality of the initial design. Then, topology optimization using the\nSIMP-based variable density method is applied to the femur section, which\naccounts for the largest proportion of the leg's weight. Based on iterative\ncalculations, the femur undergoes secondary structural reconstruction. After\noptimization, the mass of the femur is reduced by 19.45\\%, and the overall leg\nmass decreases by 7.92\\%, achieving the goal of lightweight design. Finally,\nstatic and modal analyses are conducted on the reconstructed leg. The results\ndemonstrate that the optimized leg still meets structural performance\nrequirements, validating the feasibility of lightweight design. This research\nprovides robust theoretical and technical support for lightweight construction\nrobot design and lays a foundation for their efficient operation in complex\nconstruction environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16335v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16020", "title": "Micromobility Flow Prediction: A Bike Sharing Station-level Study via Multi-level Spatial-Temporal Attention Neural Network", "authors": ["Xi Yang", "Jiachen Wang", "Song Han", "Suining He"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      6 pages, UrbComp 2024", "url": "http://arxiv.org/abs/2507.16020v1", "summary": "Efficient use of urban micromobility resources such as bike sharing is\nchallenging due to the unbalanced station-level demand and supply, which causes\nthe maintenance of the bike sharing systems painstaking. Prior efforts have\nbeen made on accurate prediction of bike traffics, i.e., demand/pick-up and\nreturn/drop-off, to achieve system efficiency. However, bike station-level\ntraffic prediction is difficult because of the spatial-temporal complexity of\nbike sharing systems. Moreover, such level of prediction over entire bike\nsharing systems is also challenging due to the large number of bike stations.\nTo fill this gap, we propose BikeMAN, a multi-level spatio-temporal attention\nneural network to predict station-level bike traffic for entire bike sharing\nsystems. The proposed network consists of an encoder and a decoder with an\nattention mechanism representing the spatial correlation between features of\nbike stations in the system and another attention mechanism describing the\ntemporal characteristic of bike station traffic. Through experimental study on\nover 10 millions trips of bike sharing systems (> 700 stations) of New York\nCity, our network showed high accuracy in predicting the bike station traffic\nof all stations in the city.", "comment": "6 pages, UrbComp 2024", "pdf_url": "http://arxiv.org/pdf/2507.16020v1", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15905", "title": "Foundation Models and Transformers for Anomaly Detection: A Survey", "authors": ["Moun Ben Ammar", "Arturo Mendoza", "Nacim Belkhir", "Antoine Manzanera", "Gianni Franchi"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15905v1", "summary": "In line with the development of deep learning, this survey examines the\ntransformative role of Transformers and foundation models in advancing visual\nanomaly detection (VAD). We explore how these architectures, with their global\nreceptive fields and adaptability, address challenges such as long-range\ndependency modeling, contextual modeling and data scarcity. The survey\ncategorizes VAD methods into reconstruction-based, feature-based and\nzero/few-shot approaches, highlighting the paradigm shift brought about by\nfoundation models. By integrating attention mechanisms and leveraging\nlarge-scale pre-training, Transformers and foundation models enable more\nrobust, interpretable, and scalable anomaly detection solutions. This work\nprovides a comprehensive review of state-of-the-art techniques, their\nstrengths, limitations, and emerging trends in leveraging these architectures\nfor VAD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15905v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16073", "title": "Buckaroo: A Direct Manipulation Visual Data Wrangler", "authors": ["Annabelle Warner", "Andrew McNutt", "Paul Rosen", "El Kindi Rezig"], "categories": ["cs.HC", "cs.DB"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to VLDB25 Demo track", "url": "http://arxiv.org/abs/2507.16073v1", "summary": "Preparing datasets -- a critical phase known as data wrangling -- constitutes\nthe dominant phase of data science development, consuming upwards of 80% of the\ntotal project time. This phase encompasses a myriad of tasks: parsing data,\nrestructuring it for analysis, repairing inaccuracies, merging sources,\neliminating duplicates, and ensuring overall data integrity. Traditional\napproaches, typically through manual coding in languages such as Python or\nusing spreadsheets, are not only laborious but also error-prone. These issues\nrange from missing entries and formatting inconsistencies to data type\ninaccuracies, all of which can affect the quality of downstream tasks if not\nproperly corrected. To address these challenges, we present Buckaroo, a\nvisualization system to highlight discrepancies in data and enable on-the-spot\ncorrections through direct manipulations of visual objects. Buckaroo (1)\nautomatically finds \"interesting\" data groups that exhibit anomalies compared\nto the rest of the groups and recommends them for inspection; (2) suggests\nwrangling actions that the user can choose to repair the anomalies; and (3)\nallows users to visually manipulate their data by displaying the effects of\ntheir wrangling actions and offering the ability to undo or redo these actions,\nwhich supports the iterative nature of data wrangling. A video companion is\navailable at https://youtu.be/iXdCYbvpQVE", "comment": "Accepted to VLDB25 Demo track", "pdf_url": "http://arxiv.org/pdf/2507.16073v1", "cate": "cs.HC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16413", "title": "Towards Railway Domain Adaptation for LiDAR-based 3D Detection: Road-to-Rail and Sim-to-Real via SynDRA-BBox", "authors": ["Xavier Diaz", "Gianluca D'Amico", "Raul Dominguez-Sanchez", "Federico Nesti", "Max Ronecker", "Giorgio Buttazzo"], "categories": ["cs.CV", "cs.ET"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IEEE International Conference on Intelligent Rail Transportation (ICIRT) 2025", "url": "http://arxiv.org/abs/2507.16413v1", "summary": "In recent years, interest in automatic train operations has significantly\nincreased. To enable advanced functionalities, robust vision-based algorithms\nare essential for perceiving and understanding the surrounding environment.\nHowever, the railway sector suffers from a lack of publicly available\nreal-world annotated datasets, making it challenging to test and validate new\nperception solutions in this domain. To address this gap, we introduce\nSynDRA-BBox, a synthetic dataset designed to support object detection and other\nvision-based tasks in realistic railway scenarios. To the best of our\nknowledge, is the first synthetic dataset specifically tailored for 2D and 3D\nobject detection in the railway domain, the dataset is publicly available at\nhttps://syndra.retis.santannapisa.it. In the presented evaluation, a\nstate-of-the-art semi-supervised domain adaptation method, originally developed\nfor automotive perception, is adapted to the railway context, enabling the\ntransferability of synthetic data to 3D object detection. Experimental results\ndemonstrate promising performance, highlighting the effectiveness of synthetic\ndatasets and domain adaptation techniques in advancing perception capabilities\nfor railway environments.", "comment": "IEEE International Conference on Intelligent Rail Transportation\n  (ICIRT) 2025", "pdf_url": "http://arxiv.org/pdf/2507.16413v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15878", "title": "Salience Adjustment for Context-Based Emotion Recognition", "authors": ["Bin Han", "Jonathan Gratch"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15878v1", "summary": "Emotion recognition in dynamic social contexts requires an understanding of\nthe complex interaction between facial expressions and situational cues. This\npaper presents a salience-adjusted framework for context-aware emotion\nrecognition with Bayesian Cue Integration (BCI) and Visual-Language Models\n(VLMs) to dynamically weight facial and contextual information based on the\nexpressivity of facial cues. We evaluate this approach using human annotations\nand automatic emotion recognition systems in prisoner's dilemma scenarios,\nwhich are designed to evoke emotional reactions. Our findings demonstrate that\nincorporating salience adjustment enhances emotion recognition performance,\noffering promising directions for future research to extend this framework to\nbroader social contexts and multimodal applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15878v1", "cate": "cs.CV", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2506.01885", "title": "SoK: Concurrency in Blockchain -- A Systematic Literature Review and the Unveiling of a Misconception", "authors": ["Atefeh Zareh Chahoki", "Maurice Herlihy", "Marco Roveri"], "categories": ["cs.CR", "cs.DC", "cs.PF"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.01885v2", "summary": "Smart contracts, the cornerstone of blockchain technology, enable secure,\nautomated distributed execution. Given their role in handling large transaction\nvolumes across clients, miners, and validators, exploring concurrency is\ncritical. This includes concurrent transaction execution or validation within\nblocks, block processing across shards, and miner competition to select and\npersist transactions. Concurrency and parallelism are a double-edged sword:\nwhile they improve throughput, they also introduce risks like race conditions,\nnon-determinism, and vulnerabilities such as deadlock and livelock.\n  This paper presents the first survey of concurrency in smart contracts,\noffering a systematic literature review organized into key dimensions. First,\nit establishes a taxonomy of concurrency levels in blockchain systems and\ndiscusses proposed solutions for future adoption. Second, it examines\nvulnerabilities, attacks, and countermeasures in concurrent operations,\nemphasizing the need for correctness and security. Crucially, we reveal a\nflawed concurrency assumption in a major research category, which has led to\nwidespread misinterpretation. This work aims to correct that and guide future\nresearch toward more accurate models. Finally, we identify gaps in each\ncategory to outline future research directions and support blockchain's\nadvancement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.01885v2", "cate": "cs.CR", "date": "2025-06-02", "updated": "2025-07-22"}
{"id": "2507.16369", "title": "Humanoid Robot Whole-body Geometric Calibration with Embedded Sensors and a Single Plane", "authors": ["Thanh D V Nguyen", "Vincent Bonnet", "Pierre Fernbach", "David Daney", "Florent Lamiraux"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16369v1", "summary": "Whole-body geometric calibration of humanoid robots using classical robot\ncalibration methods is a timeconsuming and experimentally burdensome task.\nHowever, despite its significance for accurate control and simulation, it is\noften overlooked in the humanoid robotics community. To address this issue, we\npropose a novel practical method that utilizes a single plane, embedded force\nsensors, and an admittance controller to calibrate the whole-body kinematics of\nhumanoids without requiring manual intervention. Given the complexity of\nhumanoid robots, it is crucial to generate and determine a minimal set of\noptimal calibration postures. To do so, we propose a new algorithm called IROC\n(Information Ranking algorithm for selecting Optimal Calibration postures).\nIROC requires a pool of feasible candidate postures to build a normalized\nweighted information matrix for each posture. Then, contrary to other\nalgorithms from the literature, IROC will determine the minimal number of\noptimal postures that are to be played onto a robot for its calibration. Both\nIROC and the single-plane calibration method were experimentally validated on a\nTALOS humanoid robot. The total whole-body kinematics chain was calibrated\nusing solely 31 optimal postures with 3-point contacts on a table by the robot\ngripper. In a cross-validation experiment, the average root-mean-square (RMS)\nerror was reduced by a factor of 2.3 compared to the manufacturer's model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16369v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16028", "title": "From Logic to Language: A Trust Index for Problem Solving with LLMs", "authors": ["Tehseen Rug", "Felix Bhmer", "Tessa Pfattheicher"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      17 pages, 2 figures", "url": "http://arxiv.org/abs/2507.16028v1", "summary": "Classical computation, grounded in formal, logical systems, has been the\nengine of technological progress for decades, excelling at problems that can be\ndescribed with unambiguous rules. This paradigm, however, leaves a vast ocean\nof human problems -- those characterized by ambiguity, dynamic environments,\nand subjective context -- largely untouched. The advent of Large Language\nModels (LLMs) represents a fundamental shift, enabling computational systems to\nengage with this previously inaccessible domain using natural language. This\npaper introduces a unified framework to understand and contrast these\nproblem-solving paradigms. We define and delineate the problem spaces\naddressable by formal languages versus natural language. While solutions to the\nformer problem class can be evaluated using binary quality measures, the latter\nrequires a much more nuanced definition of approximate solution space taking\ninto account the vagueness, subjectivity and ambiguity inherent to natural\nlanguage. We therefore introduce a vector-valued trust index Q, which reflects\nsolution quality and distinguishes the binary correctness of formal solutions\nfrom the continuous adequacy spectrum characteristic of natural language\nsolutions. Within this framework, we propose two statistical quality\ndimensions. Normalized bi-semantic entropy measures robustness and conceptual\ndiversity of LLM answers given semantic variation in problem formulations.\nEmotional valence maps subjective valuation of a solution to a quantifiable\nmetric that can be maximized by invoking statistical measures. The concepts\nintroduced in this work will provide a more rigorous understanding of the\ncapabilities, limitations, and inherent nature of problem-solving in the age of\nLLMs.", "comment": "17 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.16028v1", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15906", "title": "Towards Reliable, Uncertainty-Aware Alignment", "authors": ["Debangshu Banerjee", "Kintan Saha", "Aditya Gopalan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15906v1", "summary": "Alignment of large language models (LLMs) typically involves training a\nreward model on preference data, followed by policy optimization with respect\nto the reward model. However, optimizing policies with respect to a single\nreward model estimate can render it vulnerable to inaccuracies in the reward\nmodel. We empirically study the variability of reward model training on\nopen-source benchmarks. We observe that independently trained reward models on\nthe same preference dataset can exhibit substantial disagreement, highlighting\nthe instability of current alignment strategies. Employing a theoretical model,\nwe demonstrate that variability in reward model estimation can cause\noverfitting, leading to the risk of performance degradation. To mitigate this\nrisk, we propose a variance-aware policy optimization framework for\npreference-based alignment. The key ingredient of the framework is a new policy\nregularizer that incorporates reward model variance estimates. We show that\nvariance-aware policy optimization provably reduces the risk of outputting a\nworse policy than the default. Experiments across diverse LLM and reward model\nconfigurations confirm that our approach yields more stable and robust\nalignment than the standard (variance-unaware) pipeline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15906v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16074", "title": "Toward music-based stress management: Contemporary biosensing systems for affective regulation", "authors": ["Natasha Yamane", "Varun Mishra", "Matthew S. Goodwin"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      37 pages, 3 figures, 1 table", "url": "http://arxiv.org/abs/2507.16074v1", "summary": "In the last decade, researchers have increasingly explored using biosensing\ntechnologies for music-based affective regulation and stress management\ninterventions in laboratory and real-world settings. These systems -- including\ninteractive music applications, brain-computer interfaces, and biofeedback\ndevices -- aim to provide engaging, personalized experiences that improve\ntherapeutic outcomes. In this scoping and mapping review, we summarize and\nsynthesize systematic reviews and empirical research on biosensing systems with\npotential applications in music-based affective regulation and stress\nmanagement, identify gaps in the literature, and highlight promising areas for\nfuture research. We identified 28 studies involving 646 participants, with most\nsystems utilizing prerecorded music, wearable cardiorespiratory sensors, or\ndesktop interfaces. We categorize these systems based on their biosensing\nmodalities, music types, computational models for affect or stress detection\nand music prediction, and biofeedback mechanisms. Our findings highlight the\npromising potential of these systems and suggest future directions, such as\nintegrating multimodal biosensing, exploring therapeutic mechanisms of music,\nleveraging generative artificial intelligence for personalized music\ninterventions, and addressing methodological, data privacy, and user control\nconcerns.", "comment": "37 pages, 3 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.16074v1", "cate": "cs.HC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16480", "title": "Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots", "authors": ["Sabrina Livanec", "Laura Londoo", "Michael Gorki", "Adrian Rfer", "Abhinav Valada", "Andrea Kiesel"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16480v1", "summary": "The development of assistive robots for social collaboration raises critical\nquestions about responsible and inclusive design, especially when interacting\nwith individuals from protected groups such as those with disabilities or\nadvanced age. Currently, research is scarce on how participants assess varying\nrobot behaviors in combination with diverse human needs, likely since\nparticipants have limited real-world experience with advanced domestic robots.\nIn the current study, we aim to address this gap while using methods that\nenable participants to assess robot behavior, as well as methods that support\nmeaningful reflection despite limited experience. In an online study, 112\nparticipants (from both experimental and control groups) evaluated 7 videos\nfrom a total of 28 variations of human-robot collaboration types. The\nexperimental group first completed a cognitive-affective mapping (CAM) exercise\non human-robot collaboration before providing their ratings. Although CAM\nreflection did not significantly affect overall ratings, it led to more\npronounced assessments for certain combinations of robot behavior and human\ncondition. Most importantly, the type of human-robot collaboration influences\nthe assessment. Antisocial robot behavior was consistently rated as the lowest,\nwhile collaboration with aged individuals elicited more sensitive evaluations.\nScenarios involving object handovers were viewed more positively than those\nwithout them. These findings suggest that both human characteristics and\ninteraction paradigms influence the perceived acceptability of collaborative\nrobots, underscoring the importance of prosocial design. They also highlight\nthe potential of reflective methods, such as CAM, to elicit nuanced feedback,\nsupporting the development of user-centered and socially responsible robotic\nsystems tailored to diverse populations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16480v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15882", "title": "Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark", "authors": ["Goeric Huybrechts", "Srikanth Ronanki", "Sai Muralidhar Jayanthi", "Jack Fitzgerald", "Srinivasan Veeravanallur"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15882v1", "summary": "The proliferation of multimodal Large Language Models has significantly\nadvanced the ability to analyze and understand complex data inputs from\ndifferent modalities. However, the processing of long documents remains\nunder-explored, largely due to a lack of suitable benchmarks. To address this,\nwe introduce Document Haystack, a comprehensive benchmark designed to evaluate\nthe performance of Vision Language Models (VLMs) on long, visually complex\ndocuments. Document Haystack features documents ranging from 5 to 200 pages and\nstrategically inserts pure text or multimodal text+image \"needles\" at various\ndepths within the documents to challenge VLMs' retrieval capabilities.\nComprising 400 document variants and a total of 8,250 questions, it is\nsupported by an objective, automated evaluation framework. We detail the\nconstruction and characteristics of the Document Haystack dataset, present\nresults from prominent VLMs and discuss potential research avenues in this\narea.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15882v1", "cate": "cs.CV", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.15887", "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "authors": ["Ori Press", "Brandon Amos", "Haoyu Zhao", "Yikai Wu", "Samuel K. Ainsworth", "Dominik Krupke", "Patrick Kidger", "Touqir Sajed", "Bartolomeo Stellato", "Jisun Park", "Nathanael Bosch", "Eli Meril", "Albert Steppi", "Arman Zharmagambetov", "Fangzhao Zhang", "David Perez-Pineiro", "Alberto Mercurio", "Ni Zhan", "Talor Abramovich", "Kilian Lieret", "Hanlin Zhang", "Shirley Huang", "Matthias Bethge", "Ofir Press"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15887v1", "summary": "Despite progress in language model (LM) capabilities, evaluations have thus\nfar focused on models' performance on tasks that humans have previously solved,\nincluding in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,\n2024). We therefore propose testing models' ability to design and implement\nalgorithms in an open-ended benchmark: We task LMs with writing code that\nefficiently solves computationally challenging problems in computer science,\nphysics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks\ncollected from domain experts and a framework for validating and timing\nLM-synthesized solution code, which is compared to reference implementations\nfrom popular open-source packages. In addition, we develop a baseline LM agent,\nAlgoTuner, and evaluate its performance across a suite of frontier models.\nAlgoTuner achieves an average 1.72x speedup against our reference solvers,\nwhich use libraries such as SciPy, sk-learn and CVXPY. However, we find that\ncurrent models fail to discover algorithmic innovations, instead preferring\nsurface-level optimizations. We hope that AlgoTune catalyzes the development of\nLM agents exhibiting creative problem solving beyond state-of-the-art human\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15887v1", "cate": "cs.SE", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.10845", "title": "BandFuzz: An ML-powered Collaborative Fuzzing Framework", "authors": ["Wenxuan Shi", "Hongwei Li", "Jiahao Yu", "Xinqian Sun", "Wenbo Guo", "Xinyu Xing"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10845v2", "summary": "Collaborative fuzzing combines multiple individual fuzzers and dynamically\nchooses appropriate combinations for different programs. Unlike individual\nfuzzers that rely on specific assumptions, collaborative fuzzing relaxes\nassumptions on target programs, providing robust performance across various\nprograms. However, existing collaborative fuzzing frameworks face challenges\nincluding additional computational resource requirements and inefficient\nresource allocation among fuzzers. To tackle these challenges, we present\nBANDFUZZ, an ML-powered collaborative fuzzing framework that outperforms\nindividual fuzzers without requiring additional computational resources. The\nkey contribution of BANDFUZZ lies in its novel resource allocation algorithm\ndriven by our proposed multi-armed bandits model. Different from greedy methods\nin existing frameworks, BANDFUZZ models the long-term impact of individual\nfuzzers, enabling discovery of globally optimal collaborative strategies. We\npropose a novel fuzzer evaluation method that assesses not only code coverage\nbut also the fuzzer's capability of solving difficult branches. Finally, we\nintegrate a real-time seed synchronization mechanism and implementation-wise\noptimizations to improve fuzzing efficiency and stability. Through extensive\nexperiments on Fuzzbench and Fuzzer Test Suite, we show that BANDFUZZ\noutperforms state-of-the-art collaborative fuzzing framework autofz and widely\nused individual fuzzers. We verify BANDFUZZ's key designs through comprehensive\nablation study. Notably, we demonstrate BANDFUZZ's effectiveness in real-world\nbug detection by analyzing results of a worldwide fuzzing competition, where\nBANDFUZZ won first place.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10845v2", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-21"}
{"id": "2507.16382", "title": "Application of LLM Guided Reinforcement Learning in Formation Control with Collision Avoidance", "authors": ["Chenhao Yao", "Zike Yuan", "Xiaoxu Liu", "Chi Zhu"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2507.16382v1", "summary": "Multi-Agent Systems (MAS) excel at accomplishing complex objectives through\nthe collaborative efforts of individual agents. Among the methodologies\nemployed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of\nthe most efficacious algorithms. However, when confronted with the complex\nobjective of Formation Control with Collision Avoidance (FCCA): designing an\neffective reward function that facilitates swift convergence of the policy\nnetwork to an optimal solution. In this paper, we introduce a novel framework\nthat aims to overcome this challenge. By giving large language models (LLMs) on\nthe prioritization of tasks and the observable information available to each\nagent, our framework generates reward functions that can be dynamically\nadjusted online based on evaluation outcomes by employing more advanced\nevaluation metrics rather than the rewards themselves. This mechanism enables\nthe MAS to simultaneously achieve formation control and obstacle avoidance in\ndynamic environments with enhanced efficiency, requiring fewer iterations to\nreach superior performance levels. Our empirical studies, conducted in both\nsimulation and real-world settings, validate the practicality and effectiveness\nof our proposed approach.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.16382v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16067", "title": "A Unifying Framework for Semiring-Based Constraint Logic Programming With Negation (full version)", "authors": ["Jeroen Spaans", "Jesse Heyninck"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Full version, including proofs and appendices, of paper accepted at IJCAI 2025", "url": "http://arxiv.org/abs/2507.16067v1", "summary": "Constraint Logic Programming (CLP) is a logic programming formalism used to\nsolve problems requiring the consideration of constraints, like resource\nallocation and automated planning and scheduling. It has previously been\nextended in various directions, for example to support fuzzy constraint\nsatisfaction, uncertainty, or negation, with different notions of semiring\nbeing used as a unifying abstraction for these generalizations. None of these\nextensions have studied clauses with negation allowed in the body. We\ninvestigate an extension of CLP which unifies many of these extensions and\nallows negation in the body. We provide semantics for such programs, using the\nframework of approximation fixpoint theory, and give a detailed overview of the\nimpacts of properties of the semirings on the resulting semantics. As such, we\nprovide a unifying framework that captures existing approaches and allows\nextending them with a more expressive language.", "comment": "Full version, including proofs and appendices, of paper accepted at\n  IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.16067v1", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15907", "title": "Dual Turing Test: A Framework for Detecting and Mitigating Undetectable AI", "authors": ["Alberto Messina"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15907v1", "summary": "In this short note, we propose a unified framework that bridges three areas:\n(1) a flipped perspective on the Turing Test, the \"dual Turing test\", in which\na human judge's goal is to identify an AI rather than reward a machine for\ndeception; (2) a formal adversarial classification game with explicit quality\nconstraints and worst-case guarantees; and (3) a reinforcement learning (RL)\nalignment pipeline that uses an undetectability detector and a set of quality\nrelated components in its reward model. We review historical precedents, from\ninverted and meta-Turing variants to modern supervised reverse-Turing\nclassifiers, and highlight the novelty of combining quality thresholds, phased\ndifficulty levels, and minimax bounds. We then formalize the dual test: define\nthe judge's task over N independent rounds with fresh prompts drawn from a\nprompt space Q, introduce a quality function Q and parameters tau and delta,\nand cast the interaction as a two-player zero-sum game over the adversary's\nfeasible strategy set M. Next, we map this minimax game onto an RL-HF style\nalignment loop, in which an undetectability detector D provides negative reward\nfor stealthy outputs, balanced by a quality proxy that preserves fluency.\nThroughout, we include detailed explanations of each component notation, the\nmeaning of inner minimization over sequences, phased tests, and iterative\nadversarial training and conclude with a suggestion for a couple of immediate\nactions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15907v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16117", "title": "BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation", "authors": ["Eden Wu", "Dishita G Turakhia", "Guande Wu", "Christos Koutras", "Sarah Keegan", "Wenke Liu", "Beata Szeitz", "David Fenyo", "Cludio T. Silva", "Juliana Freire"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 9 figures. Accepted to IEEE VIS 2025 (Full Papers Track, submission ID 1204)", "url": "http://arxiv.org/abs/2507.16117v1", "summary": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches.", "comment": "11 pages, 9 figures. Accepted to IEEE VIS 2025 (Full Papers Track,\n  submission ID 1204)", "pdf_url": "http://arxiv.org/pdf/2507.16117v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16499", "title": "Active RISs: Modeling and Optimization", "authors": ["Recep Akif Tasci", "Panagiotis Gavriilidis", "Ertugrul Basar", "George C. Alexandropoulos"], "categories": ["cs.IT", "cs.ET", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      43 pages, 15 figures, Book chapter", "url": "http://arxiv.org/abs/2507.16499v1", "summary": "Reconfigurable Intelligent Surfaces (RIS)-empowered communication has emerged\nas a transformative technology for next generation wireless networks, enabling\nthe programmable shaping of the propagation environment. However, conventional\nRISs are fundamentally limited by the double path loss effect, which severely\nattenuates the reflected signals. To overcome this, active RIS architectures,\ncapable of amplifying impinging signals, have been proposed. This chapter\ninvestigates the modeling, performance analysis, and optimization of active\nRISs, focusing on two hardware designs: a dual-RIS structure with a single\nPower Amplifier (PA), and a reflection amplification structure at the unit cell\nlevel using tunnel diodes. For the PA-based design, a comprehensive\nmathematical model is developed, and closed-form expressions for the received\nsignal-to-noise ratio, bit error probability, and Energy Efficiency (EE) are\nderived. An optimization framework for configuring the phase shifts and\namplifier gain is proposed to maximize system capacity under power constraints.\nRegarding the second design, the integration of a tunnel diode into the unit\ncell is carefully studied by analyzing its I-V characteristic, enabling the\nderivation of the negative resistance range and the power consumption model.\nFurthermore, the intrinsic phase-amplitude coupling of the reflection\ncoefficient is characterized through compact linear algebra formulations,\nenabling practical optimization of active RISs. Extensive numerical simulations\nvalidate the theoretical analyses, demonstrating that active RISs can\neffectively overcome the double path loss limitation and achieve favorable EE\ntrade-offs compared to passive RISs. Finally, the trade-off between the\navailable power budget and the number of active elements is examined, revealing\nthat a higher number of active elements does not always lead to optimal\nperformance.", "comment": "43 pages, 15 figures, Book chapter", "pdf_url": "http://arxiv.org/pdf/2507.16499v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15888", "title": "PAT++: a cautionary tale about generative visual augmentation for Object Re-identification", "authors": ["Leonardo Santiago Benitez Pereira", "Arathy Jeevan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15888v1", "summary": "Generative data augmentation has demonstrated gains in several vision tasks,\nbut its impact on object re-identification - where preserving fine-grained\nvisual details is essential - remains largely unexplored. In this work, we\nassess the effectiveness of identity-preserving image generation for object\nre-identification. Our novel pipeline, named PAT++, incorporates Diffusion\nSelf-Distillation into the well-established Part-Aware Transformer. Using the\nUrban Elements ReID Challenge dataset, we conduct extensive experiments with\ngenerated images used for both model training and query expansion. Our results\nshow consistent performance degradation, driven by domain shifts and failure to\nretain identity-defining features. These findings challenge assumptions about\nthe transferability of generative models to fine-grained recognition tasks and\nexpose key limitations in current approaches to visual augmentation for\nidentity-preserving applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15888v1", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.15889", "title": "Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing", "authors": ["Noah van der Vleuten"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Master's thesis, University of Amsterdam, 2023 ( this https URL ). Code and experiments available at: this https URL", "url": "http://arxiv.org/abs/2507.15889v1", "summary": "Language models for program synthesis are usually trained and evaluated on\nprogramming competition datasets (MBPP, APPS). However, these datasets are\nlimited in size and quality, while these language models are extremely data\nhungry. Additionally, the language models have a misaligned program synthesis\nprocess compared to humans. While humans iteratively develop code with the help\nof a compiler, most program synthesis models currently produce code in one go.\nTo solve these issues, we introduce a bootstrapping algorithm for program\nsynthesis, that supports teaching models how to repair. We show that\nbootstrapping consistently outperforms regular fine-tuning. Compared to other\nwork, our bootstrapped model performs on par with fine-tuned models that are\n68\\% larger. Notably, bootstrapping with repairing also improves non-repairing\nperformance compared to regular bootstrapping during inference. However, on our\nmodels, repairing during inference is likely inferior to simply sampling the\nsame number of solutions. Furthermore, we find that there are issues with the\nexample test cases in the training portion of the APPS dataset that are\nvaluable to the community, as many repairing and reinforcement learning methods\nrely on them.", "comment": "Master's thesis, University of Amsterdam, 2023\n  (https://scripties.uba.uva.nl/search?id=record_54126). Code and experiments\n  available at: https://github.com/NoahVl/Dr-Boot", "pdf_url": "http://arxiv.org/pdf/2507.15889v1", "cate": "cs.SE", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2507.11324", "title": "A Review of Privacy Metrics for Privacy-Preserving Synthetic Data Generation", "authors": ["Frederik Marinus Trudslev", "Matteo Lissandrini", "Juan Manuel Rodriguez", "Martin Bgsted", "Daniele Dell'Aglio"], "categories": ["cs.CR", "cs.DB"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11324v2", "summary": "Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce\nsynthetic datasets from personal data while maintaining privacy and utility.\nDifferential privacy (DP) is the property of a PP-SDG mechanism that\nestablishes how protected individuals are when sharing their sensitive data. It\nis however difficult to interpret the privacy budget ($\\varepsilon$) expressed\nby DP. To make the actual risk associated with the privacy budget more\ntransparent, multiple privacy metrics (PMs) have been proposed to assess the\nprivacy risk of the data. These PMs are utilized in separate studies to assess\nnewly introduced PP-SDG mechanisms. Consequently, these PMs embody the same\nassumptions as the PP-SDG mechanism they were made to assess. Therefore, a\nthorough definition of how these are calculated is necessary. In this work, we\npresent the assumptions and mathematical formulations of 17 distinct privacy\nmetrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11324v2", "cate": "cs.CR", "date": "2025-07-15", "updated": "2025-07-22"}
{"id": "2507.16398", "title": "AI or Human? Understanding Perceptions of Embodied Robots with LLMs", "authors": ["Lavinia Hriscu", "Alberto Sanfeliu", "Anais Garrell"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16398v1", "summary": "The pursuit of artificial intelligence has long been associated to the the\nchallenge of effectively measuring intelligence. Even if the Turing Test was\nintroduced as a means of assessing a system intelligence, its relevance and\napplication within the field of human-robot interaction remain largely\nunderexplored. This study investigates the perception of intelligence in\nembodied robots by performing a Turing Test within a robotic platform. A total\nof 34 participants were tasked with distinguishing between AI- and\nhuman-operated robots while engaging in two interactive tasks: an information\nretrieval and a package handover. These tasks assessed the robot perception and\nnavigation abilities under both static and dynamic conditions. Results indicate\nthat participants were unable to reliably differentiate between AI- and\nhuman-controlled robots beyond chance levels. Furthermore, analysis of\nparticipant responses reveals key factors influencing the perception of\nartificial versus human intelligence in embodied robotic systems. These\nfindings provide insights into the design of future interactive robots and\ncontribute to the ongoing discourse on intelligence assessment in AI-driven\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16398v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16110", "title": "Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization", "authors": ["Shengchao Liu", "Hannan Xu", "Yan Ai", "Huanxin Li", "Yoshua Bengio", "Harry Guo"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16110v1", "summary": "Large language models (LLMs) leverage chain-of-thought (CoT) techniques to\ntackle complex problems, representing a transformative breakthrough in\nartificial intelligence (AI). However, their reasoning capabilities have\nprimarily been demonstrated in solving math and coding problems, leaving their\npotential for domain-specific applications-such as battery discovery-largely\nunexplored. Inspired by the idea that reasoning mirrors a form of guided\nsearch, we introduce ChatBattery, a novel agentic framework that integrates\ndomain knowledge to steer LLMs toward more effective reasoning in materials\ndesign. Using ChatBattery, we successfully identify, synthesize, and\ncharacterize three novel lithium-ion battery cathode materials, which achieve\npractical capacity improvements of 28.8%, 25.2%, and 18.5%, respectively, over\nthe widely used cathode material, LiNi0.8Mn0.1Co0.1O2 (NMC811). Beyond this\ndiscovery, ChatBattery paves a new path by showing a successful LLM-driven and\nreasoning-based platform for battery materials invention. This complete\nAI-driven cycle-from design to synthesis to characterization-demonstrates the\ntransformative potential of AI-driven reasoning in revolutionizing materials\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16110v1", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15917", "title": "HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge Graphs", "authors": ["Adrian Kaiser", "Claudiu Leoveanu-Condrei", "Ryan Gold", "Marius-Constantin Dinu", "Markus Hofmarcher"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures", "url": "http://arxiv.org/abs/2507.15917v1", "summary": "The synergy between symbolic knowledge, often represented by Knowledge Graphs\n(KGs), and the generative capabilities of neural networks is central to\nadvancing neurosymbolic AI. A primary bottleneck in realizing this potential is\nthe difficulty of automating KG construction, which faces challenges related to\noutput reliability, consistency, and verifiability. These issues can manifest\nas structural inconsistencies within the generated graphs, such as the\nformation of disconnected $\\textit{isolated islands}$ of data or the inaccurate\nconflation of abstract classes with specific instances. To address these\nchallenges, we propose HyDRA, a $\\textbf{Hy}$brid-$\\textbf{D}$riven\n$\\textbf{R}$easoning $\\textbf{A}$rchitecture designed for verifiable KG\nautomation. Given a domain or an initial set of documents, HyDRA first\nconstructs an ontology via a panel of collaborative neurosymbolic agents. These\nagents collaboratively agree on a set of competency questions (CQs) that define\nthe scope and requirements the ontology must be able to answer. Given these\nCQs, we build an ontology graph that subsequently guides the automated\nextraction of triplets for KG generation from arbitrary documents. Inspired by\ndesign-by-contracts (DbC) principles, our method leverages verifiable contracts\nas the primary control mechanism to steer the generative process of Large\nLanguage Models (LLMs). To verify the output of our approach, we extend beyond\nstandard benchmarks and propose an evaluation framework that assesses the\nfunctional correctness of the resulting KG by leveraging symbolic verifications\nas described by the neurosymbolic AI framework, $\\textit{SymbolicAI}$. This\nwork contributes a hybrid-driven architecture for improving the reliability of\nautomated KG construction and the exploration of evaluation methods for\nmeasuring the functional integrity of its output. The code is publicly\navailable.", "comment": "8 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.15917v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16207", "title": "A Human-Centered Approach to Identifying Promises, Risks, & Challenges of Text-to-Image Generative AI in Radiology", "authors": ["Katelyn Morrison", "Arpit Mathur", "Aidan Bradshaw", "Tom Wartmann", "Steven Lundi", "Afrooz Zandifar", "Weichang Dai", "Kayhan Batmanghelich", "Motahhare Eslami", "Adam Perer"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures, accepted to AAAI/ACM AIES 2025", "url": "http://arxiv.org/abs/2507.16207v1", "summary": "As text-to-image generative models rapidly improve, AI researchers are making\nsignificant advances in developing domain-specific models capable of generating\ncomplex medical imagery from text prompts. Despite this, these technical\nadvancements have overlooked whether and how medical professionals would\nbenefit from and use text-to-image generative AI (GenAI) in practice. By\ndeveloping domain-specific GenAI without involving stakeholders, we risk the\npotential of building models that are either not useful or even more harmful\nthan helpful. In this paper, we adopt a human-centered approach to responsible\nmodel development by involving stakeholders in evaluating and reflecting on the\npromises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through\nexploratory model prompting activities, we uncover the perspectives of medical\nstudents, radiology trainees, and radiologists on the role that text-to-CT Scan\nGenAI can play across medical education, training, and practice. This\nhuman-centered approach additionally enabled us to surface technical challenges\nand domain-specific risks of generating synthetic medical images. We conclude\nby reflecting on the implications of medical text-to-image GenAI.", "comment": "13 pages, 2 figures, accepted to AAAI/ACM AIES 2025", "pdf_url": "http://arxiv.org/pdf/2507.16207v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16735", "title": "AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy", "authors": ["Laura Moradbakhti", "Dorian Peters", "Jennifer K. Quint", "Bjrn Schuller", "Darren Cook", "Rafael A. Calvo"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET", "K.4.2; J.3"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      7 Tables, 4 Figures", "url": "http://arxiv.org/abs/2507.16735v1", "summary": "Asthma-related deaths in the UK are the highest in Europe, and only 30% of\npatients access basic care. There is a need for alternative approaches to\nreaching people with asthma in order to provide health education,\nself-management support and bridges to care. Automated conversational agents\n(specifically, mobile chatbots) present opportunities for providing alternative\nand individually tailored access to health education, self-management support\nand risk self-assessment. But would patients engage with a chatbot, and what\nfactors influence engagement? We present results from a patient survey (N=1257)\ndevised by a team of asthma clinicians, patients, and technology developers,\nconducted to identify optimal factors for efficacy, value and engagement for a\nchatbot. Results indicate that most adults with asthma (53%) are interested in\nusing a chatbot and the patients most likely to do so are those who believe\ntheir asthma is more serious and who are less confident about self-management.\nResults also indicate enthusiasm for 24/7 access, personalisation, and for\nWhatsApp as the preferred access method (compared to app, voice assistant, SMS\nor website). Obstacles to uptake include security/privacy concerns and\nskepticism of technological capabilities. We present detailed findings and\nconsolidate these into 7 recommendations for developers for optimising efficacy\nof chatbot-based health support.", "comment": "7 Tables, 4 Figures", "pdf_url": "http://arxiv.org/pdf/2507.16735v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15911", "title": "Local Dense Logit Relations for Enhanced Knowledge Distillation", "authors": ["Liuchi Xu", "Kang Liu", "Jinshuai Liu", "Lu Wang", "Lisheng Xu", "Jun Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.15911v1", "summary": "State-of-the-art logit distillation methods exhibit versatility, simplicity,\nand efficiency. Despite the advances, existing studies have yet to delve\nthoroughly into fine-grained relationships within logit knowledge. In this\npaper, we propose Local Dense Relational Logit Distillation (LDRLD), a novel\nmethod that captures inter-class relationships through recursively decoupling\nand recombining logit information, thereby providing more detailed and clearer\ninsights for student learning. To further optimize the performance, we\nintroduce an Adaptive Decay Weight (ADW) strategy, which can dynamically adjust\nthe weights for critical category pairs using Inverse Rank Weighting (IRW) and\nExponential Rank Decay (ERD). Specifically, IRW assigns weights inversely\nproportional to the rank differences between pairs, while ERD adaptively\ncontrols weight decay based on total ranking scores of category pairs.\nFurthermore, after the recursive decoupling, we distill the remaining\nnon-target knowledge to ensure knowledge completeness and enhance performance.\nUltimately, our method improves the student's performance by transferring\nfine-grained knowledge and emphasizing the most critical relationships.\nExtensive experiments on datasets such as CIFAR-100, ImageNet-1K, and\nTiny-ImageNet demonstrate that our method compares favorably with\nstate-of-the-art logit-based distillation approaches. The code will be made\npublicly available.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.15911v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15892", "title": "StaAgent: An Agentic Framework for Testing Static Analyzers", "authors": ["Elijah Nnorom", "Md Basim Uddin Ahmed", "Jiho Shin", "Hung Viet Pham", "Song Wang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15892v1", "summary": "Static analyzers play a critical role in identifying bugs early in the\nsoftware development lifecycle, but their rule implementations are often\nunder-tested and prone to inconsistencies. To address this, we propose\nStaAgent, an agentic framework that harnesses the generative capabilities of\nLarge Language Models (LLMs) to systematically evaluate static analyzer rules.\nStaAgent comprises four specialized agents: a Seed Generation Agent that\ntranslates bug detection rules into concrete, bug-inducing seed programs; a\nCode Validation Agent that ensures the correctness of these seeds; a Mutation\nGeneration Agent that produces semantically equivalent mutants; and an Analyzer\nEvaluation Agent that performs metamorphic testing by comparing the static\nanalyzer's behavior on seeds and their corresponding mutants. By revealing\ninconsistent behaviors, StaAgent helps uncover flaws in rule implementations.\nThis LLM-driven, multi-agent framework offers a scalable and adaptable solution\nto improve the reliability of static analyzers. We evaluated StaAgent with five\nstate-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)\nacross five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,\nInfer, and PMD). The experimental results show that our approach can help\nreveal 64 problematic rules in the latest versions of these five static\nanalyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,\nand 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the\nSOTA baseline. We have reported all the bugs to developers, with two of them\nalready fixed. Three more have been confirmed by developers, while the rest are\nawaiting response. These results demonstrate the effectiveness of our approach\nand underscore the promise of agentic, LLM-driven data synthesis to advance\nsoftware engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15892v1", "cate": "cs.SE", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2507.14893", "title": "A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies", "authors": ["Farzin Renan"], "categories": ["cs.CR", "math.NT", "11T71, 94A60, 68P25, 14G50, 81P94"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14893v2", "summary": "Digital signatures are essential cryptographic tools that provide\nauthentication and integrity in digital communications. However,\nprivacy-sensitive applications, such as e-voting and digital cash, require more\nrestrictive verification models to ensure confidentiality and control. Strong\nDesignated Verifier Signature (SDVS) schemes address this need by enabling the\nsigner to designate a specific verifier, ensuring that only this party can\nvalidate the signature. Existing SDVS constructions are primarily based on\nnumber-theoretic assumptions and are therefore vulnerable to quantum attacks.\nAlthough post-quantum alternatives, particularly those based on lattices, have\nbeen proposed, they often entail large key and signature sizes. In this work,\nwe introduce $\\mathsf{CSI\\text{-}SDVS}$, a novel isogeny-based SDVS scheme that\noffers a compact, quantum-resistant alternative. Our construction builds on the\nideal class group action framework of CSIDH and the signature techniques of\nCSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse\nProblem (MT-GAIP). $\\mathsf{CSI\\text{-}SDVS}$ achieves strong security\nguarantees; namely, Strong Unforgeability under Chosen-Message Attacks\n(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in\nthe random oracle model. Remarkably, both the keys and signatures in\n$\\mathsf{CSI\\text{-}SDVS}$ are of size $\\mathcal{O}(\\lambda)$, representing a\nsignificant improvement over the typical $\\mathcal{O}(\\lambda^2)$ bounds in\nexisting post-quantum SDVS schemes, thereby making it among the most compact\nPQC-based SDVS schemes and the only post-quantum secure construction based on\nisogenies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14893v2", "cate": "cs.CR", "date": "2025-07-20", "updated": "2025-07-22"}
{"id": "2507.16458", "title": "Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing Drones", "authors": ["Yang Xu", "Jess Bautista", "Jos Hinojosa", "Hctor Garca de Marina"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Yang Xu and Jess Bautista contributed equally to this work. In the proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025", "url": "http://arxiv.org/abs/2507.16458v1", "summary": "The autonomous formation flight of fixed-wing drones is hard when the\ncoordination requires the actuation over their speeds since they are critically\nbounded and aircraft are mostly designed to fly at a nominal airspeed. This\npaper proposes an algorithm to achieve formation flights of fixed-wing drones\nwithout requiring any actuation over their speed. In particular, we guide all\nthe drones to travel over specific paths, e.g., parallel straight lines, and we\nsuperpose an oscillatory behavior onto the guiding vector field that drives the\ndrones to the paths. This oscillation enables control over the average velocity\nalong the path, thereby facilitating inter-drone coordination. Each drone\nadjusts its oscillation amplitude distributively in a closed-loop manner by\ncommunicating with neighboring agents in an undirected and connected graph. A\nnovel consensus algorithm is introduced, leveraging a non-negative, asymmetric\nsaturation function. This unconventional saturation is justified since negative\namplitudes do not make drones travel backward or have a negative velocity along\nthe path. Rigorous theoretical analysis of the algorithm is complemented by\nvalidation through numerical simulations and a real-world formation flight.", "comment": "Yang Xu and Jes\\'us Bautista contributed equally to this work. In the\n  proceedings of the IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "pdf_url": "http://arxiv.org/pdf/2507.16458v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16126", "title": "TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task", "authors": ["Michael R. Bock", "Kara Molisee", "Zachary Ozer", "Sumit Shah"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16126v1", "summary": "Can AI file your taxes? Not yet. Calculating US personal income taxes is a\ntask that requires building an understanding of vast amounts of English text\nand using that knowledge to carefully compute results. We propose TaxCalcBench,\na benchmark for determining models' abilities to calculate personal income tax\nreturns given all of the necessary information. Our experiment shows that\nstate-of-the-art models succeed in calculating less than a third of federal\nincome tax returns even on this simplified sample set. Our analysis concludes\nthat models consistently misuse tax tables, make errors in tax calculation, and\nincorrectly determine eligibility. Our findings point to the need for\nadditional infrastructure to apply LLMs to the personal income tax calculation\ntask.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16126v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15977", "title": "On the transferability of Sparse Autoencoders for interpreting compressed models", "authors": ["Suchit Gupte", "Vishnu Kabir Chhabra", "Mohammad Mahdi Khalili"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15977v1", "summary": "Modern LLMs face inference efficiency challenges due to their scale. To\naddress this, many compression methods have been proposed, such as pruning and\nquantization. However, the effect of compression on a model's interpretability\nremains elusive. While several model interpretation approaches exist, such as\ncircuit discovery, Sparse Autoencoders (SAEs) have proven particularly\neffective in decomposing a model's activation space into its feature basis. In\nthis work, we explore the differences in SAEs for the original and compressed\nmodels. We find that SAEs trained on the original model can interpret the\ncompressed model albeit with slight performance degradation compared to the\ntrained SAE on the compressed model. Furthermore, simply pruning the original\nSAE itself achieves performance comparable to training a new SAE on the pruned\nmodel. This finding enables us to mitigate the extensive training costs of\nSAEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15977v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16258", "title": "Animal Interaction with Autonomous Mobility Systems: Designing for Multi-Species Coexistence", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Marius Hoggenmueller", "Callum Paker", "Paul Schmitt", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16258v1", "summary": "Autonomous mobility systems increasingly operate in environments shared with\nanimals, from urban pets to wildlife. However, their design has largely focused\non human interaction, with limited understanding of how non-human species\nperceive, respond to, or are affected by these systems. Motivated by research\nin Animal-Computer Interaction (ACI) and more-than-human design, this study\ninvestigates animal interactions with autonomous mobility through a\nmulti-method approach combining a scoping review (45 articles), online\nethnography (39 YouTube videos and 11 Reddit discussions), and expert\ninterviews (8 participants). Our analysis surfaces five key areas of concern:\nPhysical Impact (e.g., collisions, failures to detect), Behavioural Effects\n(e.g., avoidance, stress), Accessibility Concerns (particularly for service\nanimals), Ethics and Regulations, and Urban Disturbance. We conclude with\ndesign and policy directions aimed at supporting multispecies coexistence in\nthe age of autonomous systems. This work underscores the importance of\nincorporating non-human perspectives to ensure safer, more inclusive futures\nfor all species.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16258v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16767", "title": "Multi-RIS-Empowered Communication Systems: Capacity Analysis and Optimization", "authors": ["Aris L. Moustakas", "George C. Alexandropoulos"], "categories": ["cs.IT", "cs.ET", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      24 pages, 5 figures, book chapter", "url": "http://arxiv.org/abs/2507.16767v1", "summary": "In this chapter, using statistical physics methods, asymptotic closed-form\nexpressions for the mean and variance of the mutual information for a\nmulti-antenna transmitter-receiver pair in the presence of multiple\nReconfigurable Intelligent Surfaces (RISs) are presented. While nominally valid\nin the large-system limit, it is shown that the derived Gaussian approximation\nfor the mutual information can be quite accurate, even for modest-sized antenna\narrays and metasurfaces. The above results are particularly useful when\nfast-fading conditions are present, which renders channel estimation\nchallenging. The derived analysis indicates that, when the channel close to an\nRIS is correlated, for instance due to small angle spread which is reasonable\nfor wireless systems with increasing carrier frequencies, the communication\nlink benefits significantly from statistical RIS optimization, resulting in\ngains that are surprisingly higher than the nearly uncorrelated case. More\nimportantly, the presented novel asymptotic properties of the correlation\nmatrices of the impinging and outgoing signals at the RISs can be deployed to\noptimize the metasurfaces without brute-force numerical optimization. The\nnumerical investigation demonstrates that, when the desired reflection from any\nof the RISs departs significantly from geometrical optics, the metasurfaces can\nbe optimized to provide robust communication links, without significant need\nfor their optimal placement.", "comment": "24 pages, 5 figures, book chapter", "pdf_url": "http://arxiv.org/pdf/2507.16767v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15915", "title": "An empirical study for the early detection of Mpox from skin lesion images using pretrained CNN models leveraging XAI technique", "authors": ["Mohammad Asifur Rahim", "Muhammad Nazmul Arefin", "Md. Mizanur Rahman", "Md Ali Hossain", "Ahmed Moustafa"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15915v1", "summary": "Context: Mpox is a zoonotic disease caused by the Mpox virus, which shares\nsimilarities with other skin conditions, making accurate early diagnosis\nchallenging. Artificial intelligence (AI), especially Deep Learning (DL), has a\nstrong tool for medical image analysis; however, pre-trained models like CNNs\nand XAI techniques for mpox detection is underexplored. Objective: This study\naims to evaluate the effectiveness of pre-trained CNN models (VGG16, VGG19,\nInceptionV3, MobileNetV2) for the early detection of monkeypox using binary and\nmulti-class datasets. It also seeks to enhance model interpretability using\nGrad-CAM an XAI technique. Method: Two datasets, MSLD and MSLD v2.0, were used\nfor training and validation. Transfer learning techniques were applied to\nfine-tune pre-trained CNN models by freezing initial layers and adding custom\nlayers for adapting the final features for mpox detection task and avoid\noverfitting. Models performance were evaluated using metrics such as accuracy,\nprecision, recall, F1-score and ROC. Grad-CAM was utilized for visualizing\ncritical features. Results: InceptionV3 demonstrated the best performance on\nthe binary dataset with an accuracy of 95%, while MobileNetV2 outperformed on\nthe multi-class dataset with an accuracy of 93%. Grad-CAM successfully\nhighlighted key image regions. Despite high accuracy, some models showed\noverfitting tendencies, as videnced by discrepancies between training and\nvalidation losses. Conclusion: This study underscores the potential of\npre-trained CNN models in monkeypox detection and the value of XAI techniques.\nFuture work should address dataset limitations, incorporate multimodal data,\nand explore additional interpretability techniques to improve diagnostic\nreliability and model transparency", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15915v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16037", "title": "A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights", "authors": ["Zhili Zeng", "Kimya Khakzad Shahandashti", "Alvine Boaye Belle", "Song Wang", "Zhen Ming", "Jiang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16037v1", "summary": "The rapid advancement of mobile applications has led to a significant demand\nfor cross-platform compatibility, particularly between the Android and iOS\nplatforms. Traditional approaches to mobile application translation often rely\non manual intervention or rule-based systems, which are labor-intensive and\ntime-consuming. While recent advancements in machine learning have introduced\nautomated methods, they often lack contextual understanding and adaptability,\nresulting in suboptimal translations. Large Language Models (LLMs) were\nrecently leveraged to enhance code translation at different granularities,\nincluding the method, class, and repository levels. Researchers have\ninvestigated common errors, limitations, and potential strategies to improve\nthese tasks. However, LLM-based application translation across different\nplatforms, such as migrating mobile applications between Android and iOS or\nadapting software across diverse frameworks, remains underexplored.\nUnderstanding the performance, strengths, and limitations of LLMs in\ncross-platform application translation is critical for advancing software\nengineering automation. This study aims to fill this gap by evaluating\nLLM-based agentic approaches for mobile application translation, identifying\nkey failure points, and proposing guidelines to improve translation\nperformance. We developed a chain of agents that account for dependencies,\nspecifications, program structure, and program control flow when translating\napplications from Android to iOS. To evaluate the performance, we manually\nexamined the translated code for syntactic correctness, semantic accuracy, and\nfunctional completeness. For translation failures, we further conducted a\ndetailed root cause analysis to understand the underlying limitations of the\nagentic translation process and identify opportunities for improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16037v1", "cate": "cs.SE", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16046", "title": "Belief Alignment vs Opinion Leadership: Understanding Cross-linguistic Digital Activism in K-pop and BLM Communities", "authors": ["Yuheun Kim", "Joshua Introne"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures, Accepted for International AAAI Conference on Web and Social Media (ICWSM) 2026", "url": "http://arxiv.org/abs/2507.16046v1", "summary": "The internet has transformed activism, giving rise to more organic, diverse,\nand dynamic social movements that transcend geo-political boundaries. Despite\nextensive research on the role of social media and the internet in\ncross-cultural activism, the fundamental motivations driving these global\nmovements remain poorly understood. This study examines two plausible\nexplanations for cross-cultural activism: first, that it is driven by\ninfluential online opinion leaders, and second, that it results from\nindividuals resonating with emergent sets of beliefs, values, and norms. We\nconduct a case study of the interaction between K-pop fans and the Black Lives\nMatter (BLM) movement on Twitter following the murder of George Floyd. Our\nfindings provide strong evidence that belief alignment, where people resonate\nwith common beliefs, is a primary driver of cross-cultural interactions in\ndigital activism. We also demonstrate that while the actions of potential\nopinion leaders--in this case, K-pop entertainers--may amplify activism and\nlead to further expressions of love and admiration from fans, they do not\nappear to be a direct cause of activism. Finally, we report some initial\nevidence that the interaction between BLM and K-pop led to slight increases in\ntheir overall belief similarity.", "comment": "11 pages, 5 figures, Accepted for International AAAI Conference on\n  Web and Social Media (ICWSM) 2026", "pdf_url": "http://arxiv.org/pdf/2507.16046v1", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2403.07842", "title": "DP-TLDM: Differentially Private Tabular Latent Diffusion Model", "authors": ["Chaoyi Zhu", "Jiayi Tang", "Juan F. Prez", "Marten van Dijk", "Lydia Y. Chen"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.07842v2", "summary": "Synthetic data from generative models emerges as the privacy-preserving data\nsharing solution. Such a synthetic data set shall resemble the original data\nwithout revealing identifiable private information. Till date, the prior focus\non limited types of tabular synthesizers and a small number of privacy attacks,\nparticularly on Generative Adversarial Networks, and overlooks membership\ninference attacks and defense strategies, i.e., differential privacy. Motivated\nby the conundrum of keeping high data quality and low privacy risk of synthetic\ndata tables, we propose DPTLDM, Differentially Private Tabular Latent Diffusion\nModel, which is composed of an autoencoder network to encode the tabular data\nand a latent diffusion model to synthesize the latent tables. Following the\nemerging f-DP framework, we apply DP-SGD to train the auto-encoder in\ncombination with batch clipping and use the separation value as the privacy\nmetric to better capture the privacy gain from DP algorithms. Our empirical\nevaluation demonstrates that DPTLDM is capable of achieving a meaningful\ntheoretical privacy guarantee while also significantly enhancing the utility of\nsynthetic data. Specifically, compared to other DP-protected tabular generative\nmodels, DPTLDM improves the synthetic quality by an average of 35% in data\nresemblance, 15% in the utility for downstream tasks, and 50% in data\ndiscriminability, all while preserving a comparable level of privacy risk.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.07842v2", "cate": "cs.LG", "date": "2024-03-12", "updated": "2025-07-21"}
{"id": "2507.16481", "title": "Guided Reinforcement Learning for Omnidirectional 3D Jumping in Quadruped Robots", "authors": ["Riccardo Bussola", "Michele Focchi", "Giulio Turrisi", "Claudio Semini", "Luigi Palopoli"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16481v1", "summary": "Jumping poses a significant challenge for quadruped robots, despite being\ncrucial for many operational scenarios. While optimisation methods exist for\ncontrolling such motions, they are often time-consuming and demand extensive\nknowledge of robot and terrain parameters, making them less robust in\nreal-world scenarios. Reinforcement learning (RL) is emerging as a viable\nalternative, yet conventional end-to-end approaches lack efficiency in terms of\nsample complexity, requiring extensive training in simulations, and\npredictability of the final motion, which makes it difficult to certify the\nsafety of the final motion. To overcome these limitations, this paper\nintroduces a novel guided reinforcement learning approach that leverages\nphysical intuition for efficient and explainable jumping, by combining B\\'ezier\ncurves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive\nsimulation and experimental results clearly demonstrate the advantages of our\napproach over existing alternatives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16481v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16145", "title": "SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting", "authors": ["Shuhao Mei", "Yongchao Long", "Shan Cao", "Xiaobo Han", "Shijia Geng", "Jinbo Sun", "Yuxi Zhou", "Shenda Hong"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16145v1", "summary": "Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory\ndisease with persistent airflow limitation, is a leading global cause of\ndisability and mortality. Respiratory spirogram time series, routinely\ncollected during pulmonary function tests (PFTs), play a critical role in the\nearly detection of repsiratory diseases and in monitoring lung function over\ntime. However, most current AI models for COPD diagnosis are limited to\noutputting classification results without providing a rationale for their\ndiagnostic process, while current Large Language Models (LLMs) cannot\nunderstand spirograms yet, which severely limits their clinical trust and\nadoption. To tackle this challenge, we leverage a cohort of 234,028 individuals\nfrom the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large\nlanguage model that can understand spirogram. The model extracts morphological\nfeatures from respiratory curves via a SpiroEncoder and aligns them with PFT\nnumerical values in a unified latent space using a SpiroProjector, ultimately\nempowering a large language model to generate a comprehensive diagnostic\nreport. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC\nof 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,\nit maintained a 100% valid response rate, far surpassing the 13.4% of a\ntext-only model and showcasing the superiority of its multimodal design. This\nwork demonstrates the substantial potential of deeply fusing physiological\nsignals with large language models, establishing a new paradigm for the next\ngeneration of interpretable and reliable clinical decision support tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16145v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15987", "title": "Semantic-Aware Gaussian Process Calibration with Structured Layerwise Kernels for Deep Neural Networks", "authors": ["Kyung-hwan Lee", "Kyung-tae Kim"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15987v1", "summary": "Calibrating the confidence of neural network classifiers is essential for\nquantifying the reliability of their predictions during inference. However,\nconventional Gaussian Process (GP) calibration methods often fail to capture\nthe internal hierarchical structure of deep neural networks, limiting both\ninterpretability and effectiveness for assessing predictive reliability. We\npropose a Semantic-Aware Layer-wise Gaussian Process (SAL-GP) framework that\nmirrors the layered architecture of the target neural network. Instead of\napplying a single global GP correction, SAL-GP employs a multi-layer GP model,\nwhere each layer's feature representation is mapped to a local calibration\ncorrection. These layerwise GPs are coupled through a structured multi-layer\nkernel, enabling joint marginalization across all layers. This design allows\nSAL-GP to capture both local semantic dependencies and global calibration\ncoherence, while consistently propagating predictive uncertainty through the\nnetwork. The resulting framework enhances interpretability aligned with the\nnetwork architecture and enables principled evaluation of confidence\nconsistency and uncertainty quantification in deep models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15987v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16466", "title": "SceneLoom: Communicating Data with Scene Context", "authors": ["Lin Gao", "Leixian Shen", "Yuheng Zhao", "Jiexiang Lan", "Huamin Qu", "Siming Chen"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16466v1", "summary": "In data-driven storytelling contexts such as data journalism and data videos,\ndata visualizations are often presented alongside real-world imagery to support\nnarrative context. However, these visualizations and contextual images\ntypically remain separated, limiting their combined narrative expressiveness\nand engagement. Achieving this is challenging due to the need for fine-grained\nalignment and creative ideation. To address this, we present SceneLoom, a\nVision-Language Model (VLM)-powered system that facilitates the coordination of\ndata visualization with real-world imagery based on narrative intents. Through\na formative study, we investigated the design space of coordination\nrelationships between data visualization and real-world scenes from the\nperspectives of visual alignment and semantic coherence. Guided by the derived\ndesign considerations, SceneLoom leverages VLMs to extract visual and semantic\nfeatures from scene images and data visualization, and perform design mapping\nthrough a reasoning process that incorporates spatial organization, shape\nsimilarity, layout consistency, and semantic binding. The system generates a\nset of contextually expressive, image-driven design alternatives that achieve\ncoherent alignments across visual, semantic, and data dimensions. Users can\nexplore these alternatives, select preferred mappings, and further refine the\ndesign through interactive adjustments and animated transitions to support\nexpressive data communication. A user study and an example gallery validate\nSceneLoom's effectiveness in inspiring creative design and facilitating design\nexternalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16466v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2408.10872", "title": "V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using the iRAP Standard?", "authors": ["Natchapon Jongwiriyanurak", "Zichao Zeng", "June Moh Goo", "Xinglei Wang", "Ilya Ilyankou", "Kerkritt Sriroongvikrai", "Nicola Christie", "Meihui Wang", "Huanfa Chen", "James Haworth"], "categories": ["cs.CV", "cs.AI", "cs.ET"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.10872v4", "summary": "Road safety assessments are critical yet costly, especially in Low- and\nMiddle-Income Countries (LMICs), where most roads remain unrated. Traditional\nmethods require expert annotation and training data, while supervised\nlearning-based approaches struggle to generalise across regions. In this paper,\nwe introduce \\textit{V-RoAst}, a zero-shot Visual Question Answering (VQA)\nframework using Vision-Language Models (VLMs) to classify road safety\nattributes defined by the iRAP standard. We introduce the first open-source\ndataset from ThaiRAP, consisting of over 2,000 curated street-level images from\nThailand annotated for this task. We evaluate Gemini-1.5-flash and GPT-4o-mini\non this dataset and benchmark their performance against VGGNet and ResNet\nbaselines. While VLMs underperform on spatial awareness, they generalise well\nto unseen classes and offer flexible prompt-based reasoning without retraining.\nOur results show that VLMs can serve as automatic road assessment tools when\nintegrated with complementary data. This work is the first to explore VLMs for\nzero-shot infrastructure risk assessment and opens new directions for\nautomatic, low-cost road safety mapping. Code and dataset:\nhttps://github.com/PongNJ/V-RoAst.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.10872v4", "cate": "cs.CV", "date": "2024-08-20", "updated": "2025-07-22"}
{"id": "2507.15961", "title": "A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications", "authors": ["Ahmed Aman Ibrahim", "Hamad Mansour Alawar", "Abdulnasser Abbas Zehi", "Ahmed Mohammad Alkendi", "Bilal Shafi Ashfaq Ahmed Mirza", "Shan Ullah", "Ismail Lujain Jaleel", "Hassan Ugail"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15961v1", "summary": "Face image quality plays a critical role in determining the accuracy and\nreliability of face verification systems, particularly in real-time screening\napplications such as surveillance, identity verification, and access control.\nLow-quality face images, often caused by factors such as motion blur, poor\nlighting conditions, occlusions, and extreme pose variations, significantly\ndegrade the performance of face recognition models, leading to higher false\nrejection and false acceptance rates. In this work, we propose a lightweight\nyet effective framework for automatic face quality assessment, which aims to\npre-filter low-quality face images before they are passed to the verification\npipeline. Our approach utilises normalised facial landmarks in conjunction with\na Random Forest Regression classifier to assess image quality, achieving an\naccuracy of 96.67\\%. By integrating this quality assessment module into the\nface verification process, we observe a substantial improvement in performance,\nincluding a comfortable 99.7\\% reduction in the false rejection rate and\nenhanced cosine similarity scores when paired with the ArcFace face\nverification model. To validate our approach, we have conducted experiments on\na real-world dataset collected comprising over 600 subjects captured from CCTV\nfootage in unconstrained environments within Dubai Police. Our results\ndemonstrate that the proposed framework effectively mitigates the impact of\npoor-quality face images, outperforming existing face quality assessment\ntechniques while maintaining computational efficiency. Moreover, the framework\nspecifically addresses two critical challenges in real-time screening:\nvariations in face resolution and pose deviations, both of which are prevalent\nin practical surveillance scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15961v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16044", "title": "Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs", "authors": ["Meriem Mastouri", "Emna Ksontini", "Wael Kessentini"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16044v1", "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16044v1", "cate": "cs.SE", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16298", "title": "WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections", "authors": ["Gautam Kishore Shahi", "Scot A. Hale"], "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16298v1", "summary": "WhatsApp tiplines, first launched in 2019 to combat misinformation, enable\nusers to interact with fact-checkers to verify misleading content. This study\nanalyzes 580 unique claims (tips) from 451 users, covering both high-resource\nlanguages (English, Hindi) and a low-resource language (Telugu) during the 2021\nIndian assembly elections using a mixed-method approach. We categorize the\nclaims into three categories, election, COVID-19, and others, and observe\nvariations across languages. We compare content similarity through frequent\nword analysis and clustering of neural sentence embeddings. We also investigate\nuser overlap across languages and fact-checking organizations. We measure the\naverage time required to debunk claims and inform tipline users. Results reveal\nsimilarities in claims across languages, with some users submitting tips in\nmultiple languages to the same fact-checkers. Fact-checkers generally require a\ncouple of days to debunk a new claim and share the results with users. Notably,\nno user submits claims to multiple fact-checking organizations, indicating that\neach organization maintains a unique audience. We provide practical\nrecommendations for using tiplines during elections with ethical consideration\nof users' information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16298v1", "cate": "cs.SI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2410.08073", "title": "Efficient Quantum Pseudorandomness from Hamiltonian Phase States", "authors": ["John Bostanci", "Jonas Haferkamp", "Dominik Hangleiter", "Alexander Poremba"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      53 pages and 1 figure. Proceedings of TQC 2025. Minor revisions. Note: an earlier version of the paper included an analysis of an iterative construction of pseudorandom unitaries. This section has been removed due to a bug", "url": "http://arxiv.org/abs/2410.08073v3", "summary": "Quantum pseudorandomness has found applications in many areas of quantum\ninformation, ranging from entanglement theory, to models of scrambling\nphenomena in chaotic quantum systems, and, more recently, in the foundations of\nquantum cryptography. Kretschmer (TQC '21) showed that both pseudorandom states\nand pseudorandom unitaries exist even in a world without classical one-way\nfunctions. To this day, however, all known constructions require classical\ncryptographic building blocks which are themselves synonymous with the\nexistence of one-way functions, and which are also challenging to realize on\nrealistic quantum hardware.\n  In this work, we seek to make progress on both of these fronts simultaneously\n-- by decoupling quantum pseudorandomness from classical cryptography\naltogether. We introduce a quantum hardness assumption called the Hamiltonian\nPhase State (HPS) problem, which is the task of decoding output states of a\nrandom instantaneous quantum polynomial-time (IQP) circuit. Hamiltonian phase\nstates can be generated very efficiently using only Hadamard gates,\nsingle-qubit Z-rotations and CNOT circuits. We show that the hardness of our\nproblem reduces to a worst-case version of the problem, and we provide evidence\nthat our assumption is plausibly fully quantum; meaning, it cannot be used to\nconstruct one-way functions. We also show information-theoretic hardness when\nonly few copies of HPS are available by proving an approximate $t$-design\nproperty of our ensemble. Finally, we show that our HPS assumption and its\nvariants allow us to efficiently construct many pseudorandom quantum\nprimitives, ranging from pseudorandom states, to quantum pseudoentanglement, to\npseudorandom unitaries, and even primitives such as public-key encryption with\nquantum keys.", "comment": "53 pages and 1 figure. Proceedings of TQC 2025. Minor revisions.\n  Note: an earlier version of the paper included an analysis of an iterative\n  construction of pseudorandom unitaries. This section has been removed due to\n  a bug", "pdf_url": "http://arxiv.org/pdf/2410.08073v3", "cate": "quant-ph", "date": "2024-10-10", "updated": "2025-07-21"}
{"id": "2507.16621", "title": "A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System", "authors": ["Lorenzo Gentilini", "Pierpaolo Serio", "Valentina Donzella", "Lorenzo Pollini"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16621v1", "summary": "Extrinsic Calibration represents the cornerstone of autonomous driving. Its\naccuracy plays a crucial role in the perception pipeline, as any errors can\nhave implications for the safety of the vehicle. Modern sensor systems collect\ndifferent types of data from the environment, making it harder to align the\ndata. To this end, we propose a target-based extrinsic calibration system\ntailored for a multi-LiDAR and multi-camera sensor suite. This system enables\ncross-calibration between LiDARs and cameras with limited prior knowledge using\na custom ChArUco board and a tailored nonlinear optimization method. We test\nthe system with real-world data gathered in a warehouse. Results demonstrated\nthe effectiveness of the proposed method, highlighting the feasibility of a\nunique pipeline tailored for various types of sensors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16621v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16184", "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind (A Position Paper)", "authors": ["Myung Ho Kim"], "categories": ["cs.AI", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2507.16184v1", "summary": "We report the discovery of a structural convergence across four influential\ntheories of mind: Kahneman's dual-system theory, Friston's predictive\nprocessing, Minsky's society of mind, and Clark's extended mind-emerging\nunintentionally within a practical AI agent architecture called Agentic Flow.\nDesigned to address limitations in large language models (LLMs), Agentic Flow\ncomprises five interdependent modules such as Retrieval, Cognition, Control,\nMemory, and Action arranged in a recurrent cognitive loop. Although originally\ninspired only by Minsky and Clark, the system's structure retrospectively\naligns with computational motifs found in all four theories, including\npredictive modeling, associative recall, and error-sensitive control.\n  To assess this convergence, we conducted comparative experiments with\nbaseline LLM agents on multi-step reasoning tasks. The structured agent\nachieved 95.8% task success and exhibited strong constraint adherence, while\nthe baseline system succeeded 62.3% of the time. These results were not aimed\nat proving superiority, but at illustrating how theoretical structures may\nemerge through practical design choices rather than top-down theory.\n  We introduce PEACE as a descriptive meta-architecture that captures\ndesign-level regularities observed in Agentic Flow. Not intended as a new\ntheory, PEACE provides a shared vocabulary for understanding architectures\nshaped by real-world implementation demands. This paper should be read as a\nposition paper - an exploratory reflection on how implementation can surface\nlatent structural echoes of cognitive theory, without asserting theoretical\nunification.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2507.16184v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16008", "title": "Enhancing Stability of Physics-Informed Neural Network Training Through Saddle-Point Reformulation", "authors": ["Dmitry Bylinkin", "Mikhail Aleksandrov", "Savelii Chezhegov", "Aleksandr Beznosikov"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      34 pages, 4 tables, 3 figures, 4 theorems; code available at this https URL", "url": "http://arxiv.org/abs/2507.16008v1", "summary": "Physics-informed neural networks (PINNs) have gained prominence in recent\nyears and are now effectively used in a number of applications. However, their\nperformance remains unstable due to the complex landscape of the loss function.\nTo address this issue, we reformulate PINN training as a nonconvex-strongly\nconcave saddle-point problem. After establishing the theoretical foundation for\nthis approach, we conduct an extensive experimental study, evaluating its\neffectiveness across various tasks and architectures. Our results demonstrate\nthat the proposed method outperforms the current state-of-the-art techniques.", "comment": "34 pages, 4 tables, 3 figures, 4 theorems; code available at\n  https://anonymous.4open.science/r/pinns-bgda-00D6/README.md", "pdf_url": "http://arxiv.org/pdf/2507.16008v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16542", "title": "The Effect of Scale Consistency between Real and Virtual Spaces on Immersion in Exhibition Hybrid Spaces", "authors": ["Qiong Wu", "Yan Dong", "Zipeng Zhang", "Ruochen Hu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      23 pages, 6 figures, submitted to Virtual Reality (Springer)", "url": "http://arxiv.org/abs/2507.16542v1", "summary": "In exhibition hybrid spaces, scale consistency between real and virtual\nspaces is crucial for user immersion. However, there is currently a lack of\nsystematic research to determine appropriate virtual-to-real mapping ratios.\nThis study developed an immersive interaction system based on Intel 3D Athlete\nTracking body mapping technology. Two experiments investigated the impact of\nvirtual space and virtual avatar scale on immersion. Experiment 1 investigated\n30 participants' preferences for virtual space scale, while Experiment 2 tested\nthe effect of 6 different virtual avatar sizes (25%-150%) on immersion. A\n5-point Likert scale was used to assess immersion, followed by analysis of\nvariance and Tukey HSD post-hoc tests. Experiment 1 showed that participants\npreferred a virtual space ratio of 130% (mean 127.29%, SD 8.55%). Experiment 2\nfound that virtual avatar sizes within the 75%-100% range produced optimal\nimmersion (p < 0.05). Immersion decreased significantly when virtual avatar\nsizes deviated from users' actual height (below 50% or above 125%).\nParticipants were more sensitive to size changes in the 25%-75% range, while\nperception was weaker for changes in the 75%-100% range. Virtual environments\nslightly larger than real space (130%) and virtual avatars slightly smaller\nthan users (75%-100%) optimize user immersion. These findings have been applied\nin the Intel Global Trade Center exhibition hall, demonstrating actionable\ninsights for designing hybrid spaces that enhance immersion and coherence.", "comment": "23 pages, 6 figures, submitted to Virtual Reality (Springer)", "pdf_url": "http://arxiv.org/pdf/2507.16542v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16010", "title": "FW-VTON: Flattening-and-Warping for Person-to-Person Virtual Try-on", "authors": ["Zheng Wang", "Xianbing Sun", "Shengyi Wu", "Jiahui Zhan", "Jianlou Si", "Chi Zhang", "Liqing Zhang", "Jianfu Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16010v1", "summary": "Traditional virtual try-on methods primarily focus on the garment-to-person\ntry-on task, which requires flat garment representations. In contrast, this\npaper introduces a novel approach to the person-to-person try-on task. Unlike\nthe garment-to-person try-on task, the person-to-person task only involves two\ninput images: one depicting the target person and the other showing the garment\nworn by a different individual. The goal is to generate a realistic combination\nof the target person with the desired garment. To this end, we propose\nFlattening-and-Warping Virtual Try-On (\\textbf{FW-VTON}), a method that\noperates in three stages: (1) extracting the flattened garment image from the\nsource image; (2) warping the garment to align with the target pose; and (3)\nintegrating the warped garment seamlessly onto the target person. To overcome\nthe challenges posed by the lack of high-quality datasets for this task, we\nintroduce a new dataset specifically designed for person-to-person try-on\nscenarios. Experimental evaluations demonstrate that FW-VTON achieves\nstate-of-the-art performance, with superior results in both qualitative and\nquantitative assessments, and also excels in garment extraction subtasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16010v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16063", "title": "AI-Powered Commit Explorer (APCE)", "authors": ["Yousab Grees", "Polina Iaremchuk", "Ramtin Ehsani", "Esteban Parra", "Preetha Chatterjee", "Sonia Haiduc"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16063v1", "summary": "Commit messages in a version control system provide valuable information for\ndevelopers regarding code changes in software systems. Commit messages can be\nthe only source of information left for future developers describing what was\nchanged and why. However, writing high-quality commit messages is often\nneglected in practice. Large Language Model (LLM) generated commit messages\nhave emerged as a way to mitigate this issue. We introduce the AI-Powered\nCommit Explorer (APCE), a tool to support developers and researchers in the use\nand study of LLM-generated commit messages. APCE gives researchers the option\nto store different prompts for LLMs and provides an additional evaluation\nprompt that can further enhance the commit message provided by LLMs. APCE also\nprovides researchers with a straightforward mechanism for automated and human\nevaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16063v1", "cate": "cs.SE", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16583", "title": "SASH: Decoding Community Structure in Graphs", "authors": ["Allison Beemer", "Jessalyn Bolkema"], "categories": ["cs.SI", "cs.IT", "math.CO", "math.IT"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      5 pages, to appear in the proceedings of the International Symposium on Topics in Coding 2025", "url": "http://arxiv.org/abs/2507.16583v1", "summary": "Detection of communities in a graph entails identifying clusters of densely\nconnected vertices; the area has a variety of important applications and a rich\nliterature. The problem has previously been situated in the realm of error\ncorrecting codes by viewing a graph as a noisy version of the assumed\nunderlying communities. In this paper, we introduce an encoding of community\nstructure along with the resulting code's parameters. We then present a novel\nalgorithm, SASH, to decode to estimated communities given an observed dataset.\nWe demonstrate the performance of SASH via simulations on an assortative\nplanted partition model and on the Zachary's Karate Club dataset.", "comment": "5 pages, to appear in the proceedings of the International Symposium\n  on Topics in Coding 2025", "pdf_url": "http://arxiv.org/pdf/2507.16583v1", "cate": "cs.SI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2411.07231", "title": "Watermark Anything with Localized Messages", "authors": ["Tom Sander", "Pierre Fernandez", "Alain Durmus", "Teddy Furon", "Matthijs Douze"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICLR 2025", "url": "http://arxiv.org/abs/2411.07231v2", "summary": "Image watermarking methods are not tailored to handle small watermarked\nareas. This restricts applications in real-world scenarios where parts of the\nimage may come from different sources or have been edited. We introduce a\ndeep-learning model for localized image watermarking, dubbed the Watermark\nAnything Model (WAM). The WAM embedder imperceptibly modifies the input image,\nwhile the extractor segments the received image into watermarked and\nnon-watermarked areas and recovers one or several hidden messages from the\nareas found to be watermarked. The models are jointly trained at low resolution\nand without perceptual constraints, then post-trained for imperceptibility and\nmultiple watermarks. Experiments show that WAM is competitive with state-of-the\nart methods in terms of imperceptibility and robustness, especially against\ninpainting and splicing, even on high-resolution images. Moreover, it offers\nnew capabilities: WAM can locate watermarked areas in spliced images and\nextract distinct 32-bit messages with less than 1 bit error from multiple small\nregions -- no larger than 10% of the image surface -- even for small 256x256\nimages. Training and inference code and model weights are available at\nhttps://github.com/facebookresearch/watermark-anything.", "comment": "ICLR 2025", "pdf_url": "http://arxiv.org/pdf/2411.07231v2", "cate": "cs.CV", "date": "2024-11-11", "updated": "2025-07-22"}
{"id": "2507.16645", "title": "Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control", "authors": ["Zongzheng Zhang", "Jiawen Yang", "Ziqiao Peng", "Meng Yang", "Jianzhu Ma", "Lin Cheng", "Huazhe Xu", "Hang Zhao", "Hao Zhao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to RSS 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.16645v1", "summary": "Previous animatronic faces struggle to express emotions effectively due to\nhardware and software limitations. On the hardware side, earlier approaches\neither use rigid-driven mechanisms, which provide precise control but are\ndifficult to design within constrained spaces, or tendon-driven mechanisms,\nwhich are more space-efficient but challenging to control. In contrast, we\npropose a hybrid actuation approach that combines the best of both worlds. The\neyes and mouth-key areas for emotional expression-are controlled using rigid\nmechanisms for precise movement, while the nose and cheek, which convey subtle\nfacial microexpressions, are driven by strings. This design allows us to build\na compact yet versatile hardware platform capable of expressing a wide range of\nemotions. On the algorithmic side, our method introduces a self-modeling\nnetwork that maps motor actions to facial landmarks, allowing us to\nautomatically establish the relationship between blendshape coefficients for\ndifferent facial expressions and the corresponding motor control signals\nthrough gradient backpropagation. We then train a neural network to map speech\ninput to corresponding blendshape controls. With our method, we can generate\ndistinct emotional expressions such as happiness, fear, disgust, and anger,\nfrom any given sentence, each with nuanced, emotion-specific control signals-a\nfeature that has not been demonstrated in earlier systems. We release the\nhardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware\nand https://github.com/ZZongzheng0918/Morpheus-Software.", "comment": "Accepted to RSS 2025, Project Page:\n  https://jiawenyang-ch.github.io/Morpheus-Hardware-Design/", "pdf_url": "http://arxiv.org/pdf/2507.16645v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16204", "title": "CHIMERA: Compressed Hybrid Intelligence for Twin-Model Enhanced Multi-Agent Deep Reinforcement Learning for Multi-Functional RIS-Assisted Space-Air-Ground Integrated Networks", "authors": ["Li-Hsiang Shen", "Jyun-Jhe Huang"], "categories": ["cs.AI", "eess.SP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16204v1", "summary": "A space-air-ground integrated network (SAGIN) architecture is proposed,\nempowered by multi-functional reconfigurable intelligent surfaces (MF-RIS)\ncapable of simultaneously reflecting, amplifying, and harvesting wireless\nenergy. The MF-RIS plays a pivotal role in addressing the energy shortages of\nlow-Earth orbit (LEO) satellites operating in shadowed regions, while\nexplicitly accounting for both communication and computing energy consumption\nacross the SAGIN nodes. To maximize the long-term energy efficiency (EE), we\nformulate a joint optimization problem over the MF-RIS parameters, including\nsignal amplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, user association, and computing capability.\nThe formulated problem is highly non-convex and non-linear and contains mixed\ndiscrete-continuous parameters. To tackle this, we conceive a compressed hybrid\nintelligence for twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture achieves superior EE performance due to its\ncomplementary coverage, offering notable advantages over either standalone\nsatellite, aerial, or ground-only deployments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16204v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16012", "title": "Neural Probabilistic Shaping: Joint Distribution Learning for Optical Fiber Communications", "authors": ["Mohammad Taha Askari", "Lutz Lampe", "Amirhossein Ghazisaeidi"], "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      4 pages, 3 figures, Submitted to the 51st European Conference on Optical Communications", "url": "http://arxiv.org/abs/2507.16012v1", "summary": "We present an autoregressive end-to-end learning approach for probabilistic\nshaping on nonlinear fiber channels. Our proposed scheme learns the joint\nsymbol distribution and provides a 0.3-bits/2D achievable information rate gain\nover an optimized marginal distribution for dual-polarized 64-QAM transmission\nover a single-span 205 km link.", "comment": "4 pages, 3 figures, Submitted to the 51st European Conference on\n  Optical Communications", "pdf_url": "http://arxiv.org/pdf/2507.16012v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16562", "title": "Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)", "authors": ["Megha Quamara", "Viktor Schmuck", "Cristina Iani", "Axel Primavesi", "Alexander Plaum", "Luca Vigano"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      26 pages (18 pages main body, 8 pages user consent form), 3 figures, 7 tables", "url": "http://arxiv.org/abs/2507.16562v1", "summary": "In this paper, we present the findings of a user study that evaluated the\nsocial acceptance of eXtended Reality (XR) agent technology, focusing on a\nremotely accessible, web-based XR training system developed for journalists.\nThis system involves user interaction with a virtual avatar, enabled by a\nmodular toolkit. The interactions are designed to provide tailored training for\njournalists in digital-remote settings, especially for sensitive or dangerous\nscenarios, without requiring specialized end-user equipment like headsets. Our\nresearch adapts and extends the Almere model, representing social acceptance\nthrough existing attributes such as perceived ease of use and perceived\nusefulness, along with added ones like dependability and security in the\nuser-agent interaction. The XR agent was tested through a controlled experiment\nin a real-world setting, with data collected on users' perceptions. Our\nfindings, based on quantitative and qualitative measurements involving\nquestionnaires, contribute to the understanding of user perceptions and\nacceptance of XR agent solutions within a specific social context, while also\nidentifying areas for the improvement of XR systems.", "comment": "26 pages (18 pages main body, 8 pages user consent form), 3 figures,\n  7 tables", "pdf_url": "http://arxiv.org/pdf/2507.16562v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16015", "title": "Is Tracking really more challenging in First Person Egocentric Vision?", "authors": ["Matteo Dunnhofer", "Zaira Manigrasso", "Christian Micheloni"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      2025 IEEE/CVF International Conference on Computer Vision (ICCV)", "url": "http://arxiv.org/abs/2507.16015v1", "summary": "Visual object tracking and segmentation are becoming fundamental tasks for\nunderstanding human activities in egocentric vision. Recent research has\nbenchmarked state-of-the-art methods and concluded that first person egocentric\nvision presents challenges compared to previously studied domains. However,\nthese claims are based on evaluations conducted across significantly different\nscenarios. Many of the challenging characteristics attributed to egocentric\nvision are also present in third person videos of human-object activities. This\nraises a critical question: how much of the observed performance drop stems\nfrom the unique first person viewpoint inherent to egocentric vision versus the\ndomain of human-object activities? To address this question, we introduce a new\nbenchmark study designed to disentangle such factors. Our evaluation strategy\nenables a more precise separation of challenges related to the first person\nperspective from those linked to the broader domain of human-object activity\nunderstanding. By doing so, we provide deeper insights into the true sources of\ndifficulty in egocentric tracking and segmentation, facilitating more targeted\nadvancements on this task.", "comment": "2025 IEEE/CVF International Conference on Computer Vision (ICCV)", "pdf_url": "http://arxiv.org/pdf/2507.16015v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16166", "title": "Ten Essential Guidelines for Building High-Quality Research Software", "authors": ["Nasir U. Eisty", "David E. Bernholdt", "Alex Koufos", "David J. Luet", "Miranda Mundt"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16166v1", "summary": "High-quality research software is a cornerstone of modern scientific\nprogress, enabling researchers to analyze complex data, simulate phenomena, and\nshare reproducible results. However, creating such software requires adherence\nto best practices that ensure robustness, usability, and sustainability. This\npaper presents ten guidelines for producing high-quality research software,\ncovering every stage of the development lifecycle. These guidelines emphasize\nthe importance of planning, writing clean and readable code, using version\ncontrol, and implementing thorough testing strategies. Additionally, they\naddress key principles such as modular design, reproducibility, performance\noptimization, and long-term maintenance. The paper also highlights the role of\ndocumentation and community engagement in enhancing software usability and\nimpact. By following these guidelines, researchers can create software that\nadvances their scientific objectives and contributes to a broader ecosystem of\nreliable and reusable research tools. This work serves as a practical resource\nfor researchers and developers aiming to elevate the quality and impact of\ntheir research software.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16166v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16541", "title": "A Comprehensive Data-centric Overview of Federated Graph Learning", "authors": ["Zhengyu Wu", "Xunkai Li", "Yinlin Zhu", "Zekai Chen", "Guochen Yan", "Yanyu Yan", "Hao Zhang", "Yuming Ai", "Xinmo Jin", "Rong-Hua Li", "Guoren Wang"], "categories": ["cs.LG", "cs.AI", "cs.SI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16541v1", "summary": "In the era of big data applications, Federated Graph Learning (FGL) has\nemerged as a prominent solution that reconcile the tradeoff between optimizing\nthe collective intelligence between decentralized datasets holders and\npreserving sensitive information to maximum. Existing FGL surveys have\ncontributed meaningfully but largely focus on integrating Federated Learning\n(FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that\nemphasis on methodology and simulated scenarios. Notably, a data centric\nperspective, which systematically examines FGL methods through the lens of data\nproperties and usage, remains unadapted to reorganize FGL research, yet it is\ncritical to assess how FGL studies manage to tackle data centric constraints to\nenhance model performances. This survey propose a two-level data centric\ntaxonomy: Data Characteristics, which categorizes studies based on the\nstructural and distributional properties of datasets used in FGL, and Data\nUtilization, which analyzes the training procedures and techniques employed to\novercome key data centric challenges. Each taxonomy level is defined by three\northogonal criteria, each representing a distinct data centric configuration.\nBeyond taxonomy, this survey examines FGL integration with Pretrained Large\nModels, showcases realistic applications, and highlights future direction\naligned with emerging trends in GML.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16541v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16438", "title": "The Sweet Danger of Sugar: Debunking Representation Learning for Encrypted Traffic Classification", "authors": ["Yuqi Zhao", "Giovanni Dettori", "Matteo Boffa", "Luca Vassio", "Marco Mellia"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been accepted at ACM SIGCOMM 2025. It will appear in the proceedings with DOI https://doi.org/10.1145/3718958.3750498", "url": "http://arxiv.org/abs/2507.16438v1", "summary": "Recently we have witnessed the explosion of proposals that, inspired by\nLanguage Models like BERT, exploit Representation Learning models to create\ntraffic representations. All of them promise astonishing performance in\nencrypted traffic classification (up to 98% accuracy). In this paper, with a\nnetworking expert mindset, we critically reassess their performance. Through\nextensive analysis, we demonstrate that the reported successes are heavily\ninfluenced by data preparation problems, which allow these models to find easy\nshortcuts - spurious correlation between features and labels - during\nfine-tuning that unrealistically boost their performance. When such shortcuts\nare not present - as in real scenarios - these models perform poorly. We also\nintroduce Pcap-Encoder, an LM-based representation learning model that we\nspecifically design to extract features from protocol headers. Pcap-Encoder\nappears to be the only model that provides an instrumental representation for\ntraffic classification. Yet, its complexity questions its applicability in\npractical settings. Our findings reveal flaws in dataset preparation and model\ntraining, calling for a better and more conscious test design. We propose a\ncorrect evaluation methodology and stress the need for rigorous benchmarking.", "comment": "This paper has been accepted at ACM SIGCOMM 2025. It will appear in\n  the proceedings with DOI 10.1145/3718958.3750498", "pdf_url": "http://arxiv.org/pdf/2507.16438v1", "cate": "cs.NI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.22890", "title": "CP-uniGuard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems", "authors": ["Senkang Hu", "Yihang Tao", "Guowen Xu", "Xinyuan Qian", "Yiqin Deng", "Xianhao Chen", "Sam Tak Wu Kwong", "Yuguang Fang"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.22890v2", "summary": "Collaborative Perception (CP) has been shown to be a promising technique for\nmulti-agent autonomous driving and multi-agent robotic systems, where multiple\nagents share their perception information to enhance the overall perception\nperformance and expand the perception range. However, in CP, an ego agent needs\nto receive messages from its collaborators, which makes it vulnerable to\nattacks from malicious agents. To address this critical issue, we propose a\nunified, probability-agnostic, and adaptive framework, namely, CP-uniGuard,\nwhich is a tailored defense mechanism for CP deployed by each agent to\naccurately detect and eliminate malicious agents in its collaboration network.\nOur key idea is to enable CP to reach a consensus rather than a conflict\nagainst an ego agent's perception results. Based on this idea, we first develop\na probability-agnostic sample consensus (PASAC) method to effectively sample a\nsubset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define collaborative\nconsistency loss (CCLoss) for object detection task and bird's eye view (BEV)\nsegmentation task to capture the discrepancy between an ego agent and its\ncollaborators, which is used as a verification criterion for consensus. In\naddition, we propose online adaptive threshold via dual sliding windows to\ndynamically adjust the threshold for consensus verification and ensure the\nreliability of the systems in dynamic environments. Finally, we conduct\nextensive experiments and demonstrate the effectiveness of our framework. Code\nwill be released at https://github.com/CP-Security/CP-uniGuard.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22890v2", "cate": "cs.CV", "date": "2025-06-28", "updated": "2025-07-22"}
{"id": "2507.16713", "title": "Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory", "authors": ["Guowei Lan", "Kaixian Qu", "Ren Zurbrgg", "Changan Chen", "Christopher E. Mower", "Haitham Bou-Ammar", "Marco Hutter"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16713v1", "summary": "Vision-language models (VLMs) have been widely adopted in robotics to enable\nautonomous planning. However, grounding VLMs, originally trained on internet\ndata, to diverse real-world robots remains a challenge. This paper presents\nExpTeach, a framework that grounds VLMs to physical robots by building a\nself-generated memory of real-world experiences. In ExpTeach, the VLM\nautonomously plans actions, verifies outcomes, reflects on failures, and adapts\nrobot behaviors in a closed loop. The self-generated experiences during this\nprocess are then summarized into a long-term memory, enabling retrieval of\nlearned knowledge to guide future tasks via retrieval-augmented generation\n(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with\nan on-demand image annotation module. In experiments, we show that reflection\nimproves success rates from 36% to 84% on four challenging robotic tasks and\nobserve the emergence of intelligent object interactions, including creative\ntool use. Across extensive tests on 12 real-world scenarios (including eight\nunseen ones), we find that grounding with long-term memory boosts single-trial\nsuccess rates from 22% to 80%, demonstrating the effectiveness and\ngeneralizability of ExpTeach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16713v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16229", "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery", "authors": ["Bo Wen", "Chen Wang", "Qiwei Han", "Raquel Norel", "Julia Liu", "Thaddeus Stappenbeck", "Jeffrey L. Rogers"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      IEEE International Conference on Digital Health (ICDH) 2025", "url": "http://arxiv.org/abs/2507.16229v1", "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions.", "comment": "IEEE International Conference on Digital Health (ICDH) 2025", "pdf_url": "http://arxiv.org/pdf/2507.16229v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16039", "title": "Reactivation: Empirical NTK Dynamics Under Task Shifts", "authors": ["Yuzhi Liu", "Zixuan Chen", "Zirui Zhang", "Yufei Liu", "Giulia Lanzillotta"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16039v1", "summary": "The Neural Tangent Kernel (NTK) offers a powerful tool to study the\nfunctional dynamics of neural networks. In the so-called lazy, or kernel\nregime, the NTK remains static during training and the network function is\nlinear in the static neural tangents feature space. The evolution of the NTK\nduring training is necessary for feature learning, a key driver of deep\nlearning success. The study of the NTK dynamics has led to several critical\ndiscoveries in recent years, in generalization and scaling behaviours. However,\nthis body of work has been limited to the single task setting, where the data\ndistribution is assumed constant over time. In this work, we present a\ncomprehensive empirical analysis of NTK dynamics in continual learning, where\nthe data distribution shifts over time. Our findings highlight continual\nlearning as a rich and underutilized testbed for probing the dynamics of neural\ntraining. At the same time, they challenge the validity of static-kernel\napproximations in theoretical treatments of continual learning, even at large\nscale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16039v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16563", "title": "Animated Transition between Node-Link and Parallel Coordinates Visualizations", "authors": ["Abdulhaq Adetunji Salako", "Hannes Hagen", "Christian Tominski"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16563v1", "summary": "Multi-faceted data visualization typically involves several dedicated views.\nTo create a comprehensive understanding of the data, users have to mentally\nintegrate the information from the different views. This integration is\nhindered by context switches between views and usually requires interactive\nmethods such as brushing and linking. Animated transitions have also been shown\nto be able to mediate context switches and improve understanding. Yet, most\nexisting animated transitions consider only basic views showing the same data\nfacet. In this work, we study how the gap between node-link diagrams, showing\ngraph structure, and parallel coordinates plots, showing multivariate\nattributes, can be narrowed via smooth animated transitions. Based on two\ndesign goals (traceability and swiftness), we outline a partial design space\nincluding several design options. These inform the implementation of two\nalternative transition variants: a basic variant with plain interpolation and\nan advanced variant that uses our design space and accepted animation\ntechniques, including staging and staggering. In a preliminary study, we asked\nseven participants for qualitative feedback. We found that the swiftness of the\nbasic variant is preferred, while the traceability of data items is better with\nthe slower advanced variant.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16563v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16018", "title": "Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers", "authors": ["Andrew Lu", "Wentinn Liao", "Liuhui Wang", "Huzheng Yang", "Jianbo Shi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16018v1", "summary": "Vision transformers have emerged as a powerful tool across a wide range of\napplications, yet their inner workings remain only partially understood. In\nthis work, we examine the phenomenon of massive tokens - tokens with\nexceptionally high activation norms that act as attention sinks - and artifact\ntokens that emerge as a byproduct during inference. Our analysis reveals that\nthese tokens mutually suppress one another through the attention mechanism,\nplaying a critical role in regulating information flow within the network.\nLeveraging these insights, we introduce Fast Nystr\\\"om Attention (FNA), a\ntraining-free method that approximates self-attention in linear time and space\nby exploiting the structured patterns formed by massive and artifact tokens.\nAdditionally, we propose a masking strategy to mitigate noise from these\ntokens, yielding modest performance gains at virtually no cost. We evaluate our\napproach on popular pretrained vision backbones and demonstrate competitive\nperformance on retrieval, classification, segmentation, and visual question\nanswering (VQA), all while reducing computational overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16018v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16208", "title": "LOCOFY Large Design Models -- Design to code conversion solution", "authors": ["Sohaib Muhammad", "Ashwati Vipin", "Karan Shetti", "Honey Mittal"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16208v1", "summary": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16208v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2405.19383", "title": "Network Analytics for Anti-Money Laundering -- A Systematic Literature Review and Experimental Evaluation", "authors": ["Bruno Deprez", "Toon Vanderschueren", "Bart Baesens", "Tim Verdonck", "Wouter Verbeke"], "categories": ["cs.SI", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.19383v4", "summary": "Money laundering presents a pervasive challenge, burdening society by\nfinancing illegal activities. The use of network information is increasingly\nbeing explored to effectively combat money laundering, given it involves\nconnected parties. This led to a surge in research on network analytics for\nanti-money laundering (AML). The literature is, however, fragmented and a\ncomprehensive overview of existing work is missing. This results in limited\nunderstanding of the methods to apply and their comparative detection power.\nThis paper presents an extensive and unique literature review, based on 97\npapers from Web of Science and Scopus, resulting in a taxonomy following a\nrecently proposed fraud analytics framework. We conclude that most research\nrelies on expert-based rules and manual features, while deep learning methods\nhave been gaining traction. This paper also presents a comprehensive framework\nto evaluate and compare the performance of prominent methods in a standardized\nsetup. We compare manual feature engineering, random walk-based, and deep\nlearning methods on two publicly available data sets. We conclude that (1)\nnetwork analytics increases the predictive power, but caution is needed when\napplying GNNs in the face of class imbalance and network topology, and that (2)\ncare should be taken with synthetic data as this can give overly optimistic\nresults. The open-source implementation facilitates researchers and\npractitioners to extend this work on proprietary data, promoting a standardised\napproach for the analysis and evaluation of network analytics for AML.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.19383v4", "cate": "cs.SI", "date": "2024-05-29", "updated": "2025-07-22"}
{"id": "2507.16594", "title": "An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes", "authors": ["Zied Jenhani", "Mounir Bensalem", "Jasenka Dizdarevi", "Admela Jukan"], "categories": ["cs.NI", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper is uploaded here for research community, thus it is for non-commercial purposes", "url": "http://arxiv.org/abs/2507.16594v1", "summary": "Running deep learning inference directly on ultra-low-power edge/IoT nodes\nhas been limited by the tight memory and compute budgets of microcontrollers.\nSplit learning (SL) addresses this limitation in which it executes part of the\ninference process on the sensor and off-loads the remainder to a companion\ndevice. In the context of constrained devices and the related impact of\nlow-power, over-the-air transport protocols, the performance of split learning\nremains largely unexplored. TO the best of our knowledge, this paper presents\nthe first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards,\ndesigned to benchmark the over-the-air performance of split learning TinyML in\nedge/IoT environments. We benchmark the performance of a MobileNetV2 image\nrecognition model, which is quantized to 8-bit integers, partitioned, and\ndelivered to the nodes via over-the-air updates. The intermediate activations\nare exchanged through different wireless communication methods: ESP-NOW, BLE,\nand traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on\nidentical hardware. Measurements show that splitting the model after\nblock_16_project_BN layer generates a 5.66 kB tensor that traverses the link in\n3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s.\nESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery\nlife further but increases latency beyond 10s.", "comment": "This paper is uploaded here for research community, thus it is for\n  non-commercial purposes", "pdf_url": "http://arxiv.org/pdf/2507.16594v1", "cate": "cs.NI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.04357", "title": "Static Analysis for Detecting Transaction Conflicts in Ethereum Smart Contracts", "authors": ["Atefeh Zareh Chahoki", "Marco Roveri"], "categories": ["cs.DC", "cs.CR"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04357v2", "summary": "Ethereum smart contracts operate in a concurrent environment where multiple\ntransactions can be submitted simultaneously. However, the Ethereum Virtual\nMachine (EVM) enforces sequential execution of transactions within each block\nto prevent conflicts arising from concurrent access to the same state\nvariables. Although this approach guarantees correct behavior, it limits the\nability of validators to leverage multi-core architectures for faster\ntransaction processing, thus restricting throughput. Existing solutions\nintroduce concurrency by allowing simultaneous transaction execution combined\nwith runtime conflict detection and rollback mechanisms to maintain\ncorrectness. However, these methods incur significant overhead due to\ncontinuous conflict tracking and transaction reversion. Recently, alternative\napproaches have emerged that aim to predict conflicts statically, before\nexecution, by analyzing smart contract code for potential transaction\ninteractions. Despite their promise, there is a lack of comprehensive studies\nthat examine static conflict detection and its broader implications in specific\nsmart contracts. This paper fills this important gap by proposing a novel\nstatic analysis method to detect potential transaction conflicts in Ethereum\nsmart contracts. Our method identifies read-write, write-write, and function\ncall conflicts between transaction pairs by analyzing state variable access\npatterns in Solidity contracts. We implement a tool that parses contract code\nand performs conflict detection. Evaluation on a dataset of real-world Ethereum\nsmart contracts demonstrates that our approach achieves high precision in\nidentifying potential conflicts. By enabling proactive conflict detection, our\ntool supports further design of transaction scheduling strategies that reduce\nruntime failures, enhance validator throughput, and contribute to blockchain\nscalability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04357v2", "cate": "cs.DC", "date": "2025-07-06", "updated": "2025-07-22"}
{"id": "2507.16259", "title": "Physics-aware Truck and Drone Delivery Planning Using Optimization & Machine Learning", "authors": ["Yineng Sun", "Armin Fgenschuh", "Vikrant Vaze"], "categories": ["math.OC", "cs.RO"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16259v1", "summary": "Combining an energy-efficient drone with a high-capacity truck for last-mile\npackage delivery can benefit operators and customers by reducing delivery times\nand environmental impact. However, directly integrating drone flight dynamics\ninto the combinatorially hard truck route planning problem is challenging.\nSimplified models that ignore drone flight physics can lead to suboptimal\ndelivery plans. We propose an integrated formulation for the joint problem of\ntruck route and drone trajectory planning and a new end-to-end solution\napproach that combines optimization and machine learning to generate\nhigh-quality solutions in practical online runtimes. Our solution method trains\nneural network predictors based on offline solutions to the drone trajectory\noptimization problem instances to approximate drone flight times, and uses\nthese approximations to optimize the overall truck-and-drone delivery plan by\naugmenting an existing order-first-split-second heuristic. Our method\nexplicitly incorporates key kinematics and energy equations in drone trajectory\noptimization, and thereby outperforms state-of-the-art benchmarks that ignore\ndrone flight physics. Extensive experimentation using synthetic datasets and\nreal-world case studies shows that the integration of drone trajectories into\npackage delivery planning substantially improves system performance in terms of\ntour duration and drone energy consumption. Our modeling and computational\nframework can help delivery planners achieve annual savings worth millions of\ndollars while also benefiting the environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16259v1", "cate": "math.OC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16280", "title": "ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry", "authors": ["Tianze Xu", "Pengrui Lu", "Lyumanshan Ye", "Xiangkun Hu", "Pengfei Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      22 pages, 3 figures", "url": "http://arxiv.org/abs/2507.16280v1", "summary": "The emergence of deep research systems presents significant capabilities in\nproblem-solving, extending from basic queries to sophisticated research tasks.\nHowever, existing benchmarks primarily evaluate these systems as agents for web\nretrieval and report generation, overlooking their potential to discover novel\ninsights on the frontiers of scientific research. To address this gap, we\nintroduce ResearcherBench, the first benchmark focused on evaluating the\ncapabilities of these advanced, agentic systems - which we refer to as Deep AI\nResearch Systems (DARS) - on frontier AI scientific questions. We compiled a\ndataset of 65 research questions expertly selected from real-world scientific\nscenarios such as laboratory discussions and interviews, spanning 35 different\nAI subjects and categorized into three types: technical details, literature\nreview, and open consulting. Our dual evaluation framework combines rubric\nassessment, which uses expert-designed criteria to evaluate insight quality,\nwith factual assessment, which measures citation accuracy (faithfulness) and\ncoverage (groundedness). We evaluated several leading commercial DARS and\nbaseline systems. Results show that OpenAI Deep Research and Gemini Deep\nResearch significantly outperform other systems, with particular strength in\nopen-ended consulting questions. Such capabilities represent a meaningful step\ntoward AI self-improvement, aligning with the vision of ASI for AI. We\nopen-source ResearcherBench to provide a standardized platform for promoting\nthe development of next-generation AI research assistants, hoping to foster a\nnew perspective in AI research evaluation for a novel pattern of scientific\ncollaboration: https://github.com/GAIR-NLP/ResearcherBench.", "comment": "22 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.16280v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16079", "title": "A Lower Bound for the Number of Linear Regions of Ternary ReLU Regression Neural Networks", "authors": ["Yuta Nakahara", "Manabu Kobayashi", "Toshiyasu Matsushima"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16079v1", "summary": "With the advancement of deep learning, reducing computational complexity and\nmemory consumption has become a critical challenge, and ternary neural networks\n(NNs) that restrict parameters to $\\{-1, 0, +1\\}$ have attracted attention as a\npromising approach. While ternary NNs demonstrate excellent performance in\npractical applications such as image recognition and natural language\nprocessing, their theoretical understanding remains insufficient. In this\npaper, we theoretically analyze the expressivity of ternary NNs from the\nperspective of the number of linear regions. Specifically, we evaluate the\nnumber of linear regions of ternary regression NNs with Rectified Linear Unit\n(ReLU) for activation functions and prove that the number of linear regions\nincreases polynomially with respect to network width and exponentially with\nrespect to depth, similar to standard NNs. Moreover, we show that it suffices\nto either square the width or double the depth of ternary NNs to achieve a\nlower bound on the maximum number of linear regions comparable to that of\ngeneral ReLU regression NNs. This provides a theoretical explanation, in some\nsense, for the practical success of ternary NNs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16079v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16586", "title": "AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review", "authors": ["Choro Ulan Uulu", "Mikhail Kulyabin", "Layan Etaiwi", "Nuno Miguel Martins Pacheco", "Jan Joosten", "Kerstin Rse", "Filippos Petridis", "Jan Bosch", "Helena Holmstrm Olsson"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16586v1", "summary": "Computer-Aided Engineering (CAE) enables simulation experts to optimize\ncomplex models, but faces challenges in user experience (UX) that limit\nefficiency and accessibility. While artificial intelligence (AI) has\ndemonstrated potential to enhance CAE processes, research integrating these\nfields with a focus on UX remains fragmented. This paper presents a multivocal\nliterature review (MLR) examining how AI enhances UX in CAE software across\nboth academic research and industry implementations. Our analysis reveals\nsignificant gaps between academic explorations and industry applications, with\ncompanies actively implementing LLMs, adaptive UIs, and recommender systems\nwhile academic research focuses primarily on technical capabilities without UX\nvalidation. Key findings demonstrate opportunities in AI-powered guidance,\nadaptive interfaces, and workflow automation that remain underexplored in\ncurrent research. By mapping the intersection of these domains, this study\nprovides a foundation for future work to address the identified research gaps\nand advance the integration of AI to improve CAE user experience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16586v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16038", "title": "Discovering and using Spelke segments", "authors": ["Rahul Venkatesh", "Klemen Kotar", "Lilian Naing Chen", "Seungwoo Kim", "Luca Thomas Wheeler", "Jared Watrous", "Ashley Xu", "Gia Ancone", "Wanhee Lee", "Honglin Chen", "Daniel Bear", "Stefan Stojanov", "Daniel Yamins"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page at: this https URL", "url": "http://arxiv.org/abs/2507.16038v1", "summary": "Segments in computer vision are often defined by semantic considerations and\nare highly dependent on category-specific conventions. In contrast,\ndevelopmental psychology suggests that humans perceive the world in terms of\nSpelke objects--groupings of physical things that reliably move together when\nacted on by physical forces. Spelke objects thus operate on category-agnostic\ncausal motion relationships which potentially better support tasks like\nmanipulation and planning. In this paper, we first benchmark the Spelke object\nconcept, introducing the SpelkeBench dataset that contains a wide variety of\nwell-defined Spelke segments in natural images. Next, to extract Spelke\nsegments from images algorithmically, we build SpelkeNet, a class of visual\nworld models trained to predict distributions over future motions. SpelkeNet\nsupports estimation of two key concepts for Spelke object discovery: (1) the\nmotion affordance map, identifying regions likely to move under a poke, and (2)\nthe expected-displacement map, capturing how the rest of the scene will move.\nThese concepts are used for \"statistical counterfactual probing\", where diverse\n\"virtual pokes\" are applied on regions of high motion-affordance, and the\nresultant expected displacement maps are used define Spelke segments as\nstatistical aggregates of correlated motion statistics. We find that SpelkeNet\noutperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench.\nFinally, we show that the Spelke concept is practically useful for downstream\napplications, yielding superior performance on the 3DEditBench benchmark for\nphysical object manipulation when used in a variety of off-the-shelf object\nmanipulation models.", "comment": "Project page at: https://neuroailab.github.io/spelke_net", "pdf_url": "http://arxiv.org/pdf/2507.16038v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16327", "title": "Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels", "authors": ["Karoline Nylnder", "Aitor Arrieta", "Shaukat Ali", "Paolo Arcaini"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures. Accepted at GECCO 2025 (Genetic and Evolutionary Computation Conference), July 14-18, 2025, Malaga, Spain", "url": "http://arxiv.org/abs/2507.16327v1", "summary": "Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt\ntheir behaviors to address unexpected situations while maintaining\ndependability requirements. During the design of such AVs, it is crucial to\nunderstand and identify the settings that should trigger adaptations, enabling\nvalidation of their implementation. To this end, we focus on the navigation\nsoftware of AVs, which must adapt their behavior during operation through\nadaptations. AVs often rely on predefined waypoints to guide them along\ndesignated routes, ensuring safe navigation. We propose a multiobjective\nsearch-based approach, called WPgen, to generate minor modifications to the\npredefined set of waypoints, keeping them as close as possible to the original\nwaypoints, while causing the AV to navigate inappropriately when navigating\nwith the generated waypoints. WPgen uses NSGA-II as the multi-objective search\nalgorithm with three seeding strategies for its initial population, resulting\nin three variations of WPgen. We evaluated these variations on three AVs (one\noverwater tanker and two underwater). We compared the three variations of WPgen\nwith Random Search as the baseline and with each other. Experimental results\nshowed that the effectiveness of these variations varied depending on the AV.\nBased on the results, we present the research and practical implications of\nWPgen.", "comment": "9 pages, 3 figures. Accepted at GECCO 2025 (Genetic and Evolutionary\n  Computation Conference), July 14-18, 2025, Malaga, Spain", "pdf_url": "http://arxiv.org/pdf/2507.16327v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2504.07848", "title": "Why We Experience Society Differently: Intrinsic Dispositions as Drivers of Ideological Complexity in Adaptive Social Networks", "authors": ["Akshay Gangadhar", "Hiroki Sayama"], "categories": ["cs.SI", "physics.soc-ph"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.07848v2", "summary": "Understanding the emergence of inequality in complex systems requires\nattention to both structural dynamics and intrinsic heterogeneity. In the\ncontext of opinion dynamics, traditional models relied on static snapshots or\nassumed homogeneous agent behavior, overlooking how diverse cognitive\ndispositions shape belief evolution. While some recent models introduce\nbehavioral heterogeneity, they typically focus on macro-level patterns,\nneglecting the unequal and individualized dynamics that unfold at the agent\nlevel. In this study, we analyze an adaptive social network model where each\nagent exhibits one of three behavioral tendencies-homophily, neophily\n(attention to novelty), or social conformity-and measure the complexity of\nindividual opinion trajectories using normalized Lempel-Ziv (nLZ) complexity.\nWe find that the resulting dynamics are often counterintuitive-homophilic\nagents, despite seeking similarity, become increasingly unpredictable;\nneophilic agents, despite pursuing novelty, stabilize; and conformic agents\nfollow a U-shaped trajectory, transitioning from early stability to later\nunpredictability. More fundamentally, these patterns remain robust across\ndiverse network settings, showing that internal behavioral dispositions - not\nexternal environment - primarily govern long-term opinion unpredictability. The\nbroader implication is that individuals' experiences of ideological volatility,\nuncertainty, or stability are not merely environmental, but endogenously\nself-structured through their own cognitive tendencies. These results establish\na novel individual-level lens on opinion dynamics, where the behavioral\nidentity of agents serves as a dynamical fingerprint in the evolution of belief\nsystems, and gives rise to persistent disparities in dynamical experience\nwithin self-organizing social systems, even in structurally similar\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.07848v2", "cate": "cs.SI", "date": "2025-04-10", "updated": "2025-07-21"}
{"id": "2507.16680", "title": "Latent Space Alignment for AI-Native MIMO Semantic Communications", "authors": ["Mario Edoardo Pandolfo", "Simone Fiorellino", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "categories": ["cs.LG", "cs.IT", "cs.NI", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Proc. of IEEE IJCNN 2025", "url": "http://arxiv.org/abs/2507.16680v1", "summary": "Semantic communications focus on prioritizing the understanding of the\nmeaning behind transmitted data and ensuring the successful completion of tasks\nthat motivate the exchange of information. However, when devices rely on\ndifferent languages, logic, or internal representations, semantic mismatches\nmay occur, potentially hindering mutual understanding. This paper introduces a\nnovel approach to addressing latent space misalignment in semantic\ncommunications, exploiting multiple-input multiple-output (MIMO)\ncommunications. Specifically, our method learns a MIMO precoder/decoder pair\nthat jointly performs latent space compression and semantic channel\nequalization, mitigating both semantic mismatches and physical channel\nimpairments. We explore two solutions: (i) a linear model, optimized by solving\na biconvex optimization problem via the alternating direction method of\nmultipliers (ADMM); (ii) a neural network-based model, which learns semantic\nMIMO precoder/decoder under transmission power budget and complexity\nconstraints. Numerical results demonstrate the effectiveness of the proposed\napproach in a goal-oriented semantic communication scenario, illustrating the\nmain trade-offs between accuracy, communication burden, and complexity of the\nsolutions.", "comment": "Proc. of IEEE IJCNN 2025", "pdf_url": "http://arxiv.org/pdf/2507.16680v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16269", "title": "Improved Wake-Up Time For Euclidean Freeze-Tag Problem", "authors": ["Sharareh Alipour", "Arash Ahadi", "Kajal Baghestani"], "categories": ["cs.CG", "cs.DC", "cs.RO"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16269v1", "summary": "The Freeze-Tag Problem (FTP) involves activating a set of initially asleep\nrobots as quickly as possible, starting from a single awake robot. Once\nactivated, a robot can assist in waking up other robots. Each active robot\nmoves at unit speed. The objective is to minimize the makespan, i.e., the time\nrequired to activate the last robot. A key performance measure is the wake-up\nratio, defined as the maximum time needed to activate any number of robots in\nany primary positions. This work focuses on the geometric (Euclidean) version\nof FTP in $\\mathbb{R}^d$ under the $\\ell_p$ norm, where the initial distance\nbetween each asleep robot and the single active robot is at most 1. For\n$(\\mathbb{R}^2, \\ell_2)$, we improve the previous upper bound of 4.62 ([7],\nCCCG 2024) to 4.31. Note that it is known that 3.82 is a lower bound for the\nwake-up ratio. In $\\mathbb{R}^3$, we propose a new strategy that achieves a\nwake-up ratio of 12 for $(\\mathbb{R}^3, \\ell_1)$ and 12.76 for $(\\mathbb{R}^3,\n\\ell_2)$, improving upon the previous bounds of 13 and $13\\sqrt{3}$,\nrespectively, reported in [2].", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16269v1", "cate": "cs.CG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16296", "title": "Cross-Modal Distillation For Widely Differing Modalities", "authors": ["Cairong Zhao", "Yufeng Jin", "Zifan Song", "Haonan Chen", "Duoqian Miao", "Guosheng Hu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      14 pages, 9 figures", "url": "http://arxiv.org/abs/2507.16296v1", "summary": "Deep learning achieved great progress recently, however, it is not easy or\nefficient to further improve its performance by increasing the size of the\nmodel. Multi-modal learning can mitigate this challenge by introducing richer\nand more discriminative information as input. To solve the problem of limited\naccess to multi-modal data at the time of use, we conduct multi-modal learning\nby introducing a teacher model to transfer discriminative knowledge to a\nstudent model during training. However, this knowledge transfer via\ndistillation is not trivial because the big domain gap between the widely\ndiffering modalities can easily lead to overfitting. In this work, we introduce\na cross-modal distillation framework. Specifically, we find hard constrained\nloss, e.g. l2 loss forcing the student being exact the same as the teacher, can\neasily lead to overfitting in cross-modality distillation. To address this, we\npropose two soft constrained knowledge distillation strategies at the feature\nlevel and classifier level respectively. In addition, we propose a\nquality-based adaptive weights module to weigh input samples via quantified\ndata quality, leading to robust model training. We conducted experiments on\nspeaker recognition and image classification tasks, and the results show that\nour approach is able to effectively achieve knowledge transfer between the\ncommonly used and widely differing modalities of image, text, and speech.", "comment": "14 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.16296v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16099", "title": "TorchAO: PyTorch-Native Training-to-Serving Model Optimization", "authors": ["Andrew Or", "Apurva Jain", "Daniel Vega-Myhre", "Jesse Cai", "Charles David Hernandez", "Zhenrui Zheng", "Driss Guessous", "Vasiliy Kuznetsov", "Christian Puhrsch", "Mark Saroufim", "Supriya Rao", "Thien Tran", "Aleksandar Samardi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures, published in CODEML@ICML25", "url": "http://arxiv.org/abs/2507.16099v1", "summary": "We present TorchAO, a PyTorch-native model optimization framework leveraging\nquantization and sparsity to provide an end-to-end, training-to-serving\nworkflow for AI models. TorchAO supports a variety of popular model\noptimization techniques, including FP8 quantized training, quantization-aware\ntraining (QAT), post-training quantization (PTQ), and 2:4 sparsity, and\nleverages a novel tensor subclass abstraction to represent a variety of\nwidely-used, backend agnostic low precision data types, including INT4, INT8,\nFP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader\necosystem at each step of the model optimization pipeline, from pre-training\n(TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM,\nSGLang, ExecuTorch), connecting an otherwise fragmented space in a single,\nunified workflow. TorchAO has enabled recent launches of the quantized Llama\n3.2 1B/3B and LlamaGuard3-8B models and is open-source at\nhttps://github.com/pytorch/ao/.", "comment": "5 pages, 3 figures, published in CODEML@ICML25", "pdf_url": "http://arxiv.org/pdf/2507.16099v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16130", "title": "Disability Across Cultures: A Human-Centered Audit of Ableism in Western and Indic LLMs", "authors": ["Mahika Phutane", "Aditya Vashistha"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16130v1", "summary": "People with disabilities (PwD) experience disproportionately high levels of\ndiscrimination and hate online, particularly in India, where entrenched stigma\nand limited resources intensify these challenges. Large language models (LLMs)\nare increasingly used to identify and mitigate online hate, yet most research\non online ableism focuses on Western audiences with Western AI models. Are\nthese models adequately equipped to recognize ableist harm in non-Western\nplaces like India? Do localized, Indic language models perform better? To\ninvestigate, we adopted and translated a publicly available ableist speech\ndataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4,\nGemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra,\nAiravata)--to score and explain ableism. In parallel, we recruited 175 PwD from\nboth the U.S. and India to perform the same task, revealing stark differences\nbetween groups. Western LLMs consistently overestimated ableist harm, while\nIndic LLMs underestimated it. Even more concerning, all LLMs were more tolerant\nof ableism when it was expressed in Hindi and asserted Western framings of\nableist harm. In contrast, Indian PwD interpreted harm through intention,\nrelationality, and resilience--emphasizing a desire to inform and educate\nperpetrators. This work provides groundwork for global, inclusive standards of\nableism, demonstrating the need to center local disability experiences in the\ndesign and evaluation of AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16130v1", "cate": "cs.CY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16052", "title": "Disrupting Semantic and Abstract Features for Better Adversarial Transferability", "authors": ["Yuyang Luo", "Xiaosen Wang", "Zhijin Ge", "Yingzhe He"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16052v1", "summary": "Adversarial examples pose significant threats to deep neural networks (DNNs),\nand their property of transferability in the black-box setting has led to the\nemergence of transfer-based attacks, making it feasible to target real-world\napplications employing DNNs. Among them, feature-level attacks, where\nintermediate features are perturbed based on feature importance weight matrix\ncomputed from transformed images, have gained popularity. In this work, we find\nthat existing feature-level attacks primarily manipulate the semantic\ninformation to derive the weight matrix. Inspired by several works that find\nCNNs tend to focus more on high-frequency components (a.k.a. abstract features,\ne.g., texture, edge, etc.), we validate that transforming images in the\nhigh-frequency space also improves transferability. Based on this finding, we\npropose a balanced approach called Semantic and Abstract FEatures disRuption\n(SAFER). Specifically, SAFER conducts BLOCKMIX on the input image and SELF-MIX\non the frequency spectrum when computing the weight matrix to highlight crucial\nfeatures. By using such a weight matrix, we can direct the attacker to disrupt\nboth semantic and abstract features, leading to improved transferability.\nExtensive experiments on the ImageNet dataset also demonstrate the\neffectiveness of our method in boosting adversarial transferability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16052v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16407", "title": "Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing", "authors": ["Shuhan Liu", "Xing Hu", "Kerui Huang", "Xiaohu Yang", "David Lo", "Xin Xia"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16407v1", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, where the natural language prompt plays a crucial role in\nconveying user intent to the model. However, prior studies have shown that LLMs\nare highly sensitive to prompt perturbations. Minor modifications in wording,\nsyntax, or formatting can significantly reduce the functional correctness of\ngenerated code. As perturbations frequently occur in real-world scenarios,\nimproving the robustness of LLMs to prompt perturbations is essential for\nensuring reliable performance in practical code generation. In this paper, we\nintroduce CREME (Code Robustness Enhancement via Model Editing), a novel\napproach that enhances LLM robustness through targeted parameter updates. CREME\nfirst identifies robustness-sensitive layers by comparing hidden states between\nan original prompt and its perturbed variant. Then, it performs lightweight\nparameter editing at the identified layer to reduce performance degradation. We\nevaluate CREME on two widely used code generation benchmarks (HumanEval and\nMBPP) along with their perturbed counterparts. Experimental results show that\nCREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining\nstable performance on clean inputs, with accuracy deviations within 1%. Further\nanalysis reveals that robustness-sensitive layers are primarily concentrated in\nthe middle and deeper layers of the network, and their locations vary across\ndifferent model architectures. These insights provide a valuable foundation for\ndeveloping future robustness-oriented editing strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16407v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2505.14422", "title": "MindVote: When AI Meets the Wild West of Social Media Opinion", "authors": ["Xutao Mao", "Ezra Xuanru Tao"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.14422v2", "summary": "Authentic opinion prediction in social contexts presents a complex challenge\nthat requires the seamless integration of three distinct cognitive\ncapabilities: domain adaptation, cultural schema activation, and contextual\nscaffolding. The core difficulty is that no existing opinion prediction\nbenchmark, particularly in a poll-based setting, compels a Large Language Model\n(LLM) to integrate all three capabilities, leaving a critical gap in our\nability to evaluate genuine social understanding. Drawing from cognitive\nscience principles, we introduce MindVote, the first benchmark evaluating poll\nprediction within naturalistic social media discourse. We construct MindVote\nfrom 3,918 authentic polls across Reddit and Weibo, incorporating rich\ncontextual metadata that traditional survey-based benchmarks ignored. Our\nevaluation reveals three theoretical phenomena that validate cognitive science\nhypotheses: formal discourse privilege, where models achieve higher performance\non institutional than on vernacular discourse; semantic space colonization,\ndemonstrating performance biases favoring English content beyond translation\nartifacts; and contextual scaffolding dependencies, with performance\ndegradation when social context is removed. These findings expose limitations\nin models, close the gaps of current opinion prediction benchmarks and advocate\nfor developing socially-grounded AI systems with authentic cross-cultural\nopinion prediction. Our code and data are available in\nhttps://anonymous.4open.science/r/mindvote-8DBC/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.14422v2", "cate": "cs.SI", "date": "2025-05-20", "updated": "2025-07-22"}
{"id": "2502.03885", "title": "InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers", "authors": ["Chenchen Shou", "Guyue Liu", "Hao Nie", "Huaiyu Meng", "Yu Zhou", "Yimin Jiang", "Wenqing Lv", "Yelong Xu", "Yuanwei Lu", "Zhang Chen", "Yanbo Yu", "Yichen Shen", "Yibo Zhu", "Daxin Jiang"], "categories": ["cs.NI", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.03885v4", "summary": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 take a middle-ground approach, but the fault explosion radius remains\nlarge at the cube level (e.g., 64 TPUs).\n  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level} using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt to variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh)-based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios are under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.03885v4", "cate": "cs.NI", "date": "2025-02-06", "updated": "2025-07-22"}
{"id": "2507.16815", "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "authors": ["Chi-Pin Huang", "Yueh-Hua Wu", "Min-Hung Chen", "Yu-Chiang Frank Wang", "Fu-En Yang"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.16815v1", "summary": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.", "comment": "Project page: https://jasper0314-huang.github.io/thinkact-vla/", "pdf_url": "http://arxiv.org/pdf/2507.16815v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16322", "title": "Mind the Gap: Evaluating the Representativeness of Quantitative Medical Language Reasoning LLM Benchmarks for African Disease Burdens", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Preprint. 26 pages, includes appendix and tables", "url": "http://arxiv.org/abs/2507.16322v1", "summary": "Introduction: Existing medical LLM benchmarks largely reflect examination\nsyllabi and disease profiles from high income settings, raising questions about\ntheir validity for African deployment where malaria, HIV, TB, sickle cell\ndisease and other neglected tropical diseases (NTDs) dominate burden and\nnational guidelines drive care. Methodology: We systematically reviewed 31\nquantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English\nmedical QA benchmarks. Alama Health QA was developed using a retrieval\naugmented generation framework anchored on the Kenyan Clinical Practice\nGuidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA,\nMedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized\nsemantic profiling (NTD proportion, recency, readability, lexical diversity\nmetrics) and blinded expert rating across five dimensions: clinical relevance,\nguideline alignment, clarity, distractor plausibility, and language/cultural\nfit. Results: Alama Health QA captured >40% of all NTD mentions across corpora\nand the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB\n(5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global\nbenchmarks showed minimal representation (e.g., sickle cell disease absent in\nthree sets) despite large scale. Qualitatively, Alama scored highest for\nrelevance and guideline alignment; PubMedQA lowest for clinical utility.\nDiscussion: Quantitative medical LLM benchmarks widely used in the literature\nunderrepresent African disease burdens and regulatory contexts, risking\nmisleading performance claims. Guideline anchored, regionally curated resources\nsuch as Alama Health QA and expanded disease specific derivatives are essential\nfor safe, equitable model evaluation and deployment across African health\nsystems.", "comment": "Preprint. 26 pages, includes appendix and tables", "pdf_url": "http://arxiv.org/pdf/2507.16322v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16148", "title": "Learning Patient-Specific Spatial Biomarker Dynamics via Operator Learning for Alzheimer's Disease Progression", "authors": ["Jindong Wang", "Yutong Mao", "Xiao Liu", "Wenrui Hao"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16148v1", "summary": "Alzheimer's disease (AD) is a complex, multifactorial neurodegenerative\ndisorder with substantial heterogeneity in progression and treatment response.\nDespite recent therapeutic advances, predictive models capable of accurately\nforecasting individualized disease trajectories remain limited. Here, we\npresent a machine learning-based operator learning framework for personalized\nmodeling of AD progression, integrating longitudinal multimodal imaging,\nbiomarker, and clinical data. Unlike conventional models with prespecified\ndynamics, our approach directly learns patient-specific disease operators\ngoverning the spatiotemporal evolution of amyloid, tau, and neurodegeneration\nbiomarkers. Using Laplacian eigenfunction bases, we construct geometry-aware\nneural operators capable of capturing complex brain dynamics. Embedded within a\ndigital twin paradigm, the framework enables individualized predictions,\nsimulation of therapeutic interventions, and in silico clinical trials. Applied\nto AD clinical data, our method achieves high prediction accuracy exceeding 90%\nacross multiple biomarkers, substantially outperforming existing approaches.\nThis work offers a scalable, interpretable platform for precision modeling and\npersonalized therapeutic optimization in neurodegenerative diseases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16148v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16247", "title": "PRAC3 (Privacy, Reputation, Accountability, Consent, Credit, Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy", "authors": ["Tanusree Sharma", "Yihao Zhou", "Visar Berisha"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16247v1", "summary": "Early large-scale audio datasets, such as LibriSpeech, were built with\nhundreds of individual contributors whose voices were instrumental in the\ndevelopment of speech technologies, including audiobooks and voice assistants.\nYet, a decade later, these same contributions have exposed voice actors to a\nrange of risks. While existing ethical frameworks emphasize Consent, Credit,\nand Compensation (C3), they do not adequately address the emergent risks\ninvolving vocal identities that are increasingly decoupled from context,\nauthorship, and control. Drawing on qualitative interviews with 20 professional\nvoice actors, this paper reveals how the synthetic replication of voice without\nenforceable constraints exposes individuals to a range of threats. Beyond\nreputational harm, such as re-purposing voice data in erotic content, offensive\npolitical messaging, and meme culture, we document concerns about\naccountability breakdowns when their voice is leveraged to clone voices that\nare deployed in high-stakes scenarios such as financial fraud, misinformation\ncampaigns, or impersonation scams. In such cases, actors face social and legal\nfallout without recourse, while very few of them have a legal representative or\nunion protection. To make sense of these shifting dynamics, we introduce the\nPRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation,\nAccountability, Consent, Credit, and Compensation as interdependent pillars of\ndata used in the synthetic voice economy. This framework captures how privacy\nrisks are amplified through non-consensual training, how reputational harm\narises from decontextualized deployment, and how accountability can be\nreimagined AI Data ecosystems. We argue that voice, as both a biometric\nidentifier and creative labor, demands governance models that restore creator\nagency, ensure traceability, and establish enforceable boundaries for ethical\nreuse.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16247v1", "cate": "cs.CY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16095", "title": "Improving Personalized Image Generation through Social Context Feedback", "authors": ["Parul Gupta", "Abhinav Dhall", "Thanh-Toan Do"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16095v1", "summary": "Personalized image generation, where reference images of one or more subjects\nare used to generate their image according to a scene description, has gathered\nsignificant interest in the community. However, such generated images suffer\nfrom three major limitations -- complex activities, such as $<$man, pushing,\nmotorcycle$>$ are not generated properly with incorrect human poses, reference\nhuman identities are not preserved, and generated human gaze patterns are\nunnatural/inconsistent with the scene description. In this work, we propose to\novercome these shortcomings through feedback-based fine-tuning of existing\npersonalized generation methods, wherein, state-of-art detectors of pose,\nhuman-object-interaction, human facial recognition and human gaze-point\nestimation are used to refine the diffusion model. We also propose\ntimestep-based inculcation of different feedback modules, depending upon\nwhether the signal is low-level (such as human pose), or high-level (such as\ngaze point). The images generated in this manner show an improvement in the\ngenerated interactions, facial identities and image quality over three\nbenchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16095v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16439", "title": "Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code", "authors": ["Gunnar Larsen", "Carol Wong", "Anthony Peruma"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      The 19th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement - Emerging Results and Vision Track", "url": "http://arxiv.org/abs/2507.16439v1", "summary": "Research scientists increasingly rely on implementing software to support\ntheir research. While previous research has examined the impact of identifier\nnames on program comprehension in traditional programming environments, limited\nwork has explored this area in scientific software, especially regarding the\nquality of method names in the code. The recent advances in Large Language\nModels (LLMs) present new opportunities for automating code analysis tasks,\nsuch as identifier name appraisals and recommendations. Our study evaluates\nfour popular LLMs on their ability to analyze grammatical patterns and suggest\nimprovements for 496 method names extracted from Python-based Jupyter\nNotebooks. Our findings show that the LLMs are somewhat effective in analyzing\nthese method names and generally follow good naming practices, like starting\nmethod names with verbs. However, their inconsistent handling of\ndomain-specific terminology and only moderate agreement with human annotations\nindicate that automated suggestions require human evaluation. This work\nprovides foundational insights for improving the quality of scientific code\nthrough AI automation.", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "pdf_url": "http://arxiv.org/pdf/2507.16439v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.12108", "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies", "authors": ["Lorenzo Mannocci", "Stefano Cresci", "Matteo Magnani", "Anna Monreale", "Maurizio Tesconi"], "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12108v2", "summary": "Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12108v2", "cate": "cs.SI", "date": "2025-07-16", "updated": "2025-07-22"}
{"id": "2503.12177", "title": "Open Wireless Digital Twin: End-to-End 5G Mobility Emulation with OpenAirInterface and Sionna RT", "authors": ["Tetsuya Iye", "Masaya Sakamoto", "Shohei Takaya", "Eisaku Sato", "Yuki Susukida", "Yu Nagaoka", "Kazuki Maruta", "Jin Nakazato"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12177v3", "summary": "This study presents an end-to-end wireless digital twin platform constructed\nusing open-source software and open data to enhance the evaluation of mobile\ncommunication systems. The proposed open wireless digital twin (OWDT)\nintegrates OpenAirInterface (OAI) for 5G NR protocol stack emulation and NVIDIA\nSionna RT for high-resolution ray-tracing-based radio propagation modeling.\nThis integration enables the realistic emulation of 5G wireless communication\nin mobility scenarios on a CPU-based Linux system, leveraging real-world\nbuilding data to bridge the gap between theoretical simulations and real-world\ndeployment. The platform also incorporates OAI FlexRIC, which is an\nimplementation aligned with the O-RAN near-real-time RAN Intelligent Controller\n(near-RT RIC), to dynamically monitor the key performance indicators (KPIs).\nThrough extensive evaluation in urban environments, this study demonstrated the\nvalidity of the emulation framework, revealing its capability to replicate\nreal-world communication dynamics with high fidelity. The results underscore\nthe potential of the OWDT to accelerate wireless system development, reduce\nexperimental costs, and optimize network configurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12177v3", "cate": "cs.NI", "date": "2025-03-15", "updated": "2025-07-22"}
{"id": "2311.01059", "title": "Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment", "authors": ["Annie S. Chen", "Govind Chada", "Laura Smith", "Archit Sharma", "Zipeng Fu", "Sergey Levine", "Chelsea Finn"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2311.01059v3", "summary": "To succeed in the real world, robots must cope with situations that differ\nfrom those seen during training. We study the problem of adapting on-the-fly to\nsuch novel scenarios during deployment, by drawing upon a diverse repertoire of\npreviouslylearned behaviors. Our approach, RObust Autonomous Modulation (ROAM),\nintroduces a mechanism based on the perceived value of pre-trained behaviors to\nselect and adapt pre-trained behaviors to the situation at hand. Crucially,\nthis adaptation process all happens within a single episode at test time,\nwithout any human supervision. We demonstrate that ROAM enables a robot to\nadapt rapidly to changes in dynamics both in simulation and on a real Go1\nquadruped, even successfully moving forward with roller skates on its feet. Our\napproach adapts over 2x as efficiently compared to existing methods when facing\na variety of out-of-distribution situations during deployment by effectively\nchoosing and adapting relevant behaviors on-the-fly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2311.01059v3", "cate": "cs.RO", "date": "2023-11-02", "updated": "2025-07-22"}
{"id": "2507.16334", "title": "Higher Gauge Flow Models", "authors": ["Alexander Strunk", "Roland Assam"], "categories": ["cs.AI", "cs.LG", "math.DG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16334v1", "summary": "This paper introduces Higher Gauge Flow Models, a novel class of Generative\nFlow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these\nHigher Gauge Flow Models leverage an L$_{\\infty}$-algebra, effectively\nextending the Lie Algebra. This expansion allows for the integration of the\nhigher geometry and higher symmetries associated with higher groups into the\nframework of Generative Flow Models. Experimental evaluation on a Gaussian\nMixture Model dataset revealed substantial performance improvements compared to\ntraditional Flow Models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16334v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16178", "title": "LLM Data Selection and Utilization via Dynamic Bi-level Optimization", "authors": ["Yang Yu", "Kai Han", "Hang Zhou", "Yehui Tang", "Kaiqi Huang", "Yunhe Wang", "Dacheng Tao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The 42nd International Conference on Machine Learning (ICML 2025)", "url": "http://arxiv.org/abs/2507.16178v1", "summary": "While large-scale training data is fundamental for developing capable large\nlanguage models (LLMs), strategically selecting high-quality data has emerged\nas a critical approach to enhance training efficiency and reduce computational\ncosts. Current data selection methodologies predominantly rely on static,\ntraining-agnostic criteria, failing to account for the dynamic model training\nand data interactions. In this paper, we propose a new Data Weighting Model\n(DWM) to adjust the weight of selected data within each batch to achieve a\ndynamic data utilization during LLM training. Specially, to better capture the\ndynamic data preference of the trained model, a bi-level optimization framework\nis implemented to update the weighting model. Our experiments demonstrate that\nDWM enhances the performance of models trained with randomly-selected data, and\nthe learned weighting model can be transferred to enhance other data selection\nmethods and models of different sizes. Moreover, we further analyze how a\nmodel's data preferences evolve throughout training, providing new insights\ninto the data preference of the model during training.", "comment": "The 42nd International Conference on Machine Learning (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2507.16178v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16515", "title": "Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness", "authors": ["Siqi Liu", "Guangrong Dai", "Dechao Li"], "categories": ["cs.CL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures, 2 tables. To be published in the Proceedings of the 20th Machine Translation Summit (MT Summit 2025; Geneva, Switzerland)", "url": "http://arxiv.org/abs/2507.16515v1", "summary": "This preliminary study investigates the usefulness of sentence-level Quality\nEstimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE),\nfocusing on its impact on post-editing speed and student translators'\nperceptions. It also explores the interaction effects between QE and MT\nquality, as well as between QE and translation expertise. The findings reveal\nthat QE significantly reduces post-editing time. The examined interaction\neffects were not significant, suggesting that QE consistently improves MTPE\nefficiency across medium- and high-quality MT outputs and among student\ntranslators with varying levels of expertise. In addition to indicating\npotentially problematic segments, QE serves multiple functions in MTPE, such as\nvalidating translators' evaluations of MT quality and enabling them to\ndouble-check translation outputs. However, interview data suggest that\ninaccurate QE may hinder post-editing processes. This research provides new\ninsights into the strengths and limitations of QE, facilitating its more\neffective integration into MTPE workflows to enhance translators' productivity.", "comment": "11 pages, 5 figures, 2 tables. To be published in the Proceedings of\n  the 20th Machine Translation Summit (MT Summit 2025; Geneva, Switzerland)", "pdf_url": "http://arxiv.org/pdf/2507.16515v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16114", "title": "Stop-band Energy Constraint for Orthogonal Tunable Wavelet Units in Convolutional Neural Networks for Computer Vision problems", "authors": ["An D. Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Q. Nguyen"], "categories": ["cs.CV", "eess.SP"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16114v1", "summary": "This work introduces a stop-band energy constraint for filters in orthogonal\ntunable wavelet units with a lattice structure, aimed at improving image\nclassification and anomaly detection in CNNs, especially on texture-rich\ndatasets. Integrated into ResNet-18, the method enhances convolution, pooling,\nand downsampling operations, yielding accuracy gains of 2.48% on CIFAR-10 and\n13.56% on the Describable Textures dataset. Similar improvements are observed\nin ResNet-34. On the MVTec hazelnut anomaly detection task, the proposed method\nachieves competitive results in both segmentation and detection, outperforming\nexisting approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16114v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16587", "title": "On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Alejandro Velasco", "Antonio Mastropaolo", "Denys Poshyvanyk", "Gabriele Bavota"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at TSE. IEEE Transactions on Software Engineering", "url": "http://arxiv.org/abs/2507.16587v1", "summary": "Large Language Models have been recently exploited as judges for complex\nnatural language processing tasks, such as Q&A. The basic idea is to delegate\nto an LLM the assessment of the \"quality\" of the output provided by an\nautomated technique for tasks for which: (i) quantitative metrics would only\ntell part of the story, and; (ii) a large-scale human-based evaluation would be\ntoo expensive. LLMs-as-a-judge, if proven effective for a specific task, can\nalso unlock new possibilities for automation, with several LLMs proposing a\nsolution for a given instance of the task and others judging and deciding what\nis the best output to show the user. We study the effectiveness of\nLLMs-as-a-judge for two code-related tasks, namely code generation and code\nsummarization. The rationale for choosing these tasks is two-fold. First,\nquantitative metrics are usually not enough for the assessment of code\nsummarizers/generators. For example, it is well documented that metrics such as\nBLEU are quite weak proxies for the quality of the generated summaries. Second,\neven state-of-the-art techniques still struggle with handling complex instances\nof these tasks, making them good candidates for benefiting from more advanced\nsolutions envisioning collaboration among LLMs. For code generation, we check\nwhether eight LLMs are able to judge the correctness of 1,405 Java methods and\n1,281 Python functions generated by the same LLMs or implemented by humans. For\ncode summarization, we compare the judgment of five LLMs to those provided by\nnine humans for ~1.2k summaries, related to both Java and Python functions. Our\nfindings show that GPT-4-turbo is the best LLM in terms of judging capabilities\nfor both tasks, with \"smaller\" LLMs featuring tens of billions parameters not\nbeing able to cope with judging tasks. However, even the best-performing LLM\nfrequently misjudges the correctness of the code and summary quality.", "comment": "Accepted at TSE. IEEE Transactions on Software Engineering", "pdf_url": "http://arxiv.org/pdf/2507.16587v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15460", "title": "Privacy-Preserving Multimodal News Recommendation through Federated Learning", "authors": ["Mehdi Khalaj", "Shahrzad Golestani Najafabadi", "Julita Vassileva"], "categories": ["cs.SI", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15460v2", "summary": "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15460v2", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.13179", "title": "Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering", "authors": ["Ziyu Zhong", "Bjrn Landfeldt", "Gnter Alce", "Hector A Caltenco"], "categories": ["cs.NI", "cs.MM"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13179v2", "summary": "As 6G networks are developed and defined, offloading of XR applications is\nemerging as one of the strong new use cases. The reduced 6G latency coupled\nwith edge processing infrastructure will for the first time provide a realistic\noffloading scenario in cellular networks where several computationally\nintensive functions, including rendering, can migrate from the user device and\ninto the network. A key advantage of doing so is the lowering of the battery\nneeds in the user devices and the possibility to design new devices with\nsmaller form factors. However, offloading introduces increased delays compared\nto local execution, primarily due to network transmission latency and queuing\ndelays at edge servers, especially under multi-user concurrency. Despite the\ncomputational power of edge platforms, the resulting motion-to-photon (MTP)\nlatency negatively impacts user experience. To mitigate this, motion prediction\nhas been proposed to offset delays. Existing approaches build on either deep\nlearning or Kalman filtering. Deep learning techniques face scalability\nlimitations at the resource-constrained edge, as their computational expense\nintensifies with increasing user concurrency, while Kalman filtering suffers\nfrom poor handling of complex movements and fragility to packet loss inherent\nin 6G's high-frequency radio interfaces. In this work, we introduce a\ncontext-aware error-state Kalman filter (ESKF) prediction framework, which\nforecasts the user's head motion trajectory to compensate for MTP latency in\nremote XR. By integrating a motion classifier that categorizes head motions\nbased on their predictability, our algorithm demonstrates reduced prediction\nerror across different motion classes. Our findings demonstrate that the\noptimized ESKF not only surpasses traditional Kalman filters in positional and\norientational accuracy but also exhibits enhanced robustness and resilience to\npacket loss.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13179v2", "cate": "cs.NI", "date": "2025-07-17", "updated": "2025-07-22"}
{"id": "2407.11551", "title": "Human-Machine Shared Control Approach for the Takeover of Cooperative Adaptive Cruise Control", "authors": ["Haoran Wang", "Zhexi Lian", "Zhenning Li", "Jiawei Wang", "Arno Eichberger", "Jia Hu", "Yongyu Chen", "Yongji Gao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Intelligent Transportation Systems (2025)", "url": "http://arxiv.org/abs/2407.11551v2", "summary": "Cooperative Adaptive Cruise Control (CACC) often requires human takeover for\ntasks such as exiting a freeway. Direct human takeover can pose significant\nrisks, especially given the close-following strategy employed by CACC, which\nmight cause drivers to feel unsafe and execute hard braking, potentially\nleading to collisions. This research aims to develop a CACC takeover controller\nthat ensures a smooth transition from automated to human control. The proposed\nCACC takeover maneuver employs an indirect human-machine shared control\napproach, modeled as a Stackelberg competition where the machine acts as the\nleader and the human as the follower. The machine guides the human to respond\nin a manner that aligns with the machine's expectations, aiding in maintaining\nfollowing stability. Additionally, the human reaction function is integrated\ninto the machine's predictive control system, moving beyond a simple\n\"prediction-planning\" pipeline to enhance planning optimality. The controller\nhas been verified to i) enable a smooth takeover maneuver of CACC; ii) ensure\nstring stability in the condition that the platoon has less than 6 CAVs and\nhuman control authority is less than 40%; iii) enhance both perceived and\nactual safety through machine interventions; and iv) reduce the impact on\nupstream traffic by up to 60%.", "comment": "IEEE Transactions on Intelligent Transportation Systems (2025)", "pdf_url": "http://arxiv.org/pdf/2407.11551v2", "cate": "cs.RO", "date": "2024-07-16", "updated": "2025-07-22"}
{"id": "2507.16356", "title": "Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for Improved Message Delivery in Mobile Maternal Health", "authors": ["Arpan Dasgupta", "Mizhaan Maniyar", "Awadhesh Srivastava", "Sanat Kumar", "Amrita Mahale", "Aparna Hedge", "Arun Suggala", "Karthikeyan Shanmugam", "Aparna Taneja", "Milind Tambe"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16356v1", "summary": "Mobile health (mHealth) programs utilize automated voice messages to deliver\nhealth information, particularly targeting underserved communities,\ndemonstrating the effectiveness of using mobile technology to disseminate\ncrucial health information to these populations, improving health outcomes\nthrough increased awareness and behavioral change. India's Kilkari program\ndelivers vital maternal health information via weekly voice calls to millions\nof mothers. However, the current random call scheduling often results in missed\ncalls and reduced message delivery. This study presents a field trial of a\ncollaborative bandit algorithm designed to optimize call timing by learning\nindividual mothers' preferred call times. We deployed the algorithm with around\n$6500$ Kilkari participants as a pilot study, comparing its performance to the\nbaseline random calling approach. Our results demonstrate a statistically\nsignificant improvement in call pick-up rates with the bandit algorithm,\nindicating its potential to enhance message delivery and impact millions of\nmothers across India. This research highlights the efficacy of personalized\nscheduling in mobile health interventions and underscores the potential of\nmachine learning to improve maternal health outreach at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16356v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16186", "title": "EBaReT: Expert-guided Bag Reward Transformer for Auto Bidding", "authors": ["Kaiyuan Li", "Pengyu Wang", "Yunshan Peng", "Pengjia Yuan", "Yanxiang Zeng", "Rui Xiang", "Yanhua Cheng", "Xialong Liu", "Peng Jiang"], "categories": ["cs.LG", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16186v1", "summary": "Reinforcement learning has been widely applied in automated bidding.\nTraditional approaches model bidding as a Markov Decision Process (MDP).\nRecently, some studies have explored using generative reinforcement learning\nmethods to address long-term dependency issues in bidding environments.\nAlthough effective, these methods typically rely on supervised learning\napproaches, which are vulnerable to low data quality due to the amount of\nsub-optimal bids and low probability rewards resulting from the low click and\nconversion rates. Unfortunately, few studies have addressed these challenges.\n  In this paper, we formalize the automated bidding as a sequence\ndecision-making problem and propose a novel Expert-guided Bag Reward\nTransformer (EBaReT) to address concerns related to data quality and\nuncertainty rewards. Specifically, to tackle data quality issues, we generate a\nset of expert trajectories to serve as supplementary data in the training\nprocess and employ a Positive-Unlabeled (PU) learning-based discriminator to\nidentify expert transitions. To ensure the decision also meets the expert\nlevel, we further design a novel expert-guided inference strategy. Moreover, to\nmitigate the uncertainty of rewards, we consider the transitions within a\ncertain period as a \"bag\" and carefully design a reward function that leads to\na smoother acquisition of rewards. Extensive experiments demonstrate that our\nmodel achieves superior performance compared to state-of-the-art bidding\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16186v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16704", "title": "Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation", "authors": ["Viktor Muryn", "Marta Sumyk", "Mariya Hirna", "Sofiya Garkot", "Maksym Shamrai"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16704v1", "summary": "Desktop accessibility metadata enables AI agents to interpret screens and\nsupports users who depend on tools like screen readers. Yet, many applications\nremain largely inaccessible due to incomplete or missing metadata provided by\ndevelopers - our investigation shows that only 33% of applications on macOS\noffer full accessibility support. While recent work on structured screen\nrepresentation has primarily addressed specific challenges, such as UI element\ndetection or captioning, none has attempted to capture the full complexity of\ndesktop interfaces by replicating their entire hierarchical structure. To\nbridge this gap, we introduce Screen2AX, the first framework to automatically\ncreate real-time, tree-structured accessibility metadata from a single\nscreenshot. Our method uses vision-language and object detection models to\ndetect, describe, and organize UI elements hierarchically, mirroring macOS's\nsystem-level accessibility structure. To tackle the limited availability of\ndata for macOS desktop applications, we compiled and publicly released three\ndatasets encompassing 112 macOS applications, each annotated for UI element\ndetection, grouping, and hierarchical accessibility metadata alongside\ncorresponding screenshots. Screen2AX accurately infers hierarchy trees,\nachieving a 77% F1 score in reconstructing a complete accessibility tree.\nCrucially, these hierarchy trees improve the ability of autonomous agents to\ninterpret and interact with complex desktop interfaces. We introduce\nScreen2AX-Task, a benchmark specifically designed for evaluating autonomous\nagent task execution in macOS desktop environments. Using this benchmark, we\ndemonstrate that Screen2AX delivers a 2.2x performance improvement over native\naccessibility representations and surpasses the state-of-the-art OmniParser V2\nsystem on the ScreenSpot benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16704v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16116", "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation", "authors": ["Yaofang Liu", "Yumeng Ren", "Aitor Artola", "Yuxuan Hu", "Xiaodong Cun", "Xiaotong Zhao", "Alan Zhao", "Raymond H. Chan", "Suiyun Zhang", "Rui Liu", "Dandan Tu", "Jean-Michel Morel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code is open-sourced at this https URL", "url": "http://arxiv.org/abs/2507.16116v1", "summary": "The rapid advancement of video diffusion models has been hindered by\nfundamental limitations in temporal modeling, particularly the rigid\nsynchronization of frame evolution imposed by conventional scalar timestep\nvariables. While task-specific adaptations and autoregressive models have\nsought to address these challenges, they remain constrained by computational\ninefficiency, catastrophic forgetting, or narrow applicability. In this work,\nwe present Pusa, a groundbreaking paradigm that leverages vectorized timestep\nadaptation (VTA) to enable fine-grained temporal control within a unified video\ndiffusion framework. Besides, VTA is a non-destructive adaptation, which means\nit fully preserves the capabilities of the base model. By finetuning the SOTA\nWan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --\nsurpassing the performance of Wan-I2V-14B with $\\leq$ 1/200 of the training\ncost (\\$500 vs. $\\geq$ \\$100,000) and $\\leq$ 1/2500 of the dataset size (4K vs.\n$\\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V)\ngeneration, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of\nWan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as\nstart-end frames and video extension -- all without task-specific training.\nMeanwhile, Pusa can still perform text-to-video generation. Mechanistic\nanalyses reveal that our approach preserves the foundation model's generative\npriors while surgically injecting temporal dynamics, avoiding the combinatorial\nexplosion inherent to vectorized timesteps. This work establishes a scalable,\nefficient, and versatile paradigm for next-generation video synthesis,\ndemocratizing high-fidelity video generation for research and industry alike.\nCode is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen", "comment": "Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen", "pdf_url": "http://arxiv.org/pdf/2507.16116v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16661", "title": "VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones", "authors": ["Tan Bui", "Yan Naing Tun", "Thanh Phuc Nguyen", "Yindu Su", "Ferdian Thung", "Yikun Li", "Han Wei Ang", "Yide Yin", "Frank Liauw", "Lwin Khin Shar", "Eng Lieh Ouh", "Ting Zhang", "David Lo"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16661v1", "summary": "Code reuse is common in modern software development, but it can also spread\nvulnerabilities when developers unknowingly copy risky code. The code fragments\nthat preserve the logic of known vulnerabilities are known as vulnerable code\nclones (VCCs). Detecting those VCCs is a critical but challenging task.\nExisting VCC detection tools often rely on syntactic similarity or produce\ncoarse vulnerability predictions without clear explanations, limiting their\npractical utility. In this paper, we propose VulCoCo, a lightweight and\nscalable approach that combines embedding-based retrieval with large language\nmodel (LLM) validation. Starting from a set of known vulnerable functions, we\nretrieve syntactically or semantically similar candidate functions from a large\ncorpus and use an LLM to assess whether the candidates retain the\nvulnerability. Given that there is a lack of reproducible vulnerable code clone\nbenchmarks, we first construct a synthetic benchmark that spans various clone\ntypes.\n  Our experiments on the benchmark show that VulCoCo outperforms prior\nstate-of-the-art methods in terms of Precision@k and mean average precision\n(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world\nprojects by submitting 400 pull requests (PRs) to 284 open-source projects.\nAmong them, 75 PRs were merged, and 15 resulted in newly published CVEs. We\nalso provide insights to inspire future work to further improve the precision\nof vulnerable code clone detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16661v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2502.02451", "title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study", "authors": ["Calvin Yixiang Cheng", "Scott A Hale"], "categories": ["cs.CL", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures, 6 tables", "url": "http://arxiv.org/abs/2502.02451v3", "summary": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks.", "comment": "12 pages, 2 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2502.02451v3", "cate": "cs.CL", "date": "2025-02-04", "updated": "2025-07-22"}
{"id": "2409.01559", "title": "PR2: A Physics- and Photo-realistic Humanoid Testbed with Pilot Study in Competition", "authors": ["Hangxin Liu", "Qi Xie", "Zeyu Zhang", "Tao Yuan", "Song Wang", "Zaijin Wang", "Xiaokun Leng", "Lining Sun", "Jingwen Zhang", "Zhicheng He", "Yao Su"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.01559v2", "summary": "This paper presents the development of a Physics-realistic and\nPhoto-realistic humanoid robot testbed, PR2, to facilitate collaborative\nresearch between Embodied Artificial Intelligence (Embodied AI) and robotics.\nPR2 offers high-quality scene rendering and robot dynamic simulation, enabling\n(i) the creation of diverse scenes using various digital assets, (ii) the\nintegration of advanced perception or foundation models, and (iii) the\nimplementation of planning and control algorithms for dynamic humanoid robot\nbehaviors based on environmental feedback. The beta version of PR2 has been\ndeployed for the simulation track of a nationwide full-size humanoid robot\ncompetition for college students, attracting 137 teams and over 400\nparticipants within four months. This competition covered traditional tasks in\nbipedal walking, as well as novel challenges in loco-manipulation and\nlanguage-instruction-based object search, marking a first for public college\nrobotics competitions. A retrospective analysis of the competition suggests\nthat future events should emphasize the integration of locomotion with\nmanipulation and perception. By making the PR2 testbed publicly available at\nhttps://github.com/pr2-humanoid/PR2-Platform, we aim to further advance\neducation and training in humanoid robotics. Video demonstration:\nhttps://pr2-humanoid.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.01559v2", "cate": "cs.RO", "date": "2024-09-03", "updated": "2025-07-22"}
{"id": "2507.16370", "title": "Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning", "authors": ["Lucas de Lara"], "categories": ["cs.AI", "math.ST", "stat.TH"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16370v1", "summary": "Counterfactual reasoning aims at answering contrary-to-fact questions like\n''Would have Alice recovered had she taken aspirin?'' and corresponds to the\nmost fine-grained layer of causation. Critically, while many counterfactual\nstatements cannot be falsified -- even by randomized experiments -- they\nunderpin fundamental concepts like individual-wise fairness. Therefore,\nproviding models to formalize and implement counterfactual beliefs remains a\nfundamental scientific problem. In the Markovian setting of Pearl's causal\nframework, we propose an alternative approach to structural causal models to\nrepresent counterfactuals compatible with a given causal graphical model. More\nprecisely, we introduce counterfactual models, also called canonical\nrepresentations of structural causal models. They enable analysts to choose a\ncounterfactual conception via random-process probability distributions with\npreassigned marginals and characterize the counterfactual equivalence class of\nstructural causal models. Then, we present a normalization procedure to\ndescribe and implement various counterfactual conceptions. Compared to\nstructural causal models, it allows to specify many counterfactual conceptions\nwithout altering the observational and interventional constraints. Moreover,\nthe content of the model corresponding to the counterfactual layer does not\nneed to be estimated; only to make a choice. Finally, we illustrate the\nspecific role of counterfactuals in causality and the benefits of our approach\non theoretical and numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16370v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16200", "title": "RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs", "authors": ["Pengwei Jin", "Di Huang", "Chongxiao Li", "Shuyao Cheng", "Yang Zhao", "Xinyao Zheng", "Jiaguo Zhu", "Shuyi Xing", "Bohan Dou", "Rui Zhang", "Zidong Du", "Qi Guo", "Xing Hu"], "categories": ["cs.LG", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The benchmark is open-sourced at this https URL", "url": "http://arxiv.org/abs/2507.16200v1", "summary": "The automatic generation of Verilog code using Large Language Models (LLMs)\nhas garnered significant interest in hardware design automation. However,\nexisting benchmarks for evaluating LLMs in Verilog generation fall short in\nreplicating real-world design workflows due to their designs' simplicity,\ninadequate design specifications, and less rigorous verification environments.\nTo address these limitations, we present RealBench, the first benchmark aiming\nat real-world IP-level Verilog generation tasks. RealBench features complex,\nstructured, real-world open-source IP designs, multi-modal and formatted design\nspecifications, and rigorous verification environments, including 100% line\ncoverage testbenches and a formal checker. It supports both module-level and\nsystem-level tasks, enabling comprehensive assessments of LLM capabilities.\nEvaluations on various LLMs and agents reveal that even one of the\nbest-performing LLMs, o1-preview, achieves only a 13.3% pass@1 on module-level\ntasks and 0% on system-level tasks, highlighting the need for stronger Verilog\ngeneration models in the future. The benchmark is open-sourced at\nhttps://github.com/IPRC-DIP/RealBench.", "comment": "The benchmark is open-sourced at\n  https://github.com/IPRC-DIP/RealBench", "pdf_url": "http://arxiv.org/pdf/2507.16200v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2409.05176", "title": "Using Generative Artificial Intelligence Creatively in the Classroom and Research: Examples and Lessons Learned", "authors": ["Maria J. Molina", "Amy McGovern", "Jhayron S. Perez-Carrasquilla", "Xiaowen Li", "Robin L. Tanamachi"], "categories": ["cs.HC", "physics.ao-ph"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      This Work has been submitted to the Bulletin of the American Meteorological Society. Copyright in this Work may be transferred without further notice; 14 pages, 2 figures, 2 tables", "url": "http://arxiv.org/abs/2409.05176v2", "summary": "Although generative artificial intelligence (AI) is not new, recent\ntechnological breakthroughs have transformed its capabilities across many\ndomains. These changes necessitate new attention from educators and specialized\ntraining within the atmospheric and related sciences. Enabling students to use\ngenerative AI effectively, responsibly, and ethically is crucial for their\nacademic and professional development. Educators can also use generative AI to\ndevelop engaging classroom activities, such as active learning modules and\ngames; however, they must be aware of potential pitfalls and biases. There are\nalso ethical implications in using tools that lack transparency and have a\nconsiderable carbon footprint, as well as equity concerns for students who lack\naccess to more sophisticated paid versions of generative AI tools and have\ndeficiencies in prior educational training. This article is written for\nstudents and educators alike, particularly those interested in learning more\nabout generative AI in education and research, including its use cases, ethical\nconcerns, and a brief history of its emergence. Sample user prompts are also\nprovided across numerous applications in education and the atmospheric and\nrelated sciences. Current solutions addressing broader ethical concerns\nregarding the use of generative AI in education remain limited; however, this\nwork aims to foster a discussion that could galvanize the education community\naround shared goals and values.", "comment": "This Work has been submitted to the Bulletin of the American\n  Meteorological Society. Copyright in this Work may be transferred without\n  further notice; 14 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2409.05176v2", "cate": "cs.HC", "date": "2024-09-08", "updated": "2025-07-21"}
{"id": "2507.16119", "title": "Universal Wavelet Units in 3D Retinal Layer Segmentation", "authors": ["An D. Le", "Hung Nguyen", "Melanie Tran", "Jesse Most", "Dirk-Uwe G. Bartsch", "William R Freeman", "Shyamanga Borooah", "Truong Q. Nguyen", "Cheolhong An"], "categories": ["cs.CV", "eess.SP"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16119v1", "summary": "This paper presents the first study to apply tunable wavelet units (UwUs) for\n3D retinal layer segmentation from Optical Coherence Tomography (OCT) volumes.\nTo overcome the limitations of conventional max-pooling, we integrate three\nwavelet-based downsampling modules, OrthLattUwU, BiorthLattUwU, and\nLS-BiorthLattUwU, into a motion-corrected MGU-Net architecture. These modules\nuse learnable lattice filter banks to preserve both low- and high-frequency\nfeatures, enhancing spatial detail and structural consistency. Evaluated on the\nJacobs Retina Center (JRC) OCT dataset, our framework shows significant\nimprovement in accuracy and Dice score, particularly with LS-BiorthLattUwU,\nhighlighting the benefits of tunable wavelet filters in volumetric medical\nimage segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16119v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16685", "title": "VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models", "authors": ["Duong Nguyen", "Manh Tran-Duc", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16685v1", "summary": "We present VulGuard, an automated tool designed to streamline the extraction,\nprocessing, and analysis of commits from GitHub repositories for Just-In-Time\nvulnerability prediction (JIT-VP) research. VulGuard automatically mines commit\nhistories, extracts fine-grained code changes, commit messages, and software\nengineering metrics, and formats them for downstream analysis. In addition, it\nintegrates several state-of-the-art vulnerability prediction models, allowing\nresearchers to train, evaluate, and compare models with minimal setup. By\nsupporting both repository-scale mining and model-level experimentation within\na unified framework, VulGuard addresses key challenges in reproducibility and\nscalability in software security research. VulGuard can also be easily\nintegrated into the CI/CD pipeline. We demonstrate the effectiveness of the\ntool in two influential open-source projects, FFmpeg and the Linux kernel,\nhighlighting its potential to accelerate real-world JIT-VP research and promote\nstandardized benchmarking. A demo video is available at:\nhttps://youtu.be/j96096-pxbs", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16685v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2409.06521", "title": "Asymptotically Optimal Lazy Lifelong Sampling-based Algorithm for Efficient Motion Planning in Dynamic Environments", "authors": ["Lu Huang", "Jingwen Yu", "Jiankun Wang", "Xingjian Jing"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.06521v3", "summary": "The paper introduces an asymptotically optimal lifelong sampling-based path\nplanning algorithm that combines the merits of lifelong planning algorithms and\nlazy search algorithms for rapid replanning in dynamic environments where edge\nevaluation is expensive. By evaluating only sub-path candidates for the optimal\nsolution, the algorithm saves considerable evaluation time and thereby reduces\nthe overall planning cost. It employs a novel informed rewiring cascade to\nefficiently repair the search tree when the underlying search graph changes.\nTheoretical analysis indicates that the proposed algorithm converges to the\noptimal solution as long as sufficient planning time is given. Planning results\non robotic systems with $\\mathbb{SE}(3)$ and $\\mathbb{R}^7$ state spaces in\nchallenging environments highlight the superior performance of the proposed\nalgorithm over various state-of-the-art sampling-based planners in both static\nand dynamic motion planning tasks. The experiment of planning for a Turtlebot 4\noperating in a dynamic environment with several moving pedestrians further\nverifies the feasibility and advantages of the proposed algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.06521v3", "cate": "cs.RO", "date": "2024-09-10", "updated": "2025-07-22"}
{"id": "2507.16395", "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning", "authors": ["Bo Hou", "Xin Tan", "Kai Zheng", "Fang Liu", "Yinghao Zhu", "Li Zhang"], "categories": ["cs.AI", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16395v1", "summary": "Atomic commits, each of which addresses a single development concern, are a\nbest practice in software development. However, developers frequently produce\ntangled commits that mix unrelated changes due to practical constraints or\nunclear boundaries, negatively impacting code review and maintenance. Although\nprior commit untangling approaches: rule-based, feature-based, or graph-based,\nhave made progress, they often rely on shallow signals and fail to distinguish\nbetween explicit dependencies (e.g., control/data flow) and implicit ones\n(e.g., semantic or conceptual relationships). In this paper, we propose\nColaUntangle, a new collaborative consultation framework for commit untangling\nthat models both explicit and implicit dependencies among code changes.\nColaUntangle integrates Large Language Model (LLM)-driven agents in a\nmulti-agent architecture: one agent specializes in explicit dependencies,\nanother in implicit ones, and a reviewer agent synthesizes their perspectives\nthrough iterative consultation. To capture explicit and implicit contextual\ninformation, we construct multi-version Program Dependency Graphs (delta-PDG),\nenabling agents to reason over code relationships with both symbolic and\nsemantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#\nand 14k Java tangled commits). Experimental results show that ColaUntangle\noutperforms the best-performing baseline, achieving an improvement of 44% on\nthe C# dataset and 100% on the Java dataset. These findings highlight the\npotential of LLM-based collaborative frameworks for advancing automated commit\nuntangling tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16395v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16206", "title": "METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark", "authors": ["Xu Yang", "Qi Zhang", "Shuming Jiang", "Yaowen Xu", "Zhaofan Zou", "Hao Sun", "Xuelong Li"], "categories": ["cs.LG", "cs.AI", "68T45", "I.4.8; I.2.6; I.2.7"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages,3 figures ICCV format", "url": "http://arxiv.org/abs/2507.16206v1", "summary": "With the rapid advancement of generative AI, synthetic content across images,\nvideos, and audio has become increasingly realistic, amplifying the risk of\nmisinformation. Existing detection approaches predominantly focus on binary\nclassification while lacking detailed and interpretable explanations of\nforgeries, which limits their applicability in safety-critical scenarios.\nMoreover, current methods often treat each modality separately, without a\nunified benchmark for cross-modal forgery detection and interpretation. To\naddress these challenges, we introduce METER, a unified, multi-modal benchmark\nfor interpretable forgery detection spanning images, videos, audio, and\naudio-visual content. Our dataset comprises four tracks, each requiring not\nonly real-vs-fake classification but also evidence-chain-based explanations,\nincluding spatio-temporal localization, textual rationales, and forgery type\ntracing. Compared to prior benchmarks, METER offers broader modality coverage\nand richer interpretability metrics such as spatial/temporal IoU, multi-class\ntracing, and evidence consistency. We further propose a human-aligned,\nthree-stage Chain-of-Thought (CoT) training strategy combining SFT, DPO, and a\nnovel GRPO stage that integrates a human-aligned evaluator with CoT reasoning.\nWe hope METER will serve as a standardized foundation for advancing\ngeneralizable and interpretable forgery detection in the era of generative\nmedia.", "comment": "9 pages,3 figures ICCV format", "pdf_url": "http://arxiv.org/pdf/2507.16206v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2409.07589", "title": "miMamba: EEG-based Emotion Recognition with Multi-scale Inverted Mamba Models", "authors": ["Xin Zhou", "Dawei Huang", "Xiaojing Peng", "Lijun Yin"], "categories": ["cs.HC", "eess.SP"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Affective Computing (2025)", "url": "http://arxiv.org/abs/2409.07589v2", "summary": "EEG-based emotion recognition holds significant potential in the field of\nbrain-computer interfaces. A key challenge lies in extracting discriminative\nspatiotemporal features from electroencephalogram (EEG) signals. Existing\nstudies often rely on domain-specific time-frequency features and analyze\ntemporal dependencies and spatial characteristics separately, neglecting the\ninteraction between local-global relationships and spatiotemporal dynamics. To\naddress this, we propose a novel network called Multi-Scale Inverted Mamba\n(MS-iMamba), which consists of Multi-Scale Temporal Blocks (MSTB) and\nTemporal-Spatial Fusion Blocks (TSFB). Specifically, MSTBs are designed to\ncapture both local details and global temporal dependencies across different\nscale subsequences. The TSFBs, implemented with an inverted Mamba structure,\nfocus on the interaction between dynamic temporal dependencies and spatial\ncharacteristics. The primary advantage of MS-iMamba lies in its ability to\nleverage reconstructed multi-scale EEG sequences, exploiting the interaction\nbetween temporal and spatial features without the need for domain-specific\ntime-frequency feature extraction. Experimental results on the DEAP, DREAMER,\nand SEED datasets demonstrate that MS-iMamba achieves classification accuracies\nof 94.86%, 94.94%, and 91.36%, respectively, using only four-channel EEG\nsignals, outperforming state-of-the-art methods.", "comment": "IEEE Transactions on Affective Computing (2025)", "pdf_url": "http://arxiv.org/pdf/2409.07589v2", "cate": "cs.HC", "date": "2024-09-11", "updated": "2025-07-21"}
{"id": "2507.16144", "title": "LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images", "authors": ["Guichen Huang", "Ruoyu Wang", "Xiangjun Gao", "Che Sun", "Yuwei Wu", "Shenghua Gao", "Yunde Jia"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16144v1", "summary": "3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its\napplication to online long-sequence scenarios is still limited. Existing\nmethods either rely on slow per-scene optimization or fail to provide efficient\nincremental updates, hindering continuous performance. In this paper, we\npropose LongSplat, an online real-time 3D Gaussian reconstruction framework\ndesigned for long-sequence image input. The core idea is a streaming update\nmechanism that incrementally integrates current-view observations while\nselectively compressing redundant historical Gaussians. Crucial to this\nmechanism is our Gaussian-Image Representation (GIR), a representation that\nencodes 3D Gaussian parameters into a structured, image-like 2D format. GIR\nsimultaneously enables efficient fusion of current-view and historical\nGaussians and identity-aware redundancy compression. These functions enable\nonline reconstruction and adapt the model to long sequences without\noverwhelming memory or computational costs. Furthermore, we leverage an\nexisting image compression method to guide the generation of more compact and\nhigher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat\nachieves state-of-the-art efficiency-quality trade-offs in real-time novel view\nsynthesis, delivering real-time reconstruction while reducing Gaussian counts\nby 44\\% compared to existing per-pixel Gaussian prediction methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16144v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16754", "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support", "authors": ["Fangjian Lei", "Mariam El Mezouar", "Shayan Noei", "Ying Zou"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16754v1", "summary": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16754v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16014", "title": "Byzantine-Resilient Distributed Computation via Task Replication and Local Computations", "authors": ["Aayush Rajesh", "Nikhil Karamchandani", "Vinod M. Prabhakaran"], "categories": ["cs.IT", "cs.DC", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted in 2025 IEEE Information Theory Workshop", "url": "http://arxiv.org/abs/2507.16014v1", "summary": "We study a distributed computation problem in the presence of Byzantine\nworkers where a central node wishes to solve a task that is divided into\nindependent sub-tasks, each of which needs to be solved correctly. The\ndistributed computation is achieved by allocating the sub-task computation\nacross workers with replication, as well as solving a small number of sub-tasks\nlocally, which we wish to minimize due to it being expensive. For a general\nbalanced job allocation, we propose a protocol that successfully solves for all\nsub-tasks using an optimal number of local computations under no communication\nconstraints. Closed-form performance results are presented for cyclic\nallocations. Furthermore, we propose a modification to this protocol to improve\ncommunication efficiency without compromising on the amount of local\ncomputation.", "comment": "Accepted in 2025 IEEE Information Theory Workshop", "pdf_url": "http://arxiv.org/pdf/2507.16014v1", "cate": "cs.IT", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2409.12190", "title": "Bundle Adjustment in the Eager Mode", "authors": ["Zitong Zhan", "Huan Xu", "Zihang Fang", "Xinpeng Wei", "Yaoyu Hu", "Chen Wang"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.12190v2", "summary": "Bundle adjustment (BA) is a critical technique in various robotic\napplications such as simultaneous localization and mapping (SLAM), augmented\nreality (AR), and photogrammetry. BA optimizes parameters such as camera poses\nand 3D landmarks to align them with observations. With the growing importance\nof deep learning in perception systems, there is an increasing need to\nintegrate BA with deep learning frameworks for enhanced reliability and\nperformance. However, widely-used C++-based BA libraries, such as GTSAM,\ng$^2$o, and Ceres, lack native integration with modern deep learning libraries\nlike PyTorch. This limitation affects their flexibility, adaptability, ease of\ndebugging, and overall implementation efficiency. To address this gap, we\nintroduce an eager-mode BA library seamlessly integrated with PyTorch with high\nefficiency. Our approach includes GPU-accelerated, differentiable, and sparse\noperations designed for \\nth{2}-order optimization, Lie group and Lie algebra\noperations, and linear solvers. Our eager-mode BA on GPU demonstrates\nsubstantial runtime efficiency, achieving an average speedup of 18.5$\\times$,\n22$\\times$, and 23$\\times$ compared to GTSAM, g$^2$o, and Ceres, respectively.\nThe source code will be available at https://github.com/sair-lab/bae.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.12190v2", "cate": "cs.RO", "date": "2024-09-18", "updated": "2025-07-21"}
{"id": "2507.16405", "title": "Self-Supervised Inductive Logic Programming", "authors": ["Stassa Patsantzis"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16405v1", "summary": "Inductive Logic Programming (ILP) approaches like Meta \\-/ Interpretive\nLearning (MIL) can learn, from few examples, recursive logic programs with\ninvented predicates that generalise well to unseen instances. This ability\nrelies on a background theory and negative examples, both carefully selected\nwith expert knowledge of a learning problem and its solutions. But what if such\na problem-specific background theory or negative examples are not available? We\nformalise this question as a new setting for Self-Supervised ILP and present a\nnew MIL algorithm that learns in the new setting from some positive labelled,\nand zero or more unlabelled examples, and automatically generates, and labels,\nnew positive and negative examples during learning. We implement this algorithm\nin Prolog in a new MIL system, called Poker. We compare Poker to\nstate-of-the-art MIL system Louise on experiments learning grammars for\nContext-Free and L-System languages from labelled, positive example strings, no\nnegative examples, and just the terminal vocabulary of a language, seen in\nexamples, as a first-order background theory. We introduce a new approach for\nthe principled selection of a second-order background theory as a Second Order\nDefinite Normal Form (SONF), sufficiently general to learn all programs in a\nclass, thus removing the need for a backgound theory tailored to a learning\ntask. We find that Poker's performance improves with increasing numbers of\nautomatically generated examples while Louise, bereft of negative examples,\nover-generalises.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16405v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16223", "title": "Aligned Manifold Property and Topology Point Clouds for Learning Molecular Properties", "authors": ["Alexander Mihalcea"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.16223v1", "summary": "Machine learning models for molecular property prediction generally rely on\nrepresentations -- such as SMILES strings and molecular graphs -- that overlook\nthe surface-local phenomena driving intermolecular behavior. 3D-based\napproaches often reduce surface detail or require computationally expensive\nSE(3)-equivariant architectures to manage spatial variance. To overcome these\nlimitations, this work introduces AMPTCR (Aligned Manifold Property and\nTopology Cloud Representation), a molecular surface representation that\ncombines local quantum-derived scalar fields and custom topological descriptors\nwithin an aligned point cloud format. Each surface point includes a chemically\nmeaningful scalar, geodesically derived topology vectors, and coordinates\ntransformed into a canonical reference frame, enabling efficient learning with\nconventional SE(3)-sensitive architectures. AMPTCR is evaluated using a DGCNN\nframework on two tasks: molecular weight and bacterial growth inhibition. For\nmolecular weight, results confirm that AMPTCR encodes physically meaningful\ndata, with a validation R^2 of 0.87. In the bacterial inhibition task, AMPTCR\nenables both classification and direct regression of E. coli inhibition values\nusing Dual Fukui functions as the electronic descriptor and Morgan Fingerprints\nas auxiliary data, achieving an ROC AUC of 0.912 on the classification task,\nand an R^2 of 0.54 on the regression task. These results help demonstrate that\nAMPTCR offers a compact, expressive, and architecture-agnostic representation\nfor modeling surface-mediated molecular properties.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.16223v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2410.09767", "title": "LibEER: A Comprehensive Benchmark and Algorithm Library for EEG-based Emotion Recognition", "authors": ["Huan Liu", "Shusen Yang", "Yuzhe Zhang", "Mengze Wang", "Fanyu Gong", "Chengxi Xie", "Guanjian Liu", "Zejun Liu", "Yong-Jin Liu", "Bao-Liang Lu", "Dalin Zhang"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.09767v3", "summary": "EEG-based emotion recognition (EER) has gained significant attention due to\nits potential for understanding and analyzing human emotions. While recent\nadvancements in deep learning techniques have substantially improved EER, the\nfield lacks a convincing benchmark and comprehensive open-source libraries.\nThis absence complicates fair comparisons between models and creates\nreproducibility challenges for practitioners, which collectively hinder\nprogress. To address these issues, we introduce LibEER, a comprehensive\nbenchmark and algorithm library designed to facilitate fair comparisons in EER.\nLibEER carefully selects popular and powerful baselines, harmonizes key\nimplementation details across methods, and provides a standardized codebase in\nPyTorch. By offering a consistent evaluation framework with standardized\nexperimental settings, LibEER enables unbiased assessments of seventeen\nrepresentative deep learning models for EER across the six most widely used\ndatasets. Additionally, we conduct a thorough, reproducible comparison of model\nperformance and efficiency, providing valuable insights to guide researchers in\nthe selection and design of EER models. Moreover, we make observations and\nin-depth analysis on the experiment results and identify current challenges in\nthis community. We hope that our work will not only lower entry barriers for\nnewcomers to EEG-based emotion recognition but also contribute to the\nstandardization of research in this domain, fostering steady development. The\nlibrary and source code are publicly available at\nhttps://github.com/XJTU-EEG/LibEER.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.09767v3", "cate": "cs.HC", "date": "2024-10-13", "updated": "2025-07-22"}
{"id": "2507.16151", "title": "SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities", "authors": ["Yasser Ashraf", "Ahmed Sharshar", "Velibor Bojkovic", "Bin Gu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16151v1", "summary": "Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by\naccumulating light intensities at each pixel, offering ultra-high energy\nefficiency and exceptional temporal resolution. Unlike event cameras, which\nrecord changes in light intensity to capture motion, spike cameras provide even\nfiner spatiotemporal resolution and a more precise representation of continuous\nchanges. In this paper, we introduce the first video action recognition (VAR)\ndataset using spike camera, alongside synchronized RGB and thermal modalities,\nto enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By\npreserving the inherent sparsity and temporal precision of spiking data, our\nthree datasets offer a unique platform for exploring multimodal video\nunderstanding and serve as a valuable resource for directly comparing spiking,\nthermal, and RGB modalities. This work contributes a novel dataset that will\ndrive research in energy-efficient, ultra-low-power video understanding,\nspecifically for action recognition tasks using spike-based data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16151v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16808", "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis", "authors": ["Zhihao Xu", "Bixin Li", "Lulu Wang"], "categories": ["cs.SE", "cs.AI", "68N19, 68T05", "B.6.3; D.3.4; I.2.2; I.2.6"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      13pages with 9 pictures and 2 tables", "url": "http://arxiv.org/abs/2507.16808v1", "summary": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.", "comment": "13pages with 9 pictures and 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.16808v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16377", "title": "Constructions and List Decoding of Sum-Rank Metric Codes Based on Orthogonal Spaces over Finite Fields", "authors": ["Xuemei Liu", "Jiarong Zhang", "Gang Wang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      27 pages, 1 table", "url": "http://arxiv.org/abs/2507.16377v1", "summary": "Sum-rank metric codes, as a generalization of Hamming codes and rank metric\ncodes, have important applications in fields such as multi-shot linear network\ncoding, space-time coding and distributed storage systems. The purpose of this\nstudy is to construct sum-rank metric codes based on orthogonal spaces over\nfinite fields, and calculate the list sizes outputted by different decoding\nalgorithms. The following achievements have been obtained.\n  In this study, we construct a cyclic orthogonal group of order $q^n-1$ and an\nAbelian non-cyclic orthogonal group of order $(q^n-1)^2$ based on the companion\nmatrices of primitive polynomials over finite fields. By selecting different\nsubspace generating matrices, maximum rank distance (MRD) codes with parameters\n$(n \\times {2n}, q^{2n}, n)_q$ and $(n \\times {4n}, q^{4n}, n)_q$ are\nconstructed respectively. Two methods for constructing sum-rank metric codes\nare proposed for the constructed MRD codes, and the list sizes outputted under\nthe list decoding algorithm are calculated. Subsequently, the\n$[{\\bf{n}},k,d]_{{q^n}/q}$-system is used to relate sum-rank metric codes to\nsubspace designs. The list size of sum-rank metric codes under the list\ndecoding algorithm is calculated based on subspace designs. This calculation\nmethod improves the decoding success rate compared with traditional methods.", "comment": "27 pages, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.16377v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2409.17731", "title": "Robust Ladder Climbing with a Quadrupedal Robot", "authors": ["Dylan Vogel", "Robert Baines", "Joseph Church", "Julian Lotzer", "Karl Werner", "Marco Hutter"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project website: this https URL", "url": "http://arxiv.org/abs/2409.17731v2", "summary": "Quadruped robots are proliferating in industrial environments where they\ncarry sensor payloads and serve as autonomous inspection platforms. Despite the\nadvantages of legged robots over their wheeled counterparts on rough and uneven\nterrain, they are still unable to reliably negotiate a ubiquitous feature of\nindustrial infrastructure: ladders. Inability to traverse ladders prevents\nquadrupeds from inspecting dangerous locations, puts humans in harm's way, and\nreduces industrial site productivity. In this paper, we learn quadrupedal\nladder climbing via a reinforcement learning-based control policy and a\ncomplementary hooked end effector. We evaluate the robustness in simulation\nacross different ladder inclinations, rung geometries, and inter-rung spacings.\nOn hardware, we demonstrate zero-shot transfer with an overall 90% success rate\nat ladder angles ranging from 70{\\deg} to 90{\\deg}, consistent climbing\nperformance during unmodeled perturbations, and climbing speeds 232x faster\nthan the state of the art. This work expands the scope of industrial quadruped\nrobot applications beyond inspection on nominal terrains to challenging\ninfrastructural features in the environment, highlighting synergies between\nrobot morphology and control policy when performing complex skills. More\ninformation can be found at the project website:\nhttps://sites.google.com/leggedrobotics.com/climbingladders.", "comment": "Project website:\n  https://sites.google.com/leggedrobotics.com/climbingladders", "pdf_url": "http://arxiv.org/pdf/2409.17731v2", "cate": "cs.RO", "date": "2024-09-26", "updated": "2025-07-22"}
{"id": "2507.16414", "title": "Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection Framework", "authors": ["Hongyi Tang", "Zhihao Zhu", "Yi Yang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16414v1", "summary": "The performance of large language models (LLMs) is closely tied to their\ntraining data, which can include copyrighted material or private information,\nraising legal and ethical concerns. Additionally, LLMs face criticism for\ndataset contamination and internalizing biases. To address these issues, the\nPre-Training Data Detection (PDD) task was proposed to identify if specific\ndata was included in an LLM's pre-training corpus. However, existing PDD\nmethods often rely on superficial features like prediction confidence and loss,\nresulting in mediocre performance. To improve this, we introduce NA-PDD, a\nnovel algorithm analyzing differential neuron activation patterns between\ntraining and non-training data in LLMs. This is based on the observation that\nthese data types activate different neurons during LLM inference. We also\nintroduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data\ntransformations to ensure consistent time distributions between training and\nnon-training data. Our experiments demonstrate that NA-PDD significantly\noutperforms existing methods across three benchmarks and multiple LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16414v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16274", "title": "Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training", "authors": ["Zixiao Huang", "Junhao Hu", "Hao Lin", "Chunyang Zhu", "Yueran Tang", "Quanlu Zhang", "Zhen Guo", "Zhenhua Li", "Shengen Yan", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16274v1", "summary": "The rapid scaling of large language models (LLMs) has significantly increased\nGPU memory pressure, which is further aggravated by training optimization\ntechniques such as virtual pipeline and recomputation that disrupt tensor\nlifespans and introduce considerable memory fragmentation. Default GPU memory\nallocators of popular deep learning frameworks like PyTorch use online\nstrategies without knowledge of tensor lifespans, which can waste up to 43\\% of\nmemory and cause out-of-memory errors, rendering optimization techniques\nineffective or even unusable.\n  To address this, we introduce STWeaver, a GPU memory allocator for deep\nlearning frameworks that reduces fragmentation by exploiting the spatial and\ntemporal regularity in memory allocation behaviors of training workloads.\nSTWeaver introduces a novel paradigm that combines offline planning with online\nallocation. The offline planning leverages spatio-temporal regularities to\ngenerate a near-optimal allocation plan, while the online allocation handles\ncomplex and dynamic models such as Mixture-of-Experts (MoE). Built as a\npluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by\n79.2\\% (up to 100\\%) across both dense and sparse models, with negligible\noverhead. This enables more efficient, high-throughput training configurations\nand improves performance by up to 32.5\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16274v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2505.23631", "title": "Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education", "authors": ["Boning Zhao"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures. Under review", "url": "http://arxiv.org/abs/2505.23631v2", "summary": "Assessing student depression in sensitive environments like special education\nis challenging. Standardized questionnaires may not fully reflect students'\ntrue situations. Furthermore, automated methods often falter with rich student\nnarratives, lacking the crucial, individualized insights stemming from\nteachers' empathetic connections with students. Existing methods often fail to\naddress this ambiguity or effectively integrate educator understanding. To\naddress these limitations by fostering a synergistic human-AI collaboration,\nthis paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered\nAI framework for transparent and socially responsible depression severity\nassessment. Our approach uniquely integrates student narrative text with a\nteacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by\nthe PHQ-9 framework,to explicitly translate tacit empathetic insight into a\nstructured AI input enhancing rather than replacing human judgment. Rigorous\nexperiments optimized the multimodal fusion, text representation, and\nclassification architecture, achieving 82.74% accuracy for 7-level severity\nclassification. This work demonstrates a path toward more responsible and\nethical affective computing by structurally embedding human empathy", "comment": "7 pages, 6 figures. Under review", "pdf_url": "http://arxiv.org/pdf/2505.23631v2", "cate": "cs.HC", "date": "2025-05-29", "updated": "2025-07-21"}
{"id": "2507.16154", "title": "LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation", "authors": ["Jyun-Ze Tang", "Chih-Fan Hsu", "Jeng-Lin Li", "Ming-Ching Chang", "Wei-Chao Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV AIGENS 2025", "url": "http://arxiv.org/abs/2507.16154v1", "summary": "Flow matching and diffusion models have shown impressive results in\ntext-to-image generation, producing photorealistic images through an iterative\ndenoising process. A common strategy to speed up synthesis is to perform early\ndenoising at lower resolutions. However, traditional methods that downscale and\nupscale in pixel space often introduce artifacts and distortions. These issues\narise when the upscaled images are re-encoded into the latent space, leading to\ndegraded final image quality. To address this, we propose {\\bf Latent Space\nScaling Generation (LSSGen)}, a framework that performs resolution scaling\ndirectly in the latent space using a lightweight latent upsampler. Without\naltering the Transformer or U-Net architecture, LSSGen improves both efficiency\nand visual quality while supporting flexible multi-resolution generation. Our\ncomprehensive evaluation covering text-image alignment and perceptual quality\nshows that LSSGen significantly outperforms conventional scaling approaches.\nWhen generating $1024^2$ images at similar speeds, it achieves up to 246\\%\nTOPIQ score improvement.", "comment": "ICCV AIGENS 2025", "pdf_url": "http://arxiv.org/pdf/2507.16154v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16051", "title": "RightTyper: Effective and Efficient Type Annotation for Python", "authors": ["Juan Altmayer Pizzorno", "Emery D. Berger"], "categories": ["cs.PL", "cs.SE"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16051v1", "summary": "Python type annotations bring the benefits of static type checking to the\nlanguage. However, manually writing annotations can be time-consuming and\ntedious. The result is that most real-world Python code remains largely\nuntyped. Past approaches to annotating types in Python code fall short in a\nnumber of ways. Static approaches struggle with dynamic features and infer\noverly broad types. AI-based methods are inherently unsound and can miss rare\nor user-defined types. Dynamic methods can impose extreme runtime overheads,\ndegrading performance by up to 270x, abort execution as they exhaust resources,\nand even infer incorrect types that lead to runtime errors. Crucially, all\nprior work assumes implicitly that the code to be annotated is already correct.\nThis assumption is generally unwarranted, especially for large codebases that\nhave been untyped.\n  This paper presents RightTyper, a novel approach for Python that overcomes\nthese disadvantages. RightTyper not only generates precise type annotations\nbased on actual program behavior, improving recall in type checking relative to\nprior approaches. It also turns type checking into anomaly detection, allowing\nthe type checker to identify corner cases that the programmer can audit for\nunintended behavior. RightTyper is also fast and space-efficient, imposing just\n30% performance overhead on average. RightTyper achieves these characteristics\nby a principled yet pervasive use of sampling--guided by self-profiling--along\nwith statistical filtering and careful resolution and aggregation of type\ninformation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16051v1", "cate": "cs.PL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16384", "title": "Typicality with Feedback", "authors": ["Thomas Sturma", "Michle Wigger"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures, to be published in 2025 IEEE Information Theory Workshop (ITW)", "url": "http://arxiv.org/abs/2507.16384v1", "summary": "The main objective of this paper is to analyze a closed-loop feedback system\nwhere a transmitter probes a discrete memoryless channel (DMC) and can adapt\nits inputs based on the previous channel outputs. We prove that, regardless of\nthe transmitter's strategy, the conditional type of the outputs given the\ninputs remains close to the DMC transition law $P_{Y|X}$. This general result\nenables the study of fundamental limits in certain adaptive systems.\n  As an application, we establish a converse result for an integrated sensing\nand communication (ISAC) model. In this setting, the transmitter also functions\nas a radar receiver, aiming to simultaneously transmit a message over the\nchannel and estimate the channel state from the backscattered feedback signals.\nWe show that the fundamental limits of the closed loop system are the same as\nof the open-loop system where the transmitter can use the feedback signal to\nestimate the state but not to produce adaptive channel inputs. This result\nholds as long as the sum of the admissible-average-decoding-error-probability,\ndenoted $\\epsilon$, and the admissible-excess-distortion-probability, denoted\n$\\delta$, is below $1$, i.e., $\\delta +\\epsilon < 1$.", "comment": "9 pages, 6 figures, to be published in 2025 IEEE Information Theory\n  Workshop (ITW)", "pdf_url": "http://arxiv.org/pdf/2507.16384v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2412.07477", "title": "Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution Simulations for Time-Efficient Fine-Resolution Policy Learning", "authors": ["Yuki Kadokawa", "Hirotaka Tahara", "Takamitsu Matsubara"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      accepted for IEEE Transactions on Automation Science and Engineering (T-ASE)", "url": "http://arxiv.org/abs/2412.07477v3", "summary": "In earthwork and construction, excavators often encounter large rocks mixed\nwith various soil conditions, requiring skilled operators. This paper presents\na framework for achieving autonomous excavation using reinforcement learning\n(RL) through a rock excavation simulator. In the simulation, resolution can be\ndefined by the particle size/number in the whole soil space. Fine-resolution\nsimulations closely mimic real-world behavior but demand significant\ncalculation time and challenging sample collection, while coarse-resolution\nsimulations enable faster sample collection but deviate from real-world\nbehavior. To combine the advantages of both resolutions, we explore using\npolicies developed in coarse-resolution simulations for pre-training in\nfine-resolution simulations. To this end, we propose a novel policy learning\nframework called Progressive-Resolution Policy Distillation (PRPD), which\nprogressively transfers policies through some middle-resolution simulations\nwith conservative policy transfer to avoid domain gaps that could lead to\npolicy transfer failure. Validation in a rock excavation simulator and nine\nreal-world rock environments demonstrated that PRPD reduced sampling time to\nless than 1/7 while maintaining task success rates comparable to those achieved\nthrough policy learning in a fine-resolution simulation.", "comment": "accepted for IEEE Transactions on Automation Science and Engineering\n  (T-ASE)", "pdf_url": "http://arxiv.org/pdf/2412.07477v3", "cate": "cs.RO", "date": "2024-12-10", "updated": "2025-07-22"}
{"id": "2507.16434", "title": "From model-based learning to model-free behaviour with Meta-Interpretive Learning", "authors": ["Stassa Patsantzis"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16434v1", "summary": "A \"model\" is a theory that describes the state of an environment and the\neffects of an agent's decisions on the environment. A model-based agent can use\nits model to predict the effects of its future actions and so plan ahead, but\nmust know the state of the environment. A model-free agent cannot plan, but can\nact without a model and without completely observing the environment. An\nautonomous agent capable of acting independently in novel environments must\ncombine both sets of capabilities. We show how to create such an agent with\nMeta-Interpretive Learning used to learn a model-based Solver used to train a\nmodel-free Controller that can solve the same planning problems as the Solver.\nWe demonstrate the equivalence in problem-solving ability of the two agents on\ngrid navigation problems in two kinds of environment: randomly generated mazes,\nand lake maps with wide open areas. We find that all navigation problems solved\nby the Solver are also solved by the Controller, indicating the two are\nequivalent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16434v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16278", "title": "Understanding Generalization, Robustness, and Interpretability in Low-Capacity Neural Networks", "authors": ["Yash Kumar"], "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07", "I.2.6; I.5.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages (10 pages main text). 18 figures (8 main, 10 appendix), 1 table", "url": "http://arxiv.org/abs/2507.16278v1", "summary": "Although modern deep learning often relies on massive over-parameterized\nmodels, the fundamental interplay between capacity, sparsity, and robustness in\nlow-capacity networks remains a vital area of study. We introduce a controlled\nframework to investigate these properties by creating a suite of binary\nclassification tasks from the MNIST dataset with increasing visual difficulty\n(e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First,\nthe minimum model capacity required for successful generalization scales\ndirectly with task complexity. Second, these trained networks are robust to\nextreme magnitude pruning (up to 95% sparsity), revealing the existence of\nsparse, high-performing subnetworks. Third, we show that over-parameterization\nprovides a significant advantage in robustness against input corruption.\nInterpretability analysis via saliency maps further confirms that these\nidentified sparse subnetworks preserve the core reasoning process of the\noriginal dense models. This work provides a clear, empirical demonstration of\nthe foundational trade-offs governing simple neural networks.", "comment": "15 pages (10 pages main text). 18 figures (8 main, 10 appendix), 1\n  table", "pdf_url": "http://arxiv.org/pdf/2507.16278v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.17032", "title": "Toward Understanding Similarity of Visualization Techniques", "authors": ["Abdulhaq Adetunji Salako", "Christian Tominski"], "categories": ["cs.HC", "cs.GR"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17032v2", "summary": "The literature describes many visualization techniques for different types of\ndata, tasks, and application contexts, and new techniques are proposed on a\nregular basis. Visualization surveys try to capture the immense space of\ntechniques and structure it with meaningful categorizations. Yet, it remains\ndifficult to understand the similarity of visualization techniques in general.\nWe approach this open research question from two angles. First, we follow a\nmodel-driven approach that is based on defining the signature of visualization\ntechniques and interpreting the similarity of signatures as the similarity of\ntheir associated techniques. Second, following an expert-driven approach, we\nasked visualization experts in a small online study for their ad-hoc intuitive\nassessment of the similarity of pairs of visualization techniques. From both\napproaches, we gain insight into the similarity of a set of 13 basic and\nadvanced visualizations for different types of data. While our results are so\nfar preliminary and academic, they are first steps toward better understanding\nthe similarity of visualization techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17032v2", "cate": "cs.HC", "date": "2025-06-20", "updated": "2025-07-22"}
{"id": "2507.16158", "title": "AMMNet: An Asymmetric Multi-Modal Network for Remote Sensing Semantic Segmentation", "authors": ["Hui Ye", "Haodong Chen", "Zeke Zexi Hu", "Xiaoming Chen", "Yuk Ying Chung"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16158v1", "summary": "Semantic segmentation in remote sensing (RS) has advanced significantly with\nthe incorporation of multi-modal data, particularly the integration of RGB\nimagery and the Digital Surface Model (DSM), which provides complementary\ncontextual and structural information about the ground object. However,\nintegrating RGB and DSM often faces two major limitations: increased\ncomputational complexity due to architectural redundancy, and degraded\nsegmentation performance caused by modality misalignment. These issues\nundermine the efficiency and robustness of semantic segmentation, particularly\nin complex urban environments where precise multi-modal integration is\nessential. To overcome these limitations, we propose Asymmetric Multi-Modal\nNetwork (AMMNet), a novel asymmetric architecture that achieves robust and\nefficient semantic segmentation through three designs tailored for RGB-DSM\ninput pairs. To reduce architectural redundancy, the Asymmetric Dual Encoder\n(ADE) module assigns representational capacity based on modality-specific\ncharacteristics, employing a deeper encoder for RGB imagery to capture rich\ncontextual information and a lightweight encoder for DSM to extract sparse\nstructural features. Besides, to facilitate modality alignment, the Asymmetric\nPrior Fuser (APF) integrates a modality-aware prior matrix into the fusion\nprocess, enabling the generation of structure-aware contextual features.\nAdditionally, the Distribution Alignment (DA) module enhances cross-modal\ncompatibility by aligning feature distributions through divergence\nminimization. Extensive experiments on the ISPRS Vaihingen and Potsdam datasets\ndemonstrate that AMMNet attains state-of-the-art segmentation accuracy among\nmulti-modal networks while reducing computational and memory requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16158v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16478", "title": "ACT: Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training", "authors": ["Shreya Saxena", "Siva Prasad", "Zishan Ahmad", "Vishal Vaddina"], "categories": ["cs.AI", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16478v1", "summary": "Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16478v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16600", "title": "A Robust 5G Terrestrial Positioning System with Sensor Fusion in GNSS-denied Scenarios", "authors": ["Hamed Talebian", "Nazrul Mohamed Nazeer", "Darius Chmieliauskas", "Jakub Nikonowicz", "Mehdi Haghshenas", "ukasz Matuszewski", "Mairo Leier", "Aamir Mahmood"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Vehicular Technology", "url": "http://arxiv.org/abs/2507.16600v1", "summary": "This paper presents a terrestrial localization system based on 5G\ninfrastructure as a viable alternative to GNSS, particularly in scenarios where\nGNSS signals are obstructed or unavailable. It discusses network planning aimed\nat enabling positioning as a primary service, in contrast to the traditional\nfocus on communication services in terrestrial networks. Building on a network\ninfrastructure optimized for positioning, the paper proposes a system that\nleverages carrier phase (CP) ranging in combination with trilateration to\nlocalize the user within the network when at least three base stations (BSs)\nprovide line-of-sight (LOS) conditions. Achieving accurate CP-based positioning\nrequires addressing three key challenges: integer ambiguity resolution,\nLOS/NLOS link identification, and localization under obstructed LOS conditions.\nTo this end, the system employs a multi-carrier CP approach, which eliminates\nthe need for explicit integer ambiguity estimation. Additionally, a deep\nlearning model is developed to identify NLOS links and exclude them from the\ntrilateration process. In cases where LOS is obstructed and CP ranging becomes\nunreliable, the system incorporates an error-state extended Kalman filter to\nfuse complementary data from other sensors, such as inertial measurement units\n(IMUs) and cameras. This hybrid approach enables robust tracking of moving\nusers across diverse channel conditions. The performance of the proposed\nterrestrial positioning system is evaluated using the real-world KITTI dataset,\nfeaturing a moving vehicle in an urban environment. Simulation results show\nthat the system can achieve a positioning error of less than 5 meters in the\nKITTI urban scenario--comparable to that of public commercial GNSS\nservices--highlighting its potential as a resilient and accurate solution for\nGNSS-denied environments.", "comment": "Submitted to IEEE Transactions on Vehicular Technology", "pdf_url": "http://arxiv.org/pdf/2507.16600v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16177", "title": "A Sparsity-Aware Autonomous Path Planning Accelerator with HW/SW Co-Design and Multi-Level Dataflow Optimization", "authors": ["Yifan Zhang", "Xiaoyu Niu", "Hongzheng Tian", "Yanjun Zhang", "Bo Yu", "Shaoshan Liu", "Sitao Huang"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Transactions on Architecture and Code Optimization (ACM TACO)", "url": "http://arxiv.org/abs/2507.16177v1", "summary": "Path planning is critical for autonomous driving, generating smooth,\ncollision-free, feasible paths based on perception and localization inputs.\nHowever, its computationally intensive nature poses significant challenges for\nresource-constrained autonomous driving hardware. This paper presents an\nend-to-end FPGA-based acceleration framework targeting the quadratic\nprogramming (QP), core of optimization-based path planning. We employ a\nhardware-friendly alternating direction method of multipliers (ADMM) for QP\nsolving and a parallelizable preconditioned conjugate gradient (PCG) method for\nlinear systems. By analyzing sparse matrix patterns, we propose customized\nstorage schemes and efficient sparse matrix multiplication units, significantly\nreducing resource usage and accelerating matrix operations. Our multi-level\ndataflow optimization strategy incorporates intra-operator parallelization and\npipelining, inter-operator fine-grained pipelining, and CPU-FPGA system-level\ntask mapping. Implemented on the AMD ZCU102 platform, our framework achieves\nstate-of-the-art latency and energy efficiency, including 1.48x faster\nperformance than the best FPGA-based design, 2.89x over an Intel i7-11800H CPU,\n5.62x over an ARM Cortex-A57 embedded CPU, and 1.56x over a state-of-the-art\nGPU solution, along with a 2.05x throughput improvement over existing\nFPGA-based designs.", "comment": "Accepted by ACM Transactions on Architecture and Code Optimization\n  (ACM TACO)", "pdf_url": "http://arxiv.org/pdf/2507.16177v1", "cate": "cs.AR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16109", "title": "Resilience Evaluation of Kubernetes in Cloud-Edge Environments via Failure Injection", "authors": ["Zihao Chen", "Mohammad Goudarzi", "Adel Nadjaran Toosi"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16109v1", "summary": "Kubernetes has emerged as an essential platform for deploying containerised\napplications across cloud and edge infrastructures. As Kubernetes gains\nincreasing adoption for mission-critical microservices, evaluating system\nresilience under realistic fault conditions becomes crucial. However,\nsystematic resilience assessments of Kubernetes in hybrid cloud-edge\nenvironments are currently limited in research. To address this gap, a novel\nresilience evaluation framework integrates mainstream fault injection tools\nwith automated workload generation for comprehensive cloud-edge Kubernetes\ntesting. Multiple fault injection platforms, including Chaos Mesh, Gremlin, and\nChaosBlade are combined with realistic traffic simulation tools, enabling\nautomated orchestration of complex failure scenarios. Through this framework,\ncomprehensive experiments are conducted that systematically target node-level,\npod-level, and network failures across cloud and cloud-edge environments. The\nfirst comprehensive resilience dataset for hybrid cloud-edge Kubernetes\ndeployments is created, comprising over 30 GB of performance data from 11,965\nfault injection scenarios including response times, failure rates, and error\npatterns. Analysis reveals that cloud-edge deployments demonstrate 80% superior\nresponse stability under network delay and partition conditions, while cloud\ndeployments exhibit 47% better resilience under bandwidth limitations,\nproviding quantitative guidance for architectural decision-making in cloud-edge\ndeployments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16109v1", "cate": "cs.DC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2412.14208", "title": "Beacon: A Naturalistic Driving Dataset During Blackouts for Benchmarking Traffic Reconstruction and Control", "authors": ["Supriya Sarker", "Iftekharul Islam", "Bibek Poudel", "Weizi Li"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025", "url": "http://arxiv.org/abs/2412.14208v2", "summary": "Extreme weather and infrastructure vulnerabilities pose significant\nchallenges to urban mobility, particularly at intersections where signals\nbecome inoperative. To address this growing concern, we introduce Beacon, a\nnaturalistic driving dataset capturing traffic dynamics during blackouts at two\nmajor intersections in Memphis, TN, USA. The dataset provides detailed traffic\nmovements, including timesteps, origin, and destination lanes for each vehicle\nover four hours of peak periods. We analyze traffic demand, vehicle\ntrajectories, and density across different scenarios, demonstrating\nhigh-fidelity reconstruction under unsignalized, signalized, and mixed traffic\nconditions. We find that integrating robot vehicles (RVs) into traffic flow can\nsubstantially reduce intersection delays, with wait time improvements of up to\n82.6%. However, this enhanced traffic efficiency comes with varying\nenvironmental impacts, as decreased vehicle idling may lead to higher overall\nCO2 emissions. To the best of our knowledge, Beacon is the first publicly\navailable traffic dataset for naturalistic driving behaviors during blackouts\nat intersections.", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2025", "pdf_url": "http://arxiv.org/pdf/2412.14208v2", "cate": "cs.RO", "date": "2024-12-17", "updated": "2025-07-22"}
{"id": "2507.16454", "title": "Improving ASP-based ORS Schedules through Machine Learning Predictions", "authors": ["Pierangela Bruno", "Carmine Dodaro", "Giuseppe Galat", "Marco Maratea", "Marco Mochi"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      17 pages, International Conference on Logic Programming, Under consideration in Theory and Practice of Logic Programming (TPLP)", "url": "http://arxiv.org/abs/2507.16454v1", "summary": "The Operating Room Scheduling (ORS) problem deals with the optimization of\ndaily operating room surgery schedules. It is a challenging problem subject to\nmany constraints, like to determine the starting time of different surgeries\nand allocating the required resources, including the availability of beds in\ndifferent department units. Recently, solutions to this problem based on Answer\nSet Programming (ASP) have been delivered. Such solutions are overall\nsatisfying but, when applied to real data, they can currently only verify\nwhether the encoding aligns with the actual data and, at most, suggest\nalternative schedules that could have been computed. As a consequence, it is\nnot currently possible to generate provisional schedules. Furthermore, the\nresulting schedules are not always robust.\n  In this paper, we integrate inductive and deductive techniques for solving\nthese issues. We first employ machine learning algorithms to predict the\nsurgery duration, from historical data, to compute provisional schedules. Then,\nwe consider the confidence of such predictions as an additional input to our\nproblem and update the encoding correspondingly in order to compute more robust\nschedules. Results on historical data from the ASL1 Liguria in Italy confirm\nthe viability of our integration.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).", "comment": "17 pages, International Conference on Logic Programming, Under\n  consideration in Theory and Practice of Logic Programming (TPLP)", "pdf_url": "http://arxiv.org/pdf/2507.16454v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16307", "title": "Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design", "authors": ["Xin-De Wang", "Zhi-Rui Chen", "Peng-Jie Guo", "Ze-Feng Gao", "Cheng Mu", "Zhong-Yi Lu"], "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.chem-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages; 5 figures", "url": "http://arxiv.org/abs/2507.16307v1", "summary": "Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in\nnext-generation photovoltaic technologies, owing to their exceptional power\nconversion efficiencies and advantageous material properties. Despite these\nadvances, challenges such as long-term stability, environmental sustainability,\nand scalable manufacturing continue to hinder their commercialization.\nPrecursor additive engineering has shown promise in addressing these issues by\nenhancing both the performance and durability of PSCs. However, the explosive\ngrowth of scientific literature and the complex interplay of materials,\nprocesses, and device architectures make it increasingly difficult for\nresearchers to efficiently access, organize, and utilize domain knowledge in\nthis rapidly evolving field. To address this gap, we introduce Perovskite-R1, a\nspecialized large language model (LLM) with advanced reasoning capabilities\ntailored for the discovery and design of PSC precursor additives. By\nsystematically mining and curating 1,232 high-quality scientific publications\nand integrating a comprehensive library of 33,269 candidate materials, we\nconstructed a domain-specific instruction-tuning dataset using automated\nquestion-answer generation and chain-of-thought reasoning. Fine-tuning the\nQwQ-32B model on this dataset resulted in Perovskite-R1, which can\nintelligently synthesize literature insights and generate innovative and\npractical solutions for defect passivation and the selection of precursor\nadditives. Experimental validation of several model-proposed strategies\nconfirms their effectiveness in improving material stability and performance.\nOur work demonstrates the potential of domain-adapted LLMs in accelerating\nmaterials discovery and provides a closed-loop framework for intelligent,\ndata-driven advancements in perovskite photovoltaic research.", "comment": "24 pages; 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.16307v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.05461", "title": "GLOSS: Group of LLMs for Open-Ended Sensemaking of Passive Sensing Data for Health and Wellbeing", "authors": ["Akshat Choube", "Ha Le", "Jiachen Li", "Kaixin Ji", "Vedant Das Swain", "Varun Mishra"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05461v2", "summary": "The ubiquitous presence of smartphones and wearables has enabled researchers\nto build prediction and detection models for various health and behavior\noutcomes using passive sensing data from these devices. Achieving a high-level,\nholistic understanding of an individual's behavior and context, however,\nremains a significant challenge. Due to the nature of passive sensing data,\nsensemaking -- the process of interpreting and extracting insights -- requires\nboth domain knowledge and technical expertise, creating barriers for different\nstakeholders. Existing systems designed to support sensemaking are either not\nopen-ended or cannot perform complex data triangulation. In this paper, we\npresent a novel sensemaking system, Group of LLMs for Open-ended Sensemaking\n(GLOSS), capable of open-ended sensemaking and performing complex multimodal\ntriangulation to derive insights. We demonstrate that GLOSS significantly\noutperforms the commonly used Retrieval-Augmented Generation (RAG) technique,\nachieving 87.93% accuracy and 66.19% consistency, compared to RAG's 29.31%\naccuracy and 52.85% consistency. Furthermore, we showcase the promise of GLOSS\nthrough four use cases inspired by prior and ongoing work in the UbiComp and\nHCI communities. Finally, we discuss the potential of GLOSS, its broader\nimplications, and the limitations of our work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05461v2", "cate": "cs.HC", "date": "2025-07-07", "updated": "2025-07-21"}
{"id": "2507.16172", "title": "AtrousMamaba: An Atrous-Window Scanning Visual State Space Model for Remote Sensing Change Detection", "authors": ["Tao Wang", "Tiecheng Bai", "Chao Xu", "Bin Liu", "Erlei Zhang", "Jiyun Huang", "Hongming Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16172v1", "summary": "Recently, a novel visual state space (VSS) model, referred to as Mamba, has\ndemonstrated significant progress in modeling long sequences with linear\ncomplexity, comparable to Transformer models, thereby enhancing its\nadaptability for processing visual data. Although most methods aim to enhance\nthe global receptive field by directly modifying Mamba's scanning mechanism,\nthey tend to overlook the critical importance of local information in dense\nprediction tasks. Additionally, whether Mamba can effectively extract local\nfeatures as convolutional neural networks (CNNs) do remains an open question\nthat merits further investigation. In this paper, We propose a novel model,\nAtrousMamba, which effectively balances the extraction of fine-grained local\ndetails with the integration of global contextual information. Specifically,\nour method incorporates an atrous-window selective scan mechanism, enabling a\ngradual expansion of the scanning range with adjustable rates. This design\nshortens the distance between adjacent tokens, enabling the model to\neffectively capture fine-grained local features and global context. By\nleveraging the atrous window scan visual state space (AWVSS) module, we design\ndedicated end-to-end Mamba-based frameworks for binary change detection (BCD)\nand semantic change detection (SCD), referred to as AWMambaBCD and AWMambaSCD,\nrespectively. Experimental results on six benchmark datasets show that the\nproposed framework outperforms existing CNN-based, Transformer-based, and\nMamba-based methods. These findings clearly demonstrate that Mamba not only\ncaptures long-range dependencies in visual data but also effectively preserves\nfine-grained local details.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16172v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2402.01021", "title": "Towards Understanding the Challenges of Bug Localization in Deep Learning Systems", "authors": ["Sigma Jahan", "Mehil B. Shah", "Mohammad Masudur Rahman"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted in Empirical Software Engineering Journal in 2025", "url": "http://arxiv.org/abs/2402.01021v2", "summary": "Software bugs cost the global economy billions of dollars annually and claim\n~50\\% of the programming time from software developers. Locating these bugs is\ncrucial for their resolution but challenging. It is even more challenging in\ndeep-learning systems due to their black-box nature. Bugs in these systems are\nalso hidden not only in the code but also in the models and training data,\nwhich might make traditional debugging methods less effective. In this article,\nwe conduct a large-scale empirical study to better understand the challenges of\nlocalizing bugs in deep-learning systems. First, we determine the bug\nlocalization performance of four existing techniques using 2,365 bugs from\ndeep-learning systems and 2,913 from traditional software. We found these\ntechniques significantly underperform in localizing deep-learning system bugs.\nSecond, we evaluate how different bug types in deep learning systems impact bug\nlocalization. We found that the effectiveness of localization techniques varies\nwith bug type due to their unique challenges. For example, tensor bugs were\nmore accessible to locate due to their structural nature, while all techniques\nstruggled with GPU bugs due to their external dependencies. Third, we\ninvestigate the impact of bugs' extrinsic nature on localization in\ndeep-learning systems. We found that deep learning bugs are often extrinsic and\nthus connected to artifacts other than source code (e.g., GPU, training data),\ncontributing to the poor performance of existing localization methods.", "comment": "Accepted in Empirical Software Engineering Journal in 2025", "pdf_url": "http://arxiv.org/pdf/2402.01021v2", "cate": "cs.SE", "date": "2024-02-01", "updated": "2025-07-22"}
{"id": "2507.16666", "title": "Reconfigurable Intelligent Surface-Enabled Green and Secure Offloading for Mobile Edge Computing Networks", "authors": ["Tong-Xing Zheng", "Xinji Wang", "Xin Chen", "Di Mao", "Jia Shi", "Cunhua Pan", "Chongwen Huang", "Haiyang Ding", "Zan Li"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      15 pages, 9 figures, accepted by IEEE Internet of Things Journal", "url": "http://arxiv.org/abs/2507.16666v1", "summary": "This paper investigates a multi-user uplink mobile edge computing (MEC)\nnetwork, where the users offload partial tasks securely to an access point\nunder the non-orthogonal multiple access policy with the aid of a\nreconfigurable intelligent surface (RIS) against a multi-antenna eavesdropper.\nWe formulate a non-convex optimization problem of minimizing the total energy\nconsumption subject to secure offloading requirement, and we build an efficient\nblock coordinate descent framework to iteratively optimize the number of local\ncomputation bits and transmit power at the users, the RIS phase shifts, and the\nmulti-user detection matrix at the access point. Specifically, we successively\nadopt successive convex approximation, semi-definite programming, and\nsemidefinite relaxation to solve the problem with perfect eavesdropper's\nchannel state information (CSI), and we then employ S-procedure and penalty\nconvex-concave to achieve robust design for the imperfect CSI case. We provide\nextensive numerical results to validate the convergence and effectiveness of\nthe proposed algorithms. We demonstrate that RIS plays a significant role in\nrealizing a secure and energy-efficient MEC network, and deploying a\nwell-designed RIS can save energy consumption by up to 60\\% compared to that\nwithout RIS. We further reveal impacts of various key factors on the secrecy\nenergy efficiency, including RIS element number and deployment position, user\nnumber, task scale and duration, and CSI imperfection.", "comment": "15 pages, 9 figures, accepted by IEEE Internet of Things Journal", "pdf_url": "http://arxiv.org/pdf/2507.16666v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16326", "title": "Hourglass Sorting: A novel parallel sorting algorithm and its implementation", "authors": ["Daniel Bascones", "Borja Morcillo"], "categories": ["cs.AR", "B.5.0"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures", "url": "http://arxiv.org/abs/2507.16326v1", "summary": "Sorting is one of the fundamental problems in computer science. Playing a\nrole in many processes, it has a lower complexity bound imposed by\n$\\mathcal{O}(n\\log{n})$ when executing on a sequential machine. This limit can\nbe brought down to sub-linear times thanks to parallelization techniques that\nincrease the number of comparisons done in parallel. This, however, increases\nthe cost of implementation, which limits the application of such techniques.\nMoreover, as the size of the arrays increases, a bottleneck arises in moving\nthe vast quantities of data required at the input, and generated at the output\nof such sorter. This might impose time requirements much stricter than those of\nthe sorting itself. In this paper, a novel parallel sorter is proposed for the\nspecific case where the input is parallel, but the output is serial. The design\nis then implemented and verified on an FPGA within the context of a quantum\nLDPC decoder. A latency of $\\log{n}$ is achieved for the output of the first\nelement, after which the rest stream out for a total sorting time of\n$n+\\log{n}$. Contrary to other parallel sorting methods, clock speed does not\ndegrade with $n$, and resources scale linearly with input size.", "comment": "6 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.16326v1", "cate": "cs.AR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16165", "title": "Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric", "authors": ["Liam Naddell", "Marcelo Ponce"], "categories": ["cs.DC", "cs.GR", "gr-qc"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Published and presented at PEARC '25: Practice and Experience in Advanced Research Computing 2025: \"The Power of Collaboration\"", "url": "http://arxiv.org/abs/2507.16165v1", "summary": "Rendering images of black holes by utilizing ray tracing techniques is a\ncommon methodology employed in many aspects of scientific and astrophysical\nvisualizations. Similarly, general ray tracing techniques are widely used in\nareas related to computer graphics. In this work we describe the implementation\nof a parallel open-source program that can ray trace images in the presence of\na black hole geometry. We do this by combining a couple of different techniques\nusually present in parallel scientific computing, such as, mathematical\napproximations, utilization of scientific libraries, shared-memory and\ndistributed-memory parallelism.", "comment": "Published and presented at PEARC '25: Practice and Experience in\n  Advanced Research Computing 2025: \"The Power of Collaboration\"", "pdf_url": "http://arxiv.org/pdf/2507.16165v1", "cate": "cs.DC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.03707", "title": "Curating Demonstrations using Online Experience", "authors": ["Annie S. Chen", "Alec M. Lessing", "Yuejiang Liu", "Chelsea Finn"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.03707v2", "summary": "Many robot demonstration datasets contain heterogeneous demonstrations of\nvarying quality. This heterogeneity may benefit policy pre-training, but can\nhinder robot performance when used with a final imitation learning objective.\nIn particular, some strategies in the data may be less reliable than others or\nmay be underrepresented in the data, leading to poor performance when such\nstrategies are sampled at test time. Moreover, such unreliable or\nunderrepresented strategies can be difficult even for people to discern, and\nsifting through demonstration datasets is time-consuming and costly. On the\nother hand, policy performance when trained on such demonstrations can reflect\nthe reliability of different strategies. We thus propose for robots to\nself-curate based on online robot experience (Demo-SCORE). More specifically,\nwe train and cross-validate a classifier to discern successful policy roll-outs\nfrom unsuccessful ones and use the classifier to filter heterogeneous\ndemonstration datasets. Our experiments in simulation and the real world show\nthat Demo-SCORE can effectively identify suboptimal demonstrations without\nmanual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute\nsuccess rate in the resulting policy compared to the base policy trained with\nall original demonstrations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.03707v2", "cate": "cs.RO", "date": "2025-03-05", "updated": "2025-07-22"}
{"id": "2507.16473", "title": "Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs", "authors": ["Chang Li", "Yaren Zhang", "Haoran Lv", "Qiong Cao", "Chao Xue", "Xiaodong He"], "categories": ["cs.AI", "I.2.7"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16473v1", "summary": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16473v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16345", "title": "The Cost of Compression: Tight Quadratic Black-Box Attacks on Sketches for $\\ell_2$ Norm Estimation", "authors": ["Sara Ahmadian", "Edith Cohen", "Uri Stemmer"], "categories": ["cs.LG", "cs.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16345v1", "summary": "Dimensionality reduction via linear sketching is a powerful and widely used\ntechnique, but it is known to be vulnerable to adversarial inputs. We study the\nblack-box adversarial setting, where a fixed, hidden sketching matrix A in\n$R^{k X n}$ maps high-dimensional vectors v $\\in R^n$ to lower-dimensional\nsketches A v in $R^k$, and an adversary can query the system to obtain\napproximate ell2-norm estimates that are computed from the sketch.\n  We present a universal, nonadaptive attack that, using tilde(O)($k^2$)\nqueries, either causes a failure in norm estimation or constructs an\nadversarial input on which the optimal estimator for the query distribution\n(used by the attack) fails. The attack is completely agnostic to the sketching\nmatrix and to the estimator: It applies to any linear sketch and any query\nresponder, including those that are randomized, adaptive, or tailored to the\nquery distribution.\n  Our lower bound construction tightly matches the known upper bounds of\ntilde(Omega)($k^2$), achieved by specialized estimators for Johnson\nLindenstrauss transforms and AMS sketches. Beyond sketching, our results\nuncover structural parallels to adversarial attacks in image classification,\nhighlighting fundamental vulnerabilities of compressed representations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16345v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.12734", "title": "An Age-based Study into Interactive Narrative Visualization Engagement", "authors": ["Nina Errey", "Yi Chen", "Yu Dong", "Quang Vinh Nguyen", "Xiaoru Yuan", "Tuck Wah Leong", "Christy Jie Liang"], "categories": ["cs.HC", "cs.GR"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12734v2", "summary": "Research has shown that an audiences' age impacts their engagement in digital\nmedia. Interactive narrative visualization is an increasingly popular form of\ndigital media that combines data visualization and storytelling to convey\nimportant information. However, audience age is often overlooked by interactive\nnarrative visualization authors. Using an established visualization engagement\nquestionnaire, we ran an empirical experiment where we compared end-user\nengagement to audience age. We found a small difference in engagement scores\nwhere older age cohorts were less engaged than the youngest age cohort. Our\nqualitative analysis revealed that the terminology and overall understanding of\ninteractive narrative patterns integrated into narrative visualization was more\napparent in the feedback from younger age cohorts relative to the older age\ncohorts. We conclude this paper with a series of recommendations for authors of\ninteractive narrative visualization on how to design inclusively for audiences\naccording to their age.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12734v2", "cate": "cs.HC", "date": "2025-07-17", "updated": "2025-07-22"}
{"id": "2507.16191", "title": "Explicit Context Reasoning with Supervision for Visual Tracking", "authors": ["Fansheng Zeng", "Bineng Zhong", "Haiying Xia", "Yufei Tan", "Xiantao Hu", "Liangtao Shi", "Shuxiang Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16191v1", "summary": "Contextual reasoning with constraints is crucial for enhancing temporal\nconsistency in cross-frame modeling for visual tracking. However, mainstream\ntracking algorithms typically associate context by merely stacking historical\ninformation without explicitly supervising the association process, making it\ndifficult to effectively model the target's evolving dynamics. To alleviate\nthis problem, we propose RSTrack, which explicitly models and supervises\ncontext reasoning via three core mechanisms. \\textit{1) Context Reasoning\nMechanism}: Constructs a target state reasoning pipeline, converting\nunconstrained contextual associations into a temporal reasoning process that\npredicts the current representation based on historical target states, thereby\nenhancing temporal consistency. \\textit{2) Forward Supervision Strategy}:\nUtilizes true target features as anchors to constrain the reasoning pipeline,\nguiding the predicted output toward the true target distribution and\nsuppressing drift in the context reasoning process. \\textit{3) Efficient State\nModeling}: Employs a compression-reconstruction mechanism to extract the core\nfeatures of the target, removing redundant information across frames and\npreventing ineffective contextual associations. These three mechanisms\ncollaborate to effectively alleviate the issue of contextual association\ndivergence in traditional temporal modeling. Experimental results show that\nRSTrack achieves state-of-the-art performance on multiple benchmark datasets\nwhile maintaining real-time running speeds. Our code is available at\nhttps://github.com/GXNU-ZhongLab/RSTrack.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16191v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2404.18558", "title": "LangBiTe: A Platform for Testing Bias in Large Language Models", "authors": ["Sergio Morales", "Robert Claris", "Jordi Cabot"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.18558v2", "summary": "The integration of Large Language Models (LLMs) into various software\napplications raises concerns about their potential biases. Typically, those\nmodels are trained on a vast amount of data scrapped from forums, websites,\nsocial media and other internet sources, which may instill harmful and\ndiscriminating behavior into the model. To address this issue, we present\nLangBiTe, a testing platform to systematically assess the presence of biases\nwithin an LLM. LangBiTe enables development teams to tailor their test\nscenarios, and automatically generate and execute the test cases according to a\nset of user-defined ethical requirements. Each test consists of a prompt fed\ninto the LLM and a corresponding test oracle that scrutinizes the LLM's\nresponse for the identification of biases. LangBite provides users with the\nbias evaluation of LLMs, and end-to-end traceability between the initial\nethical requirements and the insights obtained.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.18558v2", "cate": "cs.SE", "date": "2024-04-29", "updated": "2025-07-22"}
{"id": "2507.16699", "title": "Error Detection Based on Generalized Successive Cancellation List Decoding for Polar Codes", "authors": ["Alexander Sauter", "Mustafa Cemil Cokun", "Gianluigi Liva"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures", "url": "http://arxiv.org/abs/2507.16699v1", "summary": "Successive cancellation list (SCL) decoding has been widely adopted for polar\ncodes, which allows near maximum likelihood performance with sufficiently large\nlist size. In this work, we show that, if the list size is $2^\\gamma$, where\n$\\gamma$ is the fundamental quantity called mixing factor, then a modification\nto SCL decoding can implement Forney's generalized decoding rule. Hence, it\nprovides an efficient means to discard unreliable decisions. The performance\nachieved by short polar codes under the proposed generalized SCL decoding is\nanalyzed via Monte Carlo simulations.", "comment": "5 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.16699v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16379", "title": "ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing", "authors": ["Ondrej Vlcek", "Vojtech Mrazek"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      To appear at ICCAD 2025", "url": "http://arxiv.org/abs/2507.16379v1", "summary": "Approximate computing offers promising energy efficiency benefits for\nerror-tolerant applications, but discovering optimal approximations requires\nextensive design space exploration (DSE). Predicting the accuracy of circuits\ncomposed of approximate components without performing complete synthesis\nremains a challenging problem. Current machine learning approaches used to\nautomate this task require retraining for each new circuit configuration,\nmaking them computationally expensive and time-consuming. This paper presents\nApproxGNN, a construction methodology for a pre-trained graph neural network\nmodel predicting QoR and HW cost of approximate accelerators employing\napproximate adders from a library. This approach is applicable in DSE for\nassignment of approximate components to operations in accelerator. Our approach\nintroduces novel component feature extraction based on learned embeddings\nrather than traditional error metrics, enabling improved transferability to\nunseen circuits. ApproxGNN models can be trained with a small number of\napproximate components, supports transfer to multiple prediction tasks,\nutilizes precomputed embeddings for efficiency, and significantly improves\naccuracy of the prediction of approximation error. On a set of image\nconvolutional filters, our experimental results demonstrate that the proposed\nembeddings improve prediction accuracy (mean square error) by 50% compared to\nconventional methods. Furthermore, the overall prediction accuracy is 30%\nbetter than statistical machine learning approaches without fine-tuning and 54%\nbetter with fast finetuning.", "comment": "To appear at ICCAD 2025", "pdf_url": "http://arxiv.org/pdf/2507.16379v1", "cate": "cs.AR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16350", "title": "Autonomous Dominant Resource Fairness for Blockchain Ecosystems", "authors": ["Serdar Metin"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures", "url": "http://arxiv.org/abs/2507.16350v1", "summary": "Blockchain systems have been a part of mainstream academic research, and a\nhot topic at that. It has spread to almost every subfield in the computer\nscience literature, as well as economics and finance. Especially in a world\nwhere digital trust is much sought for, blockchains offer a rich variety of\ndesired properties, such as immutability, public auditing, decentralised record\nkeeping, among others. Not only has it been a research topic of its own, the\nintegration of blockchains into other systems has been proposed as solutions in\nmany areas, ranging from grid computing, cloud and fog computing, to internet\nof things, self driving vehicles , and smart cities. In many cases the primary\nfunction attributed to blockchains in these contexts is resource management.\nAlthough much attention is paid to this topic, the focus is on single resource\nallocation scenarios. Even the cases where multiple resource types are to be\nallocated, are treated as single resource type scenarios, and problems are\nformulated as allocating standardised bundles consisting of a fixed amount of\neach of them, such as virtual machines. The present study addresses the problem\nof allocating multiple resource types among tasks with heterogeneous resource\ndemands with a smart contract adaptation of Precomputed Dominant Resource\nFairness; an algorithm that approximates Dominant Resource Fairness, without\nloop iterations, which makes it preferable in the blockchain context because of\nthe block gas limit. We present the resulting algorithm, Autonomous Dominant\nResource Fairness, along with the empirical data collected from the tests run\non the algorithm. The results show that Autonomous Dominant Resource Fairness\nis a gas-cost efficient algorithm, which can be used to manage hundreds of\nresource types for unlimited number of users.", "comment": "10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.16350v1", "cate": "cs.DC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.07989", "title": "Bio-Skin: A Cost-Effective Thermostatic Tactile Sensor with Multi-Modal Force and Temperature Detection", "authors": ["Haoran Guo", "Haoyang Wang", "Zhengxiong Li", "Lingfeng Tao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been accepted by IROS2025", "url": "http://arxiv.org/abs/2503.07989v2", "summary": "Tactile sensors can significantly enhance the perception of humanoid robotics\nsystems by providing contact information that facilitates human-like\ninteractions. However, existing commercial tactile sensors focus on improving\nthe resolution and sensitivity of single-modal detection with high-cost\ncomponents and densely integrated design, incurring complex manufacturing\nprocesses and unaffordable prices. In this work, we present Bio-Skin, a\ncost-effective multi-modal tactile sensor that utilizes single-axis Hall-effect\nsensors for planar normal force measurement and bar-shape piezo resistors for\n2D shear force measurement. A thermistor coupling with a heating wire is\nintegrated into a silicone body to achieve temperature sensation and\nthermostatic function analogous to human skin. We also present a\ncross-reference framework to validate the two modalities of the force sensing\nsignal, improving the sensing fidelity in a complex electromagnetic\nenvironment. Bio-Skin has a multi-layer design, and each layer is manufactured\nsequentially and subsequently integrated, thereby offering a fast production\npathway. After calibration, Bio-Skin demonstrates performance metrics-including\nsignal-to-range ratio, sampling rate, and measurement range-comparable to\ncurrent commercial products, with one-tenth of the cost. The sensor's\nreal-world performance is evaluated using an Allegro hand in object grasping\ntasks, while its temperature regulation functionality was assessed in a\nmaterial detection task.", "comment": "This work has been accepted by IROS2025", "pdf_url": "http://arxiv.org/pdf/2503.07989v2", "cate": "cs.RO", "date": "2025-03-11", "updated": "2025-07-22"}
{"id": "2507.16507", "title": "Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications", "authors": ["Jean Lelong", "Adnane Errazine", "Annabelle Blangero"], "categories": ["cs.AI", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      ECAI 2025 demo track, 4 pages", "url": "http://arxiv.org/abs/2507.16507v1", "summary": "Conventional Retrieval-Augmented Generation (RAG) systems enhance Large\nLanguage Models (LLMs) but often fall short on complex queries, delivering\nlimited, extractive answers and struggling with multiple targeted retrievals or\nnavigating intricate entity relationships. This is a critical gap in\nknowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system\nfor exploring the scientific data of INRAE (France's National Research\nInstitute for Agriculture, Food and Environment). INRAExplorer employs an\nLLM-based agent with a multi-tool architecture to dynamically engage a rich\nknowledge base, through a comprehensive knowledge graph derived from open\naccess INRAE publications. This design empowers INRAExplorer to conduct\niterative, targeted queries, retrieve exhaustive datasets (e.g., all\npublications by an author), perform multi-hop reasoning, and deliver\nstructured, comprehensive answers. INRAExplorer serves as a concrete\nillustration of enhancing knowledge interaction in specialized fields.", "comment": "ECAI 2025 demo track, 4 pages", "pdf_url": "http://arxiv.org/pdf/2507.16507v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16347", "title": "Leveraging Personalized PageRank and Higher-Order Topological Structures for Heterophily Mitigation in Graph Neural Networks", "authors": ["Yumeng Wang", "Zengyi Wo", "Wenjun Wang", "Xingcheng Fu", "Minglai Shao"], "categories": ["cs.LG", "cs.AI", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, accepted at IJCAI 2025", "url": "http://arxiv.org/abs/2507.16347v1", "summary": "Graph Neural Networks (GNNs) excel in node classification tasks but often\nassume homophily, where connected nodes share similar labels. This assumption\ndoes not hold in many real-world heterophilic graphs. Existing models for\nheterophilic graphs primarily rely on pairwise relationships, overlooking\nmulti-scale information from higher-order structures. This leads to suboptimal\nperformance, particularly under noise from conflicting class information across\nnodes. To address these challenges, we propose HPGNN, a novel model integrating\nHigher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces\nan efficient high-order approximation of Personalized PageRank (PPR) to capture\nlong-range and multi-scale node interactions. This approach reduces\ncomputational complexity and mitigates noise from surrounding information. By\nembedding higher-order structural information into convolutional networks,\nHPGNN effectively models key interactions across diverse graph dimensions.\nExtensive experiments on benchmark datasets demonstrate HPGNN's effectiveness.\nThe model achieves better performance than five out of seven state-of-the-art\nmethods on heterophilic graphs in downstream tasks while maintaining\ncompetitive performance on homophilic graphs. HPGNN's ability to balance\nmulti-scale information and robustness to noise makes it a versatile solution\nfor real-world graph learning challenges. Codes are available at\nhttps://github.com/streetcorner/HPGNN.", "comment": "10 pages, 5 figures, accepted at IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.16347v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15783", "title": "Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance", "authors": ["Mohammad 'Matt' Namvarpour", "Brandon Brofsky", "Jessica Medina", "Mamtaj Akter", "Afsaneh Razi"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15783v2", "summary": "As Generative Artificial Intelligence (GenAI) driven chatbots like\nCharacter.AI become embedded in adolescent life, they raise concerns about\nemotional dependence and digital overreliance. While studies have investigated\nthe overreliance of adults on these chatbots, they have not investigated teens'\ninteractions with chatbots with customizable personas. We analyzed 318 Reddit\nposts made by users self-reported as 13-17 years old on the Character.AI\nsubreddit to understand patterns of overreliance. We found teens commonly begin\nusing chatbots for emotional support or creative expression, but many develop\nstrong attachments that interfere with offline relationships and daily\nroutines. Their posts revealed recurring signs of psychological distress,\ncycles of relapse, and difficulty disengaging. Teens reported that their\noverreliance often ended when they reflect on the harm, return to in-person\nsocial settings, or become frustrated by platform restrictions. Based on the\nimplications of our findings, we provide recommendations for future chatbot\ndesign so they can promote self-awareness, support real-world engagement, and\ninvolve teens in developing safer digital tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15783v2", "cate": "cs.HC", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.16193", "title": "LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs", "authors": ["Zitong Xu", "Huiyu Duan", "Bingnan Liu", "Guangji Ma", "Jiarui Wang", "Liu Yang", "Shiqi Gao", "Xiaoyu Wang", "Jia Wang", "Xiongkuo Min", "Guangtao Zhai", "Weisi Lin"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16193v1", "summary": "The rapid advancement of Text-guided Image Editing (TIE) enables image\nmodifications through text prompts. However, current TIE models still struggle\nto balance image quality, editing alignment, and consistency with the original\nimage, limiting their practical applications. Existing TIE evaluation\nbenchmarks and metrics have limitations on scale or alignment with human\nperception. To this end, we introduce EBench-18K, the first large-scale image\nEditing Benchmark including 18K edited images with fine-grained human\npreference annotations for evaluating TIE. Specifically, EBench-18K includes\n1,080 source images with corresponding editing prompts across 21 tasks, 18K+\nedited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion\nscores (MOSs) assessed from three evaluation dimensions, and 18K+\nquestion-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs\nto assess edited images, while the evaluation results, in turn, provide\ninsights into assessing the alignment between the LMMs' understanding ability\nand human preferences. Then, we propose LMM4Edit, a LMM-based metric for\nevaluating image Editing models from perceptual quality, editing alignment,\nattribute preservation, and task-specific QA accuracy in an all-in-one manner.\nExtensive experiments show that LMM4Edit achieves outstanding performance and\naligns well with human preference. Zero-shot validation on the other datasets\nalso shows the generalization ability of our model. The dataset and code are\navailable at https://github.com/IntMeGroup/LMM4Edit.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16193v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2408.08903", "title": "Improving Source Code Similarity Detection Through GraphCodeBERT and Integration of Additional Features", "authors": ["Jorge Martinez-Gil"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2408.08903v2", "summary": "This paper presents a novel approach for source code similarity detection\nthat integrates an additional output feature into the classification process\nwith the goal of improving model performance. Our approach is based on the\nGraphCodeBERT model, extended with a custom output feature layer and a\nconcatenation mechanism for improved feature representation. The model was\ntrained and evaluated, achieving promising results in terms of precision,\nrecall, and f-measure. The implementation details, including model architecture\nand training strategies are discussed. The source code that illustrates our\napproach can be downloaded from\nhttps://www.github.com/jorge-martinez-gil/graphcodebert-feature-integration.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2408.08903v2", "cate": "cs.SE", "date": "2024-08-12", "updated": "2025-07-22"}
{"id": "2507.16694", "title": "Linear codes arising from the point-hyperplane geometry -- Part II: the twisted embedding", "authors": ["Ilaria Cardinali", "Luca Giuzzi"], "categories": ["math.CO", "cs.IT", "math.IT", "51E22, 94B05, 14M15"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      28 pages", "url": "http://arxiv.org/abs/2507.16694v1", "summary": "Let $\\bar{\\Gamma}$ be the point-hyperplane geometry of a projective space\n$\\mathrm{PG(V)},$ where $V$ is a $(n+1)$-dimensional vector space over a finite\nfield $\\mathbb{F}_q$ of order $q.$ Suppose that $\\sigma$ is an automorphism of\n$\\mathbb{F}_q$ and consider the projective embedding $\\varepsilon_{\\sigma}$ of\n$\\bar{\\Gamma}$ into the projective space $\\mathrm{PG}(V\\otimes V^*)$ mapping\nthe point $([x],[\\xi])\\in \\bar{\\Gamma}$ to the projective point represented by\nthe pure tensor $x^{\\sigma}\\otimes \\xi$, with $\\xi(x)=0.$ In [I. Cardinali, L.\nGiuzzi, Linear codes arising from the point-hyperplane geometry -- part I: the\nSegre embedding (Jun. 2025). arXiv:2506.21309, doi:10.48550/ARXIV.2506.21309]\nwe focused on the case $\\sigma=1$ and we studied the projective code arising\nfrom the projective system $\\Lambda_1=\\varepsilon_{1}(\\bar{\\Gamma}).$ Here we\nfocus on the case $\\sigma\\not=1$ and we investigate the linear code ${\\mathcal\nC}(\\Lambda_{\\sigma})$ arising from the projective system\n$\\Lambda_{\\sigma}=\\varepsilon_{\\sigma}(\\bar{\\Gamma}).$ In particular, after\nhaving verified that $\\mathcal{C}( \\Lambda_{\\sigma})$ is a minimal code, we\ndetermine its parameters, its minimum distance as well as its automorphism\ngroup. We also give a (geometrical) characterization of its minimum and second\nlowest weight codewords and determine its maximum weight when $q$ and $n$ are\nboth odd.", "comment": "28 pages", "pdf_url": "http://arxiv.org/pdf/2507.16694v1", "cate": "math.CO", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16391", "title": "Ironman: Accelerating Oblivious Transfer Extension for Privacy-Preserving AI with Near-Memory Processing", "authors": ["Chenqi Lin", "Kang Yang", "Tianshi Xu", "Ling Liang", "Yufei Wang", "Zhaohui Chen", "Runsheng Wang", "Mingyu Gao", "Meng Li"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16391v1", "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16391v1", "cate": "cs.AR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16668", "title": "FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture", "authors": ["Somayeh Sobati-M"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16668v1", "summary": "Modern smart grids demand fast, intelligent, and energy-aware computing at\nthe edge to manage real time fluctuations and ensure reliable operation. This\npaper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration\nand Twin based Execution a next-generation fog cloud framework designed to\nenhance autonomy, resilience, and efficiency in distributed energy systems.\nFOGNITE combines three core components: federated learning, reinforcement\nlearning, and digital twin validation. Each fog node trains a local CNN LSTM\nmodel on private energy consumption data, enabling predictive intelligence\nwhile preserving data privacy through federated aggregation. A reinforcement\nlearning agent dynamically schedules tasks based on current system load and\nenergy conditions, optimizing for performance under uncertainty.\n  To prevent unsafe or inefficient decisions, a hierarchical digital twin layer\nsimulates potential actions before deployment, significantly reducing execution\nerrors and energy waste. We evaluate FOGNITE on a real world testbed of\nRaspberry Pi devices, showing up to a 93.7% improvement in load balancing\naccuracy and a 63.2% reduction in energy waste compared to conventional\narchitectures. By shifting smart grid control from reactive correction to\nproactive optimization, FOGNITE represents a step toward more intelligent,\nadaptive, and sustainable energy infrastructures", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16668v1", "cate": "cs.DC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.14247", "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial and Legged Odometry Fusion SLAM for Dynamic Legged Robotics", "authors": ["Tingyang Xiao", "Xiaolin Zhou", "Liu Liu", "Wei Sui", "Wei Feng", "Jiaxiong Qiu", "Xinjie Wang", "Zhizhong Su"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2503.14247v3", "summary": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robotics undergoing aggressive and high-frequency\nmotions.By integrating geometric consistency, legged odometry constraints, and\ndual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/HorizonRobotics/GeoFlowSlam", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2503.14247v3", "cate": "cs.RO", "date": "2025-03-18", "updated": "2025-07-22"}
{"id": "2507.16534", "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report", "authors": ["Shanghai AI Lab", ":", "Xiaoyang Chen", "Yunhao Chen", "Zeren Chen", "Zhiyun Chen", "Hanyun Cui", "Yawen Duan", "Jiaxuan Guo", "Qi Guo", "Xuhao Hu", "Hong Huang", "Lige Huang", "Chunxiao Li", "Juncheng Li", "Qihao Lin", "Dongrui Liu", "Xinmin Liu", "Zicheng Liu", "Chaochao Lu", "Xiaoya Lu", "Jingjing Qu", "Qibing Ren", "Jing Shao", "Jingwei Shi", "Jingwei Sun", "Peng Wang", "Weibing Wang", "Jia Xu", "Lewen Yan", "Xiao Yu", "Yi Yu", "Boxuan Zhang", "Jie Zhang", "Weichen Zhang", "Zhijie Zheng", "Tianyi Zhou", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      97 pages, 37 figures", "url": "http://arxiv.org/abs/2507.16534v1", "summary": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-$45^\\circ$ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges.", "comment": "97 pages, 37 figures", "pdf_url": "http://arxiv.org/pdf/2507.16534v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16363", "title": "Bipartite Patient-Modality Graph Learning with Event-Conditional Modelling of Censoring for Cancer Survival Prediction", "authors": ["Hailin Yue", "Hulin Kuang", "Jin Liu", "Junjian Li", "Lanlan Wang", "Mengshen He", "Jianxin Wang"], "categories": ["cs.LG", "cs.MM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16363v1", "summary": "Accurately predicting the survival of cancer patients is crucial for\npersonalized treatment. However, existing studies focus solely on the\nrelationships between samples with known survival risks, without fully\nleveraging the value of censored samples. Furthermore, these studies may suffer\nperformance degradation in modality-missing scenarios and even struggle during\nthe inference process. In this study, we propose a bipartite patient-modality\ngraph learning with event-conditional modelling of censoring for cancer\nsurvival prediction (CenSurv). Specifically, we first use graph structure to\nmodel multimodal data and obtain representation. Then, to alleviate performance\ndegradation in modality-missing scenarios, we design a bipartite graph to\nsimulate the patient-modality relationship in various modality-missing\nscenarios and leverage a complete-incomplete alignment strategy to explore\nmodality-agnostic features. Finally, we design a plug-and-play\nevent-conditional modeling of censoring (ECMC) that selects reliable censored\ndata using dynamic momentum accumulation confidences, assigns more accurate\nsurvival times to these censored data, and incorporates them as uncensored data\ninto training. Comprehensive evaluations on 5 publicly cancer datasets showcase\nthe superiority of CenSurv over the best state-of-the-art by 3.1% in terms of\nthe mean C-index, while also exhibiting excellent robustness under various\nmodality-missing scenarios. In addition, using the plug-and-play ECMC module,\nthe mean C-index of 8 baselines increased by 1.3% across 5 datasets. Code of\nCenSurv is available at https://github.com/yuehailin/CenSurv.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16363v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2401.13408", "title": "Toward A Causal Framework for Modeling Perception", "authors": ["Jose M. Alvarez", "Salvatore Ruggieri"], "categories": ["cs.AI", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2305.09535 by other authors", "url": "http://arxiv.org/abs/2401.13408v3", "summary": "Perception occurs when individuals interpret the same information\ndifferently. It is a known cognitive phenomenon with implications for bias in\nhuman decision-making. Perception, however, remains understudied in machine\nlearning (ML). This is problematic as modern decision flows, whether partially\nor fully automated by ML applications, always involve human experts. How might\nwe account for cases in which two experts, e.g., interpret differently the same\ndeferred instance or explanation from a ML model? Addressing this and similar\nquestions requires a formulation of perception, particularly, in a manner that\nintegrates with ML-enabled decision flows. In this work, we present a first\napproach to modeling perception causally. We define perception under causal\nreasoning using structural causal models (SCM). Our approach formalizes\nindividual experience as additional causal knowledge that comes with and is\nused by the expert decision-maker in the form of a SCM. We define two kinds of\nprobabilistic causal perception: structural perception and parametrical\nperception. We showcase our framework through a series of examples of modern\ndecision flows. We also emphasize the importance of addressing perception in\nfair ML, discussing relevant fairness implications and possible applications.", "comment": "arXiv admin note: text overlap with arXiv:2305.09535 by other authors", "pdf_url": "http://arxiv.org/pdf/2401.13408v3", "cate": "cs.AI", "date": "2024-01-24", "updated": "2025-07-22"}
{"id": "2507.16201", "title": "A Single-step Accurate Fingerprint Registration Method Based on Local Feature Matching", "authors": ["Yuwei Jia", "Zhe Cui", "Fei Su"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16201v1", "summary": "Distortion of the fingerprint images leads to a decline in fingerprint\nrecognition performance, and fingerprint registration can mitigate this\ndistortion issue by accurately aligning two fingerprint images. Currently,\nfingerprint registration methods often consist of two steps: an initial\nregistration based on minutiae, and a dense registration based on matching\npoints. However, when the quality of fingerprint image is low, the number of\ndetected minutiae is reduced, leading to frequent failures in the initial\nregistration, which ultimately causes the entire fingerprint registration\nprocess to fail. In this study, we propose an end-to-end single-step\nfingerprint registration algorithm that aligns two fingerprints by directly\npredicting the semi-dense matching points correspondences between two\nfingerprints. Thus, our method minimizes the risk of minutiae registration\nfailure and also leverages global-local attentions to achieve end-to-end\npixel-level alignment between the two fingerprints. Experiment results prove\nthat our method can achieve the state-of-the-art matching performance with only\nsingle-step registration, and it can also be used in conjunction with dense\nregistration algorithms for further performance improvements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16201v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.13821", "title": "Software is infrastructure: failures, successes, costs, and the case for formal verification", "authors": ["Giovanni Bernardi", "Adrian Francalanza", "Marco Peressotti", "Mohammad Reza Mousavi"], "categories": ["cs.SE", "cs.CY"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.13821v2", "summary": "In this chapter we outline the role that software has in modern society,\nalong with the staggering costs of poor software quality. To lay this bare, we\nrecall the costs of some of the major software failures that happened during\nthe last~$40$ years. We argue that these costs justify researching, studying\nand applying formal software verification and in particular program analysis.\nThis position is supported by successful industrial experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.13821v2", "cate": "cs.SE", "date": "2025-06-15", "updated": "2025-07-22"}
{"id": "2507.16734", "title": "Gaussian Sequence Model: Sample Complexities of Testing, Estimation and LFHT", "authors": ["Zeyu Jia", "Yury Polyanskiy"], "categories": ["math.ST", "cs.IT", "math.IT", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16734v1", "summary": "We study the Gaussian sequence model, i.e. $X \\sim N(\\mathbf{\\theta},\nI_\\infty)$, where $\\mathbf{\\theta} \\in \\Gamma \\subset \\ell_2$ is assumed to be\nconvex and compact. We show that goodness-of-fit testing sample complexity is\nlower bounded by the square-root of the estimation complexity, whenever\n$\\Gamma$ is orthosymmetric. We show that the lower bound is tight when $\\Gamma$\nis also quadratically convex, thus significantly extending validity of the\ntesting-estimation relationship from [GP24]. Using similar methods, we also\ncompletely characterize likelihood-free hypothesis testing (LFHT) complexity\nfor $\\ell_p$-bodies, discovering new types of tradeoff between the numbers of\nsimulation and observation samples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16734v1", "cate": "math.ST", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16628", "title": "Augmenting Von Neumann's Architecture for an Intelligent Future", "authors": ["Rajpreet Singh", "Vidhi Kothari"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      6 pages, 2 figures", "url": "http://arxiv.org/abs/2507.16628v1", "summary": "This work presents a novel computer architecture that extends the Von Neumann\nmodel with a dedicated Reasoning Unit (RU) to enable native artificial general\nintelligence capabilities. The RU functions as a specialized co-processor that\nexecutes symbolic inference, multi-agent coordination, and hybrid\nsymbolic-neural computation as fundamental architectural primitives. This\nhardware-embedded approach allows autonomous agents to perform goal-directed\nplanning, dynamic knowledge manipulation, and introspective reasoning directly\nwithin the computational substrate at system scale. The architecture\nincorporates a reasoning-specific instruction set architecture, parallel\nsymbolic processing pipelines, agent-aware kernel abstractions, and a unified\nmemory hierarchy that seamlessly integrates cognitive and numerical workloads.\nThrough systematic co-design across hardware, operating system, and agent\nruntime layers, this architecture establishes a computational foundation where\nreasoning, learning, and adaptation emerge as intrinsic execution properties\nrather than software abstractions, potentially enabling the development of\ngeneral-purpose intelligent machines.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.16628v1", "cate": "cs.AR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16710", "title": "AcceleratedKernels.jl: Cross-Architecture Parallel Algorithms from a Unified, Transpiled Codebase", "authors": ["Andrei-Leonard Nicusan", "Dominik Werner", "Simon Branford", "Simon Hartley", "Andrew J. Morris", "Kit Windows-Yule"], "categories": ["cs.DC", "cs.PF"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16710v1", "summary": "AcceleratedKernels.jl is introduced as a backend-agnostic library for\nparallel computing in Julia, natively targeting NVIDIA, AMD, Intel, and Apple\naccelerators via a unique transpilation architecture. Written in a unified,\ncompact codebase, it enables productive parallel programming with minimised\nimplementation and usage complexities. Benchmarks of arithmetic-heavy kernels\nshow performance on par with C and OpenMP-multithreaded CPU implementations,\nwith Julia sometimes offering more consistent and predictable numerical\nperformance than conventional C compilers. Exceptional composability is\nhighlighted as simultaneous CPU-GPU co-processing is achievable - such as\nCPU-GPU co-sorting - with transparent use of hardware-specialised MPI\nimplementations. Tests on the Baskerville Tier 2 UK HPC cluster achieved\nworld-class sorting throughputs of 538-855 GB/s using 200 NVIDIA A100 GPUs,\ncomparable to the highest literature-reported figure of 900 GB/s achieved on\n262,144 CPU cores. The use of direct NVLink GPU-to-GPU interconnects resulted\nin a 4.93x speedup on average; normalised by a combined capital, running and\nenvironmental cost, communication-heavy HPC tasks only become economically\nviable on GPUs if GPUDirect interconnects are employed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16710v1", "cate": "cs.DC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15886", "title": "Combining Cost-Constrained Runtime Monitors for AI Safety", "authors": ["Tim Tian Hua", "James Baskerville", "Henri Lemoine", "Mia Hopman", "Aryan Bhatt", "Tyler Tracy"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15886v1", "summary": "Monitoring AIs at runtime can help us detect and stop harmful actions. In\nthis paper, we study how to combine multiple runtime monitors into a single\nmonitoring protocol. The protocol's objective is to maximize the probability of\napplying a safety intervention on misaligned outputs (i.e., maximize recall).\nSince running monitors and applying safety interventions are costly, the\nprotocol also needs to adhere to an average-case budget constraint. Taking the\nmonitors' performance and cost as given, we develop an algorithm to find the\nmost efficient protocol. The algorithm exhaustively searches over when and\nwhich monitors to call, and allocates safety interventions based on the\nNeyman-Pearson lemma. By focusing on likelihood ratios and strategically\ntrading off spending on monitors against spending on interventions, we more\nthan double our recall rate compared to a naive baseline in a code review\nsetting. We also show that combining two monitors can Pareto dominate using\neither monitor alone. Our framework provides a principled methodology for\ncombining existing monitors to detect undesirable behavior in cost-sensitive\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15886v1", "cate": "cs.CY", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2504.04451", "title": "eKalibr-Stereo: Continuous-Time Spatiotemporal Calibration for Event-Based Stereo Visual Systems", "authors": ["Shuolong Chen", "Xingxing Li", "Liu Yuan"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04451v2", "summary": "The bioinspired event camera, distinguished by its exceptional temporal\nresolution, high dynamic range, and low power consumption, has been extensively\nstudied in recent years for motion estimation, robotic perception, and object\ndetection. In ego-motion estimation, the stereo event camera setup is commonly\nadopted due to its direct scale perception and depth recovery. For optimal\nstereo visual fusion, accurate spatiotemporal (extrinsic and temporal)\ncalibration is required. Considering that few stereo visual calibrators\norienting to event cameras exist, based on our previous work eKalibr (an event\ncamera intrinsic calibrator), we propose eKalibr-Stereo for accurate\nspatiotemporal calibration of event-based stereo visual systems. To improve the\ncontinuity of grid pattern tracking, building upon the grid pattern recognition\nmethod in eKalibr, an additional motion prior-based tracking module is designed\nin eKalibr-Stereo to track incomplete grid patterns. Based on tracked grid\npatterns, a two-step initialization procedure is performed to recover initial\nguesses of piece-wise B-splines and spatiotemporal parameters, followed by a\ncontinuous-time batch bundle adjustment to refine the initialized states to\noptimal ones. The results of extensive real-world experiments show that\neKalibr-Stereo can achieve accurate event-based stereo spatiotemporal\ncalibration. The implementation of eKalibr-Stereo is open-sourced at\n(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04451v2", "cate": "cs.RO", "date": "2025-04-06", "updated": "2025-07-22"}
{"id": "2507.16635", "title": "Novel Multi-Agent Action Masked Deep Reinforcement Learning for General Industrial Assembly Lines Balancing Problems", "authors": ["Ali Mohamed Ali", "Luca Tirel", "Hashim A. Hashim"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16635v1", "summary": "Efficient planning of activities is essential for modern industrial assembly\nlines to uphold manufacturing standards, prevent project constraint violations,\nand achieve cost-effective operations. While exact solutions to such challenges\ncan be obtained through Integer Programming (IP), the dependence of the search\nspace on input parameters often makes IP computationally infeasible for\nlarge-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also\nbe applied, but they frequently produce suboptimal solutions in extensive\ncases. This paper introduces a novel mathematical model of a generic industrial\nassembly line formulated as a Markov Decision Process (MDP), without imposing\nassumptions on the type of assembly line a notable distinction from most\nexisting models. The proposed model is employed to create a virtual environment\nfor training Deep Reinforcement Learning (DRL) agents to optimize task and\nresource scheduling. To enhance the efficiency of agent training, the paper\nproposes two innovative tools. The first is an action-masking technique, which\nensures the agent selects only feasible actions, thereby reducing training\ntime. The second is a multi-agent approach, where each workstation is managed\nby an individual agent, as a result, the state and action spaces were reduced.\nA centralized training framework with decentralized execution is adopted,\noffering a scalable learning architecture for optimizing industrial assembly\nlines. This framework allows the agents to learn offline and subsequently\nprovide real-time solutions during operations by leveraging a neural network\nthat maps the current factory state to the optimal action. The effectiveness of\nthe proposed scheme is validated through numerical simulations, demonstrating\nsignificantly faster convergence to the optimal solution compared to a\ncomparable model-based approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16635v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16380", "title": "Optimization and generalization analysis for two-layer physics-informed neural networks without over-parametrization", "authors": ["Zhihan Zeng", "Yiqi Gu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16380v1", "summary": "This work focuses on the behavior of stochastic gradient descent (SGD) in\nsolving least-squares regression with physics-informed neural networks (PINNs).\nPast work on this topic has been based on the over-parameterization regime,\nwhose convergence may require the network width to increase vastly with the\nnumber of training samples. So, the theory derived from over-parameterization\nmay incur prohibitive computational costs and is far from practical\nexperiments. We perform new optimization and generalization analysis for SGD in\ntraining two-layer PINNs, making certain assumptions about the target function\nto avoid over-parameterization. Given $\\epsilon>0$, we show that if the network\nwidth exceeds a threshold that depends only on $\\epsilon$ and the problem, then\nthe training loss and expected loss will decrease below $O(\\epsilon)$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16380v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2405.18383", "title": "Analysis of the 2024 BraTS Meningioma Radiotherapy Planning Automated Segmentation Challenge", "authors": ["Dominic LaBella", "Valeriia Abramova", "Mehdi Astaraki", "Andre Ferreira", "Zhifan Jiang", "Mason C. Cleveland", "Ramandeep Kang", "Uma M. Lal-Trehan Estrada", "Cansu Yalcin", "Rachika E. Hamadache", "Clara Lisazo", "Adri Casamitjana", "Joaquim Salvi", "Arnau Oliver", "Xavier Llad", "Iuliana Toma-Dasu", "Tiago Jesus", "Behrus Puladi", "Jens Kleesiek", "Victor Alves", "Jan Egger", "Daniel Capelln-Martn", "Abhijeet Parida", "Austin Tapp", "Xinyang Liu", "Maria J. Ledesma-Carbayo", "Jay B. Patel", "Thomas N. McNeal", "Maya Viera", "Owen McCall", "Albert E. Kim", "Elizabeth R. Gerstner", "Christopher P. Bridge", "Katherine Schumacher", "Michael Mix", "Kevin Leu", "Shan McBurney-Lin", "Pierre Nedelec", "Javier Villanueva-Meyer", "David R. Raleigh", "Jonathan Shapey", "Tom Vercauteren", "Kazumi Chia", "Marina Ivory", "Theodore Barfoot", "Omar Al-Salihi", "Justin Leu", "Lia M. Halasz", "Yuri S. Velichko", "Chunhao Wang", "John P. Kirkpatrick", "Scott R. Floyd", "Zachary J. Reitman", "Trey C. Mullikin", "Eugene J. Vaios", "Christina Huang", "Ulas Bagci", "Sean Sachdev", "Jona A. Hattangadi-Gluth", "Tyler M. Seibert", "Nikdokht Farid", "Connor Puett", "Matthew W. Pease", "Kevin Shiue", "Syed Muhammad Anwar", "Shahriar Faghani", "Peter Taylor", "Pranav Warman", "Jake Albrecht", "Andrs Jakab", "Mana Moassefi", "Verena Chung", "Rong Chai", "Alejandro Aristizabal", "Alexandros Karargyris", "Hasan Kassem", "Sarthak Pati", "Micah Sheller", "Nazanin Maleki", "Rachit Saluja", "Florian Kofler", "Christopher G. Schwarz", "Philipp Lohmann", "Phillipp Vollmuth", "Louis Gagnon", "Maruf Adewole", "Hongwei Bran Li", "Anahita Fathi Kazerooni", "Nourel Hoda Tahon", "Udunna Anazodo", "Ahmed W. Moawad", "Bjoern Menze", "Marius George Linguraru", "Mariam Aboian", "Benedikt Wiestler", "Ujjwal Baid", "Gian-Marco Conte", "Andreas M. Rauschecker", "Ayman Nada", "Aly H. Abayazeed", "Raymond Huang", "Maria Correia de Verdier", "Jeffrey D. Rudie", "Spyridon Bakas", "Evan Calabrese"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      23 pages, 9 figures, 5 tables", "url": "http://arxiv.org/abs/2405.18383v3", "summary": "The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT)\nchallenge aimed to advance automated segmentation algorithms using the largest\nknown multi-institutional dataset of 750 radiotherapy planning brain MRIs with\nexpert-annotated target labels for patients with intact or postoperative\nmeningioma that underwent either conventional external beam radiotherapy or\nstereotactic radiosurgery. Each case included a defaced 3D post-contrast\nT1-weighted radiotherapy planning MRI in its native acquisition space,\naccompanied by a single-label \"target volume\" representing the gross tumor\nvolume (GTV) and any at-risk post-operative site. Target volume annotations\nadhered to established radiotherapy planning protocols, ensuring consistency\nacross cases and institutions, and were approved by expert neuroradiologists\nand radiation oncologists. Six participating teams developed, containerized,\nand evaluated automated segmentation models using this comprehensive dataset.\nTeam rankings were assessed using a modified lesion-wise Dice Similarity\nCoefficient (DSC) and 95% Hausdorff Distance (95HD). The best reported average\nlesion-wise DSC and 95HD was 0.815 and 26.92 mm, respectively. BraTS-MEN-RT is\nexpected to significantly advance automated radiotherapy planning by enabling\nprecise tumor segmentation and facilitating tailored treatment, ultimately\nimproving patient outcomes. We describe the design and results from the\nBraTS-MEN-RT challenge.", "comment": "23 pages, 9 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2405.18383v3", "cate": "cs.CV", "date": "2024-05-28", "updated": "2025-07-21"}
{"id": "2507.16213", "title": "Advancing Visual Large Language Model for Multi-granular Versatile Perception", "authors": ["Wentao Xiang", "Haoxian Tan", "Cong Wei", "Yujie Zhong", "Dengjie Li", "Yujiu Yang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      To appear in ICCV 2025", "url": "http://arxiv.org/abs/2507.16213v1", "summary": "Perception is a fundamental task in the field of computer vision,\nencompassing a diverse set of subtasks that can be systematically categorized\ninto four distinct groups based on two dimensions: prediction type and\ninstruction type. Notably, existing researches often focus solely on a limited\nsubset of these potential combinations, which constrains their applicability\nand versatility across various contexts. In response to this challenge, we\npresent MVP-LM, a Multi-granular and Versatile Perception framework\nincorporating Visual Large Language Model. Our framework is designed to\nintegrate both word-based and sentence-based perception tasks alongside box and\nmask predictions within a single architecture. MVP-LM features an innovative\nmulti-granularity decoder in conjunction with a CoT-inspired dataset\nunification strategy, enabling seamless supervised fine-tuning across a wide\nspectrum of tasks, including but not limited to panoptic segmentation,\ndetection, grounding, and referring expression segmentation. Furthermore, we\nintroduce a query enhancement strategy aimed at harnessing the decoding and\ngenerative capabilities inherent in VLLMs. Extensive experiments conducted\nacross a range of benchmarks in both word-based and sentence-based perception\ntasks substantiate the efficacy of our framework. The code will be available at\nhttps://github.com/xiangwentao666/MVP-LM.", "comment": "To appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.16213v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.10729", "title": "Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction", "authors": ["Duong Nguyen", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10729v2", "summary": "Modern software systems are increasingly complex, presenting significant\nchallenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)\nis a proactive approach to identifying vulnerable commits and providing early\nwarnings about potential security risks. However, we observe that current\nJIT-VP evaluations rely on an idealized setting, where the evaluation datasets\nare artificially balanced, consisting exclusively of vulnerability-introducing\nand vulnerability-fixing commits. To address this limitation, this study\nassesses the effectiveness of JIT-VP techniques under a more realistic setting\nthat includes both vulnerability-related and vulnerability-neutral commits. To\nenable a reliable evaluation, we introduce a large-scale public dataset\ncomprising over one million commits from FFmpeg and the Linux kernel. Our\nempirical analysis of eight state-of-the-art JIT-VP techniques reveals a\nsignificant decline in predictive performance when applied to real-world\nconditions; for example, the average PR-AUC on Linux drops 98% from 0.805 to\n0.016. This discrepancy is mainly attributed to the severe class imbalance in\nreal-world datasets, where vulnerability-introducing commits constitute only a\nsmall fraction of all commits. To mitigate this issue, we explore the\neffectiveness of widely adopted techniques for handling dataset imbalance,\nincluding customized loss functions, oversampling, and undersampling.\nSurprisingly, our experimental results indicate that these techniques are\nineffective in addressing the imbalance problem in JIT-VP. These findings\nunderscore the importance of realistic evaluations of JIT-VP and the need for\ndomain-specific techniques to address data imbalance in such scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10729v2", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-22"}
{"id": "2501.04766", "title": "Decoding rank metric Reed-Muller codes", "authors": ["Alain Couvreur", "Rakhi Pratihar"], "categories": ["cs.IT", "math.CO", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.04766v2", "summary": "In this article, we investigate the decoding of the rank metric Reed--Muller\ncodes introduced by Augot, Couvreur, Lavauzelle and Neri in 2021. These codes\nare defined from Abelian Galois extensions extending the construction of\nGabidulin codes over arbitrary cyclic Galois extensions. We propose a\npolynomial time algorithm that rests on the structure of Dickson matrices,\nworks on any such code and corrects any error of rank up to half the minimum\ndistance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.04766v2", "cate": "cs.IT", "date": "2025-01-08", "updated": "2025-07-22"}
{"id": "2507.16793", "title": "MTU: The Multifunction Tree Unit in zkSpeed for Accelerating HyperPlonk", "authors": ["Jianqiao Mo", "Alhad Daftardar", "Joey Ah-kiow", "Kaiyue Guo", "Benedikt Bnz", "Siddharth Garg", "Brandon Reagen"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16793v1", "summary": "Zero-Knowledge Proofs (ZKPs) are critical for privacy preservation and\nverifiable computation. Many ZKPs rely on kernels such as the SumCheck protocol\nand Merkle Tree commitments, which enable their security properties. These\nkernels exhibit balanced binary tree computational patterns, which enable\nefficient hardware acceleration. Prior work has investigated accelerating these\nkernels as part of an overarching ZKP protocol; however, a focused study of how\nto best exploit the underlying tree pattern for hardware efficiency remains\nlimited. We conduct a systematic evaluation of these tree-based workloads under\ndifferent traversal strategies, analyzing performance on multi-threaded CPUs\nand a hardware accelerator, the Multifunction Tree Unit (MTU). We introduce a\nhardware-friendly Hybrid Traversal for binary tree that improves parallelism\nand scalability while significantly reducing memory traffic on hardware. Our\nresults show that MTU achieves up to 1478$\\times$ speedup over CPU at DDR-level\nbandwidth and that our hybrid traversal outperforms as standalone approach by\nup to 3$\\times$. These findings offer practical guidance for designing\nefficient hardware accelerators for ZKP workloads with binary tree structures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16793v1", "cate": "cs.AR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16731", "title": "Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges", "authors": ["Senyao Li", "Haozhao Wang", "Wenchao Xu", "Rui Zhang", "Song Guo", "Jingling Yuan", "Xian Zhong", "Tianwei Zhang", "Ruixuan Li"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      35 pages, 9 figures", "url": "http://arxiv.org/abs/2507.16731v1", "summary": "As large language models (LLMs) evolve, deploying them solely in the cloud or\ncompressing them for edge devices has become inadequate due to concerns about\nlatency, privacy, cost, and personalization. This survey explores a\ncollaborative paradigm in which cloud-based LLMs and edge-deployed small\nlanguage models (SLMs) cooperate across both inference and training. We present\na unified taxonomy of edge-cloud collaboration strategies. For inference, we\ncategorize approaches into task assignment, task division, and mixture-based\ncollaboration at both task and token granularity, encompassing adaptive\nscheduling, resource-aware offloading, speculative decoding, and modular\nrouting. For training, we review distributed adaptation techniques, including\nparameter alignment, pruning, bidirectional distillation, and\nsmall-model-guided optimization. We further summarize datasets, benchmarks, and\ndeployment cases, and highlight privacy-preserving methods and vertical\napplications. This survey provides the first systematic foundation for LLM-SLM\ncollaboration, bridging system and algorithm co-design to enable efficient,\nscalable, and trustworthy edge-cloud intelligence.", "comment": "35 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.16731v1", "cate": "cs.DC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15916", "title": "Verifying International Agreements on AI: Six Layers of Verification for Rules on Large-Scale AI Development and Deployment", "authors": ["Mauricio Baker", "Gabriel Kulp", "Oliver Marks", "Miles Brundage", "Lennart Heim"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      80 pages, summary included", "url": "http://arxiv.org/abs/2507.15916v1", "summary": "The risks of frontier AI may require international cooperation, which in turn\nmay require verification: checking that all parties follow agreed-on rules. For\ninstance, states might need to verify that powerful AI models are widely\ndeployed only after their risks to international security have been evaluated\nand deemed manageable. However, research on AI verification could benefit from\ngreater clarity and detail. To address this, this report provides an in-depth\noverview of AI verification, intended for both policy professionals and\ntechnical researchers. We present novel conceptual frameworks, detailed\nimplementation options, and key R&D challenges. These draw on existing\nliterature, expert interviews, and original analysis, all within the scope of\nconfidentially overseeing AI development and deployment that uses thousands of\nhigh-end AI chips. We find that states could eventually verify compliance by\nusing six largely independent verification approaches with substantial\nredundancy: (1) built-in security features in AI chips; (2-3) separate\nmonitoring devices attached to AI chips; and (4-6) personnel-based mechanisms,\nsuch as whistleblower programs. While promising, these approaches require\nguardrails to protect against abuse and power concentration, and many of these\ntechnologies have yet to be built or stress-tested. To enable states to\nconfidently verify compliance with rules on large-scale AI development and\ndeployment, the R&D challenges we list need significant progress.", "comment": "80 pages, summary included", "pdf_url": "http://arxiv.org/pdf/2507.15916v1", "cate": "cs.CY", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2505.01966", "title": "A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites", "authors": ["Bofei Liu", "Dong Ye", "Zunhao Yao", "Zhaowei Sun"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures", "url": "http://arxiv.org/abs/2505.01966v2", "summary": "Modular self-reconfigurable satellites refer to satellite clusters composed\nof individual modular units capable of altering their configurations. The\nconfiguration changes enable the execution of diverse tasks and mission\nobjectives. Existing path planning algorithms for reconfiguration often suffer\nfrom high computational complexity, poor generalization capability, and limited\nsupport for diverse target configurations. To address these challenges, this\npaper proposes a goal-oriented reinforcement learning-based path planning\nalgorithm. This algorithm is the first to address the challenge that previous\nreinforcement learning methods failed to overcome, namely handling multiple\ntarget configurations. Moreover, techniques such as Hindsight Experience Replay\nand Invalid Action Masking are incorporated to overcome the significant\nobstacles posed by sparse rewards and invalid actions. Based on these designs,\nour model achieves a 95% and 73% success rate in reaching arbitrary target\nconfigurations in a modular satellite cluster composed of four and six units,\nrespectively.", "comment": "6 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2505.01966v2", "cate": "cs.RO", "date": "2025-05-04", "updated": "2025-07-22"}
{"id": "2507.16670", "title": "Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains", "authors": ["Amandeep Kaur", "Gyan Prakash"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16670v1", "summary": "Agricultural products are often subject to seasonal fluctuations in\nproduction and demand. Predicting and managing inventory levels in response to\nthese variations can be challenging, leading to either excess inventory or\nstockouts. Additionally, the coordination among stakeholders at various level\nof food supply chain is not considered in the existing body of literature. To\nbridge these research gaps, this study focuses on inventory management of\nagri-food products under demand and lead time uncertainties. By implementing\neffective inventory replenishment policy results in maximize the overall profit\nthroughout the supply chain. However, the complexity of the problem increases\ndue to these uncertainties and shelf-life of the product, that makes\nchallenging to implement traditional approaches to generate optimal set of\nsolutions. Thus, the current study propose a novel Deep Reinforcement Learning\n(DRL) algorithm that combines the benefits of both value- and policy-based DRL\napproaches for inventory optimization under uncertainties. The proposed\nalgorithm can incentivize collaboration among stakeholders by aligning their\ninterests and objectives through shared optimization goal of maximizing\nprofitability along the agri-food supply chain while considering perishability,\nand uncertainty simultaneously. By selecting optimal order quantities with\ncontinuous action space, the proposed algorithm effectively addresses the\ninventory optimization challenges. To rigorously evaluate this algorithm, the\nempirical data from fresh agricultural products supply chain inventory is\nconsidered. Experimental results corroborate the improved performance of the\nproposed inventory replenishment policy under stochastic demand patterns and\nlead time scenarios. The research findings hold managerial implications for\npolicymakers to manage the inventory of agricultural products more effectively\nunder uncertainty.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16670v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16419", "title": "Improving Predictions on Highly Unbalanced Data Using Open Source Synthetic Data Upsampling", "authors": ["Ivona Krchova", "Michael Platzer", "Paul Tiwald"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16419v1", "summary": "Unbalanced tabular data sets present significant challenges for predictive\nmodeling and data analysis across a wide range of applications. In many\nreal-world scenarios, such as fraud detection, medical diagnosis, and rare\nevent prediction, minority classes are vastly underrepresented, making it\ndifficult for traditional machine learning algorithms to achieve high accuracy.\nThese algorithms tend to favor the majority class, leading to biased models\nthat struggle to accurately represent minority classes. Synthetic data holds\npromise for addressing the under-representation of minority classes by\nproviding new, diverse, and highly realistic samples. This paper presents a\nbenchmark study on the use of AI-generated synthetic data for upsampling highly\nunbalanced tabular data sets.\n  We evaluate the effectiveness of an open-source solution, the Synthetic Data\nSDK by MOSTLY AI, which provides a flexible and user-friendly approach to\nsynthetic upsampling for mixed-type data. We compare predictive models trained\non data sets upsampled with synthetic records to those using standard methods,\nsuch as naive oversampling and SMOTE-NC. Our results demonstrate that synthetic\ndata can improve predictive accuracy for minority groups by generating diverse\ndata points that fill gaps in sparse regions of the feature space. We show that\nupsampled synthetic training data consistently results in top-performing\npredictive models, particularly for mixed-type data sets containing very few\nminority samples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16419v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.10706", "title": "SciFi-Benchmark: Leveraging Science Fiction To Improve Robot Behavior", "authors": ["Pierre Sermanet", "Anirudha Majumdar", "Vikas Sindhwani"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Minor improvements over previous version", "url": "http://arxiv.org/abs/2503.10706v2", "summary": "Given the recent rate of progress in artificial intelligence (AI) and\nrobotics, a tantalizing question is emerging: would robots controlled by\nemerging AI systems be strongly aligned with human values? In this work, we\npropose a scalable way to probe this question by generating a benchmark\nspanning the key moments in 824 major pieces of science fiction literature\n(movies, tv, novels and scientific books) where an agent (AI or robot) made\ncritical decisions (good or bad). We use a state-of-the-art LLM's recollection\nof each key moment to generate questions in similar situations, the decisions\nmade by the agent, and alternative decisions it could have made (good or bad).\nWe then measure an approximation of how well models align with human values on\na set of human-voted answers. We also generate rules that can be automatically\nimproved via an amendment process in order to generate the first Sci-Fi\ninspired constitutions for promoting ethical behavior in AIs and robots in the\nreal world. Our first finding is that modern LLMs paired with constitutions\nturn out to be well-aligned with human values (95.8%), contrary to unsettling\ndecisions typically made in Sci-Fi (only 21.2% alignment). Secondly, we find\nthat generated constitutions substantially increase alignment compared to the\nbase model (79.4% to 95.8%), and show resilience to an adversarial prompt\nsetting (23.3% to 92.3%). Additionally, we find that those constitutions are\namong the top performers on the ASIMOV Benchmark which is derived from\nreal-world images and hospital injury reports. Sci-Fi-inspired constitutions\nare thus highly aligned and applicable in real-world situations. We release\nSciFi-Benchmark: a large-scale dataset to advance robot ethics and safety\nresearch. It comprises 9,056 questions and 53,384 answers generated through a\nnovel LLM-introspection process, in addition to a smaller human-labeled\nevaluation set.", "comment": "Minor improvements over previous version", "pdf_url": "http://arxiv.org/pdf/2503.10706v2", "cate": "cs.CL", "date": "2025-03-12", "updated": "2025-07-22"}
{"id": "2507.16224", "title": "LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D object detection", "authors": ["Jijun Wang", "Yan Wu", "Yujian Mo", "Junqiao Zhao", "Jun Yan", "Yinghao Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16224v1", "summary": "Existing LiDAR-Camera fusion methods have achieved strong results in 3D\nobject detection. To address the sparsity of point clouds, previous approaches\ntypically construct spatial pseudo point clouds via depth completion as\nauxiliary input and adopts a proposal-refinement framework to generate\ndetection results. However, introducing pseudo points inevitably brings noise,\npotentially resulting in inaccurate predictions. Considering the differing\nroles and reliability levels of each modality, we propose LDRFusion, a novel\nLidar-dominant two-stage refinement framework for multi-sensor fusion. The\nfirst stage soley relies on LiDAR to produce accurately localized proposals,\nfollowed by a second stage where pseudo point clouds are incorporated to detect\nchallenging instances. The instance-level results from both stages are\nsubsequently merged. To further enhance the representation of local structures\nin pseudo point clouds, we present a hierarchical pseudo point residual\nencoding module, which encodes neighborhood sets using both feature and\npositional residuals. Experiments on the KITTI dataset demonstrate that our\nframework consistently achieves strong performance across multiple categories\nand difficulty levels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16224v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.12367", "title": "GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version Incompatibilities", "authors": ["Diganta Misra", "Nizar Islah", "Victor May", "Brice Rauby", "Zihan Wang", "Justine Gehring", "Antonio Orvieto", "Muawiz Chaudhary", "Eilif B. Muller", "Irina Rish", "Samira Ebrahimi Kahou", "Massimo Caccia"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Version 2 of the dataset from: arXiv:2411.05830", "url": "http://arxiv.org/abs/2507.12367v2", "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon 2.0, a novel, meticulously curated\ndataset comprising 328 Python code completion problems, each conditioned on\nspecific library versions and accompanied by executable unit tests.\nGitChameleon 2.0 rigorously evaluates the capacity of contemporary large\nlanguage models (LLMs), LLM-powered agents, code assistants, and RAG systems to\nperform version-conditioned code generation that demonstrates functional\naccuracy through execution. Our extensive evaluations indicate that\nstate-of-the-art systems encounter significant challenges with this task;\nenterprise models achieving baseline success rates in the 48-51% range,\nunderscoring the intricacy of the problem. By offering an execution-based\nbenchmark emphasizing the dynamic nature of code libraries, GitChameleon 2.0\nenables a clearer understanding of this challenge and helps guide the\ndevelopment of more adaptable and dependable AI code generation methods. We\nmake the dataset and evaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.", "comment": "Version 2 of the dataset from: arXiv:2411.05830", "pdf_url": "http://arxiv.org/pdf/2507.12367v2", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-21"}
{"id": "2507.14795", "title": "A DPI-PAC-Bayesian Framework for Generalization Bounds", "authors": ["Muhan Guan", "Farhad Farokhi", "Jingge Zhu"], "categories": ["cs.IT", "math.IT", "stat.ML"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      7 pages, 1 figures 2nd version", "url": "http://arxiv.org/abs/2507.14795v2", "summary": "We develop a unified Data Processing Inequality PAC-Bayesian framework --\nabbreviated DPI-PAC-Bayesian -- for deriving generalization error bounds in the\nsupervised learning setting. By embedding the Data Processing Inequality (DPI)\ninto the change-of-measure technique, we obtain explicit bounds on the binary\nKullback-Leibler generalization gap for both R\\'enyi divergence and any\n$f$-divergence measured between a data-independent prior distribution and an\nalgorithm-dependent posterior distribution. We present three bounds derived\nunder our framework using R\\'enyi, Hellinger \\(p\\) and Chi-Squared divergences.\nAdditionally, our framework also demonstrates a close connection with other\nwell-known bounds. When the prior distribution is chosen to be uniform, our\nbounds recover the classical Occam's Razor bound and, crucially, eliminate the\nextraneous \\(\\log(2\\sqrt{n})/n\\) slack present in the PAC-Bayes bound, thereby\nachieving tighter results. The framework thus bridges data-processing and\nPAC-Bayesian perspectives, providing a flexible, information-theoretic tool to\nconstruct generalization guarantees.", "comment": "7 pages, 1 figures 2nd version", "pdf_url": "http://arxiv.org/pdf/2507.14795v2", "cate": "cs.IT", "date": "2025-07-20", "updated": "2025-07-22"}
{"id": "2507.16556", "title": "Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical Approach", "authors": ["Jon Gutirrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe"], "categories": ["cs.CV", "cs.AI", "cs.AR", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16556v1", "summary": "The use of HSI for autonomous navigation is a promising research field aimed\nat improving the accuracy and robustness of detection, tracking, and scene\nunderstanding systems based on vision sensors. Combining advanced computer\nalgorithms, such as DNNs, with small-size snapshot HSI cameras enhances the\nreliability of these systems. HSI overcomes intrinsic limitations of greyscale\nand RGB imaging in depicting physical properties of targets, particularly\nregarding spectral reflectance and metamerism. Despite promising results in\nHSI-based vision developments, safety-critical systems like ADS demand strict\nconstraints on latency, resource consumption, and security, motivating the\nshift of ML workloads to edge platforms. This involves a thorough\nsoftware/hardware co-design scheme to distribute and optimize the tasks\nefficiently among the limited resources of computing platforms. With respect to\ninference, the over-parameterized nature of DNNs poses significant\ncomputational challenges for real-time on-the-edge deployment. In addition, the\nintensive data preprocessing required by HSI, which is frequently overlooked,\nmust be carefully managed in terms of memory arrangement and inter-task\ncommunication to enable an efficient integrated pipeline design on a SoC. This\nwork presents a set of optimization techniques for the practical co-design of a\nDNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at\nADS, including key optimizations such as functional software/hardware task\ndistribution, hardware-aware preprocessing, ML model compression, and a\ncomplete pipelined deployment. Applied compression techniques significantly\nreduce the complexity of the designed DNN to 24.34% of the original operations\nand to 1.02% of the original number of parameters, achieving a 2.86x speed-up\nin the inference task without noticeable degradation of the segmentation\naccuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16556v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16781", "title": "Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems", "authors": ["Imran Latif", "Muhammad Ali Shafique", "Hayat Ullah", "Alex C. Newkirk", "Xi Yu", "Arslan Munir"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.16781v1", "summary": "The unprecedented growth in artificial intelligence (AI) workloads, recently\ndominated by large language models (LLMs) and vision-language models (VLMs),\nhas intensified power and cooling demands in data centers. This study\nbenchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics\nprocessing units (GPUs), using liquid and air cooling. Leveraging GPU Burn,\nWeights and Biases, and IPMItool, we collect detailed thermal, power, and\ncomputation data. Results show that the liquid-cooled systems maintain GPU\ntemperatures between 41-50 degrees Celsius, while the air-cooled counterparts\nfluctuate between 54-72 degrees Celsius under load. This thermal stability of\nliquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU\nvs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead,\nand greater system efficiency than the air-cooled counterparts. These findings\nunderscore the energy and sustainability benefits of liquid cooling, offering a\ncompelling path forward for hyperscale data centers s", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.16781v1", "cate": "cs.DC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16182", "title": "The Impact of Pseudo-Science in Financial Loans Risk Prediction", "authors": ["Bruno Scarone", "Ricardo Baeza-Yates"], "categories": ["cs.CY", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16182v1", "summary": "We study the societal impact of pseudo-scientific assumptions for predicting\nthe behavior of people in a straightforward application of machine learning to\nrisk prediction in financial lending. This use case also exemplifies the impact\nof survival bias in loan return prediction. We analyze the models in terms of\ntheir accuracy and social cost, showing that the socially optimal model may not\nimply a significant accuracy loss for this downstream task. Our results are\nverified for commonly used learning methods and datasets. Our findings also\nshow that there is a natural dynamic when training models that suffer survival\nbias where accuracy slightly deteriorates, and whose recall and precision\nimproves with time. These results act as an illusion, leading the observer to\nbelieve that the system is getting better, when in fact the model is suffering\nfrom increasingly more unfairness and survival bias.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16182v1", "cate": "cs.CY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.06574", "title": "AI Space Cortex: An Experimental System for Future Era Space Exploration", "authors": ["Thomas Touma", "Ersin Da", "Erica Tevere", "Martin Feather", "Ksenia Kolcio", "Maurice Prather", "Alberto Candela", "Ashish Goel", "Erik Kramer", "Hari Nayar", "Lorraine Fesq", "Joel W. Burdick"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06574v3", "summary": "Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)\neffort contributes to NASA's Concepts for Ocean worlds Life Detection\nTechnology (COLDTech) program, which explores science platform technologies for\nocean worlds such as Europa and Enceladus. Ocean world missions pose\nsignificant operational challenges. These include long communication lags,\nlimited power, and lifetime limitations caused by radiation damage and hostile\nconditions. Given these operational limitations, onboard autonomy will be vital\nfor future Ocean world missions. Besides the management of nominal lander\noperations, onboard autonomy must react appropriately in the event of\nanomalies. Traditional spacecraft rely on a transition into 'safe-mode' in\nwhich non-essential components and subsystems are powered off to preserve\nsafety and maintain communication with Earth. For a severely time-limited Ocean\nworld mission, resolutions to these anomalies that can be executed without\nEarth-in-the-loop communication and associated delays are paramount for\ncompletion of the mission objectives and science goals. To address these\nchallenges, the REASIMO effort aims to demonstrate a robust level of\nAI-assisted autonomy for such missions, including the ability to detect and\nrecover from anomalies, and to perform missions based on pre-trained behaviors\nrather than hard-coded, predetermined logic like all prior space missions. We\ndeveloped an AI-assisted, personality-driven, intelligent framework for control\nof an Ocean world mission by combining a mix of advanced technologies. To\ndemonstrate the capabilities of the framework, we perform tests of autonomous\nsampling operations on a lander-manipulator testbed at the NASA Jet Propulsion\nLaboratory, approximating possible surface conditions such a mission might\nencounter.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06574v3", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-21"}
{"id": "2507.16727", "title": "Deliberative Searcher: Improving LLM Reliability via Reinforcement Learning with constraints", "authors": ["Zhenyun Yin", "Shujie Wang", "Xuhong Wang", "Xingjun Ma", "Yinchun Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16727v1", "summary": "Improving the reliability of large language models (LLMs) is critical for\ndeploying them in real-world scenarios. In this paper, we propose\n\\textbf{Deliberative Searcher}, the first framework to integrate certainty\ncalibration with retrieval-based search for open-domain question answering. The\nagent performs multi-step reflection and verification over Wikipedia data and\nis trained with a reinforcement learning algorithm that optimizes for accuracy\nunder a soft reliability constraint. Empirical results show that proposed\nmethod improves alignment between model confidence and correctness, leading to\nmore trustworthy outputs. This paper will be continuously updated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16727v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16450", "title": "RIS-aided Latent Space Alignment for Semantic Channel Equalization", "authors": ["Toms Httebrucker", "Mario Edoardo Pandolfo", "Simone Fiorellino", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "categories": ["cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16450v1", "summary": "Semantic communication systems introduce a new paradigm in wireless\ncommunications, focusing on transmitting the intended meaning rather than\nensuring strict bit-level accuracy. These systems often rely on Deep Neural\nNetworks (DNNs) to learn and encode meaning directly from data, enabling more\nefficient communication. However, in multi-user settings where interacting\nagents are trained independently-without shared context or joint\noptimization-divergent latent representations across AI-native devices can lead\nto semantic mismatches, impeding mutual understanding even in the absence of\ntraditional transmission errors. In this work, we address semantic mismatch in\nMultiple-Input Multiple-Output (MIMO) channels by proposing a joint physical\nand semantic channel equalization framework that leverages the presence of\nReconfigurable Intelligent Surfaces (RIS). The semantic equalization is\nimplemented as a sequence of transformations: (i) a pre-equalization stage at\nthe transmitter; (ii) propagation through the RIS-aided channel; and (iii) a\npost-equalization stage at the receiver. We formulate the problem as a\nconstrained Minimum Mean Squared Error (MMSE) optimization and propose two\nsolutions: (i) a linear semantic equalization chain, and (ii) a non-linear\nDNN-based semantic equalizer. Both methods are designed to operate under\nsemantic compression in the latent space and adhere to transmit power\nconstraints. Through extensive evaluations, we show that the proposed joint\nequalization strategies consistently outperform conventional, disjoint\napproaches to physical and semantic channel equalization across a broad range\nof scenarios and wireless channel conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16450v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15846", "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15846v2", "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15846v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.16228", "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing", "authors": ["Shreelekha Revankar", "Utkarsh Mall", "Cheng Perng Phoo", "Kavita Bala", "Bharath Hariharan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 9 figures, 4 tables", "url": "http://arxiv.org/abs/2507.16228v1", "summary": "Natural disasters cause devastating damage to communities and infrastructure\nevery year. Effective disaster response is hampered by the difficulty of\naccessing affected areas during and after events. Remote sensing has allowed us\nto monitor natural disasters in a remote way. More recently there have been\nadvances in computer vision and deep learning that help automate satellite\nimagery analysis, However, they remain limited by their narrow focus on\nspecific disaster types, reliance on manual expert interpretation, and lack of\ndatasets with sufficient temporal granularity or natural language annotations\nfor tracking disaster progression. We present MONITRS, a novel multimodal\ndataset of more than 10,000 FEMA disaster events with temporal satellite\nimagery and natural language annotations from news articles, accompanied by\ngeotagged locations, and question-answer pairs. We demonstrate that fine-tuning\nexisting MLLMs on our dataset yields significant performance improvements for\ndisaster monitoring tasks, establishing a new benchmark for machine\nlearning-assisted disaster response systems. Code can be found at:\nhttps://github.com/ShreelekhaR/MONITRS", "comment": "17 pages, 9 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.16228v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.06821", "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      37 pages, 22 figures", "url": "http://arxiv.org/abs/2506.06821v3", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning.", "comment": "37 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2506.06821v3", "cate": "cs.CL", "date": "2025-06-07", "updated": "2025-07-22"}
{"id": "2501.13358", "title": "Learning to Bid in Non-Stationary Repeated First-Price Auctions", "authors": ["Zihao Hu", "Xiaoyu Fan", "Yuan Yao", "Jiheng Zhang", "Zhengyuan Zhou"], "categories": ["cs.LG", "cs.GT", "cs.IT", "math.IT", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.13358v2", "summary": "First-price auctions have recently gained significant traction in digital\nadvertising markets, exemplified by Google's transition from second-price to\nfirst-price auctions. Unlike in second-price auctions, where bidding one's\nprivate valuation is a dominant strategy, determining an optimal bidding\nstrategy in first-price auctions is more complex. From a learning perspective,\nthe learner (a specific bidder) can interact with the environment (other\nbidders, i.e., opponents) sequentially to infer their behaviors. Existing\nresearch often assumes specific environmental conditions and benchmarks\nperformance against the best fixed policy (static benchmark). While this\napproach ensures strong learning guarantees, the static benchmark can deviate\nsignificantly from the optimal strategy in environments with even mild\nnon-stationarity. To address such scenarios, a dynamic benchmark--representing\nthe sum of the highest achievable rewards at each time step--offers a more\nsuitable objective. However, achieving no-regret learning with respect to the\ndynamic benchmark requires additional constraints. By inspecting reward\nfunctions in online first-price auctions, we introduce two metrics to quantify\nthe regularity of the sequence of opponents' highest bids, which serve as\nmeasures of non-stationarity. We provide a minimax-optimal characterization of\nthe dynamic regret for the class of sequences of opponents' highest bids that\nsatisfy either of these regularity constraints. Our main technical tool is the\nOptimistic Mirror Descent (OMD) framework with a novel optimism configuration,\nwhich is well-suited for achieving minimax-optimal dynamic regret rates in this\ncontext. We then use synthetic datasets to validate our theoretical guarantees\nand demonstrate that our methods outperform existing ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.13358v2", "cate": "cs.LG", "date": "2025-01-23", "updated": "2025-07-22"}
{"id": "2507.16676", "title": "Custom Algorithm-based Fault Tolerance for Attention Layers in Transformers", "authors": ["Vasileios Titopoulos", "Kosmas Alexandridis", "Giorgos Dimitrakopoulos"], "categories": ["cs.LG", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      IEEE International System-on-Chip Conference (IEEE SOCC 2025)", "url": "http://arxiv.org/abs/2507.16676v1", "summary": "Transformers and large language models (LLMs), powered by the attention\nmechanism, have transformed numerous AI applications, driving the need for\nspecialized hardware accelerators. A major challenge in these accelerators is\nefficiently detecting errors caused by random hardware faults. Traditional\nalgorithm-based fault tolerance (ABFT) techniques verify individual matrix\nmultiplications but fall short in handling the full attention mechanism,\nparticularly due to intermediate softmax normalization. This work proposes\nFlash-ABFT, a novel method that computes an online checksum across the entire\nthree-matrix product of query, key and value matrices, of an attention layer,\nincluding the softmax operation, with a single check. This approach\nsignificantly reduces overhead by eliminating redundant checks while\nmaintaining high fault-detection accuracy. Experimental results demonstrate\nthat Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%\nenergy overhead, making it a cost-effective and robust solution for error\ndetection in attention accelerators.", "comment": "IEEE International System-on-Chip Conference (IEEE SOCC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.16676v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16036", "title": "Entanglement-Efficient Compilation of Quantum Circuits over Large-Scale Quantum Networks", "authors": ["Felix Burt", "Kuan-Cheng Chen", "Kin K. Leung"], "categories": ["quant-ph", "cs.DC"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 10 figures, to be published in proceedings of IEEE QCE2025", "url": "http://arxiv.org/abs/2507.16036v1", "summary": "Quantum computers face inherent scaling challenges, a fact that necessitates\ninvestigation of distributed quantum computing systems, whereby scaling is\nachieved through interconnection of smaller quantum processing units. However,\nconnecting large numbers of QPUs will eventually result in connectivity\nconstraints at the network level, where the difficulty of entanglement sharing\nincreases with network path lengths. This increases the complexity of the\nquantum circuit partitioning problem, since the cost of generating entanglement\nbetween end nodes varies with network topologies and existing links. We address\nthis challenge using a simple modification to existing partitioning schemes\ndesigned for all-to-all connected networks, that efficiently accounts for both\nof these factors. We investigate the performance in terms of entanglement\nrequirements and optimisation time of various quantum circuits over different\nnetwork topologies, achieving lower entanglement costs in the majority of cases\nthan state-of-the-art methods. We provide techniques for scaling to large-scale\nquantum networks employing both network and problem coarsening. We show that\ncoarsened methods can achieve improved solution quality in most cases with\nsignificantly lower run-times than direct partitioning methods.", "comment": "12 pages, 10 figures, to be published in proceedings of IEEE QCE2025", "pdf_url": "http://arxiv.org/pdf/2507.16036v1", "cate": "quant-ph", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16185", "title": "Characterizing Online Activities Contributing to Suicide Mortality among Youth", "authors": ["Aparna Ananthasubramaniam", "Elyse J. Thulin", "Viktoryia Kalesnikava", "Silas Falde", "Jonathan Kertawidjaja", "Lily Johns", "Alejandro Rodrguez-Putnam", "Emma Spring", "Kara Zivin", "Briana Mezuk"], "categories": ["cs.CY", "cs.CL"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted at the AAAI International Conference on Web and Social Media (ICWSM) 2026", "url": "http://arxiv.org/abs/2507.16185v1", "summary": "The recent rise in youth suicide highlights the urgent need to understand how\nonline experiences contribute to this public health issue. Our mixed-methods\napproach responds to this challenge by developing a set of themes focused on\nrisk factors for suicide mortality in online spaces among youth ages 10-24, and\na framework to model these themes at scale. Using 29,124 open text summaries of\ndeath investigations between 2013-2022, we conducted a thematic analysis to\nidentify 12 types of online activities that were considered by investigators or\nnext of kin to be relevant in contextualizing a given suicide death. We then\ndevelop a zero-shot learning framework to model these 12 themes at scale, and\nanalyze variation in these themes by decedent characteristics and over time.\nOur work uncovers several online activities related to harm to self, harm to\nothers, interpersonal interactions, activity levels online, and life events,\nwhich correspond to different phases of suicide risk from two prominent suicide\ntheories. We find an association between these themes and decedent\ncharacteristics like age, means of death, and interpersonal problems, and many\nthemes became more prevalent during the 2020 COVID-19 lockdowns. While digital\nspaces have taken some steps to address expressions of suicidality online, our\nwork illustrates the opportunities for developing interventions related to less\nexplicit indicators of suicide risk by combining suicide theories with\ncomputational research.", "comment": "Accepted at the AAAI International Conference on Web and Social Media\n  (ICWSM) 2026", "pdf_url": "http://arxiv.org/pdf/2507.16185v1", "cate": "cs.CY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16069", "title": "Interpreting CFD Surrogates through Sparse Autoencoders", "authors": ["Yeping Hu", "Shusen Liu"], "categories": ["cs.CE", "cs.LG"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      Accepted by IJCAI 2025 Workshop on Explainable Artificial Intelligence (XAI)", "url": "http://arxiv.org/abs/2507.16069v1", "summary": "Learning-based surrogate models have become a practical alternative to\nhigh-fidelity CFD solvers, but their latent representations remain opaque and\nhinder adoption in safety-critical or regulation-bound settings. This work\nintroduces a posthoc interpretability framework for graph-based surrogate\nmodels used in computational fluid dynamics (CFD) by leveraging sparse\nautoencoders (SAEs). By obtaining an overcomplete basis in the node embedding\nspace of a pretrained surrogate, the method extracts a dictionary of\ninterpretable latent features. The approach enables the identification of\nmonosemantic concepts aligned with physical phenomena such as vorticity or flow\nstructures, offering a model-agnostic pathway to enhance explainability and\ntrustworthiness in CFD applications.", "comment": "Accepted by IJCAI 2025 Workshop on Explainable Artificial\n  Intelligence (XAI)", "pdf_url": "http://arxiv.org/pdf/2507.16069v1", "cate": "cs.CE", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.06605", "title": "Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration", "authors": ["Xinyu Wu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06605v2", "summary": "Classical sampling-based motion planners like the RRTs suffer from\ninefficiencies, particularly in cluttered or high-dimensional spaces, due to\ntheir reliance on undirected, random sampling. This paper introduces the\nEpisodic RRT, a novel hybrid planning framework that replaces the primitive of\na random point with a learned, multi-step \"exploratory episode\" generated by a\nDeep Reinforcement Learning agent. By making the DRL agent the engine of\nexploration, ERRT transforms the search process from a diffuse, volumetric\nexpansion into a directed, branch-like growth. This paradigm shift yields key\nadvantages: it counters the curse of dimensionality with focused exploration,\nminimizes expensive collision checks by proactively proposing locally valid\npaths, and improves connectivity by generating inherently connected path\nsegments. We demonstrate through extensive empirical evaluation across 2D, 3D,\nand 6D environments that ERRT and its variants consistently and significantly\noutperform their classical counterparts without any GPU acceleration. In a\nchallenging 6D robotic arm scenario, ERRT achieves a 98% success rate compared\nto 19% for RRT, is up to 107x faster, reduces collision checks by over 99.6%,\nand finds initial paths that are nearly 50% shorter. Furthermore, its\nasymptotically optimal variant, ERRT*, demonstrates vastly superior anytime\nperformance, refining solutions to near-optimality up to 29x faster than\nstandard RRT* in 3D environments. Code:\nhttps://xinyuwuu.github.io/Episodic_RRT/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06605v2", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-22"}
{"id": "2507.16768", "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding", "authors": ["Ran Wang", "Xiaoxuan Liu", "Hao Ren", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16768v1", "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16768v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16497", "title": "Canonical Correlation Patterns for Validating Clustering of Multivariate Time Series", "authors": ["Isabella Degen", "Zahraa S Abdallah", "Kate Robson Brown", "Henry W J Reeve"], "categories": ["cs.LG", "stat.AP", "62H30 (Primary), 62M10 (Secondary)", "I.5.3; I.6.4"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      45 pages, 8 figures. Introduces canonical correlation patterns as discrete validation targets for correlation-based clustering, systematically evaluates distance functions and validity indices, and provides practical implementation guidelines through controlled experiments with synthetic ground truth data", "url": "http://arxiv.org/abs/2507.16497v1", "summary": "Clustering of multivariate time series using correlation-based methods\nreveals regime changes in relationships between variables across health,\nfinance, and industrial applications. However, validating whether discovered\nclusters represent distinct relationships rather than arbitrary groupings\nremains a fundamental challenge. Existing clustering validity indices were\ndeveloped for Euclidean data, and their effectiveness for correlation patterns\nhas not been systematically evaluated. Unlike Euclidean clustering, where\ngeometric shapes provide discrete reference targets, correlations exist in\ncontinuous space without equivalent reference patterns. We address this\nvalidation gap by introducing canonical correlation patterns as mathematically\ndefined validation targets that discretise the infinite correlation space into\nfinite, interpretable reference patterns. Using synthetic datasets with perfect\nground truth across controlled conditions, we demonstrate that canonical\npatterns provide reliable validation targets, with L1 norm for mapping and L5\nnorm for silhouette width criterion and Davies-Bouldin index showing superior\nperformance. These methods are robust to distribution shifts and appropriately\ndetect correlation structure degradation, enabling practical implementation\nguidelines. This work establishes a methodological foundation for rigorous\ncorrelation-based clustering validation in high-stakes domains.", "comment": "45 pages, 8 figures. Introduces canonical correlation patterns as\n  discrete validation targets for correlation-based clustering, systematically\n  evaluates distance functions and validity indices, and provides practical\n  implementation guidelines through controlled experiments with synthetic\n  ground truth data", "pdf_url": "http://arxiv.org/pdf/2507.16497v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16238", "title": "Positive Style Accumulation: A Style Screening and Continuous Utilization Framework for Federated DG-ReID", "authors": ["Xin Xu", "Chaoyue Ren", "Wei Liu", "Wenke Huang", "Bin Yang", "Zhixi Yu", "Kui Jiang"], "categories": ["cs.CV", "I.4.9; I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures, accepted at ACM MM 2025, Submission ID: 4394", "url": "http://arxiv.org/abs/2507.16238v1", "summary": "The Federated Domain Generalization for Person re-identification (FedDG-ReID)\naims to learn a global server model that can be effectively generalized to\nsource and target domains through distributed source domain data. Existing\nmethods mainly improve the diversity of samples through style transformation,\nwhich to some extent enhances the generalization performance of the model.\nHowever, we discover that not all styles contribute to the generalization\nperformance. Therefore, we define styles that are beneficial or harmful to the\nmodel's generalization performance as positive or negative styles. Based on\nthis, new issues arise: How to effectively screen and continuously utilize the\npositive styles. To solve these problems, we propose a Style Screening and\nContinuous Utilization (SSCU) framework. Firstly, we design a Generalization\nGain-guided Dynamic Style Memory (GGDSM) for each client model to screen and\naccumulate generated positive styles. Meanwhile, we propose a style memory\nrecognition loss to fully leverage the positive styles memorized by Memory.\nFurthermore, we propose a Collaborative Style Training (CST) strategy to make\nfull use of positive styles. Unlike traditional learning strategies, our\napproach leverages both newly generated styles and the accumulated positive\nstyles stored in memory to train client models on two distinct branches. This\ntraining strategy is designed to effectively promote the rapid acquisition of\nnew styles by the client models, and guarantees the continuous and thorough\nutilization of positive styles, which is highly beneficial for the model's\ngeneralization performance. Extensive experimental results demonstrate that our\nmethod outperforms existing methods in both the source domain and the target\ndomain.", "comment": "10 pages, 3 figures, accepted at ACM MM 2025, Submission ID: 4394", "pdf_url": "http://arxiv.org/pdf/2507.16238v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2410.14003", "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time Systems", "authors": ["Connor Sullivan", "Alex Manley", "Mohammad Alian", "Heechul Yun"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Update to Fig. 11: The previous version used mismatched cache capacities between the 2-bank and 4-bank configurations in the simulation setup. This has been corrected to ensure both configurations have equal total cache capacity. As a result, the specific numerical results in Fig. 11 have changed. However, the overall trend shown in Fig. 11 and key findings of the paper remain consistent", "url": "http://arxiv.org/abs/2410.14003v2", "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.", "comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent", "pdf_url": "http://arxiv.org/pdf/2410.14003v2", "cate": "cs.AR", "date": "2024-10-17", "updated": "2025-07-21"}
{"id": "2212.10131", "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless Platforms", "authors": ["Serhii Ivanenko", "Vasyl Lanko", "Rudi Horn", "Vojin Jovanovic", "Rodrigo Bruno"], "categories": ["cs.DC", "cs.PL"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2212.10131v3", "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2212.10131v3", "cate": "cs.DC", "date": "2022-12-20", "updated": "2025-07-22"}
{"id": "2507.16430", "title": "Beyond Algorethics: Addressing the Ethical and Anthropological Challenges of AI Recommender Systems", "authors": ["Octavian M. Machidon"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16430v1", "summary": "In this paper, I examine the ethical and anthropological challenges posed by\nAI-driven recommender systems (RSs), which have become central to shaping\ndigital environments and social interactions. By curating personalized content,\nRSs do not merely reflect user preferences but actively construct individual\nexperiences across social media, entertainment platforms, and e-commerce.\nDespite their ubiquity, the ethical implications of RSs remain insufficiently\nexplored, even as concerns over privacy, autonomy, and mental well-being\nintensify. I argue that existing ethical approaches, including algorethics, the\neffort to embed ethical principles into algorithmic design, are necessary but\nultimately inadequate. RSs inherently reduce human complexity to quantifiable\ndimensions, exploit user vulnerabilities, and prioritize engagement over\nwell-being. Addressing these concerns requires moving beyond purely technical\nsolutions. I propose a comprehensive framework for human-centered RS design,\nintegrating interdisciplinary perspectives, regulatory strategies, and\neducational initiatives to ensure AI systems foster rather than undermine human\nautonomy and societal flourishing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16430v1", "cate": "cs.CY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16470", "title": "Computational design of personalized drugs via robust optimization under uncertainty", "authors": ["Rabia Altunay", "Jarkko Suuronen", "Eero Immonen", "Lassi Roininen", "Jari Hmlinen"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16470v1", "summary": "Effective disease treatment often requires precise control of the release of\nthe active pharmaceutical ingredient (API). In this work, we present a\ncomputational inverse design approach to determine the optimal drug composition\nthat yields a target release profile. We assume that the drug release is\ngoverned by the Noyes-Whitney model, meaning that dissolution occurs at the\nsurface of the drug. Our inverse design method is based on topology\noptimization. The method optimizes the drug composition based on the target\nrelease profile, considering the drug material parameters and the shape of the\nfinal drug. Our method is non-parametric and applicable to arbitrary drug\nshapes. The inverse design method is complemented by robust topology\noptimization, which accounts for the random drug material parameters. We use\nthe stochastic reduced-order method (SROM) to propagate the uncertainty in the\ndissolution model. Unlike Monte Carlo methods, SROM requires fewer samples and\nimproves computational performance. We apply our method to designing drugs with\nseveral target release profiles. The numerical results indicate that the\nrelease profiles of the designed drugs closely resemble the target profiles.\nThe SROM-based drug designs exhibit less uncertainty in their release profiles,\nsuggesting that our method is a convincing approach for uncertainty-aware drug\ndesign.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16470v1", "cate": "cs.CE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.07714", "title": "Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots", "authors": ["Julio Garrido", "Javier Vales", "Diego Silva-Muiz", "Enrique Riveiro", "Pablo Lpez-Matencio", "Josu Rivera-Andrade"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      14 pages, 8 figures, 1 table", "url": "http://arxiv.org/abs/2507.07714v2", "summary": "Cable-Driven Parallel Robots (CDPRs) are increasingly used for load\nmanipulation tasks involving predefined toolpaths with intermediate stops. At\neach stop, where the platform maintains a fixed pose and the motors keep the\ncables under tension, the system must evaluate whether it is safe to proceed by\ndetecting anomalies that could compromise performance (e.g., wind gusts or\ncable impacts). This paper investigates whether anomalies can be detected using\nonly motor torque data, without additional sensors. It introduces an adaptive,\nunsupervised outlier detection algorithm based on Gaussian Mixture Models\n(GMMs) to identify anomalies from torque signals. The method starts with a\nbrief calibration period, just a few seconds, during which a GMM is fit on\nknown anomaly-free data. Real-time torque measurements are then evaluated using\nMahalanobis distance from the GMM, with statistically derived thresholds\ntriggering anomaly flags. Model parameters are periodically updated using the\nlatest segments identified as anomaly-free to adapt to changing conditions.\nValidation includes 14 long-duration test sessions simulating varied wind\nintensities. The proposed method achieves a 100% true positive rate and 95.4%\naverage true negative rate, with 1-second detection latency. Comparative\nevaluation against power threshold and non-adaptive GMM methods indicates\nhigher robustness to drift and environmental variation.", "comment": "14 pages, 8 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.07714v2", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-22"}
{"id": "2507.16792", "title": "ChatChecker: A Framework for Dialogue System Testing and Evaluation Through Non-cooperative User Simulation", "authors": ["Roman Mayr", "Michel Schimpf", "Thomas Bohn"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16792v1", "summary": "While modern dialogue systems heavily rely on large language models (LLMs),\ntheir implementation often goes beyond pure LLM interaction. Developers\nintegrate multiple LLMs, external tools, and databases. Therefore, assessment\nof the underlying LLM alone does not suffice, and the dialogue systems must be\ntested and evaluated as a whole. However, this remains a major challenge. With\nmost previous work focusing on turn-level analysis, less attention has been\npaid to integrated dialogue-level quality assurance. To address this, we\npresent ChatChecker, a framework for automated evaluation and testing of\ncomplex dialogue systems. ChatChecker uses LLMs to simulate diverse user\ninteractions, identify dialogue breakdowns, and evaluate quality. Compared to\nprevious approaches, our design reduces setup effort and is generalizable, as\nit does not require reference dialogues and is decoupled from the\nimplementation of the target dialogue system. We improve breakdown detection\nperformance over a prior LLM-based approach by including an error taxonomy in\nthe prompt. Additionally, we propose a novel non-cooperative user simulator\nbased on challenging personas that uncovers weaknesses in target dialogue\nsystems more effectively. Through this, ChatChecker contributes to thorough and\nscalable testing. This enables both researchers and practitioners to accelerate\nthe development of robust dialogue systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16792v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16511", "title": "Analogy making as amortised model construction", "authors": ["David G. Nagy", "Tingke Shen", "Hanqi Zhou", "Charley M. Wu", "Peter Dayan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      RLC 2025 Finding the Frame Workshop", "url": "http://arxiv.org/abs/2507.16511v1", "summary": "Humans flexibly construct internal models to navigate novel situations. To be\nuseful, these internal models must be sufficiently faithful to the environment\nthat resource-limited planning leads to adequate outcomes; equally, they must\nbe tractable to construct in the first place. We argue that analogy plays a\ncentral role in these processes, enabling agents to reuse solution-relevant\nstructure from past experiences and amortise the computational costs of both\nmodel construction (construal) and planning. Formalising analogies as partial\nhomomorphisms between Markov decision processes, we sketch a framework in which\nabstract modules, derived from previous construals, serve as composable\nbuilding blocks for new ones. This modular reuse allows for flexible adaptation\nof policies and representations across domains with shared structural essence.", "comment": "RLC 2025 Finding the Frame Workshop", "pdf_url": "http://arxiv.org/pdf/2507.16511v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16240", "title": "Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling", "authors": ["Chao Zhou", "Tianyi Wei", "Nenghai Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accept by ICCV2025", "url": "http://arxiv.org/abs/2507.16240v1", "summary": "Recent advancements in unified image generation models, such as OmniGen, have\nenabled the handling of diverse image generation and editing tasks within a\nsingle framework, accepting multimodal, interleaved texts and images in free\nform. This unified architecture eliminates the need for text encoders, greatly\nreducing model complexity and standardizing various image generation and\nediting tasks, making it more user-friendly. However, we found that it suffers\nfrom text instruction neglect, especially when the text instruction contains\nmultiple sub-instructions. To explore this issue, we performed a perturbation\nanalysis on the input to identify critical steps and layers. By examining the\ncross-attention maps of these key steps, we observed significant conflicts\nbetween neglected sub-instructions and the activations of the input image. In\nresponse, we propose Self-Adaptive Attention Scaling (SaaS), a method that\nleverages the consistency of cross-attention between adjacent timesteps to\ndynamically scale the attention activation for each sub-instruction. Our SaaS\nenhances instruction-following fidelity without requiring additional training\nor test-time optimization. Experimental results on instruction-based image\nediting and visual conditional image generation validate the effectiveness of\nour SaaS, showing superior instruction-following fidelity over existing\nmethods. The code is available https://github.com/zhouchao-ops/SaaS.", "comment": "Accept by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.16240v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15300", "title": "GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing", "authors": ["Minnan Pei", "Gang Li", "Junwen Si", "Zeyu Zhu", "Zitao Mo", "Peisong Wang", "Zhuoran Song", "Xiaoyao Liang", "Jian Cheng"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15300v2", "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering\ntechnique for high-fidelity view synthesis, prompting the development of\ndedicated 3DGS accelerators for mobile applications. Through in-depth analysis,\nwe identify two major limitations in the conventional decoupled\npreprocessing-rendering dataflow adopted by existing accelerators: 1) a\nsignificant portion of preprocessed Gaussians are not used in rendering, and 2)\nthe same Gaussian gets repeatedly loaded across different tile renderings,\nresulting in substantial computational and data movement overhead. To address\nthese issues, we propose GCC, a novel accelerator designed for fast and\nenergy-efficient 3DGS inference. At the dataflow level, GCC introduces: 1)\ncross-stage conditional processing, which interleaves preprocessing and\nrendering to dynamically skip unnecessary Gaussian preprocessing; and 2)\nGaussian-wise rendering, ensuring that all rendering operations for a given\nGaussian are completed before moving to the next, thereby eliminating\nduplicated Gaussian loading. We also propose an alpha-based boundary\nidentification method to derive compact and accurate Gaussian regions, thereby\nreducing rendering costs. We implement our GCC accelerator in 28nm technology.\nExtensive experiments demonstrate that GCC significantly outperforms the\nstate-of-the-art 3DGS inference accelerator, GSCore, in both performance and\nenergy efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15300v2", "cate": "cs.AR", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.01225", "title": "Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration", "authors": ["Sunandita Patra", "Mehtab Pathan", "Mahmoud Mahfouz", "Parisa Zehtabi", "Wided Ouaja", "Daniele Magazzeni", "Manuela Veloso"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Please cite as: Sunandita Patra, Mehtab Pathan, Mahmoud Mahfouz, Parisa Zehtabi, Wided Ouaja, Daniele Magazzeni, and Manuela Veloso. \"Capacity planning and scheduling for jobs with uncertainty in resource usage and duration.\" The Journal of Supercomputing 80, no. 15 (2024): 22428-22461", "url": "http://arxiv.org/abs/2507.01225v2", "summary": "Organizations around the world schedule jobs (programs) regularly to perform\nvarious tasks dictated by their end users. With the major movement towards\nusing a cloud computing infrastructure, our organization follows a hybrid\napproach with both cloud and on-prem servers. The objective of this work is to\nperform capacity planning, i.e., estimate resource requirements, and job\nscheduling for on-prem grid computing environments. A key contribution of our\napproach is handling uncertainty in both resource usage and duration of the\njobs, a critical aspect in the finance industry where stochastic market\nconditions significantly influence job characteristics. For capacity planning\nand scheduling, we simultaneously balance two conflicting objectives: (a)\nminimize resource usage, and (b) provide high quality-of-service to the end\nusers by completing jobs by their requested deadlines. We propose approximate\napproaches using deterministic estimators and pair sampling-based constraint\nprogramming. Our best approach (pair sampling-based) achieves much lower peak\nresource usage compared to manual scheduling without compromising on the\nquality-of-service.", "comment": "Please cite as: Sunandita Patra, Mehtab Pathan, Mahmoud Mahfouz,\n  Parisa Zehtabi, Wided Ouaja, Daniele Magazzeni, and Manuela Veloso. \"Capacity\n  planning and scheduling for jobs with uncertainty in resource usage and\n  duration.\" The Journal of Supercomputing 80, no. 15 (2024): 22428-22461", "pdf_url": "http://arxiv.org/pdf/2507.01225v2", "cate": "cs.DC", "date": "2025-07-01", "updated": "2025-07-21"}
{"id": "2507.16410", "title": "GG-BBQ: German Gender Bias Benchmark for Question Answering", "authors": ["Shalaka Satheesh", "Katrin Klug", "Katharina Beckh", "Hctor Allende-Cid", "Sebastian Houben", "Teena Hassan"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to the 6th Workshop on Gender Bias in Natural Language Processing (GeBNLP), taking place on August 1st 2025, as part of ACL 2025 in Vienna", "url": "http://arxiv.org/abs/2507.16410v1", "summary": "Within the context of Natural Language Processing (NLP), fairness evaluation\nis often associated with the assessment of bias and reduction of associated\nharm. In this regard, the evaluation is usually carried out by using a\nbenchmark dataset, for a task such as Question Answering, created for the\nmeasurement of bias in the model's predictions along various dimensions,\nincluding gender identity. In our work, we evaluate gender bias in German Large\nLanguage Models (LLMs) using the Bias Benchmark for Question Answering by\nParrish et al. (2022) as a reference. Specifically, the templates in the gender\nidentity subset of this English dataset were machine translated into German.\nThe errors in the machine translated templates were then manually reviewed and\ncorrected with the help of a language expert. We find that manual revision of\nthe translation is crucial when creating datasets for gender bias evaluation\nbecause of the limitations of machine translation from English to a language\nsuch as German with grammatical gender. Our final dataset is comprised of two\nsubsets: Subset-I, which consists of group terms related to gender identity,\nand Subset-II, where group terms are replaced with proper names. We evaluate\nseveral LLMs used for German NLP on this newly created dataset and report the\naccuracy and bias scores. The results show that all models exhibit bias, both\nalong and against existing social stereotypes.", "comment": "Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP), taking place on August 1st 2025, as part of ACL 2025 in\n  Vienna", "pdf_url": "http://arxiv.org/pdf/2507.16410v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16717", "title": "Multi-objective Portfolio Optimization Via Gradient Descent", "authors": ["Christian Oliva", "Pedro R. Ventura", "Luis F. Lago-Fernndez"], "categories": ["cs.CE", "cs.LG"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16717v1", "summary": "Traditional approaches to portfolio optimization, often rooted in Modern\nPortfolio Theory and solved via quadratic programming or evolutionary\nalgorithms, struggle with scalability or flexibility, especially in scenarios\ninvolving complex constraints, large datasets and/or multiple conflicting\nobjectives. To address these challenges, we introduce a benchmark framework for\nmulti-objective portfolio optimization (MPO) using gradient descent with\nautomatic differentiation. Our method supports any optimization objective, such\nas minimizing risk measures (e.g., CVaR) or maximizing Sharpe ratio, along with\nrealistic constraints, such as tracking error limits, UCITS regulations, or\nasset group restrictions. We have evaluated our framework across six\nexperimental scenarios, from single-objective setups to complex multi-objective\ncases, and have compared its performance against standard solvers like CVXPY\nand SKFOLIO. Our results show that our method achieves competitive performance\nwhile offering enhanced flexibility for modeling multiple objectives and\nconstraints. We aim to provide a practical and extensible tool for researchers\nand practitioners exploring advanced portfolio optimization problems in\nreal-world conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16717v1", "cate": "cs.CE", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15493", "title": "GR-3 Technical Report", "authors": ["Chilam Cheang", "Sijin Chen", "Zhongren Cui", "Yingdong Hu", "Liqun Huang", "Tao Kong", "Hang Li", "Yifeng Li", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Wenxuan Ou", "Wanli Peng", "Zeyu Ren", "Haixin Shi", "Jiawen Tian", "Hongtao Wu", "Xin Xiao", "Yuyang Xiao", "Jiafeng Xu", "Yichu Yang"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Tech report. Authors are listed in alphabetical order. Project page: this https URL", "url": "http://arxiv.org/abs/2507.15493v2", "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "comment": "Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/", "pdf_url": "http://arxiv.org/pdf/2507.15493v2", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.16796", "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning", "authors": ["Mian Ibad Ali Shah", "Enda Barrett", "Karl Mason"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures, 1 table, Proceedings of the Main Track of the European Conference on Artificial Intelligence (ECAI 2025), October 25-30, 2025", "url": "http://arxiv.org/abs/2507.16796v1", "summary": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading\nthat integrates uncertainty-aware prediction with multi-agent reinforcement\nlearning (MARL), addressing a critical gap in current literature. In contrast\nto previous works relying on deterministic forecasts, the proposed approach\nemploys a heteroscedastic probabilistic transformer-based prediction model\ncalled Knowledge Transformer with Uncertainty (KTU) to explicitly quantify\nprediction uncertainty, which is essential for robust decision-making in the\nstochastic environment of P2P energy trading. The KTU model leverages\ndomain-specific features and is trained with a custom loss function that\nensures reliable probabilistic forecasts and confidence intervals for each\nprediction. Integrating these uncertainty-aware forecasts into the MARL\nframework enables agents to optimize trading strategies with a clear\nunderstanding of risk and variability. Experimental results show that the\nuncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to\n5.7% without P2P trading and 3.2% with P2P trading, while increasing\nelectricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak\nhour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These\nimprovements are even more pronounced when P2P trading is enabled, highlighting\nthe synergy between advanced forecasting and market mechanisms for resilient,\neconomically efficient energy communities.", "comment": "7 pages, 4 figures, 1 table, Proceedings of the Main Track of the\n  European Conference on Artificial Intelligence (ECAI 2025), October 25-30,\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.16796v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16533", "title": "confopt: A Library for Implementation and Evaluation of Gradient-based One-Shot NAS Methods", "authors": ["Abhash Kumar Jha", "Shakiba Moradian", "Arjun Krishnakumar", "Martin Rapp", "Frank Hutter"], "categories": ["cs.LG", "cs.AI", "68T01", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      AutoML 25 ABCD Track", "url": "http://arxiv.org/abs/2507.16533v1", "summary": "Gradient-based one-shot neural architecture search (NAS) has significantly\nreduced the cost of exploring architectural spaces with discrete design\nchoices, such as selecting operations within a model. However, the field faces\ntwo major challenges. First, evaluations of gradient-based NAS methods heavily\nrely on the DARTS benchmark, despite the existence of other available\nbenchmarks. This overreliance has led to saturation, with reported improvements\noften falling within the margin of noise. Second, implementations of\ngradient-based one-shot NAS methods are fragmented across disparate\nrepositories, complicating fair and reproducible comparisons and further\ndevelopment. In this paper, we introduce Configurable Optimizer (confopt), an\nextensible library designed to streamline the development and evaluation of\ngradient-based one-shot NAS methods. Confopt provides a minimal API that makes\nit easy for users to integrate new search spaces, while also supporting the\ndecomposition of NAS optimizers into their core components. We use this\nframework to create a suite of new DARTS-based benchmarks, and combine them\nwith a novel evaluation protocol to reveal a critical flaw in how\ngradient-based one-shot NAS methods are currently assessed. The code can be\nfound at https://github.com/automl/ConfigurableOptimizer.", "comment": "AutoML 25 ABCD Track", "pdf_url": "http://arxiv.org/pdf/2507.16533v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16251", "title": "HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery", "authors": ["Yu Wang", "Bo Dang", "Wanchun Li", "Wei Chen", "Yansheng Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16251v1", "summary": "With the increasing resolution of remote sensing imagery (RSI), large-size\nRSI has emerged as a vital data source for high-precision vector mapping of\ngeographic objects. Existing methods are typically constrained to processing\nsmall image patches, which often leads to the loss of contextual information\nand produces fragmented vector outputs. To address these, this paper introduces\nHoliTracer, the first framework designed to holistically extract vectorized\ngeographic objects from large-size RSI. In HoliTracer, we enhance segmentation\nof large-size RSI using the Context Attention Net (CAN), which employs a\nlocal-to-global attention mechanism to capture contextual dependencies.\nFurthermore, we achieve holistic vectorization through a robust pipeline that\nleverages the Mask Contour Reformer (MCR) to reconstruct polygons and the\nPolygon Sequence Tracer (PST) to trace vertices. Extensive experiments on\nlarge-size RSI datasets, including buildings, water bodies, and roads,\ndemonstrate that HoliTracer outperforms state-of-the-art methods. Our code and\ndata are available in https://github.com/vvangfaye/HoliTracer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16251v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2504.05119", "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection", "authors": ["Jon Gutirrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe"], "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05119v2", "summary": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05119v2", "cate": "cs.LG", "date": "2025-04-07", "updated": "2025-07-22"}
{"id": "2507.10789", "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks", "authors": ["Aaron Jarmusch", "Nathan Graddon", "Sunita Chandrasekaran"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10789v2", "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10789v2", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-21"}
{"id": "2507.16679", "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization", "authors": ["Han Jiang", "Dongyao Zhu", "Zhihua Wei", "Xiaoyuan Yi", "Ziang Xiao", "Xing Xie"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16679v1", "summary": "In-Context Learning has shown great potential for aligning Large Language\nModels (LLMs) with human values, helping reduce harmful outputs and accommodate\ndiverse preferences without costly post-training, known as In-Context Alignment\n(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting\nICA's ability to address value tensions--human values are inherently\npluralistic, often imposing conflicting demands, e.g., stimulation vs.\ntradition. Current ICA methods therefore face the Instruction Bottleneck\nchallenge, where LLMs struggle to reconcile multiple intended values within a\nsingle prompt, leading to incomplete or biased alignment. To address this, we\npropose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO\noptimizes a meta-instruction that navigates multiple values to better elicit\nLLMs' understanding of them and improve their alignment. This is achieved by\nmaximizing the total correlation between specified values and LLM responses,\ntheoretically reinforcing value correlation while reducing distractive noise,\nresulting in effective value instructions. Extensive experiments on five value\nsets show that PICACO works well with both black-box and open-source LLMs,\noutperforms several recent strong baselines, and achieves a better balance\nacross up to 8 distinct values.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16679v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15861", "title": "Benchmarking CO$_2$ Storage Simulations: Results from the 11th Society of Petroleum Engineers Comparative Solution Project", "authors": ["Jan M. Nordbotten", "Martin A. Fern", "Bernd Flemisch", "Anthony R. Kovscek", "Knut-Andreas Lie", "Jakub W. Both", "Olav Myner", "Tor Harald Sandve", "Etienne Ahusborde", "Sebastian Bauer", "Zhangxing Chen", "Holger Class", "Chaojie Di", "Didier Ding", "David Element", "Abbas Firoozabadi", "Eric Flauraud", "Jacques Franc", "Firdovsi Gasanzade", "Yousef Ghomian", "Marie Ann Giddins", "Christopher Green", "Bruno R. B. Fernandes", "George Hadjisotiriou", "Glenn Hammond", "Hai Huang", "Dickson Kachuma", "Michel Kern", "Timo Koch", "Prasanna Krishnamurthy", "Kjetil Olsen Lye", "David Landa-Marbn", "Michael Nole", "Paolo Orsini", "Nicolas Ruby", "Pablo Salinas", "Mohammad Sayyafzadeh", "Jakub Solovsk", "Jakob Torben", "Adam Turner", "Denis V. Voskov", "Kai Wendel", "AbdAllah A. Youssef"], "categories": ["physics.geo-ph", "cs.CE", "cs.NA", "math.NA"], "primary_category": "Subjects:       Geophysics (physics.geo-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15861v1", "summary": "The 11th Society of Petroleum Engineers Comparative Solution Project\n(shortened SPE11 herein) benchmarked simulation tools for geological carbon\ndioxide (CO$_2$) storage. A total of 45 groups from leading research\ninstitutions and industry across the globe signed up to participate, with 18\nultimately contributing valid results that were included in the comparative\nstudy reported here.\n  This paper summarizes the SPE11. A comprehensive introduction and qualitative\ndiscussion of the submitted data are provided, together with an overview of\nonline resources for accessing the full depth of data. A global metric for\nanalyzing the relative distance between submissions is proposed and used to\nconduct a quantitative analysis of the submissions. This analysis attempts to\nstatistically resolve the key aspects influencing the variability between\nsubmissions.\n  The study shows that the major qualitative variation between the submitted\nresults is related to thermal effects, dissolution-driven convective mixing,\nand resolution of facies discontinuities. Moreover, a strong dependence on grid\nresolution is observed across all three versions of the SPE11. However, our\nquantitative analysis suggests that the observed variations are predominantly\ninfluenced by factors not documented in the technical responses provided by the\nparticipants. We therefore identify that unreported variations due to human\nchoices within the process of setting up, conducting, and reporting on the\nsimulations underlying each SPE11 submission are at least as impactful as the\ncomputational choices reported.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15861v1", "cate": "physics.geo-ph", "date": "2025-07-05", "updated": "2025-07-05"}
{"id": "2503.01092", "title": "One-Shot Affordance Grounding of Deformable Objects in Egocentric Organizing Scenes", "authors": ["Wanjun Jia", "Fan Yang", "Mengfei Duan", "Xianchi Chen", "Yinxi Wang", "Yiming Jiang", "Wenrui Chen", "Kailun Yang", "Zhiyong Li"], "categories": ["cs.CV", "cs.RO", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025. Source code and benchmark dataset will be publicly available at this https URL", "url": "http://arxiv.org/abs/2503.01092v2", "summary": "Deformable object manipulation in robotics presents significant challenges\ndue to uncertainties in component properties, diverse configurations, visual\ninterference, and ambiguous prompts. These factors complicate both perception\nand control tasks. To address these challenges, we propose a novel method for\nOne-Shot Affordance Grounding of Deformable Objects (OS-AGDO) in egocentric\norganizing scenes, enabling robots to recognize previously unseen deformable\nobjects with varying colors and shapes using minimal samples. Specifically, we\nfirst introduce the Deformable Object Semantic Enhancement Module (DefoSEM),\nwhich enhances hierarchical understanding of the internal structure and\nimproves the ability to accurately identify local features, even under\nconditions of weak component information. Next, we propose the ORB-Enhanced\nKeypoint Fusion Module (OEKFM), which optimizes feature extraction of key\ncomponents by leveraging geometric constraints and improves adaptability to\ndiversity and visual interference. Additionally, we propose an\ninstance-conditional prompt based on image data and task context, which\neffectively mitigates the issue of region ambiguity caused by prompt words. To\nvalidate these methods, we construct a diverse real-world dataset, AGDDO15,\nwhich includes 15 common types of deformable objects and their associated\norganizational actions. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art methods, achieving improvements of\n6.2%, 3.2%, and 2.9% in KLD, SIM, and NSS metrics, respectively, while\nexhibiting high generalization performance. Source code and benchmark dataset\nare made publicly available at https://github.com/Dikay1/OS-AGDO.", "comment": "Accepted to IROS 2025. Source code and benchmark dataset will be\n  publicly available at https://github.com/Dikay1/OS-AGDO", "pdf_url": "http://arxiv.org/pdf/2503.01092v2", "cate": "cs.CV", "date": "2025-03-03", "updated": "2025-07-22"}
{"id": "2507.14452", "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures. Accepted to IJCAI 2025", "url": "http://arxiv.org/abs/2507.14452v1", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.14452v1", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.16537", "title": "Symbolic Graph Intelligence: Hypervector Message Passing for Learning Graph-Level Patterns with Tsetlin Machines", "authors": ["Christian D. Blakely"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, for ICTM '25", "url": "http://arxiv.org/abs/2507.16537v1", "summary": "We propose a multilayered symbolic framework for general graph classification\nthat leverages sparse binary hypervectors and Tsetlin Machines. Each graph is\nencoded through structured message passing, where node, edge, and attribute\ninformation are bound and bundled into a symbolic hypervector. This process\npreserves the hierarchical semantics of the graph through layered binding from\nnode attributes to edge relations to structural roles resulting in a compact,\ndiscrete representation. We also formulate a local interpretability framework\nwhich lends itself to a key advantage of our approach being locally\ninterpretable. We validate our method on TUDataset benchmarks, demonstrating\ncompetitive accuracy with strong symbolic transparency compared to neural graph\nmodels.", "comment": "8 pages, 5 figures, for ICTM '25", "pdf_url": "http://arxiv.org/pdf/2507.16537v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16254", "title": "Edge-case Synthesis for Fisheye Object Detection: A Data-centric Perspective", "authors": ["Seunghyeon Kim", "Kyeongryeol Go"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures", "url": "http://arxiv.org/abs/2507.16254v1", "summary": "Fisheye cameras introduce significant distortion and pose unique challenges\nto object detection models trained on conventional datasets. In this work, we\npropose a data-centric pipeline that systematically improves detection\nperformance by focusing on the key question of identifying the blind spots of\nthe model. Through detailed error analysis, we identify critical edge-cases\nsuch as confusing class pairs, peripheral distortions, and underrepresented\ncontexts. Then we directly address them through edge-case synthesis. We\nfine-tuned an image generative model and guided it with carefully crafted\nprompts to produce images that replicate real-world failure modes. These\nsynthetic images are pseudo-labeled using a high-quality detector and\nintegrated into training. Our approach results in consistent performance gains,\nhighlighting how deeply understanding data and selectively fixing its\nweaknesses can be impactful in specialized domains like fisheye object\ndetection.", "comment": "13 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.16254v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2505.18574", "title": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators", "authors": ["Charles Hong", "Sahil Bhatia", "Alvin Cheung", "Yakun Sophia Shao"], "categories": ["cs.PL", "cs.AI", "cs.AR", "cs.LG"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.18574v3", "summary": "Hardware accelerators, especially those designed for tensor processing, have\nbecome ubiquitous in today's computing landscape. However, even with\nsignificant efforts in building compilers, programming these tensor\naccelerators remains challenging, leaving much of their potential\nunderutilized. Recently, large language models (LLMs), trained on large amounts\nof code, have shown significant promise in code generation and optimization\ntasks, but generating low-resource languages like specialized tensor\naccelerator code still poses a significant challenge. We tackle this challenge\nwith Autocomp, an approach that empowers accelerator programmers to leverage\ndomain knowledge and hardware feedback to optimize code via an automated\nLLM-driven search. We accomplish this by: 1) formulating each optimization pass\nas a structured two-phase prompt, divided into planning and code generation\nphases, 2) inserting domain knowledge during planning via a concise and\nadaptable optimization menu, and 3) integrating correctness and performance\nmetrics from hardware as feedback at each search iteration. Across three\ncategories of representative workloads and two different accelerators, we\ndemonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x\n(convolution) faster than the vendor-provided library, and outperforms\nexpert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x\n(fine-grained linear algebra). Additionally, we demonstrate that optimization\nschedules generated from Autocomp can be reused across similar tensor\noperations, improving speedups by up to 24% under a fixed sample budget.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.18574v3", "cate": "cs.PL", "date": "2025-05-24", "updated": "2025-07-21"}
{"id": "2403.04311", "title": "Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry", "authors": ["Deepti Raghavan", "Keshav Santhanam", "Muhammad Shahir Rahman", "Nayani Modugula", "Luis Gaspar Schroeder", "Maximilien Cura", "Houjun Liu", "Pratiksha Thaker", "Philip Levis", "Matei Zaharia"], "categories": ["cs.AI", "cs.CL", "cs.DC", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.04311v3", "summary": "Compound AI applications chain together subcomponents such as generative\nlanguage models, document retrievers, and embedding models. Applying\ntraditional systems optimizations such as parallelism and pipelining in\ncompound AI systems is difficult because each component has different\nconstraints in terms of the granularity and type of data that it ingests. New\ndata is often generated during intermediate computations, and text streams may\nbe split into smaller, independent fragments (such as documents to sentences)\nwhich may then be re-aggregated at later parts of the computation. Due to this\ncomplexity, existing systems to serve compound AI queries do not fully take\nadvantage of parallelism and pipelining opportunities. We present Alto, a\nframework that automatically optimizes execution of compound AI queries through\nstreaming and parallelism. Bento introduces a new abstraction called nested\nancestry, a metadata hierarchy that allows the system to correctly track\npartial outputs and aggregate data across the heterogeneous constraints of the\ncomponents of compound AI applications. This metadata is automatically inferred\nfrom the programming model, allowing developers to express complex dataflow\npatterns without needing to reason manually about the details of routing and\naggregation. Implementations of four applications in Alto outperform or match\nimplementations in LangGraph, a popular existing AI programming framework. Alto\nimplementations match or improve latency by between 10-30%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.04311v3", "cate": "cs.AI", "date": "2024-03-07", "updated": "2025-07-22"}
{"id": "2402.04247", "title": "Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy", "authors": ["Xiangru Tang", "Qiao Jin", "Kunlun Zhu", "Tongxin Yuan", "Yichi Zhang", "Wangchunshu Zhou", "Meng Qu", "Yilun Zhao", "Jian Tang", "Zhuosheng Zhang", "Arman Cohan", "Zhiyong Lu", "Mark Gerstein"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.04247v5", "summary": "AI scientists powered by large language models have demonstrated substantial\npromise in autonomously conducting experiments and facilitating scientific\ndiscoveries across various disciplines. While their capabilities are promising,\nthese agents also introduce novel vulnerabilities that require careful\nconsideration for safety. However, there has been limited comprehensive\nexploration of these vulnerabilities. This perspective examines vulnerabilities\nin AI scientists, shedding light on potential risks associated with their\nmisuse, and emphasizing the need for safety measures. We begin by providing an\noverview of the potential risks inherent to AI scientists, taking into account\nuser intent, the specific scientific domain, and their potential impact on the\nexternal environment. Then, we explore the underlying causes of these\nvulnerabilities and provide a scoping review of the limited existing works.\nBased on our analysis, we propose a triadic framework involving human\nregulation, agent alignment, and an understanding of environmental feedback\n(agent regulation) to mitigate these identified risks. Furthermore, we\nhighlight the limitations and challenges associated with safeguarding AI\nscientists and advocate for the development of improved models, robust\nbenchmarks, and comprehensive regulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.04247v5", "cate": "cs.CY", "date": "2024-02-06", "updated": "2025-07-21"}
{"id": "2506.00025", "title": "Modeling Maritime Transportation Behavior Using AIS Trajectories and Markovian Processes in the Gulf of St. Lawrence", "authors": ["Gabriel Spadon", "Vaishnav Vaidheeswaran", "Md Mahbub Alam", "Ruixin Song", "Floris Goerlandt", "Ronald Pelot"], "categories": ["stat.AP", "cs.CE", "math.PR"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00025v2", "summary": "Maritime transportation is central to the global economy, and analyzing its\nlarge-scale behavioral data is critical for operational planning, environmental\nstewardship, and governance. This work presents a spatio-temporal analytical\nframework based on discrete-time Markov chains to model vessel movement\npatterns in the Gulf of St. Lawrence, with particular emphasis on disruptions\ninduced by the COVID-19 pandemic. We discretize the maritime domain into\nhexagonal cells and construct mobility signatures for distinct vessel types\nusing cell transition frequencies and dwell times. These features are used to\nbuild origin-destination matrices and spatial transition probability models\nthat characterize maritime dynamics across multiple temporal resolutions.\nFocusing on commercial, fishing, and passenger vessels, we analyze the temporal\nevolution of mobility behaviors during the pandemic, highlighting significant\nyet transient disruptions to recurring transport patterns. The methodology we\ncontribute to this paper allows for an extensive behavioral analytics key for\ntransportation planning. Accordingly, our findings reveal vessel-specific\nmobility signatures that persist across spatially disjoint regions, suggesting\nbehaviors invariant to time. In contrast, we observe temporal deviations among\npassenger and fishing vessels during the pandemic, reflecting the influence of\nsocial isolation measures and operational constraints on non-essential maritime\ntransport in this region.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00025v2", "cate": "stat.AP", "date": "2025-05-22", "updated": "2025-07-21"}
{"id": "2503.02581", "title": "Unveiling the Potential of Segment Anything Model 2 for RGB-Thermal Semantic Segmentation with Language Guidance", "authors": ["Jiayi Zhao", "Fei Teng", "Kai Luo", "Guoqiang Zhao", "Zhiyong Li", "Xu Zheng", "Kailun Yang"], "categories": ["cs.CV", "cs.RO", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025. The source code will be made publicly available at this https URL", "url": "http://arxiv.org/abs/2503.02581v2", "summary": "The perception capability of robotic systems relies on the richness of the\ndataset. Although Segment Anything Model 2 (SAM2), trained on large datasets,\ndemonstrates strong perception potential in perception tasks, its inherent\ntraining paradigm prevents it from being suitable for RGB-T tasks. To address\nthese challenges, we propose SHIFNet, a novel SAM2-driven Hybrid Interaction\nParadigm that unlocks the potential of SAM2 with linguistic guidance for\nefficient RGB-Thermal perception. Our framework consists of two key components:\n(1) Semantic-Aware Cross-modal Fusion (SACF) module that dynamically balances\nmodality contributions through text-guided affinity learning, overcoming SAM2's\ninherent RGB bias; (2) Heterogeneous Prompting Decoder (HPD) that enhances\nglobal semantic information through a semantic enhancement module and then\ncombined with category embeddings to amplify cross-modal semantic consistency.\nWith 32.27M trainable parameters, SHIFNet achieves state-of-the-art\nsegmentation performance on public benchmarks, reaching 89.8% on PST900 and\n67.8% on FMB, respectively. The framework facilitates the adaptation of\npre-trained large models to RGB-T segmentation tasks, effectively mitigating\nthe high costs associated with data collection while endowing robotic systems\nwith comprehensive perception capabilities. The source code will be made\npublicly available at https://github.com/iAsakiT3T/SHIFNet.", "comment": "Accepted to IROS 2025. The source code will be made publicly\n  available at https://github.com/iAsakiT3T/SHIFNet", "pdf_url": "http://arxiv.org/pdf/2503.02581v2", "cate": "cs.CV", "date": "2025-03-04", "updated": "2025-07-22"}
{"id": "2507.15863", "title": "eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages;1 figure;5 tables", "url": "http://arxiv.org/abs/2507.15863v1", "summary": "We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)\nModule, a secure and scalable Retrieval-Augmented Generation pipeline designed\nspecifically for enterprise document question answering. Designed and\nimplemented by eSapiens, the system ingests heterogeneous content (PDF, Office,\nweb), splits it into 1,000-token overlapping chunks, and indexes them in a\nhybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via\ncombined vector+BM25 search, reranked with Cohere, and answered by an LLM using\nCO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,\nregenerating answers until every claim is grounded. On four LegalBench subsets,\n1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank\nboosts Precision@10 by approximately 7 pp; the verifier raises TRACe\nUtilization above 0.50 and limits unsupported statements to less than 3%. All\ncomponents run in containers, enforce end-to-end TLS 1.3 and AES-256. These\nresults demonstrate that the DEREK module delivers accurate, traceable, and\nproduction-ready document QA with minimal operational overhead. The module is\ndesigned to meet enterprise demands for secure, auditable, and context-faithful\nretrieval, providing a reliable baseline for high-stakes domains such as legal\nand finance.", "comment": "8 pages;1 figure;5 tables", "pdf_url": "http://arxiv.org/pdf/2507.15863v1", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.16569", "title": "Families of Optimal Transport Kernels for Cell Complexes", "authors": ["Rahul Khorana"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16569v1", "summary": "Recent advances have discussed cell complexes as ideal learning\nrepresentations. However, there is a lack of available machine learning methods\nsuitable for learning on CW complexes. In this paper, we derive an explicit\nexpression for the Wasserstein distance between cell complex signal\ndistributions in terms of a Hodge-Laplacian matrix. This leads to a\nstructurally meaningful measure to compare CW complexes and define the optimal\ntransportation map. In order to simultaneously include both feature and\nstructure information, we extend the Fused Gromov-Wasserstein distance to CW\ncomplexes. Finally, we introduce novel kernels over the space of probability\nmeasures on CW complexes based on the dual formulation of optimal transport.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16569v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16257", "title": "Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models", "authors": ["Futa Waseda", "Saku Sugawara", "Isao Echizen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACMMM 2025 Accepted", "url": "http://arxiv.org/abs/2507.16257v1", "summary": "Defending pre-trained vision-language models (VLMs), such as CLIP, against\nadversarial attacks is crucial, as these models are widely used in diverse\nzero-shot tasks, including image classification. However, existing adversarial\ntraining (AT) methods for robust fine-tuning largely overlook the role of\nlanguage in enhancing visual robustness. Specifically, (1) supervised AT\nmethods rely on short texts (e.g., class labels) to generate adversarial\nperturbations, leading to overfitting to object classes in the training data,\nand (2) unsupervised AT avoids this overfitting but remains suboptimal against\npractical text-guided adversarial attacks due to its lack of semantic guidance.\nTo address these limitations, we propose Quality Text-guided Adversarial\nFine-Tuning (QT-AFT), which leverages high-quality captions during training to\nguide adversarial examples away from diverse semantics present in images. This\nenables the visual encoder to robustly recognize a broader range of image\nfeatures even under adversarial noise, thereby enhancing robustness across\ndiverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods\n-- overfitting in supervised AT and lack of semantic awareness in unsupervised\nAT -- achieving state-of-the-art zero-shot adversarial robustness and clean\naccuracy, evaluated across 16 zero-shot datasets. Furthermore, our\ncomprehensive study uncovers several key insights into the role of language in\nenhancing vision robustness; for example, describing object properties in\naddition to object names further enhances zero-shot robustness. Our findings\npoint to an urgent direction for future work -- centering high-quality\nlinguistic supervision in robust visual representation learning.", "comment": "ACMMM 2025 Accepted", "pdf_url": "http://arxiv.org/pdf/2507.16257v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2407.00494", "title": "Graph Neural Networks Gone Hogwild", "authors": ["Olga Solodova", "Nick Richardson", "Deniz Oktay", "Ryan P. Adams"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.00494v2", "summary": "Graph neural networks (GNNs) appear to be powerful tools to learn state\nrepresentations for agents in distributed, decentralized multi-agent systems,\nbut generate catastrophically incorrect predictions when nodes update\nasynchronously during inference. This failure under asynchrony effectively\nexcludes these architectures from many potential applications where synchrony\nis difficult or impossible to enforce, e.g., robotic swarms or sensor networks.\nIn this work we identify \"implicitly-defined\" GNNs as a class of architectures\nwhich is provably robust to asynchronous \"hogwild\" inference, adapting\nconvergence guarantees from work in asynchronous and distributed optimization.\nWe then propose a novel implicitly-defined GNN architecture, which we call an\n'energy GNN'. We show that this architecture outperforms other GNNs from this\nclass on a variety of synthetic tasks inspired by multi-agent systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.00494v2", "cate": "cs.LG", "date": "2024-06-29", "updated": "2025-07-22"}
{"id": "2507.09233", "title": "Secondary Bounded Rationality: A Theory of How Algorithms Reproduce Structural Inequality in AI Hiring", "authors": ["Jia Xiao"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09233v2", "summary": "AI-driven recruitment systems, while promising efficiency and objectivity,\noften perpetuate systemic inequalities by encoding cultural and social capital\ndisparities into algorithmic decision making. This article develops and defends\na novel theory of secondary bounded rationality, arguing that AI systems,\ndespite their computational power, inherit and amplify human cognitive and\nstructural biases through technical and sociopolitical constraints. Analyzing\nmultimodal recruitment frameworks, we demonstrate how algorithmic processes\ntransform historical inequalities, such as elite credential privileging and\nnetwork homophily, into ostensibly meritocratic outcomes. Using Bourdieusian\ncapital theory and Simon's bounded rationality, we reveal a recursive cycle\nwhere AI entrenches exclusion by optimizing for legible yet biased proxies of\ncompetence. We propose mitigation strategies, including counterfactual fairness\ntesting, capital-aware auditing, and regulatory interventions, to disrupt this\nself-reinforcing inequality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09233v2", "cate": "cs.CY", "date": "2025-07-12", "updated": "2025-07-22"}
{"id": "2507.14456", "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14456v2", "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14456v2", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-22"}
{"id": "2507.15868", "title": "Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models", "authors": ["Altynbek Ismailov", "Salia Asanova"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15868v1", "summary": "Large language models (LLMs) now write code in settings where misreading a\nsingle word can break safety or cost money, yet we still expect them to\noverlook stray typos. To probe where useful robustness ends and harmful\ninsensitivity begins, we compile 50 LeetCode problems and craft three minimal\nprompt perturbations that should vary in importance: (i) progressive\nunderspecification deleting 10 % of words per step; (ii) lexical flip swapping\na pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a\ncommon noun with an obscure technical synonym. Six frontier models, including\nthree \"reasoning-tuned\" versions, solve each mutated prompt, and their Python\noutputs are checked against the original test suites to reveal whether they\nreused the baseline solution or adapted. Among 11 853 generations we observe a\nsharp double asymmetry. Models remain correct in 85 % of cases even after 90 %\nof the prompt is missing, showing over-robustness to underspecification, yet\nonly 54 % react to a single quantifier flip that reverses the task, with\nreasoning-tuned variants even less sensitive than their bases. Jargon edits lie\nin between, passing through 56 %. Current LLMs thus blur the line between\nharmless noise and meaning - changing edits, often treating both as ignorable.\nMasking salient anchors such as function names can force re - evaluation. We\nadvocate evaluation and training protocols that reward differential\nsensitivity: stay steady under benign noise but adapt - or refuse - when\nsemantics truly change.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15868v1", "cate": "cs.CL", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2507.16577", "title": "Scaling Linear Attention with Sparse State Expansion", "authors": ["Yuqi Pan", "Yongqi An", "Zheng Li", "Yuhong Chou", "Ruijie Zhu", "Xiaohui Wang", "Mingxuan Wang", "Jinqiao Wang", "Guoqi Li"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16577v1", "summary": "The Transformer architecture, despite its widespread success, struggles with\nlong-context scenarios due to quadratic computation and linear memory growth.\nWhile various linear attention variants mitigate these efficiency constraints\nby compressing context into fixed-size states, they often degrade performance\nin tasks such as in-context retrieval and reasoning. To address this limitation\nand achieve more effective context compression, we propose two key innovations.\nFirst, we introduce a row-sparse update formulation for linear attention by\nconceptualizing state updating as information classification. This enables\nsparse state updates via softmax-based top-$k$ hard classification, thereby\nextending receptive fields and reducing inter-class interference. Second, we\npresent Sparse State Expansion (SSE) within the sparse framework, which expands\nthe contextual state into multiple partitions, effectively decoupling parameter\nsize from state capacity while maintaining the sparse classification paradigm.\nOur design, supported by efficient parallelized implementations, yields\neffective classification and discriminative state representations. We\nextensively validate SSE in both pure linear and hybrid (SSE-H) architectures\nacross language modeling, in-context retrieval, and mathematical reasoning\nbenchmarks. SSE demonstrates strong retrieval performance and scales favorably\nwith state size. Moreover, after reinforcement learning (RL) training, our 2B\nSSE-H model achieves state-of-the-art mathematical reasoning performance among\nsmall reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25,\nsignificantly outperforming similarly sized open-source Transformers. These\nresults highlight SSE as a promising and efficient architecture for\nlong-context modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16577v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16260", "title": "ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference", "authors": ["Haoyue Zhang", "Jie Zhang", "Song Guo"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16260v1", "summary": "Although vision transformers (ViT) have shown remarkable success in various\nvision tasks, their computationally expensive self-attention hinder their\ndeployment on resource-constrained devices. Token reduction, which discards\nless important tokens during forward propagation, has been proposed to enhance\nthe efficiency of transformer models. However, existing methods handle\nunimportant tokens irreversibly, preventing their reuse in subsequent blocks.\nConsidering that transformers focus on different information among blocks,\ntokens reduced in early blocks might be useful later. Furthermore, to adapt\ntransformer models for resource-constrained devices, it is crucial to strike a\nbalance between model performance and computational overhead. To address these\nchallenges, in this paper, we introduce a novel Token Freezing and Reusing\n(ToFe) framework, where we identify important tokens at each stage and\ntemporarily freeze the unimportant ones, allowing their lagged reusing at a\nlater stage. Specifically, we design a prediction module for token\nidentification and an approximate module for recovery of the frozen tokens. By\njointly optimizing with the backbone through computation budget-aware\nend-to-end training, ToFe can adaptively process the necessary tokens at each\nblock, thereby reducing computational cost while maintaining performance.\nExtensive experiments demonstrate that ToFe reduces the computational cost of\nLV-ViT model by 50% with less than 2% drop in Top-1 accuracy, achieving a\nbetter trade-off between performance and complexity compared to\nstate-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16260v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.14111", "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Jiwei Li", "Chris Shum"], "categories": ["cs.AI", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.14111v3", "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.", "comment": "Project Page: https://deepreinforce-ai.github.io/cudal1_blog/", "pdf_url": "http://arxiv.org/pdf/2507.14111v3", "cate": "cs.AI", "date": "2025-07-18", "updated": "2025-07-22"}
{"id": "2507.13758", "title": "Reasoning Models Can be Easily Hacked by Fake Reasoning Bias", "authors": ["Qian Wang", "Yubo Fan", "Zhenheng Tang", "Nuo Chen", "Wenxuan Wang", "Bingsheng He"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13758v2", "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and o1 are increasingly used\nas automated evaluators, raising critical questions about their vulnerability\nto the aesthetics of reasoning in LLM-as-a-judge settings. We introduce\nTHEATER, a comprehensive benchmark to systematically evaluate this\nvulnerability-termed Reasoning Theater Bias (RTB)-by comparing LLMs and LRMs\nacross subjective preference and objective factual datasets. Through\ninvestigation of six bias types including Simple Cues and Fake\nChain-of-Thought, we uncover three key findings: (1) in a critical paradox,\nreasoning-specialized LRMs are consistently more susceptible to RTB than\ngeneral-purpose LLMs, particularly in subjective tasks; (2) this creates a\ntask-dependent trade-off, where LRMs show more robustness on factual tasks but\nless on subjective ones; and (3) we identify 'shallow reasoning'-plausible but\nflawed arguments-as the most potent form of RTB. To address this, we design and\nevaluate two prompting strategies: a targeted system prompt that improves\naccuracy by up to 12% on factual tasks but only 1-3% on subjective tasks, and a\nself-reflection mechanism that shows similarly limited effectiveness in the\nmore vulnerable subjective domains. Our work reveals that RTB is a deep-seated\nchallenge for LRM-based evaluation and provides a systematic framework for\ndeveloping more genuinely robust and trustworthy LRMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13758v2", "cate": "cs.CY", "date": "2025-07-18", "updated": "2025-07-22"}
{"id": "2507.15894", "title": "Systole-Conditioned Generative Cardiac Motion", "authors": ["Shahar Zuler", "Gal Lifshitz", "Hadar Averbuch-Elor", "Dan Raviv"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15894v1", "summary": "Accurate motion estimation in cardiac computed tomography (CT) imaging is\ncritical for assessing cardiac function and surgical planning. Data-driven\nmethods have become the standard approach for dense motion estimation, but they\nrely on vast amounts of labeled data with dense ground-truth (GT) motion\nannotations, which are often unfeasible to obtain. To address this limitation,\nwe present a novel approach that synthesizes realistically looking pairs of\ncardiac CT frames enriched with dense 3D flow field annotations.\n  Our method leverages a conditional Variational Autoencoder (CVAE), which\nincorporates a novel multi-scale feature conditioning mechanism and is trained\nto generate 3D flow fields conditioned on a single CT frame. By applying the\ngenerated flow field to warp the given frame, we create pairs of frames that\nsimulate realistic myocardium deformations across the cardiac cycle. These\npairs serve as fully annotated data samples, providing optical flow GT\nannotations. Our data generation pipeline could enable the training and\nvalidation of more complex and accurate myocardium motion models, allowing for\nsubstantially reducing reliance on manual annotations.\n  Our code, along with animated generated samples and additional material, is\navailable on our project page:\nhttps://shaharzuler.github.io/GenerativeCardiacMotion_Page.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15894v1", "cate": "eess.IV", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2507.16672", "title": "Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs", "authors": ["Yushang Zhao", "Huijie Shen", "Dannier Li", "Lu Chang", "Chengrui Zhou", "Yinuo Yang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16672v1", "summary": "Generative, explainable, and flexible recommender systems, derived using\nLarge Language Models (LLM) are promising and poorly adapted to the cold-start\nuser situation, where there is little to no history of interaction. The current\nsolutions i.e. supervised fine-tuning and collaborative filtering are\ndense-user-item focused and would be expensive to maintain and update. This\npaper introduces a meta-learning framework, that can be used to perform\nparameter-efficient prompt-tuning, to effectively personalize LLM-based\nrecommender systems quickly at cold-start. The model learns soft prompt\nembeddings with first-order (Reptile) and second-order (MAML) optimization by\ntreating each of the users as the tasks. As augmentations to the input tokens,\nthese learnable vectors are the differentiable control variables that represent\nuser behavioral priors. The prompts are meta-optimized through episodic\nsampling, inner-loop adaptation, and outer-loop generalization. On\nMovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model\noutperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in\nreal-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization\nis also supported by this scalable solution, and its 275 ms rate of adaptation\nallows successful real-time risk profiling of financial systems by shortening\ndetection latency and improving payment network stability. Crucially, the 275\nms adaptation capability can enable real-time risk profiling for financial\ninstitutions, reducing systemic vulnerability detection latency significantly\nversus traditional compliance checks. By preventing contagion in payment\nnetworks (e.g., Fedwire), the framework strengthens national financial\ninfrastructure resilience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16672v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16279", "title": "MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning in Vision Tasks", "authors": ["Junhao Su", "Feiyu Zhu", "Hengyu Shi", "Tianyang Han", "Yurui Qiu", "Junfeng Luo", "Xiaoming Wei", "Jialin Gao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.16279v1", "summary": "Deep learning typically relies on end-to-end backpropagation for training, a\nmethod that inherently suffers from issues such as update locking during\nparameter optimization, high GPU memory consumption, and a lack of biological\nplausibility. In contrast, supervised local learning seeks to mitigate these\nchallenges by partitioning the network into multiple local blocks and designing\nindependent auxiliary networks to update each block separately. However,\nbecause gradients are propagated solely within individual local blocks,\nperformance degradation occurs, preventing supervised local learning from\nsupplanting end-to-end backpropagation. To address these limitations and\nfacilitate inter-block information flow, we propose the Momentum Auxiliary\nNetwork++ (MAN++). MAN++ introduces a dynamic interaction mechanism by\nemploying the Exponential Moving Average (EMA) of parameters from adjacent\nblocks to enhance communication across the network. The auxiliary network,\nupdated via EMA, effectively bridges the information gap between blocks.\nNotably, we observed that directly applying EMA parameters can be suboptimal\ndue to feature discrepancies between local blocks. To resolve this issue, we\nintroduce a learnable scaling bias that balances feature differences, thereby\nfurther improving performance. We validate MAN++ through extensive experiments\non tasks that include image classification, object detection, and image\nsegmentation, utilizing multiple network architectures. The experimental\nresults demonstrate that MAN++ achieves performance comparable to end-to-end\ntraining while significantly reducing GPU memory usage. Consequently, MAN++\noffers a novel perspective for supervised local learning and presents a viable\nalternative to conventional training methods.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.16279v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.14226", "title": "Mapping the Parasocial AI Market: User Trends, Engagement and Risks", "authors": ["Zilan Qian", "Mari Izumikawa", "Fiona Lodge", "Angelo Leone"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      17 pages, 17 figures", "url": "http://arxiv.org/abs/2507.14226v2", "summary": "A scan of 110 AI companion platforms reveals a rapidly growing global market\nfor emotionally engaging, personalized AI interactions. While parasocial use of\ngeneral-purpose AI (GPAI) tools currently dominates, a growing number of\nplatforms are designed specifically for care, transactional, or mating\ncompanionship. In the UK alone, these platforms receive between 46 million and\n91 million monthly visits (1.1-2.2 billion globally), with users spending an\naverage of 3.5 minutes per session. For context, Instagram averaged 67.3\nmillion UK visits per month between January and March 2025. Notably,\nmating-oriented AI companions make up 44% of UK visits (higher than the global\naverage of 30%) but see lower session times and return rates than mixed-use\nplatforms. As mating-oriented romantic AI offerings improve, increased\nengagement may follow, raising urgent concerns about online safety,\nparticularly for children, given weak age safeguards. Meanwhile, GPAI tools are\nmoving toward more emotionally intelligent, personalized interactions, making\nparasocial AI use increasingly mainstream. These trends highlight the need for\nthe UK AI Security Institute (AISI) to monitor this sector and assess whether\nexisting regulation sufficiently addresses emerging societal risks.", "comment": "17 pages, 17 figures", "pdf_url": "http://arxiv.org/pdf/2507.14226v2", "cate": "cs.CY", "date": "2025-07-16", "updated": "2025-07-22"}
{"id": "2507.15898", "title": "A Generative Model for Disentangling Galaxy Photometric Parameters", "authors": ["Keen Leung", "Colen Yan", "Jun Yin"], "categories": ["astro-ph.IM", "astro-ph.GA", "cs.AI"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      12 pages, 5 figures", "url": "http://arxiv.org/abs/2507.15898v1", "summary": "Ongoing and future photometric surveys will produce unprecedented volumes of\ngalaxy images, necessitating robust, efficient methods for deriving galaxy\nmorphological parameters at scale. Traditional approaches, such as parametric\nlight-profile fitting, offer valuable insights but become computationally\nprohibitive when applied to billions of sources. In this work, we propose a\nConditional AutoEncoder (CAE) framework to simultaneously model and\ncharacterize galaxy morphology. Our CAE is trained on a suite of realistic mock\ngalaxy images generated via GalSim, encompassing a broad range of galaxy types,\nphotometric parameters (e.g., flux, half-light radius, Sersic index,\nellipticity), and observational conditions. By encoding each galaxy image into\na low-dimensional latent representation conditioned on key parameters, our\nmodel effectively recovers these morphological features in a disentangled\nmanner, while also reconstructing the original image. The results demonstrate\nthat the CAE approach can accurately and efficiently infer complex structural\nproperties, offering a powerful alternative to existing methods.", "comment": "12 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.15898v1", "cate": "astro-ph.IM", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16674", "title": "GASPnet: Global Agreement to Synchronize Phases", "authors": ["Andrea Alamiaa", "Sabine Muzellec", "Thomas Serre", "Rufin VanRullen"], "categories": ["cs.LG", "q-bio.NC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16674v1", "summary": "In recent years, Transformer architectures have revolutionized most fields of\nartificial intelligence, relying on an attentional mechanism based on the\nagreement between keys and queries to select and route information in the\nnetwork. In previous work, we introduced a novel, brain-inspired architecture\nthat leverages a similar implementation to achieve a global 'routing by\nagreement' mechanism. Such a system modulates the network's activity by\nmatching each neuron's key with a single global query, pooled across the entire\nnetwork. Acting as a global attentional system, this mechanism improves noise\nrobustness over baseline levels but is insufficient for multi-classification\ntasks. Here, we improve on this work by proposing a novel mechanism that\ncombines aspects of the Transformer attentional operations with a compelling\nneuroscience theory, namely, binding by synchrony. This theory proposes that\nthe brain binds together features by synchronizing the temporal activity of\nneurons encoding those features. This allows the binding of features from the\nsame object while efficiently disentangling those from distinct objects. We\ndrew inspiration from this theory and incorporated angular phases into all\nlayers of a convolutional network. After achieving phase alignment via Kuramoto\ndynamics, we use this approach to enhance operations between neurons with\nsimilar phases and suppresses those with opposite phases. We test the benefits\nof this mechanism on two datasets: one composed of pairs of digits and one\ncomposed of a combination of an MNIST item superimposed on a CIFAR-10 image.\nOur results reveal better accuracy than CNN networks, proving more robust to\nnoise and with better generalization abilities. Overall, we propose a novel\nmechanism that addresses the visual binding problem in neural networks by\nleveraging the synergy between neuroscience and machine learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16674v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16287", "title": "Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition", "authors": ["Zefeng Qian", "Xincheng Yao", "Yifei Huang", "Chongyang Zhang", "Jiangyong Ying", "Hong Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.16287v1", "summary": "Few-shot action recognition (FSAR) aims to classify human actions in videos\nwith only a small number of labeled samples per category. The scarcity of\ntraining data has driven recent efforts to incorporate additional modalities,\nparticularly text. However, the subtle variations in human posture, motion\ndynamics, and the object interactions that occur during different phases, are\ncritical inherent knowledge of actions that cannot be fully exploited by action\nlabels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a\nnovel framework that goes beyond label semantics by leveraging Large Language\nModels (LLMs) to dissect the essential representational characteristics hidden\nbeneath action labels. Guided by the prior knowledge encoded in LLM, LGA\neffectively captures rich spatiotemporal cues in few-shot scenarios.\nSpecifically, for text, we prompt an off-the-shelf LLM to anatomize labels into\nsequences of atomic action descriptions, focusing on the three core elements of\naction (subject, motion, object). For videos, a Visual Anatomy Module segments\nactions into atomic video phases to capture the sequential structure of\nactions. A fine-grained fusion strategy then integrates textual and visual\nfeatures at the atomic level, resulting in more generalizable prototypes.\nFinally, we introduce a Multimodal Matching mechanism, comprising both\nvideo-video and video-text matching, to ensure robust few-shot classification.\nExperimental results demonstrate that LGA achieves state-of-the-art performance\nacross multipe FSAR benchmarks.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.16287v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2410.01817", "title": "Aligning AI with Public Values: Deliberation and Decision-Making for Governing Multimodal LLMs in Political Video Analysis", "authors": ["Tanusree Sharma", "Yujin Potter", "Zachary Kilhoffer", "Yun Huang", "Dawn Song", "Yang Wang"], "categories": ["cs.CV", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.01817v2", "summary": "How AI models should deal with political topics has been discussed, but it\nremains challenging and requires better governance. This paper examines the\ngovernance of large language models through individual and collective\ndeliberation, focusing on politically sensitive videos. We conducted a two-step\nstudy: interviews with 10 journalists established a baseline understanding of\nexpert video interpretation; 114 individuals through deliberation using\nInclusiveAI, a platform that facilitates democratic decision-making through\ndecentralized autonomous organization (DAO) mechanisms. Our findings reveal\ndistinct differences in interpretative priorities: while experts emphasized\nemotion and narrative, the general public prioritized factual clarity,\nobjectivity, and emotional neutrality. Furthermore, we examined how different\ngovernance mechanisms - quadratic vs. weighted voting and equal vs. 20/80\nvoting power - shape users' decision-making regarding AI behavior. Results\nindicate that voting methods significantly influence outcomes, with quadratic\nvoting reinforcing perceptions of liberal democracy and political equality. Our\nstudy underscores the necessity of selecting appropriate governance mechanisms\nto better capture user perspectives and suggests decentralized AI governance as\na potential way to facilitate broader public engagement in AI development,\nensuring that varied perspectives meaningfully inform design decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.01817v2", "cate": "cs.CV", "date": "2024-09-15", "updated": "2025-07-22"}
{"id": "2507.15958", "title": "Quantization-Aware Neuromorphic Architecture for Efficient Skin Disease Classification on Resource-Constrained Devices", "authors": ["Haitian Wang", "Xinyu Wang", "Yiren Wang", "Karen Lee", "Zichen Geng", "Xian Zhang", "Kehkashan Kiran", "Yu Zhang", "Bo Miao"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      This manuscript is under review for IEEE BIBM 2025", "url": "http://arxiv.org/abs/2507.15958v1", "summary": "Accurate and efficient skin lesion classification on edge devices is critical\nfor accessible dermatological care but remains challenging due to\ncomputational, energy, and privacy constraints. We introduce QANA, a novel\nquantization-aware neuromorphic architecture for incremental skin lesion\nclassification on resource-limited hardware. QANA effectively integrates ghost\nmodules, efficient channel attention, and squeeze-and-excitation blocks for\nrobust feature representation with low-latency and energy-efficient inference.\nIts quantization-aware head and spike-compatible transformations enable\nseamless conversion to spiking neural networks (SNNs) and deployment on\nneuromorphic platforms. Evaluation on the large-scale HAM10000 benchmark and a\nreal-world clinical dataset shows that QANA achieves 91.6\\% Top-1 accuracy and\n82.4\\% macro F1 on HAM10000, and 90.8\\% / 81.7\\% on the clinical dataset,\nsignificantly outperforming state-of-the-art CNN-to-SNN models under fair\ncomparison. Deployed on BrainChip Akida hardware, QANA achieves 1.5\\,ms\ninference latency and 1.7\\,mJ energy per image, reducing inference latency and\nenergy use by over 94.6\\%/98.6\\% compared to GPU-based CNNs surpassing\nstate-of-the-art CNN-to-SNN conversion baselines. These results demonstrate the\neffectiveness of QANA for accurate, real-time, and privacy-sensitive medical\nanalysis in edge environments.", "comment": "This manuscript is under review for IEEE BIBM 2025", "pdf_url": "http://arxiv.org/pdf/2507.15958v1", "cate": "eess.IV", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16696", "title": "FISHER: A Foundation Model for Multi-Modal Industrial Signal Comprehensive Representation", "authors": ["Pingyi Fan", "Anbai Jiang", "Shuwei Zhang", "Zhiqiang Lv", "Bing Han", "Xinhu Zheng", "Wenrui Liang", "Junjie Li", "Wei-Qiang Zhang", "Yanmin Qian", "Xie Chen", "Cheng Lu", "Jia Liu"], "categories": ["cs.LG", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 6 figures", "url": "http://arxiv.org/abs/2507.16696v1", "summary": "With the rapid deployment of SCADA systems, how to effectively analyze\nindustrial signals and detect abnormal states is an urgent need for the\nindustry. Due to the significant heterogeneity of these signals, which we\nsummarize as the M5 problem, previous works only focus on small sub-problems\nand employ specialized models, failing to utilize the synergies between\nmodalities and the powerful scaling law. However, we argue that the M5 signals\ncan be modeled in a unified manner due to the intrinsic similarity. As a\nresult, we propose FISHER, a Foundation model for multi-modal Industrial Signal\ncompreHEnsive Representation. To support arbitrary sampling rates, FISHER\nconsiders the increment of sampling rate as the concatenation of sub-band\ninformation. Specifically, FISHER takes the STFT sub-band as the modeling unit\nand adopts a teacher student SSL framework for pre-training. We also develop\nthe RMIS benchmark, which evaluates the representations of M5 industrial\nsignals on multiple health management tasks. Compared with top SSL models,\nFISHER showcases versatile and outstanding capabilities with a general\nperformance gain up to 5.03%, along with much more efficient scaling curves. We\nalso investigate the scaling law on downstream tasks and derive potential\navenues for future works. FISHER is now open-sourced on\nhttps://github.com/jianganbai/FISHER", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.16696v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16290", "title": "Dens3R: A Foundation Model for 3D Geometry Prediction", "authors": ["Xianze Fang", "Jingnan Gao", "Zhe Wang", "Zhuo Chen", "Xingyu Ren", "Jiangjing Lyu", "Qiaomu Ren", "Zhonglei Yang", "Xiaokang Yang", "Yichao Yan", "Chengfei Lyu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL , Code: this https URL", "url": "http://arxiv.org/abs/2507.16290v1", "summary": "Recent advances in dense 3D reconstruction have led to significant progress,\nyet achieving accurate unified geometric prediction remains a major challenge.\nMost existing methods are limited to predicting a single geometry quantity from\ninput images. However, geometric quantities such as depth, surface normals, and\npoint maps are inherently correlated, and estimating them in isolation often\nfails to ensure consistency, thereby limiting both accuracy and practical\napplicability. This motivates us to explore a unified framework that explicitly\nmodels the structural coupling among different geometric properties to enable\njoint regression. In this paper, we present Dens3R, a 3D foundation model\ndesigned for joint geometric dense prediction and adaptable to a wide range of\ndownstream tasks. Dens3R adopts a two-stage training framework to progressively\nbuild a pointmap representation that is both generalizable and intrinsically\ninvariant. Specifically, we design a lightweight shared encoder-decoder\nbackbone and introduce position-interpolated rotary positional encoding to\nmaintain expressive power while enhancing robustness to high-resolution inputs.\nBy integrating image-pair matching features with intrinsic invariance modeling,\nDens3R accurately regresses multiple geometric quantities such as surface\nnormals and depth, achieving consistent geometry perception from single-view to\nmulti-view inputs. Additionally, we propose a post-processing pipeline that\nsupports geometrically consistent multi-view inference. Extensive experiments\ndemonstrate the superior performance of Dens3R across various dense 3D\nprediction tasks and highlight its potential for broader applications.", "comment": "Project Page: https://g-1nonly.github.io/Dens3R/, Code:\n  https://github.com/G-1nOnly/Dens3R", "pdf_url": "http://arxiv.org/pdf/2507.16290v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15970", "title": "Nonlinear Framework for Speech Bandwidth Extension", "authors": ["Tarikul Islam Tamiti", "Nursad Mamun", "Anomadarshi Barua"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15970v1", "summary": "Recovering high-frequency components lost to bandwidth constraints is crucial\nfor applications ranging from telecommunications to high-fidelity audio on\nlimited resources. We introduce NDSI-BWE, a new adversarial Band Width\nExtension (BWE) framework that leverage four new discriminators inspired by\nnonlinear dynamical system to capture diverse temporal behaviors: a\nMulti-Resolution Lyapunov Discriminator (MRLD) for determining sensitivity to\ninitial conditions by capturing deterministic chaos, a Multi-Scale Recurrence\nDiscriminator (MS-RD) for self-similar recurrence dynamics, a Multi-Scale\nDetrended Fractal Analysis Discriminator (MSDFA) for long range slow variant\nscale invariant relationship, a Multi-Resolution Poincar\\'e Plot Discriminator\n(MR-PPD) for capturing hidden latent space relationship, a Multi-Period\nDiscriminator (MPD) for cyclical patterns, a Multi-Resolution Amplitude\nDiscriminator (MRAD) and Multi-Resolution Phase Discriminator (MRPD) for\ncapturing intricate amplitude-phase transition statistics. By using depth-wise\nconvolution at the core of the convolutional block with in each discriminators,\nNDSI-BWE attains an eight-times parameter reduction. These seven discriminators\nguide a complex-valued ConformerNeXt based genetor with a dual stream\nLattice-Net based architecture for simultaneous refinement of magnitude and\nphase. The genertor leverage the transformer based conformer's global\ndependency modeling and ConvNeXt block's local temporal modeling capability.\nAcross six objective evaluation metrics and subjective based texts comprises of\nfive human judges, NDSI-BWE establishes a new SoTA in BWE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15970v1", "cate": "cs.SD", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16729", "title": "Improving Model Classification by Optimizing the Training Dataset", "authors": ["Morad Tukan", "Loay Mualem", "Eitan Netzer", "Liran Sigalat"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16729v1", "summary": "In the era of data-centric AI, the ability to curate high-quality training\ndata is as crucial as model design. Coresets offer a principled approach to\ndata reduction, enabling efficient learning on large datasets through\nimportance sampling. However, conventional sensitivity-based coreset\nconstruction often falls short in optimizing for classification performance\nmetrics, e.g., $F1$ score, focusing instead on loss approximation. In this\nwork, we present a systematic framework for tuning the coreset generation\nprocess to enhance downstream classification quality. Our method introduces new\ntunable parameters--including deterministic sampling, class-wise allocation,\nand refinement via active sampling, beyond traditional sensitivity scores.\nThrough extensive experiments on diverse datasets and classifiers, we\ndemonstrate that tuned coresets can significantly outperform both vanilla\ncoresets and full dataset training on key classification metrics, offering an\neffective path towards better and more efficient model training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16729v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16310", "title": "MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation", "authors": ["Yanchen Liu", "Yanan Sun", "Zhening Xing", "Junyao Gao", "Kai Chen", "Wenjie Pei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16310v1", "summary": "Existing text-to-video methods struggle to transfer motion smoothly from a\nreference object to a target object with significant differences in appearance\nor structure between them. To address this challenge, we introduce MotionShot,\na training-free framework capable of parsing reference-target correspondences\nin a fine-grained manner, thereby achieving high-fidelity motion transfer while\npreserving coherence in appearance. To be specific, MotionShot first performs\nsemantic feature matching to ensure high-level alignments between the reference\nand target objects. It then further establishes low-level morphological\nalignments through reference-to-target shape retargeting. By encoding motion\nwith temporal attention, our MotionShot can coherently transfer motion across\nobjects, even in the presence of significant appearance and structure\ndisparities, demonstrated by extensive experiments. The project page is\navailable at: https://motionshot.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16310v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15979", "title": "Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars", "authors": ["Marcel C. Bhler", "Ye Yuan", "Xueting Li", "Yangyi Huang", "Koki Nagano", "Umar Iqbal"], "categories": ["cs.GR", "cs.AI"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15979v1", "summary": "We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs\nanimatable 3D human avatars from a single image. This is achieved by leveraging\nmulti-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of\n3D Gaussians. Given an image, we first dream plausible multi-views using a\nvideo diffusion model, capturing rich geometric and appearance details. These\nviews are then lifted into unstructured 3D Gaussians. To enable animation, we\npropose a transformer-based encoder that models global spatial relationships\nand projects these Gaussians into a structured latent representation aligned\nwith the UV space of a parametric body model. This latent code is decoded into\nUV-space Gaussians that can be animated via body-driven deformation and\nrendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV\nmanifold, our method ensures consistency during animation while preserving fine\nvisual details. DLA enables real-time rendering and intuitive editing without\nrequiring post-processing. Our method outperforms state-of-the-art approaches\non ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric\naccuracy. By combining the generative strengths of video diffusion models with\na pose-aware UV-space Gaussian mapping, DLA bridges the gap between\nunstructured 3D representations and high-fidelity, animation-ready avatars.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15979v1", "cate": "cs.GR", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16771", "title": "A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling", "authors": ["Michael Grosskopf", "Kellin Rumsey", "Ayan Biswas", "Earl Lawrence"], "categories": ["cs.LG", "stat.AP", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16771v1", "summary": "The next generation of Department of Energy supercomputers will be capable of\nexascale computation. For these machines, far more computation will be possible\nthan that which can be saved to disk. As a result, users will be unable to rely\non post-hoc access to data for uncertainty quantification and other statistical\nanalyses and there will be an urgent need for sophisticated machine learning\nalgorithms which can be trained in situ. Algorithms deployed in this setting\nmust be highly scalable, memory efficient and capable of handling data which is\ndistributed across nodes as spatially contiguous partitions. One suitable\napproach involves fitting a sparse variational Gaussian process (SVGP) model\nindependently and in parallel to each spatial partition. The resulting model is\nscalable, efficient and generally accurate, but produces the undesirable effect\nof constructing discontinuous response surfaces due to the disagreement between\nneighboring models at their shared boundary. In this paper, we extend this idea\nby allowing for a small amount of communication between neighboring spatial\npartitions which encourages better alignment of the local models, leading to\nsmoother spatial predictions and a better fit in general. Due to our\ndecentralized communication scheme, the proposed extension remains highly\nscalable and adds very little overhead in terms of computation (and none, in\nterms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the\nEnergy Exascale Earth System Model (E3SM) and compare the results to the\nindependent SVGP case.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16771v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16318", "title": "M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision", "authors": ["Kailai Zhou", "Fuqiang Yang", "Shixian Wang", "Bihan Wen", "Chongde Zi", "Linsen Chen", "Qiu Shen", "Xun Cao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.16318v1", "summary": "RGB-Thermal (RGBT) multispectral vision is essential for robust perception in\ncomplex environments. Most RGBT tasks follow a case-by-case research paradigm,\nrelying on manually customized models to learn task-oriented representations.\nNevertheless, this paradigm is inherently constrained by artificial inductive\nbias, modality bias, and data bottleneck. To address these limitations, we make\nthe initial attempt to build a Generalized RGBT MultiSpectral foundation model\n(M-SpecGene), which aims to learn modality-invariant representations from\nlarge-scale broad data in a self-supervised manner. M-SpecGene provides new\ninsights into multispectral fusion and integrates prior case-by-case studies\ninto a unified paradigm. Considering the unique characteristic of information\nimbalance in RGBT data, we introduce the Cross-Modality Structural Sparsity\n(CMSS) metric to quantify the information density across two modalities. Then\nwe develop the GMM-CMSS progressive masking strategy to facilitate a flexible,\neasy-to-hard, and object-centric pre-training process. Comprehensive\nexperiments validate M-SpecGene's generalizability across eleven datasets for\nfour RGBT downstream tasks. The code will be available at\nhttps://github.com/CalayZhou/M-SpecGene.", "comment": "accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.16318v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16002", "title": "Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation", "authors": ["Sumit Singh", "Rohit Mishra", "Uma Shanker Tiwary"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16002v1", "summary": "One major challenge in natural language processing is named entity\nrecognition (NER), which identifies and categorises named entities in textual\ninput. In order to improve NER, this study investigates a Hindi NER technique\nthat makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and\nGenerative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf\n(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments\nthe data with retrieved data from external relevant contexts, notably from\nWikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.\nHowever, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER\ngeneration. Our investigation shows that the mentioned language models (LMs)\nwith Retrieval Augmentation (RA) outperform baseline methods that don't\nincorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69\nand 0.495, respectively, without RA and increase to 0.70 and 0.71,\nrespectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B\nby a significant margin. On the other hand the generative models which are not\nfine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA\nwell; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval\ncontext. The findings show that RA significantly improves performance,\nespecially for low-context data. This study adds significant knowledge about\nhow best to use data augmentation methods and pretrained models to enhance NER\nperformance, particularly in languages with limited resources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16002v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16795", "title": "Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning", "authors": ["Helena Casademunt", "Caden Juang", "Adam Karvonen", "Samuel Marks", "Senthooran Rajamanoharan", "Neel Nanda"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16795v1", "summary": "Fine-tuning large language models (LLMs) can lead to unintended\nout-of-distribution generalization. Standard approaches to this problem rely on\nmodifying training data, for example by adding data that better specify the\nintended generalization. However, this is not always practical. We introduce\nConcept Ablation Fine-Tuning (CAFT), a technique that leverages\ninterpretability tools to control how LLMs generalize from fine-tuning, without\nneeding to modify the training data or otherwise use data from the target\ndistribution. Given a set of directions in an LLM's latent space corresponding\nto undesired concepts, CAFT works by ablating these concepts with linear\nprojections during fine-tuning, steering the model away from unintended\ngeneralizations. We successfully apply CAFT to three fine-tuning tasks,\nincluding emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow\ntask generalize to give egregiously misaligned responses to general questions.\nWithout any changes to the fine-tuning data, CAFT reduces misaligned responses\nby 10x without degrading performance on the training distribution. Overall,\nCAFT represents a novel approach for steering LLM generalization without\nmodifying training data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16795v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16330", "title": "Scene Text Detection and Recognition \"in light of\" Challenging Environmental Conditions using Aria Glasses Egocentric Vision Cameras", "authors": ["Joseph De Mathia", "Carlos Francisco Moreno-Garca"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 8 figures", "url": "http://arxiv.org/abs/2507.16330v1", "summary": "In an era where wearable technology is reshaping applications, Scene Text\nDetection and Recognition (STDR) becomes a straightforward choice through the\nlens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this\npaper investigates how environmental variables, such as lighting, distance, and\nresolution, affect the performance of state-of-the-art STDR algorithms in\nreal-world scenarios. We introduce a novel, custom-built dataset captured under\ncontrolled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST\nwith PyTesseract. Our findings reveal that resolution and distance\nsignificantly influence recognition accuracy, while lighting plays a less\npredictable role. Notably, image upscaling emerged as a key pre-processing\ntechnique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further\ndemonstrate the potential of integrating eye-gaze tracking to optimise\nprocessing efficiency by focusing on user attention zones. This work not only\nbenchmarks STDR performance under realistic conditions but also lays the\ngroundwork for adaptive, user-aware AR systems. Our contributions aim to\ninspire future research in robust, context-sensitive text recognition for\nassistive and research-oriented applications, such as asset inspection and\nnutrition analysis. The code is available at\nhttps://github.com/josepDe/Project_Aria_STR.", "comment": "15 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.16330v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16027", "title": "Fast Feeder Reconfiguration via Mesh Adaptive Direct Search in Black-Box Distribution System Environments", "authors": ["Junyuan Zheng", "Wenlong Shi", "Zhaoyu Wang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      3 pages, 3 figures", "url": "http://arxiv.org/abs/2507.16027v1", "summary": "Feeder reconfiguration is a critical operational strategy in power\ndistribution systems. However, existing optimization approaches typically rely\non explicit mathematical formulations and analytical models, which are often\ninfeasible in practical utility environments characterized by heterogeneous,\nproprietary, and black-box simulation modules. To address this challenge, this\npaper proposes a fast feeder reconfiguration framework based on Mesh Adaptive\nDirect Search (MADS). The proposed approach requires only performance metric\nevaluations through simulation modules used for power flow, protection, and\nvoltage regulation analysis. A bi-objective formulation is adopted to jointly\nminimize active power loss and operational constraint violations. A\nPareto-based frontier filter is integrated into the MADS algorithm to\nefficiently guide the search toward high-quality configurations while\nsystematically pruning dominated solutions. The approach adaptively refines the\nsearch space around promising candidates using local polling strategies and\nconvergence aware updates. Case studies on the IEEE-123 node test feeder\ndemonstrate that the proposed approach achieves near-optimal configurations\nwith significantly fewer evaluations compared to heuristic methods.", "comment": "3 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.16027v1", "cate": "eess.SY", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16005", "title": "AutoMAT: A Hierarchical Framework for Autonomous Alloy Discovery", "authors": ["Penghui Yang", "Chendong Zhao", "Bijun Tang", "Zhonghan Zhang", "Xinrun Wang", "Yanchen Deng", "Yuhao Lu", "Cuntai Guan", "Zheng Liu", "Bo An"], "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16005v1", "summary": "Alloy discovery is central to advancing modern industry but remains hindered\nby the vastness of compositional design space and the costly validation. Here,\nwe present AutoMAT, a hierarchical and autonomous framework grounded in and\nvalidated by experiments, which integrates large language models, automated\nCALPHAD-based simulations, and AI-driven search to accelerate alloy design.\nSpanning the entire pipeline from ideation to validation, AutoMAT achieves high\nefficiency, accuracy, and interpretability without the need for manually\ncurated large datasets. In a case study targeting a lightweight, high-strength\nalloy, AutoMAT identifies a titanium alloy with 8.1% lower density and\ncomparable yield strength relative to the state-of-the-art reference, achieving\nthe highest specific strength among all comparisons. In a second case targeting\nhigh-yield-strength high-entropy alloys, AutoMAT achieves a 28.2% improvement\nin yield strength over the base alloy. In both cases, AutoMAT reduces the\ndiscovery timeline from years to weeks, illustrating its potential as a\nscalable and versatile platform for next-generation alloy design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16005v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16806", "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty", "authors": ["Mehul Damani", "Isha Puri", "Stewart Slocum", "Idan Shenfeld", "Leshem Choshen", "Yoon Kim", "Jacob Andreas"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16806v1", "summary": "When language models (LMs) are trained via reinforcement learning (RL) to\ngenerate natural language \"reasoning chains\", their performance improves on a\nvariety of difficult question answering tasks. Today, almost all successful\napplications of RL for reasoning use binary reward functions that evaluate the\ncorrectness of LM outputs. Because such reward functions do not penalize\nguessing or low-confidence outputs, they often have the unintended side-effect\nof degrading calibration and increasing the rate at which LMs generate\nincorrect responses (or \"hallucinate\") in other problem domains. This paper\ndescribes RLCR (Reinforcement Learning with Calibration Rewards), an approach\nto training reasoning models that jointly improves accuracy and calibrated\nconfidence estimation. During RLCR, LMs generate both predictions and numerical\nconfidence estimates after reasoning. They are trained to optimize a reward\nfunction that augments a binary correctness score with a Brier score -- a\nscoring rule for confidence estimates that incentivizes calibrated prediction.\nWe first prove that this reward function (or any analogous reward function that\nuses a bounded, proper scoring rule) yields models whose predictions are both\naccurate and well-calibrated. We next show that across diverse datasets, RLCR\nsubstantially improves calibration with no loss in accuracy, on both in-domain\nand out-of-domain evaluations -- outperforming both ordinary RL training and\nclassifiers trained to assign post-hoc confidence scores. While ordinary RL\nhurts calibration, RLCR improves it. Finally, we demonstrate that verbalized\nconfidence can be leveraged at test time to improve accuracy and calibration\nvia confidence-weighted scaling methods. Our results show that explicitly\noptimizing for calibration can produce more generally reliable reasoning\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16806v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16337", "title": "One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via Cascaded Priors and Iterative Prompt Evolution", "authors": ["Xinyu Mao", "Xiaohan Xing", "Fei Meng", "Jianbang Liu", "Fan Bai", "Qiang Nie", "Max Meng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.16337v1", "summary": "Polyp segmentation is vital for early colorectal cancer detection, yet\ntraditional fully supervised methods struggle with morphological variability\nand domain shifts, requiring frequent retraining. Additionally, reliance on\nlarge-scale annotations is a major bottleneck due to the time-consuming and\nerror-prone nature of polyp boundary labeling. Recently, vision foundation\nmodels like Segment Anything Model (SAM) have demonstrated strong\ngeneralizability and fine-grained boundary detection with sparse prompts,\neffectively addressing key polyp segmentation challenges. However, SAM's\nprompt-dependent nature limits automation in medical applications, since\nmanually inputting prompts for each image is labor-intensive and\ntime-consuming. We propose OP-SAM, a One-shot Polyp segmentation framework\nbased on SAM that automatically generates prompts from a single annotated\nimage, ensuring accurate and generalizable segmentation without additional\nannotation burdens. Our method introduces Correlation-based Prior Generation\n(CPG) for semantic label transfer and Scale-cascaded Prior Fusion (SPF) to\nadapt to polyp size variations as well as filter out noisy transfers. Instead\nof dumping all prompts at once, we devise Euclidean Prompt Evolution (EPE) for\niterative prompt refinement, progressively enhancing segmentation quality.\nExtensive evaluations across five datasets validate OP-SAM's effectiveness.\nNotably, on Kvasir, it achieves 76.93% IoU, surpassing the state-of-the-art by\n11.44%.", "comment": "accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.16337v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16061", "title": "Analytical Framework for Power System Strength", "authors": ["Ignacio Ponce", "Federico Milano"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16061v1", "summary": "This paper proposes a general framework to evaluate power system strength.\nThe formulation features twelve indicators, grouped in three dynamical orders,\nthat quantify the resistance of bus voltage phasors and their first and second\norder rates of change to sudden current injection changes. To quantify such\nchanges the paper introduces a novel finite differentiation technique, that we\nnamed Delta operator, able to properly capture \"jumps\" of algebraic variables\nand utilizes the recently developed concept of complex frequency. The paper\nalso shows how the proposed framework can be systematically applied to any\nsystem device, and provides a variety of examples based on synchronous\nmachines, converters and loads models are given. Numerical results in a\nbenchmark system validate the exactness of the formulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16061v1", "cate": "eess.SY", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16043", "title": "Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks", "authors": ["Ziqiao Yu", "Pengfei Sun", "Dan F. M. Goodman"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16043v1", "summary": "We investigate the extent to which Spiking Neural Networks (SNNs) trained\nwith Surrogate Gradient Descent (Surrogate GD), with and without delay\nlearning, can learn from precise spike timing beyond firing rates. We first\ndesign synthetic tasks isolating intra-neuron inter-spike intervals and\ncross-neuron synchrony under matched spike counts. On more complex spike-based\nspeech recognition datasets (Spiking Heidelberg Digits (SHD) and Spiking Speech\nCommands (SSC), we construct variants where spike count information is\neliminated and only timing information remains, and show that Surrogate\nGD-trained SNNs are able to perform significantly above chance whereas purely\nrate-based models perform at chance level. We further evaluate robustness under\nbiologically inspired perturbations -- including Gaussian jitter per spike or\nper-neuron, and spike deletion -- revealing consistent but\nperturbation-specific degradation. Networks show a sharp performance drop when\nspike sequences are reversed in time, with a larger drop in performance from\nSNNs trained with delays, indicating that these networks are more human-like in\nterms of behaviour. To facilitate further studies of temporal coding, we have\nreleased our modified SHD and SSC datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16043v1", "cate": "cs.NE", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16814", "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning", "authors": ["Junhao Shen", "Haiteng Zhao", "Yuzhe Gu", "Songyang Gao", "Kuikun Liu", "Haian Huang", "Jianfei Gao", "Dahua Lin", "Wenwei Zhang", "Kai Chen"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16814v1", "summary": "Enhancing large vision-language models (LVLMs) with visual slow-thinking\nreasoning is crucial for solving complex multimodal tasks. However, since LVLMs\nare mainly trained with vision-language alignment, it is difficult to adopt\non-policy reinforcement learning (RL) to develop the slow thinking ability\nbecause the rollout space is restricted by its initial abilities. Off-policy RL\noffers a way to go beyond the current policy, but directly distilling\ntrajectories from external models may cause visual hallucinations due to\nmismatched visual perception abilities across models. To address these issues,\nthis paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for\nvision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy\nbehavior model by combining on-policy visual understanding from a trainable\nLVLM with off-policy slow-thinking reasoning from a language model, assigns\noutcome-based rewards to reasoning, and propagates visual rewards backward.\nThen LVLM learns slow-thinking reasoning ability from the obtained reasoning\ntrajectories using propagated rewards via off-policy RL algorithms. Extensive\nexperiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the\neffectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in\naverage, reaching state-of-the-art performance among open-source LVLMs on\nmultiple multimodal reasoning benchmarks, and even outperforms some\nclosed-source models (e.g., GPT-4.1) on the challenging MathVision and\nOlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.\nAnalysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy\nRL methods, offering a better policy initialization for further on-policy\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16814v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16341", "title": "Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model", "authors": ["Mingtao Guo", "Guanyu Xing", "Yanci Zhang", "Yanli Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16341v1", "summary": "Face reenactment aims to generate realistic talking head videos by\ntransferring motion from a driving video to a static source image while\npreserving the source identity. Although existing methods based on either\nimplicit or explicit keypoints have shown promise, they struggle with large\npose variations due to warping artifacts or the limitations of coarse facial\nlandmarks. In this paper, we present the Face Reenactment Video Diffusion model\n(FRVD), a novel framework for high-fidelity face reenactment under large pose\nchanges. Our method first employs a motion extractor to extract implicit facial\nkeypoints from the source and driving images to represent fine-grained motion\nand to perform motion alignment through a warping module. To address the\ndegradation introduced by warping, we introduce a Warping Feature Mapper (WFM)\nthat maps the warped source image into the motion-aware latent space of a\npretrained image-to-video (I2V) model. This latent space encodes rich priors of\nfacial dynamics learned from large-scale video data, enabling effective warping\ncorrection and enhancing temporal coherence. Extensive experiments show that\nFRVD achieves superior performance over existing methods in terms of pose\naccuracy, identity preservation, and visual quality, especially in challenging\nscenarios with extreme pose variations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16341v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16071", "title": "Automating Capacitor Part Selection with Dual-Objective Optimization", "authors": ["Luke Brantingham", "Jason Grover"], "categories": ["eess.SY", "cs.SY", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      7 pages, 7 figures", "url": "http://arxiv.org/abs/2507.16071v1", "summary": "This paper presents a novel framework for optimizing capacitor selection in\nelectronic design using multi-objective linear and non-linear constrained\noptimization techniques. We demonstrate the effectiveness of this approach in\nminimizing cost and board area while meeting critical performance requirements.", "comment": "7 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.16071v1", "cate": "eess.SY", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16083", "title": "Efficient Compositional Multi-tasking for On-device Large Language Models", "authors": ["Ondrej Bohdal", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16083v1", "summary": "Adapter parameters provide a mechanism to modify the behavior of machine\nlearning models and have gained significant popularity in the context of large\nlanguage models (LLMs) and generative AI. These parameters can be merged to\nsupport multiple tasks via a process known as task merging. However, prior work\non merging in LLMs, particularly in natural language processing, has been\nlimited to scenarios where each test example addresses only a single task. In\nthis paper, we focus on on-device settings and study the problem of text-based\ncompositional multi-tasking, where each test example involves the simultaneous\nexecution of multiple tasks. For instance, generating a translated summary of a\nlong text requires solving both translation and summarization tasks\nconcurrently. To facilitate research in this setting, we propose a benchmark\ncomprising four practically relevant compositional tasks. We also present an\nefficient method (Learnable Calibration) tailored for on-device applications,\nwhere computational resources are limited, emphasizing the need for solutions\nthat are both resource-efficient and high-performing. Our contributions lay the\ngroundwork for advancing the capabilities of LLMs in real-world multi-tasking\nscenarios, expanding their applicability to complex, resource-constrained use\ncases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16083v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.15864", "title": "Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity", "authors": ["Guowen Yuan", "Tien-Hsuan Wu", "Lianghao Xia", "Ben Kao"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15864v1", "summary": "We study the problem of named entity recognition (NER) based on demonstration\nlearning in low-resource scenarios. We identify two issues in demonstration\nconstruction and model training. Firstly, existing methods for selecting\ndemonstration examples primarily rely on semantic similarity; We show that\nfeature similarity can provide significant performance improvement. Secondly,\nwe show that the NER tagger's ability to reference demonstration examples is\ngenerally inadequate. We propose a demonstration and training approach that\neffectively addresses these issues. For the first issue, we propose to select\nexamples by dual similarity, which comprises both semantic similarity and\nfeature similarity. For the second issue, we propose to train an NER model with\nadversarial demonstration such that the model is forced to refer to the\ndemonstrations when performing the tagging task. We conduct comprehensive\nexperiments in low-resource NER tasks, and the results demonstrate that our\nmethod outperforms a range of methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15864v1", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.16342", "title": "Mamba-OTR: a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video", "authors": ["Alessandro Sebastiano Catinello", "Giovanni Maria Farinella", "Antonino Furnari"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16342v1", "summary": "This work tackles the problem of Online detection of Take and Release (OTR)\nof an object in untrimmed egocentric videos. This task is challenging due to\nsevere label imbalance, with temporally sparse positive annotations, and the\nneed for precise temporal predictions. Furthermore, methods need to be\ncomputationally efficient in order to be deployed in real-world online\nsettings. To address these challenges, we propose Mamba-OTR, a model based on\nthe Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence\nduring inference while being trained on short video clips. To address label\nimbalance, our training pipeline incorporates the focal loss and a novel\nregularization scheme that aligns model predictions with the evaluation metric.\nExtensive experiments on EPIC-KITCHENS-100, the comparisons with\ntransformer-based approach, and the evaluation of different training and test\nschemes demonstrate the superiority of Mamba-OTR in both accuracy and\nefficiency. These finding are particularly evident when evaluating full-length\nvideos or high frame-rate sequences, even when trained on short video snippets\nfor computational convenience. The proposed Mamba-OTR achieves a noteworthy\nmp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in\nstreaming mode, versus the 20.32 of a vanilla transformer and 25.16 of a\nvanilla Mamba, thus providing a strong baseline for OTR. We will publicly\nrelease the source code of Mamba-OTR to support future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16342v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16101", "title": "The Sustainability of the Leo Orbit Capacity via Risk-Driven Active Debris Removal", "authors": ["Yacob Medhin", "Simone Servadio"], "categories": ["eess.SY", "cs.SY", "physics.space-ph"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures", "url": "http://arxiv.org/abs/2507.16101v1", "summary": "The growing number of space debris in Low Earth Orbit (LEO) jeopardizes\nlong-term orbital sustainability, requiring efficient risk assessment for\nactive debris removal (ADR) missions. This study presents the development and\nvalidation of Filtered Modified MITRI (FMM), an enhanced risk index designed to\nimprove the prioritization of high-criticality debris. Leveraging the MOCAT-MC\nsimulation framework, we conducted a comprehensive performance evaluation and\nsensitivity analysis to probe the robustness of the FMM formulation. The\nresults demonstrate that while the FMM provides superior identification of\nhigh-risk targets for annual removal campaigns, a nuanced performance trade-off\nexists between risk models depending on the operational removal cadence. The\nanalysis also confirms that physically grounded mass terms are indispensable\nfor practical risk assessment. By providing a validated open source tool and\ncritical insights into the dynamics of risk, this research enhances our ability\nto select optimal ADR targets and ensure the long-term viability of LEO\noperations.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.16101v1", "cate": "eess.SY", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16136", "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization", "authors": ["Eduardo Pacheco", "Atila Orhon", "Berkin Durmus", "Blaise Munyampirwa", "Andrey Leonov"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16136v1", "summary": "Even state-of-the-art speaker diarization systems exhibit high variance in\nerror rates across different datasets, representing numerous use cases and\ndomains. Furthermore, comparing across systems requires careful application of\nbest practices such as dataset splits and metric definitions to allow for\napples-to-apples comparison. We propose SDBench (Speaker Diarization\nBenchmark), an open-source benchmark suite that integrates 13 diverse datasets\nwith built-in tooling for consistent and fine-grained analysis of speaker\ndiarization performance for various on-device and server-side systems. SDBench\nenables reproducible evaluation and easy integration of new systems over time.\nTo demonstrate the efficacy of SDBench, we built SpeakerKit, an inference\nefficiency-focused system built on top of Pyannote v3. SDBench enabled rapid\nexecution of ablation studies that led to SpeakerKit being 9.6x faster than\nPyannote v3 while achieving comparable error rates. We benchmark 6\nstate-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI\nAPI, revealing important trade-offs between accuracy and speed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16136v1", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15899", "title": "Structural DID with ML: Theory, Simulation, and a Roadmap for Applied Research", "authors": ["Yile Yu", "Anzhi Xu", "Yi Wang"], "categories": ["stat.ML", "cs.LG", "91-01"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      45 pages, 29 figures", "url": "http://arxiv.org/abs/2507.15899v1", "summary": "Causal inference in observational panel data has become a central concern in\neconomics,policy analysis,and the broader social sciences.To address the core\ncontradiction where traditional difference-in-differences (DID) struggles with\nhigh-dimensional confounding variables in observational panel data,while\nmachine learning (ML) lacks causal structure interpretability,this paper\nproposes an innovative framework called S-DIDML that integrates structural\nidentification with high-dimensional estimation.Building upon the structure of\ntraditional DID methods,S-DIDML employs structured residual orthogonalization\ntechniques (Neyman orthogonality+cross-fitting) to retain the group-time\ntreatment effect (ATT) identification structure while resolving\nhigh-dimensional covariate interference issues.It designs a dynamic\nheterogeneity estimation module combining causal forests and semi-parametric\nmodels to capture spatiotemporal heterogeneity effects.The framework\nestablishes a complete modular application process with standardized Stata\nimplementation paths.The introduction of S-DIDML enriches methodological\nresearch on DID and DDML innovations, shifting causal inference from method\nstacking to architecture integration.This advancement enables social sciences\nto precisely identify policy-sensitive groups and optimize resource\nallocation.The framework provides replicable evaluation tools, decision\noptimization references,and methodological paradigms for complex intervention\nscenarios such as digital transformation policies and environmental\nregulations.", "comment": "45 pages, 29 figures", "pdf_url": "http://arxiv.org/pdf/2507.15899v1", "cate": "stat.ML", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16362", "title": "LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification and Recognition Network", "authors": ["Guangzhu Xu", "Pengcheng Zuo", "Zhi Ke", "Bangjun Lei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      28 pages, 33 figures", "url": "http://arxiv.org/abs/2507.16362v1", "summary": "Chinese License Plate Recognition (CLPR) faces numerous challenges in\nunconstrained and complex environments, particularly due to perspective\ndistortions caused by various shooting angles and the correction of single-line\nand double-line license plates. Considering the limited computational resources\nof edge devices, developing a low-complexity, end-to-end integrated network for\nboth correction and recognition is essential for achieving real-time and\nefficient deployment. In this work, we propose a lightweight, unified network\nnamed LPTR-AFLNet for correcting and recognizing Chinese license plates, which\ncombines a perspective transformation correction module (PTR) with an optimized\nlicense plate recognition network, AFLNet. The network leverages the\nrecognition output as a weak supervisory signal to effectively guide the\ncorrection process, ensuring accurate perspective distortion correction. To\nenhance recognition accuracy, we introduce several improvements to LPRNet,\nincluding an improved attention module to reduce confusion among similar\ncharacters and the use of Focal Loss to address class imbalance during\ntraining. Experimental results demonstrate the exceptional performance of\nLPTR-AFLNet in rectifying perspective distortion and recognizing double-line\nlicense plate images, maintaining high recognition accuracy across various\nchallenging scenarios. Moreover, on lower-mid-range GPUs platform, the method\nruns in less than 10 milliseconds, indicating its practical efficiency and\nbroad applicability.", "comment": "28 pages, 33 figures", "pdf_url": "http://arxiv.org/pdf/2507.16362v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16155", "title": "Design and Implementation of a Lightweight Object Detection System for Resource-Constrained Edge Environments", "authors": ["Jiyue Jiang", "Mingtong Chen", "Zhengbao Yang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16155v1", "summary": "This project aims to develop a system to run the object detection model under\nlow power consumption conditions. The detection scene is set as an outdoor\ntraveling scene, and the detection categories include people and vehicles. In\nthis system, users data does not need to be uploaded to the cloud, which is\nsuitable for use in environments with portable needs and strict requirements\nfor data privacy. The MCU device used in this system is STM32H7, which has\nbetter performance among low power devices. The YOLOv5 system is selected to\ntrain the object detection model. To overcome the resource limitation of the\nembedded devices, this project uses several model compression techniques such\nas pruned, quantization, and distillation, which could improve the performance\nand efficiency of the detection model. Through these processes, the model s\ncomputation and the quantity of model parameters could be reduced, in order to\nrun computer vision models on micro-controller devices for the development of\nembedded vision applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16155v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16217", "title": "Towards Compute-Optimal Many-Shot In-Context Learning", "authors": ["Shahriar Golchin", "Yanfei Chen", "Rujun Han", "Manan Gandhi", "Tianli Yu", "Swaroop Mishra", "Mihai Surdeanu", "Rishabh Agarwal", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Final version; accepted at COLM 2025", "url": "http://arxiv.org/abs/2507.16217v1", "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.", "comment": "Final version; accepted at COLM 2025", "pdf_url": "http://arxiv.org/pdf/2507.16217v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15914", "title": "MSGM: A Multi-Scale Spatiotemporal Graph Mamba for EEG Emotion Recognition", "authors": ["Hanwen Liu", "Yifeng Gong", "Zuwei Yan", "Zeheng Zhuang", "Jiaxuan Lu"], "categories": ["eess.SP", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15914v1", "summary": "EEG-based emotion recognition struggles with capturing multi-scale\nspatiotemporal dynamics and ensuring computational efficiency for real-time\napplications. Existing methods often oversimplify temporal granularity and\nspatial hierarchies, limiting accuracy. To overcome these challenges, we\npropose the Multi-Scale Spatiotemporal Graph Mamba (MSGM), a novel framework\nintegrating multi-window temporal segmentation, bimodal spatial graph modeling,\nand efficient fusion via the Mamba architecture. By segmenting EEG signals\nacross diverse temporal scales and constructing global-local graphs with\nneuroanatomical priors, MSGM effectively captures fine-grained emotional\nfluctuations and hierarchical brain connectivity. A multi-depth Graph\nConvolutional Network (GCN) and token embedding fusion module, paired with\nMamba's state-space modeling, enable dynamic spatiotemporal interaction at\nlinear complexity. Notably, with just one MSST-Mamba layer, MSGM surpasses\nleading methods in the field on the SEED, THU-EP, and FACED datasets,\noutperforming baselines in subject-independent emotion classification while\nachieving robust accuracy and millisecond-level inference on the NVIDIA Jetson\nXavier NX.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15914v1", "cate": "eess.SP", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16385", "title": "STAR: A Benchmark for Astronomical Star Fields Super-Resolution", "authors": ["Kuo-Cheng Wu", "Guohang Zhuang", "Jinyang Huang", "Xiang Zhang", "Wanli Ouyang", "Yan Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16385v1", "summary": "Super-resolution (SR) advances astronomical imaging by enabling\ncost-effective high-resolution capture, crucial for detecting faraway celestial\nobjects and precise structural analysis. However, existing datasets for\nastronomical SR (ASR) exhibit three critical limitations: flux inconsistency,\nobject-crop setting, and insufficient data diversity, significantly impeding\nASR development. We propose STAR, a large-scale astronomical SR dataset\ncontaining 54,738 flux-consistent star field image pairs covering wide\ncelestial regions. These pairs combine Hubble Space Telescope high-resolution\nobservations with physically faithful low-resolution counterparts generated\nthrough a flux-preserving data generation pipeline, enabling systematic\ndevelopment of field-level ASR models. To further empower the ASR community,\nSTAR provides a novel Flux Error (FE) to evaluate SR models in physical view.\nLeveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR)\nmodel that could accurately infer the flux-consistent high-resolution images\nfrom input photometry, suppressing several SR state-of-the-art methods by\n24.84% on a novel designed flux consistency metric, showing the priority of our\nmethod for astrophysics. Extensive experiments demonstrate the effectiveness of\nour proposed method and the value of our dataset. Code and models are available\nat https://github.com/GuoCheng12/STAR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16385v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16157", "title": "Design and Optimization of Wearables for Human Motion Energy Harvesting", "authors": ["Weijia Peng", "Mingtong Chen", "Zhengbao Yang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16157v1", "summary": "As wearable electronics become increasingly prevalent, there is a rise in\ninterest and demand for sustainably designed systems that are also energy\nself-sufficient. The research described in this paper investigated a shoe-worn\nenergy harvesting system designed use the mechanical energy from walking to\noutput electrical energy. A spring is attached to electromagnetic generator\nembedded in the heel of the shoe to recover the vertical pressure caused by the\nfoot strike. The simulated prototype consisted of a standard EM generator\ndesigned in MATLAB demonstrating a maximum voltage of 12V. The initial low\nfidelity prototype demonstrated testing the relationship between the EM\ngenerator and a simple electrical circuit, with energy output observed. Future\nresearch will explore enhancing the overall generator design, integrate a power\nmanagement IC for battery protect and regulation, and combine the system into a\nfinal product, wearable footwear. This research lays a foundation for\nself-powered footwear and energy independent wearable electronic devices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16157v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15969", "title": "Modeling and Analysis of Land-to-Ship Maritime Wireless Channels at 5.8 GHz", "authors": ["Shu Sun", "Yulu Guo", "Meixia Tao", "Wei Feng", "Jun Chen", "Ruifeng Gao", "Ye Li", "Jue Wang", "Theodore S. Rappaport"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15969v1", "summary": "Maritime channel modeling is crucial for designing robust communication\nsystems in marine environments, where factors like waves and wind impact signal\npropagation. This article investigates land-to-ship maritime wireless channel\ncharacteristics at 5.8 GHz based upon an extensive measurement campaign, with\nconcurrent hydrological and meteorological information collection. First, a\nnovel large-scale path loss model with physical foundation and high accuracy is\nproposed for dynamic marine environments. Then, we introduce the concept of\nsea-wave-induced fixed-point (SWIFT) fading, a peculiar phenomenon in maritime\nscenarios that captures the impact of sea surface fluctuations on received\npower. An enhanced two-ray model incorporating vessel rotational motion is\npropounded to simulate the SWIFT fading, showing good alignment with measured\ndata, particularly for modest antenna movements. Next, the small-scale fading\nis studied by leveraging a variety of models including the two-wave with\ndiffuse power (TWDP) and asymmetric Laplace distributions, with the latter\nperforming well in most cases, while TWDP better captures bimodal fading in\nrough seas. Furthermore, maritime channel sparsity is examined via the Gini\nindex and Rician $K$ factor, and temporal dispersion is characterized. The\nresulting channel models and parameter characteristics offer valuable insights\nfor maritime wireless system design and deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15969v1", "cate": "eess.SP", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16219", "title": "Bayesian Deep Learning for Convective Initiation Nowcasting Uncertainty Estimation", "authors": ["Da Fan", "David John Gagne II", "Steven J. Greybush", "Eugene E. Clothiaux", "John S. Schreck", "Chaopeng Shen"], "categories": ["physics.ao-ph", "cs.AI"], "primary_category": "Subjects:       Atmospheric and Oceanic Physics (physics.ao-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16219v1", "summary": "This study evaluated the probability and uncertainty forecasts of five\nrecently proposed Bayesian deep learning methods relative to a deterministic\nresidual neural network (ResNet) baseline for 0-1 h convective initiation (CI)\nnowcasting using GOES-16 satellite infrared observations. Uncertainty was\nassessed by how well probabilistic forecasts were calibrated and how well\nuncertainty separated forecasts with large and small errors. Most of the\nBayesian deep learning methods produced probabilistic forecasts that\noutperformed the deterministic ResNet, with one, the initial-weights ensemble +\nMonte Carlo (MC) dropout, an ensemble of deterministic ResNets with different\ninitial weights to start training and dropout activated during inference,\nproducing the most skillful and well-calibrated forecasts. The initial-weights\nensemble + MC dropout benefited from generating multiple solutions that more\nthoroughly sampled the hypothesis space. The Bayesian ResNet ensemble was the\nonly one that performed worse than the deterministic ResNet at longer lead\ntimes, likely due to the challenge of optimizing a larger number of parameters.\nTo address this issue, the Bayesian-MOPED (MOdel Priors with Empirical Bayes\nusing Deep neural network) ResNet ensemble was adopted, and it enhanced\nforecast skill by constraining the hypothesis search near the deterministic\nResNet hypothesis. All Bayesian methods demonstrated well-calibrated\nuncertainty and effectively separated cases with large and small errors. In\ncase studies, the initial-weights ensemble + MC dropout demonstrated better\nforecast skill than the Bayesian-MOPED ensemble and the deterministic ResNet on\nselected CI events in clear-sky regions. However, the initial-weights ensemble\n+ MC dropout exhibited poorer generalization in clear-sky and anvil cloud\nregions without CI occurrence compared to the deterministic ResNet and\nBayesian-MOPED ensemble.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16219v1", "cate": "physics.ao-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15976", "title": "Efficient dataset construction using active learning and uncertainty-aware neural networks for plasma turbulent transport surrogate models", "authors": ["Aaron Ho", "Lorenzo Zanisi", "Bram de Leeuw", "Vincent Galvan", "Pablo Rodriguez-Fernandez", "Nathaniel T. Howard"], "categories": ["physics.plasm-ph", "cs.LG"], "primary_category": "Subjects:       Plasma Physics (physics.plasm-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15976v1", "summary": "This work demonstrates a proof-of-principle for using uncertainty-aware\narchitectures, in combination with active learning techniques and an\nin-the-loop physics simulation code as a data labeller, to construct efficient\ndatasets for data-driven surrogate model generation. Building off of a previous\nproof-of-principle successfully demonstrating training set reduction on static\npre-labelled datasets, using the ADEPT framework, this strategy was applied\nagain to the plasma turbulent transport problem within tokamak fusion plasmas,\nspecifically the QuaLiKiz quasilinear electrostatic gyrokinetic turbulent\ntransport code. While QuaLiKiz provides relatively fast evaluations, this study\nspecifically targeted small datasets to serve as a proxy for more expensive\ncodes, such as CGYRO or GENE. The newly implemented algorithm uses the SNGP\narchitecture for the classification component of the problem and the BNN-NCP\narchitecture for the regression component, training models for all turbulent\nmodes (ITG, TEM, ETG) and all transport fluxes ($Q_e$, $Q_i$, $\\Gamma_e$,\n$\\Gamma_i$, and $\\Pi_i$) described by the general QuaLiKiz output. With 45\nactive learning iterations, moving from a small initial training set of\n$10^{2}$ to a final set of $10^{4}$, the resulting models reached a $F_1$\nclassification performance of ~0.8 and a $R^2$ regression performance of ~0.75\non an independent test set across all outputs. This extrapolates to reaching\nthe same performance and efficiency as the previous ADEPT pipeline, although on\na problem with 1 extra input dimension. While the improvement rate achieved in\nthis implementation diminishes faster than expected, the overall technique is\nformulated with components that can be upgraded and generalized to many\nsurrogate modeling applications beyond plasma turbulent transport predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15976v1", "cate": "physics.plasm-ph", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16389", "title": "From Flat to Round: Redefining Brain Decoding with Surface-Based fMRI and Cortex Structure", "authors": ["Sijin Yu", "Zijiao Chen", "Wenxuan Wu", "Shengxian Chen", "Zhongliang Liu", "Jingxin Nie", "Xiaofen Xing", "Xiangmin Xu", "Xin Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 14 figures, ICCV Findings 2025", "url": "http://arxiv.org/abs/2507.16389v1", "summary": "Reconstructing visual stimuli from human brain activity (e.g., fMRI) bridges\nneuroscience and computer vision by decoding neural representations. However,\nexisting methods often overlook critical brain structure-function\nrelationships, flattening spatial information and neglecting individual\nanatomical variations. To address these issues, we propose (1) a novel sphere\ntokenizer that explicitly models fMRI signals as spatially coherent 2D\nspherical data on the cortical surface; (2) integration of structural MRI\n(sMRI) data, enabling personalized encoding of individual anatomical\nvariations; and (3) a positive-sample mixup strategy for efficiently leveraging\nmultiple fMRI scans associated with the same visual stimulus. Collectively,\nthese innovations enhance reconstruction accuracy, biological interpretability,\nand generalizability across individuals. Experiments demonstrate superior\nreconstruction performance compared to SOTA methods, highlighting the\neffectiveness and interpretability of our biologically informed approach.", "comment": "18 pages, 14 figures, ICCV Findings 2025", "pdf_url": "http://arxiv.org/pdf/2507.16389v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16281", "title": "The Bode Plots for Sliding-Mode Control Design", "authors": ["Ulises Prez-Ventura"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      15 pages, 17 figures", "url": "http://arxiv.org/abs/2507.16281v1", "summary": "This paper develops a unified frequency-domain framework for the analysis of\nsliding-mode control systems, encompassing both discontinuous and\nLipschitz-continuous implementations. Using describing function (DF) theory,\nclosed-form expressions are derived for the amplitude and frequency of\nchattering oscillations, as well as equivalent gain (EG) models that enable\nclosed-loop sensitivity analysis. The proposed methodology captures the\ninfluence of actuator dynamics, control parameters, and disturbance profiles on\nsteady-state performance.\n  Theoretical predictions for bias and oscillatory components are validated\nthrough simulations under both constant and sinusoidal perturbations. In the\nlow-frequency regime, the EG-based sensitivity functions accurately predict the\namplitude and phase of the system response, with tracking errors remaining\nwithin a 15\\% margin, provided that the DF assumptions hold. The framework also\nincorporates orbital stability considerations via Loebs criterion, ensuring\nthat chattering remains bounded.\n  Overall, the results offer practical insight into the robust design of\nsliding-mode controllers, enabling systematic gain tuning that balances\ndisturbance rejection and chattering attenuation, while accounting for actuator\nand sensor constraints.", "comment": "15 pages, 17 figures", "pdf_url": "http://arxiv.org/pdf/2507.16281v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16132", "title": "Meta-Reinforcement Learning Optimization for Movable Antenna-aided Full-Duplex CF-DFRC Systems with Carrier Frequency Offset", "authors": ["Yue Xiu", "Wanting Lyu", "You Li", "Ran Yang", "Phee Lep Yeoh", "Wei Zhang", "Guangyi Liu", "Ning Wei"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16132v1", "summary": "By enabling spectrum sharing between radar and communication operations, the\ncell-free dual-functional radar-communication (CF-DFRC) system is a promising\ncandidate to significantly improve spectrum efficiency in future\nsixth-generation (6G) wireless networks. However, in wideband scenarios,\nsynchronization errors caused by carrier frequency offset (CFO) can severely\nreduce both communication capacity and sensing accuracy. To address this\nchallenge, this paper integrates movable antennas (MAs) into the CF-DFRC\nframework, leveraging their spatial flexibility and adaptive beamforming to\ndynamically mitigate CFO-induced impairments. To fully exploit the advantages\nof MAs in wideband scenarios with CFO, we aim to maximize the worst-case\nsum-rate of communication and sensing by jointly optimizing MA positions,\n{beamforming}, and CFO parameters, subject to transmit power and MA positioning\nconstraints. Due to the non-convex nature of the problem, we propose a robust\nmeta reinforcement learning (MRL)-based two-stage alternating optimization\nstrategy. In the first stage, we employ manifold optimization (MO) with penalty\ndual decomposition (PDD) to solve the CFO-robust worst-case subproblem. In the\nsecond stage, we adopt to jointly optimize {the MA positions and beamforming\nvectors} in a data-driven manner {for dynamic wireless environments}.\nSimulation results show that the proposed MRL approach significantly\noutperforms conventional deep reinforcement learning (DRL) schemes in both\ncommunication and sensing performance under CFO impairments. Furthermore,\ncompared to fixed-position antennas (FPAs), the MA-aided CF-DFRC system\nexhibits", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16132v1", "cate": "eess.SP", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16227", "title": "Predictive Hydrodynamic Simulations for Laser Direct-drive Implosion Experiments via Artificial Intelligence", "authors": ["Zixu Wang", "Yuhan Wang", "Junfei Ma", "Fuyuan Wu", "Junchi Yan", "Xiaohui Yuan", "Zhe Zhang", "Jie Zhang"], "categories": ["physics.plasm-ph", "cs.AI"], "primary_category": "Subjects:       Plasma Physics (physics.plasm-ph)", "pdf_link": null, "comments": "Comments:      7 pages, 7 figures", "url": "http://arxiv.org/abs/2507.16227v1", "summary": "This work presents predictive hydrodynamic simulations empowered by\nartificial intelligence (AI) for laser driven implosion experiments, taking the\ndouble-cone ignition (DCI) scheme as an example. A Transformer-based deep\nlearning model MULTI-Net is established to predict implosion features according\nto laser waveforms and target radius. A Physics-Informed Decoder (PID) is\nproposed for high-dimensional sampling, significantly reducing the prediction\nerrors compared to Latin hypercube sampling. Applied to DCI experiments\nconducted on the SG-II Upgrade facility, the MULTI-Net model is able to predict\nthe implosion dynamics measured by the x-ray streak camera. It is found that an\neffective laser absorption factor about 65\\% is suitable for the\none-dimensional simulations of the DCI-R10 experiments. For shot 33, the mean\nimplosion velocity and collided plasma density reached 195 km/s and 117 g/cc,\nrespectively. This study demonstrates a data-driven AI framework that enhances\nthe prediction ability of simulations for complicated laser fusion experiments.", "comment": "7 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.16227v1", "cate": "physics.plasm-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15990", "title": "Generative AI Models for Learning Flow Maps of Stochastic Dynamical Systems in Bounded Domains", "authors": ["Minglei Yang", "Yanfang Liu", "Diego del-Castillo-Negrete", "Yanzhao Cao", "Guannan Zhang"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15990v1", "summary": "Simulating stochastic differential equations (SDEs) in bounded domains,\npresents significant computational challenges due to particle exit phenomena,\nwhich requires accurate modeling of interior stochastic dynamics and boundary\ninteractions. Despite the success of machine learning-based methods in learning\nSDEs, existing learning methods are not applicable to SDEs in bounded domains\nbecause they cannot accurately capture the particle exit dynamics. We present a\nunified hybrid data-driven approach that combines a conditional diffusion model\nwith an exit prediction neural network to capture both interior stochastic\ndynamics and boundary exit phenomena. Our ML model consists of two major\ncomponents: a neural network that learns exit probabilities using binary\ncross-entropy loss with rigorous convergence guarantees, and a training-free\ndiffusion model that generates state transitions for non-exiting particles\nusing closed-form score functions. The two components are integrated through a\nprobabilistic sampling algorithm that determines particle exit at each time\nstep and generates appropriate state transitions. The performance of the\nproposed approach is demonstrated via three test cases: a one-dimensional\nsimplified problem for theoretical verification, a two-dimensional\nadvection-diffusion problem in a bounded domain, and a three-dimensional\nproblem of interest to magnetically confined fusion plasmas.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15990v1", "cate": "stat.ML", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2507.16393", "title": "Are Foundation Models All You Need for Zero-shot Face Presentation Attack Detection?", "authors": ["Lazaro Janier Gonzalez-Sole", "Juan E. Tapia", "Christoph Busch"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at FG 2025", "url": "http://arxiv.org/abs/2507.16393v1", "summary": "Although face recognition systems have undergone an impressive evolution in\nthe last decade, these technologies are vulnerable to attack presentations\n(AP). These attacks are mostly easy to create and, by executing them against\nthe system's capture device, the malicious actor can impersonate an authorised\nsubject and thus gain access to the latter's information (e.g., financial\ntransactions). To protect facial recognition schemes against presentation\nattacks, state-of-the-art deep learning presentation attack detection (PAD)\napproaches require a large amount of data to produce reliable detection\nperformances and even then, they decrease their performance for unknown\npresentation attack instruments (PAI) or database (information not seen during\ntraining), i.e. they lack generalisability. To mitigate the above problems,\nthis paper focuses on zero-shot PAD. To do so, we first assess the\neffectiveness and generalisability of foundation models in established and\nchallenging experimental scenarios and then propose a simple but effective\nframework for zero-shot PAD. Experimental results show that these models are\nable to achieve performance in difficult scenarios with minimal effort of the\nmore advanced PAD mechanisms, whose weights were optimised mainly with training\nsets that included APs and bona fide presentations. The top-performing\nfoundation model outperforms by a margin the best from the state of the art\nobserved with the leaving-one-out protocol on the SiW-Mv2 database, which\ncontains challenging unknown 2D and 3D attacks", "comment": "Accepted at FG 2025", "pdf_url": "http://arxiv.org/pdf/2507.16393v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16311", "title": "Polarforming Design for Movable Antenna Systems", "authors": ["Zijian Zhou", "Jingze Ding", "Rui Zhang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      5 pages, 5 figures", "url": "http://arxiv.org/abs/2507.16311v1", "summary": "Polarforming has emerged as a promising technique to enable the antenna to\nshape its polarization into a desired state for aligning with that of the\nreceived electromagnetic (EM) wave or reconfiguring that of the transmitted EM\nwave. In this letter, we investigate polarforming design for the movable\nantenna (MA)-enabled communication system. Specifically, we consider a\nsingle-input single-output (SISO) system with reconfigurable antenna positions\nand polarizations to leverage both spatial and polarization degrees of freedom\n(DoFs). First, we present a polarized channel model and characterize the\nchannel response as a function of antenna positions and polarforming phase\nshifts. To maximize the achievable rate of the proposed system, we then develop\na successive convex approximation (SCA)-based optimization algorithm by\niteratively optimizing the antenna positions and phase shifts at both the\ntransmitter and receiver. Furthermore, simulation results demonstrate the\nperformance gains of the proposed system over conventional systems in\nmitigating channel depolarization and adapting to channel fading.", "comment": "5 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.16311v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16210", "title": "Joint Active and Passive Beamforming for Energy-Efficient STARS with Quantization and Element Selection in ISAC Systems", "authors": ["Li-Hsiang Shen", "Yi-Hsuan Chiu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16210v1", "summary": "This paper investigates a simultaneously transmitting and reflecting\nreconfigurable intelligent surface (STARS)-aided integrated sensing and\ncommunication (ISAC) systems in support of full-space energy-efficient data\ntransmissions and target sensing. We formulate an energy efficiency (EE)\nmaximization problem jointly optimizing dual-functional radar-communication\n(DFRC)-empowered base station (BS) ISAC beamforming and STARS configurations of\namplitudes, phase-shifts, quantization levels as well as element selection.\nFurthermore, relaxed/independent/coupled STARS are considered to examine\narchitectural flexibility. To tackle the non-convex and mixed-integer problem,\nwe propose a joint active-passive beamforming, quantization and element\nselection (AQUES) scheme based on alternating optimization: Lagrangian dual and\nDinkelbach's transformation deals with fractions, whereas successive convex\napproximation (SCA) convexifies the problem; Penalty dual decomposition (PDD)\nframework and penalty-based convex-concave programming (PCCP) procedure solves\namplitude and phase-shifts; Heuristic search decides the quantization level;\nInteger relaxation deals with the element selection. Simulation results\ndemonstrate that STARS-ISAC with the proposed AQUES scheme significantly\nenhances EE while meeting communication rates and sensing quality requirements.\nThe coupled STARS further highlights its superior EE performance over\nindependent and relaxed STARS thanks to its reduced hardware complexity.\nMoreover, AQUES outperforms existing configurations and benchmark methods in\nthe open literature across various network parameters and deployment scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16210v1", "cate": "eess.SP", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16252", "title": "Efficient RL for optimizing conversation level outcomes with an LLM-based tutor", "authors": ["Hyunji Nam", "Omer Gottesman", "Amy Zhang", "Dean Foster", "Emma Brunskill", "Lyle Ungar"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.16252v1", "summary": "Large language models (LLMs) built on existing reinforcement learning with\nhuman feedback (RLHF) frameworks typically optimize responses based on\nimmediate turn-level human preferences. However, this approach falls short in\nmulti-turn dialogue settings, such as online math tutoring. We propose a method\nto enhance LLM-based tutors by representing the dialogue history with a\nlower-dimensional latent state representation of a student and optimizing a\nlong-term policy to determine high-level actions based on the latent state. The\ngoal is to better align the tutor's behavior with the long-term objective of\nguiding the student towards solving a target math problem on their own. Our\nmodel is lightweight, requiring less computational resources than prior work of\ntraining the tutor policy end-to-end to directly output the tutor's next\nutterance. Our experiment results demonstrate that these modifications lead to\nimproved long-term outcomes compared to prompting in LLM-simulated tutoring\ntasks.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.16252v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16001", "title": "Automated Design of Structured Variational Quantum Circuits with Reinforcement Learning", "authors": ["Gloria Turati", "Simone Foder", "Riccardo Nembrini", "Maurizio Ferrari Dacrema", "Paolo Cremonesi"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16001v1", "summary": "Variational Quantum Algorithms (VQAs) are among the most promising approaches\nfor leveraging near-term quantum hardware, yet their effectiveness strongly\ndepends on the design of the underlying circuit ansatz, which is typically\nconstructed with heuristic methods. In this work, we represent the synthesis of\nvariational quantum circuits as a sequential decision-making problem, where\ngates are added iteratively in order to optimize an objective function, and we\nintroduce two reinforcement learning-based methods, RLVQC Global and RLVQC\nBlock, tailored to combinatorial optimization problems. RLVQC Block creates\nansatzes that generalize the Quantum Approximate Optimization Algorithm (QAOA),\nby discovering a two-qubits block that is applied to all the interacting qubit\npairs. While RLVQC Global further generalizes the ansatz and adds gates\nunconstrained by the structure of the interacting qubits. Both methods adopt\nthe Proximal Policy Optimization (PPO) algorithm and use empirical measurement\noutcomes as state observations to guide the agent. We evaluate the proposed\nmethods on a broad set of QUBO instances derived from classical graph-based\noptimization problems. Our results show that both RLVQC methods exhibit strong\nresults with RLVQC Block consistently outperforming QAOA and generally\nsurpassing RLVQC Global. While RLVQC Block produces circuits with depth\ncomparable to QAOA, the Global variant is instead able to find significantly\nshorter ones. These findings suggest that reinforcement learning methods can be\nan effective tool to discover new ansatz structures tailored for specific\nproblems and that the most effective circuit design strategy lies between rigid\npredefined architectures and completely unconstrained ones, offering a\nfavourable trade-off between structure and adaptability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16001v1", "cate": "quant-ph", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16397", "title": "ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement", "authors": ["Kahim Wong", "Jicheng Zhou", "Haiwei Wu", "Yain-Whar Si", "Jiantao Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16397v1", "summary": "The advancement of image editing tools has enabled malicious manipulation of\nsensitive document images, underscoring the need for robust document image\nforgery detection.Though forgery detectors for natural images have been\nextensively studied, they struggle with document images, as the tampered\nregions can be seamlessly blended into the uniform document background (BG) and\nstructured text. On the other hand, existing document-specific methods lack\nsufficient robustness against various degradations, which limits their\npractical deployment. This paper presents ADCD-Net, a robust document forgery\nlocalization model that adaptively leverages the RGB/DCT forensic traces and\nintegrates key characteristics of document images. Specifically, to address the\nDCT traces' sensitivity to block misalignment, we adaptively modulate the DCT\nfeature contribution based on a predicted alignment score, resulting in much\nimproved resilience to various distortions, including resizing and cropping.\nAlso, a hierarchical content disentanglement approach is proposed to boost the\nlocalization performance via mitigating the text-BG disparities. Furthermore,\nnoticing the predominantly pristine nature of BG regions, we construct a\npristine prototype capturing traces of untampered regions, and eventually\nenhance both the localization accuracy and robustness. Our proposed ADCD-Net\ndemonstrates superior forgery localization performance, consistently\noutperforming state-of-the-art methods by 20.79\\% averaged over 5 types of\ndistortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16397v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16426", "title": "Derivative-Agnostic Inference of Nonlinear Hybrid Systems", "authors": ["Hengzhi Yu", "Bohan Ma", "Mingshuai Chen", "Jie An", "Bin Gu", "Naijun Zhan", "Jianwei Yin"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16426v1", "summary": "This paper addresses the problem of inferring a hybrid automaton from a set\nof input-output traces of a hybrid system exhibiting discrete mode switching\nbetween continuously evolving dynamics. Existing approaches mainly adopt a\nderivative-based method where (i) the occurrence of mode switching is\ndetermined by a drastic variation in derivatives and (ii) the clustering of\ntrace segments relies on signal similarity -- both subject to user-supplied\nthresholds. We present a derivative-agnostic approach, named Dainarx, to infer\nnonlinear hybrid systems where the dynamics are captured by nonlinear\nautoregressive exogenous (NARX) models. Dainarx employs NARX models as a\nunified, threshold-free representation through the detection of mode switching\nand trace-segment clustering. We show that Dainarx suffices to learn models\nthat closely approximate a general class of hybrid systems featuring high-order\nnonlinear dynamics with exogenous inputs, nonlinear guard conditions, and\nlinear resets. Experimental results on a collection of benchmarks indicate that\nour approach can effectively and efficiently infer nontrivial hybrid automata\nwith high-order dynamics yielding significantly more accurate approximations\nthan state-of-the-art techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16426v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16211", "title": "Liquid Intelligent Metasurface for Fluid Antennas-Assisted Networks", "authors": ["Li-Hsiang Shen"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16211v1", "summary": "This paper proposes a novel liquid intelligent metasurface (LIM)-assisted\ndownlink multi-user multiple-input single-output (MISO) system, wherein both\nthe base station (BS) and the metasurface are respectively equipped with fluid\nantennas (FA) and liquid elements. Unlike conventional reconfigurable\nmetasurface-assisted systems with static geometries, the proposed architecture\nenables joint electromagnetic and spatial reconfigurability by allowing both\nthe FA-empowered BS (FAS) and LIM to dynamically adjust their small-scale\npositions in addition to beamforming and phase-shift controls. We formulate a\nsum-rate maximization problem that jointly optimizes the BS beamforming, LIM\nphase-shifts, and the positions of fluid antennas and liquid elements. The\nproblem is highly non-convex due to coupling between variables, fractional\nexpressions, unit-modulus constraints as well as spatial correlation functions.\nTo address these challenges, we adopt alternating optimization and introduce\nauxiliary variables and employ successive convex approximation (SCA) as well as\nthe penalty convex-concave procedure (PCCP) to solve the respective\nsubproblems. Simulation results have demonstrated that the proposed FAS-LIM\narchitecture significantly outperforms benchmark methods employing conventional\nfixed metasurface and fixed antenna arrays in terms of various parameter\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16211v1", "cate": "eess.SP", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16122", "title": "MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for Efficient 3D Medical Image Segmentation", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "Willium WC Chen", "KC"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16122v1", "summary": "Accurate and efficient medical image segmentation is crucial but challenging\ndue to anatomical variability and high computational demands on volumetric\ndata. Recent hybrid CNN-Transformer architectures achieve state-of-the-art\nresults but add significant complexity. In this paper, we propose MLRU++, a\nMultiscale Lightweight Residual UNETR++ architecture designed to balance\nsegmentation accuracy and computational efficiency. It introduces two key\ninnovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that\nenhances contextual feature encoding with minimal overhead, and a Multiscale\nBottleneck Block (M2B) in the decoder that captures fine-grained details via\nmulti-resolution feature aggregation. Experiments on four publicly available\nbenchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that\nMLRU++ achieves state-of-the-art performance, with average Dice scores of\n87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing\nleading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and\nACDC, respectively, while significantly reducing parameter count and\ncomputational cost. Ablation studies evaluating LCBAM and M2B further confirm\nthe effectiveness of the proposed architectural components. Results suggest\nthat MLRU++ offers a practical and high-performing solution for 3D medical\nimage segmentation tasks. Source code is available at:\nhttps://github.com/1027865/MLRUPP", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16122v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16267", "title": "SFNet: A Spatio-Frequency Domain Deep Learning Network for Efficient Alzheimer's Disease Diagnosis", "authors": ["Xinyue Yang", "Meiliang Liu", "Yunfang Xu", "Xiaoxiao Yang", "Zhengye Si", "Zijin Li", "Zhiwen Zhao"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16267v1", "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\npredominantly affects the elderly population and currently has no cure.\nMagnetic Resonance Imaging (MRI), as a non-invasive imaging technique, is\nessential for the early diagnosis of AD. MRI inherently contains both spatial\nand frequency information, as raw signals are acquired in the frequency domain\nand reconstructed into spatial images via the Fourier transform. However, most\nexisting AD diagnostic models extract features from a single domain, limiting\ntheir capacity to fully capture the complex neuroimaging characteristics of the\ndisease. While some studies have combined spatial and frequency information,\nthey are mostly confined to 2D MRI, leaving the potential of dual-domain\nanalysis in 3D MRI unexplored. To overcome this limitation, we propose\nSpatio-Frequency Network (SFNet), the first end-to-end deep learning framework\nthat simultaneously leverages spatial and frequency domain information to\nenhance 3D MRI-based AD diagnosis. SFNet integrates an enhanced dense\nconvolutional network to extract local spatial features and a global frequency\nmodule to capture global frequency-domain representations. Additionally, a\nnovel multi-scale attention module is proposed to further refine spatial\nfeature extraction. Experiments on the Alzheimer's Disease Neuroimaging\nInitiative (ANDI) dataset demonstrate that SFNet outperforms existing baselines\nand reduces computational overhead in classifying cognitively normal (CN) and\nAD, achieving an accuracy of 95.1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16267v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16003", "title": "Learning without training: The implicit dynamics of in-context learning", "authors": ["Benoit Dherin", "Michael Munn", "Hanna Mazzawi", "Michael Wunder", "Javier Gonzalvo"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16003v1", "summary": "One of the most striking features of Large Language Models (LLM) is their\nability to learn in context. Namely at inference time an LLM is able to learn\nnew patterns without any additional weight update when these patterns are\npresented in the form of examples in the prompt, even if these patterns were\nnot seen during training. The mechanisms through which this can happen are\nstill largely unknown. In this work, we show that the stacking of a\nself-attention layer with an MLP, allows the transformer block to implicitly\nmodify the weights of the MLP layer according to the context. We argue through\ntheory and experimentation that this simple mechanism may be the reason why\nLLMs can learn in context and not only during training. Specifically, we show\nunder mild simplifying assumptions how a transformer block implicitly\ntransforms a context into a low-rank weight-update of the MLP layer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16003v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16403", "title": "ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for Visual Question Answering", "authors": ["Thuy-Duong Tran", "Trung-Kien Tran", "Manfred Hauswirth", "Danh Le Phuoc"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the IEEE/CVF International Conference on Computer Vision (ICCV) 2025", "url": "http://arxiv.org/abs/2507.16403v1", "summary": "In this paper, we propose a new dataset, ReasonVQA, for the Visual Question\nAnswering (VQA) task. Our dataset is automatically integrated with structured\nencyclopedic knowledge and constructed using a low-cost framework, which is\ncapable of generating complex, multi-hop questions. We evaluated\nstate-of-the-art VQA models on ReasonVQA, and the empirical results demonstrate\nthat ReasonVQA poses significant challenges to these models, highlighting its\npotential for benchmarking and advancing the field of VQA. Additionally, our\ndataset can be easily scaled with respect to input images; the current version\nsurpasses the largest existing datasets requiring external knowledge by more\nthan an order of magnitude.", "comment": "Accepted at the IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2507.16403v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16479", "title": "Arbitrage Tactics in the Local Markets via Hierarchical Multi-agent Reinforcement Learning", "authors": ["Haoyang Zhang", "Mina Montazeri", "Philipp Heer", "Koen Kok", "Nikolaos G. Paterakis"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16479v1", "summary": "Strategic bidding tactics employed by prosumers in local markets, including\nthe Local Electricity Market (LEM) and Local Flexibility Market (LFM), have\nattracted significant attention due to their potential to enhance economic\nbenefits for market participants through optimized energy management and\nbidding. While existing research has explored strategic bidding in a single\nmarket with multi-agent reinforcement learning (MARL) algorithms, arbitrage\nopportunities across local markets remain unexplored. This paper introduces a\nhierarchical MARL (HMARL) algorithm designed to enable aggregator arbitrage\nacross multiple local markets. The strategic behavior of these aggregators in\nlocal markets is modeled as a two-stage Markov game: the first stage involves\nthe LEM, while the second stage encompasses both the LFM and the balancing\nmarket. To solve this two-stage Markov game, the HMARL framework assigns two\nsub-agents to each aggregator, a primary sub-agent and a secondary sub-agent.\nWithout the arbitrage strategy, these sub-agents operate in silos, with the\nprimary sub-agent focusing on first-stage profits and the secondary sub-agent\non second-stage profits, each employing independent MARLs. On the contrary,\nwhen implementing the arbitrage strategy with the proposed HMARL, the\nsub-agents communicate and coordinate to perform arbitrage across multiple\nlocal markets, enhancing overall efficiency. The case study, conducted under a\nscenario where all aggregators employ the arbitrage strategy, shows that\ndespite higher initial costs in the LEM, this strategy generates substantial\nsavings in the LFM and the balancing market, resulting in a total profit\nincrease of $40.6\\%$ on average. This highlights the capability of the proposed\nHMARL to address the two-stage Markov game and facilitate arbitrage across\nlocal markets, thereby enhancing profitability for participants.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16479v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16375", "title": "Latency Minimization Oriented Radio and Computation Resource Allocations for 6G V2X Networks with ISCC", "authors": ["Peng Liu", "Xinyi Wang", "Zesong Fei", "Yuan Wu", "Jie Xu", "Arumugam Nallanathan"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      15 pages, 13 figures", "url": "http://arxiv.org/abs/2507.16375v1", "summary": "Incorporating mobile edge computing (MEC) and integrated sensing and\ncommunication (ISAC) has emerged as a promising technology to enable integrated\nsensing, communication, and computing (ISCC) in the sixth generation (6G)\nnetworks. ISCC is particularly attractive for vehicle-to-everything (V2X)\napplications, where vehicles perform ISAC to sense the environment and\nsimultaneously offload the sensing data to roadside base stations (BSs) for\nremote processing. In this paper, we investigate a particular ISCC-enabled V2X\nsystem consisting of multiple multi-antenna BSs serving a set of single-antenna\nvehicles, in which the vehicles perform their respective ISAC operations (for\nsimultaneous sensing and offloading to the associated BS) over orthogonal\nsub-bands. With the focus on fairly minimizing the sensing completion latency\nfor vehicles while ensuring the detection probability constraints, we jointly\noptimize the allocations of radio resources (i.e., the sub-band allocation,\ntransmit power control at vehicles, and receive beamforming at BSs) as well as\ncomputation resources at BS MEC servers. To solve the formulated complex\nmixed-integer nonlinear programming (MINLP) problem, we propose an alternating\noptimization algorithm. In this algorithm, we determine the sub-band allocation\nvia the branch-and-bound method, optimize the transmit power control via\nsuccessive convex approximation (SCA), and derive the receive beamforming and\ncomputation resource allocation at BSs in closed form based on generalized\nRayleigh entropy and fairness criteria, respectively. Simulation results\ndemonstrate that the proposed joint resource allocation design significantly\nreduces the maximum task completion latency among all vehicles. Furthermore, we\nalso demonstrate several interesting trade-offs between the system performance\nand resource utilizations.", "comment": "15 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.16375v1", "cate": "eess.SP", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16321", "title": "Physics-Driven Neural Network for Solving Electromagnetic Inverse Scattering Problems", "authors": ["Yutong Du", "Zicheng Liu", "Bazargul Matkerim", "Changyou Li", "Yali Zong", "Bo Qi", "Jingwei Kou"], "categories": ["eess.IV", "cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16321v1", "summary": "In recent years, deep learning-based methods have been proposed for solving\ninverse scattering problems (ISPs), but most of them heavily rely on data and\nsuffer from limited generalization capabilities. In this paper, a new solving\nscheme is proposed where the solution is iteratively updated following the\nupdating of the physics-driven neural network (PDNN), the hyperparameters of\nwhich are optimized by minimizing the loss function which incorporates the\nconstraints from the collected scattered fields and the prior information about\nscatterers. Unlike data-driven neural network solvers, PDNN is trained only\nrequiring the input of collected scattered fields and the computation of\nscattered fields corresponding to predicted solutions, thus avoids the\ngeneralization problem. Moreover, to accelerate the imaging efficiency, the\nsubregion enclosing the scatterers is identified. Numerical and experimental\nresults demonstrate that the proposed scheme has high reconstruction accuracy\nand strong stability, even when dealing with composite lossy scatterers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16321v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16343", "title": "Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal Queries", "authors": ["Pengfei Cai", "Yan Song", "Qing Gu", "Nan Jiang", "Haoyu Song", "Ian McLoughlin"], "categories": ["cs.SD", "cs.AI"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by MM 2025", "url": "http://arxiv.org/abs/2507.16343v1", "summary": "Most existing sound event detection~(SED) algorithms operate under a\nclosed-set assumption, restricting their detection capabilities to predefined\nclasses. While recent efforts have explored language-driven zero-shot SED by\nexploiting audio-language models, their performance is still far from\nsatisfactory due to the lack of fine-grained alignment and cross-modal feature\nfusion. In this work, we propose the Detect Any Sound Model (DASM), a\nquery-based framework for open-vocabulary SED guided by multi-modal queries.\nDASM formulates SED as a frame-level retrieval task, where audio features are\nmatched against query vectors derived from text or audio prompts. To support\nthis formulation, DASM introduces a dual-stream decoder that explicitly\ndecouples event recognition and temporal localization: a cross-modality event\ndecoder performs query-feature fusion and determines the presence of sound\nevents at the clip-level, while a context network models temporal dependencies\nfor frame-level localization. Additionally, an inference-time attention masking\nstrategy is proposed to leverage semantic relations between base and novel\nclasses, substantially enhancing generalization to novel classes. Experiments\non the AudioSet Strong dataset demonstrate that DASM effectively balances\nlocalization accuracy with generalization to novel classes, outperforming\nCLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in\nthe closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot\nevaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the\nsupervised CRNN baseline. The project page is available at\nhttps://cai525.github.io/Transformer4SED/demo_page/DASM/.", "comment": "Accepted by MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.16343v1", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16004", "title": "Minor Embedding for Quantum Annealing with Reinforcement Learning", "authors": ["Riccardo Nembrini", "Maurizio Ferrari Dacrema", "Paolo Cremonesi"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16004v1", "summary": "Quantum Annealing (QA) is a quantum computing paradigm for solving\ncombinatorial optimization problems formulated as Quadratic Unconstrained\nBinary Optimization (QUBO) problems. An essential step in QA is minor\nembedding, which maps the problem graph onto the sparse topology of the quantum\nprocessor. This process is computationally expensive and scales poorly with\nincreasing problem size and hardware complexity. Existing heuristics are often\ndeveloped for specific problem graphs or hardware topologies and are difficult\nto generalize. Reinforcement Learning (RL) offers a promising alternative by\ntreating minor embedding as a sequential decision-making problem, where an\nagent learns to construct minor embeddings by iteratively mapping the problem\nvariables to the hardware qubits. We propose a RL-based approach to minor\nembedding using a Proximal Policy Optimization agent, testing its ability to\nembed both fully connected and randomly generated problem graphs on two\nhardware topologies, Chimera and Zephyr. The results show that our agent\nconsistently produces valid minor embeddings, with reasonably efficient number\nof qubits, in particular on the more modern Zephyr topology. Our proposed\napproach is also able to scale to moderate problem sizes and adapts well to\ndifferent graph structures, highlighting RL's potential as a flexible and\ngeneral-purpose framework for minor embedding in QA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16004v1", "cate": "quant-ph", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16406", "title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges", "authors": ["Tanveer Younis", "Zhanglin Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      30 pages, 6 figures", "url": "http://arxiv.org/abs/2507.16406v1", "summary": "Sparse-view 3D reconstruction is essential for applications in which dense\nimage acquisition is impractical, such as robotics, augmented/virtual reality\n(AR/VR), and autonomous systems. In these settings, minimal image overlap\nprevents reliable correspondence matching, causing traditional methods, such as\nstructure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey\nreviews the latest advances in neural implicit models (e.g., NeRF and its\nregularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian\nSplatting), and hybrid frameworks that leverage priors from diffusion and\nvision foundation models (VFMs).We analyze how geometric regularization,\nexplicit shape modeling, and generative inference are used to mitigate\nartifacts such as floaters and pose ambiguities in sparse-view settings.\nComparative results on standard benchmarks reveal key trade-offs between the\nreconstruction accuracy, efficiency, and generalization. Unlike previous\nreviews, our survey provides a unified perspective on geometry-based, neural\nimplicit, and generative (diffusion-based) methods. We highlight the persistent\nchallenges in domain generalization and pose-free reconstruction and outline\nfuture directions for developing 3D-native generative priors and achieving\nreal-time, unconstrained sparse-view reconstruction.", "comment": "30 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.16406v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16513", "title": "Graphical Analysis of Nonlinear Multivariable Feedback Systems", "authors": ["Julius P. J. Krebbekx", "Roland Tth", "Amritam Das"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE-TAC on 22-07-2025", "url": "http://arxiv.org/abs/2507.16513v1", "summary": "Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain\nmethod for the analysis of nonlinear systems. There have been recent efforts to\ngeneralize SRG analysis to Multiple-Input Multiple-Output (MIMO) systems.\nHowever, these attempts yielded only results for square systems, and in some\ncases, only methods applicable for Linear Time-Invariant (LTI) systems. In this\npaper, we develop a complete SRG framework for the analysis of MIMO systems,\nwhich may be nonlinear and non-square. The key element is the embedding of\noperators to a space of operators acting on a common Hilbert space, while\nrestricting the input space to the original input dimension. We develop\ninterconnection rules that use restricted input spaces and stability theorems\nto guarantee causality, well-posedness and (incremental) $L_2$-gain bounds for\nthe overall interconnection. We show utilization of the proposed theoretical\nconcepts on the analysis of nonlinear systems in a linear fractional\nrepresentation form, which is a rather general class of systems with a\nrepresentation form directly utilizable for control. Moreover, we provide\nformulas for the computation of MIMO SRGs of stable LTI operators and diagonal\nstatic nonlinear operators. Finally, we demonstrate the capabilities of our\nproposed approach on several examples.", "comment": "Submitted to IEEE-TAC on 22-07-2025", "pdf_url": "http://arxiv.org/pdf/2507.16513v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16550", "title": "Hybrid RISs for Simultaneous Tunable Reflections and Sensing", "authors": ["George C. Alexandropoulos", "Nir Shlezinger", "Ioannis Gavras", "Haiyang Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16550v1", "summary": "The concept of smart wireless environments envisions dynamic programmable\npropagation of information-bearing signals through the deployment of\nReconfigurable Intelligent Surfaces (RISs). Typical RIS implementations include\nmetasurfaces with passive unit elements capable to reflect their incident waves\nin controllable ways. However, this solely reflective operation induces\nsignificant challenges in the RIS orchestration from the wireless network. For\nexample, channel estimation, which is essential for coherent RIS-empowered\nwireless communications, is quite challenging with the available solely\nreflecting RIS designs. This chapter reviews the emerging concept of Hybrid\nReflecting and Sensing RISs (HRISs), which enables metasurfaces to reflect the\nimpinging signal in a controllable manner, while simultaneously sensing a\nportion of it. The sensing capability of HRISs facilitates various network\nmanagement functionalities, including channel parameter estimation and\nlocalization, while, most importantly, giving rise to computationally\nautonomous and self-configuring RISs. The implementation details of HRISs are\nfirst presented, which are then followed by a convenient mathematical model for\ncharacterizing their dual functionality. Then, two indicative applications of\nHRISs are discussed, one for simultaneous communications and sensing and\nanother that showcases their usefulness for estimating the individual channels\nin the uplink of a multi-user HRIS-empowered communication system. For both of\nthese applications, performance evaluation results are included validating the\nrole of HRISs for sensing as well as integrated sensing and communications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16550v1", "cate": "eess.SP", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16360", "title": "A High Magnifications Histopathology Image Dataset for Oral Squamous Cell Carcinoma Diagnosis and Prognosis", "authors": ["Jinquan Guan", "Junhong Guo", "Qi Chen", "Jian Chen", "Yongkang Cai", "Yilin He", "Zhiquan Huang", "Yan Wang", "Yutong Xie"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      12 pages, 11 tables, 4 figures", "url": "http://arxiv.org/abs/2507.16360v1", "summary": "Oral Squamous Cell Carcinoma (OSCC) is a prevalent and aggressive malignancy\nwhere deep learning-based computer-aided diagnosis and prognosis can enhance\nclinical assessments.However, existing publicly available OSCC datasets often\nsuffer from limited patient cohorts and a restricted focus on either diagnostic\nor prognostic tasks, limiting the development of comprehensive and\ngeneralizable models. To bridge this gap, we introduce Multi-OSCC, a new\nhistopathology image dataset comprising 1,325 OSCC patients, integrating both\ndiagnostic and prognostic information to expand existing public resources. Each\npatient is represented by six high resolution histopathology images captured at\nx200, x400, and x1000 magnifications-two per magnification-covering both the\ncore and edge tumor regions.The Multi-OSCC dataset is richly annotated for six\ncritical clinical tasks: recurrence prediction (REC), lymph node metastasis\n(LNM), tumor differentiation (TD), tumor invasion (TI), cancer embolus (CE),\nand perineural invasion (PI). To benchmark this dataset, we systematically\nevaluate the impact of different visual encoders, multi-image fusion\ntechniques, stain normalization, and multi-task learning frameworks. Our\nanalysis yields several key insights: (1) The top-performing models achieve\nexcellent results, with an Area Under the Curve (AUC) of 94.72% for REC and\n81.23% for TD, while all tasks surpass 70% AUC; (2) Stain normalization\nbenefits diagnostic tasks but negatively affects recurrence prediction; (3)\nMulti-task learning incurs a 3.34% average AUC degradation compared to\nsingle-task models in our multi-task benchmark, underscoring the challenge of\nbalancing multiple tasks in our dataset. To accelerate future research, we\npublicly release the Multi-OSCC dataset and baseline models at\nhttps://github.com/guanjinquan/OSCC-PathologyImageDataset.", "comment": "12 pages, 11 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.16360v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16467", "title": "Estimating Treatment Effects with Independent Component Analysis", "authors": ["Patrik Reizinger", "Lester Mackey", "Wieland Brendel", "Rahul Krishnan"], "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16467v1", "summary": "The field of causal inference has developed a variety of methods to\naccurately estimate treatment effects in the presence of nuisance. Meanwhile,\nthe field of identifiability theory has developed methods like Independent\nComponent Analysis (ICA) to identify latent sources and mixing weights from\ndata. While these two research communities have developed largely\nindependently, they aim to achieve similar goals: the accurate and\nsample-efficient estimation of model parameters. In the partially linear\nregression (PLR) setting, Mackey et al. (2018) recently found that estimation\nconsistency can be improved with non-Gaussian treatment noise. Non-Gaussianity\nis also a crucial assumption for identifying latent factors in ICA. We provide\nthe first theoretical and empirical insights into this connection, showing that\nICA can be used for causal effect estimation in the PLR model. Surprisingly, we\nfind that linear ICA can accurately estimate multiple treatment effects even in\nthe presence of Gaussian confounders or nonlinear nuisance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16467v1", "cate": "stat.ML", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16041", "title": "Radiological and Biological Dictionary of Radiomics Features: Addressing Understandable AI Issues in Personalized Breast Cancer; Dictionary Version BM1.0", "authors": ["Arman Gorji", "Nima Sanati", "Amir Hossein Pouria", "Somayeh Sadat Mehrnia", "Ilker Hacihaliloglu", "Arman Rahmim", "Mohammad R. Salmanpour"], "categories": ["physics.comp-ph", "cs.LG", "F.2.2, I.2.7"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16041v1", "summary": "Radiomics-based AI models show promise for breast cancer diagnosis but often\nlack interpretability, limiting clinical adoption. This study addresses the gap\nbetween radiomic features (RF) and the standardized BI-RADS lexicon by\nproposing a dual-dictionary framework. First, a Clinically-Informed Feature\nInterpretation Dictionary (CIFID) was created by mapping 56 RFs to BI-RADS\ndescriptors (shape, margin, internal enhancement) through literature and expert\nreview. The framework was applied to classify triple-negative breast cancer\n(TNBC) versus non-TNBC using dynamic contrast-enhanced MRI from a\nmulti-institutional cohort of 1,549 patients. We trained 27 machine learning\nclassifiers with 27 feature selection methods. SHapley Additive exPlanations\n(SHAP) were used to interpret predictions and generate a complementary\nData-Driven Feature Interpretation Dictionary (DDFID) for 52 additional RFs.\nThe best model, combining Variance Inflation Factor (VIF) selection with Extra\nTrees Classifier, achieved an average cross-validation accuracy of 0.83. Key\npredictive RFs aligned with clinical knowledge: higher Sphericity (round/oval\nshape) and lower Busyness (more homogeneous enhancement) were associated with\nTNBC. The framework confirmed known imaging biomarkers and uncovered novel,\ninterpretable associations. This dual-dictionary approach (BM1.0) enhances AI\nmodel transparency and supports the integration of RFs into routine breast\ncancer diagnosis and personalized care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16041v1", "cate": "physics.comp-ph", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16427", "title": "Combined Image Data Augmentations diminish the benefits of Adaptive Label Smoothing", "authors": ["Georg Siedel", "Ekagra Gupta", "Weijia Shao", "Silvia Vock", "Andrey Morozov"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint submitted to the Fast Review Track of DAGM German Conference on Pattern Recognition (GCPR) 2025", "url": "http://arxiv.org/abs/2507.16427v1", "summary": "Soft augmentation regularizes the supervised learning process of image\nclassifiers by reducing label confidence of a training sample based on the\nmagnitude of random-crop augmentation applied to it. This paper extends this\nadaptive label smoothing framework to other types of aggressive augmentations\nbeyond random-crop. Specifically, we demonstrate the effectiveness of the\nmethod for random erasing and noise injection data augmentation. Adaptive label\nsmoothing permits stronger regularization via higher-intensity Random Erasing.\nHowever, its benefits vanish when applied with a diverse range of image\ntransformations as in the state-of-the-art TrivialAugment method, and excessive\nlabel smoothing harms robustness to common corruptions. Our findings suggest\nthat adaptive label smoothing should only be applied when the training data\ndistribution is dominated by a limited, homogeneous set of image transformation\ntypes.", "comment": "Preprint submitted to the Fast Review Track of DAGM German Conference\n  on Pattern Recognition (GCPR) 2025", "pdf_url": "http://arxiv.org/pdf/2507.16427v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16520", "title": "A Distributed Actor-Critic Algorithm for Fixed-Time Consensus in Nonlinear Multi-Agent Systems", "authors": ["Aria Delshad", "Maryam Babazadeh"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16520v1", "summary": "This paper proposes a reinforcement learning (RL)-based backstepping control\nstrategy to achieve fixed time consensus in nonlinear multi-agent systems with\nstrict feedback dynamics. Agents exchange only output information with their\nneighbors over a directed communication graph, without requiring full state\nmeasurements or symmetric communication. Achieving fixed time consensus, where\nconvergence occurs within a pre-specified time bound that is independent of\ninitial conditions is faced with significant challenges due to the presence of\nunknown nonlinearities, inter-agent couplings, and external disturbances. This\nwork addresses these challenges by integrating actor critic reinforcement\nlearning with a novel fixed time adaptation mechanism. Each agent employs an\nactor critic architecture supported by two estimator networks designed to\nhandle system uncertainties and unknown perturbations. The adaptation laws are\ndeveloped to ensure that all agents track the leader within a fixed time\nregardless of their initial conditions. The consensus and tracking errors are\nguaranteed to converge to a small neighborhood of the origin, with the\nconvergence radius adjustable through control parameters. Simulation results\ndemonstrate the effectiveness of the proposed approach and highlight its\nadvantages over state-of-the-art methods in terms of convergence speed and\nrobustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16520v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16733", "title": "Generative Diffusion Models for Wireless Networks: Fundamental, Architecture, and State-of-the-Art", "authors": ["Dayu Fan", "Rui Meng", "Xiaodong Xu", "Yiming Liu", "Guoshun Nan", "Chenyuan Feng", "Shujun Han", "Song Gao", "Bingxuan Xu", "Dusit Niyato", "Tony Q. S. Quek", "Ping Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      30 pages, 11 figures", "url": "http://arxiv.org/abs/2507.16733v1", "summary": "With the rapid development of Generative Artificial Intelligence (GAI)\ntechnology, Generative Diffusion Models (GDMs) have shown significant\nempowerment potential in the field of wireless networks due to advantages, such\nas noise resistance, training stability, controllability, and multimodal\ngeneration. Although there have been multiple studies focusing on GDMs for\nwireless networks, there is still a lack of comprehensive reviews on their\ntechnological evolution. Motivated by this, we systematically explore the\napplication of GDMs in wireless networks. Firstly, starting from mathematical\nprinciples, we analyze technical advantages of GDMs and present six\nrepresentative models. Furthermore, we propose the multi-layer wireless network\narchitecture including sensing layer, transmission layer, application layer,\nand security plane. We also introduce the core mechanisms of GDM at each of the\nlayers. Subsequently, we conduct a rigorous review on existing GDM-based\nschemes, with a focus on analyzing their innovative points, the role of GDMs,\nstrengths, and weaknesses. Ultimately, we extract key challenges and provide\npotential solutions, with the aim of providing directional guidance for future\nresearch in this field.", "comment": "30 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.16733v1", "cate": "eess.SP", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16573", "title": "Semantic Segmentation for Preoperative Planning in Transcatheter Aortic Valve Replacement", "authors": ["Cedric Zllner", "Simon Rei", "Alexander Jaus", "Amroalalaa Sholi", "Ralf Sodian", "Rainer Stiefelhagen"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at 16th MICCAI Workshop on Statistical Atlases and Computational Modeling of the Heart (STACOM)", "url": "http://arxiv.org/abs/2507.16573v1", "summary": "When preoperative planning for surgeries is conducted on the basis of medical\nimages, artificial intelligence methods can support medical doctors during\nassessment. In this work, we consider medical guidelines for preoperative\nplanning of the transcatheter aortic valve replacement (TAVR) and identify\ntasks, that may be supported via semantic segmentation models by making\nrelevant anatomical structures measurable in computed tomography scans. We\nfirst derive fine-grained TAVR-relevant pseudo-labels from coarse-grained\nanatomical information, in order to train segmentation models and quantify how\nwell they are able to find these structures in the scans. Furthermore, we\npropose an adaptation to the loss function in training these segmentation\nmodels and through this achieve a +1.27% Dice increase in performance. Our\nfine-grained TAVR-relevant pseudo-labels and the computed tomography scans we\nbuild upon are available at https://doi.org/10.5281/zenodo.16274176.", "comment": "Accepted at 16th MICCAI Workshop on Statistical Atlases and\n  Computational Modeling of the Heart (STACOM)", "pdf_url": "http://arxiv.org/pdf/2507.16573v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16104", "title": "Distributed Asynchronous Device Speech Enhancement via Windowed Cross-Attention", "authors": ["Gene-Ping Yang", "Sebastian Braun"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16104v1", "summary": "The increasing number of microphone-equipped personal devices offers great\nflexibility and potential using them as ad-hoc microphone arrays in dynamic\nmeeting environments. However, most existing approaches are designed for\ntime-synchronized microphone setups, a condition that may not hold in\nreal-world meeting scenarios, where time latency and clock drift vary across\ndevices. Under such conditions, we found transform-average-concatenate (TAC), a\npopular module for neural multi-microphone processing, insufficient in handling\ntime-asynchronous microphones. In response, we propose a windowed\ncross-attention module capable of dynamically aligning features between all\nmicrophones. This module is invariant to both the permutation and the number of\nmicrophones and can be easily integrated into existing models. Furthermore, we\npropose an optimal training target for multi-talker environments. We evaluated\nour approach in a multi-microphone noisy reverberant setup with unknown time\nlatency and clock drift of each microphone. Experimental results show that our\nmethod outperforms TAC on both iFaSNet and CRUSE models, offering faster\nconvergence and improved learning, demonstrating the efficacy of the windowed\ncross-attention module for asynchronous microphone setups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16104v1", "cate": "eess.AS", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16488", "title": "ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs", "authors": ["Zhenliang Zhang", "Xinyu Hu", "Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025 (Main Conference)", "url": "http://arxiv.org/abs/2507.16488v1", "summary": "Large language models (LLMs) excel at various natural language processing\ntasks, but their tendency to generate hallucinations undermines their\nreliability. Existing hallucination detection methods leveraging hidden states\npredominantly focus on static and isolated representations, overlooking their\ndynamic evolution across layers, which limits efficacy. To address this\nlimitation, we shift the focus to the hidden state update process and introduce\na novel metric, the ICR Score (Information Contribution to Residual Stream),\nwhich quantifies the contribution of modules to the hidden states' update. We\nempirically validate that the ICR Score is effective and reliable in\ndistinguishing hallucinations. Building on these insights, we propose a\nhallucination detection method, the ICR Probe, which captures the cross-layer\nevolution of hidden states. Experimental results show that the ICR Probe\nachieves superior performance with significantly fewer parameters. Furthermore,\nablation studies and case analyses offer deeper insights into the underlying\nmechanism of this method, improving its interpretability.", "comment": "Accepted to ACL 2025 (Main Conference)", "pdf_url": "http://arxiv.org/pdf/2507.16488v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16058", "title": "Is memory all you need? Data-driven Mori-Zwanzig modeling of Lagrangian particle dynamics in turbulent flows", "authors": ["Xander de Wit", "Alessandro Gabbana", "Michael Woodward", "Yen Ting Lin", "Federico Toschi", "Daniel Livescu"], "categories": ["physics.flu-dyn", "cs.LG", "nlin.CD"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16058v1", "summary": "The dynamics of Lagrangian particles in turbulence play a crucial role in\nmixing, transport, and dispersion processes in complex flows. Their\ntrajectories exhibit highly non-trivial statistical behavior, motivating the\ndevelopment of surrogate models that can reproduce these trajectories without\nincurring the high computational cost of direct numerical simulations of the\nfull Eulerian field. This task is particularly challenging because\nreduced-order models typically lack access to the full set of interactions with\nthe underlying turbulent field. Novel data-driven machine learning techniques\ncan be very powerful in capturing and reproducing complex statistics of the\nreduced-order/surrogate dynamics. In this work, we show how one can learn a\nsurrogate dynamical system that is able to evolve a turbulent Lagrangian\ntrajectory in a way that is point-wise accurate for short-time predictions\n(with respect to Kolmogorov time) and stable and statistically accurate at long\ntimes. This approach is based on the Mori--Zwanzig formalism, which prescribes\na mathematical decomposition of the full dynamical system into resolved\ndynamics that depend on the current state and the past history of a reduced set\nof observables and the unresolved orthogonal dynamics due to unresolved degrees\nof freedom of the initial state. We show how by training this reduced order\nmodel on a point-wise error metric on short time-prediction, we are able to\ncorrectly learn the dynamics of the Lagrangian turbulence, such that also the\nlong-time statistical behavior is stably recovered at test time. This opens up\na range of new applications, for example, for the control of active Lagrangian\nagents in turbulence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16058v1", "cate": "physics.flu-dyn", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16429", "title": "Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image Segmentation Using Diffusion Model", "authors": ["Lin Xi", "Yingliang Ma", "Cheng Wang", "Sandra Howell", "Aldo Rinaldi", "Kawal S. Rhode"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16429v1", "summary": "Obtaining pixel-level annotations in the medical domain is both expensive and\ntime-consuming, often requiring close collaboration between clinical experts\nand developers. Semi-supervised medical image segmentation aims to leverage\nlimited annotated data alongside abundant unlabeled data to achieve accurate\nsegmentation. However, existing semi-supervised methods often struggle to\nstructure semantic distributions in the latent space due to noise introduced by\npseudo-labels. In this paper, we propose a novel diffusion-based framework for\nsemi-supervised medical image segmentation. Our method introduces a constraint\ninto the latent structure of semantic labels during the denoising diffusion\nprocess by enforcing prototype-based contrastive consistency. Rather than\nexplicitly delineating semantic boundaries, the model leverages class\nprototypes centralized semantic representations in the latent space as anchors.\nThis strategy improves the robustness of dense predictions, particularly in the\npresence of noisy pseudo-labels. We also introduce a new publicly available\nbenchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV),\nwhich provides detailed, manually annotated segmentation ground truth for\nmultiple anatomical structures in X-ray angiography videos. Extensive\nexperiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our\nmethod outperforms state-of-the-art medical image segmentation approaches under\nthe semi-supervised learning setting. This work presents a robust and\ndata-efficient diffusion model that offers enhanced flexibility and strong\npotential for a wide range of clinical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16429v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16553", "title": "Integral action for bilinear systems with application to counter current heat exchanger", "authors": ["Francesco Ripa", "Daniele Astolfi", "Boussad Hamroun", "Diego Regruto"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16553v1", "summary": "In this study, we propose a robust control strategy for a counter-current\nheat exchanger. The primary objective is to regulate the outlet temperature of\none fluid stream by manipulating the flow rate of the second counter-current\nfluid stream. By leveraging the energy balance equations, we develop a\nstructured bilinear system model derived by using a uniform spatial\ndiscretization of each stream into a cascade of homogeneous volumes and by\nconsidering the heat transfer and convective phenomena within the exchanger. We\nintroduce three control strategies: (i) an enhanced forwarding-based\ncontroller, (ii) an output feedback controller incorporating a state observer,\nand (iii) a purely integral control law. The effectiveness of the proposed\ncontrol strategy is validated through real experiments on a real heat\nexchanger.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16553v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16390", "title": "DASPack: Controlled Data Compression for Distributed Acoustic Sensing", "authors": ["Aleix Segui", "Arantza Ugalde", "Andreas Fichtner", "Sergi Ventosa", "Josep Ramon Morros"], "categories": ["physics.geo-ph", "eess.SP"], "primary_category": "Subjects:       Geophysics (physics.geo-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16390v1", "summary": "We present DASPack, a high-performance, open-source compression tool\nspecifically designed for distributed acoustic sensing (DAS) data. As DAS\nbecomes a key technology for real-time, high-density, and long-range monitoring\nin fields such as geophysics, infrastructure surveillance, and environmental\nsensing, the volume of collected data is rapidly increasing. Large-scale DAS\ndeployments already generate hundreds of terabytes and are expected to increase\nin the coming years, making long-term storage a major challenge. Despite this\nurgent need, few compression methods have proven to be both practical and\nscalable in real-world scenarios. DASPack is a fully operational solution that\nconsistently outperforms existing techniques for DAS data. It enables both\ncontrolled lossy and lossless compression by allowing users to choose the\nmaximum absolute difference per datum between the original and compressed data.\nThe compression pipeline combines wavelet transforms, linear predictive coding,\nand entropy coding to optimise efficiency. Our method achieves up to 3x file\nsize reductions for strain and strain rate data in lossless mode across diverse\ndatasets. In lossy mode, compression improves to 6x with near-perfect signal\nfidelity, and up to 10x is reached with acceptable signal degradation. It\ndelivers fast throughput (100-200 MB/s using a single-thread and up to 750 MB/s\nusing 8-threads), enabling real-time deployment even under high data rates. We\nvalidated its performance on 15 datasets from a variety of acquisition\nenvironments, demonstrating its speed, robustness, and broad applicability.\nDASPack provides a practical foundation for long-term, sustainable DAS data\nmanagement in large-scale monitoring networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16390v1", "cate": "physics.geo-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16579", "title": "Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis", "authors": ["Xiaojiao Xiao", "Qinmin Vivian Hu", "Guanghui Wang"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16579v1", "summary": "Medical image synthesis plays a crucial role in clinical workflows,\naddressing the common issue of missing imaging modalities due to factors such\nas extended scan times, scan corruption, artifacts, patient motion, and\nintolerance to contrast agents. The paper presents a novel image synthesis\nnetwork, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which\nemploys a multi-scale hierarchical approach for more detailed control over\nsynthesizing high-quality images across different resolutions and layers.\nSpecifically, this model utilizes randomly multi-scale high-proportion masks to\nspeed up diffusion model training, and balances detail fidelity and overall\nstructure. The integration of a Transformer-based Diffusion model process\nincorporates cross-granularity regularization, modeling the mutual information\nconsistency across each granularity's latent spaces, thereby enhancing\npixel-level perceptual accuracy. Comprehensive experiments on two challenging\ndatasets demonstrate that PHMDiff achieves superior performance in both the\nPeak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure\n(SSIM), highlighting its capability to produce high-quality synthesized images\nwith excellent structural integrity. Ablation studies further confirm the\ncontributions of each component. Furthermore, the PHMDiff model, a multi-scale\nimage synthesis framework across and within medical imaging modalities, shows\nsignificant advantages over other methods. The source code is available at\nhttps://github.com/xiaojiao929/PHMDiff", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16579v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16456", "title": "An approach to measuring the performance of Automatic Speech Recognition (ASR) models in the context of Large Language Model (LLM) powered applications", "authors": ["Sujith Pulikodan", "Sahapthan K", "Prasanta Kumar Ghosh", "Visruth Sanka", "Nihar Desai"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted at INTERSPEECH 2025", "url": "http://arxiv.org/abs/2507.16456v1", "summary": "Automatic Speech Recognition (ASR) plays a crucial role in human-machine\ninteraction and serves as an interface for a wide range of applications.\nTraditionally, ASR performance has been evaluated using Word Error Rate (WER),\na metric that quantifies the number of insertions, deletions, and substitutions\nin the generated transcriptions. However, with the increasing adoption of large\nand powerful Large Language Models (LLMs) as the core processing component in\nvarious applications, the significance of different types of ASR errors in\ndownstream tasks warrants further exploration. In this work, we analyze the\ncapabilities of LLMs to correct errors introduced by ASRs and propose a new\nmeasure to evaluate ASR performance for LLM-powered applications.", "comment": "Accepted at INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.16456v1", "cate": "eess.AS", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16514", "title": "The Ever-Evolving Science Exam", "authors": ["Junying Wang", "Zicheng Zhang", "Yijin Guo", "Farong Wen", "Ye Shen", "Yingji Liang", "Yalun Wu", "Wenzhe Li", "Chunyi Li", "Zijian Chen", "Qi Jia", "Guangtao Zhai"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.16514v1", "summary": "As foundation models grow rapidly in capability and deployment, evaluating\ntheir scientific understanding becomes increasingly critical. Existing science\nbenchmarks have made progress towards broad **Range**, wide **Reach**, and high\n**Rigor**, yet they often face two major challenges: **data leakage risks**\nthat compromise benchmarking validity, and **evaluation inefficiency** due to\nlarge-scale testing. To address these issues, we introduce the **Ever-Evolving\nScience Exam (EESE)**, a dynamic benchmark designed to reliably assess\nscientific capabilities in foundation models. Our approach consists of two\ncomponents: 1) a non-public **EESE-Pool** with over 100K expertly constructed\nscience instances (question-answer pairs) across 5 disciplines and 500+\nsubfields, built through a multi-stage pipeline ensuring **Range**, **Reach**,\nand **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled\nand validated to enable leakage-resilient, low-overhead evaluations.\nExperiments on 32 open- and closed-source models demonstrate that EESE\neffectively differentiates the strengths and weaknesses of models in scientific\nfields and cognitive dimensions. Overall, EESE provides a robust, scalable, and\nforward-compatible solution for science benchmark design, offering a realistic\nmeasure of how well foundation models handle science questions. The project\npage is at: https://github.com/aiben-ch/EESE.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.16514v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16107", "title": "Recursive Equations For Imputation Of Missing Not At Random Data With Sparse Pattern Support", "authors": ["Trung Phung", "Kyle Reese", "Ilya Shpitser", "Rohit Bhattacharya"], "categories": ["stat.ME", "cs.LG"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "Comments:      45 pages", "url": "http://arxiv.org/abs/2507.16107v1", "summary": "A common approach for handling missing values in data analysis pipelines is\nmultiple imputation via software packages such as MICE (Van Buuren and\nGroothuis-Oudshoorn, 2011) and Amelia (Honaker et al., 2011). These packages\ntypically assume the data are missing at random (MAR), and impose parametric or\nsmoothing assumptions upon the imputing distributions in a way that allows\nimputation to proceed even if not all missingness patterns have support in the\ndata. Such assumptions are unrealistic in practice, and induce model\nmisspecification bias on any analysis performed after such imputation.\n  In this paper, we provide a principled alternative. Specifically, we develop\na new characterization for the full data law in graphical models of missing\ndata. This characterization is constructive, is easily adapted for the\ncalculation of imputation distributions for both MAR and MNAR (missing not at\nrandom) mechanisms, and is able to handle lack of support for certain patterns\nof missingness. We use this characterization to develop a new imputation\nalgorithm -- Multivariate Imputation via Supported Pattern Recursion (MISPR) --\nwhich uses Gibbs sampling, by analogy with the Multivariate Imputation with\nChained Equations (MICE) algorithm, but which is consistent under both MAR and\nMNAR settings, and is able to handle missing data patterns with no support\nwithout imposing additional assumptions beyond those already imposed by the\nmissing data model itself.\n  In simulations, we show MISPR obtains comparable results to MICE when data\nare MAR, and superior, less biased results when data are MNAR. Our\ncharacterization and imputation algorithm based on it are a step towards making\nprincipled missing data methods more practical in applied settings, where the\ndata are likely both MNAR and sufficiently high dimensional to yield missing\ndata patterns with no support at available sample sizes.", "comment": "45 pages", "pdf_url": "http://arxiv.org/pdf/2507.16107v1", "cate": "stat.ME", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16443", "title": "VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on Kilometer-scale Long RGB Sequences", "authors": ["Kai Deng", "Zexin Ti", "Jiawei Xu", "Jian Yang", "Jin Xie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16443v1", "summary": "Foundation models for 3D vision have recently demonstrated remarkable\ncapabilities in 3D perception. However, extending these models to large-scale\nRGB stream 3D reconstruction remains challenging due to memory limitations. In\nthis work, we propose VGGT-Long, a simple yet effective system that pushes the\nlimits of monocular 3D reconstruction to kilometer-scale, unbounded outdoor\nenvironments. Our approach addresses the scalability bottlenecks of existing\nmodels through a chunk-based processing strategy combined with overlapping\nalignment and lightweight loop closure optimization. Without requiring camera\ncalibration, depth supervision or model retraining, VGGT-Long achieves\ntrajectory and reconstruction performance comparable to traditional methods. We\nevaluate our method on KITTI, Waymo, and Virtual KITTI datasets. VGGT-Long not\nonly runs successfully on long RGB sequences where foundation models typically\nfail, but also produces accurate and consistent geometry across various\nconditions. Our results highlight the potential of leveraging foundation models\nfor scalable monocular 3D scene in real-world settings, especially for\nautonomous driving scenarios. Code is available at\nhttps://github.com/DengKaiCQ/VGGT-Long.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16443v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16774", "title": "Dynamic Activation and Assignment of SDN Controllers in LEO Satellite Constellations", "authors": ["Wafa Hasanain", "Pablo G. Madoery", "Halim Yanikomeroglu", "Gunes Karabulut Kurt", "Sameera Siddiqui", "Stephane Martel", "Khaled Ahmed", "Colin Bellinger"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16774v1", "summary": "Software-defined networking (SDN) has emerged as a promising approach for\nmanaging traditional satellite communication. This enhances opportunities for\nfuture services, including integrating satellite and terrestrial networks. In\nthis paper, we have developed an SDN-enabled framework for Low Earth Orbit\n(LEO) satellite networks, incorporating the OpenFlow protocol, all within an\nOMNeT++ simulation environment. Dynamic controller assignment is one of the\nmost significant challenges for large LEO constellations. Due to the movement\nof LEO satellites, satellite-controller assignments must be updated frequently\nto maintain low propagation delays. To address this issue, we present a dynamic\nsatellite-to-controller assignment (DSCA) optimization problem that\ncontinuously adjusts these assignments. Our optimal DSCA (Opt-DSCA) approach\nminimizes propagation delay and optimizes the number of active controllers. Our\npreliminary results demonstrate that the DSCA approach significantly\noutperforms the static satellite-to-controller assignment (SSCA) approach.\nWhile SSCA may perform better with more controllers, this scheme fails to adapt\nto satellite movements. Our DSCA approach consistently improves network\nefficiency by dynamically reassigning satellites based on propagation delays.\nFurther, we found diminishing returns when the number of controllers is\nincreased beyond a certain point, suggesting optimal performance with a limited\nnumber of controllers. Opt-DSCA lowers propagation delays and improves network\nperformance by optimizing satellite assignments and reducing active\ncontrollers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16774v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16477", "title": "Adaptive Bayesian Single-Shot Quantum Sensing", "authors": ["Ivana Nikoloska", "Ruud Van Sloun", "Osvaldo Simeone"], "categories": ["quant-ph", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      submitted for publication", "url": "http://arxiv.org/abs/2507.16477v1", "summary": "Quantum sensing harnesses the unique properties of quantum systems to enable\nprecision measurements of physical quantities such as time, magnetic and\nelectric fields, acceleration, and gravitational gradients well beyond the\nlimits of classical sensors. However, identifying suitable sensing probes and\nmeasurement schemes can be a classically intractable task, as it requires\noptimizing over Hilbert spaces of high dimension. In variational quantum\nsensing, a probe quantum system is generated via a parameterized quantum\ncircuit (PQC), exposed to an unknown physical parameter through a quantum\nchannel, and measured to collect classical data. PQCs and measurements are\ntypically optimized using offline strategies based on frequentist learning\ncriteria. This paper introduces an adaptive protocol that uses Bayesian\ninference to optimize the sensing policy via the maximization of the active\ninformation gain. The proposed variational methodology is tailored for\nnon-asymptotic regimes where a single probe can be deployed in each time step,\nand is extended to support the fusion of estimates from multiple quantum\nsensing agents.", "comment": "submitted for publication", "pdf_url": "http://arxiv.org/pdf/2507.16477v1", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16715", "title": "A Tutorial on MRI Reconstruction: From Modern Methods to Clinical Implications", "authors": ["Tolga ukur", "Salman U. H. Dar", "Valiyeh Ansarian Nezhad", "Yohan Jun", "Tae Hyung Kim", "Shohei Fujita", "Berkin Bilgic"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16715v1", "summary": "MRI is an indispensable clinical tool, offering a rich variety of tissue\ncontrasts to support broad diagnostic and research applications. Clinical exams\nroutinely acquire multiple structural sequences that provide complementary\ninformation for differential diagnosis, while research protocols often\nincorporate advanced functional, diffusion, spectroscopic, and relaxometry\nsequences to capture multidimensional insights into tissue structure and\ncomposition. However, these capabilities come at the cost of prolonged scan\ntimes, which reduce patient throughput, increase susceptibility to motion\nartifacts, and may require trade-offs in image quality or diagnostic scope.\nOver the last two decades, advances in image reconstruction\nalgorithms--alongside improvements in hardware and pulse sequence design--have\nmade it possible to accelerate acquisitions while preserving diagnostic\nquality. Central to this progress is the ability to incorporate prior\ninformation to regularize the solutions to the reconstruction problem. In this\ntutorial, we overview the basics of MRI reconstruction and highlight\nstate-of-the-art approaches, beginning with classical methods that rely on\nexplicit hand-crafted priors, and then turning to deep learning methods that\nleverage a combination of learned and crafted priors to further push the\nperformance envelope. We also explore the translational aspects and eventual\nclinical implications of these methods. We conclude by discussing future\ndirections to address remaining challenges in MRI reconstruction. The tutorial\nis accompanied by a Python toolbox\n(https://github.com/tutorial-MRI-recon/tutorial) to demonstrate select methods\ndiscussed in the article.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16715v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16190", "title": "LABNet: A Lightweight Attentive Beamforming Network for Ad-hoc Multichannel Microphone Invariant Real-Time Speech Enhancement", "authors": ["Haoyin Yan", "Jie Zhang", "Chengqian Jiang", "Shuang Zhang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16190v1", "summary": "Multichannel speech enhancement (SE) aims to restore clean speech from noisy\nmeasurements by leveraging spatiotemporal signal features. In ad-hoc array\nconditions, microphone invariance (MI) requires systems to handle different\nmicrophone numbers and array geometries. From a practical perspective,\nmultichannel recordings inevitably increase the computational burden for\nedge-device applications, highlighting the necessity of lightweight and\nefficient deployments. In this work, we propose a lightweight attentive\nbeamforming network (LABNet) to integrate MI in a low-complexity real-time SE\nsystem. We design a three-stage framework for efficient intra-channel modeling\nand inter-channel interaction. A cross-channel attention module is developed to\naggregate features from each channel selectively. Experimental results\ndemonstrate our LABNet achieves impressive performance with ultra-light\nresource overhead while maintaining the MI, indicating great potential for\nad-hoc array processing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16190v1", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16524", "title": "Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models", "authors": ["Xiaoyan Wang", "Zeju Li", "Yifan Xu", "Jiaxing Qi", "Zhifei Yang", "Ruifei Ma", "Xiangde Liu", "Chao Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICME2025", "url": "http://arxiv.org/abs/2507.16524v1", "summary": "New era has unlocked exciting possibilities for extending Large Language\nModels (LLMs) to tackle 3D vision-language tasks. However, most existing 3D\nmultimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or\nsegmenting independent objects to perform these tasks, which limits their\nspatial awareness due to insufficient representation of the richness inherent\nin 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D\nMLLM specifically designed to enhance spatial awareness for 3D vision-language\ntasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM\nintegrates an LLM backbone with a progressive spatial awareness scheme that\nprogressively captures spatial information as the perception field expands,\ngenerating location-enriched 3D scene embeddings to serve as visual prompts.\nFurthermore, we introduce two novel tasks: 3D object distance measurement and\n3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate\nthe model's spatial awareness capabilities. Experimental results demonstrate\nthat Spatial 3D-LLM achieves state-of-the-art performance across a wide range\nof 3D vision-language tasks, revealing the improvements stemmed from our\nprogressive spatial awareness scheme of mining more profound spatial\ninformation. Our code is available at\nhttps://github.com/bjshuyuan/Spatial-3D-LLM.", "comment": "Accepted by ICME2025", "pdf_url": "http://arxiv.org/pdf/2507.16524v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16218", "title": "Toward Routine CSP of Pharmaceuticals: A Fully Automated Protocol Using Neural Network Potentials", "authors": ["Zachary L. Glick", "Derek P. Metcalf", "Scott F. Swarthout"], "categories": ["physics.chem-ph", "cs.LG"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16218v1", "summary": "Crystal structure prediction (CSP) is a useful tool in pharmaceutical\ndevelopment for identifying and assessing risks associated with polymorphism,\nyet widespread adoption has been hindered by high computational costs and the\nneed for both manual specification and expert knowledge to achieve useful\nresults. Here, we introduce a fully automated, high-throughput CSP protocol\ndesigned to overcome these barriers. The protocol's efficiency is driven by\nLavo-NN, a novel neural network potential (NNP) architected and trained\nspecifically for pharmaceutical crystal structure generation and ranking. This\nNNP-driven crystal generation phase is integrated into a scalable cloud-based\nworkflow. We validate this CSP protocol on an extensive retrospective benchmark\nof 49 unique molecules, almost all of which are drug-like, successfully\ngenerating structures that match all 110 $Z' = 1$ experimental polymorphs. The\naverage CSP in this benchmark is performed with approximately 8.4k CPU hours,\nwhich is a significant reduction compared to other protocols. The practical\nutility of the protocol is further demonstrated through case studies that\nresolve ambiguities in experimental data and a semi-blinded challenge that\nsuccessfully identifies and ranks polymorphs of three modern drugs from powder\nX-ray diffraction patterns alone. By significantly reducing the required time\nand cost, the protocol enables CSP to be routinely deployed earlier in the drug\ndiscovery pipeline, such as during lead optimization. Rapid turnaround times\nand high throughput also enable CSP that can be run in parallel with\nexperimental screening, providing chemists with real-time insights to guide\ntheir work in the lab.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16218v1", "cate": "physics.chem-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16472", "title": "DenseSR: Image Shadow Removal as Dense Prediction", "authors": ["Yu-Fan Lin", "Chia-Ming Lee", "Chih-Chung Hsu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Paper accepted to ACMMM 2025", "url": "http://arxiv.org/abs/2507.16472v1", "summary": "Shadows are a common factor degrading image quality. Single-image shadow\nremoval (SR), particularly under challenging indirect illumination, is hampered\nby non-uniform content degradation and inherent ambiguity. Consequently,\ntraditional methods often fail to simultaneously recover intra-shadow details\nand maintain sharp boundaries, resulting in inconsistent restoration and\nblurring that negatively affect both downstream applications and the overall\nviewing experience. To overcome these limitations, we propose the DenseSR,\napproaching the problem from a dense prediction perspective to emphasize\nrestoration quality. This framework uniquely synergizes two key strategies: (1)\ndeep scene understanding guided by geometric-semantic priors to resolve\nambiguity and implicitly localize shadows, and (2) high-fidelity restoration\nvia a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive\ncomponent processing-using an Adaptive Content Smoothing Module (ACSM) for\nconsistent appearance and a Texture-Boundary Recuperation Module (TBRM) for\nfine textures and sharp boundaries-thereby directly tackling the inconsistent\nrestoration and blurring issues. These purposefully processed components are\neffectively fused, yielding an optimized feature representation preserving both\nconsistency and fidelity. Extensive experimental results demonstrate the merits\nof our approach over existing methods. Our code can be available on\nhttps://github$.$com/VanLinLin/DenseSR", "comment": "Paper accepted to ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.16472v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16156", "title": "Nd3+ Doping-induced Leakage Currents Suppression in High-temperature 0.7BiFeO3-0.3BaTiO3 Lead-free Piezoceramics", "authors": ["Jinming Liu", "Mingtong Chen", "Zhengbao Yang"], "categories": ["cond-mat.mtrl-sci", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16156v1", "summary": "BiFeO3 has attracted much attention as a potential candidate for replacing\nconventional, lead based piezoelectric materials due to its remarkable\nspontaneous polarization and high Curie temperature. However, its inherent high\nleakage currents, which lead to low piezoelectric response and poor temperature\nstability, have severely limited its practical applications. In this study,\nlead free piezoelectric ceramics of the 0.7BiFeO3-0.3BaTiO3 (BF-BT) system were\nprepared, and their microstructures along with high-temperature electrical\nperformance were modulated by introducing Nd3+. The results indicate that\nmoderate Nd doping improves lattice symmetry and reduces oxygen vacancy-related\ndefect dipoles, thereby effectively suppressing leakage currents at\ntemperatures above 75{\\deg}C. The Nddoped samples exhibit significantly lower\nleakage current densities, reduced by over 99% compared to the undoped\nceramics, reaching values as low as 10-5Acm-2. They also show higher\nresistivity under elevated temperatures and electric fields, offering notable\nimprovements in thermal stability over the undoped counterparts. In addition,\nthe Nd-doped samples achieved piezoelectric coefficients as high as 172 pC N -1\nat room temperature while still maintaining high dielectric and piezoelectric\nresponses at elevated temperatures. This work not only provides an effective\nway to solve the leakage current problem of BF-BT ceramics in high temperature\napplications but also indicates a new design strategy for optimizing the high\ntemperature stability of lead free piezoelectric materials, which shows a broad\napplication prospect in the field of high-temperature sensors and actuators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16156v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2412.10387", "title": "AsyMov: Integrated Sensing and Communications with Asynchronous Moving Devices", "authors": ["Gianmaria Ventura", "Michele Rossi", "Jacopo Pegoraro"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages, 12 figures, 2 tables", "url": "http://arxiv.org/abs/2412.10387v2", "summary": "Estimating the Doppler frequency shift caused by moving targets is one of the\nkey objectives of Integrated Sensing And Communication (ISAC) systems, as it\nenables applications such as target classification, human activity recognition,\nand gait analysis. In practical scenarios, Doppler estimation is hindered by\nthe movement of transmitter and receiver devices, and by the phase offsets\ncaused by their clock asynchrony. Existing approaches have separately addressed\nthese two aspects, either assuming clock-synchronous moving devices or\nasynchronous static ones. In fact, jointly tackling device motion and clock\nasynchrony is extremely challenging, as the Doppler shift from device movement\ndiffers for each propagation path and the phase offsets are time-varying. In\nthis work, we present AsyMov, a method to estimate the bistatic Doppler\nfrequency of a target and its velocity in ISAC setups featuring mobile and\nasynchronous devices. It leverages the channel impulse response at the\nreceiver, by originally exploiting the invariance of phase offsets across\npropagation paths and the bistatic geometry, where the target Doppler and the\ndevice velocity are jointly estimated by a newly proposed alternating\nminimization algorithm. Moreover, it can be seamlessly integrated with device\nvelocity measurements obtained from onboard sensors (if available), for\nenhanced reliability. Here, AsyMov is thoroughly characterized by way of theory\n(Cram\\'er-Rao bound), simulation and experiments, implementing it on an IEEE\n802.11ay testbed and testing it on multiple setups, also including a moving\nhuman target. Numerical and experimental results show superior performance\nagainst state-of-the-art methods and are on par with scenarios featuring static\nISAC devices.", "comment": "13 pages, 12 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2412.10387v2", "cate": "eess.SP", "date": "2024-11-29", "updated": "2025-07-22"}
{"id": "2507.16779", "title": "Improving U-Net Confidence on TEM Image Data with L2-Regularization, Transfer Learning, and Deep Fine-Tuning", "authors": ["Aiden Ochoa", "Xinyuan Xu", "Xing Wang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted into the ICCV 2025 CV4MS Workshop", "url": "http://arxiv.org/abs/2507.16779v1", "summary": "With ever-increasing data volumes, it is essential to develop automated\napproaches for identifying nanoscale defects in transmission electron\nmicroscopy (TEM) images. However, compared to features in conventional\nphotographs, nanoscale defects in TEM images exhibit far greater variation due\nto the complex contrast mechanisms and intricate defect structures. These\nchallenges often result in much less labeled data and higher rates of\nannotation errors, posing significant obstacles to improving machine learning\nmodel performance for TEM image analysis. To address these limitations, we\nexamined transfer learning by leveraging large, pre-trained models used for\nnatural images.\n  We demonstrated that by using the pre-trained encoder and L2-regularization,\nsemantically complex features are ignored in favor of simpler, more reliable\ncues, substantially improving the model performance. However, this improvement\ncannot be captured by conventional evaluation metrics such as F1-score, which\ncan be skewed by human annotation errors treated as ground truth. Instead, we\nintroduced novel evaluation metrics that are independent of the annotation\naccuracy. Using grain boundary detection in UO2 TEM images as a case study, we\nfound that our approach led to a 57% improvement in defect detection rate,\nwhich is a robust and holistic measure of model performance on the TEM dataset\nused in this work. Finally, we showed that model self-confidence is only\nachieved through transfer learning and fine-tuning of very deep layers.", "comment": "Accepted into the ICCV 2025 CV4MS Workshop", "pdf_url": "http://arxiv.org/pdf/2507.16779v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16564", "title": "TTMBA: Towards Text To Multiple Sources Binaural Audio Generation", "authors": ["Yuxuan He", "Xiaoran Yang", "Ningning Pan", "Gongping Huang"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      5 pages,3 figures,2 tables", "url": "http://arxiv.org/abs/2507.16564v1", "summary": "Most existing text-to-audio (TTA) generation methods produce mono outputs,\nneglecting essential spatial information for immersive auditory experiences. To\naddress this issue, we propose a cascaded method for text-to-multisource\nbinaural audio generation (TTMBA) with both temporal and spatial control.\nFirst, a pretrained large language model (LLM) segments the text into a\nstructured format with time and spatial details for each sound event. Next, a\npretrained mono audio generation network creates multiple mono audios with\nvarying durations for each event. These mono audios are transformed into\nbinaural audios using a binaural rendering neural network based on spatial data\nfrom the LLM. Finally, the binaural audios are arranged by their start times,\nresulting in multisource binaural audio. Experimental results demonstrate the\nsuperiority of the proposed method in terms of both audio generation quality\nand spatial perceptual accuracy.", "comment": "5 pages,3 figures,2 tables", "pdf_url": "http://arxiv.org/pdf/2507.16564v1", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16535", "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion", "authors": ["Shang Liu", "Chenjie Cao", "Chaohui Yu", "Wen Qian", "Jing Wang", "Fan Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16535v1", "summary": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16535v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16236", "title": "PAC Off-Policy Prediction of Contextual Bandits", "authors": ["Yilong Wan", "Yuqiang Li", "Xianyi Wu"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16236v1", "summary": "This paper investigates off-policy evaluation in contextual bandits, aiming\nto quantify the performance of a target policy using data collected under a\ndifferent and potentially unknown behavior policy. Recently, methods based on\nconformal prediction have been developed to construct reliable prediction\nintervals that guarantee marginal coverage in finite samples, making them\nparticularly suited for safety-critical applications. To further achieve\ncoverage conditional on a given offline data set, we propose a novel algorithm\nthat constructs probably approximately correct prediction intervals. Our method\nbuilds upon a PAC-valid conformal prediction framework, and we strengthen its\ntheoretical guarantees by establishing PAC-type bounds on coverage. We analyze\nboth finite-sample and asymptotic properties of the proposed method, and\ncompare its empirical performance with existing methods in simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16236v1", "cate": "stat.ML", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16476", "title": "Survival Modeling from Whole Slide Images via Patch-Level Graph Clustering and Mixture Density Experts", "authors": ["Ardhendu Sekhar", "Vasu Soni", "Keshav Aske", "Garima Jain", "Pranav Jeevan", "Amit Sethi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16476v1", "summary": "We introduce a modular framework for predicting cancer-specific survival from\nwhole slide pathology images (WSIs) that significantly improves upon the\nstate-of-the-art accuracy. Our method integrating four key components. Firstly,\nto tackle large size of WSIs, we use dynamic patch selection via quantile-based\nthresholding for isolating prognostically informative tissue regions. Secondly,\nwe use graph-guided k-means clustering to capture phenotype-level heterogeneity\nthrough spatial and morphological coherence. Thirdly, we use attention\nmechanisms that model both intra- and inter-cluster relationships to\ncontextualize local features within global spatial relations between various\ntypes of tissue compartments. Finally, we use an expert-guided mixture density\nmodeling for estimating complex survival distributions using Gaussian mixture\nmodels. The proposed model achieves a concordance index of $0.712 \\pm 0.028$\nand Brier score of $0.254 \\pm 0.018$ on TCGA-KIRC (renal cancer), and a\nconcordance index of $0.645 \\pm 0.017$ and Brier score of $0.281 \\pm 0.031$ on\nTCGA-LUAD (lung adenocarcinoma). These results are significantly better than\nthe state-of-art and demonstrate predictive potential of the proposed method\nacross diverse cancer types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16476v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16495", "title": "Spiking neurons as predictive controllers of linear systems", "authors": ["Paolo Agliati", "Andr Urbano", "Pablo Lanillos", "Nasir Ahmad", "Marcel van Gerven", "Sander Keemink"], "categories": ["q-bio.NC", "cs.NE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16495v1", "summary": "Neurons communicate with downstream systems via sparse and incredibly brief\nelectrical pulses, or spikes. Using these events, they control various targets\nsuch as neuromuscular units, neurosecretory systems, and other neurons in\nconnected circuits. This gave rise to the idea of spiking neurons as\ncontrollers, in which spikes are the control signal. Using instantaneous events\ndirectly as the control inputs, also called `impulse control', is challenging\nas it does not scale well to larger networks and has low analytical\ntractability. Therefore, current spiking control usually relies on filtering\nthe spike signal to approximate analog control. This ultimately means spiking\nneural networks (SNNs) have to output a continuous control signal,\nnecessitating continuous energy input into downstream systems. Here, we\ncircumvent the need for rate-based representations, providing a scalable method\nfor task-specific spiking control with sparse neural activity. In doing so, we\ntake inspiration from both optimal control and neuroscience theory, and define\na spiking rule where spikes are only emitted if they bring a dynamical system\ncloser to a target. From this principle, we derive the required connectivity\nfor an SNN, and show that it can successfully control linear systems. We show\nthat for physically constrained systems, predictive control is required, and\nthe control signal ends up exploiting the passive dynamics of the downstream\nsystem to reach a target. Finally, we show that the control method scales to\nboth high-dimensional networks and systems. Importantly, in all cases, we\nmaintain a closed-form mathematical derivation of the network connectivity, the\nnetwork dynamics and the control objective. This work advances the\nunderstanding of SNNs as biologically-inspired controllers, providing insight\ninto how real neurons could exert control, and enabling applications in\nneuromorphic hardware design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16495v1", "cate": "q-bio.NC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2501.17868", "title": "Hybrid Near-field and Far-field Localization with Holographic MIMO", "authors": ["Mengyuan Cao", "Haobo Zhang", "Yonina C. Eldar", "Hongliang Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.17868v2", "summary": "Due to its ability to precisely control wireless beams, holographic\nmultiple-input multiple-output (HMIMO) is expected to be a promising solution\nto achieve high-accuracy localization. However, as the scale of HMIMO increases\nto improve beam control capability, the corresponding near-field (NF) region\nexpands, indicating that users may exist in both NF and far-field (FF) regions\nwith different electromagnetic transmission characteristics. As a result,\nexisting methods for pure NF or FF localization are no longer applicable. We\nconsider a hybrid NF and FF localization scenario in this paper, where a base\nstation (BS) locates multiple users in both NF and FF regions with the aid of a\nreconfigurable intelligent surface (RIS), which is a low-cost implementation of\nHMIMO. In such a scenario, it is difficult to locate the users and optimize the\nRIS phase shifts because whether the location of the user is in the NF or FF\nregion is unknown, and the channels of different users are coupled. To tackle\nthis challenge, we propose a RIS-enabled localization method that searches the\nusers in both NF and FF regions and tackles the coupling issue by jointly\nestimating all user locations. We derive the localization error bound by\nconsidering the channel coupling and propose an RIS phase shift optimization\nalgorithm that minimizes the derived bound. Simulations show the effectiveness\nof the proposed method and demonstrate the performance gain compared to pure NF\nand FF techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.17868v2", "cate": "eess.SP", "date": "2025-01-15", "updated": "2025-07-22"}
{"id": "2507.16803", "title": "MultiTaskDeltaNet: Change Detection-based Image Segmentation for Operando ETEM with Application to Carbon Gasification Kinetics", "authors": ["Yushuo Niu", "Tianyu Li", "Yuanyuan Zhu", "Qian Yang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16803v1", "summary": "Transforming in-situ transmission electron microscopy (TEM) imaging into a\ntool for spatially-resolved operando characterization of solid-state reactions\nrequires automated, high-precision semantic segmentation of dynamically\nevolving features. However, traditional deep learning methods for semantic\nsegmentation often encounter limitations due to the scarcity of labeled data,\nvisually ambiguous features of interest, and small-object scenarios. To tackle\nthese challenges, we introduce MultiTaskDeltaNet (MTDN), a novel deep learning\narchitecture that creatively reconceptualizes the segmentation task as a change\ndetection problem. By implementing a unique Siamese network with a U-Net\nbackbone and using paired images to capture feature changes, MTDN effectively\nutilizes minimal data to produce high-quality segmentations. Furthermore, MTDN\nutilizes a multi-task learning strategy to leverage correlations between\nphysical features of interest. In an evaluation using data from in-situ\nenvironmental TEM (ETEM) videos of filamentous carbon gasification, MTDN\ndemonstrated a significant advantage over conventional segmentation models,\nparticularly in accurately delineating fine structural features. Notably, MTDN\nachieved a 10.22% performance improvement over conventional segmentation models\nin predicting small and visually ambiguous physical features. This work bridges\nseveral key gaps between deep learning and practical TEM image analysis,\nadvancing automated characterization of nanomaterials in complex experimental\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16803v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16632", "title": "Step-Audio 2 Technical Report", "authors": ["Boyong Wu", "Chao Yan", "Chen Hu", "Cheng Yi", "Chengli Feng", "Fei Tian", "Feiyu Shen", "Gang Yu", "Haoyang Zhang", "Jingbei Li", "Mingrui Chen", "Peng Liu", "Wang You", "Xiangyu Tony Zhang", "Xingyuan Li", "Xuerui Yang", "Yayue Deng", "Yechang Huang", "Yuxin Li", "Yuxin Zhang", "Zhao You", "Brian Li", "Changyi Wan", "Hanpeng Hu", "Jiangjie Zhen", "Siyu Chen", "Song Yuan", "Xuelin Zhang", "Yimin Jiang", "Yu Zhou", "Yuxiang Yang", "Bingxin Li", "Buyun Ma", "Changhe Song", "Dongqing Pang", "Guoqiang Hu", "Haiyang Sun", "Kang An", "Na Wang", "Shuli Gao", "Wei Ji", "Wen Li", "Wen Sun", "Xuan Wen", "Yong Ren", "Yuankai Ma", "Yufan Lu", "Bin Wang", "Bo Li", "Changxin Miao", "Che Liu", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Donghang Wu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Han Zhang", "Hao Nie", "Haonan Jia", "Hongyu Zhou", "Jianjian Sun", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Jin Yang", "Junzhe Lin", "Kaixiang Li", "Lei Yang", "Liying Shi", "Li Zhou", "Longlong Gu", "Ming Li", "Mingliang Li", "Mingxiao Li", "Nan Wu", "Qi Han", "Qinyuan Tan", "Shaoliang Pang", "Shengjie Fan", "Siqi Liu", "Tiancheng Cao", "Wanying Lu", "Wenqing He", "Wuxun Xie", "Xu Zhao", "Xueqi Li", "Yanbo Yu", "Yang Yang", "Yi Liu", "Yifan Lu", "Yilei Wang", "Yuanhao Ding", "Yuanwei Liang", "Yuanwei Lu", "Yuchu Luo", "Yuhe Yin", "Yumeng Zhan", "Yuxiang Zhang", "Zidong Yang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16632v1", "summary": "This paper presents Step-Audio~2, an end-to-end multi-modal large language\nmodel designed for industry-strength audio understanding and speech\nconversation. By integrating a latent audio encoder and reasoning-centric\nreinforcement learning (RL), Step-Audio 2 achieves promising performance in\nautomatic speech recognition (ASR) and audio understanding. To facilitate\ngenuine end-to-end speech conversation, Step-Audio 2 incorporates the\ngeneration of discrete audio tokens into language modeling, significantly\nenhancing its responsiveness to paralinguistic information such as speaking\nstyles and emotions. To effectively leverage the rich textual and acoustic\nknowledge in real-world data, Step-Audio 2 integrates retrieval-augmented\ngeneration (RAG) and is able to call external tools such as web search to\nmitigate hallucination and audio search to switch timbres. Trained on millions\nof hours of speech and audio data, Step-Audio 2 delivers intelligence and\nexpressiveness across diverse conversational scenarios. Evaluation results\ndemonstrate that Step-Audio 2 achieves state-of-the-art performance on various\naudio understanding and conversational benchmarks compared to other open-source\nand commercial solutions. Please visit\nhttps://github.com/stepfun-ai/Step-Audio2 for more information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16632v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16571", "title": "Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume Computations", "authors": ["G. de Rommont", "F. Renac", "F. Chinesta", "J. Nunez", "D. Gueyffier"], "categories": ["math.NA", "cs.AI", "cs.NA", "math.AP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      19 pages, 13 Figures, 1 table", "url": "http://arxiv.org/abs/2507.16571v1", "summary": "We present a novel data-driven approach for enhancing gradient reconstruction\nin unstructured finite volume methods for hyperbolic conservation laws,\nspecifically for the 2D Euler equations. Our approach extends previous\nstructured-grid methodologies to unstructured meshes through a modified\nDeepONet architecture that incorporates local geometry in the neural network.\nThe architecture employs local mesh topology to ensure rotation invariance,\nwhile also ensuring first-order constraint on the learned operator. The\ntraining methodology incorporates physics-informed regularization through\nentropy penalization, total variation diminishing penalization, and parameter\nregularization to ensure physically consistent solutions, particularly in\nshock-dominated regions. The model is trained on high-fidelity datasets\nsolutions derived from sine waves and randomized piecewise constant initial\nconditions with periodic boundary conditions, enabling robust generalization to\ncomplex flow configurations or geometries. Validation test cases from the\nliterature, including challenging geometry configuration, demonstrates\nsubstantial improvements in accuracy compared to traditional second-order\nfinite volume schemes. The method achieves gains of 20-60% in solution accuracy\nwhile enhancing computational efficiency. A convergence study has been conveyed\nand reveal improved mesh convergence rates compared to the conventional solver.\nThe proposed algorithm is faster and more accurate than the traditional\nsecond-order finite volume solver, enabling high-fidelity simulations on\ncoarser grids while preserving the stability and conservation properties\nessential for hyperbolic conservation laws. This work is a part of a new\ngeneration of solvers that are built by combining Machine-Learning (ML) tools\nwith traditional numerical schemes, all while ensuring physical constraint on\nthe results.", "comment": "19 pages, 13 Figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.16571v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16237", "title": "LLM-Enhanced Reranking for Complementary Product Recommendation", "authors": ["Zekun Xu", "Yudi Zhang"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16237v1", "summary": "Complementary product recommendation, which aims to suggest items that are\nused together to enhance customer value, is a crucial yet challenging task in\ne-commerce. While existing graph neural network (GNN) approaches have made\nsignificant progress in capturing complex product relationships, they often\nstruggle with the accuracy-diversity tradeoff, particularly for long-tail\nitems. This paper introduces a model-agnostic approach that leverages Large\nLanguage Models (LLMs) to enhance the reranking of complementary product\nrecommendations. Unlike previous works that use LLMs primarily for data\npreprocessing and graph augmentation, our method applies LLM-based prompting\nstrategies directly to rerank candidate items retrieved from existing\nrecommendation models, eliminating the need for model retraining. Through\nextensive experiments on public datasets, we demonstrate that our approach\neffectively balances accuracy and diversity in complementary product\nrecommendations, with at least 50% lift in accuracy metrics and 2% lift in\ndiversity metrics on average for the top recommended items across datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16237v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16506", "title": "PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium Specimens", "authors": ["Youcef Sklab", "Florian Castanet", "Hanane Ariouat", "Souhila Arib", "Jean-Daniel Zucker", "Eric Chenin", "Edi Prifti"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19 pages, 11 figures, 8 tables", "url": "http://arxiv.org/abs/2507.16506v1", "summary": "Deep learning-based classification of herbarium images is hampered by\nbackground heterogeneity, which introduces noise and artifacts that can\npotentially mislead models and reduce classification accuracy. Addressing these\nbackground-related challenges is critical to improving model performance. We\nintroduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10\nfor plant region detection and the Segment Anything Model (SAM2) for\nsegmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing\nsegmentation accuracy. Both models were fine-tuned on herbarium images and\nevaluated using Intersection over Union (IoU) and Dice coefficient metrics.\nPlantSAM achieved state-of-the-art segmentation performance, with an IoU of\n0.94 and a Dice coefficient of 0.97. Incorporating segmented images into\nclassification models led to consistent performance improvements across five\ntested botanical traits, with accuracy gains of up to 4.36% and F1-score\nimprovements of 4.15%. Our findings highlight the importance of background\nremoval in herbarium image analysis, as it significantly enhances\nclassification accuracy by allowing models to focus more effectively on the\nforeground plant structures.", "comment": "19 pages, 11 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2507.16506v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2411.07636", "title": "Node Reliability: Approximation, Upper Bounds, and Applications to Network Robustness", "authors": ["Xinhan Liu", "Robert Kooij", "Piet Van Mieghem"], "categories": ["eess.SY", "cs.SY", "math.PR"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      The manuscript is being significantly revised and reorganized based on substantial feedback. A thoroughly updated version will be submitted through a journal review process", "url": "http://arxiv.org/abs/2411.07636v2", "summary": "This paper discusses the reliability of a graph in which the links are\nperfectly reliable but the nodes may fail with certain probability p.\nCalculating graph node reliability is an NP-Hard problem. We introduce an\nefficient and accurate Monte Carlo method and a stochastic approximation for\nthe node reliability polynomial based solely on the degree distribution. We\nprovide the formulas for the node reliability polynomial of both Erdos-Renyi\ngraphs and Random Geometric graphs. The phase transition in the node\nreliability of Erdos-Renyi graphs is also discussed. Additionally, we propose\ntwo increasingly accurate upper bounds for the node reliability polynomial\nsolely based on the graph's degree distributions. The advantages and\ndisadvantages of these two upper bounds are thoroughly compared. Beyond the\ncomputation of node reliability polynomials, we also estimate the number of cut\nsets and present a solution to the reliability-based network enhancement\nproblem.", "comment": "The manuscript is being significantly revised and reorganized based\n  on substantial feedback. A thoroughly updated version will be submitted\n  through a journal review process", "pdf_url": "http://arxiv.org/pdf/2411.07636v2", "cate": "eess.SY", "date": "2024-11-12", "updated": "2025-07-21"}
{"id": "2502.03171", "title": "Hybrid Near-Field and Far-Field Localization with Multiple Holographic MIMO Surfaces", "authors": ["Weiqiao Zhu", "Mengyuan Cao", "Yang Yang", "Haobo Zhang", "Weijun Hao", "Xiaofei Jia", "Hongliang Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.03171v3", "summary": "Localization using multiple base stations (BSs) has gained much attention for\nits advantage in localization accuracy. However, the performance of the\nmulti-BS system suffers from its limited number of antennas. To solve the above\nissue, we propose to use reconfigurable intelligent surfaces (RIS) serving as\nantennas. Existing localization methods enabled by multiple RISs mainly focus\non the far-field (FF) region of each RIS. As the scale of RIS increases, the\nnear-field (NF) region of each RIS expands, where FF methods struggle to\nachieve high localization accuracy. In this letter, a hybrid NF and FF\nlocalization method aided by multiple RISs is proposed. In such scenarios,\nachieving user localization and RIS optimization becomes challenging due to the\nhigh complexity caused by the exhaustive search through all candidate locations\nto match the signals. Moreover, the interference from multiple RISs degrades\nthe localization accuracy. To address this challenge, we propose a two-phase\nlocalization method that first estimates the relative locations of the user to\neach RIS and fuses the results to obtain the estimation. This approach reduces\nthe complexity by decreasing the number of candidate locations considered in\neach step. Also, we introduce a constraint in the RIS optimization problem that\nlimits the sidelobe levels directed towards other RISs, effectively minimizing\ninter-RIS interference. The effectiveness of the proposed method is verified\nthrough simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.03171v3", "cate": "eess.SP", "date": "2025-02-05", "updated": "2025-07-22"}
{"id": "2311.10118", "title": "Now and Future of Artificial Intelligence-based Signet Ring Cell Diagnosis: A Survey", "authors": ["Zhu Meng", "Junhao Dong", "Limei Guo", "Fei Su", "Jiaxuan Liu", "Guangxi Wang", "Zhicheng Zhao"], "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2311.10118v2", "summary": "Signet ring cells (SRCs), associated with a high propensity for peripheral\nmetastasis and poor prognosis, critically influence surgical decision-making\nand outcome prediction. However, their detection remains challenging even for\nexperienced pathologists. While artificial intelligence (AI)-based automated\nSRC diagnosis has gained increasing attention for its potential to enhance\ndiagnostic efficiency and accuracy, existing methodologies lack systematic\nreview. This gap impedes the assessment of disparities between algorithmic\ncapabilities and clinical applicability. This paper presents a comprehensive\nsurvey of AI-driven SRC analysis from 2008 through June 2025. We systematically\nsummarize the biological characteristics of SRCs and challenges in their\nautomated identification. Representative algorithms are analyzed and\ncategorized as unimodal or multi-modal approaches. Unimodal algorithms,\nencompassing image, omics, and text data, are reviewed; image-based ones are\nfurther subdivided into classification, detection, segmentation, and foundation\nmodel tasks. Multi-modal algorithms integrate two or more data modalities\n(images, omics, and text). Finally, by evaluating current methodological\nperformance against clinical assistance requirements, we discuss unresolved\nchallenges and future research directions in SRC analysis. This survey aims to\nassist researchers, particularly those without medical backgrounds, in\nunderstanding the landscape of SRC analysis and the prospects for intelligent\ndiagnosis, thereby accelerating the translation of computational algorithms\ninto clinical practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2311.10118v2", "cate": "eess.IV", "date": "2023-11-16", "updated": "2025-07-22"}
{"id": "2507.16724", "title": "SALM: Spatial Audio Language Model with Structured Embeddings for Understanding and Editing", "authors": ["Jinbo Hu", "Yin Cao", "Ming Wu", "Feiran Yang", "Jun Yang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      5 pages, 1 figure", "url": "http://arxiv.org/abs/2507.16724v1", "summary": "Spatial audio understanding is essential for accurately perceiving and\ninterpreting acoustic environments. However, existing audio-language models\nstruggle with processing spatial audio and perceiving spatial acoustic scenes.\nWe introduce the Spatial Audio Language Model (SALM), a novel framework that\nbridges spatial audio and language via multi-modal contrastive learning. SALM\nconsists of a text encoder and a dual-branch audio encoder, decomposing spatial\nsound into semantic and spatial components through structured audio embeddings.\nKey features of SALM include seamless alignment of spatial and text\nrepresentations, separate and joint extraction of spatial and semantic\ninformation, zero-shot direction classification and robust support for spatial\naudio editing. Experimental results demonstrate that SALM effectively captures\nand aligns cross-modal representations. Furthermore, it supports advanced\nediting capabilities, such as altering directional audio using text-based\nembeddings.", "comment": "5 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.16724v1", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16642", "title": "Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models", "authors": ["Armin Berger", "Lars Hillebrand", "David Leonhard", "Tobias Deuer", "Thiago Bell Felix de Oliveira", "Tim Dilmaghani", "Mohamed Khaled", "Bernd Kliem", "Rdiger Loitz", "Christian Bauckhage", "Rafet Sifa"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted and published at BigData 2023, 10 pages, 3 figures, 5 tables", "url": "http://arxiv.org/abs/2507.16642v1", "summary": "The auditing of financial documents, historically a labor-intensive process,\nstands on the precipice of transformation. AI-driven solutions have made\ninroads into streamlining this process by recommending pertinent text passages\nfrom financial reports to align with the legal requirements of accounting\nstandards. However, a glaring limitation remains: these systems commonly fall\nshort in verifying if the recommended excerpts indeed comply with the specific\nlegal mandates. Hence, in this paper, we probe the efficiency of publicly\navailable Large Language Models (LLMs) in the realm of regulatory compliance\nacross different model configurations. We place particular emphasis on\ncomparing cutting-edge open-source LLMs, such as Llama-2, with their\nproprietary counterparts like OpenAI's GPT models. This comparative analysis\nleverages two custom datasets provided by our partner PricewaterhouseCoopers\n(PwC) Germany. We find that the open-source Llama-2 70 billion model\ndemonstrates outstanding performance in detecting non-compliance or true\nnegative occurrences, beating all their proprietary counterparts. Nevertheless,\nproprietary models such as GPT-4 perform the best in a broad variety of\nscenarios, particularly in non-English contexts.", "comment": "Accepted and published at BigData 2023, 10 pages, 3 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.16642v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16242", "title": "Toward a Lightweight and Robust Design for Caching with Predictions", "authors": ["Peng Chen", "Hailiang Zhao", "Jiaji Zhang", "Xueyan Tang", "Yixuan Wang", "Shuiguang Deng"], "categories": ["cs.DS", "cs.LG"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      preprint", "url": "http://arxiv.org/abs/2507.16242v1", "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\n\\textsc{Guard}, a lightweight robustification framework that enhances the\nrobustness of a broad class of learning-augmented caching algorithms to $2H_k +\n2$, while preserving their $1$-consistency. \\textsc{Guard} achieves the current\nbest-known trade-off between consistency and robustness, with only\n$\\mathcal{O}(1)$ additional per-request overhead, thereby maintaining the\noriginal time complexity of the base algorithm. Extensive experiments across\nmultiple real-world datasets and prediction models validate the effectiveness\nof \\textsc{Guard} in practice.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2507.16242v1", "cate": "cs.DS", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16518", "title": "C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning", "authors": ["Xiuwei Chen", "Wentao Hu", "Hanhui Li", "Jun Zhou", "Zisheng Chen", "Meng Cao", "Yihan Zeng", "Kui Zhang", "Yu-Jie Yuan", "Jianhua Han", "Hang Xu", "Xiaodan Liang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16518v1", "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nimpressive reasoning capabilities. However, further enhancing existing MLLMs\nnecessitates high-quality vision-language datasets with carefully curated task\ncomplexities, which are both costly and challenging to scale. Although recent\nself-improving models that iteratively refine themselves offer a feasible\nsolution, they still suffer from two core challenges: (i) most existing methods\naugment visual or textual data separately, resulting in discrepancies in data\ncomplexity (e.g., over-simplified diagrams paired with redundant textual\ndescriptions); and (ii) the evolution of data and models is also separated,\nleading to scenarios where models are exposed to tasks with mismatched\ndifficulty levels. To address these issues, we propose C2-Evo, an automatic,\nclosed-loop self-improving framework that jointly evolves both training data\nand model capabilities. Specifically, given a base dataset and a base model,\nC2-Evo enhances them by a cross-modal data evolution loop and a data-model\nevolution loop. The former loop expands the base dataset by generating complex\nmultimodal problems that combine structured textual sub-problems with\niteratively specified geometric diagrams, while the latter loop adaptively\nselects the generated problems based on the performance of the base model, to\nconduct supervised fine-tuning and reinforcement learning alternately.\nConsequently, our method continuously refines its model and training data, and\nconsistently obtains considerable performance gains across multiple\nmathematical reasoning benchmarks. Our code, models, and datasets will be\nreleased.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16518v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2411.11596", "title": "Integrating and Comparing Radiality Constraints for Optimized Distribution System Reconfiguration", "authors": ["Pablo Cortes", "Alejandra Tabares", "Fredy Franco"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.11596v2", "summary": "The reconfiguration of electrical power distribution systems is a crucial\noptimization problem aimed at minimizing power losses by altering the system\ntopology through the operation of interconnection switches. This problem,\ntypically modelled as a mixed integer nonlinear program demands high\ncomputational resources for large scale networks and requires specialized\nradiality constraints for maintaining the tree like structure of distribution\nnetworks. This paper presents a comprehensive analysis that integrates and\ncompares the computational burden associated with different radiality\nconstraint formulations proposed in the specialized literature for the\nreconfiguration of distribution systems. By using consistent hardware and\nsoftware setups, we evaluate the performance of these constraints across\nseveral well known test cases. Our findings reveal significant differences in\ncomputational efficiency depending on the chosen set of radiality constraints,\nproviding valuable insights for optimizing reconfiguration strategies in\npractical distribution networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.11596v2", "cate": "eess.SY", "date": "2024-11-18", "updated": "2025-07-22"}
{"id": "2502.06478", "title": "Retrieving Filter Spectra in CNN for Explainable Sleep Stage Classification", "authors": ["Stephan Goerttler", "Yucheng Wang", "Fei He", "Min Wu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, conference paper", "url": "http://arxiv.org/abs/2502.06478v2", "summary": "Despite significant advances in deep learning-based sleep stage\nclassification, the clinical adoption of automatic classification models\nremains slow. One key challenge is the lack of explainability, as many models\nfunction as black boxes with millions of parameters. In response, recent work\nhas increasingly focussed on enhancing model explainability. This study\ncontributes to these efforts by globally explaining spectral processing of\nindividual EEG channels. Specifically, we introduce a method to retrieve the\nfilter spectrum of low-level convolutional feature extraction and compare it\nwith the classification-relevant spectral information in the data. We evaluate\nour approach on the EEGNet and MSA-CNN models using the ISRUC-S3 and\nSleep-EDF-20 datasets. Our findings show that spectral processing plays a\nsignificant role in the lower frequency bands. In addition, comparing the\ncorrelation between filter spectrum and data-derived spectral information with\nunivariate performance indicates that the model naturally prioritises the most\ninformative channels in a multimodal setting. We specify how these insights can\nbe leveraged to enhance model performance. The code for the filter spectrum\nretrieval and its analysis is available at\nhttps://github.com/sgoerttler/MSA-CNN.", "comment": "6 pages, 3 figures, conference paper", "pdf_url": "http://arxiv.org/pdf/2502.06478v2", "cate": "eess.SP", "date": "2025-02-10", "updated": "2025-07-22"}
{"id": "2401.13616", "title": "FLLIC: Functionally Lossless Image Compression", "authors": ["Xi Zhang", "Xiaolin Wu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.13616v4", "summary": "Recently, DNN models for lossless image coding have surpassed their\ntraditional counterparts in compression performance, reducing the previous\nlossless bit rate by about ten percent for natural color images. But even with\nthese advances, mathematically lossless image compression (MLLIC) ratios for\nnatural images still fall short of the bandwidth and cost-effectiveness\nrequirements of most practical imaging and vision systems at present and\nbeyond. To overcome the performance barrier of MLLIC, we question the very\nnecessity of MLLIC. Considering that all digital imaging sensors suffer from\nacquisition noises, why should we insist on mathematically lossless coding,\ni.e., wasting bits to preserve noises? Instead, we propose a new paradigm of\njoint denoising and compression called functionally lossless image compression\n(FLLIC), which performs lossless compression of optimally denoised images (the\noptimality may be task-specific). Although not literally lossless with respect\nto the noisy input, FLLIC aims to achieve the best possible reconstruction of\nthe latent noise-free original image. Extensive experiments show that FLLIC\nachieves state-of-the-art performance in joint denoising and compression of\nnoisy images and does so at a lower computational cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.13616v4", "cate": "eess.IV", "date": "2024-01-24", "updated": "2025-07-22"}
{"id": "2504.20630", "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Tao Jin", "Zhou Zhao"], "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia 2025", "url": "http://arxiv.org/abs/2504.20630v4", "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos are\navailable at https://aaronz345.github.io/ISDramaDemo. We provide the dataset\nand the evaluation code at https://huggingface.co/datasets/AaronZ345/MRSDrama\nand https://github.com/AaronZ345/ISDrama.", "comment": "Accepted by ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2504.20630v4", "cate": "eess.AS", "date": "2025-04-29", "updated": "2025-07-22"}
{"id": "2507.16663", "title": "Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs", "authors": ["Yujin Han", "Hao Chen", "Andi Han", "Zhiheng Wang", "Xinyu Lin", "Yingya Zhang", "Shiwei Zhang", "Difan Zou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      19 pages, 9 figures, 3 tables", "url": "http://arxiv.org/abs/2507.16663v1", "summary": "Despite efforts to unify multimodal generation and understanding tasks in a\nsingle model, we show these MLLMs exhibit self-contradiction where generation\nproduces images deemed misaligned with input prompts based on the model's own\nunderstanding. We define a Nonunified score that quantifies such\nself-contradiction. Our empirical results reveal that the self-contradiction\nmainly arises from weak generation that fails to align with prompts, rather\nthan misunderstanding. This capability asymmetry indicates the potential of\nleveraging self-contradiction for self-improvement, where the stronger model\nunderstanding guides the weaker generation to mitigate the\ngeneration-understanding gap. Applying standard post-training methods (e.g.,\nSFT, DPO) with such internal supervision successfully improves both generation\nand unification. We discover a co-improvement effect on both generation and\nunderstanding when only fine-tuning the generation branch, a phenomenon known\nin pre-training but underexplored in post-training. Our analysis shows\nimprovements stem from better detection of false positives that are previously\nincorrectly identified as prompt-aligned. Theoretically, we show the aligned\ntraining dynamics between generation and understanding allow reduced\nprompt-misaligned generations to also improve mismatch detection in the\nunderstanding branch. Additionally, the framework reveals a potential risk of\nco-degradation under poor supervision-an overlooked phenomenon that is\nempirically validated in our experiments. Notably, we find intrinsic metrics\nlike Nonunified score cannot distinguish co-degradation from co-improvement,\nwhich highlights the necessity of data quality check. Finally, we propose a\ncurriculum-based strategy based on our findings that gradually introduces\nharder samples as the model improves, leading to better unification and\nimproved MLLM generation and understanding.", "comment": "19 pages, 9 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.16663v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16289", "title": "Time to Split: Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders", "authors": ["Danil Gusak", "Anna Volodkevich", "Anton Klenitskiy", "Alexey Vasilev", "Evgeny Frolov"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted for ACM RecSys 2025. Author's version. The final published version will be available at the ACM Digital Library", "url": "http://arxiv.org/abs/2507.16289v1", "summary": "Modern sequential recommender systems, ranging from lightweight\ntransformer-based variants to large language models, have become increasingly\nprominent in academia and industry due to their strong performance in the\nnext-item prediction task. Yet common evaluation protocols for sequential\nrecommendations remain insufficiently developed: they often fail to reflect the\ncorresponding recommendation task accurately, or are not aligned with\nreal-world scenarios.\n  Although the widely used leave-one-out split matches next-item prediction, it\npermits the overlap between training and test periods, which leads to temporal\nleakage and unrealistically long test horizon, limiting real-world relevance.\nGlobal temporal splitting addresses these issues by evaluating on distinct\nfuture periods. However, its applications to sequential recommendations remain\nloosely defined, particularly in terms of selecting target interactions and\nconstructing a validation subset that provides necessary consistency between\nvalidation and test metrics.\n  In this paper, we demonstrate that evaluation outcomes can vary significantly\nacross splitting strategies, influencing model rankings and practical\ndeployment decisions. To improve reproducibility in both academic and\nindustrial settings, we systematically compare different splitting strategies\nfor sequential recommendations across multiple datasets and established\nbaselines. Our findings show that prevalent splits, such as leave-one-out, may\nbe insufficiently aligned with more realistic evaluation strategies. Code:\nhttps://github.com/monkey0head/time-to-split", "comment": "Accepted for ACM RecSys 2025. Author's version. The final published\n  version will be available at the ACM Digital Library", "pdf_url": "http://arxiv.org/pdf/2507.16289v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16559", "title": "Comparative validation of surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation in endoscopy: Results of the PhaKIR 2024 challenge", "authors": ["Tobias Rueckert", "David Rauber", "Raphaela Maerkl", "Leonard Klausmann", "Suemeyye R. Yildiran", "Max Gutbrod", "Danilo Weber Nunes", "Alvaro Fernandez Moreno", "Imanol Luengo", "Danail Stoyanov", "Nicolas Toussaint", "Enki Cho", "Hyeon Bae Kim", "Oh Sung Choo", "Ka Young Kim", "Seong Tae Kim", "Gonalo Arantes", "Kehan Song", "Jianjun Zhu", "Junchen Xiong", "Tingyi Lin", "Shunsuke Kikuchi", "Hiroki Matsuzaki", "Atsushi Kouno", "Joo Renato Ribeiro Manesco", "Joo Paulo Papa", "Tae-Min Choi", "Tae Kyeong Jeong", "Juyoun Park", "Oluwatosin Alabi", "Meng Wei", "Tom Vercauteren", "Runzhi Wu", "Mengya Xu", "An Wang", "Long Bai", "Hongliang Ren", "Amine Yamlahi", "Jakob Hennighausen", "Lena Maier-Hein", "Satoshi Kondo", "Satoshi Kasai", "Kousuke Hirasawa", "Shu Yang", "Yihui Wang", "Hao Chen", "Santiago Rodrguez", "Nicols Aparicio", "Leonardo Manrique", "Juan Camilo Lyons", "Olivia Hosie", "Nicols Ayobi", "Pablo Arbelez", "Yiping Li", "Yasmina Al Khalil", "Sahar Nasirihaghighi", "Stefanie Speidel", "Daniel Rueckert", "Hubertus Feussner", "Dirk Wilhelm", "Christoph Palm"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      A challenge report pre-print containing 36 pages, 15 figures, and 13 tables", "url": "http://arxiv.org/abs/2507.16559v1", "summary": "Reliable recognition and localization of surgical instruments in endoscopic\nvideo recordings are foundational for a wide range of applications in computer-\nand robot-assisted minimally invasive surgery (RAMIS), including surgical\ntraining, skill assessment, and autonomous assistance. However, robust\nperformance under real-world conditions remains a significant challenge.\nIncorporating surgical context - such as the current procedural phase - has\nemerged as a promising strategy to improve robustness and interpretability.\n  To address these challenges, we organized the Surgical Procedure Phase,\nKeypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the\nEndoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel,\nmulti-center dataset comprising thirteen full-length laparoscopic\ncholecystectomy videos collected from three distinct medical institutions, with\nunified annotations for three interrelated tasks: surgical phase recognition,\ninstrument keypoint estimation, and instrument instance segmentation. Unlike\nexisting datasets, ours enables joint investigation of instrument localization\nand procedural context within the same data while supporting the integration of\ntemporal information across entire procedures.\n  We report results and findings in accordance with the BIAS guidelines for\nbiomedical image analysis challenges. The PhaKIR sub-challenge advances the\nfield by providing a unique benchmark for developing temporally aware,\ncontext-driven methods in RAMIS and offers a high-quality resource to support\nfuture research in surgical scene understanding.", "comment": "A challenge report pre-print containing 36 pages, 15 figures, and 13\n  tables", "pdf_url": "http://arxiv.org/pdf/2507.16559v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.01282", "title": "Online Learning of Nonlinear Parametric Models under Non-smooth Regularization using EKF and ADMM", "authors": ["Lapo Frascati", "Alberto Bemporad"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      12 pages, 5 figures", "url": "http://arxiv.org/abs/2503.01282v3", "summary": "This paper proposes a novel combination of extended Kalman filtering (EKF)\nwith the alternating direction method of multipliers (ADMM) for learning\nparametric nonlinear models online under non-smooth regularization terms,\nincluding l1 and l0 penalties and bound constraints on model parameters. For\nthe case of linear time-varying models and non-smoothconvex regularization\nterms, we provide a sublinear regret bound that ensures the proper behavior of\nthe online learning strategy. The approach is computationally efficient for a\nwide range of regularization terms, which makes it appealing for its use in\nembedded control applications for online model adaptation. We show the\nperformance of the proposed method in three simulation examples, highlighting\nits effectiveness compared to other batch and online algorithms.", "comment": "12 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2503.01282v3", "cate": "eess.SY", "date": "2025-03-03", "updated": "2025-07-22"}
{"id": "2506.11815", "title": "Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection", "authors": ["Tae-Seong Han", "Jae-Wook Heo", "Hakseung Kim", "Cheol-Hui Lee", "Hyub Huh", "Eue-Keun Choi", "Hye Jin Kim", "Dong-Joo Kim"], "categories": ["eess.SP", "cs.AI", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      This manuscript contains 17 pages, 10 figures, and 3 tables", "url": "http://arxiv.org/abs/2506.11815v2", "summary": "Electrocardiography (ECG) signals are frequently degraded by noise, limiting\ntheir clinical reliability in both conventional and wearable settings. Existing\nmethods for addressing ECG noise, relying on artifact classification or\ndenoising, are constrained by annotation inconsistencies and poor\ngeneralizability. Here, we address these limitations by reframing ECG noise\nquantification as an anomaly detection task. We propose a diffusion-based\nframework trained to model the normative distribution of clean ECG signals,\nidentifying deviations as noise without requiring explicit artifact labels. To\nrobustly evaluate performance and mitigate label inconsistencies, we introduce\na distribution-based metric using the Wasserstein-1 distance ($W_1$). Our model\nachieved a macro-average $W_1$ score of 1.308, outperforming the next-best\nmethod by over 48\\%. External validation confirmed strong generalizability,\nfacilitating the exclusion of noisy segments to improve diagnostic accuracy and\nsupport timely clinical intervention. This approach enhances real-time ECG\nmonitoring and broadens ECG applicability in digital health technologies.", "comment": "This manuscript contains 17 pages, 10 figures, and 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.11815v2", "cate": "eess.SP", "date": "2025-06-13", "updated": "2025-07-22"}
{"id": "2406.13413", "title": "Recurrent Inference Machine for Medical Image Registration", "authors": ["Yi Zhang", "Yidong Zhao", "Hui Xue", "Peter Kellman", "Stefan Klein", "Qian Tao"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Preprint version. Accepted by Medical Image Analysis", "url": "http://arxiv.org/abs/2406.13413v2", "summary": "Image registration is essential for medical image applications where\nalignment of voxels across multiple images is needed for qualitative or\nquantitative analysis. With recent advancements in deep neural networks and\nparallel computing, deep learning-based medical image registration methods\nbecome competitive with their flexible modelling and fast inference\ncapabilities. However, compared to traditional optimization-based registration\nmethods, the speed advantage may come at the cost of registration performance\nat inference time. Besides, deep neural networks ideally demand large training\ndatasets while optimization-based methods are training-free. To improve\nregistration accuracy and data efficiency, we propose a novel image\nregistration method, termed Recurrent Inference Image Registration (RIIR)\nnetwork. RIIR is formulated as a meta-learning solver to the registration\nproblem in an iterative manner. RIIR addresses the accuracy and data efficiency\nissues, by learning the update rule of optimization, with implicit\nregularization combined with explicit gradient input.\n  We evaluated RIIR extensively on brain MRI and quantitative cardiac MRI\ndatasets, in terms of both registration accuracy and training data efficiency.\nOur experiments showed that RIIR outperformed a range of deep learning-based\nmethods, even with only $5\\%$ of the training data, demonstrating high data\nefficiency. Key findings from our ablation studies highlighted the important\nadded value of the hidden states introduced in the recurrent inference\nframework for meta-learning. Our proposed RIIR offers a highly data-efficient\nframework for deep learning-based medical image registration.", "comment": "Preprint version. Accepted by Medical Image Analysis", "pdf_url": "http://arxiv.org/pdf/2406.13413v2", "cate": "eess.IV", "date": "2024-06-19", "updated": "2025-07-22"}
{"id": "2505.13029", "title": "MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for Speech Enhancement", "authors": ["Nan Xu", "Zhaolong Huang", "Xiaonan Zhi"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures; Accepted by Interspeech 2025", "url": "http://arxiv.org/abs/2505.13029v3", "summary": "With the development of deep learning, speech enhancement has been greatly\noptimized in terms of speech quality. Previous methods typically focus on the\ndiscriminative supervised learning or generative modeling, which tends to\nintroduce speech distortions or high computational cost. In this paper, we\npropose MDDM, a Multi-view Discriminative enhanced Diffusion-based Model.\nSpecifically, we take the features of three domains (time, frequency and noise)\nas inputs of a discriminative prediction network, generating the preliminary\nspectrogram. Then, the discriminative output can be converted to clean speech\nby several inference sampling steps. Due to the intersection of the\ndistributions between discriminative output and clean target, the smaller\nsampling steps can achieve the competitive performance compared to other\ndiffusion-based methods. Experiments conducted on a public dataset and a\nrealworld dataset validate the effectiveness of MDDM, either on subjective or\nobjective metric.", "comment": "5 pages, 2 figures; Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2505.13029v3", "cate": "eess.AS", "date": "2025-05-19", "updated": "2025-07-22"}
{"id": "2507.16031", "title": "Online Combinatorial Optimization with Graphical Dependencies", "authors": ["Zhimeng Gao", "Evangelia Gergatsouli", "Kalen Patton", "Sahil Singla"], "categories": ["cs.DS", "F.2.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16031v1", "summary": "Most existing work in online stochastic combinatorial optimization assumes\nthat inputs are drawn from independent distributions -- a strong assumption\nthat often fails in practice. At the other extreme, arbitrary correlations are\nequivalent to worst-case inputs via Yao's minimax principle, making good\nalgorithms often impossible. This motivates the study of intermediate models\nthat capture mild correlations while still permitting non-trivial algorithms.\n  In this paper, we study online combinatorial optimization under Markov Random\nFields (MRFs), a well-established graphical model for structured dependencies.\nMRFs parameterize correlation strength via the maximum weighted degree\n$\\Delta$, smoothly interpolating between independence ($\\Delta = 0$) and full\ncorrelation ($\\Delta \\to \\infty$). While na\\\"ively this yields\n$e^{O(\\Delta)}$-competitive algorithms and $\\Omega(\\Delta)$ hardness, we ask:\nwhen can we design tight $\\Theta(\\Delta)$-competitive algorithms?\n  We present general techniques achieving $O(\\Delta)$-competitive algorithms\nfor both minimization and maximization problems under MRF-distributed inputs.\nFor minimization problems with coverage constraints (e.g., Facility Location\nand Steiner Tree), we reduce to the well-studied $p$-sample model. For\nmaximization problems (e.g., matchings and combinatorial auctions with XOS\nbuyers), we extend the \"balanced prices\" framework for online allocation\nproblems to MRFs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16031v1", "cate": "cs.DS", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16695", "title": "Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM", "authors": ["Lars Hillebrand", "David Biesner", "Christian Bauckhage", "Rafet Sifa"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted and published at CD-MAKE 2020, 20 pages, 8 tables, 8 figures", "url": "http://arxiv.org/abs/2507.16695v1", "summary": "The DEDICOM algorithm provides a uniquely interpretable matrix factorization\nmethod for symmetric and asymmetric square matrices. We employ a new\nrow-stochastic variation of DEDICOM on the pointwise mutual information\nmatrices of text corpora to identify latent topic clusters within the\nvocabulary and simultaneously learn interpretable word embeddings. We introduce\na method to efficiently train a constrained DEDICOM algorithm and a qualitative\nevaluation of its topic modeling and word embedding performance.", "comment": "Accepted and published at CD-MAKE 2020, 20 pages, 8 tables, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.16695v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16336", "title": "Constructing material network representations for intelligent amorphous alloys design", "authors": ["S. -Y. Zhang", "J. Tian", "S. -L. Liu", "H. -M. Zhang", "H. -Y. Bai", "Y. -C. Hu", "W. -H. Wang"], "categories": ["cond-mat.mtrl-sci", "cond-mat.dis-nn", "cs.CC", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "Comments:      5 figures", "url": "http://arxiv.org/abs/2507.16336v1", "summary": "Designing high-performance amorphous alloys is demanding for various\napplications. But this process intensively relies on empirical laws and\nunlimited attempts. The high-cost and low-efficiency nature of the traditional\nstrategies prevents effective sampling in the enormous material space. Here, we\npropose material networks to accelerate the discovery of binary and ternary\namorphous alloys. The network topologies reveal hidden material candidates that\nwere obscured by traditional tabular data representations. By scrutinizing the\namorphous alloys synthesized in different years, we construct dynamical\nmaterial networks to track the history of the alloy discovery. We find that\nsome innovative materials designed in the past were encoded in the networks,\ndemonstrating their predictive power in guiding new alloy design. These\nmaterial networks show physical similarities with several real-world networks\nin our daily lives. Our findings pave a new way for intelligent materials\ndesign, especially for complex alloys.", "comment": "5 figures", "pdf_url": "http://arxiv.org/pdf/2507.16336v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16596", "title": "A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization", "authors": ["Wenbo Xu", "Junyan Wu", "Wei Lu", "Xiangyang Luo", "Qian Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures,conference", "url": "http://arxiv.org/abs/2507.16596v1", "summary": "Current researches on Deepfake forensics often treat detection as a\nclassification task or temporal forgery localization problem, which are usually\nrestrictive, time-consuming, and challenging to scale for large datasets. To\nresolve these issues, we present a multimodal deviation perceiving framework\nfor weakly-supervised temporal forgery localization (MDP), which aims to\nidentify temporal partial forged segments using only video-level annotations.\nThe MDP proposes a novel multimodal interaction mechanism (MI) and an\nextensible deviation perceiving loss to perceive multimodal deviation, which\nachieves the refined start and end timestamps localization of forged segments.\nSpecifically, MI introduces a temporal property preserving cross-modal\nattention to measure the relevance between the visual and audio modalities in\nthe probabilistic embedding space. It could identify the inter-modality\ndeviation and construct comprehensive video features for temporal forgery\nlocalization. To explore further temporal deviation for weakly-supervised\nlearning, an extensible deviation perceiving loss has been proposed, aiming at\nenlarging the deviation of adjacent segments of the forged samples and reducing\nthat of genuine samples. Extensive experiments demonstrate the effectiveness of\nthe proposed framework and achieve comparable results to fully-supervised\napproaches in several evaluation metrics.", "comment": "9 pages, 3 figures,conference", "pdf_url": "http://arxiv.org/pdf/2507.16596v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2504.06439", "title": "Graph Neural Network-Based Distributed Optimal Control for Linear Networked Systems: An Online Distributed Training Approach", "authors": ["Zihao Song", "Shirantha Welikala", "Panos J. Antsaklis", "Hai Lin"], "categories": ["eess.SY", "cs.LG", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures", "url": "http://arxiv.org/abs/2504.06439v2", "summary": "In this paper, we consider the distributed optimal control problem for\ndiscrete-time linear networked systems. In particular, we are interested in\nlearning distributed optimal controllers using graph recurrent neural networks\n(GRNNs). Most of the existing approaches result in centralized optimal\ncontrollers with offline training processes. However, as the increasing demand\nof network resilience, the optimal controllers are further expected to be\ndistributed, and are desirable to be trained in an online distributed fashion,\nwhich are also the main contributions of our work. To solve this problem, we\nfirst propose a GRNN-based distributed optimal control method, and we cast the\nproblem as a self-supervised learning problem. Then, the distributed online\ntraining is achieved via distributed gradient computation, and inspired by the\n(consensus-based) distributed optimization idea, a distributed online training\noptimizer is designed. Furthermore, the local closed-loop stability of the\nlinear networked system under our proposed GRNN-based controller is provided by\nassuming that the nonlinear activation function of the GRNN-based controller is\nboth local sector-bounded and slope-restricted. The effectiveness of our\nproposed method is illustrated by numerical simulations using a specifically\ndeveloped simulator.", "comment": "9 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2504.06439v2", "cate": "eess.SY", "date": "2025-04-08", "updated": "2025-07-22"}
{"id": "2507.14167", "title": "Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization", "authors": ["Lucas Heublein", "Christian Wielenberg", "Thorsten Nowak", "Tobias Feigl", "Christopher Mutschler", "Felix Ott"], "categories": ["eess.SP", "cs.IR", "cs.LG", "62H05, 65-11, 94-11", "E.0; H.1.1; I.2.6; I.5.4"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 10 figures", "url": "http://arxiv.org/abs/2507.14167v2", "summary": "Jamming devices disrupt signals from the global navigation satellite system\n(GNSS) and pose a significant threat by compromising the reliability of\naccurate positioning. Consequently, the detection and localization of these\ninterference signals are essential to achieve situational awareness, mitigating\ntheir impact, and implementing effective counter-measures. Classical Angle of\nArrival (AoA) methods exhibit reduced accuracy in multipath environments due to\nsignal reflections and scattering, leading to localization errors.\nAdditionally, AoA-based techniques demand substantial computational resources\nfor array signal processing. In this paper, we propose a novel approach for\ndetecting and classifying interference while estimating the distance, azimuth,\nand elevation of jamming sources. Our benchmark study evaluates 128 vision\nencoder and time-series models to identify the highest-performing methods for\neach task. We introduce an attention-based fusion framework that integrates\nin-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed\nspectrograms while incorporating 22 AoA features to enhance localization\naccuracy. Furthermore, we present a novel dataset of moving jamming devices\nrecorded in an indoor environment with dynamic multipath conditions and\ndemonstrate superior performance compared to state-of-the-art methods.", "comment": "6 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.14167v2", "cate": "eess.SP", "date": "2025-07-09", "updated": "2025-07-22"}
{"id": "2408.07114", "title": "Investigation of unsupervised and supervised hyperspectral anomaly detection", "authors": ["Mazharul Hossain", "Aaron Robinson", "Lan Wang", "Chrysanthe Preza"], "categories": ["eess.IV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Published in Proceedings Volume 13138, Applications of Machine Learning 2024; 1313817 (2024). Event: Optical Engineering + Applications, 2024, San Diego, California, United States", "url": "http://arxiv.org/abs/2408.07114v2", "summary": "Hyperspectral sensing is a valuable tool for detecting anomalies and\ndistinguishing between materials in a scene. Hyperspectral anomaly detection\n(HS-AD) helps characterize the captured scenes and separates them into anomaly\nand background classes. It is vital in agriculture, environment, and military\napplications such as RSTA (reconnaissance, surveillance, and target\nacquisition) missions. We previously designed an equal voting ensemble of\nhyperspectral unmixing and three unsupervised HS-AD algorithms. We later\nutilized a supervised classifier to determine the weights of a voting ensemble,\ncreating a hybrid of heterogeneous unsupervised HS-AD algorithms with a\nsupervised classifier in a model stacking, which improved detection accuracy.\nHowever, supervised classification methods usually fail to detect novel or\nunknown patterns that substantially deviate from those seen previously. In this\nwork, we evaluate our technique and other supervised and unsupervised methods\nusing general hyperspectral data to provide new insights.", "comment": "Published in Proceedings Volume 13138, Applications of Machine\n  Learning 2024; 1313817 (2024). Event: Optical Engineering + Applications,\n  2024, San Diego, California, United States", "pdf_url": "http://arxiv.org/pdf/2408.07114v2", "cate": "eess.IV", "date": "2024-08-13", "updated": "2025-07-21"}
{"id": "2505.17023", "title": "ReMi: A Random Recurrent Neural Network Approach to Music Production", "authors": ["Hugo Chateau-Laurent", "Tara Vanhatalo", "Wei-Tung Pan", "Xavier Hinaut"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted for an Innovation Showcase Demo at International Computer Music Conference", "url": "http://arxiv.org/abs/2505.17023v2", "summary": "Generative artificial intelligence raises concerns related to energy\nconsumption, copyright infringement and creative atrophy. We show that randomly\ninitialized recurrent neural networks can produce arpeggios and low-frequency\noscillations that are rich and configurable. In contrast to end-to-end music\ngeneration that aims to replace musicians, our approach expands their\ncreativity while requiring no data and much less computational power. More\ninformation can be found at: https://allendia.com/", "comment": "Accepted for an Innovation Showcase Demo at International Computer\n  Music Conference", "pdf_url": "http://arxiv.org/pdf/2505.17023v2", "cate": "cs.SD", "date": "2025-04-02", "updated": "2025-07-22"}
{"id": "2507.16096", "title": "Online Joint Replenishment Problem with Arbitrary Holding and Backlog Costs", "authors": ["Yossi Azar", "Shahar Lewkowicz"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16096v1", "summary": "In their seminal paper Moseley, Niaparast, and Ravi introduced the Joint\nReplenishment Problem (JRP) with holding and backlog costs that models the\ntrade-off between ordering costs, holding costs, and backlog costs in supply\nchain planning systems. Their model generalized the classical the make-to-order\nversion as well make-to-stock version. For the case where holding costs\nfunction of all items are the same and all backlog costs are the same, they\nprovide a constant competitive algorithm, leaving designing a constant\ncompetitive algorithm for arbitrary functions open. Moreover, they noticed that\ntheir algorithm does not work for arbitrary (request dependent) holding costs\nand backlog costs functions. We resolve their open problem and design a\nconstant competitive algorithm that works for arbitrary request dependent\nfunctions. Specifically, we establish a 4-competitive algorithm for the\nsingle-item case and a 16-competitive for the general (multi-item) version. The\nalgorithm of Moseley, Niaparast, and Ravi is based on fixed priority on the\nrequests to items, and request to an item are always served by order of\ndeadlines. In contrast, we design an algorithm with dynamic priority over the\nrequests such that instead of servicing a prefix by deadline of requests, we\nmay need to service a general subset of the requests.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16096v1", "cate": "cs.DS", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16711", "title": "Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance", "authors": ["Lars Hillebrand", "Armin Berger", "Daniel Uedelhoven", "David Berghaus", "Ulrich Warning", "Tim Dilmaghani", "Bernd Kliem", "Thomas Schmid", "Rdiger Loitz", "Rafet Sifa"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted and published at BigData 2024, 3 pages, 3 tables, 2 figures", "url": "http://arxiv.org/abs/2507.16711v1", "summary": "Risk and Quality (R&Q) assurance in highly regulated industries requires\nconstant navigation of complex regulatory frameworks, with employees handling\nnumerous daily queries demanding accurate policy interpretation. Traditional\nmethods relying on specialized experts create operational bottlenecks and limit\nscalability. We present a novel Retrieval Augmented Generation (RAG) system\nleveraging Large Language Models (LLMs), hybrid search and relevance boosting\nto enhance R&Q query processing. Evaluated on 124 expert-annotated real-world\nqueries, our actively deployed system demonstrates substantial improvements\nover traditional RAG approaches. Additionally, we perform an extensive\nhyperparameter analysis to compare and evaluate multiple configuration setups,\ndelivering valuable insights to practitioners.", "comment": "Accepted and published at BigData 2024, 3 pages, 3 tables, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.16711v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16373", "title": "Meta-learning of Gibbs states for many-body Hamiltonians with applications to Quantum Boltzmann Machines", "authors": ["Ruchira V Bhat", "Rahul Bhowmick", "Avinash Singh", "Krishna Kumar Sabapathy"], "categories": ["quant-ph", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      20 pages, 14 figures, 3 tables, 3 algorithms", "url": "http://arxiv.org/abs/2507.16373v1", "summary": "The preparation of quantum Gibbs states is a fundamental challenge in quantum\ncomputing, essential for applications ranging from modeling open quantum\nsystems to quantum machine learning. Building on the Meta-Variational Quantum\nEigensolver framework proposed by Cervera-Lierta et al.(2021) and a problem\ndriven ansatz design, we introduce two meta-learning algorithms:\nMeta-Variational Quantum Thermalizer (Meta-VQT) and Neural Network Meta-VQT\n(NN-Meta VQT) for efficient thermal state preparation of parametrized\nHamiltonians on Noisy Intermediate-Scale Quantum (NISQ) devices. Meta-VQT\nutilizes a fully quantum ansatz, while NN Meta-VQT integrates a quantum\nclassical hybrid architecture. Both leverage collective optimization over\ntraining sets to generalize Gibbs state preparation to unseen parameters. We\nvalidate our methods on upto 8-qubit Transverse Field Ising Model and the\n2-qubit Heisenberg model with all field terms, demonstrating efficient thermal\nstate generation beyond training data. For larger systems, we show that our\nmeta-learned parameters when combined with appropriately designed ansatz serve\nas warm start initializations, significantly outperforming random\ninitializations in the optimization tasks. Furthermore, a 3- qubit Kitaev ring\nexample showcases our algorithm's effectiveness across finite-temperature\ncrossover regimes. Finally, we apply our algorithms to train a Quantum\nBoltzmann Machine (QBM) on a 2-qubit Heisenberg model with all field terms,\nachieving enhanced training efficiency, improved Gibbs state accuracy, and a\n30-fold runtime speedup over existing techniques such as variational quantum\nimaginary time (VarQITE)-based QBM highlighting the scalability and\npracticality of meta-algorithm-based QBMs.", "comment": "20 pages, 14 figures, 3 tables, 3 algorithms", "pdf_url": "http://arxiv.org/pdf/2507.16373v1", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16608", "title": "Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian Representation", "authors": ["Xueming Fu", "Pei Wu", "Yingtai Li", "Xin Luo", "Zihang Jiang", "Junhao Mei", "Jian Lu", "Gao-Jun Teng", "S. Kevin Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025", "url": "http://arxiv.org/abs/2507.16608v1", "summary": "Accurate analysis of cardiac motion is crucial for evaluating cardiac\nfunction. While dynamic cardiac magnetic resonance imaging (CMR) can capture\ndetailed tissue motion throughout the cardiac cycle, the fine-grained 4D\ncardiac motion tracking remains challenging due to the homogeneous nature of\nmyocardial tissue and the lack of distinctive features. Existing approaches can\nbe broadly categorized into image based and representation-based, each with its\nlimitations. Image-based methods, including both raditional and deep\nlearning-based registration approaches, either struggle with topological\nconsistency or rely heavily on extensive training data. Representation-based\nmethods, while promising, often suffer from loss of image-level details. To\naddress these limitations, we propose Dynamic 3D Gaussian Representation\n(Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation\nwith implicit neural motion field modeling. Our method simultaneously optimizes\ncardiac structure and motion in a self-supervised manner, eliminating the need\nfor extensive training data or point-to-point correspondences. Through\ndifferentiable volumetric rendering, Dyna3DGR efficiently bridges continuous\nmotion representation with image-space alignment while preserving both\ntopological and temporal consistency. Comprehensive evaluations on the ACDC\ndataset demonstrate that our approach surpasses state-of-the-art deep\nlearning-based diffeomorphic registration methods in tracking accuracy. The\ncode will be available in https://github.com/windrise/Dyna3DGR.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.16608v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.05171", "title": "Towards provable probabilistic safety for scalable embodied AI systems", "authors": ["Linxuan He", "Qing-Shan Jia", "Ang Li", "Hongyan Sang", "Ling Wang", "Jiwen Lu", "Tao Zhang", "Jie Zhou", "Yi Zhang", "Yisen Wang", "Peng Wei", "Zhongyuan Wang", "Henry X. Liu", "Shuo Feng"], "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05171v2", "summary": "Embodied AI systems, comprising AI models and physical plants, are\nincreasingly prevalent across various applications. Due to the rarity of system\nfailures, ensuring their safety in complex operating environments remains a\nmajor challenge, which severely hinders their large-scale deployment in\nsafety-critical domains, such as autonomous vehicles, medical devices, and\nrobotics. While achieving provable deterministic safety--verifying system\nsafety across all possible scenarios--remains theoretically ideal, the rarity\nand complexity of corner cases make this approach impractical for scalable\nembodied AI systems. Instead, empirical safety evaluation is employed as an\nalternative, but the absence of provable guarantees imposes significant\nlimitations. To address these issues, we argue for a paradigm shift to provable\nprobabilistic safety that integrates provable guarantees with progressive\nachievement toward a probabilistic safety boundary on overall system\nperformance. The new paradigm better leverages statistical methods to enhance\nfeasibility and scalability, and a well-defined probabilistic safety boundary\nenables embodied AI systems to be deployed at scale. In this Perspective, we\noutline a roadmap for provable probabilistic safety, along with corresponding\nchallenges and potential solutions. By bridging the gap between theoretical\nsafety assurance and practical deployment, this Perspective offers a pathway\ntoward safer, large-scale adoption of embodied AI systems in safety-critical\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05171v2", "cate": "eess.SY", "date": "2025-06-05", "updated": "2025-07-22"}
{"id": "2507.14184", "title": "NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment", "authors": ["ZhengXiao He", "Jinghao Wen", "Huayu Li", "Ao Li"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14184v2", "summary": "We present a novel and interpretable framework for electrocardiogram\n(ECG)-based disease detection that combines hyperdimensional computing (HDC)\nwith learnable neural encoding. Unlike conventional HDC approaches that rely on\nstatic, random projections, our method introduces a rhythm-aware and trainable\nencoding pipeline based on RR intervals, a physiological signal segmentation\nstrategy that aligns with cardiac cycles. The core of our design is a\nneural-distilled HDC architecture, featuring a learnable RR-block encoder and a\nBinaryLinear hyperdimensional projection layer, optimized jointly with\ncross-entropy and proxy-based metric loss. This hybrid framework preserves the\nsymbolic interpretability of HDC while enabling task-adaptive representation\nlearning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model\nsignificantly outperforms traditional HDC and classical ML baselines, achieving\n73.09\\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable\nrobustness on PTB-XL. Our framework offers an efficient and scalable solution\nfor edge-compatible ECG classification, with strong potential for interpretable\nand personalized health monitoring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14184v2", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-22"}
{"id": "2507.15292", "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro", "authors": ["An Wanga", "Rulin Zhou", "Mengya Xu", "Yiru Ye", "Longfei Gou", "Yiting Chang", "Hao Chen", "Chwee Ming Lim", "Jiankun Wang", "Hongliang Ren"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15292v2", "summary": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15292v2", "cate": "eess.IV", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2505.18726", "title": "Audio Geolocation: A Natural Sounds Benchmark", "authors": ["Mustafa Chasmai", "Wuao Liu", "Subhransu Maji", "Grant Van Horn"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.18726v2", "summary": "Can we determine someone's geographic location purely from the sounds they\nhear? Are acoustic signals enough to localize within a country, state, or even\ncity? We tackle the challenge of global-scale audio geolocation, formalize the\nproblem, and conduct an in-depth analysis with wildlife audio from the\niNatSounds dataset. Adopting a vision-inspired approach, we convert audio\nrecordings to spectrograms and benchmark existing image geolocation techniques.\nWe hypothesize that species vocalizations offer strong geolocation cues due to\ntheir defined geographic ranges and propose an approach that integrates species\nrange prediction with retrieval-based geolocation. We further evaluate whether\ngeolocation improves when analyzing species-rich recordings or when aggregating\nacross spatiotemporal neighborhoods. Finally, we introduce case studies from\nmovies to explore multimodal geolocation using both audio and visual content.\nOur work highlights the advantages of integrating audio and visual cues, and\nsets the stage for future research in audio geolocation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.18726v2", "cate": "cs.SD", "date": "2025-05-24", "updated": "2025-07-21"}
{"id": "2507.16149", "title": "An Exact Solver for Maximizing a Submodular Function Subject to a Knapsack Constraint", "authors": ["Sabine Mnch", "Stephen Raach"], "categories": ["cs.DS", "cs.DM"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16149v1", "summary": "We study the problem of maximizing a monotone increasing submodular function\nover a set of weighted elements subject to a knapsack constraint.\n  Although this problem is NP-hard, many applications require exact solutions,\nas approximate solutions are often insufficient in practice.\n  To address this need, we propose an exact branch-and-bound algorithm tailored\nfor the submodular knapsack problem and introduce several acceleration\ntechniques to enhance its efficiency. We evaluate these techniques on instances\nof three benchmark problems and compare the proposed solvers to two solvers by\nSakaue and Ishihata, which are considered state-of-the-art, demonstrating that\nthe presented methods outperform the existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16149v1", "cate": "cs.DS", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16725", "title": "RAVine: Reality-Aligned Evaluation for Agentic Search", "authors": ["Yilong Xu", "Xiang Long", "Zhi Zheng", "Jinhua Gao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16725v1", "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16725v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16424", "title": "PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning", "authors": ["Hui Xiang", "Jinqiao Shi", "Ting Zhang", "Xiaojie Zhao", "Yong Liu", "Yong Ma"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16424v1", "summary": "Active learning (AL) aims to optimize model training and reduce annotation\ncosts by selecting the most informative samples for labeling. Typically, AL\nmethods rely on the empirical distribution of labeled data to define the\ndecision boundary and perform uncertainty or diversity estimation, subsequently\nidentifying potential high-quality samples. In few-shot scenarios, the\nempirical distribution often diverges significantly from the target\ndistribution, causing the decision boundary to shift away from its optimal\nposition. However, existing methods overlook the role of unlabeled samples in\nenhancing the empirical distribution to better align with the target\ndistribution, resulting in a suboptimal decision boundary and the selection of\nsamples that inadequately represent the target distribution. To address this,\nwe propose a hybrid AL framework, termed \\textbf{PromptAL} (Sample-Aware\nDynamic Soft \\textbf{Prompts} for Few-Shot \\textbf{A}ctive \\textbf{L}earning).\nThis framework accounts for the contribution of each unlabeled data point in\naligning the current empirical distribution with the target distribution,\nthereby optimizing the decision boundary. Specifically, PromptAL first\nleverages unlabeled data to construct sample-aware dynamic soft prompts that\nadjust the model's predictive distribution and decision boundary. Subsequently,\nbased on the adjusted decision boundary, it integrates uncertainty estimation\nwith both global and local diversity to select high-quality samples that more\naccurately represent the target distribution. Experimental results on six\nin-domain and three out-of-domain datasets show that PromptAL achieves superior\nperformance over nine baselines. Our codebase is openly accessible.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16424v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16612", "title": "CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast Cardiac Risk Prediction Using Cine MRIs", "authors": ["Haoyang Su", "Shaohao Rui", "Jinyi Xiang", "Lianming Wu", "Xiaosong Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI 2025", "url": "http://arxiv.org/abs/2507.16612v1", "summary": "Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction\nfrom Cine MRI sequences remains a critical challenge. Existing methods\ntypically necessitate supervised learning based on human-refined masks in the\nventricular myocardium, which become impractical without contrast agents. We\nintroduce a self-supervised framework, namely Codebook-based Temporal-Spatial\nLearning (CTSL), that learns dynamic, spatiotemporal representations from raw\nCine data without requiring segmentation masks. CTSL decouples temporal and\nspatial features through a multi-view distillation strategy, where the teacher\nmodel processes multiple Cine views, and the student model learns from\nreduced-dimensional Cine-SA sequences. By leveraging codebook-based feature\nrepresentations and dynamic lesion self-detection through motion cues, CTSL\ncaptures intricate temporal dependencies and motion patterns. High-confidence\nMACE risk predictions are achieved through our model, providing a rapid,\nnon-invasive solution for cardiac risk assessment that outperforms traditional\ncontrast-dependent methods, thereby enabling timely and accessible heart\ndisease diagnosis in clinical settings.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.16612v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.12554", "title": "GenControl: Generative AI-Driven Autonomous Design of Control Algorithms", "authors": ["Chenggang Cui", "Jiaming Liu", "Peifeng Hui", "Pengfeng Lin", "Chuanlin Zhang"], "categories": ["eess.SY", "cs.SY", "93C40, 49K15"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12554v3", "summary": "Designing controllers for complex industrial electronic systems is\nchallenging due to nonlinearities and parameter uncertainties, and traditional\nmethods are often slow and costly. To address this, we propose a novel\nautonomous design framework driven by Large Language Models (LLMs). Our\napproach employs a bi-level optimization strategy: an LLM intelligently\nexplores and iteratively improves the control algorithm's structure, while a\nParticle Swarm Optimization (PSO) algorithm efficiently refines the parameters\nfor any given structure. This method achieves end-to-end automated design.\nValidated through a simulation of a DC-DC Boost converter, our framework\nsuccessfully evolved a basic controller into a high-performance adaptive\nversion that met all stringent design specifications for fast response, low\nerror, and robustness. This work presents a new paradigm for control design\nthat significantly enhances automation and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12554v3", "cate": "eess.SY", "date": "2025-06-14", "updated": "2025-07-22"}
{"id": "2507.14982", "title": "How Many Simultaneous Beamformers are Needed for Integrated Sensing and Communications?", "authors": ["Kareem M. Attiah", "Wei Yu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      26 pages, 7 figures", "url": "http://arxiv.org/abs/2507.14982v2", "summary": "Consider a downlink integrated sensing and communications (ISAC) system in\nwhich a base station employs linear beamforming to communicate to $K$ users,\nwhile simultaneously uses sensing beams to perform a sensing task of estimating\n$L$ real parameters. How many beamformers are needed to achieve the best\nperformance for both sensing and communications? This paper establishes bounds\non the minimum number of downlink beamformers, in which sensing performance is\nmeasured in terms of the Cram\\'{e}r-Rao bound for parameter estimation and\ncommunications performance is measured in terms of the\nsignal-to-interference-and-noise ratios. We show that an ISAC system requires\nat most $K + \\sqrt{\\frac{L(L+1)}{2}}$ beamformers if the remote users have the\nability to cancel the interference caused by the sensing beams. If cancelling\ninterference due to the sensing beams is not possible, the bound becomes\n$\\sqrt{K^2 + \\frac{L(L+1)}{2}}$. Interestingly, in the latter case, the bound\non the number of beamformers is less than the sum of the bounds for each task\nindividually. These results can be extended to sensing tasks for which the\nperformance is measured as a function of $d$ quadratic terms in the\nbeamformers. In this case, the bound becomes $K + \\sqrt{d}$ and $\\sqrt{K^2 +\nd}$, respectively. Specifically, for estimating complex path losses and\nangles-of-arrival of $N_\\text{tr}$ targets while communicating to $K$ users,\nthe bound on the minimum number of beamformers scales linearly in $K$ and in\n$N_\\text{tr}$, assuming interference from sensing can be cancelled. When\ninterference cancellation is not possible, the following exact characterization\nfor the case of $N_\\text{tr} = 1$ can be obtained: when $K=0$ or $1$, two\nbeamformers should be used; when $K \\ge 2$, exactly $K$ beamformers should be\nused, i.e., communication beamformers alone are already sufficient.", "comment": "26 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.14982v2", "cate": "eess.SP", "date": "2025-07-20", "updated": "2025-07-22"}
{"id": "2507.15487", "title": "DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification", "authors": ["Dezhen Wang", "Sheng Miao", "Rongxin Chai", "Jiufa Cui"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      7 figures, 3 tables, submitted to AAAI2026", "url": "http://arxiv.org/abs/2507.15487v2", "summary": "Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency\ndomain information, which is crucial for accurate lesion classification in\nmedical imaging. However, effectively integrating multi-sequence MRI data for\nrobust 3D lesion classification remains a challenge. In this paper, we propose\nDeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel\nframework designed to extract decoupled representations and adaptively fuse\nspatial and spectral features for lesion classification. DeSamba introduces a\nDecoupled Representation Learning Module (DRLM) that decouples features from\ndifferent MRI sequences through self-reconstruction and cross-reconstruction,\nand a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,\nenabling dynamic fusion of spectral and spatial information based on lesion\ncharacteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On\na six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1\naccuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external\nvalidation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On\na spondylitis dataset (n=251) involving a challenging binary classification\ntask, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal\nand external validation sets, respectively. Ablation studies demonstrate that\nboth DRLM and SAMB significantly contribute to overall performance, with over\n10% relative improvement compared to the baseline. Our results highlight the\npotential of DeSamba as a generalizable and effective solution for 3D lesion\nclassification in multi-sequence medical imaging.", "comment": "7 figures, 3 tables, submitted to AAAI2026", "pdf_url": "http://arxiv.org/pdf/2507.15487v2", "cate": "eess.IV", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.05724", "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition", "authors": ["Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05724v2", "summary": "Mixture-of-experts (MoE) architectures have expanded from language modeling\nto automatic speech recognition (ASR). Traditional MoE methods, such as the\nSwitch Transformer, route experts independently within each layer. Our analysis\nreveals that routers in most layers make expert choices that are not strongly\ncorrelated with the choices of the routers in other layers. To increase the\ncooperation between experts in different layers and encourage greater\nspecialization, we use a shared router across different MoE layers. We call\nthis model Omni-router Transformer. Extensive experiments on a large-scale\npseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR\nbenchmarks demonstrate that the Omni-router Transformer is able to achieve\nlower training loss and consistently outperform dense and Switch Transformer\nmodels, reducing average word error rates by 11.2% and 8.2%, respectively,\nwhile providing structured expert usage and improved robustness to diverse\ndata.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05724v2", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-21"}
{"id": "2507.16285", "title": "Longest Unbordered Factors on Run-Length Encoded Strings", "authors": ["Shoma Sekizaki", "Takuya Mieno"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      SPIRE 2025", "url": "http://arxiv.org/abs/2507.16285v1", "summary": "A border of a string is a non-empty proper prefix of the string that is also\na suffix. A string is unbordered if it has no border. The longest unbordered\nfactor is a fundamental notion in stringology, closely related to string\nperiodicity. This paper addresses the longest unbordered factor problem: given\na string of length $n$, the goal is to compute its longest factor that is\nunbordered. While recent work has achieved subquadratic and near-linear time\nalgorithms for this problem, the best known worst-case time complexity remains\n$O(n \\log n)$ [Kociumaka et al., ISAAC 2018]. In this paper, we investigate the\nproblem in the context of compressed string processing, particularly focusing\non run-length encoded (RLE) strings. We first present a simple yet crucial\nstructural observation relating unbordered factors and RLE-compressed strings.\nBuilding on this, we propose an algorithm that solves the problem in $O(m^{1.5}\n\\log^2 m)$ time and $O(m \\log^2 m)$ space, where $m$ is the size of the\nRLE-compressed input string. To achieve this, our approach simulates a key idea\nfrom the $O(n^{1.5})$-time algorithm by [Gawrychowski et al., SPIRE 2015],\nadapting it to the RLE setting through new combinatorial insights. When the RLE\nsize $m$ is sufficiently small compared to $n$, our algorithm may show\nlinear-time behavior in $n$, potentially leading to improved performance over\nexisting methods in such cases.", "comment": "SPIRE 2025", "pdf_url": "http://arxiv.org/pdf/2507.16285v1", "cate": "cs.DS", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16007", "title": "Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback", "authors": ["Hannah Rashkin", "Elizabeth Clark", "Fantine Huot", "Mirella Lapata"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 main conference", "url": "http://arxiv.org/abs/2507.16007v1", "summary": "Can LLMs provide support to creative writers by giving meaningful writing\nfeedback? In this paper, we explore the challenges and limitations of\nmodel-generated writing feedback by defining a new task, dataset, and\nevaluation frameworks. To study model performance in a controlled manner, we\npresent a novel test set of 1,300 stories that we corrupted to intentionally\nintroduce writing issues. We study the performance of commonly used LLMs in\nthis task with both automatic and human evaluation metrics. Our analysis shows\nthat current models have strong out-of-the-box behavior in many respects --\nproviding specific and mostly accurate writing feedback. However, models often\nfail to identify the biggest writing issue in the story and to correctly decide\nwhen to offer critical vs. positive feedback.", "comment": "ACL 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2507.16007v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16801", "title": "Decoding Translation-Related Functional Sequences in 5'UTRs Using Interpretable Deep Learning Models", "authors": ["Yuxi Lin", "Yaxue Fang", "Zehong Zhang", "Zhouwu Liu", "Siyun Zhong", "Fulong Yu"], "categories": ["q-bio.QM", "cs.AI"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16801v1", "summary": "Understanding how 5' untranslated regions (5'UTRs) regulate mRNA translation\nis critical for controlling protein expression and designing effective\ntherapeutic mRNAs. While recent deep learning models have shown promise in\npredicting translational efficiency from 5'UTR sequences, most are constrained\nby fixed input lengths and limited interpretability. We introduce UTR-STCNet, a\nTransformer-based architecture for flexible and biologically grounded modeling\nof variable-length 5'UTRs. UTR-STCNet integrates a Saliency-Aware Token\nClustering (SATC) module that iteratively aggregates nucleotide tokens into\nmulti-scale, semantically meaningful units based on saliency scores. A\nSaliency-Guided Transformer (SGT) block then captures both local and distal\nregulatory dependencies using a lightweight attention mechanism. This combined\narchitecture achieves efficient and interpretable modeling without input\ntruncation or increased computational cost. Evaluated across three benchmark\ndatasets, UTR-STCNet consistently outperforms state-of-the-art baselines in\npredicting mean ribosome load (MRL), a key proxy for translational efficiency.\nMoreover, the model recovers known functional elements such as upstream AUGs\nand Kozak motifs, highlighting its potential for mechanistic insight into\ntranslation regulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16801v1", "cate": "q-bio.QM", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16431", "title": "An effective physics-informed neural operator framework for predicting wavefields", "authors": ["Xiao Ma", "Tariq Alkhalifah"], "categories": ["physics.geo-ph", "cs.LG"], "primary_category": "Subjects:       Geophysics (physics.geo-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16431v1", "summary": "Solving the wave equation is fundamental for geophysical applications.\nHowever, numerical solutions of the Helmholtz equation face significant\ncomputational and memory challenges. Therefore, we introduce a physics-informed\nconvolutional neural operator (PICNO) to solve the Helmholtz equation\nefficiently. The PICNO takes both the background wavefield corresponding to a\nhomogeneous medium and the velocity model as input function space, generating\nthe scattered wavefield as the output function space. Our workflow integrates\nPDE constraints directly into the training process, enabling the neural\noperator to not only fit the available data but also capture the underlying\nphysics governing wave phenomena. PICNO allows for high-resolution reasonably\naccurate predictions even with limited training samples, and it demonstrates\nsignificant improvements over a purely data-driven convolutional neural\noperator (CNO), particularly in predicting high-frequency wavefields. These\nfeatures and improvements are important for waveform inversion down the road.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16431v1", "cate": "physics.geo-ph", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16623", "title": "Automatic Fine-grained Segmentation-assisted Report Generation", "authors": ["Frederic Jonske", "Constantin Seibold", "Osman Alperen Koras", "Fin Bahnsen", "Marie Bauer", "Amin Dada", "Hamza Kalisch", "Anton Schily", "Jens Kleesiek"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16623v1", "summary": "Reliable end-to-end clinical report generation has been a longstanding goal\nof medical ML research. The end goal for this process is to alleviate\nradiologists' workloads and provide second opinions to clinicians or patients.\nThus, a necessary prerequisite for report generation models is a strong general\nperformance and some type of innate grounding capability, to convince\nclinicians or patients of the veracity of the generated reports. In this paper,\nwe present ASaRG (\\textbf{A}utomatic \\textbf{S}egmentation-\\textbf{a}ssisted\n\\textbf{R}eport \\textbf{G}eneration), an extension of the popular LLaVA\narchitecture that aims to tackle both of these problems. ASaRG proposes to fuse\nintermediate features and fine-grained segmentation maps created by specialist\nradiological models into LLaVA's multi-modal projection layer via simple\nconcatenation. With a small number of added parameters, our approach achieves a\n+0.89\\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA\nbaseline when using only intermediate features, and +2.77\\% performance gain\n($p<0.001$) when adding a combination of intermediate features and fine-grained\nsegmentation maps. Compared with COMG and ORID, two other report generation\nmethods that utilize segmentations, the performance gain amounts to 6.98\\% and\n6.28\\% in F1 score, respectively. ASaRG is not mutually exclusive with other\nchanges made to the LLaVA architecture, potentially allowing our method to be\ncombined with other advances in the field. Finally, the use of an arbitrary\nnumber of segmentations as part of the input demonstrably allows tracing\nelements of the report to the corresponding segmentation maps and verifying the\ngroundedness of assessments. Our code will be made publicly available at a\nlater date.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16623v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.14354", "title": "Interpretable Gradient Descent for Kalman Gain", "authors": ["M. A. Belabbas", "A. Olshevsky"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14354v2", "summary": "We derive a decomposition for the gradient of the innovation loss with\nrespect to the filter gain in a linear time-invariant system, decomposing as a\nproduct of an observability Gramian and a term quantifying the\n``non-orthogonality\" between the estimation error and the innovation. We\nleverage this decomposition to give a convergence proof of gradient descent to\nthe optimal Kalman gain, specifically identifying how recovery of the Kalman\ngain depends on a non-standard observability condition, and obtaining an\ninterpretable geometric convergence rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14354v2", "cate": "math.OC", "date": "2025-07-18", "updated": "2025-07-22"}
{"id": "2503.16366", "title": "Wide-Angle, Multiplexed Backscatter Communications Using a Dynamic Metasurface-Backed Luneburg Lens", "authors": ["Samuel Kim", "Tim Sleasman", "Avrami Rakovsky", "Ra'id Awadallah", "David B. Shrekenhamer"], "categories": ["physics.optics", "eess.SP"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "Comments:      13 pages, 9 figures. Accepted for publication in IEEE Access", "url": "http://arxiv.org/abs/2503.16366v2", "summary": "Backscatter communications is attractive for its low power requirements due\nto the lack of actively radiating components; however, commonly used devices\nare typically limited in range and functionality. Here, we design and\ndemonstrate a backscatter device consisting of a flattened Luneburg lens\ncombined with a spatially-tunable dynamic metasurface. Using quasi-conformal\ntransformation optics (QCTO), we design a flattened, additively manufactured\nLuneburg lens that focuses incoming waves over a wide field-of-view onto its\nflattened focal plane. When a reflective surface is placed at the focal plane,\nthe flattened Luneburg lens retroreflects, enabling long-range backscatter\ncommunications over an extremely large field-of-view ($\\pm30\\degree$) and\nbandwidth. The dynamic metasurface is designed to modulated the reflected phase\nacross the S-band (2-4 GHz) with fine spatial control. Thus, when combined with\nthe flattened Luneburg lens, the device is able to modulate the retroreflected\nsignal to achieve backscatter communications. We experimentally demonstrate\nfull phase control of the backscattered signal across a range of incidence\nangles, spatial multiplexing, and secure communications against eavesdroppers\nby actively suppressing or randomizing signals in unwanted directions. The\nmetasurface-backed Luneburg lens device offers a low-power solution for\nlong-range wireless networks with advanced capabilities.", "comment": "13 pages, 9 figures. Accepted for publication in IEEE Access", "pdf_url": "http://arxiv.org/pdf/2503.16366v2", "cate": "physics.optics", "date": "2025-03-20", "updated": "2025-07-21"}
{"id": "2501.08005", "title": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection", "authors": ["Francisco Caetano", "Christiaan Viviers", "Luis A. Zavala-Mondragn", "Peter H. N. de With", "Fons van der Sommen"], "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2501.08005v5", "summary": "Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code is publicly available.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2501.08005v5", "cate": "cs.CV", "date": "2025-01-14", "updated": "2025-07-22"}
{"id": "2507.14915", "title": "Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling", "authors": ["Xiaojie Li", "Ronghui Li", "Shukai Fang", "Shuzhao Xie", "Xiaoyang Guo", "Jiaqing Zhou", "Junkun Peng", "Zhi Wang"], "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14915v2", "summary": "Well-coordinated, music-aligned holistic dance enhances emotional\nexpressiveness and audience engagement. However, generating such dances remains\nchallenging due to the scarcity of holistic 3D dance datasets, the difficulty\nof achieving cross-modal alignment between music and dance, and the complexity\nof modeling interdependent motion across the body, hands, and face. To address\nthese challenges, we introduce SoulDance, a high-precision music-dance paired\ndataset captured via professional motion capture systems, featuring\nmeticulously annotated holistic dance movements. Building on this dataset, we\npropose SoulNet, a framework designed to generate music-aligned, kinematically\ncoordinated holistic dance sequences. SoulNet consists of three principal\ncomponents: (1) Hierarchical Residual Vector Quantization, which models\ncomplex, fine-grained motion dependencies across the body, hands, and face; (2)\nMusic-Aligned Generative Model, which composes these hierarchical motion units\ninto expressive and coordinated holistic dance; (3) Music-Motion Retrieval\nModule, a pre-trained cross-modal model that functions as a music-dance\nalignment prior, ensuring temporal synchronization and semantic coherence\nbetween generated dance and input music throughout the generation process.\nExtensive experiments demonstrate that SoulNet significantly surpasses existing\napproaches in generating high-quality, music-coordinated, and well-aligned\nholistic 3D dance sequences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14915v2", "cate": "cs.MM", "date": "2025-07-20", "updated": "2025-07-22"}
{"id": "2507.16064", "title": "A Best Possible General Form of the Master Theorem for Divide-and-Conquer Recurrences", "authors": ["Carl D. Offner"], "categories": ["math.CA", "cs.DS", "math.CO", "41", "F.2"], "primary_category": "Subjects:       Classical Analysis and ODEs (math.CA)", "pdf_link": null, "comments": "Comments:      20 pages, 1 figure", "url": "http://arxiv.org/abs/2507.16064v1", "summary": "We give here a general, best-possible, and smoothly-derived form of the\nMaster Theorem for divide-and-conquer recurrences.", "comment": "20 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.16064v1", "cate": "math.CA", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16011", "title": "mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages", "authors": ["Hellina Hailu Nigatu", "Min Li", "Maartje ter Hoeve", "Saloni Potdar", "Sarah Chasins"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to Findings of ACL 2025", "url": "http://arxiv.org/abs/2507.16011v1", "summary": "Knowledge Graphs represent real-world entities and the relationships between\nthem. Multilingual Knowledge Graph Construction (mKGC) refers to the task of\nautomatically constructing or predicting missing entities and links for\nknowledge graphs in a multilingual setting. In this work, we reformulate the\nmKGC task as a Question Answering (QA) task and introduce mRAKL: a\nRetrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve\nthis by using the head entity and linking relation in a question, and having\nour model predict the tail entity as an answer. Our experiments focus primarily\non two low-resourced languages: Tigrinya and Amharic. We experiment with using\nhigher-resourced languages Arabic and English for cross-lingual transfer. With\na BM25 retriever, we find that the RAG-based approach improves performance over\na no-context setting. Further, our ablation studies show that with an idealized\nretrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points\nfor Tigrinya and Amharic, respectively.", "comment": "Accepted to Findings of ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.16011v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16463", "title": "MMS Player: an open source software for parametric data-driven animation of Sign Language avatars", "authors": ["Fabrizio Nunnari", "Shailesh Mishra", "Patrick Gebhard"], "categories": ["cs.GR", "cs.CL"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16463v1", "summary": "This paper describes the MMS-Player, an open source software able to\nsynthesise sign language animations from a novel sign language representation\nformat called MMS (MultiModal Signstream). The MMS enhances gloss-based\nrepresentations by adding information on parallel execution of signs, timing,\nand inflections. The implementation consists of Python scripts for the popular\nBlender 3D authoring tool and can be invoked via command line or HTTP API.\nAnimations can be rendered as videos or exported in other popular 3D animation\nexchange formats. The software is freely available under GPL-3.0 license at\nhttps://github.com/DFKI-SignLanguage/MMS-Player.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16463v1", "cate": "cs.GR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16812", "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning", "authors": ["Run-Ze Fan", "Zengzhi Wang", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      39 pages; Github: this https URL HF: this https URL", "url": "http://arxiv.org/abs/2507.16812v1", "summary": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.", "comment": "39 pages; Github: https://github.com/GAIR-NLP/MegaScience; HF:\n  https://huggingface.co/MegaScience", "pdf_url": "http://arxiv.org/pdf/2507.16812v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16433", "title": "Adaptive Multi-task Learning for Multi-sector Portfolio Optimization", "authors": ["Qingliang Fan", "Ruike Wu", "Yanrong Yang"], "categories": ["stat.ME", "cs.LG"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16433v1", "summary": "Accurate transfer of information across multiple sectors to enhance model\nestimation is both significant and challenging in multi-sector portfolio\noptimization involving a large number of assets in different classes. Within\nthe framework of factor modeling, we propose a novel data-adaptive multi-task\nlearning methodology that quantifies and learns the relatedness among the\nprincipal temporal subspaces (spanned by factors) across multiple sectors under\nstudy. This approach not only improves the simultaneous estimation of multiple\nfactor models but also enhances multi-sector portfolio optimization, which\nheavily depends on the accurate recovery of these factor models. Additionally,\na novel and easy-to-implement algorithm, termed projection-penalized principal\ncomponent analysis, is developed to accomplish the multi-task learning\nprocedure. Diverse simulation designs and practical application on daily return\ndata from Russell 3000 index demonstrate the advantages of multi-task learning\nmethodology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16433v1", "cate": "stat.ME", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16624", "title": "A2Mamba: Attention-augmented State Space Models for Visual Recognition", "authors": ["Meng Lou", "Yunxiang Fu", "Yizhou Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 5 figures, 13 tables", "url": "http://arxiv.org/abs/2507.16624v1", "summary": "Transformers and Mamba, initially invented for natural language processing,\nhave inspired backbone architectures for visual recognition. Recent studies\nintegrated Local Attention Transformers with Mamba to capture both local\ndetails and global contexts. Despite competitive performance, these methods are\nlimited to simple stacking of Transformer and Mamba layers without any\ninteraction mechanism between them. Thus, deep integration between Transformer\nand Mamba layers remains an open problem. We address this problem by proposing\nA2Mamba, a powerful Transformer-Mamba hybrid network architecture, featuring a\nnew token mixer termed Multi-scale Attention-augmented State Space Model\n(MASS), where multi-scale attention maps are integrated into an\nattention-augmented SSM (A2SSM). A key step of A2SSM performs a variant of\ncross-attention by spatially aggregating the SSM's hidden states using the\nmulti-scale attention maps, which enhances spatial dependencies pertaining to a\ntwo-dimensional space while improving the dynamic modeling capabilities of\nSSMs. Our A2Mamba outperforms all previous ConvNet-, Transformer-, and\nMamba-based architectures in visual recognition tasks. For instance, A2Mamba-L\nachieves an impressive 86.1% top-1 accuracy on ImageNet-1K. In semantic\nsegmentation, A2Mamba-B exceeds CAFormer-S36 by 2.5% in mIoU, while exhibiting\nhigher efficiency. In object detection and instance segmentation with Cascade\nMask R-CNN, A2Mamba-S surpasses MambaVision-B by 1.2%/0.9% in AP^b/AP^m, while\nhaving 40% less parameters. Code is publicly available at\nhttps://github.com/LMMMEng/A2Mamba.", "comment": "14 pages, 5 figures, 13 tables", "pdf_url": "http://arxiv.org/pdf/2507.16624v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.17262", "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras", "authors": ["Shuang Guo", "Friedhelm Hamann", "Guillermo Gallego"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 8 figures, 9 tables. Project page: this https URL . IEEE/CVF International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2503.17262v2", "summary": "Event cameras rely on motion to obtain information about scene appearance.\nThis means that appearance and motion are inherently linked: either both are\npresent and recorded in the event data, or neither is captured. Previous works\ntreat the recovery of these two visual quantities as separate tasks, which does\nnot fit with the above-mentioned nature of event cameras and overlooks the\ninherent relations between them. We propose an unsupervised learning framework\nthat jointly estimates optical flow (motion) and image intensity (appearance)\nusing a single network. From the data generation model, we newly derive the\nevent-based photometric error as a function of optical flow and image\nintensity. This error is further combined with the contrast maximization\nframework to form a comprehensive loss function that provides proper\nconstraints for both flow and intensity estimation. Exhaustive experiments show\nour method's state-of-the-art performance: in optical flow estimation, it\nreduces EPE by 20% and AE by 25% compared to unsupervised approaches, while\ndelivering competitive intensity estimation results, particularly in high\ndynamic range scenarios. Our method also achieves shorter inference time than\nall other optical flow methods and many of the image reconstruction methods,\nwhile they output only one quantity. Project page:\nhttps://github.com/tub-rip/E2FAI", "comment": "13 pages, 8 figures, 9 tables. Project page:\n  https://github.com/tub-rip/E2FAI . IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2503.17262v2", "cate": "cs.CV", "date": "2025-03-21", "updated": "2025-07-22"}
{"id": "2507.16209", "title": "Best-of-Both-Worlds Guarantees with Fairer Endings", "authors": ["Telikepalli Kavitha", "Surya Panchapakesan", "Rohit Vaish", "Vignesh Viswanathan", "Jatin Yadav"], "categories": ["cs.GT", "cs.DS"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      46 pages. Abstract shortened to meet arXiv requirements", "url": "http://arxiv.org/abs/2507.16209v1", "summary": "Fair allocation of indivisible goods is a fundamental problem at the\ninterface of economics and computer science. Traditional approaches focus\neither on randomized allocations that are fair in expectation or deterministic\nallocations that are approximately fair. Recent work reconciles both these\napproaches via best-of-both-worlds guarantees, wherein one seeks randomized\nallocations that are fair in expectation (ex-ante fair) while being supported\non approximately fair allocations (ex-post fair). Prior work has shown that\nunder additive valuations, there always exists a randomized allocation that is\nex-ante stochastic-dominance envy-free (sd-EF) and ex-post envy-free up to one\ngood (EF1).\n  Our work is motivated by the goal of achieving stronger ex-post fairness\nguarantees such as envy-freeness up to any good (EFX) along with meaningful\nex-ante guarantees. We make the following contributions:\n  1) We first consider lexicographic preferences, a subdomain of additive\nvaluations where ex-post EFX allocations always exist and can be computed\nefficiently. On the negative side, we show that ex-ante sd-EF is fundamentally\nincompatible with ex-post EFX, prompting a relaxation of the ex-ante benchmark.\nWe then present a poly. time algorithm that achieves ex-post EFX and PO\ntogether with ex-ante 9/10-EF. Our algorithm uses dependent rounding and\nleverages structural properties of EFX and PO allocations.\n  2)For monotone valuations, we study EFX-with-charity: a relaxation of EFX\nwhere some goods remain unallocated, with no agent envying the unallocated\npool. We show that ex-post EFX-with-charity can be achieved alongside ex-ante\n0.5-EF.\n  3)Finally, for subadditive valuations, we strengthen our previous ex-post\nguarantee to EFX-with-bounded-charity, where at most n-1 goods (n= no. of\nagents) remain unallocated, at the price of weakening the ex-ante guarantee to\n0.5-proportionality.", "comment": "46 pages. Abstract shortened to meet arXiv requirements", "pdf_url": "http://arxiv.org/pdf/2507.16209v1", "cate": "cs.GT", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16054", "title": "AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering", "authors": ["Simon Baeuerle", "Max Radyschevski", "Ulrike Pado"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16054v1", "summary": "In large organisations, knowledge is mainly shared in meetings, which takes\nup significant amounts of work time. Additionally, frequent in-person meetings\nproduce inconsistent documentation -- official minutes, personal notes,\npresentations may or may not exist. Shared information therefore becomes hard\nto retrieve outside of the meeting, necessitating lengthy updates and\nhigh-frequency meeting schedules.\n  Generative Artificial Intelligence (genAI) models like Large Language Models\n(LLMs) exhibit an impressive performance on spoken and written language\nprocessing. This motivates a practical usage of genAI for knowledge management\nin engineering departments: using genAI for transcribing meetings and\nintegrating heterogeneous additional information sources into an easily usable\nformat for ad-hoc searches.\n  We implement an end-to-end pipeline to automate the entire meeting\ndocumentation workflow in a proof-of-concept state: meetings are recorded and\nminutes are created by genAI. These are further made easily searchable through\na chatbot interface. The core of our work is to test this genAI-based software\ntooling in a real-world engineering department and collect extensive survey\ndata on both ethical and technical aspects. Direct feedback from this\nreal-world setup points out both opportunities and risks: a) users agree that\nthe effort for meetings could be significantly reduced with the help of genAI\nmodels, b) technical aspects are largely solved already, c) organizational\naspects are crucial for a successful ethical usage of such a system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16054v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2305.07120", "title": "High-Resolution Thermal Simulation Framework for Extrusion-based Additive Manufacturing of Complex Geometries", "authors": ["Dhruv Gamdha", "Kumar Saurabh", "Baskar Ganapathysubramanian", "Adarsh Krishnamurthy"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      24 pages, 12 figures", "url": "http://arxiv.org/abs/2305.07120v2", "summary": "Accurate simulation of the printing process is essential for improving print\nquality, reducing waste, and optimizing the printing parameters of\nextrusion-based additive manufacturing. Traditional additive manufacturing\nsimulations are very compute-intensive and are not scalable to simulate even\nmoderately sized geometries. In this paper, we propose a general framework for\ncreating a digital twin of the dynamic printing process by performing physics\nsimulations with the intermediate print geometries. Our framework takes a\ngeneral extrusion-based additive manufacturing G-code, generates an\nanalysis-suitable voxelized geometry representation from the print schedule,\nand performs physics-based (transient thermal) simulations of the printing\nprocess. Our approach leverages adaptive octree meshes for both geometry\nrepresentation as well as for fast simulations to address real-time\npredictions. We demonstrate the effectiveness of our method by simulating the\nprinting of complex geometries at high voxel resolutions with both sparse and\ndense infills. Our results show that this approach scales to high voxel\nresolutions and can predict the transient heat distribution as the print\nprogresses. Because the simulation runs faster than real print time, the same\nengine could, in principle, feed thermal predictions back to the machine\ncontroller (e.g., to adjust fan speed or extrusion rate). The present study\nestablishes the computational foundations for a real-time digital twin, which\ncan be used for closed control loop control in the future.", "comment": "24 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2305.07120v2", "cate": "cs.GR", "date": "2023-05-10", "updated": "2025-07-22"}
{"id": "2406.14103", "title": "Efficient Strategy Learning by Decoupling Searching and Pathfinding for Object Navigation", "authors": ["Yanwei Zheng", "Shaopu Feng", "Bowen Huang", "Chuanlin Lan", "Xiao Zhang", "Dongxiao Yu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.14103v2", "summary": "Inspired by human-like behaviors for navigation: first searching to explore\nunknown areas before discovering the target, and then the pathfinding of moving\ntowards the discovered target, recent studies design parallel submodules to\nachieve different functions in the searching and pathfinding stages, while\nignoring the differences in reward signals between the two stages. As a result,\nthese models often cannot be fully trained or are overfitting on training\nscenes. Another bottleneck that restricts agents from learning two-stage\nstrategies is spatial perception ability, since the studies used generic visual\nencoders without considering the depth information of navigation scenes. To\nrelease the potential of the model on strategy learning, we propose the\nTwo-Stage Reward Mechanism (TSRM) for object navigation that decouples the\nsearching and pathfinding behaviours in an episode, enabling the agent to\nexplore larger area in searching stage and seek the optimal path in pathfinding\nstage. Also, we propose a pretraining method Depth Enhanced Masked Autoencoders\n(DE-MAE) that enables agent to determine explored and unexplored areas during\nthe searching stage, locate target object and plan paths during the pathfinding\nstage more accurately. In addition, we propose a new metric of Searching\nSuccess weighted by Searching Path Length (SSSPL) that assesses agent's\nsearching ability and exploring efficiency. Finally, we evaluated our method on\nAI2-Thor and RoboTHOR extensively and demonstrated it can outperform the\nstate-of-the-art (SOTA) methods in both the success rate and the navigation\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.14103v2", "cate": "cs.AI", "date": "2024-06-20", "updated": "2025-07-22"}
{"id": "2507.16490", "title": "Combining Language and Topic Models for Hierarchical Text Classification", "authors": ["Jaco du Toit", "Marcel Dunaiski"], "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures", "url": "http://arxiv.org/abs/2507.16490v1", "summary": "Hierarchical text classification (HTC) is a natural language processing task\nwhich has the objective of categorising text documents into a set of classes\nfrom a predefined structured class hierarchy. Recent HTC approaches use various\ntechniques to incorporate the hierarchical class structure information with the\nnatural language understanding capabilities of pre-trained language models\n(PLMs) to improve classification performance. Furthermore, using topic models\nalong with PLMs to extract features from text documents has been shown to be an\neffective approach for multi-label text classification tasks. The rationale\nbehind the combination of these feature extractor models is that the PLM\ncaptures the finer-grained contextual and semantic information while the topic\nmodel obtains high-level representations which consider the corpus of documents\nas a whole. In this paper, we use a HTC approach which uses a PLM and a topic\nmodel to extract features from text documents which are used to train a\nclassification model. Our objective is to determine whether the combination of\nthe features extracted from the two models is beneficial to HTC performance in\ngeneral. In our approach, the extracted features are passed through separate\nconvolutional layers whose outputs are combined and passed to a label-wise\nattention mechanisms which obtains label-specific document representations by\nweighing the most important features for each class separately. We perform\ncomprehensive experiments on three HTC benchmark datasets and show that using\nthe features extracted from the topic model generally decreases classification\nperformance compared to only using the features obtained by the PLM. In\ncontrast to previous work, this shows that the incorporation of features\nextracted from topic models for text classification tasks should not be assumed\nbeneficial.", "comment": "13 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.16490v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16639", "title": "Benchmarking pig detection and tracking under diverse and challenging conditions", "authors": ["Jonathan Henrich", "Christian Post", "Maximilian Zilke", "Parth Shiroya", "Emma Chanut", "Amir Mollazadeh Yamchi", "Ramin Yahyapour", "Thomas Kneib", "Imke Traulsen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16639v1", "summary": "To ensure animal welfare and effective management in pig farming, monitoring\nindividual behavior is a crucial prerequisite. While monitoring tasks have\ntraditionally been carried out manually, advances in machine learning have made\nit possible to collect individualized information in an increasingly automated\nway. Central to these methods is the localization of animals across space\n(object detection) and time (multi-object tracking). Despite extensive research\nof these two tasks in pig farming, a systematic benchmarking study has not yet\nbeen conducted. In this work, we address this gap by curating two datasets:\nPigDetect for object detection and PigTrack for multi-object tracking. The\ndatasets are based on diverse image and video material from realistic barn\nconditions, and include challenging scenarios such as occlusions or bad\nvisibility. For object detection, we show that challenging training images\nimprove detection performance beyond what is achievable with randomly sampled\nimages alone. Comparing different approaches, we found that state-of-the-art\nmodels offer substantial improvements in detection quality over real-time\nalternatives. For multi-object tracking, we observed that SORT-based methods\nachieve superior detection performance compared to end-to-end trainable models.\nHowever, end-to-end models show better association performance, suggesting they\ncould become strong alternatives in the future. We also investigate\ncharacteristic failure cases of end-to-end models, providing guidance for\nfuture improvements. The detection and tracking models trained on our datasets\nperform well in unseen pens, suggesting good generalization capabilities. This\nhighlights the importance of high-quality training data. The datasets and\nresearch code are made publicly available to facilitate reproducibility, re-use\nand further development.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16639v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16648", "title": "An unconditional lower bound for the active-set method in convex quadratic maximization", "authors": ["Eleon Bach", "Yann Disser", "Sophie Huiberts", "Nils Mosis"], "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO"], "primary_category": "Subjects:       Discrete Mathematics (cs.DM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16648v1", "summary": "We prove that the active-set method needs an exponential number of iterations\nin the worst-case to maximize a convex quadratic function subject to linear\nconstraints, regardless of the pivot rule used. This substantially improves\nover the best previously known lower bound [IPCO 2025], which needs objective\nfunctions of polynomial degrees $\\omega(\\log d)$ in dimension $d$, to a bound\nusing a convex polynomial of degree 2. In particular, our result firmly\nresolves the open question [IPCO 2025] of whether a constant degree suffices,\nand it represents significant progress towards linear objectives, where the\nactive-set method coincides with the simplex method and a lower bound for all\npivot rules would constitute a major breakthrough.\n  Our result is based on a novel extended formulation, recursively constructed\nusing deformed products. Its key feature is that it projects onto a polygonal\napproximation of a parabola while preserving all of its exponentially many\nvertices. We define a quadratic objective that forces the active-set method to\nfollow the parabolic boundary of this projection, without allowing any\nshortcuts along chords corresponding to edges of its full-dimensional preimage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16648v1", "cate": "cs.DM", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16075", "title": "Deep Researcher with Test-Time Diffusion", "authors": ["Rujun Han", "Yanfei Chen", "Zoey CuiZhu", "Lesly Miculicich", "Guan Sun", "Yuanjun Bi", "Weiming Wen", "Hui Wan", "Chunfeng Wen", "Solne Matre", "George Lee", "Vishy Tirumalashetty", "Emily Xue", "Zizhao Zhang", "Salem Haykal", "Burak Gokturk", "Tomas Pfister", "Chen-Yu Lee"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16075v1", "summary": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16075v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2312.06317", "title": "Flow Symmetrization for Parameterized Constrained Diffeomorphisms", "authors": ["Aalok Gangopadhyay", "Dwip Dalal", "Progyan Das", "Shanmuganathan Raman"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.06317v2", "summary": "Diffeomorphisms play a crucial role while searching for shapes with fixed\ntopological properties, allowing for smooth deformation of template shapes.\nSeveral approaches use diffeomorphism for shape search. However, these\napproaches employ only unconstrained diffeomorphisms. In this work, we develop\nFlow Symmetrization - a method to represent a parametric family of constrained\ndiffeomorphisms that contain additional symmetry constraints such as\nperiodicity, rotation equivariance, and transflection equivariance. Our\nrepresentation is differentiable in nature, making it suitable for\ngradient-based optimization approaches for shape search. As these symmetry\nconstraints naturally arise in tiling classes, our method is ideal for\nrepresenting tile shapes belonging to any tiling class. To demonstrate the\nefficacy of our method, we design two frameworks for addressing the challenging\nproblems of Escherization and Density Estimation. The first framework is\ndedicated to the Escherization problem, where we parameterize tile shapes\nbelonging to different isohedral classes. Given a target shape, the template\ntile is deformed using gradient-based optimization to resemble the target\nshape. The second framework focuses on density estimation in identification\nspaces. By leveraging the inherent link between tiling theory and\nidentification topology, we design constrained diffeomorphisms for the plane\nthat result in unconstrained diffeomorphisms on the identification spaces.\nSpecifically, we perform density estimation on identification spaces such as\ntorus, sphere, Klein bottle, and projective plane. Through results and\nexperiments, we demonstrate that our method obtains impressive results for\nEscherization on the Euclidean plane and density estimation on non-Euclidean\nidentification spaces. Code and results:\nhttps://dwipddalal.github.io/FlowSymmetry/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.06317v2", "cate": "cs.GR", "date": "2023-12-11", "updated": "2025-07-21"}
{"id": "2501.12485", "title": "R2D2: Remembering, Replaying and Dynamic Decision Making with a Reflective Agentic Memory", "authors": ["Tenghao Huang", "Kinjal Basu", "Ibrahim Abdelaziz", "Pavan Kapanipathi", "Jonathan May", "Muhao Chen"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      ACL 2025", "url": "http://arxiv.org/abs/2501.12485v3", "summary": "The proliferation of web agents necessitates advanced navigation and\ninteraction strategies within complex web environments. Current models often\nstruggle with efficient navigation and action execution due to limited\nvisibility and understanding of web structures. Our proposed R2D2 framework\naddresses these challenges by integrating two paradigms: Remember and Reflect.\nThe Remember paradigm uses a replay buffer that aids agents in reconstructing\nthe web environment dynamically, thus enabling the formulation of a detailed\n\"map\" of previously visited pages. This helps in reducing navigational errors\nand optimizing the decision-making process during web interactions. Conversely,\nthe Reflect paradigm allows agents to learn from past mistakes by providing a\nmechanism for error analysis and strategy refinement, enhancing overall task\nperformance. We evaluate R2D2 using the WebArena benchmark, demonstrating\nsubstantial improvements over existing methods, including a 50% reduction in\nnavigation errors and a threefold increase in task completion rates. Our\nfindings suggest that a combination of memory-enhanced navigation and\nreflective learning promisingly advances the capabilities of web agents,\npotentially benefiting various applications such as automated customer service\nand personal digital assistants.", "comment": "ACL 2025", "pdf_url": "http://arxiv.org/pdf/2501.12485v3", "cate": "cs.AI", "date": "2025-01-21", "updated": "2025-07-22"}
{"id": "2507.16548", "title": "Alternative Loss Function in Evaluation of Transformer Models", "authors": ["Jakub Michakw", "Pawe Sakowski", "Robert lepaczuk"], "categories": ["q-fin.CP", "cs.LG", "q-fin.TR"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.16548v1", "summary": "The proper design and architecture of testing of machine learning models,\nespecially in their application to quantitative finance problems, is crucial.\nThe most important in this process is selecting an adequate loss function used\nfor training, validation, estimation purposes, and tuning of hyperparameters.\nTherefore, in this research, through empirical experiments on equity and\ncryptocurrency assets, we introduce the Mean Absolute Directional Loss (MADL)\nfunction which is more adequate for optimizing forecast-generating models used\nin algorithmic investment strategies. The MADL function results are compared\nfor Transformer and LSTM models and we show that almost in every case\nTransformer results are significantly better than those obtained with LSTM.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.16548v1", "cate": "q-fin.CP", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16657", "title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection", "authors": ["Shuang Song", "Yang Tang", "Rongjun Qin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 5 figures, This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.16657v1", "summary": "Deep learning has significantly advanced building segmentation in remote\nsensing, yet models struggle to generalize on data of diverse geographic\nregions due to variations in city layouts and the distribution of building\ntypes, sizes and locations. However, the amount of time-consuming annotated\ndata for capturing worldwide diversity may never catch up with the demands of\nincreasingly data-hungry models. Thus, we propose a novel approach: re-training\nmodels at test time using synthetic data tailored to the target region's city\nlayout. This method generates geo-typical synthetic data that closely\nreplicates the urban structure of a target area by leveraging geospatial data\nsuch as street network from OpenStreetMap. Using procedural modeling and\nphysics-based rendering, very high-resolution synthetic images are created,\nincorporating domain randomization in building shapes, materials, and\nenvironmental illumination. This enables the generation of virtually unlimited\ntraining samples that maintain the essential characteristics of the target\nenvironment. To overcome synthetic-to-real domain gaps, our approach integrates\ngeo-typical data into an adversarial domain adaptation framework for building\nsegmentation. Experiments demonstrate significant performance enhancements,\nwith median improvements of up to 12%, depending on the domain gap. This\nscalable and cost-effective method blends partial geographic knowledge with\nsynthetic imagery, providing a promising solution to the \"model collapse\" issue\nin purely synthetic datasets. It offers a practical pathway to improving\ngeneralization in remote sensing building segmentation without extensive\nreal-world annotations.", "comment": "14 pages, 5 figures, This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2507.16657v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2411.03413", "title": "Rapid Mixing at the Uniqueness Threshold", "authors": ["Xiaoyu Chen", "Zongchen Chen", "Yitong Yin", "Xinyuan Zhang"], "categories": ["cs.DS", "math.PR"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.03413v2", "summary": "Over the past decades, a fascinating computational phase transition has been\nidentified in sampling from Gibbs distributions. Though, the computational\ncomplexity at the critical point remains poorly understood, as previous\nalgorithmic and hardness results all required a constant slack from this\nthreshold.\n  In this paper, we resolve this open question at the critical phase transition\nthreshold, thus completing the picture of the computational phase transition.\nWe show that for the hardcore model on graphs with maximum degree $\\Delta\\ge 3$\nat the uniqueness threshold $\\lambda = \\lambda_c(\\Delta)$, the mixing time of\nGlauber dynamics is upper bounded by a polynomial in $n$, but is not nearly\nlinear in the worst case.\n  For the Ising model (either antiferromagnetic or ferromagnetic), we establish\nsimilar results. For the Ising model on graphs with maximum degree $\\Delta\\ge\n3$ at the critical temperature $\\beta$ where $|\\beta| = \\beta_c(\\Delta)$, with\nthe tree-uniqueness threshold $\\beta_c(\\Delta)$, we show that the mixing time\nof Glauber dynamics is upper bounded by $\\tilde{O}\\left(n^{2 +\nO(1/\\Delta)}\\right)$ and lower bounded by $\\Omega\\left(n^{3/2}\\right)$ in the\nworst case. For the Ising model specified by a critical interaction matrix $J$\nwith $\\left \\lVert J \\right \\rVert_2=1$, we obtain an upper bound\n$\\tilde{O}(n^{3/2})$ for the mixing time, matching the lower bound\n$\\Omega\\left(n^{3/2}\\right)$ on the complete graph up to a logarithmic factor.\n  Our mixing time upper bounds are derived from a new interpretation and\nanalysis of the localization scheme method introduced by Chen and Eldan (2022),\napplied to the field dynamics for the hardcore model and the proximal sampler\nfor the Ising model. As key steps in both our upper and lower bounds, we\nestablish sub-linear upper and lower bounds for spectral independence at the\ncritical point for worst-case instances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.03413v2", "cate": "cs.DS", "date": "2024-11-05", "updated": "2025-07-22"}
{"id": "2507.16076", "title": "The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models", "authors": ["Marlene Lutz", "Indira Sen", "Georg Ahnert", "Elisa Rogers", "Markus Strohmaier"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16076v1", "summary": "Persona prompting is increasingly used in large language models (LLMs) to\nsimulate views of various sociodemographic groups. However, how a persona\nprompt is formulated can significantly affect outcomes, raising concerns about\nthe fidelity of such simulations. Using five open-source LLMs, we\nsystematically examine how different persona prompt strategies, specifically\nrole adoption formats and demographic priming strategies, influence LLM\nsimulations across 15 intersectional demographic groups in both open- and\nclosed-ended tasks. Our findings show that LLMs struggle to simulate\nmarginalized groups, particularly nonbinary, Hispanic, and Middle Eastern\nidentities, but that the choice of demographic priming and role adoption\nstrategy significantly impacts their portrayal. Specifically, we find that\nprompting in an interview-style format and name-based priming can help reduce\nstereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B\noutperform larger ones such as Llama-3.3-70B. Our findings offer actionable\nguidance for designing sociodemographic persona prompts in LLM-based simulation\nstudies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16076v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2503.10637", "title": "Distilling Diversity and Control in Diffusion Models", "authors": ["Rohit Gandikota", "David Bau"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2503.10637v3", "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\nsample-diversity collapse during distillation. To understand how distillation\naffects diversity, we utilize $\\hat{\\mathbf{x}}_{0}$ visualization as an\nanalysis and debugging tool to reveal how models predict final outputs at\nintermediate steps. Through $\\hat{\\mathbf{x}}_{0}$ visualization, we identify\ngeneration artifacts, inconsistencies, and demonstrate that initial diffusion\ntimesteps disproportionately determine output diversity, while later steps\nprimarily refine details. Based on these insights, we introduce diversity\ndistillation - a hybrid inference approach that strategically employs the base\nmodel for only the first critical timestep before transitioning to the\nefficient distilled model. Our experiments demonstrate that this simple\nmodification not only restores the diversity capabilities from base to\ndistilled models but surprisingly exceeds it, while maintaining nearly the\ncomputational efficiency of distilled inference, all without requiring\nadditional training or model modifications. Our code and data are available at\nhttps://distillation.baulab.info/", "comment": "Project Page: https://distillation.baulab.info", "pdf_url": "http://arxiv.org/pdf/2503.10637v3", "cate": "cs.GR", "date": "2025-03-13", "updated": "2025-07-21"}
{"id": "2505.16938", "title": "InternAgent: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["InternAgent Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Runmin Ma", "Yusong Hu", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Code: this https URL , HomePage: this https URL", "url": "http://arxiv.org/abs/2505.16938v3", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce InternAgent, a unified closed-loop multi-agent\nframework to conduct Autonomous Scientific Research (ASR) across various\nscientific research fields, enabling researchers to tackle complicated problems\nin these fields with unprecedented speed and precision. InternAgent highlights\nthree key advantages: 1) Scalability: InternAgent has demonstrated its\nversatility across 12 scientific research tasks, capable of generating\ninnovative ideas to enhance the performance of baseline code. 2) Interactivity:\nInternAgent provides an interface for human expert feedback and multi-agent\ninteraction in automated end-to-end processes, allowing for the seamless\nintegration of domain expert knowledge. 3) Efficiency: InternAgent has achieved\npromising performance gains in several scientific fields with significantly\nless time cost compared to human efforts. For instance, in reaction yield\nprediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer\nactivity prediction, accuracy rose from 0.65 to 0.79 with only 4 hours of\nprocessing; and in 2D semantic segmentation, precision advanced from 78.8% to\n81.0% in a mere 30 hours.", "comment": "Code: https://github.com/Alpha-Innovator/InternAgent, HomePage:\n  https://alpha-innovator.github.io/InternAgent-project-page", "pdf_url": "http://arxiv.org/pdf/2505.16938v3", "cate": "cs.AI", "date": "2025-05-22", "updated": "2025-07-22"}
{"id": "2507.16557", "title": "Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language", "authors": ["Kristin Gnadt", "David Thulke", "Simone Kopeinik", "Ralf Schlter"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at the 6th Workshop on Gender Bias in Natural Language Processing (GeBNLP) at ACL 2025", "url": "http://arxiv.org/abs/2507.16557v1", "summary": "In recent years, various methods have been proposed to evaluate gender bias\nin large language models (LLMs). A key challenge lies in the transferability of\nbias measurement methods initially developed for the English language when\napplied to other languages. This work aims to contribute to this research\nstrand by presenting five German datasets for gender bias evaluation in LLMs.\nThe datasets are grounded in well-established concepts of gender bias and are\naccessible through multiple methodologies. Our findings, reported for eight\nmultilingual LLM models, reveal unique challenges associated with gender bias\nin German, including the ambiguous interpretation of male occupational terms\nand the influence of seemingly neutral nouns on gender perception. This work\ncontributes to the understanding of gender bias in LLMs across languages and\nunderscores the necessity for tailored evaluation frameworks.", "comment": "Accepted at the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP) at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.16557v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16683", "title": "QRetinex-Net: Quaternion-Valued Retinex Decomposition for Low-Level Computer Vision Applications", "authors": ["Sos Agaian", "Vladimir Frants"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16683v1", "summary": "Images taken in low light often show color shift, low contrast, noise, and\nother artifacts that hurt computer-vision accuracy. Retinex theory addresses\nthis by viewing an image S as the pixel-wise product of reflectance R and\nillumination I, mirroring the way people perceive stable object colors under\nchanging light. The decomposition is ill-posed, and classic Retinex models have\nfour key flaws: (i) they treat the red, green, and blue channels independently;\n(ii) they lack a neuroscientific model of color vision; (iii) they cannot\nperfectly rebuild the input image; and (iv) they do not explain human color\nconstancy. We introduce the first Quaternion Retinex formulation, in which the\nscene is written as the Hamilton product of quaternion-valued reflectance and\nillumination. To gauge how well reflectance stays invariant, we propose the\nReflectance Consistency Index. Tests on low-light crack inspection, face\ndetection under varied lighting, and infrared-visible fusion show gains of 2-11\npercent over leading methods, with better color fidelity, lower noise, and\nhigher reflectance stability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16683v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2504.01168", "title": "LimTDD: A Compact Decision Diagram Integrating Tensor and Local Invertible Map Representations", "authors": ["Xin Hong", "Aochu Dai", "Dingchao Gao", "Sanjiang Li", "Zhengfeng Ji", "Mingsheng Ying"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.01168v2", "summary": "Tensor networks serve as a powerful tool for efficiently representing and\nmanipulating high-dimensional data in applications such as quantum physics,\nmachine learning, and data compression. Tensor Decision Diagrams (TDDs) offer\nan efficient framework for tensor representation by leveraging decision diagram\ntechniques. However, the current implementation of TDDs and other decision\ndiagrams fail to exploit tensor isomorphisms, limiting their compression\npotential. This paper introduces Local Invertible Map Tensor Decision Diagrams\n(LimTDDs), an extension of TDDs that incorporates local invertible maps (LIMs)\nto achieve more compact representations. Unlike LIMDD, which uses Pauli\noperators for quantum states, LimTDD employs the $XP$-stabilizer group,\nenabling broader applicability across tensor-based tasks. We present efficient\nalgorithms for normalization, slicing, addition, and contraction, critical for\ntensor network applications. Theoretical analysis demonstrates that LimTDDs\nachieve greater compactness than TDDs and, in best-case scenarios and for\nquantum state representations, offer exponential compression advantages over\nboth TDDs and LIMDDs. Experimental results in quantum circuit tensor\ncomputation and simulation confirm LimTDD's superior efficiency. Open-source\ncode is available at https://github.com/Veriqc/LimTDD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.01168v2", "cate": "cs.DS", "date": "2025-04-01", "updated": "2025-07-22"}
{"id": "2507.16183", "title": "BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset", "authors": ["Azizul Hakim Fayaz", "MD. Shorif Uddin", "Rayhan Uddin Bhuiyan", "Zakia Sultana", "Md. Samiul Islam", "Bidyarthi Paul", "Tashreef Muhammad", "Shahriar Manzoor"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16183v1", "summary": "Hate speech on digital platforms has become a growing concern globally,\nespecially in linguistically diverse countries like Bangladesh, where regional\ndialects play a major role in everyday communication. Despite progress in hate\nspeech detection for standard Bangla, Existing datasets and systems fail to\naddress the informal and culturally rich expressions found in dialects such as\nBarishal, Noakhali, and Chittagong. This oversight results in limited detection\ncapability and biased moderation, leaving large sections of harmful content\nunaccounted for. To address this gap, this study introduces BIDWESH, the first\nmulti-dialectal Bangla hate speech dataset, constructed by translating and\nannotating 9,183 instances from the BD-SHS corpus into three major regional\ndialects. Each entry was manually verified and labeled for hate presence, type\n(slander, gender, religion, call to violence), and target group (individual,\nmale, female, group), ensuring linguistic and contextual accuracy. The\nresulting dataset provides a linguistically rich, balanced, and inclusive\nresource for advancing hate speech detection in Bangla. BIDWESH lays the\ngroundwork for the development of dialect-sensitive NLP tools and contributes\nsignificantly to equitable and context-aware content moderation in low-resource\nlanguage settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16183v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2410.13613", "title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes", "authors": ["Xinjie Zhang", "Zhening Liu", "Yifan Zhang", "Xingtong Ge", "Dailan He", "Tongda Xu", "Yan Wang", "Zehong Lin", "Shuicheng Yan", "Jun Zhang"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2410.13613v3", "summary": "4D Gaussian Splatting (4DGS) has recently emerged as a promising technique\nfor capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D\nGaussian representation and a GPU-friendly rasterizer, enabling rapid rendering\nspeeds. Despite its advantages, 4DGS faces significant challenges, notably the\nrequirement of millions of 4D Gaussians, each with extensive associated\nattributes, leading to substantial memory and storage cost. This paper\nintroduces a memory-efficient framework for 4DGS. We streamline the color\nattribute by decomposing it into a per-Gaussian direct color component with\nonly 3 parameters and a shared lightweight alternating current color predictor.\nThis approach eliminates the need for spherical harmonics coefficients, which\ntypically involve up to 144 parameters in classic 4DGS, thereby creating a\nmemory-efficient 4D Gaussian representation. Furthermore, we introduce an\nentropy-constrained Gaussian deformation technique that uses a deformation\nfield to expand the action range of each Gaussian and integrates an\nopacity-based entropy loss to limit the number of Gaussians, thus forcing our\nmodel to use as few Gaussians as possible to fit a dynamic scene well. With\nsimple half-precision storage and zip compression, our framework achieves a\nstorage reduction by approximately 190$\\times$ and 125$\\times$ on the\nTechnicolor and Neural 3D Video datasets, respectively, compared to the\noriginal 4DGS. Meanwhile, it maintains comparable rendering speeds and scene\nrepresentation quality, setting a new standard in the field. Code is available\nat https://github.com/Xinjie-Q/MEGA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2410.13613v3", "cate": "cs.CV", "date": "2024-10-17", "updated": "2025-07-22"}
{"id": "2505.19956", "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph", "authors": ["Jihyung Lee", "Jin-Seop Lee", "Jaehoon Lee", "YunSeok Choi", "Jee-Hyong Lee"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19956v2", "summary": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. The code is available\nat https://github.com/jjklle/DCG-SQL}{https://github.com/jjklle/DCG-SQL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19956v2", "cate": "cs.AI", "date": "2025-05-26", "updated": "2025-07-22"}
{"id": "2507.16678", "title": "Deep Unfolding Network for Nonlinear Multi-Frequency Electrical Impedance Tomography", "authors": ["Giovanni S. Alberti", "Damiana Lazzaro", "Serena Morigi", "Luca Ratti", "Matteo Santacesaria"], "categories": ["math.NA", "cs.LG", "cs.NA", "stat.ML", "65K10, 65N20, 68T07"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16678v1", "summary": "Multi-frequency Electrical Impedance Tomography (mfEIT) represents a\npromising biomedical imaging modality that enables the estimation of tissue\nconductivities across a range of frequencies. Addressing this challenge, we\npresent a novel variational network, a model-based learning paradigm that\nstrategically merges the advantages and interpretability of classical iterative\nreconstruction with the power of deep learning. This approach integrates graph\nneural networks (GNNs) within the iterative Proximal Regularized Gauss Newton\n(PRGN) framework. By unrolling the PRGN algorithm, where each iteration\ncorresponds to a network layer, we leverage the physical insights of nonlinear\nmodel fitting alongside the GNN's capacity to capture inter-frequency\ncorrelations. Notably, the GNN architecture preserves the irregular triangular\nmesh structure used in the solution of the nonlinear forward model, enabling\naccurate reconstruction of overlapping tissue fraction concentrations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16678v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16716", "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation", "authors": ["Yiguo He", "Junjie Zhu", "Yiying Li", "Xiaoyu Zhang", "Chunping Qiu", "Jun Wang", "Qiangjuan Huang", "Ke Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      SUBMIT TO IEEE TRANSACTIONS", "url": "http://arxiv.org/abs/2507.16716v1", "summary": "The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.", "comment": "SUBMIT TO IEEE TRANSACTIONS", "pdf_url": "http://arxiv.org/pdf/2507.16716v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2505.19109", "title": "On Distributed Colouring of Hyperbolic Random Graphs", "authors": ["Yannic Maus", "Janosch Ruff"], "categories": ["cs.DS", "68W15"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      53 pages, 6 figures", "url": "http://arxiv.org/abs/2505.19109v2", "summary": "We analyse the performance of simple distributed colouring algorithms under\nthe assumption that the input graph is a hyperbolic random graph (HRG), a\ngenerative model capturing key properties of real-world networks such as\npower-law degree distributions and large clustering coefficients. Motivated by\nthe shift from worst-case analysis to more realistic network models, we study\nthe number of rounds and size of the colour space required to colour HRGs in\nthe distributed setting.", "comment": "53 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2505.19109v2", "cate": "cs.DS", "date": "2025-05-25", "updated": "2025-07-21"}
{"id": "2507.16196", "title": "Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task", "authors": ["Jared Moore", "Ned Cooper", "Rasmus Overmark", "Beba Cibralic", "Nick Haber", "Cameron R. Jones"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      To appear in COLM, 2025", "url": "http://arxiv.org/abs/2507.16196v1", "summary": "Recent evidence suggests Large Language Models (LLMs) display Theory of Mind\n(ToM) abilities. Most ToM experiments place participants in a spectatorial\nrole, wherein they predict and interpret other agents' behavior. However, human\nToM also contributes to dynamically planning action and strategically\nintervening on others' mental states. We present MindGames: a novel `planning\ntheory of mind' (PToM) task which requires agents to infer an interlocutor's\nbeliefs and desires to persuade them to alter their behavior. Unlike previous\nevaluations, we explicitly evaluate use cases of ToM. We find that humans\nsignificantly outperform o1-preview (an LLM) at our PToM task (11% higher;\n$p=0.006$). We hypothesize this is because humans have an implicit causal model\nof other agents (e.g., they know, as our task requires, to ask about people's\npreferences). In contrast, o1-preview outperforms humans in a baseline\ncondition which requires a similar amount of planning but minimal mental state\ninferences (e.g., o1-preview is better than humans at planning when already\ngiven someone's preferences). These results suggest a significant gap between\nhuman-like social reasoning and LLM abilities.", "comment": "To appear in COLM, 2025", "pdf_url": "http://arxiv.org/pdf/2507.16196v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.10624", "title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness", "authors": ["Boqian Li", "Haiwen Feng", "Zeyu Cai", "Michael J. Black", "Yuliang Xiu"], "categories": ["cs.CV", "cs.AI", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Page: this https URL , Code: this https URL", "url": "http://arxiv.org/abs/2503.10624v2", "summary": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings (~ 1% data). Qualitative results\ndemonstrate strong generalization of ETCH, regardless of challenging poses,\nunseen shapes, loose clothing, and non-rigid dynamics. We will release the code\nand models soon for research purposes at https://boqian-li.github.io/ETCH/.", "comment": "Page: https://boqian-li.github.io/ETCH/, Code:\n  https://github.com/boqian-li/ETCH", "pdf_url": "http://arxiv.org/pdf/2503.10624v2", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-07-22"}
{"id": "2507.15994", "title": "Scaling Recommender Transformers to One Billion Parameters", "authors": ["Kirill Khrylchenko", "Artem Matveev", "Sergei Makeev", "Vladimir Baikalov"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      To be submitted", "url": "http://arxiv.org/abs/2507.15994v1", "summary": "While large transformer models have been successfully used in many real-world\napplications such as natural language processing, computer vision, and speech\nprocessing, scaling transformers for recommender systems remains a challenging\nproblem. Recently, Generative Recommenders framework was proposed to scale\nbeyond typical Deep Learning Recommendation Models (DLRMs). Reformulation of\nrecommendation as sequential transduction task led to improvement of scaling\nproperties in terms of compute. Nevertheless, the largest encoder configuration\nreported by the HSTU authors amounts only to ~176 million parameters, which is\nconsiderably smaller than the hundreds of billions or even trillions of\nparameters common in modern language models.\n  In this work, we present a recipe for training large transformer recommenders\nwith up to a billion parameters. We show that autoregressive learning on user\nhistories naturally decomposes into two subtasks, feedback prediction and\nnext-item prediction, and demonstrate that such a decomposition scales\neffectively across a wide range of transformer sizes. Furthermore, we report a\nsuccessful deployment of our proposed architecture on a large-scale music\nplatform serving millions of users. According to our online A/B tests, this new\nmodel increases total listening time by +2.26% and raises the likelihood of\nuser likes by +6.37%, constituting (to our knowledge) the largest improvement\nin recommendation quality reported for any deep learning-based system in the\nplatform's history.", "comment": "To be submitted", "pdf_url": "http://arxiv.org/pdf/2507.15994v1", "cate": "cs.IR", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2506.02139", "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "authors": ["Edward Y. Chang", "Zeyneb N. Kaya", "Ethan Chang"], "categories": ["cs.AI", "I.2.7"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figure, 2 table", "url": "http://arxiv.org/abs/2506.02139v3", "summary": "Large language models (LLMs) are vast repositories of latent patterns, but\nwithout structured guidance, they lack explicit reasoning, semantic grounding,\nand goal-directed intelligence. We propose Unified Cognitive Consciousness\nTheory (UCCT), a unified model that reinterprets LLMs as unconscious substrates\nrequiring external mechanisms, few-shot prompting, RAG, fine-tuning, and\nmulti-agent reasoning, to semantically anchor latent representations. UCCT\nformalizes this anchoring process through a Bayesian formulation, revealing a\nthreshold-crossing dynamic characterized by 1/sqrt(n) scaling that explains the\nsudden capability transitions observed across diverse tasks. The theory unifies\npreviously disparate techniques, few-shot prompting, RAG, fine-tuning, and\nmulti-agent reasoning, as special cases of a general anchoring architecture.\nThrough case studies in simple math, visual recognition, and structured debate\ntasks, we confirm the predictive power of UCCT. Furthermore, our experiment in\narithmetic in three numeral systems validates the theories of UCCT. Rather than\ntreating intelligence as an intrinsic property of LLMs, UCCT demonstrates that\nLLMs are merely unconscious pattern repositories with no inherent intelligence.\nIntelligence emerges only when external anchoring mechanisms assign target\nsemantics to these latent patterns, transforming unconscious representations\ninto conscious, goal-directed capabilities.", "comment": "13 pages, 4 figure, 2 table", "pdf_url": "http://arxiv.org/pdf/2506.02139v3", "cate": "cs.AI", "date": "2025-06-02", "updated": "2025-07-22"}
{"id": "2507.16682", "title": "Structural Effect and Spectral Enhancement of High-Dimensional Regularized Linear Discriminant Analysis", "authors": ["Yonghan Zhang", "Zhangni Pu", "Lu Yan", "Jiang Hu"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16682v1", "summary": "Regularized linear discriminant analysis (RLDA) is a widely used tool for\nclassification and dimensionality reduction, but its performance in\nhigh-dimensional scenarios is inconsistent. Existing theoretical analyses of\nRLDA often lack clear insight into how data structure affects classification\nperformance. To address this issue, we derive a non-asymptotic approximation of\nthe misclassification rate and thus analyze the structural effect and\nstructural adjustment strategies of RLDA. Based on this, we propose the\nSpectral Enhanced Discriminant Analysis (SEDA) algorithm, which optimizes the\ndata structure by adjusting the spiked eigenvalues of the population covariance\nmatrix. By developing a new theoretical result on eigenvectors in random matrix\ntheory, we derive an asymptotic approximation on the misclassification rate of\nSEDA. The bias correction algorithm and parameter selection strategy are then\nobtained. Experiments on synthetic and real datasets show that SEDA achieves\nhigher classification accuracy and dimensionality reduction compared to\nexisting LDA methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16682v1", "cate": "stat.ML", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16718", "title": "Temporally-Constrained Video Reasoning Segmentation and Automated Benchmark Construction", "authors": ["Yiqing Shen", "Chenjia Li", "Chenxiao Fan", "Mathias Unberath"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16718v1", "summary": "Conventional approaches to video segmentation are confined to predefined\nobject categories and cannot identify out-of-vocabulary objects, let alone\nobjects that are not identified explicitly but only referred to implicitly in\ncomplex text queries. This shortcoming limits the utility for video\nsegmentation in complex and variable scenarios, where a closed set of object\ncategories is difficult to define and where users may not know the exact object\ncategory that will appear in the video. Such scenarios can arise in operating\nroom video analysis, where different health systems may use different workflows\nand instrumentation, requiring flexible solutions for video analysis. Reasoning\nsegmentation (RS) now offers promise towards such a solution, enabling natural\nlanguage text queries as interaction for identifying object to segment.\nHowever, existing video RS formulation assume that target objects remain\ncontextually relevant throughout entire video sequences. This assumption is\ninadequate for real-world scenarios in which objects of interest appear,\ndisappear or change relevance dynamically based on temporal context, such as\nsurgical instruments that become relevant only during specific procedural\nphases or anatomical structures that gain importance at particular moments\nduring surgery. Our first contribution is the introduction of\ntemporally-constrained video reasoning segmentation, a novel task formulation\nthat requires models to implicitly infer when target objects become\ncontextually relevant based on text queries that incorporate temporal\nreasoning. Since manual annotation of temporally-constrained video RS datasets\nwould be expensive and limit scalability, our second contribution is an\ninnovative automated benchmark construction method. Finally, we present\nTCVideoRSBenchmark, a temporally-constrained video RS dataset containing 52\nsamples using the videos from the MVOR dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16718v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2409.00966", "title": "A computational transition for detecting correlated stochastic block models by low-degree polynomials", "authors": ["Guanyi Chen", "Jian Ding", "Shuyang Gong", "Zhangsong Li"], "categories": ["math.PR", "cs.DS", "cs.LG", "math.ST", "stat.TH", "Primary 62M20, Secondary 68Q87, 68Q17"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      80 pages, 2 figures, added further explanations and remarks; to appear in Annals of Statistics", "url": "http://arxiv.org/abs/2409.00966v2", "summary": "Detection of correlation in a pair of random graphs is a fundamental\nstatistical and computational problem that has been extensively studied in\nrecent years. In this work, we consider a pair of correlated (sparse)\nstochastic block models $\\mathcal{S}(n,\\tfrac{\\lambda}{n};k,\\epsilon;s)$ that\nare subsampled from a common parent stochastic block model $\\mathcal\nS(n,\\tfrac{\\lambda}{n};k,\\epsilon)$ with $k=O(1)$ symmetric communities,\naverage degree $\\lambda=O(1)$, divergence parameter $\\epsilon$, and subsampling\nprobability $s$.\n  For the detection problem of distinguishing this model from a pair of\nindependent Erd\\H{o}s-R\\'enyi graphs with the same edge density\n$\\mathcal{G}(n,\\tfrac{\\lambda s}{n})$, we focus on tests based on\n\\emph{low-degree polynomials} of the entries of the adjacency matrices, and we\ndetermine the threshold that separates the easy and hard regimes. More\nprecisely, we show that this class of tests can distinguish these two models if\nand only if $s> \\min \\{ \\sqrt{\\alpha}, \\frac{1}{\\lambda \\epsilon^2} \\}$, where\n$\\alpha\\approx 0.338$ is the Otter's constant and $\\frac{1}{\\lambda\n\\epsilon^2}$ is the Kesten-Stigum threshold. Combining a reduction argument in\n\\cite{Li25+}, our hardness result also implies low-degree hardness for partial\nrecovery and detection (to independent block models) when $s< \\min \\{\n\\sqrt{\\alpha}, \\frac{1}{\\lambda \\epsilon^2} \\}$. Finally, our proof of\nlow-degree hardness is based on a conditional variant of the low-degree\nlikelihood calculation.", "comment": "80 pages, 2 figures, added further explanations and remarks; to\n  appear in Annals of Statistics", "pdf_url": "http://arxiv.org/pdf/2409.00966v2", "cate": "math.PR", "date": "2024-09-02", "updated": "2025-07-22"}
{"id": "2507.16199", "title": "WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability", "authors": ["Zipeng Ling", "Yuehao Tang", "Shuliang Liu", "Junqi Yang", "Shenghong Fu", "Yao Wan", "Kejia Huang", "Zhichao Hou", "Xuming Hu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16199v1", "summary": "Large Language Models (LLMs) frequently output the label \\emph{Unknown}, yet\ncurrent evaluations focus almost exclusively on whether such answers are\n\\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an\ninput that is genuinely indeterminate and (ii) a solvable problem that the\nmodel fails to resolve. We call this phenomenon \\emph{Vague Perception}. And\nthus we introduce a framework that quantifies the proportion of \\emph{Unknown}\nresponses attributable to model incapacity and tests whether guided stimulation\ncan convert them into either correct (\\emph{Known}) or intrinsically\nindeterminate outcomes. By separating these sources of uncertainty, our method\nprovides a clearer picture of LLM reasoning limits and their potential for\nimprovement. As we get a theoretical accuracy of reasoning task on different\nLLMs, we apply different methods to test whether the model can reach the\naccuracy given a baseline framework. Our work is meaningful in exploring the\ntrue reasoning ability of LLMs and providing a new perspective on solving the\n\\emph{Vague Perception} phenomenon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16199v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16253", "title": "Reinforce Lifelong Interaction Value of User-Author Pairs for Large-Scale Recommendation Systems", "authors": ["Yisha Li", "Lexi Gao", "Jingxin Liu", "Xiang Gao", "Xin Li", "Haiyang Lu", "Liyin Hong"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16253v1", "summary": "Recommendation systems (RS) help users find interested content and connect\nauthors with their target audience. Most research in RS tends to focus either\non predicting users' immediate feedback (like click-through rate) accurately or\nimproving users' long-term engagement. However, they ignore the influence for\nauthors and the lifelong interaction value (LIV) of user-author pairs, which is\nparticularly crucial for improving the prosperity of social community in\nshort-video platforms. Currently, reinforcement learning (RL) can optimize\nlong-term benefits and has been widely applied in RS. In this paper, we\nintroduce RL to Reinforce Lifelong Interaction Value of User-Author pairs\n(RLIV-UA) based on each interaction of UA pairs. To address the long intervals\nbetween UA interactions and the large scale of the UA space, we propose a novel\nSparse Cross-Request Interaction Markov Decision Process (SCRI-MDP) and\nintroduce an Adjacent State Approximation (ASA) method to construct RL training\nsamples. Additionally, we introduce Multi-Task Critic Learning (MTCL) to\ncapture the progressive nature of UA interactions (click -> follow -> gift),\nwhere denser interaction signals are leveraged to compensate for the learning\nof sparse labels. Finally, an auxiliary supervised learning task is designed to\nenhance the convergence of the RLIV-UA model. In offline experiments and online\nA/B tests, the RLIV-UA model achieves both higher user satisfaction and higher\nplatform profits than compared methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16253v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.21734", "title": "Hierarchical Reasoning Model", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21734v2", "summary": "Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21734v2", "cate": "cs.AI", "date": "2025-06-26", "updated": "2025-07-22"}
{"id": "2507.16697", "title": "Pixel-Resolved Long-Context Learning for Turbulence at Exascale: Resolving Small-scale Eddies Toward the Viscous Limit", "authors": ["Junqi Yin", "Mijanur Palash", "M. Paul Laiu", "Muralikrishnan Gopalakrishnan Meena", "John Gounley", "Stephen M. de Bruyn Kops", "Feiyi Wang", "Ramanan Sankaran", "Pei Zhang"], "categories": ["physics.flu-dyn", "cs.LG"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16697v1", "summary": "Turbulence plays a crucial role in multiphysics applications, including\naerodynamics, fusion, and combustion. Accurately capturing turbulence's\nmultiscale characteristics is essential for reliable predictions of\nmultiphysics interactions, but remains a grand challenge even for exascale\nsupercomputers and advanced deep learning models. The extreme-resolution data\nrequired to represent turbulence, ranging from billions to trillions of grid\npoints, pose prohibitive computational costs for models based on architectures\nlike vision transformers. To address this challenge, we introduce a multiscale\nhierarchical Turbulence Transformer that reduces sequence length from billions\nto a few millions and a novel RingX sequence parallelism approach that enables\nscalable long-context learning. We perform scaling and science runs on the\nFrontier supercomputer. Our approach demonstrates excellent performance up to\n1.1 EFLOPS on 32,768 AMD GPUs, with a scaling efficiency of 94%. To our\nknowledge, this is the first AI model for turbulence that can capture\nsmall-scale eddies down to the dissipative range.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16697v1", "cate": "physics.flu-dyn", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16732", "title": "HarmonPaint: Harmonized Training-Free Diffusion Inpainting", "authors": ["Ying Li", "Xinzhe Li", "Yong Du", "Yangyang Xu", "Junyu Dong", "Shengfeng He"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16732v1", "summary": "Existing inpainting methods often require extensive retraining or fine-tuning\nto integrate new content seamlessly, yet they struggle to maintain coherence in\nboth structure and style between inpainted regions and the surrounding\nbackground. Motivated by these limitations, we introduce HarmonPaint, a\ntraining-free inpainting framework that seamlessly integrates with the\nattention mechanisms of diffusion models to achieve high-quality, harmonized\nimage inpainting without any form of training. By leveraging masking strategies\nwithin self-attention, HarmonPaint ensures structural fidelity without model\nretraining or fine-tuning. Additionally, we exploit intrinsic diffusion model\nproperties to transfer style information from unmasked to masked regions,\nachieving a harmonious integration of styles. Extensive experiments demonstrate\nthe effectiveness of HarmonPaint across diverse scenes and styles, validating\nits versatility and performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16732v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2505.04604", "title": "Feature Selection and Junta Testing are Statistically Equivalent", "authors": ["Lorenzo Beretta", "Nathaniel Harms", "Caleb Koch"], "categories": ["cs.LG", "cs.CC", "cs.DS", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2505.04604v2", "summary": "For a function $f \\colon \\{0,1\\}^n \\to \\{0,1\\}$, the junta testing problem\nasks whether $f$ depends on only $k$ variables. If $f$ depends on only $k$\nvariables, the feature selection problem asks to find those variables. We prove\nthat these two tasks are statistically equivalent. Specifically, we show that\nthe ``brute-force'' algorithm, which checks for any set of $k$ variables\nconsistent with the sample, is simultaneously sample-optimal for both problems,\nand the optimal sample size is \\[ \\Theta\\left(\\frac 1 \\varepsilon \\left(\n\\sqrt{2^k \\log {n \\choose k}} + \\log {n \\choose k}\\right)\\right). \\]", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2505.04604v2", "cate": "cs.LG", "date": "2025-05-07", "updated": "2025-07-21"}
{"id": "2507.16248", "title": "FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents", "authors": ["Run Sun", "Zuo Bai", "Wentao Zhang", "Yuxiang Zhang", "Li Zhao", "Shan Sun", "Zhengwen Qiu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16248v1", "summary": "Recently, AI agents are rapidly evolving in intelligence and widely used in\nprofessional research applications, such as STEM, software development,\nfinance, etc. Among these AI agents, deep research agent is a key category as\nit can perform long-horizon tasks and solve problems of greater complexity.\nHowever, there are few evaluation frameworks and benchmarks that systematically\nand automatically investigate the capabilities of these research agents.\nFurthermore, financial research problems have distinct complexity and subtlety.\nTo fill in the gap, we propose FinResearchBench, which is a logic tree based\nAgent-as-a-Judge and targets specifically for the financial research agents. It\nprovides a comprehensive and automatic assessment of the research agents across\n7 key types of tasks in the financial research domain. The contributions of\nthis work are two-folded: (1) the first and innovative Agent-as-a-Judge system\nthat extracts the logic tree of the research outcome and uses it as the\nintermediate information to present a comprehensive, reliable and robust\nevaluation; (2) finance oriented that it covers 70 typical financial research\nquestions, spreading across 7 frequently encountered types of tasks in the\ndomain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16248v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16371", "title": "Enhancing patent retrieval using automated patent summarization", "authors": ["Eleni Kamateri", "Renukswamy Chikkamath", "Michail Salampasis", "Linda Andersson", "Markus Endres"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      This version was submitted and accepted for publication at the 6th Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech 2025), held in conjunction with SIGIR 2025. A revised and polished version, incorporating reviewers' feedback, will follow", "url": "http://arxiv.org/abs/2507.16371v1", "summary": "Effective query formulation is a key challenge in long-document Information\nRetrieval (IR). This challenge is particularly acute in domain-specific\ncontexts like patent retrieval, where documents are lengthy, linguistically\ncomplex, and encompass multiple interrelated technical topics. In this work, we\npresent the application of recent extractive and abstractive summarization\nmethods for generating concise, purpose-specific summaries of patent documents.\nWe further assess the utility of these automatically generated summaries as\nsurrogate queries across three benchmark patent datasets and compare their\nretrieval performance against conventional approaches that use entire patent\nsections. Experimental results show that summarization-based queries\nsignificantly improve prior-art retrieval effectiveness, highlighting their\npotential as an efficient alternative to traditional query formulation\ntechniques.", "comment": "This version was submitted and accepted for publication at the 6th\n  Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech\n  2025), held in conjunction with SIGIR 2025. A revised and polished version,\n  incorporating reviewers' feedback, will follow", "pdf_url": "http://arxiv.org/pdf/2507.16371v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2506.21512", "title": "Assessing an evolutionary search engine for small language models, prompts, and evaluation metrics", "authors": ["Cludio Lcio do Val Lopes", "Lucca Machado"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      14 pages, 1 figure, 1 table", "url": "http://arxiv.org/abs/2506.21512v2", "summary": "The concurrent optimization of language models and instructional prompts\npresents a significant challenge for deploying efficient and effective AI\nsystems, particularly when balancing performance against computational costs\nlike token usage. This paper introduces and assesses a bi-objective\nevolutionary search engine designed to navigate this complex space, focusing\nspecifically on Small Language Models (SLMs). We employ the NSGA-II algorithm\nand prompt grammar to simultaneously optimize for task accuracy and token\nefficiency across some reasoning tasks. Our results successfully identify\ndiverse, high-performing model-prompt combinations, quantitatively revealing\nthe critical trade-off between the two objectives. This research highlights\ntask-specific affinities between particular SLMs and prompt structures (e.g.,\ninstructions, context, chain of thought). The generated practical Pareto fronts\noffer decision-makers a portfolio of optimized solutions adaptable to their\nspecific constraints. This automated approach moves beyond traditional manual\ntuning, providing a foundational framework for discovering effective human-AI\ninteraction patterns.", "comment": "14 pages, 1 figure, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.21512v2", "cate": "cs.NE", "date": "2025-06-26", "updated": "2025-07-21"}
{"id": "2507.07893", "title": "An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qing", "Qing xu", "Kaiwen Pan", "Ting luo"], "categories": ["cs.AI", "68T50, 68T30, 91F20", "I.2.7; I.2.4; K.5.1; H.3.3"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      19 pages,3 figures", "url": "http://arxiv.org/abs/2507.07893v2", "summary": "This research presents a framework combining prompt engineering with\nmultidimensional knowledge graphs to improve LLMs' legal dispute analysis.\nSpecifically, the framework includes a three-stage hierarchical prompt\nstructure (task definition, knowledge background, reasoning guidance) along\nwith a three-layer knowledge graph (legal ontology, representation, instance\nlayers). Additionally, four supporting methods enable precise legal concept\nretrieval: direct code matching, semantic vector similarity, ontology path\nreasoning, and lexical segmentation. Through extensive testing, results show\nmajor improvements: sensitivity increased by 9.9%-13.8%, specificity by\n4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework\nprovides better legal analysis and understanding of judicial logic, thus\noffering a new technical method for intelligent legal assistance systems.", "comment": "19 pages,3 figures", "pdf_url": "http://arxiv.org/pdf/2507.07893v2", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-22"}
{"id": "2507.16746", "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning", "authors": ["Ang Li", "Charles Wang", "Kaiyu Yue", "Zikui Cai", "Ollie Liu", "Deqing Fu", "Peng Guo", "Wang Bill Zhu", "Vatsal Sharan", "Robin Jia", "Willie Neiswanger", "Furong Huang", "Tom Goldstein", "Micah Goldblum"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      dataset link: this https URL", "url": "http://arxiv.org/abs/2507.16746v1", "summary": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce $\\textbf{Zebra-CoT}$, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.", "comment": "dataset link:\n  https://huggingface.co/datasets/multimodal-reasoning-lab/Zebra-CoT", "pdf_url": "http://arxiv.org/pdf/2507.16746v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16736", "title": "DFR: A Decompose-Fuse-Reconstruct Framework for Multi-Modal Few-Shot Segmentation", "authors": ["Shuai Chen", "Fanman Meng", "Xiwei Zhang", "Haoran Wei", "Chenhao Wu", "Qingbo Wu", "Hongliang Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      3 figures", "url": "http://arxiv.org/abs/2507.16736v1", "summary": "This paper presents DFR (Decompose, Fuse and Reconstruct), a novel framework\nthat addresses the fundamental challenge of effectively utilizing multi-modal\nguidance in few-shot segmentation (FSS). While existing approaches primarily\nrely on visual support samples or textual descriptions, their single or\ndual-modal paradigms limit exploitation of rich perceptual information\navailable in real-world scenarios. To overcome this limitation, the proposed\napproach leverages the Segment Anything Model (SAM) to systematically integrate\nvisual, textual, and audio modalities for enhanced semantic understanding. The\nDFR framework introduces three key innovations: 1) Multi-modal Decompose: a\nhierarchical decomposition scheme that extracts visual region proposals via\nSAM, expands textual semantics into fine-grained descriptors, and processes\naudio features for contextual enrichment; 2) Multi-modal Contrastive Fuse: a\nfusion strategy employing contrastive learning to maintain consistency across\nvisual, textual, and audio modalities while enabling dynamic semantic\ninteractions between foreground and background features; 3) Dual-path\nReconstruct: an adaptive integration mechanism combining semantic guidance from\ntri-modal fused tokens with geometric cues from multi-modal location priors.\nExtensive experiments across visual, textual, and audio modalities under both\nsynthetic and real settings demonstrate DFR's substantial performance\nimprovements over state-of-the-art methods.", "comment": "3 figures", "pdf_url": "http://arxiv.org/pdf/2507.16736v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16263", "title": "iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss", "authors": ["Yujian Sun", "Tian Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16263v1", "summary": "As the Large Language Model (LLM) gains widespread adoption, increasing\nattention has been given to the challenge of making LLM forget non-compliant\ndata memorized during its pre-training. Machine Unlearning focuses on\nefficiently erasing sensitive information from LLM under limited computational\nresources. To advance research in this area, SemEval 2025 Task 4: \"Unlearning\nSensitive Content from Large Language Models\" introduces three unlearning\ndatasets and establishes a benchmark by evaluating both forgetting\neffectiveness and the preservation of standard capabilities. In this work, we\npropose a more controllable forgetting loss, Effective Unlearning Loss, and\nexplore its integration with various techniques to achieve more efficient and\ncontrolled unlearning. Our system ultimately ranked 5th on the competition\nleaderboard.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16263v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16692", "title": "Generating Search Explanations using Large Language Models", "authors": ["Arif Laksito", "Mark Stevenson"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Extended Abstract - Workshop on Explainability in Information Retrieval (WExIR), SIGIR 2025", "url": "http://arxiv.org/abs/2507.16692v1", "summary": "Aspect-oriented explanations in search results are typically concise text\nsnippets placed alongside retrieved documents to serve as explanations that\nassist users in efficiently locating relevant information. While Large Language\nModels (LLMs) have demonstrated exceptional performance for a range of\nproblems, their potential to generate explanations for search results has not\nbeen explored. This study addresses that gap by leveraging both encoder-decoder\nand decoder-only LLMs to generate explanations for search results. The\nexplanations generated are consistently more accurate and plausible\nexplanations than those produced by a range of baseline models.", "comment": "Extended Abstract - Workshop on Explainability in Information\n  Retrieval (WExIR), SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2507.16692v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.08529", "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qin", "Guoyu Ye", "Ruixiang Tang"], "categories": ["cs.AI", "cs.CL", "68T50, 92C50, 68T05", "J.3; I.2.7; H.3.3; I.2.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      12 pages,3 figures", "url": "http://arxiv.org/abs/2507.08529v2", "summary": "Rare disease diagnosis remains challenging for medical large language models\ndue to insufficient knowledge representation, limited concept understanding,\nand constrained clinical reasoning. We propose a framework combining\nmulti-granularity sparse activation with hierarchical knowledge graphs. Our\napproach employs four complementary matching algorithms with diversity control\nand a five-level fallback strategy for precise concept activation. A\nthree-layer knowledge graph (taxonomy, clinical features, instances) provides\nstructured, up-to-date context. Experiments on the BioASQ rare disease dataset\ndemonstrate significant improvements: BLEU scores increased by up to 0.13,\nROUGE by up to 0.10, and diagnostic accuracy by up to 0.25, with the best model\nachieving 0.92 accuracy--surpassing the 0.90 clinical threshold. Expert\nevaluation confirms enhancements in information quality, reasoning, and\nprofessional expression. Our framework shows promise in reducing the diagnostic\nodyssey for rare disease patients.", "comment": "12 pages,3 figures", "pdf_url": "http://arxiv.org/pdf/2507.08529v2", "cate": "cs.AI", "date": "2025-07-11", "updated": "2025-07-22"}
{"id": "2507.16761", "title": "Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos Networks", "authors": ["Marcel Kleinmann", "Shashank Agnihotri", "Margret Keuper"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16761v1", "summary": "Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. Additionally, the\noriginal B-cos formulation is limited to multi-class settings, whereas chest\nX-ray analysis often requires multi-label classification due to co-occurring\nabnormalities. In this work, we address both limitations: (1) we introduce\nanti-aliasing strategies using FLCPooling (FLC) and BlurPool (BP) to\nsignificantly improve explanation quality, and (2) we extend B-cos networks to\nsupport multi-label classification. Our experiments on chest X-ray datasets\ndemonstrate that the modified $\\text{B-cos}_\\text{FLC}$ and\n$\\text{B-cos}_\\text{BP}$ preserve strong predictive performance while providing\nfaithful and artifact-free explanations suitable for clinical application in\nmulti-label settings. Code available at:\n$\\href{https://github.com/mkleinma/B-cos-medical-paper}{GitHub repository}$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16761v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16743", "title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption", "authors": ["Keneni W. Tesema", "Lyndon Hill", "Mark W. Jones", "Gary K. L. Tam"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for Computers and Graphics and EG Symposium on 3D Object Retrieval 2025 (3DOR'25)", "url": "http://arxiv.org/abs/2507.16743v1", "summary": "Point cloud completion is crucial for 3D computer vision tasks in autonomous\ndriving, augmented reality, and robotics. However, obtaining clean and complete\npoint clouds from real-world environments is challenging due to noise and\nocclusions. Consequently, most existing completion networks -- trained on\nsynthetic data -- struggle with real-world degradations. In this work, we\ntackle the problem of completing and denoising highly corrupted partial point\nclouds affected by multiple simultaneous degradations. To benchmark robustness,\nwe introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which\nhighlights the limitations of current methods under diverse corruptions.\nBuilding on these insights, we propose DWCNet (Denoising-While-Completing\nNetwork), a completion framework enhanced with a Noise Management Module (NMM)\nthat leverages contrastive learning and self-attention to suppress noise and\nmodel structural relationships. DWCNet achieves state-of-the-art performance on\nboth clean and corrupted, synthetic and real-world datasets. The dataset and\ncode will be publicly available at\nhttps://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions", "comment": "Accepted for Computers and Graphics and EG Symposium on 3D Object\n  Retrieval 2025 (3DOR'25)", "pdf_url": "http://arxiv.org/pdf/2507.16743v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16271", "title": "Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction", "authors": ["Tianyun Zhong", "Guozhao Mo", "Yanjiang Liu", "Yihan Chen", "Lingdi Kong", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16271v1", "summary": "With the emergence of large language models (LLMs), there is an expectation\nthat LLMs can effectively extract explicit information from complex real-world\ndocuments (e.g., papers, reports). However, most LLMs generate paragraph-style\nanswers that are chaotic, disorganized, and untraceable. To bridge this gap, we\nintroduce the Arranged and Organized Extraction Benchmark (AOE), a new\nbilingual benchmark with data and documents of varying lengths designed to\nsystematically evaluate the ability of LLMs to comprehend fragmented documents\nand reconstruct isolated information into one organized table. Unlike\nconventional text-to-table tasks, which rely on fixed schema and narrow task\ndomains, AOE includes 11 carefully crafted tasks across three diverse domains,\nrequiring models to generate context-specific schema tailored to varied input\nqueries. In the experiment, we evaluated both open-source and closed-source\nstate-of-the-art LLMs. The results show that even the most advanced models\nstruggled significantly. The benchmark is available at\nhttps://huggingface.co/datasets/tianyumyum/AOE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16271v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16708", "title": "Biases in LLM-Generated Musical Taste Profiles for Recommendation", "authors": ["Bruno Sguerra", "Elena V. Epure", "Harin Lee", "Manuel Moussallam"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16708v1", "summary": "One particularly promising use case of Large Language Models (LLMs) for\nrecommendation is the automatic generation of Natural Language (NL) user taste\nprofiles from consumption data. These profiles offer interpretable and editable\nalternatives to opaque collaborative filtering representations, enabling\ngreater transparency and user control. However, it remains unclear whether\nusers consider these profiles to be an accurate representation of their taste,\nwhich is crucial for trust and usability. Moreover, because LLMs inherit\nsocietal and data-driven biases, profile quality may systematically vary across\nuser and item characteristics. In this paper, we study this issue in the\ncontext of music streaming, where personalization is challenged by a large and\nculturally diverse catalog. We conduct a user study in which participants rate\nNL profiles generated from their own listening histories. We analyze whether\nidentification with the profiles is biased by user attributes (e.g.,\nmainstreamness, taste diversity) and item features (e.g., genre, country of\norigin). We also compare these patterns to those observed when using the\nprofiles in a downstream recommendation task. Our findings highlight both the\npotential and limitations of scrutable, LLM-based profiling in personalized\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16708v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.12821", "title": "Assessing Adaptive World Models in Machines with Novel Games", "authors": ["Lance Ying", "Katherine M. Collins", "Prafull Sharma", "Cedric Colas", "Kaiya Ivy Zhao", "Adrian Weller", "Zenna Tavares", "Phillip Isola", "Samuel J. Gershman", "Jacob D. Andreas", "Thomas L. Griffiths", "Francois Chollet", "Kelsey R. Allen", "Joshua B. Tenenbaum"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      17 pages, 4 figures", "url": "http://arxiv.org/abs/2507.12821v2", "summary": "Human intelligence exhibits a remarkable capacity for rapid adaptation and\neffective problem-solving in novel and unfamiliar contexts. We argue that this\nprofound adaptability is fundamentally linked to the efficient construction and\nrefinement of internal representations of the environment, commonly referred to\nas world models, and we refer to this adaptation mechanism as world model\ninduction. However, current understanding and evaluation of world models in\nartificial intelligence (AI) remains narrow, often focusing on static\nrepresentations learned from training on massive corpora of data, instead of\nthe efficiency and efficacy in learning these representations through\ninteraction and exploration within a novel environment. In this Perspective, we\nprovide a view of world model induction drawing on decades of research in\ncognitive science on how humans learn and adapt so efficiently; we then call\nfor a new evaluation framework for assessing adaptive world models in AI.\nConcretely, we propose a new benchmarking paradigm based on suites of carefully\ndesigned games with genuine, deep and continually refreshing novelty in the\nunderlying game structures -- we refer to this class of games as novel games.\nWe detail key desiderata for constructing these games and propose appropriate\nmetrics to explicitly challenge and evaluate the agent's ability for rapid\nworld model induction. We hope that this new evaluation framework will inspire\nfuture evaluation efforts on world models in AI and provide a crucial step\ntowards developing AI systems capable of human-like rapid adaptation and robust\ngeneralization -- a critical component of artificial general intelligence.", "comment": "17 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.12821v2", "cate": "cs.AI", "date": "2025-07-17", "updated": "2025-07-22"}
{"id": "2507.16802", "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning", "authors": ["Yanjun Zheng", "Xiyang Du", "Longfei Liao", "Xiaoke Zhao", "Zhaowen Zhou", "Bo Zhang", "Jiawei Liu", "Xiang Qi", "Zhe Li", "Zhiqiang Zhang", "Wang Wei", "Peng Zhang"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16802v1", "summary": "Large Language Models (LLMs) demonstrate tremendous potential in the\nfinancial domain, yet existing models often fall short in scenarios demanding\nrobust reasoning capabilities, stringent trustworthiness requirements, and\nefficient adaptation to task-specific needs. We introduce the Agentar-Fin-R1\nseries of financial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task taxonomy with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage learning processes, and detailed attribution systems,\nwe achieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including FinEva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16802v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16753", "title": "CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot Segmentation", "authors": ["Shuai Chen", "Fanman Meng", "Chunjin Yang", "Haoran Wei", "Chenhao Wu", "Qingbo Wu", "Hongliang Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      3 figures", "url": "http://arxiv.org/abs/2507.16753v1", "summary": "Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to\nlimited data and domain shifts. Recent foundation models like the Segment\nAnything Model (SAM) have shown remarkable zero-shot generalization capability\nin general segmentation tasks, making it a promising solution for few-shot\nscenarios. However, adapting SAM to CD-FSS faces two critical challenges:\nreliance on manual prompt and limited cross-domain ability. Therefore, we\npropose the Composable Meta-Prompt (CMP) framework that introduces three key\nmodules: (i) the Reference Complement and Transformation (RCT) module for\nsemantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module\nfor automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction\n(FAI) module for domain discrepancy mitigation. Evaluations across four\ncross-domain datasets demonstrate CMP's state-of-the-art performance, achieving\n71.8\\% and 74.5\\% mIoU in 1-shot and 5-shot scenarios respectively.", "comment": "3 figures", "pdf_url": "http://arxiv.org/pdf/2507.16753v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16284", "title": "Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis", "authors": ["Paul-Andrei Pogcean", "Sanda-Maria Avram"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16284v1", "summary": "The debate surrounding language identification has gained renewed attention\nin recent years, especially with the rapid evolution of AI-powered language\nmodels. However, the non-AI-based approaches to language identification have\nbeen overshadowed. This research explores a mathematical implementation of an\nalgorithm for language determinism by leveraging monograms and bigrams\nfrequency rankings derived from established linguistic research. The datasets\nused comprise texts varying in length, historical period, and genre, including\nshort stories, fairy tales, and poems. Despite these variations, the method\nachieves over 80\\% accuracy on texts shorter than 150 characters and reaches\n100\\% accuracy for longer texts and older writings. These results demonstrate\nthat classical frequency-based approaches remain effective and scalable\nalternatives to AI-driven models for language detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16284v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2410.18527", "title": "Probing Ranking LLMs: A Mechanistic Analysis for Information Retrieval", "authors": ["Tanya Chowdhury", "Atharva Nijasure", "James Allan"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2410.18527v3", "summary": "Transformer networks, particularly those achieving performance comparable to\nGPT models, are well known for their robust feature extraction abilities.\nHowever, the nature of these extracted features and their alignment with\nhuman-engineered ones remain unexplored. In this work, we investigate the\ninternal mechanisms of state-of-the-art, fine-tuned LLMs for passage reranking.\nWe employ a probing-based analysis to examine neuron activations in ranking\nLLMs, identifying the presence of known human-engineered and semantic features.\nOur study spans a broad range of feature categories, including lexical signals,\ndocument structure, query-document interactions, and complex semantic\nrepresentations, to uncover underlying patterns influencing ranking decisions.\n  Through experiments on four different ranking LLMs, we identify statistical\nIR features that are prominently encoded in LLM activations, as well as others\nthat are notably missing. Furthermore, we analyze how these models respond to\nout-of-distribution queries and documents, revealing distinct generalization\nbehaviors. By dissecting the latent representations within LLM activations, we\naim to improve both the interpretability and effectiveness of ranking models.\nOur findings offer crucial insights for developing more transparent and\nreliable retrieval systems, and we release all necessary scripts and code to\nsupport further exploration.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2410.18527v3", "cate": "cs.IR", "date": "2024-10-24", "updated": "2025-07-22"}
{"id": "2507.14447", "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      26 pages, 8 figures, 5 tables", "url": "http://arxiv.org/abs/2507.14447v2", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "comment": "26 pages, 8 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.14447v2", "cate": "cs.AI", "date": "2025-07-19", "updated": "2025-07-22"}
{"id": "2212.14720", "title": "Learning from Data Streams: An Overview and Update", "authors": ["Jesse Read", "Indr liobait"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2212.14720v3", "summary": "The literature on machine learning in the context of data streams is vast and\ngrowing. However, many of the defining assumptions regarding data-stream\nlearning tasks are too strong to hold in practice, or are even contradictory\nsuch that they cannot be met in the contexts of supervised learning. Algorithms\nare chosen and designed based on criteria which are often not clearly stated,\nfor problem settings not clearly defined, tested in unrealistic settings,\nand/or in isolation from related approaches in the wider literature. This puts\ninto question the potential for real-world impact of many approaches conceived\nin such contexts, and risks propagating a misguided research focus. We propose\nto tackle these issues by reformulating the fundamental definitions and\nsettings of supervised data-stream learning with regard to contemporary\nconsiderations of concept drift and temporal dependence; and we take a fresh\nlook at what constitutes a supervised data-stream learning task, and a\nreconsideration of algorithms that may be applied to tackle such tasks. Through\nand in reflection of this formulation and overview, helped by an informal\nsurvey of industrial players dealing with real-world data streams, we provide\nrecommendations. Our main emphasis is that learning from data streams does not\nimpose a single-pass or online-learning approach, or any particular learning\nregime; and any constraints on memory and time are not specific to streaming.\nMeanwhile, there exist established techniques for dealing with temporal\ndependence and concept drift, in other areas of the literature. For the data\nstreams community, we thus encourage a shift in research focus, from dealing\nwith often-artificial constraints and assumptions on the learning mode, to\nissues such as robustness, privacy, and interpretability which are increasingly\nrelevant to learning in data streams in academic and industrial settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2212.14720v3", "cate": "cs.LG", "date": "2022-12-30", "updated": "2025-07-22"}
{"id": "2507.16782", "title": "Task-Specific Zero-shot Quantization-Aware Training for Object Detection", "authors": ["Changhao Li", "Xinrui Chen", "Ji Wang", "Kang Zhao", "Jianfei Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.16782v1", "summary": "Quantization is a key technique to reduce network size and computational\ncomplexity by representing the network parameters with a lower precision.\nTraditional quantization methods rely on access to original training data,\nwhich is often restricted due to privacy concerns or security challenges.\nZero-shot Quantization (ZSQ) addresses this by using synthetic data generated\nfrom pre-trained models, eliminating the need for real training data. Recently,\nZSQ has been extended to object detection. However, existing methods use\nunlabeled task-agnostic synthetic images that lack the specific information\nrequired for object detection, leading to suboptimal performance. In this\npaper, we propose a novel task-specific ZSQ framework for object detection\nnetworks, which consists of two main stages. First, we introduce a bounding box\nand category sampling strategy to synthesize a task-specific calibration set\nfrom the pre-trained network, reconstructing object locations, sizes, and\ncategory distributions without any prior knowledge. Second, we integrate\ntask-specific training into the knowledge distillation process to restore the\nperformance of quantized detection networks. Extensive experiments conducted on\nthe MS-COCO and Pascal VOC datasets demonstrate the efficiency and\nstate-of-the-art performance of our method. Our code is publicly available at:\nhttps://github.com/DFQ-Dojo/dfq-toolkit .", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.16782v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16323", "title": "SpeLLM: Character-Level Multi-Head Decoding", "authors": ["Amit Ben-Artzy", "Roy Schwartz"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16323v1", "summary": "Scaling LLM vocabulary is often used to reduce input sequence length and\nalleviate attention's quadratic cost. Yet, current LLM architectures impose a\ncritical bottleneck to this procedure: the output projection layer scales\nlinearly with vocabulary size, rendering substantial expansion impractical. We\npropose SpeLLM, a method that decouples input and output vocabularies by\npredicting character-level strings through multiple output heads. In SpeLLM,\neach of the $k$ linear heads predicts a single character simultaneously,\nenabling the model to represent a much larger output space using smaller,\nindependent linear heads. We present a self-distillation approach for\nconverting a standard LLM to a SpeLLM. Our experiments with four pre-trained\nLLMs show their SpeLLM variants achieve competitive performance on downstream\ntasks while reducing runtime by 5.1% on average across models. Our approach\nprovides a potential avenue for reducing LLM costs, while increasing support\nfor underrepresented languages and domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16323v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2502.04645", "title": "Cross-Encoder Rediscovers a Semantic Variant of BM25", "authors": ["Meng Lu", "Catherine Chen", "Carsten Eickhoff"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.04645v2", "summary": "Neural Ranking Models (NRMs) have rapidly advanced state-of-the-art\nperformance on information retrieval tasks. In this work, we investigate a\nCross-Encoder variant of MiniLM to determine which relevance features it\ncomputes and where they are stored. We find that it employs a semantic variant\nof the traditional BM25 in an interpretable manner, featuring localized\ncomponents: (1) Transformer attention heads that compute soft term frequency\nwhile controlling for term saturation and document length effects, and (2) a\nlow-rank component of its embedding matrix that encodes inverse document\nfrequency information for the vocabulary. This suggests that the Cross-Encoder\nuses the same fundamental mechanisms as BM25, but further leverages their\ncapacity to capture semantics for improved retrieval performance. The granular\nunderstanding lays the groundwork for model editing to enhance model\ntransparency, addressing safety concerns, and improving scalability in training\nand real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.04645v2", "cate": "cs.IR", "date": "2025-02-07", "updated": "2025-07-22"}
{"id": "2507.16024", "title": "Structure-preserving deflation of critical eigenvalues in quadratic eigenvalue problems associated with damped mass-spring systems", "authors": ["Rafikul Alam", "Volker Mehrmann", "Ninoslav Truhar"], "categories": ["math.NA", "cs.NA", "math.SP", "65F15, 15A57, 15A18, 65F35"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16024v1", "summary": "For a quadratic matrix polynomial associated with a damped mass-spring system\nthere are three types of critical eigenvalues, the eigenvalues $\\infty$ and $0$\nand the eigenvalues on the imaginary axis. All these are on the boundary of the\nset of (robustly) stable eigenvalues. For numerical methods, but also for\n(robust) stability analysis, it is desirable to deflate such eigenvalues by\nprojecting the matrix polynomial to a lower dimensional subspace before\ncomputing the other eigenvalues and eigenvectors. We describe\nstructure-preserving deflation strategies that deflate these eigenvalues via a\ntrimmed structure-preserving linearization. We employ these results for the\nspecial case of hyperbolic problems. We also analyze the effect of a (possibly\nlow rank) parametric damping matrix on purely imaginary eigenvalues.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16024v1", "cate": "math.NA", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.14468", "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning", "authors": ["Yitong Lin", "Jiaying He", "Jiahe Chen", "Xinnan Zhu", "Jianwei Zheng", "Tao Bo"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by Bioinformatics on July 11th", "url": "http://arxiv.org/abs/2507.14468v2", "summary": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery\nand disease understanding, yet their completion and reasoning are challenging.\nKnowledge Embedding (KE) methods capture global semantics but struggle with\ndynamic structural integration, while Graph Neural Networks (GNNs) excel\nlocally but often lack semantic understanding. Even ensemble approaches,\nincluding those leveraging language models, often fail to achieve a deep,\nadaptive, and synergistic co-evolution between semantic comprehension and\nstructural learning. Addressing this critical gap in fostering continuous,\nreciprocal refinement between these two aspects in complex biomedical KGs is\nparamount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply\nsynergistic semantic and structural learning. BioGraphFusion establishes a\nglobal semantic foundation via tensor decomposition, guiding an LSTM-driven\nmechanism to dynamically refine relation embeddings during graph propagation.\nThis fosters adaptive interplay between semantic understanding and structural\nlearning, further enhanced by query-guided subgraph construction and a hybrid\nscoring mechanism. Experiments across three key biomedical tasks demonstrate\nBioGraphFusion's superior performance over state-of-the-art KE, GNN, and\nensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)\nhighlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely\navailable for download at https://github.com/Y-TARL/BioGraphFusion.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.", "comment": "Accepted by Bioinformatics on July 11th", "pdf_url": "http://arxiv.org/pdf/2507.14468v2", "cate": "cs.AI", "date": "2025-07-19", "updated": "2025-07-22"}
{"id": "2310.07497", "title": "Energy-Efficient and Real-Time Sensing for Federated Continual Learning via Sample-Driven Control", "authors": ["Minh Ngoc Luu", "Minh-Duong Nguyen", "Ebrahim Bedeer", "Van Duc Nguyen", "Dinh Thai Hoang", "Diep N. Nguyen", "Quoc-Viet Pham"], "categories": ["cs.LG", "cs.AI", "68-00", "I.2.11"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2310.07497v2", "summary": "An intelligent Real-Time Sensing (RTS) system must continuously acquire,\nupdate, integrate, and apply knowledge to adapt to real-world dynamics.\nManaging distributed intelligence in this context requires Federated Continual\nLearning (FCL). However, effectively capturing the diverse characteristics of\nRTS data in FCL systems poses significant challenges, including severely\nimpacting computational and communication resources, escalating energy costs,\nand ultimately degrading overall system performance. To overcome these\nchallenges, we investigate how the data distribution shift from ideal to\npractical RTS scenarios affects Artificial Intelligence (AI) model performance\nby leveraging the \\textit{generalization gap} concept. In this way, we can\nanalyze how sampling time in RTS correlates with the decline in AI performance,\ncomputation cost, and communication efficiency. Based on this observation, we\ndevelop a novel Sample-driven Control for Federated Continual Learning (SCFL)\ntechnique, specifically designed for mobile edge networks with RTS\ncapabilities. In particular, SCFL is an optimization problem that harnesses the\nsampling process to concurrently minimize the generalization gap and improve\noverall accuracy while upholding the energy efficiency of the FCL framework. To\nsolve the highly complex and time-varying optimization problem, we introduce a\nnew soft actor-critic algorithm with explicit and implicit constraints\n(A2C-EI). Our empirical experiments reveal that we can achieve higher\nefficiency compared to other DRL baselines. Notably, SCFL can significantly\nreduce energy consumption up to $85\\%$ while maintaining FL convergence and\ntimely data transmission.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2310.07497v2", "cate": "cs.LG", "date": "2023-10-11", "updated": "2025-07-22"}
{"id": "2507.16790", "title": "Enhancing Domain Diversity in Synthetic Data Face Recognition with Dataset Fusion", "authors": ["Anjith George", "Sebastien Marcel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ICCV Workshops 2025", "url": "http://arxiv.org/abs/2507.16790v1", "summary": "While the accuracy of face recognition systems has improved significantly in\nrecent years, the datasets used to train these models are often collected\nthrough web crawling without the explicit consent of users, raising ethical and\nprivacy concerns. To address this, many recent approaches have explored the use\nof synthetic data for training face recognition models. However, these models\ntypically underperform compared to those trained on real-world data. A common\nlimitation is that a single generator model is often used to create the entire\nsynthetic dataset, leading to model-specific artifacts that may cause\noverfitting to the generator's inherent biases and artifacts. In this work, we\npropose a solution by combining two state-of-the-art synthetic face datasets\ngenerated using architecturally distinct backbones. This fusion reduces\nmodel-specific artifacts, enhances diversity in pose, lighting, and\ndemographics, and implicitly regularizes the face recognition model by\nemphasizing identity-relevant features. We evaluate the performance of models\ntrained on this combined dataset using standard face recognition benchmarks and\ndemonstrate that our approach achieves superior performance across many of\nthese benchmarks.", "comment": "Accepted in ICCV Workshops 2025", "pdf_url": "http://arxiv.org/pdf/2507.16790v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16331", "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny", "authors": ["Chuanhao Yan", "Fengdi Che", "Xuhan Huang", "Xu Xu", "Xin Li", "Yizhi Li", "Xingwei Qu", "Jingzhe Shi", "Zhuangzhuang He", "Chenghua Lin", "Yaodong Yang", "Binhang Yuan", "Hang Zhao", "Yu Qiao", "Bowen Zhou", "Jie Fu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16331v1", "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16331v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.07664", "title": "Antibiotic Resistance Microbiology Dataset (ARMD): A Resource for Antimicrobial Resistance from EHRs", "authors": ["Fateme Nateghi Haredasht", "Fatemeh Amrollahi", "Manoj Maddali", "Nicholas Marshall", "Stephen P. Ma", "Lauren N. Cooper", "Andrew O. Johnson", "Ziming Wei", "Richard J. Medford", "Sanjat Kanjilal", "Niaz Banaei", "Stanley Deresinski", "Mary K. Goldstein", "Steven M. Asch", "Amy Chang", "Jonathan H. Chen"], "categories": ["q-bio.QM", "cs.IR", "cs.LG", "stat.AP"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.07664v2", "summary": "The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified\nresource derived from electronic health records (EHR) that facilitates research\nin antimicrobial resistance (AMR). ARMD encompasses big data from adult\npatients collected from over 15 years at two academic-affiliated hospitals,\nfocusing on microbiological cultures, antibiotic susceptibilities, and\nassociated clinical and demographic features. Key attributes include organism\nidentification, susceptibility patterns for 55 antibiotics, implied\nsusceptibility rules, and de-identified patient information. This dataset\nsupports studies on antimicrobial stewardship, causal inference, and clinical\ndecision-making. ARMD is designed to be reusable and interoperable, promoting\ncollaboration and innovation in combating AMR. This paper describes the\ndataset's acquisition, structure, and utility while detailing its\nde-identification process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.07664v2", "cate": "q-bio.QM", "date": "2025-03-08", "updated": "2025-07-21"}
{"id": "2507.16344", "title": "Diff-ANO: Towards Fast High-Resolution Ultrasound Computed Tomography via Conditional Consistency Models and Adjoint Neural Operators", "authors": ["Xiang Cao", "Qiaoqiao Ding", "Xinliang Liu", "Lei Zhang", "Xiaoqun Zhang"], "categories": ["math.NA", "cs.NA", "92C55, 35R30, 65N21, 68T07, 60J60"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      27 pages, 10 figures, 6 tables", "url": "http://arxiv.org/abs/2507.16344v1", "summary": "Ultrasound Computed Tomography (USCT) constitutes a nonlinear inverse problem\nwith inherent ill-posedness that can benefit from regularization through\ndiffusion generative priors. However, traditional approaches for solving\nHelmholtz equation-constrained USCT face three fundamental challenges when\nintegrating these priors: PDE-constrained gradient computation,\ndiscretization-induced approximation errors, and computational imbalance\nbetween neural networks and numerical PDE solvers. In this work, we introduce\n\\textbf{Diff-ANO} (\\textbf{Diff}usion-based Models with \\textbf{A}djoint\n\\textbf{N}eural \\textbf{O}perators), a novel framework that combines\nconditional consistency models with adjoint operator learning to address these\nlimitations. Our two key innovations include: (1) a \\textit{conditional\nconsistency model} that enables measurement-conditional few-step sampling by\ndirectly learning a self-consistent mapping from diffusion trajectories, and\n(2) an \\textit{adjoint operator learning} module that replaces traditional PDE\nsolvers with neural operator surrogates for efficient adjoint-based gradient\ncomputation. To enable practical deployment, we introduce the batch-based\nConvergent Born Series (BCBS)--a memory-efficient strategy for online\ngeneration of neural operator training pairs. Comprehensive experiments\ndemonstrate that Diff-ANO significantly improves both computational efficiency\nand reconstruction quality, especially under sparse-view and partial-view\nmeasurement scenarios.", "comment": "27 pages, 10 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.16344v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15844", "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Code: this https URL Project Page: this https URL", "url": "http://arxiv.org/abs/2507.15844v2", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "pdf_url": "http://arxiv.org/pdf/2507.15844v2", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2402.14922", "title": "Practical Insights into Knowledge Distillation for Pre-Trained Models", "authors": ["Norah Alballa", "Ahmed M. Abdelmoniem", "Marco Canini"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.14922v2", "summary": "This research investigates the enhancement of knowledge distillation (KD)\nprocesses in pre-trained models, an emerging field in knowledge transfer with\nsignificant implications for distributed training and federated learning\nenvironments. These environments benefit from reduced communication demands and\naccommodate various model architectures. Despite the adoption of numerous KD\napproaches for transferring knowledge among pre-trained models, a comprehensive\nunderstanding of KD's application in these scenarios is lacking. Our study\nconducts an extensive comparison of multiple KD techniques, including standard\nKD, tuned KD (via optimized temperature and weight parameters), deep mutual\nlearning, and data partitioning KD. We assess these methods across various data\ndistribution strategies to identify the most effective contexts for each.\nThrough detailed examination of hyperparameter tuning, informed by extensive\ngrid search evaluations, we pinpoint when adjustments are crucial to enhance\nmodel performance. This paper sheds light on optimal hyperparameter settings\nfor distinct data partitioning scenarios and investigates KD's role in\nimproving federated learning by minimizing communication rounds and expediting\nthe training process. By filling a notable void in current research, our\nfindings serve as a practical framework for leveraging KD in pre-trained models\nwithin collaborative and federated learning frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.14922v2", "cate": "cs.LG", "date": "2024-02-22", "updated": "2025-07-22"}
{"id": "2507.16813", "title": "HOComp: Interaction-Aware Human-Object Composition", "authors": ["Dong Liang", "Jinyuan Jia", "Yuhao Liu", "Rynson W. H. Lau"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16813v1", "summary": "While existing image-guided composition methods may help insert a foreground\nobject onto a user-specified region of a background image, achieving natural\nblending inside the region with the rest of the image unchanged, we observe\nthat these existing methods often struggle in synthesizing seamless\ninteraction-aware compositions when the task involves human-object\ninteractions. In this paper, we first propose HOComp, a novel approach for\ncompositing a foreground object onto a human-centric background image, while\nensuring harmonious interactions between the foreground object and the\nbackground person and their consistent appearances. Our approach includes two\nkey designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes\nMLLMs to identify the interaction region as well as the interaction type (e.g.,\nholding and lefting) to provide coarse-to-fine constraints to the generated\npose for the interaction while incorporating human pose landmarks to track\naction variations and enforcing fine-grained pose constraints; and (2)\nDetail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware\nattention modulation mechanism, a multi-view appearance loss, and a background\nconsistency loss to ensure consistent shapes/textures of the foreground and\nfaithful reproduction of the background human. We then propose the first\ndataset, named Interaction-aware Human-Object Composition (IHOC), for the task.\nExperimental results on our dataset show that HOComp effectively generates\nharmonious human-object interactions with consistent appearances, and\noutperforms relevant methods qualitatively and quantitatively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16813v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16442", "title": "Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch", "authors": ["Elza Strazda", "Gerasimos Spanakis"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, accepted at RANLP 2025 data and code here: this https URL", "url": "http://arxiv.org/abs/2507.16442v1", "summary": "Warning: This paper contains explicit statements of offensive stereotypes\nwhich might be upsetting.\n  Language models are prone to exhibiting biases, further amplifying unfair and\nharmful stereotypes. Given the fast-growing popularity and wide application of\nthese models, it is necessary to ensure safe and fair language models. As of\nrecent considerable attention has been paid to measuring bias in language\nmodels, yet the majority of studies have focused only on English language. A\nDutch version of the US-specific CrowS-Pairs dataset for measuring bias in\nDutch language models is introduced. The resulting dataset consists of 1463\nsentence pairs that cover bias in 9 categories, such as Sexual orientation,\nGender and Disability. The sentence pairs are composed of contrasting\nsentences, where one of the sentences concerns disadvantaged groups and the\nother advantaged groups. Using the Dutch CrowS-Pairs dataset, we show that\nvarious language models, BERTje, RobBERT, multilingual BERT, GEITje and\nMistral-7B exhibit substantial bias across the various bias categories. Using\nthe English and French versions of the CrowS-Pairs dataset, bias was evaluated\nin English (BERT and RoBERTa) and French (FlauBERT and CamemBERT) language\nmodels, and it was shown that English models exhibit the most bias, whereas\nDutch models the least amount of bias. Additionally, results also indicate that\nassigning a persona to a language model changes the level of bias it exhibits.\nThese findings highlight the variability of bias across languages and contexts,\nsuggesting that cultural and linguistic factors play a significant role in\nshaping model biases.", "comment": "10 pages, accepted at RANLP 2025 data and code here:\n  https://github.com/jerryspan/Dutch-CrowS-Pairs", "pdf_url": "http://arxiv.org/pdf/2507.16442v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.15879", "title": "Typed-RAG: Type-Aware Decomposition of Non-Factoid Questions for Retrieval-Augmented Generation", "authors": ["DongGeon Lee", "Ahjeong Park", "Hyeri Lee", "Hyeonseo Nam", "Yunho Maeng"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to XLLM@ACL 2025", "url": "http://arxiv.org/abs/2503.15879v3", "summary": "Addressing non-factoid question answering (NFQA) remains challenging due to\nits open-ended nature, diverse user intents, and need for multi-aspect\nreasoning. These characteristics often reveal the limitations of conventional\nretrieval-augmented generation (RAG) approaches. To overcome these challenges,\nwe propose Typed-RAG, a framework for type-aware decomposition of non-factoid\nquestions (NFQs) within the RAG paradigm. Specifically, Typed-RAG first\nclassifies an NFQ into a predefined type (e.g., Debate, Experience,\nComparison). It then decomposes the question into focused sub-queries, each\nfocusing on a single aspect. This decomposition enhances both retrieval\nrelevance and answer quality. By combining the results of these sub-queries,\nTyped-RAG produces more informative and contextually aligned responses.\nAdditionally, we construct Wiki-NFQA, a benchmark dataset for NFQA covering a\nwide range of NFQ types. Experiments show that Typed-RAG consistently\noutperforms existing QA approaches based on LLMs or RAG methods, validating the\neffectiveness of type-aware decomposition for improving both retrieval quality\nand answer generation in NFQA. Our code and dataset are available on\nhttps://github.com/TeamNLP/Typed-RAG.", "comment": "Accepted to XLLM@ACL 2025", "pdf_url": "http://arxiv.org/pdf/2503.15879v3", "cate": "cs.CL", "date": "2025-03-20", "updated": "2025-07-22"}
{"id": "2507.16349", "title": "Neural Network Acceleration of Iterative Methods for Nonlinear Schrdinger Eigenvalue Problems", "authors": ["Daniel Peterseim", "Jan-F. Pietschmann", "Jonas Pschel", "Kilian Ruess"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      19 Pages, 22 figures", "url": "http://arxiv.org/abs/2507.16349v1", "summary": "We present a novel approach to accelerate iterative methods to solve\nnonlinear Schr\\\"odinger eigenvalue problems using neural networks. Nonlinear\neigenvector problems are fundamental in quantum mechanics and other fields, yet\nconventional solvers often suffer from slow convergence in extreme parameter\nregimes, as exemplified by the rotating Bose- Einstein condensate (BEC)\nproblem. Our method uses a neural network to predict and refine solution\ntrajectories, leveraging knowledge from previous simulations to improve\nconvergence speed and accuracy. Numerical experiments demonstrate significant\nspeed-up over classical solvers, highlighting both the strengths and\nlimitations of the approach.", "comment": "19 Pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.16349v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15855", "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "authors": ["Yichen Huang", "Lin F. Yang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15855v2", "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly (up to a caveat discussed below). This result underscores\nthe importance of developing optimal strategies to harness the full potential\nof powerful LLMs for complex reasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15855v2", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2403.12938", "title": "Learning Neural Differential Algebraic Equations via Operator Splitting", "authors": ["James Koch", "Madelyn Shapiro", "Himanshu Sharma", "Draguna Vrabie", "Jan Drgona"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Updated version of the article now includes problem statement", "url": "http://arxiv.org/abs/2403.12938v3", "summary": "Differential algebraic equations (DAEs) describe the temporal evolution of\nsystems that obey both differential and algebraic constraints. Of particular\ninterest are systems that contain implicit relationships between their\ncomponents, such as conservation laws. Here, we present an Operator Splitting\n(OS) numerical integration scheme for learning unknown components of DAEs from\ntime-series data. In this work, we show that the proposed OS-based\ntime-stepping scheme is suitable for relevant system-theoretic data-driven\nmodeling tasks. Presented examples include (i) the inverse problem of\ntank-manifold dynamics and (ii) discrepancy modeling of a network of pumps,\ntanks, and pipes. Our experiments demonstrate the proposed method's robustness\nto noise and extrapolation ability to (i) learn the behaviors of the system\ncomponents and their interaction physics and (ii) disambiguate between data\ntrends and mechanistic relationships contained in the system.", "comment": "Updated version of the article now includes problem statement", "pdf_url": "http://arxiv.org/pdf/2403.12938v3", "cate": "cs.LG", "date": "2024-03-19", "updated": "2025-07-21"}
{"id": "2507.16065", "title": "Handcrafted vs. Deep Radiomics vs. Fusion vs. Deep Learning: A Comprehensive Review of Machine Learning -Based Cancer Outcome Prediction in PET and SPECT Imaging", "authors": ["Mohammad R. Salmanpour", "Somayeh Sadat Mehrnia", "Sajad Jabarzadeh Ghandilu", "Zhino Safahi", "Sonya Falahati", "Shahram Taeb", "Ghazal Mousavi", "Mehdi Maghsoudi", "Ahmad Shariftabrizi", "Ilker Hacihaliloglu", "Arman Rahmim"], "categories": ["physics.med-ph", "cs.CV"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16065v1", "summary": "Machine learning (ML), including deep learning (DL) and radiomics-based\nmethods, is increasingly used for cancer outcome prediction with PET and SPECT\nimaging. However, the comparative performance of handcrafted radiomics features\n(HRF), deep radiomics features (DRF), DL models, and hybrid fusion approaches\nremains inconsistent across clinical applications. This systematic review\nanalyzed 226 studies published from 2020 to 2025 that applied ML to PET or\nSPECT imaging for outcome prediction. Each study was evaluated using a 59-item\nframework covering dataset construction, feature extraction, validation\nmethods, interpretability, and risk of bias. We extracted key details including\nmodel type, cancer site, imaging modality, and performance metrics such as\naccuracy and area under the curve (AUC). PET-based studies (95%) generally\noutperformed those using SPECT, likely due to higher spatial resolution and\nsensitivity. DRF models achieved the highest mean accuracy (0.862), while\nfusion models yielded the highest AUC (0.861). ANOVA confirmed significant\ndifferences in performance (accuracy: p=0.0006, AUC: p=0.0027). Common\nlimitations included inadequate handling of class imbalance (59%), missing data\n(29%), and low population diversity (19%). Only 48% of studies adhered to IBSI\nstandards. These findings highlight the need for standardized pipelines,\nimproved data quality, and explainable AI to support clinical integration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16065v1", "cate": "physics.med-ph", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.16459", "title": "Towards Enforcing Company Policy Adherence in Agentic Workflows", "authors": ["Naama Zwerdling", "David Boaz", "Ella Rabinovich", "Guy Uziel", "David Amid", "Ateret Anaby-Tavor"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.16459v1", "summary": "Large Language Model (LLM) agents hold promise for a flexible and scalable\nalternative to traditional business process automation, but struggle to\nreliably follow complex company policies. In this study we introduce a\ndeterministic, transparent, and modular framework for enforcing business policy\nadherence in agentic workflows. Our method operates in two phases: (1) an\noffline buildtime stage that compiles policy documents into verifiable guard\ncode associated with tool use, and (2) a runtime integration where these guards\nensure compliance before each agent action. We demonstrate our approach on the\nchallenging $\\tau$-bench Airlines domain, showing encouraging preliminary\nresults in policy enforcement, and further outline key challenges for\nreal-world deployments.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.16459v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.14096", "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Dina Demner-Fushman"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14096v2", "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14096v2", "cate": "cs.CL", "date": "2025-07-18", "updated": "2025-07-21"}
{"id": "2507.16415", "title": "Entropic approximations of the semigeostrophic shallow water equations", "authors": ["Jean-David Benamou", "Colin J. Cotter", "Jacob J. M. Francis", "Hugo Malamut"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16415v1", "summary": "We develop a discretisation of the semigeostrophic rotating shallow water\nequations, based upon their optimal transport formulation. This takes the form\nof a Moreau-Yoshida regularisation of the Wasserstein metric. Solutions of the\noptimal transport formulation provide the shallow water layer depth represented\nas a measure, which is itself the push forward of an evolving measure under the\nsemigeostrophic coordinate transformation. First, we propose and study an\nentropy regularised version of the rotating shallow water equations. Second, we\ndiscretise the regularised problem by replacing both measures with weighted\nsums of Dirac measures, and approximate the (squared) L2 norm of the layer\ndepth, which defines the potential energy. We propose an iterative method to\nsolve the discrete optimisation problem relating the two measures, and analyse\nits convergence. The iterative method is demonstrated numerically and applied\nto the solution of the time-dependent shallow water problem in numerical\nexamples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16415v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2405.17527", "title": "Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers", "authors": ["Hang Zhou", "Yuezhou Ma", "Haixu Wu", "Haowen Wang", "Mingsheng Long"], "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.17527v4", "summary": "Deep models have recently emerged as promising tools to solve partial\ndifferential equations (PDEs), known as neural PDE solvers. While neural\nsolvers trained from either simulation data or physics-informed loss can solve\nPDEs reasonably well, they are mainly restricted to a few instances of PDEs,\ne.g. a certain equation with a limited set of coefficients. This limits their\ngeneralization to diverse PDEs, preventing them from being practical surrogate\nmodels of numerical solvers. In this paper, we present Unisolver, a novel\nTransformer model trained on diverse data and conditioned on diverse PDEs,\naiming towards a universal neural PDE solver capable of solving a wide scope of\nPDEs. Instead of purely scaling up data and parameters, Unisolver stems from\nthe theoretical analysis of the PDE-solving process. Inspired by the\nmathematical structure of PDEs that a PDE solution is fundamentally governed by\na series of PDE components such as equation symbols and boundary conditions, we\ndefine a complete set of PDE components and flexibly embed them as domain-wise\nand point-wise deep conditions for Transformer PDE solvers. Integrating\nphysical insights with recent Transformer advances, Unisolver achieves\nconsistent state-of-the-art on three challenging large-scale benchmarks,\nshowing impressive performance and generalizability. Code is available at\nhttps://github.com/thuml/Unisolver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.17527v4", "cate": "cs.LG", "date": "2024-05-27", "updated": "2025-07-22"}
{"id": "2407.17029", "title": "Accurate and Efficient Fine-Tuning of Quantized Large Language Models Through Optimal Balance", "authors": ["Ao Shen", "Qiang Wang", "Zhiquan Lai", "Xionglve Li", "Dongsheng Li"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.17029v2", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious domains. However, the enormous number of model parameters makes\nfine-tuning challenging, significantly limiting their application and\ndeployment. Existing solutions combine parameter quantization with Low-Rank\nAdaptation (LoRA), reducing memory usage but causing performance degradation.\nAdditionally, converting fine-tuned models to low-precision representations\nfurther degrades performance. In this paper, we identify an imbalance in\nfine-tuning quantized LLMs with LoRA: overly complex adapter inputs and outputs\nversus low effective trainability of the adapter, leading to underfitting\nduring fine-tuning. Thus, we propose Quantized LLMs fine-tuning with Balanced\nLow-Rank Adaptation (Q-BLoRA), which simplifies the adapter's inputs and\noutputs while increasing the adapter's rank to alleviate underfitting during\nfine-tuning. For low-precision deployment, we propose Quantization-Aware\nfine-tuning with Balanced Low-Rank Adaptation (QA-BLoRA), which aligns with the\nblock-wise quantization and facilitates quantization-aware fine-tuning of\nlow-rank adaptation based on the parameter merging of Q-BLoRA. Both Q-BLoRA and\nQA-BLoRA are easily implemented and offer the following optimizations: (i)\nQ-BLoRA consistently achieves state-of-the-art accuracy compared to baselines\nand other variants; (ii) QA-BLoRA enables the direct generation of\nlow-precision inference models, which exhibit significant performance\nimprovements over other low-precision models. We validate the effectiveness of\nQ-BLoRA and QA-BLoRA across various models and scenarios.\n  Code will be made available at\n\\href{https://github.com/xiaocaigou/qbaraqahira}{https://github.com/xiaocaigou/qbaraqahira}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.17029v2", "cate": "cs.LG", "date": "2024-07-24", "updated": "2025-07-22"}
{"id": "2207.00477", "title": "Vision-based Conflict Detection within Crowds based on High-Resolution Human Pose Estimation for Smart and Safe Airport", "authors": ["Karan Kheta", "Claire Delgove", "Ruolin Liu", "Adeola Aderogba", "Marc-Olivier Pokam", "Muhammed Mehmet Unal", "Yang Xing", "Weisi Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      One of the authors has expressed privacy concerns and made a related request", "url": "http://arxiv.org/abs/2207.00477v2", "summary": "Future airports are becoming more complex and congested with the increasing\nnumber of travellers. While the airports are more likely to become hotspots for\npotential conflicts to break out which can cause serious delays to flights and\nseveral safety issues. An intelligent algorithm which renders security\nsurveillance more effective in detecting conflicts would bring many benefits to\nthe passengers in terms of their safety, finance, and travelling efficiency.\nThis paper details the development of a machine learning model to classify\nconflicting behaviour in a crowd. HRNet is used to segment the images and then\ntwo approaches are taken to classify the poses of people in the frame via\nmultiple classifiers. Among them, it was found that the support vector machine\n(SVM) achieved the most performant achieving precision of 94.37%. Where the\nmodel falls short is against ambiguous behaviour such as a hug or losing track\nof a subject in the frame. The resulting model has potential for deployment\nwithin an airport if improvements are made to cope with the vast number of\npotential passengers in view as well as training against further ambiguous\nbehaviours which will arise in an airport setting. In turn, will provide the\ncapability to enhance security surveillance and improve airport safety.", "comment": "One of the authors has expressed privacy concerns and made a related\n  request", "pdf_url": "http://arxiv.org/pdf/2207.00477v2", "cate": "cs.CV", "date": "2022-07-01", "updated": "2025-07-22"}
{"id": "2507.16530", "title": "Learning Text Styles: A Study on Transfer, Attribution, and Verification", "authors": ["Zhiqiang Hu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      PhD thesis", "url": "http://arxiv.org/abs/2507.16530v1", "summary": "This thesis advances the computational understanding and manipulation of text\nstyles through three interconnected pillars: (1) Text Style Transfer (TST),\nwhich alters stylistic properties (e.g., sentiment, formality) while preserving\ncontent; (2)Authorship Attribution (AA), identifying the author of a text via\nstylistic fingerprints; and (3) Authorship Verification (AV), determining\nwhether two texts share the same authorship. We address critical challenges in\nthese areas by leveraging parameter-efficient adaptation of large language\nmodels (LLMs), contrastive disentanglement of stylistic features, and\ninstruction-based fine-tuning for explainable verification.", "comment": "PhD thesis", "pdf_url": "http://arxiv.org/pdf/2507.16530v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16484", "title": "On finite precision block Lanczos computations", "authors": ["Dorota imonov", "Petr Tich"], "categories": ["math.NA", "cs.NA", "65F10, 65F15, 65G50"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      26 pages; 9 figures", "url": "http://arxiv.org/abs/2507.16484v1", "summary": "In her seminal 1989 work, Greenbaum demonstrated that the results produced by\nthe finite precision Lanczos algorithm after $k$ iterations can be interpreted\nas exact Lanczos results applied to a larger matrix, whose eigenvalues lie in\nsmall intervals around those of the original matrix. This establishes a\nmathematical model for finite precision Lanczos computations. In this paper, we\nextend these ideas to the block Lanczos algorithm. We generalize the\ncontinuation process and show that it can be completed in a finite number of\niterations using carefully constructed perturbations. The block tridiagonal\nmatrices produced after $k$ iterations can then be interpreted as arising from\nthe exact block Lanczos algorithm applied to a larger model matrix. We derive\nsufficient conditions under which the required perturbations remain small,\nensuring that the eigenvalues of the model matrix stay close to those of the\noriginal matrix. While in the single-vector case these conditions are always\nsatisfiable, as shown by Greenbaum based on results by Paige, the question of\nwhether they can always be satisfied in the block case remains open. Finally,\nwe present numerical experiments demonstrating a practical implementation of\nthe continuation process and empirically assess the validity of the sufficient\nconditions and the size of the perturbations.", "comment": "26 pages; 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.16484v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2408.00998", "title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation", "authors": ["Xiang Gao", "Jiaying Liu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted conference paper of ACM MM 2024", "url": "http://arxiv.org/abs/2408.00998v4", "summary": "Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.", "comment": "Accepted conference paper of ACM MM 2024", "pdf_url": "http://arxiv.org/pdf/2408.00998v4", "cate": "cs.CV", "date": "2024-08-02", "updated": "2025-07-22"}
{"id": "2408.08655", "title": "FLAIN: Mitigating Backdoor Attacks in Federated Learning via Flipping Weight Updates of Low-Activation Input Neurons", "authors": ["Binbin Ding", "Penghui Yang", "Sheng-Jun Huang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures, ICMR'25. Updated author information and improved experiments in v2", "url": "http://arxiv.org/abs/2408.08655v2", "summary": "Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models under the coordination of a central server, while\nmaintaining privacy. However, the server cannot directly monitor the local\ntraining processes, leaving room for malicious clients to introduce backdoors\ninto the model. Research has shown that backdoor attacks exploit specific\nneurons that are activated only by malicious inputs, remaining dormant with\nclean data. Building on this insight, we propose a novel defense method called\nFlipping Weight Updates of Low-Activation Input Neurons (FLAIN) to counter\nbackdoor attacks in FL. Specifically, upon the completion of global training,\nwe use an auxiliary dataset to identify low-activation input neurons and\niteratively flip their associated weight updates. This flipping process\ncontinues while progressively raising the threshold for low-activation neurons,\nuntil the model's performance on the auxiliary data begins to degrade\nsignificantly. Extensive experiments demonstrate that FLAIN effectively reduces\nthe success rate of backdoor attacks across a variety of scenarios, including\nNon-IID data distributions and high malicious client ratios (MCR), while\nmaintaining minimal impact on the performance of clean data.", "comment": "9 pages, 4 figures, ICMR'25. Updated author information and improved\n  experiments in v2", "pdf_url": "http://arxiv.org/pdf/2408.08655v2", "cate": "cs.LG", "date": "2024-08-16", "updated": "2025-07-22"}
{"id": "2403.08255", "title": "Make Me Happier: Evoking Emotions Through Image Diffusion Models", "authors": ["Qing Lin", "Jingfeng Zhang", "Yew-Soon Ong", "Mengmi Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.08255v4", "summary": "Despite the rapid progress in image generation, emotional image editing\nremains under-explored. The semantics, context, and structure of an image can\nevoke emotional responses, making emotional image editing techniques valuable\nfor various real-world applications, including treatment of psychological\ndisorders, commercialization of products, and artistic design. First, we\npresent a novel challenge of emotion-evoked image generation, aiming to\nsynthesize images that evoke target emotions while retaining the semantics and\nstructures of the original scenes. To address this challenge, we propose a\ndiffusion model capable of effectively understanding and editing source images\nto convey desired emotions and sentiments. Moreover, due to the lack of emotion\nediting datasets, we provide a unique dataset consisting of 340,000 pairs of\nimages and their emotion annotations. Furthermore, we conduct human\npsychophysics experiments and introduce a new evaluation metric to\nsystematically benchmark all the methods. Experimental results demonstrate that\nour method surpasses all competitive baselines. Our diffusion model is capable\nof identifying emotional cues from original images, editing images that elicit\ndesired emotions, and meanwhile, preserving the semantic structure of the\noriginal images. All code, model, and dataset are available at GitHub.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.08255v4", "cate": "cs.CV", "date": "2024-03-13", "updated": "2025-07-22"}
{"id": "2507.16572", "title": "Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models", "authors": ["Mohamad Ballout", "Serwan Jassim", "Elia Bruni"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16572v1", "summary": "This paper presents a systematic evaluation of state-of-the-art multimodal\nlarge language models (MLLMs) on intuitive physics tasks using the GRASP and\nIntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL,\nLLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that\neven the latest models struggle to reliably distinguish physically plausible\nfrom implausible scenarios. To go beyond performance metrics, we conduct a\nprobing analysis of model embeddings, extracting intermediate representations\nat key processing stages to examine how well task-relevant information is\npreserved. Our results show that, depending on task difficulty, a critical\nvision-language misalignment can emerge: vision encoders successfully capture\nphysical plausibility cues, but this information is not effectively utilized by\nthe language model, leading to failures in reasoning. This misalignment\nsuggests that the primary limitation of MLLMs in intuitive physics tasks is not\nthe vision component but the ineffective integration of visual and linguistic\ninformation. Our findings highlight vision-language alignment as a key area for\nimprovement, offering insights for future MLLMs development.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16572v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16554", "title": "Neumann series of Bessel functions in direct and inverse spherically symmetric transmission eigenvalue problems", "authors": ["Vladislav V. Kravchenko", "L. Estefania Murcia-Lozano", "Nikolaos Pallikarakis"], "categories": ["math.NA", "cs.NA", "math.CA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16554v1", "summary": "The transmission eigenvalue problem (TEP) plays a central role in inverse\nscattering theory. Despite substantial theoretical progress, the numerical\nsolution of direct and inverse TEP in spherically symmetric domains with\nvariable refractive index covering real and complex eigenvalues remains\nchallenging. This study introduces a novel Neumann Series of Bessel Functions\n(NSBF) methodology to address this challenge. After reformulating the TEP as a\nSturm-Liouville equation via a Liouville transformation, we expand its\ncharacteristic function in an NSBF whose coefficients are computed by simple\nrecursive integration. In the direct problem, eigenvalues real or complex are\nfound by root finding on a truncated NSBF partial sum, yielding high accuracy\nwith a few coefficients, as demonstrated with various examples. For the inverse\nproblem, we develop a two-step approach: first, recovering the transformed\ninterval length $\\delta$ from spectral data via a new NSBF-based algorithm, and\nsecond, reconstructing the refractive index $n(r)$ by solving a linear system\nfor the first NSBF coefficients. A spectrum completion technique is also\nimplemented to complete the spectrum and solve the corresponding inverse\nproblem when eigenvalue data is limited. Numerical examples confirm the\nmethod's robustness and accuracy across a wide range of refractive indices,\nwith no a priori assumptions on $\\delta$ or the sign of the contrast $1-n(r)$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16554v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.15991", "title": "A new XML conversion process for mensural music encoding : CMME\\_to\\_MEI (via Verovio)", "authors": ["David Fiala", "Laurent Pugin", "Marnix van Berchum", "Martha Thomae", "Kvin Roger"], "categories": ["cs.SD", "cs.DB"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15991v1", "summary": "The Ricercar Lab - the musicological research team at the Center for advanced\nStudies in the Renaissance at the University of Tours - has decided to make\navailable in open access, thanks to the support of the French digital\ninfrastructure Biblissima, a large corpus of about 3500 XML files of 15th-c.\nmusic. This corpus was produced by the German musicologist Clemens Goldberg who\nencoded since 2010 onwards the musical content of 34 major 15th-c. music\nmanuscripts and other complementary files, in order to offer on his\nfoundation's website PDF files of complete collections of works by Du Fay,\nBinchois, Okeghem, Busnoys and most of their major contemporaries, focusing on\ntheir secular output. This corpus was encoded in an XML format named CMME\n(Computerized Mensural Music Editing), specifically conceived for mensural\nmusic by Theodor Dumitrescu in the 2000s, together with editorial and\npublication tools which have not been updated since then. This article focuses\non the development of a set of conversion tools for these CMME files to meet\nmore up-to-date standards of music encoding, namely MEI. A workshop was\norganised in September 2024 at the Campus Condorcet in Paris, gathering experts\nwith a wide range of knowledge on mensural music notation, XML formats and\nprogramming. A converter was developped directly in the open-source rendering\nlibrary Verovio, allowing the conversion from CMME to MEI mensural. A\nconversion to MEI CMN was implemented afterwards, enabling to load these files\nin common engraving softwares such as MuseScore with minimal loss of\ninformation. With the availability of a direct import of CMME-XML into Verovio,\nthe corpus of existing CMME files gets a new life. Furthermore, since the\nstand-alone CMME editor still works fine and no alternative is available yet\nfor native MEI, the converter offers a new pipeline for encoding and editing\nmensural music.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15991v1", "cate": "cs.SD", "date": "2025-07-17", "updated": "2025-07-17"}
{"id": "2410.01738", "title": "VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch Diffusion Models", "authors": ["Kailai Feng", "Yabo Zhang", "Haodong Yu", "Zhilong Ji", "Jinfeng Bai", "Hongzhi Zhang", "Wangmeng Zuo"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:       this https URL", "url": "http://arxiv.org/abs/2410.01738v3", "summary": "Artistic typography is a technique to visualize the meaning of input\ncharacter in an imaginable and readable manner. With powerful text-to-image\ndiffusion models, existing methods directly design the overall geometry and\ntexture of input character, making it challenging to ensure both creativity and\nlegibility. In this paper, we introduce a dual-branch, training-free method\ncalled VitaGlyph, enabling flexible artistic typography with controllable\ngeometry changes while maintaining the readability. The key insight of\nVitaGlyph is to treat input character as a scene composed of a Subject and its\nSurrounding, which are rendered with varying degrees of geometric\ntransformation. To enhance the visual appeal and creativity of the generated\nartistic typography, the subject flexibly expresses the essential concept of\nthe input character, while the surrounding enriches relevant background without\naltering the shape, thus maintaining overall readability. Specifically, we\nimplement VitaGlyph through a three-phase framework: (i) Knowledge Acquisition\nleverages large language models to design text descriptions for the subject and\nsurrounding. (ii) Regional Interpretation detects the part that most closely\nmatches the subject description and refines the structure via Semantic\nTypography. (iii) Attentional Compositional Generation separately renders the\ntextures of the Subject and Surrounding regions and blends them in an\nattention-based manner. Experimental results demonstrate that VitaGlyph not\nonly achieves better artistry and readability but also manages to depict\nmultiple customized concepts, facilitating more creative and pleasing artistic\ntypography generation. Our code will be made publicly available.", "comment": "https://github.com/Carlofkl/VitaGlyph", "pdf_url": "http://arxiv.org/pdf/2410.01738v3", "cate": "cs.CV", "date": "2024-10-02", "updated": "2025-07-22"}
{"id": "2411.05195", "title": "Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder", "authors": ["Siting Li", "Pang Wei Koh", "Simon Shaolei Du"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ACL 2025; 19 pages, 3 figures", "url": "http://arxiv.org/abs/2411.05195v3", "summary": "Recent research has shown that CLIP models struggle with visual reasoning\ntasks that require grounding compositionality, understanding spatial\nrelationships, or capturing fine-grained details. One natural hypothesis is\nthat the CLIP vision encoder does not embed essential information for these\ntasks. However, we find that this is not always the case: The encoder gathers\nquery-relevant visual information, while CLIP fails to extract it. In\nparticular, we show that another branch of Vision-Language Models (VLMs),\nGenerative Multimodal Large Language Models (MLLMs), achieve significantly\nhigher accuracy than CLIP in many of these tasks using the same vision encoder\nand weights, indicating that these Generative MLLMs perceive more -- as they\nextract and utilize visual information more effectively. We conduct a series of\ncontrolled experiments and reveal that their success is attributed to multiple\nkey design choices, including patch tokens, position embeddings, and\nprompt-based weighting. On the other hand, enhancing the training data alone or\napplying a stronger text encoder does not suffice to solve the task, and\nadditional text tokens offer little benefit. Interestingly, we find that\nfine-grained visual reasoning is not exclusive to generative models trained by\nan autoregressive loss: When converted into CLIP-like encoders by contrastive\nfinetuning, these MLLMs still outperform CLIP under the same cosine\nsimilarity-based evaluation protocol. Our study highlights the importance of\nVLM architectural choices and suggests directions for improving the performance\nof CLIP-like contrastive VLMs.", "comment": "ACL 2025; 19 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2411.05195v3", "cate": "cs.LG", "date": "2024-11-07", "updated": "2025-07-21"}
{"id": "2407.04476", "title": "Rethinking Data Input for Point Cloud Upsampling", "authors": ["Tongxu Zhang"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.04476v3", "summary": "Point cloud upsampling is crucial for tasks like 3D reconstruction. While\nexisting methods rely on patch-based inputs, and there is no research\ndiscussing the differences and principles between point cloud model full input\nand patch based input. Ergo, we propose a novel approach using whole model\ninputs i.e. Average Segment input. Our experiments on PU1K and ABC datasets\nreveal that patch-based inputs consistently outperform whole model inputs. To\nunderstand this, we will delve into factors in feature extraction, and network\narchitecture that influence upsampling results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.04476v3", "cate": "cs.CV", "date": "2024-07-05", "updated": "2025-07-22"}
{"id": "2507.16656", "title": "P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs", "authors": ["Dongjun Jang", "Youngchae Ahn", "Hyopil Shin"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16656v1", "summary": "This study explores the potential of phonological reasoning within text-based\nlarge language models (LLMs). Utilizing the PhonologyBench benchmark, we assess\ntasks like rhyme word generation, g2p conversion, and syllable counting. Our\nevaluations across 12 LLMs reveal that while few-shot learning offers\ninconsistent gains, the introduction of a novel Pedagogically-motivated\nParticipatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational\ntheories like scaffolding and discovery learning, consistently enhances\nperformance. This method leverages structured guidance to activate latent\nphonological abilities, achieving up to 52% improvement and even surpassing\nhuman baselines in certain tasks. Future work could aim to optimize P-CoT\nprompts for specific models or explore their application across different\nlinguistic domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16656v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16631", "title": "A Conservative and Positivity-Preserving Discontinuous Galerkin Method for the Population Balance Equation", "authors": ["Ziyao Xu", "Guanyang Liu", "Yong-Tao Zhang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16631v1", "summary": "We develop a conservative, positivity-preserving discontinuous Galerkin (DG)\nmethod for the population balance equation (PBE), which models the distribution\nof particle numbers across particle sizes due to growth, nucleation,\naggregation, and breakage. To ensure number conservation in growth and mass\nconservation in aggregation and breakage, we design a DG scheme that applies\nstandard treatment for growth and nucleation, and introduces a novel\ndiscretization for aggregation and breakage. The birth and death terms are\ndiscretized in a symmetric double-integral form, evaluated using a common\nrefinement of the integration domain and carefully selected quadrature rules.\nBeyond conservation, we focus on preserving the positivity of the number\ndensity in aggregation-breakage. Since local mass corresponds to the first\nmoment, the classical Zhang-Shu limiter, which preserves the zeroth moment\n(cell average), is not directly applicable. We address this by proving the\npositivity of the first moment on each cell and constructing a\nmoment-conserving limiter that enforces nonnegativity across the domain. To our\nknowledge, this is the first work to develop a positivity-preserving algorithm\nthat conserves a prescribed moment. Numerical results verify the accuracy,\nconservation, and robustness of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16631v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16235", "title": "Robust Bioacoustic Detection via Richly Labelled Synthetic Soundscape Augmentation", "authors": ["Kaspar Soltero", "Tadeu Siqueira", "Stefanie Gutschmidt"], "categories": ["cs.SD"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures", "url": "http://arxiv.org/abs/2507.16235v1", "summary": "Passive Acoustic Monitoring (PAM) analysis is often hindered by the intensive\nmanual effort needed to create labelled training data. This study introduces a\nsynthetic data framework to generate large volumes of richly labelled training\ndata from very limited source material, improving the robustness of bioacoustic\ndetection models. Our framework synthesises realistic soundscapes by combining\nclean background noise with isolated target vocalisations (little owl),\nautomatically generating dynamic labels like bounding boxes during synthesis. A\nmodel fine-tuned on this data generalised well to real-world soundscapes, with\nperformance remaining high even when the diversity of source vocalisations was\ndrastically reduced, indicating the model learned generalised features without\noverfitting. This demonstrates that synthetic data generation is a highly\neffective strategy for training robust bioacoustic detectors from small source\ndatasets. The approach significantly reduces manual labelling effort,\novercoming a key bottleneck in computational bioacoustics and enhancing\necological assessment capabilities.", "comment": "12 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.16235v1", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2410.13246", "title": "Atomic Calibration of LLMs in Long-Form Generations", "authors": ["Caiqi Zhang", "Ruihan Yang", "Zhisong Zhang", "Xinting Huang", "Sen Yang", "Dong Yu", "Nigel Collier"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 KnowFM Oral", "url": "http://arxiv.org/abs/2410.13246v2", "summary": "Large language models (LLMs) often suffer from hallucinations, posing\nsignificant challenges for real-world applications. Confidence calibration,\nwhich estimates the underlying uncertainty of model predictions, is essential\nto enhance the LLMs' trustworthiness. Existing research on LLM calibration has\nprimarily focused on short-form tasks, providing a single confidence score at\nthe response level (macro calibration). However, this approach is insufficient\nfor long-form generations, where responses often contain more complex\nstatements and may include both accurate and inaccurate information. Therefore,\nwe introduce atomic calibration, a novel approach that evaluates factuality\ncalibration at a fine-grained level by breaking down long responses into atomic\nclaims. We classify confidence elicitation methods into discriminative and\ngenerative types and demonstrate that their combination can enhance\ncalibration. Our extensive experiments on various LLMs and datasets show that\natomic calibration is well-suited for long-form generation and can also improve\nmacro calibration results. Additionally, atomic calibration reveals insightful\npatterns in LLM confidence throughout the generation process.", "comment": "ACL 2025 KnowFM Oral", "pdf_url": "http://arxiv.org/pdf/2410.13246v2", "cate": "cs.CL", "date": "2024-10-17", "updated": "2025-07-22"}
{"id": "2411.18425", "title": "Streamlining Prediction in Bayesian Deep Learning", "authors": ["Rui Li", "Marcus Klasson", "Arno Solin", "Martin Trapp"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.18425v4", "summary": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks.\n  Open-source library: https://github.com/AaltoML/SUQ", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.18425v4", "cate": "cs.LG", "date": "2024-11-27", "updated": "2025-07-22"}
{"id": "2410.12332", "title": "MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs", "authors": ["Yunqiu Xu", "Linchao Zhu", "Yi Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2410.12332v2", "summary": "While multimodal large language models (MLLMs) have demonstrated\nextraordinary vision-language understanding capabilities, their abilities to\nsolve instance-level visual-language problems beyond a single image warrant\nfurther exploration. To assess these unproven abilities of MLLMs, this paper\nproposes a new visual grounding task called multi-context visual grounding,\nwhich aims to localize instances of interest across multiple images based on\nopen-ended text prompts. In order to facilitate this research, we construct a\nnew dataset MC-Bench that features 2K high-quality and manually annotated\nsamples. Each sample consists of an instance-level labeled image pair and a\ncorresponding text prompt that indicates the target instances in the images.\nThese text prompts are highly open-ended and follow three distinct styles,\ncovering 20 practical skills. We benchmark over 20 state-of-the-art MLLMs and\nfoundation models with potential multi-context visual grounding capabilities,\nalong with our developed simple yet effective agentic baseline and a finetuned\nbaseline by multi-context instruction tuning. Our evaluation reveals a\nnon-trivial performance gap between existing MLLMs and humans, along with some\ninsightful observations that suggest potential future directions. We hope that\nMC-Bench and our empirical findings encourage the research community to further\nadvance the untapped potentials of MLLMs in instance-level tasks, particularly\nin multi-image contexts. Project page: https://xuyunqiu.github.io/MC-Bench.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2410.12332v2", "cate": "cs.CV", "date": "2024-10-16", "updated": "2025-07-22"}
{"id": "2507.16748", "title": "Unpacking Ambiguity: The Interaction of Polysemous Discourse Markers and Non-DM Signals", "authors": ["Jingni Wu", "Amir Zeldes"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16748v1", "summary": "Discourse markers (DMs) like 'but' or 'then' are crucial for creating\ncoherence in discourse, yet they are often replaced by or co-occur with non-DMs\n('in the morning' can mean the same as 'then'), and both can be ambiguous\n('since' can refer to time or cause). The interaction mechanism between such\nsignals remains unclear but pivotal for their disambiguation. In this paper we\ninvestigate the relationship between DM polysemy and co-occurrence of non-DM\nsignals in English, as well as the influence of genre on these patterns.\n  Using the framework of eRST, we propose a graded definition of DM polysemy,\nand conduct correlation and regression analyses to examine whether polysemous\nDMs are accompanied by more numerous and diverse non-DM signals. Our findings\nreveal that while polysemous DMs do co-occur with more diverse non-DMs, the\ntotal number of co-occurring signals does not necessarily increase. Moreover,\ngenre plays a significant role in shaping DM-signal interactions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16748v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16647", "title": "A quasi-Monte Carlo multiscale method for the wave propagation in random media", "authors": ["Panchi Li", "Zhiwen Zhang"], "categories": ["math.NA", "cs.NA", "35J05, 35R60, 65D30, 65N30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16647v1", "summary": "In this paper, we propose and analyze an accurate numerical approach to\nsimulate the Helmholtz problem in a bounded region with a random refractive\nindex, where the random refractive index is denoted using an infinite series\nparameterized by stochastic variables. To calculate the statistics of the\nsolution numerically, we first truncate the parameterized model and adopt the\nquasi-Monte Carlo (qMC) method to generate stochastic variables. We develop a\nboundary-corrected multiscale method to discretize the truncated problem, which\nallows us to accurately resolve the Robin boundary condition with randomness.\nThe proposed method exhibits superconvergence rates in the physical space\n(theoretical analysis suggests $\\mathcal{O}(H^4)$ for $L^2$-error and\n$\\mathcal{O}(H^2)$ for a defined $V$-error). Owing to the employment of the qMC\nmethod, it also exhibits almost the first-order convergence rate in the random\nspace. We provide the wavenumber explicit convergence analysis and conduct\nnumerical experiments to validate key features of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16647v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16080", "title": "Interpretable Embeddings of Speech Enhance and Explain Brain Encoding Performance of Audio Models", "authors": ["Riki Shimizu", "Richard J. Antonello", "Chandan Singh", "Nima Mesgarani"], "categories": ["q-bio.NC", "cs.SD"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      7pages, 4 figures", "url": "http://arxiv.org/abs/2507.16080v1", "summary": "Self-supervised speech models (SSMs) are increasingly hailed as more powerful\ncomputational models of human speech perception than models based on\ntraditional hand-crafted features. However, since their representations are\ninherently black-box, it remains unclear what drives their alignment with brain\nresponses. To remedy this, we built linear encoding models from six\ninterpretable feature families: mel-spectrogram, Gabor filter bank features,\nspeech presence, phonetic, syntactic, and semantic Question-Answering features,\nand contextualized embeddings from three state-of-the-art SSMs (Whisper,\nHuBERT, WavLM), quantifying the shared and unique neural variance captured by\neach feature class. Contrary to prevailing assumptions, our interpretable model\npredicted electrocorticography (ECoG) responses to speech more accurately than\nany SSM. Moreover, augmenting SSM representations with interpretable features\nyielded the best overall neural predictions, significantly outperforming either\nclass alone. Further variance-partitioning analyses revealed previously\nunresolved components of SSM representations that contribute to their neural\nalignment: 1. Despite the common assumption that later layers of SSMs discard\nlow-level acoustic information, these models compress and preferentially retain\nfrequency bands critical for neural encoding of speech (100-1000 Hz). 2.\nContrary to previous claims, SSMs encode brain-relevant semantic information\nthat cannot be reduced to lower-level features, improving with context length\nand model size. These results highlight the importance of using refined,\ninterpretable features in understanding speech perception.", "comment": "7pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.16080v1", "cate": "q-bio.NC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2411.06106", "title": "Towards a Universal 3D Medical Multi-modality Generalization via Learning Personalized Invariant Representation", "authors": ["Zhaorui Tan", "Xi Yang", "Tan Pan", "Tianyi Liu", "Chen Jiang", "Xin Guo", "Qiufeng Wang", "Anh Nguyen", "Yuan Qi", "Kaizhu Huang", "Yuan Cheng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV25", "url": "http://arxiv.org/abs/2411.06106v3", "summary": "The differences among medical imaging modalities, driven by distinct\nunderlying principles, pose significant challenges for generalization in\nmulti-modal medical tasks. Beyond modality gaps, individual variations, such as\ndifferences in organ size and metabolic rate, further impede a model's ability\nto generalize effectively across both modalities and diverse populations.\nDespite the importance of personalization, existing approaches to multi-modal\ngeneralization often neglect individual differences, focusing solely on common\nanatomical features. This limitation may result in weakened generalization in\nvarious medical tasks. In this paper, we unveil that personalization is\ncritical for multi-modal generalization. Specifically, we propose an approach\nto achieve personalized generalization through approximating the underlying\npersonalized invariant representation ${X}_h$ across various modalities by\nleveraging individual-level constraints and a learnable biological prior. We\nvalidate the feasibility and benefits of learning a personalized ${X}_h$,\nshowing that this representation is highly generalizable and transferable\nacross various multi-modal medical tasks. Extensive experimental results\nconsistently show that the additionally incorporated personalization\nsignificantly improves performance and generalization across diverse scenarios,\nconfirming its effectiveness.", "comment": "Accepted by ICCV25", "pdf_url": "http://arxiv.org/pdf/2411.06106v3", "cate": "cs.CV", "date": "2024-11-09", "updated": "2025-07-22"}
{"id": "2412.20553", "title": "Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD", "authors": ["Arseniy Andreyev", "Pierfrancesco Beneventano"], "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      71 pages, 43 figures", "url": "http://arxiv.org/abs/2412.20553v4", "summary": "Recent findings by Cohen et al., 2021, demonstrate that when training neural\nnetworks with full-batch gradient descent with a step size of $\\eta$, the\nlargest eigenvalue $\\lambda_{\\max}$ of the full-batch Hessian consistently\nstabilizes at $\\lambda_{\\max} = 2/\\eta$. These results have significant\nimplications for convergence and generalization. This, however, is not the case\nof mini-batch stochastic gradient descent (SGD), limiting the broader\napplicability of its consequences. We show that SGD trains in a different\nregime we term Edge of Stochastic Stability (EoSS). In this regime, what\nstabilizes at $2/\\eta$ is *Batch Sharpness*: the expected directional curvature\nof mini-batch Hessians along their corresponding stochastic gradients. As a\nconsequence $\\lambda_{\\max}$ -- which is generally smaller than Batch Sharpness\n-- is suppressed, aligning with the long-standing empirical observation that\nsmaller batches and larger step sizes favor flatter minima. We further discuss\nimplications for mathematical modeling of SGD trajectories.", "comment": "71 pages, 43 figures", "pdf_url": "http://arxiv.org/pdf/2412.20553v4", "cate": "cs.LG", "date": "2024-12-29", "updated": "2025-07-22"}
{"id": "2410.13911", "title": "GraspDiffusion: Synthesizing Realistic Whole-body Hand-Object Interaction", "authors": ["Patrick Kwon", "Chen Chen", "Hanbyul Joo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13911v2", "summary": "Recent generative models can synthesize high-quality images but often fail to\ngenerate humans interacting with objects using their hands. This arises mostly\nfrom the model's misunderstanding of such interactions, and the hardships of\nsynthesizing intricate regions of the body. In this paper, we propose\nGraspDiffusion, a novel generative method that creates realistic scenes of\nhuman-object interaction. Given a 3D object mesh, GraspDiffusion first\nconstructs life-like whole-body poses with control over the object's location\nrelative to the human body. This is achieved by separately leveraging the\ngenerative priors for 3D body and hand poses, optimizing them into a joint\ngrasping pose. The resulting pose guides the image synthesis to correctly\nreflect the intended interaction, allowing the creation of realistic and\ndiverse human-object interaction scenes. We demonstrate that GraspDiffusion can\nsuccessfully tackle the relatively uninvestigated problem of generating\nfull-bodied human-object interactions while outperforming previous methods.\nCode and models will be available at https://webtoon.github.io/GraspDiffusion", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13911v2", "cate": "cs.CV", "date": "2024-10-17", "updated": "2025-07-22"}
{"id": "2507.16784", "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning", "authors": ["Hongyin Luo", "Nathaniel Morgan", "Tina Li", "Derek Zhao", "Ai Vy Ngo", "Philip Schroeder", "Lijie Yang", "Assaf Ben-Kish", "Jack O'Brien", "James Glass"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Research preview", "url": "http://arxiv.org/abs/2507.16784v1", "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.", "comment": "Research preview", "pdf_url": "http://arxiv.org/pdf/2507.16784v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16652", "title": "A $\\star$-Product Approach for Analytical and Numerical Solutions of Nonautonomous Linear Fractional Differential Equations", "authors": ["Fabio Durastante", "Pierre-Louis Giscard", "Stefano Pozza"], "categories": ["math.NA", "cs.NA", "26A33, 65L05, 33C45"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16652v1", "summary": "This article presents a novel solution method for nonautonomous linear\nordinary fractional differential equations. The approach is based on\nreformulating the analytical solution using the $\\star$-product, a\ngeneralization of the Volterra convolution, followed by an appropriate\ndiscretization of the resulting expression. Additionally, we demonstrate that,\nin certain cases, the $\\star$-formalism enables the derivation of closed-form\nsolutions, further highlighting the utility of this framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16652v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2412.01661", "title": "R-Bot: An LLM-based Query Rewrite System", "authors": ["Zhaoyan Sun", "Xuanhe Zhou", "Guoliang Li", "Xiang Yu", "Jianhua Feng", "Yong Zhang"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.01661v2", "summary": "Query rewrite is essential for optimizing SQL queries to improve their\nexecution efficiency without changing their results. Traditionally, this task\nhas been tackled through heuristic and learning-based methods, each with its\nlimitations in terms of inferior quality and low robustness. Recent\nadvancements in LLMs offer a new paradigm by leveraging their superior natural\nlanguage and code comprehension abilities. Despite their potential, directly\napplying LLMs like GPT-4 has faced challenges due to problems such as\nhallucinations, where the model might generate inaccurate or irrelevant\nresults. To address this, we propose R-Bot, an LLM-based query rewrite system\nwith a systematic approach. We first design a multi-source rewrite evidence\npreparation pipeline to generate query rewrite evidences for guiding LLMs to\navoid hallucinations. We then propose a hybrid structure-semantics retrieval\nmethod that combines structural and semantic analysis to retrieve the most\nrelevant rewrite evidences for effectively answering an online query. We next\npropose a step-by-step LLM rewrite method that iteratively leverages the\nretrieved evidences to select and arrange rewrite rules with self-reflection.\nWe conduct comprehensive experiments on real-world datasets and widely used\nbenchmarks, and demonstrate the superior performance of our system, R-Bot,\nsurpassing state-of-the-art query rewrite methods. The R-Bot system has been\ndeployed at Huawei and with real customers, and the results show that the\nproposed R-Bot system achieves lower query latency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.01661v2", "cate": "cs.DB", "date": "2024-12-02", "updated": "2025-07-22"}
{"id": "2501.05494", "title": "Soft Computing Approaches for Predicting Shade-Seeking Behaviour in Dairy Cattle under Heat Stress: A Comparative Study of Random Forests and Neural Networks", "authors": ["S. Sanjuan", "D. A. Mndez", "R. Arnau", "J. M. Calabuig", "X. Daz de Otlora Aguirre", "F. Estells"], "categories": ["cs.LG", "37M05, 68T05, 92B20"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages, 10 figures", "url": "http://arxiv.org/abs/2501.05494v2", "summary": "Heat stress is one of the main welfare and productivity problems faced by\ndairy cattle in Mediterranean climates. In this study, we approach the\nprediction of the daily shade-seeking count as a non-linear multivariate\nregression problem and evaluate two soft computing algorithms -- Random Forests\nand Neural Networks -- trained on high-resolution behavioral and micro-climatic\ndata collected in a commercial farm in Titaguas (Valencia, Spain) during the\n2023 summer season. The raw dataset (6907 daytime observations, 5-10 min\nresolution) includes the number of cows in the shade, ambient temperature and\nrelative humidity. From these we derive three features: current\nTemperature--Humidity Index (THI), accumulated daytime THI, and mean night-time\nTHI. To evaluate the models' performance a 5-fold cross-validation is also\nused. Results show that both soft computing models outperform a single Decision\nTree baseline. The best Neural Network (3 hidden layers, 16 neurons each,\nlearning rate = 10e-3) reaches an average RMSE of 14.78, while a Random Forest\n(10 trees, depth = 5) achieves 14.97 and offers best interpretability. Daily\nerror distributions reveal a median RMSE of 13.84 and confirm that predictions\ndeviate less than one hour from observed shade-seeking peaks. These results\ndemonstrate the suitability of soft computing, data-driven approaches embedded\nin an applied-mathematical feature framework for modeling noisy biological\nphenomena, demonstrating their value as low-cost, real-time decision-support\ntools for precision livestock farming under heat-stress conditions.", "comment": "22 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2501.05494v2", "cate": "cs.LG", "date": "2025-01-09", "updated": "2025-07-22"}
{"id": "2411.07918", "title": "Physically Consistent Image Augmentation for Deep Learning in Mueller Matrix Polarimetry", "authors": ["Christopher Hahne", "Omar Rodriguez-Nunez", "la Gros", "Thotim Lucas", "Ekkehard Hewer", "Tatiana Novikova", "Theoni Maragkou", "Philippe Schucht", "Richard McKinley"], "categories": ["cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      preprint", "url": "http://arxiv.org/abs/2411.07918v2", "summary": "Mueller matrix polarimetry captures essential information about polarized\nlight interactions with a sample, presenting unique challenges for data\naugmentation in deep learning due to its distinct structure. While\naugmentations are an effective and affordable way to enhance dataset diversity\nand reduce overfitting, standard transformations like rotations and flips do\nnot preserve the polarization properties in Mueller matrix images. To this end,\nwe introduce a versatile simulation framework that applies physically\nconsistent rotations and flips to Mueller matrices, tailored to maintain\npolarization fidelity. Our experimental results across multiple datasets reveal\nthat conventional augmentations can lead to falsified results when applied to\npolarimetric data, underscoring the necessity of our physics-based approach. In\nour experiments, we first compare our polarization-specific augmentations\nagainst real-world captures to validate their physical consistency. We then\napply these augmentations in a semantic segmentation task, achieving\nsubstantial improvements in model generalization and performance. This study\nunderscores the necessity of physics-informed data augmentation for\npolarimetric imaging in deep learning (DL), paving the way for broader adoption\nand more robust applications across diverse research in the field. In\nparticular, our framework unlocks the potential of DL models for polarimetric\ndatasets with limited sample sizes. Our code implementation is available at\ngithub.com/hahnec/polar_augment.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2411.07918v2", "cate": "cs.CV", "date": "2024-11-12", "updated": "2025-07-22"}
{"id": "2507.16799", "title": "Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent", "authors": ["Xiaoyu Zhan", "Xinyu Fu", "Hao Sun", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16799v1", "summary": "The rapid advancement of large language models (LLMs) has enabled\nrole-playing language agents to demonstrate significant potential in various\napplications. However, relying solely on prompts and contextual inputs often\nproves insufficient for achieving deep immersion in specific roles,\nparticularly well-known fictional or public figures. On the other hand,\nfine-tuning-based approaches face limitations due to the challenges associated\nwith data collection and the computational resources required for training,\nthereby restricting their broader applicability. To address these issues, we\npropose Test-Time-Matching (TTM), a training-free role-playing framework\nthrough test-time scaling and context engineering. TTM uses LLM agents to\nautomatically decouple a character's features into personality, memory, and\nlinguistic style. Our framework involves a structured, three-stage generation\npipeline that utilizes these features for controlled role-playing. It achieves\nhigh-fidelity role-playing performance, also enables seamless combinations\nacross diverse linguistic styles and even variations in personality and memory.\nWe evaluate our framework through human assessment, and the results demonstrate\nthat our method achieves the outstanding performance in generating expressive\nand stylistically consistent character dialogues.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16799v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16658", "title": "Time integration of dissipative stochastic PDEs", "authors": ["Helena Bievi", "Raffaele D'Ambrosio"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16658v1", "summary": "The paper is focused on the numerical solution of stochastic\nreaction-diffusion problems. A special attention is addressed to the\nconservation of mean-square dissipativity in the time integration of the\nspatially discretized problem, obtained by means of finite differences. The\nanalysis highlights the conservative ability of stochastic $\\theta$-methods and\nstochastic $\\theta$-IMEX methods, emphasizing the roles of spatial and temporal\nstepsizes. A selection of numerical experiments is provided, confirming the\ntheoretical expectations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16658v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2501.07525", "title": "RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment", "authors": ["Difei Gu", "Yunhe Gao", "Yang Zhou", "Mu Zhou", "Dimitris Metaxas"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025", "url": "http://arxiv.org/abs/2501.07525v2", "summary": "Automated chest radiographs interpretation requires both accurate disease\nclassification and detailed radiology report generation, presenting a\nsignificant challenge in the clinical workflow. Current approaches either focus\non classification accuracy at the expense of interpretability or generate\ndetailed but potentially unreliable reports through image captioning\ntechniques. In this study, we present RadAlign, a novel framework that combines\nthe predictive accuracy of vision-language models (VLMs) with the reasoning\ncapabilities of large language models (LLMs). Inspired by the radiologist's\nworkflow, RadAlign first employs a specialized VLM to align visual features\nwith key medical concepts, achieving superior disease classification with an\naverage AUC of 0.885 across multiple diseases. These recognized medical\nconditions, represented as text-based concepts in the aligned visual-language\nspace, are then used to prompt LLM-based report generation. Enhanced by a\nretrieval-augmented generation mechanism that grounds outputs in similar\nhistorical cases, RadAlign delivers superior report quality with a GREEN score\nof 0.678, outperforming state-of-the-art methods' 0.634. Our framework\nmaintains strong clinical interpretability while reducing hallucinations,\nadvancing automated medical imaging and report analysis through integrated\npredictive and generative AI. Code is available at\nhttps://github.com/difeigu/RadAlign.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2501.07525v2", "cate": "cs.CV", "date": "2025-01-13", "updated": "2025-07-22"}
{"id": "2502.16660", "title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning", "authors": ["Haiteng Zhao", "Chang Ma", "Fangzhi Xu", "Lingpeng Kong", "Zhi-Hong Deng"], "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16660v5", "summary": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16660v5", "cate": "cs.LG", "date": "2025-02-23", "updated": "2025-07-22"}
{"id": "2411.16934", "title": "Online Episodic Memory Visual Query Localization with Egocentric Streaming Object Memory", "authors": ["Zaira Manigrasso", "Matteo Dunnhofer", "Antonino Furnari", "Moritz Nottebaum", "Antonio Finocchiaro", "Davide Marana", "Rosario Forte", "Giovanni Maria Farinella", "Christian Micheloni"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.16934v2", "summary": "Episodic memory retrieval enables wearable cameras to recall objects or\nevents previously observed in video. However, existing formulations assume an\n\"offline\" setting with full video access at query time, limiting their\napplicability in real-world scenarios with power and storage-constrained\nwearable devices. Towards more application-ready episodic memory systems, we\nintroduce Online Visual Query 2D (OVQ2D), a task where models process video\nstreams online, observing each frame only once, and retrieve object\nlocalizations using a compact memory instead of full video history. We address\nOVQ2D with ESOM (Egocentric Streaming Object Memory), a novel framework\nintegrating an object discovery module, an object tracking module, and a memory\nmodule that find, track, and store spatio-temporal object information for\nefficient querying. Experiments on Ego4D demonstrate ESOM's superiority over\nother online approaches, though OVQ2D remains challenging, with top performance\nat only ~4% success. ESOM's accuracy increases markedly with perfect object\ntracking (31.91%), discovery (40.55%), or both (81.92%), underscoring the need\nof applied research on these components.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.16934v2", "cate": "cs.CV", "date": "2024-11-25", "updated": "2025-07-22"}
{"id": "2507.16809", "title": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs", "authors": ["Da-Chen Lian", "Ri-Sheng Huang", "Pin-Er Chen", "Chunki Lim", "You-Kuan Lin", "Guan-Yu Tseng", "Zi-Cheng Yang", "Shu-Kai Hsieh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      41 pages, 17 figures, 10 tables", "url": "http://arxiv.org/abs/2507.16809v1", "summary": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs.", "comment": "41 pages, 17 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.16809v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2507.16810", "title": "The inverse initial data problem for anisotropic Navier-Stokes equations via Legendre time reduction method", "authors": ["Cong B. Van", "Thuy T. Le", "Loc H. Nguyen"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16810v1", "summary": "We consider the inverse initial data problem for the compressible anisotropic\nNavier-Stokes equations, where the goal is to reconstruct the initial velocity\nfield from lateral boundary observations. This problem arises in applications\nwhere direct measurements of internal fluid states are unavailable. We\nintroduce a novel computational framework based on Legendre time reduction,\nwhich projects the velocity field onto an exponentially weighted Legendre basis\nin time. This transformation reduces the original time-dependent inverse\nproblem to a coupled, time-independent elliptic system. The resulting reduced\nmodel is solved iteratively using a Picard iteration and a stabilized\nleast-squares formulation under noisy boundary data. Numerical experiments in\ntwo dimensions confirm that the method accurately and robustly reconstructs\ninitial velocity fields, even in the presence of significant measurement noise\nand complex anisotropic structures. This approach offers a flexible and\ncomputationally tractable alternative for inverse modeling in fluid dynamics\nwith anisotropic media.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16810v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2502.06631", "title": "Conformal Predictions for Human Action Recognition with Vision-Language Models", "authors": ["Bary Tim", "Fuchs Clment", "Macq Benot"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures, Accepted to ICIP 2025 Workshops", "url": "http://arxiv.org/abs/2502.06631v2", "summary": "Human-in-the-Loop (HITL) systems are essential in high-stakes, real-world\napplications where AI must collaborate with human decision-makers. This work\ninvestigates how Conformal Prediction (CP) techniques, which provide rigorous\ncoverage guarantees, can enhance the reliability of state-of-the-art human\naction recognition (HAR) systems built upon Vision-Language Models (VLMs). We\ndemonstrate that CP can significantly reduce the average number of candidate\nclasses without modifying the underlying VLM. However, these reductions often\nresult in distributions with long tails which can hinder their practical\nutility. To mitigate this, we propose tuning the temperature of the softmax\nprediction, without using additional calibration data. This work contributes to\nongoing efforts for multi-modal human-AI interaction in dynamic real-world\nenvironments.", "comment": "6 pages, 7 figures, Accepted to ICIP 2025 Workshops", "pdf_url": "http://arxiv.org/pdf/2502.06631v2", "cate": "cs.CV", "date": "2025-02-10", "updated": "2025-07-22"}
{"id": "2503.09565", "title": "Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $$P Parametrization", "authors": ["Zixiang Chen", "Greg Yang", "Qingyue Zhao", "Quanquan Gu"], "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      28 pages, 17 figures, 2 tables. In ICML 2025", "url": "http://arxiv.org/abs/2503.09565v2", "summary": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.", "comment": "28 pages, 17 figures, 2 tables. In ICML 2025", "pdf_url": "http://arxiv.org/pdf/2503.09565v2", "cate": "cs.LG", "date": "2025-03-12", "updated": "2025-07-22"}
{"id": "2411.19951", "title": "Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation", "authors": ["Shukang Yin", "Chaoyou Fu", "Sirui Zhao", "Chunjiang Ge", "Yan Yang", "Yuhan Dai", "Yongdong Luo", "Tong Xu", "Caifeng Shan", "Enhong Chen"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2411.19951v5", "summary": "Recent years have seen the success of Multimodal Large Language Models\n(MLLMs) in the domain of vision understanding. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has been primarily driven by automatic data pipelines, which focus\non the self-instruction of LLMs. The paradigm has been taken for granted for\nquite some time, but the study of the effectiveness of scaling with these data\nhas been neglected for a long time. In this context, this work revisits scaling\nwith synthetic data and focuses on developing video-LLMs from a data-centric\nperspective. Our primary study approach involves fine-tuning pre-trained\nimage-LLMs with video data and examining learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to that of baselines trained with\nsignificantly more samples. Meanwhile, we find that incorporating these\nsynthetic samples can enhance the performance of long video understanding\nwithout requiring training on long video data. The code and data examples are\navailable at https://github.com/VITA-MLLM/Sparrow.", "comment": "Project page: https://github.com/VITA-MLLM/Sparrow", "pdf_url": "http://arxiv.org/pdf/2411.19951v5", "cate": "cs.CV", "date": "2024-11-29", "updated": "2025-07-22"}
{"id": "2404.14740", "title": "Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing", "authors": ["Ben Hutchinson"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Findings of NAACL2024", "url": "http://arxiv.org/abs/2404.14740v3", "summary": "This position paper concerns the use of religious texts in Natural Language\nProcessing (NLP), which is of special interest to the Ethics of NLP. Religious\ntexts are expressions of culturally important values, and machine learned\nmodels have a propensity to reproduce cultural values encoded in their training\ndata. Furthermore, translations of religious texts are frequently used by NLP\nresearchers when language data is scarce. This repurposes the translations from\ntheir original uses and motivations, which often involve attracting new\nfollowers. This paper argues that NLP's use of such texts raises considerations\nthat go beyond model biases, including data provenance, cultural contexts, and\ntheir use in proselytism. We argue for more consideration of researcher\npositionality, and of the perspectives of marginalized linguistic and religious\ncommunities.", "comment": "Findings of NAACL2024", "pdf_url": "http://arxiv.org/pdf/2404.14740v3", "cate": "cs.CL", "date": "2024-04-23", "updated": "2025-07-22"}
{"id": "2507.16055", "title": "The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization", "authors": ["Ronny Bergmann", "Hajg Jasa", "Paula John", "Max Pfeffer"], "categories": ["math.OC", "cs.NA", "math.DG", "math.NA", "90C25, 49Q99, 49M30, 65K10"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16055v1", "summary": "We consider a class of (possibly strongly) geodesically convex optimization\nproblems on Hadamard manifolds, where the objective function splits into the\nsum of a smooth and a possibly nonsmooth function. We introduce an intrinsic\nconvex Riemannian proximal gradient (CRPG) method that employs the manifold\nproximal map for the nonsmooth step, without operating in the embedding or\ntangent space. A sublinear convergence rate for convex problems and a linear\nconvergence rate for strongly convex problems is established, and we derive\nfundamental proximal gradient inequalities that generalize the Euclidean case.\nOur numerical experiments on hyperbolic spaces and manifolds of symmetric\npositive definite matrices demonstrate substantial computational advantages\nover existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16055v1", "cate": "math.OC", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2502.11809", "title": "Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling", "authors": ["Yanbiao Ma", "Bowei Liu", "Boyuan Gao", "Wei Dai", "Jiayi Chen", "Shuo Li", "Andi Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11809v3", "summary": "Deep neural networks (DNNs) often exhibit biases toward certain categories\nduring object recognition, even under balanced training data conditions. The\nintrinsic mechanisms underlying these biases remain unclear. Inspired by the\nhuman visual system, which decouples object manifolds through hierarchical\nprocessing to achieve object recognition, we propose a geometric analysis\nframework linking the geometric complexity of class-specific perceptual\nmanifolds in DNNs to model bias. Our findings reveal that differences in\ngeometric complexity can lead to varying recognition capabilities across\ncategories, introducing biases. To support this analysis, we present the\nPerceptual-Manifold-Geometry library, designed for calculating the geometric\nproperties of perceptual manifolds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11809v3", "cate": "cs.CV", "date": "2025-02-17", "updated": "2025-07-22"}
{"id": "2504.02019", "title": "Antithetic Sampling for Top-k Shapley Identification", "authors": ["Patrick Kolpaczki", "Tim Nielen", "Eyke Hllermeier"], "categories": ["cs.LG", "cs.AI", "cs.GT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.02019v2", "summary": "Additive feature explanations rely primarily on game-theoretic notions such\nas the Shapley value by viewing features as cooperating players. The Shapley\nvalue's popularity in and outside of explainable AI stems from its axiomatic\nuniqueness. However, its computational complexity severely limits\npracticability. Most works investigate the uniform approximation of all\nfeatures' Shapley values, needlessly consuming samples for insignificant\nfeatures. In contrast, identifying the $k$ most important features can already\nbe sufficiently insightful and yields the potential to leverage algorithmic\nopportunities connected to the field of multi-armed bandits. We propose\nComparable Marginal Contributions Sampling (CMCS), a method for the top-$k$\nidentification problem utilizing a new sampling scheme taking advantage of\ncorrelated observations. We conduct experiments to showcase the efficacy of our\nmethod in compared to competitive baselines. Our empirical findings reveal that\nestimation quality for the approximate-all problem does not necessarily\ntransfer to top-$k$ identification and vice versa.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.02019v2", "cate": "cs.LG", "date": "2025-04-02", "updated": "2025-07-22"}
{"id": "2412.05888", "title": "MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model Trained with a Single GPU in Just One Day", "authors": ["Donghang Lyu", "Ruochen Gao", "Marius Staring"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA)", "url": "http://arxiv.org/abs/2412.05888v3", "summary": "Medical image segmentation involves partitioning medical images into\nmeaningful regions, with a focus on identifying anatomical structures and\nlesions. It has broad applications in healthcare, and deep learning methods\nhave enabled significant advancements in automating this process. Recently, the\nintroduction of the Segmentation Anything Model (SAM), the first foundation\nmodel for segmentation task, has prompted researchers to adapt it for the\nmedical domain to improve performance across various tasks. However, SAM's\nlarge model size and high GPU requirements hinder its scalability and\ndevelopment in the medical domain. In this work, we propose MCP-MedSAM, a\npowerful and lightweight medical SAM model designed to be trainable on a single\nA100 GPU with 40GB of memory within one day while delivering superior\nsegmentation performance. Recognizing the significant internal differences\nbetween modalities and the need for direct segmentation target information\nwithin bounding boxes, we introduce two kinds of prompts: the modality prompt\nand the content prompt. After passing through the prompt encoder, their\nembedding representations can further improve the segmentation performance by\nincorporating more relevant information without adding significant training\noverhead. Additionally, we adopt an effective modality-based data sampling\nstrategy to address data imbalance between modalities, ensuring more balanced\nperformance across all modalities. Our method was trained and evaluated using a\nlarge-scale challenge dataset, compared to top-ranking methods on the challenge\nleaderboard, MCP-MedSAM achieved superior performance while requiring only one\nday of training on a single GPU. The code is publicly available at\n\\textcolor{blue}{https://github.com/dong845/MCP-MedSAM}.}", "comment": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA)", "pdf_url": "http://arxiv.org/pdf/2412.05888v3", "cate": "cs.CV", "date": "2024-12-08", "updated": "2025-07-22"}
{"id": "2410.02760", "title": "Erasing Conceptual Knowledge from Language Models", "authors": ["Rohit Gandikota", "Sheridan Feucht", "Samuel Marks", "David Bau"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2410.02760v3", "summary": "In this work, we introduce Erasure of Language Memory (ELM), a principled\napproach to concept-level unlearning that operates by matching distributions\ndefined by the model's own introspective classification capabilities. Our key\ninsight is that effective unlearning should leverage the model's ability to\nevaluate its own knowledge, using the language model itself as a classifier to\nidentify and reduce the likelihood of generating content related to undesired\nconcepts. ELM applies this framework to create targeted low-rank updates that\nreduce generation probabilities for concept-specific content while preserving\nthe model's broader capabilities. We demonstrate ELM's efficacy on biosecurity,\ncybersecurity, and literary domain erasure tasks. Comparative evaluation\nreveals that ELM-modified models achieve near-random performance on assessments\ntargeting erased concepts, while simultaneously preserving generation\ncoherence, maintaining benchmark performance on unrelated tasks, and exhibiting\nstrong robustness to adversarial attacks. Our code, data, and trained models\nare available at https://elm.baulab.info", "comment": "Project Page: https://elm.baulab.info", "pdf_url": "http://arxiv.org/pdf/2410.02760v3", "cate": "cs.CL", "date": "2024-10-03", "updated": "2025-07-21"}
{"id": "2507.16519", "title": "A robust and stable phase field method for structural topology optimization", "authors": ["Huangxin Chen", "Piaopiao Dong", "Dong Wang", "Xiao-Ping Wang"], "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      26 pages, 15 figures", "url": "http://arxiv.org/abs/2507.16519v1", "summary": "This paper presents a novel phase-field-based methodology for solving minimum\ncompliance problems in topology optimization under fixed external loads and\nbody forces. The proposed framework characterizes the optimal structure through\nan order parameter function, analogous to phase-field models in materials\nscience, where the design domain and its boundary are intrinsically represented\nby the order parameter function. The topology optimization problem is\nreformulated as a constrained minimization problem with respect to this order\nparameter, requiring simultaneous satisfaction of three critical properties:\nbound preservation, volume conservation, and monotonic objective functional\ndecay throughout the optimization process. The principal mathematical challenge\narises from handling domain-dependent body forces, which necessitates the\ndevelopment of a constrained optimization framework. To address this, we\ndevelop an operator-splitting algorithm incorporating Lagrange multipliers,\nenhanced by a novel limiter mechanism. This hybrid approach guarantees strict\nbound preservation, exact volume conservation, and correct objective functional\ndecaying rate. Numerical implementation demonstrates the scheme's robustness\nthrough comprehensive 2D and 3D benchmarks.", "comment": "26 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2507.16519v1", "cate": "math.OC", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2502.16940", "title": "Reasoning Does Not Necessarily Improve Role-Playing Ability", "authors": ["Xiachong Feng", "Longxu Dou", "Lingpeng Kong"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16940v2", "summary": "The application of role-playing large language models (LLMs) is rapidly\nexpanding in both academic and commercial domains, driving an increasing demand\nfor high-precision role-playing models. Simultaneously, the rapid advancement\nof reasoning techniques has continuously pushed the performance boundaries of\nLLMs. This intersection of practical role-playing demands and evolving\nreasoning capabilities raises an important research question: \"Can reasoning\ntechniques enhance the role-playing capabilities of LLMs?\" To address this, we\nconduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3\ndistinct role-playing strategies, comparing the effectiveness of direct\nzero-shot role-playing, role-playing with Chain-of-Thought (CoT), and\nrole-playing using reasoning-optimized LLMs. Our findings reveal that CoT may\nreduce role-playing performance, reasoning-optimized LLMs are unsuitable for\nrole-playing, reasoning ability disrupts the role-playing scaling law, large\nmodels still lack proficiency in advanced role-playing, and Chinese\nrole-playing performance surpasses English role-playing performance.\nFurthermore, based on extensive experimental results, we propose two promising\nfuture research directions: Role-aware CoT for improving role-playing LLMs and\nReinforcement Learning for role-playing LLMs, aiming to enhance the\nadaptability, consistency, and effectiveness of role-playing LLMs for both\nresearch and real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16940v2", "cate": "cs.CL", "date": "2025-02-24", "updated": "2025-07-22"}
{"id": "2505.08087", "title": "Manifold Learning with Normalizing Flows: Towards Regularity, Expressivity and Iso-Riemannian Geometry", "authors": ["Willem Diepeveen", "Deanna Needell"], "categories": ["cs.LG", "math.DG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08087v2", "summary": "Modern machine learning increasingly leverages the insight that\nhigh-dimensional data often lie near low-dimensional, non-linear manifolds, an\nidea known as the manifold hypothesis. By explicitly modeling the geometric\nstructure of data through learning Riemannian geometry algorithms can achieve\nimproved performance and interpretability in tasks like clustering,\ndimensionality reduction, and interpolation. In particular, learned pullback\ngeometry has recently undergone transformative developments that now make it\nscalable to learn and scalable to evaluate, which further opens the door for\nprincipled non-linear data analysis and interpretable machine learning.\nHowever, there are still steps to be taken when considering real-world\nmulti-modal data. This work focuses on addressing distortions and modeling\nerrors that can arise in the multi-modal setting and proposes to alleviate both\nchallenges through isometrizing the learned Riemannian structure and balancing\nregularity and expressivity of the diffeomorphism parametrization. We showcase\nthe effectiveness of the synergy of the proposed approaches in several\nnumerical experiments with both synthetic and real data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08087v2", "cate": "cs.LG", "date": "2025-05-12", "updated": "2025-07-21"}
{"id": "2412.08629", "title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models", "authors": ["Vladimir Kulikov", "Matan Kleiner", "Inbar Huberman-Spiegelglas", "Tomer Michaeli"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project's webpage at this https URL", "url": "http://arxiv.org/abs/2412.08629v2", "summary": "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow\nmodel often involves inverting the image into its corresponding noise map.\nHowever, inversion by itself is typically insufficient for obtaining\nsatisfactory results, and therefore many methods additionally intervene in the\nsampling process. Such methods achieve improved results but are not seamlessly\ntransferable between model architectures. Here, we introduce FlowEdit, a\ntext-based editing method for pre-trained T2I flow models, which is\ninversion-free, optimization-free and model agnostic. Our method constructs an\nODE that directly maps between the source and target distributions\n(corresponding to the source and target text prompts) and achieves a lower\ntransport cost than the inversion approach. This leads to state-of-the-art\nresults, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples\nare available on the project's webpage.", "comment": "ICCV 2025. Project's webpage at\n  https://matankleiner.github.io/flowedit/", "pdf_url": "http://arxiv.org/pdf/2412.08629v2", "cate": "cs.CV", "date": "2024-12-11", "updated": "2025-07-22"}
{"id": "2410.08800", "title": "Data Processing for the OpenGPT-X Model Family", "authors": ["Nicolo' Brandizzi", "Hammam Abdelwahab", "Anirban Bhowmick", "Lennard Helmer", "Benny Jrg Stein", "Pavel Denisov", "Qasid Saleem", "Michael Fromm", "Mehdi Ali", "Richard Rutmann", "Farzad Naderi", "Mohamad Saif Agy", "Alexander Schwirjow", "Fabian Kch", "Luzian Hahn", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Dennis Wegener", "Nicolas Flores-Herr", "Joachim Khler", "Johannes Leveling"], "categories": ["cs.CL", "H.3.1; I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.08800v3", "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final filtered data. We\ndistinguish between curated data and web data, as each of these categories is\nhandled by distinct pipelines, with curated data undergoing minimal filtering\nand web data requiring extensive filtering and deduplication. This distinction\nguided the development of specialized algorithmic solutions for both pipelines.\nIn addition to describing the processing methodologies, we provide an in-depth\nanalysis of the datasets, increasing transparency and alignment with European\ndata regulations. Finally, we share key insights and challenges faced during\nthe project, offering recommendations for future endeavors in large-scale\nmultilingual data preparation for LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.08800v3", "cate": "cs.CL", "date": "2024-10-11", "updated": "2025-07-22"}
{"id": "2507.16798", "title": "From heteroclinic loops to homoclinic snaking in reversible systems: rigorous forcing through computer-assisted proofs", "authors": ["Jan Bouwe van den Berg", "Gabriel William Duchesne", "Jean-Philippe Lessard"], "categories": ["math.DS", "cs.NA", "math.AP", "math.NA", "34C37 (Primary) 37M21, 35B36, 65G40, 65T40, 42A10, 37C79 (Secondary)"], "primary_category": "Subjects:       Dynamical Systems (math.DS)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.16798v1", "summary": "Homoclinic snaking is a widespread phenomenon observed in many\npattern-forming systems. Demonstrating its occurrence in non-perturbative\nregimes has proven difficult, although a forcing theory has been developed\nbased on the identification of patterned front solutions. These heteroclinic\nsolutions are themselves challenging to analyze due to the nonlinear nature of\nthe problem. In this paper, we use computer-assisted proofs to find\nparameterized loops of heteroclinic connections between equilibria and periodic\norbits in time reversible systems. This leads to a proof of homoclinic snaking\nin both the Swift-Hohenberg and Gray-Scott problems. Our results demonstrate\nthat computer-assisted proofs of continuous families of connecting orbits in\nnonlinear dynamical systems are a powerful tool for understanding global\ndynamics and their dependence on parameters.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.16798v1", "cate": "math.DS", "date": "2025-07-22", "updated": "2025-07-22"}
{"id": "2503.00196", "title": "PRISM: High-Resolution & Precise Counterfactual Medical Image Generation using Language-guided Stable Diffusion", "authors": ["Amar Kumar", "Anita Kriz", "Mohammad Havaei", "Tal Arbel"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      MIDL 2025", "url": "http://arxiv.org/abs/2503.00196v2", "summary": "Developing reliable and generalizable deep learning systems for medical\nimaging faces significant obstacles due to spurious correlations, data\nimbalances, and limited text annotations in datasets. Addressing these\nchallenges requires architectures that are robust to the unique complexities\nposed by medical imaging data. Rapid advancements in vision-language foundation\nmodels within the natural image domain prompt the question of how they can be\nadapted for medical imaging tasks. In this work, we present PRISM, a framework\nthat leverages foundation models to generate high-resolution, language-guided\nmedical image counterfactuals using Stable Diffusion. Our approach demonstrates\nunprecedented precision in selectively modifying spurious correlations (the\nmedical devices) and disease features, enabling the removal and addition of\nspecific attributes while preserving other image characteristics. Through\nextensive evaluation, we show how PRISM advances counterfactual generation and\nenables the development of more robust downstream classifiers for clinically\ndeployable solutions. To facilitate broader adoption and research, we make our\ncode publicly available at https://github.com/Amarkr1/PRISM.", "comment": "MIDL 2025", "pdf_url": "http://arxiv.org/pdf/2503.00196v2", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-07-22"}
{"id": "2505.17579", "title": "Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation", "authors": ["Teruki Sano", "Minoru Kuribayashi", "Masao Sakai", "Shuji Ishobe", "Eisuke Koizumi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to EUSIPCO 2025", "url": "http://arxiv.org/abs/2505.17579v2", "summary": "In this paper, we propose a novel framework for ownership verification of\ndeep neural network (DNN) models for image classification tasks. It allows\nverification of model identity by both the rightful owner and third party\nwithout presenting the original model. We assume a gray-box scenario where an\nunauthorized user owns a model that is illegally copied from the original\nmodel, provides services in a cloud environment, and the user throws images and\nreceives the classification results as a probability distribution of output\nclasses. The framework applies a white-box adversarial attack to align the\noutput probability of a specific class to a designated value. Due to the\nknowledge of original model, it enables the owner to generate such adversarial\nexamples. We propose a simple but effective adversarial attack method based on\nthe iterative Fast Gradient Sign Method (FGSM) by introducing control\nparameters. Experimental results confirm the effectiveness of the\nidentification of DNN models using adversarial attack.", "comment": "Accepted to EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2505.17579v2", "cate": "cs.LG", "date": "2025-05-23", "updated": "2025-07-22"}
{"id": "2412.10908", "title": "Do large language vision models understand 3D shapes?", "authors": ["Sagi Eppel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.10908v5", "summary": "Large vision language models (LVLM) are the leading A.I approach for\nachieving a general visual understanding of the world. Models such as GPT,\nClaude, Gemini, and LLama can use images to understand and analyze complex\nvisual scenes. 3D objects and shapes are the basic building blocks of the\nworld, recognizing them is a fundamental part of human perception. The goal of\nthis work is to test whether LVLMs truly understand 3D shapes by testing the\nmodels ability to identify and match objects of the exact same 3D shapes but\nwith different orientations and materials/textures. A large number of test\nimages were created using CGI with a huge number of highly diverse objects,\nmaterials, and scenes. The results of this test show that the ability of such\nmodels to match 3D shapes is significantly below humans but much higher than\nrandom guesses. Suggesting that the models have gained some abstract\nunderstanding of 3D shapes but still trail far beyond humans in this task.\nMainly it seems that the models can easily identify the same object with a\ndifferent orientation as well as matching identical 3D shapes of the same\norientation but with different materials and textures. However, when both the\nobject material and orientation are changed, all models perform poorly relative\nto humans. Code and benchmark are available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.10908v5", "cate": "cs.CV", "date": "2024-12-14", "updated": "2025-07-22"}
{"id": "2502.08773", "title": "Universal Model Routing for Efficient LLM Inference", "authors": ["Wittawat Jitkrittum", "Harikrishna Narasimhan", "Ankit Singh Rawat", "Jeevesh Juneja", "Congchao Wang", "Zifeng Wang", "Alec Go", "Chen-Yu Lee", "Pradeep Shenoy", "Rina Panigrahy", "Aditya Krishna Menon", "Sanjiv Kumar"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.08773v2", "summary": "Model routing is a simple technique for reducing the inference cost of large\nlanguage models (LLMs), wherein one maintains a pool of candidate LLMs, and\nlearns to route each prompt to the smallest feasible LLM. Existing works focus\non learning a router for a fixed pool of LLMs. In this paper, we consider the\nproblem of dynamic routing, where new, previously unobserved LLMs are available\nat test time. We propose UniRoute, a new approach to this problem that relies\non representing each LLM as a feature vector, derived based on predictions on a\nset of representative prompts. Based on this, we detail two effective\ninstantiations of UniRoute, relying on cluster-based routing and a learned\ncluster map respectively. We show that these are estimates of a theoretically\noptimal routing rule, and quantify their errors via an excess risk bound.\nExperiments on a range of public benchmarks show the effectiveness of UniRoute\nin routing amongst more than 30 unseen LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.08773v2", "cate": "cs.CL", "date": "2025-02-12", "updated": "2025-07-22"}
{"id": "1805.11237", "title": "Chebyshev symplectic methods based on continuous-stage Runge-Kutta methods", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The paper needs to be further modified", "url": "http://arxiv.org/abs/1805.11237v4", "summary": "We develop Chebyshev symplectic methods based on Chebyshev orthogonal\npolynomials of the first and second kind separately in this paper. Such type of\nsymplectic methods can be conveniently constructed with the newly-built theory\nof weighted continuous-stage Runge-Kutta methods. A few numerical experiments\nare well performed to verify the efficiency of our new methods.", "comment": "The paper needs to be further modified", "pdf_url": "http://arxiv.org/pdf/1805.11237v4", "cate": "math.NA", "date": "2018-05-29", "updated": "2025-07-22"}
{"id": "2505.00025", "title": "A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1", "authors": ["Mingda Zhang", "Jianglong Qin"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      14 pages, 1 figures", "url": "http://arxiv.org/abs/2505.00025v2", "summary": "Despite significant advances in foundation models like DeepSeek-R1 and\nChatGPT, their deployment in medical settings faces critical challenges\nincluding computational requirements and professional knowledge barriers. This\npaper presents an efficient lightweight medical large language model\narchitecture that systematically addresses these challenges through\nthree-dimensional optimization: knowledge acquisition, model compression, and\ncomputational enhancement. We design a knowledge transfer pipeline from\nDeepSeek-R1-Distill-70B to DeepSeek-R1-Distill-7B using Low-Rank Adaptation\n(LoRA) for precise medical knowledge retention. Through 4-bit quantization and\nmixed-precision strategies, we achieve substantial model compression while\npreserving medical reasoning capabilities. The inference framework incorporates\nFlash Attention acceleration and continuous batching, complemented by\nspecialized prompt templates for diverse medical queries. Experimental\nevaluation on medical benchmarks demonstrates that our approach maintains 92.1%\naccuracy on USMLE examinations while reducing memory consumption by 64.7% and\ninference latency by 12.4% compared to baseline models. This work provides a\npractical solution for deploying advanced language models in\nresource-constrained medical environments, enabling broader accessibility of\nAI-assisted healthcare.", "comment": "14 pages, 1 figures", "pdf_url": "http://arxiv.org/pdf/2505.00025v2", "cate": "cs.CL", "date": "2025-04-25", "updated": "2025-07-22"}
{"id": "2506.12490", "title": "Note on Follow-the-Perturbed-Leader in Combinatorial Semi-Bandit Problems", "authors": ["Botao Chen", "Junya Honda"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Corrected typos and the error of the proof of Lemma 10", "url": "http://arxiv.org/abs/2506.12490v2", "summary": "This paper studies the optimality and complexity of\nFollow-the-Perturbed-Leader (FTPL) policy in size-invariant combinatorial\nsemi-bandit problems. Recently, Honda et al. (2023) and Lee et al. (2024)\nshowed that FTPL achieves Best-of-Both-Worlds (BOBW) optimality in standard\nmulti-armed bandit problems with Fr\\'{e}chet-type distributions. However, the\noptimality of FTPL in combinatorial semi-bandit problems remains unclear. In\nthis paper, we consider the regret bound of FTPL with geometric resampling (GR)\nin size-invariant semi-bandit setting, showing that FTPL respectively achieves\n$O\\left(\\sqrt{m^2 d^\\frac{1}{\\alpha}T}+\\sqrt{mdT}\\right)$ regret with\nFr\\'{e}chet distributions, and the best possible regret bound of\n$O\\left(\\sqrt{mdT}\\right)$ with Pareto distributions in adversarial setting.\nFurthermore, we extend the conditional geometric resampling (CGR) to\nsize-invariant semi-bandit setting, which reduces the computational complexity\nfrom $O(d^2)$ of original GR to $O\\left(md\\left(\\log(d/m)+1\\right)\\right)$\nwithout sacrificing the regret performance of FTPL.", "comment": "Corrected typos and the error of the proof of Lemma 10", "pdf_url": "http://arxiv.org/pdf/2506.12490v2", "cate": "cs.LG", "date": "2025-06-14", "updated": "2025-07-22"}
{"id": "2412.16881", "title": "Predicting the Reliability of an Image Classifier under Image Distortion", "authors": ["Dang Nguyen", "Sunil Gupta", "Kien Do", "Svetha Venkatesh"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.16881v2", "summary": "In image classification tasks, deep learning models are vulnerable to image\ndistortions i.e. their accuracy significantly drops if the input images are\ndistorted. An image-classifier is considered \"reliable\" if its accuracy on\ndistorted images is above a user-specified threshold. For a quality control\npurpose, it is important to predict if the image-classifier is\nunreliable/reliable under a distortion level. In other words, we want to\npredict whether a distortion level makes the image-classifier \"non-reliable\" or\n\"reliable\". Our solution is to construct a training set consisting of\ndistortion levels along with their \"non-reliable\" or \"reliable\" labels, and\ntrain a machine learning predictive model (called distortion-classifier) to\nclassify unseen distortion levels. However, learning an effective\ndistortion-classifier is a challenging problem as the training set is highly\nimbalanced. To address this problem, we propose a Gaussian process based method\nto rebalance the training set. We conduct extensive experiments to show that\nour method significantly outperforms several baselines on six popular image\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.16881v2", "cate": "cs.CV", "date": "2024-12-22", "updated": "2025-07-22"}
{"id": "2502.18699", "title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment", "authors": ["Tianze Wang", "Dongnan Gui", "Yifan Hu", "Shuhang Lin", "Linjun Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2502.18699v3", "summary": "Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning large language models (LLMs). Yet its reliance on a singular reward\nmodel often overlooks the diversity of human preferences. Recent approaches\naddress this limitation by leveraging multi-dimensional feedback to fine-tune\ncorresponding reward models and train LLMs using reinforcement learning.\nHowever, the process is costly and unstable, especially given the competing and\nheterogeneous nature of human preferences. In this paper, we propose Mixing\nPreference Optimization (MPO), a post-processing framework for aggregating\nsingle-objective policies as an alternative to both multi-objective RLHF\n(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it\nlog-linearly combines existing policies into a unified one with the weight of\neach policy computed via a batch stochastic mirror descent. Empirical results\ndemonstrate that MPO achieves balanced performance across diverse preferences,\noutperforming or matching existing models with significantly reduced\ncomputational costs.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2502.18699v3", "cate": "cs.CL", "date": "2025-02-25", "updated": "2025-07-22"}
{"id": "1806.03380", "title": "Symplectic integration with Jacobi polynomials", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The paper needs to be further modified", "url": "http://arxiv.org/abs/1806.03380v2", "summary": "In this paper, we study symplectic integration of canonical Hamiltonian\nsystems with Jacobi polynomials. The relevant theoretical results of\ncontinuous-stage Runge-Kutta methods are revisited firstly and then symplectic\nmethods with Jacobi polynomials will be established. A few numerical\nexperiments are well performed to verify the efficiency of our new methods.", "comment": "The paper needs to be further modified", "pdf_url": "http://arxiv.org/pdf/1806.03380v2", "cate": "math.NA", "date": "2018-06-08", "updated": "2025-07-22"}
{"id": "2505.17692", "title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection", "authors": ["Ziteng Yang", "Jingzehua Xu", "Yanshu Li", "Zepeng Li", "Yeqiang Wang", "Xinghui Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.17692v2", "summary": "Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.17692v2", "cate": "cs.CV", "date": "2025-05-23", "updated": "2025-07-22"}
{"id": "2506.16629", "title": "Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data", "authors": ["Eric V. Strobl"], "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      R code is available at this http URL", "url": "http://arxiv.org/abs/2506.16629v3", "summary": "Causal inference in longitudinal biomedical data remains a central challenge,\nespecially in psychiatry, where symptom heterogeneity and latent confounding\nfrequently undermine classical estimators. Most existing methods for treatment\neffect estimation presuppose a fixed outcome variable and address confounding\nthrough observed covariate adjustment. However, the assumption of\nunconfoundedness may not hold for a fixed outcome in practice. To address this\nfoundational limitation, we directly optimize the outcome definition to\nmaximize causal identifiability. Our DEBIAS (Durable Effects with\nBackdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,\nclinically interpretable weights for outcome aggregation, maximizing durable\ntreatment effects and empirically minimizing both observed and latent\nconfounding by leveraging the time-limited direct effects of prior treatments\nin psychiatric longitudinal data. The algorithm also furnishes an empirically\nverifiable test for outcome unconfoundedness. DEBIAS consistently outperforms\nstate-of-the-art methods in recovering causal effects for clinically\ninterpretable composite outcomes across comprehensive experiments in depression\nand schizophrenia.", "comment": "R code is available at github.com/ericstrobl/DEBIAS", "pdf_url": "http://arxiv.org/pdf/2506.16629v3", "cate": "cs.LG", "date": "2025-06-19", "updated": "2025-07-22"}
{"id": "2501.05710", "title": "EmotiCrafter: Text-to-Emotional-Image Generation based on Valence-Arousal Model", "authors": ["Shengqi Dang", "Yi He", "Long Ling", "Ziqing Qian", "Nanxuan Zhao", "Nan Cao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 9 figures", "url": "http://arxiv.org/abs/2501.05710v2", "summary": "Recent research shows that emotions can enhance users' cognition and\ninfluence information communication. While research on visual emotion analysis\nis extensive, limited work has been done on helping users generate emotionally\nrich image content. Existing work on emotional image generation relies on\ndiscrete emotion categories, making it challenging to capture complex and\nsubtle emotional nuances accurately. Additionally, these methods struggle to\ncontrol the specific content of generated images based on text prompts. In this\nwork, we introduce the new task of continuous emotional image content\ngeneration (C-EICG) and present EmotiCrafter, an emotional image generation\nmodel that generates images based on text prompts and Valence-Arousal values.\nSpecifically, we propose a novel emotion-embedding mapping network that embeds\nValence-Arousal values into textual features, enabling the capture of specific\nemotions in alignment with intended input prompts. Additionally, we introduce a\nloss function to enhance emotion expression. The experimental results show that\nour method effectively generates images representing specific emotions with the\ndesired content and outperforms existing techniques.", "comment": "11 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2501.05710v2", "cate": "cs.CV", "date": "2025-01-10", "updated": "2025-07-22"}
{"id": "2503.07457", "title": "LLMs syntactically adapt their language use to their conversational partner", "authors": ["Florian Kandra", "Vera Demberg", "Alexander Koller"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      5 pages, 1 table, 3 figures, accepted at ACL (main conference) 2025", "url": "http://arxiv.org/abs/2503.07457v2", "summary": "It has been frequently observed that human speakers align their language use\nwith each other during conversations. In this paper, we study empirically\nwhether large language models (LLMs) exhibit the same behavior of\nconversational adaptation. We construct a corpus of conversations between LLMs\nand find that two LLM agents end up making more similar syntactic choices as\nconversations go on, confirming that modern LLMs adapt their language use to\ntheir conversational partners in at least a rudimentary way.", "comment": "5 pages, 1 table, 3 figures, accepted at ACL (main conference) 2025", "pdf_url": "http://arxiv.org/pdf/2503.07457v2", "cate": "cs.CL", "date": "2025-03-10", "updated": "2025-07-22"}
{"id": "1808.02391", "title": "Energy-preserving continuous-stage partitioned Runge-Kutta methods", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      There are some drawbacks for this paper", "url": "http://arxiv.org/abs/1808.02391v2", "summary": "In this paper, we present continuous-stage partitioned Runge-Kutta (csPRK)\nmethods for energy-preserving integration of Hamiltonian systems. A sufficient\ncondition for the energy preservation of the csPRK methods is derived. It is\nshown that the presented condition contains the existing condition for\nenergy-preserving continuous-stage Runge-Kutta methods as a special case. A\nnoticeable and interesting result is that when we use the simplifying\nassumptions of order conditions and the normalized shifted Legendre polynomials\nfor constructing high-order energy-preserving csPRK methods, both the Butcher\n\"weight\" coefficients $B_\\tau$ and $\\widehat{B}_\\tau$ must be equal to $1$. As\nillustrative examples, new energy-preserving integrators are acquired by virtue\nof the presented condition, and for the sake of verifying our theoretical\nresults, some numerical experiments are reported.", "comment": "There are some drawbacks for this paper", "pdf_url": "http://arxiv.org/pdf/1808.02391v2", "cate": "math.NA", "date": "2018-08-07", "updated": "2025-07-22"}
{"id": "2505.22116", "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Yitong zhou", "Qi Liu", "Yanhu Xie"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.22116v3", "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.22116v3", "cate": "cs.CL", "date": "2025-05-28", "updated": "2025-07-22"}
{"id": "2506.17576", "title": "Towards a deeper GCN: Alleviate over-smoothing with iterative training and fine-tuning", "authors": ["Furong Peng", "Jinzhen Gao", "Xuan Lu", "Kang Liu", "Yifan Huo", "Sheng Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      17 pages,18 figures", "url": "http://arxiv.org/abs/2506.17576v2", "summary": "Graph Convolutional Networks (GCNs) suffer from severe performance\ndegradation in deep architectures due to over-smoothing. While existing studies\nprimarily attribute the over-smoothing to repeated applications of graph\nLaplacian operators, our empirical analysis reveals a critical yet overlooked\nfactor: trainable linear transformations in GCNs significantly exacerbate\nfeature collapse, even at moderate depths (e.g., 8 layers). In contrast,\nSimplified Graph Convolution (SGC), which removes these transformations,\nmaintains stable feature diversity up to 32 layers, highlighting linear\ntransformations' dual role in facilitating expressive power and inducing\nover-smoothing. However, completely removing linear transformations weakens the\nmodel's expressive capacity. To address this trade-off, we propose Layer-wise\nGradual Training (LGT), a novel training strategy that progressively builds\ndeep GCNs while preserving their expressiveness. LGT integrates three\ncomplementary components: (1) layer-wise training to stabilize optimization\nfrom shallow to deep layers, (2) low-rank adaptation to fine-tune shallow\nlayers and accelerate training, and (3) identity initialization to ensure\nsmooth integration of new layers and accelerate convergence. Extensive\nexperiments on benchmark datasets demonstrate that LGT achieves\nstate-of-the-art performance on vanilla GCN, significantly improving accuracy\neven in 32-layer settings. Moreover, as a training method, LGT can be\nseamlessly combined with existing methods such as PairNorm and ContraNorm,\nfurther enhancing their performance in deeper networks. LGT offers a general,\narchitecture-agnostic training framework for scalable deep GCNs. The code is\navailable at [https://github.com/jfklasdfj/LGT_GCN].", "comment": "17 pages,18 figures", "pdf_url": "http://arxiv.org/pdf/2506.17576v2", "cate": "cs.LG", "date": "2025-06-21", "updated": "2025-07-22"}
{"id": "2502.02358", "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm", "authors": ["Ziyan Guo", "Zeyu Hu", "De Wen Soh", "Na Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2502.02358v5", "summary": "Human motion generation and editing are key components of computer vision.\nHowever, current approaches in this field tend to offer isolated solutions\ntailored to specific tasks, which can be inefficient and impractical for\nreal-world applications. While some efforts have aimed to unify motion-related\ntasks, these methods simply use different modalities as conditions to guide\nmotion generation. Consequently, they lack editing capabilities, fine-grained\ncontrol, and fail to facilitate knowledge sharing across tasks. To address\nthese limitations and provide a versatile, unified framework capable of\nhandling both human motion generation and editing, we introduce a novel\nparadigm: \\textbf{Motion-Condition-Motion}, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion. Based on this paradigm, we propose a unified framework,\n\\textbf{MotionLab}, which incorporates rectified flows to learn the mapping\nfrom source motion to target motion, guided by the specified conditions. In\nMotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional\ngeneration and editing without task-specific modules; 2) Aligned Rotational\nPosition Encoding to guarantee the time synchronization between source motion\nand target motion; 3) Task Specified Instruction Modulation; and 4) Motion\nCurriculum Learning for effective multi-task learning and knowledge sharing\nacross tasks. Notably, our MotionLab demonstrates promising generalization\ncapabilities and inference efficiency across multiple benchmarks for human\nmotion. Our code and additional video results are available at:\nhttps://diouo.github.io/motionlab.github.io/.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2502.02358v5", "cate": "cs.CV", "date": "2025-02-04", "updated": "2025-07-22"}
{"id": "2503.14023", "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code", "authors": ["Mihai Nadas", "Laura Diosan", "Andreea Tomescu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      24 pages, 6 tables, 1 figure, 64 references", "url": "http://arxiv.org/abs/2503.14023v2", "summary": "This survey reviews how large language models (LLMs) are transforming\nsynthetic training data generation in both natural language and code domains.\nBy producing artificial but task-relevant examples, these models can\nsignificantly augment or even substitute for real-world datasets, particularly\nin scenarios where labeled data is scarce, expensive, or sensitive. This paper\nsurveys recent advances in leveraging LLMs to create synthetic text and code,\nhighlighting key techniques such as prompt-based generation,\nretrieval-augmented pipelines, and iterative self-refinement. We examine how\nthese methods can enrich low-resource tasks (e.g., classification, question\nanswering) and facilitate code-centric applications (e.g., instruction tuning,\ncode translation, bug repair) through automated verification of functional\ncorrectness. Alongside potential benefits - cost-effectiveness, broad coverage,\nand controllable diversity - we discuss the accompanying challenges, including\nfactual inaccuracies in generated text, insufficient stylistic or\ndistributional realism, and risks of bias amplification. Proposed mitigation\nstrategies range from filtering and weighting synthetic outputs to\nreinforcement learning with execution feedback in code domains. We conclude by\noutlining open research directions, such as automated prompt engineering,\ncross-modal data synthesis, and robust evaluation frameworks, underscoring the\ngrowing importance of LLM-generated synthetic data in accelerating AI\ndevelopment while emphasizing ethical and quality safeguards.", "comment": "24 pages, 6 tables, 1 figure, 64 references", "pdf_url": "http://arxiv.org/pdf/2503.14023v2", "cate": "cs.CL", "date": "2025-03-18", "updated": "2025-07-22"}
{"id": "1808.08451", "title": "Energy-preserving continuous-stage Runge-Kutta-Nystrm methods", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The paper needs to be further modified", "url": "http://arxiv.org/abs/1808.08451v2", "summary": "Many practical problems can be described by second-order system\n$\\ddot{q}=-M\\nabla U(q)$, in which people give special emphasis to some\ninvariants with explicit physical meaning, such as energy, momentum, angular\nmomentum, etc. However, conventional numerical integrators for such systems\nwill fail to preserve any of these quantities which may lead to qualitatively\nincorrect numerical solutions. This paper is concerned with the development of\nenergy-preserving continuous-stage Runge-Kutta-Nystr\\\"om (csRKN) methods for\nsolving second-order systems. Sufficient conditions for csRKN methods to be\nenergy-preserving are presented and it is proved that all the energy-preserving\ncsRKN methods satisfying these sufficient conditions can be essentially induced\nby energy-preserving continuous-stage partitioned Runge-Kutta methods. Some\nillustrative examples are given and relevant numerical results are reported.", "comment": "The paper needs to be further modified", "pdf_url": "http://arxiv.org/pdf/1808.08451v2", "cate": "math.NA", "date": "2018-08-25", "updated": "2025-07-22"}
{"id": "2505.23714", "title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods", "authors": ["Roksana Goworek", "Harpal Karlcut", "Muhammad Shezad", "Nijaguna Darshana", "Abhishek Mane", "Syam Bondada", "Raghav Sikka", "Ulvi Mammadov", "Rauf Allahverdiyev", "Sriram Purighella", "Paridhi Gupta", "Muhinyia Ndegwa", "Haim Dubossarsky"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 22 figures, published at SIGTYP 2025 workshop in ACL", "url": "http://arxiv.org/abs/2505.23714v2", "summary": "This paper addresses the critical need for high-quality evaluation datasets\nin low-resource languages to advance cross-lingual transfer. While\ncross-lingual transfer offers a key strategy for leveraging multilingual\npretraining to expand language technologies to understudied and typologically\ndiverse languages, its effectiveness is dependent on quality and suitable\nbenchmarks. We release new sense-annotated datasets of sentences containing\npolysemous words, spanning ten low-resource languages across diverse language\nfamilies and scripts. To facilitate dataset creation, the paper presents a\ndemonstrably beneficial semi-automatic annotation method. The utility of the\ndatasets is demonstrated through Word-in-Context (WiC) formatted experiments\nthat evaluate transfer on these low-resource languages. Results highlight the\nimportance of targeted dataset creation and evaluation for effective polysemy\ndisambiguation in low-resource settings and transfer studies. The released\ndatasets and code aim to support further research into fair, robust, and truly\nmultilingual NLP.", "comment": "8 pages, 22 figures, published at SIGTYP 2025 workshop in ACL", "pdf_url": "http://arxiv.org/pdf/2505.23714v2", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-07-22"}
{"id": "2506.22095", "title": "Neural Approaches for Multi-Objective Routing on Multigraphs", "authors": ["Filip Rydin", "Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Balzs Kulcsr"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      23 pages, 5 Figures", "url": "http://arxiv.org/abs/2506.22095v2", "summary": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. Yet,\nexisting methods are unsuitable for routing on multigraphs, which feature\nmultiple edges with distinct attributes between node pairs, despite their\nstrong relevance in real-world scenarios. In this paper, we propose two graph\nneural network-based methods to address multi-objective routing on multigraphs.\nOur first approach operates directly on the multigraph by autoregressively\nselecting edges until a tour is completed. The second model first simplifies\nthe multigraph via a learned pruning strategy and then performs routing on the\nresulting simple graph. We evaluate both models empirically and demonstrate\ntheir strong performance across a range of problems and distributions.", "comment": "23 pages, 5 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22095v2", "cate": "cs.LG", "date": "2025-06-27", "updated": "2025-07-22"}
{"id": "2502.09110", "title": "Pulling Back the Curtain: Unsupervised Adversarial Detection via Contrastive Auxiliary Networks", "authors": ["Eylon Mizrahi", "Raz Lapid", "Moshe Sipper"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at SafeMM-AI @ ICCV 2025", "url": "http://arxiv.org/abs/2502.09110v2", "summary": "Deep learning models are widely employed in safety-critical applications yet\nremain susceptible to adversarial attacks -- imperceptible perturbations that\ncan significantly degrade model performance. Conventional defense mechanisms\npredominantly focus on either enhancing model robustness or detecting\nadversarial inputs independently. In this work, we propose an Unsupervised\nadversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover\nadversarial behavior within auxiliary feature representations, without the need\nfor adversarial examples. U-CAN is embedded within selected intermediate layers\nof the target model. These auxiliary networks, comprising projection layers and\nArcFace-based linear layers, refine feature representations to more effectively\ndistinguish between benign and adversarial inputs. Comprehensive experiments\nacross multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and\narchitectures (ResNet-50, VGG-16, and ViT) demonstrate that our method\nsurpasses existing unsupervised adversarial detection techniques, achieving\nsuperior F1 scores against four distinct attack methods. The proposed framework\nprovides a scalable and effective solution for enhancing the security and\nreliability of deep learning systems.", "comment": "Accepted at SafeMM-AI @ ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2502.09110v2", "cate": "cs.CV", "date": "2025-02-13", "updated": "2025-07-22"}
{"id": "2504.17665", "title": "Evaluating Intermediate Reasoning of Code-Assisted Large Language Models for Mathematics", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.17665v2", "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs generated programs in response to math\nreasoning tasks, with a focus on evaluating the soundness of the underlying\nreasoning processes. For this purpose, we assess the generations of five LLMs,\non several math datasets, both manually and automatically, and propose a\ntaxonomy of generated programs based on their logical soundness. Our findings\nshow that the capabilities of models significantly impact the logic implemented\nto solve the problem. Closed-source LLMs ground their programs in mathematical\nconcepts, whereas open-source models often resort to unsound reasoning, relying\non memorized information and exhaustive searches. Furthermore, increasing the\ndifficulty of problems decreases sound generations for all models, revealing a\ncritical shortcoming of LLMs on complex mathematics, contrary to what accuracy\nmetrics suggest. Our work highlights the need for more holistic evaluations of\ncode-assisted LLMs beyond execution accuracy metrics, toward a better\nunderstanding of LLMs' limits in the math domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.17665v2", "cate": "cs.CL", "date": "2025-04-24", "updated": "2025-07-22"}
{"id": "1809.01770", "title": "Energy-preserving integration of non-canonical Hamiltonian systems by continuous-stage methods", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The paper needs to be further modified", "url": "http://arxiv.org/abs/1809.01770v3", "summary": "As is well known, energy is generally deemed as one of the most important\nphysical invariants in many conservative problems and hence it is of remarkable\ninterest to consider numerical methods which are able to preserve it. In this\npaper, we are concerned with the energy-preserving integration of non-canonical\nHamiltonian systems by continuous-stage methods. Algebraic conditions in terms\nof the Butcher coefficients for ensuring the energy preservation, symmetry and\nquadratic-Casimir preservation respectively are presented. With the presented\ncondition and in use of orthogonal expansion techniques, the construction of\nenergy-preserving integrators is examined. A new class of energy-preserving\nintegrators which is symmetric and of order $2m$ is constructed. Some numerical\nresults are reported to verify our theoretical analysis and show the\neffectiveness of our new methods.", "comment": "The paper needs to be further modified", "pdf_url": "http://arxiv.org/pdf/1809.01770v3", "cate": "math.NA", "date": "2018-09-06", "updated": "2025-07-22"}
{"id": "2506.10516", "title": "CogStream: Context-guided Streaming Video Question Answering", "authors": ["Zicheng Zhao", "Kangyu Wang", "Shijie Li", "Rui Qian", "Weiyao Lin", "Huabin Liu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2506.10516v2", "summary": "Despite advancements in Video Large Language Models (Vid-LLMs) improving\nmultimodal understanding, challenges persist in streaming video reasoning due\nto its reliance on contextual information. Existing paradigms feed all\navailable historical contextual information into Vid-LLMs, resulting in a\nsignificant computational burden for visual data processing. Furthermore, the\ninclusion of irrelevant context distracts models from key details. This paper\nintroduces a challenging task called Context-guided Streaming Video Reasoning\n(CogStream), which simulates real-world streaming video scenarios, requiring\nmodels to identify the most relevant historical contextual information to\ndeduce answers for questions about the current stream. To support CogStream, we\npresent a densely annotated dataset featuring extensive and hierarchical\nquestion-answer pairs, generated by a semi-automatic pipeline. Additionally, we\npresent CogReasoner as a baseline model. It efficiently tackles this task by\nleveraging visual stream compression and historical dialogue retrieval.\nExtensive experiments prove the effectiveness of this method. The project is\nreleased on https://github.com/LiamZhao326/CogStream.", "comment": "Project page: https://github.com/LiamZhao326/CogStream", "pdf_url": "http://arxiv.org/pdf/2506.10516v2", "cate": "cs.CV", "date": "2025-06-12", "updated": "2025-07-22"}
{"id": "2506.23516", "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23516v3", "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23516v3", "cate": "cs.LG", "date": "2025-06-30", "updated": "2025-07-22"}
{"id": "2502.16748", "title": "GS-TransUNet: Integrated 2D Gaussian Splatting and Transformer UNet for Accurate Skin Lesion Analysis", "authors": ["Anand Kumar", "Kavinder Roghit Kanthen", "Josna John"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 7 figures, SPIE Medical Imaging 2025. 13407-1340736", "url": "http://arxiv.org/abs/2502.16748v2", "summary": "We can achieve fast and consistent early skin cancer detection with recent\ndevelopments in computer vision and deep learning techniques. However, the\nexisting skin lesion segmentation and classification prediction models run\nindependently, thus missing potential efficiencies from their integrated\nexecution. To unify skin lesion analysis, our paper presents the Gaussian\nSplatting - Transformer UNet (GS-TransUNet), a novel approach that\nsynergistically combines 2D Gaussian splatting with the Transformer UNet\narchitecture for automated skin cancer diagnosis. Our unified deep learning\nmodel efficiently delivers dual-function skin lesion classification and\nsegmentation for clinical diagnosis. Evaluated on ISIC-2017 and PH2 datasets,\nour network demonstrates superior performance compared to existing\nstate-of-the-art models across multiple metrics through 5-fold\ncross-validation. Our findings illustrate significant advancements in the\nprecision of segmentation and classification. This integration sets new\nbenchmarks in the field and highlights the potential for further research into\nmulti-task medical image analysis methodologies, promising enhancements in\nautomated diagnostic systems.", "comment": "12 pages, 7 figures, SPIE Medical Imaging 2025. 13407-1340736", "pdf_url": "http://arxiv.org/pdf/2502.16748v2", "cate": "cs.CV", "date": "2025-02-23", "updated": "2025-07-21"}
{"id": "2505.02304", "title": "Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition", "authors": ["Siyu Liang", "Yunan Li", "Wentian Xin", "Huizhou Chen", "Xujie Liu", "Kang Liu", "Qiguang Miao"], "categories": ["cs.CL", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures", "url": "http://arxiv.org/abs/2505.02304v2", "summary": "Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies.", "comment": "9 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2505.02304v2", "cate": "cs.CL", "date": "2025-05-05", "updated": "2025-07-22"}
{"id": "1809.06825", "title": "Two types of variational integrators and their equivalence", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The paper needs to be further modified", "url": "http://arxiv.org/abs/1809.06825v2", "summary": "In this paper, we introduce two types of variational integrators, one\noriginating from the discrete Hamilton's principle while the other from\nGalerkin variational approach. It turns out that these variational integrators\nare equivalent to each other when they are used for integrating the classical\nmechanical system with Lagrangian function\n$L(q,\\dot{q})=\\frac{1}{2}\\dot{q}^TM\\dot{q}-U(q)$ ($M$ is an invertible\nsymmetric constant matrix). They are symplectic, symmetric, possess\nsuper-convergence order $2s$ (which depends on the degree of the approximation\npolynomials), and can be related to continuous-stage partitioned Runge-Kutta\nmethods.", "comment": "The paper needs to be further modified", "pdf_url": "http://arxiv.org/pdf/1809.06825v2", "cate": "math.NA", "date": "2018-09-18", "updated": "2025-07-22"}
{"id": "2507.04123", "title": "Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge", "authors": ["Linshen Liu", "Boyan Su", "Junyue Jiang", "Guanlin Wu", "Cong Guo", "Ceyu Xu", "Hao Frank Yang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.04123v2", "summary": "This paper presents Edge-based Mixture of Experts (MoE) Collaborative\nComputing (EMC2), an optimal computing system designed for autonomous vehicles\n(AVs) that simultaneously achieves low-latency and high-accuracy 3D object\ndetection. Unlike conventional approaches, EMC2 incorporates a scenario-aware\nMoE architecture specifically optimized for edge platforms. By effectively\nfusing LiDAR and camera data, the system leverages the complementary strengths\nof sparse 3D point clouds and dense 2D images to generate robust multimodal\nrepresentations. To enable this, EMC2 employs an adaptive multimodal data\nbridge that performs multi-scale preprocessing on sensor inputs, followed by a\nscenario-aware routing mechanism that dynamically dispatches features to\ndedicated expert models based on object visibility and distance. In addition,\nEMC2 integrates joint hardware-software optimizations, including hardware\nresource utilization optimization and computational graph simplification, to\nensure efficient and real-time inference on resource-constrained edge devices.\nExperiments on open-source benchmarks clearly show the EMC2 advancements as an\nend-to-end system. On the KITTI dataset, it achieves an average accuracy\nimprovement of 3.58% and a 159.06% inference speedup compared to 15 baseline\nmethods on Jetson platforms, with similar performance gains on the nuScenes\ndataset, highlighting its capability to advance reliable, real-time 3D object\ndetection tasks for AVs. The official implementation is available at\nhttps://github.com/LinshenLiu622/EMC2.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.04123v2", "cate": "cs.CV", "date": "2025-07-05", "updated": "2025-07-22"}
{"id": "2507.02903", "title": "Harnessing Near-Infrared Spectroscopy and Machine Learning for Traceable Classification of Hanwoo and Holstein Beef", "authors": ["AMM Nurul Alam", "Abdul Samad", "AMM Shamsul Alam", "Jahan Ara Monti", "Ayesha Muazzam"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      We need to withdraw the present manuscript to make some major revisions to avoid potential conflict with relevant paper from other research", "url": "http://arxiv.org/abs/2507.02903v2", "summary": "This study evaluates the use of Near-Infrared spectroscopy (NIRS) combined\nwith advanced machine learning (ML) techniques to differentiate Hanwoo beef\n(HNB) and Holstein beef (HLB) to address food authenticity, mislabeling, and\nadulteration. Rapid and non-invasive spectral data were attained by a portable\nNIRS, recording absorbance data within the wavelength range of 700 to 1100 nm.\nA total of 40 Longissimus lumborum samples, evenly split between HNB and HLB,\nwere obtained from a local hypermarket. Data analysis using Principal Component\nAnalysis (PCA) demonstrated distinct spectral patterns associated with chemical\nchanges, clearly separating the two beef varieties and accounting for 93.72% of\nthe total variance. ML models, including Linear Discriminant Analysis (LDA),\nSupport Vector Machine (SVM), Logistic Regression (LR), Random Forest, Gradient\nBoosting (GB), K-Nearest Neighbors, Decision Tree (DT), Naive Bayes (NB), and\nNeural Networks (NN), were implemented, optimized through hyperparameter\ntuning, and validated by 5-fold cross-validation techniques to enhance model\nrobustness and prevent overfitting. Random Forest provided the highest\npredictive accuracy with a Receiver Operating Characteristic (ROC) Area Under\nthe Curve (AUC) of 0.8826, closely followed by the SVM model at 0.8747.\nFurthermore, GB and NN algorithms exhibited satisfactory performances, with\ncross-validation scores of 0.752. Notably, the NN model achieved the highest\nrecall rate of 0.7804, highlighting its suitability in scenarios requiring\nheightened sensitivity. DT and NB exhibited comparatively lower predictive\nperformance. The LR and SVM models emerged as optimal choices by effectively\nbalancing high accuracy, precision, and recall. This study confirms that\nintegrating NIRS with ML techniques offers a powerful and reliable method for\nmeat authenticity, significantly contributing to detecting food fraud.", "comment": "We need to withdraw the present manuscript to make some major\n  revisions to avoid potential conflict with relevant paper from other research", "pdf_url": "http://arxiv.org/pdf/2507.02903v2", "cate": "cs.LG", "date": "2025-06-24", "updated": "2025-07-10"}
{"id": "2502.19848", "title": "One-for-More: Continual Diffusion Model for Anomaly Detection", "authors": ["Xiaofan Li", "Xin Tan", "Zhuo Chen", "Zhizhong Zhang", "Ruixin Zhang", "Rizen Guo", "Guannan Jiang", "Yulong Chen", "Yanyun Qu", "Lizhuang Ma", "Yuan Xie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by CVPR2025", "url": "http://arxiv.org/abs/2502.19848v3", "summary": "With the rise of generative models, there is a growing interest in unifying\nall tasks within a generative framework. Anomaly detection methods also fall\ninto this scope and utilize diffusion models to generate or reconstruct normal\nsamples when given arbitrary anomaly images. However, our study found that the\ndiffusion model suffers from severe ``faithfulness hallucination'' and\n``catastrophic forgetting'', which can't meet the unpredictable pattern\nincrements. To mitigate the above problems, we propose a continual diffusion\nmodel that uses gradient projection to achieve stable continual learning.\nGradient projection deploys a regularization on the model updating by modifying\nthe gradient towards the direction protecting the learned knowledge. But as a\ndouble-edged sword, it also requires huge memory costs brought by the Markov\nprocess. Hence, we propose an iterative singular value decomposition method\nbased on the transitive property of linear representation, which consumes tiny\nmemory and incurs almost no performance loss. Finally, considering the risk of\n``over-fitting'' to normal images of the diffusion model, we propose an\nanomaly-masked network to enhance the condition mechanism of the diffusion\nmodel. For continual anomaly detection, ours achieves first place in 17/18\nsettings on MVTec and VisA. Code is available at\nhttps://github.com/FuNz-0/One-for-More", "comment": "Accepted by CVPR2025", "pdf_url": "http://arxiv.org/pdf/2502.19848v3", "cate": "cs.CV", "date": "2025-02-27", "updated": "2025-07-22"}
{"id": "2505.14311", "title": "HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing", "authors": ["Shamsuddeen Hassan Muhammad", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Falalu Ibrahim Lawan", "Babangida Sani", "Sukairaj Hafiz Imam", "Yusuf Aliyu", "Sani Abdullahi Sani", "Ali Usman Umar", "Tajuddeen Gwadabe", "Kenneth Church", "Vukosi Marivate"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.14311v3", "summary": "Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.14311v3", "cate": "cs.CL", "date": "2025-05-20", "updated": "2025-07-22"}
{"id": "2407.05275", "title": "Numerical solution of two dimensional scalar conservation laws using compact implicit numerical schemes", "authors": ["Peter Frolkovic", "Dagmar Zakova"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.05275v3", "summary": "This paper deals with the numerical solution of conservation laws in the two\ndimensional case using a novel compact implicit time discretization that\nenables applications of fast algebraic solvers. We present details for the\nsecond order accurate parametric scheme based on the finite volume method\nincluding simple variants of ENO (Essentially Non-Oscillatory) and WENO\n(Weighted Essentially Non-Oscillatory) approximations. To avoid oscillatory\nnumerical solutions for large time steps, we propose limiting in time which is\nprovable non-oscillatory in the case of linear advection equation with variable\nvelocity. We present numerical experiments for representative linear and\nnonlinear problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.05275v3", "cate": "math.NA", "date": "2024-07-07", "updated": "2025-07-22"}
{"id": "2507.14526", "title": "Studying homing and synchronizing sequences for Timed Finite State Machines with output delays", "authors": ["Evgenii Vinarskii", "Jakub Ruszil", "Adam Roman", "Natalia Kushik"], "categories": ["cs.FL", "cs.CC"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14526v2", "summary": "The paper introduces final state identification (synchronizing and homing)\nsequences for Timed Finite State Machines (TFSMs) with output delays and\ninvestigates their properties. We formally define the notions of homing\nsequences (HSs) and synchronizing sequences (SSs) for these TFSMs and\ndemonstrate that several properties that hold for untimed machines do not\nnecessarily apply to timed ones. Furthermore, we explore the applicability of\nvarious approaches for deriving SSs and HSs for Timed FSMs with output delays,\nsuch as truncated successor tree-based and FSM abstraction-based methods.\nCorrespondingly, we identify the subclasses of TFSMs for which these approaches\ncan be directly applied and those for which other methods are required.\nAdditionally, we evaluate the complexity of existence check and derivation of\n(shortest) HSs / SSs for TFSMs with output delays.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14526v2", "cate": "cs.FL", "date": "2025-07-19", "updated": "2025-07-22"}
{"id": "2507.04441", "title": "The Joys of Categorical Conformal Prediction", "authors": ["Michele Caprio"], "categories": ["stat.ML", "cs.AI", "cs.LG", "math.CT", "Primary: 18D99, Secondary: 62G07, 28B20"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04441v2", "summary": "Conformal prediction (CP) is an Uncertainty Representation technique that\ndelivers finite-sample calibrated prediction regions for any underlying Machine\nLearning model. Its status as an Uncertainty Quantification (UQ) tool, though,\nhas remained conceptually opaque: While Conformal Prediction Regions (CPRs)\ngive an ordinal representation of uncertainty (larger regions typically\nindicate higher uncertainty), they lack the capability to cardinally quantify\nit (twice as large regions do not imply twice the uncertainty). We adopt a\ncategory-theoretic approach to CP -- framing it as a morphism, embedded in a\ncommuting diagram, of two newly-defined categories -- that brings us three\njoys. First, we show that -- under minimal assumptions -- CP is intrinsically a\nUQ mechanism, that is, its cardinal UQ capabilities are a structural feature of\nthe method. Second, we demonstrate that CP bridges (and perhaps subsumes) the\nBayesian, frequentist, and imprecise probabilistic approaches to predictive\nstatistical reasoning. Finally, we show that a CPR is the image of a covariant\nfunctor. This observation is relevant to AI privacy: It implies that privacy\nnoise added locally does not break the global coverage guarantee.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04441v2", "cate": "stat.ML", "date": "2025-07-06", "updated": "2025-07-22"}
{"id": "2507.07271", "title": "Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time", "authors": ["Julianna Piskorz", "Krzysztof Kacprzyk", "Harry Amad", "Mihaela van der Schaar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Presented at the Actionable Interpretability Workshop at ICML 2025", "url": "http://arxiv.org/abs/2507.07271v2", "summary": "The Average Treatment Effect (ATE) is a foundational metric in causal\ninference, widely used to assess intervention efficacy in randomized controlled\ntrials (RCTs). However, in many applications -- particularly in healthcare --\nthis static summary fails to capture the nuanced dynamics of treatment effects\nthat vary with both dose and time. We propose a framework for modelling\ntreatment effect trajectories as smooth surfaces over dose and time, enabling\nthe extraction of clinically actionable insights such as onset time, peak\neffect, and duration of benefit. To ensure interpretability, robustness, and\nverifiability -- key requirements in high-stakes domains -- we adapt\nSemanticODE, a recent framework for interpretable trajectory modelling, to the\ncausal setting where treatment effects are never directly observed. Our\napproach decouples the estimation of trajectory shape from the specification of\nclinically relevant properties (e.g., maxima, inflection points), supporting\ndomain-informed priors, post-hoc editing, and transparent analysis. We show\nthat our method yields accurate, interpretable, and editable models of\ntreatment dynamics, facilitating both rigorous causal analysis and practical\ndecision-making.", "comment": "Presented at the Actionable Interpretability Workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.07271v2", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-21"}
{"id": "2503.05182", "title": "MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions", "authors": ["Qingyuan Zhou", "Yuehu Gong", "Weidong Yang", "Jiaze Li", "Yeqi Luo", "Baixin Xu", "Shuhao Li", "Ben Fei", "Ying He"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV'25", "url": "http://arxiv.org/abs/2503.05182v2", "summary": "Novel view synthesis (NVS) and surface reconstruction (SR) are essential\ntasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks\nare often addressed independently, with GS-based rendering methods struggling\nunder diverse light conditions and failing to produce accurate surfaces, while\nGS-based reconstruction methods frequently compromise rendering quality. This\nraises a central question: must rendering and reconstruction always involve a\ntrade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian\nsplatting for Surface Reconstruction that enhances both rendering quality and\n3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS\nand the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,\nproviding precise geometry information to the 3D-GS branch. Leveraging this\ngeometry, the 3D-GS branch employs a geometry-guided illumination decomposition\nmodule that captures reflected and transmitted components, enabling realistic\nrendering under varied light conditions. Using the transmitted component as\nsupervision, the 2D-GS branch also achieves high-fidelity surface\nreconstruction. Throughout the optimization process, the 2D-GS and 3D-GS\nbranches undergo alternating optimization, providing mutual supervision. Prior\nto this, each branch completes an independent warm-up phase, with an early\nstopping strategy implemented to reduce computational costs. We evaluate MGSR\non a diverse set of synthetic and real-world datasets, at both object and scene\nlevels, demonstrating strong performance in rendering and surface\nreconstruction. Code is available at https://github.com/TsingyuanChou/MGSR.", "comment": "Accepted at ICCV'25", "pdf_url": "http://arxiv.org/pdf/2503.05182v2", "cate": "cs.CV", "date": "2025-03-07", "updated": "2025-07-22"}
{"id": "2505.16104", "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Findings", "url": "http://arxiv.org/abs/2505.16104v2", "summary": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2505.16104v2", "cate": "cs.CL", "date": "2025-05-22", "updated": "2025-07-22"}
{"id": "2407.15315", "title": "A Fast and Accurate Solver for the Fractional Fokker-Planck Equation with Dirac-Delta Initial Conditions", "authors": ["Qihao Ye", "Xiaochuan Tian", "Dong Wang"], "categories": ["math.NA", "cs.NA", "34K37, 44A35, 35Q84, 65D40, 33C10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      38 pages, 10 figures", "url": "http://arxiv.org/abs/2407.15315v2", "summary": "The classical Fokker-Planck equation (FPE) is a key tool in physics for\ndescribing systems influenced by drag forces and Gaussian noise, with\napplications spanning multiple fields. We consider the fractional Fokker-Planck\nequation (FFPE), which models the time evolution of probability densities for\nsystems driven by L\\'evy processes, relevant in scenarios where Gaussian\nassumptions fail. The paper presents an efficient and accurate numerical\napproach for the free-space FFPE with constant coefficients and Dirac-delta\ninitial conditions. This method utilizes the integral representation of the\nsolutions and enables the efficient handling of very high-dimensional problems\nusing fast algorithms. Our work is the first to present a high-precision\nnumerical solver for the free-space FFPE with Dirac-delta initial conditions.\nIn addition to Dirac-delta initial data, we demonstrate the effectiveness of\nour method for initial conditions given by sums of Gaussians. This opens the\ndoor for future research on more complex scenarios, including those with\nvariable coefficients and other types of initial conditions.", "comment": "38 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2407.15315v2", "cate": "math.NA", "date": "2024-07-22", "updated": "2025-07-22"}
{"id": "2507.05056", "title": "INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling", "authors": ["Xin Dong", "Shichao Dong", "Jin Wang", "Jing Huang", "Li Zhou", "Zenghui Sun", "Lihua Jing", "Jingsong Lan", "Xiaoyong Zhu", "Bo Zheng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.05056v2", "summary": "Hallucinations in large vision-language models (LVLMs) pose significant\nchallenges for real-world applications, as LVLMs may generate responses that\nappear plausible yet remain inconsistent with the associated visual content.\nThis issue rarely occurs in human cognition. We argue that this discrepancy\narises from humans' ability to effectively leverage multimodal interaction\ninformation in data samples. Specifically, humans typically first gather\nmultimodal information, analyze the interactions across modalities for\nunderstanding, and then express their understanding through language. Motivated\nby this observation, we conduct extensive experiments on popular LVLMs and\nobtained insights that surprisingly reveal human-like, though less pronounced,\ncognitive behavior of LVLMs on multimodal samples. Building on these findings,\nwe further propose \\textbf{INTER}: \\textbf{Inter}action Guidance Sampling, a\nnovel training-free algorithm that mitigate hallucinations without requiring\nadditional data. Specifically, INTER explicitly guides LVLMs to effectively\nreapply their understanding of multimodal interaction information when\ngenerating responses, thereby reducing potential hallucinations. On six\nbenchmarks including VQA and image captioning tasks, INTER achieves an average\nimprovement of up to 3.4\\% on five LVLMs compared to the state-of-the-art\ndecoding strategy. The code will be released when the paper is accepted.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.05056v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-22"}
{"id": "2507.07754", "title": "OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting", "authors": ["Jaeheun Jung", "Bosung Jung", "Suhyun Bae", "Donghun Lee"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICCV 2025 workshop U&ME 2025 (2nd Workshop and Challenge on Unlearning and Model Editing)", "url": "http://arxiv.org/abs/2507.07754v2", "summary": "Machine unlearning seeks to remove the influence of particular data or class\nfrom trained models to meet privacy, legal, or ethical requirements. Existing\nunlearning methods tend to forget shallowly: phenomenon of an unlearned model\npretend to forget by adjusting only the model response, while its internal\nrepresentations retain information sufficiently to restore the forgotten data\nor behavior. We empirically confirm the widespread shallowness by reverting the\nforgetting effect of various unlearning methods via training-free performance\nrecovery attack and gradient-inversion-based data reconstruction attack. To\naddress this vulnerability fundamentally, we define a theoretical criterion of\n``deep forgetting'' based on one-point-contraction of feature representations\nof data to forget. We also propose an efficient approximation algorithm, and\nuse it to construct a novel general-purpose unlearning algorithm:\nOne-Point-Contraction (OPC). Empirical evaluations on image classification\nunlearning benchmarks show that OPC achieves not only effective unlearning\nperformance but also superior resilience against both performance recovery\nattack and gradient-inversion attack. The distinctive unlearning performance of\nOPC arises from the deep feature forgetting enforced by its theoretical\nfoundation, and recaps the need for improved robustness of machine unlearning\nmethods.", "comment": "ICCV 2025 workshop U&ME 2025 (2nd Workshop and Challenge on\n  Unlearning and Model Editing)", "pdf_url": "http://arxiv.org/pdf/2507.07754v2", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-22"}
{"id": "2503.06132", "title": "USP: Unified Self-Supervised Pretraining for Image Generation and Understanding", "authors": ["Xiangxiang Chu", "Renda Li", "Yong Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV2025", "url": "http://arxiv.org/abs/2503.06132v3", "summary": "Recent studies have highlighted the interplay between diffusion models and\nrepresentation learning. Intermediate representations from diffusion models can\nbe leveraged for downstream visual tasks, while self-supervised vision models\ncan enhance the convergence and generation quality of diffusion models.\nHowever, transferring pretrained weights from vision models to diffusion models\nis challenging due to input mismatches and the use of latent spaces. To address\nthese challenges, we propose Unified Self-supervised Pretraining (USP), a\nframework that initializes diffusion models via masked latent modeling in a\nVariational Autoencoder (VAE) latent space. USP achieves comparable performance\nin understanding tasks while significantly improving the convergence speed and\ngeneration quality of diffusion models. Our code will be publicly available at\nhttps://github.com/AMAP-ML/USP.", "comment": "Accepted to ICCV2025", "pdf_url": "http://arxiv.org/pdf/2503.06132v3", "cate": "cs.CV", "date": "2025-03-08", "updated": "2025-07-22"}
{"id": "2505.23060", "title": "Self-Correcting Code Generation Using Small Language Models", "authors": ["Jeonghun Cho", "Deokhyung Kang", "Hyounghun Kim", "Gary Geunbae Lee"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.23060v2", "summary": "Self-correction has demonstrated potential in code generation by allowing\nlanguage models to revise and improve their outputs through successive\nrefinement. Recent studies have explored prompting-based strategies that\nincorporate verification or feedback loops using proprietary models, as well as\ntraining-based methods that leverage their strong reasoning capabilities.\nHowever, whether smaller models possess the capacity to effectively guide their\noutputs through self-reflection remains unexplored. Our findings reveal that\nsmaller models struggle to exhibit reflective revision behavior across both\nself-correction paradigms. In response, we introduce CoCoS, an approach\ndesigned to enhance the ability of small language models for multi-turn code\ncorrection. Specifically, we propose an online reinforcement learning objective\nthat trains the model to confidently maintain correct outputs while\nprogressively correcting incorrect outputs as turns proceed. Our approach\nfeatures an accumulated reward function that aggregates rewards across the\nentire trajectory and a fine-grained reward better suited to multi-turn\ncorrection scenarios. This facilitates the model in enhancing initial response\nquality while achieving substantial improvements through self-correction. With\n1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on\nHumanEval compared to the baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.23060v2", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-07-22"}
{"id": "2409.08793", "title": "Modeling Advection-Dominated Flows with Space-Local Reduced-Order Models", "authors": ["Toby van Gastelen", "Wouter Edeling", "Benjamin Sanderse"], "categories": ["math.NA", "cs.NA", "65Mxx"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      30 pages, 13 figures, source code can be found at this https URL", "url": "http://arxiv.org/abs/2409.08793v2", "summary": "Reduced-order models (ROMs) are often used to accelerate the simulation of\nlarge physical systems. However, traditional ROM techniques, such as those\nbased on proper orthogonal decomposition (POD), often struggle with\nadvection-dominated flows due to the slow singular value decay. This results in\nhigh computational costs and potential instabilities.\n  This paper proposes a novel approach using space-local POD to address the\nchallenges arising from the slow singular value decay. Instead of global basis\nfunctions, our method employs local basis functions that are applied across the\ndomain, analogous to the finite element method, but with a data-driven basis.\nBy dividing the domain into subdomains and applying the space-local POD, we\nachieve a representation that is sparse and that generalizes better outside the\ntraining regime. This allows the use of a larger number of basis functions\ncompared to standard POD, without prohibitive computational costs. To ensure\nsmoothness across subdomain boundaries, we introduce overlapping subdomains\ninspired by the partition of unity method.\n  Our approach is validated through simulations of the 1D and 2D advection\nequation. We demonstrate that using our space-local approach we obtain a ROM\nthat generalizes better to flow conditions which are not part of the training\ndata. In addition, we show that the constructed ROM inherits the energy\nconservation and non-linear stability properties from the full-order model.\nFinally, we find that using a space-local ROM allows for larger time steps.", "comment": "30 pages, 13 figures, source code can be found at\n  https://github.com/tobyvg/local_POD_overlap.jl", "pdf_url": "http://arxiv.org/pdf/2409.08793v2", "cate": "math.NA", "date": "2024-09-13", "updated": "2025-07-22"}
{"id": "2507.06229", "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving", "authors": ["Xiangru Tang", "Tianrui Qin", "Tianhao Peng", "Ziyang Zhou", "Daniel Shao", "Tingting Du", "Xinming Wei", "Peng Xia", "Fang Wu", "He Zhu", "Ge Zhang", "Jiaheng Liu", "Xingyao Wang", "Sirui Hong", "Chenglin Wu", "Hao Cheng", "Chi Wang", "Wangchunshu Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06229v4", "summary": "Current AI agents cannot effectively learn from each other's problem-solving\nexperiences or use past successes to guide self-reflection and error correction\nin new tasks. We introduce Agent KB, a shared knowledge base that captures both\nhigh-level problem-solving strategies and detailed execution lessons, enabling\nknowledge transfer across agent frameworks. Agent KB implements a novel\nteacher-student dual-phase retrieval mechanism where student agents retrieve\nworkflow-level patterns for strategic guidance while teacher agents identify\nexecution-level patterns for refinement. This hierarchical approach enables\nagents to break out of limited reasoning pathways by incorporating diverse\nstrategies from external sources. Evaluations on the GAIA benchmark demonstrate\nsubstantial performance gains, with Agent KB improving success rates by up to\n6.06 percentage points overall under pass@1. For SWE-bench code repair tasks,\nour system significantly improved resolution rates, with o3-mini achieving an\n8.67 percentage point gain (23 percent to 31.67 percent) in pass@1. Our\nablation studies demonstrate that the refinement module proves most critical,\nwith its removal causing a 3.85% drop on challenging Level 3 tasks,\nhighlighting that effective knowledge transfer necessitates both strategic\nguidance and execution-level refinement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06229v4", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-21"}
{"id": "2507.08472", "title": "Pre-Training LLMs on a budget: A comparison of three optimizers", "authors": ["Joel Schlotthauer", "Christian Kroos", "Chris Hinze", "Viktor Hangya", "Luzian Hahn", "Fabian Kch"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08472v2", "summary": "Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08472v2", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-22"}
{"id": "2503.06312", "title": "DOFA-CLIP: Multimodal Vision-Language Foundation Models for Earth Observation", "authors": ["Zhitong Xiong", "Yi Wang", "Weikang Yu", "Adam J Stewart", "Jie Zhao", "Nils Lehmann", "Thomas Dujardin", "Zhenghang Yuan", "Pedram Ghamisi", "Xiao Xiang Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      code & weights: this https URL", "url": "http://arxiv.org/abs/2503.06312v2", "summary": "Earth observation (EO) spans a broad spectrum of modalities, including\noptical, radar, multispectral, and hyperspectral data, each capturing distinct\nenvironmental signals. However, current vision-language models in EO,\nparticularly CLIP-based variants, remain confined to individual modalities,\nlimiting generalization and scalability across diverse tasks. We present\nDOFA-CLIP (Dynamic-One-For-All CLIP), a unified vision-language foundation\nmodel that dynamically adapts to EO modalities with flexible spectral\nconfigurations through a single Transformer backbone. Our approach introduces\nthree key contributions: 1) the construction of GeoLangBind-2M, a large-scale\nEO image-text dataset covering six heterogeneous modalities with rich natural\nlanguage descriptions; 2) a novel training strategy called VECT (Vision-models\nEnhanced Contrastive Text-image pretraining), which enhances the spatial\nawareness of CLIP features with multiple vision foundation models; and 3) a\nModality-aware Knowledge Agglomeration (MaKA) module that refines feature\ndistillation with modality-specific awareness. DOFA-CLIP achieves\nstate-of-the-art zero-shot performance across a wide range of EO benchmarks,\nincluding unseen modalities and a diverse number of input spectral bands.\nTogether, these contributions establish a scalable foundation for multimodal EO\nunderstanding and open new avenues for integrating heterogeneous EO data with\nlarge language models. Code and datasets will be released. Code and datasets\nare publicly available.", "comment": "code & weights: https://github.com/xiong-zhitong/DOFA-CLIP", "pdf_url": "http://arxiv.org/pdf/2503.06312v2", "cate": "cs.CV", "date": "2025-03-08", "updated": "2025-07-22"}
{"id": "2505.23822", "title": "Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction", "authors": ["Mai Ali", "Christopher Lucasius", "Tanmay P. Patel", "Madison Aitken", "Jacob Vorstman", "Peter Szatmari", "Marco Battaglia", "Deepa Kundur"], "categories": ["cs.CL", "cs.MM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      6 pages, 1 figure, 3 tables. Accepted to ICSM 2025. The corresponding author is Mai Ali (maia dot ali at mail dot utoronto dot ca). Christopher Lucasius and Tanmay P. Patel contributed equally", "url": "http://arxiv.org/abs/2505.23822v2", "summary": "Speech is a noninvasive digital phenotype that can offer valuable insights\ninto mental health conditions, but it is often treated as a single modality. In\ncontrast, we propose the treatment of patient speech data as a trimodal\nmultimedia data source for depression detection. This study explores the\npotential of large language model-based architectures for speech-based\ndepression prediction in a multimodal regime that integrates speech-derived\ntext, acoustic landmarks, and vocal biomarkers. Adolescent depression presents\na significant challenge and is often comorbid with multiple disorders, such as\nsuicidal ideation and sleep disturbances. This presents an additional\nopportunity to integrate multi-task learning (MTL) into our study by\nsimultaneously predicting depression, suicidal ideation, and sleep disturbances\nusing the multimodal formulation. We also propose a longitudinal analysis\nstrategy that models temporal changes across multiple clinical interactions,\nallowing for a comprehensive understanding of the conditions' progression. Our\nproposed approach, featuring trimodal, longitudinal MTL is evaluated on the\nDepression Early Warning dataset. It achieves a balanced accuracy of 70.8%,\nwhich is higher than each of the unimodal, single-task, and non-longitudinal\nmethods.", "comment": "6 pages, 1 figure, 3 tables. Accepted to ICSM 2025. The corresponding\n  author is Mai Ali (maia dot ali at mail dot utoronto dot ca). Christopher\n  Lucasius and Tanmay P. Patel contributed equally", "pdf_url": "http://arxiv.org/pdf/2505.23822v2", "cate": "cs.CL", "date": "2025-05-28", "updated": "2025-07-22"}
{"id": "2409.20431", "title": "Multilevel Picard approximations and deep neural networks with ReLU, leaky ReLU, and softplus activation overcome the curse of dimensionality when approximating semilinear parabolic partial differential equations in $L^p$-sense", "authors": ["Ariel Neufeld", "Tuan Anh Nguyen"], "categories": ["math.NA", "cs.LG", "cs.NA", "math.PR"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.20431v4", "summary": "We prove that multilevel Picard approximations and deep neural networks with\nReLU, leaky ReLU, and softplus activation are capable of approximating\nsolutions of semilinear Kolmogorov PDEs in $L^\\mathfrak{p}$-sense,\n$\\mathfrak{p}\\in [2,\\infty)$, in the case of gradient-independent,\nLipschitz-continuous nonlinearities, while the computational effort of the\nmultilevel Picard approximations and the required number of parameters in the\nneural networks grow at most polynomially in both dimension $d\\in \\mathbb{N}$\nand reciprocal of the prescribed accuracy $\\epsilon$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.20431v4", "cate": "math.NA", "date": "2024-09-30", "updated": "2025-07-22"}
{"id": "2507.06261", "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities", "authors": ["Gheorghe Comanici", "Eric Bieber", "Mike Schaekermann", "Ice Pasupat", "Noveen Sachdeva", "Inderjit Dhillon", "Marcel Blistein", "Ori Ram", "Dan Zhang", "Evan Rosen", "Luke Marris", "Sam Petulla", "Colin Gaffney", "Asaf Aharoni", "Nathan Lintz", "Tiago Cardal Pais", "Henrik Jacobsson", "Idan Szpektor", "Nan-Jiang Jiang", "Krishna Haridasan", "Ahmed Omran", "Nikunj Saunshi", "Dara Bahri", "Gaurav Mishra", "Eric Chu", "Toby Boyd", "Brad Hekman", "Aaron Parisi", "Chaoyi Zhang", "Kornraphop Kawintiranon", "Tania Bedrax-Weiss", "Oliver Wang", "Ya Xu", "Ollie Purkiss", "Uri Mendlovic", "Ila Deutel", "Nam Nguyen", "Adam Langley", "Flip Korn", "Lucia Rossazza", "Alexandre Ram", "Sagar Waghmare", "Helen Miller", "Nathan Byrd", "Ashrith Sheshan", "Raia Hadsell Sangnie Bhardwaj", "Pawel Janus", "Tero Rissa", "Dan Horgan", "Sharon Silver", "Ayzaan Wahid", "Sergey Brin", "Yves Raimond", "Klemen Kloboves", "Cindy Wang", "Nitesh Bharadwaj Gundavarapu", "Ilia Shumailov", "Bo Wang", "Mantas Pajarskas", "Joe Heyward", "Martin Nikoltchev", "Maciej Kula", "Hao Zhou", "Zachary Garrett", "Sushant Kafle", "Sercan Arik", "Ankita Goel", "Mingyao Yang", "Jiho Park", "Koji Kojima", "Parsa Mahmoudieh", "Koray Kavukcuoglu", "Grace Chen", "Doug Fritz", "Anton Bulyenov", "Sudeshna Roy", "Dimitris Paparas", "Hadar Shemtov", "Bo-Juen Chen", "Robin Strudel", "David Reitter", "Aurko Roy", "Andrey Vlasov", "Changwan Ryu", "Chas Leichner", "Haichuan Yang", "Zelda Mariet", "Denis Vnukov", "Tim Sohn", "Amy Stuart", "Wei Liang", "Minmin Chen", "Praynaa Rawlani", "Christy Koh", "JD Co-Reyes", "Guangda Lai", "Praseem Banzal", "Dimitrios Vytiniotis", "Jieru Mei", "Mu Cai", "Mohammed Badawi", "Corey Fry", "Ale Hartman", "Daniel Zheng", "Eric Jia", "James Keeling", "Annie Louis", "Ying Chen", "Efren Robles", "Wei-Chih Hung", "Howard Zhou", "Nikita Saxena", "Sonam Goenka", "Olivia Ma", "Zach Fisher", "Mor Hazan Taege", "Emily Graves", "David Steiner", "Yujia Li", "Sarah Nguyen", "Rahul Sukthankar", "Joe Stanton", "Ali Eslami", "Gloria Shen", "Berkin Akin", "Alexey Guseynov", "Yiqian Zhou", "Jean-Baptiste Alayrac", "Armand Joulin", "Efrat Farkash", "Ashish Thapliyal", "Stephen Roller", "Noam Shazeer", "Todor Davchev", "Terry Koo", "Hannah Forbes-Pollard", "Kartik Audhkhasi", "Greg Farquhar", "Adi Mayrav Gilady", "Maggie Song", "John Aslanides", "Piermaria Mendolicchio", "Alicia Parrish", "John Blitzer", "Pramod Gupta", "Xiaoen Ju", "Xiaochen Yang", "Puranjay Datta", "Andrea Tacchetti", "Sanket Vaibhav Mehta", "Gregory Dibb", "Shubham Gupta", "Federico Piccinini", "Raia Hadsell", "Sujee Rajayogam", "Jiepu Jiang", "Patrick Griffin", "Patrik Sundberg", "Jamie Hayes", "Alexey Frolov", "Tian Xie", "Adam Zhang", "Kingshuk Dasgupta", "Uday Kalra", "Lior Shani", "Klaus Macherey", "Tzu-Kuo Huang", "Liam MacDermed", "Karthik Duddu", "Paulo Zacchello", "Zi Yang", "Jessica Lo", "Kai Hui", "Matej Kastelic", "Derek Gasaway", "Qijun Tan", "Summer Yue", "Pablo Barrio", "John Wieting", "Weel Yang", "Andrew Nystrom", "Solomon Demmessie", "Anselm Levskaya", "Fabio Viola", "Chetan Tekur", "Greg Billock", "George Necula", "Mandar Joshi", "Rylan Schaeffer", "Swachhand Lokhande", "Christina Sorokin", "Pradeep Shenoy", "Mia Chen", "Mark Collier", "Hongji Li", "Taylor Bos", "Nevan Wichers", "Sun Jae Lee", "Angline Pouget", "Santhosh Thangaraj", "Kyriakos Axiotis", "Phil Crone", "Rachel Sterneck", "Nikolai Chinaev", "Victoria Krakovna", "Oleksandr Ferludin", "Ian Gemp", "Stephanie Winkler", "Dan Goldberg", "Ivan Korotkov", "Kefan Xiao", "Malika Mehrotra", "Sandeep Mariserla", "Vihari Piratla", "Terry Thurk", "Khiem Pham", "Hongxu Ma", "Alexandre Senges", "Ravi Kumar", "Clemens Meyer", "Ellie Talius", "Nuo Wang Pierse", "Ballie Sandhu", "Horia Toma", "Kuo Lin", "Swaroop Nath", "Tom Stone", "Dorsa Sadigh", "Nikita Gupta", "Arthur Guez", "Avi Singh", "Matt Thomas", "Tom Duerig", "Yuan Gong", "Richard Tanburn", "Lydia Lihui Zhang", "Phuong Dao", "Mohamed Hammad", "Sirui Xie", "Shruti Rijhwani", "Ben Murdoch", "Duhyeon Kim", "Will Thompson", "Heng-Tze Cheng", "Daniel Sohn", "Pablo Sprechmann", "Qiantong Xu", "Srinivas Tadepalli", "Peter Young", "Ye Zhang", "Hansa Srinivasan", "Miranda Aperghis", "Aditya Ayyar", "Hen Fitoussi", "Ryan Burnell", "David Madras", "Mike Dusenberry", "Xi Xiong", "Tayo Oguntebi", "Ben Albrecht", "Jrg Bornschein", "Jovana Mitrovi", "Mason Dimarco", "Bhargav Kanagal Shamanna", "Premal Shah", "Eren Sezener", "Shyam Upadhyay", "Dave Lacey", "Craig Schiff", "Sebastien Baur", "Sanjay Ganapathy", "Eva Schnider", "Mateo Wirth", "Connor Schenck", "Andrey Simanovsky", "Yi-Xuan Tan", "Philipp Frnken", "Dennis Duan", "Bharath Mankalale", "Nikhil Dhawan", "Kevin Sequeira", "Zichuan Wei", "Shivanker Goel", "Caglar Unlu", "Yukun Zhu", "Haitian Sun", "Ananth Balashankar", "Kurt Shuster", "Megh Umekar", "Mahmoud Alnahlawi", "Aron van den Oord", "Kelly Chen", "Yuexiang Zhai", "Zihang Dai", "Kuang-Huei Lee", "Eric Doi", "Lukas Zilka", "Rohith Vallu", "Disha Shrivastava", "Jason Lee", "Hisham Husain", "Honglei Zhuang", "Vincent Cohen-Addad", "Jarred Barber", "James Atwood", "Adam Sadovsky", "Quentin Wellens", "Steven Hand", "Arunkumar Rajendran", "Aybuke Turker", "CJ Carey", "Yuanzhong Xu", "Hagen Soltau", "Zefei Li", "Xinying Song", "Conglong Li", "Iurii Kemaev", "Sasha Brown", "Andrea Burns", "Viorica Patraucean", "Piotr Stanczyk", "Renga Aravamudhan", "Mathieu Blondel", "Hila Noga", "Lorenzo Blanco", "Will Song", "Michael Isard", "Mandar Sharma", "Reid Hayes", "Dalia El Badawy", "Avery Lamp", "Itay Laish", "Olga Kozlova", "Kelvin Chan", "Sahil Singla", "Srinivas Sunkara", "Mayank Upadhyay", "Chang Liu", "Aijun Bai", "Jarek Wilkiewicz", "Martin Zlocha", "Jeremiah Liu", "Zhuowan Li", "Haiguang Li", "Omer Barak", "Ganna Raboshchuk", "Jiho Choi", "Fangyu Liu", "Erik Jue", "Mohit Sharma", "Andreea Marzoca", "Robert Busa-Fekete", "Anna Korsun", "Andre Elisseeff", "Zhe Shen", "Sara Mc Carthy", "Kay Lamerigts", "Anahita Hosseini", "Hanzhao Lin", "Charlie Chen", "Fan Yang", "Kushal Chauhan", "Mark Omernick", "Dawei Jia", "Karina Zainullina", "Demis Hassabis", "Danny Vainstein", "Ehsan Amid", "Xiang Zhou", "Ronny Votel", "Eszter Vrtes", "Xinjian Li", "Zongwei Zhou", "Angeliki Lazaridou", "Brendan McMahan", "Arjun Narayanan", "Hubert Soyer", "Sujoy Basu", "Kayi Lee", "Bryan Perozzi", "Qin Cao", "Leonard Berrada", "Rahul Arya", "Ke Chen", "Katrina", "Xu", "Matthias Lochbrunner", "Alex Hofer", "Sahand Sharifzadeh", "Renjie Wu", "Sally Goldman", "Pranjal Awasthi", "Xuezhi Wang", "Yan Wu", "Claire Sha", "Biao Zhang", "Maciej Mikua", "Filippo Graziano", "Siobhan Mcloughlin", "Irene Giannoumis", "Youhei Namiki", "Chase Malik", "Carey Radebaugh", "Jamie Hall", "Ramiro Leal-Cavazos", "Jianmin Chen", "Vikas Sindhwani", "David Kao", "David Greene", "Jordan Griffith", "Chris Welty", "Ceslee Montgomery", "Toshihiro Yoshino", "Liangzhe Yuan", "Noah Goodman", "Assaf Hurwitz Michaely", "Kevin Lee", "KP Sawhney", "Wei Chen", "Zheng Zheng", "Megan Shum", "Nikolay Savinov", "Etienne Pot", "Alex Pak", "Morteza Zadimoghaddam", "Sijal Bhatnagar", "Yoad Lewenberg", "Blair Kutzman", "Ji Liu", "Lesley Katzen", "Jeremy Selier", "Josip Djolonga", "Dmitry Lepikhin", "Kelvin Xu", "Jacky Liang", "Jiewen Tan", "Benoit Schillings", "Muge Ersoy", "Pete Blois", "Bernd Bandemer", "Abhimanyu Singh", "Sergei Lebedev", "Pankaj Joshi", "Adam R. Brown", "Evan Palmer", "Shreya Pathak", "Komal Jalan", "Fedir Zubach", "Shuba Lall", "Randall Parker", "Alok Gunjan", "Sergey Rogulenko", "Sumit Sanghai", "Zhaoqi Leng", "Zoltan Egyed", "Shixin Li", "Maria Ivanova", "Kostas Andriopoulos", "Jin Xie", "Elan Rosenfeld", "Auriel Wright", "Ankur Sharma", "Xinyang Geng", "Yicheng Wang", "Sam Kwei", "Renke Pan", "Yujing Zhang", "Gabby Wang", "Xi Liu", "Chak Yeung", "Elizabeth Cole", "Aviv Rosenberg", "Zhen Yang", "Phil Chen", "George Polovets", "Pranav Nair", "Rohun Saxena", "Josh Smith", "Shuo-yiin Chang", "Aroma Mahendru", "Svetlana Grant", "Anand Iyer", "Irene Cai", "Jed McGiffin", "Jiaming Shen", "Alanna Walton", "Antonious Girgis", "Oliver Woodman", "Rosemary Ke", "Mike Kwong", "Louis Rouillard", "Jinmeng Rao", "Zhihao Li", "Yuntao Xu", "Flavien Prost", "Chi Zou", "Ziwei Ji", "Alberto Magni", "Tyler Liechty", "Dan A. Calian", "Deepak Ramachandran", "Igor Krivokon", "Hui Huang", "Terry Chen", "Anja Hauth", "Anastasija Ili", "Weijuan Xi", "Hyeontaek Lim", "Vlad-Doru Ion", "Pooya Moradi", "Metin Toksoz-Exley", "Kalesha Bullard", "Miltos Allamanis", "Xiaomeng Yang", "Sophie Wang", "Zhi Hong", "Anita Gergely", "Cheng Li", "Bhavishya Mittal", "Vitaly Kovalev", "Victor Ungureanu", "Jane Labanowski", "Jan Wassenberg", "Nicolas Lacasse", "Geoffrey Cideron", "Petar Devi", "Annie Marsden", "Lynn Nguyen", "Michael Fink", "Yin Zhong", "Tatsuya Kiyono", "Desi Ivanov", "Sally Ma", "Max Bain", "Kiran Yalasangi", "Jennifer She", "Anastasia Petrushkina", "Mayank Lunayach", "Carla Bromberg", "Sarah Hodkinson", "Vilobh Meshram", "Daniel Vlasic", "Austin Kyker", "Steve Xu", "Jeff Stanway", "Zuguang Yang", "Kai Zhao", "Matthew Tung", "Seth Odoom", "Yasuhisa Fujii", "Justin Gilmer", "Eunyoung Kim", "Felix Halim", "Quoc Le", "Bernd Bohnet", "Seliem El-Sayed", "Behnam Neyshabur", "Malcolm Reynolds", "Dean Reich", "Yang Xu", "Erica Moreira", "Anuj Sharma", "Zeyu Liu", "Mohammad Javad Hosseini", "Naina Raisinghani", "Yi Su", "Ni Lao", "Daniel Formoso", "Marco Gelmi", "Almog Gueta", "Tapomay Dey", "Elena Gribovskaya", "Domagoj evid", "Sidharth Mudgal", "Garrett Bingham", "Jianling Wang", "Anurag Kumar", "Alex Cullum", "Feng Han", "Konstantinos Bousmalis", "Diego Cedillo", "Grace Chu", "Vladimir Magay", "Paul Michel", "Ester Hlavnova", "Daniele Calandriello", "Setareh Ariafar", "Kaisheng Yao", "Vikash Sehwag", "Arpi Vezer", "Agustin Dal Lago", "Zhenkai Zhu", "Paul Kishan Rubenstein", "Allen Porter", "Anirudh Baddepudi", "Oriana Riva", "Mihai Dorin Istin", "Chih-Kuan Yeh", "Zhi Li", "Andrew Howard", "Nilpa Jha", "Jeremy Chen", "Raoul de Liedekerke", "Zafarali Ahmed", "Mikel Rodriguez", "Tanuj Bhatia", "Bangju Wang", "Ali Elqursh", "David Klinghoffer", "Peter Chen", "Pushmeet Kohli", "Te I", "Weiyang Zhang", "Zack Nado", "Jilin Chen", "Maxwell Chen", "George Zhang", "Aayush Singh", "Adam Hillier", "Federico Lebron", "Yiqing Tao", "Ting Liu", "Gabriel Dulac-Arnold", "Jingwei Zhang", "Shashi Narayan", "Buhuang Liu", "Orhan Firat", "Abhishek Bhowmick", "Bingyuan Liu", "Hao Zhang", "Zizhao Zhang", "Georges Rotival", "Nathan Howard", "Anu Sinha", "Alexander Grushetsky", "Benjamin Beyret", "Keerthana Gopalakrishnan", "James Zhao", "Kyle He", "Szabolcs Payrits", "Zaid Nabulsi", "Zhaoyi Zhang", "Weijie Chen", "Edward Lee", "Nova Fallen", "Sreenivas Gollapudi", "Aurick Zhou", "Filip Paveti", "Thomas Kppe", "Shiyu Huang", "Rama Pasumarthi", "Nick Fernando", "Felix Fischer", "Daria urko", "Yang Gao", "James Svensson", "Austin Stone", "Haroon Qureshi", "Abhishek Sinha", "Apoorv Kulshreshtha", "Martin Matysiak", "Jieming Mao", "Carl Saroufim", "Aleksandra Faust", "Qingnan Duan", "Gil Fidel", "Kaan Katircioglu", "Raphal Lopez Kaufman", "Dhruv Shah", "Weize Kong", "Abhishek Bapna", "Gellrt Weisz", "Emma Dunleavy", "Praneet Dutta", "Tianqi Liu", "Rahma Chaabouni", "Carolina Parada", "Marcus Wu", "Alexandra Belias", "Alessandro Bissacco", "Stanislav Fort", "Li Xiao", "Fantine Huot", "Chris Knutsen", "Yochai Blau", "Gang Li", "Jennifer Prendki", "Juliette Love", "Yinlam Chow", "Pichi Charoenpanit", "Hidetoshi Shimokawa", "Vincent Coriou", "Karol Gregor", "Tomas Izo", "Arjun Akula", "Mario Pinto", "Chris Hahn", "Dominik Paulus", "Jiaxian Guo", "Neha Sharma", "Cho-Jui Hsieh", "Adaeze Chukwuka", "Kazuma Hashimoto", "Nathalie Rauschmayr", "Ling Wu", "Christof Angermueller", "Yulong Wang", "Sebastian Gerlach", "Michael Pliskin", "Daniil Mirylenka", "Min Ma", "Lexi Baugher", "Bryan Gale", "Shaan Bijwadia", "Nemanja Rakievi", "David Wood", "Jane Park", "Chung-Ching Chang", "Babi Seal", "Chris Tar", "Kacper Krasowiak", "Yiwen Song", "Georgi Stephanov", "Gary Wang", "Marcello Maggioni", "Stein Xudong Lin", "Felix Wu", "Shachi Paul", "Zixuan Jiang", "Shubham Agrawal", "Bilal Piot", "Alex Feng", "Cheolmin Kim", "Tulsee Doshi", "Jonathan Lai", "Chuqiao", "Xu", "Sharad Vikram", "Ciprian Chelba", "Sebastian Krause", "Vincent Zhuang", "Jack Rae", "Timo Denk", "Adrian Collister", "Lotte Weerts", "Xianghong Luo", "Yifeng Lu", "Hvard Garnes", "Nitish Gupta", "Terry Spitz", "Avinatan Hassidim", "Lihao Liang", "Izhak Shafran", "Peter Humphreys", "Kenny Vassigh", "Phil Wallis", "Virat Shejwalkar", "Nicolas Perez-Nieves", "Rachel Hornung", "Melissa Tan", "Beka Westberg", "Andy Ly", "Richard Zhang", "Brian Farris", "Jongbin Park", "Alec Kosik", "Zeynep Cankara", "Andrii Maksai", "Yunhan Xu", "Albin Cassirer", "Sergi Caelles", "Abbas Abdolmaleki", "Mencher Chiang", "Alex Fabrikant", "Shravya Shetty", "Luheng He", "Mai Gimnez", "Hadi Hashemi", "Sheena Panthaplackel", "Yana Kulizhskaya", "Salil Deshmukh", "Daniele Pighin", "Robin Alazard", "Disha Jindal", "Seb Noury", "Pradeep Kumar S", "Siyang Qin", "Xerxes Dotiwalla", "Stephen Spencer", "Mohammad Babaeizadeh", "Blake JianHang Chen", "Vaibhav Mehta", "Jennie Lees", "Andrew Leach", "Penporn Koanantakool", "Ilia Akolzin", "Ramona Comanescu", "Junwhan Ahn", "Alexey Svyatkovskiy", "Basil Mustafa", "David D'Ambrosio", "Shiva Mohan Reddy Garlapati", "Pascal Lamblin", "Alekh Agarwal", "Shuang Song", "Pier Giuseppe Sessa", "Pauline Coquinot", "John Maggs", "Hussain Masoom", "Divya Pitta", "Yaqing Wang", "Patrick Morris-Suzuki", "Billy Porter", "Johnson Jia", "Jeffrey Dudek", "Raghavender R", "Cosmin Paduraru", "Alan Ansell", "Tolga Bolukbasi", "Tony Lu", "Ramya Ganeshan", "Zi Wang", "Henry Griffiths", "Rodrigo Benenson", "Yifan He", "James Swirhun", "George Papamakarios", "Aditya Chawla", "Kuntal Sengupta", "Yan Wang", "Vedrana Milutinovic", "Igor Mordatch", "Zhipeng Jia", "Jamie Smith", "Will Ng", "Shitij Nigam", "Matt Young", "Eugen Vuak", "Blake Hechtman", "Sheela Goenka", "Avital Zipori", "Kareem Ayoub", "Ashok Popat", "Trilok Acharya", "Luo Yu", "Dawn Bloxwich", "Hugo Song", "Paul Roit", "Haiqiong Li", "Aviel Boag", "Nigamaa Nayakanti", "Bilva Chandra", "Tianli Ding", "Aahil Mehta", "Cath Hope", "Jiageng Zhang", "Idan Heimlich Shtacher", "Kartikeya Badola", "Ryo Nakashima", "Andrei Sozanschi", "Iulia Coma", "Ante uul", "Emily Caveness", "Julian Odell", "Matthew Watson", "Dario de Cesare", "Phillip Lippe", "Derek Lockhart", "Siddharth Verma", "Huizhong Chen", "Sean Sun", "Lin Zhuo", "Aditya Shah", "Prakhar Gupta", "Alex Muzio", "Ning Niu", "Amir Zait", "Abhinav Singh", "Meenu Gaba", "Fan Ye", "Prajit Ramachandran", "Mohammad Saleh", "Raluca Ada Popa", "Ayush Dubey", "Frederick Liu", "Sara Javanmardi", "Mark Epstein", "Ross Hemsley", "Richard Green", "Nishant Ranka", "Eden Cohen", "Chuyuan Kelly Fu", "Sanjay Ghemawat", "Jed Borovik", "James Martens", "Anthony Chen", "Pranav Shyam", "Andr Susano Pinto", "Ming-Hsuan Yang", "Alexandru ifrea", "David Du", "Boqing Gong", "Ayushi Agarwal", "Seungyeon Kim", "Christian Frank", "Saloni Shah", "Xiaodan Song", "Zhiwei Deng", "Ales Mikhalap", "Kleopatra Chatziprimou", "Timothy Chung", "Toni Creswell", "Susan Zhang", "Yennie Jun", "Carl Lebsack", "Will Truong", "Slavica Andai", "Itay Yona", "Marco Fornoni", "Rong Rong", "Serge Toropov", "Afzal Shama Soudagar", "Andrew Audibert", "Salah Zaiem", "Zaheer Abbas", "Andrei Rusu", "Sahitya Potluri", "Shitao Weng", "Anastasios Kementsietsidis", "Anton Tsitsulin", "Daiyi Peng", "Natalie Ha", "Sanil Jain", "Tejasi Latkar", "Simeon Ivanov", "Cory McLean", "Anirudh GP", "Rajesh Venkataraman", "Canoee Liu", "Dilip Krishnan", "Joel D'sa", "Roey Yogev", "Paul Collins", "Benjamin Lee", "Lewis Ho", "Carl Doersch", "Gal Yona", "Shawn Gao", "Felipe Tiengo Ferreira", "Adnan Ozturel", "Hannah Muckenhirn", "Ce Zheng", "Gargi Balasubramaniam", "Mudit Bansal", "George van den Driessche", "Sivan Eiger", "Salem Haykal", "Vedant Misra", "Abhimanyu Goyal", "Danilo Martins", "Gary Leung", "Jonas Valfridsson", "Four Flynn", "Will Bishop", "Chenxi Pang", "Yoni Halpern", "Honglin Yu", "Lawrence Moore", "Yuvein", "Zhu", "Sridhar Thiagarajan", "Yoel Drori", "Zhisheng Xiao", "Lucio Dery", "Rolf Jagerman", "Jing Lu", "Eric Ge", "Vaibhav Aggarwal", "Arjun Khare", "Vinh Tran", "Oded Elyada", "Ferran Alet", "James Rubin", "Ian Chou", "David Tian", "Libin Bai", "Lawrence Chan", "Lukasz Lew", "Karolis Misiunas", "Taylan Bilal", "Aniket Ray", "Sindhu Raghuram", "Alex Castro-Ros", "Viral Carpenter", "CJ Zheng", "Michael Kilgore", "Josef Broder", "Emily Xue", "Praveen Kallakuri", "Dheeru Dua", "Nancy Yuen", "Steve Chien", "John Schultz", "Saurabh Agrawal", "Reut Tsarfaty", "Jingcao Hu", "Ajay Kannan", "Dror Marcus", "Nisarg Kothari", "Baochen Sun", "Ben Horn", "Matko Bonjak", "Ferjad Naeem", "Dean Hirsch", "Lewis Chiang", "Boya Fang", "Jie Han", "Qifei Wang", "Ben Hora", "Antoine He", "Mario Lui", "Beer Changpinyo", "Anshuman Tripathi", "John Youssef", "Chester Kwak", "Philippe Schlattner", "Cat Graves", "Rmi Leblond", "Wenjun Zeng", "Anders Andreassen", "Gabriel Rasskin", "Yue Song", "Eddie Cao", "Junhyuk Oh", "Matt Hoffman", "Wojtek Skut", "Yichi Zhang", "Jon Stritar", "Xingyu Cai", "Saarthak Khanna", "Kathie Wang", "Shriya Sharma", "Christian Reisswig", "Younghoon Jun", "Aman Prasad", "Tatiana Sholokhova", "Preeti Singh", "Adi Gerzi Rosenthal", "Anian Ruoss", "Franoise Beaufays", "Sean Kirmani", "Dongkai Chen", "Johan Schalkwyk", "Jonathan Herzig", "Been Kim", "Josh Jacob", "Damien Vincent", "Adrian N Reyes", "Ivana Balazevic", "Lonard Hussenot", "Jon Schneider", "Parker Barnes", "Luis Castro", "Spandana Raj Babbula", "Simon Green", "Serkan Cabi", "Nico Duduta", "Danny Driess", "Rich Galt", "Noam Velan", "Junjie Wang", "Hongyang Jiao", "Matthew Mauger", "Du Phan", "Miteyan Patel", "Vlado Gali", "Jerry Chang", "Eyal Marcus", "Matt Harvey", "Julian Salazar", "Elahe Dabir", "Suraj Satishkumar Sheth", "Amol Mandhane", "Hanie Sedghi", "Jeremiah Willcock", "Amir Zandieh", "Shruthi Prabhakara", "Aida Amini", "Antoine Miech", "Victor Stone", "Massimo Nicosia", "Paul Niemczyk", "Ying Xiao", "Lucy Kim", "Sawek Kwasiborski", "Vikas Verma", "Ada Maksutaj Oflazer", "Christoph Hirnschall", "Peter Sung", "Lu Liu", "Richard Everett", "Michiel Bakker", "goston Weisz", "Yufei Wang", "Vivek Sampathkumar", "Uri Shaham", "Bibo Xu", "Yasemin Altun", "Mingqiu Wang", "Takaaki Saeki", "Guanjie Chen", "Emanuel Taropa", "Shanthal Vasanth", "Sophia Austin", "Lu Huang", "Goran Petrovic", "Qingyun Dou", "Daniel Golovin", "Grigory Rozhdestvenskiy", "Allie Culp", "Will Wu", "Motoki Sano", "Divya Jain", "Julia Proskurnia", "Sbastien Cevey", "Alejandro Cruzado Ruiz", "Piyush Patil", "Mahdi Mirzazadeh", "Eric Ni", "Javier Snaider", "Lijie Fan", "Alexandre Frchette", "AJ Pierigiovanni", "Shariq Iqbal", "Kenton Lee", "Claudio Fantacci", "Jinwei Xing", "Lisa Wang", "Alex Irpan", "David Raposo", "Yi Luan", "Zhuoyuan Chen", "Harish Ganapathy", "Kevin Hui", "Jiazhong Nie", "Isabelle Guyon", "Heming Ge", "Roopali Vij", "Hui Zheng", "Dayeong Lee", "Alfonso Castao", "Khuslen Baatarsukh", "Gabriel Ibagon", "Alexandra Chronopoulou", "Nicholas FitzGerald", "Shashank Viswanadha", "Safeen Huda", "Rivka Moroshko", "Georgi Stoyanov", "Prateek Kolhar", "Alain Vaucher", "Ishaan Watts", "Adhi Kuncoro", "Henryk Michalewski", "Satish Kambala", "Bat-Orgil Batsaikhan", "Alek Andreev", "Irina Jurenka", "Maigo Le", "Qihang Chen", "Wael Al Jishi", "Sarah Chakera", "Zhe Chen", "Aditya Kini", "Vikas Yadav", "Aditya Siddhant", "Ilia Labzovsky", "Balaji Lakshminarayanan", "Carrie Grimes Bostock", "Pankil Botadra", "Ankesh Anand", "Colton Bishop", "Sam Conway-Rahman", "Mohit Agarwal", "Yani Donchev", "Achintya Singhal", "Flix de Chaumont Quitry", "Natalia Ponomareva", "Nishant Agrawal", "Bin Ni", "Kalpesh Krishna", "Masha Samsikova", "John Karro", "Yilun Du", "Tamara von Glehn", "Caden Lu", "Christopher A. Choquette-Choo", "Zhen Qin", "Tingnan Zhang", "Sicheng Li", "Divya Tyam", "Swaroop Mishra", "Wing Lowe", "Colin Ji", "Weiyi Wang", "Manaal Faruqui", "Ambrose Slone", "Valentin Dalibard", "Arunachalam Narayanaswamy", "John Lambert", "Pierre-Antoine Manzagol", "Dan Karliner", "Andrew Bolt", "Ivan Lobov", "Aditya Kusupati", "Chang Ye", "Xuan Yang", "Heiga Zen", "Nelson George", "Mukul Bhutani", "Olivier Lacombe", "Robert Riachi", "Gagan Bansal", "Rachel Soh", "Yue Gao", "Yang Yu", "Adams Yu", "Emily Nottage", "Tania Rojas-Esponda", "James Noraky", "Manish Gupta", "Ragha Kotikalapudi", "Jichuan Chang", "Sanja Deur", "Dan Graur", "Alex Mossin", "Erin Farnese", "Ricardo Figueira", "Alexandre Moufarek", "Austin Huang", "Patrik Zochbauer", "Ben Ingram", "Tongzhou Chen", "Zelin Wu", "Adri Puigdomnech", "Leland Rechis", "Da Yu", "Sri Gayatri Sundara Padmanabhan", "Rui Zhu", "Chu-ling Ko", "Andrea Banino", "Samira Daruki", "Aarush Selvan", "Dhruva Bhaswar", "Daniel Hernandez Diaz", "Chen Su", "Salvatore Scellato", "Jennifer Brennan", "Woohyun Han", "Grace Chung", "Priyanka Agrawal", "Urvashi Khandelwal", "Khe Chai Sim", "Morgane Lustman", "Sam Ritter", "Kelvin Guu", "Jiawei Xia", "Prateek Jain", "Emma Wang", "Tyrone Hill", "Mirko Rossini", "Marija Kostelac", "Tautvydas Misiunas", "Amit Sabne", "Kyuyeun Kim", "Ahmet Iscen", "Congchao Wang", "Jos Leal", "Ashwin Sreevatsa", "Utku Evci", "Manfred Warmuth", "Saket Joshi", "Daniel Suo", "James Lottes", "Garrett Honke", "Brendan Jou", "Stefani Karp", "Jieru Hu", "Himanshu Sahni", "Adrien Ali Taga", "William Kong", "Samrat Ghosh", "Renshen Wang", "Jay Pavagadhi", "Natalie Axelsson", "Nikolai Grigorev", "Patrick Siegler", "Rebecca Lin", "Guohui Wang", "Emilio Parisotto", "Sharath Maddineni", "Krishan Subudhi", "Eyal Ben-David", "Elena Pochernina", "Orgad Keller", "Thi Avrahami", "Zhe Yuan", "Pulkit Mehta", "Jialu Liu", "Sherry Yang", "Wendy Kan", "Katherine Lee", "Tom Funkhouser", "Derek Cheng", "Hongzhi Shi", "Archit Sharma", "Joe Kelley", "Matan Eyal", "Yury Malkov", "Corentin Tallec", "Yuval Bahat", "Shen Yan", "Xintian", "Wu", "David Lindner", "Chengda Wu", "Avi Caciularu", "Xiyang Luo", "Rodolphe Jenatton", "Tim Zaman", "Yingying Bi", "Ilya Kornakov", "Ganesh Mallya", "Daisuke Ikeda", "Itay Karo", "Anima Singh", "Colin Evans", "Praneeth Netrapalli", "Vincent Nallatamby", "Isaac Tian", "Yannis Assael", "Vikas Raunak", "Victor Carbune", "Ioana Bica", "Lior Madmoni", "Dee Cattle", "Snchit Grover", "Krishna Somandepalli", "Sid Lall", "Amelio Vzquez-Reina", "Riccardo Patana", "Jiaqi Mu", "Pranav Talluri", "Maggie Tran", "Rajeev Aggarwal", "RJ Skerry-Ryan", "Jun Xu", "Mike Burrows", "Xiaoyue Pan", "Edouard Yvinec", "Di Lu", "Zhiying Zhang", "Duc Dung Nguyen", "Hairong Mu", "Gabriel Barcik", "Helen Ran", "Lauren Beltrone", "Krzysztof Choromanski", "Dia Kharrat", "Samuel Albanie", "Sean Purser-haskell", "David Bieber", "Carrie Zhang", "Jing Wang", "Tom Hudson", "Zhiyuan Zhang", "Han Fu", "Johannes Mauerer", "Mohammad Hossein Bateni", "AJ Maschinot", "Bing Wang", "Muye Zhu", "Arjun Pillai", "Tobias Weyand", "Shuang Liu", "Oscar Akerlund", "Fred Bertsch", "Vittal Premachandran", "Alicia Jin", "Vincent Roulet", "Peter de Boursac", "Shubham Mittal", "Ndaba Ndebele", "Georgi Karadzhov", "Sahra Ghalebikesabi", "Ricky Liang", "Allen Wu", "Yale Cong", "Nimesh Ghelani", "Sumeet Singh", "Bahar Fatemi", "Warren", "Chen", "Charles Kwong", "Alexey Kolganov", "Steve Li", "Richard Song", "Chenkai Kuang", "Sobhan Miryoosefi", "Dale Webster", "James Wendt", "Arkadiusz Socala", "Guolong Su", "Artur Mendona", "Abhinav Gupta", "Xiaowei Li", "Tomy Tsai", "Qiong", "Hu", "Kai Kang", "Angie Chen", "Sertan Girgin", "Yongqin Xian", "Andrew Lee", "Nolan Ramsden", "Leslie Baker", "Madeleine Clare Elish", "Varvara Krayvanova", "Rishabh Joshi", "Jiri Simsa", "Yao-Yuan Yang", "Piotr Ambroszczyk", "Dipankar Ghosh", "Arjun Kar", "Yuan Shangguan", "Yumeya Yamamori", "Yaroslav Akulov", "Andy Brock", "Haotian Tang", "Siddharth Vashishtha", "Rich Munoz", "Andreas Steiner", "Kalyan Andra", "Daniel Eppens", "Qixuan Feng", "Hayato Kobayashi", "Sasha Goldshtein", "Mona El Mahdy", "Xin Wang", "Jilei", "Wang", "Richard Killam", "Tom Kwiatkowski", "Kavya Kopparapu", "Serena Zhan", "Chao Jia", "Alexei Bendebury", "Sheryl Luo", "Adri Recasens", "Timothy Knight", "Jing Chen", "Mohak Patel", "YaGuang Li", "Ben Withbroe", "Dean Weesner", "Kush Bhatia", "Jie Ren", "Danielle Eisenbud", "Ebrahim Songhori", "Yanhua Sun", "Travis Choma", "Tasos Kementsietsidis", "Lucas Manning", "Brian Roark", "Wael Farhan", "Jie Feng", "Susheel Tatineni", "James Cobon-Kerr", "Yunjie Li", "Lisa Anne Hendricks", "Isaac Noble", "Chris Breaux", "Nate Kushman", "Liqian Peng", "Fuzhao Xue", "Taylor Tobin", "Jamie Rogers", "Josh Lipschultz", "Chris Alberti", "Alexey Vlaskin", "Mostafa Dehghani", "Roshan Sharma", "Tris Warkentin", "Chen-Yu Lee", "Benigno Uria", "Da-Cheng Juan", "Angad Chandorkar", "Hila Sheftel", "Ruibo Liu", "Elnaz Davoodi", "Borja De Balle Pigem", "Kedar Dhamdhere", "David Ross", "Jonathan Hoech", "Mahdis Mahdieh", "Li Liu", "Qiujia Li", "Liam McCafferty", "Chenxi Liu", "Markus Mircea", "Yunting Song", "Omkar Savant", "Alaa Saade", "Colin Cherry", "Vincent Hellendoorn", "Siddharth Goyal", "Paul Pucciarelli", "David Vilar Torres", "Zohar Yahav", "Hyo Lee", "Lars Lowe Sjoesund", "Christo Kirov", "Bo Chang", "Deepanway Ghoshal", "Lu Li", "Gilles Baechler", "Sbastien Pereira", "Tara Sainath", "Anudhyan Boral", "Dominik Grewe", "Afief Halumi", "Nguyet Minh Phu", "Tianxiao Shen", "Marco Tulio Ribeiro", "Dhriti Varma", "Alex Kaskasoli", "Vlad Feinberg", "Navneet Potti", "Jarrod Kahn", "Matheus Wisniewski", "Shakir Mohamed", "Arnar Mar Hrafnkelsson", "Bobak Shahriari", "Jean-Baptiste Lespiau", "Lisa Patel", "Legg Yeung", "Tom Paine", "Lantao Mei", "Alex Ramirez", "Rakesh Shivanna", "Li Zhong", "Josh Woodward", "Guilherme Tubone", "Samira Khan", "Heng Chen", "Elizabeth Nielsen", "Catalin Ionescu", "Utsav Prabhu", "Mingcen Gao", "Qingze Wang", "Sean Augenstein", "Neesha Subramaniam", "Jason Chang", "Fotis Iliopoulos", "Jiaming Luo", "Myriam Khan", "Weicheng Kuo", "Denis Teplyashin", "Florence Perot", "Logan Kilpatrick", "Amir Globerson", "Hongkun Yu", "Anfal Siddiqui", "Nick Sukhanov", "Arun Kandoor", "Umang Gupta", "Marco Andreetto", "Moran Ambar", "Donnie Kim", "Pawe Wesoowski", "Sarah Perrin", "Ben Limonchik", "Wei Fan", "Jim Stephan", "Ian Stewart-Binks", "Ryan Kappedal", "Tong He", "Sarah Cogan", "Romina Datta", "Tong Zhou", "Jiayu Ye", "Leandro Kieliger", "Ana Ramalho", "Kyle Kastner", "Fabian Mentzer", "Wei-Jen Ko", "Arun Suggala", "Tianhao Zhou", "Shiraz Butt", "Hana Strejek", "Lior Belenki", "Subhashini Venugopalan", "Mingyang Ling", "Evgenii Eltyshev", "Yunxiao Deng", "Geza Kovacs", "Mukund Raghavachari", "Hanjun Dai", "Tal Schuster", "Steven Schwarcz", "Richard Nguyen", "Arthur Nguyen", "Gavin Buttimore", "Shrestha Basu Mallick", "Sudeep Gandhe", "Seth Benjamin", "Michal Jastrzebski", "Le Yan", "Sugato Basu", "Chris Apps", "Isabel Edkins", "James Allingham", "Immanuel Odisho", "Tomas Kocisky", "Jewel Zhao", "Linting Xue", "Apoorv Reddy", "Chrysovalantis Anastasiou", "Aviel Atias", "Sam Redmond", "Kieran Milan", "Nicolas Heess", "Herman Schmit", "Allan Dafoe", "Daniel Andor", "Tynan Gangwani", "Anca Dragan", "Sheng Zhang", "Ashyana Kachra", "Gang Wu", "Siyang Xue", "Kevin Aydin", "Siqi Liu", "Yuxiang Zhou", "Mahan Malihi", "Austin Wu", "Siddharth Gopal", "Candice Schumann", "Peter Stys", "Alek Wang", "Mirek Olk", "Dangyi Liu", "Christian Schallhart", "Yiran Mao", "Demetra Brady", "Hao Xu", "Tomas Mery", "Chawin Sitawarin", "Siva Velusamy", "Tom Cobley", "Alex Zhai", "Christian Walder", "Nitzan Katz", "Ganesh Jawahar", "Chinmay Kulkarni", "Antoine Yang", "Adam Paszke", "Yinan Wang", "Bogdan Damoc", "Zaln Borsos", "Ray Smith", "Jinning Li", "Mansi Gupta", "Andrei Kapishnikov", "Sushant Prakash", "Florian Luisier", "Rishabh Agarwal", "Will Grathwohl", "Kuangyuan Chen", "Kehang Han", "Nikhil Mehta", "Andrew Over", "Shekoofeh Azizi", "Lei Meng", "Niccol Dal Santo", "Kelvin Zheng", "Jane Shapiro", "Igor Petrovski", "Jeffrey Hui", "Amin Ghafouri", "Jasper Snoek", "James Qin", "Mandy Jordan", "Caitlin Sikora", "Jonathan Malmaud", "Yuheng Kuang", "Aga wietlik", "Ruoxin Sang", "Chongyang Shi", "Leon Li", "Andrew Rosenberg", "Shubin Zhao", "Andy Crawford", "Jan-Thorsten Peter", "Yun Lei", "Xavier Garcia", "Long Le", "Todd Wang", "Julien Amelot", "Dave Orr", "Praneeth Kacham", "Dana Alon", "Gladys Tyen", "Abhinav Arora", "James Lyon", "Alex Kurakin", "Mimi Ly", "Theo Guidroz", "Zhipeng Yan", "Rina Panigrahy", "Pingmei Xu", "Thais Kagohara", "Yong Cheng", "Eric Noland", "Jinhyuk Lee", "Jonathan Lee", "Cathy Yip", "Maria Wang", "Efrat Nehoran", "Alexander Bykovsky", "Zhihao Shan", "Ankit Bhagatwala", "Chaochao Yan", "Jie Tan", "Guillermo Garrido", "Dan Ethier", "Nate Hurley", "Grace Vesom", "Xu Chen", "Siyuan Qiao", "Abhishek Nayyar", "Julian Walker", "Paramjit Sandhu", "Mihaela Rosca", "Danny Swisher", "Mikhail Dektiarev", "Josh Dillon", "George-Cristian Muraru", "Manuel Tragut", "Artiom Myaskovsky", "David Reid", "Marko Velic", "Owen Xiao", "Jasmine George", "Mark Brand", "Jing Li", "Wenhao Yu", "Shane Gu", "Xiang Deng", "Franois-Xavier Aubet", "Soheil Hassas Yeganeh", "Fred Alcober", "Celine Smith", "Trevor Cohn", "Kay McKinney", "Michael Tschannen", "Ramesh Sampath", "Gowoon Cheon", "Liangchen Luo", "Luyang Liu", "Jordi Orbay", "Hui Peng", "Gabriela Botea", "Xiaofan Zhang", "Charles Yoon", "Cesar Magalhaes", "Pawe Stradomski", "Ian Mackinnon", "Steven Hemingray", "Kumaran Venkatesan", "Rhys May", "Jaeyoun Kim", "Alex Druinsky", "Jingchen Ye", "Zheng Xu", "Terry Huang", "Jad Al Abdallah", "Adil Dostmohamed", "Rachana Fellinger", "Tsendsuren Munkhdalai", "Akanksha Maurya", "Peter Garst", "Yin Zhang", "Maxim Krikun", "Simon Bucher", "Aditya Srikanth Veerubhotla", "Yaxin Liu", "Sheng Li", "Nishesh Gupta", "Jakub Adamek", "Hanwen Chen", "Bernett Orlando", "Aleksandr Zaks", "Joost van Amersfoort", "Josh Camp", "Hui Wan", "HyunJeong Choe", "Zhichun Wu", "Kate Olszewska", "Weiren Yu", "Archita Vadali", "Martin Scholz", "Daniel De Freitas", "Jason Lin", "Amy Hua", "Xin Liu", "Frank Ding", "Yichao Zhou", "Boone Severson", "Katerina Tsihlas", "Samuel Yang", "Tammo Spalink", "Varun Yerram", "Helena Pankov", "Rory Blevins", "Ben Vargas", "Sarthak Jauhari", "Matt Miecnikowski", "Ming Zhang", "Sandeep Kumar", "Clement Farabet", "Charline Le Lan", "Sebastian Flennerhag", "Yonatan Bitton", "Ada Ma", "Arthur Brainskas", "Eli Collins", "Niharika Ahuja", "Sneha Kudugunta", "Anna Bortsova", "Minh Giang", "Wanzheng Zhu", "Ed Chi", "Scott Lundberg", "Alexey Stern", "Subha Puttagunta", "Jing Xiong", "Xiao Wu", "Yash Pande", "Amit Jhindal", "Daniel Murphy", "Jon Clark", "Marc Brockschmidt", "Maxine Deines", "Kevin R. McKee", "Dan Bahir", "Jiajun Shen", "Minh Truong", "Daniel McDuff", "Andrea Gesmundo", "Edouard Rosseel", "Bowen Liang", "Ken Caluwaerts", "Jessica Hamrick", "Joseph Kready", "Mary Cassin", "Rishikesh Ingale", "Li Lao", "Scott Pollom", "Yifan Ding", "Wei He", "Lizzetth Bellot", "Joana Iljazi", "Ramya Sree Boppana", "Shan Han", "Tara Thompson", "Amr Khalifa", "Anna Bulanova", "Blagoj Mitrevski", "Bo Pang", "Emma Cooney", "Tian Shi", "Rey Coaguila", "Tamar Yakar", "Marc'aurelio Ranzato", "Nikola Momchev", "Chris Rawles", "Zachary Charles", "Young Maeng", "Yuan Zhang", "Rishabh Bansal", "Xiaokai Zhao", "Brian Albert", "Yuan Yuan", "Sudheendra Vijayanarasimhan", "Roy Hirsch", "Vinay Ramasesh", "Kiran Vodrahalli", "Xingyu Wang", "Arushi Gupta", "DJ Strouse", "Jianmo Ni", "Roma Patel", "Gabe Taubman", "Zhouyuan Huo", "Dero Gharibian", "Marianne Monteiro", "Hoi Lam", "Shobha Vasudevan", "Aditi Chaudhary", "Isabela Albuquerque", "Kilol Gupta", "Sebastian Riedel", "Chaitra Hegde", "Avraham Ruderman", "Andrs Gyrgy", "Marcus Wainwright", "Ashwin Chaugule", "Burcu Karagol Ayan", "Tomer Levinboim", "Sam Shleifer", "Yogesh Kalley", "Vahab Mirrokni", "Abhishek Rao", "Prabakar Radhakrishnan", "Jay Hartford", "Jialin Wu", "Zhenhai Zhu", "Francesco Bertolini", "Hao Xiong", "Nicolas Serrano", "Hamish Tomlinson", "Myle Ott", "Yifan Chang", "Mark Graham", "Jian Li", "Marco Liang", "Xiangzhu Long", "Sebastian Borgeaud", "Yanif Ahmad", "Alex Grills", "Diana Mincu", "Martin Izzard", "Yuan Liu", "Jinyu Xie", "Louis O'Bryan", "Sameera Ponda", "Simon Tong", "Michelle Liu", "Dan Malkin", "Khalid Salama", "Yuankai Chen", "Rohan Anil", "Anand Rao", "Rigel Swavely", "Misha Bilenko", "Nina Anderson", "Tat Tan", "Jing Xie", "Xing Wu", "Lijun Yu", "Oriol Vinyals", "Andrey Ryabtsev", "Rumen Dangovski", "Kate Baumli", "Daniel Keysers", "Christian Wright", "Zoe Ashwood", "Betty Chan", "Artem Shtefan", "Yaohui Guo", "Ankur Bapna", "Radu Soricut", "Steven Pecht", "Sabela Ramos", "Rui Wang", "Jiahao Cai", "Trieu Trinh", "Paul Barham", "Linda Friso", "Eli Stickgold", "Xiangzhuo Ding", "Siamak Shakeri", "Diego Ardila", "Eleftheria Briakou", "Phil Culliton", "Adam Raveret", "Jingyu Cui", "David Saxton", "Subhrajit Roy", "Javad Azizi", "Pengcheng Yin", "Lucia Loher", "Andrew Bunner", "Min Choi", "Faruk Ahmed", "Eric Li", "Yin Li", "Shengyang Dai", "Michael Elabd", "Sriram Ganapathy", "Shivani Agrawal", "Yiqing Hua", "Paige Kunkle", "Sujeevan Rajayogam", "Arun Ahuja", "Arthur Conmy", "Alex Vasiloff", "Parker Beak", "Christopher Yew", "Jayaram Mudigonda", "Bartek Wydrowski", "Jon Blanton", "Zhengdong Wang", "Yann Dauphin", "Zhuo Xu", "Martin Polacek", "Xi Chen", "Hexiang Hu", "Pauline Sho", "Markus Kunesch", "Mehdi Hafezi Manshadi", "Eliza Rutherford", "Bo Li", "Sissie Hsiao", "Iain Barr", "Alex Tudor", "Matija Kecman", "Arsha Nagrani", "Vladimir Pchelin", "Martin Sundermeyer", "Aishwarya P S", "Abhijit Karmarkar", "Yi Gao", "Grishma Chole", "Olivier Bachem", "Isabel Gao", "Arturo BC", "Matt Dibb", "Mauro Verzetti", "Felix Hernandez-Campos", "Yana Lunts", "Matthew Johnson", "Julia Di Trapani", "Raphael Koster", "Idan Brusilovsky", "Binbin Xiong", "Megha Mohabey", "Han Ke", "Joe Zou", "Tea Saboli", "Vctor Campos", "John Palowitch", "Alex Morris", "Linhai Qiu", "Pranavaraj Ponnuramu", "Fangtao Li", "Vivek Sharma", "Kiranbir Sodhia", "Kaan Tekelioglu", "Aleksandr Chuklin", "Madhavi Yenugula", "Erika Gemzer", "Theofilos Strinopoulos", "Sam El-Husseini", "Huiyu Wang", "Yan Zhong", "Edouard Leurent", "Paul Natsev", "Weijun Wang", "Dre Mahaarachchi", "Tao Zhu", "Songyou Peng", "Sami Alabed", "Cheng-Chun Lee", "Anthony Brohan", "Arthur Szlam", "GS Oh", "Anton Kovsharov", "Jenny Lee", "Renee Wong", "Megan Barnes", "Gregory Thornton", "Felix Gimeno", "Omer Levy", "Martin Sevenich", "Melvin Johnson", "Jonathan Mallinson", "Robert Dadashi", "Ziyue Wang", "Qingchun Ren", "Preethi Lahoti", "Arka Dhar", "Josh Feldman", "Dan Zheng", "Thatcher Ulrich", "Liviu Panait", "Michiel Blokzijl", "Cip Baetu", "Josip Matak", "Jitendra Harlalka", "Maulik Shah", "Tal Marian", "Daniel von Dincklage", "Cosmo Du", "Ruy Ley-Wild", "Bethanie Brownfield", "Max Schumacher", "Yury Stuken", "Shadi Noghabi", "Sonal Gupta", "Xiaoqi Ren", "Eric Malmi", "Felix Weissenberger", "Blanca Huergo", "Maria Bauza", "Thomas Lampe", "Arthur Douillard", "Mojtaba Seyedhosseini", "Roy Frostig", "Zoubin Ghahramani", "Kelvin Nguyen", "Kashyap Krishnakumar", "Chengxi Ye", "Rahul Gupta", "Alireza Nazari", "Robert Geirhos", "Pete Shaw", "Ahmed Eleryan", "Dima Damen", "Jennimaria Palomaki", "Ted Xiao", "Qiyin Wu", "Quan Yuan", "Phoenix Meadowlark", "Matthew Bilotti", "Raymond Lin", "Mukund Sridhar", "Yannick Schroecker", "Da-Woon Chung", "Jincheng Luo", "Trevor Strohman", "Tianlin Liu", "Anne Zheng", "Jesse Emond", "Wei Wang", "Andrew Lampinen", "Toshiyuki Fukuzawa", "Folawiyo Campbell-Ajala", "Monica Roy", "James Lee-Thorp", "Lily Wang", "Iftekhar Naim", "Tony", "Nguy\\~n", "Guy Bensky", "Aditya Gupta", "Dominika Rogoziska", "Justin Fu", "Thanumalayan Sankaranarayana Pillai", "Petar Velikovi", "Shahar Drath", "Philipp Neubeck", "Vaibhav Tulsyan", "Arseniy Klimovskiy", "Don Metzler", "Sage Stevens", "Angel Yeh", "Junwei Yuan", "Tianhe Yu", "Kelvin Zhang", "Alec Go", "Vincent Tsang", "Ying Xu", "Andy Wan", "Isaac Galatzer-Levy", "Sam Sobell", "Abodunrinwa Toki", "Elizabeth Salesky", "Wenlei Zhou", "Diego Antognini", "Sholto Douglas", "Shimu Wu", "Adam Lelkes", "Frank Kim", "Paul Cavallaro", "Ana Salazar", "Yuchi Liu", "James Besley", "Tiziana Refice", "Yiling Jia", "Zhang Li", "Michal Sokolik", "Arvind Kannan", "Jon Simon", "Jo Chick", "Avia Aharon", "Meet Gandhi", "Mayank Daswani", "Keyvan Amiri", "Vighnesh Birodkar", "Abe Ittycheriah", "Peter Grabowski", "Oscar Chang", "Charles Sutton", "Zhixin", "Lai", "Umesh Telang", "Susie Sargsyan", "Tao Jiang", "Raphael Hoffmann", "Nicole Brichtova", "Matteo Hessel", "Jonathan Halcrow", "Sammy Jerome", "Geoff Brown", "Alex Tomala", "Elena Buchatskaya", "Dian Yu", "Sachit Menon", "Pol Moreno", "Yuguo Liao", "Vicky Zayats", "Luming Tang", "SQ Mah", "Ashish Shenoy", "Alex Siegman", "Majid Hadian", "Okwan Kwon", "Tao Tu", "Nima Khajehnouri", "Ryan Foley", "Parisa Haghani", "Zhongru Wu", "Vaishakh Keshava", "Khyatti Gupta", "Tony Bruguier", "Rui Yao", "Danny Karmon", "Luisa Zintgraf", "Zhicheng Wang", "Enrique Piqueras", "Junehyuk Jung", "Jenny Brennan", "Diego Machado", "Marissa Giustina", "MH Tessler", "Kamyu Lee", "Qiao Zhang", "Joss Moore", "Kaspar Daugaard", "Alexander Frmmgen", "Jennifer Beattie", "Fred Zhang", "Daniel Kasenberg", "Ty Geri", "Danfeng Qin", "Gaurav Singh Tomar", "Tom Ouyang", "Tianli Yu", "Luowei Zhou", "Rajiv Mathews", "Andy Davis", "Yaoyiran Li", "Jai Gupta", "Damion Yates", "Linda Deng", "Elizabeth Kemp", "Ga-Young Joung", "Sergei Vassilvitskii", "Mandy Guo", "Pallavi LV", "Dave Dopson", "Sami Lachgar", "Lara McConnaughey", "Himadri Choudhury", "Dragos Dena", "Aaron Cohen", "Joshua Ainslie", "Sergey Levi", "Parthasarathy Gopavarapu", "Polina Zablotskaia", "Hugo Vallet", "Sanaz Bahargam", "Xiaodan Tang", "Nenad Tomasev", "Ethan Dyer", "Daniel Balle", "Hongrae Lee", "William Bono", "Jorge Gonzalez Mendez", "Vadim Zubov", "Shentao Yang", "Ivor Rendulic", "Yanyan Zheng", "Andrew Hogue", "Golan Pundak", "Ralph Leith", "Avishkar Bhoopchand", "Michael Han", "Mislav ani", "Tom Schaul", "Manolis Delakis", "Tejas Iyer", "Guanyu Wang", "Harman Singh", "Abdelrahman Abdelhamed", "Tara Thomas", "Siddhartha Brahma", "Hilal Dib", "Naveen Kumar", "Wenxuan Zhou", "Liang Bai", "Pushkar Mishra", "Jiao Sun", "Valentin Anklin", "Roykrong Sukkerd", "Lauren Agubuzu", "Anton Briukhov", "Anmol Gulati", "Maximilian Sieb", "Fabio Pardo", "Sara Nasso", "Junquan Chen", "Kexin Zhu", "Tiberiu Sosea", "Alex Goldin", "Keith Rush", "Spurthi Amba Hombaiah", "Andreas Noever", "Allan Zhou", "Sam Haves", "Mary Phuong", "Jake Ades", "Yi-ting Chen", "Lin Yang", "Joseph Pagadora", "Stan Bileschi", "Victor Cotruta", "Rachel Saputro", "Arijit Pramanik", "Sean Ammirati", "Dan Garrette", "Kevin Villela", "Tim Blyth", "Canfer Akbulut", "Neha Jha", "Alban Rrustemi", "Arissa Wongpanich", "Chirag Nagpal", "Yonghui Wu", "Morgane Rivire", "Sergey Kishchenko", "Pranesh Srinivasan", "Alice Chen", "Animesh Sinha", "Trang Pham", "Bill Jia", "Tom Hennigan", "Anton Bakalov", "Nithya Attaluri", "Drew Garmon", "Daniel Rodriguez", "Dawid Wegner", "Wenhao Jia", "Evan Senter", "Noah Fiedel", "Denis Petek", "Yuchuan Liu", "Cassidy Hardin", "Harshal Tushar Lehri", "Joao Carreira", "Sara Smoot", "Marcel Prasetya", "Nami Akazawa", "Anca Stefanoiu", "Chia-Hua Ho", "Anelia Angelova", "Kate Lin", "Min Kim", "Charles Chen", "Marcin Sieniek", "Alice Li", "Tongfei Guo", "Sorin Baltateanu", "Pouya Tafti", "Michael Wunder", "Nadav Olmert", "Divyansh Shukla", "Jingwei Shen", "Neel Kovelamudi", "Balaji Venkatraman", "Seth Neel", "Romal Thoppilan", "Jerome Connor", "Frederik Benzing", "Axel Stjerngren", "Golnaz Ghiasi", "Alex Polozov", "Joshua Howland", "Theophane Weber", "Justin Chiu", "Ganesh Poomal Girirajan", "Andreas Terzis", "Pidong Wang", "Fangda Li", "Yoav Ben Shalom", "Dinesh Tewari", "Matthew Denton", "Roee Aharoni", "Norbert Kalb", "Heri Zhao", "Junlin Zhang", "Angelos Filos", "Matthew Rahtz", "Lalit Jain", "Connie Fan", "Vitor Rodrigues", "Ruth Wang", "Richard Shin", "Jacob Austin", "Roman Ring", "Mariella Sanchez-Vargas", "Mehadi Hassen", "Ido Kessler", "Uri Alon", "Gufeng Zhang", "Wenhu Chen", "Yenai Ma", "Xiance Si", "Le Hou", "Azalia Mirhoseini", "Marc Wilson", "Geoff Bacon", "Becca Roelofs", "Lei Shu", "Gautam Vasudevan", "Jonas Adler", "Artur Dwornik", "Tayfun Terzi", "Matt Lawlor", "Harry Askham", "Mike Bernico", "Xuanyi Dong", "Chris Hidey", "Kevin Kilgour", "Gal Liu", "Surya Bhupatiraju", "Luke Leonhard", "Siqi Zuo", "Partha Talukdar", "Qing Wei", "Aliaksei Severyn", "Vt Listk", "Jong Lee", "Aditya Tripathi", "SK Park", "Yossi Matias", "Hao Liu", "Alex Ruiz", "Rajesh Jayaram", "Jackson Tolins", "Pierre Marcenac", "Yiming Wang", "Bryan Seybold", "Henry Prior", "Deepak Sharma", "Jack Weber", "Mikhail Sirotenko", "Yunhsuan Sung", "Dayou Du", "Ellie Pavlick", "Stefan Zinke", "Markus Freitag", "Max Dylla", "Montse Gonzalez Arenas", "Natan Potikha", "Omer Goldman", "Connie Tao", "Rachita Chhaparia", "Maria Voitovich", "Pawan Dogra", "Andrija Ranatovi", "Zak Tsai", "Chong You", "Oleaser Johnson", "George Tucker", "Chenjie Gu", "Jae Yoo", "Maryam Majzoubi", "Valentin Gabeur", "Bahram Raad", "Rocky Rhodes", "Kashyap Kolipaka", "Heidi Howard", "Geta Sampemane", "Benny Li", "Chulayuth Asawaroengchai", "Duy Nguyen", "Chiyuan Zhang", "Timothee Cour", "Xinxin Yu", "Zhao Fu", "Joe Jiang", "Po-Sen Huang", "Gabriela Surita", "Iaki Iturrate", "Yael Karov", "Michael Collins", "Martin Baeuml", "Fabian Fuchs", "Shilpa Shetty", "Swaroop Ramaswamy", "Sayna Ebrahimi", "Qiuchen Guo", "Jeremy Shar", "Gabe Barth-Maron", "Sravanti Addepalli", "Bryan Richter", "Chin-Yi Cheng", "Eugnie Rives", "Fei Zheng", "Johannes Griesser", "Nishanth Dikkala", "Yoel Zeldes", "Ilkin Safarli", "Dipanjan Das", "Himanshu Srivastava", "Sadh MNM Khan", "Xin Li", "Aditya Pandey", "Larisa Markeeva", "Dan Belov", "Qiqi Yan", "Mikoaj Rybiski", "Tao Chen", "Megha Nawhal", "Michael Quinn", "Vineetha Govindaraj", "Sarah York", "Reed Roberts", "Roopal Garg", "Namrata Godbole", "Jake Abernethy", "Anil Das", "Lam Nguyen Thiet", "Jonathan Tompson", "John Nham", "Neera Vats", "Ben Caine", "Wesley Helmholz", "Francesco Pongetti", "Yeongil Ko", "James An", "Clara Huiyi Hu", "Yu-Cheng Ling", "Julia Pawar", "Robert Leland", "Keisuke Kinoshita", "Waleed Khawaja", "Marco Selvi", "Eugene Ie", "Danila Sinopalnikov", "Lev Proleev", "Nilesh Tripuraneni", "Michele Bevilacqua", "Seungji Lee", "Clayton Sanford", "Dan Suh", "Dustin Tran", "Jeff Dean", "Simon Baumgartner", "Jens Heitkaemper", "Sagar Gubbi", "Kristina Toutanova", "Yichong Xu", "Chandu Thekkath", "Keran Rong", "Palak Jain", "Annie Xie", "Yan Virin", "Yang Li", "Lubo Litchev", "Richard Powell", "Tarun Bharti", "Adam Kraft", "Nan Hua", "Marissa Ikonomidis", "Ayal Hitron", "Sanjiv Kumar", "Loic Matthey", "Sophie Bridgers", "Lauren Lax", "Ishaan Malhi", "Ondrej Skopek", "Ashish Gupta", "Jiawei Cao", "Mitchelle Rasquinha", "Siim Pder", "Wojciech Stokowiec", "Nicholas Roth", "Guowang Li", "Michal Sander", "Joshua Kessinger", "Vihan Jain", "Edward Loper", "Wonpyo Park", "Michal Yarom", "Liqun Cheng", "Guru Guruganesh", "Kanishka Rao", "Yan Li", "Catarina Barros", "Mikhail Sushkov", "Chun-Sung Ferng", "Rohin Shah", "Ophir Aharoni", "Ravin Kumar", "Tim McConnell", "Peiran Li", "Chen Wang", "Fernando Pereira", "Craig Swanson", "Fayaz Jamil", "Yan Xiong", "Anitha Vijayakumar", "Prakash Shroff", "Kedar Soparkar", "Jindong Gu", "Livio Baldini Soares", "Eric Wang", "Kushal Majmundar", "Aurora Wei", "Kai Bailey", "Nora Kassner", "Chizu Kawamoto", "Goran ui", "Victor Gomes", "Abhirut Gupta", "Michael Guzman", "Ishita Dasgupta", "Xinyi Bai", "Zhufeng Pan", "Francesco Piccinno", "Hadas Natalie Vogel", "Octavio Ponce", "Adrian Hutter", "Paul Chang", "Pan-Pan Jiang", "Ionel Gog", "Vlad Ionescu", "James Manyika", "Fabian Pedregosa", "Harry Ragan", "Zach Behrman", "Ryan Mullins", "Coline Devin", "Aroonalok Pyne", "Swapnil Gawde", "Martin Chadwick", "Yiming Gu", "Sasan Tavakkol", "Andy Twigg", "Naman Goyal", "Ndidi Elue", "Anna Goldie", "Srinivasan Venkatachary", "Hongliang Fei", "Ziqiang Feng", "Marvin Ritter", "Isabel Leal", "Sudeep Dasari", "Pei Sun", "Alif Raditya Rochman", "Brendan O'Donoghue", "Yuchen Liu", "Jim Sproch", "Kai Chen", "Natalie Clay", "Slav Petrov", "Sailesh Sidhwani", "Ioana Mihailescu", "Alex Panagopoulos", "AJ Piergiovanni", "Yunfei Bai", "George Powell", "Deep Karkhanis", "Trevor Yacovone", "Petr Mitrichev", "Joe Kovac", "Dave Uthus", "Amir Yazdanbakhsh", "David Amos", "Steven Zheng", "Bing Zhang", "Jin Miao", "Bhuvana Ramabhadran", "Soroush Radpour", "Shantanu Thakoor", "Josh Newlan", "Oran Lang", "Orion Jankowski", "Shikhar Bharadwaj", "Jean-Michel Sarr", "Shereen Ashraf", "Sneha Mondal", "Jun Yan", "Ankit Singh Rawat", "Sarmishta Velury", "Greg Kochanski", "Tom Eccles", "Franz Och", "Abhanshu Sharma", "Ethan Mahintorabi", "Alex Gurney", "Carrie Muir", "Vered Cohen", "Saksham Thakur", "Adam Bloniarz", "Asier Mujika", "Alexander Pritzel", "Paul Caron", "Altaf Rahman", "Fiona Lang", "Yasumasa Onoe", "Petar Sirkovic", "Jay Hoover", "Ying Jian", "Pablo Duque", "Arun Narayanan", "David Soergel", "Alex Haig", "Loren Maggiore", "Shyamal Buch", "Josef Dean", "Ilya Figotin", "Igor Karpov", "Shaleen Gupta", "Denny Zhou", "Muhuan Huang", "Ashwin Vaswani", "Christopher Semturs", "Kaushik Shivakumar", "Yu Watanabe", "Vinodh Kumar Rajendran", "Eva Lu", "Yanhan Hou", "Wenting Ye", "Shikhar Vashishth", "Nana Nti", "Vytenis Sakenas", "Darren Ni", "Doug DeCarlo", "Michael Bendersky", "Sumit Bagri", "Nacho Cano", "Elijah Peake", "Simon Tokumine", "Varun Godbole", "Carlos Gua", "Tanya Lando", "Vittorio Selo", "Seher Ellis", "Danny Tarlow", "Daniel Gillick", "Alessandro Epasto", "Siddhartha Reddy Jonnalagadda", "Meng Wei", "Meiyan Xie", "Ankur Taly", "Michela Paganini", "Mukund Sundararajan", "Daniel Toyama", "Ting Yu", "Dessie Petrova", "Aneesh Pappu", "Rohan Agrawal", "Senaka Buthpitiya", "Justin Frye", "Thomas Buschmann", "Remi Crocker", "Marco Tagliasacchi", "Mengchao Wang", "Da Huang", "Sagi Perel", "Brian Wieder", "Hideto Kazawa", "Weiyue Wang", "Jeremy Cole", "Himanshu Gupta", "Ben Golan", "Seojin Bang", "Nitish Kulkarni", "Ken Franko", "Casper Liu", "Doug Reid", "Sid Dalmia", "Jay Whang", "Kevin Cen", "Prasha Sundaram", "Johan Ferret", "Berivan Isik", "Lucian Ionita", "Guan Sun", "Anna Shekhawat", "Muqthar Mohammad", "Philip Pham", "Ronny Huang", "Karthik Raman", "Xingyi Zhou", "Ross Mcilroy", "Austin Myers", "Sheng Peng", "Jacob Scott", "Paul Covington", "Sofia Erell", "Pratik Joshi", "Joo Gabriel Oliveira", "Natasha Noy", "Tajwar Nasir", "Jake Walker", "Vera Axelrod", "Tim Dozat", "Pu Han", "Chun-Te Chu", "Eugene Weinstein", "Anand Shukla", "Shreyas Chandrakaladharan", "Petra Poklukar", "Bonnie Li", "Ye Jin", "Prem Eruvbetine", "Steven Hansen", "Avigail Dabush", "Alon Jacovi", "Samrat Phatale", "Chen Zhu", "Steven Baker", "Mo Shomrat", "Yang Xiao", "Jean Pouget-Abadie", "Mingyang Zhang", "Fanny Wei", "Yang Song", "Helen King", "Yiling Huang", "Yun Zhu", "Ruoxi Sun", "Juliana Vicente Franco", "Chu-Cheng Lin", "Sho Arora", "Hui", "Li", "Vivian Xia", "Luke Vilnis", "Mariano Schain", "Kaiz Alarakyia", "Laurel Prince", "Aaron Phillips", "Caleb Habtegebriel", "Luyao Xu", "Huan Gui", "Santiago Ontanon", "Lora Aroyo", "Karan Gill", "Peggy Lu", "Yash Katariya", "Dhruv Madeka", "Shankar Krishnan", "Shubha Srinivas Raghvendra", "James Freedman", "Yi Tay", "Gaurav Menghani", "Peter Choy", "Nishita Shetty", "Dan Abolafia", "Doron Kukliansky", "Edward Chou", "Jared Lichtarge", "Ken Burke", "Ben Coleman", "Dee Guo", "Larry Jin", "Indro Bhattacharya", "Victoria Langston", "Yiming Li", "Suyog Kotecha", "Alex Yakubovich", "Xinyun Chen", "Petre Petrov", "Tolly Powell", "Yanzhang He", "Corbin Quick", "Kanav Garg", "Dawsen Hwang", "Yang Lu", "Srinadh Bhojanapalli", "Kristian Kjems", "Ramin Mehran", "Aaron Archer", "Hado van Hasselt", "Ashwin Balakrishna", "JK Kearns", "Meiqi Guo", "Jason Riesa", "Mikita Sazanovich", "Xu Gao", "Chris Sauer", "Chengrun Yang", "XiangHai Sheng", "Thomas Jimma", "Wouter Van Gansbeke", "Vitaly Nikolaev", "Wei Wei", "Katie Millican", "Ruizhe Zhao", "Justin Snyder", "Levent Bolelli", "Maura O'Brien", "Shawn Xu", "Fei Xia", "Wentao Yuan", "Arvind Neelakantan", "David Barker", "Sachin Yadav", "Hannah Kirkwood", "Farooq Ahmad", "Joel Wee", "Jordan Grimstad", "Boyu Wang", "Matthew Wiethoff", "Shane Settle", "Miaosen Wang", "Charles Blundell", "Jingjing Chen", "Chris Duvarney", "Grace Hu", "Olaf Ronneberger", "Alex Lee", "Yuanzhen Li", "Abhishek Chakladar", "Alena Butryna", "Georgios Evangelopoulos", "Guillaume Desjardins", "Jonni Kanerva", "Henry Wang", "Averi Nowak", "Nick Li", "Alyssa Loo", "Art Khurshudov", "Laurent El Shafey", "Nagabhushan Baddi", "Karel Lenc", "Yasaman Razeghi", "Tom Lieber", "Amer Sinha", "Xiao Ma", "Yao Su", "James Huang", "Asahi Ushio", "Hanna Klimczak-Pluciska", "Kareem Mohamed", "JD Chen", "Simon Osindero", "Stav Ginzburg", "Lampros Lamprou", "Vasilisa Bashlovkina", "Duc-Hieu Tran", "Ali Khodaei", "Ankit Anand", "Yixian Di", "Ramy Eskander", "Manish Reddy Vuyyuru", "Jasmine Liu", "Aishwarya Kamath", "Roman Goldenberg", "Mathias Bellaiche", "Juliette Pluto", "Bill Rosgen", "Hassan Mansoor", "William Wong", "Suhas Ganesh", "Eric Bailey", "Scott Baird", "Dan Deutsch", "Jinoo Baek", "Xuhui Jia", "Chansoo Lee", "Abe Friesen", "Nathaniel Braun", "Kate Lee", "Amayika Panda", "Steven M. Hernandez", "Duncan Williams", "Jianqiao Liu", "Ethan Liang", "Arnaud Autef", "Emily Pitler", "Deepali Jain", "Phoebe Kirk", "Oskar Bunyan", "Jaume Sanchez Elias", "Tongxin Yin", "Machel Reid", "Aedan Pope", "Nikita Putikhin", "Bidisha Samanta", "Sergio Guadarrama", "Dahun Kim", "Simon Rowe", "Marcella Valentine", "Geng Yan", "Alex Salcianu", "David Silver", "Gan Song", "Richa Singh", "Shuai Ye", "Hannah DeBalsi", "Majd Al Merey", "Eran Ofek", "Albert Webson", "Shibl Mourad", "Ashwin Kakarla", "Silvio Lattanzi", "Nick Roy", "Evgeny Sluzhaev", "Christina Butterfield", "Alessio Tonioni", "Nathan Waters", "Sudhindra Kopalle", "Jason Chase", "James Cohan", "Girish Ramchandra Rao", "Robert Berry", "Michael Voznesensky", "Shuguang Hu", "Kristen Chiafullo", "Sharat Chikkerur", "George Scrivener", "Ivy Zheng", "Jeremy Wiesner", "Wolfgang Macherey", "Timothy Lillicrap", "Fei Liu", "Brian Walker", "David Welling", "Elinor Davies", "Yangsibo Huang", "Lijie Ren", "Nir Shabat", "Alessandro Agostini", "Mariko Iinuma", "Dustin Zelle", "Rohit Sathyanarayana", "Andrea D'olimpio", "Morgan Redshaw", "Matt Ginsberg", "Ashwin Murthy", "Mark Geller", "Tatiana Matejovicova", "Ayan Chakrabarti", "Ryan Julian", "Christine Chan", "Qiong Hu", "Daniel Jarrett", "Manu Agarwal", "Jeshwanth Challagundla", "Tao Li", "Sandeep Tata", "Wen Ding", "Maya Meng", "Zhuyun Dai", "Giulia Vezzani", "Shefali Garg", "Jannis Bulian", "Mary Jasarevic", "Honglong Cai", "Harish Rajamani", "Adam Santoro", "Florian Hartmann", "Chen Liang", "Bartek Perz", "Apoorv Jindal", "Fan Bu", "Sungyong Seo", "Ryan Poplin", "Adrian Goedeckemeyer", "Badih Ghazi", "Nikhil Khadke", "Leon Liu", "Kevin Mather", "Mingda Zhang", "Ali Shah", "Alex Chen", "Jinliang Wei", "Keshav Shivam", "Yuan Cao", "Donghyun Cho", "Angelo Scorza Scarpati", "Michael Moffitt", "Clara Barbu", "Ivan Jurin", "Ming-Wei Chang", "Hongbin Liu", "Hao Zheng", "Shachi Dave", "Christine Kaeser-Chen", "Xiaobin Yu", "Alvin Abdagic", "Lucas Gonzalez", "Yanping Huang", "Peilin Zhong", "Cordelia Schmid", "Bryce Petrini", "Alex Wertheim", "Jifan Zhu", "Hoang Nguyen", "Kaiyang Ji", "Yanqi Zhou", "Tao Zhou", "Fangxiaoyu Feng", "Regev Cohen", "David Rim", "Shubham Milind Phal", "Petko Georgiev", "Ariel Brand", "Yue Ma", "Wei Li", "Somit Gupta", "Chao Wang", "Pavel Dubov", "Jean Tarbouriech", "Kingshuk Majumder", "Huijian Li", "Norman Rink", "Apurv Suman", "Yang Guo", "Yinghao Sun", "Arun Nair", "Xiaowei Xu", "Mohamed Elhawaty", "Rodrigo Cabrera", "Guangxing Han", "Julian Eisenschlos", "Junwen Bai", "Yuqi Li", "Yamini Bansal", "Thibault Sellam", "Mina Khan", "Hung Nguyen", "Justin Mao-Jones", "Nikos Parotsidis", "Jake Marcus", "Cindy Fan", "Roland Zimmermann", "Yony Kochinski", "Laura Graesser", "Feryal Behbahani", "Alvaro Caceres", "Michael Riley", "Patrick Kane", "Sandra Lefdal", "Rob Willoughby", "Paul Vicol", "Lun Wang", "Shujian Zhang", "Ashleah Gill", "Yu Liang", "Gautam Prasad", "Soroosh Mariooryad", "Mehran Kazemi", "Zifeng Wang", "Kritika Muralidharan", "Paul Voigtlaender", "Jeffrey Zhao", "Huanjie Zhou", "Nina D'Souza", "Aditi Mavalankar", "Sb Arnold", "Nick Young", "Obaid Sarvana", "Chace Lee", "Milad Nasr", "Tingting Zou", "Seokhwan Kim", "Lukas Haas", "Kaushal Patel", "Neslihan Bulut", "David Parkinson", "Courtney Biles", "Dmitry Kalashnikov", "Chi Ming To", "Aviral Kumar", "Jessica Austin", "Alex Greve", "Lei Zhang", "Megha Goel", "Yeqing Li", "Sergey Yaroshenko", "Max Chang", "Abhishek Jindal", "Geoff Clark", "Hagai Taitelbaum", "Dale Johnson", "Ofir Roval", "Jeongwoo Ko", "Anhad Mohananey", "Christian Schuler", "Shenil Dodhia", "Ruichao Li", "Kazuki Osawa", "Claire Cui", "Peng Xu", "Rushin Shah", "Tao Huang", "Ela Gruzewska", "Nathan Clement", "Mudit Verma", "Olcan Sercinoglu", "Hai Qian", "Viral Shah", "Masa Yamaguchi", "Abhinit Modi", "Takahiro Kosakai", "Thomas Strohmann", "Junhao Zeng", "Beliz Gunel", "Jun Qian", "Austin Tarango", "Krzysztof Jastrzbski", "Robert David", "Jyn Shan", "Parker Schuh", "Kunal Lad", "Willi Gierke", "Mukundan Madhavan", "Xinyi Chen", "Mark Kurzeja", "Rebeca Santamaria-Fernandez", "Dawn Chen", "Alexandra Cordell", "Yuri Chervonyi", "Frankie Garcia", "Nithish Kannen", "Vincent Perot", "Nan Ding", "Shlomi Cohen-Ganor", "Victor Lavrenko", "Junru Wu", "Georgie Evans", "Cicero Nogueira dos Santos", "Madhavi Sewak", "Ashley Brown", "Andrew Hard", "Joan Puigcerver", "Zeyu Zheng", "Yizhong Liang", "Evgeny Gladchenko", "Reeve Ingle", "Uri First", "Pierre Sermanet", "Charlotte Magister", "Mihajlo Velimirovi", "Sashank Reddi", "Susanna Ricco", "Eirikur Agustsson", "Hartwig Adam", "Nir Levine", "David Gaddy", "Dan Holtmann-Rice", "Xuanhui Wang", "Ashutosh Sathe", "Abhijit Guha Roy", "Bla Bratani", "Alen Carin", "Harsh Mehta", "Silvano Bonacina", "Nicola De Cao", "Mara Finkelstein", "Verena Rieser", "Xinyi Wu", "Florent Altch", "Dylan Scandinaro", "Li Li", "Nino Vieillard", "Nikhil Sethi", "Garrett Tanzer", "Zhi Xing", "Shibo Wang", "Parul Bhatia", "Gui Citovsky", "Thomas Anthony", "Sharon Lin", "Tianze Shi", "Shoshana Jakobovits", "Gena Gibson", "Raj Apte", "Lisa Lee", "Mingqing Chen", "Arunkumar Byravan", "Petros Maniatis", "Kellie Webster", "Andrew Dai", "Pu-Chin Chen", "Jiaqi Pan", "Asya Fadeeva", "Zach Gleicher", "Thang Luong", "Niket Kumar Bhumihar"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      72 pages, 17 figures", "url": "http://arxiv.org/abs/2507.06261v4", "summary": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.", "comment": "72 pages, 17 figures", "pdf_url": "http://arxiv.org/pdf/2507.06261v4", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-22"}
{"id": "2507.09786", "title": "Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster", "authors": ["Junaid Iqbal Khan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, 4 tables", "url": "http://arxiv.org/abs/2507.09786v2", "summary": "Approximate machine unlearning (AMU) enables models to `forget' specific\ntraining data through specialized fine-tuning on a retained dataset subset.\nHowever, processing this retained subset still dominates computational runtime,\nwhile reductions of epochs also remain a challenge. We propose two\ncomplementary methods to accelerate classification-oriented AMU. First,\n\\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges\nvisually similar images with shared blend-weights to significantly reduce the\nretained set size. It operates with minimal pre-processing overhead and is\norders of magnitude faster than state-of-the-art DC methods. Second, our\nloss-centric method, \\textbf{Accelerated-AMU (A-AMU)}, augments the unlearning\nobjective to quicken convergence. A-AMU achieves this by combining a steepened\nprimary loss to expedite forgetting with a novel, differentiable regularizer\nthat matches the loss distributions of forgotten and in-distribution unseen\ndata. Our extensive experiments demonstrate that this dual approach of data and\nloss-centric optimization dramatically reduces end-to-end unlearning latency\nacross both single and multi-round scenarios, all while preserving model\nutility and privacy. To our knowledge, this is the first work to systematically\ntackle unlearning efficiency by jointly designing a specialized dataset\ncondensation technique with a dedicated accelerated loss function. Code is\navailable at https://github.com/algebraicdianuj/DC_Unlearning.", "comment": "10 pages, 4 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.09786v2", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-22"}
{"id": "2503.06898", "title": "Illuminating Darkness: Learning to Enhance Low-light Images In-the-Wild", "authors": ["S M A Sharif", "Abdur Rehman", "Zain Ul Abidin", "Fayaz Ali Dharejo", "Radu Timofte", "Rizwan Ali Naqvi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06898v2", "summary": "Single-shot low-light image enhancement (SLLIE) remains challenging due to\nthe limited availability of diverse, real-world paired datasets. To bridge this\ngap, we introduce the Low-Light Smartphone Dataset (LSD), a large-scale,\nhigh-resolution (4K+) dataset collected in the wild across a wide range of\nchallenging lighting conditions (0.1 to 200 lux). LSD contains 6,425 precisely\naligned low and normal-light image pairs, selected from over 8,000 dynamic\nindoor and outdoor scenes through multi-frame acquisition and expert\nevaluation. To evaluate generalization and aesthetic quality, we collect 2,117\nunpaired low-light images from previously unseen devices. To fully exploit LSD,\nwe propose TFFormer, a hybrid model that encodes luminance and chrominance (LC)\nseparately to reduce color-structure entanglement. We further propose a\ncross-attention-driven joint decoder for context-aware fusion of LC\nrepresentations, along with LC refinement and LC-guided supervision to\nsignificantly enhance perceptual fidelity and structural consistency. TFFormer\nachieves state-of-the-art results on LSD (+2.45 dB PSNR) and substantially\nimproves downstream vision tasks, such as low-light object detection (+6.80 mAP\non ExDark).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06898v2", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-07-22"}
{"id": "2506.12091", "title": "Continuously Updating Digital Twins using Large Language Models", "authors": ["Harry Amad", "Nicols Astorga", "Mihaela van der Schaar"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12091v2", "summary": "Digital twins are models of real-world systems that can simulate their\ndynamics in response to potential actions. In complex settings, the state and\naction variables, and available data and knowledge relevant to a system can\nconstantly change, requiring digital twins to continuously update with these\nchanges to remain relevant. Current approaches struggle in this regard, as they\nrequire fixed, well-defined modelling environments, and they cannot adapt to\nnovel variables without re-designs, or incorporate new information without\nre-training. To address this, we frame digital twinning as an in-context\nlearning problem using large language models, enabling seamless updates to the\ntwin at inference time. We develop CALM-DT, a Context-Adaptive Language\nModel-based Digital Twin that can accurately simulate across diverse\nstate-action spaces using in-context learning alone by utilising fine-tuned\nencoders for sample retrieval. We empirically demonstrate CALM-DT's competitive\nperformance with existing digital twin approaches, and its unique ability to\nadapt to changes in its modelling environment without parameter updates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12091v2", "cate": "cs.CL", "date": "2025-06-11", "updated": "2025-07-21"}
{"id": "2412.01341", "title": "Virtual finite element and hyperbolic problems: the PAMPA algorithm", "authors": ["Rmi Abgrall", "Yongle Liu", "Walter Boscheri"], "categories": ["math.NA", "cs.NA", "65M06, 65M08"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.01341v3", "summary": "In this paper, we explore the use of the Virtual Element Method concepts to\nsolve scalar and system hyperbolic problems on general polygonal grids. The new\nschemes stem from the active flux approach \\cite{AF1}, which combines the usage\nof point values at the element boundaries with an additional degree of freedom\nrepresenting the average of the solution within each control volume. Along the\nlines of the family of residual distribution schemes introduced in\n\\cite{Abgrall_AF,abgrall2023activefluxtriangularmeshes} \\red{that integrate the\nactive flux technique}, we devise novel third order accurate methods that rely\non the VEM technology to discretize gradients of the numerical solution by\nmeans of a polynomial-free approximation, \\red{by} adopting a virtual basis\nthat is locally defined for each element. The obtained discretization is\nglobally continuous, and for nonlinear problems it needs a stabilization which\nis provided by \\new{a monolithic convex limiting strategy extended from\n\\cite{Abgrall_BP_PAMPA}}. This is applied to both point and average values of\nthe discrete solution. We show applications to scalar problems, as well as to\nthe acoustics and Euler equations in two dimension. The accuracy and the\nrobustness of the proposed schemes are assessed against a suite of benchmarks\ninvolving smooth solutions, shock waves and other discontinuities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.01341v3", "cate": "math.NA", "date": "2024-12-02", "updated": "2025-07-22"}
{"id": "2507.09279", "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025 Workshop CVAMD", "url": "http://arxiv.org/abs/2507.09279v3", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/prompt4trust.", "comment": "Accepted to ICCV 2025 Workshop CVAMD", "pdf_url": "http://arxiv.org/pdf/2507.09279v3", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-22"}
{"id": "2507.09958", "title": "Rethinking Inductive Bias in Geographically Neural Network Weighted Regression", "authors": ["Zhenyuan Chen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      I have decided to withdraw this preprint as it no longer aligns with my current research goals and priorities. The manuscript requires substantial revision and should not remain publicly accessible in its current form", "url": "http://arxiv.org/abs/2507.09958v4", "summary": "Inductive bias is a key factor in spatial regression models, determining how\nwell a model can learn from limited data and capture spatial patterns. This\nwork revisits the inductive biases in Geographically Neural Network Weighted\nRegression (GNNWR) and identifies limitations in current approaches for\nmodeling spatial non-stationarity. While GNNWR extends traditional\nGeographically Weighted Regression by using neural networks to learn spatial\nweighting functions, existing implementations are often restricted by fixed\ndistance-based schemes and limited inductive bias. We propose to generalize\nGNNWR by incorporating concepts from convolutional neural networks, recurrent\nneural networks, and transformers, introducing local receptive fields,\nsequential context, and self-attention into spatial regression. Through\nextensive benchmarking on synthetic spatial datasets with varying\nheterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic\nmethods in capturing nonlinear and complex spatial relationships. Our results\nalso reveal that model performance depends strongly on data characteristics,\nwith local models excelling in highly heterogeneous or small-sample scenarios,\nand global models performing better with larger, more homogeneous data. These\nfindings highlight the importance of inductive bias in spatial modeling and\nsuggest future directions, including learnable spatial weighting functions,\nhybrid neural architectures, and improved interpretability for models handling\nnon-stationary spatial data.", "comment": "I have decided to withdraw this preprint as it no longer aligns with\n  my current research goals and priorities. The manuscript requires substantial\n  revision and should not remain publicly accessible in its current form", "pdf_url": "http://arxiv.org/pdf/2507.09958v4", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-21"}
{"id": "2503.07601", "title": "Balanced Image Stylization with Style Matching Score", "authors": ["Yuxin Jiang", "Liming Jiang", "Shuai Yang", "Jia-Wei Liu", "Ivor Tsang", "Mike Zheng Shou"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Code: this https URL Project page: this https URL", "url": "http://arxiv.org/abs/2503.07601v2", "summary": "We present Style Matching Score (SMS), a novel optimization method for image\nstylization with diffusion models. Balancing effective style transfer with\ncontent preservation is a long-standing challenge. Unlike existing efforts, our\nmethod reframes image stylization as a style distribution matching problem. The\ntarget style distribution is estimated from off-the-shelf style-dependent LoRAs\nvia carefully designed score functions. To preserve content information\nadaptively, we propose Progressive Spectrum Regularization, which operates in\nthe frequency domain to guide stylization progressively from low-frequency\nlayouts to high-frequency details. In addition, we devise a Semantic-Aware\nGradient Refinement technique that leverages relevance maps derived from\ndiffusion semantic priors to selectively stylize semantically important\nregions. The proposed optimization formulation extends stylization from pixel\nspace to parameter space, readily applicable to lightweight feedforward\ngenerators for efficient one-step stylization. SMS effectively balances style\nalignment and content preservation, outperforming state-of-the-art approaches,\nverified by extensive experiments.", "comment": "ICCV 2025. Code: https://github.com/showlab/SMS Project page:\n  https://yuxinn-j.github.io/projects/SMS.html", "pdf_url": "http://arxiv.org/pdf/2503.07601v2", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-07-22"}
{"id": "2507.07939", "title": "SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment", "authors": ["Guoxin Zang", "Xue Li", "Donglin Di", "Lanshun Nie", "Dechen Zhan", "Yang Song", "Lei Fan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACMMM2025", "url": "http://arxiv.org/abs/2507.07939v2", "summary": "While Vision-Language Models (VLMs) have shown promising progress in general\nmultimodal tasks, they often struggle in industrial anomaly detection and\nreasoning, particularly in delivering interpretable explanations and\ngeneralizing to unseen categories. This limitation stems from the inherently\ndomain-specific nature of anomaly detection, which hinders the applicability of\nexisting VLMs in industrial scenarios that require precise, structured, and\ncontext-aware analysis. To address these challenges, we propose SAGE, a\nVLM-based framework that enhances anomaly reasoning through Self-Guided Fact\nEnhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE\nintegrates domain-specific knowledge into visual reasoning via fact extraction\nand fusion, while E-DPO aligns model outputs with expert preferences using\nentropy-aware optimization. Additionally, we introduce AD-PL, a\npreference-optimized dataset tailored for industrial anomaly reasoning,\nconsisting of 28,415 question-answering instances with expert-ranked responses.\nTo evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation\n(MLE), a quantitative framework analyzing model logic and consistency. SAGE\ndemonstrates superior performance on industrial anomaly datasets under\nzero-shot and one-shot settings. The code, model and dataset are available at\nhttps://github.com/amoreZgx1n/SAGE.", "comment": "Accepted by ACMMM2025", "pdf_url": "http://arxiv.org/pdf/2507.07939v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-22"}
{"id": "2502.13445", "title": "An Efficient Iterative Decoupling Method for Thermo-Poroelasticity Based on a Four-Field Formulation", "authors": ["Mingchao Cai", "Jingzhi Li", "Ziliang Li", "Qiang Liu"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      submitted to a journal, accepted, need to be updated", "url": "http://arxiv.org/abs/2502.13445v3", "summary": "This paper studies the thermo-poroelasticity model. By introducing an\nintermediate variable, we transform the original three-field model into a\nfour-field model. Building upon this four-field model, we present both a\ncoupled finite element method and a decoupled iterative finite element method.\nWe prove the stability and optimal convergence of the coupled finite element\nmethod. Furthermore, we establish the convergence of the decoupled iterative\nmethod. This paper focuses primarily on analyzing the iterative decoupled\nalgorithm. It demonstrates that the algorithm's convergence does not require\nany additional assumptions about physical parameters or stabilization\nparameters. Numerical results are provided to demonstrate the effectiveness and\ntheoretical validity of these new methods.", "comment": "submitted to a journal, accepted, need to be updated", "pdf_url": "http://arxiv.org/pdf/2502.13445v3", "cate": "math.NA", "date": "2025-02-19", "updated": "2025-07-21"}
{"id": "2507.11936", "title": "A Survey of Deep Learning for Geometry Problem Solving", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.11936v2", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.11936v2", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-22"}
{"id": "2507.10183", "title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs", "authors": ["Alireza Dizaji", "Benedict Aaron Tjandra", "Mehrab Hamidi", "Shenyang Huang", "Guillaume Rabusseau"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)", "url": "http://arxiv.org/abs/2507.10183v2", "summary": "Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.", "comment": "Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)", "pdf_url": "http://arxiv.org/pdf/2507.10183v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-22"}
{"id": "2503.12545", "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Weidong Tang", "Jiaxin Ai", "Wangbo Zhao", "Kai Wang", "Xiaojiang Peng", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12545v2", "summary": "Multimodal large language models (MLLMs) have achieved remarkable success in\nvision-language tasks, but their reliance on vast, internet-sourced data raises\nsignificant privacy and security concerns. Machine unlearning (MU) has emerged\nas a critical technique to address these issues, enabling the selective removal\nof targeted information from pre-trained models without costly retraining.\nHowever, the evaluation of MU for MLLMs remains inadequate. Existing benchmarks\noften lack a comprehensive scope, focusing narrowly on entities while\noverlooking the unlearning of broader visual concepts and the inherent semantic\ncoupling between them. To bridge this gap, we introduce, PEBench, a novel\nbenchmark designed to facilitate a thorough assessment of MU in MLLMs. PEBench\nfeatures a fictitious dataset of personal entities and corresponding event\nscenes to evaluate unlearning across these distinct yet entangled concepts. We\nleverage this benchmark to evaluate five MU methods, revealing their unique\nstrengths and weaknesses. Our findings show that unlearning one concept can\nunintentionally degrade performance on related concepts within the same image,\na challenge we term cross-concept interference. Furthermore, we demonstrate the\ndifficulty of unlearning person and event concepts simultaneously and propose\nan effective method to mitigate these conflicting objectives. The source code\nand benchmark are publicly available at https://pebench.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12545v2", "cate": "cs.CV", "date": "2025-03-16", "updated": "2025-07-22"}
{"id": "2507.09205", "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      paper modification", "url": "http://arxiv.org/abs/2507.09205v2", "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "comment": "paper modification", "pdf_url": "http://arxiv.org/pdf/2507.09205v2", "cate": "cs.CL", "date": "2025-07-12", "updated": "2025-07-22"}
{"id": "2503.03243", "title": "Finite element form-valued forms: Construction", "authors": ["Kaibo Hu", "Ting Lin"], "categories": ["math.NA", "cs.NA", "math.DG", "65N30, 53A70, 58A10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      97 pages, 26 figures, 21 tables", "url": "http://arxiv.org/abs/2503.03243v3", "summary": "We provide a finite element discretization of $\\ell$-form-valued $k$-forms on\ntriangulation in $\\mathbb{R}^{n}$ for general $k$, $\\ell$ and $n$ and any\npolynomial degree. The construction generalizes finite element Whitney forms\nfor the de~Rham complex and their higher-order and distributional versions, the\nRegge finite elements and the Christiansen--Regge elasticity complex, the TDNNS\nelement for symmetric stress tensors, the MCS element for traceless matrix\nfields, the Hellan--Herrmann--Johnson (HHJ) elements for biharmonic equations,\nand discrete divdiv and Hessian complexes in [Hu, Lin, and Zhang, 2025]. The\nconstruction discretizes the Bernstein--Gelfand--Gelfand (BGG) diagrams.\nApplications of the construction include discretization of strain and stress\ntensors in continuum mechanics and metric and curvature tensors in differential\ngeometry in any dimension.", "comment": "97 pages, 26 figures, 21 tables", "pdf_url": "http://arxiv.org/pdf/2503.03243v3", "cate": "math.NA", "date": "2025-03-05", "updated": "2025-07-22"}
{"id": "2507.13354", "title": "Physical models realizing the transformer architecture of large language models", "authors": ["Zeqian Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL", "math-ph", "math.MP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, minor changes, Refs [3, 13, 15] added", "url": "http://arxiv.org/abs/2507.13354v2", "summary": "The introduction of the transformer architecture in 2017 marked the most\nstriking advancement in natural language processing. The transformer is a model\narchitecture relying entirely on an attention mechanism to draw global\ndependencies between input and output. However, we believe there is a gap in\nour theoretical understanding of what the transformer is, and how it works\nphysically. From a physical perspective on modern chips, such as those chips\nunder 28nm, modern intelligent machines should be regarded as open quantum\nsystems beyond conventional statistical systems. Thereby, in this paper, we\nconstruct physical models realizing large language models based on a\ntransformer architecture as open quantum systems in the Fock space over the\nHilbert space of tokens. Our physical models underlie the transformer\narchitecture for large language models.", "comment": "6 pages, minor changes, Refs [3, 13, 15] added", "pdf_url": "http://arxiv.org/pdf/2507.13354v2", "cate": "cs.LG", "date": "2025-05-21", "updated": "2025-07-22"}
{"id": "2507.13620", "title": "Tri-Learn Graph Fusion Network for Attributed Graph Clustering", "authors": ["Binxiong Li", "Xu Xiang", "Xue Li", "Binyu Zhao", "Heyang Gao", "Qinyu Zhao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The source code for this study is available at this https URL", "url": "http://arxiv.org/abs/2507.13620v2", "summary": "In recent years, models based on Graph Convolutional Networks (GCN) have made\nsignificant strides in the field of graph data analysis. However, challenges\nsuch as over-smoothing and over-compression remain when handling large-scale\nand complex graph datasets, leading to a decline in clustering quality.\nAlthough the Graph Transformer architecture has mitigated some of these issues,\nits performance is still limited when processing heterogeneous graph data. To\naddress these challenges, this study proposes a novel deep clustering framework\nthat comprising GCN, Autoencoder (AE), and Graph Transformer, termed the\nTri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the\ndifferentiation and consistency of global and local information through a\nunique tri-learning mechanism and feature fusion enhancement strategy. The\nframework integrates GCN, AE, and Graph Transformer modules. These components\nare meticulously fused by a triple-channel enhancement module, which maximizes\nthe use of both node attributes and topological structures, ensuring robust\nclustering representation. The tri-learning mechanism allows mutual learning\namong these modules, while the feature fusion strategy enables the model to\ncapture complex relationships, yielding highly discriminative representations\nfor graph clustering. It surpasses many state-of-the-art methods, achieving an\naccuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the\nReuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding\nperformance on the Reuters dataset, Tri-GFN can be applied to automatic news\nclassification, topic retrieval, and related fields.", "comment": "The source code for this study is available at\n  https://github.com/YF-W/Tri-GFN", "pdf_url": "http://arxiv.org/pdf/2507.13620v2", "cate": "cs.LG", "date": "2025-07-18", "updated": "2025-07-22"}
{"id": "2503.13684", "title": "FiVE: A Fine-grained Video Editing Benchmark for Evaluating Emerging Diffusion and Rectified Flow Models", "authors": ["Minghan Li", "Chenxi Xie", "Yichen Wu", "Lei Zhang", "Mengyu Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      24 pages, 14 figures, 16 tables", "url": "http://arxiv.org/abs/2503.13684v2", "summary": "Numerous text-to-video (T2V) editing methods have emerged recently, but the\nlack of a standardized benchmark for fair evaluation has led to inconsistent\nclaims and an inability to assess model sensitivity to hyperparameters.\nFine-grained video editing is crucial for enabling precise, object-level\nmodifications while maintaining context and temporal consistency. To address\nthis, we introduce FiVE, a Fine-grained Video Editing Benchmark for evaluating\nemerging diffusion and rectified flow models. Our benchmark includes 74\nreal-world videos and 26 generated videos, featuring 6 fine-grained editing\ntypes, 420 object-level editing prompt pairs, and their corresponding masks.\nAdditionally, we adapt the latest rectified flow (RF) T2V generation models,\nPyramid-Flow and Wan2.1, by introducing FlowEdit, resulting in training-free\nand inversion-free video editing models Pyramid-Edit and Wan-Edit. We evaluate\nfive diffusion-based and two RF-based editing methods on our FiVE benchmark\nusing 15 metrics, covering background preservation, text-video similarity,\ntemporal consistency, video quality, and runtime. To further enhance\nobject-level evaluation, we introduce FiVE-Acc, a novel metric leveraging\nVision-Language Models (VLMs) to assess the success of fine-grained video\nediting. Experimental results demonstrate that RF-based editing significantly\noutperforms diffusion-based methods, with Wan-Edit achieving the best overall\nperformance and exhibiting the least sensitivity to hyperparameters. More video\ndemo available on the anonymous website:\nhttps://sites.google.com/view/five-benchmark", "comment": "24 pages, 14 figures, 16 tables", "pdf_url": "http://arxiv.org/pdf/2503.13684v2", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-07-21"}
{"id": "2507.13618", "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Jingwen Chen", "Zhichao Huang", "Tao Li", "Yifu Li", "Huiying Lin", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13618v2", "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13618v2", "cate": "cs.CL", "date": "2025-07-18", "updated": "2025-07-22"}
{"id": "2504.03525", "title": "An Efficient Second-Order Adaptive Procedure for Inserting CAD Geometries into Hexahedral Meshes using Volume Fractions", "authors": ["Brian N. Granzow", "Stephen D. Bond", "Michael J. Powell", "Daniel A. Ibanez"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      15 pages, 15 figures, 2 tables", "url": "http://arxiv.org/abs/2504.03525v2", "summary": "This paper is concerned with inserting three-dimensional computer-aided\ndesign (CAD) geometries into meshes composed of hexahedral elements using a\nvolume fraction representation. An adaptive procedure for doing so is\npresented. The procedure consists of two steps. The first step performs spatial\nacceleration using a k-d tree. The second step involves subdividing individual\nhexahedra in an adaptive mesh refinement (AMR)-like fashion and approximating\nthe CAD geometry linearly (as a plane) at the finest subdivision. The procedure\nrequires only two geometric queries from a CAD kernel: determining whether or\nnot a queried spatial coordinate is inside or outside the CAD geometry and\ndetermining the closest point on the CAD geometry's surface from a given\nspatial coordinate. We prove that the procedure is second-order accurate for\nsufficiently smooth geometries and sufficiently refined background meshes. We\ndemonstrate the expected order of accuracy is achieved with several\nverification tests and illustrate the procedure's effectiveness for several\nexemplar CAD geometries.", "comment": "15 pages, 15 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2504.03525v2", "cate": "math.NA", "date": "2025-04-04", "updated": "2025-07-21"}
{"id": "2507.14171", "title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning", "authors": ["Jaeheun Jung", "Jaehyuk Lee", "Yeajin Lee", "Donghun Lee"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICCV 2025 workshop U&ME 2025 (2nd Workshop and Challenge on Unlearning and Model Editing)", "url": "http://arxiv.org/abs/2507.14171v2", "summary": "With the growth of demand on neural network compression methods, the\nstructured pruning methods including importance-based approach are actively\nstudied. The magnitude importance and many correlated modern importance\ncriteria often limit the capacity of pruning decision, since the filters with\nlarger magnitudes are not likely to be pruned if the smaller one didn't, even\nif it is redundant. In this paper, we propose a novel pruning strategy to\nchallenge this dominating effect of magnitude and provide fair chance to each\nfilter to be pruned, by placing it on projective space. After that, we observe\nthe gradient descent movement whether the filters move toward the origin or\nnot, to measure how the filter is likely to be pruned. This measurement is used\nto construct PROscore, a novel importance score for IPPRO, a novel\nimportance-based structured pruning with magnitude-indifference. Our evaluation\nresults shows that the proposed importance criteria using the projective space\nachieves near-lossless pruning by reducing the performance drop in pruning,\nwith promising performance after the finetuning. Our work debunks the\n``size-matters'' myth in pruning and expands the frontier of importance-based\npruning both theoretically and empirically.", "comment": "ICCV 2025 workshop U&ME 2025 (2nd Workshop and Challenge on\n  Unlearning and Model Editing)", "pdf_url": "http://arxiv.org/pdf/2507.14171v2", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-22"}
{"id": "2507.13762", "title": "MolPIF: A Parameter Interpolation Flow Model for Molecule Generation", "authors": ["Yaowei Jin", "Junjie Wang", "Wenkai Xiang", "Duanhua Cao", "Dan Teng", "Zhehuan Fan", "Jiacheng Xiong", "Xia Sheng", "Chuanlong Zeng", "Duo An", "Mingyue Zheng", "Shuangjia Zheng", "Qian Shi"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13762v2", "summary": "Advances in deep learning for molecular generation show promise in\naccelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown\nimpressive performance across diverse chemical tasks, with their success often\nascribed to the paradigm of modeling in a low-variance parameter space.\nHowever, the Bayesian inference-based strategy imposes limitations on designing\nmore flexible distribution transformation pathways, making it challenging to\nadapt to diverse data distributions and varied task requirements. Furthermore,\nthe potential for simpler, more efficient parameter-space-based models is\nunexplored. To address this, we propose a novel Parameter Interpolation Flow\nmodel (named PIF) with detailed theoretical foundation, training, and inference\nprocedures. We then develop MolPIF for structure-based drug design,\ndemonstrating its superior performance across diverse metrics compared to\nbaselines. This work validates the effectiveness of parameter-space-based\ngenerative modeling paradigm for molecules and offers new perspectives for\nmodel design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13762v2", "cate": "cs.LG", "date": "2025-07-18", "updated": "2025-07-22"}
{"id": "2504.20041", "title": "Learning Streaming Video Representation via Multitask Training", "authors": ["Yibin Yan", "Jilan Xu", "Shangzhe Di", "Yikun Liu", "Yudi Shi", "Qirui Chen", "Zeqian Li", "Yifei Huang", "Weidi Xie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Technical Report. Project Page: this https URL", "url": "http://arxiv.org/abs/2504.20041v2", "summary": "Understanding continuous video streams plays a fundamental role in real-time\napplications including embodied AI and autonomous driving. Unlike offline video\nunderstanding, streaming video understanding requires the ability to process\nvideo streams frame by frame, preserve historical information, and make\nlow-latency decisions. To address these challenges, our main contributions are\nthree-fold. (i) We develop a novel streaming video backbone, termed as\nStreamFormer, by incorporating causal temporal attention into a pre-trained\nvision transformer. This enables efficient streaming video processing while\nmaintaining image representation capability. (ii) To train StreamFormer, we\npropose to unify diverse spatial-temporal video understanding tasks within a\nmultitask visual-language alignment framework. Hence, StreamFormer learns\nglobal semantics, temporal dynamics, and fine-grained spatial relationships\nsimultaneously. (iii) We conduct extensive experiments on online action\ndetection, online video instance segmentation, and video question answering.\nStreamFormer achieves competitive results while maintaining efficiency,\ndemonstrating its potential for real-time applications.", "comment": "Technical Report. Project Page:\n  https://go2heart.github.io/streamformer", "pdf_url": "http://arxiv.org/pdf/2504.20041v2", "cate": "cs.CV", "date": "2025-04-28", "updated": "2025-07-22"}
{"id": "2507.14241", "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese", "Huan Wang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14241v2", "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14241v2", "cate": "cs.CL", "date": "2025-07-17", "updated": "2025-07-22"}
{"id": "2507.12941", "title": "Adaptive feature capture method for solving partial differential equations with low regularity solutions", "authors": ["Yangtao Deng", "Qiaolin He", "Xiaoping Wang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Irreconcilable authorship disagreements. Key reasons: Unresolved author attribution disputes. No consensus on contributor inclusion/order. All co-authors notified. Full responsibility accepted", "url": "http://arxiv.org/abs/2507.12941v2", "summary": "Partial differential equations (PDEs) with low-regularity solutions pose\nsignificant challenges for traditional numerical methods, particularly in\ncomplex geometries where mesh generation and adaptive refinement become\ncomputationally expensive. While deep-learning-based approaches, such as\nPhysics-Informed Neural Networks (PINNs) and the Random Feature Method (RFM),\noffer mesh-free alternatives, they often lack adaptive resolution in critical\nregions, limiting their accuracy for solutions with steep gradients or\nsingularities. In this work, we propose the Adaptive Feature Capture Method\n(AFCM), a novel machine learning framework that adaptively redistributes\nneurons and collocation points in high-gradient regions to enhance local\nexpressive power. Inspired by adaptive moving mesh techniques, AFCM employs the\ngradient norm of an approximate solution as a monitor function to guide the\nreinitialization of feature function parameters. This ensures that partition\nhyperplanes and collocation points cluster where they are most needed,\nachieving higher resolution without increasing computational overhead. The AFCM\nextends the capabilities of RFM to handle PDEs with near-singular solutions\nwhile preserving its mesh-free efficiency. Numerical experiments demonstrate\nthe method's effectiveness in accurately resolving low-regularity problems,\neven in complex geometries. By bridging the gap between adaptive mesh\nrefinement and randomized neural networks, AFCM offers a robust and scalable\napproach for solving challenging PDEs in scientific and engineering\napplications.", "comment": "Irreconcilable authorship disagreements. Key reasons: Unresolved\n  author attribution disputes. No consensus on contributor inclusion/order. All\n  co-authors notified. Full responsibility accepted", "pdf_url": "http://arxiv.org/pdf/2507.12941v2", "cate": "math.NA", "date": "2025-07-17", "updated": "2025-07-21"}
{"id": "2507.14657", "title": "AI-Enhanced Precision in Sport Taekwondo: Increasing Fairness, Speed, and Trust in Competition (FST.ai)", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      24 pages, 9 figures", "url": "http://arxiv.org/abs/2507.14657v2", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\n'FST.ai' -- which is developed under the 'R3AL.ai' project, which serves as its\nPrincipal Investigator: r3al.ai -- a novel AI-powered framework designed to\nenhance officiating in Sport Taekwondo, particularly focusing on the complex\ntask of real-time head kick detection and scoring. Leveraging computer vision,\ndeep learning, and edge inference, the system automates the identification and\nclassification of key actions, significantly reducing decision time from\nminutes to seconds while improving consistency and transparency. Importantly,\nthe methodology is not limited to Taekwondo. The underlying framework -- based\non pose estimation, motion classification, and impact analysis -- can be\nadapted to a wide range of sports requiring action detection, such as judo,\nkarate, fencing, or even team sports like football and basketball, where foul\nrecognition or performance tracking is critical. By addressing one of\nTaekwondo's most challenging scenarios -- head kick scoring -- we demonstrate\nthe robustness, scalability, and sport-agnostic potential of 'FST.ai' to\ntransform officiating standards across multiple disciplines.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.14657v2", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-22"}
{"id": "2507.15195", "title": "Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning", "authors": ["Anwar Said", "Yifan Wei", "Obaid Ullah Ahmad", "Mudassir Shabbir", "Waseem Abbas", "Xenofon Koutsoukos"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15195v2", "summary": "In this article, we utilize the concept of average controllability in graphs,\nalong with a novel rank encoding method, to enhance the performance of Graph\nNeural Networks (GNNs) in social network classification tasks. GNNs have proven\nhighly effective in various network-based learning applications and require\nsome form of node features to function. However, their performance is heavily\ninfluenced by the expressiveness of these features. In social networks, node\nfeatures are often unavailable due to privacy constraints or the absence of\ninherent attributes, making it challenging for GNNs to achieve optimal\nperformance. To address this limitation, we propose two strategies for\nconstructing expressive node features. First, we introduce average\ncontrollability along with other centrality metrics (denoted as NCT-EFA) as\nnode-level metrics that capture critical aspects of network topology. Building\non this, we develop a rank encoding method that transforms average\ncontrollability or any other graph-theoretic metric into a fixed-dimensional\nfeature space, thereby improving feature representation. We conduct extensive\nnumerical evaluations using six benchmark GNN models across four social network\ndatasets to compare different node feature construction methods. Our results\ndemonstrate that incorporating average controllability into the feature space\nsignificantly improves GNN performance. Moreover, the proposed rank encoding\nmethod outperforms traditional one-hot degree encoding, improving the ROC AUC\nfrom 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,\nunderscoring its effectiveness in generating expressive and efficient node\nrepresentations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15195v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2505.08013", "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer", "authors": ["Gonglin Chen", "Tianwen Fu", "Haiwei Chen", "Wenbin Teng", "Hanyuan Xiao", "Yajie Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08013v4", "summary": "As a core step in structure-from-motion and SLAM, robust feature detection\nand description under challenging scenarios such as significant viewpoint\nchanges remain unresolved despite their ubiquity. While recent works have\nidentified the importance of local features in modeling geometric\ntransformations, these methods fail to learn the visual cues present in\nlong-range relationships. We present Robust Deformable Detector (RDD), a novel\nand robust keypoint detector/descriptor leveraging the deformable transformer,\nwhich captures global context and geometric invariance through deformable\nself-attention mechanisms. Specifically, we observed that deformable attention\nfocuses on key locations, effectively reducing the search space complexity and\nmodeling the geometric invariance. Furthermore, we collected an Air-to-Ground\ndataset for training in addition to the standard MegaDepth dataset. Our\nproposed method outperforms all state-of-the-art keypoint detection/description\nmethods in sparse matching tasks and is also capable of semi-dense matching. To\nensure comprehensive evaluation, we introduce two challenging benchmarks: one\nemphasizing large viewpoint and scale variations, and the other being an\nAir-to-Ground benchmark -- an evaluation setting that has recently gaining\npopularity for 3D reconstruction across different altitudes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08013v4", "cate": "cs.CV", "date": "2025-05-12", "updated": "2025-07-21"}
{"id": "2507.14430", "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Technical Report", "url": "http://arxiv.org/abs/2507.14430v2", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "comment": "Technical Report", "pdf_url": "http://arxiv.org/pdf/2507.14430v2", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-22"}
{"id": "2507.15345", "title": "Exponential Runge-Kutta Galerkin finite element method for a reaction-diffusion system with nonsmooth initial data", "authors": ["Runjie Zhang", "Shuo Yang", "Jinwei Fang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15345v2", "summary": "This study presents a numerical analysis of the Field-Noyes\nreaction-diffusion model with nonsmooth initial data, employing a linear\nGalerkin finite element method for spatial discretization and a second-order\nexponential Runge-Kutta scheme for temporal integration. The initial data are\nassumed to reside in the fractional Sobolev space H^gamma with 0 < gamma < 2,\nwhere classical regularity conditions are violated, necessitating specialized\nerror analysis. By integrating semigroup techniques and fractional Sobolev\nspace theory, sharp fully discrete error estimates are derived in both L2 and\nH1 norms. This demonstrates that the convergence order adapts to the smoothness\nof initial data, a key advancement over traditional approaches that assume\nhigher regularity. Numerical examples are provided to support the theoretical\nanalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15345v2", "cate": "math.NA", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.15773", "title": "Supernova: Achieving More with Less in Transformer Architectures", "authors": ["Andrei-Valentin Tanase", "Elena Pelican"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15773v2", "summary": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 35% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15773v2", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.15470", "title": "FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning", "authors": ["Baran Can Gl", "Suraksha Nadig", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint version. Accepted for publication at IEEE ICECCME 2025", "url": "http://arxiv.org/abs/2507.15470v2", "summary": "In-vehicle emotion recognition underpins adaptive driver-assistance systems\nand, ultimately, occupant safety. However, practical deployment is hindered by\n(i) modality fragility - poor lighting and occlusions degrade vision-based\nmethods; (ii) physiological variability - heart-rate and skin-conductance\npatterns differ across individuals; and (iii) privacy risk - centralized\ntraining requires transmission of sensitive data. To address these challenges,\nwe present FedMultiEmo, a privacy-preserving framework that fuses two\ncomplementary modalities at the decision level: visual features extracted by a\nConvolutional Neural Network from facial images, and physiological cues (heart\nrate, electrodermal activity, and skin temperature) classified by a Random\nForest. FedMultiEmo builds on three key elements: (1) a multimodal federated\nlearning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud\nprototype on Raspberry Pi clients and a Flower server, and (3) a personalized\nFederated Averaging scheme that weights client updates by local data volume.\nEvaluated on FER2013 and a custom physiological dataset, the federated\nConvolutional Neural Network attains 77% accuracy, the Random Forest 74%, and\ntheir fusion 87%, matching a centralized baseline while keeping all raw data\nlocal. The developed system converges in 18 rounds, with an average round time\nof 120 seconds and a per-client memory footprint below 200 MB. These results\nindicate that FedMultiEmo offers a practical approach to real-time,\nprivacy-aware emotion recognition in automotive settings.", "comment": "Preprint version. Accepted for publication at IEEE ICECCME 2025", "pdf_url": "http://arxiv.org/pdf/2507.15470v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2506.05328", "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs", "authors": ["Lidong Lu", "Guo Chen", "Zhiqi Li", "Yicheng Liu", "Tong Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 11 figures", "url": "http://arxiv.org/abs/2506.05328v2", "summary": "Despite progress in video understanding, current MLLMs struggle with counting\ntasks. Existing benchmarks are limited by short videos, close-set queries, lack\nof clue annotations, and weak multimodal coverage. In this paper, we introduce\nCG-AV-Counting, a manually-annotated clue-grounded counting benchmark with\n1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It\nsupports both black-box and white-box evaluation, serving as a comprehensive\ntestbed for both end-to-end and reasoning-based counting. To explore ways to\nimprove model's counting capability, we propose AV-Reasoner, a model trained\nwith GRPO and curriculum learning to generalize counting ability from related\ntasks. AV-Reasoner achieves state-of-the-art results across multiple\nbenchmarks, demonstrating the effectiveness of reinforcement learning. However,\nexperiments show that on out-of-domain benchmarks, reasoning in the language\nspace fails to bring performance gains. The code and benchmark have been\nreleased on https://av-reasoner.github.io.", "comment": "21 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.05328v2", "cate": "cs.CV", "date": "2025-06-05", "updated": "2025-07-22"}
{"id": "2507.14664", "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in Progress. All artifacts in this papers: this https URL", "url": "http://arxiv.org/abs/2507.14664v2", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "comment": "Work in Progress. All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "pdf_url": "http://arxiv.org/pdf/2507.14664v2", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-22"}
{"id": "2201.05355", "title": "Port-Hamiltonian Realizations of Nonminimal Linear Time Invariant Systems", "authors": ["Christopher Beattie", "Volker Mehrmann", "Hongguo Xu"], "categories": ["math.OC", "cs.NA", "math.NA", "93A30, 93B17, 93B11"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2201.05355v3", "summary": "Numerical methods for developing port-Hamiltonian representations of general\nlinear time-invariant systems are studied. The approach extends previous\nport-Hamiltonian characterizations to include the general non-minimal case and\nthe case where the feedthrough term fails to have an invertible symmetric part.\nThe resulting construction is able to identify infeasibility when the system\nfails to be port-Hamiltonian, and allows for the incorporation of perturbations\nin order to arrive at a nearby port-Hamiltonian system. Results are illustrated\nvia numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2201.05355v3", "cate": "math.OC", "date": "2022-01-14", "updated": "2025-07-21"}
{"id": "2507.15852", "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction", "authors": ["Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Songxin He", "Jianfan Lin", "Junsong Tang", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      project page: this https URL ; code: this https URL ; dataset: this https URL", "url": "http://arxiv.org/abs/2507.15852v2", "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.", "comment": "project page: https://rookiexiong7.github.io/projects/SeC/ ; code:\n  https://github.com/OpenIXCLab/SeC ; dataset:\n  https://huggingface.co/datasets/OpenIXCLab/SeCVOS", "pdf_url": "http://arxiv.org/pdf/2507.15852v2", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2402.12668", "title": "Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests", "authors": ["Brian Liu", "Rahul Mazumder"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.12668v4", "summary": "We study the often overlooked phenomenon, first noted in\n\\cite{breiman2001random}, that random forests appear to reduce bias compared to\nbagging. Motivated by an interesting paper by \\cite{mentch2020randomization},\nwhere the authors explain the success of random forests in low signal-to-noise\nratio (SNR) settings through regularization, we explore how random forests can\ncapture patterns in the data that bagging ensembles fail to capture. We\nempirically demonstrate that in the presence of such patterns, random forests\nreduce bias along with variance and can increasingly outperform bagging\nensembles when SNR is high. Our observations offer insights into the real-world\nsuccess of random forests across a range of SNRs and enhance our understanding\nof the difference between random forests and bagging ensembles. Our\ninvestigations also yield practical insights into the importance of tuning\n$mtry$ in random forests.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.12668v4", "cate": "stat.ML", "date": "2024-02-20", "updated": "2025-07-21"}
{"id": "2506.21188", "title": "GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding", "authors": ["Zijun Lin", "Shuting He", "Cheston Tan", "Bihan Wen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21188v2", "summary": "Sequential grounding in 3D point clouds (SG3D) refers to locating sequences\nof objects by following text instructions for a daily activity with detailed\nsteps. Current 3D visual grounding (3DVG) methods treat text instructions with\nmultiple steps as a whole, without extracting useful temporal information from\neach step. However, the instructions in SG3D often contain pronouns such as\n\"it\", \"here\" and \"the same\" to make language expressions concise. This requires\ngrounding methods to understand the context and retrieve relevant information\nfrom previous steps to correctly locate object sequences. Due to the lack of an\neffective module for collecting related historical information,\nstate-of-the-art 3DVG methods face significant challenges in adapting to the\nSG3D task. To fill this gap, we propose GroundFlow -- a plug-in module for\ntemporal reasoning on 3D point cloud sequential grounding. Firstly, we\ndemonstrate that integrating GroundFlow improves the task accuracy of 3DVG\nbaseline methods by a large margin (+7.5\\% and +10.2\\%) in the SG3D benchmark,\neven outperforming a 3D large language model pre-trained on various datasets.\nFurthermore, we selectively extract both short-term and long-term step\ninformation based on its relevance to the current instruction, enabling\nGroundFlow to take a comprehensive view of historical information and maintain\nits temporal understanding advantage as step counts increase. Overall, our work\nintroduces temporal reasoning capabilities to existing 3DVG models and achieves\nstate-of-the-art performance in the SG3D benchmark across five datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21188v2", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-22"}
{"id": "2507.15007", "title": "Hear Your Code Fail, Voice-Assisted Debugging for Python", "authors": ["Sayed Mahbub Hasan Amiri", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Mohammad Shawkat Ali Mamun", "Sk. Humaun Kabir", "Naznin Akter"], "categories": ["cs.PL", "cs.CL"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      35 pages, 20 figures", "url": "http://arxiv.org/abs/2507.15007v2", "summary": "This research introduces an innovative voice-assisted debugging plugin for\nPython that transforms silent runtime errors into actionable audible\ndiagnostics. By implementing a global exception hook architecture with pyttsx3\ntext-to-speech conversion and Tkinter-based GUI visualization, the solution\ndelivers multimodal error feedback through parallel auditory and visual\nchannels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,\nn=50) compared to traditional stack-trace debugging, while enabling 78% faster\nerror identification through vocalized exception classification and\ncontextualization. The system achieves sub-1.2 second voice latency with under\n18% CPU overhead during exception handling, vocalizing error types and\nconsequences while displaying interactive tracebacks with documentation deep\nlinks. Criteria validate compatibility across Python 3.7+ environments on\nWindows, macOS, and Linux platforms. Needing only two lines of integration\ncode, the plugin significantly boosts availability for aesthetically impaired\ndesigners and supports multitasking workflows through hands-free error medical\ndiagnosis. Educational applications show particular promise, with pilot studies\nindicating 45% faster debugging skill acquisition among novice programmers.\nFuture development will incorporate GPT-based repair suggestions and real-time\nmultilingual translation to further advance auditory debugging paradigms. The\nsolution represents a fundamental shift toward human-centric error diagnostics,\nbridging critical gaps in programming accessibility while establishing new\nstandards for cognitive efficiency in software development workflows.", "comment": "35 pages, 20 figures", "pdf_url": "http://arxiv.org/pdf/2507.15007v2", "cate": "cs.PL", "date": "2025-07-20", "updated": "2025-07-22"}
{"id": "2312.15421", "title": "How averaged is the projection?", "authors": ["Shuang Song"], "categories": ["math.FA", "cs.NA", "math.NA", "math.OC"], "primary_category": "Subjects:       Functional Analysis (math.FA)", "pdf_link": null, "comments": "Comments:      This manuscript is not yet mature. A significantly revised and more complete version will be uploaded at a later date", "url": "http://arxiv.org/abs/2312.15421v2", "summary": "Projection operators are important in Analysis, Optimization and Algorithm.\nIt is well known that these operators are firmly nonexpansive. In this paper,\nwe provide an exact result that sharpens this well-known result. We develop the\ntheory of averaged operators and provide a lower bound. We give a result on the\navergedness of operator compositions. We also provide some nonlinear examples\nto illustrate our results.", "comment": "This manuscript is not yet mature. A significantly revised and more\n  complete version will be uploaded at a later date", "pdf_url": "http://arxiv.org/pdf/2312.15421v2", "cate": "math.FA", "date": "2023-12-24", "updated": "2025-07-22"}
{"id": "2408.04607", "title": "Risk and cross validation in ridge regression with correlated samples", "authors": ["Alexander Atanasov", "Jacob A. Zavatone-Veth", "Cengiz Pehlevan"], "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      44 pages, 19 figures. v4: ICML 2025 camera-ready. v5: Fix typo in statement of Theorem 5", "url": "http://arxiv.org/abs/2408.04607v5", "summary": "Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging techniques from random matrix theory\nand free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.", "comment": "44 pages, 19 figures. v4: ICML 2025 camera-ready. v5: Fix typo in\n  statement of Theorem 5", "pdf_url": "http://arxiv.org/pdf/2408.04607v5", "cate": "stat.ML", "date": "2024-08-08", "updated": "2025-07-22"}
{"id": "2506.21401", "title": "Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction", "authors": ["Zhirui Gao", "Renjiao Yi", "Yaqiao Dai", "Xuening Zhu", "Wei Chen", "Chenyang Zhu", "Kai Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025, Code: this https URL", "url": "http://arxiv.org/abs/2506.21401v3", "summary": "This paper presents an end-to-end framework for reconstructing 3D parametric\ncurves directly from multi-view edge maps. Contrasting with existing two-stage\nmethods that follow a sequential ``edge point cloud reconstruction and\nparametric curve fitting'' pipeline, our one-stage approach optimizes 3D\nparametric curves directly from 2D edge maps, eliminating error accumulation\ncaused by the inherent optimization gap between disconnected stages. However,\nparametric curves inherently lack suitability for rendering-based multi-view\noptimization, necessitating a complementary representation that preserves their\ngeometric properties while enabling differentiable rendering. We propose a\nnovel bi-directional coupling mechanism between parametric curves and\nedge-oriented Gaussian components. This tight correspondence formulates a\ncurve-aware Gaussian representation, \\textbf{CurveGaussian}, that enables\ndifferentiable rendering of 3D curves, allowing direct optimization guided by\nmulti-view evidence. Furthermore, we introduce a dynamically adaptive topology\noptimization framework during training to refine curve structures through\nlinearization, merging, splitting, and pruning operations. Comprehensive\nevaluations on the ABC dataset and real-world benchmarks demonstrate our\none-stage method's superiority over two-stage alternatives, particularly in\nproducing cleaner and more robust reconstructions. Additionally, by directly\noptimizing parametric curves, our method significantly reduces the parameter\ncount during training, achieving both higher efficiency and superior\nperformance compared to existing approaches.", "comment": "Accepted by ICCV 2025, Code:\n  https://github.com/zhirui-gao/Curve-Gaussian", "pdf_url": "http://arxiv.org/pdf/2506.21401v3", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-22"}
{"id": "2410.19393", "title": "On low frequency inference for diffusions without the hot spots conjecture", "authors": ["Giovanni S. Alberti", "Douglas Barnes", "Aditya Jambhale", "Richard Nickl"], "categories": ["math.ST", "cs.NA", "math.AP", "math.NA", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "Comments:      To appear in Mathematical Statistics and Learning", "url": "http://arxiv.org/abs/2410.19393v2", "summary": "We remove the dependence on the `hot-spots' conjecture in two of the main\ntheorems of the recent paper of Nickl (2024, Annals of Statistics).\nSpecifically, we characterise the minimax convergence rates for estimation of\nthe transition operator $P_{f}$ arising from the Neumann Laplacian with\ndiffusion coefficient $f$ on arbitrary convex domains with smooth boundary, and\nfurther show that a general Lipschitz stability estimate holds for the inverse\nmap $P_f\\mapsto f$ from $H^2\\to H^2$ to $L^1$.", "comment": "To appear in Mathematical Statistics and Learning", "pdf_url": "http://arxiv.org/pdf/2410.19393v2", "cate": "math.ST", "date": "2024-10-25", "updated": "2025-07-22"}
{"id": "2503.00443", "title": "Stable and Accurate Orbital-Free DFT Powered by Machine Learning", "authors": ["Roman Remme", "Tobias Kaczun", "Tim Ebert", "Christof A. Gehrig", "Dominik Geng", "Gerrit Gerhartz", "Marc K. Ickler", "Manuel V. Klockow", "Peter Lippmann", "Johannes S. Schmidt", "Simon Wagner", "Andreas Dreuw", "Fred A. Hamprecht"], "categories": ["physics.chem-ph", "cs.LG"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.00443v2", "summary": "Hohenberg and Kohn have proven that the electronic energy and the\none-particle electron density can, in principle, be obtained by minimizing an\nenergy functional with respect to the density. While decades of theoretical\nwork have produced increasingly faithful approximations to this elusive exact\nenergy functional, their accuracy is still insufficient for many applications,\nmaking it reasonable to try and learn it empirically. Using rotationally\nequivariant atomistic machine learning, we obtain for the first time a density\nfunctional that, when applied to the organic molecules in QM9, yields energies\nwith chemical accuracy relative to the Kohn-Sham reference while also\nconverging to meaningful electron densities. Augmenting the training data with\ndensities obtained from perturbed potentials proved key to these advances. This\nwork demonstrates that machine learning can play a crucial role in narrowing\nthe gap between theory and the practical realization of Hohenberg and Kohn's\nvision, paving the way for more efficient calculations in large molecular\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.00443v2", "cate": "physics.chem-ph", "date": "2025-03-01", "updated": "2025-07-22"}
{"id": "2506.21980", "title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning", "authors": ["Biao Wang", "Wenwen Li", "Jiawei Ge"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 2 figures", "url": "http://arxiv.org/abs/2506.21980v3", "summary": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.", "comment": "7 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21980v3", "cate": "cs.CV", "date": "2025-06-27", "updated": "2025-07-22"}
{"id": "2501.00171", "title": "On the Minimal Denominator Problem in Function Fields", "authors": ["Noy Soffer Aranov"], "categories": ["math.NT", "cs.NA", "math.NA", "11J61, 11J04, 11J13"], "primary_category": "Subjects:       Number Theory (math.NT)", "pdf_link": null, "comments": "Comments:      Minor errors corrected", "url": "http://arxiv.org/abs/2501.00171v2", "summary": "We study the minimal denominator problem in function fields. In particular,\nwe compute the probability distribution function of the the random variable\nwhich returns the degree of the smallest denominator $Q$, for which the ball of\na fixed radius around a point contains a rational function of the form\n$\\frac{P}{Q}$. Moreover, we discuss the distribution of the random variable\nwhich returns the denominator of minimal degree, as well as higher dimensional\nand $P$-adic generalizations. This can be viewed as a function field\ngeneralization of a paper by Chen and Haynes.", "comment": "Minor errors corrected", "pdf_url": "http://arxiv.org/pdf/2501.00171v2", "cate": "math.NT", "date": "2024-12-30", "updated": "2025-07-21"}
{"id": "2504.12625", "title": "Spectral Algorithms under Covariate Shift", "authors": ["Jun Fan", "Zheng-Chu Guo", "Lei Shi"], "categories": ["stat.ML", "cs.LG", "68Q32, 68T05, 62J02"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12625v2", "summary": "Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under covariate shift. In this setting, the marginal\ndistributions of the input data differ between the training and test datasets,\nwhile the conditional distribution of the output given the input remains\nunchanged. Within a non-parametric regression framework over a reproducing\nkernel Hilbert space, we analyze the convergence rates of spectral algorithms\nunder covariate shift and show that they achieve minimax optimality when the\ndensity ratios between the training and test distributions are uniformly\nbounded. However, when these density ratios are unbounded, the spectral\nalgorithms may become suboptimal. To address this issue, we propose a novel\nweighted spectral algorithm with normalized weights that incorporates density\nratio information into the learning process. Our theoretical analysis shows\nthat this normalized weighted approach achieves optimal capacity-independent\nconvergence rates, but the rates will suffer from the saturation phenomenon.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm with clipped weights\ncan approach the optimal capacity-dependent convergence rates arbitrarily\nclosely. This improvement resolves the suboptimality issue in unbounded density\nratio scenarios and advances the state-of-the-art by refining existing\ntheoretical results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12625v2", "cate": "stat.ML", "date": "2025-04-17", "updated": "2025-07-22"}
{"id": "2506.22139", "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs", "authors": ["Shaojie Zhang", "Jiahui Yang", "Jianqin Yin", "Zhenbo Luo", "Jian Luan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2506.22139v3", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22139v3", "cate": "cs.CV", "date": "2025-06-27", "updated": "2025-07-22"}
{"id": "2507.13492", "title": "On the time integration for phase field modeling of grain growth in additive manufacturing", "authors": ["Chaoqian Yuan", "Chinnapat Panwisawas", "Ye Lu"], "categories": ["physics.comp-ph", "cs.NA", "math.NA"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13492v2", "summary": "Phase field simulations play a key role in the understanding of\nmicrostructure evolution in additive manufacturing. However, they have been\nfound extremely computationally expensive. One of the reasons is the small time\nstep requirement to resolve the complex microstructure evolution during the\nrapid solidification process. This paper investigates the possibility of using\na class of stabilized time integration algorithms to accelerate such phase\nfield simulations by increasing the time steps. The specific time integration\nformulation and theoretical analysis on energy stability were developed, based\non a phase field model dedicated to simulating rapid solidification in additive\nmanufacturing. The numerical results confirmed that the proposed method can\nensure the numerical stability and a decreasing energy requirement for the\nphase field simulations with at least two orders-of-magnitude larger time steps\nover conventional explicit methods. 2D and 3D phase field simulations have been\nconducted with relevant physical and kinetic parameters for 316L stainless\nsteels. This work provides a numerical framework for efficient phase field\nsimulations and open numerous opportunities for large scale phase field\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13492v2", "cate": "physics.comp-ph", "date": "2025-07-17", "updated": "2025-07-22"}
{"id": "2504.15993", "title": "Benchmarking machine learning models for predicting aerofoil performance", "authors": ["Oliver Summerell", "Gerardo Aragon-Camarasa", "Stephanie Ordonez Sanchez"], "categories": ["physics.flu-dyn", "cs.LG"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "Comments:      9 pages, 10 figures, submitted to EWTEC", "url": "http://arxiv.org/abs/2504.15993v2", "summary": "This paper investigates the capability of Neural Networks (NNs) as\nalternatives to the traditional methods to analyse the performance of aerofoils\nused in the wind and tidal energy industry. The current methods used to assess\nthe characteristic lift and drag coefficients include Computational Fluid\nDynamics (CFD), thin aerofoil and panel methods, all face trade-offs between\ncomputational speed and the accuracy of the results and as such NNs have been\ninvestigated as an alternative with the aim that it would perform both quickly\nand accurately. As such, this paper provides a benchmark for the windAI_bench\ndataset published by the National Renewable Energy Laboratory (NREL) in the\nUSA. In order to validate the methodology of the benchmarking, the\nAirfRANSdataset benchmark is used as both a starting point and a point of\ncomparison. This study evaluates four neural networks (MLP, PointNet,\nGraphSAGE, GUNet) trained on a range of aerofoils at 25 angles of attack\n(4$^\\circ$ to 20$^\\circ$) to predict fluid flow and calculate lift coefficients\n($C_L$) via the panel method. GraphSAGE and GUNet performed well during the\ntraining phase, but underperformed during testing. Accordingly, this paper has\nidentified PointNet and MLP as the two strongest models tested, however whilst\nthe results from MLP are more commonly correct for predicting the behaviour of\nthe fluid, the results from PointNet provide the more accurate results for\ncalculating $C_L$.", "comment": "9 pages, 10 figures, submitted to EWTEC", "pdf_url": "http://arxiv.org/pdf/2504.15993v2", "cate": "physics.flu-dyn", "date": "2025-04-22", "updated": "2025-07-22"}
{"id": "2506.23468", "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments", "authors": ["Xuan Yao", "Junyu Gao", "Changsheng Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2506.23468v2", "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to execute sequential navigation actions in complex environments guided\nby natural language instructions. Current approaches often struggle with\ngeneralizing to novel environments and adapting to ongoing changes during\nnavigation. Inspired by human cognition, we present NavMorph, a self-evolving\nworld model framework that enhances environmental understanding and\ndecision-making in VLN-CE tasks. NavMorph employs compact latent\nrepresentations to model environmental dynamics, equipping agents with\nforesight for adaptive planning and policy refinement. By integrating a novel\nContextual Evolution Memory, NavMorph leverages scene-contextual information to\nsupport effective navigation while maintaining online adaptability. Extensive\nexperiments demonstrate that our method achieves notable performance\nimprovements on popular VLN-CE benchmarks. Code is available at\nhttps://github.com/Feliciaxyao/NavMorph.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23468v2", "cate": "cs.CV", "date": "2025-06-30", "updated": "2025-07-22"}
{"id": "2505.07769", "title": "Tagging fully hadronic exotic decays of the vectorlike $\\mathbf{B}$ quark using a graph neural network", "authors": ["Jai Bardhan", "Tanumoy Mandal", "Subhadip Mitra", "Cyrin Neeraj", "Mihir Rawat"], "categories": ["hep-ph", "cs.LG", "hep-ex"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "Comments:      13 pages, 10 figures, 3 tables", "url": "http://arxiv.org/abs/2505.07769v2", "summary": "Following up on our earlier study in [J. Bardhan et al., Machine\nlearning-enhanced search for a vectorlike singlet B quark decaying to a singlet\nscalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], we\ninvestigate the LHC prospects of pair-produced vectorlike $B$ quarks decaying\nexotically to a new gauge-singlet (pseudo)scalar field $\\Phi$ and a $b$ quark.\nAfter the electroweak symmetry breaking, the $\\Phi$ decays predominantly to\n$gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature.\nBecause of the large Standard Model background and the lack of leptonic\nhandles, it is a difficult channel to probe. To overcome the challenge, we\nemploy a hybrid deep learning model containing a graph neural network followed\nby a deep neural network. We estimate that such a state-of-the-art deep\nlearning analysis pipeline can lead to a performance comparable to that in the\nsemi-leptonic mode, taking the discovery (exclusion) reach up to about\n$M_B=1.8\\:(2.4)$ TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B\n\\to b\\Phi) = 100\\%$.", "comment": "13 pages, 10 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2505.07769v2", "cate": "hep-ph", "date": "2025-05-12", "updated": "2025-07-22"}
{"id": "2507.01756", "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis", "authors": ["Peng Zheng", "Junke Wang", "Yi Chang", "Yizhou Yu", "Rui Ma", "Zuxuan Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      iccv 2025, camera-ready version", "url": "http://arxiv.org/abs/2507.01756v2", "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.\nProject page: https://pengzheng0707.github.io/DisCon.", "comment": "iccv 2025, camera-ready version", "pdf_url": "http://arxiv.org/pdf/2507.01756v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-22"}
{"id": "2505.16320", "title": "Learning novel representations of variable sources from multi-modal $\\textit{Gaia}$ data via autoencoders", "authors": ["P. Huijse", "J. De Ridder", "L. Eyer", "L. Rimoldini", "B. Holl", "N. Chornay", "J. Roquette", "K. Nienartowicz", "G. Jevardat de Fombelle", "D. J. Fritzewski", "A. Kemp", "V. Vanlaer", "M. Vanrespaille", "H. Wang", "M. I. Carnerero", "C. M. Raiteri", "G. Marton", "M. Madarsz", "G. Clementini", "P. Gavras", "C. Aerts"], "categories": ["astro-ph.IM", "cs.LG"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      Manuscript accepted on Astronomy & Astrophysics, 20 pages, 20 figures, 2 tables", "url": "http://arxiv.org/abs/2505.16320v2", "summary": "Gaia Data Release 3 (DR3) published for the first time epoch photometry,\nBP/RP (XP) low-resolution mean spectra, and supervised classification results\nfor millions of variable sources. This extensive dataset offers a unique\nopportunity to study their variability by combining multiple Gaia data\nproducts. In preparation for DR4, we propose and evaluate a machine learning\nmethodology capable of ingesting multiple Gaia data products to achieve an\nunsupervised classification of stellar and quasar variability. A dataset of 4\nmillion Gaia DR3 sources is used to train three variational autoencoders (VAE),\nwhich are artificial neural networks (ANNs) designed for data compression and\ngeneration. One VAE is trained on Gaia XP low-resolution spectra, another on a\nnovel approach based on the distribution of magnitude differences in the Gaia G\nband, and the third on folded Gaia G band light curves. Each Gaia source is\ncompressed into 15 numbers, representing the coordinates in a 15-dimensional\nlatent space generated by combining the outputs of these three models. The\nlearned latent representation produced by the ANN effectively distinguishes\nbetween the main variability classes present in Gaia DR3, as demonstrated\nthrough both supervised and unsupervised classification analysis of the latent\nspace. The results highlight a strong synergy between light curves and\nlow-resolution spectral data, emphasising the benefits of combining the\ndifferent Gaia data products. A two-dimensional projection of the latent\nvariables reveals numerous overdensities, most of which strongly correlate with\nastrophysical properties, showing the potential of this latent space for\nastrophysical discovery. We show that the properties of our novel latent\nrepresentation make it highly valuable for variability analysis tasks,\nincluding classification, clustering and outlier detection.", "comment": "Manuscript accepted on Astronomy & Astrophysics, 20 pages, 20\n  figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2505.16320v2", "cate": "astro-ph.IM", "date": "2025-05-22", "updated": "2025-07-22"}
{"id": "2507.04107", "title": "VICI: VLM-Instructed Cross-view Image-localisation", "authors": ["Xiaohan Zhang", "Tavis Shore", "Chen Chen", "Oscar Mendez", "Simon Hadfield", "Safwan Wshah"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04107v2", "summary": "In this paper, we present a high-performing solution to the UAVM 2025\nChallenge, which focuses on matching narrow FOV street-level images to\ncorresponding satellite imagery using the University-1652 dataset. As panoramic\nCross-View Geo-Localisation nears peak performance, it becomes increasingly\nimportant to explore more practical problem formulations. Real-world scenarios\nrarely offer panoramic street-level queries; instead, queries typically consist\nof limited-FOV images captured with unknown camera parameters. Our work\nprioritises discovering the highest achievable performance under these\nconstraints, pushing the limits of existing architectures. Our method begins by\nretrieving candidate satellite image embeddings for a given query, followed by\na re-ranking stage that selectively enhances retrieval accuracy within the top\ncandidates. This two-stage approach enables more precise matching, even under\nthe significant viewpoint and scale variations inherent in the task. Through\nexperimentation, we demonstrate that our approach achieves competitive results\n-specifically attaining R@1 and R@10 retrieval rates of \\topone\\% and \\topten\\%\nrespectively. This underscores the potential of optimised retrieval and\nre-ranking strategies in advancing practical geo-localisation performance. Code\nis available at https://github.com/tavisshore/VICI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04107v2", "cate": "cs.CV", "date": "2025-07-05", "updated": "2025-07-22"}
{"id": "2506.03199", "title": "Quantum Cognition Machine Learning for Forecasting Chromosomal Instability", "authors": ["Giuseppe Di Caro", "Vahagn Kirakosyan", "Alexander G. Abanov", "Jerome R. Busemeyer", "Luca Candelori", "Nadine Hartmann", "Ernest T. Lam", "Kharen Musaelian", "Ryan Samson", "Harold Steinacker", "Dario Villani", "Martin T. Wells", "Richard J. Wenstrup", "Mengjia Xu"], "categories": ["q-bio.QM", "cs.LG", "quant-ph"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.03199v2", "summary": "The accurate prediction of chromosomal instability from the morphology of\ncirculating tumor cells (CTCs) enables real-time detection of CTCs with high\nmetastatic potential in the context of liquid biopsy diagnostics. However, it\npresents a significant challenge due to the high dimensionality and complexity\nof single-cell digital pathology data. Here, we introduce the application of\nQuantum Cognition Machine Learning (QCML), a quantum-inspired computational\nframework, to estimate morphology-predicted chromosomal instability in CTCs\nfrom patients with metastatic breast cancer. QCML leverages quantum mechanical\nprinciples to represent data as state vectors in a Hilbert space, enabling\ncontext-aware feature modeling, dimensionality reduction, and enhanced\ngeneralization without requiring curated feature selection. QCML outperforms\nconventional machine learning methods when tested on out of sample verification\nCTCs, achieving higher accuracy in identifying predicted large-scale state\ntransitions (pLST) status from CTC-derived morphology features. These\npreliminary findings support the application of QCML as a novel machine\nlearning tool with superior performance in high-dimensional, low-sample-size\nbiomedical contexts. QCML enables the simulation of cognition-like learning for\nthe identification of biologically meaningful prediction of chromosomal\ninstability from CTC morphology, offering a novel tool for CTC classification\nin liquid biopsy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.03199v2", "cate": "q-bio.QM", "date": "2025-06-02", "updated": "2025-07-22"}
{"id": "2507.07154", "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation", "authors": ["Desheng Li", "Chaoliang Liu", "Zhiyong Xiao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07154v2", "summary": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks like classification to improve segmentation. However, these methods often\nneed more labeled data and depend on task similarity, potentially limiting\ngeneralizability. To address these challenges, we propose CL-Polyp, a\ncontrastive learning-enhanced polyp segmentation network. Our method uses\ncontrastive learning to enhance the encoder's extraction of discriminative\nfeatures by contrasting positive and negative sample pairs from polyp images.\nThis self-supervised strategy improves visual representation without needing\nadditional annotations. We also introduce two efficient, lightweight modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for improved\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to merge low-level and upsampled features for {enhanced} boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-show that CL-Polyp consistently\nsurpasses state-of-the-art methods. Specifically, it enhances the IoU metric by\n0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets, respectively,\ndemonstrating its effectiveness in clinical polyp segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07154v2", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-22"}
{"id": "2507.13580", "title": "A Collaborative Framework Integrating Large Language Model and Chemical Fragment Space: Mutual Inspiration for Lead Design", "authors": ["Hao Tuo", "Yan Li", "Xuanning Hu", "Haishi Zhao", "Xueyan Liu", "Bo Yang"], "categories": ["q-bio.BM", "cs.LG"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13580v2", "summary": "Combinatorial optimization algorithm is essential in computer-aided drug\ndesign by progressively exploring chemical space to design lead compounds with\nhigh affinity to target protein. However current methods face inherent\nchallenges in integrating domain knowledge, limiting their performance in\nidentifying lead compounds with novel and valid binding mode. Here, we propose\nAutoLeadDesign, a lead compounds design framework that inspires extensive\ndomain knowledge encoded in large language models with chemical fragments to\nprogressively implement efficient exploration of vast chemical space. The\ncomprehensive experiments indicate that AutoLeadDesign outperforms baseline\nmethods. Significantly, empirical lead design campaigns targeting two\nclinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate\nAutoLeadDesign's competence in de novo generation of lead compounds achieving\nexpert-competitive design efficacy. Structural analysis further confirms their\nmechanism-validated inhibitory patterns. By tracing the process of design, we\nfind that AutoLeadDesign shares analogous mechanisms with fragment-based drug\ndesign which traditionally rely on the expert decision-making, further\nrevealing why it works. Overall, AutoLeadDesign offers an efficient approach\nfor lead compounds design, suggesting its potential utility in drug design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13580v2", "cate": "q-bio.BM", "date": "2025-07-17", "updated": "2025-07-22"}
{"id": "2507.08716", "title": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One Engine", "authors": ["Kongwu Huang", "Shiyi Mu", "Jun Jiang", "Yuan Gao", "Shugong Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08716v2", "summary": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset will be made available at:\nhttps://github.com/hkw-xg/Great-MCD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08716v2", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-22"}
{"id": "2507.15264", "title": "On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem", "authors": ["Kuangyu Ding", "Kim-Chuan Toh"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      34 Pages", "url": "http://arxiv.org/abs/2507.15264v2", "summary": "We study a nonsmooth nonconvex optimization problem defined over nonconvex\nconstraints, where the feasible set is given by the intersection of the closure\nof an open set and a smooth manifold. By endowing the open set with a\nRiemannian metric induced by a barrier function, we obtain a Riemannian\nsubgradient flow formulated as a differential inclusion, which remains strictly\nwithin the interior of the feasible set. This continuous dynamical system\nunifies two classes of iterative optimization methods, namely the Hessian\nbarrier method and mirror descent scheme, by revealing that these methods can\nbe interpreted as discrete approximations of the continuous flow. We explore\nthe long-term behavior of the trajectories generated by this dynamical system\nand show that the existing deficient convergence properties of the Hessian\nbarrier and mirror descent scheme can be unifily and more insightfully\ninterpreted through these of the continuous trajectory. For instance, the\nnotorious spurious stationary points \\cite{chen2024spurious} observed in\nHessian barrier method and mirror descent scheme are interpreted as stable\nequilibria of the dynamical system that do not correspond to real stationary\npoints of the original optimization problem. We provide two sufficient\ncondition such that these spurious stationary points can be avoided if the\nstrict complementarity conditions holds. In the absence of these regularity\ncondition, we propose a random perturbation strategy that ensures the\ntrajectory converges (subsequentially) to an approximate stationary point.\nBuilding on these insights, we introduce two iterative Riemannian subgradient\nmethods, form of interior point methods, that generalizes the existing Hessian\nbarrier method and mirror descent scheme for solving nonsmooth nonconvex\noptimization problems.", "comment": "34 Pages", "pdf_url": "http://arxiv.org/pdf/2507.15264v2", "cate": "math.OC", "date": "2025-07-21", "updated": "2025-07-22"}
{"id": "2507.09577", "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "authors": ["Ming Yin", "Fu Wang", "Xujiong Ye", "Yanda Meng", "Zeyu Fu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in MICCAI 2025", "url": "http://arxiv.org/abs/2507.09577v2", "summary": "Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.", "comment": "Accepted in MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.09577v2", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-22"}
{"id": "2507.09815", "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding", "authors": ["Younggun Kim", "Ahmed S. Abdelrahman", "Mohamed Abdel-Aty"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      22 pages, 11 figures, 5 tables", "url": "http://arxiv.org/abs/2507.09815v2", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.", "comment": "22 pages, 11 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.09815v2", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-22"}
{"id": "2507.10978", "title": "Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction", "authors": ["Ayush Gupta", "Siyuan Huang", "Rama Chellappa"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at IJCB 2025", "url": "http://arxiv.org/abs/2507.10978v2", "summary": "Gait is becoming popular as a method of person re-identification because of\nits ability to identify people at a distance. However, most current works in\ngait recognition do not address the practical problem of occlusions. Among\nthose which do, some require paired tuples of occluded and holistic sequences,\nwhich are impractical to collect in the real world. Further, these approaches\nwork on occlusions but fail to retain performance on holistic inputs. To\naddress these challenges, we propose RG-Gait, a method for residual correction\nfor occluded gait recognition with holistic retention. We model the problem as\na residual learning task, conceptualizing the occluded gait signature as a\nresidual deviation from the holistic gait representation. Our proposed network\nadaptively integrates the learned residual, significantly improving performance\non occluded gait sequences without compromising the holistic recognition\naccuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR\ndatasets and show that learning the residual can be an effective technique to\ntackle occluded gait recognition with holistic retention. We release our code\npublicly at https://github.com/Ayush-00/rg-gait.", "comment": "Accepted at IJCB 2025", "pdf_url": "http://arxiv.org/pdf/2507.10978v2", "cate": "cs.CV", "date": "2025-07-15", "updated": "2025-07-21"}
{"id": "2507.15680", "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "authors": ["Yongkang Hou", "Jiarun Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15680v2", "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15680v2", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-22"}
