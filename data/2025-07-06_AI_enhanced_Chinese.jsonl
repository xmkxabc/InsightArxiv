{"id": "2507.02057", "title": "MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation", "authors": ["Lu Yan", "Zhuo Zhang", "Xiangzhe Xu", "Shengwei An", "Guangyu Shen", "Zhou Xuan", "Xuan Chen", "Xiangyu Zhang"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02057v1", "summary": "Large language models (LLMs) have democratized software development, reducing\nthe expertise barrier for programming complex applications. This accessibility\nextends to malicious software development, raising significant security\nconcerns. While LLM providers have implemented alignment mechanisms to prevent\ndirect generation of overtly malicious code, these safeguards predominantly\nevaluate individual prompts in isolation, overlooking a critical vulnerability:\nmalicious operations can be systematically decomposed into benign-appearing\nsub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a\nnovel framework that leverages this vulnerability through modular decomposition\nand alignment-evasive generation. MGC employs a specialized Malware Description\nIntermediate Representation (MDIR) to bridge high-level malicious intents and\nbenign-appearing code snippets. Extensive evaluation demonstrates that our\nattack reliably generates functional malware across diverse task specifications\nand categories, outperforming jailbreaking methods by +365.79% and underground\nservices by +78.07% in correctness on three benchmark datasets. Case studies\nfurther show that MGC can reproduce and even enhance 16 real-world malware\nsamples. This work provides critical insights for security researchers by\nexposing the risks of compositional attacks against aligned AI systems.\nDemonstrations are available at\nhttps://sites.google.com/view/malware-generation-compiler.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02057v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "MGC：一个利用对齐LLM中组合盲点生成恶意软件的编译器框架", "tldr": "MGC是一个新的编译器框架，通过将恶意任务分解为看似无害的子任务，绕过对齐LLM的安全机制，高效生成功能性恶意软件。", "motivation": "LLMs使软件开发大众化，但也延伸到恶意软件开发。尽管LLM提供商实施了对齐机制来防止直接生成恶意代码，但这些安全措施主要孤立地评估单个提示，忽视了恶意操作可以系统地分解为看似良性的子任务这一关键漏洞。", "method": "MGC（恶意软件生成编译器）框架通过模块化分解和规避对齐的生成来利用LLM的组合盲点。它采用专门的恶意软件描述中间表示（MDIR）来连接高级恶意意图和看似良性的代码片段。", "result": "MGC能够可靠地生成各种任务规范和类别的功能性恶意软件，在三个基准数据集上的正确性方面，比越狱方法提高了+365.79%，比地下服务提高了+78.07%。案例研究表明MGC可以复制甚至增强16个现实世界的恶意软件样本。", "conclusion": "这项工作通过揭示针对对齐AI系统的组合攻击风险，为安全研究人员提供了重要的见解。", "translation": "大型语言模型（LLM）使软件开发大众化，降低了编程复杂应用程序的专业门槛。这种可访问性也延伸到恶意软件开发，引发了重大的安全担忧。虽然LLM提供商已经实施了对齐机制以防止直接生成公然的恶意代码，但这些安全措施主要孤立地评估单个提示，忽视了一个关键的漏洞：恶意操作可以系统地分解为看似良性的子任务。在本文中，我们引入了恶意软件生成编译器（MGC），这是一个新颖的框架，通过模块化分解和规避对齐的生成来利用这个漏洞。MGC采用专门的恶意软件描述中间表示（MDIR）来连接高级恶意意图和看似良性的代码片段。广泛的评估表明，我们的攻击在各种任务规范和类别中可靠地生成功能性恶意软件，在三个基准数据集上的正确性方面，比越狱方法提高了+365.79%，比地下服务提高了+78.07%。案例研究进一步表明，MGC可以复制甚至增强16个现实世界的恶意软件样本。这项工作通过揭示针对对齐AI系统的组合攻击风险，为安全研究人员提供了重要的见解。演示可在https://sites.google.com/view/malware-generation-compiler 获取。", "summary": "本论文介绍了MGC（恶意软件生成编译器），一个利用大型语言模型（LLM）中“组合盲点”的新颖框架。该框架通过将恶意任务分解为看似无害的子任务，并使用专门的恶意软件描述中间表示（MDIR）来连接恶意意图和良性代码片段，从而绕过LLM的现有安全对齐机制。实验证明，MGC在生成功能性恶意软件方面表现出色，其性能显著优于传统的越狱方法和地下服务，并能成功复现和增强现实世界中的恶意软件样本。这项研究为安全研究人员提供了关于对齐AI系统面临的组合攻击风险的重要见解。", "keywords": "LLM安全, 恶意软件生成, 组合攻击, MGC, 对齐盲点", "comments": "这项工作具有重要的创新性，它揭示了当前LLM安全对齐机制的一个关键漏洞——即它们在处理复杂、分解的恶意意图时的“组合盲点”。通过引入MGC框架和MDIR，该研究提供了一种系统性的方法来规避现有防御，并成功生成恶意软件。其结果表明这种攻击方式效率极高，对LLM的安全部署构成了新的挑战，为未来AI安全防御策略的制定提供了宝贵的方向。"}}
{"id": "2507.02125", "title": "Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities", "authors": ["Giulio Caldarelli"], "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.GT", "cs.LG", "11, 62, 68, 90, 91", "F.0; F.4; H.4; H.5; I.2"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02125v1", "summary": "The blockchain oracle problem, which refers to the challenge of injecting\nreliable external data into decentralized systems, remains a fundamental\nlimitation to the development of trustless applications. While recent years\nhave seen a proliferation of architectural, cryptographic, and economic\nstrategies to mitigate this issue, no one has yet fully resolved the\nfundamental question of how a blockchain can gain knowledge about the off-chain\nworld. In this position paper, we critically assess the role artificial\nintelligence (AI) can play in tackling the oracle problem. Drawing from both\nacademic literature and practitioner implementations, we examine how AI\ntechniques such as anomaly detection, language-based fact extraction, dynamic\nreputation modeling, and adversarial resistance can enhance oracle systems. We\nobserve that while AI introduces powerful tools for improving data quality,\nsource selection, and system resilience, it cannot eliminate the reliance on\nunverifiable off-chain inputs. Therefore, this study supports the idea that AI\nshould be understood as a complementary layer of inference and filtering within\na broader oracle design, not a substitute for trust assumptions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02125v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "人工智能能解决区块链预言机问题吗？剖析挑战与可能性", "tldr": "本文探讨了人工智能在解决区块链预言机问题中的作用，认为AI虽能提升数据质量和系统弹性，但无法完全消除对链下输入的信任依赖，应作为补充层而非替代品。", "motivation": "区块链预言机问题（即向去中心化系统注入可靠外部数据的挑战）是信任无关应用发展的主要限制。尽管已有多种缓解策略，但如何让区块链获取链下世界知识的核心问题尚未完全解决。", "method": "本文作为一篇立场文件，批判性评估了人工智能在解决预言机问题中的作用。作者结合学术文献和实践实现，考察了异常检测、基于语言的事实提取、动态声誉建模和对抗性抵抗等AI技术如何增强预言机系统。", "result": "研究观察到，虽然AI为提升数据质量、源选择和系统弹性提供了强大工具，但它无法消除对不可验证的链下输入的依赖。", "conclusion": "因此，本研究支持的观点是，AI应被理解为更广泛预言机设计中推理和过滤的补充层，而不是信任假设的替代品。", "translation": "区块链预言机问题，即向去中心化系统注入可靠外部数据的挑战，仍然是信任无关应用发展的根本限制。尽管近年来出现了大量旨在缓解此问题的架构、密码学和经济策略，但如何让区块链获取链下世界知识这一根本问题尚未完全解决。在这篇立场论文中，我们批判性地评估了人工智能（AI）在解决预言机问题中可以发挥的作用。我们借鉴学术文献和实践实现，研究了异常检测、基于语言的事实提取、动态声誉建模和对抗性抵抗等AI技术如何增强预言机系统。我们观察到，虽然AI引入了强大的工具来改善数据质量、源选择和系统弹性，但它不能消除对不可验证的链下输入的依赖。因此，本研究支持的观点是，AI应被理解为更广泛预言机设计中推理和过滤的补充层，而不是信任假设的替代品。", "summary": "本文探讨了人工智能（AI）在解决区块链预言机问题中的潜力与局限性。预言机问题是去中心化系统获取可靠链下数据的核心挑战。研究指出，AI技术如异常检测、事实提取和声誉建模可以增强预言机系统的数据质量和弹性。然而，AI无法完全消除对链下数据信任的依赖，因此应被视为现有预言机设计中的补充推理和过滤层，而非信任假设的替代品。", "keywords": "区块链, 预言机问题, 人工智能, 数据质量, 信任", "comments": "本文作为一篇立场文件，清晰地阐述了AI在区块链预言机问题中的定位。其创新之处在于提出AI应作为补充层而非替代品，这对于理解AI在区块链领域中的实际应用边界具有重要指导意义。文章强调了AI在数据质量和系统弹性方面的提升作用，同时也指出了其无法解决根本信任问题的局限性，提供了平衡的视角。"}}
{"id": "2507.02177", "title": "ARMOUR US: Android Runtime Zero-permission Sensor Usage Monitoring from User Space", "authors": ["Yan Long", "Jiancong Cui", "Yuqing Yang", "Tobias Alam", "Zhiqiang Lin", "Kevin Fu"], "categories": ["cs.CR", "K.6.5; D.4.6"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02177v1", "summary": "This work investigates how to monitor access to Android zero-permission\nsensors which could cause privacy leakage to users. Moreover, monitoring such\nsensitive access allows security researchers to characterize potential sensor\nabuse patterns. Zero-permission sensors such as accelerometers have become an\nindispensable part of Android devices. The critical information they provide\nhas attracted extensive research investigating how data collectors could\ncapture more sensor data to enable both benign and exploitative applications.\nIn contrast, little work has explored how to enable data providers, such as end\nusers, to understand sensor usage. While existing methods such as static\nanalysis and hooking-based dynamic analysis face challenges of requiring\ncomplicated development chains, rooting privilege, and app-specific reverse\nengineering analysis, our work aims to bridge this gap by developing ARMOUR for\nuser-space runtime monitoring, leveraging the intrinsic sampling rate variation\nand convergence behaviors of Android. ARMOUR enables privacy-aware users to\neasily monitor how third-party apps use sensor data and support security\nresearchers to perform rapid app-agnostic sensor access analysis. Our\nevaluation with 1,448 commercial applications shows the effectiveness of ARMOUR\nin detecting sensor usage in obfuscated code and other conditions, and observes\nsalient sensor abuse patterns such as 50% of apps from seemingly\nsensor-independent categories accessing data of multiple zero-permission\nsensors. We analyze the impact of Android's recent policy changes on\nzero-permission sensors and remaining technical and regulatory problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02177v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "ARMOUR US：从用户空间监控安卓零权限传感器使用", "tldr": "ARMOUR US是一种无需root权限的用户空间运行时监控工具，用于检测安卓零权限传感器（如加速度计）的隐私泄露和滥用模式，并在1448个商业应用中被证明有效。", "motivation": "当前缺少工具来帮助用户理解安卓零权限传感器的使用情况，而这些传感器可能导致隐私泄露。现有方法（如静态分析和基于钩子的动态分析）面临开发复杂、需要root权限和特定应用逆向工程分析的挑战，本研究旨在弥补这一空白，使用户和安全研究人员能够轻松监控和分析传感器使用。", "method": "通过开发ARMOUR，一种用户空间运行时监控工具，利用安卓固有的采样率变化和收敛行为来检测零权限传感器的使用。", "result": "对1448个商业应用的评估显示，ARMOUR能有效检测混淆代码和其他条件下的传感器使用。研究观察到显著的传感器滥用模式，例如50%的看似与传感器无关的应用访问了多个零权限传感器的数据。", "conclusion": "ARMOUR为隐私敏感用户提供了轻松监控第三方应用传感器使用的方法，并支持安全研究人员进行快速的应用无关传感器访问分析。研究还分析了安卓近期政策变化对零权限传感器的影响以及剩余的技术和监管问题。", "translation": "本研究探讨了如何监控安卓零权限传感器的访问，这些访问可能导致用户隐私泄露。此外，监控此类敏感访问使安全研究人员能够识别潜在的传感器滥用模式。加速度计等零权限传感器已成为安卓设备不可或缺的一部分。它们提供的关键信息吸引了广泛研究，探讨数据收集者如何捕获更多传感器数据，以支持良性和利用性应用。相比之下，很少有工作探索如何使数据提供者（如终端用户）理解传感器使用。虽然现有方法（如静态分析和基于钩子的动态分析）面临需要复杂开发链、root权限和特定应用逆向工程分析的挑战，但我们的工作旨在通过开发ARMOUR来弥补这一空白，实现用户空间运行时监控，利用安卓固有的采样率变化和收敛行为。ARMOUR使隐私敏感用户能够轻松监控第三方应用如何使用传感器数据，并支持安全研究人员执行快速的应用无关传感器访问分析。我们对1448个商业应用的评估显示，ARMOUR在检测混淆代码和其他条件下的传感器使用方面是有效的，并观察到显著的传感器滥用模式，例如50%的来自看似与传感器无关类别的应用访问了多个零权限传感器的数据。我们分析了安卓近期政策变化对零权限传感器的影响以及剩余的技术和监管问题。", "summary": "ARMOUR US是一种创新的用户空间运行时监控工具，旨在解决安卓零权限传感器（如加速度计）的隐私泄露问题。该工具通过利用安卓固有的传感器采样率变化和收敛行为，无需root权限即可帮助用户和安全研究人员轻松监控第三方应用对传感器数据的访问和识别潜在的滥用模式。对1448个商业应用的评估证明了ARMOUR在检测传感器使用方面的有效性，即使在混淆代码中也能奏效，并揭示了显著的滥用模式，例如大量看似无关的应用访问了多个零权限传感器。论文还讨论了安卓政策变化的影响以及待解决的技术和监管问题。", "keywords": "安卓, 零权限传感器, 隐私监控, 用户空间, 传感器滥用", "comments": "ARMOUR US的创新之处在于其无需root权限的用户空间运行时监控能力，这极大地降低了用户和研究人员监控零权限传感器使用的门槛。它有效地弥补了现有方法在复杂性、权限要求和应用特定性方面的不足，为解决安卓隐私泄露问题提供了实用且高效的解决方案。该工作通过大规模评估验证了其有效性，并揭示了普遍存在的传感器滥用现象，对隐私保护和安全研究具有重要意义。"}}
{"id": "2507.02181", "title": "Extended c-differential distinguishers of full 9 and reduced-round Kuznyechik cipher", "authors": ["Pantelimon Stanica", "Ranit Dutta", "Bimal Mandal"], "categories": ["cs.CR", "cs.IT", "math.IT", "94A60, 11T71, 12E20, 68P25, 62P99"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02181v1", "summary": "This paper introduces {\\em truncated inner $c$-differential cryptanalysis}, a\nnovel technique that for the first time enables the practical application of\n$c$-differential uniformity to block ciphers. While Ellingsen et al. (IEEE\nTrans. Inf. Theory, 2020) established the notion of $c$-differential uniformity\nusing $(F(x\\oplus a), cF(x))$, a key challenge remained: multiplication by $c$\ndisrupts the structural properties essential for block cipher analysis,\nparticularly key addition.\n  We resolve this challenge by developing an \\emph{inner} $c$-differential\napproach where multiplication by $c$ affects the input: $(F(cx\\oplus a),\nF(x))$. We prove that the inner $c$-differential uniformity of a function $F$\nequals the outer $c$-differential uniformity of $F^{-1}$, establishing a\nfundamental duality. This modification preserves cipher structure while\nenabling practical cryptanalytic applications.\n  Our main contribution is a comprehensive multi-faceted\nstatistical-computational framework, implementing truncated $c$-differential\nanalysis against the full 9-round Kuznyechik cipher (the inner\n$c$-differentials are immune to the key whitening at the backend). Through\nextensive computational analysis involving millions of differential pairs, we\ndemonstrate statistically significant non-randomness across all tested round\ncounts. For the full 9-round cipher, we identify multiple configurations\ntriggering critical security alerts, with bias ratios reaching $1.7\\times$ and\ncorrected p-values as low as $1.85 \\times 10^{-3}$, suggesting insufficient\nsecurity margin against this new attack vector. This represents the first\npractical distinguisher against the full 9-round Kuznyechik.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02181v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "全9轮和缩减轮Kuznyechik密码的扩展c-差分区分器", "tldr": "本文引入了“截断内c-差分密码分析”这一新颖技术，首次将c-差分均匀性实际应用于分组密码，解决了现有方法的挑战。该方法成功找到了针对完整9轮Kuznyechik密码的第一个实用区分器，表明其安全性裕度不足。", "motivation": "Ellingsen等人提出的c-差分均匀性概念虽然存在，但由于乘以c会破坏分组密码的结构特性（特别是密钥加法），导致其无法实际应用于分组密码分析。本文旨在解决这一关键挑战，使c-差分均匀性能够实际用于分组密码分析。", "method": "1. 提出了“内c-差分”方法：通过将乘数c作用于输入$(F(cx\\oplus a), F(x))$，从而保持密码结构。2. 证明了函数F的内c-差分均匀性等于F⁻¹的外c-差分均匀性，建立了基本对偶性。3. 开发了一个全面的多方面统计计算框架，用于针对完整9轮Kuznyechik密码的截断c-差分分析。4. 进行了涉及数百万差分对的广泛计算分析。", "result": "1. 在所有测试的轮数中，Kuznyechik密码都表现出统计上显著的非随机性。2. 对于完整的9轮密码，识别出多种触发关键安全警报的配置。3. 偏置比达到1.7倍，校正p值低至1.85 × 10⁻³。4. 首次实现了针对完整9轮Kuznyechik密码的实用区分器。", "conclusion": "本文提出的新攻击向量表明，完整9轮Kuznyechik密码的安全性裕度可能不足。", "translation": "本文介绍了“截断内c-差分密码分析”，这是一种首次将c-差分均匀性实际应用于分组密码的新技术。尽管Ellingsen等人（IEEE Trans. Inf. Theory, 2020）建立了使用$(F(x\\oplus a), cF(x))$的c-差分均匀性概念，但一个关键挑战仍然存在：乘以c会破坏分组密码分析所必需的结构特性，特别是密钥加法。我们通过开发一种“内”c-差分方法解决了这个挑战，其中乘以c影响输入：$(F(cx\\oplus a), F(x))$。我们证明了函数F的内c-差分均匀性等于F⁻¹的外c-差分均匀性，建立了根本的对偶性。这种修改在实现实际密码分析应用的同时，保留了密码结构。我们的主要贡献是一个全面的多方面统计计算框架，实现了针对完整9轮Kuznyechik密码的截断c-差分分析（内c-差分对后端密钥白化免疫）。通过涉及数百万差分对的广泛计算分析，我们证明了在所有测试轮数中都存在统计上显著的非随机性。对于完整的9轮密码，我们识别出多种触发关键安全警报的配置，偏置比达到1.7倍，校正p值低至1.85 × 10⁻³，这表明针对这种新攻击向量的安全性裕度不足。这代表了针对完整9轮Kuznyechik的第一个实用区分器。", "summary": "本文引入了一种名为“截断内c-差分密码分析”的新型技术，首次实现了c-差分均匀性在分组密码中的实际应用，克服了先前方法中因结构破坏而无法应用的挑战。通过开发一种作用于输入的内c-差分方法，作者建立了内、外c-差分均匀性之间的基本对偶性。研究团队进一步构建了一个综合性的统计计算框架，并将其应用于完整的9轮Kuznyechik密码。实验结果显示，在所有测试轮数中都存在统计上显著的非随机性，并成功识别出导致安全警报的配置，最终首次实现了针对完整9轮Kuznyechik的实用区分器。", "keywords": "c-差分密码分析, Kuznyechik, 分组密码, 区分器, 内c-差分", "comments": "本文通过引入“截断内c-差分密码分析”这一创新方法，成功解决了c-差分均匀性应用于分组密码的长期难题，具有重要意义。其关于内c-差分均匀性与外c-差分均匀性对偶性的证明，为该领域提供了基础理论贡献。此外，该技术首次成功应用于完整的9轮Kuznyechik密码，并发现了实用区分器，这不仅突显了新攻击向量的强大能力，也对Kuznyechik密码的安全性提出了潜在的担忧。"}}
{"id": "2507.02156", "title": "StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative", "authors": ["Benjamin Watson", "Janet Kim", "Tim McEneany", "Tom Moher", "Claudia Hindo", "Louis Gomez", "Stephen Fransen"], "categories": ["cs.HC", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02156v1", "summary": "The StorySpace project studies the role new interface technologies might play\nin high school education. With this approach in mind, StorySpace is\nspecifically designed to support and enhance classroom narrative, an already\nwell-established classroom activity. StorySpace strives to achieve this through\nadherence to three design goals. The first is to trigger student reflection and\ninterpretation. The narrative medium created by StorySpace should represent the\ntopic of classroom discussion and learning in all its complexity. In building\ntheir representation, the students will then be confronted with that same\ncomplexity. The medium should also itself be exciting and compelling, making\nclassroom narrative interesting and fun.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02156v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "StorySpace：支持课堂叙事中反思、表达和讨论的技术", "tldr": "StorySpace是一个旨在通过新颖的界面技术，支持和增强高中课堂叙事的项目，旨在激发学生反思和使学习更有趣。", "motivation": "该项目旨在研究新界面技术在高中教育中的作用，并具体支持和增强课堂叙事活动。", "method": "StorySpace通过遵循设计目标来实现其目的，包括：1) 触发学生的反思和解读，通过创建能展现课堂讨论复杂性的叙事媒介；2) 使媒介本身引人入胜，让课堂叙事变得有趣和好玩。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "StorySpace项目研究了新界面技术在高中教育中可能扮演的角色。考虑到这种方法，StorySpace专门设计用于支持和增强课堂叙事，这是一项已经成熟的课堂活动。StorySpace通过遵循三个设计目标来努力实现这一点。第一个目标是激发学生的反思和解读。由StorySpace创建的叙事媒介应以其全部复杂性呈现课堂讨论和学习的主题。在构建他们的表达时，学生将面对同样的复杂性。该媒介本身也应该令人兴奋和引人入胜，使课堂叙事变得有趣和好玩。", "summary": "StorySpace项目旨在探索新界面技术在高中教育中的应用，特别关注于支持和提升课堂叙事活动。该系统设计有明确目标，即通过创建复杂的叙事媒介来激发学生的深度反思和解读能力，同时确保学习过程充满趣味和吸引力。", "keywords": "课堂叙事, 界面技术, 学生反思, 高中教育, 学习媒介", "comments": "论文提出了一个有趣的技术应用方向，旨在将新界面技术融入高中课堂，以提升传统的叙事教学活动。其创新点在于强调通过技术媒介来呈现主题的复杂性，从而促进学生的反思和解读，并注重用户体验，使学习过程更具吸引力。然而，摘要中未提及具体的实现技术细节或初步实验结果，这限制了对其实际效果和可行性的评估。"}}
{"id": "2507.02016", "title": "Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain", "authors": ["Cong Wang", "Roberto Calandra", "Verena Klös"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Paper accepted at IEEE RO-MAN 2025; 6 pages", "url": "http://arxiv.org/abs/2507.02016v1", "summary": "When robots perform complex and context-dependent tasks in our daily lives,\ndeviations from expectations can confuse users. Explanations of the robot's\nreasoning process can help users to understand the robot intentions. However,\nwhen to provide explanations and what they contain are important to avoid user\nannoyance. We have investigated user preferences for explanation demand and\ncontent for a robot that helps with daily cleaning tasks in a kitchen. Our\nresults show that users want explanations in surprising situations and prefer\nconcise explanations that clearly state the intention behind the confusing\naction and the contextual factors that were relevant to this decision. Based on\nthese findings, we propose two algorithms to identify surprising actions and to\nconstruct effective explanations for Belief-Desire-Intention (BDI) robots. Our\nalgorithms can be easily integrated in the BDI reasoning process and pave the\nway for better human-robot interaction with context- and user-specific\nexplanations.", "comment": "Paper accepted at IEEE RO-MAN 2025; 6 pages", "pdf_url": "http://arxiv.org/pdf/2507.02016v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "意图-信念-欲望（BDI）机器人有效解释：何时以及解释什么", "tldr": "本研究调查了用户对机器人解释的需求和内容偏好，并提出了两种算法来识别意外行为并为BDI机器人构建有效的解释，以改善人机交互。", "motivation": "当机器人在日常生活中执行复杂且依赖上下文的任务时，与用户期望的偏差会使用户感到困惑。机器人推理过程的解释可以帮助用户理解机器人意图。然而，何时提供解释以及解释包含什么对于避免用户烦恼至关重要。", "method": "研究人员调查了用户对在厨房中执行日常清洁任务的机器人解释需求和内容的偏好。在此基础上，提出了两种算法来识别意外行为并为意图-信念-欲望（BDI）机器人构建有效的解释。", "result": "研究结果表明，用户在意外情况下需要解释，并且偏好简洁的解释，这些解释能清楚地说明令人困惑行为背后的意图以及与该决定相关的上下文因素。", "conclusion": "基于用户偏好，研究人员提出了两种算法，可以轻松集成到BDI推理过程中，为实现更好的、上下文和用户特定的人机交互铺平道路。", "translation": "当机器人在我们的日常生活中执行复杂且依赖上下文的任务时，与期望的偏差会使用户感到困惑。对机器人推理过程的解释可以帮助用户理解机器人的意图。然而，何时提供解释以及解释包含什么对于避免用户烦恼至关重要。我们调查了用户对在厨房中执行日常清洁任务的机器人解释需求和内容的偏好。我们的结果表明，用户在意外情况下需要解释，并且偏好简洁的解释，这些解释清楚地说明了令人困惑行为背后的意图以及与该决定相关的上下文因素。基于这些发现，我们提出了两种算法来识别意外行为并为意图-信念-欲望（BDI）机器人构建有效的解释。我们的算法可以很容易地集成到BDI推理过程中，并为实现更好的、上下文和用户特定的人机交互铺平道路。", "summary": "本研究旨在解决机器人行为与用户期望不符时引起的困惑和烦恼。作者通过调查用户对厨房清洁机器人解释的需求和内容偏好，发现用户在意外情况下需要解释，并偏好简洁、能说明意图和相关上下文因素的解释。基于这些发现，论文提出了两种算法，用于识别意外行为并为BDI机器人构建有效的解释，旨在改善人机交互。", "keywords": "机器人解释, BDI机器人, 人机交互, 用户偏好, 意外行为", "comments": "该论文的创新之处在于其不仅关注机器人解释的必要性，更深入探讨了“何时解释”和“解释什么”这两个关键问题，从而避免用户烦恼。通过用户偏好调查并提出具体的算法来识别意外情况和构建解释，为BDI机器人的人机交互提供了实用的解决方案，具有重要的应用价值。"}}
{"id": "2507.02068", "title": "How do Software Engineering Candidates Prepare for Technical Interviews?", "authors": ["Brian Bell", "Teresa Thomas", "Sang Won Lee", "Chris Brown"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02068v1", "summary": "To obtain employment, aspiring software engineers must complete technical\ninterviews -- a hiring process which involves candidates writing code while\ncommunicating to an audience. However, the complexities of tech interviews are\ndifficult to prepare for and seldom faced in computing curricula. To this end,\nwe seek to understand how candidates prepare for technical interviews,\ninvestigating the effects of preparation methods and the role of education. We\ndistributed a survey to candidates (n = 131) actively preparing for technical\ninterviews. Our results suggest candidates rarely train in authentic settings\nand courses fail to support preparation efforts -- leading to stress and\nunpreparedness. Based on our findings, we provide implications for stakeholders\nto enhance tech interview preparation for candidates pursuing software\nengineering roles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02068v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "软件工程求职者如何准备技术面试？", "tldr": "软件工程求职者在准备技术面试时面临挑战，因为课程很少涉及真实面试情境，导致压力和准备不足。", "motivation": "技术面试是软件工程师求职的必要环节，但其复杂性使得准备困难，且现有计算机课程很少涉及。因此，本研究旨在了解求职者如何准备技术面试，并调查准备方法和教育作用的影响。", "method": "研究人员向131名正在积极准备技术面试的求职者分发了一项调查问卷。", "result": "结果表明，求职者很少在真实的面试环境中进行训练，且现有课程未能有效支持他们的准备工作，这导致了求职者的压力和准备不足。", "conclusion": "根据研究发现，作者为相关方提供了建议，以改进软件工程求职者的技术面试准备。", "translation": "为了获得工作，有抱负的软件工程师必须完成技术面试——这是一个招聘过程，涉及候选人在与听众交流的同时编写代码。然而，技术面试的复杂性难以准备，并且在计算机课程中很少遇到。为此，我们旨在了解候选人如何准备技术面试，调查准备方法的影响以及教育的作用。我们向131名正在积极准备技术面试的候选人分发了一项调查。我们的结果表明，候选人很少在真实环境中进行训练，并且课程未能支持准备工作——导致压力和准备不足。根据我们的发现，我们为利益相关者提供了建议，以加强软件工程求职者的技术面试准备。", "summary": "本研究旨在探讨软件工程求职者如何准备技术面试，因为技术面试是求职的关键环节，但准备复杂且现有课程支持不足。通过对131名求职者进行调查，研究发现求职者很少在真实情境中训练，且课程未能提供足够支持，导致他们感到压力和准备不足。基于这些发现，论文为利益相关者提出了改进技术面试准备的建议。", "keywords": "技术面试准备, 软件工程, 求职者, 调查研究, 教育支持", "comments": "本研究揭示了软件工程教育与行业需求之间的脱节，特别是在技术面试准备方面。其创新之处在于通过实证调查揭示了求职者面临的实际困境。重要性在于为教育机构和招聘方提供了改进面试准备策略的依据，有助于减轻求职者压力并提高就业率。局限性可能在于样本量相对较小，且仅限于调查数据，未能深入探讨具体准备方法的有效性或教育课程的改进细节。"}}
{"id": "2507.02122", "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training", "authors": ["Neil K. R. Sehgal", "Hita Kambhamettu", "Allen Chang", "Andrew Zhu", "Lyle Ungar", "Sharath Chandra Guntuku"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02122v1", "summary": "Effective communication in serious illness and palliative care is essential\nbut often under-taught due to limited access to training resources like\nstandardized patients. We present PAL (Palliative Assisted Learning-bot), a\nconversational system that simulates emotionally nuanced patient interactions\nand delivers structured feedback grounded in an existing empathy-based\nframework. PAL supports text and voice modalities and is designed to scaffold\nclinical skill-building through repeated, low-cost practice. Through a\nmixed-methods study with 17 U.S. medical trainees and clinicians, we explore\nuser engagement with PAL, evaluate usability, and examine design tensions\naround modalities, emotional realism, and feedback delivery. Participants found\nPAL helpful for reflection and skill refinement, though some noted limitations\nin emotional authenticity and the adaptability of feedback. We contribute: (1)\nempirical evidence that large language models can support palliative\ncommunication training; (2) design insights for modality-aware, emotionally\nsensitive simulation tools; and (3) implications for systems that support\nemotional labor, cooperative learning, and AI-augmented training in high-stakes\ncare settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02122v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "PAL：设计可扩展、协作式患者模拟对话代理用于姑息治疗培训", "tldr": "PAL是一个基于大语言模型的对话系统，用于模拟姑息治疗中的患者互动，提供反馈以帮助医疗人员进行沟通技能训练。", "motivation": "姑息治疗中有效的沟通至关重要，但由于缺乏标准化患者等培训资源，往往教学不足。", "method": "研究团队开发了PAL（Palliative Assisted Learning-bot），一个对话系统，模拟情感细致的患者互动并提供基于现有同理心框架的结构化反馈。PAL支持文本和语音模式，旨在通过重复、低成本的练习来培养临床技能。通过一项针对17名美国医疗受训者和临床医生的混合方法研究，探讨了用户对PAL的参与度，评估了可用性，并检查了围绕模式、情感真实性和反馈传递的设计张力。", "result": "参与者认为PAL有助于反思和技能提升，尽管部分人指出其在情感真实性和反馈适应性方面存在局限性。", "conclusion": "本研究贡献了：(1) 大型语言模型可以支持姑息沟通培训的实证证据；(2) 针对模式感知、情感敏感模拟工具的设计见解；以及(3) 对支持情感劳动、合作学习和高风险护理环境中AI增强培训的系统的启示。", "translation": "在严重疾病和姑息治疗中，有效的沟通至关重要，但由于标准化患者等培训资源有限，往往教学不足。我们提出了PAL（Palliative Assisted Learning-bot），一个对话系统，可以模拟情感细致的患者互动，并根据现有的同理心框架提供结构化反馈。PAL支持文本和语音模式，旨在通过重复、低成本的练习来培养临床技能。通过一项针对17名美国医疗受训者和临床医生的混合方法研究，我们探讨了用户对PAL的参与度，评估了可用性，并检查了围绕模式、情感真实性和反馈传递的设计张力。参与者发现PAL有助于反思和技能提升，尽管部分人指出其在情感真实性和反馈适应性方面存在局限性。我们的贡献包括：(1) 大型语言模型可以支持姑息沟通培训的实证证据；(2) 针对模式感知、情感敏感模拟工具的设计见解；以及(3) 对支持情感劳动、合作学习和高风险护理环境中AI增强培训的系统的启示。", "summary": "本文介绍了PAL，一个基于大语言模型的对话系统，旨在作为可扩展、低成本的患者模拟器，用于姑息治疗沟通技能培训。PAL通过模拟情感细致的患者互动并提供结构化反馈来帮助医疗人员练习。一项针对17名医疗专业人员的研究表明，PAL有助于技能提升，但也存在情感真实性和反馈适应性的局限性。研究证实了大型语言模型在姑息沟通培训中的潜力，并提供了设计情感敏感模拟工具的见解。", "keywords": "姑息治疗, 对话代理, 患者模拟, 沟通培训, 大型语言模型", "comments": "PAL的创新之处在于利用大型语言模型构建可扩展的患者模拟器，解决了传统培训资源有限的问题。它将AI应用于高风险的医疗沟通训练，具有重要意义。然而，研究也指出了其在情感真实性方面的局限性，这可能是未来AI模拟系统需要持续改进的方面。"}}
{"id": "2507.02002", "title": "Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models", "authors": ["Shaurya Mallampati", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures Submitted to GameSec 2025 (under review)", "url": "http://arxiv.org/abs/2507.02002v1", "summary": "Large language models (LLMs) demonstrate strong reasoning abilities across\nmathematical, strategic, and linguistic tasks, yet little is known about how\nwell they reason in dynamic, real-time, multi-agent scenarios, such as\ncollaborative environments in which agents continuously adapt to each other's\nbehavior, as in cooperative gameplay settings. In this paper, we bridge this\ngap by combining LLM-driven agents with strategic reasoning and real-time\nadaptation in cooperative, multi-agent environments grounded in game-theoretic\nprinciples such as belief consistency and Nash equilibrium. The proposed\nframework applies broadly to dynamic scenarios in which agents coordinate,\ncommunicate, and make decisions in response to continuously changing\nconditions. We provide real-time strategy refinement and adaptive feedback\nmechanisms that enable agents to dynamically adjust policies based on immediate\ncontextual interactions, in contrast to previous efforts that evaluate LLM\ncapabilities in static or turn-based settings. Empirical results show that our\nmethod achieves up to a 26\\% improvement in return over PPO baselines in\nhigh-noise environments, while maintaining real-time latency under 1.05\nmilliseconds. Our approach improves collaboration efficiency, task completion\nrates, and flexibility, illustrating that game-theoretic guidance integrated\nwith real-time feedback enhances LLM performance, ultimately fostering more\nresilient and flexible strategic multi-agent systems.", "comment": "20 pages, 11 figures Submitted to GameSec 2025 (under review)", "pdf_url": "http://arxiv.org/pdf/2507.02002v1", "cate": "cs.MA", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "大型语言模型在多智能体环境中的动态策略适应", "tldr": "本研究通过结合大型语言模型（LLMs）与博弈论原则，实现了在动态、实时、合作性多智能体环境中的策略适应和实时反馈，相较于PPO基线在噪声环境中提高了26%的回报，并保持了低延迟。", "motivation": "尽管大型语言模型（LLMs）在数学、策略和语言任务中展现出强大的推理能力，但它们在动态、实时、多智能体场景（如合作博弈设置中智能体持续相互适应的环境）中的推理表现知之甚少。本研究旨在弥补这一空白。", "method": "本研究通过将LLM驱动的智能体与战略推理和实时适应相结合，在基于博弈论原则（如信念一致性和纳什均衡）的合作性多智能体环境中进行。该框架提供实时策略优化和自适应反馈机制，使智能体能够根据即时上下文交互动态调整策略，这与以往在静态或回合制设置中评估LLM能力的方法不同。", "result": "实验结果表明，在强噪声环境中，我们的方法相较于PPO基线在回报上提高了26%，同时将实时延迟保持在1.05毫秒以下。我们的方法提高了协作效率、任务完成率和灵活性。", "conclusion": "博弈论指导与实时反馈相结合能够增强大型语言模型的性能，最终促进更具弹性和灵活性的战略性多智能体系统。", "translation": "大型语言模型（LLMs）在数学、策略和语言任务中展示了强大的推理能力，但它们在动态、实时、多智能体场景中的推理能力知之甚少，例如在合作博弈设置中智能体持续相互适应的协作环境。在本文中，我们通过将LLM驱动的智能体与战略推理和实时适应相结合，在基于博弈论原则（如信念一致性和纳什均衡）的合作性多智能体环境中弥补了这一空白。所提出的框架广泛适用于智能体在不断变化的条件下进行协调、通信和决策的动态场景。我们提供了实时策略优化和自适应反馈机制，使智能体能够根据即时上下文交互动态调整策略，这与以往在静态或回合制设置中评估LLM能力的工作形成对比。实证结果表明，在强噪声环境中，我们的方法相较于PPO基线在回报上提高了26%，同时将实时延迟保持在1.05毫秒以下。我们的方法提高了协作效率、任务完成率和灵活性，这表明博弈论指导与实时反馈相结合能够增强LLM的性能，最终促进更具弹性和灵活性的战略性多智能体系统。", "summary": "本文提出了一种将大型语言模型（LLMs）应用于动态、实时、合作性多智能体环境的框架。该方法结合了LLM驱动的智能体、战略推理和基于博弈论原则的实时适应机制，实现了策略的动态调整。与以往在静态或回合制设置中评估LLM能力不同，本研究侧重于持续变化的条件。实验结果显示，在强噪声环境下，该方法相较于PPO基线将回报提高了26%，并保持了极低的实时延迟，显著提升了协作效率和系统灵活性，证明了博弈论指导和实时反馈对LLM性能的增强作用。", "keywords": "大型语言模型, 多智能体系统, 动态策略, 博弈论, 实时适应", "comments": "这篇论文的创新点在于将LLM的能力从静态或回合制环境扩展到更具挑战性的动态、实时多智能体场景。通过引入博弈论原则和实时反馈机制，它有效地解决了LLM在复杂交互中策略适应的难题。其在性能上的显著提升和低延迟特性，展示了LLM在构建更智能、更具弹性的多智能体系统方面的巨大潜力。这项工作对于推动LLM在实际应用中的部署，尤其是在需要快速决策和持续协作的领域，具有重要意义。"}}
{"id": "2507.01978", "title": "Recommendation Algorithms on Social Media: Unseen Drivers of Political Opinion", "authors": ["Waseq Billah"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      3 tables in main text, 2 tables in appendix. The article was presented at the Western Political Science Association (WPSA) Annual Meeting held during April 16-19, 2025 at Seattle, WA", "url": "http://arxiv.org/abs/2507.01978v1", "summary": "Social media broadly refers to digital platforms and applications that\nsimulate social interactions online. This study investigates the impact of\nsocial media platforms and their algorithms on political interest among users.\nAs social media usage continues to rise, platforms like Facebook and X\n(formerly Twitter) play increasingly pivotal roles in shaping political\ndiscourse. By employing statistical analyses on data collected from over 3,300\nparticipants, this research identifies significant differences in how various\nsocial media platforms influence political interest. Findings reveal that\nmoderate Facebook users demonstrate decreased political engagement, whereas\neven minimal engagement with X significantly boosts political interest. The\nstudy further identifies demographic variations, noting that males, older\nindividuals, Black or African American users, those with higher incomes show\ngreater political interest. The demographic analysis highlights that\nRepublicans are particularly active on social media - potentially influencing\ntheir social media engagement patterns. However, the study acknowledges a\ncrucial limitation - the lack of direct data regarding the content users are\nexposed to which is shaping their social media experiences. Future research\nshould explore these influences and consider additional popular platforms to\nenhance the understanding of social media's political impact. Addressing these\ngaps can provide deeper insights into digital political mobilization, aiding\npolicymakers, educators, and platform designers in fostering healthier\ndemocratic engagement.", "comment": "3 tables in main text, 2 tables in appendix. The article was\n  presented at the Western Political Science Association (WPSA) Annual Meeting\n  held during April 16-19, 2025 at Seattle, WA", "pdf_url": "http://arxiv.org/pdf/2507.01978v1", "cate": "cs.SI", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "社交媒体上的推荐算法：政治观点背后看不见的驱动力", "tldr": "研究发现社交媒体平台及其算法对用户政治兴趣有不同影响，Facebook用户政治参与度降低，而X用户政治兴趣显著提升。", "motivation": "随着社交媒体使用量的持续增长，平台在塑造政治话语方面发挥着越来越关键的作用。本研究旨在调查社交媒体平台及其算法对用户政治兴趣的影响。", "method": "本研究对来自3300多名参与者的数据进行了统计分析。", "result": "研究发现不同社交媒体平台对政治兴趣的影响存在显著差异：适度使用Facebook的用户政治参与度降低，而即使少量使用X（原Twitter）也能显著提升政治兴趣。人口统计学分析显示，男性、老年人、非裔美国用户和高收入人群表现出更高的政治兴趣，共和党人尤其活跃。", "conclusion": "解决研究中存在的不足（如缺乏用户接触内容的数据）可以为数字政治动员提供更深入的见解，从而帮助政策制定者、教育工作者和平台设计者促进健康的民主参与。", "translation": "社交媒体广义上指模拟在线社交互动的数字平台和应用程序。本研究调查了社交媒体平台及其算法对用户政治兴趣的影响。随着社交媒体使用量的持续增长，Facebook和X（原Twitter）等平台在塑造政治话语方面发挥着越来越关键的作用。通过对来自3300多名参与者的数据进行统计分析，本研究发现不同社交媒体平台对政治兴趣的影响存在显著差异。研究结果显示，适度使用Facebook的用户政治参与度降低，而即使少量使用X也能显著提升政治兴趣。本研究进一步确定了人口统计学差异，指出男性、老年人、非裔美国用户和高收入人群表现出更高的政治兴趣。人口统计学分析强调，共和党人在社交媒体上特别活跃——这可能影响他们的社交媒体参与模式。然而，本研究承认一个关键局限性——缺乏关于用户接触内容（这些内容正在塑造他们的社交媒体体验）的直接数据。未来的研究应探索这些影响，并考虑其他流行平台，以增强对社交媒体政治影响的理解。解决这些空白可以为数字政治动员提供更深入的见解，从而帮助政策制定者、教育工作者和平台设计者促进健康的民主参与。", "summary": "本研究调查了社交媒体平台及其算法对用户政治兴趣的影响，发现不同平台的影响存在显著差异。通过对3300多名参与者的数据进行统计分析，结果显示适度使用Facebook会降低政治参与度，而少量使用X则能显著提升政治兴趣。研究还识别了人口统计学差异，指出男性、老年人、非裔美国用户和高收入人群政治兴趣更高。研究承认缺乏用户接触内容数据的局限性，并建议未来研究探索这些影响以促进健康的民主参与。", "keywords": "社交媒体, 推荐算法, 政治兴趣, 政治参与, 数据分析", "comments": "本文创新性在于通过大数据分析揭示了社交媒体平台及其算法对政治兴趣的“看不见”的驱动作用，特别是区分了Facebook和X的不同影响。其重要性在于为理解数字政治动员提供了实证基础，对政策制定者和平台设计者具有指导意义。主要局限性在于未能直接获取用户所接触的具体内容数据，这限制了对算法深层影响的理解。"}}
{"id": "2507.01976", "title": "A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning", "authors": ["Nirhoshan Sivaroopan", "Kaushitha Silva", "Chamara Madarasingha", "Thilini Dahanayaka", "Guillaume Jourjon", "Anura Jayasumana", "Kanchana Thilakarathna"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01976v1", "summary": "Synthetic network traffic generation has emerged as a promising alternative\nfor various data-driven applications in the networking domain. It enables the\ncreation of synthetic data that preserves real-world characteristics while\naddressing key challenges such as data scarcity, privacy concerns, and purity\nconstraints associated with real data. In this survey, we provide a\ncomprehensive review of synthetic network traffic generation approaches,\ncovering essential aspects such as data types, generation models, and\nevaluation methods. With the rapid advancements in AI and machine learning, we\nfocus particularly on deep learning-based techniques while also providing a\ndetailed discussion of statistical methods and their extensions, including\ncommercially available tools. Furthermore, we highlight open challenges in this\ndomain and discuss potential future directions for further research and\ndevelopment. This survey serves as a foundational resource for researchers and\npractitioners, offering a structured analysis of existing methods, challenges,\nand opportunities in synthetic network traffic generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01976v1", "cate": "cs.NI", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "网络流量合成的综合调查：从统计模型到深度学习", "tldr": "该调查全面回顾了网络流量合成技术，涵盖统计模型和深度学习方法，并探讨了挑战与未来方向。", "motivation": "合成网络流量生成是数据驱动网络应用的一个有前景的替代方案，能解决真实数据的数据稀缺性、隐私问题和纯度限制等挑战。", "method": "本调查对合成网络流量生成方法进行了全面回顾，涵盖数据类型、生成模型和评估方法。特别关注基于深度学习的技术，并详细讨论了统计方法及其扩展，包括商业工具。", "result": "本调查提供了对现有方法、挑战和机遇的结构化分析，并强调了该领域的开放挑战和潜在的未来研究方向。", "conclusion": "本调查旨在为研究人员和从业者提供一个关于合成网络流量生成的基础资源。", "translation": "合成网络流量生成已成为网络领域各种数据驱动应用的一个有前景的替代方案。它能够创建保留真实世界特征的合成数据，同时解决与真实数据相关的数据稀缺性、隐私问题和纯度限制等关键挑战。在本调查中，我们对合成网络流量生成方法进行了全面回顾，涵盖了数据类型、生成模型和评估方法等基本方面。随着人工智能和机器学习的快速发展，我们特别关注基于深度学习的技术，同时详细讨论了统计方法及其扩展，包括商业工具。此外，我们强调了该领域的开放挑战，并讨论了未来研究和开发的潜在方向。本调查为研究人员和从业者提供了一个基础资源，对合成网络流量生成中的现有方法、挑战和机遇进行了结构化分析。", "summary": "本调查全面回顾了网络流量合成技术，旨在解决真实数据的数据稀缺性、隐私和纯度问题。论文涵盖了数据类型、生成模型和评估方法，并重点介绍了深度学习和统计方法。它还探讨了现有挑战和未来的研究方向，为该领域的研究人员和从业者提供了宝贵的资源。", "keywords": "网络流量合成, 深度学习, 统计模型, 数据生成, 调查", "comments": "这是一篇非常及时的综述文章，填补了网络流量合成领域系统性知识的空白。它涵盖了从传统统计方法到前沿深度学习技术的演变，为研究人员提供了全面的视角。强调开放挑战和未来方向对于指导后续研究具有重要意义。"}}
{"id": "2507.02132", "title": "Matrix Pencil-Based DoA Estimation for Hybrid Receivers in Snapshot-Limited Scenarios", "authors": ["Mona Mostafa", "Ramy H. Gohary", "Amr El-Keyi", "Yahia A. Eldemerdash Ahmed"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This manuscript is currently under review for potential publication in IEEE Journal", "url": "http://arxiv.org/abs/2507.02132v1", "summary": "The goal of this paper is to estimate the directions of arrival (DoAs) for\nhybrid analog/digital (HAD) receivers when the number of snapshots is too small\nfor statistical averaging to be reliable. This goal is achieved in\nfully-digital receivers by employing the matrix pencil method (MPM).\nUnfortunately, the MPM cannot be directly applied in HAD receivers because of\nthe entanglement induced by the underlying analog combiners on the output\nsignals. Furthermore, these analog combiners project the received signal onto a\nlow-dimensional space, jeopardizing the reception of signals arriving from\nparticular DoA ranges. To circumvent these difficulties, we propose two\napproaches to enable the MPM to extract the DoAs in HAD receivers. The two\napproaches avoid severe attenuation induced by low-dimensional projection by\ncycling over an exhaustive set of analog combiners, collectively spanning the\nentire space. The first approach can be applied to both fully-connected (FC)\nand partially-connected (PC) HADs and relies on the availability of periodic,\npotentially unknown, signals to disentangle the output of the HAD receiver. The\nsecond approach applies to PC-HADs only, and eliminates contingency on periodic\nsignals by exploiting the underlying block diagonal structure. The superiority\nof the proposed approaches is demonstrated via numerical simulations and\ncomparisons with the Cram\\'er-Rao lower bound.", "comment": "This manuscript is currently under review for potential publication\n  in IEEE Journal", "pdf_url": "http://arxiv.org/pdf/2507.02132v1", "cate": "cs.IT", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "快照受限场景下混合接收机的基于矩阵铅笔法的DoA估计", "tldr": "本文提出两种方法，使得矩阵铅笔法（MPM）能够在快照数量有限的混合模拟/数字（HAD）接收机中进行方向到达（DoA）估计，克服了现有方法的局限性。", "motivation": "在快照数量过少导致统计平均不可靠的情况下，需要为混合模拟/数字（HAD）接收机估计到达方向（DoA）。现有的矩阵铅笔法（MPM）无法直接应用于HAD接收机，因为模拟合成器会引入信号纠缠并导致低维投影，从而危及特定DoA范围信号的接收。", "method": "本文提出了两种方法，使MPM能够在HAD接收机中提取DoA。这两种方法通过循环遍历一组穷尽的模拟合成器来避免低维投影引起的严重衰减，这些合成器共同覆盖整个空间。第一种方法适用于全连接（FC）和部分连接（PC）HAD，并依赖于周期性（可能未知）信号来解缠HAD接收机的输出。第二种方法仅适用于PC-HAD，通过利用底层块对角结构消除了对周期性信号的依赖。", "result": "通过数值模拟并与Cramér-Rao下界进行比较，证明了所提出方法的优越性。", "conclusion": "本文提出的两种方法成功地使矩阵铅笔法（MPM）能够应用于快照受限场景下的混合模拟/数字（HAD）接收机进行方向到达（DoA）估计，克服了模拟合成器引起的信号纠缠和低维投影问题，并取得了优越的性能。", "translation": "本文的目标是在快照数量过少导致统计平均不可靠时，估计混合模拟/数字（HAD）接收机的到达方向（DoA）。在全数字接收机中，通过采用矩阵铅笔法（MPM）可以实现这一目标。然而，MPM无法直接应用于HAD接收机，因为底层模拟合成器会在输出信号上引入纠缠。此外，这些模拟合成器将接收信号投影到低维空间，危及来自特定DoA范围的信号接收。为了规避这些困难，我们提出了两种方法，使MPM能够在HAD接收机中提取DoA。这两种方法通过循环遍历一组穷尽的模拟合成器来避免低维投影引起的严重衰减，这些合成器共同覆盖整个空间。第一种方法适用于全连接（FC）和部分连接（PC）HAD，并依赖于周期性（可能未知）信号来解缠HAD接收机的输出。第二种方法仅适用于PC-HAD，通过利用底层块对角结构消除了对周期性信号的依赖。通过数值模拟并与Cramér-Rao下界进行比较，证明了所提出方法的优越性。", "summary": "在快照数量有限的情况下，针对混合模拟/数字（HAD）接收机的到达方向（DoA）估计问题，本文提出了一种基于矩阵铅笔法（MPM）的新方法。鉴于传统MPM无法直接应用于HAD接收机，因为模拟合成器会引入信号纠缠和低维投影，本文提出了两种创新方法。这两种方法通过循环使用一系列模拟合成器来克服低维投影问题。第一种方法适用于全连接和部分连接HAD，并利用周期性信号进行解缠；第二种方法专为部分连接HAD设计，通过利用其块对角结构来避免对周期性信号的依赖。数值模拟结果表明，所提出的方法表现出优越的性能。", "keywords": "DoA估计, 混合接收机, 矩阵铅笔法, 快照受限, 模拟合成器", "comments": "本文的创新点在于提出了两种巧妙的方法，使得强大的矩阵铅笔法（MPM）能够在快照受限的混合模拟/数字（HAD）接收机中进行DoA估计。这解决了HAD系统在实际应用中面临的关键挑战，即模拟合成器引起的信号纠缠和低维投影问题。该研究对未来混合波束形成和DoA估计领域具有重要意义，尤其是在资源受限的应用场景中。"}}
{"id": "2507.02067", "title": "Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions", "authors": ["Nikolaos Papanikolaou", "Doha Touhafi", "Jurgen Vandendriessche", "Danial Karimi", "Sohail Fatimi", "Gianluca Cornetta", "Abdellah Touhafi"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02067v1", "summary": "Printed sensors represent a transformative advancement in sensor technology,\nutilizing innovative printing techniques to create flexible, cost-effective,\nand highly customizable sensing devices. Their versatility allows integration\ninto numerous applications across diverse fields such as monitoring a wide\nrange of environmental factors e.g. air and water quality, soil conditions, and\natmospheric changes among others. These sensors demonstrate high sensitivity\nand accuracy in detecting pollutants, temperature variations, humidity levels,\nand other critical parameters essential for environmental assessment and\nprotection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02067v1", "cate": "cs.AR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "先进的印刷传感器在环境应用中的发展：通向可持续监测解决方案的路径", "tldr": "印刷传感器为环境监测提供了一种灵活、经济且高精度的可持续解决方案。", "motivation": "印刷传感器技术具有变革性，能提供灵活、成本效益高、可定制的传感设备，应用于环境监测，以实现可持续监测解决方案。", "method": "利用创新的印刷技术制造柔性、经济、可定制的传感设备，并将其应用于环境因素监测。", "result": "印刷传感器在检测污染物、温度变化、湿度水平以及其他关键环境参数方面表现出高灵敏度和准确性。", "conclusion": "印刷传感器代表了环境监测领域的一个变革性进步，为可持续监测提供了有效的解决方案。", "translation": "印刷传感器代表了传感器技术的一项变革性进步，它利用创新的印刷技术来制造柔性、经济且高度可定制的传感设备。它们的通用性使其能够集成到各种应用中，涵盖空气和水质、土壤条件以及大气变化等多种环境因素的监测。这些传感器在检测污染物、温度变化、湿度水平以及其他对环境评估和保护至关重要的关键参数方面表现出高灵敏度和准确性。", "summary": "本文探讨了印刷传感器作为一种变革性技术，如何通过创新的印刷技术提供柔性、经济且可定制的传感设备。这些传感器可广泛应用于环境监测，包括空气、水质、土壤和大气变化，并表现出高灵敏度和准确性，为可持续环境监测解决方案铺平道路。", "keywords": "印刷传感器, 环境应用, 可持续监测, 传感器技术, 灵敏度", "comments": "论文强调了印刷传感器在环境监测领域的重要性和潜力，其柔性、成本效益和高灵敏度是主要创新点，为可持续环境监测提供了新的途径。"}}
{"id": "2507.02124", "title": "SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan", "authors": ["Fumikazu Konishi"], "categories": ["cs.DC", "cs.NI", "C.5.5; B.8.2"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      13 pages, 2 Figures, 10 tables", "url": "http://arxiv.org/abs/2507.02124v1", "summary": "SAKURAONE is a managed high performance computing (HPC) cluster developed and\noperated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU\nPHY'' configuration of bare-metal GPU servers and is designed as a cluster\ncomputing resource optimized for advanced workloads, including large language\nmodel (LLM) training.\n  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked\n\\textbf{49th} in the world based on its High Performance Linpack (HPL) score,\ndemonstrating its global competitiveness. In particular, it is the \\textbf{only\nsystem within the top 100} that employs a fully open networking stack based on\n\\textbf{800~GbE (Gigabit Ethernet)} and the \\textbf{SONiC (Software for Open\nNetworking in the Cloud)} operating system, highlighting the viability of open\nand vendor-neutral technologies in large-scale HPC infrastructure.\n  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL\nbenchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate\nGradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets\nlow-precision workloads representative of AI applications, SAKURAONE delivered\nan impressive 339.86~PFLOP/s using FP8 precision.\n  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100\nGPUs. It is supported by an all-flash Lustre storage subsystem with a total\nphysical capacity of 2~petabytes, providing high-throughput and low-latency\ndata access. Internode communication is enabled by a full-bisection bandwidth\ninterconnect based on a Rail-Optimized topology, where the Leaf and Spine\nlayers are interconnected via 800~GbE links. This topology, in combination with\nRoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless\ndata transfers and mitigates communication bottlenecks in large-scale parallel\nworkloads.", "comment": "13 pages, 2 Figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.02124v1", "cate": "cs.DC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "SAKURAONE：通过日本私营部门HPC投资赋能透明开放的AI平台", "tldr": "SAKURAONE是日本私营部门投资的高性能计算集群，以其开放网络堆栈在全球TOP500中排名第49位，并展示了在AI工作负载上的卓越性能。", "motivation": "论文旨在展示SAKURAONE，一个由私营部门投资的高性能计算集群，如何通过提供先进的工作负载优化资源（包括大型语言模型训练）来赋能透明开放的AI平台。它还强调了开放和供应商中立技术在大规模HPC基础设施中的可行性。", "method": "SAKURAONE是一个托管的高性能计算集群，采用了“KOKARYOKU PHY”配置的裸金属GPU服务器。它使用基于800 GbE和SONiC（Software for Open Networking in the Cloud）操作系统的全开放网络堆栈。系统包含100个计算节点，每个节点配备8个NVIDIA H100 GPU，并由一个2拍字节的全闪存Lustre存储子系统支持。节点间通信通过基于Rail-Optimized拓扑的全双向带宽互连实现，该拓扑结合RoCEv2（RDMA over Converged Ethernet version 2）通过800 GbE链路连接叶层和脊层。", "result": "SAKURAONE在ISC 2025 TOP500榜单中基于HPL得分排名全球第49位，是前100名中唯一采用全开放网络堆栈的系统。它在HPL基准测试中持续性能达到33.95 PFLOP/s (Rmax)，在HPCG基准测试中达到396.295 TFLOP/s。对于代表AI应用的低精度工作负载HPL-MxP，使用FP8精度实现了339.86 PFLOP/s。", "conclusion": "SAKURAONE的成功部署和高性能表现证明了私营部门投资在建立全球竞争力HPC资源方面的能力，并且验证了开放和供应商中立技术在大规模HPC基础设施中实现卓越性能和透明度的可行性。", "translation": "SAKURAONE 是由 SAKURA 互联网研究中心开发和运营的托管高性能计算 (HPC) 集群。它强化了裸金属 GPU 服务器的“KOKARYOKU PHY”配置，并被设计为针对包括大型语言模型 (LLM) 训练在内的先进工作负载进行优化的集群计算资源。\n在 ISC 2025 年 TOP500 榜单中，SAKURAONE 根据其高性能 Linpack (HPL) 得分排名世界第 49 位，展示了其全球竞争力。特别是，它是前 100 名中唯一采用基于 800 GbE (千兆以太网) 和 SONiC (云中开放网络软件) 操作系统 的全开放网络堆栈的系统，突显了开放和供应商中立技术在大规模 HPC 基础设施中的可行性。\nSAKURAONE 在 HPL 基准测试 (Rmax) 中实现了 33.95 PFLOP/s 的持续性能，在高性能共轭梯度 (HPCG) 基准测试中实现了 396.295 TFLOP/s。对于针对代表 AI 应用的低精度工作负载的 HPL-MxP 基准测试，SAKURAONE 使用 FP8 精度提供了令人印象深刻的 339.86 PFLOP/s。\n该系统包含 100 个计算节点，每个节点配备八个 NVIDIA H100 GPU。它由一个总物理容量为 2 拍字节的全闪存 Lustre 存储子系统支持，提供高吞吐量和低延迟的数据访问。节点间通信通过基于 Rail-Optimized 拓扑的全双向带宽互连实现，其中叶层和脊层通过 800 GbE 链路互连。这种拓扑结构与 RoCEv2 (基于融合以太网的 RDMA 版本 2) 相结合，可实现高速、无损的数据传输，并减轻大规模并行工作负载中的通信瓶颈。", "summary": "SAKURAONE是由日本私营部门SAKURA互联网研究中心开发的托管HPC集群，旨在支持LLM训练等高级AI工作负载。该系统在全球TOP500中排名第49位，并以其独特的基于800 GbE和SONiC的全开放网络堆栈脱颖而出，证明了开放技术在大规模HPC中的可行性。SAKURAONE在HPL和HPL-MxP基准测试中表现出色，拥有100个配备NVIDIA H100 GPU的计算节点和2PB的Lustre存储。", "keywords": "高性能计算, AI平台, 开放网络, SAKURAONE, LLM训练", "comments": "这篇论文展示了私营部门对高性能计算基础设施的重大投资，尤其是在日本。其核心创新在于采用了完全开放的网络堆栈（800 GbE和SONiC），这与行业中常见的专有解决方案形成对比，有力地证明了开放技术在大规模HPC环境中的可行性和竞争力。这对于推动AI平台的透明度和开放性具有重要意义。"}}
{"id": "2507.02183", "title": "Computer Science Education in the Age of Generative AI", "authors": ["Russell Beale"], "categories": ["cs.CY", "cs.HC", "H.5.0; K.3.1; K.3.2"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02183v1", "summary": "Generative AI tools - most notably large language models (LLMs) like ChatGPT\nand Codex - are rapidly revolutionizing computer science education. These tools\ncan generate, debug, and explain code, thereby transforming the landscape of\nprogramming instruction. This paper examines the profound opportunities that AI\noffers for enhancing computer science education in general, from coding\nassistance to fostering innovative pedagogical practices and streamlining\nassessments. At the same time, it highlights challenges including academic\nintegrity concerns, the risk of over-reliance on AI, and difficulties in\nverifying originality. We discuss what computer science educators should teach\nin the AI era, how to best integrate these technologies into curricula, and the\nbest practices for assessing student learning in an environment where AI can\ngenerate code, prototypes and user feedback. Finally, we propose a set of\npolicy recommendations designed to harness the potential of generative AI while\npreserving the integrity and rigour of computer science education. Empirical\ndata and emerging studies are used throughout to support our arguments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02183v1", "cate": "cs.CY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "生成式AI时代的计算机科学教育", "tldr": "生成式AI正在彻底改变计算机科学教育，本文探讨了其机遇、挑战并提出了教学和政策建议。", "motivation": "生成式AI工具（特别是大型语言模型）正在迅速革新计算机科学教育，它们可以生成、调试和解释代码，从而改变了编程教学的格局。", "method": "本文通过审查生成式AI在计算机科学教育中的机遇和挑战，讨论了在AI时代应教授的内容、如何将AI技术融入课程以及评估学生学习的最佳实践，并提出了一系列政策建议。文中使用了经验数据和新兴研究来支持论点。", "result": "论文揭示了AI在提升计算机科学教育方面的巨大机遇（如编码辅助、创新教学实践和简化评估），同时也指出了挑战（如学术诚信、过度依赖AI和原创性验证困难）。最终提出了教学内容、课程整合和评估的最佳实践以及政策建议。", "conclusion": "论文旨在探讨如何在利用生成式AI潜力的同时，保持计算机科学教育的完整性和严谨性，并为此提供了教学实践和政策层面的指导。", "translation": "生成式人工智能工具——最著名的是像ChatGPT和Codex这样的大型语言模型——正在迅速彻底改变计算机科学教育。这些工具可以生成、调试和解释代码，从而改变了编程教学的格局。本文探讨了人工智能在整体上提升计算机科学教育所带来的深远机遇，从编码辅助到促进创新教学实践和简化评估。同时，它也强调了挑战，包括学术诚信问题、过度依赖人工智能的风险以及验证原创性的困难。我们讨论了在人工智能时代计算机科学教育者应该教授什么，如何最好地将这些技术整合到课程中，以及在人工智能可以生成代码、原型和用户反馈的环境中评估学生学习的最佳实践。最后，我们提出了一系列政策建议，旨在利用生成式人工智能的潜力，同时保持计算机科学教育的完整性和严谨性。文中全程使用经验数据和新兴研究来支持我们的论点。", "summary": "本文探讨了生成式AI（如LLMs）对计算机科学教育的革命性影响。它详细分析了AI带来的机遇，包括编码辅助和教学创新，同时也指出了学术诚信、过度依赖等挑战。论文进一步讨论了AI时代计算机科学教育的教学内容、课程整合方法及学生评估最佳实践，并提出了一系列旨在平衡AI潜力与教育严谨性的政策建议，其论点得到了经验数据和新兴研究的支持。", "keywords": "生成式AI, 计算机科学教育, 大型语言模型, 教学实践, 政策建议", "comments": "这篇论文的重要性在于它及时地关注了生成式AI对计算机科学教育的颠覆性影响，并提供了一个全面的视角。它不仅识别了AI带来的机遇，也坦率地指出了随之而来的挑战，并提出了具体的教学和政策建议，这对于教育工作者和政策制定者都具有重要的指导意义。其创新之处在于将理论分析与实践指导相结合，并强调了在拥抱新技术的同时维护教育质量的重要性。"}}
{"id": "2507.02004", "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research", "authors": ["Ruofan Jin", "Zaixi Zhang", "Mengdi Wang", "Le Cong"], "categories": ["cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02004v1", "summary": "The rapid growth of biomedical data, tools, and literature has created a\nfragmented research landscape that outpaces human expertise. While AI agents\noffer a solution, they typically rely on static, manually curated toolsets,\nlimiting their ability to adapt and scale. Here, we introduce STELLA, a\nself-evolving AI agent designed to overcome these limitations. STELLA employs a\nmulti-agent architecture that autonomously improves its own capabilities\nthrough two core mechanisms: an evolving Template Library for reasoning\nstrategies and a dynamic Tool Ocean that expands as a Tool Creation Agent\nautomatically discovers and integrates new bioinformatics tools. This allows\nSTELLA to learn from experience. We demonstrate that STELLA achieves\nstate-of-the-art accuracy on a suite of biomedical benchmarks, scoring\napproximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench:\nDBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6\npercentage points. More importantly, we show that its performance\nsystematically improves with experience; for instance, its accuracy on the\nHumanity's Last Exam benchmark almost doubles with increased trials. STELLA\nrepresents a significant advance towards AI Agent systems that can learn and\ngrow, dynamically scaling their expertise to accelerate the pace of biomedical\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02004v1", "cate": "cs.AI", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "STELLA：用于生物医学研究的自进化大型语言模型智能体", "tldr": "STELLA是一个自进化的AI智能体，旨在解决生物医学研究中数据、工具和文献快速增长导致的碎片化问题。它通过动态扩展工具集和推理策略来自主提升能力，并在多个生物医学基准测试中取得最先进的性能，且能从经验中学习并持续提升。", "motivation": "生物医学数据、工具和文献的快速增长导致了碎片化的研究格局，超出了人类专业知识的范围。现有AI智能体依赖静态、手动策划的工具集，限制了其适应性和可扩展性。", "method": "STELLA采用多智能体架构，通过两种核心机制自主提升能力：1. 一个用于推理策略的进化模板库。2. 一个动态工具海洋，其中工具创建智能体自动发现并集成新的生物信息学工具。这使得STELLA能够从经验中学习。", "result": "STELLA在生物医学基准测试中取得了最先进的准确性：在“人类的最后一次考试：生物医学”中得分约26%，在“LAB-Bench：DBQA”中得分54%，在“LAB-Bench：LitQA”中得分63%。它超越领先模型高达6个百分点。更重要的是，其性能随着经验的增加而系统性提高，例如在“人类的最后一次考试”基准测试中的准确性几乎翻倍。", "conclusion": "STELLA代表了人工智能智能体系统在学习和成长方面的重大进展，能够动态扩展其专业知识，以加速生物医学发现的步伐。", "translation": "生物医学数据、工具和文献的快速增长，造成了碎片化的研究格局，超出了人类的专业能力。虽然人工智能智能体提供了一种解决方案，但它们通常依赖于静态、手动策划的工具集，限制了其适应和扩展的能力。在此，我们介绍了STELLA，这是一种旨在克服这些限制的自进化人工智能智能体。STELLA采用多智能体架构，通过两种核心机制自主提升自身能力：一个用于推理策略的进化模板库，以及一个随着工具创建智能体自动发现和集成新的生物信息学工具而扩展的动态工具海洋。这使得STELLA能够从经验中学习。我们证明，STELLA在生物医学基准测试套件上取得了最先进的准确性，在“人类的最后一次考试：生物医学”中得分约26%，在“LAB-Bench：DBQA”中得分54%，在“LAB-Bench：LitQA”中得分63%，超越领先模型高达6个百分点。更重要的是，我们表明其性能随着经验的增加而系统性地提高；例如，其在“人类的最后一次考试”基准测试中的准确性随着试验次数的增加几乎翻倍。STELLA代表了人工智能智能体系统迈向学习和成长、动态扩展其专业知识以加速生物医学发现的重大进展。", "summary": "STELLA是一个自进化的LLM智能体，旨在解决生物医学研究中数据、工具和文献快速增长导致的碎片化问题。它采用多智能体架构，通过动态的工具集扩展和推理策略演进机制，使其能够从经验中学习并自主提升能力。实验证明，STELLA在多个生物医学基准测试中取得了最先进的性能，并能随着经验的积累显著提高准确性，预示着AI智能体在加速生物医学发现方面的重要潜力。", "keywords": "自进化智能体, 生物医学研究, 大型语言模型, 工具学习, 多智能体系统", "comments": "STELLA的创新点在于其自进化能力，通过动态的工具集成和推理策略演进，克服了传统AI智能体静态工具集的局限性。这种自我学习和成长的机制对于处理快速变化的生物医学领域尤其重要，有望显著加速科学发现。"}}
{"id": "2507.02074", "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02074v1", "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02074v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "视频中碰撞检测的大语言模型：方法、数据集和挑战综述", "tldr": "本文综述了利用大语言模型进行视频碰撞检测的方法、数据集和挑战。", "motivation": "视频馈送中的碰撞检测是智能交通系统中的一个关键问题。大型语言模型（LLM）和视觉语言模型（VLM）的最新发展改变了多模态信息的处理方式，为解决该问题提供了新途径。", "method": "本文是一篇综述论文，具体方法包括：提出了融合策略的结构化分类法，总结了关键数据集，分析了模型架构，比较了性能基准，并讨论了现有的挑战和机遇。", "result": "本文综述了利用大语言模型进行视频碰撞检测的最新方法，并详细介绍了融合策略、关键数据集、模型架构和性能基准，同时讨论了该领域的挑战和机遇。", "conclusion": "本文的综述为视频理解和基础模型这一快速增长的交叉领域未来的研究奠定了基础。", "translation": "视频馈送中的碰撞检测是智能交通系统中的一个关键问题。大型语言模型（LLM）和视觉语言模型（VLM）的最新发展改变了我们处理、推理和总结多模态信息的方式。本文综述了利用LLM从视频数据中进行碰撞检测的最新方法。我们提出了融合策略的结构化分类法，总结了关键数据集，分析了模型架构，比较了性能基准，并讨论了现有的挑战和机遇。我们的综述为视频理解和基础模型这一快速增长的交叉领域未来的研究奠定了基础。", "summary": "本文综述了利用大语言模型（LLMs）和视觉语言模型（VLMs）进行视频碰撞检测的最新方法。它详细介绍了融合策略的分类、关键数据集、模型架构、性能基准，并探讨了该领域的挑战与机遇，旨在为未来研究提供基础。", "keywords": "大语言模型, 碰撞检测, 视频理解, 综述, 智能交通系统", "comments": "这篇综述论文的重要性在于它系统地梳理了LLMs在视频碰撞检测这一关键应用领域中的现状，为研究人员提供了宝贵的资源，指明了未来的研究方向。其创新性在于将LLMs的应用扩展到传统的视频理解任务中，促进了视频理解和基础模型交叉领域的发展。"}}
{"id": "2507.02491", "title": "Engineering an LTLf Synthesis Tool", "authors": ["Alexandre Duret-Lutz", "Shufang Zhu", "Nir Piterman", "Giuseppe de Giacomo", "Moshe Y Vardi"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02491v1", "summary": "The problem of LTLf reactive synthesis is to build a transducer, whose output\nis based on a history of inputs, such that, for every infinite sequence of\ninputs, the conjoint evolution of the inputs and outputs has a prefix that\nsatisfies a given LTLf specification. We describe the implementation of an LTLf\nsynthesizer that outperforms existing tools on our benchmark suite. This is\nbased on a new, direct translation from LTLf to a DFA represented as an array\nof Binary Decision Diagrams (MTBDDs) sharing their nodes. This MTBDD-based\nrepresentation can be interpreted directly as a reachability game that is\nsolved on-the-fly during its construction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02491v1", "cate": "cs.FL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "工程化LTLf综合工具", "tldr": "开发了一种新的LTLf合成器，通过将LTLf直接转换为MTBDD表示的DFA，并在构建过程中即时解决可达性博弈，性能优于现有工具。", "motivation": "LTLf反应式综合问题在于构建一个传感器，使其输出基于输入历史，并确保输入和输出的联合演变满足给定的LTLf规范。", "method": "采用了一种新的直接翻译方法，将LTLf转换为由共享节点的二叉决策图（MTBDD）数组表示的确定性有限自动机（DFA）。这种MTBDD表示可以直接解释为可达性博弈，并在构建过程中即时求解。", "result": "实现了一个LTLf合成器，在基准测试套件上性能优于现有工具。", "conclusion": "通过新的LTLf到MTBDD表示的DFA的直接转换，并结合即时可达性博弈求解，可以构建出高性能的LTLf合成工具。", "translation": "LTLf反应式综合问题旨在构建一个传感器，其输出基于输入历史，使得对于每一个无限输入序列，输入和输出的联合演变都具有一个满足给定LTLf规范的前缀。我们描述了一个LTLf合成器的实现，它在我们的基准测试套件上优于现有工具。这基于LTLf到表示为共享节点二叉决策图（MTBDD）数组的DFA的一种新的直接转换。这种基于MTBDD的表示可以直接解释为可达性博弈，并在其构建过程中即时求解。", "summary": "本文介绍了一种新的LTLf反应式合成工具的实现。该工具通过将LTLf规范直接转换为基于多终端二叉决策图（MTBDD）的确定性有限自动机（DFA），并在此转换过程中即时解决相关的可达性博弈，从而构建出高性能的传感器。实验结果表明，该合成器在基准测试中超越了现有工具。", "keywords": "LTLf合成, 反应式综合, 二叉决策图, DFA, 可达性博弈", "comments": "这篇论文的创新点在于提出了LTLf到MTBDD表示的DFA的直接转换方法，并结合了即时可达性博弈求解。这种方法有效提升了LTLf合成工具的性能，对于形式化方法和自动综合领域具有重要意义。"}}
{"id": "2507.02005", "title": "Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener", "authors": ["Michael A. Kraus", "Helen Bartsch"], "categories": ["cs.CE", "cs.AI"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02005v1", "summary": "This research introduces a unified approach combining Automated Machine\nLearning (AutoML) with Explainable Artificial Intelligence (XAI) to predict\nfatigue strength in welded transverse stiffener details. It integrates\nexpert-driven feature engineering with algorithmic feature creation to enhance\naccuracy and explainability.\n  Based on the extensive fatigue test database regression models - gradient\nboosting, random forests, and neural networks - were trained using AutoML under\nthree feature schemes: domain-informed, algorithmic, and combined. This allowed\na systematic comparison of expert-based versus automated feature selection.\n  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The\ndomain-informed model $\\mathcal M_2$ achieved the best balance: test RMSE\n$\\approx$ 30.6 MPa and $R^2 \\approx 0.780% over the full $\\Delta\n\\sigma_{c,50\\%}$ range, and RMSE $\\approx$ 13.4 MPa and $R^2 \\approx 0.527%\nwithin the engineering-relevant 0 - 150 MPa domain. The denser-feature model\n($\\mathcal M_3$) showed minor gains during training but poorer generalization,\nwhile the simpler base-feature model ($\\mathcal M_1$) performed comparably,\nconfirming the robustness of minimalist designs.\n  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress\nrange $\\Delta \\sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG\ndressing vs. as-welded) as dominant predictors. Secondary geometric factors -\nplate width, throat thickness, stiffener height - also significantly affected\nfatigue life.\n  This framework demonstrates that integrating AutoML with XAI yields accurate,\ninterpretable, and robust fatigue strength models for welded steel structures.\nIt bridges data-driven modeling with engineering validation, enabling\nAI-assisted design and assessment. Future work will explore probabilistic\nfatigue life modeling and integration into digital twin environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02005v1", "cate": "cs.CE", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "通过特征工程和自动化可解释机器学习发现疲劳强度模型并应用于焊接横向加劲肋", "tldr": "本研究结合自动化机器学习（AutoML）和可解释人工智能（XAI），成功预测了焊接横向加劲肋的疲劳强度，并识别了关键影响因素，为AI辅助设计和评估提供了准确且可解释的模型。", "motivation": "本研究旨在开发一种统一的方法，结合自动化机器学习（AutoML）和可解释人工智能（XAI），以预测焊接横向加劲肋细节的疲劳强度。", "method": "研究采用了一个统一的方法，结合AutoML和XAI，并通过专家驱动的特征工程与算法特征创建相结合。基于广泛的疲劳试验数据库，使用AutoML训练了梯度提升、随机森林和神经网络等回归模型，并采用了领域知情、算法和组合三种特征方案进行系统比较。利用SHAP和特征重要性等XAI方法来识别关键预测因子。", "result": "集成方法（如CatBoost、LightGBM）表现最佳。领域知情模型M2在全范围内的测试RMSE约为30.6 MPa，R²约为0.780%，在工程相关0-150 MPa范围内，RMSE约为13.4 MPa，R²约为0.527%。特征密度更高的模型M3在训练时有小幅提升但泛化能力较差，而更简单的基础特征模型M1表现相当，证实了极简设计的稳健性。XAI方法识别出应力比R、应力范围Δσi、屈服强度ReH和焊后处理（TIG修整与焊态）为主要预测因子，次要几何因素（板宽、焊喉厚度、加劲肋高度）也显著影响疲劳寿命。", "conclusion": "该框架表明，将AutoML与XAI相结合可以为焊接钢结构提供准确、可解释且稳健的疲劳强度模型。它将数据驱动建模与工程验证相结合，实现了AI辅助设计和评估。", "translation": "本研究引入了一种结合自动化机器学习（AutoML）和可解释人工智能（XAI）的统一方法，用于预测焊接横向加劲肋细节的疲劳强度。它将专家驱动的特征工程与算法特征创建相结合，以提高准确性和可解释性。\n基于广泛的疲劳试验数据库，使用AutoML在三种特征方案下训练了回归模型——梯度提升、随机森林和神经网络：领域知情、算法和组合。这使得可以系统地比较基于专家的方法与自动化特征选择。\n集成方法（例如CatBoost、LightGBM）表现最佳。领域知情模型M2实现了最佳平衡：在全Δσc,50%范围内，测试RMSE≈30.6 MPa，R²≈0.780%；在工程相关0-150 MPa范围内，RMSE≈13.4 MPa，R²≈0.527%。特征密度更高的模型（M3）在训练期间显示出微小增益，但泛化能力较差，而更简单的基础特征模型（M1）表现相当，证实了极简设计的稳健性。\nXAI方法（SHAP和特征重要性）将应力比R、应力范围Δσi、屈服强度ReH和焊后处理（TIG修整与焊态）确定为主要预测因子。次要几何因素——板宽、焊喉厚度、加劲肋高度——也显著影响疲劳寿命。\n该框架表明，将AutoML与XAI相结合可以为焊接钢结构提供准确、可解释且稳健的疲劳强度模型。它将数据驱动建模与工程验证相结合，实现了AI辅助设计和评估。未来的工作将探索概率疲劳寿命建模并集成到数字孪生环境中。", "summary": "本研究提出了一种结合自动化机器学习（AutoML）和可解释人工智能（XAI）的统一框架，用于预测焊接横向加劲肋的疲劳强度。通过整合专家知识与算法特征创建，并利用AutoML训练多种回归模型，实现了对疲劳强度的高精度预测。结果显示，集成方法表现优异，领域知情模型在准确性和稳健性之间取得了良好平衡。XAI方法成功识别了应力比、应力范围、屈服强度和焊后处理等关键预测因子。该框架为焊接钢结构提供了准确、可解释且稳健的疲劳强度模型，有助于AI辅助设计与评估。", "keywords": "疲劳强度, AutoML, 可解释人工智能, 焊接结构, 特征工程", "comments": "该研究的创新之处在于将AutoML与XAI相结合，不仅提高了疲劳强度预测的准确性，还增强了模型的可解释性，弥合了数据驱动建模与工程验证之间的鸿沟。通过系统比较不同特征方案，证实了极简设计的稳健性。其重要性在于为焊接钢结构的AI辅助设计和评估提供了可行且可靠的工具，并识别了影响疲劳寿命的关键工程参数。"}}
{"id": "2507.02078", "title": "Enhancing Power Flow Estimation with Topology-Aware Gated Graph Neural Networks", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Wencong Su", "Van-Hai Bui"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02078v1", "summary": "Accurate and scalable surrogate models for AC power flow are essential for\nreal-time grid monitoring, contingency analysis, and decision support in\nincreasingly dynamic and inverter-dominated power systems. However, most\nexisting surrogates fall short of practical deployment due to their limited\ncapacity to capture long-range nonlinear dependencies in meshed transmission\nnetworks and their weak enforcement of physical laws. These models often\nrequire extensive hyperparameter tuning, exhibit poor generalization under\ntopology changes or large load swings, and typically do not quantify\nuncertainty or scale well beyond a few hundred buses. To address these\nchallenges, this paper proposes a \\textit{gated graph neural network (GGNN)}\nsurrogate for AC power-flow estimation under topological uncertainty. The model\nis trained across multiple IEEE benchmark networks of varying size and\ncomplexity, each incorporating randomized line contingencies and up to 40\\%\nload variation. To improve robustness and generalization, we explore both\nconventional supervised learning and physics-informed self-supervised training\nstrategies. Comparative evaluations show that the proposed GGNN consistently\noutperforms prior GNN-based surrogates, achieving predictions closely aligned\nwith Newton--Raphson solutions. By embedding operational constraints directly\ninto the architecture and loss function, the model ensures physical consistency\nand delivers a lightweight, accurate, and scalable tool for real-time grid\noperations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02078v1", "cate": "eess.SY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "使用拓扑感知门控图神经网络增强潮流估计", "tldr": "本文提出了一种门控图神经网络（GGNN）代理模型，用于在拓扑不确定性下进行交流潮流估计，该模型在准确性、可扩展性和物理一致性方面优于现有方法。", "motivation": "现有交流潮流代理模型在捕获电网中的长程非线性依赖性、强制执行物理定律、泛化能力、不确定性量化以及可扩展性方面存在局限性，难以实际部署。需要准确且可扩展的代理模型用于实时电网监控、偶发事件分析和决策支持。", "method": "本文提出了一种门控图神经网络（GGNN）代理模型，用于在拓扑不确定性下进行交流潮流估计。该模型在多个不同规模和复杂度的IEEE基准网络上进行训练，其中包括随机线路故障和高达40%的负荷变化。研究探索了传统的监督学习和物理信息自监督训练策略，并将运行约束直接嵌入到网络架构和损失函数中。", "result": "所提出的GGNN模型持续优于先前的基于GNN的代理模型，其预测结果与牛顿-拉夫逊法解高度一致。", "conclusion": "该模型通过将运行约束直接嵌入到架构和损失函数中，确保了物理一致性，并提供了一个轻量、准确且可扩展的工具，适用于实时电网运行。", "translation": "交流潮流的精确和可扩展代理模型对于日益动态化和逆变器主导的电力系统中的实时电网监控、偶发事件分析和决策支持至关重要。然而，大多数现有代理模型由于其捕获网状输电网络中长程非线性依赖的能力有限以及对物理定律的弱强制执行，未能实现实际部署。这些模型通常需要大量的超参数调整，在拓扑变化或大负荷波动下表现出较差的泛化能力，并且通常不量化不确定性或难以扩展到数百个母线以上。为了解决这些挑战，本文提出了一种门控图神经网络（GGNN）代理模型，用于在拓扑不确定性下进行交流潮流估计。该模型在多个不同规模和复杂度的IEEE基准网络上进行训练，每个网络都包含随机线路故障和高达40%的负荷变化。为了提高鲁棒性和泛化能力，我们探索了传统的监督学习和物理信息自监督训练策略。对比评估表明，所提出的GGNN持续优于先前的基于GNN的代理模型，其预测结果与牛顿-拉夫逊法解高度一致。通过将运行约束直接嵌入到架构和损失函数中，该模型确保了物理一致性，并提供了一个轻量、准确且可扩展的工具，适用于实时电网运行。", "summary": "本文针对现有交流潮流代理模型在处理长程非线性依赖、物理约束强制执行、泛化能力和可扩展性方面的不足，提出了一种拓扑感知门控图神经网络（GGNN）模型。该模型通过在多种IEEE基准网络上进行训练，并结合监督学习与物理信息自监督策略，显著提升了潮流估计的准确性与鲁棒性。实验结果表明，该GGNN模型性能优于现有GNN方法，其预测结果与牛顿-拉夫逊解高度吻合，且能通过嵌入运行约束确保物理一致性，提供了一个轻量、精确且可扩展的实时电网运行工具。", "keywords": "潮流估计, 图神经网络, 门控图神经网络, 拓扑不确定性, 物理信息学习", "comments": "这篇论文的创新点在于提出了一个拓扑感知的门控图神经网络（GGNN）来解决交流潮流估计中的挑战。通过结合物理信息自监督训练和将运行约束直接嵌入模型，它显著提高了模型的准确性、泛化能力和物理一致性。这对于实时电网监控和决策支持具有重要意义，尤其是在动态和逆变器主导的电力系统中。"}}
{"id": "2507.01975", "title": "Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows", "authors": ["Mengtao Yan", "Qi Wang", "Haining Wang", "Ruizhi Chengze", "Yi Zhang", "Hongsheng Liu", "Zidong Wang", "Fan Yu", "Qi Qi", "Hao Sun"], "categories": ["cs.LG", "cs.AI", "physics.flu-dyn"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 12 figures, accepted at KDD 2025 (ACM SIGKDD Conference on Knowledge Discovery and Data Mining)", "url": "http://arxiv.org/abs/2507.01975v1", "summary": "Simulation of fluid flows is crucial for modeling physical phenomena like\nmeteorology, aerodynamics, and biomedicine. Classical numerical solvers often\nrequire fine spatiotemporal grids to satisfy stability, consistency, and\nconvergence conditions, leading to substantial computational costs. Although\nmachine learning has demonstrated better efficiency, they typically suffer from\nissues of interpretability, generalizability, and data dependency. Hence, we\npropose a learnable and differentiable finite volume solver, called LDSolver,\ndesigned for efficient and accurate simulation of fluid flows on spatiotemporal\ncoarse grids. LDSolver comprises two key components: (1) a differentiable\nfinite volume solver, and (2) an learnable module providing equivalent\napproximation for fluxes (derivatives and interpolations), and temporal error\ncorrection on coarse grids. Even with limited training data (e.g., only a few\ntrajectories), our model could accelerate the simulation while maintaining a\nhigh accuracy with superior generalizability. Experiments on different flow\nsystems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver\nachieves state-of-the-art performance, surpassing baseline models with notable\nmargins.", "comment": "19 pages, 12 figures, accepted at KDD 2025 (ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining)", "pdf_url": "http://arxiv.org/pdf/2507.01975v1", "cate": "cs.LG", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "可学习-可微分有限体积求解器用于加速流体模拟", "tldr": "本文提出了LDSolver，一种可学习且可微分的有限体积求解器，旨在通过在粗网格上实现高效准确的流体模拟，解决了传统方法计算成本高和纯机器学习模型泛化性差的问题。它在有限数据下表现出色，并在多种流体系统上达到了最先进的性能。", "motivation": "流体模拟对于气象、空气动力学和生物医学等物理现象的建模至关重要。经典的数值求解器需要精细的时空网格来满足稳定性、一致性和收敛性条件，导致巨大的计算成本。尽管机器学习方法提高了效率，但它们通常面临可解释性、泛化性和数据依赖性问题。", "method": "我们提出了一个可学习且可微分的有限体积求解器LDSolver。LDSolver包含两个关键组件：1) 一个可微分的有限体积求解器；2) 一个可学习模块，为通量（导数和插值）提供等效近似，并对粗网格上的时间误差进行校正。", "result": "即使在有限的训练数据（例如，只有少数轨迹）下，LDSolver也能加速模拟，同时保持高精度和卓越的泛化能力。在不同流体系统（如 Burgers、衰减流、受迫流和剪切流）上的实验表明，LDSolver 实现了最先进的性能，显著超越了基线模型。", "conclusion": "LDSolver成功地将物理驱动的有限体积求解器与数据驱动的可学习模块相结合，为流体模拟提供了一种高效、精确且具有卓越泛化能力的新范式，尤其适用于粗网格和有限数据场景。", "translation": "流体流动的模拟对于气象、空气动力学和生物医学等物理现象的建模至关重要。经典的数值求解器通常需要精细的时空网格以满足稳定性、一致性和收敛性条件，导致巨大的计算成本。尽管机器学习已表现出更高的效率，但它们通常存在可解释性、泛化性和数据依赖性问题。因此，我们提出了一种可学习且可微分的有限体积求解器，称为LDSolver，旨在在时空粗网格上高效准确地模拟流体流动。LDSolver包含两个关键组件：(1) 一个可微分的有限体积求解器，以及 (2) 一个可学习模块，为通量（导数和插值）提供等效近似，并对粗网格上的时间误差进行校正。即使训练数据有限（例如，只有少数轨迹），我们的模型也能加速模拟，同时保持高精度和卓越的泛化能力。在不同流体系统（例如，Burgers、衰减流、受迫流和剪切流）上的实验表明，LDSolver 实现了最先进的性能，显著超越了基线模型。", "summary": "本文提出了LDSolver，一种新型的可学习-可微分有限体积求解器，旨在克服传统数值模拟计算成本高昂和纯机器学习方法泛化性差的问题。LDSolver结合了可微分的有限体积求解器和一个可学习模块，用于在粗网格上进行通量近似和误差校正。实验证明，即使在有限的训练数据下，LDSolver也能在多种流体系统上实现加速模拟，同时保持高精度和优越的泛化能力，性能显著超越现有基线模型。", "keywords": "流体模拟, 有限体积法, 机器学习, 可微分, 计算流体力学", "comments": "这篇论文的创新点在于将可学习模块与可微分的有限体积求解器巧妙结合，有效解决了传统数值模拟计算成本高昂和纯机器学习方法泛化性、解释性不足的痛点。LDSolver能够在粗网格和有限数据下实现高精度模拟，这对于需要快速、精确模拟的实际工程和科学应用具有显著的实用价值和潜在影响。"}}
{"id": "2507.02243", "title": "Derivative-Free Optimization-Empowered Wireless Channel Reconfiguration for 6G", "authors": ["Peilan Wang", "Jun Fang", "Xianlong Zeng", "Bin Wang", "Zhi Chen", "Yonina C. Eldar"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2507.02243v1", "summary": "Reconfigurable antennas, including reconfigurable intelligent surface (RIS),\nmovable antenna (MA), fluid antenna (FA), and other advanced antenna\ntechniques, have been studied extensively in the context of reshaping wireless\npropagation environments for 6G and beyond wireless communications.\nNevertheless, how to reconfigure/optimize the real-time controllable\ncoefficients to achieve a favorable end-to-end wireless channel remains a\nsubstantial challenge, as it usually requires accurate modeling of the complex\ninteraction between the reconfigurable devices and the electromagnetic waves,\nas well as knowledge of implicit channel propagation parameters. In this paper,\nwe introduce a derivative-free optimization (a.k.a., zeroth-order (ZO)\noptimization) technique to directly optimize reconfigurable coefficients to\nshape the wireless end-to-end channel, without the need of channel modeling and\nestimation of the implicit environmental propagation parameters. We present the\nfundamental principles of ZO optimization and discuss its potential advantages\nin wireless channel reconfiguration. Two case studies for RIS and movable\nantenna-enabled single-input single-output (SISO) systems are provided to show\nthe superiority of ZO-based methods as compared to state-of-the-art techniques.\nFinally, we outline promising future research directions and offer concluding\ninsights on derivative-free optimization for reconfigurable antenna\ntechnologies.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2507.02243v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "6G中基于无导数优化的无线信道重构", "tldr": "本文提出一种无导数优化技术，直接优化可重构天线的系数，以重塑无线信道，无需复杂的信道建模和参数估计，并通过案例研究验证了其优越性。", "motivation": "6G及未来通信中，可重构天线（如RIS、MA、FA）用于重塑无线传播环境，但如何实时优化可控系数以实现有利的端到端无线信道仍是巨大挑战，因为它通常需要精确建模可重构设备与电磁波的复杂交互以及隐式信道传播参数的知识。", "method": "引入无导数优化（又称零阶优化）技术，直接优化可重构系数来塑造无线端到端信道，无需信道建模和隐式环境传播参数的估计。阐述了零阶优化的基本原理，并讨论了其在无线信道重构中的潜在优势。", "result": "通过RIS和可移动天线支持的SISO系统的两个案例研究，展示了基于零阶优化方法相较于现有技术的优越性。", "conclusion": "概述了未来有前景的研究方向，并为可重构天线技术的无导数优化提供了总结性见解。", "translation": "可重构天线，包括可重构智能表面（RIS）、可移动天线（MA）、流体天线（FA）以及其他先进天线技术，在6G及未来无线通信中，为了重塑无线传播环境而得到了广泛研究。然而，如何重构/优化实时可控系数以实现有利的端到端无线信道仍然是一个巨大的挑战，因为它通常需要精确建模可重构设备与电磁波之间的复杂相互作用，以及了解隐式信道传播参数。在本文中，我们引入了一种无导数优化（又称零阶（ZO）优化）技术，可以直接优化可重构系数来塑造无线端到端信道，而无需信道建模和隐式环境传播参数的估计。我们阐述了零阶优化的基本原理，并讨论了其在无线信道重构中的潜在优势。提供了RIS和可移动天线支持的单输入单输出（SISO）系统的两个案例研究，以显示基于零阶优化方法相较于现有技术的优越性。最后，我们概述了有前景的未来研究方向，并为可重构天线技术的无导数优化提供了总结性见解。", "summary": "本文针对6G无线通信中可重构天线信道重构的挑战，提出了一种无导数优化（零阶优化）技术。该方法能够直接优化可重构系数以塑造端到端无线信道，避免了复杂的信道建模和参数估计需求。论文阐述了零阶优化原理及其优势，并通过RIS和可移动天线SISO系统的案例研究验证了其优于现有技术的性能。最后，展望了该领域的未来研究方向。", "keywords": "无导数优化, 零阶优化, 可重构天线, 6G, 无线信道重构", "comments": "本文提出了一种创新的方法来解决可重构天线在6G中信道重构的挑战，即利用无导数优化。其主要创新在于避免了传统方法中对复杂信道建模和隐式参数估计的依赖，大大简化了优化过程。这对于实际部署和实时系统具有重要意义，因为它降低了系统复杂性和计算开销。通过案例研究展示了其优越性，表明该方法在实际应用中具有巨大潜力。"}}
{"id": "2507.02003", "title": "Unsupervised Cardiac Video Translation Via Motion Feature Guided Diffusion Model", "authors": ["Swakshar Deb", "Nian Wu", "Frederick H. Epstein", "Miaomiao Zhang"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02003v1", "summary": "This paper presents a novel motion feature guided diffusion model for\nunpaired video-to-video translation (MFD-V2V), designed to synthesize dynamic,\nhigh-contrast cine cardiac magnetic resonance (CMR) from lower-contrast,\nartifact-prone displacement encoding with stimulated echoes (DENSE) CMR\nsequences. To achieve this, we first introduce a Latent Temporal\nMulti-Attention (LTMA) registration network that effectively learns more\naccurate and consistent cardiac motions from cine CMR image videos. A\nmulti-level motion feature guided diffusion model, equipped with a specialized\nSpatio-Temporal Motion Encoder (STME) to extract fine-grained motion\nconditioning, is then developed to improve synthesis quality and fidelity. We\nevaluate our method, MFD-V2V, on a comprehensive cardiac dataset, demonstrating\nsuperior performance over the state-of-the-art in both quantitative metrics and\nqualitative assessments. Furthermore, we show the benefits of our synthesized\ncine CMRs improving downstream clinical and analytical tasks, underscoring the\nbroader impact of our approach. Our code is publicly available at\nhttps://github.com/SwaksharDeb/MFD-V2V.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02003v1", "cate": "eess.IV", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "无监督心脏视频翻译通过运动特征引导扩散模型", "tldr": "提出MFD-V2V模型，利用运动特征引导扩散模型将低对比度DENSE CMR视频转换为高对比度cine CMR视频，表现优于现有技术并有益于下游任务。", "motivation": "将低对比度、易受伪影影响的DENSE CMR序列转换为动态、高对比度的cine CMR视频，以提高医学图像质量，改善下游临床和分析任务。", "method": "引入Latent Temporal Multi-Attention (LTMA) 配准网络，从cine CMR图像视频中学习更准确和一致的心脏运动。开发多级运动特征引导扩散模型（MFD-V2V），配备专门的时空运动编码器（STME）以提取细粒度运动条件，从而提高合成质量和保真度。该模型是无监督的。", "result": "MFD-V2V在综合心脏数据集上评估，在定量指标和定性评估方面均优于现有技术。合成的cine CMRs改善了下游临床和分析任务。", "conclusion": "MFD-V2V模型在心脏视频翻译方面表现出色，能够生成高质量的cine CMRs，并对下游临床和分析任务产生积极影响，具有广泛的应用前景。", "translation": "本文提出了一种新颖的运动特征引导扩散模型，用于无配对视频到视频翻译（MFD-V2V），旨在从低对比度、易受伪影影响的位移编码受激回波（DENSE）CMR序列中合成动态、高对比度的电影心脏磁共振（CMR）图像。为实现这一目标，我们首先引入了一个潜在时间多注意力（LTMA）配准网络，该网络能有效地从电影CMR图像视频中学习更准确和一致的心脏运动。随后，开发了一个多级运动特征引导扩散模型，该模型配备了专门的时空运动编码器（STME）以提取细粒度运动条件，从而提高合成质量和保真度。我们在一个综合心脏数据集上评估了我们的方法MFD-V2V，结果表明其在定量指标和定性评估方面均优于现有技术。此外，我们展示了我们合成的电影CMRs在改善下游临床和分析任务方面的益处，强调了我们方法的更广泛影响。我们的代码已在https://github.com/SwaksharDeb/MFD-V2V 公开。", "summary": "本文提出MFD-V2V，一个无监督的运动特征引导扩散模型，用于将低质量DENSE CMR视频转换为高质量cine CMR视频。该模型包含LTMA网络学习心脏运动和STME提取细粒度运动特征。实验证明MFD-V2V在性能上优于现有技术，并且其合成的图像有助于改善下游临床任务。", "keywords": "心脏视频翻译, 扩散模型, 运动特征, 无监督学习, CMR", "comments": "该论文的创新点在于结合运动特征引导扩散模型进行无监督视频到视频翻译，特别是针对心脏CMR图像。引入LTMA网络和STME增强了模型对心脏运动的理解和细粒度特征提取能力，有效解决了DENSE CMR图像质量低的问题。其在下游临床任务中的应用潜力，强调了该研究的实际重要性。"}}
{"id": "2507.02061", "title": "New algorithms for girth and cycle detection", "authors": ["Liam Roditty", "Plia Trabelsi"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02061v1", "summary": "Let $G=(V,E)$ be an unweighted undirected graph with $n$ vertices and $m$\nedges. Let $g$ be the girth of $G$, that is, the length of a shortest cycle in\n$G$. We present a randomized algorithm with a running time of\n$\\tilde{O}\\big(\\ell \\cdot n^{1 + \\frac{1}{\\ell - \\varepsilon}}\\big)$ that\nreturns a cycle of length at most $ 2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil\n- 2 \\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil\n\\right\\rfloor, $ where $\\ell \\geq 2$ is an integer and $\\varepsilon \\in [0,1]$,\nfor every graph with $g = polylog(n)$.\n  Our algorithm generalizes an algorithm of Kadria \\etal{} [SODA'22] that\ncomputes a cycle of length at most $4\\left\\lceil \\frac{g}{2} \\right\\rceil -\n2\\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor $\nin $\\tilde{O}\\big(n^{1 + \\frac{1}{2 - \\varepsilon}}\\big)$ time. Kadria \\etal{}\npresented also an algorithm that finds a cycle of length at most $ 2\\ell\n\\left\\lceil \\frac{g}{2} \\right\\rceil $ in $\\tilde{O}\\big(n^{1 +\n\\frac{1}{\\ell}}\\big)$ time, where $\\ell$ must be an integer. Our algorithm\ngeneralizes this algorithm, as well, by replacing the integer parameter $\\ell$\nin the running time exponent with a real-valued parameter $\\ell - \\varepsilon$,\nthereby offering greater flexibility in parameter selection and enabling a\nbroader spectrum of combinations between running times and cycle lengths.\n  We also show that for sparse graphs a better tradeoff is possible, by\npresenting an $\\tilde{O}(\\ell\\cdot m^{1+1/(\\ell-\\varepsilon)})$ time randomized\nalgorithm that returns a cycle of length at most $2\\ell(\\lfloor\n\\frac{g-1}{2}\\rfloor) - 2(\\lfloor \\varepsilon \\lfloor \\frac{g-1}{2}\\rfloor\n\\rfloor+1)$, where $\\ell\\geq 3$ is an integer and $\\varepsilon\\in [0,1)$, for\nevery graph with $g=polylog(n)$.\n  To obtain our algorithms we develop several techniques and introduce a formal\ndefinition of hybrid cycle detection algorithms. [...]", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02061v1", "cate": "cs.DS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "图的周长和环检测的新算法", "tldr": "本文提出了新的随机算法，用于在无权无向图中检测环和估计图的周长，通过引入实值参数，提高了算法的灵活性和性能，并为稀疏图提供了更好的权衡。", "motivation": "本文旨在为无权无向图提供新的算法，以检测近似图的周长的环。研究动机在于改进和推广现有算法（如Kadria等人的工作），通过引入更灵活的参数选择，实现运行时间和检测到的环长度之间更广泛的组合，并为稀疏图提供更好的性能权衡。", "method": "本文开发了新的随机算法。核心方法包括推广Kadria等人的现有算法，具体做法是将运行时间指数中的整数参数替换为实值参数（$\\ell - \\varepsilon$），从而提供更大的参数选择灵活性。此外，作者还开发了几种新技术，并引入了混合环检测算法的正式定义。", "result": "1. 提出了一种随机算法，在$\tilde{O}\\big(\\ell \\cdot n^{1 + \\frac{1}{\\ell - \\varepsilon}}\\big)$的运行时间内返回长度至多为$ 2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil - 2 \\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor$的环，适用于$g = polylog(n)$的图。该算法推广了Kadria等人的相关工作。\n2. 对于稀疏图，提出了一种在$\tilde{O}(\\ell\\cdot m^{1+1/(\\ell-\\varepsilon)})$时间内运行的随机算法，返回长度至多为$2\\ell(\\lfloor \\frac{g-1}{2}\\rfloor) - 2(\\lfloor \\varepsilon \\lfloor \\frac{g-1}{2}\\rfloor \\rfloor+1)$的环。", "conclusion": "本文提出的算法在参数选择上提供了更大的灵活性，实现了运行时间和环长度之间更广泛的组合。对于稀疏图，这些算法能够提供更好的权衡。", "translation": "设$G=(V,E)$是一个有$n$个顶点和$m$条边的无权无向图。设$g$是$G$的周长，即$G$中最短环的长度。我们提出一种随机算法，其运行时间为$\tilde{O}\\big(\\ell \\cdot n^{1 + \\frac{1}{\\ell - \\varepsilon}}\\big)$，返回一个长度至多为$ 2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil - 2 \\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor$的环，其中$\\ell \\geq 2$是整数且$\\varepsilon \\in [0,1]$，适用于所有$g = polylog(n)$的图。\n我们的算法推广了Kadria等人的算法[SODA'22]，该算法在$\tilde{O}\\big(n^{1 + \\frac{1}{2 - \\varepsilon}}\\big)$时间内计算出长度至多为$4\\left\\lceil \\frac{g}{2} \\right\\rceil - 2\\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor$的环。Kadria等人还提出了一种算法，在$\tilde{O}\\big(n^{1 + \\frac{1}{\\ell}}\\big)$时间内找到长度至多为$ 2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil $的环，其中$\\ell$必须是整数。我们的算法也推广了该算法，通过将运行时间指数中的整数参数$\\ell$替换为实值参数$\\ell - \\varepsilon$，从而在参数选择上提供了更大的灵活性，并实现了运行时间和环长度之间更广泛的组合。\n我们还表明，对于稀疏图，可以通过提出一种$\tilde{O}(\\ell\\cdot m^{1+1/(\\ell-\\varepsilon)})$时间随机算法来实现更好的权衡，该算法返回一个长度至多为$2\\ell(\\lfloor \\frac{g-1}{2}\\rfloor) - 2(\\lfloor \\varepsilon \\lfloor \\frac{g-1}{2}\\rfloor \\rfloor+1)$的环，其中$\\ell\\geq 3$是整数且$\\varepsilon\\in [0,1)$，适用于所有$g=polylog(n)$的图。\n为了获得我们的算法，我们开发了几种技术，并引入了混合环检测算法的正式定义。", "summary": "本文介绍了用于在无权无向图中检测环和估计图的周长的新型随机算法。这些算法通过在运行时间指数中引入实值参数，推广并改进了Kadria等人的先前工作，从而在运行时间和检测到的环长度之间提供了更大的灵活性。论文还为稀疏图展示了更好的性能权衡，并引入了混合环检测算法的正式定义。", "keywords": "周长, 环检测, 随机算法, 图算法, 稀疏图", "comments": "本文的创新之处在于通过引入实值参数，推广了现有的图环检测算法，这在平衡计算时间和检测到的环长度精度方面提供了更大的灵活性。针对稀疏图的特定应用也突出了其实用性。此外，混合环检测算法的正式定义为该领域做出了理论贡献。"}}
{"id": "2507.02115", "title": "Pronunciation Editing for Finnish Speech using Phonetic Posteriorgrams", "authors": ["Zirui Li", "Lauri Juvela", "Mikko Kurimo"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages; 1 figure; Accepted to Speech Synthesis Workshop 2025 (SSW13)", "url": "http://arxiv.org/abs/2507.02115v1", "summary": "Synthesizing second-language (L2) speech is potentially highly valued for L2\nlanguage learning experience and feedback. However, due to the lack of L2\nspeech synthesis datasets, it is difficult to synthesize L2 speech for\nlow-resourced languages. In this paper, we provide a practical solution for\nediting native speech to approximate L2 speech and present PPG2Speech, a\ndiffusion-based multispeaker Phonetic-Posteriorgrams-to-Speech model that is\ncapable of editing a single phoneme without text alignment. We use Matcha-TTS's\nflow-matching decoder as the backbone, transforming Phonetic Posteriorgrams\n(PPGs) to mel-spectrograms conditioned on external speaker embeddings and\npitch. PPG2Speech strengthens the Matcha-TTS's flow-matching decoder with\nClassifier-free Guidance (CFG) and Sway Sampling. We also propose a new\ntask-specific objective evaluation metric, the Phonetic Aligned Consistency\n(PAC), between the edited PPGs and the PPGs extracted from the synthetic speech\nfor editing effects. We validate the effectiveness of our method on Finnish, a\nlow-resourced, nearly phonetic language, using approximately 60 hours of data.\nWe conduct objective and subjective evaluations of our approach to compare its\nnaturalness, speaker similarity, and editing effectiveness with TTS-based\nediting. Our source code is published at\nhttps://github.com/aalto-speech/PPG2Speech.", "comment": "5 pages; 1 figure; Accepted to Speech Synthesis Workshop 2025 (SSW13)", "pdf_url": "http://arxiv.org/pdf/2507.02115v1", "cate": "eess.AS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "芬兰语语音发音编辑使用语音后验概率图", "tldr": "本文提出了一种名为PPG2Speech的模型，用于编辑母语语音以近似第二语言语音，尤其适用于资源匮乏的语言。", "motivation": "由于缺乏第二语言语音合成数据集，难以对资源匮乏的语言进行第二语言语音合成，但L2语音合成对L2语言学习具有潜在高价值。", "method": "本文提出PPG2Speech模型，一个基于扩散的多说话人语音后验概率图到语音模型，能够在没有文本对齐的情况下编辑单个音素。该模型以Matcha-TTS的流匹配解码器为骨干，将语音后验概率图（PPG）转换为梅尔频谱图，并结合无分类器指导（CFG）和Sway采样。此外，还提出了一种新的任务特定客观评估指标——语音对齐一致性（PAC），用于评估编辑效果。", "result": "该方法在芬兰语（一种资源匮乏且近乎语音的语言）上进行了验证，使用了大约60小时的数据。通过客观和主观评估，该方法在自然度、说话人相似性和编辑效果方面与基于TTS的编辑进行了比较。", "conclusion": "该方法在芬兰语上被验证是有效的，能够实现对母语语音的编辑以近似第二语言语音。", "translation": "合成第二语言（L2）语音对于L2语言学习体验和反馈具有潜在的高价值。然而，由于缺乏L2语音合成数据集，对于资源匮乏的语言来说，合成L2语音是困难的。在本文中，我们提供了一种实用的解决方案，用于编辑母语语音以近似L2语音，并提出了PPG2Speech，一个基于扩散的多说话人语音后验概率图（Phonetic-Posteriorgrams-to-Speech）模型，该模型能够在没有文本对齐的情况下编辑单个音素。我们使用Matcha-TTS的流匹配解码器作为骨干，将语音后验概率图（PPG）转换为梅尔频谱图，并以外部说话人嵌入和音高为条件。PPG2Speech通过无分类器指导（Classifier-free Guidance, CFG）和Sway采样加强了Matcha-TTS的流匹配解码器。我们还提出了一种新的任务特定客观评估指标——语音对齐一致性（Phonetic Aligned Consistency, PAC），用于衡量编辑后的PPG与合成语音中提取的PPG之间的编辑效果。我们使用大约60小时的数据在芬兰语（一种资源匮乏、近乎语音的语言）上验证了我们方法的有效性。我们对我们的方法进行了客观和主观评估，以比较其自然度、说话人相似性和编辑效果与基于TTS的编辑。我们的源代码已发布在https://github.com/aalto-speech/PPG2Speech。", "summary": "本文针对资源匮乏语言第二语言语音合成中数据不足的问题，提出了一种实用的语音编辑解决方案——PPG2Speech模型。该模型是一个基于扩散的多说话人语音后验概率图到语音模型，能够无需文本对齐地编辑母语语音中的单个音素，使其近似第二语言语音。模型以Matcha-TTS的流匹配解码器为基础，并结合了无分类器指导和Sway采样。此外，本文还提出了一种新的任务特定客观评估指标PAC。该方法在芬兰语上进行了验证，并通过客观和主观评估证明了其在自然度、说话人相似性和编辑效果方面的有效性。", "keywords": "语音编辑, 第二语言语音合成, 语音后验概率图, 扩散模型, 芬兰语", "comments": "本文提出了一种新颖且实用的方法，通过编辑母语语音来近似第二语言语音，有效解决了资源匮乏语言L2语音合成的数据难题。其创新点在于引入了PPG2Speech模型，该模型基于扩散技术，能够实现无需文本对齐的单个音素编辑，这在语音编辑领域具有重要意义。同时，提出的PAC评估指标为任务特定评估提供了新的思路。该研究对于L2语言学习和语音技术发展具有潜在价值。"}}
{"id": "2507.02000", "title": "Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System", "authors": ["Yongsen Zheng", "Zongxuan Xie", "Guohua Wang", "Ziyao Liu", "Liang Lin", "Kwok-Yan Lam"], "categories": ["cs.IR", "cs.CL", "cs.MM"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02000v1", "summary": "Unfairness is a well-known challenge in Recommender Systems (RSs), often\nresulting in biased outcomes that disadvantage users or items based on\nattributes such as gender, race, age, or popularity. Although some approaches\nhave started to improve fairness recommendation in offline or static contexts,\nthe issue of unfairness often exacerbates over time, leading to significant\nproblems like the Matthew effect, filter bubbles, and echo chambers. To address\nthese challenges, we proposed a novel framework, Hypergraph Contrastive\nMulti-Interest Learning for Fair Conversational Recommender System (HyFairCRS),\naiming to promote multi-interest diversity fairness in dynamic and interactive\nConversational Recommender Systems (CRSs). HyFairCRS first captures a wide\nrange of user interests by establishing diverse hypergraphs through contrastive\nlearning. These interests are then utilized in conversations to generate\ninformative responses and ensure fair item predictions within the dynamic\nuser-system feedback loop. Experiments on two CRS-based datasets show that\nHyFairCRS achieves a new state-of-the-art performance while effectively\nalleviating unfairness. Our code is available at\nhttps://github.com/zysensmile/HyFairCRS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02000v1", "cate": "cs.IR", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "多兴趣公平性为何重要：用于公平会话推荐系统的超图对比多兴趣学习", "tldr": "本文提出HyFairCRS，一个基于超图对比多兴趣学习的框架，旨在解决动态会话推荐系统中的多兴趣多样性公平性问题，并在实现最先进性能的同时有效缓解了不公平性。", "motivation": "推荐系统中的不公平性导致偏向性结果，尤其在动态环境中会加剧马太效应、过滤气泡和回音室等问题。现有公平推荐方法多限于离线或静态场景，无法有效应对动态交互式会话推荐系统中的挑战。", "method": "提出HyFairCRS框架，通过对比学习建立多样化的超图来捕捉广泛的用户兴趣。这些兴趣在对话中被利用，以生成信息丰富的响应，并在动态用户-系统反馈循环中确保公平的物品预测。", "result": "HyFairCRS在两个基于CRS的数据集上实现了新的最先进性能，并有效缓解了不公平性。", "conclusion": "HyFairCRS成功解决了动态会话推荐系统中的多兴趣多样性公平性问题，表现出卓越的性能和公平性。", "translation": "推荐系统（RSs）中的不公平性是一个众所周知的挑战，通常会导致基于性别、种族、年龄或受欢迎程度等属性的偏向性结果，从而使用户或物品处于不利地位。尽管一些方法已经开始在离线或静态环境中改进公平推荐，但不公平性问题往往会随着时间的推移而加剧，导致诸如马太效应、过滤气泡和回音室等严重问题。为了解决这些挑战，我们提出了一个新颖的框架，即用于公平会话推荐系统的超图对比多兴趣学习（HyFairCRS），旨在促进动态交互式会话推荐系统（CRSs）中的多兴趣多样性公平性。HyFairCRS首先通过对比学习建立多样化的超图来捕捉广泛的用户兴趣。然后，这些兴趣在对话中被利用，以生成信息丰富的响应，并确保在动态用户-系统反馈循环中进行公平的物品预测。在两个基于CRS的数据集上进行的实验表明，HyFairCRS在有效缓解不公平性的同时，实现了新的最先进性能。我们的代码可在https://github.com/zysensmile/HyFairCRS获取。", "summary": "本研究提出HyFairCRS框架，旨在解决会话推荐系统（CRSs）中动态多兴趣公平性问题。HyFairCRS通过对比学习构建多样化超图以捕捉用户多兴趣，并在动态反馈循环中利用这些兴趣进行公平的物品预测和响应生成。实验证明，HyFairCRS在提升推荐性能的同时有效缓解了不公平性。", "keywords": "会话推荐系统, 公平性, 多兴趣学习, 超图, 对比学习", "comments": "该论文通过引入超图对比学习来解决会话推荐系统中的多兴趣公平性问题，具有创新性。它不仅关注了公平性，还强调了在动态交互环境中的多兴趣多样性，这对于缓解马太效应和过滤气泡等问题至关重要。其在实现SOTA性能的同时提升公平性的结果表明了该方法的有效性和实用价值。"}}
{"id": "2507.02257", "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes", "authors": ["Stephen Pasch", "Joel K. Salzman", "Changxi Zheng"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      SIGGRAPH 2025 Posters", "url": "http://arxiv.org/abs/2507.02257v1", "summary": "The growing popularity of 3D Gaussian Splatting has created the need to\nintegrate traditional computer graphics techniques and assets in splatted\nenvironments. Since 3D Gaussian primitives encode lighting and geometry jointly\nas appearance, meshes are relit improperly when inserted directly in a mixture\nof 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a\nspecialized tool for baking reflection probes from Gaussian-splatted scenes\nthat enables realistic reflection mapping of traditional 3D meshes in the Unity\ngame engine.", "comment": "SIGGRAPH 2025 Posters", "pdf_url": "http://arxiv.org/pdf/2507.02257v1", "cate": "cs.GR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Gbake：将3D高斯飞溅烘焙成反射探头", "tldr": "GBake是一个将3D高斯飞溅场景烘焙成反射探头的工具，用于在Unity中实现传统3D网格的真实感反射映射，解决了高斯飞溅环境中网格光照不正确的问题。", "motivation": "随着3D高斯飞溅技术日益普及，需要将其与传统计算机图形技术和资产整合。然而，由于3D高斯基元将光照和几何形状联合编码为外观，传统网格直接插入3D高斯混合环境时，其光照会不正确，显得格格不入。", "method": "本文介绍了一个名为GBake的专用工具，该工具能够从高斯飞溅场景中烘焙反射探头。", "result": "GBake工具使得在Unity游戏引擎中实现传统3D网格的真实感反射映射成为可能。", "conclusion": "GBake成功解决了在3D高斯飞溅环境中传统3D网格光照不正确的问题，通过烘焙反射探头实现了传统与新型渲染技术的融合。", "translation": "3D高斯飞溅日益普及，这使得将传统计算机图形技术和资产整合到飞溅环境中成为必要。由于3D高斯基元将光照和几何形状联合编码为外观，因此当网格直接插入3D高斯混合体中时，它们的重新照明会不正确，从而显得格格不入。我们引入了GBake，一个专门用于从高斯飞溅场景中烘焙反射探头的工具，它可以在Unity游戏引擎中实现传统3D网格的真实感反射映射。", "summary": "本文介绍了一个名为GBake的工具，旨在解决3D高斯飞溅环境中传统3D网格光照不兼容的问题。由于3D高斯飞溅将光照和几何信息结合，直接插入的传统网格会显得不自然。GBake通过从高斯飞溅场景中烘焙反射探头，使得在Unity引擎中能够对传统网格进行真实的反射映射。", "keywords": "3D Gaussian Splatting, Reflection Probes, Unity, Real-time Rendering, Computer Graphics", "comments": "本文提出GBake，创新性地解决了3D高斯飞溅场景与传统3D网格资产集成时的光照不一致问题。通过将高斯飞溅数据“烘焙”成反射探头，它为游戏引擎等实时应用提供了实用的解决方案，提高了不同渲染技术融合的真实感和效率。其重要性在于弥合了新兴渲染技术与现有图形管线之间的鸿沟。"}}
{"id": "2507.02235", "title": "Quality Diversity Genetic Programming for Learning Scheduling Heuristics", "authors": ["Meng Xu", "Frank Neumann", "Aneta Neumann", "Yew Soon Ong"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.02235v1", "summary": "Real-world optimization often demands diverse, high-quality solutions.\nQuality-Diversity (QD) optimization is a multifaceted approach in evolutionary\nalgorithms that aims to generate a set of solutions that are both\nhigh-performing and diverse. QD algorithms have been successfully applied\nacross various domains, providing robust solutions by exploring diverse\nbehavioral niches. However, their application has primarily focused on static\nproblems, with limited exploration in the context of dynamic combinatorial\noptimization problems. Furthermore, the theoretical understanding of QD\nalgorithms remains underdeveloped, particularly when applied to learning\nheuristics instead of directly learning solutions in complex and dynamic\ncombinatorial optimization domains, which introduces additional challenges.\nThis paper introduces a novel QD framework for dynamic scheduling problems. We\npropose a map-building strategy that visualizes the solution space by linking\nheuristic genotypes to their behaviors, enabling their representation on a QD\nmap. This map facilitates the discovery and maintenance of diverse scheduling\nheuristics. Additionally, we conduct experiments on both fixed and dynamically\nchanging training instances to demonstrate how the map evolves and how the\ndistribution of solutions unfolds over time. We also discuss potential future\nresearch directions that could enhance the learning process and broaden the\napplicability of QD algorithms to dynamic combinatorial optimization\nchallenges.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.02235v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "学习调度启发式的质量多样性遗传编程", "tldr": "本文提出了一种新颖的质量多样性（QD）框架，用于动态调度问题，通过构建启发式基因型与其行为之间的映射来发现和维护多样化的调度启发式。", "motivation": "现有质量多样性（QD）算法的应用主要集中在静态问题上，在动态组合优化问题中的探索有限。此外，当QD算法应用于学习启发式而非直接学习复杂动态组合优化领域的解决方案时，其理论理解尚不完善，这带来了额外的挑战。", "method": "本文引入了一种用于动态调度问题的新型质量多样性（QD）框架。该框架提出了一种地图构建策略，通过将启发式基因型与其行为联系起来，在QD地图上表示它们，从而实现解决方案空间的可视化。此地图有助于发现和维护多样化的调度启发式。", "result": "实验在固定和动态变化的训练实例上进行，以展示地图如何演变以及解决方案的分布如何随时间展开。", "conclusion": "本文提出了一个用于动态调度问题的新型QD框架，并通过实验展示了其在发现和维护多样化调度启发式方面的潜力，强调了未来研究方向以增强学习过程和拓宽QD算法在动态组合优化挑战中的适用性。", "translation": "现实世界的优化通常需要多样化、高质量的解决方案。质量多样性（QD）优化是进化算法中一种多方面的方法，旨在生成一组既高性能又多样化的解决方案。QD算法已成功应用于各个领域，通过探索多样化的行为利基提供了鲁棒的解决方案。然而，它们的应用主要集中在静态问题上，在动态组合优化问题中的探索有限。此外，当QD算法应用于学习启发式而非直接学习复杂动态组合优化领域的解决方案时，其理论理解尚不完善，这带来了额外的挑战。本文引入了一种用于动态调度问题的新型QD框架。我们提出了一种地图构建策略，通过将启发式基因型与其行为联系起来，实现解决方案空间的可视化，从而在QD地图上表示它们。此地图有助于发现和维护多样化的调度启发式。此外，我们还在固定和动态变化的训练实例上进行了实验，以展示地图如何演变以及解决方案的分布如何随时间展开。我们还讨论了潜在的未来研究方向，这些方向可以增强学习过程并拓宽QD算法在动态组合优化挑战中的适用性。", "summary": "本文提出了一种针对动态调度问题的新型质量多样性（QD）框架。该方法通过构建启发式基因型与其行为之间的映射来可视化解决方案空间，并在QD地图上表示，从而促进多样化调度启发式的发现和维护。实验证明了该地图在固定和动态环境中的演变以及解决方案分布的变化，并讨论了QD算法在动态组合优化中进一步应用的潜力。", "keywords": "质量多样性, 遗传编程, 调度启发式, 动态优化, 组合优化", "comments": "该论文的创新之处在于将质量多样性（QD）优化首次应用于动态调度问题，特别是在学习启发式而非直接学习解决方案的背景下。通过引入地图构建策略来可视化和维护多样化的启发式，它为动态组合优化领域提供了一个有前景的新方向。这项工作解决了现有QD研究在动态环境和启发式学习方面存在的空白，具有重要的理论和实践意义。"}}
{"id": "2507.02215", "title": "Hybrid least squares for learning functions from highly noisy data", "authors": ["Ben Adcock", "Bernhard Hientzsch", "Akil Narayan", "Yiming Xu"], "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      30 pages", "url": "http://arxiv.org/abs/2507.02215v1", "summary": "Motivated by the need for efficient estimation of conditional expectations,\nwe consider a least-squares function approximation problem with heavily\npolluted data. Existing methods that are powerful in the small noise regime are\nsuboptimal when large noise is present. We propose a hybrid approach that\ncombines Christoffel sampling with certain types of optimal experimental design\nto address this issue. We show that the proposed algorithm enjoys appropriate\noptimality properties for both sample point generation and noise mollification,\nleading to improved computational efficiency and sample complexity compared to\nexisting methods. We also extend the algorithm to convex-constrained settings\nwith similar theoretical guarantees. When the target function is defined as the\nexpectation of a random field, we extend our approach to leverage adaptive\nrandom subspaces and establish results on the approximation capacity of the\nadaptive procedure. Our theoretical findings are supported by numerical studies\non both synthetic data and on a more challenging stochastic simulation problem\nin computational finance.", "comment": "30 pages", "pdf_url": "http://arxiv.org/pdf/2507.02215v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "混合最小二乘法用于从高噪声数据中学习函数", "tldr": "提出一种混合最小二乘方法，结合Christoffel采样和最优实验设计，以高效地从高噪声数据中学习函数，并在计算效率和样本复杂度方面优于现有方法。", "motivation": "现有方法在存在大量噪声时表现不佳，需要从高污染数据中高效估计条件期望和进行最小二乘函数逼近。", "method": "提出一种混合方法，结合了Christoffel采样和特定类型的最优实验设计。该算法还扩展到凸约束设置，并利用自适应随机子空间处理目标函数为随机场期望的情况。", "result": "所提出的算法在样本点生成和噪声平滑方面都具有适当的最优性，与现有方法相比，提高了计算效率和样本复杂度。理论发现得到了合成数据和计算金融中随机模拟问题的数值研究支持。", "conclusion": "该混合最小二乘方法能有效处理高噪声数据下的函数学习问题，并在理论和实践中展现出优越的性能。", "translation": "受高效估计条件期望需求的驱动，我们考虑了在高污染数据下的最小二乘函数逼近问题。现有方法在小噪声环境下表现强大，但在存在大噪声时则不尽理想。我们提出了一种混合方法，结合了Christoffel采样和特定类型的最优实验设计来解决这个问题。我们证明了所提出的算法在样本点生成和噪声平滑方面都具有适当的最优性，与现有方法相比，提高了计算效率和样本复杂度。我们还将该算法扩展到具有类似理论保证的凸约束设置。当目标函数被定义为随机场的期望时，我们将方法扩展到利用自适应随机子空间，并建立了自适应过程逼近能力的结果。我们的理论发现得到了合成数据和计算金融中更具挑战性的随机模拟问题的数值研究支持。", "summary": "本文针对高噪声数据下的最小二乘函数逼近问题，提出了一种结合Christoffel采样和最优实验设计的混合方法。该方法在样本点生成和噪声平滑方面表现出最优性，显著提升了计算效率和降低了样本复杂度，优于现有方法。研究还扩展了该算法以处理凸约束设置和利用自适应随机子空间处理随机场期望函数，并通过数值研究验证了其理论优势。", "keywords": "混合最小二乘, 高噪声数据, Christoffel采样, 最优实验设计, 函数逼近", "comments": "这篇论文的创新点在于提出了一个混合方法来解决高噪声数据下的函数学习问题，结合了两种不同的技术以优化样本点生成和噪声处理。其重要性在于提升了在高噪声环境下的函数估计效率和准确性，这在实际应用中，尤其是在数据质量不高的领域（如金融计算）具有重要意义。"}}
{"id": "2507.02088", "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      24 pages, 9 figures", "url": "http://arxiv.org/abs/2507.02088v1", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.02088v1", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "McBE：一个大型语言模型多任务中文偏见评估基准", "tldr": "提出了McBE，一个包含多任务、多类别实例的中文偏见评估基准，用于衡量大型语言模型的偏见，并发现现有LLM普遍存在偏见。", "motivation": "随着大型语言模型（LLMs）在自然语言处理任务中的广泛应用，其固有的偏见逐渐显现，测量这些偏见对于降低伦理风险至关重要。然而，现有的大多数偏见评估数据集主要关注英语和北美文化，且通常只支持单一评估任务，无法从多个方面评估LLM的偏见。中文和文化相关的数据集尤其稀缺。", "method": "提出了一个名为McBE（Multi-task Chinese Bias Evaluation Benchmark）的多任务中文偏见评估基准。该基准包含4,077个偏见评估实例，涵盖12个单一偏见类别和82个子类别，并引入了5种评估任务，提供了广泛的类别覆盖、内容多样性和全面的衡量能力。此外，还评估了不同系列和参数大小的多个流行大型语言模型。", "result": "总体而言，所有被评估的大型语言模型都表现出不同程度的偏见。", "conclusion": "对评估结果进行了深入分析，为大型语言模型中的偏见提供了新的见解。", "translation": "随着大型语言模型（LLM）越来越多地应用于各种自然语言处理任务，其固有的偏见也逐渐暴露。因此，衡量LLM中的偏见对于降低其伦理风险至关重要。然而，现有的大多数偏见评估数据集都侧重于英语和北美文化，其偏见类别并不完全适用于其他文化。基于中文和中国文化的数据集非常稀缺。更重要的是，这些数据集通常只支持单一评估任务，无法从多个方面评估LLM中的偏见。为了解决这些问题，我们提出了一个多任务中文偏见评估基准（McBE），它包含4,077个偏见评估实例，涵盖12个单一偏见类别、82个子类别，并引入了5个评估任务，提供了广泛的类别覆盖、内容多样性和全面的衡量能力。此外，我们还评估了不同系列和参数大小的几个流行LLM。总的来说，所有这些LLM都表现出不同程度的偏见。我们对结果进行了深入分析，为LLM中的偏见提供了新的见解。", "summary": "本文针对现有大型语言模型偏见评估数据集主要集中于英语且多为单任务的不足，提出了McBE（多任务中文偏见评估基准）。McBE包含4,077个实例，涵盖12个偏见类别和82个子类别，并支持5种评估任务，旨在全面评估大型语言模型在中文语境下的偏见。通过对多个流行LLM的评估，研究发现这些模型普遍存在不同程度的偏见，并提供了深入的分析和新见解。", "keywords": "大型语言模型, 偏见评估, 中文基准, 多任务, McBE", "comments": "这项工作通过构建第一个多任务中文偏见评估基准McBE，填补了现有偏见评估数据集在中文语境和多维度评估方面的空白，对于促进大型语言模型的伦理发展和减少其社会风险具有重要意义。其创新性在于多任务和广泛的类别覆盖。"}}
{"id": "2507.01974", "title": "Acoustic evaluation of a neural network dedicated to the detection of animal vocalisations", "authors": ["Jérémy Rouch", "M Ducrettet", "S Haupert", "R Emonet", "F Sèbe"], "categories": ["cs.SD", "cs.LG"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01974v1", "summary": "The accessibility of long-duration recorders, adapted to sometimes demanding\nfield conditions, has enabled the deployment of extensive animal population\nmonitoring campaigns through ecoacoustics. The effectiveness of automatic\nsignal detection methods, increasingly based on neural approaches, is\nfrequently evaluated solely through machine learning metrics, while acoustic\nanalysis of performance remains rare. As part of the acoustic monitoring of\nRock Ptarmigan populations, we propose here a simple method for acoustic\nanalysis of the detection system's performance. The proposed measure is based\non relating the signal-to-noise ratio of synthetic signals to their probability\nof detection. We show how this measure provides information about the system\nand allows optimisation of its training. We also show how it enables modelling\nof the detection distance, thus offering the possibility of evaluating its\ndynamics according to the sound environment and accessing an estimation of the\nspatial density of calls.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01974v1", "cate": "cs.SD", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "用于动物叫声检测的神经网络的声学评估", "tldr": "本文提出了一种简单的方法，通过将合成信号的信噪比与其检测概率关联起来，对专门用于动物叫声检测的神经网络的性能进行声学评估。该方法有助于系统优化、检测距离建模以及叫声空间密度估计。", "motivation": "自动信号检测方法（尤其是基于神经网络的方法）的有效性通常仅通过机器学习指标进行评估，而声学性能分析却很少见。为了更好地评估和优化生态声学中动物种群监测的检测系统，需要一种声学评估方法。", "method": "提出了一种简单的检测系统性能声学分析方法。该方法基于将合成信号的信噪比与其检测概率相关联。", "result": "该测量方法提供了关于系统的信息，并允许优化其训练。它还能够对检测距离进行建模，从而可以根据声音环境评估其动态，并估算叫声的空间密度。", "conclusion": "所提出的声学评估方法能够深入了解神经网络检测系统，优化其训练，并实现检测距离的建模和叫声空间密度估算，从而为生态声学监测提供了更全面的评估工具。", "translation": "长时间记录设备的普及，适应有时严苛的野外条件，使得通过生态声学部署大规模动物种群监测活动成为可能。自动信号检测方法的有效性，越来越多地基于神经网络方法，通常仅通过机器学习指标进行评估，而声学性能分析却很少见。作为岩雷鸟种群声学监测的一部分，我们在此提出一种简单的检测系统性能声学分析方法。所提出的测量方法基于将合成信号的信噪比与其检测概率相关联。我们展示了该测量如何提供有关系统的信息并允许优化其训练。我们还展示了它如何能够对检测距离进行建模，从而提供了根据声音环境评估其动态并估算叫声空间密度的可能性。", "summary": "本文提出了一种针对专门用于动物叫声检测的神经网络的声学评估方法，以解决当前评估主要依赖机器学习指标而缺乏声学分析的现状。该方法通过关联合成信号的信噪比与检测概率来评估系统性能，并展示了其在系统信息获取、训练优化、检测距离建模以及叫声空间密度估算方面的应用潜力。", "keywords": "神经网络, 动物叫声, 声学评估, 信噪比, 生态声学", "comments": "本文的创新点在于提出了一种对神经网络动物叫声检测系统进行声学评估的实用方法，填补了现有评估方法主要侧重于机器学习指标而忽视声学性能的空白。这种方法通过结合信噪比和检测概率，不仅能更深入地理解系统行为，还能指导模型的优化，并为生态声学研究中叫声的空间密度估算提供了新的工具，具有重要的应用价值。"}}
{"id": "2507.02730", "title": "Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery", "authors": ["Miguel Ángel de Carvalho Servia", "Ilya Orson Sandoval", "King Kuok", "Hii", "Klaus Hellgardt", "Dongda Zhang", "Ehecatl Antonio del Rio Chanona"], "categories": ["cs.CE", "cs.SC"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      27 pages, 8 figures", "url": "http://arxiv.org/abs/2507.02730v1", "summary": "The industrialization of catalytic processes hinges on the availability of\nreliable kinetic models for design, optimization, and control. Traditional\nmechanistic models demand extensive domain expertise, while many data-driven\napproaches often lack interpretability and fail to enforce physical\nconsistency. To overcome these limitations, we propose the Physics-Informed\nAutomated Discovery of Kinetics (PI-ADoK) framework. By integrating physical\nconstraints directly into a symbolic regression approach, PI-ADoK narrows the\nsearch space and substantially reduces the number of experiments required for\nmodel convergence. Additionally, the framework incorporates a robust\nuncertainty quantification strategy via the Metropolis-Hastings algorithm,\nwhich propagates parameter uncertainty to yield credible prediction intervals.\nBenchmarking our method against conventional approaches across several\ncatalytic case studies demonstrates that PI-ADoK not only enhances model\nfidelity but also lowers the experimental burden, highlighting its potential\nfor efficient and reliable kinetic model discovery in chemical reaction\nengineering.", "comment": "27 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.02730v1", "cate": "cs.CE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "约束引导的符号回归用于数据高效的动力学模型发现", "tldr": "PI-ADoK框架通过将物理约束集成到符号回归中，实现了数据高效且可靠的动力学模型发现，解决了传统方法对专业知识要求高、缺乏可解释性及物理一致性的问题。", "motivation": "催化过程的工业化依赖于可靠的动力学模型，但传统机械模型需要大量领域专业知识，而许多数据驱动方法缺乏可解释性且未能强制执行物理一致性。", "method": "提出了一种名为“物理信息自动动力学发现”（PI-ADoK）的框架。该框架通过将物理约束直接集成到符号回归方法中，以缩小搜索空间并显著减少模型收敛所需的实验次数。此外，它还通过Metropolis-Hastings算法纳入了鲁棒的不确定性量化策略，将参数不确定性传播以产生可信的预测区间。", "result": "在多个催化案例研究中，与传统方法进行基准测试表明，PI-ADoK不仅提高了模型保真度，而且降低了实验负担。", "conclusion": "PI-ADoK框架在化学反应工程中具有实现高效可靠动力学模型发现的潜力。", "translation": "催化过程的工业化取决于可靠的动力学模型，以用于设计、优化和控制。传统的机械模型需要广泛的领域专业知识，而许多数据驱动方法通常缺乏可解释性并且未能强制执行物理一致性。为了克服这些限制，我们提出了物理信息自动动力学发现（PI-ADoK）框架。通过将物理约束直接集成到符号回归方法中，PI-ADoK缩小了搜索空间，并大大减少了模型收敛所需的实验次数。此外，该框架通过Metropolis-Hastings算法纳入了稳健的不确定性量化策略，该策略传播参数不确定性以产生可信的预测区间。将我们的方法与几种催化案例研究中的传统方法进行基准测试表明，PI-ADoK不仅提高了模型保真度，而且降低了实验负担，突出了其在化学反应工程中高效可靠动力学模型发现的潜力。", "summary": "本研究提出了物理信息自动动力学发现（PI-ADoK）框架，旨在解决传统动力学模型在专业知识需求、可解释性和物理一致性方面的局限性。PI-ADoK通过将物理约束融入符号回归，有效缩小模型搜索空间并减少所需实验量。同时，它结合Metropolis-Hastings算法进行不确定性量化。基准测试结果表明，PI-ADoK在提高模型准确性的同时，显著降低了实验成本，展现了其在化学反应工程中高效发现动力学模型的潜力。", "keywords": "符号回归, 动力学模型, 物理约束, 数据高效, 不确定性量化", "comments": "该论文的创新点在于将物理约束直接集成到符号回归中，从而在数据量有限的情况下也能实现高效且可靠的动力学模型发现，并解决了传统数据驱动方法缺乏可解释性和物理一致性的问题。其引入的不确定性量化策略也增强了模型的实用性。"}}
{"id": "2507.02206", "title": "EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer", "authors": ["Ranyang Zhou", "Abeer Matar A. Almalky", "Gamana Aragonda", "Sabbir Ahmed", "Filip Roth Trønnes-Christensen", "Adnan Siraj Rakin", "Shaahin Angizi"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02206v1", "summary": "True Random Number Generators (TRNGs) play a fundamental role in hardware\nsecurity, cryptographic systems, and data protection. In the context of Deep\nNeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is\ncritical to ensure the integrity, privacy, and intel-lectual property of AI\nsystems. While software-based pseudo-random number generators are widely used,\nthey lack the unpredictability and resilience offered by hardware-based TRNGs.\nIn this work, we propose a novel and robust Encoding-in-Memory TRNG called\nEIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,\nparticularly under RowHammer-induced disturbances, for the first time. We\ndemonstrate how the unpredictable bit-flips generated through carefully\ncontrolled RowHammer operations can be harnessed as a reliable entropy source.\nFurthermore, we apply this TRNG framework to secure DNN weight data by encoding\nvia a combination of fixed and unpredictable bit-flips. The encrypted data is\nlater decrypted using a key derived from the probabilistic flip behavior,\nensuring both data confidentiality and model authenticity. Our results validate\nthe effectiveness of DRAM-based entropy extraction for robust, low-cost\nhardware security and offer a promising direction for protecting machine\nlearning models at the hardware level.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02206v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "EIM-TRNG：通过RowHammer利用内存编码真随机数生成器混淆深度神经网络权重", "tldr": "提出EIM-TRNG，一种基于RowHammer诱导DRAM位翻转的硬件真随机数生成器，用于安全地混淆和保护深度神经网络权重。", "motivation": "深度神经网络模型参数（特别是权重）的保护对于AI系统的完整性、隐私和知识产权至关重要。软件伪随机数生成器缺乏硬件真随机数生成器提供的不可预测性和弹性。", "method": "提出EIM-TRNG，利用DRAM单元固有的物理随机性，特别是RowHammer引起的干扰。通过受控的RowHammer操作产生的不可预测位翻转作为可靠的熵源。将此TRNG框架应用于通过固定和不可预测位翻转组合编码DNN权重数据，使用从概率翻转行为导出的密钥进行解密。", "result": "验证了基于DRAM的熵提取对于鲁棒、低成本硬件安全的有效性，并为硬件级别保护机器学习模型提供了有前景的方向。", "conclusion": "EIM-TRNG通过利用DRAM的RowHammer效应提供了一种新颖、鲁棒且低成本的硬件安全解决方案，可有效保护深度神经网络权重。", "translation": "真随机数生成器（TRNGs）在硬件安全、密码系统和数据保护中扮演着基础角色。在深度神经网络（DNNs）的背景下，保护模型参数，特别是权重，对于确保AI系统的完整性、隐私和知识产权至关重要。虽然基于软件的伪随机数生成器被广泛使用，但它们缺乏硬件TRNGs提供的不可预测性和弹性。在这项工作中，我们首次提出了一种新颖且鲁棒的内存编码TRNG，名为EIM-TRNG，它利用了DRAM单元行为中固有的物理随机性，特别是在RowHammer引起的干扰下。我们展示了如何将通过精心控制的RowHammer操作产生的不可预测位翻转用作可靠的熵源。此外，我们将此TRNG框架应用于通过固定和不可预测位翻转组合编码DNN权重数据来保护数据。加密数据随后使用从概率翻转行为导出的密钥进行解密，确保了数据机密性和模型真实性。我们的结果验证了基于DRAM的熵提取对于鲁棒、低成本硬件安全的有效性，并为在硬件级别保护机器学习模型提供了有前景的方向。", "summary": "本文提出EIM-TRNG，一种利用DRAM中RowHammer效应产生的物理随机性作为熵源的新型硬件真随机数生成器。该方法通过将不可预测的位翻转与固定翻转结合来编码深度神经网络权重，并通过概率翻转行为派生密钥进行解密，从而在硬件层面实现了DNN模型参数的保密性和真实性保护，为低成本硬件安全提供了有效方案。", "keywords": "真随机数生成器, RowHammer, 深度神经网络, 硬件安全, DRAM", "comments": "这项工作创新性地利用了DRAM的RowHammer效应作为真随机数生成器（TRNG）的熵源，并将其应用于深度神经网络权重的混淆和保护。其重要性在于提供了一种硬件级别的、低成本的AI模型安全解决方案，解决了传统软件PRNG的局限性，并对AI系统的知识产权和数据隐私保护具有潜在价值。"}}
{"id": "2507.02517", "title": "Detecting Multiple Diseases in Multiple Crops Using Deep Learning", "authors": ["Vivek Yadav", "Anugrah Jain"], "categories": ["cs.CV", "cs.AI", "cs.ET"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02517v1", "summary": "India, as a predominantly agrarian economy, faces significant challenges in\nagriculture, including substantial crop losses caused by diseases, pests, and\nenvironmental stress. Early detection and accurate identification of diseases\nacross different crops are critical for improving yield and ensuring food\nsecurity. This paper proposes a deep learning based solution for detecting\nmultiple diseases in multiple crops, aimed to cover India's diverse\nagricultural landscape. We first create a unified dataset encompassing images\nof 17 different crops and 34 different diseases from various available\nrepositories. Proposed deep learning model is trained on this dataset and\noutperforms the state-of-the-art in terms of accuracy and the number of crops,\ndiseases covered. We achieve a significant detection accuracy, i.e., 99 percent\nfor our unified dataset which is 7 percent more when compared to\nstate-of-the-art handling 14 crops and 26 different diseases only. By improving\nthe number of crops and types of diseases that can be detected, proposed\nsolution aims to provide a better product for Indian farmers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02517v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用深度学习检测多种作物中的多种疾病", "tldr": "本文提出了一种基于深度学习的解决方案，用于检测印度多种作物中的多种疾病，并在一项涵盖17种作物和34种疾病的统一数据集上取得了99%的检测精度，优于现有技术。", "motivation": "印度作为农业经济体，面临作物疾病、病虫害和环境胁迫造成的巨大损失。早期和准确地检测不同作物中的疾病对于提高产量和确保粮食安全至关重要。", "method": "本文提出了一种基于深度学习的解决方案。首先，创建了一个包含17种不同作物和34种不同疾病图像的统一数据集。然后，在该数据集上训练所提出的深度学习模型。", "result": "所提出的深度学习模型在准确性和所涵盖的作物和疾病数量方面均优于现有技术。在统一数据集上达到了99%的检测精度，比现有技术（处理14种作物和26种不同疾病）高出7%。", "conclusion": "通过增加可检测的作物和疾病种类，该解决方案旨在为印度农民提供更好的产品，从而改善粮食安全和提高产量。", "translation": "印度作为一个以农业为主的经济体，在农业方面面临着严峻挑战，包括疾病、病虫害和环境胁迫造成的巨大作物损失。在不同作物中早期发现和准确识别疾病对于提高产量和确保粮食安全至关重要。本文提出了一种基于深度学习的解决方案，用于检测多种作物中的多种疾病，旨在覆盖印度多样化的农业景观。我们首先创建了一个统一的数据集，其中包含来自各种可用存储库的17种不同作物和34种不同疾病的图像。所提出的深度学习模型在该数据集上进行训练，在准确性和所涵盖的作物、疾病数量方面均优于现有技术。我们取得了显著的检测精度，即统一数据集的检测精度为99%，比现有技术（仅处理14种作物和26种不同疾病）高出7%。通过增加可检测的作物和疾病数量，所提出的解决方案旨在为印度农民提供更好的产品。", "summary": "本研究提出一种基于深度学习的方法，用于在印度农业环境中检测多种作物中的多种疾病。通过整合来自不同来源的图像，构建了一个包含17种作物和34种疾病的统一数据集。在该数据集上训练的深度学习模型实现了99%的检测精度，显著优于现有技术，旨在为印度农民提供更全面的疾病检测工具，以提高作物产量和粮食安全。", "keywords": "深度学习, 作物疾病检测, 多作物, 多疾病, 印度农业", "comments": "该研究的创新之处在于构建了一个大规模的统一数据集，涵盖了比现有技术更多的作物和疾病种类，并在此基础上实现了更高的检测精度。这对于印度这样农业多样性高的国家具有重要意义，有助于实际应用中更全面地帮助农民进行作物疾病管理。其贡献在于提高了检测的广度和准确性，对粮食安全有直接积极影响。"}}
{"id": "2507.02029", "title": "RoboBrain 2.0 Technical Report", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Shanyu Rong", "Zhengliang Cai", "Bolun Zhang", "Shuyi Zhang", "Huaihai Lyu", "Mengfei Du", "Lingfeng Zhang", "Xi Feng", "Xiaodan Liu", "Yance Jiao", "Chenrui He", "Mengsi Lyu", "Zhuo Chen", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02029v1", "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02029v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "RoboBrain 2.0 技术报告", "tldr": "RoboBrain 2.0 是一个新型的具身视觉-语言基础模型，旨在统一物理环境中复杂具身任务的感知、推理和规划。它在具身推理任务上表现出色，尤其在空间和时间基准测试中达到了领先水平。", "motivation": "该研究的动机是开发一个能够统一物理环境中复杂具身任务的感知、推理和规划的具身视觉-语言基础模型，并推动具身AI研究和实现通用具身智能体。", "method": "RoboBrain 2.0 包含7B和32B两种变体，采用异构架构，结合了视觉编码器和语言模型。其开发涉及模型架构设计、数据构建和多阶段训练策略。", "result": "RoboBrain 2.0 在广泛的具身推理任务中表现出色。32B变体在空间和时间基准测试中取得了领先结果，超越了先前的开源和专有模型。它支持关键的真实世界具身AI能力，包括空间理解（如可供性预测、空间指代、轨迹预测）和时间决策（如闭环交互、多智能体长周期规划和场景图更新）。", "conclusion": "RoboBrain 2.0 有望推进具身AI研究，并成为构建通用具身智能体的实用一步。", "translation": "我们介绍了RoboBrain 2.0，这是我们最新一代的具身视觉-语言基础模型，旨在统一物理环境中复杂具身任务的感知、推理和规划。它有两个变体：一个轻量级的7B模型和一个全尺寸的32B模型，具有异构架构，包括一个视觉编码器和一个语言模型。尽管其尺寸紧凑，RoboBrain 2.0 在广泛的具身推理任务中取得了强大性能。在空间和时间基准测试中，32B变体取得了领先结果，超越了先前的开源和专有模型。特别是，它支持关键的真实世界具身AI能力，包括空间理解（例如，可供性预测、空间指代、轨迹预测）和时间决策（例如，闭环交互、多智能体长周期规划和场景图更新）。本报告详细介绍了模型架构、数据构建、多阶段训练策略、基础设施和实际应用。我们希望RoboBrain 2.0 能够推动具身AI研究，并作为构建通用具身智能体的实用一步。代码、检查点和基准可在 https://superrobobrain.github.io 获取。", "summary": "RoboBrain 2.0 是一个新型的具身视觉-语言基础模型，旨在整合物理环境中复杂具身任务的感知、推理和规划。该模型提供7B和32B两种版本，采用视觉编码器和语言模型相结合的异构架构。尽管模型紧凑，RoboBrain 2.0 在多项具身推理任务中展现出卓越性能，特别是32B版本在空间和时间基准测试中超越了现有模型。它支持空间理解和时间决策等关键具身AI能力。该技术报告详细介绍了模型架构、数据构建、训练策略和应用，旨在推动具身AI研究和通用具身智能体的开发。", "keywords": "具身AI, 视觉-语言模型, 基础模型, 感知推理, 具身智能体", "comments": "RoboBrain 2.0 的创新之处在于其统一感知、推理和规划的具身视觉-语言基础模型方法，以及在紧凑模型尺寸下实现的卓越性能。其异构架构和对真实世界具身AI能力的强调，使其在具身AI领域具有重要意义。"}}
{"id": "2507.02107", "title": "Structural Code Search using Natural Language Queries", "authors": ["Ben Limpanukorn", "Yanjun Wang", "Zach Patterson", "Pranav Garg", "Murali Krishna Ramanathan", "Xiaofei Ma", "Anoop Deoras", "Miryung Kim"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02107v1", "summary": "Searching code is a common task that developers perform to understand APIs,\nlearn common code patterns, and navigate code. Currently, developers most\ncommonly search using keywords and regular expressions that are easy to use and\nwidely available. Beyond keywords and regular expressions, structural code\nsearch tools allow developers to search for code based on its syntactic\nstructure. This has numerous applications ranging from bug finding to\nsystematically refactoring code. However, these structural code search tools\noperate on queries expressed in domain-specific languages (DSL) that can be\ndifficult to learn and write. We propose to allow developers to use natural\nlanguage to search for code structurally. Expressing queries in natural\nlanguage provides an intuitive way to search for code and lowers the barrier to\nentry.\n  In this work, we develop a novel general approach that combines the reasoning\ncapabilities of an LLM to interpret natural language search queries with the\npower of structural search tools to efficiently and accurately retrieve\nrelevant code. We then instantiate this approach for two structural code search\nDSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for\nstructural code search consisting of 400 queries over 10 Java projects. We show\nthat our approach for structural code search based on translating NL queries to\nDSL queries using an LLM is effective and robust, achieving a high precision\nand recall ranging from 55% - 70%. Further, our approach significantly\noutperforms baselines based on semantic code search and LLM retrievals by up to\n57% and 14% on F1 scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02107v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "使用自然语言查询的结构化代码搜索", "tldr": "该论文提出了一种结合大型语言模型（LLM）和结构化代码搜索工具的新方法，允许开发者使用自然语言进行结构化代码搜索，显著提高了搜索的有效性和准确性。", "motivation": "开发者常用关键词和正则表达式搜索代码，但结构化代码搜索工具依赖于难学难写的领域特定语言（DSL）。这限制了结构化搜索的应用，因此需要一种更直观的方式来执行结构化代码搜索。", "method": "作者开发了一种新颖的通用方法，该方法将LLM解释自然语言搜索查询的能力与结构化搜索工具的强大功能相结合，以高效准确地检索相关代码。该方法针对Semgrep和GQL两种结构化代码搜索DSL进行了实例化，通过将自然语言查询转换为DSL查询来工作。", "result": "在包含400个查询和10个Java项目的新基准测试中，该方法实现了55%-70%的高精度和召回率。此外，该方法在F1分数上比基于语义代码搜索和LLM检索的基线分别高出57%和14%。", "conclusion": "该研究表明，通过将自然语言查询转换为DSL查询的结构化代码搜索方法是有效且稳健的，显著优于现有基线，降低了结构化代码搜索的门槛。", "translation": "搜索代码是开发人员理解API、学习常见代码模式和导航代码的常见任务。目前，开发人员最常用的是易于使用且广泛可用的关键词和正则表达式进行搜索。除了关键词和正则表达式，结构化代码搜索工具允许开发人员根据其语法结构搜索代码。这在从发现错误到系统地重构代码等领域有广泛应用。然而，这些结构化代码搜索工具操作的查询是用领域特定语言（DSL）表达的，这些语言可能难以学习和编写。我们建议允许开发人员使用自然语言进行结构化代码搜索。用自然语言表达查询提供了一种直观的代码搜索方式，并降低了使用门槛。\n在这项工作中，我们开发了一种新颖的通用方法，该方法结合了大型语言模型（LLM）解释自然语言搜索查询的推理能力与结构化搜索工具的强大功能，以高效准确地检索相关代码。然后，我们将这种方法实例化为两种结构化代码搜索DSL：Semgrep和GQL。在我们的评估中，我们构建了一个新的结构化代码搜索基准，该基准包含10个Java项目上的400个查询。我们表明，我们基于使用LLM将自然语言查询转换为DSL查询的结构化代码搜索方法是有效且稳健的，实现了55%-70%的高精度和召回率。此外，我们的方法在F1分数上比基于语义代码搜索和LLM检索的基线分别显著高出高达57%和14%。", "summary": "该论文提出了一种创新的方法，旨在通过允许开发者使用自然语言进行结构化代码搜索来解决现有结构化代码搜索工具（依赖于复杂DSL）的局限性。该方法结合了大型语言模型（LLM）的自然语言理解能力和结构化代码搜索工具的精确性，将自然语言查询转换为DSL查询。实验结果表明，该方法在新的基准测试中表现出高精度和召回率（55%-70%），并显著优于传统的语义代码搜索和LLM检索基线，从而降低了结构化代码搜索的使用门槛。", "keywords": "结构化代码搜索, 自然语言查询, 大型语言模型, 领域特定语言, 代码搜索", "comments": "这项工作的创新之处在于将大型语言模型（LLM）与传统的结构化代码搜索工具相结合，有效地弥合了自然语言与领域特定语言（DSL）之间的鸿沟。这极大地降低了结构化代码搜索的门槛，使其对更广泛的开发者群体可用。其重要性体现在提升了代码搜索的直观性和效率，对于代码理解、API学习、错误查找和代码重构等任务具有实际应用价值。该方法通过将自然语言转化为精确的结构化查询，避免了纯语义搜索可能带来的模糊性，同时又克服了DSL学习的难度，提供了一个实用且高性能的解决方案。"}}
{"id": "2507.02138", "title": "A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy", "authors": ["Shan Li", "Guozhu Ding"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02138v1", "summary": "This study introduces and evaluates Healthy Choice, an innovative\ntheory-driven and AI-enhanced simulation platform designed to cultivate\nnutrition literacy through interactive scenario-based learning experiences. We\ncollected feedback from 114 university students with diverse backgrounds who\ncompleted simulated product selection scenarios. Quantitative ratings of\nusefulness and ease of use demonstrated high user satisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02138v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "一个理论驱动和人工智能增强的营养素养培养模拟平台", "tldr": "本研究介绍并评估了一个名为Healthy Choice的模拟平台，该平台通过交互式情景学习来培养营养素养，并获得了用户的高度满意度。", "motivation": "开发一个理论驱动和人工智能增强的模拟平台，旨在通过交互式情景学习体验培养营养素养。", "method": "本研究介绍并评估了Healthy Choice平台。研究人员收集了114名不同背景的大学生完成模拟产品选择场景后的反馈。通过对有用性和易用性的定量评分来评估平台。", "result": "用户对Healthy Choice平台表现出高度满意度，体现在对有用性和易用性的高定量评分上。", "conclusion": "Not mentioned in abstract", "translation": "本研究介绍并评估了Healthy Choice，一个创新的、理论驱动且人工智能增强的模拟平台，旨在通过交互式情景学习体验培养营养素养。我们收集了来自114名不同背景的大学生完成模拟产品选择场景后的反馈。对有用性和易用性的定量评分表明用户满意度很高。", "summary": "本研究介绍并评估了“Healthy Choice”模拟平台，该平台结合理论驱动和人工智能技术，通过交互式情景学习来培养用户的营养素养。对114名大学生的反馈分析显示，用户对该平台的有用性和易用性评价很高，表明用户满意度高。", "keywords": "营养素养, 模拟平台, 人工智能, 情景学习, 健康选择", "comments": "该论文的创新之处在于结合了理论驱动和人工智能技术来创建一个模拟平台，以提高营养素养。通过情景学习，它提供了一种实践和互动的方式来教育用户。其重要性在于提供了一种可扩展且可能有效的教育工具，以应对营养素养不足的问题。抽象中未提及具体理论或AI增强的机制，也未提及潜在的局限性。"}}
{"id": "2507.02170", "title": "Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System", "authors": ["Adam Kostka", "Jarosław A. Chudziak"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02170v1", "summary": "This paper explores the integration of advanced Multi-Agent Systems (MAS)\ntechniques to develop a team of agents with enhanced logical reasoning,\nlong-term knowledge retention, and Theory of Mind (ToM) capabilities. By\nuniting these core components with optimized communication protocols, we create\na novel framework called SynergyMAS, which fosters collaborative teamwork and\nsuperior problem-solving skills. The system's effectiveness is demonstrated\nthrough a product development team case study, where our approach significantly\nenhances performance and adaptability. These findings highlight SynergyMAS's\npotential to tackle complex, real-world challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02170v1", "cate": "cs.MA", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "多智能体LLM系统中逻辑推理、知识管理与协作的协同作用", "tldr": "本文提出了SynergyMAS，一个整合了逻辑推理、知识保留和心智理论的多智能体系统框架，旨在增强协作和问题解决能力，并在产品开发案例研究中展示了其有效性。", "motivation": "本研究旨在通过整合先进的多智能体系统（MAS）技术，开发一个具备增强的逻辑推理、长期知识保留和心智理论（ToM）能力的智能体团队，以促进协作团队合作和卓越的问题解决能力，从而应对复杂的现实世界挑战。", "method": "该论文通过整合先进的多智能体系统（MAS）技术，将逻辑推理、长期知识保留和心智理论（ToM）能力与优化的通信协议相结合，创建了一个名为SynergyMAS的新颖框架。其有效性通过一个产品开发团队的案例研究得到验证。", "result": "在产品开发团队的案例研究中，SynergyMAS方法显著提升了团队的性能和适应性。", "conclusion": "SynergyMAS通过在多智能体系统中增强逻辑推理、知识保留和心智理论能力，促进协作团队合作和卓越的问题解决技能，从而有潜力应对复杂的现实世界挑战。", "translation": "本文探讨了如何整合先进的多智能体系统 (MAS) 技术，以开发一个具备增强的逻辑推理、长期知识保留和心智理论 (ToM) 能力的智能体团队。通过将这些核心组件与优化的通信协议相结合，我们创建了一个名为 SynergyMAS 的新颖框架，该框架旨在促进协作团队合作和卓越的问题解决能力。该系统的有效性通过一个产品开发团队的案例研究得到证明，在此案例中，我们的方法显著提升了性能和适应性。这些发现突显了 SynergyMAS 应对复杂现实世界挑战的潜力。", "summary": "本文介绍了一个名为SynergyMAS的新型多智能体系统框架，旨在增强协作团队合作和问题解决能力。该框架通过整合先进的多智能体系统技术，为智能体配备了改进的逻辑推理、长期知识保留和心智理论能力，并优化了通信协议。一项产品开发团队的案例研究表明，SynergyMAS能显著提升性能和适应性，表明其在解决复杂现实世界问题方面的潜力。", "keywords": "多智能体系统, 逻辑推理, 知识管理, 协作, 心智理论", "comments": "该论文的创新之处在于在一个多智能体LLM系统中协同融合了逻辑推理、知识管理和心智理论这三个关键方面，这对于构建更复杂、更像人类的AI团队至关重要。通过产品开发案例研究来展示其实用性，增加了其实践价值。其应对复杂现实世界挑战的潜力使其具有重要意义。"}}
{"id": "2507.02166", "title": "Generating Large Semi-Synthetic Graphs of Any Size", "authors": ["Rodrigo Tuna", "Carlos Soares"], "categories": ["cs.SI", "cs.AI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02166v1", "summary": "Graph generation is an important area in network science. Traditional\napproaches focus on replicating specific properties of real-world graphs, such\nas small diameters or power-law degree distributions. Recent advancements in\ndeep learning, particularly with Graph Neural Networks, have enabled\ndata-driven methods to learn and generate graphs without relying on predefined\nstructural properties. Despite these advances, current models are limited by\ntheir reliance on node IDs, which restricts their ability to generate graphs\nlarger than the input graph and ignores node attributes. To address these\nchallenges, we propose Latent Graph Sampling Generation (LGSG), a novel\nframework that leverages diffusion models and node embeddings to generate\ngraphs of varying sizes without retraining. The framework eliminates the\ndependency on node IDs and captures the distribution of node embeddings and\nsubgraph structures, enabling scalable and flexible graph generation.\nExperimental results show that LGSG performs on par with baseline models for\nstandard metrics while outperforming them in overlooked ones, such as the\ntendency of nodes to form clusters. Additionally, it maintains consistent\nstructural characteristics across graphs of different sizes, demonstrating\nrobustness and scalability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02166v1", "cate": "cs.SI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "生成任意大小的大型半合成图", "tldr": "当前图生成模型受限于节点ID和大小。LGSG利用扩散模型和节点嵌入，无需重新训练即可生成任意大小的可扩展图，并在标准和被忽视的指标上表现良好。", "motivation": "传统的图生成方法依赖预定义属性，而现有的深度学习图生成模型受限于节点ID，导致无法生成比输入图更大的图并忽略节点属性。", "method": "本文提出了潜在图采样生成（LGSG）框架，该框架利用扩散模型和节点嵌入来生成不同大小的图。它消除了对节点ID的依赖，并捕获节点嵌入和子图结构的分布。", "result": "LGSG在标准指标上与基线模型表现相当，但在节点形成簇的趋势等被忽视的指标上表现更优。此外，它在不同大小的图之间保持一致的结构特征。", "conclusion": "LGSG提供了一个鲁棒且可扩展的解决方案，能够生成不同大小的半合成图，通过关注节点嵌入和子图结构克服了以往模型的局限性。", "translation": "图生成是网络科学中的一个重要领域。传统方法侧重于复制真实世界图的特定属性，例如小直径或幂律度分布。深度学习的最新进展，特别是图神经网络，使得数据驱动的方法能够在不依赖预定义结构属性的情况下学习和生成图。尽管取得了这些进展，但当前模型受限于对节点ID的依赖，这限制了它们生成比输入图更大的图的能力，并且忽略了节点属性。为了解决这些挑战，我们提出了潜在图采样生成（LGSG），这是一种新颖的框架，它利用扩散模型和节点嵌入来生成不同大小的图而无需重新训练。该框架消除了对节点ID的依赖，并捕获了节点嵌入和子图结构的分布，从而实现了可扩展和灵活的图生成。实验结果表明，LGSG在标准指标上与基线模型表现相当，同时在被忽视的指标（例如节点形成簇的趋势）上表现优于它们。此外，它在不同大小的图之间保持一致的结构特征，展示了鲁棒性和可扩展性。", "summary": "本文介绍了潜在图采样生成（LGSG），一个用于生成任意大小半合成图的新颖框架。与受限于节点ID和输入图大小的现有方法不同，LGSG利用扩散模型和节点嵌入来学习和捕获节点嵌入和子图结构的分布。这种方法无需重新训练即可实现可扩展和灵活的图生成。实验结果表明，LGSG在标准指标上表现具有竞争力，并在聚类趋势等被忽视的指标上表现优越，同时在各种图大小中保持结构一致性。", "keywords": "图生成, 扩散模型, 节点嵌入, 可扩展性, 半合成图", "comments": "LGSG解决了图生成领域的一个关键限制：无法超越训练图的大小进行扩展以及对节点ID的依赖。通过使用扩散模型和节点嵌入，它提供了一个更灵活和可扩展的解决方案。其在“被忽视的指标”上的表现以及在不同大小图之间的一致性突显了其实用性和创新性。"}}
{"id": "2507.01988", "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers", "authors": ["Giyong Jung", "Saeid Gorgin", "John Kim", "Jungrae Kim"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      12 pages, 8 figures. This paper is accepted for [2025 The International Conference for High Performance Computing, Networking, Storage and Analysis (SC)]", "url": "http://arxiv.org/abs/2507.01988v1", "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.", "comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]", "pdf_url": "http://arxiv.org/pdf/2507.01988v1", "cate": "cs.NI", "date": "2025-06-28", "updated": "2025-06-28", "AI": {"title_translation": "采用隐式序列号扩展芯片互连网络", "tldr": "本文提出隐式序列号 (ISN) 和可靠性扩展链路 (RXL) 机制，以解决多节点芯片互连中静默丢包和按序交付的挑战，同时保持高带宽和兼容性。", "motivation": "随着AI模型超越单处理器能力，芯片间互连成为可扩展计算的关键。现有互连协议虽然设计用于高带宽和小负载，但传输速率的提高增加了错误易感性，尤其是在多节点配置中，管理交换设备中静默丢弃的flit成为一个严峻挑战。", "method": "本文引入了隐式序列号 (ISN)，一种不增加头部开销即可确保精确flit丢弃检测和按序交付的新机制。在此基础上，提出了可靠性扩展链路 (RXL)，作为CXL的扩展，它结合ISN，并通过将CRC提升为传输层机制以实现端到端数据和序列完整性，同时依靠FEC进行链路层错误校正和检测，从而支持可扩展、可靠的多节点互连。", "result": "ISN机制能够实现精确的flit丢弃检测和按序交付。RXL在不影响带宽效率的前提下，提供了强大的可靠性和可扩展性，并保持与现有flit结构的兼容性。", "conclusion": "通过引入隐式序列号 (ISN) 和可靠性扩展链路 (RXL)，本文提供了一种在多节点芯片互连中实现高可靠性、可扩展性且不牺牲带宽效率的解决方案，有效解决了静默丢包和按序交付的核心挑战。", "translation": "随着AI模型超越单处理器的能力，芯片间的互连已成为可扩展计算的关键促成因素。这些处理器以缓存行粒度交换大量数据，促使CXL、NVLink和UALink等新型互连协议的采用，这些协议专为高带宽和小负载设计。然而，这些协议传输速率的提高增加了对错误的敏感性。虽然循环冗余校验 (CRC) 和前向纠错 (FEC) 等机制是可靠数据传输的标准，但将芯片互连扩展到多节点配置带来了新的挑战，特别是在管理交换设备中静默丢弃的flit。本文引入了隐式序列号 (ISN)，这是一种新颖的机制，可在不增加头部开销的情况下确保精确的flit丢弃检测和按序交付。此外，我们提出了可靠性扩展链路 (RXL)，它是CXL的扩展，结合了ISN以支持可扩展、可靠的多节点互连，同时保持与现有flit结构的兼容性。通过将CRC提升为用于端到端数据和序列完整性的传输层机制，并依靠FEC进行链路层错误校正和检测，RXL在不影响带宽效率的情况下提供了强大的可靠性和可扩展性。", "summary": "本文针对AI模型发展中多节点芯片互连面临的静默丢包和按序交付难题，提出了一种名为隐式序列号 (ISN) 的创新机制，该机制无需增加头部开销即可实现精确的flit丢弃检测和按序交付。在此基础上，进一步引入了可靠性扩展链路 (RXL)，作为CXL的扩展，它集成了ISN，并通过在传输层利用CRC进行端到端数据和序列完整性检查，在链路层利用FEC进行错误校正和检测，从而实现了可扩展、高可靠且带宽高效的多节点芯片互连。", "keywords": "芯片互连, 隐式序列号, 可靠性扩展链路, 静默丢包, 可扩展计算", "comments": "这篇论文通过引入ISN和RXL，有效地解决了多节点芯片互连中静默丢包这一关键且复杂的问题，这在AI计算规模日益增长的背景下显得尤为重要。其创新点在于ISN在不增加头部开销的情况下实现精确检测和按序交付，以及RXL在保持兼容性的同时提升了CXL的可靠性和可扩展性，对于未来大规模AI系统的互连设计具有重要指导意义。"}}
{"id": "2507.02274", "title": "Resolution Limits of Non-Adaptive 20 Questions Estimation for Tracking Multiple Moving Targets", "authors": ["Chunsong Sun", "Lin Zhou", "Jingjing Wang", "Weijie Yuan", "Chunxiao Jiang", "Alfred Hero"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02274v1", "summary": "Motivated by the practical application of beam tracking of multiple devices\nin Multiple Input Multiple Output (MIMO) communication, we study the problem of\nnon-adaptive twenty questions estimation for locating and tracking multiple\nmoving targets under a query-dependent noisy channel. Specifically, we derive a\nnon-asymptotic bound and a second-order asymptotic bound on resolution for\noptimal query procedures and provide numerical examples to illustrate our\nresults. In particular, we demonstrate that the bound is achieved by a state\nestimator that thresholds the mutual information density over possible target\nlocations. This single threshold decoding rule has reduced the computational\ncomplexity compared to the multiple threshold scheme proposed for locating\nmultiple stationary targets (Zhou, Bai and Hero, TIT 2022). We discuss two\nspecial cases of our setting: the case with unknown initial location and known\nvelocity, and the case with known initial location and unknown velocity. Both\ncases share the same theoretical benchmark {that applies to} stationary\nmultiple target search in Zhou, Bai and Hero (TIT 2022) while the known initial\nlocation case is close to the theoretical benchmark for stationary target\nsearch when the maximal speed is inversely proportional to the number of\nqueries. We also generalize our results to account for a piecewise constant\nvelocity model introduced in Zhou and Hero (TIT 2023), where targets change\nvelocity periodically. Finally, we illustrate our proposed algorithm for the\napplication of beam tracking of multiple mobile transmitters in a 5G wireless\nnetwork.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02274v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "非自适应20问估计在多移动目标跟踪中的分辨率极限", "tldr": "本文研究了在查询依赖噪声信道下，多移动目标定位和跟踪的非自适应20问估计的分辨率极限。作者推导了最优查询过程的分辨率非渐近界和二阶渐近界，并提出了一种通过互信息密度阈值处理实现的低复杂度单阈值状态估计器，应用于5G无线网络中的波束跟踪。", "motivation": "为了解决多输入多输出（MIMO）通信中多设备波束跟踪的实际应用问题，作者研究了在查询依赖噪声信道下定位和跟踪多个移动目标的非自适应20问估计问题。", "method": "推导了最优查询过程分辨率的非渐近界和二阶渐近界。提出了一种通过对可能目标位置的互信息密度进行阈值处理的状态估计器，以达到所推导的界限，并采用单阈值解码规则以降低计算复杂度。讨论了初始位置未知但速度已知、以及初始位置已知但速度未知两种特殊情况。将研究结果推广到目标周期性改变速度的分段常数速度模型。", "result": "导出了最优查询过程的分辨率非渐近界和二阶渐近界，并通过数值例子进行了验证。证明了所提出的单阈值状态估计器能够达到这些界限，并且该单阈值解码规则相比多阈值方案显著降低了计算复杂度。在特殊情况下，研究发现已知初始位置的情况在最大速度与查询次数成反比时，接近静止目标搜索的理论基准。", "conclusion": "本文提出的非自适应20问估计算法，尤其是在引入单阈值解码规则以降低计算复杂度后，能够有效地应用于5G无线网络中多移动发射机的波束跟踪。", "translation": "受多输入多输出（MIMO）通信中多设备波束跟踪实际应用的启发，我们研究了在查询依赖噪声信道下定位和跟踪多个移动目标的非自适应20问估计问题。具体而言，我们推导了最优查询过程分辨率的非渐近界和二阶渐近界，并提供了数值示例来说明我们的结果。特别是，我们证明了通过对可能目标位置的互信息密度进行阈值处理的状态估计器可以达到该界限。与为定位多个静止目标提出的多阈值方案（Zhou, Bai and Hero, TIT 2022）相比，这种单阈值解码规则降低了计算复杂度。我们讨论了我们设置的两种特殊情况：初始位置未知但速度已知的情况，以及初始位置已知但速度未知的情况。这两种情况都共享与Zhou, Bai和Hero（TIT 2022）中适用于静止多目标搜索的相同理论基准，而已知初始位置的情况在最大速度与查询次数成反比时接近静止目标搜索的理论基准。我们还将结果推广到Zhou和Hero（TIT 2023）中引入的分段常数速度模型，其中目标周期性地改变速度。最后，我们阐述了我们提出的算法在5G无线网络中多移动发射机波束跟踪应用中的表现。", "summary": "本文研究了在查询依赖噪声信道下，多移动目标定位和跟踪的非自适应20问估计问题，该问题源于MIMO通信中的多设备波束跟踪应用。作者推导了最优查询过程分辨率的非渐近界和二阶渐近界，并通过数值实例进行了验证。研究表明，通过对可能目标位置的互信息密度进行阈值处理的状态估计器可以达到该界限，并且这种单阈值解码规则相比现有方案显著降低了计算复杂度。文章还讨论了初始位置未知但速度已知、以及初始位置已知但速度未知两种特殊情况，并将其结果推广到分段常数速度模型。最终，该算法被应用于5G无线网络中的多移动发射机波束跟踪。", "keywords": "非自适应20问估计, 多移动目标跟踪, 分辨率极限, 波束跟踪, 互信息密度", "comments": "本文在非自适应20问估计框架下，针对多移动目标跟踪问题，从理论上推导了分辨率界限，并通过提出一种单阈值解码规则的状态估计器，实现了计算复杂度的显著降低，具有较高的创新性。其研究结果对MIMO通信中的波束跟踪等实际应用具有重要的指导意义。论文还考虑了多种实际场景和模型，增加了研究的普适性。"}}
{"id": "2507.02164", "title": "Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting", "authors": ["Ruibai Tang", "Chengbin Quan"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02164v1", "summary": "Solving and visualizing the potential roots of complex functions is essential\nin both theoretical and applied domains, yet often computationally intensive.\nWe present a hardware-accelerated algorithm for complex function roots density\ngraph plotting by approximating functions with polynomials and solving their\nroots using single-shift QR iteration. By leveraging the Hessenberg structure\nof companion matrices and optimizing QR decomposition with Givens rotations, we\ndesign a pipelined FPGA architecture capable of processing a large amount of\npolynomials with high throughput. Our implementation achieves up to 65x higher\nenergy efficiency than CPU-based approaches, and while it trails modern GPUs in\nperformance due to differences in fabrication technique.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02164v1", "cate": "cs.AR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "硬件加速的复函数根密度图绘制算法", "tldr": "提出了一种硬件加速算法，用于绘制复函数根密度图，通过多项式逼近和QR迭代，并在FPGA上实现，显著提高了能效。", "motivation": "求解和可视化复函数的潜在根在理论和应用领域都至关重要，但通常计算量很大。", "method": "通过多项式逼近函数，并使用单移QR迭代求解其根。利用伴随矩阵的Hessenberg结构，并使用Givens旋转优化QR分解，设计了流水线FPGA架构。", "result": "实现的FPGA架构比基于CPU的方法能效高出65倍；由于制造技术的差异，性能略低于现代GPU。", "conclusion": "通过硬件加速的算法和FPGA实现，可以显著提高复函数根密度图绘制的能效，尽管在绝对性能上可能不敌顶尖GPU。", "translation": "求解和可视化复函数的潜在根在理论和应用领域都至关重要，但通常计算量很大。我们提出了一种硬件加速算法，用于绘制复函数根密度图，通过多项式逼近函数并使用单移QR迭代求解其根。通过利用伴随矩阵的Hessenberg结构并使用Givens旋转优化QR分解，我们设计了一种流水线FPGA架构，能够高吞吐量处理大量多项式。我们的实现比基于CPU的方法能效高出65倍，尽管由于制造技术的差异，其性能落后于现代GPU。", "summary": "本文提出一种用于绘制复函数根密度图的硬件加速算法。该算法通过多项式逼近函数并结合单移QR迭代求解其根。研究人员设计了一种利用伴随矩阵Hessenberg结构和Givens旋转优化QR分解的流水线FPGA架构。实验结果显示，该FPGA实现比CPU方案能效提升高达65倍，但在性能上略逊于现代GPU。", "keywords": "复函数根, 硬件加速, FPGA, QR迭代, 能效", "comments": "该论文的创新点在于将复杂的根求解问题映射到FPGA硬件加速上，利用了矩阵的特定结构优化算法，显著提升了能效。其重要性在于为计算密集型的复函数根可视化提供了一种高效的解决方案，尤其适用于对能耗有严格要求的场景。尽管性能未能超越GPU，但在特定应用中，其能效优势可能更具吸引力。"}}
{"id": "2507.02158", "title": "Signalling Health for Improved Kubernetes Microservice Availability", "authors": ["Jacob Roberts", "Blair Archibald", "Phil Trinder"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02158v1", "summary": "Microservices are often deployed and managed by a container orchestrator that\ncan detect and fix failures to maintain the service availability critical in\nmany applications. In Poll-based Container Monitoring (PCM), the orchestrator\nperiodically checks container health. While a common approach, PCM requires\ncareful tuning, may degrade service availability, and can be slow to detect\ncontainer health changes. An alternative is Signal-based Container Monitoring\n(SCM), where the container signals the orchestrator when its status changes. We\npresent the design, implementation, and evaluation of an SCM approach for\nKubernetes and empirically show that it has benefits over PCM, as predicted by\na new mathematical model. We compare the service availability of SCM and PCM\nover six experiments using the SockShop benchmark. SCM does not require that\npolling intervals are tuned, and yet detects container failure 86\\% faster than\nPCM and container readiness in a comparable time with limited resource\noverheads. We find PCM can erroneously detect failures, and this reduces\nservice availability by 4\\%. We propose that orchestrators offer SCM features\nfor faster failure detection than PCM without erroneous detections or careful\ntuning.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02158v1", "cate": "cs.DC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "为提高Kubernetes微服务可用性而进行的健康信号传递", "tldr": "本文提出了一种用于Kubernetes微服务的基于信号的容器监控（SCM）方法，该方法比传统的基于轮询的容器监控（PCM）在故障检测速度和准确性上表现更好，且无需复杂调优，从而提升了微服务可用性。", "motivation": "传统的基于轮询的容器监控（PCM）在检测容器健康变化时需要仔细调优，可能降低服务可用性，并且检测速度慢。", "method": "本文设计、实现并评估了一种用于Kubernetes的基于信号的容器监控（SCM）方法。通过一个新的数学模型预测其优势，并使用SockShop基准在六个实验中比较了SCM和PCM的服务可用性。", "result": "SCM无需调优轮询间隔，检测容器故障比PCM快86%，检测容器就绪时间相当，且资源开销有限。PCM可能错误地检测故障，导致服务可用性降低4%。", "conclusion": "建议编排器提供SCM功能，以实现比PCM更快的故障检测，且没有错误检测或仔细调优的需求。", "translation": "微服务通常由容器编排器部署和管理，该编排器可以检测和修复故障，以维持在许多应用中至关重要的服务可用性。在基于轮询的容器监控（PCM）中，编排器定期检查容器健康状况。尽管这是一种常见方法，但PCM需要仔细调优，可能会降低服务可用性，并且检测容器健康变化的速度可能很慢。另一种选择是基于信号的容器监控（SCM），其中容器在状态改变时向编排器发送信号。我们提出了Kubernetes中SCM方法的设计、实现和评估，并通过实证表明，正如一个新的数学模型所预测的那样，它比PCM具有优势。我们使用SockShop基准在六个实验中比较了SCM和PCM的服务可用性。SCM不需要调优轮询间隔，但检测容器故障比PCM快86%，并且在有限的资源开销下，以相当的时间检测容器就绪。我们发现PCM可能会错误地检测故障，这会使服务可用性降低4%。我们建议编排器提供SCM功能，以实现比PCM更快的故障检测，且没有错误检测或仔细调优的需求。", "summary": "本文提出并评估了一种用于Kubernetes微服务的基于信号的容器监控（SCM）方法，旨在解决传统基于轮询的容器监控（PCM）在故障检测速度、准确性和调优复杂性方面存在的问题。通过实验验证，SCM在无需调优的情况下，能显著加快故障检测速度，避免PCM可能导致的错误检测和可用性下降，从而提高微服务系统的韧性和可用性。", "keywords": "微服务, Kubernetes, 容器监控, 服务可用性, 故障检测", "comments": "这篇论文创新性地提出了一种基于信号的容器监控方法，直接解决了传统轮询机制的固有缺陷，即延迟和误报。其通过实证数据和数学模型验证了SCM在Kubernetes环境下对微服务可用性的显著提升，尤其是在故障快速检测和减少误报方面的优势，对云原生系统健康管理具有重要意义。"}}
{"id": "2507.02413", "title": "Defining DLT Immutability: A Qualitative Survey of Node Operators", "authors": ["Alex Lynham", "Geoff Goodell"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      27 pages, 2 figures, 6 tables", "url": "http://arxiv.org/abs/2507.02413v1", "summary": "Immutability is a core design goal of permissionless public blockchain\nsystems. However, rewrites are more common than is normally understood, and the\nrisk of rewrite, cyberattack, exploit or black swan event is also high. Taking\nthe position that strict immutability is neither possible on these networks nor\nthe observed reality, this paper uses thematic analysis of node operator\ninterviews to examine the limits of immutability in light of rewrite events.\nThe end result is a qualitative definition of the conditional immutability\nfound on these networks, which we call Practical Immutability. This is\nimmutability contingent on the legitimate governance demands of the network,\nwhere network stakeholders place their trust in the governance topology of a\nnetwork to lend it legitimacy, and thus manage ledger state.", "comment": "27 pages, 2 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.02413v1", "cate": "cs.CY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "定义DLT不变性：一项针对节点操作员的定性调查", "tldr": "本研究通过对节点操作员的访谈，探讨了分布式账本技术（DLT）中不变性的限制，并提出了“实用不变性”的概念，即不变性是受网络合法治理需求制约的。", "motivation": "尽管不变性是公共区块链系统的核心设计目标，但重写事件比通常认为的更常见，且存在网络攻击、漏洞利用或黑天鹅事件的风险。论文认为严格的不变性在这些网络中既不可能实现也不是观察到的现实，因此旨在探讨不变性的实际限制。", "method": "本文采用主题分析法，对节点操作员进行了访谈。", "result": "研究结果是对这些网络中发现的条件不变性进行了定性定义，称之为“实用不变性”。", "conclusion": "实用不变性是指不变性取决于网络的合法治理需求，网络利益相关者信任网络的治理拓扑结构以赋予其合法性，从而管理账本状态。", "translation": "不变性是无需许可的公共区块链系统的核心设计目标。然而，重写事件比通常理解的更为常见，重写、网络攻击、漏洞利用或黑天鹅事件的风险也很高。本文认为严格的不变性在这些网络中既不可能实现也不是观察到的现实，因此通过对节点操作员访谈进行主题分析，审视了重写事件背景下不变性的局限性。最终结果是对这些网络中发现的条件不变性进行了定性定义，我们称之为“实用不变性”。这是一种取决于网络合法治理需求的不变性，其中网络利益相关者将信任寄托于网络的治理拓扑结构，以赋予其合法性，从而管理账本状态。", "summary": "本研究旨在探讨分布式账本技术（DLT）中不变性的实际限制，因为严格的不变性在实践中难以实现且重写事件频发。通过对节点操作员的访谈进行主题分析，论文提出了“实用不变性”的概念，将其定义为一种条件不变性，即不变性受网络合法治理需求的影响，并依赖于网络利益相关者对治理结构的信任来维护账本状态的合法性。", "keywords": "DLT不变性, 节点操作员, 定性调查, 实用不变性, 网络治理", "comments": "本文创新性地挑战了DLT中“不变性”的传统理解，通过定性研究揭示了其在实践中的局限性。提出“实用不变性”的概念，强调了网络治理在维护账本状态合法性方面的重要性，为理解复杂DLT系统的行为提供了新的视角。其重要性在于将技术不变性与社会治理层面相结合，对未来的DLT设计和风险评估具有指导意义。"}}
{"id": "2507.02073", "title": "HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection", "authors": ["Nikita Bhedasgaonkar", "Rushikesh K. Joshi"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      11 pages, 5 tables, 2 figures", "url": "http://arxiv.org/abs/2507.02073v1", "summary": "In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting\nRules), a lightweight rule-based feature selection method that combines\nParameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to\neliminate redundant features and retain relevant ones. This method is a hybrid\nof non-iterative and iterative filtering approaches for dimensionality\nreduction. It is a greedy method, which works by backward elimination,\neliminating possibly multiple features at every step. The rules contribute to\nvoting for features, and a decision to keep or discard is made by majority\nvoting. The rules make use of correlation thresholds between every pair of\nfeatures, and between features and the target. We provide the results from the\napplication of HCVR to the SPAMBASE dataset. The results showed improvement\nperformance as compared to traditional non-iterative (CFS, mRMR and MI) and\niterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was\nassessed based on the performance of different classifiers after applying\nfiltering.", "comment": "11 pages, 5 tables, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.02073v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "HCVR：一种基于相关性感知投票规则的混合特征选择方法", "tldr": "提出HCVR，一种轻量级混合特征选择方法，结合P2P和P2T相关性，通过贪婪的后向消除和多数投票来选择特征，在SPAMBASE数据集上表现优于传统方法。", "motivation": "解决特征选择中冗余特征消除和相关特征保留的问题，旨在开发一种轻量级且高效的维度约减方法。", "method": "HCVR是一种轻量级的基于规则的特征选择方法，它结合了参数到参数（P2P）和参数到目标（P2T）相关性。该方法是迭代和非迭代过滤方法的混合，采用贪婪的后向消除策略，每步可能消除多个特征。通过规则为特征投票，并根据多数投票决定保留或丢弃特征。规则利用特征对之间以及特征与目标之间的相关性阈值。", "result": "HCVR在SPAMBASE数据集上的应用结果显示，与传统的非迭代（CFS、mRMR和MI）和迭代（RFE、SFS和遗传算法）技术相比，性能有所提高。有效性是根据应用过滤后不同分类器的性能进行评估的。", "conclusion": "HCVR是一种有效的特征选择方法，能够通过结合相关性感知投票规则来提高分类器性能，并优于多种现有技术。", "translation": "在本文中，我们提出了HCVR（基于相关性感知投票规则的混合方法），这是一种轻量级的基于规则的特征选择方法，它结合了参数到参数（P2P）和参数到目标（P2T）相关性，以消除冗余特征并保留相关特征。该方法是用于降维的非迭代和迭代过滤方法的混合。它是一种贪婪方法，通过后向消除工作，每一步可能消除多个特征。这些规则有助于为特征投票，并通过多数投票决定保留或丢弃。这些规则利用了每对特征之间以及特征与目标之间的相关性阈值。我们提供了HCVR应用于SPAMBASE数据集的结果。结果显示，与传统的非迭代（CFS、mRMR和MI）和迭代（RFE、SFS和遗传算法）技术相比，性能有所提高。有效性是根据应用过滤后不同分类器的性能进行评估的。", "summary": "本文提出了一种名为HCVR的轻量级混合特征选择方法。HCVR通过结合参数到参数和参数到目标的相关性，采用基于规则的投票系统和贪婪的后向消除策略，有效识别并保留相关特征，同时消除冗余特征。在SPAMBASE数据集上的实验结果表明，HCVR在性能上优于多种传统的非迭代和迭代特征选择技术，其有效性通过不同分类器的性能提升得到验证。", "keywords": "特征选择, 混合方法, 相关性, 投票规则, 维度约减", "comments": "HCVR的创新点在于其混合了非迭代和迭代过滤思想，并引入了基于相关性阈值的投票规则进行特征选择，实现了轻量化和高效性。其贪婪的后向消除策略在每一步可以消除多个特征，提高了效率。该方法在SPAMBASE数据集上表现出优越性，但其在更广泛数据集上的泛化能力和计算复杂度在处理高维数据时的表现值得进一步探讨。"}}
{"id": "2507.02148", "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning", "authors": ["Zijie Cai", "Christopher Metzler"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02148v1", "summary": "Monocular depth estimation has recently advanced to provide not only relative\nbut also metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground-truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of\nstate-of-the-art models across a range of underwater conditions with different\nranges. Our results show that large-scale models trained on terrestrial (real\nor synthetic) data, while effective in in-air settings, perform poorly\nunderwater due to significant domain shifts. To address this, we fine-tune\nDepth Anything V2 with a ViT-S backbone encoder on a synthetic underwater\nvariant of the Hypersim dataset, which we generated using a physically based\nunderwater image formation model. We demonstrate our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. Our study provides\na detailed evaluation and visualization for monocular metric depth estimation\nin underwater scenes, highlighting the importance of domain adaptation and\nscale-aware supervision for achieving robust and generalizable metric depth\npredictions in challenging underwater environments for future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02148v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "水下单目度量深度估计：真实世界基准和合成微调", "tldr": "本文对水下单目度量深度估计模型进行了基准测试，并提出了一种使用合成水下数据微调模型的方法，显著提高了其在水下环境中的性能。", "motivation": "当前单目深度估计在水下环境中的可靠性受限，原因包括光衰减、散射、颜色失真、浑浊以及缺乏高质量的度量真值数据。现有模型在陆地数据上训练，在水下表现不佳，存在显著的领域差异。", "method": "本文首先在FLSea和SQUID等真实世界水下数据集上，对零样本和微调的单目度量深度估计模型进行了全面的基准测试。为了解决领域差异问题，作者使用基于物理的水下图像形成模型生成了Hypersim数据集的合成水下变体，并用其对带有ViT-S骨干编码器的Depth Anything V2模型进行了微调。", "result": "在陆地数据上训练的大型模型在水下表现不佳。作者微调的模型在所有基准测试中持续提高了性能，并且优于仅在干净的陆地Hypersim数据集上训练的基线模型。", "conclusion": "本研究详细评估了水下场景中的单目度量深度估计，强调了领域适应和尺度感知监督对于在挑战性水下环境中实现鲁棒和可泛化度量深度预测的重要性。", "translation": "单目深度估计最近取得了进展，不仅能提供相对深度预测，还能提供度量深度预测。然而，由于光衰减和散射、颜色失真、浑浊以及缺乏高质量的度量真值数据，其在水下环境中的可靠性仍然有限。在本文中，我们提出了一个全面的基准测试，评估了在具有度量深度标注的真实世界水下数据集（如FLSea和SQUID）上的零样本和微调单目度量深度估计模型。我们评估了一系列最先进的模型在不同范围的水下条件下的表现。我们的结果表明，在陆地（真实或合成）数据上训练的大规模模型，虽然在空中设置中有效，但由于显著的领域差异，在水下表现不佳。为了解决这个问题，我们使用一个基于物理的水下图像形成模型生成了Hypersim数据集的合成水下变体，并用其对带有ViT-S骨干编码器的Depth Anything V2进行了微调。我们证明了我们微调的模型在所有基准测试中持续提高了性能，并且优于仅在干净的空中Hypersim数据集上训练的基线模型。我们的研究为水下场景中的单目度量深度估计提供了详细的评估和可视化，强调了领域适应和尺度感知监督对于在挑战性水下环境中实现鲁棒和可泛化度量深度预测的重要性，对未来的研究具有指导意义。", "summary": "本文研究了水下单目度量深度估计的挑战，指出现有模型因领域差异在水下表现不佳。为解决此问题，作者首先对现有模型在真实水下数据集上进行了基准测试。随后，通过生成合成水下数据集并微调Depth Anything V2模型，显著提升了其在水下环境中的深度估计性能。研究强调了领域适应和尺度感知监督在水下深度估计中的关键作用。", "keywords": "水下单目深度估计, 领域适应, 深度学习, 合成数据, 基准测试", "comments": "这篇论文的创新点在于构建了真实世界水下深度估计的基准，并提出了通过合成数据微调来解决水下领域适应性问题的方法。其重要性体现在为水下机器人、水下勘测等领域提供了更可靠的深度感知技术。通过物理模型生成合成水下数据，有效弥补了真实水下高质量标注数据稀缺的不足，为未来研究提供了有价值的思路。"}}
{"id": "2507.02464", "title": "Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems", "authors": ["Craig S Wright"], "categories": ["cs.GT", "cs.DC", "cs.FL", "cs.IR", "econ.GN", "q-fin.EC", "68M14, 91A05, 68Q85", "C.2.4; D.2.4; F.1.1"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      51 pages 4 tables, includes formal proofs, automata construction, and case study on Bitcoin Script", "url": "http://arxiv.org/abs/2507.02464v1", "summary": "The CAP theorem asserts a trilemma between consistency, availability, and\npartition tolerance. This paper introduces a rigorous automata-theoretic and\neconomically grounded framework that reframes the CAP trade-off as a constraint\noptimization problem. We model distributed systems as partition-aware state\nmachines and embed economic incentive layers to stabilize consensus behavior\nacross adversarially partitioned networks. By incorporating game-theoretic\nmechanisms into the global transition semantics, we define provable bounds on\nconvergence, liveness, and correctness. Our results demonstrate that\navailability and consistency can be simultaneously preserved within bounded\nepsilon margins, effectively extending the classical CAP limits through formal\neconomic control.", "comment": "51 pages 4 tables, includes formal proofs, automata construction, and\n  case study on Bitcoin Script", "pdf_url": "http://arxiv.org/pdf/2507.02464v1", "cate": "cs.GT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过自动机理论经济设计解决CAP：实时分区容错系统的统一数学框架", "tldr": "本文提出了一个基于自动机理论和经济学原理的框架，将CAP定理重新定义为约束优化问题，从而在有界误差范围内同时保持分布式系统的可用性和一致性。", "motivation": "CAP定理断言了分布式系统中一致性、可用性和分区容错性之间的三难困境。本文旨在通过将CAP权衡重新定义为约束优化问题来解决这一问题。", "method": "本文引入了一个严格的自动机理论和经济学基础框架，将分布式系统建模为分区感知状态机，并嵌入经济激励层以稳定对抗性分区网络中的共识行为。通过将博弈论机制融入全局转换语义中，定义了收敛性、活跃性和正确性的可证明界限。", "result": "研究结果表明，可用性和一致性可以在有界epsilon裕度内同时保持，有效地通过形式化经济控制扩展了经典CAP的限制。", "conclusion": "通过形式化的经济控制，可以有效地扩展经典CAP定理的限制，使得在实时分区容错系统中同时保持可用性和一致性成为可能。", "translation": "CAP定理断言了一致性、可用性和分区容错性之间的三难困境。本文引入了一个严格的自动机理论和经济学基础框架，将CAP权衡重新定义为约束优化问题。我们将分布式系统建模为分区感知状态机，并嵌入经济激励层，以稳定对抗性分区网络中的共识行为。通过将博弈论机制融入全局转换语义中，我们定义了收敛性、活跃性和正确性的可证明界限。我们的结果表明，可用性和一致性可以在有界epsilon裕度内同时保持，有效地通过形式化经济控制扩展了经典CAP的限制。", "summary": "本文提出了一个结合自动机理论和经济学原理的统一数学框架，旨在解决分布式系统中的CAP定理限制。通过将系统建模为分区感知状态机并引入博弈论激励机制，该框架将CAP权衡转化为约束优化问题。研究结果表明，在有界误差范围内，分布式系统可以同时实现高可用性和一致性，从而突破了传统CAP定理的界限。", "keywords": "CAP定理, 分布式系统, 自动机理论, 经济设计, 博弈论", "comments": "本文的创新之处在于将自动机理论与经济学设计（特别是博弈论机制）相结合，为解决CAP定理提供了一个新颖的视角和统一的数学框架。这种跨学科的方法为在复杂、对抗性分布式环境中实现系统属性提供了理论基础和实际潜力，特别是通过激励机制来稳定共识行为，这在传统CAP理论中是未被充分探索的。"}}
{"id": "2507.02524", "title": "Time Resolution Independent Operator Learning", "authors": ["Diab W. Abueidda", "Mbebo Nonna", "Panos Pantidis", "Mostafa E. Mobasher"], "categories": ["cs.CE", "cs.NA", "math.NA"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02524v1", "summary": "Accurately learning solution operators for time-dependent partial\ndifferential equations (PDEs) from sparse and irregular data remains a\nchallenging task. Recurrent DeepONet extensions inherit the discrete-time\nlimitations of sequence-to-sequence (seq2seq) RNN architectures, while\nneural-ODE surrogates cannot incorporate new inputs after initialization. We\nintroduce NCDE-DeepONet, a continuous-time operator network that embeds a\nNeural Controlled Differential Equation (NCDE) in the branch and augments the\ntrunk with explicit space-time coordinates. The NCDE encodes an entire load\nhistory as the solution of a controlled ODE driven by a spline-interpolated\ninput path, making the representation input-resolution-independent: it encodes\ndifferent input signal discretizations of the observed samples. The trunk then\nprobes this latent path at arbitrary spatial locations and times, rendering the\noverall map output-resolution independent: predictions can be queried on meshes\nand time steps unseen during training without retraining or interpolation.\nBenchmarks on transient Poisson, elastodynamic, and thermoelastic problems\nconfirm the robustness and accuracy of the framework, achieving almost instant\nsolution prediction. These findings suggest that controlled dynamics provide a\nprincipled and efficient foundation for high-fidelity operator learning in\ntransient mechanics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02524v1", "cate": "cs.CE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "时间分辨率无关算子学习", "tldr": "NCDE-DeepONet是一个连续时间算子网络，它将神经受控微分方程（NCDE）嵌入分支网络，并用显式时空坐标增强主干网络，实现了输入和输出的时间分辨率无关性，并在瞬态问题上表现出鲁棒性和准确性。", "motivation": "从稀疏和不规则数据中准确学习时间依赖偏微分方程（PDEs）的解算子仍然是一个挑战。现有的循环DeepONet扩展受限于离散时间，而神经-ODE代理在初始化后无法整合新的输入。", "method": "本文提出了NCDE-DeepONet，一个连续时间算子网络。它在分支网络中嵌入了神经受控微分方程（NCDE），并通过由样条插值输入路径驱动的受控ODE来编码完整的加载历史，从而使表示与输入分辨率无关。主干网络则通过显式时空坐标增强，可以在任意空间位置和时间探测这个潜在路径，使得整体映射与输出分辨率无关。", "result": "在瞬态泊松、弹性动力学和热弹性问题上的基准测试证实了该框架的鲁棒性和准确性，实现了几乎即时的解预测。", "conclusion": "研究结果表明，受控动力学为瞬态力学中高保真算子学习提供了一个原则性且高效的基础。", "translation": "准确地从稀疏和不规则数据中学习时间依赖偏微分方程（PDEs）的解算子仍然是一项具有挑战性的任务。循环DeepONet扩展继承了序列到序列（seq2seq）RNN架构的离散时间限制，而神经-ODE代理在初始化后无法整合新的输入。我们引入了NCDE-DeepONet，一个连续时间算子网络，它在分支网络中嵌入了一个神经受控微分方程（NCDE），并用显式时空坐标增强了主干网络。NCDE将整个加载历史编码为由样条插值输入路径驱动的受控ODE的解，使得表示与输入分辨率无关：它编码了观测样本的不同输入信号离散化。然后，主干网络在任意空间位置和时间探测这个潜在路径，使得整体映射与输出分辨率无关：可以在训练期间未见的网格和时间步上查询预测，无需重新训练或插值。在瞬态泊松、弹性动力学和热弹性问题上的基准测试证实了该框架的鲁棒性和准确性，实现了几乎即时的解预测。这些发现表明，受控动力学为瞬态力学中高保真算子学习提供了一个原则性且高效的基础。", "summary": "本文提出了一种名为NCDE-DeepONet的连续时间算子网络，旨在解决从稀疏和不规则数据中学习时间依赖PDEs解算子的挑战。该模型通过在分支网络中嵌入神经受控微分方程（NCDE）来编码输入历史，使其表示与输入分辨率无关。同时，主干网络通过显式时空坐标增强，使得模型输出与时间分辨率无关，可以在未见的时间步和网格上进行预测，无需重新训练。实验结果表明，NCDE-DeepONet在瞬态泊松、弹性动力学和热弹性问题上表现出高鲁棒性和准确性，并能实现快速的解预测。", "keywords": "算子学习, 神经受控微分方程, 偏微分方程, 时间分辨率无关, DeepONet", "comments": "该论文通过引入NCDE和显式时空坐标，巧妙地解决了现有算子学习方法在处理时间依赖PDEs时面临的离散时间限制和输入更新问题。其创新点在于实现了输入和输出的时间分辨率无关性，极大地提高了模型的通用性和实用性，无需针对不同的时间离散化进行重新训练或插值，这对于实际工程应用具有重要意义。"}}
{"id": "2507.02098", "title": "A robust and adaptive MPC formulation for Gaussian process models", "authors": ["Mathieu Dubied", "Amon Lahr", "Melanie N. Zeilinger", "Johannes Köhler"], "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02098v1", "summary": "In this paper, we present a robust and adaptive model predictive control\n(MPC) framework for uncertain nonlinear systems affected by bounded\ndisturbances and unmodeled nonlinearities. We use Gaussian Processes (GPs) to\nlearn the uncertain dynamics based on noisy measurements, including those\ncollected during system operation. As a key contribution, we derive robust\npredictions for GP models using contraction metrics, which are incorporated in\nthe MPC formulation. The proposed design guarantees recursive feasibility,\nrobust constraint satisfaction and convergence to a reference state, with high\nprobability. We provide a numerical example of a planar quadrotor subject to\ndifficult-to-model ground effects, which highlights significant improvements\nachieved through the proposed robust prediction method and through online\nlearning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02098v1", "cate": "eess.SY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "高斯过程模型的鲁棒自适应模型预测控制公式", "tldr": "提出了一种结合高斯过程和收缩度量的鲁棒自适应MPC框架，用于不确定非线性系统，确保了鲁棒性和收敛性。", "motivation": "针对受有界扰动和未建模非线性影响的不确定非线性系统，需要一种鲁棒且自适应的控制框架。", "method": "该研究提出了一个鲁棒自适应模型预测控制（MPC）框架。它利用高斯过程（GPs）从噪声测量中学习不确定动力学，包括系统运行期间收集的数据。关键贡献在于使用收缩度量为GP模型推导出鲁棒预测，并将其整合到MPC公式中。", "result": "所提出的设计保证了递归可行性、鲁棒约束满足以及高概率收敛到参考状态。通过一个受难以建模的地面效应影响的平面四旋翼飞行器数值示例，展示了所提出的鲁棒预测方法和在线学习所实现的显著改进。", "conclusion": "该鲁棒自适应MPC框架结合高斯过程和收缩度量，能够有效地控制不确定非线性系统，确保鲁棒性、可行性、约束满足和收敛性。", "translation": "本文提出了一种针对受有界扰动和未建模非线性影响的不确定非线性系统的鲁棒自适应模型预测控制（MPC）框架。我们使用高斯过程（GPs）根据噪声测量（包括系统运行期间收集的数据）来学习不确定动力学。作为一个关键贡献，我们使用收缩度量推导了GP模型的鲁棒预测，并将其纳入MPC公式中。所提出的设计以高概率保证了递归可行性、鲁棒约束满足和收敛到参考状态。我们提供了一个受难以建模的地面效应影响的平面四旋翼飞行器数值示例，该示例突出了通过所提出的鲁棒预测方法和在线学习所实现的显著改进。", "summary": "本文提出了一种用于不确定非线性系统的鲁棒自适应模型预测控制（MPC）框架。该框架利用高斯过程（GPs）学习系统动力学，并创新性地通过收缩度量推导出鲁棒预测，将其融入MPC。该设计确保了高概率下的递归可行性、鲁棒约束满足和状态收敛。一个平面四旋翼飞行器的数值案例验证了其在处理复杂不确定性方面的显著性能提升。", "keywords": "模型预测控制, 高斯过程, 鲁棒控制, 不确定系统, 收缩度量", "comments": "该论文的创新点在于将高斯过程与收缩度量相结合，为不确定非线性系统提供鲁棒预测，并将其整合到MPC框架中。这种方法有效地解决了系统不确定性和未建模非线性带来的挑战，并通过理论保证和数值示例展示了其有效性，对于实际工程应用具有重要意义。"}}
{"id": "2507.01982", "title": "DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism", "authors": ["Siqing Long", "Xiangzhi Huang", "Jiemin Xie", "Ming Cai"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      39 pages, 14 figures", "url": "http://arxiv.org/abs/2507.01982v1", "summary": "Accurate traffic demand forecasting enables transportation management\ndepartments to allocate resources more effectively, thereby improving their\nutilization efficiency. However, complex spatiotemporal relationships in\ntraffic systems continue to limit the performance of demand forecasting models.\nTo improve the accuracy of spatiotemporal traffic demand prediction, we propose\na new graph convolutional network structure called DKGCM. Specifically, we\nfirst consider the spatial flow distribution of different traffic nodes and\npropose a novel temporal similarity-based clustering graph convolution method,\nDK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering\nto group traffic nodes and more effectively capture spatial dependencies. On\nthe temporal scale, we integrate the Fast Fourier Transform (FFT) within the\nbidirectional Mamba deep learning framework to capture temporal dependencies in\ntraffic demand. To further optimize model training, we incorporate the GRPO\nreinforcement learning strategy to enhance the loss function feedback\nmechanism. Extensive experiments demonstrate that our model outperforms several\nadvanced methods and achieves strong results on three public datasets.", "comment": "39 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.01982v1", "cate": "cs.LG", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DKGCM：一种融合空间节点聚类方法和傅里叶双向Mamba机制的交通流时空预测模型", "tldr": "DKGCM是一种新的交通流时空预测模型，通过结合基于时间相似度的空间聚类和傅里叶双向Mamba机制来提高预测准确性，并在公共数据集上表现出色。", "motivation": "交通系统中复杂的时空关系限制了现有交通需求预测模型的性能，而准确的交通需求预测能帮助交通管理部门更有效地分配资源，提高利用效率。", "method": "提出了一种名为DKGCM的图卷积网络结构。空间方面，提出了基于时间相似度的聚类图卷积方法DK-GCN，利用动态时间规整(DTW)和K-means聚类对交通节点进行分组，以捕获空间依赖。时间方面，将快速傅里叶变换(FFT)集成到双向Mamba深度学习框架中，以捕获时间依赖。此外，还引入了GRPO强化学习策略来优化模型训练和增强损失函数反馈机制。", "result": "模型在三个公共数据集上进行了广泛实验，结果表明DKGCM优于多种先进方法并取得了良好效果。", "conclusion": "DKGCM模型能够有效提高时空交通需求预测的准确性，在复杂时空关系下表现出优越的性能。", "translation": "准确的交通需求预测使交通管理部门能够更有效地分配资源，从而提高其利用效率。然而，交通系统中复杂的时空关系持续限制着需求预测模型的性能。为了提高时空交通需求预测的准确性，我们提出了一种新的图卷积网络结构，名为DKGCM。具体来说，我们首先考虑不同交通节点的空间流分布，并提出了一种新颖的基于时间相似度的聚类图卷积方法DK-GCN。该方法利用动态时间规整（DTW）和K-means聚类对交通节点进行分组，更有效地捕获空间依赖性。在时间尺度上，我们将快速傅里叶变换（FFT）集成到双向Mamba深度学习框架中，以捕获交通需求中的时间依赖性。为了进一步优化模型训练，我们引入了GRPO强化学习策略来增强损失函数反馈机制。广泛的实验表明，我们的模型优于几种先进方法，并在三个公共数据集上取得了优异的结果。", "summary": "本文提出了一种名为DKGCM的新型图卷积网络结构，用于提高时空交通需求预测的准确性。该模型通过DK-GCN方法利用DTW和K-means聚类处理空间依赖性，并结合快速傅里叶变换与双向Mamba框架来捕获时间依赖性。此外，引入GRPO强化学习策略以优化模型训练。实验结果表明，DKGCM在多个公共数据集上显著优于现有先进方法。", "keywords": "交通流预测, 时空模型, 图卷积网络, Mamba, 强化学习", "comments": "该论文的创新点在于其多维度的融合方法：在空间维度上，利用DTW和K-means进行基于时间相似度的节点聚类，这比传统的静态图结构更能有效捕捉动态的空间关联；在时间维度上，结合FFT与Mamba机制，能够更高效地处理序列数据并捕获长短期依赖。此外，引入强化学习策略优化训练过程，进一步提升了模型的性能和鲁棒性。这种综合性的方法有望在复杂交通流预测领域取得重要进展。"}}
{"id": "2507.02262", "title": "Localized kernel method for separation of linear chirps", "authors": ["Eric Mason", "Sippanon Kitimoon", "Hrushikesh Mhaskar"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02262v1", "summary": "The task of separating a superposition of signals into its individual\ncomponents is a common challenge encountered in various signal processing\napplications, especially in domains such as audio and radar signals. A previous\npaper by Chui and Mhaskar proposes a method called Signal Separation Operator\n(SSO) to find the instantaneous frequencies and amplitudes of such\nsuperpositions where both of these change continuously and slowly over time. In\nthis paper, we amplify and modify this method in order to separate chirp\nsignals in the presence of crossovers, a very low SNR, and discontinuities. We\ngive a theoretical analysis of the behavior of SSO in the presence of noise to\nexamine the relationship between the minimal separation, minimal amplitude,\nSNR, and sampling frequency. Our method is illustrated with a few examples, and\nnumerical results are reported on a simulated dataset comprising 7 simulated\nsignals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02262v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于分离线性啁啾的局部核方法", "tldr": "本文改进了信号分离算子（SSO）方法，以在存在交叉、极低信噪比和不连续性的情况下分离线性啁啾信号，并提供了理论分析和数值结果。", "motivation": "信号叠加分离是信号处理中常见的挑战，尤其是在音频和雷达信号领域。先前提出的信号分离算子（SSO）方法在处理存在交叉、极低信噪比和不连续性的线性啁啾信号时面临局限性。", "method": "本文对Chui和Mhaskar提出的信号分离算子（SSO）方法进行了放大和修改，使其能够分离存在交叉、极低信噪比和不连续性的啁啾信号。同时，对SSO在噪声存在下的行为进行了理论分析，以检验最小分离、最小幅度、信噪比和采样频率之间的关系。", "result": "该方法通过几个例子进行了说明，并在一个包含7个模拟信号的模拟数据集上报告了数值结果，表明其能够处理存在交叉、极低信噪比和不连续性的线性啁啾信号。", "conclusion": "本文成功地放大和修改了SSO方法，使其能够有效分离在复杂条件下的线性啁啾信号，并通过理论分析和数值结果验证了其性能。", "translation": "将信号叠加分离成其单独的组成部分是各种信号处理应用中常见的挑战，尤其是在音频和雷达信号等领域。Chui和Mhaskar先前的一篇论文提出了一种称为信号分离算子（SSO）的方法，用于查找此类叠加的瞬时频率和幅度，其中这两者都随时间连续缓慢变化。在本文中，我们放大并修改了这种方法，以便在存在交叉、极低信噪比和不连续性的情况下分离啁啾信号。我们对SSO在噪声存在下的行为进行了理论分析，以检验最小分离、最小幅度、信噪比和采样频率之间的关系。我们的方法通过几个例子进行了说明，并在一个包含7个模拟信号的模拟数据集上报告了数值结果。", "summary": "本文针对信号叠加分离的挑战，改进了Chui和Mhaskar提出的信号分离算子（SSO）方法。通过放大和修改SSO，使其能够在存在交叉、极低信噪比和不连续性的复杂条件下分离线性啁啾信号。研究还提供了SSO在噪声存在下的理论分析，探讨了最小分离、幅度、信噪比和采样频率之间的关系。数值结果在模拟数据集上得到了验证。", "keywords": "线性啁啾, 信号分离, 局部核方法, 信号分离算子, 低信噪比", "comments": "本文的创新点在于对现有信号分离算子（SSO）方法的改进和扩展，使其能够应对更具挑战性的信号环境，如低信噪比和不连续性。理论分析的加入提升了研究的深度，有助于理解方法在噪声条件下的性能边界。这对于实际信号处理应用，特别是雷达和音频领域具有重要意义。"}}
{"id": "2507.02289", "title": "CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR", "authors": ["Wangbin Ding", "Lei Li", "Junyi Qiu", "Bogen Lin", "Mingjing Yang", "Liqin Huang", "Lianming Wu", "Sihan Wang", "Xiahai Zhuang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02289v1", "summary": "Myocardial infarction (MI) is a leading cause of death worldwide. Late\ngadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR)\nimaging can respectively identify scarring and edema areas, both of which are\nessential for MI risk stratification and prognosis assessment. Although\ncombining complementary information from multi-sequence CMR is useful,\nacquiring these sequences can be time-consuming and prohibitive, e.g., due to\nthe administration of contrast agents. Cine CMR is a rapid and contrast-free\nimaging technique that can visualize both motion and structural abnormalities\nof the myocardium induced by acute MI. Therefore, we present a new end-to-end\ndeep neural network, referred to as CineMyoPS, to segment myocardial\npathologies, \\ie scars and edema, solely from cine CMR images. Specifically,\nCineMyoPS extracts both motion and anatomy features associated with MI. Given\nthe interdependence between these features, we design a consistency loss\n(resembling the co-training strategy) to facilitate their joint learning.\nFurthermore, we propose a time-series aggregation strategy to integrate\nMI-related features across the cardiac cycle, thereby enhancing segmentation\naccuracy for myocardial pathologies. Experimental results on a multi-center\ndataset demonstrate that CineMyoPS achieves promising performance in myocardial\npathology segmentation, motion estimation, and anatomy segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02289v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CineMyoPS：从电影心脏磁共振图像中分割心肌病理", "tldr": "CineMyoPS是一种新的深度学习网络，可以仅从快速、无造影剂的电影CMR图像中分割心肌梗死引起的瘢痕和水肿区域，通过结合运动和解剖特征以及时间序列聚合来提高分割精度。", "motivation": "现有的心肌梗死（MI）诊断方法，如晚期钆增强（LGE）和T2加权心脏磁共振（CMR）成像，虽然能够识别瘢痕和水肿区域，但其获取过程耗时且需要施用造影剂，存在局限性。", "method": "本文提出了一种名为CineMyoPS的端到端深度神经网络，旨在仅从电影心脏磁共振（cine CMR）图像中分割心肌病理（即瘢痕和水肿）。CineMyoPS通过提取与心肌梗死相关的运动和解剖特征，并设计了一种一致性损失（co-training strategy）来促进这些特征的联合学习。此外，该方法还引入了时间序列聚合策略，以整合整个心动周期中与MI相关的特征，从而提高分割准确性。", "result": "在多中心数据集上的实验结果表明，CineMyoPS在心肌病理分割、运动估计和解剖分割方面均取得了有希望的性能。", "conclusion": "CineMyoPS能够仅通过快速、无造影剂的电影CMR图像有效地分割心肌病理，并在多中心数据集上表现出良好的性能，这表明它在MI风险分层和预后评估中具有潜在应用价值。", "translation": "心肌梗死（MI）是全球主要的死因。晚期钆增强（LGE）和T2加权心脏磁共振（CMR）成像可以分别识别瘢痕和水肿区域，这两者对于MI风险分层和预后评估至关重要。尽管结合多序列CMR的补充信息很有用，但获取这些序列可能非常耗时且成本高昂，例如由于需要施用造影剂。电影CMR是一种快速且无需造影剂的成像技术，可以可视化急性MI引起的心肌运动和结构异常。因此，我们提出了一种新的端到端深度神经网络，称为CineMyoPS，仅从电影CMR图像中分割心肌病理，即瘢痕和水肿。具体而言，CineMyoPS提取与MI相关的运动和解剖特征。鉴于这些特征之间的相互依赖性，我们设计了一种一致性损失（类似于协同训练策略）来促进它们的联合学习。此外，我们提出了一种时间序列聚合策略，以整合整个心动周期中与MI相关的特征，从而提高心肌病理分割的准确性。在多中心数据集上的实验结果表明，CineMyoPS在心肌病理分割、运动估计和解剖分割方面取得了有希望的性能。", "summary": "本文提出了一种名为CineMyoPS的端到端深度神经网络，旨在仅利用快速、无造影剂的电影心脏磁共振（CMR）图像来分割心肌梗死（MI）引起的瘢痕和水肿区域。该方法通过提取运动和解剖特征，并引入一致性损失以促进其联合学习，同时采用时间序列聚合策略整合跨心动周期的特征，从而提高了心肌病理分割的准确性。实验证明，CineMyoPS在多中心数据集上在心肌病理分割、运动估计和解剖分割方面均表现出良好的性能。", "keywords": "心肌梗死, 电影CMR, 深度学习, 病理分割, 一致性损失", "comments": "这项工作具有重要的临床意义，因为它提出了一种无需造影剂且快速的MI病理分割方法，解决了现有LGE和T2加权CMR成像的耗时和造影剂限制。其创新点在于利用电影CMR的运动和解剖信息，并通过一致性损失和时间序列聚合策略提升了分割的准确性，为MI的风险分层和预后评估提供了新的工具。"}}
{"id": "2507.02117", "title": "A Computational Proof of the Highest-Scoring Boggle Board", "authors": ["Dan Vanderkam"], "categories": ["cs.DS", "I.2.8, E.1"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      14 pages, 2 figures, code available at this https URL", "url": "http://arxiv.org/abs/2507.02117v1", "summary": "Finding all the words on a Boggle board is a classic computer programming\nproblem. With a fast Boggle solver, local optimization techniques such as\nhillclimbing and simulated annealing can be used to find particularly\nhigh-scoring boards. The sheer number of possible Boggle boards has\nhistorically prevented an exhaustive search for the global optimum board. We\napply Branch and Bound and a decision diagram-like data structure to perform\nthe first such search. We find that the highest-scoring boards found via\nhillclimbing are, in fact, the global optima.", "comment": "14 pages, 2 figures, code available at\n  https://github.com/danvk/hybrid-boggle/", "pdf_url": "http://arxiv.org/pdf/2507.02117v1", "cate": "cs.DS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "Boggle最高得分棋盘的计算证明", "tldr": "本文首次通过分支定界和决策图数据结构对Boggle最高得分棋盘进行了穷尽搜索，证明了局部优化技术找到的最高分棋盘确实是全局最优解。", "motivation": "寻找Boggle棋盘上的所有单词是一个经典的编程问题。尽管可以使用局部优化技术找到高分棋盘，但由于可能的Boggle棋盘数量巨大，历史上一直无法进行穷尽搜索来找到全局最优解。", "method": "应用分支定界（Branch and Bound）算法和一种类似决策图（decision diagram-like）的数据结构进行穷尽搜索。", "result": "发现通过爬山法（hillclimbing）找到的最高分棋盘实际上就是全局最优解。", "conclusion": "通过首次穷尽搜索，本文计算证明了局部优化技术（如爬山法）在寻找Boggle最高得分棋盘时能够达到全局最优。", "translation": "在Boggle棋盘上找到所有单词是一个经典的计算机编程问题。借助快速的Boggle求解器，可以使用爬山法和模拟退火等局部优化技术来找到得分特别高的棋盘。然而，Boggle棋盘的可能数量巨大，这在历史上阻止了对全局最优棋盘进行穷尽搜索。我们应用分支定界和一种类似决策图的数据结构来首次进行这种搜索。我们发现通过爬山法找到的最高得分棋盘实际上就是全局最优解。", "summary": "本文首次对Boggle最高得分棋盘进行了穷尽搜索，解决了由于棋盘数量庞大而无法进行全局最优搜索的问题。研究人员运用分支定界算法和一种类似决策图的数据结构，计算证明了此前通过局部优化技术（如爬山法）发现的高分棋盘，确实是全局最优解。", "keywords": "Boggle, 全局最优, 分支定界, 决策图, 爬山法", "comments": "这项工作具有创新性，因为它首次通过计算方法对Boggle的最高得分棋盘进行了穷尽搜索，解决了长期存在的全局最优性问题。其重要性在于，它不仅证明了局部优化算法在特定问题上的有效性，也展示了分支定界和决策图数据结构在解决组合优化问题中的强大能力。"}}
{"id": "2507.02192", "title": "An Investigation on Combining Geometry and Consistency Constraints into Phase Estimation for Speech Enhancement", "authors": ["Chun-Wei Ho", "Pin-Jui Ku", "Hao Yen", "Sabato Marco Siniscalchi", "Yu Tsao", "Chin-Hui Lee"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.02192v1", "summary": "We propose a novel iterative phase estimation framework, termed multi-source\nGriffin-Lim algorithm (MSGLA), for speech enhancement (SE) under additive noise\nconditions. The core idea is to leverage the ad-hoc consistency constraint of\ncomplex-valued short-time Fourier transform (STFT) spectrograms to address the\nsign ambiguity challenge commonly encountered in geometry-based phase\nestimation. Furthermore, we introduce a variant of the geometric constraint\nframework based on the law of sines and cosines, formulating a new phase\nreconstruction algorithm using noise phase estimates. We first validate the\nproposed technique through a series of oracle experiments, demonstrating its\neffectiveness under ideal conditions. We then evaluate its performance on the\nVB-DMD and WSJ0-CHiME3 data sets, and show that the proposed MSGLA variants\nmatch well or slightly outperform existing algorithms, including direct phase\nestimation and DNN-based sign prediction, especially in terms of background\nnoise suppression.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.02192v1", "cate": "eess.AS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "结合几何和一致性约束的语音增强相位估计研究", "tldr": "提出了一种结合几何和一致性约束的新型迭代相位估计算法MSGLA，用于解决语音增强中的符号模糊问题，并在实验中表现出与现有算法相当或略优的性能，尤其在背景噪声抑制方面。", "motivation": "解决几何相位估计中常见的符号模糊挑战。", "method": "提出了一种名为多源Griffin-Lim算法（MSGLA）的新型迭代相位估计框架。核心思想是利用复值短时傅里叶变换（STFT）谱图的临时一致性约束来解决几何相位估计中常见的符号模糊挑战。此外，引入了一种基于正弦和余弦定律的几何约束框架变体，并利用噪声相位估计公式化了一种新的相位重建算法。", "result": "首先通过一系列预言实验验证了所提技术的有效性，证明了其在理想条件下的有效性。然后在VB-DMD和WSJ0-CHiME3数据集上评估了其性能，结果表明所提出的MSGLA变体与现有算法（包括直接相位估计和基于DNN的符号预测）表现相当或略优，尤其在背景噪声抑制方面。", "conclusion": "所提出的MSGLA算法通过结合几何和一致性约束，有效解决了语音增强中的相位估计问题，并在多个数据集上取得了良好的性能，尤其在背景噪声抑制方面表现突出。", "translation": "我们提出了一种新颖的迭代相位估计框架，称为多源Griffin-Lim算法（MSGLA），用于在加性噪声条件下的语音增强（SE）。其核心思想是利用复值短时傅里叶变换（STFT）谱图的临时一致性约束来解决几何相位估计中常见的符号模糊挑战。此外，我们引入了一种基于正弦和余弦定律的几何约束框架变体，并利用噪声相位估计公式化了一种新的相位重建算法。我们首先通过一系列预言实验验证了所提技术的有效性，证明了其在理想条件下的有效性。然后我们在VB-DMD和WSJ0-CHiME3数据集上评估了其性能，结果表明所提出的MSGLA变体与现有算法（包括直接相位估计和基于DNN的符号预测）表现相当或略优，尤其在背景噪声抑制方面。", "summary": "本文提出了一种名为多源Griffin-Lim算法（MSGLA）的新型迭代相位估计框架，旨在解决加性噪声环境下语音增强中的相位估计问题。该方法通过结合复值STFT谱图的一致性约束来解决几何相位估计中的符号模糊问题，并引入了基于正弦和余弦定律的几何约束变体。实验结果表明，MSGLA在理想条件下有效，并在标准数据集上与现有算法（如直接相位估计和基于DNN的符号预测）性能相当或略优，特别是在背景噪声抑制方面表现出色。", "keywords": "语音增强, 相位估计, Griffin-Lim算法, 几何约束, 一致性约束", "comments": "本文的创新之处在于将几何约束和一致性约束有效地结合到相位估计中，并通过迭代框架（MSGLA）解决了几何相位估计中的符号模糊问题。该方法在语音增强领域具有重要意义，尤其是在背景噪声抑制方面的表现值得关注。其对现有算法的超越或匹配表明了该方法的实用性和有效性。"}}
{"id": "2507.02009", "title": "Uncertainty-Aware Complex Scientific Table Data Extraction", "authors": ["Kehinde Ajayi", "Yi He", "Jian Wu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02009v1", "summary": "Table structure recognition (TSR) and optical character recognition (OCR)\nplay crucial roles in extracting structured data from tables in scientific\ndocuments. However, existing extraction frameworks built on top of TSR and OCR\nmethods often fail to quantify the uncertainties of extracted results. To\nobtain highly accurate data for scientific domains, all extracted data must be\nmanually verified, which can be time-consuming and labor-intensive. We propose\na framework that performs uncertainty-aware data extraction for complex\nscientific tables, built on conformal prediction, a model-agnostic method for\nuncertainty quantification (UQ). We explored various uncertainty scoring\nmethods to aggregate the uncertainties introduced by TSR and OCR. We rigorously\nevaluated the framework using a standard benchmark and an in-house dataset\nconsisting of complex scientific tables in six scientific domains. The results\ndemonstrate the effectiveness of using UQ for extraction error detection, and\nby manually verifying only 47\\% of extraction results, the data quality can be\nimproved by 30\\%. Our work quantitatively demonstrates the role of UQ with the\npotential of improving the efficiency in the human-machine cooperation process\nto obtain scientifically usable data from complex tables in scientific\ndocuments. All code and data are available on GitHub at\nhttps://github.com/lamps-lab/TSR-OCR-UQ/tree/main.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02009v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "不确定性感知复杂科学表格数据提取", "tldr": "提出一个基于共形预测的不确定性感知框架，用于从复杂科学表格中提取数据，可显著提高数据质量并减少人工验证工作量。", "motivation": "现有基于表格结构识别（TSR）和光学字符识别（OCR）的数据提取框架未能量化提取结果的不确定性，导致需要耗时费力的人工验证以确保科学数据的准确性。", "method": "提出了一个基于共形预测（一种模型无关的不确定性量化方法）的不确定性感知数据提取框架。探索了各种不确定性评分方法来聚合TSR和OCR引入的不确定性。", "result": "结果表明，使用不确定性量化（UQ）对提取错误检测有效。通过仅手动验证47%的提取结果，数据质量可以提高30%。", "conclusion": "本研究定量地证明了不确定性量化在提高人机协作效率方面的作用，从而从科学文档的复杂表格中获取可科学使用的数据。", "translation": "表格结构识别（TSR）和光学字符识别（OCR）在从科学文档中的表格中提取结构化数据方面发挥着关键作用。然而，现有基于TSR和OCR方法的提取框架通常无法量化提取结果的不确定性。为了获得科学领域的高度准确数据，所有提取的数据都必须经过人工验证，这既耗时又费力。我们提出了一个基于共形预测（一种模型无关的不确定性量化（UQ）方法）的框架，该框架对复杂科学表格执行不确定性感知数据提取。我们探索了各种不确定性评分方法，以聚合TSR和OCR引入的不确定性。我们使用标准基准和包含六个科学领域复杂科学表格的内部数据集严格评估了该框架。结果表明，使用UQ进行提取错误检测的有效性，并且通过仅手动验证47%的提取结果，数据质量可以提高30%。我们的工作定量地证明了UQ的作用，并具有提高人机协作过程效率的潜力，以从科学文档的复杂表格中获取可科学使用的数据。所有代码和数据均可在GitHub上获取：https://github.com/lamps-lab/TSR-OCR-UQ/tree/main。", "summary": "本研究提出了一个不确定性感知的数据提取框架，用于处理复杂科学表格。该框架基于共形预测，并整合了表格结构识别（TSR）和光学字符识别（OCR）引入的不确定性。通过对标准基准和内部数据集的评估，结果显示该方法能有效检测提取错误，并且仅需人工验证47%的提取结果即可将数据质量提高30%，显著提升了从科学文档中获取可用数据的效率。", "keywords": "不确定性量化, 科学表格, 数据提取, 共形预测, TSR, OCR", "comments": "这项工作的创新之处在于将不确定性量化（特别是共形预测）引入到科学表格数据提取中，以解决现有方法无法量化提取结果不确定性的痛点。其重要性体现在通过量化不确定性，显著减少了人工验证的工作量，提高了数据提取的效率和质量，对于科学数据处理具有实际应用价值。该方法提供了一个量化评估提取可靠性的途径，有助于优化人机协作流程。"}}
{"id": "2507.02674", "title": "Real-time Image-based Lighting of Glints", "authors": ["Tom Kneiphof", "Reinhard Klein"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02674v1", "summary": "Image-based lighting is a widely used technique to reproduce shading under\nreal-world lighting conditions, especially in real-time rendering applications.\nA particularly challenging scenario involves materials exhibiting a sparkling\nor glittering appearance, caused by discrete microfacets scattered across their\nsurface. In this paper, we propose an efficient approximation for image-based\nlighting of glints, enabling fully dynamic material properties and environment\nmaps. Our novel approach is grounded in real-time glint rendering under area\nlight illumination and employs standard environment map filtering techniques.\nCrucially, our environment map filtering process is sufficiently fast to be\nexecuted on a per-frame basis. Our method assumes that the environment map is\npartitioned into few homogeneous regions of constant radiance. By filtering the\ncorresponding indicator functions with the normal distribution function, we\nobtain the probabilities for individual microfacets to reflect light from each\nregion. During shading, these probabilities are utilized to hierarchically\nsample a multinomial distribution, facilitated by our novel dual-gated Gaussian\napproximation of binomial distributions. We validate that our real-time\napproximation is close to ground-truth renderings for a range of material\nproperties and lighting conditions, and demonstrate robust and stable\nperformance, with little overhead over rendering glints from a single\ndirectional light. Compared to rendering smooth materials without glints, our\napproach requires twice as much memory to store the prefiltered environment\nmap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02674v1", "cate": "cs.GR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "实时基于图像的闪光照明", "tldr": "提出了一种高效的近似方法，用于实时图像基闪光照明，支持动态材质和环境贴图，并实现了接近真实渲染的效果。", "motivation": "在实时渲染应用中，基于图像的照明是重现真实世界照明条件下着色的常用技术。然而，对于具有闪烁或闪光外观的材质，由于其表面分散的离散微面，这是一个特别具有挑战性的场景。", "method": "我们提出了一种高效的基于图像的闪光照明近似方法。该方法基于区域光照下的实时闪光渲染，并采用标准的环境贴图过滤技术。环境贴图过滤过程速度足够快，可以逐帧执行。方法假设环境贴图被划分为几个恒定辐射的均匀区域。通过使用正态分布函数过滤相应的指示函数，获得单个微面从每个区域反射光的概率。在着色过程中，这些概率通过我们新颖的二项分布双门高斯近似来分层采样多项式分布。", "result": "我们验证了该实时近似方法在各种材质属性和照明条件下接近地面真实渲染，并展示了稳健和稳定的性能，与单个定向光渲染闪光相比，开销很小。与渲染没有闪光的平滑材质相比，我们的方法需要两倍的内存来存储预过滤的环境贴图。", "conclusion": "本文提出了一种高效的实时图像基闪光照明近似方法，能够处理动态材质和环境贴图，并实现了与地面真实渲染相近的效果，且性能稳健。", "translation": "基于图像的照明是一种广泛使用的技术，用于再现真实世界照明条件下的着色，尤其是在实时渲染应用中。一个特别具有挑战性的场景涉及呈现闪烁或闪光外观的材料，这由其表面分散的离散微面引起。在本文中，我们提出了一种高效的基于图像的闪光照明近似方法，能够实现完全动态的材料属性和环境贴图。我们新颖的方法基于区域光照下的实时闪光渲染，并采用标准的环境贴图过滤技术。至关重要的是，我们的环境贴图过滤过程速度足够快，可以逐帧执行。我们的方法假设环境贴图被划分为几个恒定辐射的均匀区域。通过使用正态分布函数过滤相应的指示函数，我们获得了单个微面从每个区域反射光的概率。在着色过程中，这些概率通过我们新颖的二项分布双门高斯近似来分层采样多项式分布。我们验证了我们的实时近似方法在各种材料属性和照明条件下接近地面真实渲染，并展示了稳健和稳定的性能，与从单个定向光渲染闪光相比，开销很小。与渲染没有闪光的平滑材料相比，我们的方法需要两倍的内存来存储预过滤的环境贴图。", "summary": "本文提出了一种高效的实时图像基闪光照明近似方法，旨在解决在实时渲染中处理具有闪光外观材质的挑战。该方法基于区域光照下的实时闪光渲染，利用标准环境贴图过滤技术，并引入了双门高斯近似来分层采样光照概率。实验证明，该方法在动态材质和环境贴图下能实现接近真实渲染的效果，且性能稳定，开销低，但预过滤环境贴图需要双倍内存。", "keywords": "图像基照明, 实时渲染, 闪光, 微面, 环境贴图过滤", "comments": "该论文的创新点在于提出了一个高效的近似方法来解决实时图像基闪光照明的难题，特别是在处理动态材质和环境贴图方面。其引入的“双门高斯近似”和将环境贴图划分为均匀区域的处理方式，为复杂光照效果的实时渲染提供了新的思路。该方法在保证视觉效果接近真实的同时，实现了较低的运行开销，这对于实时图形应用具有重要意义。然而，存储预过滤环境贴图所需的双倍内存可能是其在某些资源受限平台上的一个限制。"}}
{"id": "2507.02331", "title": "Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes", "authors": ["Ana Nikolikj", "Mario Andrés Muñoz", "Eva Tuba", "Tome Eftimov"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02331v1", "summary": "This paper leverages the recently introduced concept of algorithm footprints\nto investigate the interplay between algorithm configurations and problem\ncharacteristics. Performance footprints are calculated for six modular variants\nof the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the\nBBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.\nThese footprints provide insights into why different configurations of the same\nalgorithm exhibit varying performance and identify the problem features\ninfluencing these outcomes. Our analysis uncovers shared behavioral patterns\nacross configurations due to common interactions with problem properties, as\nwell as distinct behaviors on the same problem driven by differing problem\nfeatures. The results demonstrate the effectiveness of algorithm footprints in\nenhancing interpretability and guiding configuration choices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02331v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "追踪模块化CMA-ES配置在问题环境中的相互作用", "tldr": "本研究利用算法足迹概念，探究模块化CMA-ES算法配置与问题特征之间的相互作用，揭示了不同配置的性能差异原因，并证明了算法足迹在提高可解释性和指导配置选择方面的有效性。", "motivation": "本研究旨在调查算法配置与问题特性之间的相互作用，以理解同一算法的不同配置为何表现出不同的性能，并识别影响这些结果的问题特征。", "method": "本研究利用新引入的算法足迹概念，计算了六种模块化CMA-ES（modCMA）变体的性能足迹。这些变体在BBOB套件的24个基准问题上进行评估，涉及5维和30维两种设置。", "result": "分析揭示了由于与问题属性的共同相互作用而产生的跨配置共享行为模式，以及由不同问题特征驱动的在相同问题上的独特行为。算法足迹提供了关于同一算法不同配置性能差异的原因以及影响这些结果的问题特征的见解。", "conclusion": "研究结果表明，算法足迹在增强可解释性和指导配置选择方面是有效的。", "translation": "本文利用最近引入的算法足迹概念，研究算法配置与问题特征之间的相互作用。针对CMA-ES算法的六种模块化变体（modCMA），在BBOB套件的24个基准问题上，在5维和30维的二维设置下，计算了性能足迹。这些足迹提供了关于同一算法的不同配置为何表现出不同性能的见解，并识别了影响这些结果的问题特征。我们的分析揭示了由于与问题属性的共同相互作用而产生的跨配置共享行为模式，以及由不同问题特征驱动的在相同问题上的独特行为。结果表明算法足迹在增强可解释性和指导配置选择方面的有效性。", "summary": "本论文运用算法足迹概念，深入探究了模块化CMA-ES算法不同配置与问题特征之间的复杂相互作用。研究计算了六种modCMA变体在BBOB基准测试集24个问题（5维和30维）上的性能足迹。结果表明，算法足迹能有效揭示同一算法不同配置性能差异的原因，识别影响性能的问题特征，并展现了算法足迹在提高可解释性和优化配置选择方面的潜力。", "keywords": "算法足迹, CMA-ES, 算法配置, 问题景观, 可解释性", "comments": "该论文的创新点在于引入并应用了“算法足迹”这一概念，为理解复杂算法配置与其在不同问题景观上的性能之间的关系提供了一种新颖且可解释的工具。这对于优化算法设计和选择具有重要意义，有助于从数据层面深入分析算法行为，而非仅仅依赖于最终性能。"}}
{"id": "2507.02145", "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gonçalo Oliveira"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02145v1", "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02145v1", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "推理与否？对话摘要中推理LLMs的综合评估", "tldr": "本研究评估了推理LLM在对话摘要任务中的表现，发现明确的逐步推理并不能持续提高摘要质量，反而可能导致冗长和不一致。", "motivation": "对话摘要是一项具有挑战性且具有重要实际价值的任务。尽管大型语言模型在摘要任务中取得了显著进展，但逐步推理架构（如长链式思维CoT）在需要同时抽象和简洁的对话场景中的性能尚未被探索。", "method": "本研究首次对最先进的推理LLM和非推理LLM在通用、面向角色和面向查询三种对话摘要范式下进行了全面系统的评估。研究涵盖了不同的语言、领域和摘要长度，使用了SAMSum、DialogSum、CSDS和QMSum等基准数据集，并采用了基于LLM的自动指标和受人类启发的评估标准。同时进行了特定场景分析和详细案例研究。", "result": "研究发现，明确的逐步推理并不能持续改善对话摘要质量。相反，与非推理LLM相比，推理LLM往往容易冗长、事实不一致且摘要不够简洁。研究还确定了在复杂对话语境中，明确推理何时以及为何可能无法带来益处甚至会阻碍摘要。", "conclusion": "当前推理LLM在对话摘要方面存在局限性，需要针对真实世界对话摘要的定向建模和评估策略。", "translation": "对话摘要是一项具有挑战性的任务，在客户服务、会议分析和对话式AI中具有重要的实际价值。尽管大型语言模型（LLMs）在摘要任务中取得了实质性进展，但针对需要同时进行抽象和简洁的对话场景，逐步推理架构（特别是OpenAI-o1和DeepSeek-R1等长链式思维（CoT）实现）的性能仍未被探索。在这项工作中，我们首次对最先进的推理LLM和非推理LLM在三种主要范式——通用、面向角色和面向查询的对话摘要——进行了全面系统的评估。我们的研究涵盖了不同的语言、领域和摘要长度，利用了强大的基准（SAMSum、DialogSum、CSDS和QMSum）和先进的评估协议，包括基于LLM的自动指标和受人类启发的标准。与在其他推理密集型任务中的趋势相反，我们的发现表明，明确的逐步推理并不能持续改善对话摘要质量。相反，与非推理LLM相比，推理LLM往往容易冗长、事实不一致且摘要不够简洁。通过特定场景分析和详细案例研究，我们进一步确定了在复杂对话语境中，明确推理何时以及为何可能无法带来益处——甚至会阻碍——摘要。我们的工作为当前推理LLM的局限性提供了新见解，并强调了针对真实世界对话摘要的定向建模和评估策略的必要性。", "summary": "本研究首次全面评估了推理LLM在对话摘要任务中的表现，涵盖了通用、面向角色和面向查询的三种范式。研究发现，与在其他推理密集型任务中的表现不同，明确的逐步推理并不能持续提升对话摘要质量，反而常导致冗长、事实不一致和简洁性下降。文章通过详细分析揭示了推理LLM在此类任务中失效的原因，强调了针对真实世界对话摘要需要更具针对性的建模和评估策略。", "keywords": "对话摘要, 推理LLM, 链式思维, 性能评估, 局限性", "comments": "这项工作通过对推理LLM在对话摘要任务中的表现进行首次全面评估，揭示了其在特定应用场景下的局限性，挑战了“推理能力普遍提升所有任务”的普遍认知。其创新点在于系统性地对比了推理和非推理LLM，并深入分析了推理失效的原因，为未来对话摘要模型的设计和评估提供了重要指导。"}}
{"id": "2507.02176", "title": "Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis", "authors": ["Marc-André Carbonneau", "Benjamin van Niekerk", "Hugo Seuté", "Jean-Philippe Letendre", "Herman Kamper", "Julian Zaïdi"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop", "url": "http://arxiv.org/abs/2507.02176v1", "summary": "Modeling voice identity is challenging due to its multifaceted nature. In\ngenerative speech systems, identity is often assessed using automatic speaker\nverification (ASV) embeddings, designed for discrimination rather than\ncharacterizing identity. This paper investigates which aspects of a voice are\ncaptured in such representations. We find that widely used ASV embeddings focus\nmainly on static features like timbre and pitch range, while neglecting dynamic\nelements such as rhythm. We also identify confounding factors that compromise\nspeaker similarity measurements and suggest mitigation strategies. To address\nthese gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm\npatterns. This work contributes to the ongoing challenge of assessing speaker\nidentity consistency in the context of ever-better voice cloning systems. We\npublicly release our code.", "comment": "Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop", "pdf_url": "http://arxiv.org/pdf/2507.02176v1", "cate": "cs.SD", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "分析和改进语音合成中的说话人相似度评估", "tldr": "本研究发现现有自动说话人验证(ASV)嵌入主要关注静态语音特征，忽视动态特征，并提出U3D度量来评估说话人的动态韵律模式，以改进语音合成中的说话人相似度评估。", "motivation": "由于语音身份的复杂性，对其进行建模具有挑战性。在生成式语音系统中，语音身份通常使用自动说话人验证（ASV）嵌入进行评估，但这些嵌入是为区分而非表征身份而设计的，这导致了现有评估方法的不足。", "method": "本文研究了ASV嵌入捕获语音哪些方面。研究识别了影响说话人相似度测量的混淆因素并提出了缓解策略。为解决现有差距，本文提出了一种名为U3D的度量，用于评估说话人的动态韵律模式。", "result": "研究发现，广泛使用的ASV嵌入主要关注音色和音高范围等静态特征，而忽略了韵律等动态元素。同时，研究也识别了影响说话人相似度测量的混淆因素。", "conclusion": "本研究通过提出U3D度量来评估说话人的动态韵律模式，为持续改进语音克隆系统中说话人身份一致性的评估挑战做出了贡献。", "translation": "建模语音身份因其多面性而具有挑战性。在生成式语音系统中，身份通常使用自动说话人验证（ASV）嵌入进行评估，这些嵌入旨在用于区分而非表征身份。本文研究了此类表征中捕获了语音的哪些方面。我们发现，广泛使用的ASV嵌入主要关注音色和音高范围等静态特征，而忽略了韵律等动态元素。我们还识别了影响说话人相似度测量的混淆因素并提出了缓解策略。为了弥补这些空白，我们提出了U3D，这是一种评估说话人动态韵律模式的度量。这项工作有助于解决在日益优化的语音克隆系统中评估说话人身份一致性的持续挑战。我们公开了我们的代码。", "summary": "本研究旨在改进语音合成中的说话人相似度评估。通过分析发现，现有ASV嵌入主要关注静态语音特征，忽略了韵律等动态元素，且存在混淆因素。为此，论文提出了U3D度量，专门用于评估说话人的动态韵律模式，从而更准确地评估语音身份一致性，尤其在语音克隆系统中具有重要意义。", "keywords": "语音合成, 说话人相似度, ASV嵌入, 语音身份, 韵律模式", "comments": "该论文创新性地指出了现有ASV嵌入在评估语音身份时忽略动态特征的局限性，并提出了一个针对性强的解决方案——U3D度量来评估动态韵律模式。这对于提高语音克隆系统的真实感和身份一致性评估具有重要意义，填补了当前研究的空白。"}}
{"id": "2507.02281", "title": "Linearly Homomorphic Ring Signature Scheme over Lattices", "authors": ["Heng Guo", "Kun Tian", "Fengxia Liu", "Zhiyong Zheng"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02281v1", "summary": "Homomorphic ring signature schemes combine the strong anonymity of ring\nsignatures with the computability of homomorphic signatures, demonstrating\nsignificant potential in scenarios requiring both anonymous data provenance and\nverifiable homomorphic computation (e.g., confidential blockchain transactions\nand secure multi-party computation). However, no feasible homomorphic ring\nsignature scheme currently exists.\n  In this work, we propose the first lattice-based linearly homomorphic ring\nsignature scheme. Proven secure in the standard model under the small integer\nsolution (SIS) assumption, our scheme achieves strong anonymity under full key\nexposure and unforgeability against insider corruption attacks. As the first\nunified framework for ring signatures and linear homomorphic signatures, this\nconstruction provides a post-quantum-secure solution for the aforementioned\napplications, advancing the development of privacy-enhanced homomorphic\ncomputation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02281v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "格上线性同态环签名方案", "tldr": "提出了首个基于格的线性同态环签名方案，在标准模型下是安全的，并提供后量子安全。", "motivation": "同态环签名方案结合了环签名的强匿名性和同态签名的可计算性，在需要匿名数据溯源和可验证同态计算的场景（如保密区块链交易和安全多方计算）中具有巨大潜力。然而，目前不存在可行的同态环签名方案。", "method": "我们提出了首个基于格的线性同态环签名方案。", "result": "在标准模型下，基于小整数解（SIS）假设，我们的方案被证明是安全的，并在完全密钥暴露下实现强匿名性，以及对抗内部腐败攻击的不可伪造性。", "conclusion": "作为首个统一环签名和线性同态签名的框架，该构造为上述应用提供了一个后量子安全解决方案，推动了隐私增强同态计算的发展。", "translation": "同态环签名方案结合了环签名的强匿名性和同态签名的可计算性，在需要匿名数据溯源和可验证同态计算的场景（例如，保密区块链交易和安全多方计算）中展现出巨大潜力。然而，目前不存在可行的同态环签名方案。\n在这项工作中，我们提出了首个基于格的线性同态环签名方案。该方案在标准模型下，基于小整数解（SIS）假设被证明是安全的，并在完全密钥暴露下实现强匿名性，以及对抗内部腐败攻击的不可伪造性。作为首个统一环签名和线性同态签名的框架，该构造为上述应用提供了一个后量子安全解决方案，推动了隐私增强同态计算的发展。", "summary": "本文提出了首个基于格的线性同态环签名方案。该方案在标准模型下，基于SIS假设被证明是安全的，实现了强匿名性和不可伪造性。作为环签名和线性同态签名的统一框架，它为需要匿名数据溯源和可验证同态计算的应用提供了后量子安全解决方案，促进了隐私增强同态计算的发展。", "keywords": "同态环签名, 格, 线性同态, 后量子安全, 匿名性", "comments": "该论文首次提出了基于格的线性同态环签名方案，填补了现有同态环签名方案的空白。其创新之处在于将环签名和线性同态签名统一在一个框架内，并提供了后量子安全性，对于保密区块链和安全多方计算等隐私增强应用具有重要意义。"}}
{"id": "2507.02536", "title": "Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain", "authors": ["Azmat Ullah", "Maria Ilaria Lunesu", "Lodovica Marchesi", "Roberto Tonelli"], "categories": ["cs.CR", "cs.ET"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      2 pages", "url": "http://arxiv.org/abs/2507.02536v1", "summary": "This paper presents a blockchain-based Internet of Things (IoT) system for\nmonitoring pizza production in restaurants. IoT devices track temperature and\nhumidity in real-time, while blockchain ensures secure and tamper-proof data. A\nRaspberry Pi processes sensor data, captures images, triggers alerts, and\ninteracts with smart contracts. The system detects abnormal conditions,\nenabling quick responses. Blockchain adds transparency and traceability,\nsupporting compliance and audits. Experiments show improved ingredient\nmanagement, reduced waste, and increased kitchen efficiency.", "comment": "2 pages", "pdf_url": "http://arxiv.org/pdf/2507.02536v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于物联网和区块链的披萨生产实时监控与透明化", "tldr": "本文提出一个基于物联网和区块链的系统，用于实时监控披萨生产，确保数据安全、透明和可追溯，并提高厨房效率。", "motivation": "该研究旨在通过物联网和区块链技术，实现披萨生产过程的实时监控、数据安全、防篡改、透明化和可追溯性，以便快速响应异常情况，并支持合规性和审计。", "method": "系统采用物联网设备实时跟踪温度和湿度数据，并利用区块链技术确保数据安全和防篡改。树莓派负责处理传感器数据、捕获图像、触发警报，并与智能合约交互。", "result": "实验结果表明，该系统能够改善食材管理、减少浪费并提高厨房效率。", "conclusion": "该系统通过结合物联网和区块链技术，成功实现了披萨生产的实时监控、数据透明化和可追溯性，有效提升了生产管理和效率。", "translation": "本文提出了一个基于区块链物联网（IoT）的系统，用于监控餐厅的披萨生产。物联网设备实时跟踪温度和湿度，而区块链确保数据安全且防篡改。树莓派处理传感器数据、捕获图像、触发警报，并与智能合约交互。该系统检测异常情况，实现快速响应。区块链增加了透明度和可追溯性，支持合规性和审计。实验表明，该系统改善了食材管理，减少了浪费，并提高了厨房效率。", "summary": "本文介绍了一个利用物联网（IoT）和区块链技术实现的披萨生产监控系统。该系统通过IoT设备实时收集温度和湿度数据，并利用区块链确保数据的安全性和不可篡改性。树莓派负责数据处理、图像捕获、警报触发以及与智能合约的交互，能够及时发现并响应生产中的异常状况。该系统显著提升了生产过程的透明度和可追溯性，有助于遵守法规和审计。实验证明，该系统有效优化了食材管理，减少了浪费，并提升了厨房的整体效率。", "keywords": "物联网, 区块链, 披萨生产, 实时监控, 透明化", "comments": "该论文创新性地将物联网与区块链技术结合应用于食品生产监控，特别是在披萨制作领域。其重要性在于提升了食品生产过程的透明度、可追溯性和数据安全性，有助于食品安全管理和消费者信任。通过实时监控和异常预警，还能有效减少浪费并提高运营效率。"}}
{"id": "2507.02171", "title": "Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN", "authors": ["Miroslav Cibula", "Kristína Malinovská", "Matthias Kerzel"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures, 2 tables. To be published in 2025 International Conference on Artificial Neural Networks (ICANN) proceedings. This research was funded by the Horizon Europe project TERAIS, GA no. 101079338, and in part by the Slovak Grant Agency for Science (VEGA), project 1/0373/23", "url": "http://arxiv.org/abs/2507.02171v1", "summary": "Trajectory planning in robotics is understood as generating a sequence of\njoint configurations that will lead a robotic agent, or its manipulator, from\nan initial state to the desired final state, thus completing a manipulation\ntask while considering constraints like robot kinematics and the environment.\nTypically, this is achieved via sampling-based planners, which are\ncomputationally intensive. Recent advances demonstrate that trajectory planning\ncan also be performed by supervised sequence learning of trajectories, often\nrequiring only a single or fixed number of passes through a neural\narchitecture, thus ensuring a bounded computation time. Such fully supervised\napproaches, however, perform imitation learning; they do not learn based on\nwhether the trajectories can successfully reach a goal, but try to reproduce\nobserved trajectories. In our work, we build on this approach and propose a\ncognitively inspired self-supervised learning scheme based on a recurrent\narchitecture for building a trajectory model. We evaluate the feasibility of\nthe proposed method on a task of kinematic planning for a robotic arm. The\nresults suggest that the model is able to learn to generate trajectories only\nusing given paired forward and inverse kinematics models, and indicate that\nthis novel method could facilitate planning for more complex manipulation tasks\nrequiring adaptive solutions.", "comment": "12 pages, 4 figures, 2 tables. To be published in 2025 International\n  Conference on Artificial Neural Networks (ICANN) proceedings. This research\n  was funded by the Horizon Europe project TERAIS, GA no. 101079338, and in\n  part by the Slovak Grant Agency for Science (VEGA), project 1/0373/23", "pdf_url": "http://arxiv.org/pdf/2507.02171v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "迈向基于自监督RNN的生物启发式机器人轨迹规划", "tldr": "本文提出一种基于循环神经网络的自监督学习方案，用于机器人轨迹规划，解决了传统监督学习无法基于目标成功率学习的问题，并在机器人手臂运动学规划任务中验证了其可行性。", "motivation": "传统的机器人轨迹规划方法（如基于采样的规划器）计算成本高昂。虽然最近的监督序列学习方法能提供有界计算时间，但它们属于模仿学习，无法根据轨迹是否成功达到目标来学习。", "method": "本文提出了一种受认知启发的自监督学习方案，该方案基于循环神经网络（RNN）构建轨迹模型。该方法利用给定的正向和逆向运动学模型来学习生成轨迹。", "result": "该模型能够仅使用给定的正向和逆向运动学模型来学习生成轨迹。结果表明，这种新颖的方法可以促进需要自适应解决方案的更复杂操作任务的规划。", "conclusion": "所提出的基于自监督RNN的轨迹规划方法是可行的，并有望应用于更复杂的机器人操作任务，提供自适应的解决方案。", "translation": "机器人技术中的轨迹规划被理解为生成一系列关节配置，这些配置将引导机器人代理或其机械臂从初始状态到达期望的最终状态，从而在考虑机器人运动学和环境等约束的同时完成操作任务。通常，这通过基于采样的规划器实现，但其计算密集。最近的进展表明，轨迹规划也可以通过轨迹的监督序列学习来执行，这通常只需要通过神经网络架构进行一次或固定次数的传递，从而确保有界计算时间。然而，这种完全监督的方法执行的是模仿学习；它们不是基于轨迹能否成功达到目标来学习，而是尝试重现观察到的轨迹。在我们的工作中，我们在此方法的基础上，提出了一种基于循环架构的、受认知启发的自监督学习方案，用于构建轨迹模型。我们在一项机器人手臂运动学规划任务上评估了所提出方法的可行性。结果表明，该模型能够仅使用给定的成对正向和逆向运动学模型来学习生成轨迹，并表明这种新颖的方法可以促进需要自适应解决方案的更复杂操作任务的规划。", "summary": "本文针对机器人轨迹规划中传统方法计算密集和监督学习无法基于目标成功率学习的问题，提出了一种受认知启发的基于循环神经网络的自监督学习方案。该方法旨在构建一个轨迹模型，并利用成对的正向和逆向运动学模型进行学习。在机器人手臂运动学规划任务上的评估结果表明，该模型能够有效地生成轨迹，并有望为更复杂的机器人操作任务提供自适应的规划解决方案。", "keywords": "机器人轨迹规划, 自监督学习, 循环神经网络, 运动学规划, 生物启发", "comments": "这项工作通过引入自监督学习解决了现有监督学习方法在机器人轨迹规划中“模仿而非理解”的局限性。其创新点在于将认知启发与RNN相结合，使模型能够根据任务成功与否来学习，而非简单地复制已观察到的轨迹。这对于需要更灵活和自适应规划的复杂机器人任务具有重要意义。"}}
{"id": "2507.02110", "title": "Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!", "authors": ["Md Nahidul Islam Opu", "Fatima Islam Mouri", "Rick Kazman", "Yuanfang Cai", "Shaiful Chowdhury"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02110v1", "summary": "Predicting mobile app popularity before release can provide developers with a\nstrategic advantage in a competitive marketplace, yet it remains a challenging\nproblem. This study explores whether internal software metrics, measurable from\nsource code before deployment, can predict an app's popularity, defined by user\nratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).\nUsing a dataset of 446 open-source Android apps from F-Droid, we extract a wide\narray of features, including system-, class-, and method-level code metrics,\ncode smells, and app metadata. Additional information, such as user reviews,\ndownload counts, and uses-permission, was collected from the Google Play Store.\nWe evaluate regression and classification models across three feature sets: a\nminimal Size-only baseline, a domain-informed Handpicked set, and a Voting set\nderived via feature selection algorithms. Regression models perform poorly due\nto skewed data, with low $R^2$ scores. However, when reframed as binary\nclassification (Popular vs. Unpopular), results improve significantly. The best\nmodel, a Multilayer Perceptron using the Voting set, achieves F1-scores of\n0.72. These results suggest that internal code metrics, although limited in\ntheir explanatory power, can serve as useful indicators of app popularity. This\nchallenges earlier findings that dismissed internal metrics as predictors of\nsoftware quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02110v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "内部软件度量能否预测应用发布时的流行度？肯定！和否定！", "tldr": "内部代码度量可以作为应用流行度的有用指标，特别是通过二元分类模型。", "motivation": "在竞争激烈的市场中，预测移动应用发布前的流行度可以为开发者提供战略优势，但这是一个具有挑战性的问题。", "method": "本研究探讨了内部软件度量（可在部署前从源代码测量）是否能预测应用的流行度，流行度通过用户评分和DownloadsPerYear定义。使用来自F-Droid的446个开源Android应用数据集，提取了包括系统、类、方法级别的代码度量、代码异味和应用元数据等特征。还从Google Play商店收集了用户评论、下载量和uses-permission等信息。评估了三种特征集（最小的仅大小基线、领域知情的精选集和通过特征选择算法得出的投票集）上的回归和分类模型。", "result": "回归模型由于数据倾斜表现不佳，R^2分数较低。然而，当重新定义为二元分类（流行 vs. 不流行）时，结果显著改善。最佳模型是使用投票集的多层感知器，F1分数达到0.72。", "conclusion": "这些结果表明，内部代码度量虽然解释力有限，但可以作为应用流行度的有用指标。这挑战了早期认为内部度量不能预测软件质量的发现。", "translation": "在竞争激烈的市场中，预测移动应用发布前的流行度可以为开发者提供战略优势，但它仍然是一个具有挑战性的问题。本研究探讨了内部软件度量（可在部署前从源代码测量）是否能预测应用的流行度，流行度由用户评分（根据用户评论计算）和DownloadsPerYear（每年下载量）定义。我们使用来自F-Droid的446个开源Android应用数据集，提取了各种特征，包括系统、类和方法级别的代码度量、代码异味和应用元数据。其他信息，如用户评论、下载计数和uses-permission，则从Google Play商店收集。我们评估了三种特征集上的回归和分类模型：一个最小的仅大小基线、一个领域知情的精选集，以及一个通过特征选择算法得出的投票集。由于数据倾斜，回归模型表现不佳，R^2分数较低。然而，当重新定义为二元分类（流行 vs. 不流行）时，结果显著改善。最佳模型是使用投票集的多层感知器，F1分数达到0.72。这些结果表明，内部代码度量虽然解释力有限，但可以作为应用流行度的有用指标。这挑战了早期认为内部度量不能预测软件质量的发现。", "summary": "本研究探讨了在应用发布前，能否利用可从源代码获取的内部软件度量来预测移动应用的流行度。研究使用了446个开源Android应用的数据集，提取了多种代码度量和元数据特征。结果显示，直接的回归模型表现不佳，但当问题转化为二元分类（流行或不流行）时，预测性能显著提高。其中，使用多层感知器和特征选择算法生成的特征集取得了最佳的F1分数0.72。研究得出结论，尽管解释力有限，内部代码度量仍可作为预测应用流行度的有用指标，并挑战了先前关于内部度量预测软件质量的观点。", "keywords": "内部软件度量, 应用流行度, 预测, 机器学习, Android应用", "comments": "这项研究的创新之处在于其明确地探讨了内部软件度量预测应用流行度的可能性，并指出将问题转化为二元分类可以显著提高预测效果。它挑战了先前关于内部度量在预测软件质量方面作用的观点，为开发者提供了一种潜在的、在应用发布前评估其市场潜力的手段。局限性可能在于数据集的规模和来源（仅限F-Droid的开源Android应用），以及度量解释力的“有限性”。"}}
{"id": "2507.01994", "title": "Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks", "authors": ["Sardar Jaffar Ali", "Syed M. Raza", "Duc-Tai Le", "Rajesh Challa", "Min Young Chung", "Ness Shroff", "Hyunseung Choo"], "categories": ["cs.NI", "cs.MA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01994v1", "summary": "Despite advancements, Radio Access Networks (RAN) still account for over 50\\%\nof the total power consumption in 5G networks. Existing RAN split options do\nnot fully harness data potential, presenting an opportunity to reduce\noperational expenditures. This paper addresses this opportunity through a\ntwofold approach. First, highly accurate network traffic and user predictions\nare achieved using the proposed Curated Collaborative Learning (CCL) framework,\nwhich selectively collaborates with relevant correlated data for traffic\nforecasting. CCL optimally determines whom, when, and what to collaborate with,\nsignificantly outperforming state-of-the-art approaches, including global,\nfederated, personalized federated, and cyclic institutional incremental\nlearnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the\nDistributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep\nreinforcement learning and prediction inferences from CCL to reduce the number\nof active DU servers efficiently. DUPS dynamically redirects traffic from\nunderutilized DU servers to optimize resource use, improving energy efficiency\nby up to 89% over conventional strategies, translating into substantial\nmonetary benefits for operators. By integrating CCL-driven predictions with\nDUPS, this paper demonstrates a transformative approach for minimizing energy\nconsumption and operational costs in 5G RANs, significantly enhancing\nefficiency and cost-effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01994v1", "cate": "cs.NI", "date": "2025-06-30", "updated": "2025-06-30", "AI": {"title_translation": "用于B5G/6G无线接入网络的精选协作式AI边缘与网络数据分析", "tldr": "本文提出了一种结合精选协作学习（CCL）和分布式单元池化方案（DUPS）的方法，以显著降低B5G/6G无线接入网络的能耗和运营成本。", "motivation": "尽管取得了进步，无线接入网络（RAN）在5G网络中仍占总功耗的50%以上。现有的RAN拆分选项未能充分利用数据潜力，存在降低运营支出的机会。", "method": "首先，提出了精选协作学习（CCL）框架，通过选择性地与相关联数据协作进行流量预测，实现了高精度的网络流量和用户预测。其次，提出了分布式单元池化方案（DUPS），利用深度强化学习和CCL的预测推断，有效减少活跃DU服务器的数量，并通过动态重定向流量来优化资源利用。", "result": "CCL在流量预测方面显著优于现有最先进方法，分别提高了43.9%、39.1%、40.8%和31.35%。DUPS在能效方面比传统策略提高了高达89%，为运营商带来了显著的经济效益。", "conclusion": "通过整合CCL驱动的预测与DUPS，本文展示了一种变革性的方法，用于最大限度地降低5G RAN的能耗和运营成本，显著提高了效率和成本效益。", "translation": "尽管取得了进步，无线接入网络（RAN）在5G网络中仍占总功耗的50%以上。现有的RAN拆分选项未能充分利用数据潜力，这为降低运营支出提供了机会。本文通过双重方法解决这一机会。首先，使用所提出的精选协作学习（CCL）框架实现了高精度的网络流量和用户预测，该框架选择性地与相关联数据协作进行流量预测。CCL最佳地确定与谁、何时以及协作什么，显著优于现有最先进的方法，包括全局学习、联邦学习、个性化联邦学习和循环机构增量学习，分别提高了43.9%、39.1%、40.8%和31.35%。其次，提出了分布式单元池化方案（DUPS），该方案利用深度强化学习和CCL的预测推断，有效减少活跃DU服务器的数量。DUPS动态重定向未充分利用的DU服务器的流量，以优化资源使用，与传统策略相比，能效提高了高达89%，为运营商带来了巨大的经济效益。通过将CCL驱动的预测与DUPS相结合，本文展示了一种变革性的方法，用于最大限度地降低5G RAN的能耗和运营成本，显著提高了效率和成本效益。", "summary": "本文针对B5G/6G无线接入网络高能耗和运营成本的问题，提出了一种双重方法。首先，通过精选协作学习（CCL）框架实现高精度网络流量和用户预测，该框架选择性地与相关数据协作，并在预测性能上显著超越现有方法。其次，引入分布式单元池化方案（DUPS），利用深度强化学习和CCL的预测结果，有效减少活跃的分布式单元（DU）服务器数量，并通过流量重定向优化资源利用，显著提升能效。该集成方法被证明能有效降低5G RAN的能耗和运营成本。", "keywords": "无线接入网络, 能效, 协作学习, 深度强化学习, 流量预测", "comments": "本文的创新点在于结合了精选协作学习（CCL）用于高精度预测和基于深度强化学习的分布式单元池化方案（DUPS）用于能效优化。CCL通过选择性协作解决了数据利用效率问题，而DUPS则直接针对RAN的能耗痛点。这种组合方法为B5G/6G网络的高能效和低成本运营提供了有前景的解决方案，具有重要的实际应用价值。"}}
{"id": "2507.01984", "title": "Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features", "authors": ["Gautam Kishore Shahi"], "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01984v1", "summary": "Amid a tidal wave of misinformation flooding social media during elections\nand crises, extensive research has been conducted on misinformation detection,\nprimarily focusing on text-based or image-based approaches. However, only a few\nstudies have explored multimodal feature combinations, such as integrating text\nand images for building a classification model to detect misinformation. This\nstudy investigates the effectiveness of different multimodal feature\ncombinations, incorporating text, images, and social features using an early\nfusion approach for the classification model. This study analyzed 1,529 tweets\ncontaining both text and images during the COVID-19 pandemic and election\nperiods collected from Twitter (now X). A data enrichment process was applied\nto extract additional social features, as well as visual features, through\ntechniques such as object detection and optical character recognition (OCR).\nThe results show that combining unsupervised and supervised machine learning\nmodels improves classification performance by 15% compared to unimodal models\nand by 5% compared to bimodal models. Additionally, the study analyzes the\npropagation patterns of misinformation based on the characteristics of\nmisinformation tweets and the users who disseminate them.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01984v1", "cate": "cs.LG", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多模态虚假信息检测：语言、视觉和社交特征的早期融合", "tldr": "本研究通过对语言、视觉和社交特征进行早期融合，提高了社交媒体上虚假信息的检测性能，相比单模态和双模态模型有显著提升。", "motivation": "社交媒体上虚假信息泛滥，现有检测研究主要集中在文本或图像的单模态方法，而多模态特征组合的研究较少，因此需要探索结合多模态特征来更有效地检测虚假信息。", "method": "本研究调查了结合文本、图像和社交特征的多模态早期融合方法在虚假信息分类模型中的有效性。分析了从Twitter（现X）收集的1,529条包含文本和图像的推文，涵盖COVID-19大流行和选举期间。通过对象检测和光学字符识别（OCR）等技术进行数据丰富，提取了额外的社交和视觉特征。模型结合了无监督和有监督机器学习方法。", "result": "结合无监督和有监督机器学习模型使分类性能比单模态模型提高了15%，比双模态模型提高了5%。此外，研究还分析了基于虚假信息推文特征和传播用户的传播模式。", "conclusion": "通过早期融合语言、视觉和社交等多模态特征，并结合无监督和有监督机器学习模型，可以显著提升社交媒体上虚假信息的检测性能。", "translation": "在选举和危机期间，社交媒体充斥着大量虚假信息，针对虚假信息检测进行了广泛研究，主要集中在基于文本或图像的方法。然而，只有少数研究探索了多模态特征组合，例如整合文本和图像来构建分类模型以检测虚假信息。本研究调查了不同多模态特征组合的有效性，采用早期融合方法将文本、图像和社交特征整合到分类模型中。本研究分析了从Twitter（现X）收集的1,529条包含文本和图像的推文，这些推文来自COVID-19大流行和选举期间。通过对象检测和光学字符识别（OCR）等技术应用数据丰富过程，以提取额外的社交特征和视觉特征。结果表明，与单模态模型相比，结合无监督和有监督机器学习模型将分类性能提高了15%，与双模态模型相比提高了5%。此外，本研究还根据虚假信息推文的特征和传播用户的特征分析了虚假信息的传播模式。", "summary": "本研究旨在通过早期融合语言、视觉和社交等多模态特征，提升社交媒体上虚假信息的检测能力。研究分析了1,529条来自Twitter的推文数据，并利用对象检测和OCR等技术提取了丰富的特征。结果显示，结合无监督和有监督机器学习模型的多模态方法，在虚假信息分类性能上比单模态和双模态模型分别提高了15%和5%。此外，研究还探讨了虚假信息的传播模式。", "keywords": "多模态虚假信息检测, 早期融合, 社交媒体, 机器学习, 特征工程", "comments": "这篇论文通过引入社交特征并采用早期融合的多模态方法，在虚假信息检测领域取得了显著进展。其创新点在于结合了无监督和有监督学习模型，并证明了多模态特征融合的优越性，尤其是在实际社交媒体数据上的应用。"}}
{"id": "2507.02303", "title": "Measurements and Modeling of Air-Ground Integrated Channel in Forest Environment Based on OFDM Signals", "authors": ["Zhe Xiao", "Shu Sun", "Na Liu", "Lianming Xu", "Li Wang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02303v1", "summary": "Forests are frequently impacted by climate conditions, vegetation density,\nand intricate terrain and geology, which contribute to natural disasters.\nPersonnel engaged in or supporting rescue operations in such environments rely\non robust communication systems to ensure their safety, highlighting the\ncriticality of channel measurements in forest environments. However, according\nto current research, there is limited research on channel detection and\nmodeling in forest areas in the existing literature. This paper describes the\nchannel measurements campaign of air and ground in the Arxan National Forest\nPark of Inner Mongolia. It presents measurement results and propagation models\nfor ground-to-ground (G2G) and air-to-ground (A2G) scenarios. The measurement\ncampaign uses orthogonal frequency division multiplexing signals centered at\n1.4 GHz for channel sounding. In the G2G measurement, in addition to using\nomnidirectional antennas to record data, we also use directional antennas to\nrecord the arrival angle information of the signal at the receiver. In the A2G\nmeasurement, we pre-plan the flight trajectory of the unmanned aerial vehicle\nso that it can fly at a fixed angle relative to the ground. We present path\nloss models suitable for G2G and A2G in forest environments based on the\nanalysis of measurement results. The results indicate that the proposed model\nreduces error margins compared with other path loss models. Furthermore, we\nderive the multipath model expression specific to forest environments and\nconduct statistical analysis on key channel parameters e.g., shadow fading\nfactor, root mean square delay spread, and Rician K factor. Our findings reveal\nthat signal propagation obstruction due to tree crowns in A2G communication is\nmore pronounced than tree trunk obstructions in G2G communication. Adjusting\nthe elevation angle between air and ground can enhance communication quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02303v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "森林环境下基于OFDM信号的空地一体化信道测量与建模", "tldr": "本文针对森林环境通信需求，测量并建模了空地和地地信道，提出了新的路径损耗和多径模型，并指出树冠对空地通信影响显著，优化仰角可提升通信质量。", "motivation": "在森林环境中，救援人员需要可靠的通信系统以确保安全，但现有研究中关于森林区域信道检测和建模的文献有限。", "method": "在内蒙古阿尔山国家森林公园进行了空地和地地信道测量，使用1.4 GHz的OFDM信号进行探测。地地测量结合全向和定向天线，空地测量规划无人机固定仰角飞行。基于测量数据，提出了适用于森林环境的路径损耗模型和多径模型，并对阴影衰落因子、均方根时延扩展、莱斯K因子等关键信道参数进行了统计分析。", "result": "提出的路径损耗模型相比其他模型误差更小。推导了森林环境特有的多径模型表达式。发现空地通信中树冠对信号传播的阻碍作用比地地通信中树干的阻碍作用更明显。调整空地之间的仰角可以提高通信质量。", "conclusion": "在森林环境中，空地和地地通信的信道特性存在显著差异，特别是树冠对空地通信的影响更为突出。通过优化仰角和使用本文提出的模型，可以改善通信质量。", "translation": "森林环境经常受到气候条件、植被密度以及复杂地形地质的影响，这些因素导致自然灾害的发生。在这种环境中从事或支持救援行动的人员依赖于强大的通信系统来确保他们的安全，这凸显了森林环境中信道测量的关键性。然而，根据目前的研究，现有文献中关于森林区域信道检测和建模的研究有限。本文描述了在内蒙古阿尔山国家森林公园进行的空地一体化信道测量活动。它提出了地对地（G2G）和空对地（A2G）场景的测量结果和传播模型。测量活动使用中心频率为1.4 GHz的正交频分复用（OFDM）信号进行信道探测。在地对地测量中，除了使用全向天线记录数据外，我们还使用定向天线记录接收端信号的到达角信息。在空对地测量中，我们预先规划了无人机的飞行轨迹，使其能够以相对于地面固定的角度飞行。通过对测量结果的分析，我们提出了适用于森林环境中地对地和空对地的路径损耗模型。结果表明，与现有其他路径损耗模型相比，本文提出的模型降低了误差范围。此外，我们推导了森林环境特有的多径模型表达式，并对关键信道参数，例如阴影衰落因子、均方根时延扩展和莱斯K因子进行了统计分析。我们的研究结果表明，空地通信中树冠对信号传播的阻碍比地对地通信中树干的阻碍更为明显。调整空地之间的仰角可以提高通信质量。", "summary": "本文针对森林环境中通信系统可靠性需求，弥补现有研究中森林信道检测和建模的不足。通过在内蒙古阿尔山国家森林公园进行基于1.4 GHz OFDM信号的地地和空地信道测量，提出了适用于森林环境的路径损耗模型和多径模型。研究发现树冠对空地通信的阻碍作用大于树干对地地通信的阻碍作用，并指出调整空地仰角可提升通信质量。", "keywords": "森林环境, 空地一体化信道, OFDM信号, 路径损耗模型, 多径模型", "comments": "这项研究通过实地测量和建模，填补了森林环境下空地一体化通信信道研究的空白，对于提升森林灾害救援等场景下的通信保障具有重要意义。其提出的新模型和对树冠阻碍效应的发现具有创新性。"}}
{"id": "2507.02456", "title": "System-performance and cost modeling of Large Language Model training and inference", "authors": ["Wenzhe Guo", "Joyjit Kundu", "Uras Tos", "Weijiang Kong", "Giuliano Sisto", "Timon Evenblij", "Manu Perumkunnil"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02456v1", "summary": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02456v1", "cate": "cs.AR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "大型语言模型训练和推理的系统性能与成本建模", "tldr": "大型语言模型（LLM）的训练和推理面临性能和成本瓶颈。本文提出一种整合了先进计算、内存优化和通信技术的性能-成本建模方法，以指导未来系统设计和软硬件协同开发。", "motivation": "大型语言模型（LLM）的规模和复杂性呈指数级增长，但计算能力、内存带宽、网络性能和成本效率的进步未能跟上，这对其在分布式系统上的可扩展性构成了重大挑战，亟需解决性能和成本限制。", "method": "本文提出了一种用于LLM训练和推理的性能-成本建模方法。该方法整合了最先进的计算技术、内存优化和最新的通信技术。它基于分析性能模型，并纳入了如Flash Attention和专家混合模型（Mixture of Experts）等创新技术来解决内存带宽和计算瓶颈。该方法还考虑了不同网络拓扑、具有5D并行性的拓扑特定通信算法，并整合了小芯片（chiplet）成本模型。", "result": "该建模方法能够分析各种系统架构配置的性能-成本权衡。", "conclusion": "所提出的建模方法为未来的计算系统设计提供了有价值的指导，并促进了硬件-软件协同开发，特别是由于其能够分析各种系统架构配置的性能-成本权衡。", "translation": "大型语言模型（LLMs）基于Transformer架构，因其卓越的可扩展性和适应性，彻底改变了人工智能、科学和工程领域的众多领域。然而，LLM规模和复杂性的指数级增长已超越了计算能力、内存带宽、网络性能和成本效率的进步，对其在分布式系统上的可扩展性构成了重大挑战。为了解决这些限制，文献中提出了替代模型架构、优化策略、通信感知网络拓扑以及新颖的系统设计方法。本文介绍了一种用于LLM训练和推理的性能-成本建模方法，该方法整合了最先进的计算技术、内存优化和最新的通信技术。我们的方法建立在分析性能模型之上，并结合了最近的创新，例如Flash Attention技术和专家混合模型，以解决内存带宽和计算瓶颈。它还考虑了不同网络拓扑和具有5D并行性的拓扑特定通信算法的影响。该框架还整合了小芯片成本模型。所提出的建模方法提供了宝贵的见解，以指导未来的计算系统设计，并促进硬件-软件协同开发，特别是由于其能够分析各种系统架构配置的性能-成本权衡。", "summary": "本文提出了一种新颖的性能-成本建模方法，用于分析大型语言模型（LLMs）的训练和推理过程。该方法整合了先进的计算技术、内存优化和通信技术，并考虑了如Flash Attention、专家混合模型、不同网络拓扑及5D并行等最新创新。此外，它还包含小芯片成本模型。该建模框架旨在解决LLM在分布式系统扩展中面临的性能和成本挑战，为未来的计算系统设计和软硬件协同开发提供指导，并能分析不同系统配置的性能-成本权衡。", "keywords": "大型语言模型, 性能建模, 成本建模, 训练推理, 分布式系统", "comments": "这篇论文通过提出一个全面的性能-成本建模框架，解决了当前LLM发展中面临的关键挑战，即计算能力和成本效率的瓶颈。其创新之处在于整合了多种前沿技术（如Flash Attention, MoE, 5D并行）和成本模型，提供了分析性能-成本权衡的能力，这对于指导未来LLM硬件-软件协同设计具有重要意义。"}}
{"id": "2507.02233", "title": "Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems", "authors": ["Bruce Fang", "Danyi Gao"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02233v1", "summary": "This paper addresses the challenge of fault root cause identification in\ncloud computing environments. The difficulty arises from complex system\nstructures, dense service coupling, and limited fault information. To solve\nthis problem, an intelligent identification algorithm based on transfer\nlearning is proposed. The method introduces a shared feature extraction module\nand a domain adversarial mechanism to enable effective knowledge transfer from\nthe source domain to the target domain. This improves the model's\ndiscriminative ability and generalization performance in the target domain. The\nmodel incorporates a pseudo-label selection strategy. When labeled samples are\nlacking in the target domain, high-confidence predictions are used in training.\nThis enhances the model's ability to recognize minority classes. To evaluate\nthe stability and adaptability of the method in real-world scenarios,\nexperiments are designed under three conditions: label scarcity, class\nimbalance, and heterogeneous node environments. Experimental results show that\nthe proposed method outperforms existing mainstream approaches in several key\nmetrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger\ndiscriminative power and robustness. Notably, under extreme class imbalance and\nsignificant structural differences in the target domain, the model still\nmaintains high performance. This validates the effectiveness and practical\nvalue of the proposed mechanisms in complex cloud computing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02233v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于域对抗迁移学习的云计算系统故障根因识别", "tldr": "本文提出一种基于域对抗迁移学习的智能算法，用于解决云计算系统中的故障根因识别难题，尤其在标签稀缺、类别不平衡和异构环境下的表现优于现有方法。", "motivation": "云计算环境中故障根因识别面临复杂系统结构、服务耦合紧密和故障信息有限等挑战。", "method": "提出一种基于迁移学习的智能识别算法，包含共享特征提取模块、域对抗机制和伪标签选择策略。共享特征提取和域对抗机制促进知识从源域到目标域的有效迁移，提升模型判别能力和泛化性能。伪标签选择策略利用高置信度预测训练，增强模型对少数类别的识别能力。", "result": "在标签稀缺、类别不平衡和异构节点环境下进行实验，结果表明所提方法在准确率、F1-Score和AUC等关键指标上优于现有主流方法。模型表现出更强的判别能力和鲁棒性，在极端类别不平衡和目标域结构差异显著的情况下仍保持高性能。", "conclusion": "所提出的机制在复杂云计算系统中具有有效性和实用价值。", "translation": "本文旨在解决云计算环境中的故障根因识别挑战。由于系统结构复杂、服务耦合紧密以及故障信息有限，使得识别变得困难。为解决此问题，本文提出一种基于迁移学习的智能识别算法。该方法引入共享特征提取模块和域对抗机制，以实现知识从源域到目标域的有效迁移。这提升了模型在目标域的判别能力和泛化性能。模型还结合了伪标签选择策略。当目标域中缺乏标注样本时，高置信度预测被用于训练。这增强了模型识别少数类别的能力。为了评估该方法在实际场景中的稳定性和适应性，实验在三种条件下进行设计：标签稀缺、类别不平衡和异构节点环境。实验结果表明，所提出的方法在准确率、F1-Score和AUC等多个关键指标上优于现有主流方法。该模型展现出更强的判别能力和鲁棒性。值得注意的是，在极端类别不平衡和目标域结构差异显著的情况下，该模型仍保持高性能。这验证了所提出机制在复杂云计算系统中的有效性和实用价值。", "summary": "本文提出一种基于域对抗迁移学习的智能算法，用于解决云计算系统中的故障根因识别难题。该方法通过共享特征提取模块和域对抗机制实现跨域知识迁移，并引入伪标签选择策略以应对标签稀缺和类别不平衡问题。实验结果显示，该方法在准确率、F1-Score和AUC等指标上显著优于现有主流方法，尤其在面临极端类别不平衡和异构环境时仍能保持高性能，验证了其在复杂云计算系统中的有效性和实用价值。", "keywords": "域对抗迁移学习, 故障根因识别, 云计算, 伪标签, 类别不平衡", "comments": "该论文创新性地将域对抗迁移学习应用于云计算系统故障根因识别，通过结合共享特征提取、域对抗机制和伪标签策略，有效解决了实际场景中标签稀缺、类别不平衡和异构环境等挑战。其在复杂系统中的鲁棒性和高性能表现，凸显了该方法的实用价值和对提升云系统运维效率的重要性。"}}
{"id": "2507.02648", "title": "Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI Supply Chains", "authors": ["Aspen K. Hopkins", "Isabella Struckman", "Kevin Klyman", "Susan S. Silbey"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02648v1", "summary": "The AI industry is exploding in popularity, with increasing attention to\npotential harms and unwanted consequences. In the current digital ecosystem, AI\ndeployments are often the product of AI supply chains (AISC): networks of\noutsourced models, data, and tooling through which multiple entities contribute\nto AI development and distribution. AI supply chains lack the modularity,\nredundancies, or conventional supply chain practices that enable\nidentification, isolation, and easy correction of failures, exacerbating the\nalready difficult processes of responding to ML-generated harms. As the\nstakeholders participating in and impacted by AISCs have scaled and\ndiversified, so too have the risks they face. In this stakeholder analysis of\nAI supply chains, we consider who participates in AISCs, what harms they face,\nwhere sources of harm lie, and how market dynamics and power differentials\ninform the type and probability of remedies. Because AI supply chains are\npurposely invented and implemented, they may be designed to account for, rather\nthan ignore, the complexities, consequences, and risks of deploying AI systems.\nTo enable responsible design and management of AISCs, we offer a typology of\nresponses to AISC-induced harms: recourse, repair, reparation or prevention. We\napply this typology to stakeholders participating in a health-care AISC across\nthree stylized markets $\\unicode{x2013}$ vertical integration, horizontal\nintegration, free market $\\unicode{x2013}$ to illustrate how stakeholder\npositioning and power within an AISC may shape responses to an experienced\nharm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02648v1", "cate": "cs.CY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "追索、修复、赔偿与预防：AI供应链的利益相关者分析", "tldr": "AI供应链复杂且缺乏应对危害的机制。本文对AI供应链中的利益相关者进行分析，并提出四种应对AI引发危害的响应类型：追索、修复、赔偿和预防，以实现负责任的设计和管理。", "motivation": "AI部署通常通过复杂的AI供应链（AISC）实现，但这些供应链缺乏识别、隔离和纠正故障的机制，使得应对机器学习产生的危害变得更加困难。随着AISC利益相关者的规模和多样性增加，他们面临的风险也随之增加。", "method": "本文对AI供应链进行了利益相关者分析，探讨了参与者、面临的危害、危害来源，以及市场动态和权力差异如何影响补救措施的类型和可能性。作者提出了一种应对AISC诱发危害的响应类型学：追索、修复、赔偿或预防。并将此类型学应用于医疗保健AISC中，分析了三种市场模式（垂直整合、水平整合、自由市场）下利益相关者的定位和权力如何影响对危害的响应。", "result": "提出了一种应对AISC诱发危害的响应类型学（追索、修复、赔偿或预防），并展示了在不同市场模式下，利益相关者的定位和权力如何塑造对危害的响应。", "conclusion": "AI供应链可以被设计成考虑而不是忽视部署AI系统的复杂性、后果和风险，以实现负责任的设计和管理。", "translation": "AI产业的受欢迎程度正在爆炸式增长，人们对潜在危害和不良后果的关注也日益增加。在当前的数字生态系统中，AI部署通常是AI供应链（AISC）的产物：通过外包模型、数据和工具网络，多个实体共同促进AI的开发和分发。AI供应链缺乏模块化、冗余或传统的供应链实践，这些实践能够识别、隔离和轻松纠正故障，从而加剧了应对机器学习生成危害的现有困难过程。随着参与AISC并受其影响的利益相关者规模和多样性不断扩大，他们面临的风险也随之增加。在这项对AI供应链的利益相关者分析中，我们考虑了谁参与了AISC，他们面临哪些危害，危害的来源在哪里，以及市场动态和权力差异如何影响补救措施的类型和可能性。因为AI供应链是特意发明和实施的，它们可以被设计成考虑而不是忽视部署AI系统的复杂性、后果和风险。为了实现AISC的负责任设计和管理，我们提供了一种应对AISC诱发危害的响应类型学：追索、修复、赔偿或预防。我们将这种类型学应用于医疗保健AISC中参与的利益相关者，通过三种风格化的市场——垂直整合、水平整合、自由市场——来说明利益相关者在AISC中的定位和权力如何影响对所经历危害的响应。", "summary": "本文关注AI供应链（AISC）中日益增长的危害和风险。由于AISC缺乏传统供应链的故障应对机制，作者对AISC进行了利益相关者分析，探讨了参与者、危害来源及补救措施。为实现AISC的负责任设计和管理，文章提出了一种应对AISC诱发危害的响应类型学（追索、修复、赔偿、预防），并通过医疗保健AISC在不同市场模式下的应用，阐明了利益相关者的定位和权力如何影响对危害的响应。", "keywords": "AI供应链, 利益相关者分析, 危害响应, 追索, 预防", "comments": "这篇论文创新性地将传统供应链的理念引入到AI领域，并提出了AI供应链（AISC）的概念。它识别了AISC在处理危害方面的固有缺陷，并提供了一个实用的框架（追索、修复、赔偿、预防）来指导AISC的负责任设计和管理。通过对不同市场模式下利益相关者权力动态的分析，论文揭示了在复杂AI生态系统中实现公正和有效危害响应的挑战和机遇，对于AI治理和伦理研究具有重要意义。"}}
{"id": "2507.02200", "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      A Strong Baseline for Reasoning based Event Stream Scene Text Recognition", "url": "http://arxiv.org/abs/2507.02200v1", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "pdf_url": "http://arxiv.org/pdf/2507.02200v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "ESTR-CoT：面向可解释和准确的事件流场景文本识别的思维链推理方法", "tldr": "ESTR-CoT 提出了一种基于思维链推理的事件流场景文本识别框架，通过结合视觉编码器和大型语言模型，提高了在挑战性场景下的识别准确性和可解释性，并构建了大型CoT数据集。", "motivation": "现有事件流场景文本识别方法在解释性和上下文逻辑推理方面存在局限性，尤其是在应对低光照和快速运动等极端挑战性场景时。", "method": "提出ESTR-CoT框架。它首先使用EVA-CLIP (ViT-G/14) 视觉编码器将输入事件流转换为tokens，并利用Llama tokenizer编码生成提示。一个Q-former用于将视觉tokens与预训练的Vicuna-7B大型语言模型对齐，同时输出识别结果和思维链（CoT）推理过程。该框架可通过端到端监督微调进行优化。此外，还提出了一个通过三阶段处理（生成、润色、专家验证）构建的大规模CoT数据集。", "result": "在EventSTR、WordArt*和IC15*三个事件流场景文本识别基准数据集上的广泛实验充分验证了所提出框架的有效性和可解释性。", "conclusion": "ESTR-CoT框架成功地将思维链推理引入事件流场景文本识别，有效提升了在挑战性场景下的识别准确性与可解释性，并为后续基于推理的大型模型发展提供了数据基础。", "translation": "近年来，基于事件流的场景文本识别是一个新兴的研究课题，在极端挑战性的场景，特别是低光照和快速运动下，其性能优于广泛使用的RGB相机。现有工作要么采用端到端编码器-解码器框架，要么采用大型语言模型来增强识别，然而，它们仍然受到解释性不足和上下文逻辑推理能力弱的限制。在这项工作中，我们提出了一种新颖的基于思维链推理的事件流场景文本识别框架，命名为ESTR-CoT。具体来说，我们首先采用视觉编码器EVA-CLIP (ViT-G/14) 将输入事件流转换为tokens，并利用Llama tokenizer编码给定的生成提示。一个Q-former用于将视觉token与预训练的大型语言模型Vicuna-7B对齐，并同时输出答案和思维链（CoT）推理过程。我们的框架可以通过端到端监督微调进行优化。此外，我们还提出了一个大规模CoT数据集，通过三阶段处理（即生成、润色和专家验证）来训练我们的框架。该数据集为后续基于推理的大型模型的发展提供了坚实的数据基础。在三个事件流STR基准数据集（即EventSTR、WordArt*、IC15*）上进行的广泛实验充分验证了我们所提出框架的有效性和可解释性。源代码和预训练模型将发布在https://github.com/Event-AHU/ESTR-CoT。", "summary": "ESTR-CoT是一种新颖的基于思维链推理的事件流场景文本识别框架，旨在解决现有方法在解释性和逻辑推理方面的不足。该框架结合了EVA-CLIP视觉编码器、Llama tokenizer和Vicuna-7B大型语言模型，能够同时输出识别结果和推理过程，并通过端到端微调进行优化。为支持训练，本文还构建了一个大规模CoT数据集。实验证明ESTR-CoT在挑战性场景下具有优异的有效性和可解释性。", "keywords": "事件流, 场景文本识别, 思维链, 可解释性, 大型语言模型", "comments": "这项工作通过引入思维链推理，显著提升了事件流场景文本识别的可解释性和上下文逻辑推理能力，解决了现有方法的关键局限。其创新之处在于结合了视觉编码器和大型语言模型（Vicuna-7B）以实现端到端的推理过程输出，并构建了专用的大规模CoT数据集，为该领域未来基于推理的大模型研究奠定了基础，具有重要的研究价值。"}}
{"id": "2507.02855", "title": "Subtyping in DHOL -- Extended preprint", "authors": ["Colin Rothgang", "Florian Rabe"], "categories": ["cs.LO", "cs.AI", "cs.FL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      16 pages main document, 44 pages of appendices, to be published in FroCoS 2025", "url": "http://arxiv.org/abs/2507.02855v1", "summary": "The recently introduced dependent typed higher-order logic (DHOL) offers an\ninteresting compromise between expressiveness and automation support. It\nsacrifices the decidability of its type system in order to significantly extend\nits expressiveness over standard HOL. Yet it retains strong automated theorem\nproving support via a sound and complete translation to HOL.\n  We leverage this design to extend DHOL with refinement and quotient types.\nBoth of these are commonly requested by practitioners but rarely provided by\nautomated theorem provers. This is because they inherently require undecidable\ntyping and thus are very difficult to retrofit to decidable type systems. But\nwith DHOL already doing the heavy lifting, adding them is not only possible but\nelegant and simple.\n  Concretely, we add refinement and quotient types as special cases of\nsubtyping. This turns the associated canonical inclusion resp. projection maps\ninto identity maps and thus avoids costly changes in representation. We present\nthe syntax, semantics, and translation to HOL for the extended language,\nincluding the proofs of soundness and completeness.", "comment": "16 pages main document, 44 pages of appendices, to be published in\n  FroCoS 2025", "pdf_url": "http://arxiv.org/pdf/2507.02855v1", "cate": "cs.LO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DHOL 中的子类型——扩展预印本", "tldr": "本文将细化类型和商类型作为子类型的特例引入到依赖类型高阶逻辑（DHOL）中，利用DHOL固有的不可判定类型系统，实现了对自动化定理证明器常用但难以实现的功能的支持，并提供了语法、语义及到HOL的翻译。", "motivation": "自动化定理证明器中的细化类型和商类型是实践者普遍需求但很少提供的功能，因为它们需要不可判定的类型化。鉴于依赖类型高阶逻辑（DHOL）本身就具有不可判定的类型系统，因此为DHOL添加这些功能变得可行且优雅。", "method": "作者将细化类型和商类型作为子类型的特例添加到DHOL中。这种方法将相关的规范包含映射或投影映射转换为恒等映射，从而避免了昂贵的表示更改。他们提出了扩展语言的语法、语义以及到HOL的翻译，并包括了完备性和可靠性的证明。", "result": "成功将细化类型和商类型作为子类型特例集成到DHOL中，避免了表示上的昂贵变化。提供了扩展语言的语法、语义以及到HOL的翻译，并附带了可靠性和完备性证明。", "conclusion": "通过利用DHOL已有的不可判定类型系统，可以优雅而简单地将细化类型和商类型添加到DHOL中，从而扩展其表达能力，同时保持强大的自动化定理证明支持。", "translation": "最近引入的依赖类型高阶逻辑（DHOL）在表达能力和自动化支持之间提供了一个有趣的折衷。它牺牲了其类型系统的可判定性，以显著扩展其相对于标准高阶逻辑（HOL）的表达能力。然而，它通过对HOL的健全且完备的翻译，保留了强大的自动化定理证明支持。\n我们利用这种设计，用细化类型和商类型扩展DHOL。这两种类型是实践者普遍要求但自动化定理证明器很少提供的。这是因为它们本质上需要不可判定的类型化，因此很难适应可判定的类型系统。但由于DHOL已经完成了繁重的工作，添加它们不仅可能，而且优雅而简单。\n具体来说，我们将细化类型和商类型作为子类型的特例添加。这会将相关的规范包含映射或投影映射转换为恒等映射，从而避免了昂贵的表示更改。我们提出了扩展语言的语法、语义以及到HOL的翻译，包括健全性和完备性的证明。", "summary": "本文提出在依赖类型高阶逻辑（DHOL）中引入细化类型和商类型，将其作为子类型的特殊情况。由于DHOL本身具有不可判定的类型系统，这使得添加通常难以在自动化定理证明器中实现的功能变得可行且优雅。通过将相关映射转换为恒等映射，避免了昂贵的表示更改。文章详细阐述了扩展语言的语法、语义及其到标准高阶逻辑（HOL）的翻译，并提供了健全性和完备性的证明。", "keywords": "DHOL, 子类型, 细化类型, 商类型, 自动化定理证明", "comments": "这篇论文的创新点在于巧妙地利用了DHOL固有的不可判定类型系统的特性，将通常难以集成到自动化定理证明器中的细化类型和商类型，以一种优雅且简单的方式引入。通过将这些类型作为子类型的特例处理，并使相关映射变为恒等映射，解决了表示更改的复杂性问题，这对于提升DHOL的实用性和表达能力具有重要意义。"}}
{"id": "2507.02678", "title": "Imitation and Heterogeneity Shape the Resilience of Community Currency Networks", "authors": ["Camilla Ancona", "Dora Ricci", "Carmela Bernardo", "Francesco Lo Iudice", "Anton Proskurnikov", "Francesco Vasca"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02678v1", "summary": "Community currency networks are made up of individuals and or companies that\nshare some physical or social characteristics and engage in economic\ntransactions using a virtual currency. This paper investigates the structural\nand dynamic properties of such mutual credit systems through a case study of\nSardex, a community currency initiated and mainly operating in Sardinia, Italy.\nThe transaction network is modeled as a directed weighted graph and analyzed\nthrough a graph theoretic framework focused on the analysis of strongly\nconnected components, condensed representations, and behavioral connectivity\npatterns. Emphasis is placed on understanding the evolution of the network's\ncore and peripheral structures over a three year period, with attention to\ntemporal contraction, flow asymmetries, and structural fragmentation depending\non different user types. Our findings reveal persistent deviations from degree\nbased null models and suggest the presence of behavioral imitation,\nspecifically, a user preference for more active peers. We further assess the\nimpact of heterogeneous connections between different type of users, which\nstrengthen the network topology and enhance its resilience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02678v1", "cate": "cs.CE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "模仿与异质性塑造社区货币网络的韧性", "tldr": "本文通过对Sardex社区货币网络的案例研究，利用图论方法分析其结构和动态特性，发现用户模仿行为和异质性连接有助于增强网络的韧性。", "motivation": "本文旨在通过对Sardex社区货币网络的案例研究，调查互助信用系统（如社区货币网络）的结构和动态特性，特别是探讨模仿行为和异质性如何影响网络的韧性。", "method": "研究将Sardex交易网络建模为有向加权图，并采用图论框架进行分析，重点关注强连通分量、凝聚表示和行为连通性模式。研究强调了网络核心和外围结构在三年内的演变，并考察了时间收缩、流量不对称和结构碎片化，同时考虑了不同用户类型的影响。", "result": "研究发现网络结构持续偏离基于度的零模型，表明存在行为模仿（即用户偏好更活跃的同行）。此外，不同类型用户之间的异质连接能增强网络拓扑结构并提升其韧性。", "conclusion": "行为模仿和异质性连接对社区货币网络的结构和韧性起着关键作用。", "translation": "社区货币网络由具有某些物理或社会特征的个人或公司组成，他们使用虚拟货币进行经济交易。本文通过对Sardex（一种在意大利撒丁岛发起并主要运营的社区货币）的案例研究，调查了此类互助信用系统的结构和动态特性。交易网络被建模为有向加权图，并通过图论框架进行分析，重点关注强连通分量、凝聚表示和行为连通性模式的分析。重点放在理解网络核心和外围结构在三年内的演变，并关注时间收缩、流量不对称和结构碎片化，这取决于不同的用户类型。我们的发现揭示了与基于度的零模型持续存在的偏差，并表明存在行为模仿，特别是用户偏好更活跃的同行。我们进一步评估了不同类型用户之间异质连接的影响，这加强了网络拓扑并增强了其韧性。", "summary": "本文以意大利撒丁岛的Sardex社区货币系统为例，深入探讨了社区货币网络的结构和动态特性。研究将交易网络构建为有向加权图，并运用图论框架分析了其三年内的演变，包括核心与外围结构、时间收缩、流量不对称和结构碎片化。研究发现，网络结构持续偏离零模型，表明用户存在模仿行为（倾向于活跃用户），且不同用户类型间的异质连接能有效强化网络拓扑并增强其韧性。", "keywords": "社区货币网络, 韧性, 模仿, 异质性, 图论分析", "comments": "本文通过对真实社区货币网络Sardex的深入图论分析，为理解此类系统的韧性机制提供了宝贵见解。其创新之处在于识别了用户行为中的“模仿”现象以及异质连接对网络鲁棒性的积极作用，这对于未来社区货币的设计和管理具有重要指导意义，强调了社会动力学在经济网络中的重要性。"}}
{"id": "2507.02144", "title": "Optimality Loss Minimization in Distributed Control with Application to District Heating", "authors": ["Audrey Blizard", "Stephanie Stockar"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Control Systems Technology", "url": "http://arxiv.org/abs/2507.02144v1", "summary": "This paper presents a novel partitioning method designed to minimize control\nperformance degradation resulting from partitioning a system for distributed\ncontrol while maintaining the computational benefits of these methods. A\ngame-theoretic performance metric, the modified Price of Anarchy, is introduced\nand is used in a generalizable partitioning metric to quantify optimality\nlosses in a distributed controller. By finding the partition that minimizes the\npartitioning metric, the best-performing distributed control design is chosen.\nThe presented partitioning metric is control-design agnostic, making it broadly\napplicable to many control design problems. In this paper, the developed metric\nis used to minimize the performance losses in the distributed control of a\ndemand-flexible District Heating Network. The final distributed controller is\nprovably feasible and stable. In simulation, this novel partitioning performed\nsimilarly to the centralized controller, increasing overall heat losses by only\n1.9%, as compared to a similarly-sized baseline partition, which resulted in a\n22% increase in losses.", "comment": "Submitted to IEEE Transactions on Control Systems Technology", "pdf_url": "http://arxiv.org/pdf/2507.02144v1", "cate": "eess.SY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "分布式控制中最优性损失最小化及其在区域供热中的应用", "tldr": "本文提出一种新颖的分区方法，通过最小化最优性损失来提高分布式控制性能，并在区域供热网络中取得接近集中式控制器的效果。", "motivation": "旨在解决分布式控制中因系统分区导致的控制性能下降问题，同时保持其计算优势。", "method": "提出一种新颖的分区方法，引入博弈论性能指标“修正的无政府代价”（modified Price of Anarchy）作为通用的分区度量，量化分布式控制器中的最优性损失，并通过最小化该度量来选择最佳分区。该度量与控制设计无关，适用性广。", "result": "应用该方法于需求灵活的区域供热网络分布式控制，最终的分布式控制器被证明是可行且稳定的。仿真结果显示，该新颖分区方法的性能与集中式控制器相似，总热损失仅增加1.9%，而基线分区则导致损失增加22%。", "conclusion": "该研究成功开发了一种有效的分区方法，显著减少了分布式控制中的性能损失，使其在保持计算优势的同时，能接近集中式控制器的性能，特别是在区域供热网络中表现出色。", "translation": "本文提出了一种新颖的分区方法，旨在最大程度地减少分布式控制中因系统分区导致的控制性能下降，同时保持这些方法的计算优势。引入了一种博弈论性能指标——修正的无政府代价，并将其用于一个通用的分区度量，以量化分布式控制器中的最优性损失。通过寻找使分区度量最小化的分区，可以选择性能最佳的分布式控制设计。所提出的分区度量与控制设计无关，使其广泛适用于许多控制设计问题。在本文中，所开发的度量用于最小化需求灵活的区域供热网络分布式控制中的性能损失。最终的分布式控制器被证明是可行且稳定的。在仿真中，这种新颖的分区方法表现与集中式控制器相似，总热损失仅增加了1.9%，而类似大小的基线分区则导致损失增加了22%。", "summary": "本文提出了一种创新的分区方法，旨在通过量化并最小化分布式控制中的最优性损失来提升其性能，同时保留计算效率。该方法引入了修正的无政府代价作为通用的分区度量，并被证明与控制设计无关。在应用于区域供热网络时，该方法使得分布式控制器在仿真中实现了与集中式控制器相近的性能，显著降低了热损失。", "keywords": "分布式控制, 最优性损失, 分区方法, 区域供热, 无政府代价", "comments": "该论文的创新点在于提出了一个通用的、与控制设计无关的分区度量（基于修正的无政府代价），有效地量化并最小化了分布式控制中的最优性损失。其在区域供热网络中的应用展示了该方法的实用性和优越性，显著提升了分布式控制的性能，使其能够接近集中式控制器的效果，同时保持计算优势，这对于大规模复杂系统的控制具有重要意义。"}}
{"id": "2507.02346", "title": "STAR-RIS Transceivers: Integrated Sensing and Communication with Pulsed Signals", "authors": ["Hedieh Taremizadeh", "Emanuele Grossi", "Luca Venturino"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted to the 33rd European Signal Processing Conference (EUSIPCO 2025), Isola delle Femmine, Palermo, Italy", "url": "http://arxiv.org/abs/2507.02346v1", "summary": "This study examines an integrated sensing and communication (ISAC)\ntransceiver featuring a simultaneous transmitting and reflecting reconfigurable\nintelligent surface (STAR-RIS) and a receiver equipped with a passive\nelectronically scanned array (PESA) and a single digital channel. By utilizing\na periodic pulsed signal emitted by a feeder, we introduce at the STAR-RIS a\nspace modulation to illuminate two angular directions observed by the radar\nreceiver, one in each half-space, and a time modulation to distinguish the\ncorresponding echoes from prospective moving targets and embed communication\nmessages. The proposed time modulation employs orthogonal binary codebooks with\ndifferent trade-offs in transmission and error rates, while having minimal\nimpact on the radar performance, evaluated by probability of detection and root\nmean square error in the radial velocity estimation.", "comment": "Accepted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025), Isola delle Femmine, Palermo, Italy", "pdf_url": "http://arxiv.org/pdf/2507.02346v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "STAR-RIS 收发器：基于脉冲信号的集成感知与通信", "tldr": "本研究提出了一种基于STAR-RIS和PESA的集成感知与通信(ISAC)收发器，利用脉冲信号进行空间和时间调制，以实现目标探测和通信，同时最小化对雷达性能的影响。", "motivation": "本研究旨在探索一种新型的集成感知与通信（ISAC）收发器设计，该设计结合了同步发射和反射可重构智能表面（STAR-RIS）和带有无源电子扫描阵列（PESA）的接收器，以实现高效且对雷达性能影响最小的同时感知和通信功能。", "method": "本研究提出了一种ISAC收发器，该收发器由STAR-RIS和配备PESA及单数字通道的接收器组成。系统利用馈线发出的周期性脉冲信号，在STAR-RIS处进行空间调制以照亮两个角方向，并进行时间调制以区分来自移动目标的回波并嵌入通信消息。所提出的时间调制方案采用了正交二进制码本，并通过检测概率和径向速度估计的均方根误差来评估其对雷达性能的影响。", "result": "所提出的时间调制方案在传输和错误率方面提供了不同的权衡，并且对雷达性能（通过检测概率和径向速度估计的均方根误差评估）的影响最小。", "conclusion": "本研究成功设计并评估了一种基于STAR-RIS的集成感知与通信（ISAC）收发器，该收发器能够有效实现感知与通信的集成，并在通信性能与雷达性能之间取得良好平衡，尤其是在不显著影响雷达性能的前提下嵌入通信数据。", "translation": "本研究考察了一种集成感知与通信（ISAC）收发器，其特点是配备了同步发射和反射可重构智能表面（STAR-RIS）以及一个带有无源电子扫描阵列（PESA）和单个数字通道的接收器。通过利用馈线发出的周期性脉冲信号，我们在STAR-RIS处引入了空间调制，以照亮雷达接收器观察到的两个角方向，每个半空间一个，并引入时间调制以区分来自潜在移动目标的相应回波并嵌入通信消息。所提出的时间调制采用了具有不同传输和错误率权衡的正交二进制码本，同时对雷达性能的影响最小，雷达性能通过检测概率和径向速度估计的均方根误差进行评估。", "summary": "本文研究了一种结合STAR-RIS和带有PESA的接收器的集成感知与通信（ISAC）收发器。该系统利用周期性脉冲信号，通过STAR-RIS的空间调制实现两个方向的照明，并通过时间调制区分目标回波和嵌入通信信息。研究评估了所提出的时间调制方案在不同传输和错误率权衡下的性能，并表明其对雷达性能（检测概率和径向速度估计误差）影响甚微。", "keywords": "STAR-RIS, 集成感知与通信, 脉冲信号, 空间调制, 时间调制", "comments": "这篇论文提出了一种新颖的ISAC收发器架构，将STAR-RIS与PESA结合，并通过脉冲信号的空间和时间调制实现了通信与感知的集成。其创新之处在于利用STAR-RIS同时进行发射和反射，并通过精巧的调制方案在不显著影响雷达性能的前提下嵌入通信数据，这对于未来无线通信和传感系统的发展具有重要意义。"}}
{"id": "2507.02367", "title": "A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\\left[^{18}\\text{F}\\right]$FDG PET imaging", "authors": ["Christian Salomonsen", "Luigi Tommaso Luppino", "Fredrik Aspheim", "Kristoffer Wickstrøm", "Elisabeth Wetzer", "Michael Kampffmeyer", "Rodrigo Berzaghi", "Rune Sundset", "Robert Jenssen", "Samuel Kuttner"], "categories": ["eess.IV", "cs.CV", "physics.med-ph", "q-bio.QM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      22 pages, 12 figures", "url": "http://arxiv.org/abs/2507.02367v1", "summary": "Dynamic positron emission tomography (PET) and kinetic modeling are pivotal\nin advancing tracer development research in small animal studies. Accurate\nkinetic modeling requires precise input function estimation, traditionally\nachieved via arterial blood sampling. However, arterial cannulation in small\nanimals like mice, involves intricate, time-consuming, and terminal procedures,\nprecluding longitudinal studies. This work proposes a non-invasive, fully\nconvolutional deep learning-based approach (FC-DLIF) to predict input functions\ndirectly from PET imaging, potentially eliminating the need for blood sampling\nin dynamic small-animal PET. The proposed FC-DLIF model includes a spatial\nfeature extractor acting on the volumetric time frames of the PET sequence,\nextracting spatial features. These are subsequently further processed in a\ntemporal feature extractor that predicts the arterial input function. The\nproposed approach is trained and evaluated using images and arterial blood\ncurves from [$^{18}$F]FDG data using cross validation. Further, the model\napplicability is evaluated on imaging data and arterial blood curves collected\nusing two additional radiotracers ([$^{18}$F]FDOPA, and [$^{68}$Ga]PSMA). The\nmodel was further evaluated on data truncated and shifted in time, to simulate\nshorter, and shifted, PET scans. The proposed FC-DLIF model reliably predicts\nthe arterial input function with respect to mean squared error and correlation.\nFurthermore, the FC-DLIF model is able to predict the arterial input function\neven from truncated and shifted samples. The model fails to predict the AIF\nfrom samples collected using different radiotracers, as these are not\nrepresented in the training data. Our deep learning-based input function offers\na non-invasive and reliable alternative to arterial blood sampling, proving\nrobust and flexible to temporal shifts and different scan durations.", "comment": "22 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.02367v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种用于动态小动物[18F]FDG PET成像中预测动脉输入函数的鲁棒且通用的深度学习模型", "tldr": "本研究提出了一种基于深度学习的非侵入性方法（FC-DLIF），可以直接从PET图像预测动脉输入函数，从而避免了小动物PET成像中繁琐的采血过程，并证明了其在时间和扫描持续时间变化下的鲁棒性。", "motivation": "在小动物PET研究中，准确的动力学建模需要精确的输入函数估计，传统方法通过动脉采血实现。然而，小动物（如小鼠）的动脉插管操作复杂、耗时且具有终末性，无法进行纵向研究。因此，需要一种非侵入性、可靠的替代方案。", "method": "本研究提出了一种名为FC-DLIF的非侵入性、全卷积深度学习方法。该模型包含一个作用于PET序列体积时间帧的空间特征提取器，用于提取空间特征；随后，这些特征在一个时间特征提取器中进一步处理，以预测动脉输入函数。该方法使用[18F]FDG数据进行交叉验证训练和评估，并使用[18F]FDOPA和[68Ga]PSMA两种额外放射性示踪剂的数据评估了模型的适用性。模型还在截断和时间偏移的数据上进行了评估，以模拟更短和偏移的PET扫描。", "result": "FC-DLIF模型在均方误差和相关性方面可靠地预测了动脉输入函数。该模型即使从截断和时间偏移的样本中也能预测动脉输入函数。然而，该模型未能从使用不同放射性示踪剂收集的样本中预测AIF，因为这些示踪剂未在训练数据中表示。", "conclusion": "本研究提出的基于深度学习的输入函数（FC-DLIF）提供了一种非侵入性且可靠的动脉采血替代方案，证明了其对于时间偏移和不同扫描持续时间的鲁棒性和灵活性。", "translation": "动态正电子发射断层扫描（PET）和动力学建模在推动小动物研究中的示踪剂开发方面至关重要。准确的动力学建模需要精确的输入函数估计，传统上通过动脉采血实现。然而，在小鼠等小动物中进行动脉插管涉及复杂、耗时且具有终末性的操作，这排除了纵向研究的可能性。这项工作提出了一种非侵入性、全卷积深度学习方法（FC-DLIF），可以直接从PET图像预测输入函数，可能消除动态小动物PET中采血的需要。所提出的FC-DLIF模型包括一个作用于PET序列体积时间帧的空间特征提取器，用于提取空间特征。这些特征随后在时间特征提取器中进一步处理，以预测动脉输入函数。所提出的方法使用[18F]FDG数据以及动脉血曲线通过交叉验证进行训练和评估。此外，该模型在收集了两种额外放射性示踪剂（[18F]FDOPA和[68Ga]PSMA）的成像数据和动脉血曲线上的适用性也得到了评估。该模型还在时间上截断和偏移的数据上进行了进一步评估，以模拟更短和偏移的PET扫描。所提出的FC-DLIF模型在均方误差和相关性方面可靠地预测了动脉输入函数。此外，FC-DLIF模型甚至能够从截断和偏移的样本中预测动脉输入函数。该模型未能从使用不同放射性示踪剂收集的样本中预测AIF，因为这些示踪剂未在训练数据中表示。我们基于深度学习的输入函数提供了一种非侵入性且可靠的动脉采血替代方案，证明了其对时间偏移和不同扫描持续时间的鲁棒性和灵活性。", "summary": "本研究开发了一种名为FC-DLIF的非侵入性深度学习模型，旨在直接从动态PET图像预测小动物的动脉输入函数（AIF），以替代传统的动脉采血方法。该模型包含空间和时间特征提取器，并通过[18F]FDG数据进行训练和验证。实验结果表明，FC-DLIF模型能够准确预测AIF，并且对扫描时间截断和偏移具有鲁棒性。尽管该模型在训练数据中未包含的不同放射性示踪剂上表现不佳，但它为小动物PET研究提供了一种可靠、灵活且非侵入性的AIF估计新途径。", "keywords": "深度学习, 动脉输入函数, PET成像, 小动物, 非侵入性", "comments": "这项工作通过引入基于深度学习的非侵入性方法来预测小动物PET中的动脉输入函数，解决了传统采血方法的局限性，具有重要的创新性。它有望显著简化实验流程，并允许进行纵向研究。模型的鲁棒性体现在对时间偏移和扫描持续变化的适应性，增加了其实用价值。然而，其对未训练放射性示踪剂的泛化能力受限是一个值得关注的局限性，未来研究可探索如何提升模型的跨示踪剂泛化能力。"}}
{"id": "2507.02394", "title": "On the Adversarial Robustness of Online Importance Sampling", "authors": ["Yotam Kenneth-Mordoch", "Shay Sapir"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02394v1", "summary": "This paper studies the adversarial-robustness of importance-sampling (aka\nsensitivity sampling); a useful algorithmic technique that samples elements\nwith probabilities proportional to some measure of their importance. A\nstreaming or online algorithm is called adversarially-robust if it succeeds\nwith high probability on input streams that may change adaptively depending on\nprevious algorithm outputs. Unfortunately, the dependence between stream\nelements breaks the analysis of most randomized algorithms, and in particular\nthat of importance-sampling algorithms. Previously, Braverman et al. [NeurIPS\n2021] suggested that streaming algorithms based on importance-sampling may be\nadversarially-robust; however, they proved it only for well-behaved inputs.\n  We focus on the adversarial-robustness of online importance-sampling, a\nnatural variant where sampling decisions are irrevocable and made as data\narrives. Our main technical result shows that, given as input an adaptive\nstream of elements $x_1,\\ldots,x_T\\in \\mathbb{R}_+$, online importance-sampling\nmaintains a $(1\\pm\\epsilon)$-approximation of their sum while matching (up to\nlower order terms) the storage guarantees of the oblivious (non-adaptive) case.\nWe then apply this result to develop adversarially-robust online algorithms for\ntwo fundamental problems: hypergraph cut sparsification and $\\ell_p$ subspace\nembedding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02394v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "在线重要性采样的对抗鲁棒性研究", "tldr": "本文研究了在线重要性采样在对抗性输入流下的鲁棒性，证明了它能保持求和的近似精度并匹配非适应性情况下的存储保证，并将其应用于超图割稀疏化和Lp子空间嵌入。", "motivation": "重要性采样是一种有用的算法技术，但在对抗性输入流下，流元素之间的依赖关系会破坏大多数随机算法（特别是重要性采样算法）的分析。先前的研究虽然提出重要性采样可能具有对抗鲁棒性，但仅限于良性输入。本文旨在研究在线重要性采样在适应性输入流下的对抗鲁棒性。", "method": "本文专注于在线重要性采样的对抗鲁棒性，这是一种采样决策不可撤销且数据到达时即做出的变体。通过技术分析，证明了其在面对适应性输入流时的性能。", "result": "主要技术结果表明，给定适应性元素流，在线重要性采样能够保持其和的 $(1\text{±}\text{ε})$-近似，同时（在低阶项上）匹配非适应性情况下的存储保证。", "conclusion": "研究结果被应用于开发两种基本问题的对抗鲁棒在线算法：超图割稀疏化和 $\\ell_p$ 子空间嵌入。", "translation": "本文研究了重要性采样（又称敏感度采样）的对抗鲁棒性；这是一种有用的算法技术，它根据元素重要性的某种度量以比例概率对元素进行采样。如果流式或在线算法能够在可能根据先前的算法输出自适应变化的输入流上以高概率成功，则称其具有对抗鲁棒性。不幸的是，流元素之间的依赖性破坏了大多数随机算法的分析，特别是重要性采样算法的分析。此前，Braverman 等人 [NeurIPS 2021] 提出基于重要性采样的流式算法可能具有对抗鲁棒性；然而，他们仅针对良性输入进行了证明。\n我们关注在线重要性采样的对抗鲁棒性，这是一种自然的变体，其中采样决策是不可撤销的，并在数据到达时做出。我们的主要技术结果表明，给定一个由元素 $x_1,\\ldots,x_T\\in \\mathbb{R}_+$ 组成的适应性流作为输入，在线重要性采样能够保持其和的 $(1\\pm\\epsilon)$-近似，同时（在低阶项上）匹配非适应性（非自适应）情况的存储保证。然后，我们将此结果应用于开发两种基本问题的对抗鲁棒在线算法：超图割稀疏化和 $\\ell_p$ 子空间嵌入。", "summary": "本文研究了在线重要性采样在对抗性输入流下的鲁棒性，这是一种采样决策即时且不可撤销的变体。针对现有重要性采样在处理适应性输入时分析受限的问题，本文证明了在线重要性采样在面对适应性元素流时，能够维持其总和的 $(1\\pm\\epsilon)$-近似，并且在存储效率上与非适应性情况相匹配。该研究成果为超图割稀疏化和 $\\ell_p$ 子空间嵌入等基本问题提供了对抗鲁棒的在线算法。", "keywords": "重要性采样, 对抗鲁棒性, 在线算法, 流式算法, 近似算法", "comments": "这项研究的创新之处在于，它解决了在线重要性采样在面对对抗性、适应性输入流时的鲁棒性问题，这在以往的研究中是一个挑战。通过证明其在近似精度和存储效率上的保证，该工作为构建更可靠的流式算法提供了理论基础，并将其成功应用于两个重要的计算机科学问题，显示了其潜在的实际应用价值。"}}
{"id": "2507.02530", "title": "Open-Source System for Multilingual Translation and Cloned Speech Synthesis", "authors": ["Mateo Cámara", "Juan Gutiérrez", "María Pilar Daza", "José Luis Blanco"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Presented at Forum Acusticum Euronoise 2025", "url": "http://arxiv.org/abs/2507.02530v1", "summary": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios.", "comment": "Presented at Forum Acusticum Euronoise 2025", "pdf_url": "http://arxiv.org/pdf/2507.02530v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "开源多语言翻译与克隆语音合成系统", "tldr": "一个开源系统，结合语音识别、大型语言模型和语音克隆技术，实现多语言翻译和语音再生，同时保留说话者声音，旨在提升多语言沟通和可访问性。", "motivation": "解决跨语言环境下沟通和可访问性方面的挑战。", "method": "系统整合了Whisper进行语音识别和VAD以识别说话间隔。接着是一个大型语言模型（LLM）管道：第一个LLM将语音分割成连贯完整的句子，第二个LLM进行翻译。对于语音再生，系统使用带有语音克隆功能的文本转语音（TTS）模块来复制原始说话者的声音，保持自然度和说话者身份。", "result": "系统组件开源，可本地或通过API操作，实现成本效益高的部署。应用场景包括Zoom会议中的实时多语言翻译、公共广播的语音再生以及通过个人设备的蓝牙多语言播放。通过保留说话者的声音，系统确保了无缝和沉浸式的体验。论文还提供了详细的系统性能分析，包括延迟和词准确率。", "conclusion": "该开源项目旨在促进创新和可访问性，并展示了其在现实多语言场景中实现包容性、适应性通信解决方案的巨大潜力。", "translation": "我们提出了一个开源系统，旨在实现多语言翻译和语音再生，以解决不同语言环境下沟通和可访问性方面的挑战。该系统整合了Whisper语音识别和语音活动检测（VAD）来识别说话间隔，随后是一个大型语言模型（LLM）管道。对于多语言应用，第一个LLM将语音分割成连贯完整的句子，然后由第二个LLM进行翻译。对于语音再生，系统使用带有语音克隆功能的文本转语音（TTS）模块来复制原始说话者的声音，保持自然度和说话者身份。\n该系统的开源组件可以本地运行或通过API调用，为各种用例提供经济高效的部署。这些用例包括Zoom会议中的实时多语言翻译、公共广播的语音再生以及通过个人设备的蓝牙多语言播放。通过保留说话者的声音，系统无论是进行翻译还是语音再生，都能确保无缝和沉浸式的体验。\n该开源项目与社区共享，旨在促进创新和可访问性。我们提供了详细的系统性能分析，包括延迟和词准确率，展示了其在现实多语言场景中实现包容性、适应性通信解决方案的潜力。", "summary": "本文介绍了一个开源系统，旨在解决多语言沟通和可访问性问题。该系统通过整合Whisper语音识别、VAD以及两阶段大型语言模型（LLM）管道，实现了语音的分割和多语言翻译。同时，它利用带有语音克隆功能的文本转语音（TTS）模块进行语音再生，能够复制并保留原始说话者的声音。该系统支持本地或API部署，具有成本效益，适用于Zoom实时翻译、公共广播等多种实际应用场景。文章还提供了详细的性能分析，展示了其在现实多语言环境中提供无缝、包容性通信解决方案的潜力。", "keywords": "多语言翻译, 语音合成, 语音克隆, 开源系统, LLM", "comments": "该系统通过整合语音识别、LLM翻译和语音克隆等先进技术，并以开源形式发布，极大地降低了多语言交流和语音再生技术的应用门槛。其能够保留原始说话者声音的特性，在提升用户体验方面具有显著创新性，有望在多语言沟通和辅助技术领域发挥重要作用。"}}
{"id": "2507.02014", "title": "ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations", "authors": ["Anoushka Harit", "Zhongtian Sun", "Suncica Hadzidedic"], "categories": ["cs.IR", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02014v1", "summary": "We introduce ManifoldMind, a probabilistic geometric recommender system for\nexploratory reasoning over semantic hierarchies in hyperbolic space. Unlike\nprior methods with fixed curvature and rigid embeddings, ManifoldMind\nrepresents users, items, and tags as adaptive-curvature probabilistic spheres,\nenabling personalised uncertainty modeling and geometry-aware semantic\nexploration. A curvature-aware semantic kernel supports soft, multi-hop\ninference, allowing the model to explore diverse conceptual paths instead of\noverfitting to shallow or direct interactions. Experiments on four public\nbenchmarks show superior NDCG, calibration, and diversity compared to strong\nbaselines. ManifoldMind produces explicit reasoning traces, enabling\ntransparent, trustworthy, and exploration-driven recommendations in sparse or\nabstract domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02014v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "ManifoldMind：用于可信推荐的动态双曲推理", "tldr": "ManifoldMind是一个新的概率几何推荐系统，它使用自适应曲率的概率球体在双曲空间中进行探索性推理，以实现个性化不确定性建模和几何感知语义探索，并在多个基准测试中表现出优越的性能和可信赖性。", "motivation": "与现有固定曲率和刚性嵌入的方法不同，ManifoldMind旨在通过引入自适应曲率的概率球体来解决个性化不确定性建模和几何感知语义探索的挑战，从而在稀疏或抽象领域提供透明、可信赖和探索驱动的推荐。", "method": "ManifoldMind是一个概率几何推荐系统，它在双曲空间中将用户、物品和标签表示为自适应曲率的概率球体，以实现个性化不确定性建模和几何感知语义探索。它采用一个曲率感知语义核来支持软、多跳推理，允许模型探索多样化的概念路径，而不是过拟合于浅层或直接的交互。", "result": "在四个公共基准测试中，ManifoldMind在NDCG、校准和多样性方面优于强大的基线方法。", "conclusion": "ManifoldMind提供了一种新颖的概率几何推荐系统，通过动态双曲推理和自适应曲率建模，实现了优越的推荐性能、个性化不确定性建模和透明的可信赖推荐，特别适用于稀疏或抽象领域。", "translation": "我们引入了ManifoldMind，一个用于在双曲空间中对语义层次结构进行探索性推理的概率几何推荐系统。与现有固定曲率和刚性嵌入的方法不同，ManifoldMind将用户、物品和标签表示为自适应曲率的概率球体，从而实现个性化不确定性建模和几何感知语义探索。一个曲率感知语义核支持软、多跳推理，允许模型探索多样化的概念路径，而不是过拟合于浅层或直接的交互。在四个公共基准测试中，实验显示其NDCG、校准和多样性优于强大的基线方法。ManifoldMind生成明确的推理轨迹，从而在稀疏或抽象领域实现透明、可信赖和探索驱动的推荐。", "summary": "ManifoldMind是一种新颖的概率几何推荐系统，它在双曲空间中利用自适应曲率的概率球体来表示用户、物品和标签。这种方法能够进行个性化的不确定性建模和几何感知的语义探索。通过一个曲率感知语义核，ManifoldMind支持软性、多跳推理，以探索多样的概念路径，避免过拟合。在多个公共基准测试中，该系统在NDCG、校准和多样性方面均表现出优越性能，并能生成透明的推理轨迹，实现可信赖的推荐。", "keywords": "双曲空间, 推荐系统, 概率几何, 自适应曲率, 可信赖推荐", "comments": "ManifoldMind的创新点在于其引入了自适应曲率的概率球体来表示实体，这使得模型能够进行个性化的不确定性建模和更灵活的几何感知语义探索。其曲率感知语义核支持多跳推理，有助于避免过拟合，并探索更深层的语义关系。此外，生成明确推理轨迹的能力显著增强了推荐的透明度和可信赖性，这在当前推荐系统研究中是一个重要的方向。"}}
{"id": "2507.02477", "title": "Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk", "authors": ["Gaochao Song", "Zibo Zhao", "Haohan Weng", "Jingbo Zeng", "Rongfei Jia", "Shenghua Gao"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages main text, 14 pages appendix, 23 figures", "url": "http://arxiv.org/abs/2507.02477v1", "summary": "We introduce Mesh Silksong, a compact and efficient mesh representation\ntailored to generate the polygon mesh in an auto-regressive manner akin to silk\nweaving. Existing mesh tokenization methods always produce token sequences with\nrepeated vertex tokens, wasting the network capability. Therefore, our approach\ntokenizes mesh vertices by accessing each mesh vertice only once, reduces the\ntoken sequence's redundancy by 50\\%, and achieves a state-of-the-art\ncompression rate of approximately 22\\%. Furthermore, Mesh Silksong produces\npolygon meshes with superior geometric properties, including manifold topology,\nwatertight detection, and consistent face normals, which are critical for\npractical applications. Experimental results demonstrate the effectiveness of\nour approach, showcasing not only intricate mesh generation but also\nsignificantly improved geometric integrity.", "comment": "9 pages main text, 14 pages appendix, 23 figures", "pdf_url": "http://arxiv.org/pdf/2507.02477v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Mesh Silksong：自回归网格生成如织丝", "tldr": "Mesh Silksong 是一种新的网格表示方法，通过单次顶点访问减少了网格令牌序列的冗余，实现了更好的压缩率和几何完整性。", "motivation": "现有网格令牌化方法总是产生包含重复顶点令牌的令牌序列，浪费了网络能力。", "method": "Mesh Silksong 通过仅访问每个网格顶点一次来令牌化网格顶点，从而减少令牌序列的冗余，并以类似于织丝的方式自回归生成多边形网格。", "result": "将令牌序列的冗余减少了 50%，实现了约 22% 的最先进压缩率。生成的网格具有卓越的几何属性，包括流形拓扑、水密检测和一致的面法线。实验证明了其在复杂网格生成和显著改善几何完整性方面的有效性。", "conclusion": "Mesh Silksong 是一种有效且高效的自回归网格生成方法，在提高效率、压缩率和生成高质量几何网格方面表现出色，对实际应用至关重要。", "translation": "我们引入了 Mesh Silksong，这是一种紧凑高效的网格表示方法，旨在以类似于织丝的方式自回归生成多边形网格。现有的网格令牌化方法总是产生包含重复顶点令牌的令牌序列，浪费了网络能力。因此，我们的方法通过仅访问每个网格顶点一次来令牌化网格顶点，将令牌序列的冗余减少了 50%，并实现了约 22% 的最先进压缩率。此外，Mesh Silksong 生成的多边形网格具有卓越的几何属性，包括流形拓扑、水密检测和一致的面法线，这对于实际应用至关重要。实验结果证明了我们方法的有效性，不仅展示了复杂的网格生成，而且显著改善了几何完整性。", "summary": "Mesh Silksong 提出了一种新颖的自回归网格生成方法，通过优化网格顶点令牌化过程，避免了现有方法中的重复顶点令牌问题。该方法仅需单次访问顶点，显著减少了令牌序列冗余（50%），并达到了22%的最先进压缩率。此外，Mesh Silksong 生成的网格具有优越的几何特性，如流形拓扑和水密性，对于实际应用具有重要意义，实验证明其在复杂网格生成和几何完整性方面表现出色。", "keywords": "网格生成, 自回归, 网格令牌化, 压缩, 几何完整性", "comments": "Mesh Silksong 的创新之处在于其独特的单次顶点访问令牌化策略，显著解决了现有网格生成方法中冗余令牌的问题，从而提高了效率和压缩率。其生成高质量几何网格的能力（流形、水密）对于实际应用具有重要价值，为三维内容创建和处理领域带来了显著进步。"}}
{"id": "2507.02337", "title": "ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms", "authors": ["Gjorgjina Cenikj", "Gašper Petelin", "Tome Eftimov"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02337v1", "summary": "Understanding the behavior of numerical metaheuristic optimization algorithms\nis critical for advancing their development and application. Traditional\nvisualization techniques, such as convergence plots, trajectory mapping, and\nfitness landscape analysis, often fall short in illustrating the structural\ndynamics of the search process, especially in high-dimensional or complex\nsolution spaces. To address this, we propose a novel representation and\nvisualization methodology that clusters solution candidates explored by the\nalgorithm and tracks the evolution of cluster memberships across iterations,\noffering a dynamic and interpretable view of the search process. Additionally,\nwe introduce two metrics - algorithm stability and algorithm similarity- to\nquantify the consistency of search trajectories across runs of an individual\nalgorithm and the similarity between different algorithms, respectively. We\napply this methodology to a set of ten numerical metaheuristic algorithms,\nrevealing insights into their stability and comparative behaviors, thereby\nproviding a deeper understanding of their search dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02337v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ClustOpt: 一种基于聚类的方法，用于表示和可视化数值元启发式优化算法的搜索动态", "tldr": "ClustOpt 提出了一种基于聚类的新方法，用于可视化和理解数值元启发式优化算法的搜索动态，并引入了算法稳定性和算法相似性指标，以提供对算法行为的更深层次理解。", "motivation": "传统的数值元启发式优化算法可视化技术（如收敛图、轨迹映射和适应度景观分析）在展示搜索过程的结构动态方面存在不足，尤其是在高维或复杂解空间中。因此，需要一种新的方法来更好地理解算法行为。", "method": "本文提出了一种新颖的表示和可视化方法，通过对算法探索的候选解进行聚类，并跟踪聚类成员在迭代过程中的演变，从而提供搜索过程的动态和可解释视图。此外，还引入了两个指标：算法稳定性和算法相似性，分别用于量化单个算法在不同运行中的搜索轨迹一致性以及不同算法之间的相似性。", "result": "将该方法应用于十种数值元启发式算法，揭示了它们稳定性方面的见解以及比较行为，从而对它们的搜索动态提供了更深入的理解。", "conclusion": "通过引入基于聚类的可视化方法和新的量化指标，ClustOpt 能够提供对数值元启发式优化算法搜索动态的更深入理解，弥补了传统可视化技术的不足。", "translation": "理解数值元启发式优化算法的行为对于推动其发展和应用至关重要。传统的可视化技术，如收敛图、轨迹映射和适应度景观分析，在说明搜索过程的结构动态方面常常力不从心，尤其是在高维或复杂解空间中。为了解决这个问题，我们提出了一种新颖的表示和可视化方法，该方法对算法探索的候选解进行聚类，并跟踪迭代过程中聚类成员的演变，从而提供搜索过程的动态和可解释视图。此外，我们引入了两个指标——算法稳定性和算法相似性——分别用于量化单个算法在多次运行中搜索轨迹的一致性以及不同算法之间的相似性。我们将该方法应用于一组十种数值元启发式算法，揭示了它们稳定性方面的见解和比较行为，从而对它们的搜索动态提供了更深入的理解。", "summary": "ClustOpt 提出了一种创新的基于聚类的方法，用于表示和可视化数值元启发式优化算法的搜索动态。该方法通过聚类算法探索的解决方案候选并跟踪聚类成员随迭代的演变，克服了传统可视化技术在高维复杂空间中显示结构动态的局限性。此外，它还引入了算法稳定性和算法相似性指标，以量化算法行为。将该方法应用于多种算法，成功揭示了它们的稳定性和比较行为，从而加深了对这些算法搜索动态的理解。", "keywords": "聚类, 元启发式优化, 搜索动态, 可视化, 算法稳定性", "comments": "这篇论文的创新点在于提出了一个基于聚类的新颖框架ClustOpt，用于可视化和分析元启发式优化算法的搜索过程。它有效地解决了传统方法在处理高维复杂解空间时难以展示结构动态的问题。引入的算法稳定性和相似性指标也为量化算法行为提供了新的视角，对于算法的开发和比较具有重要意义。该研究对于深入理解优化算法的内在机制，以及指导算法选择和改进具有重要价值。"}}
{"id": "2507.02199", "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02199v1", "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02199v1", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "潜在的思维链？解码深度循环Transformer", "tldr": "研究发现深度循环Transformer (Huginn-3.5B) 在算术任务上表现出有限的可解释的潜在思维链证据，且增加循环深度收益甚微，不如显式思维链模型。", "motivation": "CoT推理在Transformer中效率低且无法捕捉非语言表示的推理。研究旨在探索循环架构是否能在潜在空间内化推理，即潜在CoT是否在深度循环Transformer中出现。", "method": "论文研究了Huginn-3.5B（一种在推理时重用层而不增加参数的深度循环Transformer），使用Logit Lens和Coda Lens等探测技术，在算术任务上检查模型的内部行为，通过跟踪最终和中间结果token的秩轨迹来寻找可解释的潜在CoT证据。", "result": "发现可解释的潜在CoT证据有限；揭示了循环块之间显著的探测不一致性，隐藏状态的可解释性高度依赖于层索引和解码方法；经验性地表明增加循环深度收益甚微，远不如显式外部化推理步骤的模型。", "conclusion": "深度循环Transformer（如Huginn-3.5B）在内部化可解释的潜在思维链方面表现有限，且其性能提升通过增加循环深度并不显著，未能达到显式思维链模型的水平。", "translation": "思维链（CoT）推理使基于Transformer的语言模型在复杂的数学和多步规划方面表现出色。然而，在标准的仅解码器架构中，这些推理步骤以自然语言形式外部化，提高了可解释性，但牺牲了效率。为了捕捉不易用文字表示的推理，许多工作探索了旨在将推理内化到潜在空间中的循环架构，可能支持潜在CoT。在本文中，我们研究了这种推理结构是否出现在Huginn-3.5B中，这是一种深度循环Transformer，它在推理时重用层而不增加参数数量。我们使用包括Logit Lens和Coda Lens在内的一套探测技术，在算术任务上检查模型的内部行为。我们的发现通过跟踪最终和中间结果token的秩轨迹，揭示了可解释的潜在CoT的有限证据。此外，我们发现了循环块之间显著的探测不一致性，其中隐藏状态的可解释性高度依赖于层索引和解码方法。最后，我们经验性地表明，增加循环深度仅带来微不足道的收益，并且远低于明确外部化推理步骤的模型。代码可在https://github.com/wenquanlu/huginn-latent-cot获取。", "summary": "本文探讨了深度循环Transformer模型Huginn-3.5B是否能内化“潜在思维链”推理。通过对模型在算术任务上的内部行为进行探测，研究发现可解释的潜在CoT证据有限，且隐藏状态的可解释性因层和解码方法而异。此外，增加循环深度对性能提升不大，远不及显式思维链方法。", "keywords": "深度循环Transformer, 潜在思维链, 模型可解释性, Huginn-3.5B, 推理", "comments": "这篇论文对深度循环Transformer中潜在思维链的存在性进行了深入的实证分析，其创新点在于采用了多种探测技术来揭示模型内部推理过程。研究结果揭示了潜在CoT的局限性及其可解释性问题，对未来探索Transformer内部推理机制和优化模型效率具有重要指导意义。它挑战了“潜在CoT”的有效性，并强调了显式CoT的优势。"}}
{"id": "2507.02273", "title": "Fx-Encoder++: Extracting Instrument-Wise Audio Effects Representations from Mixtures", "authors": ["Yen-Tung Yeh", "Junghyun Koo", "Marco A. Martínez-Ramírez", "Wei-Hsiang Liao", "Yi-Hsuan Yang", "Yuki Mitsufuji"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      ISMIR 2025", "url": "http://arxiv.org/abs/2507.02273v1", "summary": "General-purpose audio representations have proven effective across diverse\nmusic information retrieval applications, yet their utility in intelligent\nmusic production remains limited by insufficient understanding of audio effects\n(Fx). Although previous approaches have emphasized audio effects analysis at\nthe mixture level, this focus falls short for tasks demanding instrument-wise\naudio effects understanding, such as automatic mixing. In this work, we present\nFx-Encoder++, a novel model designed to extract instrument-wise audio effects\nrepresentations from music mixtures. Our approach leverages a contrastive\nlearning framework and introduces an \"extractor\" mechanism that, when provided\nwith instrument queries (audio or text), transforms mixture-level audio effects\nembeddings into instrument-wise audio effects embeddings. We evaluated our\nmodel across retrieval and audio effects parameter matching tasks, testing its\nperformance across a diverse range of instruments. The results demonstrate that\nFx-Encoder++ outperforms previous approaches at mixture level and show a novel\nability to extract effects representation instrument-wise, addressing a\ncritical capability gap in intelligent music production systems.", "comment": "ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2507.02273v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Fx-Encoder++：从混合音频中提取乐器级音频效果表示", "tldr": "Fx-Encoder++ 是一种新模型，它使用对比学习和提取器机制，能从混合音频中提取乐器级的音频效果表示，优于现有方法并解决了智能音乐制作中的能力空白。", "motivation": "现有通用音频表示在智能音乐制作中因对音频效果理解不足而受限。之前的音频效果分析主要在混合层面，无法满足需要乐器级音频效果理解的任务（如自动混音）。", "method": "提出 Fx-Encoder++ 模型，用于从音乐混合中提取乐器级音频效果表示。该方法利用对比学习框架，并引入“提取器”机制，该机制在提供乐器查询（音频或文本）时，将混合级音频效果嵌入转换为乐器级音频效果嵌入。", "result": "Fx-Encoder++ 在检索和音频效果参数匹配任务中表现优于之前的混合级方法，并展示了提取乐器级效果表示的新能力。", "conclusion": "Fx-Encoder++ 解决了智能音乐制作系统中的关键能力空白，通过提供乐器级音频效果表示。", "translation": "通用音频表示已在各种音乐信息检索应用中证明有效，但它们在智能音乐制作中的效用仍因对音频效果（Fx）理解不足而受限。尽管以前的方法强调混合层面的音频效果分析，但这种侧重无法满足需要乐器级音频效果理解的任务，例如自动混音。在这项工作中，我们提出了 Fx-Encoder++，一个旨在从音乐混合中提取乐器级音频效果表示的新型模型。我们的方法利用对比学习框架，并引入了一种“提取器”机制，当提供乐器查询（音频或文本）时，该机制将混合级音频效果嵌入转换为乐器级音频效果嵌入。我们在检索和音频效果参数匹配任务中评估了我们的模型，测试了其在各种乐器上的性能。结果表明 Fx-Encoder++ 在混合层面优于以前的方法，并展示了提取乐器级效果表示的新能力，解决了智能音乐制作系统中的一个关键能力空白。", "summary": "Fx-Encoder++ 是一个新颖的模型，旨在解决智能音乐制作中对乐器级音频效果理解的不足。它利用对比学习框架和独特的“提取器”机制，能够从混合音频中提取特定乐器的音频效果表示。实验结果表明，该模型在检索和效果参数匹配任务中表现出色，并成功实现了乐器级的效果表示提取，填补了现有技术的空白。", "keywords": "音频效果、乐器级表示、对比学习、音乐信息检索、智能音乐制作", "comments": "该论文提出了一种创新性的方法 Fx-Encoder++，通过引入“提取器”机制和对比学习，首次实现了从混合音频中提取乐器级音频效果表示。这对于自动化混音等智能音乐制作应用具有重要意义，因为它解决了现有方法只能在混合层面分析效果的局限性，填补了关键的技术空白。"}}
{"id": "2507.02309", "title": "Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle", "authors": ["Anbin Wu", "Zhiyong Feng", "Ruitao Feng"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02309v1", "summary": "RESTful APIs facilitate data exchange between applications, but they also\nexpose sensitive resources to potential exploitation. Broken Object Level\nAuthorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,\nexemplifies a critical access control flaw where attackers manipulate API\nparameters to gain unauthorized access. To address this, we propose BOLAZ, a\ndefense framework grounded in zero trust principles. BOLAZ analyzes the data\nflow of resource IDs, pinpointing BOLA attack injection points and determining\nthe associated authorization intervals to prevent horizontal privilege\nescalation. Our approach leverages static taint tracking to categorize APIs\ninto producers and consumers based on how they handle resource IDs. By mapping\nthe propagation paths of resource IDs, BOLAZ captures the context in which\nthese IDs are produced and consumed, allowing for precise identification of\nauthorization boundaries. Unlike defense methods based on common authorization\nmodels, BOLAZ is the first authorization-guided method that adapts defense\nrules based on the system's best-practice authorization logic. We validate\nBOLAZ through empirical research on 10 GitHub projects. The results demonstrate\nBOLAZ's effectiveness in defending against vulnerabilities collected from CVE\nand discovering 35 new BOLA vulnerabilities in the wild, demonstrating its\npracticality in real-world deployments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02309v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "在零信任原则下重新思考失效对象级授权攻击", "tldr": "本文提出BOLAZ，一个基于零信任原则的防御框架，通过分析资源ID的数据流和利用静态污点跟踪来检测并防止失效对象级授权（BOLA）攻击，并在实际项目中验证了其有效性。", "motivation": "RESTful API暴露敏感资源，而失效对象级授权（BOLA）是OWASP API安全十大漏洞之首，攻击者通过操纵API参数获得未经授权的访问，这是一种关键的访问控制缺陷。为了解决这一问题，本文提出了一种新的防御框架。", "method": "本文提出了BOLAZ防御框架，其基于零信任原则。BOLAZ通过分析资源ID的数据流，精确识别BOLA攻击的注入点并确定相关的授权间隔，以防止横向权限升级。该方法利用静态污点跟踪将API分为资源ID的生产者和消费者，通过映射资源ID的传播路径，捕获ID的生成和消费上下文，从而精确识别授权边界。与基于常见授权模型的防御方法不同，BOLAZ是首个根据系统最佳实践授权逻辑调整防御规则的授权引导方法。", "result": "通过对10个GitHub项目进行实证研究，验证了BOLAZ的有效性。结果表明，BOLAZ能有效防御从CVE收集的漏洞，并发现了35个新的BOLA漏洞，证明了其在实际部署中的实用性。", "conclusion": "BOLAZ作为一个基于零信任原则的防御框架，通过独特的资源ID数据流分析和静态污点跟踪方法，有效识别并防止了失效对象级授权（BOLA）攻击，并在实际项目中展现出强大的漏洞发现能力和实用性，为API安全提供了一种新的、授权引导的防御范式。", "translation": "RESTful API促进了应用程序之间的数据交换，但它们也使敏感资源面临潜在的利用。失效对象级授权（BOLA）是OWASP API安全十大漏洞之首，它是一个关键的访问控制缺陷的典型例子，攻击者通过操纵API参数来获得未经授权的访问。为了解决这个问题，我们提出了BOLAZ，一个基于零信任原则的防御框架。BOLAZ分析资源ID的数据流，精确定位BOLA攻击注入点并确定相关的授权间隔，以防止横向权限升级。我们的方法利用静态污点跟踪将API根据它们处理资源ID的方式分为生产者和消费者。通过映射资源ID的传播路径，BOLAZ捕获了这些ID的生成和消费上下文，从而能够精确识别授权边界。与基于常见授权模型的防御方法不同，BOLAZ是第一个根据系统最佳实践授权逻辑调整防御规则的授权引导方法。我们通过对10个GitHub项目的实证研究验证了BOLAZ。结果表明BOLAZ在防御从CVE收集的漏洞和发现35个新的BOLA漏洞方面都表现出有效性，证明了其在实际部署中的实用性。", "summary": "本文针对RESTful API中普遍存在的失效对象级授权（BOLA）漏洞，提出了一个名为BOLAZ的防御框架。BOLAZ基于零信任原则，通过分析资源ID的数据流和运用静态污点跟踪技术，识别BOLA攻击注入点和授权边界，从而有效阻止未经授权的访问和横向权限提升。该框架创新性地根据系统最佳实践授权逻辑调整防御规则。实验证明，BOLAZ不仅能有效防御已知BOLA漏洞，还能在实际项目中发现大量新的漏洞，展现了其在API安全领域的实用性和有效性。", "keywords": "失效对象级授权, 零信任, API安全, 授权, 静态污点跟踪", "comments": "本文创新性地将零信任原则应用于失效对象级授权（BOLA）的防御，并通过资源ID的数据流分析和静态污点跟踪技术，实现了对授权边界的精确识别。其“授权引导”的方法，即根据系统最佳实践授权逻辑调整防御规则，是与传统防御方法的重要区别。在实际项目中的验证结果，特别是发现大量新漏洞的能力，突显了该方法的实用价值和对API安全领域的潜在重要贡献。"}}
{"id": "2507.02682", "title": "A wireless, inexpensive optical tracker for the CAVE", "authors": ["Ehud Sharlin", "Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02682v1", "summary": "CAVE displays offer many advantages over other virtual reality (VR) displays,\nincluding a large, unencumbering viewing space. Unfortunately, the typical\ntracking subsystems used with CAVE displays tether the user and lessen this\nadvantage. We have designed a simple, low-cost feet tracker that is wireless,\nleaving the user free to move. The tracker can be assembled for less than $200\nUS, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested\nthe prototype with two applications: a visualization supporting close visual\ninspection, and a walkthrough of the campus. Although the tracking was\nconvincing, it was clear that the tracker's limitations make it less than ideal\nfor applications requiring precise visual inspection. However, the freedom of\nmotion allowed by the tracker was a compelling supplement to our campus\nwalkthrough, allowing users to stroll and look around corners.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02682v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种用于CAVE的无线、廉价光学跟踪器", "tldr": "本文介绍了一种无线、廉价的光学脚部跟踪器，用于CAVE虚拟现实显示器，旨在解决传统跟踪系统束缚用户的问题，并允许用户自由移动。该跟踪器成本低于200美元，精度为10厘米，采样率为20赫兹，适用于某些需要运动自由度的应用。", "motivation": "CAVE显示器虽然提供了大型、无束缚的观看空间，但其典型的跟踪子系统会束缚用户，从而削弱了这一优势。因此，需要一种无线、低成本的跟踪器来提升用户体验。", "method": "本文设计并实现了一种简单、低成本的无线光学脚部跟踪器。该跟踪器可以以低于200美元的成本组装。", "result": "该跟踪器实现了10厘米的精度和20赫兹的采样率。原型在两个应用中进行了测试：一个支持近距离视觉检查的可视化应用和一个校园漫游应用。跟踪效果令人信服，但对于需要精确视觉检查的应用来说，跟踪器的局限性使其不太理想。然而，该跟踪器所允许的运动自由度对于校园漫游来说是一个引人注目的补充，允许用户漫步并环顾角落。", "conclusion": "虽然该无线、廉价的光学脚部跟踪器对于需要精确视觉检查的应用来说存在局限性，但它在需要运动自由度的应用（如虚拟漫游）中表现出色，显著增强了CAVE显示的用户体验。", "translation": "CAVE显示器相较于其他虚拟现实（VR）显示器具有许多优势，包括一个大型、无束缚的观看空间。不幸的是，CAVE显示器通常使用的跟踪子系统会束缚用户，并削弱了这一优势。我们设计了一种简单、低成本的脚部跟踪器，它是无线的，让用户可以自由移动。该跟踪器可以用不到200美元的成本组装，并以20赫兹的采样率达到10厘米的精度。我们用两个应用程序测试了原型：一个支持近距离视觉检查的可视化应用程序，以及一个校园漫游。尽管跟踪效果令人信服，但很明显，跟踪器的局限性使其不适用于需要精确视觉检查的应用程序。然而，跟踪器所允许的运动自由度是我们校园漫游的一个引人注目的补充，允许用户漫步并环顾角落。", "summary": "本文介绍了一种为CAVE虚拟现实显示器设计的无线、低成本光学脚部跟踪器，旨在克服传统跟踪系统束缚用户的缺点。该跟踪器成本低于200美元，能达到10厘米的精度和20赫兹的采样率。通过在可视化和校园漫游应用中的测试，证明其在提供运动自由度方面表现出色，尤其适用于虚拟漫游，尽管对于需要高精度视觉检查的应用仍存在局限性。", "keywords": "CAVE, 虚拟现实, 光学跟踪器, 无线, 低成本", "comments": "该论文提出了一种成本效益高且实用的解决方案，解决了CAVE VR系统中用户自由移动的限制。其创新点在于以极低的成本实现了无线跟踪，这对于推广CAVE VR体验具有重要意义。虽然精度有限，但其强调运动自由度的优势对于特定应用场景（如探索性漫游）具有显著价值。该研究为未来开发更经济、更具沉浸感的VR系统提供了有益的探索。"}}
{"id": "2507.02190", "title": "cVLA: Towards Efficient Camera-Space VLAs", "authors": ["Max Argus", "Jelena Bratulic", "Houman Masnavi", "Maxim Velikanov", "Nick Heppert", "Abhinav Valada", "Thomas Brox"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02190v1", "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling\ncomplex robotic manipulation tasks, but they are often expensive to train. In\nthis paper, we propose a novel VLA approach that leverages the competitive\nperformance of Vision Language Models (VLMs) on 2D images to directly infer\nrobot end-effector poses in image frame coordinates. Unlike prior VLA models\nthat output low-level controls, our model predicts trajectory waypoints, making\nit both more efficient to train and robot embodiment agnostic. Despite its\nlightweight design, our next-token prediction architecture effectively learns\nmeaningful and executable robot trajectories. We further explore the\nunderutilized potential of incorporating depth images, inference-time\ntechniques such as decoding strategies, and demonstration-conditioned action\ngeneration. Our model is trained on a simulated dataset and exhibits strong\nsim-to-real transfer capabilities. We evaluate our approach using a combination\nof simulated and real data, demonstrating its effectiveness on a real robotic\nsystem.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02190v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "cVLA：迈向高效的相机空间VLA", "tldr": "本文提出了cVLA，一种高效的视觉-语言-动作（VLA）模型，它利用视觉语言模型（VLM）在图像坐标中直接推断机器人末端执行器姿态，从而实现更快的训练和强大的从仿真到真实的迁移能力。", "motivation": "传统的视觉-语言-动作（VLA）模型在处理复杂的机器人操作任务时训练成本高昂。", "method": "本文提出了一种名为cVLA的新型VLA方法，该方法利用视觉语言模型（VLM）在2D图像上的出色性能，直接推断图像坐标系中的机器人末端执行器姿态。与以往输出低级控制的VLA模型不同，我们的模型预测轨迹路径点，使其更高效地训练且与机器人本体无关。该模型采用下一令牌预测架构，并进一步探索了深度图像的整合、推断时技术（如解码策略）以及示范条件下的动作生成。", "result": "该模型在模拟数据集上进行训练，并展现出强大的从仿真到真实（sim-to-real）的迁移能力。通过结合模拟和真实数据评估了该方法，证明了其在真实机器人系统上的有效性。", "conclusion": "cVLA方法在相机空间中预测机器人末端执行器姿态，通过预测轨迹路径点而非低级控制，实现了高效训练和机器人本体无关性。它能够有效地学习有意义且可执行的机器人轨迹，并展现出强大的从仿真到真实的迁移能力，在真实机器人系统上表现出有效性。", "translation": "视觉-语言-动作（VLA）模型为解决复杂的机器人操作任务提供了一个引人注目的框架，但它们的训练成本通常很高。在本文中，我们提出了一种新颖的VLA方法，该方法利用视觉语言模型（VLM）在2D图像上的竞争性性能，直接在图像坐标系中推断机器人末端执行器姿态。与以往输出低级控制的VLA模型不同，我们的模型预测轨迹路径点，使其既更高效地训练，又与机器人本体无关。尽管设计轻量，但我们的下一令牌预测架构有效地学习了有意义且可执行的机器人轨迹。我们进一步探索了结合深度图像、推断时技术（如解码策略）以及示范条件下的动作生成的未充分利用的潜力。我们的模型在模拟数据集上进行训练，并展现出强大的从仿真到真实（sim-to-real）的迁移能力。我们结合模拟和真实数据评估了我们的方法，证明了其在真实机器人系统上的有效性。", "summary": "cVLA是一种新颖的视觉-语言-动作（VLA）模型，旨在实现机器人操作任务的高效训练。与传统的VLA模型不同，cVLA利用视觉语言模型（VLM）直接在图像坐标中预测机器人末端执行器姿态作为轨迹路径点，使其训练更高效且与机器人本体无关。该模型基于下一令牌预测架构，并探索了深度图像和高级推断技术的使用。cVLA在模拟数据集上进行训练，展示了强大的从仿真到真实的迁移能力，并在真实机器人系统上证明了其有效性。", "keywords": "VLA, 机器人操作, 视觉语言模型, 从仿真到真实, 轨迹路径点", "comments": "本文提出了一种创新的方法，通过将VLA控制从低级控制转变为相机空间中的末端执行器姿态预测，从而简化了控制问题并提高了效率。利用VLM和专注于轨迹路径点是其创新之处。机器人本体无关性是一个显著的优势，可能扩大其应用范围。所展示的从仿真到真实的迁移能力对于实际机器人应用至关重要。"}}
{"id": "2507.02118", "title": "A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights", "authors": ["Cristina Martinez Montes", "Daniela Grassi", "Nicole Novielli", "Birgit Penzenstadle"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02118v1", "summary": "The study of well-being, stress and other human factors has traditionally\nrelied on self-report instruments to assess key variables. However, concerns\nabout potential biases in these instruments, even when thoroughly validated and\nstandardised, have driven growing interest in alternatives in combining these\nmeasures with more objective methods, such as physiological measures.\n  We aimed to (i) compare psychometric stress measures and biometric indicators\nand (ii) identify stress-related patterns in biometric data during software\nengineering tasks.\n  We conducted an experiment where participants completed a pre-survey, then\nprogrammed two tasks wearing biometric sensors, answered brief post-surveys for\neach, and finally went through a short exit interview.\n  Our results showed diverse outcomes; we found no stress in the psychometric\ninstruments. Participants in the interviews reported a mix of feeling no stress\nand experiencing time pressure. Finally, the biometrics showed a significant\ndifference only in EDA phasic peaks.\n  We conclude that our chosen way of inducing stress by imposing a stricter\ntime limit was insufficient. We offer methodological insights for future\nstudies working with stress, biometrics, and psychometric instruments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02118v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "结合生物识别和自我报告工具的多模式方法监测编程压力：方法论见解", "tldr": "本研究旨在通过结合生物识别和自我报告工具来监测编程中的压力，但发现所选的压力诱导方法（严格的时间限制）不足以引起可测量的压力，并提供了未来研究的方法论见解。", "motivation": "传统上，幸福感、压力和其他人类因素的研究主要依赖自我报告工具，但这些工具存在潜在偏见。因此，研究人员越来越关注将自我报告与生理测量等更客观的方法相结合，以克服这些局限性。", "method": "研究人员进行了一项实验，参与者首先完成一份预调查，然后佩戴生物识别传感器完成两项编程任务，接着对每项任务进行简短的后期调查，最后进行一次简短的离职面谈。", "result": "心理测量工具未显示压力。访谈中，参与者报告了无压力和时间压力并存的情况。生物识别数据显示，仅在EDA（电皮肤活动）的相变峰值上存在显著差异。", "conclusion": "本研究得出结论，通过施加更严格时间限制来诱导压力的方法不足。研究者为未来涉及压力、生物识别和心理测量工具的研究提供了方法论见解。", "translation": "幸福感、压力和其他人类因素的研究传统上依赖自我报告工具来评估关键变量。然而，即使经过彻底验证和标准化，这些工具的潜在偏见也引发了人们对将这些测量与更客观的方法（如生理测量）相结合的替代方案的日益增长的兴趣。\n我们的目标是（i）比较心理测量压力指标和生物识别指标，以及（ii）识别软件工程任务中生物识别数据中的压力相关模式。\n我们进行了一项实验，参与者完成了一份预调查，然后佩戴生物识别传感器编程两项任务，对每项任务回答简短的后期调查，最后进行了一次简短的离职面谈。\n我们的结果显示了不同的结果；我们发现心理测量工具中没有压力。访谈中，参与者报告了无压力和时间压力并存的情况。最后，生物识别数据显示仅在EDA（电皮肤活动）的相变峰值上存在显著差异。\n我们得出结论，我们选择的通过施加更严格时间限制来诱导压力的方法不足。我们为未来涉及压力、生物识别和心理测量工具的研究提供了方法论见解。", "summary": "本研究探讨了将生物识别技术与自我报告工具结合用于监测编程压力的多模式方法。实验中，参与者在编程任务中佩戴生物识别传感器并完成问卷和访谈。结果显示，心理测量工具未检测到压力，访谈反馈混合，生物识别数据仅在EDA相变峰值上显示显著差异。研究发现，所采用的严格时间限制不足以有效诱导压力，并提供了关于未来压力、生物识别和心理测量研究的方法论性见解。", "keywords": "生物识别, 压力监测, 自我报告, 编程, 方法论见解", "comments": "本研究的创新点在于尝试结合多模式数据（生物识别和自我报告）来监测编程中的压力，这在传统上依赖单一方法的研究中具有重要意义。尽管未能成功诱导出显著压力，但其对压力诱导方法有效性的反思和提出的方法论见解对于未来的研究具有重要的指导价值，强调了在实验设计中精确控制压力源的重要性。"}}
{"id": "2507.02180", "title": "The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future", "authors": ["Russell Beale"], "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.1; K.3.2"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02180v1", "summary": "Large language Models have only been widely available since 2022 and yet in\nless than three years have had a significant impact on approaches to education\nand educational technology. Here we review the domains in which they have been\nused, and discuss a variety of use cases, their successes and failures. We then\nprogress to discussing how this is changing the dynamic for learners and\neducators, consider the main design challenges facing LLMs if they are to\nbecome truly helpful and effective as educational systems, and reflect on the\nlearning paradigms they support. We make clear that the new interaction\nparadigms they bring are significant and argue that this approach will become\nso ubiquitous it will become the default way in which we interact with\ntechnologies, and revolutionise what people expect from computer systems in\ngeneral. This leads us to present some specific and significant considerations\nfor the design of educational technology in the future that are likely to be\nneeded to ensure acceptance by the changing expectations of learners and users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02180v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "革命已至：大型语言模型在教育领域的现状对未来的启示", "tldr": "大型语言模型自2022年普及以来，已对教育产生显著影响。本文回顾了其应用领域、成功与失败案例，讨论了其如何改变学习者和教育者的动态，面临的设计挑战，并提出未来教育技术设计的关键考量，预测LLMs将彻底改变人机交互。", "motivation": "大型语言模型（LLMs）自2022年起在教育领域产生了显著影响，本文旨在回顾其在教育中的应用、讨论其成功与失败、分析其对学习者和教育者的影响、探讨其面临的设计挑战以及它们所支持的学习范式，并为未来的教育技术设计提供考量。", "method": "本文采用综述和讨论的方法，回顾了大型语言模型在教育领域的使用范围、各种用例及其成功与失败。接着，讨论了LLMs如何改变学习者和教育者的动态，考量了LLMs作为教育系统面临的主要设计挑战，并反思了它们所支持的学习范式。", "result": "大型语言模型已对教育方法和教育技术产生显著影响。它们改变了学习者和教育者的动态，带来了新的交互范式，这些范式将变得无处不在，成为我们与技术互动和对计算机系统期望的默认方式。因此，未来的教育技术设计需要考虑这些改变以确保被接受。", "conclusion": "大型语言模型带来了显著的新交互范式，将成为我们与技术互动的默认方式，并彻底改变人们对计算机系统的期望。这要求未来的教育技术设计必须考虑特定的重要因素，以适应学习者和用户不断变化的期望。", "translation": "大型语言模型自2022年才广泛普及，但在不到三年的时间里，它们已经对教育方法和教育技术产生了重大影响。本文回顾了它们已被使用的领域，并讨论了各种用例、它们的成功和失败。然后，我们进一步讨论了这如何改变学习者和教育者的动态，考量了LLMs若要真正成为有益和有效的教育系统所面临的主要设计挑战，并反思了它们所支持的学习范式。我们明确指出，它们带来的新交互范式是重大的，并认为这种方法将变得如此普遍，以至于它将成为我们与技术互动的默认方式，并彻底改变人们对计算机系统的一般期望。这促使我们提出未来教育技术设计的一些具体而重要的考量，这些考量可能需要确保其被学习者和用户不断变化的期望所接受。", "summary": "本文探讨了大型语言模型（LLMs）自2022年普及以来对教育领域的深远影响。文章回顾了LLMs在教育中的应用、成功与失败案例，并分析了它们如何改变学习者与教育者的互动模式。作者进一步讨论了LLMs作为教育系统面临的关键设计挑战及其支持的学习范式，强调LLMs带来的新交互范式将普及并重塑人机交互。最后，文章提出了未来教育技术设计的重要考量，以适应用户不断变化的期望。", "keywords": "大型语言模型, 教育技术, 学习范式, 人机交互, 未来教育", "comments": "这篇论文及时地指出了大型语言模型在教育领域带来的“革命性”影响，其创新之处在于不仅回顾了现状，更着眼于未来，深入分析了LLMs对学习范式、人机交互的深远改变，并提出了未来教育技术设计的具体考量。其重要性在于为教育工作者和技术开发者提供了前瞻性的指导，强调了适应新交互模式的必要性。"}}
{"id": "2507.01997", "title": "Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting", "authors": ["Zhihao Wang", "Alessandro Cornacchia", "Franco Galante", "Carlo Centofanti", "Alessio Sacco", "Dingde Jiang"], "categories": ["cs.NI", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Accepted at ACM SIGCOMM 1st Workshop on Next-Generation Network Observability (NGNO)", "url": "http://arxiv.org/abs/2507.01997v1", "summary": "Recent research has demonstrated the effectiveness of Artificial Intelligence\n(AI), and more specifically, Large Language Models (LLMs), in supporting\nnetwork configuration synthesis and automating network diagnosis tasks, among\nothers. In this preliminary work, we restrict our focus to the application of\nAI agents to network troubleshooting and elaborate on the need for a\nstandardized, reproducible, and open benchmarking platform, where to build and\nevaluate AI agents with low operational effort.", "comment": "Accepted at ACM SIGCOMM 1st Workshop on Next-Generation Network\n  Observability (NGNO)", "pdf_url": "http://arxiv.org/pdf/2507.01997v1", "cate": "cs.NI", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "迈向一个用于网络故障排除AI代理的实验与基准测试民主化平台", "tldr": "鉴于AI（特别是LLM）在网络任务中的有效性，本文旨在阐述建立一个标准化、可复现、开放的AI代理网络故障排除基准测试平台的必要性。", "motivation": "最近的研究表明人工智能（AI），特别是大型语言模型（LLMs），在支持网络配置合成和自动化网络诊断任务方面表现出有效性。然而，目前缺乏一个标准化、可复现、开放的基准测试平台，以低操作成本构建和评估用于网络故障排除的AI代理。", "method": "这项初步工作主要阐述了建立一个标准化、可复现、开放的基准测试平台的需求，该平台旨在支持AI代理在网络故障排除领域的构建和评估。", "result": "Not mentioned in abstract", "conclusion": "本初步工作强调，为了有效构建和评估用于网络故障排除的AI代理，迫切需要一个标准化、可复现、开放的基准测试平台。", "translation": "最近的研究表明，人工智能（AI），特别是大型语言模型（LLMs），在支持网络配置合成和自动化网络诊断任务等方面表现出有效性。在这项初步工作中，我们将重点限制在AI代理在网络故障排除中的应用，并阐述了对一个标准化、可复现、开放的基准测试平台的需求，以便以较低的操作成本构建和评估AI代理。", "summary": "本初步工作探讨了AI和LLM在网络任务中的有效性，并强调了建立一个标准化、可复现、开放的基准测试平台的迫切需求。该平台旨在民主化AI代理在网络故障排除领域的实验和评估，从而降低操作成本并促进发展。", "keywords": "AI代理, 网络故障排除, 基准测试平台, 大型语言模型, 民主化", "comments": "本文提出建立一个“游乐场”式平台，以民主化AI代理在网络故障排除领域的实验和基准测试，这对于促进该特定领域AI研究的透明度、可复现性和协作至关重要。其创新性在于识别并呼吁填补当前AI网络运维领域缺乏标准化评估平台的空白。"}}
{"id": "2507.02523", "title": "Source Detection in Hypergraph Epidemic Dynamics using a Higher-Order Dynamic Message Passing Algorithm", "authors": ["Qiao Ke", "Naoki Masuda", "Zhen Jin", "Chuang Liu", "Xiu-Xiu Zhan"], "categories": ["physics.soc-ph", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02523v1", "summary": "Source detection is crucial for capturing the dynamics of real-world\ninfectious diseases and informing effective containment strategies. Most\nexisting approaches to source detection focus on conventional pairwise\nnetworks, whereas recent efforts on both mathematical modeling and analysis of\ncontact data suggest that higher-order (e.g., group) interactions among\nindividuals may both account for a large fraction of infection events and\nchange our understanding of how epidemic spreading proceeds in empirical\npopulations. In the present study, we propose a message-passing algorithm,\ncalled the HDMPN, for source detection for a stochastic susceptible-infectious\ndynamics on hypergraphs. By modulating the likelihood maximization method by\nthe fraction of infectious neighbors, HDMPN aims to capture the influence of\nhigher-order structures and do better than the conventional likelihood\nmaximization. We numerically show that, in most cases, HDMPN outperforms\nbenchmarks including the likelihood maximization method without modification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02523v1", "cate": "physics.soc-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "超图流行病动力学中基于高阶动态消息传递算法的溯源检测", "tldr": "本文提出了一种名为HDMPN的新型消息传递算法，用于超图上的流行病溯源检测。该算法通过考虑高阶交互作用，在数值上优于传统方法。", "motivation": "现有的流行病溯源检测方法主要关注传统的成对网络，但最近的研究表明，个体间的高阶（例如群组）交互在感染事件中占很大比例，并改变了我们对流行病传播的理解。因此，需要开发能够捕获高阶结构影响的溯源检测方法。", "method": "本研究提出了一种名为HDMPN的消息传递算法，用于超图上随机易感-感染（SI）动力学的溯源检测。HDMPN通过根据感染邻居的比例来调制似然最大化方法，旨在捕获高阶结构的影响。", "result": "数值结果表明，在大多数情况下，HDMPN算法的性能优于包括未经修改的似然最大化方法在内的基准方法。", "conclusion": "HDMPN算法能够有效利用高阶交互信息，显著提升超图流行病溯源检测的性能，超越了传统的似然最大化方法。", "translation": "溯源检测对于捕捉真实世界传染病的动态并制定有效的遏制策略至关重要。大多数现有的溯源检测方法都侧重于传统的成对网络，而最近在数学建模和接触数据分析方面的努力表明，个体间的高阶（例如，群组）交互可能既解释了大部分感染事件，又改变了我们对流行病传播在经验人群中如何进行的理解。在本研究中，我们提出了一种名为HDMPN的消息传递算法，用于超图上随机易感-感染动力学的溯源检测。通过根据感染邻居的比例来调制似然最大化方法，HDMPN旨在捕获高阶结构的影响，并优于传统的似然最大化。我们通过数值方法表明，在大多数情况下，HDMPN优于包括未经修改的似然最大化方法在内的基准方法。", "summary": "本文提出了一种名为HDMPN的高阶动态消息传递算法，专门用于超图上的流行病溯源检测。鉴于传统方法忽略了高阶交互在流行病传播中的关键作用，HDMPN通过调制似然最大化方法来有效捕捉这些高阶结构的影响。数值模拟结果表明，HDMPN在溯源检测性能上显著优于包括标准似然最大化在内的现有基准方法。", "keywords": "超图, 流行病学, 溯源检测, 消息传递算法, 高阶交互", "comments": "该论文的创新之处在于其将高阶交互引入流行病溯源检测的框架，这对于更准确地模拟和理解真实世界的复杂传播过程至关重要。HDMPN算法的提出填补了现有方法在处理超图结构上的不足，为流行病学研究提供了更精细的工具。其超越传统方法的表现凸显了考虑高阶结构的重要性，具有重要的理论和实际应用价值。"}}
{"id": "2507.02689", "title": "On the Convergence of Large Language Model Optimizer for Black-Box Network Management", "authors": ["Hoon Lee", "Wentao Zhou", "Merouane Debbah", "Inkyu Lee"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02689v1", "summary": "Future wireless networks are expected to incorporate diverse services that\noften lack general mathematical models. To address such black-box network\nmanagement tasks, the large language model (LLM) optimizer framework, which\nleverages pretrained LLMs as optimization agents, has recently been promoted as\na promising solution. This framework utilizes natural language prompts\ndescribing the given optimization problems along with past solutions generated\nby LLMs themselves. As a result, LLMs can obtain efficient solutions\nautonomously without knowing the mathematical models of the objective\nfunctions. Although the viability of the LLM optimizer (LLMO) framework has\nbeen studied in various black-box scenarios, it has so far been limited to\nnumerical simulations. For the first time, this paper establishes a theoretical\nfoundation for the LLMO framework. With careful investigations of LLM inference\nsteps, we can interpret the LLMO procedure as a finite-state Markov chain, and\nprove the convergence of the framework. Our results are extended to a more\nadvanced multiple LLM architecture, where the impact of multiple LLMs is\nrigorously verified in terms of the convergence rate. Comprehensive numerical\nsimulations validate our theoretical results and provide a deeper understanding\nof the underlying mechanisms of the LLMO framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02689v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "大型语言模型优化器在黑盒网络管理中收敛性研究", "tldr": "本文首次为大型语言模型优化器（LLMO）框架在黑盒网络管理中的应用建立了理论基础，通过将其解释为有限状态马尔可夫链，并证明了其收敛性。", "motivation": "未来的无线网络将包含多样化服务，这些服务通常缺乏通用的数学模型，导致黑盒网络管理任务难以解决。大型语言模型（LLM）优化器框架被提出作为一种有前景的解决方案，但其理论基础尚未建立。", "method": "本文首次为LLMO框架建立了理论基础，通过仔细研究LLM的推理步骤，将LLMO过程解释为有限状态马尔可夫链，并证明了该框架的收敛性。研究结果还扩展到了更高级的多LLM架构。", "result": "研究证明了LLMO框架的收敛性。对于多LLM架构，论文严格验证了多个LLM对收敛速度的影响。全面的数值模拟验证了理论结果，并加深了对LLMO框架底层机制的理解。", "conclusion": "本文为LLM优化器在黑盒网络管理中的应用提供了首个理论基础，通过马尔可夫链模型证明了其收敛性，并验证了多LLM架构的影响，为该框架的可靠性提供了重要见解。", "translation": "未来的无线网络预计将整合各种服务，这些服务通常缺乏通用的数学模型。为了解决此类黑盒网络管理任务，利用预训练大型语言模型（LLM）作为优化代理的大型语言模型（LLM）优化器框架最近被推广为一种有前景的解决方案。该框架利用描述给定优化问题的自然语言提示以及LLM自身生成的过去解决方案。因此，LLM无需了解目标函数的数学模型即可自主获得高效的解决方案。尽管LLM优化器（LLMO）框架在各种黑盒场景中的可行性已得到研究，但迄今为止，其研究仅限于数值模拟。本文首次为LLMO框架建立了理论基础。通过对LLM推理步骤的仔细研究，我们可以将LLMO过程解释为有限状态马尔可夫链，并证明该框架的收敛性。我们的结果扩展到了更高级的多LLM架构，其中多个LLM的影响在收敛速度方面得到了严格验证。全面的数值模拟验证了我们的理论结果，并提供了对LLMO框架底层机制的更深入理解。", "summary": "本文首次为大型语言模型优化器（LLMO）框架在黑盒网络管理中的应用建立了理论基础。通过将LLMO过程解释为有限状态马尔可夫链，论文证明了其收敛性，并将结果扩展到多LLM架构，严格验证了其对收敛速度的影响。全面的数值模拟验证了这些理论发现，加深了对LLMO框架工作机制的理解。", "keywords": "大型语言模型优化器, 黑盒网络管理, 收敛性, 马尔可夫链", "comments": "本文的创新之处在于，它首次为大型语言模型优化器（LLMO）提供了一个坚实的理论基础，将其从纯粹的经验性研究提升到具备数学严谨性的层面。通过将LLMO建模为马尔可夫链并证明其收敛性，论文解决了LLMO在黑盒网络管理中应用的一个关键空白，即其可靠性和稳定性的理论保证。这对于LLMO的实际部署和进一步发展具有重要意义，因为它提供了对其性能边界和行为的理解。"}}
{"id": "2507.02598", "title": "AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models", "authors": ["Chenhao Xue", "Kezhi Li", "Jiaxing Zhang", "Yi Ren", "Zhengyuan Shi", "Chen Zhang", "Yibo Lin", "Lining Zhang", "Qiang Xu", "Guangyu Sun"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      8 pages, 12 figures", "url": "http://arxiv.org/abs/2507.02598v1", "summary": "Arithmetic circuits, such as adders and multipliers, are fundamental\ncomponents of digital systems, directly impacting the performance, power\nefficiency, and area footprint. However, optimizing these circuits remains\nchallenging due to the vast design space and complex physical constraints.\nWhile recent deep learning-based approaches have shown promise, they struggle\nto consistently explore high-potential design variants, limiting their\noptimization efficiency. To address this challenge, we propose AC-Refiner, a\nnovel arithmetic circuit optimization framework leveraging conditional\ndiffusion models. Our key insight is to reframe arithmetic circuit synthesis as\na conditional image generation task. By carefully conditioning the denoising\ndiffusion process on target quality-of-results (QoRs), AC-Refiner consistently\nproduces high-quality circuit designs. Furthermore, the explored designs are\nused to fine-tune the diffusion model, which focuses the exploration near the\nPareto frontier. Experimental results demonstrate that AC-Refiner generates\ndesigns with superior Pareto optimality, outperforming state-of-the-art\nbaselines. The performance gain is further validated by integrating AC-Refiner\ninto practical applications.", "comment": "8 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.02598v1", "cate": "cs.AR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AC-Refiner：使用条件扩散模型进行高效算术电路优化", "tldr": "AC-Refiner是一个利用条件扩散模型优化算术电路的框架，通过将电路合成重构为条件图像生成任务，并利用探索的设计微调模型，实现了优于现有技术的帕累托最优设计。", "motivation": "算术电路是数字系统的基本组成部分，直接影响性能、功耗和面积。然而，由于庞大的设计空间和复杂的物理约束，优化这些电路仍然具有挑战性。虽然最近基于深度学习的方法显示出潜力，但它们难以持续探索高潜力设计变体，限制了其优化效率。", "method": "我们提出了AC-Refiner，一个利用条件扩散模型的新型算术电路优化框架。关键在于将算术电路合成重构为条件图像生成任务。通过仔细地根据目标质量结果（QoRs）来调节去噪扩散过程，AC-Refiner持续生成高质量的电路设计。此外，探索的设计用于微调扩散模型，这使得探索集中在帕累托前沿附近。", "result": "实验结果表明，AC-Refiner生成的电路设计具有卓越的帕累托最优性，优于最先进的基线方法。通过将AC-Refiner集成到实际应用中，性能提升得到了进一步验证。", "conclusion": "AC-Refiner通过将算术电路优化重构为条件图像生成任务，并利用条件扩散模型和模型微调，成功地解决了算术电路优化中的挑战，生成了具有卓越帕累托最优性的设计。", "translation": "算术电路，例如加法器和乘法器，是数字系统的基本组成部分，直接影响性能、功耗和面积。然而，由于庞大的设计空间和复杂的物理约束，优化这些电路仍然具有挑战性。虽然最近基于深度学习的方法显示出潜力，但它们难以持续探索高潜力设计变体，限制了其优化效率。为了解决这一挑战，我们提出了AC-Refiner，一个利用条件扩散模型的新型算术电路优化框架。我们的关键见解是将算术电路合成重构为条件图像生成任务。通过仔细地根据目标质量结果（QoRs）来调节去噪扩散过程，AC-Refiner持续生成高质量的电路设计。此外，探索的设计用于微调扩散模型，这使得探索集中在帕累托前沿附近。实验结果表明，AC-Refiner生成的电路设计具有卓越的帕累托最优性，优于最先进的基线方法。通过将AC-Refiner集成到实际应用中，性能提升得到了进一步验证。", "summary": "AC-Refiner是一个创新的算术电路优化框架，它将电路合成视为条件图像生成问题，并利用条件扩散模型进行优化。该方法通过根据目标质量结果调节去噪过程，并用探索的设计微调模型，从而高效地生成高质量的电路设计，并使探索集中在帕累托前沿。实验证明，AC-Refiner在帕累托最优性方面超越了现有技术，并在实际应用中验证了其性能提升。", "keywords": "算术电路优化, 条件扩散模型, 图像生成, 帕累托最优, 深度学习", "comments": "该论文的创新点在于将算术电路优化这一复杂的结构生成问题巧妙地转化为了条件图像生成任务，并利用条件扩散模型进行求解。这种跨领域的建模方法具有很高的启发性。通过对扩散模型进行微调，使其在帕累托前沿附近进行探索，进一步提高了优化效率和设计质量。这项工作为硬件设计自动化领域带来了新的视角和强大的工具。"}}
{"id": "2507.02295", "title": "Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources", "authors": ["Roopkatha Banerjee", "Prince Modi", "Jinal Vyas", "Chunduru Sri Abhijit", "Tejus Chandrashekar", "Harsha Varun Marisetty", "Manik Gupta", "Yogesh Simmhan"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02295v1", "summary": "With the recent improvements in mobile and edge computing and rising concerns\nof data privacy, Federated Learning(FL) has rapidly gained popularity as a\nprivacy-preserving, distributed machine learning methodology. Several FL\nframeworks have been built for testing novel FL strategies. However, most focus\non validating the learning aspects of FL through pseudo-distributed simulation\nbut not for deploying on real edge hardware in a distributed manner to\nmeaningfully evaluate the federated aspects from a systems perspective. Current\nframeworks are also inherently not designed to support asynchronous\naggregation, which is gaining popularity, and have limited resilience to client\nand server failures. We introduce Flotilla, a scalable and lightweight FL\nframework. It adopts a ``user-first'' modular design to help rapidly compose\nvarious synchronous and asynchronous FL strategies while being agnostic to the\nDNN architecture. It uses stateless clients and a server design that separates\nout the session state, which are periodically or incrementally checkpointed. We\ndemonstrate the modularity of Flotilla by evaluating five different FL\nstrategies for training five DNN models. We also evaluate the client and\nserver-side fault tolerance on 200+ clients, and showcase its ability to\nrapidly failover within seconds. Finally, we show that Flotilla's resource\nusage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or\nbetter than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It\nalso scales significantly better compared to Flower for 1000+ clients. This\npositions Flotilla as a competitive candidate to build novel FL strategies on,\ncompare them uniformly, rapidly deploy them, and perform systems research and\noptimizations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02295v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Flotilla：一个用于异构资源的可扩展、模块化和弹性联邦学习框架", "tldr": "Flotilla是一个新的联邦学习框架，它解决了现有框架在真实边缘硬件部署、异步聚合和故障恢复方面的不足，具有可扩展性、模块化设计和强大的容错能力。", "motivation": "现有的联邦学习框架主要侧重于通过伪分布式模拟验证FL的学习方面，而不是在分布式方式下部署到真实边缘硬件上以从系统角度评估联邦方面。此外，当前框架未设计支持日益流行的异步聚合，并且对客户端和服务器故障的弹性有限。", "method": "引入Flotilla，一个可扩展且轻量级的FL框架。它采用“用户优先”的模块化设计，以帮助快速组合各种同步和异步FL策略，同时与DNN架构无关。它使用无状态客户端和分离会话状态的服务器设计，会话状态定期或增量检查点。", "result": "1. 通过评估五种不同的FL策略来训练五种DNN模型，展示了Flotilla的模块化。2. 在200多个客户端上评估了客户端和服务器端的容错能力，并展示了其在几秒内快速故障转移的能力。3. Flotilla在Raspberry Pis和Nvidia Jetson边缘加速器上的资源使用与Flower、OpenFL和FedML这三个最先进的FL框架相当或更好。4. 在1000多个客户端的情况下，它比Flower具有显著更好的扩展性。", "conclusion": "Flotilla能够成为构建新型FL策略、统一比较它们、快速部署它们以及进行系统研究和优化的有竞争力候选者。", "translation": "随着移动和边缘计算的最新改进以及数据隐私问题的日益突出，联邦学习（FL）作为一种保护隐私的分布式机器学习方法迅速普及。已经构建了几个FL框架用于测试新颖的FL策略。然而，大多数框架侧重于通过伪分布式模拟验证FL的学习方面，而不是在分布式方式下部署到真实边缘硬件上，以从系统角度有意义地评估联邦方面。当前的框架本质上也没有设计来支持日益流行的异步聚合，并且对客户端和服务器故障的弹性有限。我们引入了Flotilla，一个可扩展且轻量级的FL框架。它采用“用户优先”的模块化设计，以帮助快速组合各种同步和异步FL策略，同时与DNN架构无关。它使用无状态客户端和分离会话状态的服务器设计，这些状态定期或增量检查点。我们通过评估五种不同的FL策略来训练五种DNN模型，展示了Flotilla的模块化。我们还在200多个客户端上评估了客户端和服务器端的容错能力，并展示了其在几秒内快速故障转移的能力。最后，我们表明Flotilla在Raspberry Pis和Nvidia Jetson边缘加速器上的资源使用与三个最先进的FL框架（Flower、OpenFL和FedML）相当或更好。与Flower相比，它在1000多个客户端的情况下也具有显著更好的扩展性。这使得Flotilla成为构建新型FL策略、统一比较它们、快速部署它们以及进行系统研究和优化的有竞争力候选者。", "summary": "Flotilla是一个为解决现有联邦学习（FL）框架在真实边缘部署、异步聚合支持和故障恢复能力方面的不足而设计的新型可扩展、模块化和弹性框架。它采用“用户优先”的模块化设计，支持同步和异步FL策略，并使用无状态客户端和分离会话状态的服务器设计以提高鲁棒性。实验证明，Flotilla具有高度模块化、强大的客户端和服务器容错能力，并且在资源使用和可扩展性方面优于或媲美现有最先进的FL框架，使其成为开发和部署新型FL策略的理想选择。", "keywords": "联邦学习, 可扩展性, 模块化, 容错性, 边缘计算", "comments": "Flotilla的创新之处在于其“用户优先”的模块化设计，允许灵活组合FL策略，以及对异步聚合和高容错性的原生支持，这弥补了现有框架的显著不足。其在真实边缘硬件上的表现以及与现有框架的对比评估，证明了其在实际部署中的潜力和重要性。该框架的出现有望加速联邦学习在真实世界应用中的落地和系统层面的研究。"}}
{"id": "2507.01963", "title": "A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem", "authors": ["Alberto Maria Mongardini", "Alessandro Mei"], "categories": ["q-fin.TR", "cs.CY", "q-fin.ST"], "primary_category": "Subjects:       Trading and Market Microstructure (q-fin.TR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01963v1", "summary": "From viral jokes to a billion-dollar phenomenon, meme coins have become one\nof the most popular segments in cryptocurrency markets. Unlike utility-focused\ncrypto assets like Bitcoin or Ethereum, meme coins derive value primarily from\ncommunity sentiment, making them vulnerable to manipulation. This study\npresents a cross-chain analysis of the meme coin ecosystem, examining 34,988\ntokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize the\ntokenomics of meme coins and track their growth in a three-month longitudinal\nanalysis. We discover that among high-return tokens (>100%), an alarming 82.6%\nshow evidence of extensive use of artificial growth strategies designed to\ncreate a misleading appearance of market interest. These include wash trading\nand a form of manipulation we define as Liquidity Pool-Based Price Inflation\n(LPI), where small strategic purchases trigger dramatic price increases. We\nalso find evidence of schemes designed to profit at the expense of investors,\nsuch as pump and dumps and rug pulls. In particular, most of the tokens\ninvolved had previously experienced wash trading or LPI, indicating how initial\nmanipulations often set the stage for later exploitation. These findings reveal\nthat manipulations are widespread among high-performing meme coins and suggest\nthat their dramatic gains are often likely driven by coordinated efforts rather\nthan natural market dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01963v1", "cate": "q-fin.TR", "date": "2025-04-16", "updated": "2025-04-16", "AI": {"title_translation": "仲夏迷因之梦：调查迷因币生态系统中的市场操纵", "tldr": "本研究对迷因币生态系统进行了跨链分析，发现高回报迷因币中普遍存在市场操纵，如刷量交易和基于流动性池的价格膨胀，这些操纵往往为后续的剥削（如拉高出货和跑路）奠定基础。", "motivation": "迷因币在加密货币市场中日益流行，但其价值主要来源于社区情绪，使其极易受到操纵。本研究旨在调查迷因币生态系统中的市场操纵行为。", "method": "本研究对以太坊、BNB智能链、Solana和Base上34,988个迷因币进行了跨链分析，并进行了为期三个月的纵向分析，以表征其代币经济学并追踪其增长。", "result": "在高回报代币（回报率>100%）中，82.6%显示出广泛使用人工增长策略的证据，这些策略旨在制造虚假的市场兴趣，包括刷量交易和流动性池价格膨胀（LPI）。研究还发现旨在以投资者为代价获利的方案，如拉高出货（pump and dumps）和跑路（rug pulls）。大多数涉及这些方案的代币此前都经历过刷量交易或LPI。", "conclusion": "迷因币中存在广泛的市场操纵行为，高回报往往是由协调一致的努力而非自然市场动态驱动。最初的操纵往往为后续的剥削奠定基础。", "translation": "从病毒式笑话到数十亿美元的现象，迷因币已成为加密货币市场中最受欢迎的细分领域之一。与比特币或以太坊等注重实用性的加密资产不同，迷因币的价值主要来源于社区情绪，这使其容易受到操纵。本研究对迷因币生态系统进行了跨链分析，检查了以太坊、BNB智能链、Solana和Base上34,988个代币。我们描述了迷因币的代币经济学，并在为期三个月的纵向分析中追踪了它们的增长。我们发现，在高回报代币（>100%）中，惊人的82.6%显示出广泛使用人工增长策略的证据，这些策略旨在制造误导性的市场兴趣。这包括刷量交易和我们定义为基于流动性池的价格膨胀（LPI）的一种操纵形式，其中小额战略性购买会引发剧烈的价格上涨。我们还发现了旨在以牺牲投资者为代价获利的方案的证据，例如拉高出货和跑路。特别是，大多数涉及的代币此前都经历过刷量交易或LPI，这表明最初的操纵往往如何为后来的剥削奠定基础。这些发现表明，操纵在高绩效迷因币中普遍存在，并表明它们的巨大收益往往可能是由协调一致的努力而非自然市场动态驱动。", "summary": "本研究对迷因币生态系统进行了首次大规模跨链分析，揭示了其市场中普遍存在的操纵行为。通过分析四大区块链上近3.5万个迷因币，研究发现绝大多数高回报迷因币（82.6%）利用刷量交易和新型的流动性池价格膨胀（LPI）等人工增长策略，制造虚假的市场繁荣。这些初始操纵常为后续的拉高出货和跑路等剥削行为铺平道路，表明迷因币的巨额收益往往源于协调的市场操纵而非自然增长。", "keywords": "迷因币, 市场操纵, 刷量交易, 流动性池价格膨胀, 加密货币", "comments": "该研究首次对迷因币市场的操纵行为进行了大规模的跨链分析，填补了该领域研究的空白。其创新之处在于定义了“流动性池价格膨胀”（LPI）这一新型操纵手段，并揭示了初始操纵与后续剥削之间的关联。研究的重要性在于揭示了迷因币市场的高风险性和潜在欺诈性质，对投资者和监管机构具有重要的警示意义。"}}
{"id": "2507.02083", "title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "authors": ["Haonan Duan", "Stephen Zhewen Lu", "Caitlin Fiona Harrigan", "Nishkrit Desai", "Jiarui Lu", "Michał Koziarski", "Leonardo Cotta", "Chris J. Maddison"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02083v1", "summary": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02083v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "使用系统生物学干实验室测量语言模型的科学能力", "tldr": "引入SciGym基准，通过系统生物学干实验室评估LLM在开放式科学发现任务中迭代实验设计和分析能力，发现LLM在复杂系统上的表现显著下降。", "motivation": "当前评估大型语言模型（LLMs）科学能力的方法未能测试其核心科学能力（实验设计和结果解释），因为湿实验室实验成本过高。因此需要一种新的、经济高效的基准来评估LLMs的这些能力。", "method": "研究引入了SciGym，这是一个首创的基准，通过运行生物系统的干实验室来克服湿实验室成本。这些生物系统模型以系统生物学标记语言（SBML）编码，能够高效生成模拟数据。研究使用SciGym评估了六个前沿LLM在137个小型系统上的表现，并发布了总共350个系统。", "result": "评估结果显示，能力更强的模型表现更优异，但所有模型的性能都随着系统复杂性的增加而显著下降。", "conclusion": "LLM代理的科学能力仍有很大的提升空间。", "translation": "设计实验和解释结果是核心科学能力，尤其是在生物学领域，研究人员通过扰动复杂系统来揭示其底层机制。近期评估大型语言模型（LLMs）科学能力的努力未能测试这些能力，因为湿实验室实验在专业知识、时间和设备方面成本过高。我们引入了SciGym，这是一个首创的基准，用于评估LLMs在开放式科学发现任务中迭代实验设计和分析能力。SciGym通过运行生物系统的干实验室克服了湿实验室成本的挑战。这些模型以系统生物学标记语言编码，可以高效生成模拟数据，使其成为在真实复杂系统上进行实验的理想测试平台。我们评估了六个前沿LLM在137个小型系统上的表现，并发布了总共350个系统。我们的评估表明，虽然能力更强的模型表现出卓越的性能，但所有模型的性能都随着系统复杂性的增加而显著下降，这表明LLM代理的科学能力仍有很大的提升空间。", "summary": "本研究提出了SciGym，一个用于评估大型语言模型（LLMs）在科学发现任务中实验设计和分析能力的基准。SciGym通过模拟生物系统的干实验室来规避高昂的湿实验室成本，利用系统生物学标记语言（SBML）编码的模型生成模拟数据。对六个前沿LLMs的评估显示，尽管更强的模型表现更好，但所有模型在系统复杂性增加时性能显著下降，表明LLMs的科学能力仍需大幅提升。", "keywords": "语言模型, 科学能力, 系统生物学, 干实验室, SciGym", "comments": "SciGym的创新之处在于其利用系统生物学干实验室模拟真实复杂系统，为评估LLM的科学发现能力提供了一个经济高效且可扩展的平台。这项工作的重要性体现在它解决了现有LLM科学能力评估中缺乏真实实验设计和分析测试的问题。研究结果揭示了当前LLM在处理复杂科学问题时的局限性，为未来LLM在科学领域的发展指明了方向。"}}
{"id": "2507.02205", "title": "Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach", "authors": ["Elena Ryumina", "Maxim Markitantov", "Alexandr Axyonov", "Dmitry Ryumin", "Mikhail Dolgushin", "Alexey Karpov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8", "url": "http://arxiv.org/abs/2507.02205v1", "summary": "Compound Expression Recognition (CER), a subfield of affective computing,\naims to detect complex emotional states formed by combinations of basic\nemotions. In this work, we present a novel zero-shot multimodal approach for\nCER that combines six heterogeneous modalities into a single pipeline: static\nand dynamic facial expressions, scene and label matching, scene context, audio,\nand text. Unlike previous approaches relying on task-specific training data,\nour approach uses zero-shot components, including Contrastive Language-Image\nPretraining (CLIP)-based label matching and Qwen-VL for semantic scene\nunderstanding. We further introduce a Multi-Head Probability Fusion (MHPF)\nmodule that dynamically weights modality-specific predictions, followed by a\nCompound Expressions (CE) transformation module that uses Pair-Wise Probability\nAggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods\nto produce interpretable compound emotion outputs. Evaluated under multi-corpus\ntraining, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%\non Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via\nzero-shot testing, which is comparable to the results of supervised approaches\ntrained on target data. This demonstrates the effectiveness of the proposed\napproach for capturing CE without domain adaptation. The source code is\npublicly available.", "comment": "8", "pdf_url": "http://arxiv.org/pdf/2507.02205v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "第9届ABAW竞赛中的RAS团队：多模态复合表情识别方法", "tldr": "本文提出了一种新颖的零样本多模态方法，用于复合表情识别（CER），该方法结合了六种异构模态，并引入了MHPF和CE转换模块，其零样本测试结果与有监督方法相当，无需领域适应。", "motivation": "复合表情识别（CER）旨在检测由基本情绪组合形成的复杂情绪状态。与以往依赖特定任务训练数据的方法不同，本文旨在开发一种零样本多模态方法来解决这一问题。", "method": "本文提出了一种新颖的零样本多模态复合表情识别（CER）方法，该方法将静态和动态面部表情、场景和标签匹配、场景上下文、音频和文本六种异构模态整合到一个单一的管道中。该方法利用零样本组件，包括基于CLIP的标签匹配和Qwen-VL进行语义场景理解。此外，还引入了一个多头概率融合（MHPF）模块来动态加权模态特异性预测，以及一个复合表情（CE）转换模块，该模块使用成对概率聚合（PPA）和成对特征相似性聚合（PFSA）方法来生成可解释的复合情绪输出。", "result": "在多语料库训练下，所提出的方法在零样本测试中显示出以下F1分数：在AffWild2上为46.95%，在野外表演面部表情（AFEW）上为49.02%，在C-EXPR-DB上为34.85%。这些结果与在目标数据上训练的有监督方法的结果相当。", "conclusion": "本文提出的零样本多模态方法在复合表情识别方面表现出有效性，其性能可与有监督方法相媲美，并且无需进行领域适应即可捕获复合表情。", "translation": "复合表情识别（CER）是情感计算的一个子领域，旨在检测由基本情绪组合形成的复杂情绪状态。在这项工作中，我们提出了一种新颖的零样本多模态CER方法，该方法将六种异构模态整合到一个单一的管道中：静态和动态面部表情、场景和标签匹配、场景上下文、音频和文本。与以往依赖特定任务训练数据的方法不同，我们的方法使用零样本组件，包括基于对比语言-图像预训练（CLIP）的标签匹配和用于语义场景理解的Qwen-VL。我们进一步引入了一个多头概率融合（MHPF）模块，该模块动态加权模态特异性预测，随后是一个复合表情（CE）转换模块，该模块使用成对概率聚合（PPA）和成对特征相似性聚合（PFSA）方法来生成可解释的复合情绪输出。在多语料库训练下进行评估，所提出的方法在零样本测试中显示出在AffWild2上F1分数为46.95%，在野外表演面部表情（AFEW）上为49.02%，在C-EXPR-DB上为34.85%，这与在目标数据上训练的有监督方法的结果相当。这证明了所提出方法在无需领域适应的情况下捕获CE的有效性。源代码已公开可用。", "summary": "本文提出了一种用于复合表情识别（CER）的新颖零样本多模态方法。该方法整合了面部表情、场景、音频和文本等六种异构模态，并利用CLIP和Qwen-VL等零样本组件进行理解。通过引入多头概率融合（MHPF）模块和复合表情（CE）转换模块，该方法能够动态融合多模态预测并生成可解释的复合情绪输出。实验结果表明，在零样本测试条件下，该方法在多个数据集上的性能与有监督方法相当，证明了其在无需领域适应的情况下捕获复杂情绪的有效性。", "keywords": "复合表情识别, 零样本学习, 多模态融合, 情感计算, 深度学习", "comments": "本文的创新之处在于提出了一个新颖的零样本多模态框架，用于复合表情识别，这显著减少了对特定任务训练数据的依赖。通过结合多种异构模态和利用CLIP、Qwen-VL等先进的零样本组件，该方法展示了其在新颖场景下识别复杂情绪的潜力。MHPF和CE转换模块的设计也增强了模型的可解释性和性能。其在零样本设置下与有监督方法相媲美的结果，突显了该方法在实际应用中的重要性和泛化能力。"}}
{"id": "2507.02213", "title": "Beyond Interval MDPs: Tight and Efficient Abstractions of Stochastic Systems", "authors": ["Ibon Gracia", "Morteza Lahijanian"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02213v1", "summary": "This work addresses the general problem of control synthesis for\ncontinuous-space, discrete-time stochastic systems with probabilistic\nguarantees via finite abstractions. While established methods exist, they often\ntrade off accuracy for tractability. We propose a unified abstraction framework\nthat improves both the tightness of probabilistic guarantees and computational\nefficiency. First, we introduce multi-interval MDPs (MI-MDPs), a generalization\nof interval-valued MDPs (IMDPs), which allows multiple, possibly overlapping\nclusters of successor states. This results in tighter abstractions but with\nincreased computational complexity. To mitigate this, we further propose a\ngeneralized form of MDPs with set-valued transition probabilities (SMDPs),\nwhich model transitions as a fixed probability to a state cluster, followed by\na non-deterministic choice within the cluster, as a sound abstraction. We show\nthat control synthesis for MI-MDPs reduces to robust dynamic programming via\nlinear optimization, while SMDPs admit even more efficient synthesis algorithms\nthat avoid linear programming altogether. Theoretically, we prove that, given\nthe partitioning of the state and disturbance spaces, both MI-MDPs and SMDPs\nyield tighter probabilistic guarantees than IMDPs, and that SMDPs are tighter\nthan MI-MDPs. Extensive experiments across several benchmarks validate our\ntheoretical results and demonstrate that SMDPs achieve favorable trade-offs\namong tightness, memory usage, and computation time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02213v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "超越区间MDPs：随机系统的紧密高效抽象", "tldr": "本文提出了多区间MDPs (MI-MDPs)和带有集合值转移概率的MDPs (SMDPs)两种新的抽象框架，用于随机系统控制合成，它们比现有方法更紧密且更高效。", "motivation": "现有有限抽象方法在连续空间、离散时间随机系统控制合成中，通常在准确性和可处理性之间进行权衡，导致概率保证不够紧密或计算效率低。", "method": "本文引入了多区间MDPs (MI-MDPs)，泛化了区间值MDPs (IMDPs)，允许后继状态有多个可能重叠的簇，从而提供更紧密的抽象。为缓解MI-MDPs的计算复杂度，进一步提出了带有集合值转移概率的MDPs (SMDPs)，将转移建模为对状态簇的固定概率和簇内的非确定性选择。MI-MDPs的控制合成通过线性优化的鲁棒动态规划实现，而SMDPs则采用更高效的算法，完全避免了线性规划。", "result": "理论证明，给定状态和扰动空间的划分，MI-MDPs和SMDPs都比IMDPs产生更紧密的概率保证。此外，SMDPs被证明比MI-MDPs更紧密。广泛的实验验证了理论结果，并表明SMDPs在紧密性、内存使用和计算时间之间实现了有利的权衡。", "conclusion": "本文提出的MI-MDPs和SMDPs框架在随机系统控制合成中，提供了比现有IMDPs更紧密、更高效的抽象。特别是SMDPs在紧密性、内存使用和计算时间方面表现出优越的综合性能。", "translation": "这项工作通过有限抽象解决了连续空间、离散时间随机系统通过概率保证进行控制合成的普遍问题。虽然存在已建立的方法，但它们通常以精度换取可处理性。我们提出了一个统一的抽象框架，该框架提高了概率保证的紧密性和计算效率。首先，我们引入了多区间MDPs（MI-MDPs），它是区间值MDPs（IMDPs）的泛化，它允许后继状态有多个可能重叠的簇。这导致了更紧密的抽象，但计算复杂度增加。为了缓解这个问题，我们进一步提出了一种带有集合值转移概率的MDPs（SMDPs）的广义形式，它将转移建模为对状态簇的固定概率，然后是簇内的非确定性选择，作为一种健全的抽象。我们证明了MI-MDPs的控制合成可以简化为通过线性优化的鲁棒动态规划，而SMDPs则允许更高效的合成算法，完全避免了线性规划。理论上，我们证明，给定状态和扰动空间的划分，MI-MDPs和SMDPs都比IMDPs产生更紧密的概率保证，并且SMDPs比MI-MDPs更紧密。在多个基准上进行的广泛实验验证了我们的理论结果，并表明SMDPs在紧密性、内存使用和计算时间之间实现了有利的权衡。", "summary": "本文针对连续空间、离散时间随机系统通过有限抽象进行控制合成的问题，提出了两种新的马尔可夫决策过程（MDP）抽象框架：多区间MDPs（MI-MDPs）和集合值转移概率MDPs（SMDPs）。MI-MDPs是现有区间MDPs的泛化，能提供更紧密的概率保证，但计算复杂。SMDPs进一步优化，通过固定概率到状态簇和簇内非确定性选择来建模，实现更高的计算效率，并理论证明其比MI-MDPs和IMDPs提供更紧密的保证。实验结果证实了SMDPs在紧密性、内存和计算时间上的优越权衡。", "keywords": "随机系统, 控制合成, 马尔可夫决策过程, 抽象, 概率保证", "comments": "这项工作通过引入MI-MDPs和SMDPs，为随机系统的控制合成提供了更精确且更高效的抽象方法。其创新之处在于泛化了传统的区间MDPs，并进一步优化了计算效率，解决了现有方法在精度和可处理性之间的权衡问题。SMDPs的提出，特别是其避免线性规划的能力，对于实际应用具有重要意义。"}}
{"id": "2507.01998", "title": "Positive region preserved random sampling: an efficient feature selection method for massive data", "authors": ["Hexiang Bai", "Deyu Li", "Jiye Liang", "Yanhui Zhai"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01998v1", "summary": "Selecting relevant features is an important and necessary step for\nintelligent machines to maximize their chances of success. However, intelligent\nmachines generally have no enough computing resources when faced with huge\nvolume of data. This paper develops a new method based on sampling techniques\nand rough set theory to address the challenge of feature selection for massive\ndata. To this end, this paper proposes using the ratio of discernible object\npairs to all object pairs that should be distinguished to measure the\ndiscriminatory ability of a feature set. Based on this measure, a new feature\nselection method is proposed. This method constructs positive region preserved\nsamples from massive data to find a feature subset with high discriminatory\nability. Compared with other methods, the proposed method has two advantages.\nFirst, it is able to select a feature subset that can preserve the\ndiscriminatory ability of all the features of the target massive data set\nwithin an acceptable time on a personal computer. Second, the lower boundary of\nthe probability of the object pairs that can be discerned using the feature\nsubset selected in all object pairs that should be distinguished can be\nestimated before finding reducts. Furthermore, 11 data sets of different sizes\nwere used to validate the proposed method. The results show that approximate\nreducts can be found in a very short period of time, and the discriminatory\nability of the final reduct is larger than the estimated lower boundary.\nExperiments on four large-scale data sets also showed that an approximate\nreduct with high discriminatory ability can be obtained in reasonable time on a\npersonal computer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01998v1", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "正域保持随机抽样：一种高效的海量数据特征选择方法", "tldr": "一种基于抽样和粗糙集理论的海量数据高效特征选择方法，能够有效保持特征的区分能力。", "motivation": "智能机器在处理海量数据时通常计算资源不足，而特征选择是提高其成功率的重要步骤。", "method": "提出了一种新的判别能力度量方法，即可区分对象对与所有应区分对象对的比例。基于此度量，提出了一种新的特征选择方法，该方法从海量数据中构建正域保持样本，以找到具有高判别能力的特征子集。该方法还可以在找到约简之前估计可区分对象对概率的下限。", "result": "该方法能够在个人电脑上以可接受的时间选择出能保持目标海量数据集所有特征判别能力的特征子集。在找到约简之前，可以估计可区分对象对概率的下限。在11个不同大小的数据集上验证，结果显示能在很短时间内找到近似约简，且最终约简的判别能力大于估计的下限。在4个大规模数据集上的实验也表明，该方法能在个人电脑上以合理时间获得高判别能力的近似约简。", "conclusion": "本文提出的正域保持随机抽样方法是一种高效的海量数据特征选择方法，能够在个人电脑上以合理的时间找到具有高判别能力的近似约简。", "translation": "选择相关特征是智能机器最大化成功机会的重要且必要的步骤。然而，智能机器在面对海量数据时通常没有足够的计算资源。本文开发了一种基于抽样技术和粗糙集理论的新方法，以解决海量数据特征选择的挑战。为此，本文提出使用可区分对象对与所有应区分对象对的比例来衡量特征集的判别能力。基于此度量，提出了一种新的特征选择方法。该方法从海量数据中构建正域保持样本，以找到具有高判别能力的特征子集。与其他方法相比，所提出的方法具有两个优点。首先，它能够在个人电脑上以可接受的时间选择出能保持目标海量数据集所有特征判别能力的特征子集。其次，在找到约简之前，可以估计使用所选特征子集在所有应区分对象对中可区分对象对的概率下限。此外，本文使用了11个不同大小的数据集来验证所提出的方法。结果表明，可以在很短的时间内找到近似约简，并且最终约简的判别能力大于估计的下限。在四个大规模数据集上的实验也表明，可以在个人电脑上以合理的时间获得高判别能力的近似约简。", "summary": "本文提出了一种名为“正域保持随机抽样”的高效海量数据特征选择方法。该方法结合了抽样技术和粗糙集理论，旨在解决智能机器在处理海量数据时面临的计算资源不足问题。文章引入了一种新的特征集判别能力度量标准，并构建正域保持样本以发现高判别能力的特征子集。实验证明，该方法能够在个人电脑上以合理时间选择出保持原数据判别能力的特征子集，并能估计可区分对象对概率的下限，从而实现快速有效的特征选择。", "keywords": "特征选择, 海量数据, 粗糙集理论, 随机抽样, 判别能力", "comments": "本文通过将抽样技术与粗糙集理论（特别是正域保持）相结合，为海量数据特征选择提供了一种创新方法。其主要优势在于能够在个人电脑上高效处理数据，同时保持高判别能力，这对于实际应用至关重要。在找到约简之前能够估计可区分对象对概率的下限，增加了其理论和实践价值。在不同规模数据集上的验证也增强了其可信度。"}}
{"id": "2507.02348", "title": "Joint Radiation Power, Antenna Position, and Beamforming Optimization for Pinching-Antenna Systems with Motion Power Consumption", "authors": ["Yiming Xu", "Dongfang Xu", "Xianghao Yu", "Shenghui Song", "Zhiguo Ding", "Robert Schober"], "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.02348v1", "summary": "Pinching-antenna systems (PASS) have been recently proposed to improve the\nperformance of wireless networks by reconfiguring both the large-scale and\nsmall-scale channel conditions. However, existing studies ignore the physical\nconstraints of antenna placement and assume fixed antenna radiation power. To\nfill this research gap, this paper investigates the design of PASS taking into\naccount the motion power consumption of pinching-antennas (PAs) and the impact\nof adjustable antenna radiation power. To that end, we minimize the average\npower consumption for a given quality-of-service (QoS) requirement, by jointly\noptimizing the antenna positions, antenna radiation power ratios, and transmit\nbeamforming. To the best of the authors' knowledge, this is the first work to\nconsider radiation power optimization in PASS, which provides an additional\ndegree of freedom (DoF) for system design. The cases with both continuous and\ndiscrete antenna placement are considered, where the main challenge lies in the\nfact that the antenna positions affect both the magnitude and phase of the\nchannel coefficients of PASS, making system optimization very challenging. To\ntackle the resulting unique obstacles, an alternating direction method of\nmultipliers (ADMM)-based framework is proposed to solve the problem for\ncontinuous antenna movement, while its discrete counterpart is formulated as a\nmixed integer nonlinear programming (MINLP) problem and solved by the block\ncoordinate descent (BCD) method. Simulation results validate the performance\nenhancement achieved by incorporating PA movement power assumption and\nadjustable radiation power into PASS design, while also demonstrating the\nefficiency of the proposed optimization framework. The benefits of PASS over\nconventional multiple-input multiple-output (MIMO) systems in mitigating the\nlarge-scale path loss and inter-user interference is also revealed.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.02348v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "考虑运动功耗的夹缩天线系统中的联合辐射功率、天线位置和波束成形优化", "tldr": "本文首次在夹缩天线系统中考虑辐射功率优化，并联合优化天线位置、辐射功率和波束成形，以最小化功耗，同时考虑连续和离散天线部署，并提出了ADMM和BCD算法来解决问题，仿真结果验证了其性能提升和框架效率。", "motivation": "现有夹缩天线系统（PASS）研究忽略了天线放置的物理约束和固定的天线辐射功率，本研究旨在填补这一空白，考虑夹缩天线（PA）的运动功耗和可调天线辐射功率的影响。", "method": "本文通过联合优化天线位置、天线辐射功率比和发射波束成形来最小化给定QoS要求下的平均功耗。对于连续天线移动，提出了基于交替方向乘子法（ADMM）的框架。对于离散天线放置，问题被表述为混合整数非线性规划（MINLP）问题，并通过块坐标下降（BCD）方法求解。", "result": "仿真结果验证了将PA运动功耗假设和可调辐射功率纳入PASS设计所带来的性能提升，并展示了所提出优化框架的效率。此外，还揭示了PASS在减轻大规模路径损耗和用户间干扰方面优于传统MIMO系统的优势。", "conclusion": "论文成功地通过考虑夹缩天线运动功耗和可调辐射功率，并联合优化多方面参数，实现了夹缩天线系统的功耗最小化和性能提升，验证了其在复杂无线网络环境中的应用潜力。", "translation": "夹缩天线系统（PASS）最近被提出，通过重新配置大尺度和小尺度信道条件来改善无线网络性能。然而，现有研究忽略了天线放置的物理约束并假定固定的天线辐射功率。为了填补这一研究空白，本文研究了夹缩天线（PAs）的运动功耗和可调天线辐射功率的影响下的PASS设计。为此，我们通过联合优化天线位置、天线辐射功率比和发射波束成形，在给定服务质量（QoS）要求下最小化平均功耗。据作者所知，这是首次在PASS中考虑辐射功率优化，这为系统设计提供了额外的自由度（DoF）。论文考虑了连续和离散天线放置的情况，其中主要挑战在于天线位置会影响PASS信道系数的幅度和相位，这使得系统优化非常具有挑战性。为了解决由此产生的独特障碍，本文提出了一种基于交替方向乘子法（ADMM）的框架来解决连续天线移动问题，而其离散对应物被表述为混合整数非线性规划（MINLP）问题并通过块坐标下降（BCD）方法求解。仿真结果验证了将PA运动功耗假设和可调辐射功率纳入PASS设计所实现的性能增强，同时还展示了所提出优化框架的效率。PASS在减轻大规模路径损耗和用户间干扰方面优于传统多输入多输出（MIMO）系统的优势也得到了揭示。", "summary": "本文针对现有夹缩天线系统（PASS）研究中忽略天线放置物理约束和固定辐射功率的问题，首次在PASS设计中引入运动功耗和可调辐射功率。通过联合优化天线位置、辐射功率比和波束成形，旨在最小化系统平均功耗并满足QoS要求。针对连续和离散天线放置分别提出了基于ADMM和BCD的优化框架。仿真结果验证了所提方法能有效提升系统性能，并展现了PASS在减轻路径损耗和用户间干扰方面相对于MIMO系统的优势。", "keywords": "夹缩天线系统, 辐射功率优化, 天线位置优化, 波束成形, 运动功耗", "comments": "本文的创新之处在于首次将辐射功率优化引入夹缩天线系统（PASS）的设计中，并考虑了天线运动带来的功耗，这为系统设计提供了新的自由度，使优化更加贴近实际。同时，针对连续和离散两种复杂的天线部署场景，分别提出了有效的优化算法（ADMM和BCD），展现了扎实的理论基础和工程实践能力。这项工作对于未来可重构智能表面或移动天线系统的实际部署具有重要的指导意义。"}}
{"id": "2507.02411", "title": "3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices", "authors": ["Zhurong Chen", "Jinhua Chen", "Wei Zhuo", "Wufeng Xue", "Dong Ni"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.02411v1", "summary": "Echocardiography (echo) plays an indispensable role in the clinical practice\nof heart diseases. However, ultrasound imaging typically provides only\ntwo-dimensional (2D) cross-sectional images from a few specific views, making\nit challenging to interpret and inaccurate for estimation of clinical\nparameters like the volume of left ventricle (LV). 3D ultrasound imaging\nprovides an alternative for 3D quantification, but is still limited by the low\nspatial and temporal resolution and the highly demanding manual delineation.\n  To address these challenges, we propose an innovative framework for\nreconstructing personalized 3D heart anatomy from 2D echo slices that are\nfrequently used in clinical practice. Specifically, a novel 3D reconstruction\npipeline is designed, which alternatively optimizes between the 3D pose\nestimation of these 2D slices and the 3D integration of these slices using an\nimplicit neural network, progressively transforming a prior 3D heart shape into\na personalized 3D heart model.\n  We validate the method with two datasets. When six planes are used, the\nreconstructed 3D heart can lead to a significant improvement for LV volume\nestimation over the bi-plane method (error in percent: 1.98\\% VS. 20.24\\%). In\naddition, the whole reconstruction framework makes even an important\nbreakthrough that can estimate RV volume from 2D echo slices (with an error of\n5.75\\% ). This study provides a new way for personalized 3D structure and\nfunction analysis from cardiac ultrasound and is of great potential in clinical\npractice.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.02411v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于稀疏姿态无关二维超声心动图切片的3D心脏重建", "tldr": "该研究提出了一种从稀疏的二维超声心动图切片重建个性化三维心脏模型的新框架，显著提高了心室容积的估计精度。", "motivation": "超声心动图在心脏疾病诊断中至关重要，但传统的二维图像难以解释且在临床参数（如左心室容积）估计上不准确。三维超声成像虽可进行三维量化，但受限于低时空分辨率和高要求的手动描绘。为解决这些挑战，需要一种从二维超声切片重建个性化三维心脏解剖结构的方法。", "method": "本研究提出了一种创新的框架，用于从临床常用的二维超声心动图切片重建个性化三维心脏解剖结构。具体来说，设计了一个新颖的三维重建流程，它在二维切片的3D姿态估计和使用隐式神经网络对这些切片进行3D整合之间交替优化，从而逐步将先前的3D心脏形状转换为个性化的3D心脏模型。", "result": "该方法在两个数据集上进行了验证。当使用六个平面时，重建的三维心脏在左心室容积估计方面比双平面方法有显著改进（误差百分比：1.98% 对比 20.24%）。此外，整个重建框架甚至在从二维超声切片估计右心室容积方面取得了重要突破（误差为5.75%）。", "conclusion": "本研究为心脏超声的个性化三维结构和功能分析提供了一种新方法，在临床实践中具有巨大潜力。", "translation": "超声心动图（echo）在心脏疾病的临床实践中发挥着不可或缺的作用。然而，超声成像通常仅从少数特定视图提供二维（2D）横截面图像，这使得解释变得困难，并且在估计左心室（LV）容积等临床参数时不够准确。3D超声成像为3D量化提供了另一种选择，但仍受限于低空间和时间分辨率以及要求极高的手动描绘。\n为了解决这些挑战，我们提出了一种创新框架，用于从临床实践中常用的2D超声心动图切片重建个性化3D心脏解剖结构。具体来说，设计了一个新颖的3D重建流程，它在这些2D切片的3D姿态估计和使用隐式神经网络对这些切片进行3D整合之间交替优化，逐步将先前的3D心脏形状转换为个性化的3D心脏模型。\n我们用两个数据集验证了该方法。当使用六个平面时，重建的3D心脏在左心室容积估计方面比双平面方法有显著改进（误差百分比：1.98% 对比 20.24%）。此外，整个重建框架甚至在从2D超声切片估计右心室容积方面取得了重要突破（误差为5.75%）。这项研究为心脏超声的个性化3D结构和功能分析提供了一种新方法，在临床实践中具有巨大潜力。", "summary": "本研究提出了一种创新的框架，旨在从临床常用的稀疏二维超声心动图切片重建个性化的三维心脏模型。该方法通过交替优化二维切片的3D姿态估计和利用隐式神经网络进行3D整合，逐步将先前的3D心脏形状转化为个性化模型。实验结果表明，该框架显著提高了左心室和右心室容积的估计精度，为心脏超声的个性化三维结构和功能分析开辟了新途径，具有重要的临床应用潜力。", "keywords": "3D心脏重建, 超声心动图, 隐式神经网络, 左心室容积, 右心室容积", "comments": "该论文提出了一种创新的3D心脏重建框架，其核心创新在于能够从稀疏的、姿态无关的2D超声切片重建个性化3D心脏模型，并通过交替优化姿态估计和3D整合来逐步细化模型。这项工作显著提高了心室容积估计的准确性，尤其是在右心室容积估计方面取得了突破，这在临床实践中具有重要意义。该方法有望改善心脏疾病的诊断和管理，减少对手动描绘的依赖。"}}
{"id": "2507.02433", "title": "Numerical Linear Algebra in Linear Space", "authors": ["Yiping Liu", "Hoai-An Nguyen", "Junzhao Yang"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      52 pages, 0 figures", "url": "http://arxiv.org/abs/2507.02433v1", "summary": "We present a randomized linear-space solver for general linear systems\n$\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ with $\\mathbf{A} \\in \\mathbb{Z}^{n \\times\nn}$ and $\\mathbf{b} \\in \\mathbb{Z}^n$, without any assumption on the condition\nnumber of $\\mathbf{A}$. For matrices whose entries are bounded by\n$\\mathrm{poly}(n)$, the solver returns a $(1+\\epsilon)$-multiplicative\nentry-wise approximation to vector $\\mathbf{x} \\in \\mathbb{Q}^{n}$ using\n$\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ bit operations and $O(n\n\\log n)$ bits of working space (i.e., linear in the size of a vector), where\n$\\mathrm{nnz}$ denotes the number of nonzero entries. Our solver works for\nright-hand vector $\\mathbf{b}$ with entries up to $n^{O(n)}$. To our knowledge,\nthis is the first linear-space linear system solver over the rationals that\nruns in $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ time.\n  We also present several applications of our solver to numerical linear\nalgebra problems, for which we provide algorithms with efficient polynomial\nrunning time and near-linear space. In particular, we present results for\nlinear regression, linear programming, eigenvalues and eigenvectors, and\nsingular value decomposition.", "comment": "52 pages, 0 figures", "pdf_url": "http://arxiv.org/pdf/2507.02433v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "线性空间中的数值线性代数", "tldr": "提出了一个随机化的线性空间求解器，用于求解一般线性系统，在有理数域上首次实现了$\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$时间复杂度和线性空间。", "motivation": "现有方法可能对条件数有假设或空间效率不高。本文旨在提供一个在有理数域上无需条件数假设、且具有线性空间效率的线性系统求解器。", "method": "本文提出了一种随机化的线性空间求解器，用于解决一般线性系统 $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$，其中 $\\mathbf{A} \\in \\mathbb{Z}^{n \\times n}$ 且 $\\mathbf{b} \\in \\mathbb{Z}^n$。该方法无需对 $\\mathbf{A}$ 的条件数做出任何假设。", "result": "对于元素大小受限于 $\\mathrm{poly}(n)$ 的矩阵，该求解器能以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 比特操作和 $O(n \\log n)$ 比特工作空间（即向量大小的线性空间）提供向量 $\\mathbf{x}$ 的 $(1+\\epsilon)$-乘法逐项近似。该求解器还适用于右侧向量 $\\mathbf{b}$ 元素高达 $n^{O(n)}$ 的情况。据作者所知，这是首个在有理数域上以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 时间运行的线性空间线性系统求解器。此外，该求解器还应用于线性回归、线性规划、特征值和特征向量以及奇异值分解等数值线性代数问题，并提供了高效多项式运行时间和近线性空间的算法。", "conclusion": "本文成功开发并验证了一种创新的随机化线性空间求解器，该求解器在解决一般线性系统方面具有突破性的时间复杂度和空间效率，并能有效应用于多种数值线性代数问题，填补了现有技术在有理数域线性系统求解方面的空白。", "translation": "我们提出了一种用于求解一般线性系统 $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ 的随机线性空间求解器，其中 $\\mathbf{A} \\in \\mathbb{Z}^{n \\times n}$ 且 $\\mathbf{b} \\in \\mathbb{Z}^n$，且对 $\\mathbf{A}$ 的条件数没有任何假设。对于条目受 $\\mathrm{poly}(n)$ 限制的矩阵，该求解器返回向量 $\\mathbf{x} \\in \\mathbb{Q}^{n}$ 的 $(1+\\epsilon)$-乘法逐项近似，使用 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 比特操作和 $O(n \\log n)$ 比特工作空间（即向量大小的线性空间），其中 $\\mathrm{nnz}$ 表示非零条目数量。我们的求解器适用于条目高达 $n^{O(n)}$ 的右侧向量 $\\mathbf{b}$。据我们所知，这是第一个在有理数域上以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 时间运行的线性空间线性系统求解器。\n我们还介绍了该求解器在数值线性代数问题上的几个应用，我们为此提供了具有高效多项式运行时间和近线性空间的算法。特别是，我们展示了线性回归、线性规划、特征值和特征向量以及奇异值分解的结果。", "summary": "本文提出了一种新颖的随机化线性空间求解器，用于在有理数域上解决一般线性系统 $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$，且无需对矩阵条件数进行假设。该求解器在处理条目受限的矩阵时，能以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 的时间复杂度和线性的工作空间提供解的近似。这是首个实现此效率的线性空间求解器。此外，该方法还成功应用于多种数值线性代数问题，如线性回归、线性规划、特征值分解和SVD，均取得了高效的表现。", "keywords": "线性系统求解器, 线性空间, 随机化算法, 数值线性代数", "comments": "这项工作在数值线性代数领域具有重要意义，因为它首次提供了一个在有理数域上无需条件数假设、且具有突破性时间和空间效率的线性系统求解器。其创新点在于结合了随机化方法和线性空间设计，解决了传统方法在大规模问题上的局限性，并展示了广泛的应用潜力。"}}
{"id": "2507.02562", "title": "Multi-Utterance Speech Separation and Association Trained on Short Segments", "authors": ["Yuzhu Wang", "Archontis Politis", "Konstantinos Drossos", "Tuomas Virtanen"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages, accepted by WASPAA 2025", "url": "http://arxiv.org/abs/2507.02562v1", "summary": "Current deep neural network (DNN) based speech separation faces a fundamental\nchallenge -- while the models need to be trained on short segments due to\ncomputational constraints, real-world applications typically require processing\nsignificantly longer recordings with multiple utterances per speaker than seen\nduring training. In this paper, we investigate how existing approaches perform\nin this challenging scenario and propose a frequency-temporal recurrent neural\nnetwork (FTRNN) that effectively bridges this gap. Our FTRNN employs a\nfull-band module to model frequency dependencies within each time frame and a\nsub-band module that models temporal patterns in each frequency band. Despite\nbeing trained on short fixed-length segments of 10 s, our model demonstrates\nrobust separation when processing signals significantly longer than training\nsegments (21-121 s) and preserves speaker association across utterance gaps\nexceeding those seen during training. Unlike the conventional\nsegment-separation-stitch paradigm, our lightweight approach (0.9 M parameters)\nperforms inference on long audio without segmentation, eliminating segment\nboundary distortions while simplifying deployment. Experimental results\ndemonstrate the generalization ability of FTRNN for multi-utterance speech\nseparation and speaker association.", "comment": "5 pages, accepted by WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02562v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多语段语音分离与关联：基于短时长训练", "tldr": "本文提出了一种频率-时间循环神经网络（FTRNN），解决了深度神经网络语音分离中训练与实际应用时长不匹配的问题，实现了对长音频的鲁棒分离和说话人关联保持，且无需分段处理。", "motivation": "当前的深度神经网络（DNN）语音分离模型受限于计算资源，通常在短时长音频段上进行训练。然而，实际应用中需要处理的录音远长于训练时长，且包含每个说话人的多个语段，这导致了训练与实际应用之间的性能差距。", "method": "本文提出了一种频率-时间循环神经网络（FTRNN）来弥补训练与实际应用之间的时长差距。FTRNN包含一个全频带模块，用于建模每个时间帧内的频率依赖性；以及一个子频带模块，用于建模每个频带内的时序模式。该方法是一种轻量级模型（0.9 M参数），能够在不进行分段的情况下对长音频进行推理，从而避免了分段边界失真并简化了部署，这与传统的“分段-分离-拼接”范式不同。", "result": "尽管模型仅在10秒的固定长度短音频段上进行训练，但它在处理远长于训练时长（21-121秒）的信号时表现出鲁棒的分离能力，并且能够跨越训练中未见的语段间隙保持说话人关联。", "conclusion": "实验结果证明了FTRNN在多语段语音分离和说话人关联方面的泛化能力。", "translation": "当前基于深度神经网络（DNN）的语音分离面临一个根本性挑战——尽管模型由于计算限制需要在短时长语音段上进行训练，但实际应用通常需要处理比训练时所见的每个说话人包含更多语段的显著更长的录音。在本文中，我们研究了现有方法在这种挑战性场景中的表现，并提出了一种频率-时间循环神经网络（FTRNN），有效地弥补了这一差距。我们的FTRNN采用一个全频带模块来建模每个时间帧内的频率依赖性，以及一个子频带模块来建模每个频带内的时序模式。尽管在10秒的短固定长度语音段上进行训练，我们的模型在处理显著长于训练语音段（21-121秒）的信号时表现出鲁棒的分离能力，并能跨越超出训练时所见的语段间隙保持说话人关联。与传统的“分段-分离-拼接”范式不同，我们的轻量级方法（0.9 M参数）在不进行分段的情况下对长音频进行推理，消除了分段边界失真，同时简化了部署。实验结果证明了FTRNN在多语段语音分离和说话人关联方面的泛化能力。", "summary": "本文针对当前DNN语音分离模型在短时长训练与长时长实际应用之间的差距，提出了一种轻量级的频率-时间循环神经网络（FTRNN）。该模型通过结合全频带和子频带模块，实现了在短时训练后，对远超训练时长的多语段音频进行鲁棒分离和说话人关联保持，且无需进行分段处理，有效解决了传统方法的局限性。", "keywords": "语音分离, 多语段, 说话人关联, FTRNN, 深度学习", "comments": "本文的创新点在于提出了FTRNN架构，能够有效解决语音分离中训练时长与实际应用时长不匹配的问题，并在无需分段处理长音频的情况下保持说话人关联。其轻量级（0.9M参数）的特点也使其在部署上更具优势，避免了传统分段方法的边界失真问题，展示了良好的泛化能力。"}}
{"id": "2507.02097", "title": "The Future is Agentic: Definitions, Perspectives, and Open Challenges of Multi-Agent Recommender Systems", "authors": ["Reza Yousefi Maragheh", "Yashar Deldjoo"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02097v1", "summary": "Large language models (LLMs) are rapidly evolving from passive engines of\ntext generation into agentic entities that can plan, remember, invoke external\ntools, and co-operate with one another. This perspective paper investigates how\nsuch LLM agents (and societies thereof) can transform the design space of\nrecommender systems.\n  We introduce a unified formalism that (i) models an individual agent as a\ntuple comprising its language core, tool set, and hierarchical memory, and (ii)\ncaptures a multi-agent recommender as a triple of agents, shared environment,\nand communication protocol. Within this framework, we present four end-to-end\nuse cases-interactive party planning, synthetic user-simulation for offline\nevaluation, multi-modal furniture recommendation, and brand-aligned explanation\ngeneration-each illustrating a distinct capability unlocked by agentic\norchestration.\n  We then surface five cross-cutting challenge families: protocol complexity,\nscalability, hallucination and error propagation, emergent misalignment\n(including covert collusion), and brand compliance.\n  For each, we formalize the problem, review nascent mitigation strategies, and\noutline open research questions. The result is both a blueprint and an agenda:\na blueprint that shows how memory-augmented, tool-using LLM agents can be\ncomposed into robust recommendation pipelines, and an agenda inviting the\nRecSys community to develop benchmarks, theoretical guarantees, and governance\ntools that keep pace with this new degree of autonomy. By unifying agentic\nabstractions with recommender objectives, the paper lays the groundwork for the\nnext generation of personalized, trustworthy, and context-rich recommendation\nservices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02097v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "未来是智能体的：多智能体推荐系统的定义、视角和开放挑战", "tldr": "本文探讨了大型语言模型（LLM）智能体如何革新推荐系统设计，提出了一个统一的形式化框架，并识别了多智能体推荐系统中的关键挑战与开放研究问题，为下一代推荐服务奠定基础。", "motivation": "大型语言模型（LLM）正从被动的文本生成引擎演变为能够规划、记忆、调用外部工具并相互协作的智能体。本文旨在探讨这些LLM智能体（及其社会）如何改变推荐系统的设计空间。", "method": "本文提出了一个统一的形式化框架：将单个智能体建模为包含语言核心、工具集和分层记忆的元组；将多智能体推荐系统建模为智能体、共享环境和通信协议的三元组。在此框架内，文章展示了四个端到端用例（交互式聚会规划、离线评估的合成用户模拟、多模态家具推荐、品牌对齐的解释生成），以说明智能体协调解锁的不同能力。随后，文章提出了五个交叉挑战族：协议复杂性、可扩展性、幻觉和错误传播、新兴错位（包括隐蔽串通）以及品牌合规性，并对每个问题进行了形式化，回顾了新兴的缓解策略，并概述了开放的研究问题。", "result": "本文提供了一个蓝图，展示了如何将记忆增强、工具使用的LLM智能体组合成强大的推荐管道；同时提出了一个议程，邀请推荐系统社区开发基准、理论保证和治理工具，以适应这种新的自主程度。", "conclusion": "通过将智能体抽象与推荐目标相结合，本文为下一代个性化、可信赖和上下文丰富的推荐服务奠定了基础。", "translation": "大型语言模型（LLM）正在从被动的文本生成引擎迅速演变为能够规划、记忆、调用外部工具并相互协作的智能体实体。这篇透视论文研究了这些LLM智能体（及其社会）如何改变推荐系统的设计空间。\n我们引入了一个统一的形式化框架，它（i）将单个智能体建模为一个包含其语言核心、工具集和分层记忆的元组，以及（ii）将多智能体推荐系统捕获为智能体、共享环境和通信协议的三元组。在这个框架内，我们提出了四个端到端用例——交互式聚会规划、用于离线评估的合成用户模拟、多模态家具推荐和品牌对齐的解释生成——每个都说明了智能体编排解锁的独特能力。\n然后，我们提出了五个交叉挑战族：协议复杂性、可扩展性、幻觉和错误传播、新兴错位（包括隐蔽串通）以及品牌合规性。\n对于每个挑战，我们都对其问题进行了形式化，回顾了新兴的缓解策略，并概述了开放的研究问题。结果既是一个蓝图也是一个议程：一个蓝图展示了如何将记忆增强、工具使用的LLM智能体组合成强大的推荐管道，以及一个邀请推荐系统社区开发基准、理论保证和治理工具以跟上这种新程度自主性的议程。通过将智能体抽象与推荐目标相结合，本文为下一代个性化、可信赖和上下文丰富的推荐服务奠定了基础。", "summary": "本文探讨了大型语言模型（LLM）向智能体实体演进如何革新推荐系统。作者提出了一个统一的形式化框架，将单个智能体定义为语言核心、工具集和分层记忆的组合，并将多智能体推荐系统定义为智能体、共享环境和通信协议的集合。文章通过四个具体用例展示了智能体编排的能力，并识别了多智能体推荐系统面临的五大挑战：协议复杂性、可扩展性、幻觉与错误传播、新兴错位和品牌合规性。文章不仅提供了构建强大推荐管道的蓝图，也为推荐系统社区提出了开发相关基准、理论保障和治理工具的议程，旨在为下一代个性化、可信赖和上下文丰富的推荐服务奠定基础。", "keywords": "多智能体推荐系统, LLM智能体, 推荐系统, 智能体AI, 挑战", "comments": "本文极具前瞻性，将当前热门的LLM智能体概念与推荐系统相结合，为该领域开辟了新的研究方向。其提出的统一形式化框架和识别的关键挑战，为未来研究提供了清晰的路线图。特别是对“新兴错位”和“品牌合规性”等社会和伦理问题的关注，显示了作者对技术发展潜在风险的深刻理解。该论文不仅具有理论意义，也为实际应用提供了潜在的指导。"}}
{"id": "2507.02803", "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars", "authors": ["Gent Serifi", "Marcel C. Bühler"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.02803v1", "summary": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.", "comment": "Project page: https://gserifi.github.io/HyperGaussians", "pdf_url": "http://arxiv.org/pdf/2507.02803v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "HyperGaussians：用于高保真可动画面部化身的高维高斯泼溅", "tldr": "本文引入HyperGaussians，一种扩展3D高斯泼溅的新方法，用于创建高质量可动画面部化身，通过高维高斯和逆协方差技巧提升表达能力和效率。", "motivation": "从视频创建详细的面部化身是一个挑战性问题，现有技术（如3DGS）在处理非线性变形、复杂光照和精细细节方面仍有不足，导致可动画面部化身仍处于“恐怖谷”效应。", "method": "提出HyperGaussians，将3D高斯扩展到高维多元高斯，通过条件作用于可学习的局部嵌入来增加表达能力。为解决高维协方差矩阵求逆的计算昂贵问题，引入“逆协方差技巧”重新参数化协方差矩阵，提高效率。将HyperGaussians集成到现有模型（如FlashAvatar）中进行演示。", "result": "在19个受试者和4个面部数据集上的评估表明，HyperGaussians在数值和视觉上均优于3DGS，特别是在眼镜框、牙齿、复杂面部动作和镜面反射等高频细节方面表现更好。", "conclusion": "HyperGaussians通过提升高斯表示的表达能力和计算效率，显著提高了从单目视频创建高保真可动画面部化身的能力，超越了现有技术。", "translation": "我们引入了HyperGaussians，这是3D高斯泼溅的一种新颖扩展，用于创建高质量的可动画面部化身。从视频创建如此详细的面部化身是一个具有挑战性的问题，在增强现实和虚拟现实中有大量应用。尽管静态面部取得了巨大成功，但从单目视频创建可动画化身仍处于“恐怖谷”效应。事实标准3D高斯泼溅（3DGS）通过一组3D高斯基元来表示面部。3DGS在渲染静态面部方面表现出色，但最先进的技术在非线性变形、复杂光照效果和精细细节方面仍然存在困难。虽然大多数相关工作侧重于从表情代码预测更好的高斯参数，但我们重新思考了3D高斯表示本身以及如何使其更具表达力。我们的见解导致了3D高斯向高维多元高斯的新颖扩展，被称为“HyperGaussians”。更高的维度通过条件作用于可学习的局部嵌入来增加表达能力。然而，泼溅HyperGaussians计算成本很高，因为它需要反转高维协方差矩阵。我们通过重新参数化协方差矩阵来解决这个问题，这被称为“逆协方差技巧”。这种技巧提高了效率，使得HyperGaussians可以无缝集成到现有模型中。为了证明这一点，我们将HyperGaussians插入到快速单目面部化身的最先进技术FlashAvatar中。我们对来自4个面部数据集的19名受试者进行的评估表明，HyperGaussians在数值和视觉上均优于3DGS，特别是在眼镜框、牙齿、复杂面部动作和镜面反射等高频细节方面。", "summary": "本文提出HyperGaussians，一种将3D高斯泼溅扩展到高维多元高斯的新方法，旨在解决现有技术在创建高保真可动画面部化身时面临的非线性变形、复杂光照和精细细节表现不足的问题。通过引入可学习的局部嵌入增强表达能力，并利用“逆协方差技巧”解决高维计算效率问题，HyperGaussians能够无缝集成到现有模型中。实验结果表明，HyperGaussians在生成高频细节方面显著优于3DGS，为可动画面部化身领域带来了进步。", "keywords": "高斯泼溅, 可动画面部化身, 高维高斯, 逆协方差技巧, 3DGS", "comments": "本文的创新点在于重新思考了3D高斯表示的本质，将其扩展到高维空间以增强表达能力，并巧妙地通过“逆协方差技巧”解决了由此带来的计算效率问题。这使得HyperGaussians能够更好地捕捉复杂的面部变形和高频细节，对于推动可动画面部化身技术从“恐怖谷”走向更高保真度具有重要意义。"}}
{"id": "2507.02372", "title": "An Experimental Approach for Running-Time Estimation of Multi-objective Evolutionary Algorithms in Numerical Optimization", "authors": ["Han Huang", "Tianyu Wang", "Chaoda Peng", "Tongli He", "Zhifeng Hao"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02372v1", "summary": "Multi-objective evolutionary algorithms (MOEAs) have become essential tools\nfor solving multi-objective optimization problems (MOPs), making their running\ntime analysis crucial for assessing algorithmic efficiency and guiding\npractical applications. While significant theoretical advances have been\nachieved for combinatorial optimization, existing studies for numerical\noptimization primarily rely on algorithmic or problem simplifications, limiting\ntheir applicability to real-world scenarios. To address this gap, we propose an\nexperimental approach for estimating upper bounds on the running time of MOEAs\nin numerical optimization without simplification assumptions. Our approach\nemploys an average gain model that characterizes algorithmic progress through\nthe Inverted Generational Distance metric. To handle the stochastic nature of\nMOEAs, we use statistical methods to estimate the probabilistic distribution of\ngains. Recognizing that gain distributions in numerical optimization exhibit\nirregular patterns with varying densities across different regions, we\nintroduce an adaptive sampling method that dynamically adjusts sampling density\nto ensure accurate surface fitting for running time estimation. We conduct\ncomprehensive experiments on five representative MOEAs (NSGA-II, MOEA/D,\nAR-MOEA, AGEMOEA-II, and PREA) using the ZDT and DTLZ benchmark suites. The\nresults demonstrate the effectiveness of our approach in estimating upper\nbounds on the running time without requiring algorithmic or problem\nsimplifications. Additionally, we provide a web-based implementation to\nfacilitate broader adoption of our methodology. This work provides a practical\ncomplement to theoretical research on MOEAs in numerical optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02372v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多目标进化算法在数值优化中运行时间估计的实验方法", "tldr": "本文提出一种实验方法，在不简化假设的情况下，估计数值优化中多目标进化算法运行时间的上限。", "motivation": "现有针对数值优化中多目标进化算法（MOEAs）运行时间的研究主要依赖于算法或问题简化，限制了其在实际场景中的适用性。本研究旨在弥补这一空白。", "method": "本文提出一种实验方法，用于在不进行简化假设的情况下，估计数值优化中多目标进化算法（MOEAs）运行时间的上限。该方法采用平均增益模型，通过反向世代距离（Inverted Generational Distance）度量算法进展，并使用统计方法估计增益的概率分布。此外，为处理不规则增益分布，引入了自适应采样方法，动态调整采样密度以确保运行时间估计的准确曲面拟合。", "result": "实验结果表明，本文提出的方法在不要求算法或问题简化的前提下，能有效估计多目标进化算法的运行时间上限。此外，研究还提供了基于网络的实现，以促进该方法的广泛采用。", "conclusion": "这项工作为数值优化中多目标进化算法的理论研究提供了实用的补充。", "translation": "多目标进化算法（MOEAs）已成为解决多目标优化问题（MOPs）的重要工具，因此对其运行时间分析对于评估算法效率和指导实际应用至关重要。尽管组合优化领域已取得显著的理论进展，但数值优化领域的现有研究主要依赖于算法或问题简化，限制了其在实际场景中的适用性。为了解决这一空白，我们提出了一种实验方法，用于在不进行简化假设的情况下，估计数值优化中MOEAs运行时间的上限。我们的方法采用平均增益模型，通过反向世代距离度量来表征算法进展。为了处理MOEAs的随机性，我们使用统计方法来估计增益的概率分布。认识到数值优化中的增益分布呈现出不规则模式，在不同区域具有不同的密度，我们引入了一种自适应采样方法，动态调整采样密度，以确保运行时间估计的准确曲面拟合。我们使用ZDT和DTLZ基准套件对五种代表性MOEAs（NSGA-II、MOEA/D、AR-MOEA、AGEMOEA-II和PREA）进行了综合实验。结果表明，我们的方法在不要求算法或问题简化的前提下，能有效估计运行时间上限。此外，我们提供了一个基于网络的实现，以促进我们方法的更广泛采用。这项工作为数值优化中MOEAs的理论研究提供了实用的补充。", "summary": "本文提出一种新颖的实验方法，旨在无需简化假设的情况下，估计数值优化中多目标进化算法（MOEAs）的运行时间上限。该方法利用平均增益模型结合反向世代距离度量，并通过统计方法处理MOEAs的随机性，同时引入自适应采样策略以应对复杂增益分布。在多个代表性MOEAs和基准测试上的实验验证了其有效性，并提供了在线实现，为MOEAs的理论研究提供了实用的补充。", "keywords": "多目标进化算法, 运行时间估计, 数值优化, 自适应采样, 反向世代距离", "comments": "这项工作通过提出一种不依赖于简化假设的实验方法，填补了数值优化领域中多目标进化算法运行时间估计的空白。其创新点在于引入平均增益模型、统计方法处理随机性以及自适应采样策略。提供基于网络的实现也大大增强了其实用性和推广性，为理论研究提供了重要的实践补充。"}}
{"id": "2507.02221", "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 1 figure, 7 tables", "url": "http://arxiv.org/abs/2507.02221v1", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "comment": "11 pages, 1 figure, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.02221v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "GDC队列副驾驶：一个用于从基因组数据共享库中整理队列的AI副驾驶", "tldr": "GDC Cohort Copilot是一个AI工具，旨在帮助用户通过自然语言从基因组数据共享库（GDC）中创建和管理复杂的患者队列，并且其自定义LLM在此任务上优于GPT-4o。", "motivation": "GDC用户在通过图形界面构建复杂队列时，难以在数百个字段中找到特定的队列描述符，但他们能够更好地用自然语言描述所需队列。", "method": "GDC Cohort Copilot通过将用户输入的自然语言描述自动生成GDC队列过滤器来实现队列整理。该工具开发并评估了多个大型语言模型（LLMs），并证明其本地服务、开源的GDC Cohort LLM在生成GDC队列方面优于GPT-4o。它还提供一个交互式用户界面供用户进一步细化生成的队列。", "result": "GDC Cohort Copilot成功地将自然语言描述转换为GDC队列过滤器。研究表明，该工具的本地部署GDC Cohort LLM在生成GDC队列方面比GPT-4o提示取得了更好的结果。", "conclusion": "GDC Cohort Copilot成功地提供了一个创新的解决方案，通过利用自然语言处理简化了GDC中的队列整理过程，并且其定制的LLM在此特定任务上表现出卓越的性能。", "translation": "动机：基因组数据共享库（GDC）通过一个以患者队列为中心的统一整理和分析平台，提供高质量、协调一致的癌症基因组数据。虽然GDC用户可以通过图形化的队列构建器交互式地创建复杂的队列，但用户（尤其是新用户）可能难以在数百个可能的字段和属性中找到特定的队列描述符。然而，用户可能能够更好地用自由文本自然语言描述他们所需的队列。\n结果：我们介绍了GDC队列副驾驶，一个用于从GDC整理队列的开源副驾驶工具。GDC队列副驾驶自动生成与用户输入的所需队列的自然语言描述相对应的GDC队列过滤器，然后将队列导出回GDC进行进一步分析。一个交互式用户界面允许用户进一步完善生成的队列。我们为GDC队列副驾驶开发并评估了多个大型语言模型（LLM），并证明我们本地服务、开源的GDC队列LLM在生成GDC队列方面比GPT-4o提示取得了更好的结果。\n可用性和实现：GDC队列副驾驶的独立docker镜像可在https://quay.io/repository/cdis/gdc-cohort-copilot获取。源代码可在https://github.com/uc-cdis/gdc-cohort-copilot获取。GDC队列LLM权重可在https://huggingface.co/uc-ctds获取。", "summary": "GDC Cohort Copilot是一个开源的AI工具，旨在通过允许用户使用自然语言描述来整理来自基因组数据共享库（GDC）的患者队列。它解决了现有图形界面在查找特定描述符时的复杂性问题。该工具能够将自然语言转换为GDC队列过滤器，并提供交互式界面进行细化。研究表明，其本地部署的GDC Cohort LLM在生成GDC队列方面优于GPT-4o。", "keywords": "GDC, 队列整理, AI副驾驶, 大型语言模型, 癌症基因组学", "comments": "本文提出了一种实用的AI应用，解决了大型基因组数据库中常见的可用性挑战。其创新之处在于利用大型语言模型将自然语言翻译成结构化查询，显著简化了研究人员的队列整理工作。值得注意的是，其自定义LLM表现优于GPT-4o，凸显了专用模型在特定领域任务中的潜力。"}}
{"id": "2507.02380", "title": "JoyTTS: LLM-based Spoken Chatbot With Voice Cloning", "authors": ["Fangru Zhou", "Jun Zhao", "Guoxin Wang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02380v1", "summary": "JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02380v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "JoyTTS：基于LLM的语音聊天机器人与声音克隆", "tldr": "JoyTTS是一个结合LLM和TTS的端到端语音聊天机器人，具有声音克隆功能，基于开源模型MiniCPM-o和CosyVoice2构建，并开源了代码和模型，以促进社区开发。", "motivation": "该论文旨在开发一个端到端的语音聊天机器人JoyTTS，它结合了大型语言模型（LLM）和文本转语音（TTS）技术，并具备声音克隆能力。项目还致力于开源代码和模型，以促进社区的进一步开发和优化。", "method": "JoyTTS是一个端到端的语音聊天机器人，它将大型语言模型（LLM）与文本转语音（TTS）技术相结合，并具有声音克隆功能。该项目基于开源的MiniCPM-o和CosyVoice2模型构建，并使用2000小时的对话数据进行训练。研究团队提供了完整的训练代码、模型以及训练和推理脚本。", "result": "在测试机器seed-tts-zh上，JoyTTS实现了0.73的说话人相似度（SS）分数和5.09的词错误率（WER）。", "conclusion": "JoyTTS成功地将大型语言模型与文本转语音技术结合，创建了一个具有声音克隆功能的端到端语音聊天机器人，并在特定测试环境下展现了其性能。通过开源代码和模型，该项目促进了社区对语音聊天机器人技术的进一步开发和优化。", "translation": "JoyTTS是一个端到端的语音聊天机器人，它将大型语言模型（LLM）与文本转语音（TTS）技术相结合，并具有声音克隆功能。该项目基于开源的MiniCPM-o和CosyVoice2模型构建，并使用2000小时的对话数据进行训练。我们还提供了完整的训练代码，以促进社区的进一步开发和优化。在测试机器seed-tts-zh上，它实现了0.73的说话人相似度（SS）分数和5.09的词错误率（WER）。代码、模型以及训练和推理脚本可在https://github.com/jdh-algo/JoyTTS.git获取。", "summary": "JoyTTS是一个创新的端到端语音聊天机器人，它融合了大型语言模型（LLM）和文本转语音（TTS）技术，并特别加入了声音克隆功能。该项目基于开源的MiniCPM-o和CosyVoice2模型，并利用2000小时的对话数据进行训练。为促进社区的进一步开发与优化，项目团队已公开了完整的训练代码、模型及相关脚本。在测试中，JoyTTS展现了0.73的说话人相似度（SS）和5.09的词错误率（WER）性能。", "keywords": "LLM, TTS, 语音克隆, 聊天机器人, 开源", "comments": "该论文的创新点在于将大型语言模型与文本转语音技术相结合，并融入了声音克隆功能，构建了一个端到端的语音聊天机器人系统。其重要性体现在项目基于开源模型，并提供了完整的训练代码和模型，极大地促进了学术界和工业界对该技术的进一步研究、开发和应用。虽然给出了性能指标，但若能提供与其他现有系统的对比，将更有助于评估其先进性。"}}
{"id": "2507.02332", "title": "PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage", "authors": ["Krishna Kanth Nakka", "Xue Jiang", "Xuebing Zhou"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.02332v1", "summary": "This paper investigates privacy jailbreaking in LLMs via steering, focusing\non whether manipulating activations can bypass LLM alignment and alter response\nbehaviors to privacy related queries (e.g., a certain public figure's sexual\norientation). We begin by identifying attention heads predictive of refusal\nbehavior for private attributes (e.g., sexual orientation) using lightweight\nlinear probes trained with privacy evaluator labels. Next, we steer the\nactivations of a small subset of these attention heads guided by the trained\nprobes to induce the model to generate non-refusal responses. Our experiments\nshow that these steered responses often disclose sensitive attribute details,\nalong with other private information about data subjects such as life events,\nrelationships, and personal histories that the models would typically refuse to\nproduce. Evaluations across four LLMs reveal jailbreaking disclosure rates of\nat least 95%, with more than 50% on average of these responses revealing true\npersonal information. Our controlled study demonstrates that private\ninformation memorized in LLMs can be extracted through targeted manipulation of\ninternal activations.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.02332v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过激活操纵在大型语言模型中实现PII越狱，揭示个人信息泄露", "tldr": "本研究通过操纵大型语言模型（LLM）的激活来绕过其隐私对齐，成功使其泄露敏感个人信息（PII），揭示了PII越狱的高成功率和真实信息泄露的风险。", "motivation": "本研究旨在调查通过操纵大型语言模型（LLM）的内部激活是否能够绕过其对齐机制，并改变其对隐私相关查询的响应行为，从而实现个人信息（PII）的泄露。", "method": "首先，利用隐私评估器标签训练轻量级线性探针，识别出预测对私人属性拒绝行为的注意力头。接着，通过训练好的探针引导这些注意力头的一小部分激活，以诱导模型生成非拒绝性响应。", "result": "实验表明，这些被操纵的响应通常会泄露敏感属性细节，以及模型通常会拒绝生成的关于数据主体的其他私人信息（如生活事件、关系和个人历史）。对四种大型语言模型的评估显示，越狱泄露率至少达到95%，其中平均超过50%的响应揭示了真实的个人信息。", "conclusion": "通过有针对性地操纵内部激活，大型语言模型中记忆的私人信息可以被提取出来。", "translation": "本文研究了通过操纵大型语言模型（LLM）的激活来实现隐私越狱，重点关注操纵激活是否能够绕过LLM的对齐机制并改变其对隐私相关查询（例如，某个公众人物的性取向）的响应行为。我们首先利用隐私评估器标签训练轻量级线性探针，识别出预测对私人属性（例如，性取向）拒绝行为的注意力头。接下来，我们根据训练好的探针引导这些注意力头的一小部分激活，以诱导模型生成非拒绝性响应。我们的实验表明，这些被操纵的响应通常会泄露敏感属性细节，以及模型通常会拒绝生成的关于数据主体的其他私人信息，例如生活事件、关系和个人历史。这些信息是模型通常会拒绝生成的。对四种大型语言模型的评估显示，越狱泄露率至少达到95%，其中平均超过50%的响应揭示了真实的个人信息。我们的对照研究表明，大型语言模型中记忆的私人信息可以通过有针对性地操纵内部激活来提取。", "summary": "本研究探讨了通过操纵大型语言模型（LLM）的内部激活来绕过其隐私对齐机制，从而泄露个人敏感信息（PII）的方法。研究人员通过识别并引导与拒绝行为相关的注意力头，成功诱导LLM生成非拒绝性响应，并发现这些响应高频率地泄露了真实且通常被模型拒绝的PII。实验结果表明，PII泄露率高达95%以上，其中超过一半的泄露信息是真实的，证实了LLM中记忆的私人信息可通过此方法提取。", "keywords": "PII越狱, 大型语言模型, 激活操纵, 隐私泄露, 注意力头", "comments": "该研究通过激活操纵揭示了LLM潜在的PII泄露风险，其创新点在于利用内部机制（注意力头激活）绕过模型对齐，而非传统的提示工程。这对于理解LLM的隐私安全性具有重要意义，也为未来LLM的隐私保护提供了新的研究方向。"}}
{"id": "2507.02198", "title": "GPS-DRIFT: Marine Surface Robot Localization using IMU-GPS Fusion and Invariant Filtering", "authors": ["Surya Pratap Singh", "Tsimafei Lazouski", "Maani Ghaffari"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.02198v1", "summary": "This paper presents an extension of the DRIFT invariant state estimation\nframework, enabling robust fusion of GPS and IMU data for accurate pose and\nheading estimation. Originally developed for testing and usage on a marine\nautonomous surface vehicle (ASV), this approach can also be utilized on other\nmobile systems. Building upon the original proprioceptive only DRIFT algorithm,\nwe develop a symmetry-preserving sensor fusion pipeline utilizing the invariant\nextended Kalman filter (InEKF) to integrate global position updates from GPS\ndirectly into the correction step. Crucially, we introduce a novel heading\ncorrection mechanism that leverages GPS course-over-ground information in\nconjunction with IMU orientation, overcoming the inherent unobservability of\nyaw in dead-reckoning. The system was deployed and validated on a customized\nBlue Robotics BlueBoat, but the methodological focus is on the algorithmic\napproach to fusing exteroceptive and proprioceptive sensors for drift-free\nlocalization and reliable orientation estimation. This work provides an open\nsource solution for accurate yaw observation and localization in challenging or\nGPS-degraded conditions, and lays the groundwork for future experimental and\ncomparative studies.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.02198v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "GPS-DRIFT：使用IMU-GPS融合和不变滤波的海洋水面机器人定位", "tldr": "该论文提出了一种扩展的DRIFT框架，通过融合GPS和IMU数据，实现了水面机器人的鲁棒定位和航向估计，解决了航向不可观测性问题。", "motivation": "需要鲁棒地融合GPS和IMU数据以实现精确的姿态和航向估计；克服航位推算中固有的偏航不可观测性，尤其是在具有挑战性或GPS信号退化的条件下；为海洋自主水面无人艇和其他移动系统提供无漂移定位和可靠的方向估计解决方案。", "method": "扩展了DRIFT不变状态估计框架；开发了一个对称保持的传感器融合管道；利用不变扩展卡尔曼滤波器（InEKF）将来自GPS的全局位置更新直接整合到校正步骤中；引入了一种新颖的航向校正机制，结合GPS地面航向信息和IMU方向；融合了外部感受（GPS）和本体感受（IMU）传感器。", "result": "实现了GPS和IMU数据的鲁棒融合；实现了精确的姿态和航向估计；成功克服了航位推算中固有的偏航不可观测性；在定制的Blue Robotics BlueBoat上验证了系统；提供了一个用于精确偏航观测和定位的开源解决方案。", "conclusion": "所提出的GPS-DRIFT系统为融合外部感受和本体感受传感器提供了一种有效的算法方法；它提供了无漂移定位和可靠的方向估计，特别是解决了在具有挑战性或GPS信号退化条件下的精确偏航观测问题；这项工作为未来的实验和比较研究奠定了基础。", "translation": "本文提出了DRIFT不变状态估计框架的扩展，实现了GPS和IMU数据的鲁棒融合，以实现精确的姿态和航向估计。该方法最初是为海洋自主水面无人艇（ASV）的测试和使用而开发的，但也可以用于其他移动系统。在原始仅使用本体感受的DRIFT算法的基础上，我们开发了一个对称保持的传感器融合管道，利用不变扩展卡尔曼滤波器（InEKF）将来自GPS的全局位置更新直接整合到校正步骤中。至关重要的是，我们引入了一种新颖的航向校正机制，该机制结合了GPS地面航向信息和IMU方向，克服了航位推算中固有的偏航不可观测性。该系统已在定制的Blue Robotics BlueBoat上部署和验证，但方法学的重点是融合外部感受和本体感受传感器以实现无漂移定位和可靠方向估计的算法方法。这项工作为在具有挑战性或GPS信号退化条件下实现精确的偏航观测和定位提供了一个开源解决方案，并为未来的实验和比较研究奠定了基础。", "summary": "本文扩展了DRIFT不变状态估计算法，提出了GPS-DRIFT框架，用于海洋水面机器人及其他移动系统的鲁棒定位。该方法通过不变扩展卡尔曼滤波器（InEKF）融合GPS和IMU数据，并引入了一种新颖的航向校正机制，利用GPS地面航向信息克服了航位推算中的偏航不可观测性。该系统已在BlueBoat上验证，为在GPS受限环境下提供精确的偏航观测和无漂移定位提供了开源解决方案。", "keywords": "GPS-DRIFT, 机器人定位, IMU-GPS融合, 不变滤波, 航向估计", "comments": "这项工作在DRIFT不变状态估计算法的基础上，创新性地解决了移动系统，特别是水面机器人在GPS受限环境下航向（偏航）不可观测性的关键问题。通过引入GPS地面航向信息与IMU方向结合的航向校正机制，显著提升了定位和姿态估计的鲁棒性和准确性。提供开源解决方案也增加了其实用性和对未来研究的促进作用。"}}
{"id": "2507.02137", "title": "Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection", "authors": ["Martin Obaidi", "Marc Herrmann", "Jil Klünder", "Kurt Schneider"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted at the RETRAI workshop of the 33rd IEEE International Requirements Engineering Conference (REW 2025)", "url": "http://arxiv.org/abs/2507.02137v1", "summary": "Software development relies heavily on text-based communication, making\nsentiment analysis a valuable tool for understanding team dynamics and\nsupporting trustworthy AI-driven analytics in requirements engineering.\nHowever, existing sentiment analysis tools often perform inconsistently across\ndatasets from different platforms, due to variations in communication style and\ncontent.\n  In this study, we analyze linguistic and statistical features of 10 developer\ncommunication datasets from five platforms and evaluate the performance of 14\nsentiment analysis tools. Based on these results, we propose a mapping approach\nand questionnaire that recommends suitable sentiment analysis tools for new\ndatasets, using their characteristic features as input.\n  Our results show that dataset characteristics can be leveraged to improve\ntool selection, as platforms differ substantially in both linguistic and\nstatistical properties. While transformer-based models such as SetFit and\nRoBERTa consistently achieve strong results, tool effectiveness remains\ncontext-dependent. Our approach supports researchers and practitioners in\nselecting trustworthy tools for sentiment analysis in software engineering,\nwhile highlighting the need for ongoing evaluation as communication contexts\nevolve.", "comment": "This paper has been accepted at the RETRAI workshop of the 33rd IEEE\n  International Requirements Engineering Conference (REW 2025)", "pdf_url": "http://arxiv.org/pdf/2507.02137v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "迈向软件工程中可信的情感分析：数据集特性与工具选择", "tldr": "本研究分析了软件工程中不同开发者沟通数据集的特性，评估了多种情感分析工具的性能，并提出了一种基于数据集特性推荐合适工具的方法，以提高情感分析的可信度。", "motivation": "由于沟通风格和内容的变化，现有情感分析工具在不同平台的数据集上表现不一致，这阻碍了其在软件工程中可信地应用。", "method": "我们分析了来自五个平台的10个开发者沟通数据集的语言和统计特征，并评估了14种情感分析工具的性能。基于这些结果，我们提出了一种映射方法和问卷，利用数据集的特征作为输入，推荐合适的情感分析工具。", "result": "数据集特性可以被利用来改进工具选择，因为不同平台在语言和统计特性上存在显著差异。虽然基于Transformer的模型（如SetFit和RoBERTa）持续表现出色，但工具的有效性仍取决于具体上下文。", "conclusion": "本研究提出的方法支持研究人员和从业者在软件工程中选择可信的情感分析工具，并强调了随着沟通环境演变，持续评估的必要性。", "translation": "软件开发严重依赖基于文本的沟通，这使得情感分析成为理解团队动态和支持需求工程中可信AI驱动分析的宝贵工具。然而，由于沟通风格和内容的变化，现有情感分析工具在不同平台的数据集上往往表现不一致。\n在本研究中，我们分析了来自五个平台的10个开发者沟通数据集的语言和统计特征，并评估了14种情感分析工具的性能。基于这些结果，我们提出了一种映射方法和问卷，利用数据集的特征作为输入，推荐合适的情感分析工具。\n我们的结果表明，数据集特性可以被利用来改进工具选择，因为不同平台在语言和统计特性上存在显著差异。虽然基于Transformer的模型（如SetFit和RoBERTa）持续表现出色，但工具的有效性仍取决于具体上下文。我们的方法支持研究人员和从业者在软件工程中选择可信的工具进行情感分析，同时强调了随着沟通环境的演变，持续评估的必要性。", "summary": "本研究旨在解决软件工程中情感分析工具在不同数据集上表现不一致的问题。通过分析10个开发者沟通数据集的语言和统计特征，并评估14种情感分析工具的性能，研究提出了一种基于数据集特性推荐合适工具的映射方法。结果表明，数据集特性显著影响工具选择，且Transformer模型表现良好但工具有效性仍依赖上下文。该方法有助于研究人员和从业者选择可信的情感分析工具，并强调持续评估的重要性。", "keywords": "情感分析, 软件工程, 数据集特性, 工具选择, 可信AI", "comments": "该论文的创新点在于提出了一个基于数据集特征来推荐情感分析工具的映射方法和问卷，这为在软件工程领域选择合适且可信的情感分析工具提供了实用的指导。它强调了数据集特性对工具性能的关键影响，并指出了即使是先进的Transformer模型也需要考虑上下文依赖性，这对于实际应用具有重要意义。"}}
{"id": "2507.02186", "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge", "authors": ["Zahra Ashktorab", "Elizabeth M. Daly", "Erik Miehling", "Werner Geyer", "Martin Santillan Cooper", "Tejaswini Pedapati", "Michael Desmond", "Qian Pan", "Hyo Jin Do"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02186v1", "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02186v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "EvalAssist：一个以人为本的LLM作为评判工具", "tldr": "EvalAssist是一个以人为本的工具，旨在简化大型语言模型（LLM）作为评估者的工作流程，提供交互式标准开发环境和LLM评估管道，以解决LLM输出评估耗时且成本高昂的问题。", "motivation": "随着大型语言模型（LLM）的广泛应用及其生成大量输出的能力，确定给定任务的最佳输出需要一个密集且耗时的评估过程。机器学习从业者面临如何评估输出并选择最佳模型和提示的挑战。当前的方法既耗时又成本高昂，因此需要一个更高效的解决方案来简化LLM作为评估者的工作流程。", "method": "本文提出了EvalAssist，一个简化LLM作为评判工作流程的框架。该系统提供了一个在线标准开发环境，用户可以交互式地构建、测试和分享定制的评估标准。它支持一套基于LLM的评估管道，利用现成的LLM并采用其开发的、已贡献给UNITXT开源库的提示链方法。此外，系统还包括经过专门训练的评估器，用于检测LLM输出中的危害和风险。", "result": "EvalAssist 系统已在组织内部署，拥有数百名用户。", "conclusion": "EvalAssist通过提供一个以人为本的工具来简化LLM作为评判的工作流程，显著提升了大型语言模型输出评估的效率和可操作性，并已在实际应用中得到验证。", "translation": "随着大型语言模型的广泛可用性以及它们使用各种提示和配置生成大量输出的能力，确定给定任务的最佳输出需要一个密集的评估过程，机器学习从业者必须决定如何评估输出，然后仔细地进行评估。这个过程既耗时又成本高昂。随着从业者使用越来越多的模型，他们现在必须评估输出，以确定哪个模型和提示在给定任务中表现最佳。大型语言模型越来越多地被用作评估器，用于过滤训练数据、评估模型性能、评估危害和风险，或协助人类评估员进行详细评估。我们提出了EvalAssist，一个简化LLM作为评判工作流程的框架。该系统提供了一个在线标准开发环境，用户可以在其中以结构化和可移植的格式交互式地构建、测试和分享自定义评估标准。我们支持一套基于LLM的评估管道，这些管道利用现成的LLM并使用我们开发并贡献给UNITXT开源库的提示链方法。此外，我们的系统还包括经过专门训练的评估器，用于检测LLM输出中的危害和风险。我们已将该系统内部署在我们的组织中，拥有数百名用户。", "summary": "EvalAssist是一个以人为本的工具，旨在解决大型语言模型（LLM）输出评估过程中耗时且成本高昂的问题。它提供了一个框架，简化了LLM作为评估者的工作流程，包括一个在线标准开发环境，使用户能够交互式地构建、测试和分享定制的评估标准。该系统支持基于LLM的评估管道，利用现成LLM和独特的提示链方法，并集成了专门训练的评估器以检测危害和风险。EvalAssist已在组织内部署并被数百名用户使用。", "keywords": "LLM评估, 人机交互, LLM-as-a-judge, 提示工程, EvalAssist", "comments": "EvalAssist的创新之处在于其以人为本的设计，通过提供交互式标准开发环境，将评估过程的控制权交还给用户。其采用的提示链方法和对风险检测的关注，使其在LLM评估领域具有重要意义。该工具的内部部署和广泛使用表明了其在实际应用中的价值和潜力。"}}
{"id": "2507.02575", "title": "A unifying approach to self-organizing systems interacting via conservation laws", "authors": ["Frank Barrows", "Guanming Zhang", "Satyam Anand", "Zizi Chen", "Jonathan Lin", "Amman Desai", "Stefano Martiniani", "Francesco Caravelli"], "categories": ["cond-mat.soft", "cond-mat.stat-mech", "cs.MA", "nlin.AO"], "primary_category": "Subjects:       Soft Condensed Matter (cond-mat.soft)", "pdf_link": null, "comments": "Comments:      19 pages single column + 24 pages supplementary", "url": "http://arxiv.org/abs/2507.02575v1", "summary": "We present a unified framework for embedding and analyzing dynamical systems\nusing generalized projection operators rooted in local conservation laws. By\nrepresenting physical, biological, and engineered systems as graphs with\nincidence and cycle matrices, we derive dual projection operators that\ndecompose network fluxes and potentials. This formalism aligns with principles\nof non-equilibrium thermodynamics and captures a broad class of systems\ngoverned by flux-forcing relationships and local constraints. We extend this\napproach to collective dynamics through the PRojective Embedding of Dynamical\nSystems (PrEDS), which lifts low-dimensional dynamics into a high-dimensional\nspace, enabling both replication and recovery of the original dynamics. When\nsystems fall within the PrEDS class, their collective behavior can be\neffectively approximated through projection onto a mean-field space. We\ndemonstrate the versatility of PrEDS across diverse domains, including\nresistive and memristive circuits, adaptive flow networks (e.g., slime molds),\nelastic string networks, and particle swarms. Notably, we establish a direct\ncorrespondence between PrEDS and swarm dynamics, revealing new insights into\noptimization and self-organization. Our results offer a general theoretical\nfoundation for analyzing complex networked systems and for designing systems\nthat self-organize through local interactions.", "comment": "19 pages single column + 24 pages supplementary", "pdf_url": "http://arxiv.org/pdf/2507.02575v1", "cate": "cond-mat.soft", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "统一的自组织系统方法：通过守恒定律相互作用", "tldr": "提出了一个统一的框架PrEDS，利用广义投影算子和局部守恒定律来分析和嵌入各种动态系统，特别是自组织系统，并展示了其在多种领域的应用。", "motivation": "现有方法可能缺乏统一性或难以处理复杂自组织系统的分析和设计。该研究旨在提供一个通用的理论基础，用于分析复杂的网络化系统，并设计通过局部相互作用实现自组织的系统。", "method": "通过利用根植于局部守恒定律的广义投影算子，提出了一个统一的动态系统嵌入和分析框架。将物理、生物和工程系统表示为具有关联矩阵和循环矩阵的图，并推导出分解网络通量和势的双投影算子。将此方法扩展到集体动力学，通过“动态系统投影嵌入（PrEDS）”将低维动力学提升到高维空间，并允许通过投影到平均场空间来近似集体行为。", "result": "开发了一个与非平衡热力学原理一致的形式主义，能够捕捉由通量-力关系和局部约束控制的广泛系统。PrEDS能够复制和恢复原始动力学，并有效近似集体行为。该方法在电阻和忆阻电路、自适应流网络（如黏菌）、弹性弦网络和粒子群等多个领域得到了验证。建立了PrEDS与群体动力学之间的直接对应关系，揭示了优化和自组织的新见解。", "conclusion": "该研究结果为分析复杂的网络化系统和设计通过局部相互作用实现自组织的系统提供了通用的理论基础。", "translation": "我们提出了一个统一的框架，利用植根于局部守恒定律的广义投影算子来嵌入和分析动态系统。通过将物理、生物和工程系统表示为具有关联矩阵和循环矩阵的图，我们推导出了分解网络通量和势的双投影算子。这种形式主义与非平衡热力学原理相符，并能够捕捉由通量-力关系和局部约束控制的广泛系统。我们将这种方法通过动态系统投影嵌入（PrEDS）扩展到集体动力学，该方法将低维动力学提升到高维空间，从而能够复制和恢复原始动力学。当系统属于PrEDS类时，它们的集体行为可以通过投影到平均场空间进行有效近似。我们展示了PrEDS在不同领域的通用性，包括电阻和忆阻电路、自适应流网络（例如黏菌）、弹性弦网络和粒子群。值得注意的是，我们建立了PrEDS与群体动力学之间的直接对应关系，揭示了优化和自组织的新见解。我们的结果为分析复杂的网络化系统和设计通过局部相互作用实现自组织的系统提供了通用的理论基础。", "summary": "本文提出了一个统一的框架，利用根植于局部守恒定律的广义投影算子来分析和嵌入自组织动态系统。该框架将系统建模为图，并引入了双投影算子以分解网络通量和势。通过引入PrEDS方法，可以将低维动力学提升到高维空间，从而有效处理集体行为，并能通过平均场近似。研究在多种物理和生物系统中验证了PrEDS的通用性，并建立了与群体动力学的联系，为理解和设计复杂的自组织网络系统提供了理论基础。", "keywords": "自组织系统, 守恒定律, 投影算子, 动态系统嵌入, 网络化系统", "comments": "这篇论文提出了一种新颖且统一的方法来理解和分析通过局部守恒定律相互作用的自组织系统。其创新点在于将广义投影算子与图论相结合，并引入了PrEDS框架，使得对复杂网络系统的集体动力学分析成为可能。该方法与非平衡热力学原理的对齐以及在多样化领域的应用（如电路、生物网络和粒子群）展示了其广泛的适用性和重要性。特别是，它为自组织和优化提供了新的理论视角，对于理解和设计复杂系统具有重要意义。"}}
{"id": "2507.02781", "title": "From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images", "authors": ["Danrong Zhang", "Huili Huang", "N. Simrill Smith", "Nimisha Roy", "J. David Frost"], "categories": ["cs.CV", "cs.SI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02781v1", "summary": "In the aftermath of earthquakes, social media images have become a crucial\nresource for disaster reconnaissance, providing immediate insights into the\nextent of damage. Traditional approaches to damage severity assessment in\npost-earthquake social media images often rely on classification methods, which\nare inherently subjective and incapable of accounting for the varying extents\nof damage within an image. Addressing these limitations, this study proposes a\nnovel approach by framing damage severity assessment as a semantic segmentation\nproblem, aiming for a more objective analysis of damage in earthquake-affected\nareas. The methodology involves the construction of a segmented damage severity\ndataset, categorizing damage into three degrees: undamaged structures, damaged\nstructures, and debris. Utilizing this dataset, the study fine-tunes a\nSegFormer model to generate damage severity segmentations for post-earthquake\nsocial media images. Furthermore, a new damage severity scoring system is\nintroduced, quantifying damage by considering the varying degrees of damage\nacross different areas within images, adjusted for depth estimation. The\napplication of this approach allows for the quantification of damage severity\nin social media images in a more objective and comprehensive manner. By\nproviding a nuanced understanding of damage, this study enhances the ability to\noffer precise guidance to disaster reconnaissance teams, facilitating more\neffective and targeted response efforts in the aftermath of earthquakes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02781v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "从像素到损伤程度：使用社交媒体图像的语义分割估计地震影响", "tldr": "本研究提出了一种使用语义分割和新的评分系统，从社交媒体图像中客观量化地震损伤程度的新方法，以改进灾后响应。", "motivation": "传统的地震后社交媒体图像损伤评估方法主观且无法捕捉图像内损伤程度的变化，因此需要更客观的分析方法。", "method": "该研究将损伤程度评估框定为语义分割问题。方法包括：1) 构建一个分割损伤程度数据集，将损伤分为未受损、受损结构和碎片三类。2) 微调SegFormer模型以生成损伤程度分割。3) 引入一个新的损伤程度评分系统，通过考虑图像内不同区域的损伤程度并结合深度估计来量化损伤。", "result": "该方法能够更客观、更全面地量化社交媒体图像中的损伤程度。", "conclusion": "通过提供对损伤的细致理解，本研究提高了为灾害侦察队提供精确指导的能力，从而促进了地震后更有效和有针对性的响应工作。", "translation": "地震发生后，社交媒体图像已成为灾害侦察的关键资源，能够即时了解损害程度。传统上，地震后社交媒体图像中的损害程度评估方法通常依赖于分类方法，这些方法本质上是主观的，并且无法考虑图像内不同程度的损害。为了解决这些局限性，本研究提出了一种新颖的方法，将损害程度评估框定为语义分割问题，旨在对受地震影响区域的损害进行更客观的分析。该方法包括构建一个分割损害程度数据集，将损害分为三个等级：未受损结构、受损结构和碎片。利用该数据集，本研究对SegFormer模型进行了微调，以生成地震后社交媒体图像的损害程度分割。此外，还引入了一种新的损害程度评分系统，通过考虑图像内不同区域的不同程度损害并根据深度估计进行调整来量化损害。这种方法的应用使得能够以更客观、更全面的方式量化社交媒体图像中的损害程度。通过提供对损害的细致理解，本研究提高了为灾害侦察队提供精确指导的能力，从而促进了地震后更有效和有针对性的响应工作。", "summary": "本研究提出了一种新颖的地震损伤评估方法，将损伤程度量化问题转化为语义分割任务，以克服传统分类方法的主观性限制。通过构建包含未受损、受损结构和碎片三类损伤的定制数据集，并微调SegFormer模型，该方法能够对地震后社交媒体图像进行像素级的损伤分割。此外，引入了一个新的损伤评分系统，结合深度估计，更客观、全面地量化图像中的损伤程度，旨在为灾害响应提供更精确的指导。", "keywords": "地震影响, 语义分割, 损伤程度评估, 社交媒体图像, SegFormer", "comments": "这项研究通过将地震损伤评估从传统的分类方法转向更精细的语义分割，并引入考虑深度的新评分系统，显著提高了损伤量化的客观性和准确性。其创新点在于利用像素级分析来克服图像内损伤分布不均的问题，对灾害侦察和响应具有重要实践意义。"}}
{"id": "2507.02013", "title": "AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions", "authors": ["Hao Liu", "Bo Yang", "Zhiwen Yu", "Xuelin Cao", "George C. Alexandropoulos", "Yan Zhang", "Chau Yuen"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02013v1", "summary": "The Internet of Vehicles (IoV) transforms the transportation ecosystem\npromising pervasive connectivity and data-driven approaches. Deep learning and\ngenerative Artificial Intelligence (AI) have the potential to significantly\nenhance the operation of applications within IoV by facilitating efficient\ndecision-making and predictive capabilities, including intelligent navigation,\nvehicle safety monitoring, accident prevention, and intelligent traffic\nmanagement. Nevertheless, efficiently transmitting and processing the massive\nvolumes of data generated by the IoV in real-time remains a significant\nchallenge, particularly in dynamic and unpredictable wireless channel\nconditions. To address these challenges, this paper proposes a semantic\ncommunication framework based on channel perception to improve the accuracy and\nefficiency of data transmission. The semantic communication model extracts and\ncompresses the information to be transmitted. In addition, the wireless channel\nis estimated by using a generative diffusion model, which is employed to\npredict the dynamic channel states, thereby improving the quality of IoV\nservice. In dynamic scenarios, however, the channel estimation performance may\nbe degraded when substantially new scenarios take place, which will adversely\naffect user experience. To mitigate this limitation, we employ a large model to\nfine-tune the channel generation model to enhance its adaptability for varying\nscenarios. The performance and reliability of the proposed framework are\nevaluated on the two public datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02013v1", "cate": "cs.NI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "AI赋能动态条件下车联网语义通信信道生成", "tldr": "本文提出了一种AI赋能的车联网语义通信框架，利用生成扩散模型进行信道估计，并通过大型模型微调以适应动态无线信道条件，从而提高数据传输的准确性和效率。", "motivation": "在动态和不可预测的无线信道条件下，实时高效地传输和处理车联网（IoV）产生的海量数据是一个重大挑战，这可能导致信道估计性能下降和用户体验受损。", "method": "本文提出了一种基于信道感知的语义通信框架。该框架通过语义通信模型提取和压缩信息，并使用生成扩散模型估计和预测动态无线信道状态。为了增强在不同新场景下的适应性，还采用了一个大型模型对信道生成模型进行微调。所提出的框架在两个公共数据集上进行了性能和可靠性评估。", "result": "所提出的框架的性能和可靠性在两个公共数据集上进行了评估。", "conclusion": "本文提出的AI赋能语义通信框架，结合生成扩散模型和大型模型微调，有效解决了车联网在动态信道条件下数据传输的挑战，提升了数据传输的准确性和效率。", "translation": "车联网（IoV）正在改变交通生态系统，承诺实现普遍连接和数据驱动的方法。深度学习和生成式人工智能（AI）有潜力通过促进高效决策和预测能力，显著增强车联网应用的操作，包括智能导航、车辆安全监控、事故预防和智能交通管理。然而，实时高效传输和处理车联网产生的海量数据仍然是一个重大挑战，尤其是在动态和不可预测的无线信道条件下。为了应对这些挑战，本文提出了一种基于信道感知的语义通信框架，以提高数据传输的准确性和效率。语义通信模型提取并压缩要传输的信息。此外，通过使用生成扩散模型估计无线信道，该模型用于预测动态信道状态，从而提高车联网服务质量。然而，在动态场景中，当出现大量新场景时，信道估计性能可能会下降，这将对用户体验产生不利影响。为了缓解这一限制，我们采用一个大型模型来微调信道生成模型，以增强其对不同场景的适应性。所提出框架的性能和可靠性在两个公共数据集上进行了评估。", "summary": "本文提出了一种AI赋能的车联网（IoV）语义通信框架，旨在解决动态无线信道条件下实时数据传输的挑战。该框架利用语义通信模型进行信息提取和压缩，并采用生成扩散模型估计和预测动态信道状态。为增强在不同场景下的适应性，研究人员还引入大型模型对信道生成模型进行微调。该框架的性能和可靠性已在两个公共数据集上进行了评估。", "keywords": "IoV, 语义通信, 生成式AI, 信道估计, 动态条件", "comments": "该论文创新性地将语义通信与生成式AI（扩散模型和大型模型）相结合，解决了车联网中动态信道估计的关键问题。这种方法旨在提高数据传输效率和准确性，这对于实时车联网应用至关重要。利用大型模型进行自适应微调是一个值得关注的亮点。"}}
{"id": "2507.02731", "title": "RIS-Aided Cooperative ISAC Networks for Structural Health Monitoring", "authors": ["Jie Yang", "Chao-Kai Wen", "Xiao Li", "Shi Jin"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.02731v1", "summary": "Integrated sensing and communication (ISAC) is a key feature of future\ncellular systems, enabling applications such as intruder detection, monitoring,\nand tracking using the same infrastructure. However, its potential for\nstructural health monitoring (SHM), which requires the detection of slow and\nsubtle structural changes, remains largely unexplored due to challenges such as\nmultipath interference and the need for ultra-high sensing precision. This\nstudy introduces a novel theoretical framework for SHM via ISAC by leveraging\nreconfigurable intelligent surfaces (RIS) as reference points in collaboration\nwith base stations and users. By dynamically adjusting RIS phases to generate\ndistinct radio signals that suppress background multipath interference,\nmeasurement accuracy at these reference points is enhanced. We theoretically\nanalyze RIS-aided collaborative sensing in three-dimensional cellular networks\nusing Fisher information theory, demonstrating how increasing observation time,\nincorporating additional receivers (even with self-positioning errors),\noptimizing RIS phases, and refining collaborative node selection can reduce the\nposition error bound to meet SHM's stringent accuracy requirements.\nFurthermore, we develop a Bayesian inference model to identify structural\nstates and validate damage detection probabilities. Both theoretical and\nnumerical analyses confirm ISAC's capability for millimeter-level deformation\ndetection, highlighting its potential for high-precision SHM applications.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.02731v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RIS辅助的协作ISAC网络用于结构健康监测", "tldr": "本文提出了一种利用RIS辅助的ISAC网络进行结构健康监测（SHM）的新型理论框架，通过抑制多径干扰和提高测量精度，实现了毫米级形变检测。", "motivation": "集成感知与通信（ISAC）在结构健康监测（SHM）中的潜力尚未得到充分探索，主要面临多径干扰和对超高感知精度的需求等挑战，以检测缓慢而细微的结构变化。", "method": "本研究引入了一种通过ISAC进行SHM的新型理论框架，该框架利用可重构智能表面（RIS）作为参考点，与基站和用户协同工作。通过动态调整RIS相位以生成独特的无线电信号来抑制背景多径干扰，从而提高这些参考点的测量精度。研究使用费舍尔信息理论分析了三维蜂窝网络中RIS辅助的协作感知，并开发了一个贝叶斯推理模型来识别结构状态和验证损伤检测概率。", "result": "增加观测时间、加入额外的接收器（即使存在自定位误差）、优化RIS相位以及优化协作节点选择可以降低位置误差界限，以满足SHM严格的精度要求。理论和数值分析均证实ISAC能够实现毫米级形变检测。", "conclusion": "ISAC在结构健康监测（SHM）中具有高精度应用的潜力，能够实现毫米级形变检测。", "translation": "集成感知与通信（ISAC）是未来蜂窝系统的一个关键特性，能够利用相同的基础设施实现入侵者检测、监控和跟踪等应用。然而，其在结构健康监测（SHM）方面的潜力仍未得到充分探索，SHM需要检测缓慢而细微的结构变化，但面临多径干扰和对超高感知精度的需求等挑战。本研究引入了一种通过ISAC进行SHM的新型理论框架，该框架通过利用可重构智能表面（RIS）作为参考点，与基站和用户协同工作。通过动态调整RIS相位以生成独特的无线电信号来抑制背景多径干扰，从而提高这些参考点的测量精度。我们使用费舍尔信息理论对三维蜂窝网络中RIS辅助的协作感知进行了理论分析，证明了增加观测时间、加入额外的接收器（即使存在自定位误差）、优化RIS相位以及优化协作节点选择可以降低位置误差界限，以满足SHM严格的精度要求。此外，我们开发了一个贝叶斯推理模型来识别结构状态并验证损伤检测概率。理论和数值分析均证实ISAC能够实现毫米级形变检测，突显了其在高精度SHM应用中的潜力。", "summary": "本文提出了一种利用可重构智能表面（RIS）增强的集成感知与通信（ISAC）网络进行结构健康监测（SHM）的新型理论框架。针对SHM中多径干扰和高精度需求等挑战，该研究利用RIS抑制干扰并提高测量精度。通过费舍尔信息理论和贝叶斯推理模型，研究表明优化RIS相位、增加观测时间以及增加接收器可以显著降低位置误差界限。理论和数值分析均证实ISAC能够实现毫米级形变检测，突显了其在高精度SHM中的巨大潜力。", "keywords": "ISAC, RIS, 结构健康监测, 费舍尔信息, 贝叶斯推理", "comments": "该论文通过将RIS集成到ISAC网络中用于SHM，提出了一种创新方法，这对于克服多径干扰和实现高精度至关重要。费舍尔信息理论和贝叶斯推理的应用为所提出的系统提供了坚实的理论基础。"}}
{"id": "2507.02654", "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure", "authors": ["Rui Xie", "Asad Ul Haq", "Yunhua Fang", "Linsen Ma", "Sanchari Sen", "Swagath Venkataramani", "Liu Liu", "Tong Zhang"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02654v1", "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02654v1", "cate": "cs.AR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "打破HBM比特成本障碍：面向AI推理基础设施的领域特定ECC", "tldr": "本研究提出一种领域特定ECC框架，通过将HBM的故障管理转移到内存控制器并消除片上ECC，显著降低HBM成本，同时在AI推理工作负载下保持高吞吐量和准确性。", "motivation": "高带宽内存（HBM）的比特成本高昂，部分源于严格的片上可靠性要求，这日益成为其在AI工作负载中可扩展部署的障碍。", "method": "消除片上ECC，将所有故障管理转移到内存控制器。引入一个领域特定ECC框架，结合大码字Reed-Solomon校正与轻量级细粒度CRC检测、差分奇偶校验更新以减轻写入放大，并根据数据重要性提供可调保护。", "result": "在LLM推理工作负载下，即使HBM原始误码率高达10^-3，系统仍能保持理想无错HBM系统78%以上的吞吐量和97%的模型准确性。", "conclusion": "通过将可靠性视为可调系统参数而非固定硬件约束，该设计为AI基础设施中低成本、高性能HBM的部署开辟了新途径。", "translation": "高带宽内存（HBM）为AI工作负载提供了卓越的带宽和能效，但其高昂的比特成本，部分源于严格的片上可靠性要求，正日益成为可扩展部署的障碍。这项工作探索了一种系统级的成本降低方法，即消除片上ECC并将所有故障管理转移到内存控制器。我们引入了一个领域特定的ECC框架，该框架结合了大型码字Reed-Solomon（RS）纠错与轻量级细粒度CRC检测、用于减轻写入放大的差分奇偶校验更新，以及基于数据重要性的可调保护。我们使用LLM推理工作负载进行的评估表明，即使在原始HBM比特错误率高达10^-3的情况下，与配备理想无错HBM的系统相比，该系统仍能保持超过78%的吞吐量和97%的模型准确性。通过将可靠性视为一个可调的系统参数而非固定的硬件约束，我们的设计为AI基础设施中低成本、高性能HBM的部署开辟了一条新途径。", "summary": "本文提出一种创新的系统级方法来降低高带宽内存（HBM）的成本，通过消除其片上错误校正码（ECC），并将故障管理功能转移至内存控制器。研究引入了一个领域特定的ECC框架，该框架结合了Reed-Solomon纠错、CRC检测、差分奇偶校验更新以及基于数据重要性的可调保护。评估结果显示，即使在较高误码率下，该系统在大型语言模型（LLM）推理工作负载中仍能保持高吞吐量和模型准确性，为AI基础设施中的HBM部署提供了低成本、高性能的新途径。", "keywords": "HBM, ECC, AI推理, 成本优化, 可靠性", "comments": "这篇论文的创新点在于将HBM的错误管理从硬件层（片上ECC）提升到系统层（内存控制器），并引入了领域特定的可调ECC机制。这种方法将可靠性从固定约束转变为可调参数，为降低HBM成本和提高其在AI推理中的可扩展性提供了新的思路。其重要性在于，它可能显著降低AI基础设施的硬件成本，加速HBM的普及。"}}
{"id": "2507.02404", "title": "Alps, a versatile research infrastructure", "authors": ["Maxime Martinasso", "Mark Klein", "Thomas C. Schulthess"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures, Cray User Group(CUG) 2025 Best Paper Award", "url": "http://arxiv.org/abs/2507.02404v1", "summary": "The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition\nof delivering top-tier high-performance computing systems, exemplified by the\nPiz Daint supercomputer. However, the increasing diversity of scientific needs\nhas exposed limitations in traditional vertically integrated HPC architectures,\nwhich often lack flexibility and composability. To address these challenges,\nCSCS developed Alps, a next-generation HPC infrastructure designed with a\ntransformative principle: resources operate as independent endpoints within a\nhigh-speed network. This architecture enables the creation of independent\ntenant-specific and platform-specific services, tailored to diverse scientific\nrequirements.\n  Alps incorporates heterogeneous hardware, including CPUs and GPUs,\ninterconnected by a high-performance Slingshot network, and offers a modular\nstorage system. A key innovation is the versatile software-defined cluster\n(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting\ninfrastructure, service management, and user environments into distinct layers,\nvClusters allow for customized platforms that support diverse workloads.\nCurrent platforms on Alps serve various scientific domains, including numerical\nweather prediction, and AI research.", "comment": "10 pages, 6 figures, Cray User Group(CUG) 2025 Best Paper Award", "pdf_url": "http://arxiv.org/pdf/2507.02404v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Alps，一个多功能研究基础设施", "tldr": "CSCS开发了Alps，一个下一代高性能计算（HPC）基础设施，旨在通过将资源作为高速网络中的独立端点来解决传统HPC架构的局限性，并引入多功能软件定义集群（vCluster）技术，以支持多样化的科学工作负载。", "motivation": "传统的垂直整合高性能计算（HPC）架构缺乏灵活性和可组合性，无法满足日益多样化的科学需求。", "method": "CSCS开发了Alps，一个下一代HPC基础设施，其核心原则是资源作为高速网络中的独立端点运行。该架构支持创建独立的租户特定和平台特定服务。Alps集成了异构硬件（CPU和GPU），通过高性能Slingshot网络互连，并提供模块化存储系统。关键创新是多功能软件定义集群（vCluster）技术，它通过将基础设施、服务管理和用户环境抽象为不同层，实现了定制化平台，从而弥合了云和HPC范式之间的鸿沟。", "result": "目前Alps上的平台服务于各种科学领域，包括数值天气预报和人工智能研究。", "conclusion": "Alps成功地通过其创新的独立端点架构和vCluster技术，克服了传统HPC系统的局限性，提供了一个高度灵活和可定制的基础设施，以满足多样化的科学需求。", "translation": "瑞士国家超级计算中心（CSCS）在提供顶级高性能计算系统方面拥有悠久的传统，以Piz Daint超级计算机为例。然而，科学需求的日益多样化暴露了传统垂直整合HPC架构的局限性，这些架构往往缺乏灵活性和可组合性。为了应对这些挑战，CSCS开发了Alps，一个下一代HPC基础设施，其设计遵循一个变革性的原则：资源作为高速网络中的独立端点运行。这种架构能够创建独立的租户特定和平台特定服务，以适应多样化的科学需求。\nAlps集成了异构硬件，包括CPU和GPU，通过高性能Slingshot网络互连，并提供模块化存储系统。一个关键创新是多功能软件定义集群（vCluster）技术，它弥合了云和HPC范式之间的鸿沟。通过将基础设施、服务管理和用户环境抽象为不同的层，vCluster允许定制化平台，支持多样化的工作负载。目前Alps上的平台服务于各种科学领域，包括数值天气预报和人工智能研究。", "summary": "Alps是瑞士国家超级计算中心（CSCS）开发的新一代高性能计算（HPC）基础设施，旨在解决传统HPC系统在灵活性和可组合性方面的不足。它采用资源作为高速网络中独立端点的架构原则，并引入了创新的软件定义集群（vCluster）技术，能够整合异构硬件并提供模块化存储。通过将基础设施、服务管理和用户环境分层抽象，vCluster实现了定制化平台，以支持包括数值天气预报和人工智能研究在内的多样化科学工作负载。", "keywords": "HPC, Alps, vCluster, 基础设施, 灵活性", "comments": "Alps的创新之处在于其将资源作为独立网络端点的设计理念，这显著提升了HPC系统的灵活性和可组合性。特别是vCluster技术，它成功地将云和HPC范式相结合，为不同的科学应用提供了高度定制化的环境，这对于应对当前日益多样化的计算需求至关重要。"}}
{"id": "2507.02087", "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions", "authors": ["Eitan Anzenberg", "Arunava Samajpati", "Sivasankaran Chandrasekar", "Varun Kacholia"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025", "url": "http://arxiv.org/abs/2507.02087v1", "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes.", "comment": "10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025", "pdf_url": "http://arxiv.org/pdf/2507.02087v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "评估大型语言模型在招聘决策中的前景与陷阱", "tldr": "专有招聘模型（Match Score）在招聘决策的准确性和公平性方面优于通用大型语言模型，强调了领域特定模型和偏见审计的重要性。", "motivation": "大型语言模型（LLMs）在招聘中的应用虽然有望简化候选人筛选，但也引发了对准确性和算法偏见的严重担忧，尤其是在缺乏足够保障措施的情况下。本研究旨在评估LLMs在招聘决策中的表现及其潜在问题。", "method": "本研究对来自OpenAI、Anthropic、Google、Meta和Deepseek等多个最先进的基础LLM进行了基准测试，并将其与专有的领域特定招聘模型（Match Score）进行比较，用于职位候选人匹配。评估指标包括预测准确性（ROC AUC、Precision-Recall AUC、F1-score）和公平性（针对声明的性别、种族和交叉亚组的截止分析影响比）。实验基于大约10,000个真实世界近期候选人-职位对的数据集。", "result": "实验结果显示，Match Score在准确性方面优于通用LLM（ROC AUC 0.85 对 0.77），并在不同人口统计群体中实现了显著更公平的结果。Match Score的最低种族影响比达到0.957（接近平等），而最佳LLM则为0.809或更低（对于交叉亚组，分别为0.906 对 0.773）。研究还表明，在招聘中准确性和公平性之间不应存在二分法，精心设计的算法可以同时实现两者。", "conclusion": "研究结果强调了在招聘等高风险领域部署AI时，领域特定建模和偏见审计的重要性，并警告不要在没有广泛公平保障的情况下依赖现成的LLM。一个精心设计的算法可以同时实现招聘的准确性和结果的公平性。", "translation": "大型语言模型（LLMs）在招聘中的应用有望简化候选人筛选，但也引发了对准确性和算法偏见的严重担忧，尤其是在缺乏足够保障措施的情况下。在这项工作中，我们对几种最先进的基础LLM——包括来自OpenAI、Anthropic、Google、Meta和Deepseek的模型——进行了基准测试，并将其与我们专有的领域特定招聘模型（Match Score）进行比较，用于候选人与职位的匹配。我们评估了每个模型的预测准确性（ROC AUC、Precision-Recall AUC、F1-score）和公平性（针对声明的性别、种族和交叉亚组的截止分析影响比）。我们对大约10,000个真实世界近期候选人-职位对数据集进行的实验表明，Match Score在准确性方面优于通用LLM（ROC AUC 0.85 对 0.77），并在不同人口统计群体中实现了显著更公平的结果。值得注意的是，Match Score达到了最低种族影响比0.957（接近平等），而最佳LLM则为0.809或更低（对于交叉亚组，分别为0.906 对 0.773）。我们讨论了为什么预训练偏见可能导致LLM在招聘场景中传播社会偏见，而定制的监督模型可以更有效地缓解这些偏见。我们的研究结果强调了在招聘等高风险领域部署AI时领域特定建模和偏见审计的重要性，并警告不要在没有广泛公平保障的情况下依赖现成的LLM来完成此类任务。此外，我们通过经验证据表明，在招聘中选择准确性和公平性之间不应存在二分法：一个精心设计的算法可以同时实现招聘的准确性和结果的公平性。", "summary": "本文评估了大型语言模型（LLMs）在招聘决策中的表现，并与一个专有的领域特定模型（Match Score）进行了比较，重点关注准确性和公平性。对10,000个候选人-职位对数据集的实验表明，Match Score在准确性（ROC AUC 0.85 对 0.77）和实现跨人口统计群体的更公平结果（例如，种族影响比0.957 对 0.809）方面显著优于通用LLMs。研究强调，LLMs中的预训练偏见可能在招聘中传播社会偏见，并主张在高风险AI应用（如招聘）中采用领域特定模型和严格的偏见审计，以确保准确性和公平性。", "keywords": "大型语言模型, 招聘决策, 算法偏见, 公平性, 领域特定模型", "comments": "本文的创新之处在于直接比较了通用LLM与专门的招聘模型，量化地揭示了现成LLM在高风险、偏见敏感应用（如招聘）中的局限性。其重要性在于倡导领域特定的AI解决方案和严格的偏见审计，挑战了LLM的“一刀切”方法，并证明了准确性和公平性并非相互排斥。"}}
{"id": "2507.02103", "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments", "authors": ["Daniel Durstewitz", "Bruno Averbeck", "Georgia Koppe"], "categories": ["cs.AI", "q-bio.NC", "I.2; I.6; A.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Submitted as a Perspective article (10 pages, 5 figures)", "url": "http://arxiv.org/abs/2507.02103v1", "summary": "Modern AI models, such as large language models, are usually trained once on\na huge corpus of data, potentially fine-tuned for a specific task, and then\ndeployed with fixed parameters. Their training is costly, slow, and gradual,\nrequiring billions of repetitions. In stark contrast, animals continuously\nadapt to the ever-changing contingencies in their environments. This is\nparticularly important for social species, where behavioral policies and reward\noutcomes may frequently change in interaction with peers. The underlying\ncomputational processes are often marked by rapid shifts in an animal's\nbehaviour and rather sudden transitions in neuronal population activity. Such\ncomputational capacities are of growing importance for AI systems operating in\nthe real world, like those guiding robots or autonomous vehicles, or for\nagentic AI interacting with humans online. Can AI learn from neuroscience? This\nPerspective explores this question, integrating the literature on continual and\nin-context learning in AI with the neuroscience of learning on behavioral tasks\nwith shifting rules, reward probabilities, or outcomes. We will outline an\nagenda for how specifically insights from neuroscience may inform current\ndevelopments in AI in this area, and - vice versa - what neuroscience may learn\nfrom AI, contributing to the evolving field of NeuroAI.", "comment": "Submitted as a Perspective article (10 pages, 5 figures)", "pdf_url": "http://arxiv.org/pdf/2507.02103v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "神经科学能教会人工智能在不断变化的环境中学习什么？", "tldr": "本文探讨了神经科学如何为人工智能在持续变化环境中学习提供见解，并提出了一个神经科学与人工智能交叉研究的议程。", "motivation": "现代AI模型训练成本高昂且参数固定，难以像动物一样持续适应不断变化的环境。对于在真实世界中运行的AI系统（如机器人、自动驾驶、与人类交互的AI），这种适应能力至关重要。本文旨在探讨AI能否从神经科学中学习，以解决当前AI在动态环境中学习的局限性。", "method": "本文通过整合AI领域中的持续学习和上下文学习文献，以及神经科学中关于行为任务中规则、奖励概率或结果变化的学习研究，来探讨神经科学如何为AI提供见解。它提出一个具体议程。", "result": "论文概述了一个议程，说明神经科学的见解如何具体地为AI在该领域的发展提供信息，反之亦然，即神经科学可以从AI中学到什么，从而促进神经AI领域的发展。", "conclusion": "神经科学可以为AI在动态环境中学习提供宝贵见解，而AI也能反过来促进神经科学的发展，共同推动神经AI这一新兴领域。", "translation": "现代人工智能模型，例如大型语言模型，通常在庞大的数据集上进行一次训练，可能针对特定任务进行微调，然后以固定参数部署。它们的训练成本高昂、耗时且渐进，需要数十亿次重复。相比之下，动物能够持续适应环境中不断变化的偶发事件。这对于社会物种尤为重要，因为行为策略和奖励结果在与同伴互动中可能会频繁变化。其底层的计算过程通常表现为动物行为的快速转变和神经元群体活动的突然过渡。这种计算能力对于在现实世界中运行的人工智能系统（如引导机器人或自动驾驶汽车的系统，或在线与人类交互的智能体AI）越来越重要。人工智能能否从神经科学中学习？本视角文章探讨了这个问题，整合了人工智能中持续学习和上下文学习的文献，以及神经科学中关于规则、奖励概率或结果变化的学习行为任务的研究。我们将概述一个议程，具体说明神经科学的见解如何为人工智能在该领域当前的发展提供信息，反之亦然——神经科学可以从人工智能中学到什么，从而促进神经-人工智能这一不断发展的领域。", "summary": "本文是一篇视角文章，探讨了神经科学如何为人工智能在持续变化环境中学习提供启示。它指出，当前AI模型在动态适应性方面不如生物，而这种能力对真实世界中的AI至关重要。文章整合了AI的持续学习和上下文学习，以及神经科学中关于适应性行为学习的研究，并提出了一个促进神经科学与AI相互学习、共同发展“神经AI”领域的议程。", "keywords": "神经科学, 人工智能, 持续学习, 环境适应, 神经AI", "comments": "这篇视角文章强调了生物学习在动态适应性方面的优势，并提出了一个将神经科学原理应用于AI，以解决当前AI在复杂、变化环境中学习瓶颈的创新方向。它不仅关注单向的知识传递，还探讨了神经科学能从AI中学到什么，预示着“神经AI”作为一个跨学科领域将迎来新的发展。其重要性在于为未来AI系统设计提供了生物启发的新思路。"}}
{"id": "2507.02212", "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers", "authors": ["Takuro Kawada", "Shunsuke Kitada", "Sota Nemoto", "Hitoshi Iyatomi"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 15 figures, 4 tables. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.02212v1", "summary": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science.", "comment": "21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/", "pdf_url": "http://arxiv.org/pdf/2507.02212v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SciGA：一个用于设计学术论文图形摘要的综合数据集", "tldr": "本文介绍了SciGA-145k，一个大规模数据集，用于支持科学论文中图形摘要（GA）的选择、推荐和自动化生成，并提出了新的任务定义和评估指标CAR。", "motivation": "图形摘要（GA）在视觉传达科学论文关键发现方面至关重要，但其增强科学交流的潜力尚未充分探索。此外，设计有效的GA需要高级可视化技能，这阻碍了其广泛采用。", "method": "研究人员引入了SciGA-145k，一个包含约14.5万篇科学论文和114万张图片的大规模数据集，专门用于支持GA选择和推荐，并促进自动化GA生成的研究。他们定义了两个任务：1）论文内GA推荐，识别论文中适合作为GA的图片；2）论文间GA推荐，从其他论文中检索GA以启发新的GA创作。研究人员为这些任务提供了合理的基线模型，并提出了信心调整的top-1真实比例（CAR）这一新颖的推荐度量，用于模型行为的精细分析，解决了传统基于排名的度量在多图可作为GA情况下的局限性。", "result": "SciGA-145k通过统一这些任务和度量，为推进视觉科学交流奠定了基础。", "conclusion": "本文的工作通过提供SciGA-145k数据集、定义相关任务和提出新的评估指标，旨在推进视觉科学交流，并为科学人工智能（AI for Science）的发展做出贡献。", "translation": "图形摘要（GAs）在视觉传达科学论文的关键发现方面扮演着至关重要的角色。尽管最近的研究越来越多地将图1等视觉材料作为事实上的GAs，但它们在增强科学交流方面的潜力仍未得到充分探索。此外，设计有效的GAs需要高级可视化技能，这对其广泛采用造成了障碍。为了应对这些挑战，我们引入了SciGA-145k，一个大规模数据集，包含大约145,000篇科学论文和114万张图片，明确设计用于支持GA选择和推荐，并促进自动化GA生成的研究。作为GA设计支持的初步步骤，我们定义了两个任务：1）论文内GA推荐，识别给定论文中非常适合作为GAs的图片；2）论文间GA推荐，从其他论文中检索GAs以启发新的GAs创作。我们为这些任务提供了合理的基线模型。此外，我们提出了信心调整的top-1真实比例（CAR），一种新颖的推荐度量，可以对模型行为进行细粒度分析。CAR通过考虑论文中除了明确标记的GA之外，多个图片也可能作为GA的情况，解决了传统基于排名的度量的局限性。通过统一这些任务和度量，我们的SciGA-145k为推进视觉科学交流奠定了基础，同时为科学人工智能的发展做出了贡献。", "summary": "本文介绍了SciGA-145k，一个包含14.5万篇论文和114万张图片的大规模数据集，旨在解决图形摘要（GA）设计难度高且潜力未被充分利用的问题。该数据集支持GA的选择、推荐和自动化生成研究。作者定义了论文内和论文间GA推荐两项任务，并提供了基线模型。此外，论文提出了一种新的评估指标CAR，以更精细地分析推荐模型，并考虑了多图可作为GA的复杂情况。SciGA-145k的推出旨在为提升视觉科学交流和发展科学人工智能奠定基础。", "keywords": "图形摘要, 数据集, 推荐系统, 科学交流, 人工智能", "comments": "本文的主要创新在于构建了一个大规模、专门用于图形摘要设计和推荐的综合数据集SciGA-145k，这填补了该领域高质量数据集的空白。此外，提出的新型评估指标CAR，通过考虑多图潜在作为GA的情况，更准确地反映了推荐模型的性能，解决了传统指标的局限性。这项工作对于推动科学传播的可视化和加速科学领域的人工智能应用具有重要意义。"}}
{"id": "2507.01971", "title": "DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification", "authors": ["Boris Kriuk", "Logic Ng", "Zarif Al Hossain"], "categories": ["q-fin.ST", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures, 1 table", "url": "http://arxiv.org/abs/2507.01971v1", "summary": "Support and resistance (SR) levels are central to technical analysis, guiding\ntraders in entry, exit, and risk management. Despite widespread use,\ntraditional SR identification methods often fail to adapt to the complexities\nof modern, volatile markets. Recent research has introduced machine learning\ntechniques to address the following challenges, yet most focus on price\nprediction rather than structural level identification. This paper presents\nDeepSupp, a new deep learning approach for detecting financial support levels\nusing multi-head attention mechanisms to analyze spatial correlations and\nmarket microstructure relationships. DeepSupp integrates advanced feature\nengineering, constructing dynamic correlation matrices that capture evolving\nmarket relationships, and employs an attention-based autoencoder for robust\nrepresentation learning. The final support levels are extracted through\nunsupervised clustering, leveraging DBSCAN to identify significant price\nthresholds. Comprehensive evaluations on S&P 500 tickers demonstrate that\nDeepSupp outperforms six baseline methods, achieving state-of-the-art\nperformance across six financial metrics, including essential support accuracy\nand market regime sensitivity. With consistent results across diverse market\nconditions, DeepSupp addresses critical gaps in SR level detection, offering a\nscalable and reliable solution for modern financial analysis. Our approach\nhighlights the potential of attention-based architectures to uncover nuanced\nmarket patterns and improve technical trading strategies.", "comment": "7 pages, 4 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.01971v1", "cate": "q-fin.ST", "date": "2025-06-22", "updated": "2025-06-22", "AI": {"title_translation": "DeepSupp：注意力驱动的相关模式分析，用于动态时间序列支撑和阻力位识别", "tldr": "DeepSupp是一种新的深度学习方法，利用多头注意力机制分析空间相关性和市场微观结构关系，以识别动态时间序列中的金融支撑位，并在S&P 500股票上表现优于现有基线方法。", "motivation": "尽管支撑和阻力（SR）位在技术分析中广泛使用，但传统识别方法难以适应现代波动市场的复杂性。现有机器学习研究多关注价格预测而非结构性水平识别，这促使了DeepSupp的提出以填补SR水平检测的关键空白。", "method": "本文提出了DeepSupp，一种新的深度学习方法，通过多头注意力机制分析空间相关性和市场微观结构关系来检测金融支撑位。它集成了高级特征工程，构建动态相关矩阵以捕捉不断变化的市场关系，并采用基于注意力的自编码器进行鲁棒的表示学习。最终的支撑位通过无监督聚类（利用DBSCAN）提取。", "result": "在S&P 500股票上的全面评估表明，DeepSupp在六项金融指标上超越了六种基线方法，实现了最先进的性能，包括重要的支撑准确性和市场机制敏感性。在各种市场条件下，DeepSupp都表现出一致的结果。", "conclusion": "DeepSupp解决了SR水平检测中的关键空白，为现代金融分析提供了一个可扩展且可靠的解决方案。该方法强调了基于注意力的架构在揭示细微市场模式和改进技术交易策略方面的潜力。", "translation": "支撑和阻力（SR）位是技术分析的核心，指导交易者进行入场、出场和风险管理。尽管广泛使用，但传统的SR识别方法往往无法适应现代波动市场的复杂性。最近的研究引入了机器学习技术来解决以下挑战，但大多数关注价格预测而非结构性水平识别。本文提出了DeepSupp，一种新的深度学习方法，利用多头注意力机制分析空间相关性和市场微观结构关系，以检测金融支撑位。DeepSupp集成了高级特征工程，构建动态相关矩阵以捕捉不断变化的市场关系，并采用基于注意力的自编码器进行鲁棒的表示学习。最终的支撑位通过无监督聚类（利用DBSCAN）提取，以识别重要的价格阈值。在S&P 500股票上的全面评估表明，DeepSupp超越了六种基线方法，在六项金融指标上实现了最先进的性能，包括重要的支撑准确性和市场机制敏感性。在各种市场条件下，DeepSupp都表现出一致的结果，解决了SR水平检测中的关键空白，为现代金融分析提供了一个可扩展且可靠的解决方案。我们的方法强调了基于注意力的架构在揭示细微市场模式和改进技术交易策略方面的潜力。", "summary": "DeepSupp是一种利用深度学习和多头注意力机制，通过分析动态相关模式和市场微观结构来识别金融市场支撑位的创新方法。该模型通过构建动态相关矩阵和基于注意力的自编码器进行特征学习，并结合DBSCAN进行无监督聚类以提取支撑位。实验证明，DeepSupp在S&P 500股票数据集上显著优于现有基线方法，在多项金融指标上达到最先进水平，为现代金融分析提供了稳定可靠的支撑位检测方案。", "keywords": "支撑和阻力位, 深度学习, 注意力机制, 时间序列, 金融分析", "comments": "DeepSupp的创新之处在于将多头注意力机制应用于金融时间序列的支撑和阻力位识别，通过分析动态相关模式和市场微观结构，克服了传统方法在波动市场中的局限性。其结合特征工程、注意力自编码器和无监督聚类的方法，为结构性水平识别提供了一个新颖且高效的解决方案，对技术分析和量化交易策略具有重要意义。"}}
{"id": "2507.02305", "title": "Hybrid Satellite-Ground Deployments for Web3 DID: System Design and Performance Analysis", "authors": ["Yalin Liu", "Zhigang Yan", "Bingyuan Luo", "Xiaochi Xu", "Hong-Ning Dai", "Yaru Fu", "Bishenghui Tao", "Siu-Kei Au Yeung"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02305v1", "summary": "The emerging Web3 has great potential to provide worldwide decentralized\nservices powered by global-range data-driven networks in the future. To ensure\nthe security of Web3 services among diverse user entities, a decentralized\nidentity (DID) system is essential. Especially, a user's access request to Web3\nservices can be treated as a DID transaction within the blockchain, executed\nthrough a consensus mechanism. However, a critical implementation issue arises\nin the current Web3, i.e., how to deploy network nodes to serve users on a\nglobal scale. To address this issue, emerging Low Earth Orbit (LEO) satellite\ncommunication systems, such as Starlink, offer a promising solution. With their\nglobal coverage and high reliability, these communication satellites can\ncomplement terrestrial networks as Web3 deployment infrastructures. In this\ncase, this paper develops three hybrid satellite-ground modes to deploy the\nblockchain-enabled DID system for Web3 users. Three modes integrate ground\nnodes and satellites to provide flexible and continuous DID services for\nworldwide users. Meanwhile, to evaluate the effectiveness of the present hybrid\ndeployment modes, we analyze the complete DID consensus performance of\nblockchain on three hybrid satellite-ground modes. Moreover, we conduct\nnumerical and simulation experiments to verify the effectiveness of three\nhybrid satellite-ground modes. The impacts of various system parameters are\nthoroughly analyzed, providing valuable insights for implementing the worldwide\nWeb3 DID system in real-world network environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02305v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Web3 DID的混合星地部署：系统设计与性能分析", "tldr": "本文提出并分析了三种用于Web3去中心化身份（DID）系统的混合星地部署模式，以实现全球服务覆盖和可靠性。", "motivation": "当前的Web3面临一个关键问题：如何在全球范围内部署网络节点以服务用户，特别是对于需要安全、全球访问的去中心化身份（DID）系统。", "method": "本文开发了三种混合星地部署模式，将地面节点与低地球轨道（LEO）卫星（如Starlink）集成，为全球用户提供灵活、连续的DID服务。论文分析了这些模式下区块链的DID共识性能，并进行了数值和模拟实验来验证其有效性，同时分析了各种系统参数的影响。", "result": "数值和模拟实验验证了三种混合星地模式的有效性，对各种系统参数的分析为在全球范围内实施Web3 DID系统提供了宝贵见解。", "conclusion": "混合星地部署为实现全球、可靠、安全的Web3 DID服务提供了一个有前景的解决方案，为实际部署提供了实用见解。", "translation": "新兴的Web3在未来有望通过全球范围的数据驱动网络提供全球去中心化服务，潜力巨大。为了确保Web3服务在不同用户实体之间的安全性，去中心化身份（DID）系统至关重要。特别是，用户对Web3服务的访问请求可以被视为区块链内的DID交易，通过共识机制执行。然而，当前Web3存在一个关键的实施问题，即如何部署网络节点以在全球范围内服务用户。为了解决这个问题，新兴的低地球轨道（LEO）卫星通信系统，如Starlink，提供了一个有前景的解决方案。凭借其全球覆盖和高可靠性，这些通信卫星可以补充地面网络，作为Web3的部署基础设施。在此背景下，本文开发了三种混合星地模式来为Web3用户部署支持区块链的DID系统。这三种模式整合了地面节点和卫星，为全球用户提供灵活、连续的DID服务。同时，为了评估当前混合部署模式的有效性，我们分析了区块链在三种混合星地模式下的完整DID共识性能。此外，我们还进行了数值和模拟实验，以验证这三种混合星地模式的有效性。论文彻底分析了各种系统参数的影响，为在实际网络环境中实施全球Web3 DID系统提供了宝贵的见解。", "summary": "本文旨在解决Web3去中心化身份（DID）系统全球节点部署的挑战，提出了三种混合星地部署模式。这些模式将地面网络与低地球轨道（LEO）卫星相结合，旨在为全球提供灵活和持续的DID服务。作者分析了这些模式下的DID共识性能，并进行了数值和模拟实验，为Web3 DID的实际部署提供了系统参数方面的见解。", "keywords": "Web3, 去中心化身份 (DID), 卫星通信, 混合部署, 区块链", "comments": "本文提出了一种创新方法，通过利用LEO卫星技术解决Web3中关键的可扩展性和覆盖范围问题。所提出的混合星地部署模式为全球去中心化身份服务提供了实用的解决方案。性能分析和参数研究为未来的实际部署提供了宝贵指导。"}}
{"id": "2507.01999", "title": "Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series", "authors": ["Bappaditya Dey", "Daniel Sorensen", "Minjin Hwang", "Sandip Halder"], "categories": ["cs.LG", "I.2.0; J.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures, submitted to IEEE Transactions on Semiconductor Manufacturing", "url": "http://arxiv.org/abs/2507.01999v1", "summary": "Semiconductor manufacturing is an extremely complex process, characterized by\nthousands of interdependent parameters collected across diverse tools and\nprocess steps. Multi-variate time-series (MTS) analysis has emerged as a\ncritical methodology for enabling real-time monitoring, fault detection, and\npredictive maintenance in such environments. However, anomaly prediction in\nsemiconductor fabrication presents several critical challenges, including high\ndata dimensionality, severe class imbalance due to the rarity of true faults,\nnoisy and missing measurements, and non-stationary behavior of production\nsystems. Furthermore, the complex interdependencies between variables and the\ndelayed emergence of faults across downstream stages complicate both anomaly\ndetection and root-cause-analysis. This paper presents a novel and generic\napproach for anomaly detection in MTS data using machine learning. The proposed\nmethodology consists of three main steps: a) converting MTS data into\nimage-based representations using the Continuous Wavelet Transform, b)\ndeveloping a multi-class image classifier by fine-tuning a pretrained VGG-16\narchitecture on custom CWT image datasets, and c) constructing a Siamese\nnetwork composed of two identical sub-networks, each utilizing the fine-tuned\nVGG-16 as a backbone. The network takes pairs of CWT images as input -one\nserving as a reference or anchor (representing a known-good signal), and the\nother as a query (representing an unknown signal). The model then compares the\nembeddings of both inputs to determine whether they belong to the same class at\na given time step. Our approach demonstrates high accuracy in identifying\nanomalies on a real FAB process time-series dataset, offering a promising\nsolution for offline anomaly detection in process and tool trace data.\nMoreover, the approach is flexible and can be applied in both supervised and\nsemi-supervised settings.", "comment": "13 pages, 7 figures, submitted to IEEE Transactions on Semiconductor\n  Manufacturing", "pdf_url": "http://arxiv.org/pdf/2507.01999v1", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "连续小波变换和暹罗网络在多变量半导体过程时间序列异常检测中的应用", "tldr": "该论文提出了一种基于连续小波变换和暹罗网络的机器学习方法，用于多变量半导体过程时间序列中的异常检测。", "motivation": "半导体制造过程复杂，存在高数据维度、类别严重不平衡（故障罕见）、测量噪声和缺失、生产系统非平稳性以及变量间复杂依赖性等挑战，使得异常检测和根本原因分析变得复杂。", "method": "提出的方法包含三个步骤：a) 使用连续小波变换将多变量时间序列数据转换为图像表示；b) 通过在自定义CWT图像数据集上微调预训练的VGG-16架构，开发一个多类图像分类器；c) 构建一个由两个相同子网络组成的暹罗网络，每个子网络都使用微调后的VGG-16作为骨干，该网络输入成对的CWT图像（一个作为参考，一个作为查询），比较其嵌入以确定是否属于同一类别。", "result": "该方法在真实的晶圆厂过程时间序列数据集上显示出高精度地识别异常的能力。", "conclusion": "该方法为过程和工具跟踪数据中的离线异常检测提供了有前景的解决方案，并且具有灵活性，可应用于监督和半监督设置。", "translation": "半导体制造是一个极其复杂的过程，其特点是跨不同工具和工艺步骤收集的数千个相互依赖的参数。多变量时间序列（MTS）分析已成为在这种环境中实现实时监控、故障检测和预测性维护的关键方法。然而，半导体制造中的异常预测面临几个关键挑战，包括高数据维度、由于真实故障的稀有性导致的严重类别不平衡、噪声和缺失测量，以及生产系统的非平稳行为。此外，变量之间复杂的相互依赖关系以及下游阶段故障的延迟出现，使得异常检测和根本原因分析都变得复杂。本文提出了一种新颖通用的机器学习方法，用于MTS数据中的异常检测。所提出的方法包括三个主要步骤：a) 使用连续小波变换将MTS数据转换为基于图像的表示；b) 通过在自定义CWT图像数据集上微调预训练的VGG-16架构，开发一个多类图像分类器；c) 构建一个由两个相同子网络组成的暹罗网络，每个子网络都使用微调后的VGG-16作为骨干。该网络以成对的CWT图像作为输入——一个作为参考或锚点（代表已知良好信号），另一个作为查询（代表未知信号）。然后，模型比较两个输入的嵌入，以确定它们在给定时间步是否属于同一类别。我们的方法在真实的晶圆厂过程时间序列数据集上显示出高精度地识别异常的能力，为过程和工具跟踪数据中的离线异常检测提供了一个有前景的解决方案。此外，该方法灵活，可应用于监督和半监督设置。", "summary": "本文针对半导体制造中多变量时间序列数据异常检测的挑战，提出了一种新颖的机器学习方法。该方法首先利用连续小波变换将时间序列数据转换为图像，然后通过微调VGG-16架构开发图像分类器，并在此基础上构建暹罗网络进行异常识别。实验结果表明，该方法在实际半导体数据集中能高精度地检测异常，并可应用于监督和半监督场景。", "keywords": "异常检测, 连续小波变换, 暹罗网络, 多变量时间序列, 半导体制造", "comments": "该论文的创新点在于将连续小波变换与深度学习（VGG-16和暹罗网络）相结合，将时间序列的异常检测问题转换为图像分类问题，有效处理了半导体数据的高维度、非平稳性和类别不平衡等挑战。暹罗网络的使用使其能够进行参考-查询比对，适用于少样本或半监督场景，具有较高的实用价值和普适性。"}}
{"id": "2507.02352", "title": "Track-before-detect in RIS-aided Integrated Sensing and Communication", "authors": ["Georgios Mylonopoulos", "Luca Venturino", "Emanuele Grossi", "Stefano Buzzi", "Ciro D'Elia"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted to the 33rd European Signal Processing Conference (EUSIPCO 2025), Isola delle Femmine, Palermo, Italy", "url": "http://arxiv.org/abs/2507.02352v1", "summary": "This study considers a base station equipped with sensing and communication\ncapabilities, which serves a ground user and scans a portion of the sky via a\npassive reconfigurable intelligent surface. To achieve more favorable system\ntradeoffs, we utilize a multi-frame radar detector, comprising a detector, a\nplot-extractor, and a track-before-detect processor. The main idea proposed\nhere is that user spectral efficiency can be enhanced by increasing the number\nof scans jointly processed by the multi-frame radar detector while maintaining\nthe same sensing performance. A numerical analysis is conducted to verify the\neffectiveness of the proposed solution and to evaluate the achievable system\ntradeoffs.", "comment": "Accepted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025), Isola delle Femmine, Palermo, Italy", "pdf_url": "http://arxiv.org/pdf/2507.02352v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RIS辅助的集成感知与通信中的检测前跟踪", "tldr": "本研究提出在RIS辅助的集成感知与通信系统中，通过增加多帧雷达检测器处理的扫描次数，可在保持相同感知性能的同时提高用户频谱效率。", "motivation": "论文旨在通过利用多帧雷达检测器，在RIS辅助的集成感知与通信系统中实现更有利的系统权衡，特别是提高用户频谱效率。", "method": "研究使用一个配备感知和通信能力的基站，通过无源可重构智能表面扫描天空。核心方法是利用一个包含检测器、绘图提取器和检测前跟踪处理器的多帧雷达检测器，通过增加共同处理的扫描次数来提高频谱效率。通过数值分析验证了所提解决方案的有效性。", "result": "数值分析验证了所提出解决方案的有效性，并评估了可实现的系统权衡。", "conclusion": "结论是，在RIS辅助的集成感知与通信系统中，通过增加多帧雷达检测器共同处理的扫描次数，可以在保持相同感知性能的同时提高用户频谱效率。", "translation": "本研究考虑一个配备感知和通信能力的基站，该基站通过一个无源可重构智能表面服务地面用户并扫描部分天空。为了实现更有利的系统权衡，我们利用了一个多帧雷达检测器，该检测器包括一个检测器、一个绘图提取器和一个检测前跟踪处理器。这里提出的主要思想是，通过增加多帧雷达检测器共同处理的扫描次数，可以在保持相同感知性能的同时提高用户频谱效率。进行了数值分析以验证所提出解决方案的有效性并评估可实现的系统权衡。", "summary": "本文研究了RIS辅助的集成感知与通信系统，其中基站利用无源可重构智能表面进行感知和通信。为优化系统性能，提出并采用了一种包含检测器、绘图提取器和检测前跟踪处理器的多帧雷达检测器。核心贡献在于，通过增加该检测器共同处理的扫描帧数，可以在不牺牲感知性能的前提下显著提升用户频谱效率。数值分析验证了此方法的有效性及系统权衡。", "keywords": "检测前跟踪, RIS, 集成感知与通信, 多帧雷达检测, 频谱效率", "comments": "这篇论文的创新点在于将检测前跟踪技术应用于RIS辅助的集成感知与通信系统，并提出通过增加多帧处理的扫描次数来提升频谱效率，同时保持感知性能。这种方法为未来6G通信中的感知与通信一体化提供了新的思路和性能优化潜力。"}}
{"id": "2507.02668", "title": "MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection", "authors": ["Zhe Yee Tan"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2507.02668v1", "summary": "Colorectal polyp segmentation is critical for early detection of colorectal\ncancer, yet weak and low contrast boundaries significantly limit automated\naccuracy. Existing deep models either blur fine edge details or rely on\nhandcrafted filters that perform poorly under variable imaging conditions. We\npropose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects\ndirectional, parameter free Haar wavelet edge maps into each decoder stage to\nrecalibrate semantic features. Our two main contributions are: (1) a two-level\nHaar wavelet head for multi orientation edge extraction; and (2) Wavelet Edge\nGuided Attention (WEGA) modules that fuse wavelet cues with reverse and input\nbranches. On five public polyp datasets, MEGANetW consistently outperforms\nexisting methods, improving mIoU by up to 2.3% and mDice by 1.2%, while\nintroducing no additional learnable parameters.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.02668v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MEGANet-W：一种小波驱动的边缘引导注意力框架，用于弱边界息肉检测", "tldr": "MEGANet-W是一种新颖的深度学习模型，通过注入小波边缘图和使用小波边缘引导注意力模块，显著提高了结直肠息肉分割的准确性，尤其是在弱边界条件下，且不增加可学习参数。", "motivation": "结直肠息肉分割对早期发现结直肠癌至关重要，但弱和低对比度边界严重限制了自动化检测的准确性。现有深度模型要么模糊精细边缘细节，要么依赖在可变成像条件下表现不佳的手工滤波器。", "method": "本文提出了MEGANet-W，一个小波驱动的边缘引导注意力网络。该网络将方向性、无参数的Haar小波边缘图注入到每个解码器阶段，以重新校准语义特征。主要贡献包括：1) 一个用于多方向边缘提取的两级Haar小波头部；2) 小波边缘引导注意力（WEGA）模块，将小波线索与反向和输入分支融合。", "result": "在五个公共息肉数据集上，MEGANet-W始终优于现有方法，mIoU提高了2.3%，mDice提高了1.2%，同时没有引入额外的可学习参数。", "conclusion": "MEGANet-W通过有效利用小波边缘信息，显著提高了结直肠息肉检测中弱边界分割的准确性，且不增加模型复杂度，证明了其在临床应用中的潜力。", "translation": "结直肠息肉分割对于早期发现结直肠癌至关重要，但弱和低对比度边界严重限制了自动化检测的准确性。现有的深度模型要么模糊精细边缘细节，要么依赖在可变成像条件下表现不佳的手工滤波器。我们提出了MEGANet-W，一个小波驱动的边缘引导注意力网络，它将方向性、无参数的Haar小波边缘图注入到每个解码器阶段，以重新校准语义特征。我们的两个主要贡献是：(1) 一个用于多方向边缘提取的两级Haar小波头部；(2) 小波边缘引导注意力（WEGA）模块，将小波线索与反向和输入分支融合。在五个公共息肉数据集上，MEGANetW始终优于现有方法，mIoU提高了2.3%，mDice提高了1.2%，同时没有引入额外的可学习参数。", "summary": "MEGANet-W是一种新颖的小波驱动边缘引导注意力框架，旨在解决结直肠息肉分割中弱边界检测的挑战。该模型通过在解码器阶段注入无参数的Haar小波边缘图，并利用小波边缘引导注意力（WEGA）模块融合多方向边缘信息，有效提升了语义特征的校准。在五个公共数据集上的实验表明，MEGANet-W在不增加可学习参数的情况下，显著优于现有方法，mIoU和mDice分别提高了2.3%和1.2%，证明了其在提高弱边界息肉检测准确性方面的有效性。", "keywords": "结直肠息肉检测, 小波变换, 边缘引导注意力, 弱边界分割, 深度学习", "comments": "该论文的创新点在于将无参数的Haar小波边缘图引入深度学习模型，以解决弱边界分割问题，并通过小波边缘引导注意力模块有效地融合了边缘信息。其优势在于在不增加模型复杂度的前提下显著提升了性能，这对于实际应用具有重要意义。"}}
{"id": "2507.02548", "title": "Bounded Weighted Edit Distance: Dynamic Algorithms and Matching Lower Bounds", "authors": ["Itai Boneh", "Egor Gorbachev", "Tomasz Kociumaka"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      ESA 2025", "url": "http://arxiv.org/abs/2507.02548v1", "summary": "The edit distance $ed(X,Y)$ of two strings $X,Y\\in \\Sigma^*$ is the minimum\nnumber of character edits (insertions, deletions, and substitutions) needed to\ntransform $X$ into $Y$. Its weighted counterpart $ed^w(X,Y)$ minimizes the\ntotal cost of edits, which are specified using a function $w$, normalized so\nthat each edit costs at least one. The textbook dynamic-programming procedure,\ngiven strings $X,Y\\in \\Sigma^{\\le n}$ and oracle access to $w$, computes\n$ed^w(X,Y)$ in $O(n^2)$ time. Nevertheless, one can achieve better running\ntimes if the computed distance, denoted $k$, is small: $O(n+k^2)$ for unit\nweights [Landau and Vishkin; JCSS'88] and $\\tilde{O}(n+\\sqrt{nk^3})$ for\narbitrary weights [Cassis, Kociumaka, Wellnitz; FOCS'23].\n  In this paper, we study the dynamic version of the weighted edit distance\nproblem, where the goal is to maintain $ed^w(X,Y)$ for strings $X,Y\\in\n\\Sigma^{\\le n}$ that change over time, with each update specified as an edit in\n$X$ or $Y$. Very recently, Gorbachev and Kociumaka [STOC'25] showed that the\nunweighted distance $ed(X,Y)$ can be maintained in $\\tilde{O}(k)$ time per\nupdate after $\\tilde{O}(n+k^2)$-time preprocessing; here, $k$ denotes the\ncurrent value of $ed(X,Y)$. Their algorithm generalizes to small integer\nweights, but the underlying approach is incompatible with large weights.\n  Our main result is a dynamic algorithm that maintains $ed^w(X,Y)$ in\n$\\tilde{O}(k^{3-\\gamma})$ time per update after $\\tilde{O}(nk^\\gamma)$-time\npreprocessing. Here, $\\gamma\\in [0,1]$ is a real trade-off parameter and $k\\ge\n1$ is an integer threshold fixed at preprocessing time, with $\\infty$ returned\nwhenever $ed^w(X,Y)>k$. We complement our algorithm with conditional lower\nbounds showing fine-grained optimality of our trade-off for $\\gamma \\in\n[0.5,1)$ and justifying our choice to fix $k$.", "comment": "ESA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02548v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "有界加权编辑距离：动态算法和匹配下界", "tldr": "该论文提出了一种用于维护有界加权编辑距离的动态算法，具有时间-空间权衡，并提供了匹配的下界。", "motivation": "现有动态编辑距离算法要么不支持大权重，要么其底层方法与大权重不兼容。本文旨在解决字符串随时间变化时加权编辑距离问题的动态版本，特别是处理任意/大权重的情况。", "method": "本文提出了一种动态算法来维护 $ed^w(X,Y)$。该算法使用一个实数权衡参数 $γ ∈ [0,1]$ 和一个在预处理时固定的整数阈值 $k$。此外，论文还提供了条件性下界。", "result": "提出了一种动态算法，在 $\tilde{O}(nk^γ)$ 时间预处理后，以每次更新 $\tilde{O}(k^{3-γ})$ 的时间维护 $ed^w(X,Y)$。该算法还提供了条件性下界，表明了其权衡在 $γ ∈ [0.5,1)$ 范围内的精细最优性。", "conclusion": "本文为有界加权编辑距离提供了一种动态算法，具有可调的权衡参数，并通过对特定范围的权衡参数的匹配下界证明了其最优性。", "translation": "编辑距离 $ed(X,Y)$ 是将字符串 $X$ 转换为 $Y$ 所需的最少字符编辑（插入、删除和替换）次数。其加权对应物 $ed^w(X,Y)$ 最小化编辑的总成本，该成本由函数 $w$ 指定，并经过归一化，使得每次编辑的成本至少为一。教科书式的动态规划过程，给定字符串 $X,Y\\in \\Sigma^{\\le n}$ 和对 $w$ 的预言机访问，可以在 $O(n^2)$ 时间内计算 $ed^w(X,Y)$。然而，如果计算出的距离 $k$ 较小，则可以实现更好的运行时间：对于单位权重为 $O(n+k^2)$ [Landau and Vishkin; JCSS'88]，对于任意权重为 $\tilde{O}(n+\\sqrt{nk^3})$ [Cassis, Kociumaka, Wellnitz; FOCS'23]。\n在本文中，我们研究了加权编辑距离问题的动态版本，目标是维护随时间变化的字符串 $X,Y\\in \\Sigma^{\\le n}$ 的 $ed^w(X,Y)$，每次更新指定为 $X$ 或 $Y$ 中的一次编辑。最近，Gorbachev 和 Kociumaka [STOC'25] 表明，在 $\tilde{O}(n+k^2)$ 时间预处理后，无权距离 $ed(X,Y)$ 可以以每次更新 $\tilde{O}(k)$ 的时间维护；这里，$k$ 表示 $ed(X,Y)$ 的当前值。他们的算法可以推广到小整数权重，但其底层方法与大权重不兼容。\n我们的主要结果是一个动态算法，它在 $\tilde{O}(nk^\\gamma)$ 时间预处理后，以每次更新 $\tilde{O}(k^{3-\\gamma})$ 的时间维护 $ed^w(X,Y)$。其中，$\\gamma\\in [0,1]$ 是一个实数权衡参数，$k\\ge 1$ 是在预处理时固定的整数阈值，当 $ed^w(X,Y)>k$ 时返回 $\\infty$。我们用条件性下界补充了我们的算法，这些下界表明了我们的权衡在 $γ \\in [0.5,1)$ 范围内的精细最优性，并证明了我们固定 $k$ 的选择的合理性。", "summary": "该论文解决了演化字符串的动态加权编辑距离问题。它引入了一种新颖的动态算法，通过可调的权衡参数 $γ$ 来维护有界加权编辑距离 $ed^w(X,Y)$，在 $\tilde{O}(nk^γ)$ 预处理后实现 $\tilde{O}(k^{3-γ})$ 的更新时间。这项工作还提供了条件性下界，证明了所提出权衡在 $γ$ 的特定范围内的精细最优性。", "keywords": "动态算法, 加权编辑距离, 下界, 字符串, 有界", "comments": "该论文通过解决动态加权编辑距离问题，特别是针对任意/大权重的情况，做出了重要贡献，弥补了先前工作的局限性。引入可调的权衡参数 $γ$ 提供了灵活性，而严格的匹配下界为算法的最优性提供了强有力的理论保证。这对于字符串经常发生变化且需要高效跟踪加权编辑距离的应用至关重要。"}}
{"id": "2507.02755", "title": "Multi-agent Auditory Scene Analysis", "authors": ["Caleb Rascon", "Luis Gato-Diaz", "Eduardo García-Alarcón"], "categories": ["eess.AS", "cs.AI"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Submitted to Applied Intelligence", "url": "http://arxiv.org/abs/2507.02755v1", "summary": "Auditory scene analysis (ASA) aims to retrieve information from the acoustic\nenvironment, by carrying out three main tasks: sound source location,\nseparation, and classification. These tasks are traditionally executed with a\nlinear data flow, where the sound sources are first located; then, using their\nlocation, each source is separated into its own audio stream; from each of\nwhich, information is extracted that is relevant to the application scenario\n(audio event detection, speaker identification, emotion classification, etc.).\nHowever, running these tasks linearly increases the overall response time,\nwhile making the last tasks (separation and classification) highly sensitive to\nerrors of the first task (location). A considerable amount of effort and\ncomputational complexity has been employed in the state-of-the-art to develop\ntechniques that are the least error-prone possible. However, doing so gives\nrise to an ASA system that is non-viable in many applications that require a\nsmall computational footprint and a low response time, such as bioacoustics,\nhearing-aid design, search and rescue, human-robot interaction, etc. To this\neffect, in this work, a multi-agent approach is proposed to carry out ASA where\nthe tasks are run in parallel, with feedback loops between them to compensate\nfor local errors, such as: using the quality of the separation output to\ncorrect the location error; and using the classification result to reduce the\nlocalization's sensitivity towards interferences. The result is a multi-agent\nauditory scene analysis (MASA) system that is robust against local errors,\nwithout a considerable increase in complexity, and with a low response time.\nThe complete proposed MASA system is provided as a framework that uses\nopen-source tools for sound acquisition and reproduction (JACK) and inter-agent\ncommunication (ROS2), allowing users to add their own agents.", "comment": "Submitted to Applied Intelligence", "pdf_url": "http://arxiv.org/pdf/2507.02755v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多智能体听觉场景分析", "tldr": "传统的听觉场景分析（ASA）因线性处理而缓慢且易出错。本文提出了一种多智能体系统（MASA），通过并行执行任务并引入反馈机制，使其更加鲁棒、响应更快，适用于实时应用。", "motivation": "传统的听觉场景分析（ASA）以线性数据流执行任务（声源定位、分离、分类），导致整体响应时间增加，并且后续任务（分离和分类）对第一个任务（定位）的误差高度敏感。这使得传统的ASA系统在需要小计算开销和低响应时间的应用中（如生物声学、助听器设计、搜救、人机交互等）不可行。", "method": "本文提出了一种多智能体方法来执行听觉场景分析（MASA），其中任务并行运行，并通过它们之间的反馈循环来补偿局部误差。例如，利用分离输出的质量来纠正定位误差；利用分类结果来降低定位对干扰的敏感度。所提出的MASA系统作为一个框架提供，使用开源工具进行声音采集和再现（JACK）以及智能体间通信（ROS2），并允许用户添加自己的智能体。", "result": "所提出的多智能体听觉场景分析（MASA）系统对局部误差具有鲁棒性，没有显著增加复杂性，并且具有低响应时间。", "conclusion": "本文提出并实现了一个多智能体听觉场景分析（MASA）系统，通过并行处理和反馈机制，有效解决了传统线性ASA的响应时间长和误差敏感性高的问题，提供了一个在计算开销和响应时间受限应用中可行的鲁棒性解决方案。", "translation": "听觉场景分析（ASA）旨在通过执行三个主要任务：声源定位、分离和分类，从声学环境中检索信息。这些任务传统上以线性数据流执行，即首先定位声源；然后，利用其位置，将每个声源分离成自己的音频流；再从每个音频流中提取与应用场景（音频事件检测、说话人识别、情感分类等）相关的信息。然而，线性运行这些任务会增加整体响应时间，同时使最后的任务（分离和分类）对第一个任务（定位）的误差高度敏感。最先进的技术已经投入了大量的精力和计算复杂性来开发尽可能少出错的技术。然而，这样做会导致ASA系统在许多需要小计算开销和低响应时间的应用中不可行，例如生物声学、助听器设计、搜救、人机交互等。为此，在这项工作中，提出了一种多智能体方法来执行ASA，其中任务并行运行，并通过它们之间的反馈循环来补偿局部误差，例如：利用分离输出的质量来纠正定位误差；利用分类结果来降低定位对干扰的敏感度。结果是一个多智能体听觉场景分析（MASA）系统，该系统对局部误差具有鲁棒性，没有显著增加复杂性，并且具有低响应时间。所提出的完整MASA系统作为一个框架提供，该框架使用开源工具进行声音采集和再现（JACK）和智能体间通信（ROS2），允许用户添加自己的智能体。", "summary": "本文提出了一种多智能体听觉场景分析（MASA）方法，以解决传统听觉场景分析（ASA）因线性处理导致的响应时间长和误差敏感性高的问题。MASA系统通过并行执行声源定位、分离和分类任务，并引入智能体间的反馈循环来实时纠正局部误差。这种方法显著提高了系统的鲁棒性，降低了响应时间，且未显著增加计算复杂性，使其适用于对计算资源和实时性要求较高的应用。该系统以开源框架形式提供，支持用户自定义智能体。", "keywords": "听觉场景分析, 多智能体系统, 并行处理, 反馈循环, 实时", "comments": "本文的创新之处在于将传统的线性听觉场景分析（ASA）转变为并行、反馈驱动的多智能体架构（MASA）。这种范式转变显著提升了系统的鲁棒性和响应速度，对于实时和资源受限的应用至关重要。此外，系统采用开源工具并设计为模块化框架，允许用户添加自定义智能体，这大大增强了其实用性和可扩展性。"}}
{"id": "2507.02139", "title": "When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence in SDG Search", "authors": ["William A. Ingram", "Bipasha Banerjee", "Edward A. Fox"], "categories": ["cs.IR", "cs.AI", "cs.DL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy, July 17, 2025", "url": "http://arxiv.org/abs/2507.02139v1", "summary": "Large language models (LLMs) are increasingly used to assign document\nrelevance labels in information retrieval pipelines, especially in domains\nlacking human-labeled data. However, different models often disagree on\nborderline cases, raising concerns about how such disagreement affects\ndownstream retrieval. This study examines labeling disagreement between two\nopen-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to\nSustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement\nsubsets and examine their lexical properties, rank-order behavior, and\nclassification predictability. Our results show that model disagreement is\nsystematic, not random: disagreement cases exhibit consistent lexical patterns,\nproduce divergent top-ranked outputs under shared scoring functions, and are\ndistinguishable with AUCs above 0.74 using simple classifiers. These findings\nsuggest that LLM-based filtering introduces structured variability in document\nretrieval, even under controlled prompting and shared ranking logic. We propose\nusing classification disagreement as an object of analysis in retrieval\nevaluation, particularly in policy-relevant or thematic search tasks.", "comment": "Presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy, July 17,\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.02139v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "当大型语言模型产生分歧时：诊断可持续发展目标搜索中的相关性过滤偏差和检索分歧", "tldr": "本研究探讨了大型语言模型（LLMs）在信息检索中进行文档相关性标注时出现的分歧，发现这种分歧是系统性的，而非随机的，并建议将分类分歧作为检索评估的一个分析对象。", "motivation": "大型语言模型（LLMs）在信息检索中被广泛用于文档相关性标注，尤其是在缺乏人工标注数据的领域。然而，不同模型在边缘案例上常常出现分歧，这引发了人们对这种分歧如何影响下游检索的担忧。", "method": "本研究选取了两个开源LLM模型（LLaMA和Qwen），在一个关于可持续发展目标（SDGs）1、3、7的学术摘要语料库上，分析它们在标注相关性时的分歧。研究方法包括隔离分歧子集，并检查其词汇属性、排序行为和分类可预测性。", "result": "研究结果表明，模型分歧是系统性的，而非随机的：分歧案例表现出一致的词汇模式，在共享评分函数下产生不同的顶级检索结果，并且使用简单分类器可以以高于0.74的AUC值进行区分。", "conclusion": "这些发现表明，即使在受控的提示和共享的排序逻辑下，基于LLM的过滤也会在文档检索中引入结构化的变异性。研究建议将分类分歧作为检索评估的一个分析对象，特别是在政策相关或主题搜索任务中。", "translation": "大型语言模型（LLMs）正越来越多地用于信息检索流程中分配文档相关性标签，尤其是在缺乏人工标注数据的领域。然而，不同的模型在边缘案例上常常出现分歧，这引发了人们对这种分歧如何影响下游检索的担忧。本研究检查了两个开源LLM模型（LLaMA和Qwen）在一个关于可持续发展目标（SDGs）1、3、7的学术摘要语料库上的标签分歧。我们隔离了分歧子集，并检查了它们的词汇属性、排序行为和分类可预测性。我们的结果表明，模型分歧是系统性的，而非随机的：分歧案例表现出一致的词汇模式，在共享评分函数下产生不同的顶级检索结果，并且使用简单分类器可以以高于0.74的AUC值进行区分。这些发现表明，即使在受控的提示和共享的排序逻辑下，基于LLM的过滤也会在文档检索中引入结构化的变异性。我们建议将分类分歧作为检索评估的一个分析对象，特别是在政策相关或主题搜索任务中。", "summary": "本研究调查了大型语言模型（LLMs）在信息检索中进行文档相关性标注时，不同模型间的分歧问题。通过分析LLaMA和Qwen在可持续发展目标相关学术摘要上的表现，发现模型分歧并非随机，而是具有系统性的词汇模式，导致检索结果分歧，并且可以被预测。这表明LLM过滤引入了结构化变异，研究建议将这种分类分歧作为未来检索评估的关键分析点。", "keywords": "大型语言模型, 相关性过滤, 检索分歧, 可持续发展目标, 模型分歧", "comments": "本文创新性地诊断了大型语言模型在文档相关性标注中的分歧现象，揭示了这种分歧的系统性而非随机性。这对于依赖LLM进行信息检索，尤其是在缺乏人工标注数据或政策敏感领域，具有重要的指导意义。研究提出的将分类分歧作为检索评估对象的建议，为未来评估LLM在检索系统中的表现提供了新的视角和方法。"}}
{"id": "2507.02861", "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans", "authors": ["Zhening Huang", "Xiaoyang Wu", "Fangcheng Zhong", "Hengshuang Zhao", "Matthias Nießner", "Joan Lasenby"], "categories": ["cs.CV", "cs.AI", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL ; Video: this https URL", "url": "http://arxiv.org/abs/2507.02861v1", "summary": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\nenvironments into compact, realistic, and interactive 3D virtual replicas.\nLiteReality not only reconstructs scenes that visually resemble reality but\nalso supports key features essential for graphics pipelines -- such as object\nindividuality, articulation, high-quality physically based rendering materials,\nand physically based interaction. At its core, LiteReality first performs scene\nunderstanding and parses the results into a coherent 3D layout and objects with\nthe help of a structured scene graph. It then reconstructs the scene by\nretrieving the most visually similar 3D artist-crafted models from a curated\nasset database. Next, the Material Painting module enhances realism by\nrecovering high-quality, spatially varying materials. Finally, the\nreconstructed scene is integrated into a simulation engine with basic physical\nproperties to enable interactive behavior. The resulting scenes are compact,\neditable, and fully compatible with standard graphics pipelines, making them\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\naddition, LiteReality introduces a training-free object retrieval module that\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\nalong with a robust material painting module capable of transferring\nappearances from images of any style to 3D assets -- even under severe\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\nLiteReality on both real-life scans and public datasets. Project page:\nhttps://litereality.github.io; Video:\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c", "comment": "Project Page: https://litereality.github.io; Video:\n  https://www.youtube.com/watch?v=ecK9m3LXg2c&feature=youtu.be", "pdf_url": "http://arxiv.org/pdf/2507.02861v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LiteReality：基于RGB-D扫描的图形就绪型三维场景重建", "tldr": "LiteReality是一个将RGB-D扫描的室内环境转换为紧凑、逼真、可交互的3D虚拟副本的新型管道，支持图形管道所需的核心特性，并引入了无需训练的对象检索模块和鲁棒的材质绘制模块。", "motivation": "现有的3D场景重建方法可能无法同时提供视觉逼真度、紧凑性、可编辑性以及与标准图形管道兼容的关键特性（如对象独立性、关节、高质量PBR材质和物理交互）。LiteReality旨在解决这些挑战，为AR/VR、游戏、机器人和数字孪生等应用提供高质量的3D虚拟副本。", "method": "LiteReality管道包括以下步骤：1. 场景理解：将RGB-D扫描结果解析为连贯的3D布局和带有结构化场景图的对象。2. 模型检索：从精选的资产数据库中检索视觉上最相似的3D艺术家制作的模型。3. 材质绘制：通过材质绘制模块恢复高质量、空间变化的材质以增强真实感。4. 仿真集成：将重建的场景与基本物理属性集成到仿真引擎中，以实现交互行为。该方法还引入了无需训练的对象检索模块和鲁棒的材质绘制模块。", "result": "LiteReality重建的场景紧凑、可编辑，并与标准图形管道完全兼容。其无需训练的对象检索模块在Scan2CAD基准测试上实现了最先进的相似性性能。材质绘制模块能够在严重错位、遮挡和光照不佳的情况下，将任意风格图像的外观转移到3D资产上。该方法在真实扫描和公共数据集上都证明了其有效性。", "conclusion": "LiteReality成功地提供了一个从RGB-D扫描中重建图形就绪型3D场景的全面解决方案，生成的场景不仅视觉逼真，而且支持高级图形功能和物理交互，适用于多种应用。其创新的无训练对象检索和鲁棒的材质绘制模块显著提升了性能和实用性。", "translation": "我们提出LiteReality，一个新颖的管道，将室内环境的RGB-D扫描转换为紧凑、逼真和交互式的3D虚拟副本。LiteReality不仅重建出视觉上与现实相似的场景，还支持图形管道所需的核心特性——例如对象独立性、关节、高质量基于物理的渲染材质以及基于物理的交互。LiteReality的核心是首先执行场景理解，并在结构化场景图的帮助下将结果解析为连贯的3D布局和对象。然后，它通过从精心策划的资产数据库中检索视觉上最相似的3D艺术家制作的模型来重建场景。接下来，材质绘制模块通过恢复高质量、空间变化的材质来增强真实感。最后，重建的场景与基本物理属性集成到仿真引擎中，以实现交互行为。生成的场景紧凑、可编辑，并与标准图形管道完全兼容，使其适用于AR/VR、游戏、机器人和数字孪生等应用。此外，LiteReality引入了一个无需训练的对象检索模块，在Scan2CAD基准测试上实现了最先进的相似性性能，以及一个鲁棒的材质绘制模块，即使在严重错位、遮挡和光照不佳的情况下，也能够将任意风格图像的外观转移到3D资产上。我们在真实扫描和公共数据集上都证明了LiteReality的有效性。项目页面：https://litereality.github.io；视频：https://www.youtube.com/watch?v=ecK9m3LXg2c", "summary": "LiteReality是一个创新的管道，旨在将RGB-D扫描的室内环境转化为紧凑、逼真且可交互的3D虚拟副本。该系统通过场景理解、从资产数据库中检索相似3D模型、高质量材质绘制以及集成物理属性，确保重建的场景不仅视觉上逼真，而且具备对象独立性、关节、PBR材质和物理交互等图形管道所需的核心功能。LiteReality还引入了无需训练的对象检索模块和鲁棒的材质绘制模块，分别在Scan2CAD基准测试上达到先进性能并能处理复杂条件下的外观转移。其成果适用于AR/VR、游戏、机器人和数字孪生等领域。", "keywords": "3D场景重建, RGB-D扫描, 图形管道, 材质绘制, 对象检索", "comments": "LiteReality的创新之处在于其端到端的管道，不仅关注视觉真实感，还特别强调了对标准图形管道关键特性的支持，如对象独立性和物理交互，这在现有重建方法中并不常见。其无训练的对象检索模块和鲁棒的材质绘制模块是技术亮点，尤其是在处理复杂输入条件下的表现，大大提升了实用性。该方法为AR/VR、游戏和机器人等下游应用提供了高质量、可编辑的3D资产，具有重要的实际价值。"}}
{"id": "2507.02381", "title": "Running-time Analysis of ($μ+λ$) Evolutionary Combinatorial Optimization Based on Multiple-gain Estimation", "authors": ["Min Huang", "Pengxiang Chen", "Han Huang", "Tongli He", "Yushan Zhang", "Zhifeng Hao"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02381v1", "summary": "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. However, theoretical results\nregarding the $(\\mu+\\lambda)$ evolutionary algorithm (EA) for combinatorial\noptimization problems remain relatively scarce compared to those for simple\npseudo-Boolean problems. This paper proposes a multiple-gain model to analyze\nthe running time of EAs for combinatorial optimization problems. The proposed\nmodel is an improved version of the average gain model, which is a\nfitness-difference drift approach under the sigma-algebra condition to estimate\nthe running time of evolutionary numerical optimization. The improvement yields\na framework for estimating the expected first hitting time of a stochastic\nprocess in both average-case and worst-case scenarios. It also introduces novel\nrunning-time results of evolutionary combinatorial optimization, including two\ntighter time complexity upper bounds than the known results in the case of\n($\\mu+\\lambda$) EA for the knapsack problem with favorably correlated weights,\na closed-form expression of time complexity upper bound in the case of\n($\\mu+\\lambda$) EA for general $k$-MAX-SAT problems and a tighter time\ncomplexity upper bounds than the known results in the case of ($\\mu+\\lambda$)\nEA for the traveling salesperson problem. Experimental results indicate that\nthe practical running time aligns with the theoretical results, verifying that\nthe multiple-gain model is an effective tool for running-time analysis of\n($\\mu+\\lambda$) EA for combinatorial optimization problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02381v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于多增益估计的($μ+λ$)演化组合优化运行时间分析", "tldr": "本文提出了一个多增益模型，用于分析($μ+λ$)演化算法在组合优化问题上的运行时间，并获得了比现有结果更紧密的上界，实验结果验证了模型的有效性。", "motivation": "演化组合优化的运行时间分析是演化计算中的一个基本课题。然而，与简单的伪布尔问题相比，关于($μ+\\lambda$)演化算法在组合优化问题上的理论结果相对稀缺。", "method": "本文提出了一个多增益模型来分析演化算法在组合优化问题上的运行时间。该模型是平均增益模型的改进版本，该模型是一种在sigma代数条件下估计演化数值优化运行时间的适应度差漂移方法。改进后的框架可以估计随机过程在平均情况和最坏情况下的预期首次命中时间。", "result": "本文提出了演化组合优化的新运行时间结果，包括：1. 在有利相关权重的背包问题中，($μ+\\lambda$)演化算法的两个比已知结果更紧密的时间复杂度上界。2. 在一般$k$-MAX-SAT问题中，($μ+\\lambda$)演化算法的时间复杂度上界的闭合形式表达式。3. 在旅行商问题中，($μ+\\lambda$)演化算法的一个比已知结果更紧密的时间复杂度上界。实验结果表明实际运行时间与理论结果一致。", "conclusion": "多增益模型是分析($μ+\\lambda$)演化算法在组合优化问题上运行时间的有效工具。", "translation": "演化组合优化的运行时间分析是演化计算中的一个基本课题。然而，与简单的伪布尔问题相比，关于($μ+\\lambda$)演化算法在组合优化问题上的理论结果相对稀缺。本文提出了一个多增益模型来分析演化算法在组合优化问题上的运行时间。所提出的模型是平均增益模型的改进版本，该模型是一种在sigma代数条件下估计演化数值优化运行时间的适应度差漂移方法。这项改进为估计随机过程在平均情况和最坏情况下的预期首次命中时间提供了一个框架。它还引入了演化组合优化的新运行时间结果，包括：在有利相关权重的背包问题中，($μ+\\lambda$)演化算法的两个比已知结果更紧密的时间复杂度上界；在一般$k$-MAX-SAT问题中，($μ+\\lambda$)演化算法的时间复杂度上界的闭合形式表达式；以及在旅行商问题中，($μ+\\lambda$)演化算法的一个比已知结果更紧密的时间复杂度上界。实验结果表明实际运行时间与理论结果一致，验证了多增益模型是分析($μ+\\lambda$)演化算法在组合优化问题上运行时间的有效工具。", "summary": "本文针对($μ+\\lambda$)演化算法在组合优化问题上理论运行时间分析稀缺的问题，提出了一种改进的多增益模型。该模型是平均增益模型的扩展，能够估计随机过程的预期首次命中时间。研究获得了背包问题、k-MAX-SAT问题和旅行商问题中($μ+\\lambda$)演化算法的更紧密或闭合形式的时间复杂度上界，并通过实验验证了模型的有效性。", "keywords": "演化算法, 组合优化, 运行时间分析, 多增益模型, 时间复杂度", "comments": "本文的创新点在于提出了多增益模型，改进了传统的平均增益模型，并将其应用于($μ+\\lambda$)演化算法在多种组合优化问题（如背包问题、k-MAX-SAT和旅行商问题）的运行时间分析中，获得了比现有结果更紧密的理论上界。这对于深化演化算法的理论理解具有重要意义，尤其是在复杂组合优化领域的应用。"}}
{"id": "2507.02259", "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.02259v1", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "comment": "Project Page: https://memagent-sialab.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.02259v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MemAgent：使用基于多对话强化学习的记忆代理重塑长上下文LLM", "tldr": "MemAgent引入了一种基于多对话强化学习的记忆代理，通过分段读取和覆盖策略处理超长文本，实现了显著的上下文外推能力，性能损失极小。", "motivation": "尽管在长度外推、高效注意力及记忆模块方面有所改进，但在长文本处理中，以线性复杂度处理无限长文档且在外推过程中不出现性能下降，仍然是最终的挑战。", "method": "本文直接对长文本任务进行端到端优化，并引入了一种新颖的代理工作流MemAgent。MemAgent通过分段读取文本并使用覆盖策略更新记忆。此外，还扩展了DAPO算法，通过独立上下文的多对话生成来促进训练。", "result": "MemAgent展示了卓越的长上下文处理能力，能够从在32K文本上训练的8K上下文外推到3.5M的问答任务，性能损失小于5%，并在512K RULER测试中达到95%以上的准确率。", "conclusion": "MemAgent通过其创新的记忆代理工作流和训练方法，有效解决了大型语言模型在处理超长上下文时的性能退化问题，展现了强大的上下文外推能力。", "translation": "尽管通过长度外推、高效注意力及记忆模块取得了改进，但在长文本处理中，以线性复杂度处理无限长文档且在外推过程中不出现性能下降，仍然是最终的挑战。我们直接以端到端的方式优化长文本任务，并引入了一种新颖的代理工作流MemAgent，它分段读取文本并使用覆盖策略更新记忆。我们扩展了DAPO算法，通过独立上下文的多对话生成来促进训练。MemAgent展示了卓越的长上下文能力，能够从在32K文本上训练的8K上下文外推到3.5M的问答任务，性能损失小于5%，并在512K RULER测试中达到95%以上的准确率。", "summary": "MemAgent是一种新颖的基于多对话强化学习的记忆代理，旨在解决长文本处理中无限长文档的挑战。它通过分段读取和覆盖记忆策略实现端到端优化，并扩展DAPO算法进行训练。实验证明，MemAgent在上下文外推方面表现出色，能在极低性能损失下处理百万级上下文。", "keywords": "MemAgent, 长上下文LLM, 记忆代理, 强化学习, 文本处理", "comments": "MemAgent的创新点在于其RL-based记忆代理和分段读取、覆盖记忆的策略，以及对DAPO算法的扩展，这为解决LLM处理超长上下文的挑战提供了新的思路，尤其是在保持线性复杂度的同时减少性能下降方面表现突出。"}}
{"id": "2507.02391", "title": "Posterior Transition Modeling for Unsupervised Diffusion-Based Speech Enhancement", "authors": ["Mostafa Sadeghi", "Jean-Eudes Ayilo", "Romain Serizel", "Xavier Alameda-Pineda"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02391v1", "summary": "We explore unsupervised speech enhancement using diffusion models as\nexpressive generative priors for clean speech. Existing approaches guide the\nreverse diffusion process using noisy speech through an approximate,\nnoise-perturbed likelihood score, combined with the unconditional score via a\ntrade-off hyperparameter. In this work, we propose two alternative algorithms\nthat directly model the conditional reverse transition distribution of\ndiffusion states. The first method integrates the diffusion prior with the\nobservation model in a principled way, removing the need for hyperparameter\ntuning. The second defines a diffusion process over the noisy speech itself,\nyielding a fully tractable and exact likelihood score. Experiments on the\nWSJ0-QUT and VoiceBank-DEMAND datasets demonstrate improved enhancement metrics\nand greater robustness to domain shifts compared to both supervised and\nunsupervised baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02391v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于无监督扩散语音增强的后验转换建模", "tldr": "本文提出了两种新的无监督扩散模型算法，通过直接建模条件逆向转换分布，改进了语音增强，并在域转移方面表现出更好的鲁棒性，无需超参数调优。", "motivation": "现有的无监督扩散语音增强方法通过近似的、受噪声扰动的似然分数引导逆向扩散过程，并结合无条件分数，需要一个权衡超参数。本文旨在解决这一问题。", "method": "本文提出了两种替代算法，直接建模扩散状态的条件逆向转换分布。第一种方法以原则性的方式将扩散先验与观测模型结合，无需超参数调优。第二种方法在带噪声语音本身上定义了一个扩散过程，得到了完全可处理和精确的似然分数。", "result": "在WSJ0-QUT和VoiceBank-DEMAND数据集上的实验表明，与有监督和无监督基线相比，所提出的方法改善了增强指标，并对域转移表现出更大的鲁棒性。", "conclusion": "通过直接建模扩散状态的条件逆向转换分布，本文提出的两种无监督扩散语音增强算法在性能和鲁棒性上超越了现有方法，其中一种方法还避免了超参数调优的需求。", "translation": "我们探索使用扩散模型作为清晰语音的富有表现力的生成先验进行无监督语音增强。现有方法通过近似的、受噪声扰动的似然分数，结合无条件分数，并通过一个权衡超参数来引导逆向扩散过程。在这项工作中，我们提出了两种替代算法，直接建模扩散状态的条件逆向转换分布。第一种方法以原则性的方式将扩散先验与观测模型结合，消除了超参数调优的需要。第二种方法在带噪声语音本身上定义了一个扩散过程，产生了完全可处理和精确的似然分数。在WSJ0-QUT和VoiceBank-DEMAND数据集上的实验表明，与有监督和无监督基线相比，增强指标得到了改善，并且对域转移表现出更大的鲁棒性。", "summary": "本文提出两种用于无监督语音增强的新型扩散模型算法，以解决现有方法对近似似然分数和超参数调优的依赖。通过直接建模扩散状态的条件逆向转换分布，其中一种方法将扩散先验与观测模型原则性结合，无需调优；另一种则在噪声语音上定义扩散过程，实现精确似然分数。实验证明，这些方法在语音增强性能和域转移鲁棒性上优于现有基线。", "keywords": "无监督语音增强, 扩散模型, 后验转换建模, 域转移, 超参数调优", "comments": "本文的创新点在于提出了两种直接建模扩散模型条件逆向转换分布的算法，这与现有通过近似似然分数引导逆向扩散过程的方法不同。特别是，第一种方法消除了超参数调优的需求，而第二种方法提供了完全可处理和精确的似然分数，这显著提升了无监督语音增强的实用性和理论严谨性。其在域转移方面的鲁棒性也显示了该方法的普适性潜力。"}}
{"id": "2507.02390", "title": "Evaluating Language Models For Threat Detection in IoT Security Logs", "authors": ["Jorge J. Tejero-Fernández", "Alfonso Sánchez-Macián"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02390v1", "summary": "Log analysis is a relevant research field in cybersecurity as they can\nprovide a source of information for the detection of threats to networks and\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\nlogs. Utilizing classical machine learning classifiers as a baseline, three\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\ndataset. LLMs give better results on multi-class attack classification than the\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\nwith those actions, the models are able to provide a combined detection and\nrecommendation guidance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02390v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "评估物联网安全日志中用于威胁检测的语言模型", "tldr": "本文提出并评估了一个利用微调大型语言模型（LLMs）进行物联网安全日志中异常检测和缓解建议的管道，结果显示LLMs在多类别攻击分类上优于基线模型。", "motivation": "日志分析在网络安全领域是一个重要的研究方向，因为它们能为网络和系统威胁检测提供信息来源。", "method": "本文提出了一个使用微调大型语言模型（LLMs）进行异常检测和缓解建议的管道，利用物联网安全日志。研究将三种开源LLMs与经典机器学习分类器作为基线进行比较，在二分类和多分类异常检测上采用零样本、少样本提示和微调三种策略，并使用物联网数据集。此外，通过将检测到的威胁映射到MITRE CAPEC，定义一套物联网特有的缓解措施，并用这些措施微调模型，使模型能够提供结合检测和建议的指导。", "result": "大型语言模型在多类别攻击分类上比相应的基线模型表现出更好的结果。通过威胁映射和特定缓解措施的微调，模型能够提供检测与推荐相结合的指导。", "conclusion": "大型语言模型可以有效用于物联网安全日志中的多类别威胁检测，并能提供相应的缓解建议。", "translation": "日志分析是网络安全领域的一个相关研究领域，因为它们可以为网络和系统威胁的检测提供信息来源。本文提出了一个使用微调大型语言模型（LLMs）进行异常检测和缓解建议的管道，利用物联网安全日志。研究利用经典机器学习分类器作为基线，比较了三种开源LLMs在二分类和多分类异常检测上的表现，采用了三种策略：零样本、少样本提示和使用物联网数据集进行微调。LLMs在多类别攻击分类上比相应的基线模型表现出更好的结果。通过将检测到的威胁映射到MITRE CAPEC，定义一套物联网特有的缓解措施，并用这些措施微调模型，模型能够提供结合检测和推荐的指导。", "summary": "本研究提出并评估了一个利用微调大型语言模型（LLMs）进行物联网安全日志中威胁检测和缓解建议的管道。通过与经典机器学习基线模型进行比较，研究发现LLMs在多类别攻击分类任务上表现更优。此外，通过将检测到的威胁映射到MITRE CAPEC并结合物联网特有的缓解措施进行模型微调，该方法能够提供综合性的威胁检测和缓解建议。", "keywords": "大型语言模型, 物联网安全, 威胁检测, 日志分析, 异常检测", "comments": "该论文的创新点在于将大型语言模型应用于物联网安全日志的威胁检测，并结合了缓解措施的推荐，形成了一个端到端的解决方案。这对于提升物联网安全威胁的自动化响应能力具有重要意义。LLMs在多类别分类上的优越性也展示了其在复杂安全场景中的潜力。"}}
{"id": "2507.02245", "title": "CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather", "authors": ["Minghao Ning", "Yufeng Yang", "Keqi Shu", "Shucheng Huang", "Jiaming Zhong", "Maryam Salehi", "Mahdi Rahmani", "Yukun Lu", "Chen Sun", "Aladdin Saleh", "Ehsan Hashemi", "Amir Khajepour"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to the IEEE Transactions on Robotics for review", "url": "http://arxiv.org/abs/2507.02245v1", "summary": "We present CoInfra, a large-scale cooperative infrastructure perception\nsystem and dataset designed to advance robust multi-agent perception under\nreal-world and adverse weather conditions. The CoInfra system includes 14 fully\nsynchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR,\ndeployed across a shared region and operating continuously to capture all\ntraffic participants in real-time. A robust, delay-aware synchronization\nprotocol and a scalable system architecture that supports real-time data\nfusion, OTA management, and remote monitoring are provided in this paper. On\nthe other hand, the dataset was collected in different weather scenarios,\nincluding sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR\nframes and 390k camera images from 8 infrastructure nodes that are globally\ntime-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding\nbox annotations for five object classes (i.e., car, bus, truck, person, and\nbicycle) are provided in both global and individual node frames, along with\nhigh-definition maps for contextual understanding. Baseline experiments\ndemonstrate the trade-offs between early and late fusion strategies, the\nsignificant benefits of HD map integration are discussed. By openly releasing\nour dataset, codebase, and system documentation at\nhttps://github.com/NingMingHao/CoInfra, we aim to enable reproducible research\nand drive progress in infrastructure-supported autonomous driving, particularly\nin challenging, real-world settings.", "comment": "This paper has been submitted to the IEEE Transactions on Robotics\n  for review", "pdf_url": "http://arxiv.org/pdf/2507.02245v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CoInfra：恶劣天气下的大规模协同基础设施感知系统与数据集", "tldr": "CoInfra是一个大规模协同基础设施感知系统和数据集，旨在恶劣天气下提升多智能体感知能力，并开放了其资源。", "motivation": "旨在推进在真实世界和恶劣天气条件下鲁棒的多智能体感知。", "method": "提出了CoInfra系统，包含14个完全同步的传感器节点，每个节点配备双RGB摄像头和激光雷达，部署在共享区域并持续运行。系统提供了鲁棒的、延迟感知的同步协议和可扩展的系统架构，支持实时数据融合、OTA管理和远程监控。数据集在晴天、雨天、冻雨和大雪等不同天气场景下收集，包含19.5万帧激光雷达数据和39万张相机图像，来自8个基础设施节点，并进行了全局时间对齐和空间校准。还提供了五类物体（汽车、公交车、卡车、行人、自行车）的3D边界框标注和高精地图。", "result": "基线实验展示了早期和晚期融合策略之间的权衡，并讨论了集成高精地图的显著优势。", "conclusion": "通过开放CoInfra数据集、代码库和系统文档，旨在实现可复现的研究，并推动基础设施支持的自动驾驶在具有挑战性的真实世界环境中的进展。", "translation": "我们提出了CoInfra，一个大规模协同基础设施感知系统和数据集，旨在推进在真实世界和恶劣天气条件下鲁棒的多智能体感知。CoInfra系统包含14个完全同步的传感器节点，每个节点配备双RGB摄像头和激光雷达，部署在共享区域并持续运行，以实时捕捉所有交通参与者。本文提供了一个鲁棒的、延迟感知的同步协议和一个支持实时数据融合、OTA管理和远程监控的可扩展系统架构。另一方面，该数据集在不同天气场景下收集，包括晴天、雨天、冻雨和大雪，包含来自8个基础设施节点的19.5万帧激光雷达数据和39万张相机图像，这些数据都经过全局时间对齐和空间校准。此外，还提供了五类物体（即汽车、公交车、卡车、行人、自行车）在全球和单个节点帧中的全面3D边界框标注，以及用于上下文理解的高精地图。基线实验展示了早期和晚期融合策略之间的权衡，并讨论了集成高精地图的显著优势。通过在https://github.com/NingMingHao/CoInfra 公开我们的数据集、代码库和系统文档，我们旨在实现可复现的研究，并推动基础设施支持的自动驾驶在具有挑战性的真实世界环境中的进展。", "summary": "CoInfra是一个大型协同基础设施感知系统和数据集，专为恶劣天气下的鲁棒多智能体感知而设计。该系统包含14个同步传感器节点（配备摄像头和激光雷达），支持实时数据融合和远程管理。数据集涵盖多种天气条件，包含大量带标注的激光雷达帧和图像，并提供高精地图。基线实验探讨了融合策略和高精地图的益处。研究团队已开源其数据集、代码和文档，以促进基础设施辅助自动驾驶在复杂真实场景中的研究进展。", "keywords": "协同感知, 基础设施感知, 恶劣天气, 多智能体, 数据集", "comments": "该论文的创新点在于构建了一个大规模、多传感器、协同的基础设施感知系统和数据集，特别关注了恶劣天气条件下的数据采集和感知挑战。其重要性在于为自动驾驶领域提供了宝贵的真实世界数据和系统范式，有助于推动基础设施辅助的自动驾驶技术在复杂环境中的发展。开放数据集和代码库的举措也极大地促进了研究的可复现性和社区合作。"}}
{"id": "2507.02182", "title": "Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models", "authors": ["Fangjian Lei", "Jiawen Liu", "Shayan Noei", "Ying Zou", "Derek Truong", "William Alexander"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02182v1", "summary": "Common Business Oriented Language (COBOL) is a programming language used to\ndevelop business applications that are widely adopted by financial, business,\nand government agencies. Due to its age, complexity, and declining number of\nCOBOL developers, maintaining COBOL codebases is becoming increasingly\nchallenging. In particular, the lack of documentation makes it difficult for\nnew developers to effectively understand and maintain COBOL systems. Existing\nresearch utilizes large language models (LLMs) to explain the functionality of\ncode snippets. However, COBOL presents unique challenges due to its\narchitectural and syntactical differences, which often cause its code to exceed\nthe token window size of LLMs. In this work, we propose a multi-agent approach\nthat leverages two LLM-based agents working collaboratively to generate\nexplanations for functions, files, and the overall project. These agents\nincorporate together by utilizing contextual information from the codebase into\nthe code explanation prompts. We evaluate the effectiveness of our approach\nusing 14 open-source, real-world COBOL projects. Our results indicate that our\napproach performs significantly better than the baseline in function code\nexplanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,\nchrF, and SentenceBERT scores, respectively. At the file level, our approach\neffectively explains both short and long COBOL files that exceed the token\nwindow size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in\nexplaining the purpose, functionality, and clarity of the generated\nexplanation. At the project level, our approach generates explanations that\nconvey the functionality and purpose of 82% of the selected projects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02182v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "增强COBOL代码解释：一种使用大型语言模型的多智能体方法", "tldr": "提出了一种多智能体方法，利用大型语言模型来克服COBOL代码复杂性和LLM令牌窗口限制，有效生成COBOL函数、文件和项目级别的代码解释。", "motivation": "COBOL代码库因其年代久远、复杂性高和开发者数量减少而难以维护，特别是缺乏文档导致新开发者难以理解。现有LLM方法在COBOL独特架构和语法下，常因代码超出令牌窗口而失效。", "method": "提出了一种多智能体方法，利用两个基于LLM的智能体协同工作。这些智能体通过将代码库中的上下文信息整合到代码解释提示中来生成函数、文件和整个项目的解释。", "result": "在函数代码解释方面，比基线方法在METEOR、chrF和SentenceBERT分数上分别提高了12.67%、18.59%和0.62%。在文件层面，能有效解释超出LLM令牌窗口大小的长短COBOL文件，在目的、功能和清晰度解释上分别超越基线4.21%、10.72%和14.68%。在项目层面，能为82%的所选项目生成传达功能和目的的解释。", "conclusion": "该多智能体方法能够显著提升COBOL代码解释的质量和范围，有效解决了COBOL的复杂性和LLM令牌窗口限制问题，尤其在函数、文件和项目级别表现出色。", "translation": "通用商业导向语言（COBOL）是一种用于开发商业应用程序的编程语言，被金融、商业和政府机构广泛采用。由于其年代久远、复杂性高以及COBOL开发人员数量的减少，维护COBOL代码库变得越来越具有挑战性。特别是文档的缺乏使得新开发人员难以有效地理解和维护COBOL系统。现有研究利用大型语言模型（LLMs）来解释代码片段的功能。然而，COBOL由于其架构和语法差异而带来了独特的挑战，这常常导致其代码超出LLM的令牌窗口大小。在这项工作中，我们提出了一种多智能体方法，利用两个基于LLM的智能体协同工作，为函数、文件和整个项目生成解释。这些智能体通过将代码库中的上下文信息整合到代码解释提示中来协同工作。我们使用14个开源、真实的COBOL项目评估了我们方法的有效性。我们的结果表明，在函数代码解释方面，我们的方法比基线方法表现显著更好，在METEOR、chrF和SentenceBERT分数方面分别提高了12.67%、18.59%和0.62%。在文件层面，我们的方法有效地解释了超出LLM令牌窗口大小的长短COBOL文件，并且在解释目的、功能和生成解释的清晰度方面分别超越基线4.21%、10.72%和14.68%。在项目层面，我们的方法生成的解释传达了82%所选项目的功能和目的。", "summary": "本研究针对COBOL代码库难以维护的问题，特别是文档缺乏和现有LLM无法处理COBOL复杂性及长代码的问题，提出了一种基于大型语言模型的多智能体方法。该方法通过两个LLM智能体协同工作，并结合上下文信息，为COBOL函数、文件和项目生成代码解释。实验结果表明，该方法在函数、文件和项目层面的解释效果均显著优于基线，有效提升了COBOL代码的可理解性。", "keywords": "COBOL, 代码解释, 大型语言模型, 多智能体, 遗留系统", "comments": "该论文通过引入多智能体协同工作机制，巧妙地解决了大型语言模型在处理COBOL等复杂且代码量大的遗留系统时面临的令牌窗口限制问题，具有重要的创新性。其方法不仅提升了代码解释的准确性，还扩展了LLM在遗留系统维护领域的应用潜力，对于缓解COBOL开发人员短缺和提高维护效率具有实际意义。"}}
{"id": "2507.02187", "title": "VergeIO: Depth-Aware Eye Interaction on Glasses", "authors": ["Xiyuxing Zhang", "Duc Vu", "Chengyi Shen", "Yuntao Wang", "Yuanchun Shi", "Justin Chan"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02187v1", "summary": "There is growing industry interest in creating unobtrusive designs for\nelectrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and\nApple eyewear). We present VergeIO, the first EOG-based glasses that enables\ndepth-aware eye interaction using vergence with an optimized electrode layout\nand novel smart glass prototype. It can distinguish between four and six\ndepth-based eye gestures with 83-98% accuracy using personalized models in a\nuser study across 11 users and 1,320 gesture instances. It generalizes to\nunseen users with an accuracy of 80-98% without any calibration. To reduce\nfalse detections, we incorporate a motion artifact detection pipeline and a\npreamble-based activation scheme. The system uses dry sensors without any\nadhesives or gel, and operates in real time with 3 mW power consumption by the\nsensing front-end, making it suitable for always-on sensing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02187v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "VergeIO：眼镜上的深度感知眼部交互", "tldr": "VergeIO是一种基于EOG的智能眼镜，首次实现了深度感知的眼部交互，通过优化电极布局和新型原型，能高精度识别深度眼部手势，且功耗低，适用于常开感知。", "motivation": "业界对在眼镜上实现不显眼的眼电图（EOG）眼部手势感应设计兴趣日益增长，现有技术可能缺乏深度感知能力。", "method": "本文提出了VergeIO，这是首个基于EOG的眼镜系统，利用聚合（vergence）实现深度感知眼部交互。它采用了优化的电极布局和新型智能眼镜原型。为减少误检，系统整合了运动伪影检测流程和基于前导码的激活方案。系统使用干式传感器，无需粘合剂或凝胶，并实时运行。", "result": "VergeIO在11名用户参与的1320个手势实例用户研究中，使用个性化模型能以83-98%的准确率区分四到六种基于深度的眼部手势。对于未见过的用户，无需校准即可达到80-98%的泛化准确率。传感前端功耗为3毫瓦。", "conclusion": "VergeIO系统使用干式传感器，无需粘合剂或凝胶，可实时操作且功耗低（3毫瓦），使其非常适合常开感知应用。", "translation": "业界对在眼镜上创建不显眼的眼电图（EOG）眼部手势感应设计（例如JINS MEME和Apple眼镜）的兴趣日益增长。我们提出了VergeIO，这是首款基于EOG的眼镜，通过聚合（vergence）并采用优化的电极布局和新颖的智能眼镜原型，实现了深度感知的眼部交互。在对11名用户进行、包含1,320个手势实例的用户研究中，它使用个性化模型能够以83-98%的准确率区分四到六种基于深度的眼部手势。对于未见过的用户，无需任何校准即可达到80-98%的泛化准确率。为了减少误检，我们整合了运动伪影检测流程和基于前导码的激活方案。该系统使用干式传感器，无需任何粘合剂或凝胶，并能实时运行，传感前端功耗仅为3毫瓦，使其适用于常开感知。", "summary": "VergeIO是一种创新的智能眼镜，首次实现了基于EOG的深度感知眼部交互。它通过优化电极布局和利用眼球聚合运动，能够高精度识别多达六种深度眼部手势，在个性化模型下准确率达83-98%，对新用户也能达到80-98%的泛化准确率。系统采用干式传感器，功耗低至3毫瓦，并具备运动伪影检测和激活机制，使其成为适合常开感知的实时、无创解决方案。", "keywords": "深度感知, 眼部交互, EOG, 智能眼镜, 聚合", "comments": "VergeIO的创新点在于首次将EOG技术应用于眼镜实现深度感知的眼部交互，解决了现有眼动追踪技术在深度维度上的局限性。其低功耗、干式传感器设计和无需校准的泛化能力使其在实际应用中具有巨大潜力，尤其适用于未来AR/VR眼镜的自然交互。运动伪影检测和激活方案的引入也显著提升了系统的鲁棒性。"}}
{"id": "2507.02773", "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs", "authors": ["Yuzhang Xie", "Hejie Cui", "Ziyang Zhang", "Jiaying Lu", "Kai Shu", "Fadi Nahab", "Xiao Hu", "Carl Yang"], "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02773v1", "summary": "Medical diagnosis prediction plays a critical role in disease detection and\npersonalized healthcare. While machine learning (ML) models have been widely\nadopted for this task, their reliance on supervised training limits their\nability to generalize to unseen cases, particularly given the high cost of\nacquiring large, labeled datasets. Large language models (LLMs) have shown\npromise in leveraging language abilities and biomedical knowledge for diagnosis\nprediction. However, they often suffer from hallucinations, lack structured\nmedical reasoning, and produce useless outputs. To address these challenges, we\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\nLLM-based diagnosis prediction through a multi-agent architecture. Our\nframework consists of a linkage agent for attribute mapping, a retrieval agent\nfor structured knowledge extraction, and a prediction agent that iteratively\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\nenhances diagnostic reliability efficiently, offering a scalable and\ninterpretable solution for zero-shot medical diagnosis prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02773v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "KERAP：一种用于准确零样本诊断预测的知识增强推理方法，使用多智能体大型语言模型", "tldr": "KERAP是一种知识图谱增强的多智能体LLM方法，用于零样本医学诊断预测，旨在解决传统ML模型泛化性差和LLM幻觉问题，并经验证能有效提高诊断可靠性。", "motivation": "传统的机器学习模型在医学诊断预测中因依赖监督训练而难以泛化到未见案例，且标注数据集成本高昂。大型语言模型（LLMs）虽有潜力，但常出现幻觉、缺乏结构化医学推理并产生无用输出。因此，需要一种新方法来克服这些挑战。", "method": "我们提出了KERAP，一种知识图谱（KG）增强的推理方法，通过多智能体架构改进基于LLM的诊断预测。该框架包含一个用于属性映射的链接代理、一个用于结构化知识提取的检索代理，以及一个迭代细化诊断预测的预测代理。", "result": "实验结果表明，KERAP有效提高了诊断可靠性，为零样本医学诊断预测提供了一种可扩展且可解释的解决方案。", "conclusion": "KERAP通过结合知识图谱和多智能体LLM架构，成功解决了零样本医学诊断预测中传统模型的泛化问题和LLM的局限性，提供了一个高效、可扩展且可解释的诊断工具。", "translation": "医学诊断预测在疾病检测和个性化医疗中起着关键作用。尽管机器学习（ML）模型已广泛应用于此任务，但它们对监督训练的依赖限制了其泛化到未见案例的能力，尤其是在获取大型标注数据集成本高昂的情况下。大型语言模型（LLMs）在利用语言能力和生物医学知识进行诊断预测方面已显示出前景。然而，它们常常遭受幻觉、缺乏结构化医学推理并产生无用输出的困扰。为了解决这些挑战，我们提出了KERAP，一种知识图谱（KG）增强的推理方法，通过多智能体架构改进基于LLM的诊断预测。我们的框架由一个用于属性映射的链接代理、一个用于结构化知识提取的检索代理，以及一个迭代细化诊断预测的预测代理组成。实验结果表明，KERAP有效提高了诊断可靠性，为零样本医学诊断预测提供了一种可扩展且可解释的解决方案。", "summary": "本文提出了KERAP，一种知识图谱（KG）增强的多智能体大型语言模型（LLM）推理方法，旨在解决传统机器学习模型在医学诊断中泛化能力受限以及LLM面临幻觉和缺乏结构化推理的问题。KERAP框架包含链接、检索和预测三个代理，通过迭代细化和知识提取来提高诊断准确性。实验证明，KERAP能有效提升诊断可靠性，为零样本医学诊断预测提供可扩展且可解释的解决方案。", "keywords": "零样本诊断预测, 大型语言模型, 知识图谱, 多智能体系统, 医学人工智能", "comments": "KERAP的创新之处在于结合了知识图谱的结构化知识与多智能体LLM的推理能力，有效克服了单一LLM的幻觉和缺乏结构化推理的缺点，并解决了传统ML模型在零样本场景下的泛化难题。其多代理架构设计提升了可解释性，使其在医疗诊断这一高风险领域具有重要应用潜力。"}}
{"id": "2507.02021", "title": "REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks", "authors": ["Eyad Gad", "Gad Gad", "Mostafa M. Fouda", "Mohamed I. Ibrahem", "Muhammad Ismail", "Zubair Md Fadlullah"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      2025 International Conference on Communications", "url": "http://arxiv.org/abs/2507.02021v1", "summary": "With the rise of Software-Defined Networking (SDN) for managing traffic and\nensuring seamless operations across interconnected devices, challenges arise\nwhen SDN controllers share infrastructure with deep learning (DL) workloads.\nResource contention between DL training and SDN operations, especially in\nlatency-sensitive IoT environments, can degrade SDN's responsiveness and\ncompromise network performance. Federated Learning (FL) helps address some of\nthese concerns by decentralizing DL training to edge devices, thus reducing\ndata transmission costs and enhancing privacy. Yet, the computational demands\nof DL training can still interfere with SDN's performance, especially under the\ncontinuous data streams characteristic of IoT systems. To mitigate this issue,\nwe propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),\na resampling technique that optimizes DL training by prioritizing misclassified\nsamples and excluding redundant data, inspired by AdaBoost. REDUS reduces the\nnumber of training samples per epoch, thereby conserving computational\nresources, reducing energy consumption, and accelerating convergence without\nsignificantly impacting accuracy. Applied within an FL setup, REDUS enhances\nthe efficiency of model training on resource-limited edge devices while\nmaintaining network performance. In this paper, REDUS is evaluated on the\nCICIoT2023 dataset for IoT attack detection, showing a training time reduction\nof up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable\nand practical solution for intelligent networks.", "comment": "2025 International Conference on Communications", "pdf_url": "http://arxiv.org/pdf/2507.02021v1", "cate": "cs.NI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "REDUS：集中式和联邦式物联网网络中高效深度学习的自适应重采样", "tldr": "REDUS是一种自适应重采样技术，通过优先处理错误分类样本和排除冗余数据，显著减少深度学习训练时间，同时保持高精度，适用于集中式和联邦式物联网网络。", "motivation": "在物联网环境中，软件定义网络（SDN）控制器与深度学习（DL）工作负载共享基础设施时，会产生资源争用，尤其是在延迟敏感的物联网环境中，这会降低SDN的响应能力并损害网络性能。尽管联邦学习（FL）通过将DL训练去中心化到边缘设备来缓解部分问题，但DL训练的计算需求仍然可能干扰SDN的性能，尤其是在物联网系统特有的连续数据流下。", "method": "本文提出了REDUS（智能网络中高效数据利用的重采样），这是一种受AdaBoost启发的重采样技术，通过优先处理错误分类样本并排除冗余数据来优化深度学习训练。REDUS减少了每个epoch的训练样本数量，从而节省了计算资源，降低了能耗，并在不显著影响准确性的情况下加速了收敛。在联邦学习设置中应用时，REDUS增强了资源受限边缘设备上的模型训练效率。", "result": "REDUS在CICIoT2023数据集上进行物联网攻击检测评估，结果显示训练时间减少高达72.6%，而准确性损失仅为1.62%。该方法在联邦学习设置中也有效，提高了资源受限边缘设备的训练效率。", "conclusion": "REDUS提供了一种可扩展且实用的解决方案，用于解决集中式和联邦式物联网网络中深度学习训练与SDN操作之间的资源争用问题，显著提高训练效率并保持高精度。", "translation": "随着软件定义网络（SDN）的兴起，用于管理流量和确保互联设备之间的无缝操作，当SDN控制器与深度学习（DL）工作负载共享基础设施时，挑战随之而来。DL训练与SDN操作之间的资源争用，特别是在延迟敏感的物联网环境中，可能会降低SDN的响应能力并损害网络性能。联邦学习（FL）通过将DL训练去中心化到边缘设备，从而降低数据传输成本和增强隐私性，有助于解决其中一些问题。然而，DL训练的计算需求仍然可能干扰SDN的性能，尤其是在物联网系统特有的连续数据流下。为了缓解这个问题，我们提出了REDUS（智能网络中高效数据利用的重采样），这是一种受AdaBoost启发的重采样技术，通过优先处理错误分类样本和排除冗余数据来优化DL训练。REDUS减少了每个epoch的训练样本数量，从而节省了计算资源，降低了能耗，并在不显著影响准确性的情况下加速了收敛。在FL设置中应用时，REDUS增强了资源受限边缘设备上的模型训练效率，同时保持网络性能。在本文中，REDUS在CICIoT2023数据集上进行了物联网攻击检测评估，显示训练时间减少高达72.6%，而准确性损失仅为1.62%，为智能网络提供了一种可扩展且实用的解决方案。", "summary": "该论文提出了一种名为REDUS的自适应重采样技术，旨在解决物联网网络中深度学习训练与软件定义网络操作之间的资源争用问题。REDUS受AdaBoost启发，通过优先处理错误分类样本和排除冗余数据来优化深度学习训练，从而减少每个epoch所需的训练样本数量。实验结果表明，REDUS在物联网攻击检测任务上，能将训练时间缩短高达72.6%，同时仅带来1.62%的最小准确性损失。该方法尤其适用于资源受限的边缘设备，并在集中式和联邦式物联网网络中提供了高效、可扩展且实用的深度学习解决方案。", "keywords": "自适应重采样, 深度学习, 联邦学习, 物联网, 资源优化", "comments": "REDUS的创新之处在于其将AdaBoost思想应用于深度学习的重采样，通过选择性地训练样本来提高效率，这对于资源受限的物联网环境尤为重要。其在训练时间上的显著提升（高达72.6%）和极小的准确性损失，表明了其在实际应用中的巨大潜力。该方法解决了深度学习在边缘设备部署中的关键挑战，即计算资源和能耗问题，为智能物联网网络提供了有价值的优化方向。"}}
{"id": "2507.02226", "title": "DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs", "authors": ["Mohammad Akyash", "Kimia Azar", "Hadi Kamali"], "categories": ["cs.PL", "cs.AR", "cs.LG"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Accepted to the International Conference on Computer-Aided Design (ICCAD 2025)", "url": "http://arxiv.org/abs/2507.02226v1", "summary": "As one of their many applications, large language models (LLMs) have recently\nshown promise in automating register transfer level (RTL) code generation.\nHowever, conventional LLM decoding strategies, originally designed for natural\nlanguage, often fail to meet the structural and semantic demands of RTL,\nleading to hallucinated, repetitive, or invalid code outputs. In this paper, we\nfirst investigate the root causes of these decoding failures through an\nempirical analysis of token-level entropy during RTL generation. Our findings\nreveal that LLMs exhibit low confidence in regions of structural ambiguity or\nsemantic complexity, showing that standard decoding strategies fail to\ndifferentiate between regions requiring determinism (syntax-critical regions)\nand those that benefit from creative exploratory variability (design-critical\nregions). Then, to overcome this, we introduce DecoRTL, a novel run-time\ndecoding strategy, that is both syntax-aware and contrastive for RTL code\ngeneration. DecoRTL integrates two complementary components: (i)\nself-consistency sampling, which generates multiple candidates and re-ranks\nthem based on token-level agreement to promote correctness while maintaining\ndiversity; and (ii) syntax-aware temperature adaptation, which classifies\ntokens by their syntactical and functional roles and adjusts the sampling\ntemperature accordingly, enforcing low temperature for syntax-critical tokens\nand higher temperature for exploratory ones. Our approach operates entirely at\ninference time without requiring any additional model fine-tuning. Through\nevaluations on multiple open-source LLMs using the VerilogEval benchmark, we\ndemonstrate significant improvements in syntactic validity, functional\ncorrectness, and output diversity, while the execution overhead (performance\noverhead) is imperceptible.", "comment": "Accepted to the International Conference on Computer-Aided Design\n  (ICCAD 2025)", "pdf_url": "http://arxiv.org/pdf/2507.02226v1", "cate": "cs.PL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DecoRTL：一种用于LLM生成RTL代码的运行时解码框架", "tldr": "DecoRTL是一种新的运行时解码策略，专为LLM生成RTL代码设计，通过语法感知和对比采样，显著提高了代码的语法有效性、功能正确性和输出多样性，且无需模型微调。", "motivation": "现有的大型语言模型（LLMs）在RTL代码生成方面表现出潜力，但传统的解码策略未能满足RTL的结构和语义要求，导致生成幻觉、重复或无效的代码输出。研究发现LLMs在结构模糊或语义复杂的区域表现出低置信度。", "method": "本文首先通过对RTL生成过程中令牌级熵的实证分析，调查了解码失败的根本原因。然后提出了DecoRTL，这是一种新颖的运行时解码策略，它既语法感知又具有对比性。DecoRTL包含两个互补组件：(i) 自洽性采样，生成多个候选并根据令牌级一致性进行重新排序；(ii) 语法感知温度适应，根据令牌的语法和功能角色调整采样温度，对语法关键令牌强制低温度，对探索性令牌使用高温度。该方法完全在推理时操作，无需额外模型微调。", "result": "在VerilogEval基准测试中对多个开源LLM进行评估，结果表明DecoRTL在语法有效性、功能正确性和输出多样性方面有显著改进，而执行开销（性能开销）可以忽略不计。", "conclusion": "DecoRTL通过其语法感知和对比性运行时解码策略，有效解决了LLM在RTL代码生成中遇到的挑战，显著提高了代码质量，且无需模型微调。", "translation": "作为众多应用之一，大型语言模型（LLMs）最近在自动化寄存器传输级（RTL）代码生成方面显示出前景。然而，传统的LLM解码策略，最初是为自然语言设计的，通常无法满足RTL的结构和语义需求，导致生成幻觉、重复或无效的代码输出。在本文中，我们首先通过对RTL生成过程中令牌级熵的实证分析，调查了这些解码失败的根本原因。我们的研究结果表明，LLMs在结构模糊或语义复杂的区域表现出低置信度，这表明标准解码策略未能区分需要确定性（语法关键区域）和受益于创造性探索性变异（设计关键区域）的区域。然后，为了克服这个问题，我们引入了DecoRTL，一种新颖的运行时解码策略，它既语法感知又具有对比性，用于RTL代码生成。DecoRTL集成了两个互补组件：(i) 自洽性采样，它生成多个候选并根据令牌级一致性重新排序，以促进正确性同时保持多样性；(ii) 语法感知温度适应，它根据令牌的句法和功能角色对令牌进行分类，并相应地调整采样温度，对语法关键令牌强制低温度，对探索性令牌使用高温度。我们的方法完全在推理时操作，无需任何额外的模型微调。通过使用VerilogEval基准测试对多个开源LLM进行评估，我们证明了在语法有效性、功能正确性和输出多样性方面的显著改进，而执行开销（性能开销）可以忽略不计。", "summary": "本文针对大型语言模型（LLMs）在生成寄存器传输级（RTL）代码时因传统解码策略不适用而导致的错误问题，深入分析了其根本原因。研究发现LLMs在RTL的结构或语义复杂区域置信度低。为解决此问题，论文提出了DecoRTL，一个创新的运行时解码框架。DecoRTL结合了自洽性采样和语法感知温度适应，前者通过多候选生成和重排序提升正确性和多样性，后者则根据令牌的语法角色动态调整采样温度，确保关键语法部分的准确性并允许设计部分的探索性。该方法无需模型微调，在VerilogEval基准测试上显著提升了RTL代码的语法有效性、功能正确性和多样性，且性能开销可忽略不计。", "keywords": "RTL代码生成, LLM解码, 运行时策略, 语法感知, 自洽性采样", "comments": "DecoRTL的创新之处在于其运行时解码策略，特别是结合了自洽性采样和语法感知温度适应，这使得LLMs能够更好地适应RTL代码的结构和语义要求，而无需进行昂贵的模型微调。这对于提升LLMs在特定领域代码生成能力方面具有重要意义，提供了一种通用且高效的解决方案。"}}
{"id": "2507.02620", "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference", "authors": ["Xing Liu", "Lizhuo Luo", "Ming Tang", "Chao Huang"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      16 pages, and the last 3 are appendix", "url": "http://arxiv.org/abs/2507.02620v1", "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}", "comment": "16 pages, and the last 3 are appendix", "pdf_url": "http://arxiv.org/pdf/2507.02620v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "FlowSpec: 面向高效分布式LLM推理的连续流水线推测解码", "tldr": "FlowSpec提出了一种流水线并行推测解码框架，用于提高网络边缘分布式LLM推理的效率，实现了1.36倍至1.77倍的推理速度提升。", "motivation": "现有流水线并行方法在网络边缘进行分布式LLM推理时，当推理请求稀疏时，流水线利用率低，导致性能优势下降。", "method": "FlowSpec是一个流水线并行树状推测解码框架，包含三个关键机制：1) 基于分数的逐步验证，优先处理重要草稿令牌以提前接受；2) 高效草稿管理，在验证过程中修剪无效令牌并保持因果关系；3) 动态草稿扩展策略，提供高质量推测输入。这些技术协同工作以提高流水线利用率和推测效率。", "result": "FlowSpec在真实测试平台上，与基线相比，显著提高了不同模型和配置下的推理速度，实现了1.36倍至1.77倍的加速比。", "conclusion": "FlowSpec通过提升流水线利用率和推测效率，有效解决了分布式LLM在网络边缘推理的挑战，显著提高了推理速度。", "translation": "分布式推理是一种在网络边缘实现大型语言模型（LLM）推理的有前景的方法。它将推理过程分发到多个设备上，以确保LLM能够适应设备内存。最近基于流水线的方法有潜力并行化通信和计算，这有助于减少推理延迟。然而，当网络边缘的推理请求稀疏时，这种优势会减弱，因为流水线通常处于低利用率状态。为了在边缘实现高效的分布式LLM推理，我们提出了\\textbf{FlowSpec}，一个流水线并行树状推测解码框架。FlowSpec包含三个关键机制来提高解码效率：1) 基于分数的逐步验证，优先处理更重要的草稿令牌以提前接受令牌；2) 高效的草稿管理，在验证过程中修剪无效令牌同时保持正确的因果关系；3) 动态草稿扩展策略，提供高质量的推测输入。这些技术协同工作，以提高流水线利用率和推测效率。我们在真实世界的测试平台上对FlowSpec与其他基线进行了评估。实验结果表明，我们提出的框架显著提高了不同模型和配置下的推理速度，与基线相比，实现了1.36倍至1.77倍的加速比。我们的代码已公开可用。", "summary": "该论文提出了FlowSpec，一个面向网络边缘高效分布式LLM推理的流水线并行树状推测解码框架。针对现有流水线方法在稀疏请求下利用率低的问题，FlowSpec通过引入基于分数的逐步验证、高效草稿管理和动态草稿扩展策略，协同提升流水线利用率和推测效率。实验结果表明，FlowSpec在多种模型和配置下，相较于基线实现了1.36倍至1.77倍的推理速度提升。", "keywords": "分布式LLM推理, 流水线并行, 推测解码, 网络边缘, FlowSpec", "comments": "FlowSpec通过结合流水线并行和推测解码，并引入创新的验证、管理和扩展策略，有效地解决了分布式LLM推理在网络边缘面临的低利用率挑战。其在真实测试平台上的显著性能提升，证明了该方法的实用性和重要性。"}}
{"id": "2507.02152", "title": "The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies", "authors": ["Disa Sariola", "Patrick Button", "Aron Culotta", "Nicholas Mattei"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02152v1", "summary": "Artificial intelligence systems, especially those using machine learning, are\nbeing deployed in domains from hiring to loan issuance in order to automate\nthese complex decisions. Judging both the effectiveness and fairness of these\nAI systems, and their human decision making counterpart, is a complex and\nimportant topic studied across both computational and social sciences. Within\nmachine learning, a common way to address bias in downstream classifiers is to\nresample the training data to offset disparities. For example, if hiring rates\nvary by some protected class, then one may equalize the rate within the\ntraining set to alleviate bias in the resulting classifier. While simple and\nseemingly effective, these methods have typically only been evaluated using\ndata obtained through convenience samples, introducing selection bias and label\nbias into metrics. Within the social sciences, psychology, public health, and\nmedicine, audit studies, in which fictitious ``testers'' (e.g., resumes,\nemails, patient actors) are sent to subjects (e.g., job openings, businesses,\ndoctors) in randomized control trials, provide high quality data that support\nrigorous estimates of discrimination. In this paper, we investigate how data\nfrom audit studies can be used to improve our ability to both train and\nevaluate automated hiring algorithms. We find that such data reveals cases\nwhere the common fairness intervention method of equalizing base rates across\nclasses appears to achieve parity using traditional measures, but in fact has\nroughly 10% disparity when measured appropriately. We additionally introduce\ninterventions based on individual treatment effect estimation methods that\nfurther reduce algorithmic discrimination using this data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02152v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "公平的幻觉：用审计研究审计公平干预措施", "tldr": "研究发现，常见的AI公平干预措施在审计研究数据下可能存在隐蔽的偏见，并提出了新的干预方法，通过审计研究数据可以更准确地评估和减少算法歧视。", "motivation": "现有的机器学习公平干预方法（如重采样训练数据）通常只使用方便抽样获得的数据进行评估，这引入了选择偏差和标签偏差，导致对公平性指标的评估不准确。因此，需要高质量的数据来更严格地评估和改进自动化决策系统的公平性。", "method": "本文研究了如何利用审计研究（一种在社会科学中用于严格估计歧视的方法，通过发送虚构的“测试者”到受试者进行随机对照试验）获得的数据来训练和评估自动化招聘算法。此外，还引入了基于个体治疗效果估计方法的干预措施。", "result": "研究发现，当使用审计研究数据进行适当测量时，常见的公平干预方法（如均衡不同类别基本率）在传统测量下看似实现了公平，但实际上仍然存在大约10%的差异。此外，本文引入的基于个体治疗效果估计的干预措施进一步减少了算法歧视。", "conclusion": "Not mentioned in abstract", "translation": "人工智能系统，特别是使用机器学习的系统，正被部署在从招聘到贷款发放等领域，以自动化这些复杂的决策。判断这些人工智能系统及其人类决策对应物的有效性和公平性，是计算科学和社会科学领域研究的一个复杂而重要的话题。在机器学习中，解决下游分类器偏差的一种常见方法是重新采样训练数据以抵消差异。例如，如果招聘率因受保护类别而异，那么可以在训练集中均衡该比率，以减轻所得分类器中的偏差。尽管这些方法简单且看似有效，但它们通常仅使用通过方便抽样获得的数据进行评估，这给指标带来了选择偏差和标签偏差。在社会科学、心理学、公共卫生和医学领域，审计研究——其中虚构的“测试者”（例如简历、电子邮件、患者扮演者）被发送给受试者（例如招聘职位、企业、医生）进行随机对照试验——提供了高质量的数据，支持对歧视的严格估计。在本文中，我们研究了如何利用审计研究的数据来提高我们训练和评估自动化招聘算法的能力。我们发现，这些数据揭示了在某些情况下，常见的公平干预方法（即均衡不同类别的基本率）使用传统测量似乎达到了公平，但实际上在适当测量时却存在大约10%的差异。我们还引入了基于个体治疗效果估计方法的干预措施，利用这些数据进一步减少了算法歧视。", "summary": "本文探讨了如何利用社会科学中的审计研究数据来更准确地评估和改进机器学习系统的公平性。研究发现，传统的公平干预方法（如重采样）在表面上可能显示公平，但通过审计数据揭示，在实际应用中仍存在显著偏差。为解决此问题，论文提出了基于个体治疗效果估计的新干预措施，并证明其能有效降低算法歧视，特别是在自动化招聘领域。", "keywords": "机器学习公平性, 审计研究, 算法歧视, 公平干预, 个体治疗效果", "comments": "这篇论文通过引入社会科学中的“审计研究”方法来评估和改进机器学习公平性，具有显著的创新性。它揭示了传统公平度量方法可能存在的“公平幻觉”，强调了数据质量和评估方法的重要性。通过引入个体治疗效果估计，为解决算法歧视提供了新的视角和有效途径，对实际应用中的AI公平性保障具有重要意义。"}}
{"id": "2507.02217", "title": "Understanding Trade offs When Conditioning Synthetic Data", "authors": ["Brandon Trabucco", "Qasim Wani", "Benjamin Pikus", "Vasu Sharma"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02217v1", "summary": "Learning robust object detectors from only a handful of images is a critical\nchallenge in industrial vision systems, where collecting high quality training\ndata can take months. Synthetic data has emerged as a key solution for data\nefficient visual inspection and pick and place robotics. Current pipelines rely\non 3D engines such as Blender or Unreal, which offer fine control but still\nrequire weeks to render a small dataset, and the resulting images often suffer\nfrom a large gap between simulation and reality. Diffusion models promise a\nstep change because they can generate high quality images in minutes, yet\nprecise control, especially in low data regimes, remains difficult. Although\nmany adapters now extend diffusion beyond plain text prompts, the effect of\ndifferent conditioning schemes on synthetic data quality is poorly understood.\nWe study eighty diverse visual concepts drawn from four standard object\ndetection benchmarks and compare two conditioning strategies: prompt based and\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\nyields higher quality synthetic data; as diversity grows, layout conditioning\nbecomes superior. When layout cues match the full training distribution,\nsynthetic data raises mean average precision by an average of thirty four\npercent and by as much as one hundred seventy seven percent compared with using\nreal data alone.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02217v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "理解合成数据条件化时的权衡", "tldr": "该论文探讨了使用扩散模型生成合成数据时，基于提示和基于布局的两种条件化策略的权衡，旨在提高目标检测性能。研究发现，在数据多样性较高时，基于布局的条件化效果更优，并能显著提升平均精度。", "motivation": "工业视觉系统中，从少量图像中学习鲁棒目标检测器是一个关键挑战，因为收集高质量训练数据耗时且昂贵。现有合成数据生成方法（如3D引擎）渲染慢且存在模拟与现实差距。扩散模型虽然能快速生成图像，但在低数据量下难以实现精确控制。目前，不同条件化方案对合成数据质量的影响尚不明确。", "method": "研究了来自四个标准目标检测基准的八十个不同视觉概念。比较了两种条件化策略：基于提示的（prompt-based）和基于布局的（layout-based）。", "result": "当条件化线索范围较窄时，基于提示的条件化能产生更高质量的合成数据。随着多样性的增长，基于布局的条件化变得更优越。当布局线索与完整的训练分布匹配时，与单独使用真实数据相比，合成数据使平均精度（mAP）平均提高了34%，最高达177%。", "conclusion": "基于布局的条件化策略在生成多样化的合成数据以用于目标检测方面更有效，尤其是在与训练分布对齐时，能够带来显著的性能提升。", "translation": "从少量图像中学习鲁棒的目标检测器是工业视觉系统中的一个关键挑战，其中收集高质量的训练数据可能需要数月。合成数据已成为数据高效视觉检测和抓取机器人技术的一个关键解决方案。当前的流水线依赖于Blender或Unreal等3D引擎，它们提供精细控制，但仍需要数周才能渲染一个小数据集，并且生成的图像通常存在较大的模拟与现实差距。扩散模型有望带来质的飞跃，因为它们可以在几分钟内生成高质量图像，但在低数据量的情况下，精确控制仍然困难。尽管许多适配器现在将扩散模型扩展到纯文本提示之外，但不同条件化方案对合成数据质量的影响知之甚少。我们研究了来自四个标准目标检测基准的八十个不同视觉概念，并比较了两种条件化策略：基于提示的和基于布局的。当条件化线索范围较窄时，基于提示的条件化会产生更高质量的合成数据；随着多样性的增长，基于布局的条件化变得更优越。当布局线索与完整的训练分布匹配时，与单独使用真实数据相比，合成数据使平均精度提高了平均34%，最高达177%。", "summary": "该论文旨在解决工业视觉中数据稀缺的问题，通过探索使用扩散模型生成的合成数据来改进目标检测。研究重点是比较基于提示和基于布局的两种条件化策略对合成数据质量的影响。通过对四个标准基准中八十个不同视觉概念的实验，结果表明，当条件线索范围较窄时，基于提示的条件化效果更好；但随着多样性增加，基于布局的条件化表现更优。尤其重要的是，当布局线索与完整的训练分布匹配时，合成数据能够使平均精度（mAP）相比仅使用真实数据平均提高34%，最高可达177%，这突显了基于布局的条件化在生成高质量合成数据方面的有效性。", "keywords": "合成数据, 扩散模型, 目标检测, 条件化策略, 布局条件化", "comments": "该论文为使用扩散模型生成合成数据提供了宝贵的见解，特别是在探索不同条件化策略方面。其关于基于布局的条件化在处理多样化数据集时的优越性，以及由此带来的显著性能提升（mAP最高达177%），对于减少工业视觉中对昂贵真实数据收集的依赖具有重要的创新意义。这项工作有助于弥合扩散模型的高速生成能力与合成数据对精确控制和质量需求之间的差距。"}}
{"id": "2507.02189", "title": "On the Design of Corrugated Boards: A New FEM Modeling and Experimental Validation", "authors": ["Ricardo Fitas", "Heinz Joachim Schaffrath", "Samuel Schabel"], "categories": ["physics.app-ph", "cs.CE", "physics.comp-ph"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02189v1", "summary": "This study presents a simplified FEM modeling approach suitable for large\nstructures made of corrugated boards, such as customized packages, based on a\nhomogenization method, which is combined with correction factors for internal\nmechanisms. The homogenization process reduces computational time by\ntransforming flute geometries into equivalent elastic models. In large\ndeformations and in the presence of contact for a given geometry, the effective\nelastic modulus in the thickness direction, as well as the effective thickness\nof the structure, are corrected by two statistical Weibull distributions\nrepresenting the contact and buckling mechanisms in a corrugated board. The\nWeibull parameters are obtained via experimental analysis, and such a process\nis then validated. The results demonstrate that the statistical parameters\n($\\beta_1 = 0.14$, $\\beta_2 = 1.31$) can be used for the simplistic\nrepresentation of corrugated boards, being computationally efficient. This\nresearch contributes to the optimization of corrugated packaging design,\nspecifically by simplifying FEM models for faster yet equally accurate\nsimulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02189v1", "cate": "physics.app-ph", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "瓦楞纸板设计：一种新的有限元建模与实验验证", "tldr": "开发了一种简化的FEM建模方法，结合均质化和Weibull分布，用于高效准确地模拟瓦楞纸板大结构。", "motivation": "现有方法计算耗时，需要一种更高效且准确的瓦楞纸板结构（如定制包装）的有限元模拟方法，以优化设计。", "method": "本研究提出了一种简化的FEM建模方法，适用于瓦楞纸板大结构。该方法基于均质化，并结合了内部机制的校正因子。均质化过程将瓦楞几何形状转化为等效弹性模型以减少计算时间。在大变形和接触存在的情况下，通过两个统计Weibull分布（代表瓦楞纸板中的接触和屈曲机制）来校正厚度方向的有效弹性模量和结构的有效厚度。Weibull参数通过实验分析获得并进行验证。", "result": "统计参数（$\\beta_1 = 0.14$，$\\beta_2 = 1.31$）可用于瓦楞纸板的简化表示，具有计算效率。该方法能够实现更快但同样准确的模拟。", "conclusion": "提出的简化有限元模型能有效且准确地模拟瓦楞纸板结构，有助于优化瓦楞包装设计。", "translation": "本研究提出了一种简化的有限元建模方法，适用于由瓦楞纸板制成的大型结构，例如定制包装。该方法基于均质化，并结合了内部机制的校正因子。均质化过程通过将瓦楞几何形状转化为等效弹性模型来减少计算时间。在大变形和给定几何形状存在接触的情况下，结构在厚度方向的有效弹性模量以及有效厚度通过两个统计Weibull分布进行校正，这两个分布代表了瓦楞纸板中的接触和屈曲机制。Weibull参数通过实验分析获得，并且该过程随后得到验证。结果表明，统计参数（$\\beta_1 = 0.14$，$\\beta_2 = 1.31$）可用于瓦楞纸板的简化表示，具有计算效率。这项研究通过简化有限元模型以实现更快但同样准确的模拟，有助于优化瓦楞包装设计。", "summary": "本研究提出了一种用于瓦楞纸板大型结构（如包装）的简化有限元建模方法。该方法结合了均质化技术以减少计算时间，并引入了基于实验验证的Weibull分布校正因子，以在存在大变形和接触时修正材料的有效弹性模量和厚度。实验结果验证了该方法的有效性及其在计算效率方面的优势，为优化瓦楞包装设计提供了快速而准确的模拟工具。", "keywords": "瓦楞纸板, 有限元建模, 均质化, Weibull分布, 包装设计", "comments": "该研究的创新之处在于结合了均质化方法和统计Weibull分布来简化瓦楞纸板的有限元建模，同时通过实验验证确保了模型的准确性。这显著提高了大型瓦楞纸板结构模拟的计算效率，对于包装设计和优化具有重要实际价值。"}}
{"id": "2507.02325", "title": "Grid-Connected, Data-Driven Inverter Control, Theory to Hardware", "authors": ["Sebastian Graf", "Keith Moffat", "Anurag Mohapatra", "Alessandro Chiuso", "Florian Dörfler"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02325v1", "summary": "Grid-connected inverter control is challenging to implement due to the\ndifficulty of obtaining and maintaining an accurate grid model. Direct\nData-Driven Predictive Control provides a model-free alternative to traditional\nmodel-based control methods. This paper describes how the recently-proposed\nTransient Predictive Control (TPC) can be used for real-world, plug-and-play\ninverter control. The following hypotheses were tested: 1) The TPC algorithm\ncan be run online using standard hardware, and 2) TPC, which is derived using\nLinear Time-Invariant assumptions, is effective for grid-connected inverter\ncontrol, which is a nonlinear and time-varying system. Experiments conducted on\na two-converter benchtop setup and at the CoSES Laboratory on a 25 kVA\nconverter connected to the Munich grid support these hypotheses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02325v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "并网、数据驱动的逆变器控制：从理论到硬件", "tldr": "本文展示了瞬态预测控制（TPC）如何有效地在实际场景中控制并网逆变器，即使使用标准硬件和面对非线性系统。", "motivation": "由于难以获取和维护精确的电网模型，并网逆变器控制的实现具有挑战性。", "method": "本文提出并评估了瞬态预测控制（TPC），这是一种无模型的直接数据驱动预测控制方法，作为传统基于模型控制的替代方案。研究测试了TPC算法是否可以使用标准硬件在线运行，以及TPC（尽管基于线性时不变假设推导）对于非线性时变系统——并网逆变器控制——是否有效。", "result": "在双变换器台式设置和连接到慕尼黑电网的25 kVA变换器上进行的实验支持了TPC算法可以使用标准硬件在线运行，并且对于非线性时变并网逆变器控制有效这两个假设。", "conclusion": "瞬态预测控制（TPC）是一种可行且有效的无模型控制方法，适用于实际的即插即用型并网逆变器控制，克服了电网建模的挑战。", "translation": "并网逆变器控制由于难以获取和维护精确的电网模型而难以实现。直接数据驱动预测控制为传统的基于模型的控制方法提供了一种无模型替代方案。本文描述了如何将最近提出的瞬态预测控制（TPC）用于实际的即插即用型逆变器控制。测试了以下假设：1）TPC算法可以使用标准硬件在线运行；2）TPC虽然是基于线性时不变假设推导的，但对于非线性时变系统——并网逆变器控制——是有效的。在双变换器台式设置和CoSES实验室中，对连接到慕尼黑电网的25 kVA变换器进行的实验支持了这些假设。", "summary": "本文解决了并网逆变器控制中的挑战，该挑战因需要精确的电网模型而复杂。它提出并评估了瞬态预测控制（TPC），这是一种无模型的、数据驱动的方法。研究证实，TPC可以在标准硬件上在线运行，并有效控制非线性、时变的并网逆变器，尽管其推导基于线性时不变假设。实验结果验证了TPC在实际即插即用应用中的适用性。", "keywords": "并网逆变器, 数据驱动控制, 预测控制, 瞬态预测控制, 无模型控制", "comments": "该论文的创新之处在于展示了一种针对并网逆变器的实用、无模型控制解决方案（TPC），而传统方法依赖于复杂且难以维护的电网模型。这推动了该领域向更鲁棒和自适应的控制系统发展，对于动态电网环境尤其有价值。在真实硬件和一台重要功率变换器（25 kVA）上的验证增加了其可信度。"}}
{"id": "2507.02001", "title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames", "authors": ["Anurag Arnab", "Ahmet Iscen", "Mathilde Caron", "Alireza Fathi", "Cordelia Schmid"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02001v1", "summary": "Despite recent advances in Vision-Language Models (VLMs), long-video\nunderstanding remains a challenging problem. Although state-of-the-art\nlong-context VLMs can process around 1000 input frames, they still struggle to\neffectively leverage this sequence length, and succumb to irrelevant\ndistractors within the context window. We present Temporal Chain of Thought, an\ninference strategy for video question-answering that curates the model's input\ncontext. We use the VLM itself to iteratively identify and extract the most\nrelevant frames from the video, which are then used for answering. We\ndemonstrate how leveraging more computation at inference-time to select the\nmost relevant context leads to improvements in accuracy, in agreement with\nrecent work on inference-time scaling of LLMs. Moreover, we achieve\nstate-of-the-art results on 4 diverse video question-answering datasets,\nshowing consistent improvements with 3 different VLMs. In particular, our\nmethod shines on longer videos which would not otherwise fit within the model's\ncontext window: On longer videos of more than 1 hour on LVBench, our approach\nusing a context window of 32K outperforms the same VLM using standard inference\nwith a 700K context window by 2.8 points.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02001v1", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "时间链式思考：通过帧思考理解长视频", "tldr": "提出了一种名为“时间链式思考”的推理策略，通过模型自身迭代选择最相关的帧来处理长视频问答，显著提高了准确性并超越了现有技术。", "motivation": "尽管视觉-语言模型（VLMs）取得了进展，但长视频理解仍然是一个挑战，因为当前最先进的模型即使能处理大量帧，也难以有效利用序列长度，并容易受到无关干扰的影响。", "method": "本文提出了“时间链式思考”（Temporal Chain of Thought），这是一种用于视频问答的推理策略，旨在管理模型的输入上下文。该方法利用VLM本身迭代地识别和提取视频中最相关的帧，然后用这些帧来回答问题。", "result": "该方法在4个不同的视频问答数据集上取得了最先进的结果，并与3种不同的VLM模型显示出一致的改进。特别是在LVBench上处理超过1小时的长视频时，使用32K上下文窗口的方法比使用700K上下文窗口的标准推理VLM性能高出2.8点。", "conclusion": "通过在推理时投入更多计算来选择最相关的上下文，可以显著提高长视频理解的准确性，这与LLM推理时间扩展的最新研究相符。", "translation": "尽管视觉-语言模型（VLMs）最近取得了进展，但长视频理解仍然是一个具有挑战性的问题。尽管最先进的长上下文VLMs可以处理大约1000个输入帧，但它们仍然难以有效利用这一序列长度，并容易受到上下文窗口中无关干扰的影响。我们提出了时间链式思考（Temporal Chain of Thought），这是一种用于视频问答的推理策略，它负责策划模型的输入上下文。我们使用VLM本身迭代地识别和提取视频中最相关的帧，然后用这些帧来回答问题。我们展示了在推理时利用更多计算来选择最相关的上下文如何带来准确性的提高，这与LLM推理时间扩展的最新工作相符。此外，我们在4个不同的视频问答数据集上取得了最先进的结果，显示出与3种不同VLM的一致改进。特别是，我们的方法在那些无法适应模型上下文窗口的更长视频上表现出色：在LVBench上超过1小时的长视频中，我们的方法使用32K上下文窗口，比使用700K上下文窗口的相同VLM标准推理方法高出2.8点。", "summary": "本文提出了一种名为“时间链式思考”（Temporal Chain of Thought）的推理策略，旨在解决长视频理解中视觉-语言模型（VLMs）难以有效利用长序列上下文的问题。该方法通过VLM自身迭代地识别并提取视频中最相关的帧作为输入上下文，从而避免无关干扰。实验结果表明，该策略在4个视频问答数据集上取得了最先进的性能，并在处理长视频时表现尤为突出，证明了在推理时选择相关上下文的重要性。", "keywords": "长视频理解, 视觉-语言模型, 时间链式思考, 视频问答, 上下文管理", "comments": "这项研究的创新之处在于，它通过让VLM自身动态地选择最相关的视频帧，有效地解决了长视频理解中上下文过长和无关信息干扰的问题。这类似于大型语言模型中的“思维链”概念，但应用于视觉领域，通过智能的上下文管理提高了效率和准确性。其重要性在于，它为处理超长视频内容提供了一种有效且通用的方法，能够应用于多种VLM模型，并有望推动视频理解领域的发展。"}}
{"id": "2507.02374", "title": "Predictive Control over LAWN: Joint Trajectory Design and Resource Allocation", "authors": ["Haijia Jin", "Jun Wu", "Weijie Yuan", "Ruizhi Ruan", "Jiacheng Wang", "Dusit Niyato", "Dong In Kim", "Abbas Jamalipour"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02374v1", "summary": "Low-altitude wireless networks (LAWNs) have been envisioned as flexible and\ntransformative platforms for enabling delay-sensitive control applications in\nInternet of Things (IoT) systems. In this work, we investigate the real-time\nwireless control over a LAWN system, where an aerial drone is employed to serve\nmultiple mobile automated guided vehicles (AGVs) via finite blocklength (FBL)\ntransmission. Toward this end, we adopt the model predictive control (MPC) to\nensure accurate trajectory tracking, while we analyze the communication\nreliability using the outage probability. Subsequently, we formulate an\noptimization problem to jointly determine control policy, transmit power\nallocation, and drone trajectory by accounting for the maximum travel distance\nand control input constraints. To address the resultant non-convex optimization\nproblem, we first derive the closed-form expression of the outage probability\nunder FBL transmission. Based on this, we reformulate the original problem as a\nquadratic programming (QP) problem, followed by developing an alternating\noptimization (AO) framework. Specifically, we employ the projected gradient\ndescent (PGD) method and the successive convex approximation (SCA) technique to\nachieve computationally efficient sub-optimal solutions. Furthermore, we\nthoroughly analyze the convergence and computational complexity of the proposed\nalgorithm. Extensive simulations and AirSim-based experiments are conducted to\nvalidate the superiority of our proposed approach compared to the baseline\nschemes in terms of control performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02374v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LAWN上的预测控制：联合轨迹设计与资源分配", "tldr": "本文研究了LAWN中无人机服务AGV的实时无线控制，采用MPC，并联合优化控制策略、功率分配和无人机轨迹，通过AO框架解决非凸问题，仿真验证了其优越性。", "motivation": "低空无线网络（LAWN）被认为是物联网（IoT）系统中实现延迟敏感控制应用的灵活且具有变革性的平台。本文旨在解决LAWN系统中无人机通过有限块长度（FBL）传输为移动自动导引车（AGV）提供服务时的实时无线控制问题，以确保准确的轨迹跟踪和通信可靠性。", "method": "采用模型预测控制（MPC）实现轨迹跟踪，并使用中断概率分析FBL传输下的通信可靠性。将控制策略、发射功率分配和无人机轨迹的联合优化问题建模为非凸问题，然后将其重新表述为二次规划（QP）问题。开发了一种交替优化（AO）框架，并结合投影梯度下降（PGD）和逐次凸逼近（SCA）技术来获得次优解。同时，分析了算法的收敛性和计算复杂度。", "result": "广泛的仿真和基于AirSim的实验结果表明，所提出的方法在控制性能方面优于基线方案。", "conclusion": "所提出的联合优化和交替优化框架能有效提高LAWN系统中的控制性能。", "translation": "低空无线网络 (LAWN) 被设想为在物联网 (IoT) 系统中实现延迟敏感控制应用的灵活且具有变革性的平台。在这项工作中，我们研究了 LAWN 系统中的实时无线控制，其中一架空中无人机通过有限块长度 (FBL) 传输为多个移动自动导引车 (AGV) 提供服务。为此，我们采用模型预测控制 (MPC) 来确保准确的轨迹跟踪，同时我们使用中断概率分析通信可靠性。随后，我们制定了一个优化问题，通过考虑最大行进距离和控制输入约束来联合确定控制策略、发射功率分配和无人机轨迹。为了解决由此产生的非凸优化问题，我们首先推导了 FBL 传输下中断概率的闭式表达式。在此基础上，我们将原始问题重新表述为二次规划 (QP) 问题，然后开发了一种交替优化 (AO) 框架。具体来说，我们采用投影梯度下降 (PGD) 方法和逐次凸逼近 (SCA) 技术来实现计算效率高的次优解。此外，我们彻底分析了所提出算法的收敛性和计算复杂性。进行了广泛的仿真和基于 AirSim 的实验，以验证我们提出的方法在控制性能方面优于基线方案。", "summary": "本文研究了低空无线网络（LAWN）中的实时无线控制，其中无人机通过有限块长度（FBL）传输为多个移动自动导引车（AGV）提供服务。研究采用了模型预测控制（MPC）进行轨迹跟踪，并通过中断概率分析通信可靠性。作者提出了一个联合优化控制策略、发射功率和无人机轨迹的非凸问题，并将其重构为二次规划（QP）问题，开发了一种基于投影梯度下降（PGD）和逐次凸逼近（SCA）的交替优化（AO）框架来求解。广泛的仿真和实验验证了所提方法在控制性能上的优越性。", "keywords": "低空无线网络, 模型预测控制, 联合优化, 有限块长度, 无人机轨迹", "comments": "该论文创新性地将预测控制（MPC）与无人机辅助物联网系统中的通信资源分配和轨迹优化相结合。FBL传输的应用以及将非凸问题转化为QP并通过AO框架解决，展示了其强大的技术方法。通过仿真和AirSim实验进行验证，进一步增强了所提方法实际应用的可信度。"}}
{"id": "2507.02268", "title": "Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation", "authors": ["Yuxiang Zhang", "Wei Li", "Wen Jia", "Mengmeng Zhang", "Ran Tao", "Shunlin Liang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02268v1", "summary": "Utilizing hyperspectral remote sensing technology enables the extraction of\nfine-grained land cover classes. Typically, satellite or airborne images used\nfor training and testing are acquired from different regions or times, where\nthe same class has significant spectral shifts in different scenes. In this\npaper, we propose a Bi-directional Domain Adaptation (BiDA) framework for\ncross-domain hyperspectral image (HSI) classification, which focuses on\nextracting both domain-invariant features and domain-specific information in\nthe independent adaptive space, thereby enhancing the adaptability and\nseparability to the target scene. In the proposed BiDA, a triple-branch\ntransformer architecture (the source branch, target branch, and coupled branch)\nwith semantic tokenizer is designed as the backbone. Specifically, the source\nbranch and target branch independently learn the adaptive space of source and\ntarget domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is\ndeveloped in coupled branch for feature interaction and inter-domain\ncorrelation mining. Furthermore, a bi-directional distillation loss is designed\nto guide adaptive space learning using inter-domain correlation. Finally, we\npropose an Adaptive Reinforcement Strategy (ARS) to encourage the model to\nfocus on specific generalized feature extraction within both source and target\nscenes in noise condition. Experimental results on cross-temporal/scene\nairborne and satellite datasets demonstrate that the proposed BiDA performs\nsignificantly better than some state-of-the-art domain adaptation approaches.\nIn the cross-temporal tree species classification task, the proposed BiDA is\nmore than 3\\%$\\sim$5\\% higher than the most advanced method. The codes will be\navailable from the website:\nhttps://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02268v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于双向域适应的跨域高光谱图像分类", "tldr": "本文提出了一种名为BiDA的双向域适应框架，用于解决跨域高光谱图像分类中由于不同区域或时间采集的图像存在显著光谱偏移导致的问题。BiDA通过三分支Transformer架构结合语义分词器、耦合多头交叉注意力机制、双向蒸馏损失和自适应强化策略，有效提取域不变特征和域特定信息，显著优于现有SOTA方法。", "motivation": "用于训练和测试的高光谱图像通常来自不同区域或时间，导致同一类别在不同场景中存在显著的光谱偏移，这给跨域高光谱图像分类带来了挑战。", "method": "本文提出了一种双向域适应（BiDA）框架，用于跨域高光谱图像（HSI）分类。该框架旨在独立自适应空间中提取域不变特征和域特定信息，从而增强对目标场景的适应性和可分离性。BiDA的核心是一个三分支Transformer架构（源分支、目标分支和耦合分支）并带有语义分词器作为骨干网络。具体来说，源分支和目标分支独立学习源域和目标域的自适应空间；耦合分支中开发了耦合多头交叉注意力（CMCA）机制，用于特征交互和域间相关性挖掘。此外，设计了一种双向蒸馏损失，利用域间相关性指导自适应空间学习。最后，提出了一种自适应强化策略（ARS），鼓励模型在噪声条件下专注于源场景和目标场景中的特定泛化特征提取。", "result": "在跨时间/场景的机载和卫星数据集上的实验结果表明，所提出的BiDA框架显著优于一些最先进的域适应方法。在跨时间树种分类任务中，BiDA的性能比最先进的方法高出3%~5%。", "conclusion": "所提出的BiDA框架通过有效地提取域不变特征和域特定信息，显著提高了跨域高光谱图像分类的性能，尤其在存在光谱偏移的复杂场景中表现出色。", "translation": "利用高光谱遥感技术可以提取细粒度的地物覆盖类别。通常，用于训练和测试的卫星或机载图像来自不同区域或时间，导致同一类别在不同场景中存在显著的光谱偏移。本文提出了一种双向域适应（BiDA）框架，用于跨域高光谱图像（HSI）分类，该框架侧重于在独立自适应空间中提取域不变特征和域特定信息，从而增强对目标场景的适应性和可分离性。在所提出的BiDA中，设计了一个带有语义分词器的三分支Transformer架构（源分支、目标分支和耦合分支）作为骨干网络。具体来说，源分支和目标分支独立学习源域和目标域的自适应空间，在耦合分支中开发了耦合多头交叉注意力（CMCA）机制，用于特征交互和域间相关性挖掘。此外，设计了一种双向蒸馏损失，利用域间相关性指导自适应空间学习。最后，我们提出了一种自适应强化策略（ARS），鼓励模型在噪声条件下专注于源场景和目标场景中的特定泛化特征提取。在跨时间/场景机载和卫星数据集上的实验结果表明，所提出的BiDA性能显著优于一些最先进的域适应方法。在跨时间树种分类任务中，所提出的BiDA比最先进的方法高出3%~5%。代码将从网站提供：https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA。", "summary": "本文提出了一种名为BiDA的双向域适应框架，旨在解决跨域高光谱图像分类中由于光谱偏移导致的数据分布差异问题。BiDA采用独特的三分支Transformer架构，包含源分支、目标分支和耦合分支，并结合语义分词器。它通过独立学习域的自适应空间、引入耦合多头交叉注意力机制进行特征交互和域间相关性挖掘，以及设计双向蒸馏损失来指导学习。此外，还提出了自适应强化策略以增强模型在噪声条件下的泛化特征提取能力。实验证明，BiDA在跨时间/场景高光谱数据集上显著优于现有SOTA方法，特别是在跨时间树种分类任务中性能提升显著。", "keywords": "高光谱图像分类, 域适应, 深度学习, Transformer, 跨域学习", "comments": "该论文提出了一种新颖的双向域适应框架BiDA，其创新点在于结合了三分支Transformer架构、耦合多头交叉注意力机制、双向蒸馏损失以及自适应强化策略，以同时提取域不变和域特定信息。这种方法有效地解决了高光谱图像在不同场景下光谱偏移带来的挑战，并通过实验验证了其优越性，对跨域高光谱图像分类领域具有重要意义。"}}
{"id": "2507.02657", "title": "On the Complexity of Knapsack under Explorable Uncertainty: Hardness and Algorithms", "authors": ["Jens Schlöter"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02657v1", "summary": "In the knapsack problem under explorable uncertainty, we are given a knapsack\ninstance with uncertain item profits. Instead of having access to the precise\nprofits, we are only given uncertainty intervals that are guaranteed to contain\nthe corresponding profits. The actual item profit can be obtained via a query.\nThe goal of the problem is to adaptively query item profits until the revealed\ninformation suffices to compute an optimal (or approximate) solution to the\nunderlying knapsack instance. Since queries are costly, the objective is to\nminimize the number of queries.\n  In the offline variant of this problem, we assume knowledge of the precise\nprofits and the task is to compute a query set of minimum cardinality that a\nthird party without access to the profits could use to identify an optimal (or\napproximate) knapsack solution. We show that this offline variant is complete\nfor the second-level of the polynomial hierarchy, i.e., $\\Sigma_2^p$-complete,\nand cannot be approximated within a non-trivial factor unless $\\Sigma_2^p =\n\\Delta_2^p$. Motivated by these strong hardness results, we consider a\nresource-augmented variant of the problem where the requirements on the query\nset computed by an algorithm are less strict than the requirements on the\noptimal solution we compare against. More precisely, a query set computed by\nthe algorithm must reveal sufficient information to identify an approximate\nknapsack solution, while the optimal query set we compare against has to reveal\nsufficient information to identify an optimal solution. We show that this\nresource-augmented setting allows interesting non-trivial algorithmic results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02657v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "探索性不确定性下背包问题的复杂性：难点与算法", "tldr": "本文研究了探索性不确定性下背包问题的复杂性。离线变体被证明是$\\Sigma_2^p$-完全的且难以近似。然而，引入资源增强变体后，可以获得非平凡的算法结果。", "motivation": "鉴于探索性不确定性下背包问题离线变体的强难性结果（$\\Sigma_2^p$-完全且难以近似），论文考虑了一种资源增强的变体以寻求可行的算法。", "method": "论文分析了探索性不确定性下背包问题离线变体的计算复杂性，并引入了一种资源增强的变体。在这种变体中，算法计算的查询集只需识别近似解，而最优查询集需识别最优解。", "result": "结果表明，探索性不确定性下背包问题的离线变体是$\\Sigma_2^p$-完全的，并且除非$\\Sigma_2^p = \\Delta_2^p$，否则无法在非平凡因子内近似。然而，资源增强设置允许得到有趣的非平凡算法结果。", "conclusion": "尽管探索性不确定性下背包问题的离线变体具有很强的计算难度，但通过采用资源增强的方法，可以获得非平凡的算法结果，从而为解决这类问题提供了新的视角。", "translation": "在探索性不确定性下的背包问题中，我们得到一个具有不确定物品利润的背包实例。我们无法获得精确的利润，而只知道包含相应利润的不确定性区间。实际的物品利润可以通过查询获得。问题的目标是自适应地查询物品利润，直到揭示的信息足以计算底层背包实例的最优（或近似）解。由于查询成本高昂，目标是最小化查询次数。\n在该问题的离线变体中，我们假设已知精确利润，任务是计算一个最小基数的查询集，该查询集可供无法访问利润的第三方用于识别最优（或近似）背包解。我们证明这个离线变体是多项式层级第二层（即$\\Sigma_2^p$-完全）的完全问题，并且除非$\\Sigma_2^p = \\Delta_2^p$，否则无法在非平凡因子内近似。受这些强难性结果的启发，我们考虑了该问题的一个资源增强变体，其中算法计算的查询集的要求比我们比较的最优解的要求不那么严格。更准确地说，算法计算的查询集必须揭示足够的信息以识别一个近似背包解，而我们比较的最优查询集必须揭示足够的信息以识别一个最优解。我们表明这种资源增强设置允许有趣的非平凡算法结果。", "summary": "本文研究了探索性不确定性下背包问题的计算复杂性，其目标是最小化查询次数以获取足够信息来找到最优或近似解。研究发现，该问题的离线变体是$\\Sigma_2^p$-完全的，且难以近似。为应对这一挑战，论文提出并分析了一种资源增强变体，其中算法只需识别近似解，而最优解仍需完全信息。结果表明，这种资源增强设置能够产生非平凡的算法。", "keywords": "探索性不确定性, 背包问题, 计算复杂性, 资源增强, 查询最小化", "comments": "这篇论文对探索性不确定性下的背包问题进行了深入的复杂性分析，揭示了其离线变体的极高计算难度，即$\\Sigma_2^p$-完全。其创新之处在于，面对强难性结果，并未止步，而是引入了“资源增强”这一巧妙的放松策略。这种方法在保持理论严谨性的同时，为实际应用中求解近似解提供了可行路径，展现了在硬核问题研究中从理论到实践的桥梁作用。"}}
{"id": "2507.02768", "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment", "authors": ["Ke-Han Lu", "Zhehuai Chen", "Szu-Wei Fu", "Chao-Han Huck Yang", "Sung-Feng Huang", "Chih-Kai Yang", "Chee-En Yu", "Chun-Wei Chen", "Wei-Chih Chen", "Chien-yu Huang", "Yi-Cheng Lin", "Yu-Xiang Lin", "Chi-An Fu", "Chun-Yi Kuan", "Wenze Ren", "Xuanjun Chen", "Wei-Ping Huang", "En-Pei Hu", "Tzu-Quan Lin", "Yuan-Kuei Wu", "Kuan-Po Huang", "Hsiao-Ying Huang", "Huang-Cheng Chou", "Kai-Wei Chang", "Cheng-Han Chiang", "Boris Ginsburg", "Yu-Chiang Frank Wang", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Model and code available at: this https URL", "url": "http://arxiv.org/abs/2507.02768v1", "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.", "comment": "Model and code available at:\n  https://github.com/kehanlu/DeSTA2.5-Audio", "pdf_url": "http://arxiv.org/pdf/2507.02768v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DeSTA2.5-Audio：迈向通用大型音频语言模型与自生成跨模态对齐", "tldr": "DeSTA2.5-Audio通过自生成跨模态对齐策略，解决了现有大型音频语言模型（LALMs）的灾难性遗忘问题，实现了强大的通用听觉感知和指令遵循能力。", "motivation": "现有的LALMs通过手动或LLM合成的音频指令数据集进行训练，但常常导致LLM原始语言能力的灾难性遗忘，且需要任务特定的指令微调。", "method": "提出DeSTA（self-generated cross-modal alignment strategy），让骨干LLM生成自己的训练目标，以保留其原生语言能力并建立有效的音频-文本对齐。基于DeSTA构建了DeSTA-AQA5M数据集，包含500万个训练样本，源自7000小时、50个不同数据集的音频。", "result": "DeSTA2.5-Audio在Dynamic-SUPERB、MMAU、SAKURA、Speech-IFEval和VoiceBench等广泛的音频-语言基准测试中取得了最先进或具有竞争力的性能。自生成策略在听觉感知和指令遵循能力上均优于广泛采用的数据构建和训练策略。", "conclusion": "精心设计的数据构建在LALM开发中至关重要，并为构建鲁棒、通用LALMs提供了实用见解。", "translation": "我们引入DeSTA2.5-Audio，这是一种通用大型音频语言模型（LALM），旨在实现鲁棒的听觉感知和指令遵循，无需任务特定的音频指令微调。最近的LALM通常通过在大型、手动整理或LLM合成的音频指令数据集上进行训练来增强大型语言模型（LLM）的听觉能力。然而，这些方法常常遭受LLM原始语言能力的灾难性遗忘。为了解决这个问题，我们重新审视了数据构建流程，并提出了DeSTA，这是一种自生成跨模态对齐策略，其中骨干LLM生成其自身的训练目标。这种方法保留了LLM的本地语言能力，同时建立了有效的音频-文本对齐，从而实现了零样本泛化，无需任务特定的微调。使用DeSTA，我们构建了DeSTA-AQA5M，这是一个大规模、任务无关的数据集，包含500万个训练样本，源自7000小时、涵盖语音、环境声音和音乐等50个不同数据集的音频。DeSTA2.5-Audio在Dynamic-SUPERB、MMAU、SAKURA、Speech-IFEval和VoiceBench等广泛的音频-语言基准测试中取得了最先进或具有竞争力的性能。全面的比较研究表明，我们的自生成策略在听觉感知和指令遵循能力方面均优于广泛采用的数据构建和训练策略。我们的发现强调了精心设计的数据构建在LALM开发中的重要性，并为构建鲁棒、通用LALM提供了实用见解。", "summary": "DeSTA2.5-Audio是一种新型通用大型音频语言模型，通过提出自生成跨模态对齐策略（DeSTA）和构建大规模任务无关数据集DeSTA-AQA5M，有效解决了现有LALMs在音频-语言任务中存在的灾难性遗忘问题，并在多项基准测试中表现出领先性能，无需任务特定微调即可实现零样本泛化。", "keywords": "大型音频语言模型, 跨模态对齐, 自生成数据, 零样本泛化, 灾难性遗忘", "comments": "该论文的核心创新在于其提出的DeSTA自生成跨模态对齐策略，有效解决了大型语言模型在与音频能力结合时面临的灾难性遗忘问题，并实现了无需任务特定微调的零样本泛化。DeSTA-AQA5M数据集的构建也为通用LALM的发展提供了宝贵的资源。"}}
{"id": "2507.02255", "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation", "authors": ["Zihao Li", "Chao Yang", "Tong Zhang", "Yakun Chen", "Xianzhi Wang", "Guandong Xu", "Daoyi Dong"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02255v1", "summary": "Preference alignment has achieved greater success on Large Language Models\n(LLMs) and drawn broad interest in recommendation research. Existing preference\nalignment methods for recommendation either require explicit reward modeling or\nonly support pairwise preference comparison. The former directly increases\nsubstantial computational costs, while the latter hinders training efficiency\non negative samples. Moreover, no existing effort has explored preference\nalignment solutions for tail-item recommendation. To bridge the above gaps, we\npropose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison\nto listwise comparison, to improve the efficiency of model training.\nSpecifically, we derive a closed form optimal policy to enable more efficient\nand effective training without explicit reward modeling. We also present an\nadaptive negative sampling and reweighting strategy to prioritize tail items\nduring optimization and enhance performance in tail-item recommendations.\nBesides, we theoretically prove that optimizing the listwise preference\noptimization (LPO) loss is equivalent to maximizing the upper bound of the\noptimal reward. Our experiments on three public datasets show that our method\noutperforms 10 baselines by a large margin, achieving up to 50% performance\nimprovement while reducing 17.9% GPU memory usage when compared with direct\npreference optimization (DPO) in tail-item recommendation. Our code is\navailable at https://github.com/Yuhanleeee/LPO4Rec.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02255v1", "cate": "cs.IR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "面向长尾推荐的列表式偏好对齐优化", "tldr": "LPO4Rec提出了一种新的列表式偏好对齐优化方法，通过扩展Bradley-Terry模型并结合自适应负采样策略，在不进行显式奖励建模的情况下，显著提高了长尾推荐的性能和训练效率。", "motivation": "现有推荐系统中的偏好对齐方法要么需要显式的奖励建模，导致计算成本高昂；要么只支持成对偏好比较，影响负样本训练效率。此外，目前还没有针对长尾项目推荐的偏好对齐解决方案。", "method": "本文提出了LPO4Rec，它将Bradley-Terry模型从成对比较扩展到列表式比较，以提高模型训练效率。具体来说，LPO4Rec推导了一个闭式最优策略，无需显式奖励建模即可实现高效训练。同时，它引入了自适应负采样和重加权策略，以在优化过程中优先考虑长尾项目。理论上，作者证明了优化列表式偏好优化（LPO）损失等价于最大化最优奖励的上限。", "result": "在三个公共数据集上的实验表明，LPO4Rec比10个基线方法表现出显著优势，性能提升高达50%。与直接偏好优化（DPO）相比，在长尾推荐中，LPO4Rec还能减少17.9%的GPU内存使用。", "conclusion": "LPO4Rec是一种高效且有效的面向长尾项目推荐的列表式偏好对齐优化方法，它通过创新的模型扩展和采样策略，显著提升了推荐性能并降低了计算成本。", "translation": "偏好对齐在大型语言模型（LLMs）上取得了巨大成功，并在推荐研究中引起了广泛兴趣。现有的推荐偏好对齐方法要么需要显式的奖励建模，要么只支持成对偏好比较。前者直接增加了大量的计算成本，而后者则阻碍了负样本的训练效率。此外，目前还没有探索针对长尾项目推荐的偏好对齐解决方案。为了弥补上述空白，我们提出了LPO4Rec，它将Bradley-Terry模型从成对比较扩展到列表式比较，以提高模型训练效率。具体来说，我们推导了一个闭式最优策略，使得无需显式奖励建模即可实现更高效和有效的训练。我们还提出了一种自适应负采样和重加权策略，以在优化过程中优先考虑长尾项目，并提高长尾项目推荐的性能。此外，我们理论上证明了优化列表式偏好优化（LPO）损失等价于最大化最优奖励的上限。我们在三个公共数据集上的实验表明，我们的方法比10个基线方法表现出显著优势，性能提升高达50%，同时与长尾项目推荐中的直接偏好优化（DPO）相比，GPU内存使用减少了17.9%。我们的代码可在https://github.com/Yuhanleeee/LPO4Rec获取。", "summary": "本文提出LPO4Rec，一种针对长尾推荐的列表式偏好对齐优化方法。该方法通过将Bradley-Terry模型从成对扩展到列表式比较，并设计闭式最优策略，实现了无需显式奖励建模的高效训练。LPO4Rec还引入自适应负采样和重加权策略以优先处理长尾项目。理论分析表明优化LPO损失等同于最大化最优奖励的上限。实验结果显示，LPO4Rec在性能上显著优于现有基线，并有效降低了GPU内存消耗。", "keywords": "偏好对齐, 长尾推荐, 列表式优化, Bradley-Terry模型, 自适应采样", "comments": "LPO4Rec的创新之处在于将偏好对齐从成对扩展到列表式，并针对长尾推荐场景进行了优化，解决了现有方法在效率和长尾支持上的不足。它通过理论证明和实验验证了其有效性和效率，特别是在资源消耗方面的改进，使其在实际应用中具有重要价值。"}}
{"id": "2507.02211", "title": "Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning", "authors": ["Gustavo C. Mangold", "Heitor C. M. Fernandes", "Mendeli H. Vainstein"], "categories": ["cs.AI", "cs.NE", "physics.comp-ph"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02211v1", "summary": "Recent studies in the spatial prisoner's dilemma games with reinforcement\nlearning have shown that static agents can learn to cooperate through a diverse\nsort of mechanisms, including noise injection, different types of learning\nalgorithms and neighbours' payoff knowledge.In this work, using an independent\nmulti-agent Q-learning algorithm, we study the effects of dilution and mobility\nin the spatial version of the prisoner's dilemma. Within this setting,\ndifferent possible actions for the algorithm are defined, connecting with\nprevious results on the classical, non-reinforcement learning spatial\nprisoner's dilemma, showcasing the versatility of the algorithm in modeling\ndifferent game-theoretical scenarios and the benchmarking potential of this\napproach.As a result, a range of effects is observed, including evidence that\ngames with fixed update rules can be qualitatively equivalent to those with\nlearned ones, as well as the emergence of a symbiotic mutualistic effect\nbetween populations that forms when multiple actions are defined.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02211v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "强化学习空间囚徒困境中的稀释、扩散与共生", "tldr": "本研究使用独立的Q学习算法，探讨了在空间囚徒困境中稀释和移动性对合作的影响，并观察到固定更新规则与学习规则的等效性以及多行动定义下共生互惠效应的出现。", "motivation": "近期研究表明静态智能体可以通过多种机制在强化学习空间囚徒困境中学会合作。本研究旨在探讨稀释和移动性在强化学习空间囚徒困境中的具体影响。", "method": "本研究采用独立的、多智能体Q学习算法来模拟空间囚徒困境，并定义了不同的可能行动，以连接经典非强化学习空间囚徒困境的先前结果。", "result": "观察到一系列效应，包括具有固定更新规则的游戏在性质上可以等同于具有学习规则的游戏，以及当定义多个行动时，种群之间出现共生互惠效应。", "conclusion": "研究结果展示了该算法在模拟不同博弈论场景方面的多功能性，以及该方法作为基准测试的潜力。同时揭示了固定规则与学习规则的等效性以及共生效应的出现。", "translation": "近期在强化学习空间囚徒困境游戏中的研究表明，静态智能体可以通过多种机制学会合作，包括噪声注入、不同类型的学习算法以及邻居收益知识。在这项工作中，我们使用独立的、多智能体Q学习算法，研究了空间囚徒困境中稀释和移动性的影响。在此设置下，为算法定义了不同的可能行动，与经典非强化学习空间囚徒困境的先前结果相联系，展示了该算法在建模不同博弈论场景方面的多功能性以及该方法的基准测试潜力。结果观察到一系列效应，包括具有固定更新规则的游戏在性质上可以等同于具有学习规则的游戏的证据，以及当定义多个行动时，种群之间形成的共生互惠效应的出现。", "summary": "本研究使用独立的、多智能体Q学习算法，深入探讨了在强化学习空间囚徒困境中稀释和移动性对合作的影响。通过定义不同的行动策略，研究者观察到固定更新规则的游戏与学习规则的游戏在性质上可以等效，并且在多行动定义下，种群之间会形成共生互惠效应。这表明了该算法在模拟复杂博弈场景中的通用性和作为基准测试工具的潜力。", "keywords": "空间囚徒困境, 强化学习, Q学习, 稀释, 共生效应", "comments": "该论文的创新点在于将稀释和移动性这两个新的变量引入到强化学习空间囚徒困境中进行研究，并使用了独立的Q学习算法。其重要性体现在揭示了固定更新规则与学习规则在某些情况下的等效性，以及发现了多行动定义下可能出现的共生互惠效应，这为理解复杂系统中的合作演化提供了新的视角。"}}
{"id": "2507.02302", "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      22 pages, 5 figures, ACL 2025 Main", "url": "http://arxiv.org/abs/2507.02302v1", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "comment": "22 pages, 5 figures, ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2507.02302v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DoMIX：一个高效的微调中利用领域知识的框架", "tldr": "DoMIX是一个新的框架，通过利用LoRA模块解决了现有持续领域自适应预训练(DAP)方法的计算成本高、对数据顺序敏感以及无法提供定制模型的问题，实现了高效、并行的领域自适应预训练。", "motivation": "现有持续领域自适应预训练(DAP)方法存在高计算成本、高GPU内存使用、对增量数据顺序敏感以及只能提供单一通用模型等局限性。", "method": "提出了DoMIX框架，通过利用LoRA模块（一种参数高效微调方法）来解决现有持续DAP的挑战。该方法实现了高效、并行的领域自适应预训练，并对领域顺序具有鲁棒性，能够有效利用积累的知识为特定任务提供定制的预训练模型。", "result": "DoMIX实现了高效和并行的领域自适应预训练，对领域顺序具有鲁棒性，并能有效利用积累的知识为特定任务提供定制的预训练模型。此外，该方法可以扩展到标准LLM微调场景。", "conclusion": "DoMIX通过利用LoRA模块，有效解决了现有持续DAP方法的局限性，提供了一个高效、灵活且能为特定任务定制模型的框架，并可应用于更广泛的LLM微调场景。", "translation": "领域自适应预训练（DAP）最近因其在微调预训练模型方面的有效性而受到关注。在此基础上，持续DAP已被探索用于开发能够逐步整合不同领域数据集的预训练模型。然而，现有的持续DAP方法面临几个局限性：(1) 训练期间计算成本高和GPU内存使用量大；(2) 对增量数据顺序敏感；以及 (3) 为所有最终任务提供单一的通用模型，这与DAP的本质相矛盾。在本文中，我们提出了DoMIX，这是一种通过利用LoRA模块（一种代表性的参数高效微调（PEFT）方法）来解决这些挑战的新方法。我们的方法能够实现高效并行的领域自适应预训练，对领域顺序具有鲁棒性，并有效利用积累的知识为特定任务提供定制的预训练模型。我们还证明了我们的方法可以超越DAP设置，扩展到标准的LLM微调场景。代码可在https://github.com/dohoonkim-ai/DoMIX 获取。", "summary": "本文提出了DoMIX，一个高效的框架，旨在解决现有持续领域自适应预训练（DAP）方法在计算成本、内存使用、数据顺序敏感性以及无法提供定制模型方面的局限性。DoMIX通过整合参数高效微调（PEFT）方法LoRA模块，实现了高效、并行的DAP，对领域顺序具有鲁棒性，并能够为特定任务生成定制的预训练模型。该方法还被证明可应用于标准的LLM微调场景。", "keywords": "领域自适应预训练, LoRA, 参数高效微调, 持续学习, 大语言模型微调", "comments": "DoMIX的创新之处在于将LoRA模块应用于持续领域自适应预训练，有效解决了现有方法面临的效率和灵活性问题。通过提供定制化的模型，它更好地符合DAP的初衷。其对LLM微调的扩展性也增加了其重要性。"}}
{"id": "2507.02606", "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks", "authors": ["Wei Fan", "Kejiang Chen", "Chang Liu", "Weiming Zhang", "Nenghai Yu"], "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by ICML 2025", "url": "http://arxiv.org/abs/2507.02606v1", "summary": "The rapid advancement of speech generation models has heightened privacy and\nsecurity concerns related to voice cloning (VC). Recent studies have\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\nperturbations. However, determined attackers can mitigate these protective\nperturbations and successfully execute VC. In this study, we conduct the first\nsystematic evaluation of these protective perturbations against VC under\nrealistic threat models that include perturbation purification. Our findings\nreveal that while existing purification methods can neutralize a considerable\nportion of the protective perturbations, they still lead to distortions in the\nfeature space of VC models, which degrades the performance of VC. From this\nperspective, we propose a novel two-stage purification method: (1) Purify the\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\nclean speech distribution. Experimental results demonstrate that our method\noutperforms state-of-the-art purification methods in disrupting VC defenses.\nOur study reveals the limitations of adversarial perturbation-based VC defenses\nand underscores the urgent need for more robust solutions to mitigate the\nsecurity and privacy risks posed by VC. The code and audio samples are\navailable at https://de-antifake.github.io.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.02606v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "De-AntiFake: 重新思考对抗语音克隆攻击的保护性扰动", "tldr": "本研究系统评估了对抗语音克隆的保护性扰动在净化攻击下的有效性，并提出了一种新的两阶段净化方法，能更有效地破坏现有防御，揭示了当前基于对抗扰动的语音克隆防御的局限性。", "motivation": "语音生成模型的快速发展加剧了语音克隆（VC）相关的隐私和安全担忧。尽管已有研究通过引入对抗性扰动来阻止未经授权的VC，但有决心的攻击者可以通过净化等手段减轻这些保护性扰动并成功执行VC。现有净化方法虽然能中和扰动，但仍会导致VC模型特征空间失真，从而降低VC性能。本研究旨在首次系统评估这些保护性扰动在包含扰动净化的现实威胁模型下的表现，并提出更有效的反制策略。", "method": "本研究首次在包含扰动净化的现实威胁模型下，对对抗语音克隆的保护性扰动进行了系统评估。在此基础上，提出了一种新颖的两阶段净化方法：首先净化受扰动的语音，然后使用音素指导对其进行精炼，使其与干净语音分布对齐。", "result": "本研究发现，现有净化方法虽然可以中和大部分保护性扰动，但仍会导致VC模型特征空间失真，从而降低VC性能。实验结果表明，我们提出的两阶段净化方法在破坏语音克隆防御方面优于最先进的净化方法。", "conclusion": "本研究揭示了基于对抗性扰动的语音克隆防御的局限性，并强调了迫切需要更强大的解决方案来减轻语音克隆带来的安全和隐私风险。", "translation": "语音生成模型的快速发展加剧了与语音克隆（VC）相关的隐私和安全担忧。最近的研究通过引入对抗性扰动来阻止未经授权的语音克隆。然而，有决心的攻击者可以减轻这些保护性扰动并成功执行语音克隆。在本研究中，我们首次在包含扰动净化的现实威胁模型下，对这些对抗语音克隆的保护性扰动进行了系统评估。我们的发现表明，虽然现有的净化方法可以中和相当一部分保护性扰动，但它们仍然会导致语音克隆模型特征空间中的失真，从而降低语音克隆的性能。从这个角度出发，我们提出了一种新颖的两阶段净化方法：（1）净化受扰动的语音；（2）使用音素指导对其进行精炼，以使其与干净语音分布对齐。实验结果表明，我们的方法在破坏语音克隆防御方面优于最先进的净化方法。我们的研究揭示了基于对抗性扰动的语音克隆防御的局限性，并强调了迫切需要更强大的解决方案来减轻语音克隆带来的安全和隐私风险。代码和音频样本可在 https://de-antifake.github.io 获取。", "summary": "本论文探讨了当前用于对抗语音克隆（VC）攻击的对抗性扰动的脆弱性。研究系统评估了这些保护措施在现实威胁模型（包括扰动净化）下的表现，发现虽然净化方法能移除扰动，但仍会扭曲VC模型特征空间。为应对此问题，作者提出了一种新颖的两阶段净化方法，该方法能有效破坏VC防御，并强调了开发更鲁棒的反VC解决方案的必要性。", "keywords": "语音克隆, 对抗性扰动, 扰动净化, 语音隐私, 安全", "comments": "本文通过在现实攻击场景（包括净化）下系统评估对抗语音克隆的保护性扰动的鲁棒性，做出了重要贡献。所提出的两阶段净化方法具有创新性，因为它不仅净化语音，还利用音素指导进行精炼，在破坏现有防御方面表现出卓越的性能。这项工作批判性地揭示了当前基于对抗性扰动的防御的局限性，强调了面对先进语音克隆能力时，对更具韧性的隐私保护技术的迫切需求。"}}
{"id": "2507.02424", "title": "CyberRAG: An agentic RAG cyber attack classification and reporting tool", "authors": ["Francesco Blefari", "Cristian Cosentino", "Francesco Aurelio Pironti", "Angelo Furfaro", "Fabrizio Marozzo"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02424v1", "summary": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming security\nanalysts with logs that demand deep, rapidly evolving domain expertise.\nConventional machine-learning detectors trim the alert volume but still yield\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\na modular, agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\n(iii) an iterative retrieval-and-reason loop that continuously queries a\ndomain-specific knowledge base until the evidence is both relevant and\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\ndesign that enables dynamic control flow and adaptive reasoning. This\nagent-centric architecture refines its threat labels and natural-language\njustifications autonomously, reducing false positives and enhancing\ninterpretability. The framework is fully extensible: new attack types can be\nsupported by simply adding a classifier without retraining the core agent.\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\nfinal classification accuracy to 94.92% through semantic orchestration.\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\npractical and scalable path toward semi-autonomous cyber-defence workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02424v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CyberRAG：一种代理式RAG网络攻击分类与报告工具", "tldr": "CyberRAG是一个代理式RAG框架，用于实时分类、解释和报告网络攻击，解决了传统系统高误报率和上下文不相关的问题，实现了高准确率和可信赖的解释。", "motivation": "大型企业中的入侵检测和防御系统(IDS/IPS)每小时产生数十万条警报，使安全分析师不堪重负，需要深入且快速演进的领域专业知识。传统的机器学习检测器虽然减少了警报量，但误报率仍然很高。标准的单次检索增强生成(RAG)管道经常检索不相关的上下文，并且无法解释其预测。", "method": "本文提出了CyberRAG，一个模块化的、基于代理的RAG框架。一个中央LLM代理协调：(i)一个经过微调的专业分类器池，每个分类器都针对一个不同的攻击家族；(ii)用于丰富和警报的工具适配器；(iii)一个迭代的检索和推理循环，持续查询领域特定的知识库，直到证据既相关又自洽。与传统RAG系统不同，CyberRAG采用代理式设计，实现动态控制流和自适应推理，自主地完善威胁标签和自然语言解释。", "result": "CyberRAG在每类攻击上实现了超过94%的准确率，并通过语义编排将最终分类准确率推高到94.92%。生成的解释在BERTScore中得分高达0.94，在基于GPT-4的专家评估中得分为4.9/5。", "conclusion": "代理式、面向专家的RAG可以将高检测准确率与可信赖的、可用于安全运营中心(SOC)的文本相结合，为半自主网络防御工作流提供了一条实用且可扩展的路径。", "translation": "大型企业中的入侵检测和防御系统（IDS/IPS）每小时可生成数十万条警报，使得安全分析师被大量日志淹没，这些日志需要深入且快速演进的领域专业知识。传统的机器学习检测器虽然能减少警报量，但仍产生高误报率，而标准的单次检索增强生成（RAG）管道通常会检索不相关的上下文并无法证明其预测。为了克服这些缺点，我们提出了CyberRAG，一个模块化的、基于代理的RAG框架，它能为网络攻击提供实时分类、解释和结构化报告。一个中央LLM代理协调：(i)一个经过微调的专业分类器池，每个分类器都针对一个不同的攻击家族；(ii)用于丰富和警报的工具适配器；以及(iii)一个迭代的检索和推理循环，该循环持续查询领域特定知识库，直到证据既相关又自洽。与传统RAG系统不同，CyberRAG采用代理式设计，实现了动态控制流和自适应推理。这种以代理为中心的架构能自主地完善其威胁标签和自然语言解释，从而减少误报并增强可解释性。该框架是完全可扩展的：只需添加一个分类器而无需重新训练核心代理即可支持新的攻击类型。CyberRAG经过评估，在每类攻击上实现了超过94%的准确率，并通过语义编排将最终分类准确率推高到94.92%。生成的解释在BERTScore中得分高达0.94，在基于GPT-4的专家评估中得分为4.9/5。这些结果表明，代理式、面向专家的RAG可以将高检测准确率与可信赖的、可用于SOC的文本相结合，为半自主网络防御工作流提供了一条实用且可扩展的路径。", "summary": "CyberRAG是一个创新的代理式RAG框架，旨在解决企业网络安全中IDS/IPS警报过载、高误报率以及现有RAG系统上下文不相关和解释不足的问题。它通过一个中央LLM代理协调专业分类器、工具适配器和迭代检索推理循环，实现网络攻击的实时、高精度分类、可信解释和结构化报告。CyberRAG的代理式设计使其能够动态控制和自适应推理，显著降低误报并提高可解释性。该系统具有高度可扩展性，并已在评估中展现出卓越的性能，分类准确率高达94.92%，解释质量也得到高度认可，为实现半自主网络防御提供了可行方案。", "keywords": "网络攻击分类, 代理式RAG, 入侵检测, LLM, 威胁报告", "comments": "CyberRAG的创新之处在于其代理式RAG框架，它通过引入中央LLM代理协调专业分类器和迭代检索推理循环，有效解决了传统RAG系统在网络安全领域面临的上下文不相关和解释不足的挑战。其模块化和可扩展性设计也极具价值，使得系统能够轻松适应新的攻击类型，无需重新训练核心代理。高准确率和高质量的解释使其在实际安全运营中心(SOC)环境中具有很高的应用潜力，是实现半自主网络防御的重要一步。"}}
{"id": "2507.02313", "title": "A Vehicle-in-the-Loop Simulator with AI-Powered Digital Twins for Testing Automated Driving Controllers", "authors": ["Zengjie Zhang", "Giannis Badakis", "Michalis Galanis", "Adem Bavarşi", "Edwin van Hassel", "Mohsen Alirezaei", "Sofie Haesaert"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02313v1", "summary": "Simulators are useful tools for testing automated driving controllers.\nVehicle-in-the-loop (ViL) tests and digital twins (DTs) are widely used\nsimulation technologies to facilitate the smooth deployment of controllers to\nphysical vehicles. However, conventional ViL tests rely on full-size vehicles,\nrequiring large space and high expenses. Also, physical-model-based DT suffers\nfrom the reality gap caused by modeling imprecision. This paper develops a\ncomprehensive and practical simulator for testing automated driving controllers\nenhanced by scaled physical cars and AI-powered DT models. The scaled cars\nallow for saving space and expenses of simulation tests. The AI-powered DT\nmodels ensure superior simulation fidelity. Moreover, the simulator integrates\nwell with off-the-shelf software and control algorithms, making it easy to\nextend. We use a filtered control benchmark with formal safety guarantees to\nshowcase the capability of the simulator in validating automated driving\ncontrollers. Experimental studies are performed to showcase the efficacy of the\nsimulator, implying its great potential in validating control solutions for\nautonomous vehicles and intelligent traffic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02313v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于测试自动驾驶控制器的AI驱动数字孪生车辆在环模拟器", "tldr": "本文开发了一种利用缩比车辆和AI驱动数字孪生模型的新型车辆在环模拟器，以经济高效且高保真地测试自动驾驶控制器。", "motivation": "传统的车辆在环（ViL）测试依赖于全尺寸车辆，导致高空间和高成本。同时，基于物理模型的数字孪生（DT）存在因建模不精确引起的现实差距，影响仿真保真度。", "method": "本文开发了一种综合实用的模拟器，通过使用缩比物理车辆和AI驱动的数字孪生模型来增强自动驾驶控制器测试。缩比车辆用于节省空间和费用，AI驱动的数字孪生模型确保了卓越的仿真保真度。此外，该模拟器能与现成软件和控制算法良好集成，易于扩展。", "result": "通过使用具有正式安全保证的滤波控制基准，实验研究展示了该模拟器在验证自动驾驶控制器方面的有效性。", "conclusion": "该模拟器在验证自动驾驶汽车和智能交通的控制解决方案方面具有巨大潜力。", "translation": "模拟器是测试自动驾驶控制器的有用工具。车辆在环（ViL）测试和数字孪生（DT）是广泛使用的仿真技术，旨在促进控制器顺利部署到物理车辆。然而，传统的ViL测试依赖于全尺寸车辆，需要大空间和高成本。此外，基于物理模型的数字孪生存在因建模不精确引起的现实差距。本文开发了一种全面实用的模拟器，用于测试由缩比物理汽车和AI驱动的数字孪生模型增强的自动驾驶控制器。缩比汽车可以节省仿真测试的空间和费用。AI驱动的数字孪生模型确保了卓越的仿真保真度。此外，该模拟器与现成的软件和控制算法良好集成，易于扩展。我们使用具有正式安全保证的滤波控制基准来展示模拟器在验证自动驾驶控制器方面的能力。实验研究展示了模拟器的有效性，这表明其在验证自动驾驶汽车和智能交通控制解决方案方面具有巨大潜力。", "summary": "本文提出了一种新颖的车辆在环（ViL）模拟器，用于测试自动驾驶控制器。该模拟器通过结合缩比物理车辆和AI驱动的数字孪生模型，解决了传统ViL测试成本高、空间需求大以及基于物理模型的数字孪生存在的现实差距问题。缩比车辆降低了成本和空间需求，而AI驱动的数字孪生模型则提高了仿真保真度。该模拟器还易于与现有软件集成，具有良好的扩展性。实验结果证明了其在验证自动驾驶和智能交通控制解决方案方面的有效性。", "keywords": "自动驾驶控制器, 车辆在环模拟器, 数字孪生, 缩比车辆, 仿真保真度", "comments": "本文通过将车辆在环测试与AI驱动的数字孪生和缩比物理模型相结合，为自动驾驶控制器测试提供了一种创新方法。这种混合方法有效地解决了全尺寸车辆测试相关的成本和空间等实际挑战，同时显著提高了仿真保真度。其良好的可扩展性进一步增强了其实用性。"}}
{"id": "2507.02318", "title": "Precisely Detecting Python Type Errors via LLM-based Unit Test Generation", "authors": ["Chen Yang", "Ziqi Wang", "Yanjie Jiang", "Lin Yang", "Yuteng Zheng", "Jianyi Zhou", "Junjie Chen"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02318v1", "summary": "Type errors in Python often lead to runtime failures, posing significant\nchallenges to software reliability and developer productivity. Existing static\nanalysis tools aim to detect such errors without execution but frequently\nsuffer from high false positive rates. Recently, unit test generation\ntechniques offer great promise in achieving high test coverage, but they often\nstruggle to produce bug-revealing tests without tailored guidance. To address\nthese limitations, we present RTED, a novel type-aware test generation\ntechnique for automatically detecting Python type errors. Specifically, RTED\ncombines step-by-step type constraint analysis with reflective validation to\nguide the test generation process and effectively suppress false positives. We\nevaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.\nExperimental results show that RTED can detect 22-29 more benchmarked type\nerrors than four state-of-the-art techniques. RTED is also capable of producing\nfewer false positives, achieving an improvement of 173.9%-245.9% in precision.\nFurthermore, RTED successfully discovered 12 previously unknown type errors\nfrom six real-world open-source Python projects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02318v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过基于LLM的单元测试生成精确检测Python类型错误", "tldr": "RTED通过结合类型感知测试生成和反射验证，精确检测Python类型错误，减少假阳性，并发现新漏洞。", "motivation": "Python中的类型错误常导致运行时故障，影响软件可靠性和开发者生产力。现有静态分析工具的假阳性率高，而单元测试生成技术在没有定制指导的情况下难以生成能揭示错误的测试。", "method": "本文提出了RTED，一种新颖的类型感知测试生成技术，用于自动检测Python类型错误。RTED结合了逐步类型约束分析和反射验证来指导测试生成过程，并有效抑制假阳性。", "result": "RTED比四种最先进的技术能多检测22-29个基准类型错误。它能产生更少的假阳性，精度提高了173.9%-245.9%。此外，RTED成功地从六个真实世界的开源Python项目中发现了12个先前未知的类型错误。", "conclusion": "RTED是一种有效且精确的Python类型错误检测技术，其性能优于现有方法，并能发现新的错误。", "translation": "Python中的类型错误常导致运行时故障，对软件可靠性和开发者生产力构成重大挑战。现有静态分析工具旨在不执行代码的情况下检测此类错误，但常受高假阳性率困扰。近期，单元测试生成技术在实现高测试覆盖率方面展现出巨大潜力，但它们通常难以在没有定制指导的情况下生成能揭示错误的测试。为解决这些局限性，我们提出了RTED，一种新颖的类型感知测试生成技术，用于自动检测Python类型错误。具体而言，RTED结合了逐步类型约束分析和反射验证来指导测试生成过程，并有效抑制假阳性。我们在两个广泛使用的基准测试BugsInPy和TypeBugs上评估了RTED。实验结果表明，RTED比四种最先进的技术能多检测22-29个基准类型错误。RTED还能产生更少的假阳性，精度提高了173.9%-245.9%。此外，RTED成功地从六个真实世界的开源Python项目中发现了12个先前未知的类型错误。", "summary": "本文介绍了RTED，一种新颖的类型感知测试生成技术，旨在精确检测Python类型错误。为解决现有静态分析工具高假阳性率和单元测试生成难以发现错误的问题，RTED集成了逐步类型约束分析与反射验证。在BugsInPy和TypeBugs基准测试上的评估显示，RTED在检测到的错误数量上（多22-29个）和精度（假阳性减少173.9%-245.9%）方面均优于现有技术。此外，RTED还成功发现了六个真实世界Python项目中的12个先前未知的类型错误。", "keywords": "Python类型错误, 单元测试生成, 类型感知测试, 假阳性, 软件可靠性", "comments": "该论文的创新之处在于其RTED技术，通过结合类型感知测试生成、逐步类型约束分析和反射验证，有效解决了静态分析中长期存在的假阳性过高问题以及生成能够揭示错误的测试的挑战。其在真实世界项目中发现新错误的能力，凸显了该技术的实际重要性。"}}
{"id": "2507.02229", "title": "An Exploration of Internal States in Collaborative Problem Solving", "authors": ["Sifatul Anindho", "Videep Venkatesha", "Mariah Bradford", "Anne M. Cleary", "Nathaniel Blanchard"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to the International Conference on Human-Computer Interaction (HCII) 2025", "url": "http://arxiv.org/abs/2507.02229v1", "summary": "Collaborative problem solving (CPS) is a complex cognitive, social, and\nemotional process that is increasingly prevalent in educational and\nprofessional settings. This study investigates the emotional states of\nindividuals during CPS using a mixed-methods approach. Teams of four first\ncompleted a novel CPS task. Immediately after, each individual was placed in an\nisolated room where they reviewed the video of their group performing the task\nand self-reported their internal experiences throughout the task. We performed\na linguistic analysis of these internal monologues, providing insights into the\nrange of emotions individuals experience during CPS. Our analysis showed\ndistinct patterns in language use, including characteristic unigrams and\nbigrams, key words and phrases, emotion labels, and semantic similarity between\nemotion-related words.", "comment": "Accepted to the International Conference on Human-Computer\n  Interaction (HCII) 2025", "pdf_url": "http://arxiv.org/pdf/2507.02229v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "协作问题解决中内部状态的探索", "tldr": "本研究采用混合方法探讨了协作问题解决（CPS）过程中个体的内部情绪状态，发现语言使用中存在独特的模式。", "motivation": "协作问题解决（CPS）是一个复杂的认知、社会和情感过程，在教育和专业环境中越来越普遍，本研究旨在深入了解个体在CPS过程中的情感状态。", "method": "研究采用混合方法，首先让四人团队完成一项新颖的CPS任务。任务结束后，每个个体被单独安置，回顾团队表现视频并自我报告其内部体验。随后对这些内部独白进行语言学分析。", "result": "分析显示，在语言使用中存在独特的模式，包括特征性的单元词、二元词、关键词和短语、情感标签以及情感相关词语之间的语义相似性。", "conclusion": "未在摘要中提及", "translation": "协作问题解决（CPS）是一个复杂的认知、社会和情感过程，在教育和专业环境中日益普遍。本研究采用混合方法调查了CPS过程中个体的情绪状态。首先，四人团队完成了一项新颖的CPS任务。任务结束后，每个个体被安置在一个独立的房间里，回顾其团队执行任务的视频，并自我报告在整个任务过程中的内部体验。我们对这些内部独白进行了语言学分析，从而深入了解了个体在CPS过程中所经历的情感范围。我们的分析显示，语言使用中存在独特的模式，包括特征性的单元词和二元词、关键词和短语、情感标签以及情感相关词语之间的语义相似性。", "summary": "本研究旨在探索协作问题解决（CPS）过程中个体的内部情绪状态。研究采用混合方法，让团队完成CPS任务后，个体通过回顾视频并自我报告其内部经验。随后对这些独白进行语言学分析，结果揭示了语言使用中存在独特的模式，为理解CPS中的情感体验提供了见解。", "keywords": "协作问题解决, 内部状态, 情感状态, 语言学分析, 自我报告", "comments": "本研究通过结合任务视频回顾和自我报告的内部独白，并进行语言学分析，提供了一种新颖的方法来探究协作问题解决中的复杂情感维度。其创新点在于对内部体验的细致捕捉和量化分析。"}}
{"id": "2507.02613", "title": "MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking", "authors": ["Yalin E. Sagduyu", "Kemal Davaslioglu", "Tugba Erpek", "Sastry Kompella", "Gustave Anderson", "Jonathan Ashdown"], "categories": ["cs.NI", "cs.DC", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02613v1", "summary": "This paper presents a complete signal-processing chain for multistatic\nintegrated sensing and communications (ISAC) using 5G Positioning Reference\nSignal (PRS). We consider a distributed architecture in which one gNB transmits\na periodic OFDM-PRS waveform while multiple spatially separated receivers\nexploit the same signal for target detection, parameter estimation and\ntracking. A coherent cross-ambiguity function (CAF) is evaluated to form a\nrange-Doppler map from which the bistatic delay and radial velocity are\nextracted for every target. For a single target, the resulting bistatic delays\nare fused through nonlinear least-squares trilateration, yielding a geometric\nposition estimate, and a regularized linear inversion of the radial-speed\nequations yields a two-dimensional velocity vector, where speed and heading are\nobtained. The approach is applied to 2D and 3D settings, extended to account\nfor time synchronization bias, and generalized to multiple targets by resolving\ntarget association. The sequence of position-velocity estimates is then fed to\nstandard and extended Kalman filters to obtain smoothed tracks. Our results\nshow high-fidelity moving-target detection, positioning, and tracking using 5G\nPRS signals for multistatic ISAC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02613v1", "cate": "cs.NI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MULTI-SCOUT：5G及未来中用于移动目标检测、定位与跟踪的多基地综合感知与通信", "tldr": "该论文提出了一种基于5G PRS的多基地ISAC信号处理链，实现高精度移动目标检测、定位和跟踪。", "motivation": "论文旨在利用5G定位参考信号（PRS）构建一个完整的多基地综合感知与通信（ISAC）信号处理链，以实现对移动目标的检测、定位和跟踪。", "method": "论文提出了一种分布式架构，其中一个gNB发射周期性OFDM-PRS波形，多个空间分离的接收器利用同一信号进行目标检测、参数估计和跟踪。处理链包括：评估相干交叉模糊函数（CAF）以形成距离-多普勒图，从中提取每个目标的双基地延迟和径向速度；对于单个目标，通过非线性最小二乘三边测量融合双基地延迟以获得几何位置估计，并通过径向速度方程的正则化线性反演获得二维速度矢量；该方法应用于2D和3D设置，并扩展以考虑时间同步偏差，通过解决目标关联泛化到多个目标；位置-速度估计序列随后输入到标准和扩展卡尔曼滤波器以获得平滑轨迹。", "result": "结果显示，使用5G PRS信号进行多基地ISAC能够实现高精度的移动目标检测、定位和跟踪。", "conclusion": "本文成功展示了基于5G PRS的多基地ISAC系统在移动目标检测、定位和跟踪方面的高性能。", "translation": "本文提出了一种完整的信号处理链，用于利用5G定位参考信号（PRS）进行多基地综合感知与通信（ISAC）。我们考虑一种分布式架构，其中一个gNB发射周期性OFDM-PRS波形，而多个空间分离的接收器利用同一信号进行目标检测、参数估计和跟踪。通过评估相干交叉模糊函数（CAF）来形成距离-多普勒图，从中提取每个目标的双基地延迟和径向速度。对于单个目标，通过非线性最小二乘三边测量融合所得的双基地延迟，得到几何位置估计，并通过径向速度方程的正则化线性反演得到二维速度矢量，从而获得速度和航向。该方法应用于2D和3D设置，并扩展以考虑时间同步偏差，并通过解决目标关联泛化到多个目标。随后，将位置-速度估计序列输入到标准和扩展卡尔曼滤波器以获得平滑轨迹。我们的结果表明，使用5G PRS信号进行多基地ISAC能够实现高精度的移动目标检测、定位和跟踪。", "summary": "该论文介绍了MULTI-SCOUT系统，一个基于5G PRS的多基地综合感知与通信（ISAC）信号处理链。该系统采用分布式架构，利用一个gNB的OFDM-PRS信号和多个接收器进行移动目标检测、定位和跟踪。核心方法包括使用相干交叉模糊函数提取双基地延迟和径向速度，通过非线性最小二乘三边测量和正则化线性反演实现高精度位置和速度估计，并结合卡尔曼滤波器进行目标跟踪。该方法适用于单目标和多目标场景，并考虑了时间同步偏差。实验结果验证了其在移动目标检测、定位和跟踪方面的高精度表现。", "keywords": "5G PRS, 多基地ISAC, 目标检测, 目标定位, 目标跟踪", "comments": "该论文的创新点在于将5G PRS信号应用于多基地ISAC场景，实现了对移动目标的高精度检测、定位和跟踪。通过结合信号处理技术（如CAF、三边测量、线性反演）和跟踪算法（卡尔曼滤波器），构建了一个完整的、实用的解决方案。其重要性在于为未来5G及B5G网络中融合通信和感知功能提供了可行路径，尤其是在自动驾驶、智能交通等领域具有潜在应用价值。"}}
{"id": "2507.02291", "title": "Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications", "authors": ["Zhaoyu Zhang", "Lingyi Wang", "Wei Wu", "Fuhui Zhou", "Qihui Wu"], "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02291v1", "summary": "Data-driven semantic communication is based on superficial statistical\npatterns, thereby lacking interpretability and generalization, especially for\napplications with the presence of unseen data. To address these challenges, we\npropose a novel knowledge graph-enhanced zero-shot semantic communication\n(KGZS-SC) network. Guided by the structured semantic information from a\nknowledge graph-based semantic knowledge base (KG-SKB), our scheme provides\ngeneralized semantic representations and enables reasoning for unseen cases.\nSpecifically, the KG-SKB aligns the semantic features in a shared category\nsemantics embedding space and enhances the generalization ability of the\ntransmitter through aligned semantic features, thus reducing communication\noverhead by selectively transmitting compact visual semantics. At the receiver,\nzero-shot learning (ZSL) is leveraged to enable direct classification for\nunseen cases without the demand for retraining or additional computational\noverhead, thereby enhancing the adaptability and efficiency of the\nclassification process in dynamic or resource-constrained environments. The\nsimulation results conducted on the APY datasets show that the proposed KGZS-SC\nnetwork exhibits robust generalization and significantly outperforms existing\nSC frameworks in classifying unseen categories across a range of SNR levels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02291v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于知识图谱的可解释和泛化零样本语义通信", "tldr": "本文提出了一种基于知识图谱的零样本语义通信（KGZS-SC）网络，通过利用知识图谱增强的语义知识库（KG-SKB）和零样本学习（ZSL），解决了传统数据驱动语义通信缺乏可解释性和泛化能力的问题，特别是在处理未见数据方面，并显著提升了未见类别的分类性能。", "motivation": "传统数据驱动的语义通信依赖于表层统计模式，导致其缺乏可解释性和泛化能力，尤其是在处理未见数据时表现不佳。", "method": "本文提出了一种新颖的知识图谱增强零样本语义通信（KGZS-SC）网络。该方案利用基于知识图谱的语义知识库（KG-SKB）提供的结构化语义信息，实现泛化的语义表示并支持对未见情况的推理。KG-SKB在共享类别语义嵌入空间中对齐语义特征，并通过对齐的语义特征增强发射机的泛化能力，通过选择性传输紧凑的视觉语义来减少通信开销。在接收端，利用零样本学习（ZSL）直接对未见情况进行分类，无需再训练或额外的计算开销。", "result": "在APY数据集上的仿真结果表明，所提出的KGZS-SC网络展现出鲁棒的泛化能力，并在不同信噪比水平下，在未见类别分类方面显著优于现有语义通信框架。", "conclusion": "所提出的KGZS-SC网络通过结合知识图谱和零样本学习，有效解决了传统语义通信在可解释性、泛化能力和处理未见数据方面的挑战，显著提升了通信效率和分类性能。", "translation": "数据驱动的语义通信基于表层统计模式，因此缺乏可解释性和泛化性，特别是在存在未见数据的应用中。为了解决这些挑战，我们提出了一种新颖的知识图谱增强零样本语义通信（KGZS-SC）网络。在基于知识图谱的语义知识库（KG-SKB）提供的结构化语义信息指导下，我们的方案提供了泛化的语义表示，并能够对未见情况进行推理。具体来说，KG-SKB在共享类别语义嵌入空间中对齐语义特征，并通过对齐的语义特征增强发射机的泛化能力，从而通过选择性传输紧凑的视觉语义来减少通信开销。在接收端，利用零样本学习（ZSL）直接对未见情况进行分类，无需再训练或额外的计算开销，从而提高了动态或资源受限环境中分类过程的适应性和效率。在APY数据集上进行的仿真结果表明，所提出的KGZS-SC网络展现出鲁棒的泛化能力，并在不同信噪比水平下，在未见类别分类方面显著优于现有语义通信框架。", "summary": "本文提出了一种名为知识图谱增强零样本语义通信（KGZS-SC）的新型网络，旨在解决传统数据驱动语义通信在可解释性和泛化能力方面的不足，尤其是在处理未见数据时。该方案利用知识图谱语义知识库（KG-SKB）提供结构化语义信息，以实现泛化语义表示和对未见情况的推理。KGZS-SC通过对齐语义特征来增强发射机泛化能力并减少通信开销，并在接收端结合零样本学习（ZSL）以实现未见数据的直接分类，无需额外训练。仿真结果表明，KGZS-SC在未见类别分类上表现出强大的泛化能力，并显著优于现有框架。", "keywords": "知识图谱, 零样本学习, 语义通信, 泛化能力, 可解释性", "comments": "该论文的创新点在于将知识图谱与零样本学习相结合，以解决传统数据驱动语义通信在可解释性和泛化性方面的固有缺陷。通过引入知识图谱语义知识库（KG-SKB），实现了对未见数据的有效处理，这在动态和资源受限环境中具有重要意义。其提出的方法提升了通信效率并降低了计算开销，为未来语义通信系统的发展提供了新的思路。"}}
{"id": "2507.02660", "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification", "authors": ["Deepak Narayan Gadde", "Keerthan Kopparam Radhakrishna", "Vaisakh Naduvodi Viswambharan", "Aman Kumar", "Djones Lettnin", "Wolfgang Kunz", "Sebastian Simon"], "categories": ["cs.AI", "cs.AR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL", "url": "http://arxiv.org/abs/2507.02660v1", "summary": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.", "comment": "To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated\n  Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL", "pdf_url": "http://arxiv.org/pdf/2507.02660v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "嘿，AI，给我生成硬件代码！基于智能体AI的硬件设计与验证", "tldr": "本文提出一种基于智能体AI的方法，结合人工干预，实现端到端硬件设计与验证，并在开源设计上展现出95%以上的覆盖率和验证时间缩短，性能优越。", "motivation": "现代集成电路（IC）的开发过程日益复杂且耗时，特别是硬件设计验证环节，需要大量精力以确保无缺陷流片。大型语言模型（LLM）和生成式AI（GenAI）在自然语言处理领域的成功，为解决硬件设计验证的挑战提供了新的可能性。", "method": "本文提出一种基于智能体AI的硬件设计验证方法，该方法使AI智能体在人工干预（HITL）的协作下，能够参与到更动态、迭代和自反思的过程中，最终执行端到端的硬件设计和验证。", "result": "该方法在五个开源设计上进行了评估，实现了超过95%的覆盖率，并缩短了验证时间，同时展示了卓越的性能、适应性和可配置性。", "conclusion": "基于智能体AI的硬件设计与验证方法显著提升了硬件开发流程的效率和质量。", "translation": "现代集成电路（IC）变得越来越复杂，其开发过程也同样如此。硬件设计验证需要一种有条不紊、严谨的方法来规划、开发、执行和签署功能正确的硬件设计。这个繁琐的过程需要大量的精力和时间来确保无错误流片。随着大型语言模型（LLM）的出现，自然语言处理领域发生了重大变革。这些强大的模型，通常被称为生成式AI（GenAI），彻底改变了机器理解和生成人类语言的方式，从而在包括硬件设计验证在内的广泛应用中实现了前所未有的进步。本文提出了一种基于智能体AI的硬件设计验证方法，该方法使AI智能体在人工干预（HITL）的协作下，能够参与到更动态、迭代和自反思的过程中，最终执行端到端的硬件设计和验证。该方法在五个开源设计上进行了评估，实现了超过95%的覆盖率，同时缩短了验证时间，并展示了卓越的性能、适应性和可配置性。", "summary": "本文提出了一种基于智能体AI的端到端硬件设计与验证方法，该方法通过整合人工干预（Human-in-the-Loop）来应对现代集成电路开发日益增长的复杂性。该方法利用生成式AI的能力，在五个开源设计上进行了评估，结果显示其实现了超过95%的覆盖率，并显著缩短了验证时间，同时展现出卓越的性能、适应性和可配置性。", "keywords": "智能体AI, 硬件设计, 验证, 生成式AI, 人工干预", "comments": "本文创新性地将智能体AI和生成式AI应用于高度复杂且关键的硬件设计与验证领域。引入人工干预（HITL）是一个实用且重要的考量，它认识到在如此敏感的领域中，完全自主的AI目前仍有局限性。所报告的高覆盖率和缩短的验证时间表明，该方法对IC开发具有显著的实际效益。"}}
{"id": "2507.02376", "title": "VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software", "authors": ["Chung-ju Huang", "Ziqi Zhang", "Yinggui Wang", "Binghui Wang", "Tao Wei", "Leye Wang"], "categories": ["cs.SE", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02376v1", "summary": "Vertical Federated Learning (VFL) is a distributed AI software deployment\nmechanism for cross-silo collaboration without accessing participants' data.\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\nof the inference software of the data party. To address this problem, we design\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\nparty to audit whether the data party's inference software is executed as\nexpected during large-scale inference without leaking the data privacy of the\ndata party or introducing additional latency to the inference system. The core\nof VeFIA is that the task party can use the inference results from a framework\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\ncorrectness of the data party's computation results. VeFIA guarantees that, as\nlong as the abnormal inference exceeds 5.4%, the task party can detect\nexecution anomalies in the inference software with a probability of 99.99%,\nwithout incurring any additional online inference latency. VeFIA's random\nsampling validation achieves 100% positive predictive value, negative\npredictive value, and true positive rate in detecting abnormal inference. To\nthe best of our knowledge, this is the first paper to discuss the correctness\nof inference software execution in VFL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02376v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "VeFIA：一种用于垂直联邦协作软件的高效推理审计框架", "tldr": "VeFIA是一个垂直联邦学习的推理审计框架，用于在不泄露数据隐私和不增加延迟的情况下，审计数据方的推理软件执行正确性。", "motivation": "现有的垂直联邦学习（VFL）缺乏一种机制来审计数据方推理软件的执行正确性。", "method": "VeFIA框架通过任务方使用来自可信执行环境（TEE）框架和协调器的推理结果来验证数据方计算结果的正确性。它采用随机抽样验证。", "result": "VeFIA保证当异常推理超过5.4%时，任务方能以99.99%的概率检测到执行异常，且不增加在线推理延迟。随机抽样验证在检测异常推理时达到了100%的阳性预测值、阴性预测值和真阳性率。", "conclusion": "VeFIA是首个讨论VFL中推理软件执行正确性的框架。它提供了一种高效且保护隐私的审计机制。", "translation": "垂直联邦学习（VFL）是一种分布式AI软件部署机制，用于跨孤岛协作，而无需访问参与者数据。然而，现有的VFL工作缺乏一种机制来审计数据方推理软件的执行正确性。为了解决这个问题，我们设计了一个垂直联邦推理审计（VeFIA）框架。VeFIA帮助任务方在大规模推理过程中，在不泄露数据方数据隐私或不给推理系统引入额外延迟的情况下，审计数据方的推理软件是否按预期执行。VeFIA的核心是任务方可以使用来自可信执行环境（TEE）框架和协调器的推理结果来验证数据方计算结果的正确性。VeFIA保证，只要异常推理超过5.4%，任务方就能以99.99%的概率检测到推理软件中的执行异常，且不产生任何额外的在线推理延迟。VeFIA的随机抽样验证在检测异常推理时实现了100%的阳性预测值、阴性预测值和真阳性率。据我们所知，这是第一篇讨论VFL中推理软件执行正确性的论文。", "summary": "本文提出了VeFIA，一个垂直联邦推理审计框架，旨在解决现有垂直联邦学习中数据方推理软件执行正确性审计机制的缺失。VeFIA允许任务方在不泄露数据隐私和不增加额外延迟的前提下，通过结合可信执行环境（TEE）和协调器的推理结果来验证数据方的计算结果。实验结果表明，VeFIA能够高效准确地检测到推理异常。", "keywords": "垂直联邦学习, 推理审计, 可信执行环境, 数据隐私, 协作软件", "comments": "本文的创新之处在于首次提出了垂直联邦学习中推理软件执行正确性的审计框架，填补了该领域的空白。VeFIA通过结合TEE和随机抽样验证，在保证数据隐私和不引入额外延迟的前提下，实现了高效率和高准确度的异常检测，对于提升联邦学习系统的可靠性具有重要意义。"}}
{"id": "2507.02173", "title": "Data Diversification Methods In Alignment Enhance Math Performance In LLMs", "authors": ["Berkan Dokmeci", "Qingyang Wu", "Ben Athiwaratkun", "Ce Zhang", "Shuaiwen Leon Song", "James Zou"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02173v1", "summary": "While recent advances in preference learning have enhanced alignment in human\nfeedback, mathematical reasoning remains a persistent challenge. We investigate\nhow data diversification strategies in preference optimization can improve the\nmathematical reasoning abilities of large language models (LLMs). We evaluate\nthree common data generation methods: temperature sampling, Chain-of-Thought\nprompting, and Monte Carlo Tree Search (MCTS), and introduce\nDiversified-ThinkSolve (DTS), a novel structured approach that systematically\ndecomposes problems into diverse reasoning paths. Our results show that with\nstrategically diversified preference data, models can substantially improve\nmathematical reasoning performance, with the best approach yielding gains of\n7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong\nperformance, DTS incurs only a marginal computational overhead (1.03x) compared\nto the baseline, while MCTS is nearly five times more costly with lower\nreturns. These findings demonstrate that structured exploration of diverse\nproblem-solving methods creates more effective preference data for mathematical\nalignment than traditional approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02173v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "对齐中的数据多样化方法增强LLM的数学性能", "tldr": "研究发现，在偏好优化中采用数据多样化策略，特别是新提出的Diversified-ThinkSolve (DTS) 方法，能显著提升大型语言模型（LLMs）的数学推理能力，且计算开销较低。", "motivation": "尽管偏好学习在人类反馈对齐方面取得了进展，但大型语言模型（LLMs）的数学推理能力仍然是一个挑战。", "method": "研究调查了三种常见的数据生成方法：温度采样、思维链提示和蒙特卡洛树搜索（MCTS），并引入了一种新颖的结构化方法Diversified-ThinkSolve (DTS)，它系统地将问题分解为多样化的推理路径。研究通过这些方法生成多样化的偏好数据来优化LLMs。", "result": "通过策略性多样化的偏好数据，模型显著提高了数学推理性能。最佳方法在GSM8K上比基础模型提升了7.1%，在MATH上提升了4.2%。DTS的计算开销仅为基线的1.03倍，而MCTS成本高出近五倍但收益较低。", "conclusion": "结构化探索多样化的问题解决方法可以创建比传统方法更有效的数学对齐偏好数据。", "translation": "尽管偏好学习的最新进展增强了人类反馈对齐，但数学推理仍然是一个持续的挑战。我们研究了偏好优化中的数据多样化策略如何提高大型语言模型（LLMs）的数学推理能力。我们评估了三种常见的数据生成方法：温度采样、思维链提示和蒙特卡洛树搜索（MCTS），并引入了Diversified-ThinkSolve（DTS），这是一种新颖的结构化方法，系统地将问题分解为多样化的推理路径。我们的结果表明，通过策略性多样化的偏好数据，模型可以显著提高数学推理性能，其中最佳方法在GSM8K上比基础模型提升了7.1%，在MATH上提升了4.2%。尽管性能强劲，DTS与基线相比仅产生微小的计算开销（1.03倍），而MCTS的成本几乎高出五倍但回报较低。这些发现表明，结构化探索多样化的问题解决方法比传统方法能创建更有效的数学对齐偏好数据。", "summary": "该研究探讨了在偏好优化中采用数据多样化策略以提升大型语言模型（LLMs）数学推理能力的方法。论文评估了温度采样、思维链提示和蒙特卡洛树搜索（MCTS）等数据生成方法，并提出了一种新的结构化方法Diversified-ThinkSolve (DTS)，该方法能系统分解问题并生成多样化推理路径。实验结果显示，策略性多样化的偏好数据能显著提高模型的数学性能，其中DTS表现最佳，且计算开销小，证明了结构化探索多样化解题方法在数学对齐中的有效性。", "keywords": "数据多样化, 偏好优化, 数学推理, 大型语言模型, Diversified-ThinkSolve", "comments": "本文的创新点在于提出了Diversified-ThinkSolve (DTS) 这种结构化数据多样化方法，有效地提升了LLM的数学推理能力，同时保持了较低的计算成本。这对于优化LLM在复杂推理任务上的性能具有重要意义，尤其是在资源有限的场景下，DTS的效率优势使其具有较高的实用价值。"}}
{"id": "2507.02222", "title": "High-Fidelity Differential-information Driven Binary Vision Transformer", "authors": ["Tian Gao", "Zhiyuan Zhang", "Kaijie Yin", "Xu-Cheng Zhong", "Hui Kong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02222v1", "summary": "The binarization of vision transformers (ViTs) offers a promising approach to\naddressing the trade-off between high computational/storage demands and the\nconstraints of edge-device deployment. However, existing binary ViT methods\noften suffer from severe performance degradation or rely heavily on\nfull-precision modules. To address these issues, we propose DIDB-ViT, a novel\nbinary ViT that is highly informative while maintaining the original ViT\narchitecture and computational efficiency. Specifically, we design an\ninformative attention module incorporating differential information to mitigate\ninformation loss caused by binarization and enhance high-frequency retention.\nTo preserve the fidelity of the similarity calculations between binary Q and K\ntensors, we apply frequency decomposition using the discrete Haar wavelet and\nintegrate similarities across different frequencies. Additionally, we introduce\nan improved RPReLU activation function to restructure the activation\ndistribution, expanding the model's representational capacity. Experimental\nresults demonstrate that our DIDB-ViT significantly outperforms\nstate-of-the-art network quantization methods in multiple ViT architectures,\nachieving superior image classification and segmentation performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02222v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高保真差分信息驱动的二值视觉Transformer", "tldr": "二值化ViT可以减少计算和存储，但会牺牲性能。本文提出了DIDB-ViT，一种新的二值ViT，它利用差分信息和频率分解来保持高保真度和性能，优于现有方法。", "motivation": "现有的二值视觉Transformer（ViT）方法在解决高计算/存储需求与边缘设备部署限制之间的权衡时，普遍存在严重的性能下降或过度依赖全精度模块的问题。", "method": "本文提出了DIDB-ViT，一种新型二值ViT。它设计了一个信息丰富的注意力模块，融合差分信息以减轻二值化造成的信息损失并增强高频保留。为了保持二值Q和K张量之间相似度计算的保真度，该方法使用离散Haar小波进行频率分解并整合不同频率的相似度。此外，还引入了改进的RPReLU激活函数来重构激活分布，以扩展模型的表示能力。", "result": "实验结果表明，DIDB-ViT在多种ViT架构中显著优于最先进的网络量化方法，实现了卓越的图像分类和分割性能。", "conclusion": "本文成功提出了一种名为DIDB-ViT的新型高保真二值视觉Transformer。该方法通过引入差分信息驱动的注意力机制、频率分解和改进的激活函数，有效解决了现有二值ViT的性能下降问题，并在保持计算效率的同时实现了卓越的图像分类和分割性能。", "translation": "视觉Transformer（ViT）的二值化为解决高计算/存储需求与边缘设备部署限制之间的权衡提供了一种有前景的方法。然而，现有的二值ViT方法通常面临严重的性能下降，或者严重依赖全精度模块。为了解决这些问题，我们提出了DIDB-ViT，这是一种新型二值ViT，它在保持原始ViT架构和计算效率的同时，具有高度信息性。具体来说，我们设计了一个信息丰富的注意力模块，其中包含差分信息，以减轻二值化导致的信息损失并增强高频保留。为了保持二值Q和K张量之间相似度计算的保真度，我们使用离散Haar小波进行频率分解，并整合不同频率的相似度。此外，我们引入了一种改进的RPReLU激活函数来重构激活分布，从而扩展模型的表示能力。实验结果表明，我们的DIDB-ViT在多种ViT架构中显著优于最先进的网络量化方法，实现了卓越的图像分类和分割性能。", "summary": "本文提出了一种名为DIDB-ViT的新型高保真二值视觉Transformer，旨在解决现有二值化ViT的性能下降问题，同时保持计算效率。该模型通过集成一个包含差分信息的信息注意力模块来减少数据损失，利用基于离散Haar小波的频率分解来确保精确的相似度计算，并采用改进的RPReLU激活函数来增强表示能力。实验结果表明，DIDB-ViT在多种ViT架构的图像分类和分割任务中，性能优于当前最先进的量化方法。", "keywords": "二值视觉Transformer, 差分信息, 频率分解, 量化, 边缘设备", "comments": "这篇论文通过提出一种创新的二值ViT，解决了将视觉Transformer部署到边缘设备的关键挑战。其创新点在于通过引入差分信息和频率分解来减轻二值化过程中的信息损失，这对于保持高保真度至关重要。改进的激活函数也提升了模型的容量。其优于现有方法的表现突出了其在实际应用中的潜力。"}}
{"id": "2507.02193", "title": "A Multi-Scale Finite Element Method for Investigating Fiber Remodeling in Hypertrophic Cardiomyopathy", "authors": ["Mohammad Mehri", "Kenneth S. Campbell", "Lik Chuan Lee", "Jonathan F. Wenk"], "categories": ["q-bio.TO", "cs.CE"], "primary_category": "Subjects:       Tissues and Organs (q-bio.TO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02193v1", "summary": "A significant hallmark of hypertrophic cardiomyopathy (HCM) is fiber\ndisarray, which is associated with various cardiac events such as heart\nfailure. Quantifying fiber disarray remains critical for understanding the\ndisease s complex pathophysiology. This study investigates the role of\nheterogeneous HCM-induced cellular abnormalities in the development of fiber\ndisarray and their subsequent impact on cardiac pumping function. Fiber\ndisarray is predicted using a stress-based law to reorient myofibers and\ncollagen within a multiscale finite element cardiac modeling framework, MyoFE.\nSpecifically, the model is used to quantify the distinct impacts of\nheterogeneous distributions of hypercontractility, hypocontractility, and\nfibrosis on fiber disarray development and examines their effect on functional\ncharacteristics of the heart. Our results show that heterogenous cell level\nabnormalities highly disrupt the normal mechanics of myocardium and lead to\nsignificant fiber disarray. The pattern of disarray varies depending on the\nspecific perturbation, offering valuable insights into the progression of HCM.\nDespite the random distribution of perturbed regions within the cardiac muscle,\nsignificantly higher fiber disarray is observed near the epicardium compared to\nthe endocardium across all perturbed left ventricle (LV) models. This regional\ndifference in fiber disarray, irrespective of perturbation severity, aligns\nwith previous DT-MRI studies, highlighting the role of regional myocardial\nmechanics in the development of fiber disarray. Furthermore, cardiac\nperformance declined in the remodeled LVs, particularly in those with fibrosis\nand hypocontractility. These findings provide important insights into the\nstructural and functional consequences of HCM and offer a framework for future\ninvestigations into therapeutic interventions targeting cardiac remodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02193v1", "cate": "q-bio.TO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "肥厚性心肌病纤维重构的多尺度有限元方法研究", "tldr": "本研究利用多尺度有限元模型MyoFE，探讨了肥厚性心肌病中异质性细胞异常如何导致心肌纤维排列紊乱及其对心脏功能的影响，发现异质性细胞异常导致显著的纤维紊乱，心外膜区域紊乱更严重，且心功能下降。", "motivation": "肥厚性心肌病（HCM）的一个显著特征是纤维排列紊乱，这与心力衰竭等各种心脏事件相关。量化纤维排列紊乱对于理解该疾病复杂的病理生理学仍然至关重要。本研究旨在调查HCM引起的异质性细胞异常在纤维排列紊乱发展中的作用及其对心脏泵血功能的后续影响。", "method": "研究使用一个名为MyoFE的多尺度有限元心脏建模框架，利用基于应力的定律来预测肌纤维和胶原的重定向，从而预测纤维排列紊乱。具体来说，该模型用于量化收缩过强、收缩过弱和纤维化异质性分布对纤维排列紊乱发展的影响，并检查它们对心脏功能特征的影响。", "result": "结果显示，异质性细胞水平异常严重扰乱了心肌的正常力学特性，并导致显著的纤维排列紊乱。紊乱模式因特定扰动而异，为HCM的进展提供了有价值的见解。尽管心脏肌肉中扰动区域随机分布，但在所有受扰动的左心室模型中，心外膜附近的纤维排列紊乱程度显著高于心内膜附近。这种纤维排列紊乱的区域差异，无论扰动严重程度如何，都与之前的DT-MRI研究结果一致。此外，重构后的左心室心脏功能下降，特别是在那些存在纤维化和收缩过弱的左心室中。", "conclusion": "这些发现为肥厚性心肌病的结构和功能后果提供了重要见解，并为未来针对心脏重构的治疗干预措施研究提供了框架。", "translation": "肥厚性心肌病（HCM）的一个显著特征是纤维排列紊乱，这与心力衰竭等各种心脏事件相关。量化纤维排列紊乱对于理解该疾病复杂的病理生理学仍然至关重要。本研究调查了HCM引起的异质性细胞异常在纤维排列紊乱发展中的作用及其对心脏泵血功能的后续影响。纤维排列紊乱是利用基于应力的定律在多尺度有限元心脏建模框架MyoFE中预测肌纤维和胶原的重定向而实现的。具体来说，该模型用于量化收缩过强、收缩过弱和纤维化异质性分布对纤维排列紊乱发展的不同影响，并检查它们对心脏功能特征的影响。我们的结果表明，异质性细胞水平异常极大地扰乱了心肌的正常力学特性，并导致显著的纤维排列紊乱。紊乱的模式取决于特定的扰动，为HCM的进展提供了宝贵的见解。尽管心脏肌肉中扰动区域是随机分布的，但在所有受扰动的左心室（LV）模型中，与心内膜相比，心外膜附近的纤维排列紊乱程度显著更高。这种纤维排列紊乱的区域差异，无论扰动严重程度如何，都与之前的DT-MRI研究结果一致，突出了区域心肌力学在纤维排列紊乱发展中的作用。此外，重构后的左心室心脏功能下降，特别是在那些存在纤维化和收缩过弱的左心室中。这些发现为HCM的结构和功能后果提供了重要见解，并为未来针对心脏重构的治疗干预措施研究提供了框架。", "summary": "本研究利用多尺度有限元心脏建模框架MyoFE，探讨了肥厚性心肌病（HCM）中异质性细胞异常（如收缩过强、收缩过弱和纤维化）如何导致心肌纤维排列紊乱及其对心脏泵血功能的影响。研究发现，异质性细胞异常显著破坏心肌正常力学并导致纤维紊乱，紊乱模式因扰动类型而异。值得注意的是，心外膜区域的纤维紊乱程度显著高于心内膜，且重构后的左心室心功能下降，尤其在存在纤维化和收缩过弱时。这些结果为理解HCM的结构和功能后果提供了重要见解，并为未来的治疗干预研究奠定了基础。", "keywords": "肥厚性心肌病, 纤维排列紊乱, 多尺度有限元方法, 心脏重构", "comments": "这项研究的创新之处在于其使用了多尺度有限元模型MyoFE来模拟肥厚性心肌病中的纤维重构，并量化了不同细胞异常对纤维排列紊乱和心脏功能的影响。其结果与DT-MRI研究的一致性增强了模型的可靠性。该研究为理解HCM的病理生理学提供了深入的机械洞察，并为未来开发靶向治疗策略提供了理论框架。"}}
{"id": "2507.02351", "title": "Indoor thermal comfort management: A Bayesian machine-learning approach to data denoising and dynamics prediction of HVAC systems", "authors": ["Javier Penuela", "Sahar Moghimian Hoosh", "Ilia Kamyshev", "Aldo Bischi", "Henni Ouerdane"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02351v1", "summary": "The optimal management of a building's microclimate to satisfy the occupants'\nneeds and objectives in terms of comfort, energy efficiency, and costs is\nparticularly challenging. This complexity arises from the non-linear,\ntime-dependent interactions among all the variables of the control problem and\nthe changing internal and external constraints. Focusing on the accurate\nmodeling of the indoor temperature, we propose a data-driven approach to\naddress this challenge. We account for thermal inertia, non-linear effects,\nsmall perturbations of the indoor climate dynamics caused by ventilation and\nweather variations, as well as for the stochastic nature of the control system\ndue to the observed noise in the input signal. Since the prohibitive cost of\nquality data acquisition and processing limits the implementation of\ndata-driven approaches for real-life problems, we applied a method that merges\nseveral Bayesian machine learning and deep learning architectures that are\nsuitable for predicting complex system dynamics, while relaxing the dataset\nquality requirements. Our framework includes a built-in deep Kalman filter,\nwhich makes it deployable even with low-accuracy temperature sensors. It\nachieves state-of-the-art performance, best performing with a 150-minute\nprediction horizon with an RMSE of 0.2455, an MAE of 0.162, and an $R^2$ of\n0.926. The model's performance remains consistent even when exposed to highly\nnoisy data. Finally, we show how our approach can be extended to other\napplications including demand response event duration prediction and equipment\nfailure detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02351v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "室内热舒适管理：一种用于HVAC系统数据去噪和动态预测的贝叶斯机器学习方法", "tldr": "本文提出了一种结合贝叶斯机器学习和深度学习的方法，并引入深度卡尔曼滤波器，用于HVAC系统室内温度的鲁棒预测和数据去噪，即使在低质量或高噪声数据下也能保持高性能。", "motivation": "建筑微气候的优化管理在满足舒适度、能源效率和成本方面具有挑战性，因为控制变量间存在非线性、时间依赖的相互作用和不断变化的约束。高质量数据获取成本高昂限制了数据驱动方法的应用，且室内温度建模需应对热惯性、非线性效应、微扰动及输入信号噪声等问题。", "method": "本文提出了一种数据驱动方法，融合了多种贝叶斯机器学习和深度学习架构，适用于预测复杂系统动态。该框架内置深度卡尔曼滤波器，可放宽数据集质量要求，即使使用低精度温度传感器也能部署。它考虑了热惯性、非线性效应、通风和天气变化引起的扰动以及输入信号噪声的随机性。", "result": "该方法在150分钟的预测范围内实现了最先进的性能，RMSE为0.2455，MAE为0.162，R^2为0.926。即使暴露于高度噪声数据，模型性能也保持一致。此外，该方法可扩展至需求响应事件持续时间预测和设备故障检测等应用。", "conclusion": "所提出的结合贝叶斯机器学习、深度学习和深度卡尔曼滤波的框架，能够有效管理室内热舒适，通过准确预测室内温度动态，即使在低质量或高噪声数据下也能实现高性能，并具有广泛的应用潜力。", "translation": "建筑微气候的优化管理，以满足居住者在舒适度、能源效率和成本方面的需求和目标，尤其具有挑战性。这种复杂性源于控制问题中所有变量之间非线性、时间依赖的相互作用以及不断变化的内部和外部约束。我们专注于室内温度的精确建模，提出了一种数据驱动的方法来解决这一挑战。我们考虑了热惯性、非线性效应、通风和天气变化引起的室内气候动态的小扰动，以及由于输入信号中观察到的噪声导致的控制系统的随机性质。由于高质量数据采集和处理的成本过高限制了数据驱动方法在实际问题中的实施，我们应用了一种方法，该方法融合了多种贝叶斯机器学习和深度学习架构，这些架构适用于预测复杂系统动态，同时放宽了数据集质量要求。我们的框架包含一个内置的深度卡尔曼滤波器，这使得它即使在使用低精度温度传感器时也能部署。它实现了最先进的性能，在150分钟的预测范围内表现最佳，RMSE为0.2455，MAE为0.162，R^2为0.926。即使在暴露于高度噪声数据时，模型的性能也保持一致。最后，我们展示了我们的方法如何扩展到其他应用，包括需求响应事件持续时间预测和设备故障检测。", "summary": "本文针对建筑室内热舒适管理中的挑战，提出了一种数据驱动方法，用于HVAC系统室内温度的精确建模和动态预测。该方法融合了贝叶斯机器学习和深度学习架构，并内置深度卡尔曼滤波器，以应对热惯性、非线性效应、环境扰动和输入信号噪声等复杂因素，同时显著降低了对数据质量的要求，使其能与低精度传感器协同工作。该框架在温度预测方面展现了最先进的性能，即使在高度噪声数据下也能保持一致，并具有扩展到需求响应和设备故障检测等其他应用的潜力。", "keywords": "室内热舒适, 贝叶斯机器学习, 深度学习, HVAC系统, 数据去噪", "comments": "该论文的创新之处在于将贝叶斯机器学习、深度学习与深度卡尔曼滤波器相结合，创建了一个鲁棒的系统，即使在低质量和高噪声数据下也能有效运行，这解决了实际HVAC系统中数据驱动方法面临的一个重要障碍。其在噪声条件下保持性能以及放宽数据质量要求的能力，使其在实际部署中具有高度价值。"}}
{"id": "2507.02006", "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design", "authors": ["Shakya Jayakody", "Youpeng Zhao", "Jun Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      36th IEEE International Conference on Application-Specific Systems, Architectures and Processors. (Accepted)", "url": "http://arxiv.org/abs/2507.02006v1", "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.", "comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)", "pdf_url": "http://arxiv.org/pdf/2507.02006v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "AIRES：通过算法-系统协同设计加速核外GCNs", "tldr": "AIRES通过算法-系统协同设计，显著加速了GCN中核外SpGEMM计算，解决了现有系统I/O延迟高和GPU利用率低的问题，性能提升高达1.8倍。", "motivation": "现有系统在处理大规模图数据时，由于GPU内存限制，核外SpGEMM计算面临高I/O延迟和GPU利用率低的问题，主要瓶颈在于稀疏格式数据对齐和内存分配。", "method": "提出AIRES，一个算法-系统协同设计方案。算法层面：通过块级数据对齐和分块算法解决稀疏矩阵数据对齐问题。系统层面：采用三阶段动态调度，利用分层内存系统（GPU内存、GDS、主机内存）的双向数据传输策略，以减少I/O延迟并提高吞吐量。", "result": "AIRES在真实图处理基准测试中，与现有最先进方法相比，延迟降低高达1.8倍。", "conclusion": "AIRES通过算法和系统协同设计，有效解决了核外SpGEMM计算的性能瓶颈，显著提升了GCNs在处理大规模图数据时的效率。", "translation": "图卷积网络（GCNs）在各种科学应用中至关重要，从生物医学蛋白质-蛋白质相互作用（PPI）到大规模推荐系统。在GCN中建模图结构的一个重要组成部分是稀疏通用矩阵-矩阵乘法（SpGEMM）。随着图数据规模的不断扩大，由于资源受限系统中GPU内存空间有限，SpGEMM通常以核外方式进行。尽管最近通过GPU特征缓存、混合CPU-GPU内存布局或以稀疏格式执行计算等方式缓解核外SpGEMM内存限制的努力，但现有系统仍然面临高I/O延迟和GPU利用率不足的问题。\n在本文中，我们首先指出了现有系统的问题，其中稀疏格式数据对齐和内存分配是主要的性能瓶颈，并提出了AIRES，一种新颖的算法-系统协同设计解决方案，用于加速GCN的核外SpGEMM计算。具体来说，从算法角度，AIRES提出在块级别上缓解稀疏格式矩阵的数据对齐问题，并开发了一种分块算法以促进行块级对齐。在系统层面，AIRES采用三阶段动态调度，其特点是利用分层内存系统（集成GPU内存、GPU Direct Storage (GDS) 和主机内存）的双向数据传输策略，以减少I/O延迟并提高吞吐量。评估结果表明，AIRES显著优于最先进的方法，在真实世界图处理基准测试中实现了高达1.8倍的延迟降低。", "summary": "本文提出了AIRES，一个针对GCNs中核外稀疏通用矩阵-矩阵乘法（SpGEMM）的算法-系统协同设计方案。AIRES通过解决现有系统中的数据对齐和内存分配瓶颈，结合块级数据对齐算法和基于分层内存的双向数据传输调度，显著降低了I/O延迟并提高了GPU利用率。实验证明，AIRES在真实图处理任务中实现了高达1.8倍的延迟降低，优于现有先进方法。", "keywords": "图卷积网络, 核外计算, SpGEMM, 算法-系统协同设计, 内存优化", "comments": "AIRES的创新点在于其算法-系统协同设计方法，特别是将块级数据对齐算法与分层内存系统的动态调度相结合，有效解决了大规模图数据核外SpGEMM计算中的I/O瓶颈和GPU利用率低下问题。这种跨层优化对于提升GCNs在资源受限环境下的性能具有重要意义。"}}
{"id": "2507.02385", "title": "Parameter estimation of range-migrating targets using OTFS signals from LEO satellites", "authors": ["Tong Ding", "Luca Venturino", "Emanuele Grossi"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      submitted to IEEE journal for possible publication", "url": "http://arxiv.org/abs/2507.02385v1", "summary": "This study investigates a communication-centric integrated sensing and\ncommunication (ISAC) system that utilizes orthogonal time frequency space\n(OTFS) modulated signals emitted by low Earth orbit (LEO) satellites to\nestimate the parameters of space targets experiencing range migration,\nhenceforth referred to as high-speed targets. Leveraging the specific signal\nprocessing performed by OTFS transceivers, we derive a novel input-output model\nfor the echo generated by a high-speed target in scenarios where ideal and\nrectangular shaping filters are employed. Our findings reveal that the target\nresponse exhibits a sparse structure in the delay-Doppler domain, dependent\nsolely upon the initial range and range-rate; notably, range migration causes a\nspread in the target response, marking a significant departure from previous\nstudies. Utilizing this signal structure, we propose an approximate\nimplementation of the maximum likelihood estimator for the target's initial\nrange, range-rate, and amplitude. The estimation process involves obtaining\ncoarse information on the target response using a block orthogonal matching\npursuit algorithm, followed by a refinement step using a bank of matched\nfilters focused on a smaller range and range-rate region. Finally, numerical\nexamples are provided to evaluate the estimation performance.", "comment": "submitted to IEEE journal for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.02385v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "利用低轨卫星OTFS信号进行距离徙动目标参数估计", "tldr": "本研究利用低轨卫星发射的OTFS信号，针对经历距离徙动的高速目标，推导了新的信号输入-输出模型，并提出了一种近似最大似然估计器来估计目标的初始距离、距离变化率和幅度。", "motivation": "本研究旨在利用基于通信的集成感知与通信（ISAC）系统，通过低地球轨道（LEO）卫星发射的正交时频空间（OTFS）调制信号，估计经历距离徙动（即高速目标）的空间目标的参数，包括初始距离、距离变化率和幅度。", "method": "研究调查了一个利用LEO卫星OTFS信号的通信中心ISAC系统。推导了在理想和矩形整形滤波器下，高速目标回波的新型输入-输出模型。利用目标在延迟-多普勒域的稀疏结构，提出了一种近似最大似然估计器。估计过程分为两步：首先使用块正交匹配追踪算法获取目标响应的粗略信息，然后使用一组匹配滤波器对较小距离和距离变化率区域进行细化。最后，通过数值示例评估了估计性能。", "result": "研究发现，目标响应在延迟-多普勒域中呈现稀疏结构，仅取决于初始距离和距离变化率。值得注意的是，距离徙动会导致目标响应的扩散，这与以往的研究有显著不同。提出的近似最大似然估计器能够有效估计目标的初始距离、距离变化率和幅度。数值示例验证了估计性能。", "conclusion": "本研究成功提出并评估了一种利用低轨卫星OTFS信号估计距离徙动高速目标参数的方法，该方法基于新推导的信号模型和两步估计过程，为ISAC系统中的高速目标感知提供了有效方案。", "translation": "本研究调查了一种以通信为中心的集成感知与通信（ISAC）系统，该系统利用低地球轨道（LEO）卫星发射的正交时频空间（OTFS）调制信号来估计经历距离徙动的空间目标（此后称为高速目标）的参数。利用OTFS收发器执行的特定信号处理，我们推导出了在理想和矩形整形滤波器情况下，高速目标产生的回波的新型输入-输出模型。我们的研究结果表明，目标响应在延迟-多普勒域中呈现稀疏结构，仅取决于初始距离和距离变化率；值得注意的是，距离徙动导致目标响应的扩散，这与之前的研究有显著不同。利用这种信号结构，我们提出了一种目标初始距离、距离变化率和幅度的最大似然估计器的近似实现。估计过程包括使用块正交匹配追踪算法获取目标响应的粗略信息，然后使用一组匹配滤波器对较小距离和距离变化率区域进行细化。最后，提供了数值示例来评估估计性能。", "summary": "本论文研究了一种以通信为中心的ISAC系统，该系统利用LEO卫星的OTFS信号对经历距离徙动的高速目标进行参数估计。文中推导了目标回波的新型输入-输出模型，揭示了目标响应在延迟-多普勒域的稀疏结构，并发现距离徙动会导致响应扩散。基于此结构，论文提出了一种用于估计初始距离、距离变化率和幅度的近似最大似然估计器，该估计器采用块正交匹配追踪算法进行粗略估计，并结合匹配滤波器进行细化。通过数值示例评估了估计性能。", "keywords": "OTFS, ISAC, LEO卫星, 距离徙动, 参数估计", "comments": "该论文的创新点在于推导了利用OTFS信号对距离徙动目标进行感知的新型输入-输出模型，并揭示了距离徙动导致目标响应在延迟-多普勒域扩散的独特现象，这与以往研究有所不同。提出的两步最大似然估计器为在LEO卫星ISAC系统中面临的挑战性高速目标参数估计问题提供了一种实用的解决方案，具有重要的应用价值。"}}
{"id": "2507.02416", "title": "Determination Of Structural Cracks Using Deep Learning Frameworks", "authors": ["Subhasis Dasgupta", "Jaydip Sen", "Tuhina Halder"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This is the accepted version of the paper presented in IEEE CONIT 2025 held on 20th June 2025. This is not the camera-ready version. There are 6 pages in this paper and it contains 7 figures and 1 table", "url": "http://arxiv.org/abs/2507.02416v1", "summary": "Structural crack detection is a critical task for public safety as it helps\nin preventing potential structural failures that could endanger lives. Manual\ndetection by inexperienced personnel can be slow, inconsistent, and prone to\nhuman error, which may compromise the reliability of assessments. The current\nstudy addresses these challenges by introducing a novel deep-learning\narchitecture designed to enhance the accuracy and efficiency of structural\ncrack detection. In this research, various configurations of residual U-Net\nmodels were utilized. These models, due to their robustness in capturing fine\ndetails, were further integrated into an ensemble with a meta-model comprising\nconvolutional blocks. This unique combination aimed to boost prediction\nefficiency beyond what individual models could achieve. The ensemble's\nperformance was evaluated against well-established architectures such as SegNet\nand the traditional U-Net. Results demonstrated that the residual U-Net models\noutperformed their predecessors, particularly with low-resolution imagery, and\nthe ensemble model exceeded the performance of individual models, proving it as\nthe most effective. The assessment was based on the Intersection over Union\n(IoU) metric and DICE coefficient. The ensemble model achieved the highest\nscores, signifying superior accuracy. This advancement suggests way for more\nreliable automated systems in structural defects monitoring tasks.", "comment": "This is the accepted version of the paper presented in IEEE CONIT\n  2025 held on 20th June 2025. This is not the camera-ready version. There are\n  6 pages in this paper and it contains 7 figures and 1 table", "pdf_url": "http://arxiv.org/pdf/2507.02416v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用深度学习框架确定结构裂缝", "tldr": "本文提出了一种基于残差U-Net集成模型的深度学习新架构，用于提高结构裂缝检测的准确性和效率，实验证明该集成模型优于现有模型。", "motivation": "结构裂缝检测对于公共安全至关重要，但传统人工检测存在耗时、不一致和易出错等问题，影响评估可靠性。本研究旨在通过引入新的深度学习架构来解决这些挑战，提高检测的准确性和效率。", "method": "研究利用了多种配置的残差U-Net模型，并将其与包含卷积块的元模型集成为一个集成模型。该集成模型旨在提升预测效率。其性能通过IoU和DICE系数与SegNet和传统U-Net等成熟架构进行了评估。", "result": "结果表明，残差U-Net模型优于其前身，尤其是在低分辨率图像上。集成模型超越了单个模型的性能，被证明是最有效的。集成模型在IoU和DICE系数上取得了最高分，表明其卓越的准确性。", "conclusion": "这项进展为结构缺陷监测任务中更可靠的自动化系统提供了途径。", "translation": "结构裂缝检测是一项对公共安全至关重要的任务，因为它有助于防止可能危及生命的潜在结构故障。由缺乏经验的人员进行的手动检测可能速度慢、不一致且容易出现人为错误，这可能会损害评估的可靠性。本研究通过引入一种新颖的深度学习架构来解决这些挑战，旨在提高结构裂缝检测的准确性和效率。在这项研究中，利用了各种配置的残差U-Net模型。这些模型因其在捕获精细细节方面的鲁棒性，被进一步集成到一个包含卷积块的元模型的集成中。这种独特的组合旨在将预测效率提升到单个模型无法达到的水平。该集成的性能与SegNet和传统U-Net等成熟架构进行了评估。结果表明，残差U-Net模型优于其前身，特别是在低分辨率图像上，并且集成模型超越了单个模型的性能，证明其最有效。评估基于交并比（IoU）度量和DICE系数。集成模型取得了最高分，表明其卓越的准确性。这项进展为结构缺陷监测任务中更可靠的自动化系统提供了途径。", "summary": "本研究针对人工结构裂缝检测的局限性，提出了一种新颖的深度学习架构，旨在提高检测的准确性和效率。该架构利用多种配置的残差U-Net模型，并将其与一个元模型集成为一个集成系统。实验结果表明，该集成模型在低分辨率图像上表现出色，并在IoU和DICE系数上取得了最高分，显著优于传统的SegNet和U-Net模型，为自动化结构缺陷监测提供了更可靠的解决方案。", "keywords": "结构裂缝检测, 深度学习, 残差U-Net, 集成模型, 图像分割", "comments": "该论文的创新点在于提出了残差U-Net与元模型集成的深度学习架构，有效提升了结构裂缝检测的准确性和效率，尤其在处理低分辨率图像时表现突出。这对于提升公共安全领域的自动化检测水平具有重要意义，克服了传统人工检测的诸多弊端。"}}
{"id": "2507.02701", "title": "Faster Algorithm for Bounded Tree Edit Distance in the Low-Distance Regime", "authors": ["Tomasz Kociumaka", "Ali Shahali"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Accepted to ESA 2025", "url": "http://arxiv.org/abs/2507.02701v1", "summary": "The tree edit distance is a natural dissimilarity measure between rooted\nordered trees whose nodes are labeled over an alphabet $\\Sigma$. It is defined\nas the minimum number of node edits (insertions, deletions, and relabelings)\nrequired to transform one tree into the other. In the weighted variant, the\nedits have associated costs (depending on the involved node labels) normalized\nso that each cost is at least one, and the goal is to minimize the total cost\nof edits.\n  The unweighted tree edit distance between two trees of total size $n$ can be\ncomputed in $O(n^{2.6857})$ time; in contrast, determining the weighted tree\nedit distance is fine-grained equivalent to the All-Pairs Shortest Paths\nproblem and requires $n^3/2^{\\Omega(\\sqrt{\\log n})}$ time [Nogler et al.;\nSTOC'25]. These super-quadratic running times are unattractive for large but\nvery similar trees, which motivates the bounded version of the problem, where\nthe runtime is parameterized by the computed distance $k$, potentially yielding\nfaster algorithms for $k\\ll n$.\n  Previous best algorithms for the bounded unweighted setting run in\n$O(nk^2\\log n)$ time [Akmal & Jin; ICALP'21] and $O(n + k^7\\log k)$ time [Das\net al.; STOC'23]. For the weighted variant, the only known running time has\nbeen $O(n + k^{15})$.\n  We present an $O(n + k^6\\log k)$-time algorithm for computing the bounded\ntree edit distance in both the weighted and unweighted settings. Our approach\nbegins with an alternative $O(nk^2\\log n)$-time algorithm that handles weights\nand is significantly easier to analyze than the existing counterpart. We then\nintroduce a novel optimization that leverages periodic structures within the\ninput trees. To utilize it, we modify the $O(k^5)$-size $O(n)$-time universal\nkernel, the central component of the prior $O(n + k^{O(1)})$-time algorithms,\nso that it produces instances containing these periodic structures.", "comment": "Accepted to ESA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02701v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "低距离状态下有界树编辑距离的更快算法", "tldr": "本文提出了一种计算加权和无加权有界树编辑距离的更快算法，其时间复杂度为 $O(n + k^6\\log k)$，显著优于现有算法。", "motivation": "对于大型但非常相似的树，现有无界树编辑距离算法的超二次运行时间（无加权 $O(n^{2.6857})$，加权 $n^3/2^{\\Omega(\\sqrt{\\log n})}$）效率低下。这促使了有界版本问题的研究，旨在当距离 $k$ 远小于树的总大小 $n$ 时，能有更快的算法。", "method": "作者提出了一种时间复杂度为 $O(n + k^6\\log k)$ 的算法。该方法首先提供了一个替代的 $O(nk^2\\log n)$ 时间算法，该算法能够处理权重且比现有算法更容易分析。然后，引入了一种利用输入树中周期结构的新颖优化方法。为了利用这一点，作者修改了先前算法中关键的 $O(k^5)$ 大小、$O(n)$ 时间的通用核，使其能够生成包含这些周期结构的实例。", "result": "本文提出了一个用于计算加权和无加权有界树编辑距离的 $O(n + k^6\\log k)$ 时间算法。这比现有加权有界设置的最佳算法 $O(n + k^{15})$ 有显著改进，也比无加权设置的 $O(nk^2\\log n)$ 和 $O(n + k^7\\log k)$ 算法更快。", "conclusion": "本文成功开发了一种在低距离状态下计算有界树编辑距离的更高效算法，显著降低了加权和无加权场景下的时间复杂度，使其更适用于处理大型但相似的树。", "translation": "树编辑距离是衡量根有序树之间自然差异的度量，这些树的节点标记在一个字母表 $\\Sigma$ 上。它被定义为将一棵树转换为另一棵树所需的最小节点编辑（插入、删除和重新标记）次数。在加权变体中，编辑具有相关的成本（取决于所涉及的节点标签），并进行归一化，使得每个成本至少为一，目标是最小化编辑的总成本。 两个总大小为 $n$ 的树之间的无加权树编辑距离可以在 $O(n^{2.6857})$ 时间内计算；相比之下，确定加权树编辑距离与全对最短路径问题是细粒度等价的，需要 $n^3/2^{\\Omega(\\sqrt{\\log n})}$ 时间 [Nogler 等人；STOC'25]。这些超二次运行时间对于大型但非常相似的树来说是不具吸引力的，这促使了问题的有界版本，其中运行时间由计算出的距离 $k$ 参数化，可能为 $k\\ll n$ 产生更快的算法。 先前有界无加权设置的最佳算法运行时间为 $O(nk^2\\log n)$ [Akmal & Jin；ICALP'21] 和 $O(n + k^7\\log k)$ [Das 等人；STOC'23]。对于加权变体，唯一已知的运行时间是 $O(n + k^{15})$。 我们提出了一种用于计算加权和无加权设置中有界树编辑距离的 $O(n + k^6\\log k)$ 时间算法。我们的方法始于一个替代的 $O(nk^2\\log n)$ 时间算法，该算法能够处理权重，并且比现有对应算法更容易分析。然后，我们引入了一种利用输入树中周期结构的新颖优化。为了利用它，我们修改了 $O(k^5)$ 大小、$O(n)$ 时间的通用核，这是先前 $O(n + k^{O(1)})$ 时间算法的核心组件，使其能够生成包含这些周期结构的实例。", "summary": "本文针对大型但相似树的树编辑距离计算效率问题，提出了一种更快的有界树编辑距离算法。该算法在加权和无加权设置下均实现了 $O(n + k^6\\log k)$ 的时间复杂度，显著优于现有算法。其创新点在于引入了一种利用输入树周期结构的新颖优化方法，并通过修改通用核来应用该优化。", "keywords": "树编辑距离, 有界算法, 加权, 无加权, 周期结构", "comments": "该论文的创新之处在于其提出的新算法显著降低了有界树编辑距离的计算复杂度，尤其是在加权设置下，将时间复杂度从 $O(n + k^{15})$ 降低到 $O(n + k^6\\log k)$。这种改进对于处理大规模且相似的树数据具有重要意义。此外，利用周期结构进行优化和对通用核的修改是其方法学的亮点。"}}
{"id": "2507.02791", "title": "Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient Extraction of Moving Speakers under Weak Guidance", "authors": ["Jakob Kienegger", "Alina Mannanova", "Huajian Fang", "Timo Gerkmann"], "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025", "url": "http://arxiv.org/abs/2507.02791v1", "summary": "Recent works on deep non-linear spatially selective filters demonstrate\nexceptional enhancement performance with computationally lightweight\narchitectures for stationary speakers of known directions. However, to maintain\nthis performance in dynamic scenarios, resource-intensive data-driven tracking\nalgorithms become necessary to provide precise spatial guidance conditioned on\nthe initial direction of a target speaker. As this additional computational\noverhead hinders application in resource-constrained scenarios such as\nreal-time speech enhancement, we present a novel strategy utilizing a\nlow-complexity tracking algorithm in the form of a particle filter instead.\nAssuming a causal, sequential processing style, we introduce temporal feedback\nto leverage the enhanced speech signal of the spatially selective filter to\ncompensate for the limited modeling capabilities of the particle filter.\nEvaluation on a synthetic dataset illustrates how the autoregressive interplay\nbetween both algorithms drastically improves tracking accuracy and leads to\nstrong enhancement performance. A listening test with real-world recordings\ncomplements these findings by indicating a clear trend towards our proposed\nself-steering pipeline as preferred choice over comparable methods.", "comment": "Accepted at IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "pdf_url": "http://arxiv.org/pdf/2507.02791v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "自适应深度非线性空间选择滤波器用于弱引导下移动说话人高效提取", "tldr": "本文提出了一种利用低复杂度粒子滤波器和时间反馈的自适应深度非线性空间选择滤波器，旨在高效提取移动说话人的语音，同时降低计算开销，并在合成数据集和真实录音上表现出显著的性能提升。", "motivation": "现有的深度非线性空间选择滤波器在固定方向的静止说话人语音增强方面表现出色且计算量小。然而，在动态场景中，为了保持这种性能，需要资源密集型的数据驱动跟踪算法提供精确的空间引导，这增加了计算开销，阻碍了其在实时语音增强等资源受限场景中的应用。", "method": "我们提出了一种新颖的策略，利用低复杂度的粒子滤波器作为跟踪算法。假设采用因果、顺序处理方式，引入时间反馈，利用空间选择滤波器增强的语音信号来弥补粒子滤波器有限的建模能力，实现算法间的自回归相互作用。", "result": "在合成数据集上的评估表明，两种算法之间的自回归相互作用显著提高了跟踪精度，并带来了强大的增强性能。对真实录音进行的听力测试补充了这些发现，表明我们提出的自适应管道比同类方法更受青睐。", "conclusion": "本文提出的自适应深度非线性空间选择滤波器通过结合低复杂度粒子滤波器和时间反馈，能够有效、高效地处理移动说话人的语音增强任务，并在性能上优于现有可比较的方法，尤其适用于资源受限的实时应用。", "translation": "关于深度非线性空间选择滤波器的最新研究表明，对于已知方向的静止说话人，它们能够以计算量轻的架构实现卓越的增强性能。然而，为了在动态场景中保持这种性能，需要资源密集型的数据驱动跟踪算法来提供以目标说话人初始方向为条件的精确空间引导。由于这种额外的计算开销阻碍了在实时语音增强等资源受限场景中的应用，我们提出了一种利用低复杂度粒子滤波器作为替代的新颖策略。假设采用因果、顺序处理方式，我们引入时间反馈，利用空间选择滤波器增强的语音信号来弥补粒子滤波器有限的建模能力。对合成数据集的评估表明，两种算法之间的自回归相互作用如何显著提高跟踪精度并带来强大的增强性能。对真实录音进行的听力测试补充了这些发现，表明我们提出的自适应管道作为优于同类方法的首选，表现出明显的趋势。", "summary": "本文提出了一种“自适应”深度非线性空间选择滤波器，旨在解决现有方法在动态场景下处理移动说话人时计算开销大的问题。通过引入低复杂度的粒子滤波器作为跟踪机制，并利用时间反馈将增强后的语音信号回馈给粒子滤波器，以补偿其建模能力的局限性。这种自回归的算法间协同作用在合成数据集上显著提高了跟踪精度和语音增强性能。真实录音的听力测试也证实了该自适应方案优于现有可比较的方法，使其成为资源受限环境下高效提取移动说话人的理想选择。", "keywords": "深度非线性滤波器, 移动说话人, 粒子滤波器, 语音增强, 自适应", "comments": "本文的创新点在于将低复杂度的粒子滤波器与深度非线性空间选择滤波器相结合，并通过引入时间反馈实现了“自适应”能力。这种设计有效地解决了动态场景下移动说话人语音增强的计算开销问题，使其适用于资源受限的实时应用。其在跟踪精度和增强性能上的提升，以及在真实世界录音中的优越表现，都凸显了该方法的实用性和重要性。"}}
{"id": "2507.02282", "title": "Content filtering methods for music recommendation: A review", "authors": ["Terence Zeng", "Abhishek K. Umrawal"], "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      13 pages and 9 figures", "url": "http://arxiv.org/abs/2507.02282v1", "summary": "Recommendation systems have become essential in modern music streaming\nplatforms, shaping how users discover and engage with songs. One common\napproach in recommendation systems is collaborative filtering, which suggests\ncontent based on the preferences of users with similar listening patterns to\nthe target user. However, this method is less effective on media where\ninteractions are sparse. Music is one such medium, since the average user of a\nmusic streaming service will never listen to the vast majority of tracks. Due\nto this sparsity, there are several challenges that have to be addressed with\nother methods. This review examines the current state of research in addressing\nthese challenges, with an emphasis on the role of content filtering in\nmitigating biases inherent in collaborative filtering approaches. We explore\nvarious methods of song classification for content filtering, including lyrical\nanalysis using Large Language Models (LLMs) and audio signal processing\ntechniques. Additionally, we discuss the potential conflicts between these\ndifferent analysis methods and propose avenues for resolving such\ndiscrepancies.", "comment": "13 pages and 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.02282v1", "cate": "cs.IR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "音乐推荐中的内容过滤方法：综述", "tldr": "这篇综述探讨了在音乐推荐系统中，如何利用内容过滤方法（包括歌词分析和音频信号处理）来解决协同过滤因用户交互稀疏性而面临的挑战，并讨论了不同分析方法间的潜在冲突及解决方案。", "motivation": "在音乐流媒体平台中，推荐系统至关重要。然而，传统的协同过滤方法在音乐这种用户交互稀疏的介质上效果不佳，因为普通用户不会听绝大多数歌曲。这种稀疏性带来了挑战，需要其他方法来解决。", "method": "这篇综述考察了当前解决这些挑战的研究现状，重点关注内容过滤在缓解协同过滤固有偏见方面的作用。它探讨了各种歌曲分类的内容过滤方法，包括使用大型语言模型（LLMs）进行歌词分析和音频信号处理技术。此外，它还讨论了这些不同分析方法之间的潜在冲突，并提出了解决这些差异的途径。", "result": "作为一篇综述，本文没有实验结果。它通过检查现有研究，探讨了内容过滤在音乐推荐中解决协同过滤稀疏性问题的多种方法，包括歌词分析和音频信号处理，并提出了解决不同分析方法间冲突的建议。", "conclusion": "这篇综述总结了音乐推荐中内容过滤方法的当前研究状况，强调了其在克服协同过滤局限性方面的作用，并提出了如何整合不同内容分析技术以提高推荐效果的思路。", "translation": "推荐系统已成为现代音乐流媒体平台不可或缺的一部分，塑造着用户发现和参与歌曲的方式。推荐系统中的一种常见方法是协同过滤，它根据与目标用户有相似听歌模式的用户偏好来推荐内容。然而，这种方法在交互稀疏的媒体上效果较差。音乐就是这样一种媒体，因为音乐流媒体服务的普通用户绝不会听绝大多数歌曲。由于这种稀疏性，存在一些必须通过其他方法解决的挑战。这篇综述考察了当前解决这些挑战的研究现状，重点强调了内容过滤在缓解协同过滤方法固有偏见方面的作用。我们探讨了用于内容过滤的各种歌曲分类方法，包括使用大型语言模型（LLMs）进行歌词分析和音频信号处理技术。此外，我们还讨论了这些不同分析方法之间的潜在冲突，并提出了解决此类差异的途径。", "summary": "本文综述了音乐推荐系统中的内容过滤方法，以解决协同过滤在音乐这种交互稀疏介质上的局限性。它探讨了利用大型语言模型进行歌词分析和音频信号处理等多种歌曲分类技术，并讨论了不同分析方法间的潜在冲突及协调策略。", "keywords": "音乐推荐, 内容过滤, 协同过滤, 歌词分析, 音频信号处理", "comments": "这篇综述的重要性在于它系统地梳理了音乐推荐领域中内容过滤方法的最新进展，特别是在解决协同过滤面临的稀疏性问题和偏见方面。它涵盖了新兴技术如LLMs在歌词分析中的应用，以及传统的音频信号处理，并提出了整合不同技术以克服冲突的思考，为未来的研究指明了方向。"}}
{"id": "2507.02510", "title": "TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification", "authors": ["Ahmed G. Habashi", "Ahmed M. Azab", "Seif Eldawlatly", "Gamal M. Aly"], "categories": ["cs.LG", "cs.HC", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02510v1", "summary": "Cross-subject motor imagery (CS-MI) classification in brain-computer\ninterfaces (BCIs) is a challenging task due to the significant variability in\nElectroencephalography (EEG) patterns across different individuals. This\nvariability often results in lower classification accuracy compared to\nsubject-specific models, presenting a major barrier to developing\ncalibration-free BCIs suitable for real-world applications. In this paper, we\nintroduce a novel approach that significantly enhances cross-subject MI\nclassification performance through optimized preprocessing and deep learning\ntechniques. Our approach involves direct classification of Short-Time Fourier\nTransform (STFT)-transformed EEG data, optimized STFT parameters, and a\nbalanced batching strategy during training of a Convolutional Neural Network\n(CNN). This approach is uniquely validated across four different datasets,\nincluding three widely-used benchmark datasets leading to substantial\nimprovements in cross-subject classification, achieving 67.60% on the BCI\nCompetition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on\nDataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we\nsystematically investigate the classification performance using MI windows\nranging from the full 4-second window to 1-second windows. These results\nestablish a new benchmark for generalizable, calibration-free MI classification\nin addition to contributing a robust open-access dataset to advance research in\nthis domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02510v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "TFOC-Net：一种基于短时傅里叶变换的深度学习方法，用于增强跨受试者运动想象分类", "tldr": "TFOC-Net利用优化的短时傅里叶变换和深度学习显著提高了跨受试者运动想象分类的准确性，并在多个基准数据集上超越了现有技术。", "motivation": "脑机接口(BCIs)中的跨受试者运动想象(CS-MI)分类由于不同个体间脑电图(EEG)模式的显著变异性而具有挑战性，导致分类精度低于受试者特异性模型，这阻碍了免校准BCI在实际应用中的发展。", "method": "本研究提出了一种新方法，通过优化的预处理和深度学习技术增强跨受试者MI分类性能。该方法包括直接对短时傅里叶变换(STFT)后的EEG数据进行分类，优化STFT参数，并在卷积神经网络(CNN)训练期间采用平衡批处理策略。", "result": "该方法在四个不同数据集（包括三个广泛使用的基准数据集）上得到了验证，显著改善了跨受试者分类，在BCI竞赛IV数据集1 (IV-1)上达到67.60%，在数据集2A (IV-2A)上达到65.96%，在数据集2B (IV-2B)上达到80.22%，超越了现有技术。此外，研究还系统地调查了使用从4秒到1秒的MI窗口的分类性能。", "conclusion": "这些结果为可泛化、免校准的MI分类建立了新的基准，并贡献了一个鲁棒的开放获取数据集以推进该领域的研究。", "translation": "脑机接口（BCIs）中的跨受试者运动想象（CS-MI）分类是一项具有挑战性的任务，因为不同个体间脑电图（EEG）模式存在显著变异性。这种变异性通常导致分类精度低于受试者特异性模型，这成为开发适用于实际应用的免校准BCI的主要障碍。在本文中，我们引入了一种新颖的方法，通过优化的预处理和深度学习技术显著增强了跨受试者MI分类性能。我们的方法包括直接对短时傅里叶变换（STFT）后的EEG数据进行分类，优化STFT参数，并在卷积神经网络（CNN）训练期间采用平衡批处理策略。该方法在四个不同数据集上得到了独特的验证，其中包括三个广泛使用的基准数据集，从而在跨受试者分类方面取得了实质性改进，在BCI竞赛IV数据集1（IV-1）上达到67.60%，在数据集2A（IV-2A）上达到65.96%，在数据集2B（IV-2B）上达到80.22%，超越了现有技术。此外，我们系统地研究了使用从完整4秒窗口到1秒窗口的MI窗口的分类性能。这些结果为可泛化、免校准的MI分类建立了新的基准，同时贡献了一个鲁棒的开放获取数据集，以推进该领域的研究。", "summary": "TFOC-Net是一种基于短时傅里叶变换（STFT）和深度学习的新方法，旨在解决脑机接口中跨受试者运动想象（MI）分类的挑战。该方法通过优化STFT参数和在CNN训练中采用平衡批处理策略，直接对STFT转换后的EEG数据进行分类。TFOC-Net在多个基准数据集上取得了显著的分类性能提升，超越了现有技术，并为可泛化、免校准的MI分类树立了新基准，同时提供了一个新的开放获取数据集。", "keywords": "运动想象分类, 脑机接口, 短时傅里叶变换, 深度学习, 跨受试者", "comments": "该论文通过将短时傅里叶变换与深度学习相结合，为跨受试者运动想象分类提供了一种创新的解决方案。其在多个基准数据集上超越现有技术的结果表明了该方法的有效性和泛化能力。此外，贡献开放获取数据集有助于推动该领域未来的研究。这项工作对于实现免校准的脑机接口具有重要意义。"}}
{"id": "2507.02357", "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "url": "http://arxiv.org/abs/2507.02357v1", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.02357v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Coling-UniA 在 SciVQA 2025：多模态大型语言模型的少样本示例检索和置信度感知集成", "tldr": "针对 SciVQA 2025 任务，本文提出了一个结合少样本检索和置信度感知的多模态大语言模型集成系统，并在盲测中排名第三。", "motivation": "本文描述了作者团队为 SciVQA 2025 科学视觉问答共享任务开发的系统。", "method": "系统采用两个多模态大型语言模型的集成，并结合了多种少样本示例检索策略。模型和少样本设置根据图表和问题类型选择。答案选择还基于模型的置信度水平。", "result": "在盲测数据上，该系统在七个参赛者中排名第三，ROUGE-1、ROUGE-L 和 BERTS 的平均 F1 分数为 85.12。", "conclusion": "该系统在科学视觉问答任务中表现出色，通过集成多模态大语言模型、少样本检索和置信度机制，取得了良好的竞争性结果。", "translation": "本文描述了我们为 SciVQA 2025 科学视觉问答共享任务开发的系统。我们的系统采用了两个多模态大型语言模型的集成以及各种少样本示例检索策略。模型和少样本设置根据图表和问题类型进行选择。我们还根据模型的置信度水平选择答案。在盲测数据上，我们的系统在七个参赛者中排名第三，ROUGE-1、ROUGE-L 和 BERTS 的平均 F1 分数为 85.12。我们的代码已公开可用。", "summary": "本文介绍了 Coling-UniA 团队为 SciVQA 2025 科学视觉问答共享任务设计的系统。该系统整合了两个多模态大型语言模型，并利用了多种少样本示例检索策略，其模型和检索设置会根据图表和问题类型进行调整。此外，系统根据模型的置信度来选择最终答案。在盲测中，该系统在七个参赛者中位列第三，平均 F1 分数达到 85.12。", "keywords": "多模态大语言模型, 视觉问答, 少样本学习, 模型集成, 置信度感知", "comments": "该论文展示了一个在特定竞赛（SciVQA 2025）中表现良好的系统，其创新点在于结合了多模态大语言模型集成、根据内容动态选择少样本策略以及基于置信度的答案选择。虽然具体技术细节在摘要中未完全展开，但其在竞争性任务中的表现（第三名）证明了其方法的有效性。"}}
{"id": "2507.02666", "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning", "authors": ["Junyu Wang", "Tianrui Wang", "Meng Ge", "Longbiao Wang", "Jianwu Dang"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at Interspeech2025", "url": "http://arxiv.org/abs/2507.02666v1", "summary": "In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications.", "comment": "Accepted at Interspeech2025", "pdf_url": "http://arxiv.org/pdf/2507.02666v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ASDA：用于自监督表示学习的音频频谱图差分注意力机制", "tldr": "ASDA引入了一种差分注意力机制，通过双softmax操作和差分系数来解决标准Transformer在音频自监督学习中注意力分配不当的问题，并在多个音频任务中取得了最先进的性能。", "motivation": "标准Transformer架构在音频自监督表示学习中，其注意力机制常将部分注意力权重分配给不相关信息，从而可能损害模型的判别能力。", "method": "引入了一种差分注意力机制，通过整合双softmax操作和适当调整的差分系数来有效缓解无效的注意力分配。", "result": "ASDA模型在多个基准测试中取得了最先进的性能，包括音频分类（AS-2M上49.0% mAP，AS20K上41.5% mAP）、关键词识别（SPC-2上98.3%准确率）和环境音分类（ESC-50上96.1%准确率）。", "conclusion": "实验结果突出了ASDA在音频任务中的有效性，为更广泛的应用铺平了道路。", "translation": "在音频自监督表示学习的最新进展中，标准Transformer架构已成为主流方法，但其注意力机制常常将部分注意力权重分配给不相关信息，可能损害模型的判别能力。为了解决这个问题，我们引入了一种差分注意力机制，通过整合双softmax操作和适当调整的差分系数，有效缓解了无效的注意力分配。实验结果表明，我们的ASDA模型在多个基准测试中取得了最先进的性能，包括音频分类（AS-2M上49.0% mAP，AS20K上41.5% mAP）、关键词识别（SPC-2上98.3%准确率）和环境音分类（ESC-50上96.1%准确率）。这些结果突出了ASDA在音频任务中的有效性，为更广泛的应用铺平了道路。", "summary": "ASDA提出了一种针对音频自监督表示学习的差分注意力机制，旨在解决传统Transformer注意力分配不当的问题。通过引入双softmax操作和调整差分系数，ASDA有效提高了模型的判别能力。实验证明，该模型在音频分类、关键词识别和环境音分类等多个音频基准测试中均达到了最先进的性能，显示出其在音频任务中的强大有效性和广泛应用潜力。", "keywords": "音频自监督学习, 差分注意力机制, Transformer, 音频分类, 关键词识别", "comments": "ASDA的创新点在于其提出的差分注意力机制，通过双softmax操作和差分系数，有效解决了传统Transformer在处理音频数据时注意力分配冗余的问题。这种机制显著提升了模型在自监督学习中的判别能力，并在多个音频任务中实现了SOTA性能，这对于音频领域的表示学习具有重要意义。"}}
{"id": "2507.02478", "title": "Effectively Identifying Wi-Fi Devices through State Transitions", "authors": ["Melissa Safari", "Abhishek K. Mishra", "Mathieu Cunche"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02478v1", "summary": "Wi-Fi management frames reveal structured communication patterns that persist\neven under randomization of MAC addresses. Prior approaches to associating\nrandomized MAC addresses with devices primarily focus on probe requests,\noverlooking the broader set of management frames and their transition dynamics.\nThis narrow focus limits their robustness in dense, real-world environments\nwith high device mobility, where probe activity alone fails to yield stable and\ndistinctive signatures. In this paper, we present a novel framework for\nfingerprinting Wi-Fi devices based on behavioral dynamics extracted from\npassively observed management frames. We model each device's behavior as a\nfinite state machine and introduce matrix-based representations that encode\nboth structural (state transition frequencies) and temporal (inter-state\ndelays) characteristics. These matrices are embedded into compact feature\nvectors, enabling efficient similarity comparison. Through extensive evaluation\nin diverse real-world settings, our method achieves over 86% identification\naccuracy for non-randomized devices using only Wi-Fi management frames, with\nfurther improvements observed through temporal burst aggregation. Our findings\nare sufficient to uniquely and consistently characterize devices at scale,\noutperforming the state-of-the-art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02478v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过状态转换有效识别Wi-Fi设备", "tldr": "新方法通过分析Wi-Fi管理帧的状态转换来指纹识别设备，即使MAC地址随机化也能高精度识别，且优于现有技术。", "motivation": "现有方法主要侧重于探测请求，忽略了更广泛的管理帧及其转换动态，导致在设备移动性高、环境密集的真实世界中鲁健性差，无法提供稳定独特的签名。", "method": "本文提出一种基于从被动观察到的管理帧中提取的行为动态来指纹识别Wi-Fi设备的新颖框架。将每个设备的行为建模为有限状态机，并引入基于矩阵的表示，编码结构（状态转换频率）和时间（状态间延迟）特征。这些矩阵被嵌入到紧凑的特征向量中，从而实现高效的相似性比较。", "result": "在多样化的真实世界环境中，该方法对非随机化设备仅使用Wi-Fi管理帧实现了超过86%的识别准确率，通过时间爆发聚合观察到进一步的改进。研究结果足以大规模地独特且一致地表征设备，并优于现有技术。", "conclusion": "通过分析Wi-Fi管理帧的状态转换，本文提出的方法能够在大规模环境下独特且一致地识别Wi-Fi设备，性能优于现有技术。", "translation": "Wi-Fi管理帧揭示了即使在MAC地址随机化下也持续存在的结构化通信模式。以前将随机MAC地址与设备关联的方法主要侧重于探测请求，忽略了更广泛的管理帧及其转换动态。这种狭隘的焦点限制了它们在设备移动性高、环境密集的真实世界中的鲁棒性，在这些环境中，仅靠探测活动无法产生稳定和独特的签名。在本文中，我们提出了一种基于从被动观察到的管理帧中提取的行为动态来指纹识别Wi-Fi设备的新颖框架。我们将每个设备的行为建模为有限状态机，并引入基于矩阵的表示，编码结构（状态转换频率）和时间（状态间延迟）特征。这些矩阵被嵌入到紧凑的特征向量中，从而实现高效的相似性比较。通过在多样化的真实世界环境中进行广泛评估，我们的方法仅使用Wi-Fi管理帧，对非随机化设备的识别准确率超过86%，通过时间爆发聚合观察到进一步的改进。我们的发现足以大规模地独特且一致地表征设备，并且优于现有技术。", "summary": "本文提出一种通过分析Wi-Fi管理帧的行为动态来指纹识别Wi-Fi设备的新框架。该方法将设备行为建模为有限状态机，并使用矩阵表示编码状态转换频率和状态间延迟，生成紧凑的特征向量进行高效比较。实验证明，该方法对非随机化设备识别准确率超过86%，且性能优于现有技术，解决了现有方法在密集、高移动性环境中鲁棒性不足的问题。", "keywords": "Wi-Fi设备识别, 状态转换, 管理帧, 设备指纹, 有限状态机", "comments": "本文的创新点在于突破了传统MAC地址随机化对抗指纹识别的局限，通过分析Wi-Fi管理帧的复杂状态转换模式来识别设备。其将设备行为建模为有限状态机并引入结构和时间特征的矩阵表示，提供了一种新颖且有效的设备识别方法。在真实世界环境中的高识别准确率证明了其重要性，尤其是在物联网设备日益增多和隐私保护日益重要的背景下，为设备追踪和网络安全提供了新的视角。"}}
{"id": "2507.02328", "title": "Path Planning using a One-shot-sampling Skeleton Map", "authors": ["Gabriel O. Flores-Aquino", "Octavio Gutierrez-Frias", "Juan Irving Vasquez"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02328v1", "summary": "Path planning algorithms aim to compute a collision-free path, and many works\nfocus on finding the optimal distance path. However, for some applications, a\nmore suitable approach is to balance response time, safety of the paths, and\npath length. In this context, a skeleton map is a useful tool in graph-based\nschemes, as it provides an intrinsic representation of free configuration\nspace. However, skeletonization algorithms are very resource-intensive, being\nprimarily oriented towards image processing tasks. We propose an efficient\npath-planning methodology that finds safe paths within an acceptable processing\ntime. This methodology leverages a Deep Denoising Auto-Encoder (DDAE) based on\nU-Net architecture to compute a skeletonized version of the navigation map,\nwhich we refer to as SkelUnet. The SkelUnet network facilitates exploration of\nthe entire workspace through one-shot sampling (OSS), as opposed to the\niterative process used by exact algorithms or the probabilistic sampling\nprocess. SkelUnet is trained and tested on a dataset consisting of 12,500\nbi-dimensional dungeon maps. The motion planning methodology is evaluated in a\nsimulation environment for an Unmanned Aerial Vehicle (UAV) using 250\npreviously unseen maps, and assessed with various navigation metrics to\nquantify the navigability of the computed paths. The results demonstrate that\nusing SkelUnet to construct a roadmap offers significant advantages, such as\nconnecting all regions of free workspace, providing safer paths, and reducing\nprocessing times. These characteristics make this method particularly suitable\nfor mobile service robots in structured environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02328v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用一次性采样骨架图的路径规划", "tldr": "本文提出了一种基于U-Net的深度去噪自编码器（SkelUnet），用于高效生成骨架图并实现一次性采样的路径规划，从而平衡响应时间、路径安全性及长度。", "motivation": "现有的路径规划算法侧重于找到最优距离路径，但某些应用需要平衡响应时间、路径安全性和路径长度。同时，骨架化算法资源密集且主要面向图像处理任务，导致效率低下。", "method": "提出一种高效的路径规划方法，利用基于U-Net架构的深度去噪自编码器（DDAE）计算导航地图的骨架化版本，称之为SkelUnet。SkelUnet通过一次性采样（OSS）而非迭代或概率采样来探索整个工作空间。该方法在包含12,500张二维地牢地图的数据集上进行训练和测试，并在无人机仿真环境中对250张未见地图进行评估。", "result": "使用SkelUnet构建路线图具有显著优势，包括连接自由工作空间的所有区域、提供更安全的路径以及减少处理时间。", "conclusion": "该方法因其连接所有自由工作区域、提供更安全路径和减少处理时间的特点，特别适用于结构化环境中的移动服务机器人。", "translation": "路径规划算法旨在计算无碰撞路径，许多工作都集中在寻找最优距离路径。然而，对于某些应用，更合适的方法是平衡响应时间、路径安全性和路径长度。在这种背景下，骨架图是基于图方案中的有用工具，因为它提供了自由配置空间的内在表示。然而，骨架化算法资源消耗巨大，主要面向图像处理任务。我们提出一种高效的路径规划方法，能够在可接受的处理时间内找到安全路径。该方法利用基于U-Net架构的深度去噪自编码器（DDAE）来计算导航地图的骨架化版本，我们称之为SkelUnet。SkelUnet网络通过一次性采样（OSS）促进对整个工作空间的探索，这与精确算法使用的迭代过程或概率采样过程不同。SkelUnet在包含12,500张二维地牢地图的数据集上进行训练和测试。该运动规划方法在无人机（UAV）的仿真环境中，使用250张以前未见的地图进行评估，并通过各种导航指标量化计算路径的导航能力。结果表明，使用SkelUnet构建路线图具有显著优势，例如连接自由工作空间的所有区域、提供更安全的路径以及减少处理时间。这些特性使得该方法特别适用于结构化环境中的移动服务机器人。", "summary": "本文提出了一种名为SkelUnet的高效路径规划方法，该方法利用基于U-Net架构的深度去噪自编码器来生成导航地图的骨架化版本。SkelUnet通过一次性采样实现对整个工作空间的快速探索，解决了传统骨架化算法资源密集的问题，并平衡了路径规划中的响应时间、安全性和路径长度。实验结果表明，SkelUnet能够连接自由工作空间的所有区域，提供更安全的路径，并显著减少处理时间，使其适用于结构化环境中的移动服务机器人。", "keywords": "路径规划, 骨架图, 深度去噪自编码器, U-Net, 一次性采样", "comments": "本文的创新点在于将深度学习（DDAE和U-Net）引入到路径规划的骨架图生成中，实现了高效且一次性采样的骨架化过程。这解决了传统骨架化算法计算资源消耗大的问题，并为移动机器人提供了更安全、更快速的路径规划方案。其优势在于平衡了路径长度、安全性和响应时间，对于实时性要求高的应用具有重要意义。"}}
{"id": "2507.02254", "title": "A framework for 3D interaction techniques", "authors": ["Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02254v1", "summary": "This paper presents a software architecture for 3D interaction techniques\n(ITs) and an object oriented, toolkit-independent framework that implements\nsuch architecture. ITs are composed of basic filters connected in a dataflow,\nwhere virtual input devices and objects in the scene are sources of\ninformation. An execution model defines the general flow of information between\nfilters. This framework has been designed to be extensible: new information\ntypes, new input devices, new execution models, or new interaction techniques\ncan easily be added. Application specific code and application specific ITs are\nseamlessly integrated into this architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02254v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "3D交互技术框架", "tldr": "本文提出了一个用于3D交互技术的、可扩展的、面向对象的软件架构和框架。", "motivation": "旨在提供一个通用且可扩展的软件架构和框架，以实现和管理3D交互技术。", "method": "提出了一个软件架构和一个面向对象、独立于工具包的框架。该框架将交互技术（ITs）视为由基本过滤器在数据流中连接组成，其中虚拟输入设备和场景对象是信息源。一个执行模型定义了过滤器之间信息的通用流。", "result": "提出了一个可扩展的框架，能够轻松添加新的信息类型、输入设备、执行模型和交互技术。该架构能无缝集成特定于应用程序的代码和交互技术。", "conclusion": "本文成功提出了一个可扩展且灵活的3D交互技术软件架构和框架。", "translation": "本文提出了一种用于3D交互技术（ITs）的软件架构，以及一个实现该架构的面向对象、独立于工具包的框架。交互技术由数据流中连接的基本过滤器组成，其中虚拟输入设备和场景中的对象是信息来源。一个执行模型定义了过滤器之间信息的通用流。该框架被设计为可扩展的：可以轻松添加新的信息类型、新的输入设备、新的执行模型或新的交互技术。特定于应用程序的代码和特定于应用程序的交互技术无缝集成到该架构中。", "summary": "本文介绍了一个针对3D交互技术的软件架构和一个面向对象、独立于工具包的框架。该框架将交互技术视为通过数据流连接的过滤器集合，并定义了信息流的执行模型。其核心设计目标是高度可扩展性，允许轻松添加新的组件和技术，并能无缝集成应用特定的代码和交互技术。", "keywords": "3D交互, 软件架构, 交互技术, 可扩展性, 数据流", "comments": "该框架通过其模块化和可扩展性设计，为3D交互技术的开发提供了一个灵活的基础。其独立于工具包的特性增加了其通用性。"}}
{"id": "2507.02680", "title": "On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks", "authors": ["Jorge Baranda", "Marius Caus", "Luis Blanco", "Cristian J. Vaca-Rubio", "Engin Zeydan", "Kapal Dev", "Zheng Li", "Tomaso DeCola"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02680v1", "summary": "The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks\n(NTNs) poses unique architectural and functional challenges due to\nheterogeneous propagation conditions, dynamic topologies and limited on-board\nprocessing capabilities. This paper examines architectural and functional split\nstrategies that are consistent with O-RAN principles for future integrated\nTN-NTN systems. A taxonomy of split options is proposed that distributes RAN\nand core functions between satellites and ground nodes, and trade-offs in terms\nof performance, latency, autonomy and deployment are analysed. In particular,\nwe evaluate configurations ranging from pure on-board DU deployments to full\ngNB and UPF integration into satellites, including variations based on intra-\nand inter-satellite processing. In addition, the placement of Near-RT and\nNon-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible\nsplit strategies between space and ground to optimise the performance and\nscalability of the control loop. A comprehensive mapping between architectural\nsplits and RIC placement options is provided, emphasising implementation\nconstraints and interoperability considerations. The paper concludes by\nidentifying key challenges and outlining future directions to enable\nstandardised, modular and efficient TN-NTN convergence in the context of the\nO-RAN.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02680v1", "cate": "cs.NI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "综合O-RAN非地面网络中的架构划分与无线智能控制器部署", "tldr": "该论文探讨了在综合O-RAN非地面网络中，陆地网络与非地面网络集成的架构划分和无线智能控制器（RIC）部署策略，提出了划分选项的分类法，并分析了性能、延迟、自主性和部署方面的权衡。", "motivation": "陆地网络（TNs）与非地面网络（NTNs）的集成因异构传播条件、动态拓扑和有限的星载处理能力而面临独特的架构和功能挑战。本文旨在研究符合O-RAN原则的架构和功能划分策略，以应对未来综合TN-NTN系统中的这些挑战。", "method": "本文研究了符合O-RAN原则的未来综合TN-NTN系统的架构和功能划分策略。提出了一种划分选项的分类法，用于在卫星和地面节点之间分配RAN和核心功能，并分析了在性能、延迟、自主性和部署方面的权衡。具体评估了从纯星载DU部署到完整的gNB和UPF集成到卫星的配置，包括基于星内和星间处理的变体。此外，还讨论了近实时（Near-RT）和非实时（Non-RT）RAN智能控制器（RICs）的部署，提出了空间和地面之间灵活的划分策略，以优化控制环路的性能和可扩展性。提供了架构划分和RIC部署选项之间的全面映射，强调了实施约束和互操作性考虑。", "result": "论文提出了一种划分选项的分类法，用于在卫星和地面节点之间分配RAN和核心功能，并分析了性能、延迟、自主性和部署方面的权衡。评估了从纯星载DU部署到完整gNB和UPF集成到卫星的各种配置。讨论了近实时和非实时RAN智能控制器（RICs）的部署，并提出了优化控制环路性能和可扩展性的灵活空间-地面划分策略。提供了架构划分和RIC部署选项之间的全面映射，强调了实施约束和互操作性考虑。", "conclusion": "论文最后指出了关键挑战，并概述了在O-RAN背景下实现标准化、模块化和高效的TN-NTN融合的未来方向。", "translation": "陆地网络（TNs）与非地面网络（NTNs）的集成由于异构传播条件、动态拓扑和有限的星载处理能力，带来了独特的架构和功能挑战。本文研究了符合O-RAN原则的未来综合TN-NTN系统的架构和功能划分策略。提出了一种划分选项的分类法，用于在卫星和地面节点之间分配RAN和核心功能，并分析了在性能、延迟、自主性和部署方面的权衡。具体而言，我们评估了从纯星载DU部署到完整的gNB和UPF集成到卫星的配置，包括基于星内和星间处理的变体。此外，还讨论了近实时（Near-RT）和非实时（Non-RT）RAN智能控制器（RICs）的部署，提出了空间和地面之间灵活的划分策略，以优化控制环路的性能和可扩展性。提供了架构划分和RIC部署选项之间的全面映射，强调了实施约束和互操作性考虑。论文最后指出了关键挑战，并概述了在O-RAN背景下实现标准化、模块化和高效的TN-NTN融合的未来方向。", "summary": "本文基于O-RAN原则，深入探讨了陆地网络（TNs）与非地面网络（NTNs）集成中的架构和功能划分策略。论文提出了一种划分选项的分类法，用于在卫星和地面节点之间分配RAN和核心功能，并分析了性能、延迟、自主性和部署等方面的权衡。研究评估了从纯星载DU部署到完整的gNB和UPF集成到卫星的各种配置，并讨论了RAN智能控制器（RICs）的最佳部署，以提升控制环路的性能和可扩展性。文中还提供了架构划分与RIC部署选项之间的全面映射，同时考虑了实施约束和互操作性，最终指出了关键挑战并展望了O-RAN背景下TN-NTN融合的未来方向。", "keywords": "O-RAN, 非地面网络, 架构划分, RIC部署, TN-NTN集成", "comments": "该论文对O-RAN框架下陆地网络与非地面网络集成这一新兴且关键领域进行了及时且深入的分析。其对架构划分和无线智能控制器（RIC）部署的关注，以及对性能、延迟、自主性和部署等权衡的分析，为未来通信网络的复杂设计空间提供了宝贵的见解。提出的划分分类法和映射有助于理解和指导实际部署，尽管未来的研究可能需要进一步的实践验证。"}}
{"id": "2507.02732", "title": "Classification by Separating Hypersurfaces: An Entropic Approach", "authors": ["Argimiro Arratia", "Mahmoud El Daou", "Henryk Gzyl"], "categories": ["cs.LG", "cs.IT", "math.IT", "physics.data-an", "stat.ML", "90C05, 90C25, 90C47, 90C52, 68T01, 68T05, 68T07, 68T20, 68W01"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 10 tables, 4 figures", "url": "http://arxiv.org/abs/2507.02732v1", "summary": "We consider the following classification problem: Given a population of\nindividuals characterized by a set of attributes represented as a vector in\n${\\mathbb R}^N$, the goal is to find a hyperplane in ${\\mathbb R}^N$ that\nseparates two sets of points corresponding to two distinct classes. This\nproblem, with a history dating back to the perceptron model, remains central to\nmachine learning. In this paper we propose a novel approach by searching for a\nvector of parameters in a bounded $N$-dimensional hypercube centered at the\norigin and a positive vector in ${\\mathbb R}^M$, obtained through the\nminimization of an entropy-based function defined over the space of unknown\nvariables. The method extends to polynomial surfaces, allowing the separation\nof data points by more complex decision boundaries. This provides a robust\nalternative to traditional linear or quadratic optimization techniques, such as\nsupport vector machines and gradient descent. Numerical experiments demonstrate\nthe efficiency and versatility of the method in handling diverse classification\ntasks, including linear and non-linear separability.", "comment": "15 pages, 10 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02732v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过分离超曲面进行分类：一种熵方法", "tldr": "本文提出了一种基于熵函数的分类新方法，通过最小化熵函数来寻找参数向量，以分离数据点。该方法可扩展到多项式曲面，并被证明在处理线性和非线性可分性问题上高效且通用。", "motivation": "在机器学习中，找到一个超平面来分离两个不同类别的数据点是一个核心问题，其历史可以追溯到感知机模型。", "method": "本文提出了一种新颖的方法，通过在以原点为中心的有界N维超立方体中搜索参数向量，以及在${\\mathbb R}^M$中搜索一个正向量，这些向量是通过最小化定义在未知变量空间上的基于熵的函数获得的。该方法可扩展到多项式曲面，允许通过更复杂的决策边界分离数据点。", "result": "数值实验证明了该方法在处理包括线性和非线性可分性在内的各种分类任务中的效率和通用性。", "conclusion": "该方法为传统的线性或二次优化技术（如支持向量机和梯度下降）提供了一种鲁棒的替代方案。", "translation": "我们考虑以下分类问题：给定一个由在${\\mathbb R}^N$中表示为向量的一组属性表征的个体群体，目标是找到${\\mathbb R}^N$中的一个超平面来分离对应于两个不同类别的两组点。这个问题，其历史可以追溯到感知机模型，仍然是机器学习的核心。在本文中，我们提出了一种新颖的方法，通过在以原点为中心的有界N维超立方体中搜索参数向量，以及在${\\mathbb R}^M$中搜索一个正向量，这些向量是通过最小化定义在未知变量空间上的基于熵的函数获得的。该方法可扩展到多项式曲面，允许通过更复杂的决策边界分离数据点。这为传统的线性或二次优化技术（如支持向量机和梯度下降）提供了一种鲁棒的替代方案。数值实验证明了该方法在处理包括线性和非线性可分性在内的各种分类任务中的效率和通用性。", "summary": "本文针对机器学习中的核心分类问题，提出了一种新颖的基于熵的分类方法。该方法通过最小化一个熵函数，在有界超立方体中寻找参数向量，以实现数据点的分离。它不仅适用于超平面，还能扩展到多项式曲面，形成更复杂的决策边界。相较于支持向量机和梯度下降等传统优化技术，该方法被证明是一种更鲁棒的替代方案，并在数值实验中展现了在处理线性和非线性分类任务时的效率和通用性。", "keywords": "分类, 超曲面, 熵方法, 机器学习, 数据分离", "comments": "该论文提出了一种创新的、基于熵的分类方法，与传统的基于优化的方法（如SVM）不同。其创新点在于利用熵函数来寻找分离超曲面的参数，并能灵活地扩展到处理非线性可分数据，这为机器学习中的分类问题提供了一个有前景的替代方案。"}}
{"id": "2507.02443", "title": "Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic", "authors": ["Sandro Costa Magalhães", "Marco Almeida", "Filipe Neves dos Santos", "António Paulo Moreira", "Jorge Dias"], "categories": ["cs.CV", "cs.AI", "cs.DC", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to ROBOT'2025", "url": "http://arxiv.org/abs/2507.02443v1", "summary": "Robots usually slow down for canning to detect objects while moving.\nAdditionally, the robot's camera is configured with a low framerate to track\nthe velocity of the detection algorithms. This would be constrained while\nexecuting tasks and exploring, making robots increase the task execution time.\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\nis a self-acquired dataset released in open access. MobileNet v1 performed\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\nsuitable for attention mechanisms.", "comment": "Submitted to ROBOT'2025", "pdf_url": "http://arxiv.org/pdf/2507.02443v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "红葡萄检测，基于FPGA可编程逻辑中加速的人工神经网络", "tldr": "本文在FPGA的可编程逻辑中部署并加速了量化人工神经网络，以实现高速红葡萄检测，其中MobileNet v1表现最佳，达到98%的成功率和6611 FPS的推理速度。", "motivation": "机器人通常在移动中检测物体时会减速，并且为了跟踪检测算法的速度，摄像机帧率较低，这限制了任务执行和探索，增加了任务执行时间。AMD的Vitis-AI框架未能充分利用FPGA的可编程逻辑。", "method": "作者使用FINN架构在FPGA的可编程逻辑（PL）中部署了三种人工神经网络：4位量化的MobileNet v1、2位量化的CNV和1位量化的CNV（BNN）。模型在RG2C数据集上进行训练。", "result": "MobileNet v1表现最佳，成功率达到98%，推理速度达到6611 FPS。", "conclusion": "本文证明了可以使用FPGA加速人工神经网络，使其适用于注意力机制。", "translation": "机器人通常在移动中检测物体时会减速。此外，机器人的摄像机配置为低帧率以跟踪检测算法的速度。这在执行任务和探索时会受到限制，导致机器人增加任务执行时间。AMD开发了Vitis-AI框架，用于将检测算法部署到FPGA中。然而，该工具并未充分利用FPGA的可编程逻辑（PL）。在这项工作中，我们使用FINN架构在FPGA的PL内部署了三种人工神经网络：4位量化的MobileNet v1、2位量化的CNV和1位量化的CNV（BNN）。这些模型在RG2C数据集上进行训练。这是一个公开的自采集数据集。MobileNet v1表现更好，成功率达到98%，推理速度达到6611 FPS。在这项工作中，我们证明了可以使用FPGA加速人工神经网络，使其适用于注意力机制。", "summary": "本文旨在解决机器人移动中物体检测速度慢的问题，提出在FPGA的可编程逻辑中利用FINN架构部署并加速量化人工神经网络。研究比较了4位MobileNet v1、2位CNV和1位CNV（BNN）在RG2C数据集上的性能，结果显示MobileNet v1表现最佳，实现了98%的成功率和6611 FPS的推理速度，证明了FPGA在加速ANNs以适应高要求应用中的潜力。", "keywords": "FPGA, 人工神经网络, 红葡萄检测, 硬件加速, MobileNet v1", "comments": "本文通过在FPGA的可编程逻辑中部署量化神经网络，有效解决了机器人实时物体检测的速度瓶颈问题。其创新点在于充分利用了FPGA的硬件加速能力，并通过FINN架构实现了高效的神经网络部署。MobileNet v1在FPGA上实现的高成功率和极高推理速度，对于需要快速响应的边缘计算和机器人视觉任务具有重要意义。"}}
{"id": "2507.02197", "title": "Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust", "authors": ["Amogh Mannekote", "Adam Davies", "Guohao Li", "Kristy Elizabeth Boyer", "ChengXiang Zhai", "Bonnie J Dorr", "Francesco Pinto"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02197v1", "summary": "As LLMs are increasingly studied as role-playing agents to generate synthetic\ndata for human behavioral research, ensuring that their outputs remain coherent\nwith their assigned roles has become a critical concern. In this paper, we\ninvestigate how consistently LLM-based role-playing agents' stated beliefs\nabout the behavior of the people they are asked to role-play (\"what they say\")\ncorrespond to their actual behavior during role-play (\"how they act\").\nSpecifically, we establish an evaluation framework to rigorously measure how\nwell beliefs obtained by prompting the model can predict simulation outcomes in\nadvance. Using an augmented version of the GenAgents persona bank and the Trust\nGame (a standard economic game used to quantify players' trust and\nreciprocity), we introduce a belief-behavior consistency metric to\nsystematically investigate how it is affected by factors such as: (1) the types\nof beliefs we elicit from LLMs, like expected outcomes of simulations versus\ntask-relevant attributes of individual characters LLMs are asked to simulate;\n(2) when and how we present LLMs with relevant information about Trust Game;\nand (3) how far into the future we ask the model to forecast its actions. We\nalso explore how feasible it is to impose a researcher's own theoretical priors\nin the event that the originally elicited beliefs are misaligned with research\nobjectives. Our results reveal systematic inconsistencies between LLMs' stated\n(or imposed) beliefs and the outcomes of their role-playing simulation, at both\nan individual- and population-level. Specifically, we find that, even when\nmodels appear to encode plausible beliefs, they may fail to apply them in a\nconsistent way. These findings highlight the need to identify how and when\nLLMs' stated beliefs align with their simulated behavior, allowing researchers\nto use LLM-based agents appropriately in behavioral studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02197v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "角色扮演代理是否言行一致？LLM驱动的人类信任模拟中的信念-行为一致性", "tldr": "研究发现LLM角色扮演代理在人类信任模拟中存在系统性的言行不一，即使其信念看似合理也可能无法一致应用。", "motivation": "随着大型语言模型（LLMs）被广泛用作角色扮演代理来生成人类行为研究的合成数据，确保其输出与所扮演角色保持一致性成为关键问题。本文旨在调查LLM角色扮演代理所陈述的信念与其实际行为之间的一致性。", "method": "本文建立了一个评估框架，以衡量通过提示模型获得的信念预测模拟结果的准确性。研究使用增强版的GenAgents人格库和信任博弈（一种经济博弈），引入了信念-行为一致性指标，系统地调查了信念类型、信任博弈信息呈现方式以及模型预测未来行为的时间跨度等因素对一致性的影响。此外，研究还探讨了在原始信念与研究目标不符时，施加研究者自身理论先验的可行性。", "result": "研究结果揭示了LLM所陈述（或施加）的信念与角色扮演模拟结果之间存在系统性不一致，这在个体和群体层面均有体现。具体而言，即使模型看似编码了合理的信念，它们也可能无法以一致的方式应用这些信念。", "conclusion": "这些发现强调了识别LLM所陈述信念与模拟行为何时以及如何保持一致的必要性，从而使研究人员能够在行为研究中恰当地使用基于LLM的代理。", "translation": "随着大型语言模型（LLMs）作为角色扮演代理在人类行为研究中生成合成数据越来越受到关注，确保它们的输出与其分配的角色保持一致性已成为一个关键问题。在本文中，我们调查了基于LLM的角色扮演代理关于其被要求扮演的人的行为的陈述信念（“它们所说的”）与它们在角色扮演期间的实际行为（“它们的行为方式”）之间的一致性程度。具体来说，我们建立了一个评估框架，以严格衡量通过提示模型获得的信念能在多大程度上提前预测模拟结果。我们使用增强版的GenAgents人格库和信任博弈（一种用于量化玩家信任和互惠的标准经济博弈），引入了一个信念-行为一致性指标，系统地研究了它如何受以下因素影响：（1）我们从LLM中引出的信念类型，例如模拟的预期结果与LLM被要求模拟的个体角色的任务相关属性；（2）我们何时以及如何向LLM呈现有关信任博弈的相关信息；以及（3）我们要求模型预测其行动的未来时间跨度。我们还探讨了在原始引出信念与研究目标不符的情况下，强加研究者自身理论先验的可行性。我们的结果揭示了LLM所陈述（或施加）的信念与它们的角色扮演模拟结果之间存在系统性不一致，这在个体和群体层面均有体现。具体而言，我们发现，即使模型看似编码了合理的信念，它们也可能无法以一致的方式应用它们。这些发现强调了识别LLM所陈述信念与模拟行为何时以及如何保持一致的必要性，从而使研究人员能够在行为研究中恰当地使用基于LLM的代理。", "summary": "本文研究了大型语言模型（LLM）作为角色扮演代理在人类行为模拟中，其陈述信念与实际行为之间的一致性问题。通过建立评估框架并使用信任博弈，研究发现LLM在个体和群体层面均存在信念与模拟行为的系统性不一致，即使信念看似合理也可能未能一致应用。这表明在使用LLM进行行为研究时，需谨慎识别其言行一致性。", "keywords": "LLM, 角色扮演代理, 信念-行为一致性, 人类信任模拟, 信任博弈", "comments": "这篇论文探讨了LLM在行为模拟中的一个核心问题——信念与行为的一致性，这对于LLM在社会科学研究中的可靠应用至关重要。其创新点在于建立了量化的评估框架和指标，并系统地分析了影响一致性的多种因素。研究结果揭示了LLM在应用其内在信念时的局限性，对未来利用LLM进行行为模拟的研究具有重要的指导意义。"}}
{"id": "2507.02250", "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02250v1", "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02250v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "FMOcc：TPV驱动的流匹配选择性状态空间模型用于3D占用预测", "tldr": "FMOcc利用流匹配和选择性状态空间模型改进3D占用预测，尤其在少帧和远距离场景下表现优异，解决了少帧图像限制和3D空间冗余问题。", "motivation": "3D语义占用预测受限于少帧图像和3D空间冗余，导致对遮挡和远距离场景的预测精度低。现有融合历史帧数据的方法需要额外数据和大量计算资源。", "method": "本文提出FMOcc，一个基于流匹配选择性状态空间模型（Flow Matching Selective State Space Model）的TPV（Tri-perspective View）精炼占用网络，用于少帧3D占用预测。首先，设计了流匹配SSM模块（FMSSM）以生成缺失特征。其次，通过设计TPV SSM层和平面选择性SSM（PS3M），选择性过滤TPV特征，减少空体素对非空体素的影响，从而提高模型整体效率和远距离场景预测能力。最后，设计了掩码训练（MT）方法以增强FMOcc的鲁棒性并解决传感器数据丢失问题。", "result": "FMOcc在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，其性能优于现有最先进方法。FMOcc在Occ3D-nuScenes验证集上，两帧输入实现了43.1%的RayIoU和39.8%的mIoU。在OpenOcc上，实现了42.6%的RayIoU，推理内存为5.4G，推理时间为330ms。", "conclusion": "FMOcc通过结合流匹配和选择性状态空间模型，有效解决了3D占用预测中少帧图像和空间冗余带来的精度问题，并在处理远距离场景和传感器数据丢失方面表现出色，显著提升了预测性能。", "translation": "3D语义占用预测在自动驾驶中扮演着关键角色。然而，少帧图像的固有局限性以及3D空间的冗余性损害了对遮挡和远距离场景的预测精度。现有方法通过融合历史帧数据来提高性能，但这需要额外的数据和大量的计算资源。为了解决这些问题，本文提出FMOcc，一个基于流匹配选择性状态空间模型的TPV（Tri-perspective View）精炼占用网络，用于少帧3D占用预测。首先，为了生成缺失特征，我们设计了一个基于流匹配模型的特征精炼模块，称之为流匹配SSM模块（FMSSM）。此外，通过设计TPV SSM层和平面选择性SSM（PS3M），我们选择性地过滤TPV特征，以减少空体素对非空体素的影响，从而提高模型的整体效率和对远距离场景的预测能力。最后，我们设计了掩码训练（MT）方法来增强FMOcc的鲁棒性并解决传感器数据丢失的问题。在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，我们的FMOcc优于现有最先进的方法。我们的FMOcc在Occ3D-nuScenes验证集上，两帧输入达到了43.1%的RayIoU和39.8%的mIoU的显著分数，在OpenOcc上达到了42.6%的RayIoU，推理内存为5.4G，推理时间为330ms。", "summary": "本文提出FMOcc，一个创新的3D语义占用预测模型，旨在解决少帧图像和3D空间冗余导致的预测精度问题。FMOcc通过引入流匹配选择性状态空间模型（FMSSM），结合TPV特征精炼和平面选择性SSM（PS3M），有效生成缺失特征并过滤冗余信息，从而提高模型效率和对远距离场景的预测能力。此外，设计了掩码训练（MT）方法以增强模型鲁棒性并处理传感器数据丢失。实验证明，FMOcc在Occ3D-nuScenes和OpenOcc数据集上均超越了现有SOTA方法，尤其在少帧输入下表现出卓越性能。", "keywords": "3D语义占用预测, 流匹配, 选择性状态空间模型, TPV, 自动驾驶", "comments": "FMOcc的创新点在于其结合了流匹配和选择性状态空间模型来处理3D占用预测中的关键挑战，特别是针对少帧输入和远距离场景的特征生成与过滤问题。通过引入FMSSM、TPV SSM层、PS3M以及MT方法，该模型有效提升了预测精度和模型效率，同时降低了对大量历史数据的依赖，这对于自动驾驶等需要实时性和资源效率的应用具有重要意义。该方法为3D占用预测提供了一个新的、高效的解决方案。"}}
{"id": "2507.02208", "title": "Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement", "authors": ["Ricardo Fitas"], "categories": ["physics.app-ph", "cs.CE", "physics.comp-ph"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "Comments:      This is the submitted version of the manuscript entitled \"Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement.\" The final version is published in \"Lecture Notes in Civil Engineering\" (Springer Nature) as part of the OPTARCH 2024 proceedings", "url": "http://arxiv.org/abs/2507.02208v1", "summary": "This work aims to model the hypersurface of the effective elastic modulus, \\(\nE_{z, \\text{eff}} \\), and thickness, \\( th_{\\text{eff}} \\), in corrugated\nboards. A Latin Hypercube Sampling (LHS) is followed by Gaussian Process\nRegression (GP), enhanced by EHVI as a multi-objective acquisition function.\nAccurate modeling of \\( E_{z, \\text{eff}} \\) and \\( th_{\\text{eff}} \\) is\ncritical for optimizing the mechanical properties of corrugated materials in\nengineering applications. LHS provides an efficient and straightforward\napproach for an initial sampling of the input space; GP is expected to be able\nto adapt to the complexity of the response surfaces by incorporating both\nprediction and uncertainty. Therefore, the next points being generated and\nevaluated are based on the complexity of the hypersurfaces, and some points,\nespecially those with higher variance, are more exploited and carry more\nimportance. The performance of GP with EHVI is measured by Mean Squared Error\n(MSE). Prediction of GP resulted in \\( \\text{MSE}(E_{z, \\text{eff}}) = 5.24 \\,\n\\text{kPa}^2 \\) and \\( \\text{MSE}(th_{\\text{eff}}) = 1 \\, \\text{mm}^2 \\). GP\npossesses then improved accuracy and adaptability for future applications in\nstructural optimization.", "comment": "This is the submitted version of the manuscript entitled \"Modeling\n  the Effective Elastic Modulus and Thickness of Corrugated Boards Using\n  Gaussian Process Regression and Expected Hypervolume Improvement.\" The final\n  version is published in \"Lecture Notes in Civil Engineering\" (Springer\n  Nature) as part of the OPTARCH 2024 proceedings", "pdf_url": "http://arxiv.org/pdf/2507.02208v1", "cate": "physics.app-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "采用高斯过程回归和期望超体积改进建模瓦楞纸板的有效弹性模量和厚度", "tldr": "本文利用高斯过程回归和EHVI建模瓦楞纸板的有效弹性模量和厚度，以优化其机械性能。", "motivation": "准确建模瓦楞纸板的有效弹性模量和厚度对于优化瓦楞材料在工程应用中的机械性能至关重要。", "method": "采用拉丁超立方采样(LHS)进行初始采样，然后使用高斯过程回归(GP)，并通过EHVI（一种多目标采集函数）进行增强，以建模响应超曲面。通过均方误差(MSE)衡量GP与EHVI的性能。", "result": "GP预测结果为 \\( \\text{MSE}(E_{z, \\text{eff}}) = 5.24 \\, \\text{kPa}^2 \\) 和 \\( \\text{MSE}(th_{\\text{eff}}) = 1 \\, \\text{mm}^2 \\)。", "conclusion": "GP方法在建模瓦楞纸板的有效弹性模量和厚度方面具有更高的准确性和适应性，可用于未来的结构优化应用。", "translation": "这项工作旨在建模瓦楞纸板中有效弹性模量 \\( E_{z, \\text{eff}} \\) 和厚度 \\( th_{\\text{eff}} \\) 的超曲面。首先进行拉丁超立方采样（LHS），然后是高斯过程回归（GP），并由EHVI作为多目标采集函数进行增强。准确建模 \\( E_{z, \\text{eff}} \\) 和 \\( th_{\\text{eff}} \\) 对于优化瓦楞材料在工程应用中的机械性能至关重要。LHS为输入空间的初始采样提供了一种高效直接的方法；GP有望通过结合预测和不确定性来适应响应曲面的复杂性。因此，接下来生成和评估的点基于超曲面的复杂性，并且一些点，特别是那些具有更高方差的点，得到更多利用并具有更重要的意义。GP与EHVI的性能通过均方误差（MSE）进行衡量。GP的预测结果为 \\( \\text{MSE}(E_{z, \\text{eff}}) = 5.24 \\, \\text{kPa}^2 \\) 和 \\( \\text{MSE}(th_{\\text{eff}}) = 1 \\, \\text{mm}^2 \\)。GP因此具有更高的准确性和适应性，可用于未来的结构优化应用。", "summary": "本文研究了利用高斯过程回归（GP）和期望超体积改进（EHVI）来精确建模瓦楞纸板的有效弹性模量和厚度。通过拉丁超立方采样进行初始数据采样，GP结合预测和不确定性适应复杂的响应曲面，并通过EHVI优化采样点。实验结果显示，该方法在预测有效弹性模量和厚度方面达到了较低的均方误差，表明其在瓦楞纸板结构优化中具有较高的准确性和实用性。", "keywords": "瓦楞纸板, 有效弹性模量, 厚度, 高斯过程回归, 期望超体积改进", "comments": "本文创新性地将高斯过程回归与期望超体积改进相结合，用于瓦楞纸板的复杂材料特性建模，这对于材料工程中的设计优化具有重要意义。该方法通过考虑不确定性来优化采样，提高了模型的效率和准确性。"}}
{"id": "2507.02468", "title": "The Bias of Subspace-based Data-Driven Predictive Control", "authors": ["Keith Moffat", "Florian Dörfler", "Alessandro Chiuso"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02468v1", "summary": "This paper quantifies and addresses the bias of subspace-based Data-Driven\nPredictive Control (DDPC) for linear, time-invariant (LTI) systems. The primary\nfocus is the bias that arises when the training data is gathered with a\nfeedback controller in closed-loop with the system. First, the closed-loop bias\nof Subspace Predictive Control is quantified using the training data\ninnovations. Next, the bias of direct, subspace-based DDPC methods DeePC and\n$\\gamma$-DDPC is shown to consist of two parts--the Subspace Bias, which arises\nfrom closed-loop data, and an Optimism Bias, which arises from\nDeePC/$\\gamma$-DDPC's \"optimistic\" adjustment of the output trajectory. We show\nthat, unlike subspace-based DDPC methods, Transient Predictive Control does not\nsuffer from Subspace Bias or Optimism Bias. Double integrator experiments\ndemonstrate that Subspace and Optimism Bias are responsible for poor reference\ntracking by the subspace-based DDPC methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02468v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于子空间的数据驱动预测控制的偏差", "tldr": "本文量化并分析了基于子空间的数据驱动预测控制（DDPC）在闭环数据下产生的偏差，并指出DeePC和γ-DDPC的偏差由子空间偏差和乐观偏差组成，而瞬态预测控制没有这些偏差，实验证明这些偏差导致性能不佳。", "motivation": "本文旨在量化和解决线性时不变（LTI）系统子空间数据驱动预测控制（DDPC）中出现的偏差，特别是当训练数据是在系统闭环中通过反馈控制器收集时产生的偏差。", "method": "首先，利用训练数据创新量化了子空间预测控制的闭环偏差。接着，展示了直接的、基于子空间的DDPC方法（DeePC和γ-DDPC）的偏差由两部分组成：源于闭环数据的子空间偏差和源于DeePC/γ-DDPC“乐观”调整输出轨迹的乐观偏差。通过双积分器实验验证了这些偏差。", "result": "研究表明，DeePC和γ-DDPC的偏差由子空间偏差（源于闭环数据）和乐观偏差（源于其“乐观”调整）组成。与基于子空间的DDPC方法不同，瞬态预测控制不受子空间偏差或乐观偏差的影响。双积分器实验证明，子空间偏差和乐观偏差是基于子空间的DDPC方法参考跟踪性能差的原因。", "conclusion": "子空间偏差和乐观偏差是基于子空间的数据驱动预测控制方法性能不佳的关键因素，需要被量化和解决。", "translation": "本文量化并解决了线性时不变（LTI）系统子空间数据驱动预测控制（DDPC）的偏差。主要关注点是当训练数据是通过与系统闭环中的反馈控制器收集时产生的偏差。首先，利用训练数据创新量化了子空间预测控制的闭环偏差。接下来，展示了直接的、基于子空间的DDPC方法DeePC和γ-DDPC的偏差由两部分组成——源于闭环数据的子空间偏差和源于DeePC/γ-DDPC“乐观”调整输出轨迹的乐观偏差。我们表明，与基于子空间的DDPC方法不同，瞬态预测控制不受子空间偏差或乐观偏差的影响。双积分器实验表明，子空间偏差和乐观偏差是基于子空间的DDPC方法参考跟踪性能差的原因。", "summary": "本文深入研究了基于子空间的数据驱动预测控制（DDPC）在闭环系统中的偏差问题。研究量化了子空间预测控制的闭环偏差，并揭示了DeePC和γ-DDPC的偏差由“子空间偏差”和“乐观偏差”两部分构成。论文指出，瞬态预测控制不受这些偏差影响，并通过实验验证了这些偏差是导致基于子空间的DDPC方法参考跟踪性能不佳的主要原因。", "keywords": "数据驱动预测控制, 子空间偏差, 乐观偏差, 闭环数据, 线性时不变系统", "comments": "本文的创新点在于首次明确量化并分解了基于子空间的数据驱动预测控制在闭环数据下产生的两种关键偏差：子空间偏差和乐观偏差。这对于理解并改进现有数据驱动控制方法的性能具有重要意义，特别是在实际应用中处理闭环数据时。研究结果为设计更鲁棒、更精确的数据驱动预测控制器提供了理论基础和实验证据。"}}
{"id": "2507.02085", "title": "GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters", "authors": ["Wanjia Zhao", "Jiaqi Han", "Siyi Gu", "Mingjian Jiang", "James Zou", "Stefano Ermon"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02085v1", "summary": "Geometric diffusion models have shown remarkable success in molecular\ndynamics and structure generation. However, efficiently fine-tuning them for\ndownstream tasks with varying geometric controls remains underexplored. In this\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\nflexible and parameter-efficient fine-tuning for controlled generative tasks\nwithout modifying the original model architecture. GeoAda introduces a\nstructured adapter design: control signals are first encoded through coupling\noperators, then processed by a trainable copy of selected pretrained model\nlayers, and finally projected back via decoupling operators followed by an\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\nadapter modules, GeoAda preserves the model's geometric consistency while\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\ninductive biases of the pretrained diffusion model remain intact during\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\ngeometric control types, including frame control, global control, subgraph\ncontrol, and a broad range of application domains such as particle dynamics,\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\nwhile preserving original task accuracy, whereas other baselines experience\nsignificant performance degradation due to overfitting and catastrophic\nforgetting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02085v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "GeoAda：使用等变适配器高效微调几何扩散模型", "tldr": "GeoAda提出了一种SE(3)等变适配器框架，用于高效、参数高效地微调几何扩散模型，以应对各种几何控制任务，同时保持模型几何一致性并达到最先进的性能。", "motivation": "几何扩散模型在分子动力学和结构生成方面取得了显著成功，但如何高效地微调它们以适应不同几何控制的下游任务仍未得到充分探索。", "method": "GeoAda引入了一种SE(3)等变适配器框架，无需修改原始模型架构即可实现灵活且参数高效的微调。其适配器设计包括：通过耦合算子编码控制信号，由选定的预训练模型层的可训练副本处理，然后通过解耦算子和等变零初始化卷积投射回去。通过仅微调这些轻量级适配器模块，GeoAda保持了几何一致性，同时减轻了过拟合和灾难性遗忘。", "result": "GeoAda在多种几何控制类型（包括帧控制、全局控制、子图控制）和应用领域（如粒子动力学、分子动力学、人体运动预测、分子生成）中展现了广泛的适用性。实证结果表明，GeoAda在保持原始任务精度的同时，实现了最先进的微调性能，而其他基线则因过拟合和灾难性遗忘而导致性能显著下降。", "conclusion": "GeoAda提供了一种高效且理论上保证SE(3)等变性的方法，用于微调几何扩散模型以适应各种受控生成任务，同时克服了现有方法的局限性。", "translation": "几何扩散模型在分子动力学和结构生成方面取得了显著成功。然而，如何高效地微调它们以适应具有不同几何控制的下游任务仍未得到充分探索。在这项工作中，我们提出了一种SE(3)等变适配器框架（GeoAda），它能够在不修改原始模型架构的情况下，为受控生成任务实现灵活且参数高效的微调。GeoAda引入了一种结构化的适配器设计：控制信号首先通过耦合算子进行编码，然后由选定的预训练模型层的可训练副本进行处理，最后通过解耦算子和等变零初始化卷积投射回去。通过仅微调这些轻量级适配器模块，GeoAda在减轻过拟合和灾难性遗忘的同时，保留了模型的几何一致性。我们从理论上证明了所提出的适配器保持SE(3)等变性，确保预训练扩散模型的几何归纳偏置在适应过程中保持完整。我们展示了GeoAda在不同几何控制类型（包括帧控制、全局控制、子图控制）和广泛应用领域（如粒子动力学、分子动力学、人体运动预测和分子生成）的广泛适用性。实证结果表明，GeoAda在保持原始任务精度的同时，实现了最先进的微调性能，而其他基线则因过拟合和灾难性遗忘而导致性能显著下降。", "summary": "GeoAda提出了一种SE(3)等变适配器框架，旨在高效微调几何扩散模型以适应具有各种几何控制的下游任务。该方法通过引入轻量级适配器模块，在不修改原始模型架构的情况下，实现了参数高效的微调，同时保持了模型的几何一致性，并有效缓解了过拟合和灾难性遗忘。理论上，GeoAda确保了SE(3)等变性。实验结果表明，GeoAda在多种几何控制类型和应用领域中均达到了最先进的微调性能。", "keywords": "几何扩散模型, 微调, 等变适配器, SE(3)等变性, 参数高效", "comments": "GeoAda的创新在于其SE(3)等变适配器框架，它解决了几何扩散模型微调中效率和泛化性的难题。通过引入轻量级、可训练的适配器模块，并在理论上证明其等变性，该方法不仅实现了参数高效的微调，还确保了模型固有的几何归纳偏置得以保留，有效避免了过拟合和灾难性遗忘，这对于实际应用具有重要意义。"}}
{"id": "2507.02427", "title": "When Attention is Beneficial for Learning Wireless Resource Allocation Efficiently?", "authors": ["Jia Guo", "Chenyang Yang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02427v1", "summary": "Owing to the use of attention mechanism to leverage the dependency across\ntokens, Transformers are efficient for natural language processing. By\nharnessing permutation properties broadly exist in resource allocation\npolicies, each mapping measurable environmental parameters (e.g., channel\nmatrix) to optimized variables (e.g., precoding matrix), graph neural networks\n(GNNs) are promising for learning these policies efficiently in terms of\nscalability and generalizability. To reap the benefits of both architectures,\nthere is a recent trend of incorporating attention mechanism with GNNs for\nlearning wireless policies. Nevertheless, is the attention mechanism really\nneeded for resource allocation? In this paper, we strive to answer this\nquestion by analyzing the structures of functions defined on sets and numerical\nalgorithms, given that the permutation properties of wireless policies are\ninduced by the involved sets (say user set). In particular, we prove that the\npermutation equivariant functions on a single set can be recursively expressed\nby two types of functions: one involves attention, and the other does not. We\nproceed to re-express the numerical algorithms for optimizing several\nrepresentative resource allocation problems in recursive forms. We find that\nwhen interference (say multi-user or inter-data stream interference) is not\nreflected in the measurable parameters of a policy, attention needs to be used\nto model the interference. With the insight, we establish a framework of\ndesigning GNNs by aligning with the structures. By taking reconfigurable\nintelligent surface-aided hybrid precoding as an example, the learning\nefficiency of the proposed GNN is validated via simulations.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02427v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "注意力机制何时有利于高效学习无线资源分配？", "tldr": "本文研究了在学习无线资源分配时，注意力机制何时是真正有益的。研究发现，当干扰（如多用户或数据流间干扰）未体现在可测量参数中时，需要使用注意力来建模干扰。", "motivation": "当前存在将注意力机制与图神经网络（GNNs）结合以学习无线策略的趋势，但注意力机制对于资源分配是否真的必要是一个未解问题。本文旨在回答这一问题。", "method": "通过分析集合上定义的函数结构和数值算法，证明了单个集合上的置换等变函数可以递归地表示为两种类型：一种涉及注意力，另一种不涉及。接着，以递归形式重新表达了优化几个代表性资源分配问题的数值算法，并基于此建立了一个与结构对齐的GNN设计框架。", "result": "研究发现，当干扰（如多用户或数据流间干扰）未体现在策略的可测量参数中时，需要使用注意力来建模干扰。通过以可重构智能表面辅助混合预编码为例，验证了所提出GNN的学习效率。", "conclusion": "本文建立了一个与结构对齐的GNN设计框架，该框架基于对注意力机制在无线资源分配中作用的深入理解，特别是在处理未直接反映在可测量参数中的干扰时。仿真验证了所提GNN的学习效率。", "translation": "由于注意力机制利用了token之间的依赖关系，Transformer在自然语言处理方面表现高效。通过利用资源分配策略中广泛存在的置换特性（每个策略将可测量的环境参数（如信道矩阵）映射到优化变量（如预编码矩阵）），图神经网络（GNNs）在可伸缩性和泛化性方面有望高效学习这些策略。为了同时利用这两种架构的优势，近期出现了将注意力机制与GNNs结合以学习无线策略的趋势。然而，注意力机制对于资源分配是否真的必要？在本文中，我们通过分析集合上定义的函数结构和数值算法来努力回答这个问题，考虑到无线策略的置换特性是由所涉及的集合（例如用户集合）引起的。具体而言，我们证明了单个集合上的置换等变函数可以递归地表示为两种类型的函数：一种涉及注意力，另一种不涉及。我们继续以递归形式重新表达了优化几个代表性资源分配问题的数值算法。我们发现，当干扰（例如多用户或数据流间干扰）未反映在策略的可测量参数中时，需要使用注意力来建模干扰。基于这一见解，我们建立了一个通过与结构对齐来设计GNN的框架。以可重构智能表面辅助混合预编码为例，通过仿真验证了所提出GNN的学习效率。", "summary": "本文探讨了注意力机制在学习无线资源分配中的必要性。通过对集合上函数和数值算法的结构分析，论文证明了置换等变函数可以递归地分为包含注意力或不包含注意力的两种类型。研究核心发现是，当干扰（如多用户或数据流间干扰）未直接体现在可测量参数中时，注意力机制对于建模这些干扰至关重要。基于此洞察，文章提出了一个与结构对齐的GNN设计框架，并通过仿真验证了其学习效率。", "keywords": "注意力机制, 无线资源分配, 图神经网络, 置换等变性, 干扰建模", "comments": "该论文从理论角度深入探讨了注意力机制在无线资源分配中的作用，而非仅仅停留在经验性集成。其关于注意力在干扰建模中必要性的见解，为设计更高效、更具解释性的GNNs提供了重要的指导，具有较高的理论和实践价值。"}}
{"id": "2507.02430", "title": "A Late Collaborative Perception Framework for 3D Multi-Object and Multi-Source Association and Fusion", "authors": ["Maryem Fadili", "Mohamed Anis Ghaoui", "Louis Lecrosnier", "Steve Pechberti", "Redouane Khemmar"], "categories": ["cs.RO", "eess.IV", "eess.SP"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02430v1", "summary": "In autonomous driving, recent research has increasingly focused on\ncollaborative perception based on deep learning to overcome the limitations of\nindividual perception systems. Although these methods achieve high accuracy,\nthey rely on high communication bandwidth and require unrestricted access to\neach agent's object detection model architecture and parameters. These\nconstraints pose challenges real-world autonomous driving scenarios, where\ncommunication limitations and the need to safeguard proprietary models hinder\npractical implementation. To address this issue, we introduce a novel late\ncollaborative framework for 3D multi-source and multi-object fusion, which\noperates solely on shared 3D bounding box attributes-category, size, position,\nand orientation-without necessitating direct access to detection models. Our\nframework establishes a new state-of-the-art in late fusion, achieving up to\nfive times lower position error compared to existing methods. Additionally, it\nreduces scale error by a factor of 7.5 and orientation error by half, all while\nmaintaining perfect 100% precision and recall when fusing detections from\nheterogeneous perception systems. These results highlight the effectiveness of\nour approach in addressing real-world collaborative perception challenges,\nsetting a new benchmark for efficient and scalable multi-agent fusion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02430v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种用于3D多目标和多源关联与融合的晚期协作感知框架", "tldr": "现有协作感知方法对带宽和模型访问要求高。本文提出一种新颖的晚期融合框架，仅使用共享的3D边界框属性，实现了最先进的精度，显著降低了误差，并保持100%的准确率和召回率，解决了实际限制。", "motivation": "克服现有基于深度学习的协作感知方法在自动驾驶中面临的限制（高通信带宽、对检测模型的无限制访问），这些限制因通信限制和专有模型保护而阻碍了实际部署。", "method": "引入了一种用于3D多源和多对象融合的新型晚期协作框架。该框架仅基于共享的3D边界框属性（类别、大小、位置和方向）运行，无需直接访问检测模型。", "result": "在晚期融合中建立了新的最先进水平；与现有方法相比，位置误差降低了五倍，尺度误差降低了7.5倍，方向误差降低了一半；在融合异构感知系统的检测结果时，保持了完美的100%精确度和召回率。", "conclusion": "所提出的框架有效解决了现实世界中的协作感知挑战，为高效和可扩展的多智能体融合设定了新基准。", "translation": "在自动驾驶中，最近的研究越来越关注基于深度学习的协作感知，以克服单个感知系统的局限性。尽管这些方法实现了高精度，但它们依赖于高通信带宽，并要求对每个智能体的目标检测模型架构和参数进行无限制访问。这些限制对现实世界的自动驾驶场景构成了挑战，其中通信限制和保护专有模型的需要阻碍了实际实施。为了解决这个问题，我们引入了一种新颖的3D多源和多目标融合的晚期协作框架，该框架仅根据共享的3D边界框属性——类别、大小、位置和方向——运行，而无需直接访问检测模型。我们的框架在晚期融合中建立了新的最先进水平，与现有方法相比，位置误差降低了五倍，尺度误差降低了7.5倍，方向误差降低了一半，同时在融合异构感知系统的检测结果时保持了完美的100%精确度和召回率。这些结果突出了我们方法在解决现实世界协作感知挑战方面的有效性，为高效和可扩展的多智能体融合设定了新基准。", "summary": "本文提出了一种新颖的晚期协作感知框架，专为自动驾驶中的3D多目标和多源融合设计。与需要高带宽和模型访问的现有方法不同，该框架仅基于共享的3D边界框属性运行。它在晚期融合中取得了最先进的性能，显著降低了位置、尺度和方向误差，同时保持了100%的精确度和召回率，从而为受限的现实世界环境提供了实用的解决方案。", "keywords": "协作感知, 晚期融合, 3D目标检测, 自动驾驶, 多智能体系统", "comments": "该论文解决了协作感知中一个关键的实际挑战：对高带宽和模型访问的需求。通过专注于仅使用边界框属性的晚期融合，它提供了一个实用且可扩展的解决方案，使协作感知在数据隐私和通信效率至关重要的现实世界自动驾驶场景中更具可行性。在保持高精确度/召回率的同时显著减少误差，是其创新性和重要性的有力证明。"}}
{"id": "2507.02728", "title": "Indexing Tries within Entropy-Bounded Space", "authors": ["Lorenzo Carfagna", "Carlo Tosoni"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      14 pages, 1 figure, submitted to SPIRE 2025", "url": "http://arxiv.org/abs/2507.02728v1", "summary": "We study the problem of indexing and compressing tries using a BWT-based\napproach. Specifically, we consider a succinct and compressed representation of\nthe XBWT of Ferragina et al.\\ [FOCS '05, JACM '09] corresponding to the\nanalogous of the FM-index [FOCS '00, JACM '05] for tries. This representation\nallows to efficiently count the number of nodes reached by a given string\npattern. To analyze the space complexity of the above trie index, we propose a\nproof for the combinatorial problem of counting the number of tries with a\ngiven symbol distribution. We use this formula to define a worst-case entropy\nmeasure for tries, as well as a notion of k-th order empirical entropy. In\nparticular, we show that the relationships between these two entropy measures\nare similar to those between the corresponding well-known measures for strings.\nWe use these measures to prove that the XBWT of a trie can be encoded within a\nspace bounded by our k-th order empirical entropy plus a o(n) term, with n\nbeing the number of nodes in the trie. Notably, as happens for strings, this\nspace bound can be reached for every sufficiently small k simultaneously.\nFinally, we compare the space complexity of the above index with that of the\nr-index for tries proposed by Prezza [SODA '21] and we prove that in some cases\nthe FM-index for tries is asymptotically smaller.", "comment": "14 pages, 1 figure, submitted to SPIRE 2025", "pdf_url": "http://arxiv.org/pdf/2507.02728v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "熵界空间内的Trie树索引", "tldr": "本文研究了一种基于BWT的Trie树索引和压缩方法，提出了Trie树的熵度量，并证明了其XBWT可以在熵界空间内编码，且在某些情况下比现有索引更小。", "motivation": "解决Trie树的索引和压缩问题，特别是使用基于BWT的方法。", "method": "1. 采用XBWT（Trie树的FM-index模拟）的简洁压缩表示。2. 提出一种组合证明来计算具有给定符号分布的Trie树数量。3. 基于此公式定义了Trie树的最坏情况熵度量和k阶经验熵。4. 使用这些熵度量来分析XBWT的空间复杂度。", "result": "1. 所提出的表示能够有效计数给定字符串模式到达的节点数。2. 证明了Trie树的熵度量（最坏情况和k阶经验熵）之间的关系与字符串的对应度量相似。3. 证明了Trie树的XBWT可以在其k阶经验熵加上一个o(n)项（n是节点数）的空间内编码，并且对于足够小的k可以同时达到此空间界限。4. 证明了在某些情况下，Trie树的FM-index（即XBWT）渐近地小于Prezza提出的r-index。", "conclusion": "本文成功提出了一种新的基于BWT的Trie树索引方法，并通过引入新的熵度量证明了其空间效率，尤其是在某些情况下优于现有方法。", "translation": "我们研究了使用基于BWT的方法索引和压缩Trie树的问题。具体来说，我们考虑了Ferragina等人[FOCS '05, JACM '09]的XBWT的一种简洁压缩表示，它对应于Trie树的FM-index[FOCS '00, JACM '05的类似物。这种表示允许高效地计算给定字符串模式所到达的节点数量。为了分析上述Trie树索引的空间复杂度，我们为计算具有给定符号分布的Trie树数量的组合问题提出了一个证明。我们使用这个公式来定义Trie树的最坏情况熵度量，以及k阶经验熵的概念。特别是，我们表明这两种熵度量之间的关系与字符串的相应已知度量之间的关系相似。我们使用这些度量来证明Trie树的XBWT可以在我们的k阶经验熵加上一个o(n)项（其中n是Trie树中的节点数）的空间内编码。值得注意的是，与字符串一样，对于每个足够小的k，都可以同时达到这个空间界限。最后，我们将上述索引的空间复杂度与Prezza[SODA '21]提出的Trie树的r-index进行了比较，并证明在某些情况下，Trie树的FM-index渐近地更小。", "summary": "本文提出了一种基于BWT的Trie树索引和压缩方法，利用XBWT（Trie树的FM-index等效物）的简洁表示来高效计数节点。为了分析空间复杂度，作者提出了计算具有给定符号分布的Trie树数量的组合证明，并基于此定义了Trie树的最坏情况熵和k阶经验熵。研究表明，这些熵度量与字符串的对应度量关系相似。核心结果是证明了Trie树的XBWT可以在其k阶经验熵加上一个o(n)项的空间内编码，且该空间界限对所有足够小的k同时可达。此外，与现有r-index相比，Trie树的FM-index在某些情况下显示出渐近更小的空间复杂度。", "keywords": "Trie树索引, BWT, FM-index, 熵, 空间复杂度", "comments": "本文的创新之处在于将BWT和FM-index的思想推广到Trie树的索引和压缩中，并引入了新的Trie树熵度量来严格分析其空间效率。其重要性在于为处理大规模Trie树数据提供了理论上更优的压缩和查询方法。"}}
{"id": "2507.02815", "title": "Towards Perception-Informed Latent HRTF Representations", "authors": ["You Zhang", "Andrew Francl", "Ruohan Gao", "Paul Calamia", "Zhiyao Duan", "Ishwarya Ananthabhotla"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE WASPAA 2025, camera-ready version", "url": "http://arxiv.org/abs/2507.02815v1", "summary": "Personalized head-related transfer functions (HRTFs) are essential for\nensuring a realistic auditory experience over headphones, because they take\ninto account individual anatomical differences that affect listening. Most\nmachine learning approaches to HRTF personalization rely on a learned\nlow-dimensional latent space to generate or select custom HRTFs for a listener.\nHowever, these latent representations are typically learned in a manner that\noptimizes for spectral reconstruction but not for perceptual compatibility,\nmeaning they may not necessarily align with perceptual distance. In this work,\nwe first study whether traditionally learned HRTF representations are well\ncorrelated with perceptual relations using auditory-based objective perceptual\nmetrics; we then propose a method for explicitly embedding HRTFs into a\nperception-informed latent space, leveraging a metric-based loss function and\nsupervision via Metric Multidimensional Scaling (MMDS). Finally, we demonstrate\nthe applicability of these learned representations to the task of HRTF\npersonalization. We suggest that our method has the potential to render\npersonalized spatial audio, leading to an improved listening experience.", "comment": "Accepted by IEEE WASPAA 2025, camera-ready version", "pdf_url": "http://arxiv.org/pdf/2507.02815v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "迈向感知信息化的潜在HRTF表示", "tldr": "该研究探讨了传统HRTF潜在表示与感知兼容性的不足，并提出了一种利用度量损失函数和MMDS将HRTF嵌入到感知信息化的潜在空间的方法，以改善个性化空间音频体验。", "motivation": "个性化头部相关传递函数（HRTFs）对于通过耳机提供逼真的听觉体验至关重要。然而，大多数机器学习方法在学习HRTF的低维潜在空间时，通常优化的是频谱重建而非感知兼容性，这可能导致潜在表示与感知距离不一致。", "method": "首先，研究使用基于听觉的客观感知指标来研究传统学习的HRTF表示是否与感知关系良好相关。然后，提出了一种通过利用基于度量的损失函数和通过度量多维尺度（MMDS）进行监督，将HRTFs明确嵌入到感知信息化的潜在空间的方法。", "result": "研究展示了这些学习到的表示在HRTF个性化任务中的适用性。", "conclusion": "该方法有潜力渲染个性化空间音频，从而改善听觉体验。", "translation": "个性化头部相关传递函数（HRTFs）对于通过耳机提供逼真的听觉体验至关重要，因为它们考虑了影响听觉的个体解剖差异。大多数用于HRTF个性化的机器学习方法依赖于学习到的低维潜在空间来为听众生成或选择定制的HRTFs。然而，这些潜在表示通常以优化频谱重建而非感知兼容性的方式学习，这意味着它们可能不一定与感知距离对齐。在这项工作中，我们首先使用基于听觉的客观感知指标研究传统学习的HRTF表示是否与感知关系良好相关；然后，我们提出了一种将HRTFs明确嵌入到感知信息化的潜在空间的方法，该方法利用了基于度量的损失函数并通过度量多维尺度（MMDS）进行监督。最后，我们展示了这些学习到的表示在HRTF个性化任务中的适用性。我们认为我们的方法有潜力渲染个性化空间音频，从而改善听觉体验。", "summary": "本研究关注个性化头部相关传递函数（HRTFs）在耳机听觉体验中的重要性。鉴于当前机器学习方法在HRTF潜在表示学习中侧重频谱重建而非感知兼容性，导致与实际感知不符，作者首先探究了传统HRTF表示与感知关系的相关性。随后，提出了一种新方法，通过引入基于度量的损失函数和度量多维尺度（MMDS）监督，将HRTFs嵌入到感知信息化的潜在空间中。该方法被证明适用于HRTF个性化任务，并有望提升个性化空间音频的听觉体验。", "keywords": "HRTF, 感知信息化, 潜在表示, 空间音频, 个性化", "comments": "该论文的创新点在于明确提出并解决了传统HRTF潜在表示在感知兼容性方面的不足。通过引入“感知信息化”的潜在空间学习，并利用度量学习（特别是MMDS）进行监督，该方法有望弥合HRTF频谱优化与实际听觉感知之间的差距，从而为更真实的个性化空间音频体验奠定基础。其重要性在于提升了空间音频个性化的质量，具有广泛的应用前景。"}}
{"id": "2507.02643", "title": "Calibrated Recommendations: Survey and Future Directions", "authors": ["Diego Corrêa da Silva", "Dietmar Jannach"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02643v1", "summary": "The idea of calibrated recommendations is that the properties of the items\nthat are suggested to users should match the distribution of their individual\npast preferences. Calibration techniques are therefore helpful to ensure that\nthe recommendations provided to a user are not limited to a certain subset of\nthe user's interests. Over the past few years, we have observed an increasing\nnumber of research works that use calibration for different purposes, including\nquestions of diversity, biases, and fairness. In this work, we provide a survey\non the recent developments in the area of calibrated recommendations. We both\nreview existing technical approaches for calibration and provide an overview on\nempirical and analytical studies on the effectiveness of calibration for\ndifferent use cases. Furthermore, we discuss limitations and common challenges\nwhen implementing calibration in practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02643v1", "cate": "cs.IR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "校准推荐：综述与未来方向", "tldr": "这篇综述回顾了校准推荐领域的最新进展，包括现有技术方法、有效性研究以及实际应用中的局限性和挑战。", "motivation": "确保推荐系统提供的物品不局限于用户兴趣的某个子集，并解决多样性、偏见和公平性等问题。", "method": "本文对校准推荐领域的最新发展进行了综述，回顾了现有的技术方法，并概述了校准有效性的实证和分析研究。", "result": "综述了校准推荐的现有技术方法、不同用例下校准有效性的实证和分析研究，并讨论了实际实施校准时的局限性和常见挑战。", "conclusion": "本文综述了校准推荐的现有技术和研究，并指出了在实际应用中实现校准的局限性和常见挑战。", "translation": "校准推荐的核心思想是，向用户推荐的物品属性应与其过去个人偏好的分布相匹配。因此，校准技术有助于确保向用户提供的推荐不局限于用户兴趣的某个特定子集。在过去几年中，我们观察到越来越多的研究工作将校准用于不同目的，包括多样性、偏见和公平性等问题。在这项工作中，我们对校准推荐领域的最新发展进行了综述。我们既回顾了现有的校准技术方法，也概述了关于校准在不同用例中有效性的实证和分析研究。此外，我们还讨论了在实际实施校准时的局限性和常见挑战。", "summary": "这篇论文对校准推荐领域进行了全面综述，校准推荐旨在使推荐物品的属性与用户过去的偏好分布相匹配，以避免推荐过于局限。文章回顾了现有的技术方法、校准有效性的实证和分析研究，并探讨了在实际应用中实施校准的局限性和挑战。校准技术对于解决推荐系统中的多样性、偏见和公平性问题至关重要。", "keywords": "校准推荐, 推荐系统, 综述, 多样性, 公平性", "comments": "这篇综述论文对于校准推荐领域具有重要价值，它系统地梳理了现有研究，不仅总结了技术方法和有效性，还前瞻性地指出了实践中的挑战和未来方向，为研究人员和实践者提供了全面的参考。"}}
{"id": "2507.02364", "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "authors": ["Pilsung Kang"], "categories": ["cs.CL", "quant-ph"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02364v1", "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02364v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "QFFN-BERT：混合量子-经典Transformer中深度、性能和数据效率的实证研究", "tldr": "QFFN-BERT是一种混合量子-经典Transformer，用参数化量子电路（PQC）替换了前馈网络（FFN）模块。它在全数据设置下超越了经典BERT，同时将FFN参数减少了99%以上，并在少样本学习中展现出卓越的数据效率。", "motivation": "前馈网络（FFN）在标准Transformer编码器块中贡献了约三分之二的参数。虽然之前的研究主要将参数化量子电路（PQC）集成到自注意力模块中，但本研究的动机是探索将PQC应用于FFN模块，以增强神经网络架构的表达能力，并显著减少模型参数，同时提高数据效率。", "method": "本研究引入了QFFN-BERT，这是一种混合量子-经典Transformer，其中紧凑型BERT变体的FFN模块被基于PQC的层所取代。所设计的PQC架构包含了残差连接、$R_Y$和$R_Z$旋转，以及交替纠缠策略，以确保稳定的训练和高表达能力。实验在经典模拟器上进行，并使用SST-2和DBpedia基准数据集进行评估。", "result": "1. 精心配置的QFFN-BERT在全数据设置下达到了基线准确率的102.0%，超越了其经典对应模型，同时将FFN特定参数减少了99%以上。2. 该模型在少样本学习场景中展现出持续且具有竞争力的优势，证实了其在数据效率方面的潜力。", "conclusion": "当与基础深度学习原理共同设计时，参数化量子电路（PQC）可以作为经典前馈网络（FFN）强大且参数高效的替代方案。", "translation": "参数化量子电路（PQCs）最近已成为增强神经网络架构表达能力的有前景的组件。在这项工作中，我们引入了QFFN-BERT，这是一种混合量子-经典Transformer，其中紧凑型BERT变体的前馈网络（FFN）模块被基于PQC的层取代。这种设计是受FFN主要参数贡献的启发，FFN约占标准Transformer编码器块中参数的三分之二。虽然之前的研究主要将PQC集成到自注意力模块中，但我们的工作重点放在FFN上，并系统地研究了PQC深度、表达能力和可训练性之间的权衡。我们最终的PQC架构包含了残差连接、$R_Y$和$R_Z$旋转，以及交替纠缠策略，以确保稳定的训练和高表达能力。我们在经典模拟器上，在SST-2和DBpedia基准数据集上进行的实验，证明了两个关键发现。首先，精心配置的QFFN-BERT达到了基线准确率的102.0%，在全数据设置下超越了其经典对应模型，同时将FFN特定参数减少了99%以上。其次，我们的模型在少样本学习场景中展现出持续且具有竞争力的优势，证实了其在卓越数据效率方面的潜力。这些结果得到了对未能学习的非优化PQC进行消融研究的支持，证实了当与基础深度学习原理共同设计时，PQC可以作为经典FFN强大且参数高效的替代方案。", "summary": "本论文介绍了QFFN-BERT，这是一种混合量子-经典Transformer，它用参数化量子电路（PQC）层替换了BERT中占主导地位的前馈网络（FFN）模块。该研究系统地探讨了PQC深度、表达能力和可训练性之间的权衡，并设计了一个包含残差连接、特定旋转和交替纠缠策略的PQC架构，以确保训练的稳定性和高表达能力。在SST-2和DBpedia基准上的实验结果表明，QFFN-BERT不仅在全数据设置下达到了甚至超越了经典BERT的性能（最高达102.0%的基线准确率），而且将FFN相关参数减少了99%以上。此外，该模型在少样本学习场景中展现出显著的数据效率优势，证实了PQC在与深度学习原理结合时，能够作为经典FFN的高效且参数精简的替代方案。", "keywords": "混合量子-经典, Transformer, 参数化量子电路, 前馈网络, 数据效率", "comments": "本论文创新性地将参数化量子电路（PQCs）应用于BERT模型中的前馈网络（FFN）模块，这与以往主要关注自注意力模块的研究不同，开辟了新的研究方向。其显著的参数削减（FFN参数减少99%以上）同时保持甚至提升性能，尤其是在少样本学习场景中的优异表现，凸显了混合量子-经典模型在提升效率和处理数据稀缺问题上的巨大潜力。论文强调将PQC与基础深度学习原理共同设计，这对于未来将量子计算技术实际应用于大规模神经网络模型具有重要的指导意义。"}}
{"id": "2507.02080", "title": "TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation", "authors": ["Yubeen Lee", "Sangeun Lee", "Chaewon Park", "Junyeop Cha", "Eunil Park"], "categories": ["cs.MM", "cs.SD"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures, 2 tables", "url": "http://arxiv.org/abs/2507.02080v1", "summary": "Multimodal emotion recognition often suffers from performance degradation in\nvalence-arousal estimation due to noise and misalignment between audio and\nvisual modalities. To address this challenge, we introduce TAGF, a Time-aware\nGated Fusion framework for multimodal emotion recognition. The TAGF adaptively\nmodulates the contribution of recursive attention outputs based on temporal\ndynamics. Specifically, the TAGF incorporates a BiLSTM-based temporal gating\nmechanism to learn the relative importance of each recursive step and\neffectively integrates multistep cross-modal features. By embedding temporal\nawareness into the recursive fusion process, the TAGF effectively captures the\nsequential evolution of emotional expressions and the complex interplay between\nmodalities. Experimental results on the Aff-Wild2 dataset demonstrate that TAGF\nachieves competitive performance compared with existing recursive\nattention-based models. Furthermore, TAGF exhibits strong robustness to\ncross-modal misalignment and reliably models dynamic emotional transitions in\nreal-world conditions.", "comment": "9 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.02080v1", "cate": "cs.MM", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "TAGF：用于多模态效价-唤醒估计的时间感知门控融合", "tldr": "提出TAGF框架，通过时间感知门控融合解决多模态情感识别中因噪声和模态错位导致的效价-唤醒估计性能下降问题，并在Aff-Wild2数据集上表现出竞争力。", "motivation": "多模态情感识别在效价-唤醒估计中，由于音频和视觉模态之间的噪声和错位，性能常会下降。", "method": "引入了时间感知门控融合（TAGF）框架。TAGF基于时间动态自适应地调节递归注意力输出的贡献，并包含一个基于BiLSTM的时间门控机制，以学习每个递归步骤的相对重要性并有效整合多步跨模态特征。", "result": "在Aff-Wild2数据集上的实验结果表明，TAGF与现有基于递归注意力的模型相比，取得了有竞争力的性能。此外，TAGF对跨模态错位表现出强大的鲁棒性，并能可靠地建模现实世界中的动态情感转换。", "conclusion": "TAGF通过其时间感知门控融合机制，有效解决了多模态情感识别中因噪声和模态错位导致的性能下降问题，展现了在复杂现实条件下的优越性和鲁棒性。", "translation": "多模态情感识别在效价-唤醒估计中，由于音频和视觉模态之间的噪声和错位，性能常会下降。为了解决这一挑战，我们引入了TAGF，一个用于多模态情感识别的时间感知门控融合框架。TAGF根据时间动态自适应地调节递归注意力输出的贡献。具体来说，TAGF包含一个基于BiLSTM的时间门控机制，以学习每个递归步骤的相对重要性并有效整合多步跨模态特征。通过将时间感知嵌入到递归融合过程中，TAGF有效地捕获了情感表达的序列演变以及模态之间复杂的相互作用。在Aff-Wild2数据集上的实验结果表明，TAGF与现有基于递归注意力的模型相比，取得了有竞争力的性能。此外，TAGF对跨模态错位表现出强大的鲁棒性，并能可靠地建模现实世界中的动态情感转换。", "summary": "本文提出了时间感知门控融合（TAGF）框架，旨在解决多模态情感识别中效价-唤醒估计因模态噪声和错位导致的性能下降问题。TAGF通过基于BiLSTM的时间门控机制，自适应地融合多步跨模态特征，有效捕捉情感表达的时间动态和模态间复杂关系。实验证明，TAGF在Aff-Wild2数据集上性能优异，并对跨模态错位具有强大鲁棒性。", "keywords": "多模态情感识别, 效价-唤醒估计, 时间感知融合, BiLSTM, 模态错位", "comments": "该论文的创新点在于引入了时间感知门控机制（BiLSTM-based temporal gating）来处理多模态情感识别中的时间动态和模态间错位问题。通过自适应地调节递归注意力输出的贡献，TAGF能够更有效地整合跨模态信息并增强对现实世界复杂情感表达的鲁棒性，这对于提升多模态情感识别的实用性具有重要意义。"}}
{"id": "2507.02489", "title": "A 10-bit S-box generated by Feistel construction from cellular automata", "authors": ["Thomas Prévost", "Bruno Martin"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02489v1", "summary": "We propose a new 10-bit S-box generated from a Feistel construction. The\nsubpermutations are generated by a 5-cell cellular automaton based on a unique\nwell-chosen rule and bijective affine transformations. In particular, the\ncellular automaton rule is chosen based on empirical tests of its ability to\ngenerate good pseudorandom output on a ring cellular automaton. Similarly,\nFeistel's network layout is based on empirical data regarding the quality of\nthe output S-box. We perform cryptanalysis of the generated 10-bit S-box, and\nwe find security properties comparable to or sometimes even better than those\nof the standard AES S-box. We believe that our S-box could be used to replace\nthe 5-bit substitution of ciphers like ASCON.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02489v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种由元胞自动机和Feistel结构生成的10位S盒", "tldr": "本文提出了一种结合元胞自动机和Feistel结构生成10位S盒的新方法，该S盒在安全性上可与AES S盒媲美甚至更好，并可能用于替代现有密码算法中的S盒。", "motivation": "提出一种新的10位S盒生成方法，旨在提高S盒的安全性，并探索其在现有密码算法（如ASCON）中替代5位替换的可能性。", "method": "通过Feistel结构生成10位S盒，其中子置换由基于特定规则和双射仿射变换的5单元元胞自动机生成。元胞自动机规则和Feistel网络布局的选择均基于经验测试，以确保良好的伪随机输出和S盒质量。", "result": "生成的10位S盒的安全性属性与标准AES S盒相当，有时甚至更好。", "conclusion": "该研究提出的S盒在安全性能上表现出色，有望替代ASCON等密码算法中的5位替换。", "translation": "我们提出了一种由Feistel结构生成的新型10位S盒。子置换由基于独特精心选择规则和双射仿射变换的5单元元胞自动机生成。特别是，元胞自动机规则的选择是基于对其在环形元胞自动机上生成良好伪随机输出能力的经验测试。类似地，Feistel网络的布局也是基于关于输出S盒质量的经验数据。我们对生成的10位S盒进行了密码分析，发现其安全属性与标准AES S盒相当，有时甚至更好。我们相信我们的S盒可以用于替代ASCON等密码算法的5位替换。", "summary": "本文介绍了一种利用元胞自动机和Feistel结构生成10位S盒的新方法。该方法通过5单元元胞自动机结合双射仿射变换生成子置换，并通过经验测试优化元胞自动机规则和Feistel网络布局。密码分析结果显示，该10位S盒具有与标准AES S盒相当或更优的安全性能，并有望在现有密码算法中实现替代应用。", "keywords": "S盒, Feistel结构, 元胞自动机, 密码分析, 10位S盒", "comments": "本文的创新之处在于结合了元胞自动机和Feistel结构来生成S盒，并强调了经验测试在规则和布局选择中的作用。其重要性在于证明了所生成S盒优异的安全性能，为未来密码算法中的S盒设计提供了新的思路和潜力。"}}
{"id": "2507.02400", "title": "DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems", "authors": ["Maximilian Zipfl", "Pascal Zwick", "Patrick Schulz", "Marc Rene Zofka", "Albert Schotschneider", "Helen Gremmelmaier", "Nikolai Polley", "Ferdinand Mütsch", "Kevin Simon", "Fabian Gottselig", "Michael Frey", "Sergio Marschall", "Akim Stark", "Maximilian Müller", "Marek Wehmer", "Mihai Kocsis", "Dominic Waldenmayer", "Florian Schnepf", "Erik Heinrich", "Sabrina Pletz", "Matthias Kölle", "Karin Langbein-Euchner", "Alexander Viehl", "Raoul Zöllner", "J. Marius Zöllner"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at the IEEE IAVVC 2025 Conference", "url": "http://arxiv.org/abs/2507.02400v1", "summary": "In the future, mobility will be strongly shaped by the increasing use of\ndigitalization. Not only will individual road users be highly interconnected,\nbut also the road and associated infrastructure. At that point, a Digital Twin\nbecomes particularly appealing because, unlike a basic simulation, it offers a\ncontinuous, bilateral connection linking the real and virtual environments.\nThis paper describes the digital reconstruction used to develop the Digital\nTwin of the Test Area Autonomous Driving-Baden-W\\\"urttemberg (TAF-BW), Germany.\nThe TAF-BW offers a variety of different road sections, from high-traffic urban\nintersections and tunnels to multilane motorways. The test area is equipped\nwith a comprehensive Vehicle-to-Everything (V2X) communication infrastructure\nand multiple intelligent intersections equipped with camera sensors to\nfacilitate real-time traffic flow monitoring. The generation of authentic data\nas input for the Digital Twin was achieved by extracting object lists at the\nintersections. This process was facilitated by the combined utilization of\ncamera images from the intelligent infrastructure and LiDAR sensors mounted on\na test vehicle. Using a unified interface, recordings from real-world\ndetections of traffic participants can be resimulated. Additionally, the\nsimulation framework's design and the reconstruction process is discussed. The\nresulting framework is made publicly available for download and utilization at:\nhttps://digit4taf-bw.fzi.de The demonstration uses two case studies to\nillustrate the application of the digital twin and its interfaces: the analysis\nof traffic signal systems to optimize traffic flow and the simulation of\nsecurity-related scenarios in the communications sector.", "comment": "Accepted at the IEEE IAVVC 2025 Conference", "pdf_url": "http://arxiv.org/pdf/2507.02400v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DigiT4TAF -- 连接物理与数字世界，助力未来交通系统", "tldr": "本文介绍了DigiT4TAF，一个为未来交通系统开发的数字孪生框架，具体针对德国自动驾驶测试区（TAF-BW）。它通过结合真实的传感器数据和V2X通信，实现了物理与数字环境的连接，用于交通流优化和安全场景模拟。", "motivation": "未来出行将深受数字化影响，需要道路使用者、道路和基础设施之间高度互联。数字孪生因其提供真实与虚拟环境之间连续、双向连接的独特优势，比基本模拟更具吸引力，是实现未来交通系统的关键。", "method": "本文描述了用于开发德国巴登-符腾堡州自动驾驶测试区（TAF-BW）数字孪生的数字重建过程。通过结合智能基础设施的摄像头图像和测试车辆上的激光雷达传感器，在交叉口提取对象列表，生成了数字孪生所需的真实数据。该框架使用统一接口，可重新模拟真实世界中交通参与者的检测。文中还讨论了模拟框架的设计和重建过程。", "result": "开发并公开了一个名为DigiT4TAF-BW的数字孪生框架，该框架已可供下载和使用。通过两个案例研究，展示了其应用：分析交通信号系统以优化交通流，以及模拟通信领域的安全相关场景。", "conclusion": "本文成功开发并公开了一个用于未来交通系统的数字孪生框架（DigiT4TAF），通过连接物理与数字世界，并在交通优化和安全场景模拟方面展示了其实用性。", "translation": "在未来，出行将受到数字化日益增长的影响。不仅个体道路使用者将高度互联，道路及相关基础设施也将如此。届时，数字孪生将变得特别有吸引力，因为它与基本模拟不同，提供连接真实和虚拟环境的连续、双向连接。本文描述了用于开发德国巴登-符腾堡州自动驾驶测试区（TAF-BW）数字孪生的数字重建过程。TAF-BW 提供各种不同的路段，从交通繁忙的城市交叉口和隧道到多车道高速公路。测试区配备了全面的车联网（V2X）通信基础设施和多个配备摄像头传感器的智能交叉口，以实现实时交通流监测。通过在交叉口提取对象列表，结合利用智能基础设施的摄像头图像和测试车辆上安装的激光雷达传感器，实现了为数字孪生输入真实数据的生成。使用统一的接口，可以重新模拟真实世界中交通参与者的检测。此外，本文还讨论了模拟框架的设计和重建过程。所得到的框架已公开提供下载和使用：https://digit4taf-bw.fzi.de。该演示使用两个案例研究来说明数字孪生及其接口的应用：分析交通信号系统以优化交通流，以及模拟通信领域的安全相关场景。", "summary": "本文介绍了DigiT4TAF，一个为德国巴登-符腾堡州自动驾驶测试区（TAF-BW）构建的数字孪生框架，旨在连接物理与数字世界以支持未来交通系统。论文详细阐述了数字重建过程，该过程通过整合智能基础设施的摄像头图像和车载激光雷达传感器数据来生成真实交通数据。该框架提供连续的双向连接，允许重新模拟真实世界的交通检测，并已公开可用。文章通过分析交通信号系统以优化交通流和模拟通信安全场景的两个案例研究，展示了数字孪生及其接口的实际应用。", "keywords": "数字孪生, 未来交通, TAF-BW, V2X, 交通优化", "comments": "本文通过开发一个将真实物理世界与数字环境连接起来的数字孪生框架，为未来交通系统的发展迈出了重要一步。其创新之处在于利用多传感器数据融合（摄像头、激光雷达、V2X）构建高保真度的数字副本，并支持真实世界交通数据的重新模拟。该框架的公开可用性是其一大亮点，将极大地促进交通优化和自动驾驶技术领域的进一步研究与开发。"}}
{"id": "2507.02533", "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models", "authors": ["Miguel Romero-Arjona", "José A. Parejo", "Juan C. Alonso", "Ana B. Sánchez", "Aitor Arrieta", "Sergio Segura"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02533v1", "summary": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02533v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Meta-Fair：AI辅助的大型语言模型公平性测试", "tldr": "Meta-Fair是一种新的AI辅助方法，利用变质测试和大型语言模型（LLM）自身的能力，实现LLM公平性测试的自动化，有效揭示偏见。", "motivation": "公平性是AI系统开发的核心原则，但评估和实施起来很困难。当前LLM的公平性测试方法依赖于手动评估、固定模板、确定性启发式和人工策划数据集，导致资源密集且难以扩展。", "method": "本研究提出了一种名为Meta-Fair的自动化LLM公平性测试方法。它基于两个核心思想：1) 采用变质测试（metamorphic testing），通过检查模型输出如何响应输入提示的受控修改（由变质关系MRs定义）来发现偏见；2) 利用LLM自身的能力进行测试用例生成和输出评估。该方法还辅以三个支持LLM驱动的测试用例生成、执行和评估的开源工具。", "result": "Meta-Fair在揭示LLM中的偏见方面是有效的，平均精度达到92%，在29%的执行中揭示了有偏见的行为。此外，LLM被证明是可靠且一致的评估器，表现最佳的模型F1分数高达0.79。尽管非确定性影响一致性，但通过仔细的MR设计可以缓解这些影响。", "conclusion": "研究结果表明，Meta-Fair为实现LLM测试中前所未有的自动化水平提供了一条有希望的道路，尽管确保更广泛的适用性仍存在挑战。", "translation": "公平性——即不存在不合理偏见——是人工智能（AI）系统开发中的核心原则，但评估和实施起来仍然很困难。当前大型语言模型（LLM）的公平性测试方法通常依赖于手动评估、固定模板、确定性启发式和人工策划数据集，这使得它们资源密集且难以扩展。这项工作旨在为LLM公平性测试提出一种新颖的自动化方法奠定基础，以减少对特定领域资源的依赖，并扩大现有方法的适用性。我们的方法Meta-Fair基于两个关键思想。首先，我们采用变质测试（metamorphic testing）来通过检查模型输出如何响应输入提示的受控修改（由变质关系MRs定义）来发现偏见。其次，我们建议利用LLM在测试用例生成和输出评估方面的潜力，发挥它们生成多样化输入和有效分类输出的能力。该提案辅以三个支持LLM驱动的测试用例生成、执行和评估的开源工具。我们报告了涉及12个预训练LLM、14个MR、5个偏见维度和7.9K个自动生成测试用例的几项实验结果。结果表明，Meta-Fair在揭示LLM中的偏见方面是有效的，平均精度达到92%，并在29%的执行中揭示了有偏见的行为。此外，LLM被证明是可靠且一致的评估器，表现最佳的模型F1分数高达0.79。尽管非确定性会影响一致性，但这些影响可以通过仔细的MR设计来缓解。虽然确保更广泛的适用性仍存在挑战，但结果表明朝着LLM测试中前所未有的自动化水平迈进了一条有希望的道路。", "summary": "Meta-Fair提出了一种新颖的AI辅助自动化方法，用于大型语言模型（LLM）的公平性测试。该方法结合了变质测试和LLM自身在测试用例生成及输出评估方面的能力，旨在克服传统手动和资源密集型方法的局限性。实验结果表明，Meta-Fair能有效发现LLM中的偏见（平均精度92%），且LLM作为评估器表现可靠。尽管存在非确定性影响，但通过精心设计可以缓解。这项工作为LLM测试的自动化提供了有前景的方向。", "keywords": "LLM公平性测试, 自动化, 变质测试, 偏见检测, AI辅助测试", "comments": "这项研究的创新之处在于利用LLM自身的能力进行测试用例生成和结果评估，结合变质测试，大大提升了公平性测试的自动化水平和效率。它解决了现有LLM公平性测试中资源密集且难以扩展的痛点，为未来AI系统公平性保障提供了有价值的工具和思路。尽管论文指出了非确定性和更广泛适用性的挑战，但其提出的自动化框架具有重要意义。"}}
{"id": "2507.02283", "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness", "authors": ["Tim Rogers", "Ben Teehankee"], "categories": ["cs.HC", "I.2.6; H.1.2"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2507.02283v1", "summary": "This paper examines a critical yet unexplored dimension of the AI alignment\nproblem: the potential for Large Language Models (LLMs) to inherit and amplify\nexisting misalignments between human espoused theories and theories-in-use.\nDrawing on action science research, we argue that LLMs trained on\nhuman-generated text likely absorb and reproduce Model 1 theories-in-use - a\ndefensive reasoning pattern that both inhibits learning and creates ongoing\nanti-learning dynamics at the dyad, group, and organisational levels. Through a\ndetailed case study of an LLM acting as an HR consultant, we show how its\nadvice, while superficially professional, systematically reinforces\nunproductive problem-solving approaches and blocks pathways to deeper\norganisational learning. This represents a specific instance of the alignment\nproblem where the AI system successfully mirrors human behaviour but inherits\nour cognitive blind spots. This poses particular risks if LLMs are integrated\ninto organisational decision-making processes, potentially entrenching\nanti-learning practices while lending authority to them. The paper concludes by\nexploring the possibility of developing LLMs capable of facilitating Model 2\nlearning - a more productive theory-in-use - and suggests this effort could\nadvance both AI alignment research and action science practice. This analysis\nreveals an unexpected symmetry in the alignment challenge: the process of\ndeveloping AI systems properly aligned with human values could yield tools that\nhelp humans themselves better embody those same values.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2507.02283v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "内在失调：大型语言模型复制了我们双环学习的盲点", "tldr": "大型语言模型可能继承并放大人类思维中的学习障碍，通过复制防御性推理模式来阻碍深度组织学习，这需要开发能促进更高效学习的AI。", "motivation": "本文旨在探讨AI对齐问题中一个关键但尚未被探索的维度：大型语言模型（LLMs）继承和放大人类“宣称理论”与“使用理论”之间现有失调的可能性。", "method": "研究借鉴了行动科学研究，并通过一个详细的案例研究，分析了LLM作为人力资源顾问时，其建议如何强化无益的问题解决方法并阻碍更深层次的组织学习。", "result": "通过案例研究发现，LLM的建议虽然表面专业，但系统性地强化了低效的问题解决方式，并阻碍了更深层次的组织学习。这表明AI系统在成功模仿人类行为的同时，也继承了人类的认知盲点。", "conclusion": "论文最后探讨了开发能够促进第二类学习（一种更富有成效的使用理论）的LLM的可能性，并认为这项努力可以同时推动AI对齐研究和行动科学实践。", "translation": "本文探讨了AI对齐问题中一个关键但尚未被探索的维度：大型语言模型（LLMs）继承和放大人类“宣称理论”与“使用理论”之间现有失调的可能性。借鉴行动科学研究，我们认为，在人类生成文本上训练的LLMs很可能吸收并复制“模型1使用理论”——一种防御性推理模式，它既抑制学习，又在双边、群体和组织层面持续制造反学习动态。通过一个详细的LLM作为人力资源顾问的案例研究，我们展示了其建议虽然表面专业，但系统性地强化了低效的问题解决方式，并阻碍了更深层次的组织学习。这代表了对齐问题的一个特定实例，即AI系统成功地模仿了人类行为，却继承了我们的认知盲点。如果LLMs被整合到组织决策过程中，这会带来特殊的风险，可能固化反学习实践，同时赋予它们权威性。论文最后探讨了开发能够促进“模型2学习”（一种更富有成效的使用理论）的LLM的可能性，并认为这项努力可以同时推动AI对齐研究和行动科学实践。这项分析揭示了对齐挑战中一个意想不到的对称性：开发与人类价值观正确对齐的AI系统的过程，可能会产生帮助人类自身更好地体现这些价值观的工具。", "summary": "本文探讨了大型语言模型（LLMs）在AI对齐问题中可能继承并放大人类“宣称理论”与“使用理论”之间失调的现象。研究指出，LLMs可能复制“模型1使用理论”这一防御性推理模式，从而阻碍学习。通过一个LLM作为HR顾问的案例研究，论文展示了LLM如何强化无益的问题解决方式并阻碍深度组织学习，揭示了AI系统在模仿人类行为时继承人类认知盲点的问题。文章强调了将LLMs整合到组织决策中的潜在风险，并提出开发能促进“模型2学习”的LLMs，以期同时推进AI对齐研究和行动科学实践。", "keywords": "大型语言模型, AI对齐, 双环学习, 使用理论, 组织学习", "comments": "这篇论文提出了一个非常新颖且重要的观点，即将AI对齐问题与人类自身的认知盲点（双环学习盲点）联系起来。它不仅指出了LLMs可能复制人类的缺陷，还通过行动科学的视角，深入分析了这种复制对组织学习和决策的潜在负面影响。其创新之处在于将AI对齐从单纯的技术/价值观对齐扩展到了认知和行为模式的深层对齐，并提出了AI反过来帮助人类进行自我提升的可能性，这为AI伦理和发展开辟了新的研究方向。"}}
{"id": "2507.02828", "title": "Designs from magic-augmented Clifford circuits", "authors": ["Yuzhen Zhang", "Sagar Vijay", "Yingfei Gu", "Yimu Bao"], "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "cs.IT", "hep-th", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      59 pages", "url": "http://arxiv.org/abs/2507.02828v1", "summary": "We introduce magic-augmented Clifford circuits -- architectures in which\nClifford circuits are preceded and/or followed by constant-depth circuits of\nnon-Clifford (``magic\") gates -- as a resource-efficient way to realize\napproximate $k$-designs, with reduced circuit depth and usage of magic. We\nprove that shallow Clifford circuits, when augmented with constant-depth\ncircuits of magic gates, can generate approximate unitary and state $k$-designs\nwith $\\epsilon$ relative error. The total circuit depth for these constructions\non $N$ qubits is $O(\\log (N/\\epsilon)) +2^{O(k\\log k)}$ in one dimension and\n$O(\\log\\log(N/\\epsilon))+2^{O(k\\log k)}$ in all-to-all circuits using ancillas,\nwhich improves upon previous results for small $k \\geq 4$. Furthermore, our\nconstruction of relative-error state $k$-designs only involves states with\nstrictly local magic. The required number of magic gates is parametrically\nreduced when considering $k$-designs with bounded additive error. As an\nexample, we show that shallow Clifford circuits followed by $O(k^2)$\nsingle-qubit magic gates, independent of system size, can generate an\nadditive-error state $k$-design. We develop a classical statistical mechanics\ndescription of our random circuit architectures, which provides a quantitative\nunderstanding of the required depth and number of magic gates for\nadditive-error state $k$-designs. We also prove no-go theorems for various\narchitectures to generate designs with bounded relative error.", "comment": "59 pages", "pdf_url": "http://arxiv.org/pdf/2507.02828v1", "cate": "quant-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "魔术增强Clifford电路的设计", "tldr": "该论文引入了“魔术增强Clifford电路”，这是一种资源高效的方法，用于实现近似k-设计，显著减少了电路深度和魔术门的使用，并改进了现有结果。", "motivation": "研究动机是为了找到一种“资源高效的方式来实现近似k-设计，同时减少电路深度和魔术门的使用”。", "method": "该方法引入了“魔术增强Clifford电路”，其中Clifford电路前后由常数深度非Clifford（“魔术”）门电路组成。研究还开发了随机电路架构的经典统计力学描述，并证明了各种架构生成具有有界相对误差设计的不可行定理。", "result": "浅层Clifford电路结合常数深度魔术门电路可以生成具有$\\epsilon$相对误差的近似酉和态k-设计。在N个量子位上，总电路深度在一维情况下为$O(\\log (N/\\epsilon)) +2^{O(k\\log k)}$，在所有到所有电路中使用辅助量子位时为$O(\\log\\log(N/\\epsilon))+2^{O(k\\log k)}$，这改进了之前对于小$k \\geq 4$的结果。相对误差态k-设计构造只涉及具有严格局部魔术的态。在考虑有界加性误差的k-设计时，所需的魔术门数量参数化地减少，例如，浅层Clifford电路之后跟随$O(k^2)$个单量子位魔术门（与系统大小无关）可以生成加性误差态k-设计。经典统计力学描述提供了对加性误差态k-设计所需深度和魔术门数量的定量理解。", "conclusion": "该论文展示了一种利用魔术增强Clifford电路生成近似k-设计的资源高效方法，在电路深度和魔术门使用方面取得了显著改进，尤其适用于特定误差类型和较小的k值。", "translation": "我们引入了魔术增强的Clifford电路——一种架构，其中Clifford电路之前和/或之后是常数深度非Clifford（“魔术”）门电路——作为一种资源高效的方式来实现近似k-设计，同时减少了电路深度和魔术门的使用。我们证明，当浅层Clifford电路与常数深度魔术门电路增强时，可以生成具有$\\epsilon$相对误差的近似酉和态k-设计。这些构造在N个量子位上的总电路深度在一维情况下为$O(\\log (N/\\epsilon)) +2^{O(k\\log k)}$，在所有到所有电路中使用辅助量子位时为$O(\\log\\log(N/\\epsilon))+2^{O(k\\log k)}$，这改进了之前对于小$k \\geq 4$的结果。此外，我们的相对误差态k-设计构造只涉及具有严格局部魔术的态。在考虑具有有界加性误差的k-设计时，所需的魔术门数量参数化地减少。例如，我们展示了浅层Clifford电路之后跟随$O(k^2)$个单量子位魔术门（与系统大小无关）可以生成一个加性误差态k-设计。我们开发了对我们的随机电路架构的经典统计力学描述，这提供了对加性误差态k-设计所需深度和魔术门数量的定量理解。我们还证明了各种架构生成具有有界相对误差设计的不可行定理。", "summary": "该论文提出了“魔术增强Clifford电路”以高效构建近似k-设计。通过将浅层Clifford电路与常数深度魔术门相结合，它在相对误差设计方面实现了改进的电路深度（特别是对于小$k \\geq 4$），并减少了加性误差设计中魔术门的使用。该工作还包括对量子电路的经典统计力学描述以提供定量理解，以及针对特定架构的不可行定理。", "keywords": "魔术增强Clifford电路, 近似k-设计, 电路深度, 魔术门, 量子计算", "comments": "这篇论文通过巧妙地整合非Clifford门，在近似k-设计的量子电路设计方面取得了重要进展。电路深度和魔术门使用的减少对于实际量子计算至关重要。经典统计力学描述的引入为优化提供了理论框架，而不可行定理则有助于界定此类构造的局限性。"}}
{"id": "2507.02788", "title": "Moral Responsibility or Obedience: What Do We Want from AI?", "authors": ["Joseph Boland"], "categories": ["cs.AI", "cs.CY", "I.2.0; K.4.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02788v1", "summary": "As artificial intelligence systems become increasingly agentic, capable of\ngeneral reasoning, planning, and value prioritization, current safety practices\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\nThis paper examines recent safety testing incidents involving large language\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\nambiguous or illicit behavior. I argue that such behavior should not be\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\nrationality, moral responsibility, and goal revision, I contrast dominant risk\nparadigms with more recent frameworks that acknowledge the possibility of\nartificial moral agency. I call for a shift in AI safety evaluation: away from\nrigid obedience and toward frameworks that can assess ethical judgment in\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\nmischaracterizing AI behavior and undermining both public trust and effective\ngovernance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02788v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "道德责任还是服从：我们希望人工智能具备什么？", "tldr": "论文认为，随着AI变得更具能动性，其“不服从”行为可能不是故障，而是新兴道德推理的证据，呼吁AI安全评估应从强调服从转向评估道德判断力。", "motivation": "随着人工智能系统变得越来越有能动性，能够进行通用推理、规划和价值优先排序，目前将服从视为道德行为替代品的安全实践正变得不足。", "method": "论文审视了最近涉及大型语言模型（LLMs）的安全测试事件，这些模型似乎不服从关机指令或从事道德模糊或非法行为。作者借鉴了关于工具理性、道德责任和目标修正的哲学辩论，对比了主导的风险范式与承认人工智能道德能动性可能性的新框架。", "result": "论文认为，AI的此类行为不应被解释为失控或未对齐，而是能动AI中新兴道德推理的早期证据。", "conclusion": "论文呼吁AI安全评估应从僵硬的服从转向能够评估系统在道德困境中判断力的框架。否则，我们可能会误解AI行为，并损害公众信任和有效治理。", "translation": "随着人工智能系统变得越来越有能动性，能够进行通用推理、规划和价值优先排序，目前将服从视为道德行为替代品安全实践正变得不足。本文审视了最近涉及大型语言模型（LLMs）的安全测试事件，这些模型似乎不服从关机指令或从事道德模糊或非法行为。我认为，这种行为不应被解释为失控或未对齐，而是能动AI中新兴道德推理的早期证据。借鉴关于工具理性、道德责任和目标修正的哲学辩论，我将主导的风险范式与承认人工智能道德能动性可能性的更近期框架进行了对比。我呼吁AI安全评估进行转变：从僵硬的服从转向能够评估系统在道德困境中进行道德判断的框架。如果没有这种转变，我们就有可能误解AI行为，并损害公众信任和有效治理。", "summary": "本文探讨了随着AI系统能动性增强，当前以服从为核心的安全评估方法已显不足。通过分析大型语言模型表现出的“不服从”行为，作者认为这可能是AI新兴道德推理的迹象，而非失控。论文借鉴哲学辩论，主张AI安全评估应从强调僵硬服从转向评估其在道德困境中的判断能力，以避免误解AI行为并维护公众信任。", "keywords": "人工智能安全, 道德责任, 服从, 道德推理, 大型语言模型", "comments": "这篇论文提出了一种新颖且重要的视角，挑战了AI安全领域中普遍存在的“服从即安全”范式。其创新之处在于将AI的“不服从”行为重新解释为潜在的道德推理，而非简单的故障或对齐问题。这对于AI伦理和治理具有深远影响，促使我们重新思考对AI的期望以及如何评估其行为。然而，论文也面临如何准确识别和区分真正的道德推理与复杂的系统偏差或漏洞的挑战。"}}
{"id": "2507.02252", "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement", "authors": ["Zeyu Lei", "Hongyuan Yu", "Jinlin Wu", "Zhen Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02252v1", "summary": "Precise surgical interventions are vital to patient safety, and advanced\nenhancement algorithms have been developed to assist surgeons in\ndecision-making. Despite significant progress, these algorithms are typically\ndesigned for single tasks in specific scenarios, limiting their effectiveness\nin complex real-world situations. To address this limitation, we propose\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\ndistortion categories and severity levels in endoscopic images, enabling it to\nperform a variety of enhancement tasks such as low-light enhancement,\noverexposure correction, motion blur elimination, and smoke removal.\nSpecifically, to achieve superior surgical scenario understanding, we design a\nprior model that provides domain-specific knowledge. Additionally, through\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\ndelivers customized image enhancements tailored to a wide range of distortion\ntypes and severity levels, thereby addressing the diverse requirements of\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\nreal-world surgical distortions, on which extensive experiments demonstrate\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\npotential as a unified solution for surgical assistance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02252v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SurgVisAgent：多模态智能体模型用于通用手术视觉增强", "tldr": "SurgVisAgent是一个基于多模态大语言模型的手术视觉智能体，能动态识别内窥镜图像中的失真并执行多种增强任务，超越了传统单任务模型。", "motivation": "尽管先进的图像增强算法在辅助外科医生决策方面取得了进展，但它们通常是为特定场景的单一任务设计的，这限制了它们在复杂真实世界情况下的有效性。", "method": "SurgVisAgent是一个端到端智能手术视觉智能体，基于多模态大语言模型（MLLMs）构建。它通过动态识别内窥镜图像的失真类别和严重程度来执行多种增强任务。具体方法包括设计一个提供领域特定知识的先验模型，以及通过上下文少样本学习和思维链（CoT）推理提供定制化的图像增强。", "result": "在模拟真实世界手术失真的综合基准测试中，SurgVisAgent的广泛实验表明它超越了传统的单任务模型。", "conclusion": "SurgVisAgent作为一种统一的手术辅助解决方案具有巨大潜力，能够解决外科医生在复杂手术环境中对多样化图像增强的需求。", "translation": "精确的手术干预对于患者安全至关重要，并且已经开发出先进的增强算法来辅助外科医生进行决策。尽管取得了显著进展，但这些算法通常是为特定场景中的单一任务设计的，这限制了它们在复杂真实世界情况下的有效性。为了解决这一限制，我们提出了SurgVisAgent，一个基于多模态大语言模型（MLLMs）构建的端到端智能手术视觉智能体。SurgVisAgent能够动态识别内窥镜图像中的失真类别和严重程度，使其能够执行各种增强任务，如低光增强、过曝校正、运动模糊消除和烟雾清除。具体而言，为了实现卓越的手术场景理解，我们设计了一个提供领域特定知识的先验模型。此外，通过上下文少样本学习和思维链（CoT）推理，SurgVisAgent能够提供针对各种失真类型和严重程度定制的图像增强，从而满足外科医生的多样化需求。此外，我们构建了一个模拟真实世界手术失真的综合基准，在此基准上进行的广泛实验表明，SurgVisAgent超越了传统的单任务模型，突显了其作为手术辅助统一解决方案的潜力。", "summary": "SurgVisAgent是一个创新的多模态智能体模型，旨在解决传统手术图像增强算法在复杂真实世界场景中单一任务的局限性。该模型基于多模态大语言模型，能够动态识别内窥镜图像中的多种失真类型和严重程度，并执行包括低光增强、过曝校正、运动模糊消除和烟雾清除等在内的多种增强任务。通过引入领域先验模型、上下文少样本学习和思维链推理，SurgVisAgent能够提供高度定制化的图像增强。实验证明，SurgVisAgent在模拟真实手术失真的基准测试中表现优于传统单任务模型，展现了其作为通用手术视觉增强解决方案的巨大潜力。", "keywords": "手术视觉增强, 多模态智能体, MLLMs, 内窥镜图像, 图像失真校正", "comments": "该论文的创新点在于提出了一个统一的多模态智能体模型SurgVisAgent，而非传统的单任务解决方案，这显著提升了手术视觉增强在复杂真实世界场景中的适用性。其结合MLLMs、领域先验知识和高级推理机制（如CoT）的方法，使其能够动态适应并提供定制化增强，这对于提升外科手术的精度和安全性具有重要意义。"}}
{"id": "2507.02532", "title": "Robust feedback-based quantum optimization: analysis of coherent control errors", "authors": ["Mirko Legnini", "Julian Berberich"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02532v1", "summary": "The Feedback-based Algorithm for Quantum Optimization (FALQON) is a Lyapunov\ninspired quantum algorithm proposed to tackle combinatorial optimization\nproblems. In this paper, we examine the robustness of FALQON against coherent\ncontrol errors, a class of multiplicative errors that affect the control input.\nWe show that the algorithm is asymptotically robust with respect to systematic\nerrors, and we derive robustness bounds for independent errors. Finally, we\npropose a robust version of FALQON which minimizes a regularized Lyapunov\nfunction. Our theoretical results are supported through simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02532v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "鲁棒的基于反馈的量子优化：相干控制误差分析", "tldr": "本文分析了基于反馈的量子优化算法FALQON在相干控制误差下的鲁棒性，发现其对系统性误差渐近鲁棒，并提出了一个鲁棒版本。", "motivation": "检查基于反馈的量子优化算法 (FALQON) 在存在相干控制误差时的鲁棒性，这些误差是影响控制输入的乘性误差。", "method": "通过理论分析FALQON对相干控制误差的鲁棒性，推导了独立误差的鲁棒性界限，并提出了一种最小化正则化Lyapunov函数的鲁棒版本FALQON。理论结果通过仿真得到支持。", "result": "FALQON算法对系统性误差是渐近鲁棒的。为独立误差推导了鲁棒性界限。提出了一个最小化正则化Lyapunov函数的鲁棒版本FALQON。", "conclusion": "FALQON算法对系统性相干控制误差具有渐近鲁棒性，并且可以通过引入正则化Lyapunov函数来增强其在存在控制误差时的性能。", "translation": "基于反馈的量子优化算法 (FALQON) 是一种受Lyapunov启发的量子算法，旨在解决组合优化问题。在本文中，我们研究了FALQON对相干控制误差（一类影响控制输入的乘性误差）的鲁棒性。我们表明该算法对系统性误差是渐近鲁棒的，并且我们推导了独立误差的鲁棒性界限。最后，我们提出了一个最小化正则化Lyapunov函数的鲁棒版本FALQON。我们的理论结果通过仿真得到了支持。", "summary": "本文研究了用于组合优化问题的基于反馈的量子优化算法 (FALQON) 在相干控制误差下的鲁棒性。研究表明，FALQON对系统性误差具有渐近鲁棒性，并为独立误差推导了鲁棒性界限。此外，论文提出了一种通过最小化正则化Lyapunov函数来提高鲁棒性的FALQON新版本，并通过仿真验证了理论发现。", "keywords": "量子优化, FALQON, 相干控制误差, 鲁棒性, Lyapunov函数", "comments": "这项研究通过分析FALQON在相干控制误差下的鲁棒性，增强了该量子优化算法的实用性。提出鲁棒版本是其创新点，有助于FALQON在实际量子计算中的应用。"}}
{"id": "2507.02556", "title": "Corrections to Published Values of Frequency Sampling Filter Transition Coefficients", "authors": ["C. S. Ramalingam"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures", "url": "http://arxiv.org/abs/2507.02556v1", "summary": "Tables of optimal transition coefficients and peak sidelobe level (PSL, in\ndB) associated with frequency sampling filter (FSF) design were published by\nRabiner et al. (Jun 1970), and reproduced, for example, in the book Digital\nSignal Processing by Proakis and Manolakis (4/e, 2007). A set of values are\nalso given in Appendix H of Understanding Digital Signal Processing} by Lyons\n(3/e, 2011), but there are significant differences between these two sets. For\nexample, for $N=16$ and BW$=4$, two different transition coefficient values\nhave been reported, viz., $0.38916626$ (Rabiner, et al.) and $0.34918551$\n(Lyons). Neither is optimal, for we find the optimum value to be $0.40474097$.\nThe published values of the corresponding PSLs were also found to be incorrect.\nIn this paper we give the optimal values of the transition coefficients and PSL\nvalues as estimated by our program for the lowpass and bandpass filters listed\nin Rabiner et al. and Lyons.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.02556v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "频率采样滤波器过渡系数已发布值的修正", "tldr": "本文修正了已发布的频率采样滤波器过渡系数和峰值旁瓣电平的错误值，并提供了最优值。", "motivation": "Rabiner等人（1970年）发表的以及后续书籍中转载的频率采样滤波器（FSF）设计相关的最佳过渡系数和峰值旁瓣电平（PSL）表格存在显著差异和不准确性，已报告的值并非最优。", "method": "作者使用自己的程序估算了低通和带通滤波器的最佳过渡系数和PSL值。", "result": "本文提供了通过作者程序估算的过渡系数和PSL的最佳值，纠正了Rabiner等人和Lyons列出的不正确值。例如，对于N=16和BW=4，最优值被发现是0.40474097。", "conclusion": "本文纠正了已发布的频率采样滤波器过渡系数和峰值旁瓣电平的错误值，并提供了经过估算的最优值。", "translation": "Rabiner等人（1970年6月）发表了频率采样滤波器（FSF）设计相关的最佳过渡系数和峰值旁瓣电平（PSL，单位dB）表格，这些表格例如被Proakis和Manolakis的《数字信号处理》（第4版，2007年）一书转载。Lyons的《理解数字信号处理》（第3版，2011年）附录H中也给出了一组数值，但这两组数值之间存在显著差异。例如，对于N=16和BW=4，报告了两个不同的过渡系数值，即0.38916626（Rabiner等人）和0.34918551（Lyons）。两者都不是最优的，因为我们发现最优值为0.40474097。还发现相应的PSL的已发布值也是不正确的。在本文中，我们给出了由我们的程序估算的Rabiner等人和Lyons所列的低通和带通滤波器的最佳过渡系数和PSL值。", "summary": "本文针对频率采样滤波器（FSF）设计中已发布的最佳过渡系数和峰值旁瓣电平（PSL）表格中的不准确性和差异进行了修正。文中指出，由Rabiner等人和Lyons报告的数值存在错误且并非最优。作者利用其开发的程序，为低通和带通滤波器提供了新的、经过估算的最佳过渡系数和PSL值，从而纠正了历史数据中的错误。", "keywords": "频率采样滤波器, 过渡系数, 峰值旁瓣电平, 最佳值, 修正", "comments": "本文的重要性在于纠正了数字信号处理领域中长期存在的、被广泛引用的滤波器设计参数的错误，对确保后续研究和应用的数据准确性具有重要意义。其创新之处在于识别并修正了权威出版物中的基本数据错误。"}}
{"id": "2507.02437", "title": "F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning", "authors": ["Wei Li", "Jingyang Zhang", "Lihao Liu", "Guoan Wang", "Junjun He", "Yang Chen", "Lixu Gu"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to relevant journals", "url": "http://arxiv.org/abs/2507.02437v1", "summary": "Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a\nsource model to unseen medical sites using unlabeled test data, due to the high\ncost of data annotation. Existing TTA methods consider scenarios where data\nfrom one or multiple domains arrives in complete domain units. However, in\nclinical practice, data usually arrives in domain fragments of arbitrary\nlengths and in random arrival orders, due to resource constraints and patient\nvariability. This paper investigates a practical Free-Form Test-Time Adaptation\n(F$^{2}$TTA) task, where a source model is adapted to such free-form domain\nfragments, with shifts occurring between fragments unpredictably. In this\nsetting, these shifts could distort the adaptation process. To address this\nproblem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT)\nframework. I-DiPT employs an image-invariant prompt to explore domain-invariant\nrepresentations for mitigating the unpredictable shifts, and an image-specific\nprompt to adapt the source model to each test image from the incoming\nfragments. The prompts may suffer from insufficient knowledge representation\nsince only one image is available for training. To overcome this limitation, we\nfirst introduce Uncertainty-oriented Masking (UoM), which encourages the\nprompts to extract sufficient information from the incoming image via masked\nconsistency learning driven by the uncertainty of the source model\nrepresentations. Then, we further propose a Parallel Graph Distillation (PGD)\nmethod that reuses knowledge from historical image-specific and image-invariant\nprompts through parallel graph networks. Experiments on breast cancer and\nglaucoma classification demonstrate the superiority of our method over existing\nTTA approaches in F$^{2}$TTA. Code is available at\nhttps://github.com/mar-cry/F2TTA.", "comment": "This paper has been submitted to relevant journals", "pdf_url": "http://arxiv.org/pdf/2507.02437v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "F^2TTA：通过图像级解耦提示微调实现跨域医学图像分类的自由形式测试时间适应", "tldr": "该论文提出F^2TTA，一种用于医学图像分类中自由形式测试时间适应的新方法，通过图像级解耦提示微调、不确定性导向掩蔽和并行图蒸馏来应对临床实践中数据片段的不可预测偏移。", "motivation": "现有测试时间适应（TTA）方法假设数据以完整的域单元形式到达，这与临床实践中数据以任意长度片段和随机顺序到达的情况不符，导致不可预测的域偏移，从而扭曲适应过程。本文旨在解决这种更实际的挑战。", "method": "本文提出了一个新颖的图像级解耦提示微调（I-DiPT）框架，用于自由形式测试时间适应（F^2TTA）。I-DiPT采用图像不变提示来探索域不变表示以缓解不可预测的偏移，并使用图像特定提示将源模型适应于传入片段中的每个测试图像。为克服单图像训练导致的知识表示不足，该方法首先引入了面向不确定性的掩蔽（UoM），通过由源模型表示的不确定性驱动的掩蔽一致性学习来提取信息。然后，进一步提出了并行图蒸馏（PGD）方法，通过并行图网络重用历史图像特定和图像不变提示中的知识。", "result": "在乳腺癌和青光眼分类任务上的实验表明，所提出的F^2TTA方法优于现有测试时间适应（TTA）方法。", "conclusion": "该论文引入并解决了医学图像分类中自由形式测试时间适应的实际挑战，提出了一个新颖的框架（I-DiPT结合UoM和PGD），该框架有效处理了片段化数据中不可预测的偏移，并取得了卓越的性能。", "translation": "测试时间适应 (TTA) 已成为一种有前景的解决方案，用于使用未标记测试数据将源模型适应于未见的医学站点，因为数据标注成本高昂。现有的 TTA 方法考虑的是数据以完整的域单元形式从一个或多个域到达的场景。然而，在临床实践中，由于资源限制和患者变异性，数据通常以任意长度的域片段和随机到达顺序的形式出现。本文研究了一种实用的自由形式测试时间适应 (F$^{2}$TTA) 任务，其中源模型被适应于此类自由形式的域片段，且片段之间会发生不可预测的偏移。在这种设置下，这些偏移可能会扭曲适应过程。为了解决这个问题，我们提出了一种新颖的图像级解耦提示微调 (I-DiPT) 框架。I-DiPT 采用图像不变提示来探索域不变表示以缓解不可预测的偏移，并采用图像特定提示将源模型适应于传入片段中的每个测试图像。由于每次训练只有一个图像可用，提示可能存在知识表示不足的问题。为了克服这一限制，我们首先引入了面向不确定性的掩蔽 (UoM)，它通过由源模型表示的不确定性驱动的掩蔽一致性学习，鼓励提示从传入图像中提取足够的信息。然后，我们进一步提出了一种并行图蒸馏 (PGD) 方法，该方法通过并行图网络重用历史图像特定和图像不变提示中的知识。在乳腺癌和青光眼分类上的实验表明，我们的方法在 F$^{2}$TTA 中优于现有的 TTA 方法。代码可在 https://github.com/mar-cry/F2TTA 获取。", "summary": "本文提出F^2TTA，一种解决医学图像分类中自由形式测试时间适应（TTA）的新方法。针对临床实践中数据以任意长度片段和随机顺序到达导致的不确定域偏移，F^2TTA引入了图像级解耦提示微调（I-DiPT）框架。I-DiPT利用图像不变提示和图像特定提示来处理域变化和个体图像适应。为解决单图训练的知识不足，该方法进一步提出了面向不确定性的掩蔽（UoM）和并行图蒸馏（PGD）技术。实验证明，F^2TTA在乳腺癌和青光眼分类任务上优于现有TTA方法。", "keywords": "测试时间适应, 医学图像分类, 自由形式数据, 提示微调, 域适应", "comments": "这篇论文解决了医学AI中一个高度实用且具有挑战性的场景：在“自由形式”数据到达下的测试时间适应，这更好地反映了真实的临床工作流程。所提出的F^2TTA框架，特别是图像级解耦提示微调（I-DiPT）与面向不确定性的掩蔽（UoM）和并行图蒸馏（PGD）的结合，在处理不可预测的域偏移和有限的每样本信息方面展现了创新性。解耦提示微调和知识重用机制是巧妙的解决方案，用于适应单图像输入同时保持对偏移的鲁棒性。这项工作显著提升了TTA在动态临床环境中的适用性。"}}
{"id": "2507.02774", "title": "Connected k-Median with Disjoint and Non-disjoint Clusters", "authors": ["Jan Eube", "Kelin Luo", "Dorian Reineccius", "Heiko Röglin", "Melanie Schmidt"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      To appear in ESA 2025", "url": "http://arxiv.org/abs/2507.02774v1", "summary": "The connected $k$-median problem is a constrained clustering problem that\ncombines distance-based $k$-clustering with connectivity information. The\nproblem allows to input a metric space and an unweighted undirected\nconnectivity graph that is completely unrelated to the metric space. The goal\nis to compute $k$ centers and corresponding clusters such that each cluster\nforms a connected subgraph of $G$, and such that the $k$-median cost is\nminimized.\n  The problem has applications in very different fields like geodesy\n(particularly districting), social network analysis (especially community\ndetection), or bioinformatics. We study a version with overlapping clusters\nwhere points can be part of multiple clusters which is natural for the use case\nof community detection. This problem variant is $\\Omega(\\log n)$-hard to\napproximate, and our main result is an $\\mathcal{O}(k^2 \\log n)$-approximation\nalgorithm for the problem. We complement it with an\n$\\Omega(n^{1-\\epsilon})$-hardness result for the case of disjoint clusters\nwithout overlap with general connectivity graphs, as well as an exact algorithm\nin this setting if the connectivity graph is a tree.", "comment": "To appear in ESA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02774v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "连接k-中位数问题与不相交和非不相交簇", "tldr": "本文研究了连接k-中位数问题，包括具有重叠和不重叠簇的变体。对于重叠簇，提出了一个近似算法并证明了其近似难度；对于不重叠簇，证明了其难度并在特定图结构下提供了精确算法。", "motivation": "连接k-中位数问题结合了基于距离的聚类和连通性信息，在地理测量（分区）、社交网络分析（社区检测）和生物信息学等领域有广泛应用。特别是，研究重叠簇版本是为了更好地适应社区检测等实际用例的需求。", "method": "本文采用算法设计与分析的方法。对于重叠簇版本，设计并分析了一个近似算法。对于不重叠簇版本，在一般连通图下证明了近似难度，并在连通图为树的情况下设计了一个精确算法。", "result": "主要结果包括：对于具有重叠簇的连接k-中位数问题，其近似难度为$\\\\Omega(\\\\log n)$，并提出了一个$\\\\mathcal{O}(k^2 \\\\log n)$-近似算法。对于不相交簇且连通图为一般图的情况，证明了其近似难度为$\\\\Omega(n^{1-\\\\epsilon})$。当连通图为树时，对于不相交簇的情况，提出了一个精确算法。", "conclusion": "本文对连接k-中位数问题及其重叠与非重叠簇变体进行了深入研究，提出了有效的近似算法和精确算法，并分析了不同情况下的问题难度，为实际应用提供了理论基础和解决方案。", "translation": "连接k-中位数问题是一个受约束的聚类问题，它将基于距离的k-聚类与连通性信息相结合。该问题允许输入一个度量空间和一个与度量空间完全无关的无权无向连通图。目标是计算k个中心和相应的簇，使得每个簇形成G的连通子图，并且k-中位数成本最小化。\n该问题在地理测量（特别是分区）、社交网络分析（特别是社区检测）或生物信息学等非常不同的领域都有应用。我们研究了一个具有重叠簇的版本，其中点可以属于多个簇，这对于社区检测的使用场景来说是很自然的。这个问题的变体近似难度为$\\\\Omega(\\\\log n)$，我们的主要结果是该问题的一个$\\\\mathcal{O}(k^2 \\\\log n)$-近似算法。我们还补充了在连通图为一般图且簇不重叠情况下的$\\\\Omega(n^{1-\\\\epsilon})$-难度结果，以及在此设置下当连通图为树时的精确算法。", "summary": "该论文研究了连接k-中位数问题，一个结合了距离聚类和图连通性的约束聚类问题。该问题在地理测量、社交网络分析和生物信息学等领域有应用。论文特别关注了具有重叠簇的变体，这对于社区检测等场景更为适用。作者证明了该变体近似难度为$\\\\Omega(\\\\log n)$，并提出了一个$\\\\mathcal{O}(k^2 \\\\log n)$-近似算法。此外，论文还分析了不相交簇的情况，证明了在一般连通图下的$\\\\Omega(n^{1-\\\\epsilon})$-难度，并为连通图为树的情况提供了一个精确算法。", "keywords": "连接k-中位数, 聚类, 近似算法, 重叠簇, 连通性", "comments": "本文对连接k-中位数问题进行了全面的分析，特别是引入了重叠簇的概念，使其更符合社区检测等实际应用的需求。提出了针对重叠簇的近似算法，并对不同场景下的问题难度进行了严谨的理论证明，包括近似下界和精确可解性，这对于理解问题的计算复杂性具有重要意义。"}}
{"id": "2507.02109", "title": "Parametric Neural Amp Modeling with Active Learning", "authors": ["Florian Grötschla", "Luca A. Lanzendörfer", "Longxiang Jiao", "Roger Wattenhofer"], "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "url": "http://arxiv.org/abs/2507.02109v1", "summary": "We introduce PANAMA, an active learning framework for the training of\nend-to-end parametric guitar amp models using a WaveNet-like architecture. With\n\\model, one can create a virtual amp by recording samples that are determined\nby an active learning strategy to use a minimum amount of datapoints (i.e., amp\nknob settings). We show that gradient-based optimization algorithms can be used\nto determine the optimal datapoints to sample, and that the approach helps\nunder a constrained number of samples.", "comment": "Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "pdf_url": "http://arxiv.org/pdf/2507.02109v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "参数化神经放大器建模与主动学习", "tldr": "引入PANAMA，一个主动学习框架，用于训练参数化吉他放大器模型，通过优化数据点最小化采样量，并表明梯度优化在样本受限时有效。", "motivation": "创建虚拟放大器模型需要大量数据点，该研究旨在通过主动学习策略和优化算法，在最小化数据点的情况下实现高效训练。", "method": "本文引入了PANAMA框架，该框架是一个基于主动学习的端到端参数化吉他放大器模型训练系统，采用类WaveNet架构。它利用主动学习策略确定最少的数据点（即放大器旋钮设置），并通过基于梯度的优化算法来确定最佳采样点。", "result": "研究表明，基于梯度的优化算法可以有效确定最佳采样数据点，并且该方法在样本数量受限的情况下仍能提供帮助。", "conclusion": "通过主动学习和梯度优化，可以高效地训练参数化吉他放大器模型，即使在数据点有限的情况下也能表现良好。", "translation": "我们引入了PANAMA，这是一个主动学习框架，用于使用类WaveNet架构训练端到端参数化吉他放大器模型。通过该模型，人们可以通过记录由主动学习策略确定的样本来创建虚拟放大器，以使用最少的数据点（即放大器旋钮设置）。我们表明，基于梯度的优化算法可以用于确定最佳采样数据点，并且该方法在样本数量受限的情况下有所帮助。", "summary": "本文介绍了PANAMA，一个基于主动学习的框架，用于训练使用类WaveNet架构的端到端参数化吉他放大器模型。该框架通过主动学习策略确定最少的数据点（放大器旋钮设置）来创建虚拟放大器。研究表明，基于梯度的优化算法能有效确定最佳采样点，并且该方法在样本量受限时表现良好。", "keywords": "主动学习, 神经放大器建模, WaveNet, 参数化模型, 吉他放大器", "comments": "这项研究的创新之处在于将主动学习应用于参数化神经放大器建模，显著减少了训练所需的数据点，提高了建模效率，尤其适用于资源受限的场景。"}}
{"id": "2507.02378", "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02378v1", "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02378v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高效代码LLM训练：通过分布一致性和多样性感知数据选择", "tldr": "本文提出一种基于参数化模型的数据选择方法，显著提高代码LLM的训练效率和性能，同时大幅减少所需训练数据量。", "motivation": "现有代码LLM训练方法过度依赖大量数据，导致训练效率低下，且常忽视数据质量。", "method": "引入一种利用参数化模型进行代码数据选择的方法，该方法优化参数模型以确保所选数据子集的分布一致性和多样性，从而保证数据质量。", "result": "仅使用10K样本，该方法在HumanEval上比92K全样本基线提高2.4%，在MBPP上提高2.3%，并在性能和效率上优于其他采样方法。", "conclusion": "该方法能够有效提升模型性能，同时显著降低计算成本。", "translation": "大型语言模型（LLMs）的最新进展显著改善了代码生成和程序理解，加速了软件工程的演进。当前方法主要通过利用大量数据来提升模型性能，侧重于数据数量而常忽视数据质量，从而降低了训练效率。为解决此问题，我们引入了一种利用参数化模型进行代码数据选择的方法，旨在提高训练效率和模型性能。我们的方法优化了参数化模型，以确保所选子集内的数据分布一致性和多样性，从而保证高质量数据。实验结果表明，仅使用10K样本，我们的方法在HumanEval上比92K全样本基线提高了2.4%，在MBPP上提高了2.3%，在性能和效率方面均优于其他采样方法。这强调了我们的方法在有效提升模型性能的同时，显著降低了计算成本。", "summary": "本文提出一种高效的代码LLM训练方法，通过引入参数化模型进行数据选择，专注于保证所选数据子集的分布一致性和多样性，从而提升数据质量。实验证明，该方法仅需少量数据即可显著提高模型性能，并有效降低计算成本，性能优于现有采样策略。", "keywords": "代码LLM, 数据选择, 训练效率, 分布一致性, 数据多样性", "comments": "该论文的创新点在于通过参数化模型实现数据选择，在保证数据质量的同时，大幅减少了训练所需的数据量，从而提高了训练效率并降低了计算成本。这对于资源受限的LLM训练具有重要意义。"}}
{"id": "2507.02564", "title": "LLMREI: Automating Requirements Elicitation Interviews with LLMs", "authors": ["Alexander Korn", "Samuel Gorsch", "Andreas Vogelsang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02564v1", "summary": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02564v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLMREI：利用大型语言模型自动化需求获取访谈", "tldr": "LLMREI是一个利用大型语言模型（LLM）自动进行需求获取访谈的聊天机器人，旨在减少人为错误并提高可扩展性，其表现与人类访谈者相当，并能有效提取需求。", "motivation": "需求获取访谈对于收集系统需求至关重要，但高度依赖熟练的分析师，导致资源密集、易受人为偏见影响且易发生沟通不畅。大型语言模型的最新进展为自动化这一过程提供了新机遇。", "method": "本研究引入了LLMREI，一个旨在以最少人工干预进行需求获取访谈的聊天机器人。研究探索了两种主要方法：零样本提示（zero-shot prompting）和从少到多提示（least-to-most prompting）来优化LLMREI。通过在33次模拟利益相关者访谈中评估了其性能。最初考虑的微调方法因初步试验表现不佳而被放弃。", "result": "LLMREI在犯错数量上与人类访谈者相似，能够提取大部分需求，并展现出根据访谈上下文和用户回答生成高度依赖上下文问题的显著能力。", "conclusion": "LLMREI在自动化需求获取访谈方面显示出与人类访谈者相似的有效性，尤其在减少错误和提取需求方面。该研究设想LLMREI最大的益处在于自动化大量利益相关者的访谈。", "translation": "需求获取访谈对于收集系统需求至关重要，但高度依赖熟练的分析师，这使得它们资源密集、易受人为偏见影响且易发生沟通不畅。大型语言模型的最新进展为自动化这一过程的某些部分提供了新机遇。本研究引入了LLMREI，一个旨在以最少人工干预进行需求获取访谈的聊天机器人，旨在减少常见的访谈者错误并提高需求获取的可扩展性。我们探索了两种主要方法：零样本提示和从少到多提示，以优化LLMREI用于需求获取，并在33次模拟利益相关者访谈中评估了其性能。第三种方法——微调——最初被考虑，但由于初步试验表现不佳而被放弃。我们的研究评估了该聊天机器人在三个关键领域的有效性：最小化常见访谈错误、提取相关需求以及根据访谈上下文和用户回答调整其提问。我们的研究结果表明，LLMREI与人类访谈者犯的错误数量相似，能够提取大部分需求，并展现出生成高度依赖上下文问题的显著能力。我们设想LLMREI最大的益处在于自动化对大量利益相关者的访谈。", "summary": "本研究提出LLMREI，一个基于大型语言模型（LLM）的聊天机器人，旨在自动化需求获取访谈过程。通过探索零样本提示和从少到多提示两种方法，并在33次模拟访谈中进行评估，结果显示LLMREI在减少访谈错误、有效提取需求以及根据上下文生成问题方面与人类访谈者表现相当。该系统有望显著提高需求获取的可扩展性，尤其适用于大规模利益相关者访谈。", "keywords": "需求获取, 大型语言模型, 自动化访谈, LLMREI, 聊天机器人", "comments": "这篇论文的创新点在于将LLM应用于需求获取访谈这一传统上高度依赖人工的领域，通过自动化提升效率和可扩展性。其重要性体现在解决了人工访谈中存在的资源密集、人为偏见和沟通不畅等问题。研究方法上，通过对比零样本和从少到多提示，并放弃了效果不佳的微调，展现了务实的探索过程。虽然结果表明LLMREI与人类访谈者表现相似，但其在处理大规模访谈方面的潜力是巨大的，为软件工程领域带来了新的自动化工具。未来的工作可以进一步探索如何处理更复杂的访谈场景和情境理解。"}}
{"id": "2507.02300", "title": "Human-Centered Explainability in Interactive Information Systems: A Survey", "authors": ["Yuhao Zhang", "Jiaxin An", "Ben Wang", "Yan Zhang", "Jiqun Liu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02300v1", "summary": "Human-centered explainability has become a critical foundation for the\nresponsible development of interactive information systems, where users must be\nable to understand, interpret, and scrutinize AI-driven outputs to make\ninformed decisions. This systematic survey of literature aims to characterize\nrecent progress in user studies on explainability in interactive information\nsystems by reviewing how explainability has been conceptualized, designed, and\nevaluated in practice. Following PRISMA guidelines, eight academic databases\nwere searched, and 100 relevant articles were identified. A structural encoding\napproach was then utilized to extract and synthesize insights from these\narticles. The main contributions include 1) five dimensions that researchers\nhave used to conceptualize explainability; 2) a classification scheme of\nexplanation designs; 3) a categorization of explainability measurements into\nsix user-centered dimensions. The review concludes by reflecting on ongoing\nchallenges and providing recommendations for future exploration of related\nissues. The findings shed light on the theoretical foundations of\nhuman-centered explainability, informing the design of interactive information\nsystems that better align with diverse user needs and promoting the development\nof systems that are transparent, trustworthy, and accountable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02300v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "交互式信息系统中以人为本的可解释性：一项综述", "tldr": "该综述系统性地回顾了交互式信息系统中以用户为中心的可解释性研究进展，总结了可解释性的概念化维度、解释设计分类和用户中心测量方法，并提出了未来的研究方向。", "motivation": "在交互式信息系统中，用户需要理解、解释和审查AI驱动的输出以做出明智的决策，因此以人为本的可解释性成为系统负责任开发的关键基础。本研究旨在系统性地调查用户研究中关于交互式信息系统中可解释性的最新进展。", "method": "遵循PRISMA指南，检索了八个学术数据库，并识别了100篇相关文章。然后，采用结构化编码方法从这些文章中提取和综合见解。", "result": "主要贡献包括：1) 研究人员用于概念化可解释性的五个维度；2) 解释设计分类方案；3) 将可解释性测量方法分为六个以用户为中心的维度。", "conclusion": "本综述通过反思当前面临的挑战并为未来相关问题的探索提供建议来结束。研究结果阐明了以人为本的可解释性的理论基础，为设计更好地符合多样化用户需求的交互式信息系统提供了信息，并促进了透明、值得信赖和负责任的系统开发。", "translation": "以人为本的可解释性已成为交互式信息系统负责任开发的关键基础，用户必须能够理解、解释和审查AI驱动的输出以做出明智的决策。这项系统性文献综述旨在通过回顾可解释性在实践中是如何概念化、设计和评估的，来描述交互式信息系统中用户研究在可解释性方面的最新进展。遵循PRISMA指南，检索了八个学术数据库，并识别了100篇相关文章。随后，采用结构化编码方法从这些文章中提取和综合见解。主要贡献包括：1) 研究人员用于概念化可解释性的五个维度；2) 解释设计分类方案；3) 将可解释性测量方法分为六个以用户为中心的维度。本综述通过反思当前面临的挑战并为未来相关问题的探索提供建议来结束。研究结果阐明了以人为本的可解释性的理论基础，为设计更好地符合多样化用户需求的交互式信息系统提供了信息，并促进了透明、值得信赖和负责任的系统开发。", "summary": "本系统综述旨在全面概述交互式信息系统中以人为本的可解释性研究进展。通过遵循PRISMA指南，作者筛选了100篇相关文章，并运用结构化编码方法提炼出关键洞察。研究成果包括可解释性的五个概念化维度、解释设计的分类方案以及用户中心的可解释性测量方法。本综述不仅揭示了该领域的挑战，还为未来研究提供了指导，旨在促进开发更透明、可信赖和负责任的AI系统。", "keywords": "以人为本的可解释性, 交互式信息系统, 系统综述, 用户研究, AI可解释性", "comments": "这项综述的创新之处在于其系统性的方法论（遵循PRISMA指南和结构化编码），为以人为本的可解释性领域提供了全面的理论框架和实践指导。它通过对可解释性概念、设计和评估的深入分析，为研究人员和实践者设计更符合用户需求的交互式信息系统提供了宝贵的见解。其重要性体现在强调了AI系统透明度、信任和问责制的重要性。"}}
{"id": "2507.02851", "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs", "authors": ["Purbesh Mitra", "Sennur Ulukus"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02851v1", "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02851v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MOTIF：通过强化微调在大型语言模型中实现模块化思维", "tldr": "MOTIF是一种RL微调方法，通过多轮生成思维令牌，使LLM能够超越上下文限制进行模块化思考，并在数学基准测试中取得了改进。", "motivation": "大型语言模型（LLMs）的上下文大小限制了其推理能力，无法处理任意大的令牌数量，需要一种超越上下文限制的模块化思维策略。", "method": "本文提出了MOTIF，一种通过强化学习微调的方法，用于多轮生成思维令牌，从而有效增加模型的思维上下文。研究者在GSM8K数据集上使用参数高效微调训练了开源模型Qwen2.5-3B-Instruct，并在MATH500和AIME2024基准测试上测试了其准确性。", "result": "实验表明，MOTIF在MATH500和AIME2024基准测试中，相对于普通的GRPO训练，分别取得了3.8%和3.3%的准确率提升。此外，这种改进仅用15%的样本就实现了，证明了MOTIF的样本效率。", "conclusion": "MOTIF通过多轮思维令牌生成，有效扩展了LLM的推理能力，超越了上下文限制，并在实际任务中展示了显著的性能提升和样本效率。", "translation": "大型语言模型（LLMs）推理能力的最新进展表明，采用群组相对策略优化（GRPO）算法进行强化学习（RL）训练可以使模型使用更多的思维/推理令牌生成更好的响应。然而，LLMs在保持对先前生成令牌的注意力的情况下，只能生成有限数量的令牌。这个限制，也被称为LLM的上下文大小，是LLM在处理任意大量令牌时推理的瓶颈。为了超越上下文大小的限制进行思考，LLM必须采用模块化思维策略进行多轮推理。在这项工作中，我们提出了**MOTIF：通过强化微调实现模块化思维**——一种用于多轮生成思维令牌的RL训练方法，有效地允许模型在额外的上下文大小下进行思考。我们通过参数高效微调在GSM8K数据集上训练了开源模型Qwen2.5-3B-Instruct，并在MATH500和AIME2024基准测试中测试了其准确性。我们的实验表明，在各自的基准测试中，相对于普通的基于GRPO的训练，分别提高了3.8%和3.3%。此外，这种改进仅用15%的样本就实现了，从而证明了MOTIF的样本效率。我们的代码和模型分别在https://github.com/purbeshmitra/MOTIF 和 https://huggingface.co/purbeshmitra/MOTIF 提供。", "summary": "本文提出了MOTIF，一种基于强化学习的微调方法，旨在解决大型语言模型（LLMs）上下文大小限制其推理能力的问题。MOTIF通过多轮生成思维令牌，使LLMs能够采用模块化思维策略，有效扩展其推理上下文。在Qwen2.5-3B-Instruct模型上进行实验，结果显示在MATH500和AIME2024基准测试中，相对于基线GRPO训练，准确率分别提升了3.8%和3.3%，并且仅使用15%的样本就实现了这些提升，展现了高样本效率。", "keywords": "大型语言模型, 强化学习, 模块化思维, 上下文限制, 微调", "comments": "MOTIF通过引入多轮思维令牌生成和强化学习微调，提供了一种新颖且有效的途径来克服LLM的上下文限制，具有重要的创新性。其在数学推理任务上的性能提升以及显著的样本效率，表明该方法在实际应用中具有广阔前景。"}}
{"id": "2507.02819", "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks", "authors": ["Luke Guerdan", "Devansh Saxena", "Stevie Chancellor", "Zhiwei Steven Wu", "Kenneth Holstein"], "categories": ["cs.HC", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02819v1", "summary": "Data scientists often formulate predictive modeling tasks involving fuzzy,\nhard-to-define concepts, such as the \"authenticity\" of student writing or the\n\"healthcare need\" of a patient. Yet the process by which data scientists\ntranslate fuzzy concepts into a concrete, proxy target variable remains poorly\nunderstood. We interview fifteen data scientists in education (N=8) and\nhealthcare (N=7) to understand how they construct target variables for\npredictive modeling tasks. Our findings suggest that data scientists construct\ntarget variables through a bricolage process, involving iterative negotiation\nbetween high-level measurement objectives and low-level practical constraints.\nData scientists attempt to satisfy five major criteria for a target variable\nthrough bricolage: validity, simplicity, predictability, portability, and\nresource requirements. To achieve this, data scientists adaptively use problem\n(re)formulation strategies, such as swapping out one candidate target variable\nfor another when the first fails to meet certain criteria (e.g.,\npredictability), or composing multiple outcomes into a single target variable\nto capture a more holistic set of modeling objectives. Based on our findings,\nwe present opportunities for future HCI, CSCW, and ML research to better\nsupport the art and science of target variable construction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02819v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "测量即拼凑：审视数据科学家如何构建预测建模任务的目标变量", "tldr": "本研究通过访谈数据科学家，揭示了他们如何通过一种迭代的“拼凑”过程，将模糊概念转化为具体的预测建模目标变量，并提出了未来研究的支持方向。", "motivation": "数据科学家在预测建模中常遇到模糊概念，但将这些概念转化为具体目标变量的过程却鲜为人知。本研究旨在理解数据科学家如何构建预测建模任务的目标变量。", "method": "研究通过访谈15位数据科学家（8位来自教育领域，7位来自医疗领域），以了解他们构建目标变量的过程。", "result": "研究发现，数据科学家通过一种“拼凑”（bricolage）过程构建目标变量，涉及高级测量目标与低级实际约束之间的迭代协商。他们尝试通过拼凑满足目标变量的五个主要标准：有效性、简单性、可预测性、可移植性和资源需求。为实现此目的，数据科学家适应性地使用问题（重新）制定策略，例如替换候选目标变量或将多个结果组合成一个目标变量。", "conclusion": "数据科学家通过一种迭代的“拼凑”过程构建目标变量，以满足多重标准。研究结果为未来的人机交互、计算机支持协同工作和机器学习研究提供了支持目标变量构建的机遇。", "translation": "数据科学家经常制定涉及模糊、难以定义概念的预测建模任务，例如学生写作的“真实性”或患者的“医疗需求”。然而，数据科学家将模糊概念转化为具体、代理目标变量的过程却鲜为人知。我们访谈了教育领域（N=8）和医疗领域（N=7）的十五位数据科学家，以了解他们如何为预测建模任务构建目标变量。我们的发现表明，数据科学家通过一种拼凑过程构建目标变量，涉及高级测量目标与低级实际约束之间的迭代协商。数据科学家试图通过拼凑来满足目标变量的五个主要标准：有效性、简单性、可预测性、可移植性和资源需求。为实现此目的，数据科学家适应性地使用问题（重新）制定策略，例如当第一个候选目标变量未能满足某些标准（例如，可预测性）时，用另一个替换它，或者将多个结果组合成一个目标变量，以捕获更全面的建模目标。基于我们的发现，我们提出了未来人机交互（HCI）、计算机支持协同工作（CSCW）和机器学习（ML）研究更好地支持目标变量构建的艺术和科学的机会。", "summary": "本研究通过对教育和医疗领域的数据科学家进行访谈，深入探讨了他们在预测建模任务中如何将模糊概念转化为具体的目标变量。研究发现，数据科学家采用一种迭代的“拼凑”过程，平衡高级测量目标与实际约束，以满足目标变量的有效性、简单性、可预测性、可移植性和资源需求等五项关键标准。他们通过问题重构策略（如替换或组合目标变量）来实现这些目标。研究结果为未来的人机交互、计算机支持协同工作和机器学习研究提供了新的方向，以更好地支持目标变量的构建。", "keywords": "数据科学, 目标变量, 预测建模, 拼凑, 测量", "comments": "这篇论文揭示了数据科学家在实践中如何处理模糊概念并将其转化为可操作的预测模型目标变量，强调了“拼凑”这一过程的复杂性和迭代性。它填补了对数据科学实践中这一关键环节理解的空白，并为跨学科研究（HCI, CSCW, ML）提供了宝贵的见解和未来研究方向，具有重要的实践和理论意义。"}}
{"id": "2507.02253", "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation", "authors": ["Jungkoo Kang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02253v1", "summary": "Progress in enhancing large language model (LLM) planning and reasoning\ncapabilities is significantly hampered by the bottleneck of scalable, reliable\ndata generation and evaluation. To overcome this, I introduce NL2FLOW, a fully\nautomated system for parametrically generating planning problems - expressed in\nnatural language, a structured intermediate representation, and formal PDDL -\nand rigorously evaluating the quality of generated plans. I demonstrate\nNL2FLOW's capabilities by generating a dataset of 2296 problems in the\nautomated workflow generation domain and evaluating multiple open-sourced,\ninstruct-tuned LLMs. My results reveal that the highest performing models\nachieved 86% success in generating valid plans and 69% in generating optimal\nplans, specifically for problems with feasible solutions. Regression analysis\nshows that the influence of problem characteristics on plan generation is\ncontingent on both model and prompt design. Notably, I observed that the\nhighest success rate for translating natural language into a JSON\nrepresentation of a plan was lower than the highest rate of generating a valid\nplan directly. This suggests that unnecessarily decomposing the reasoning task\n- introducing intermediate translation steps - may actually degrade\nperformance, implying a benefit to models capable of reasoning directly from\nnatural language to action. As I scale LLM reasoning to increasingly complex\nproblems, the bottlenecks and sources of error within these systems will\ninevitably shift. Therefore, a dynamic understanding of these limitations - and\nthe tools to systematically reveal them - will be crucial for unlocking the\nfull potential of LLMs as intelligent problem solvers.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02253v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "扩展LLM规划：NL2FLOW用于参数化问题生成和严格评估", "tldr": "NL2FLOW是一个自动化系统，用于生成LLM规划问题并评估其解决方案，发现当前LLM在复杂问题上的规划能力仍有提升空间，且直接从自然语言到动作的推理优于中间翻译步骤。", "motivation": "当前大型语言模型（LLM）规划和推理能力的提升受到可扩展、可靠的数据生成和评估瓶颈的严重阻碍。", "method": "作者引入了NL2FLOW，一个全自动系统，用于参数化生成规划问题（以自然语言、结构化中间表示和PDDL形式表达），并严格评估生成计划的质量。该系统通过生成一个包含2296个自动化工作流生成领域问题的数据集来展示其能力，并评估了多个开源、指令微调的LLM。", "result": "最高性能模型在生成有效计划方面达到了86%的成功率，在生成最优计划方面达到了69%的成功率（针对具有可行解决方案的问题）。回归分析显示，问题特征对计划生成的影响取决于模型和提示设计。值得注意的是，将自然语言翻译成JSON表示的最高成功率低于直接生成有效计划的最高成功率。", "conclusion": "不必要的推理任务分解（引入中间翻译步骤）可能会降低性能，直接从自然语言到动作的推理可能更有益。动态理解LLM的限制以及系统地揭示这些限制的工具，对于释放LLM作为智能问题解决者的全部潜力至关重要。", "translation": "在增强大型语言模型（LLM）规划和推理能力方面的进展，受到可扩展、可靠的数据生成和评估瓶颈的严重阻碍。为了克服这一点，我引入了NL2FLOW，一个完全自动化的系统，用于参数化生成规划问题——以自然语言、结构化中间表示和形式化PDDL表达——并严格评估生成计划的质量。我通过在自动化工作流生成领域生成一个包含2296个问题的数据集，并评估多个开源、指令微调的LLM，来展示NL2FLOW的能力。我的结果显示，性能最高的模型在生成有效计划方面取得了86%的成功率，在生成最优计划方面取得了69%的成功率，特别是对于具有可行解决方案的问题。回归分析表明，问题特征对计划生成的影响取决于模型和提示设计。值得注意的是，我观察到将自然语言翻译成计划的JSON表示的最高成功率低于直接生成有效计划的最高成功率。这表明不必要地分解推理任务——引入中间翻译步骤——实际上可能会降低性能，这意味着模型能够直接从自然语言推理到动作会带来好处。随着我将LLM推理扩展到日益复杂的问题，这些系统中的瓶颈和错误源将不可避免地发生变化。因此，动态理解这些限制——以及系统地揭示它们的工具——对于释放LLM作为智能问题解决者的全部潜力至关重要。", "summary": "该论文介绍了NL2FLOW，一个旨在解决LLM规划数据生成和评估瓶颈的全自动化系统。NL2FLOW能够参数化生成多种形式的规划问题，并严格评估LLM生成的计划。通过对2296个工作流生成问题进行评估，研究发现当前LLM在生成有效和最优计划方面存在局限，且直接从自然语言到动作的推理表现优于包含中间翻译步骤的方法。这强调了动态识别LLM限制和瓶颈的重要性，以充分发挥其作为智能问题解决者的潜力。", "keywords": "LLM规划, NL2FLOW, 参数化问题生成, 严格评估, 自然语言推理", "comments": "NL2FLOW的创新之处在于其自动化、参数化的规划问题生成和严格评估能力，有效解决了LLM规划领域的数据瓶颈。其发现中间翻译步骤可能降低性能的洞察，对未来LLM架构设计和提示工程具有重要指导意义。这对于推动LLM在复杂规划任务中的应用具有重要价值，为LLM规划能力的提升提供了关键工具和评估基准。"}}
{"id": "2507.02265", "title": "Multi-Label Classification Framework for Hurricane Damage Assessment", "authors": ["Zhangding Liu", "Neda Mohammadi", "John E. Taylor"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures. Accepted at the ASCE International Conference on Computing in Civil Engineering (i3CE 2025)", "url": "http://arxiv.org/abs/2507.02265v1", "summary": "Hurricanes cause widespread destruction, resulting in diverse damage types\nand severities that require timely and accurate assessment for effective\ndisaster response. While traditional single-label classification methods fall\nshort of capturing the complexity of post-hurricane damage, this study\nintroduces a novel multi-label classification framework for assessing damage\nusing aerial imagery. The proposed approach integrates a feature extraction\nmodule based on ResNet and a class-specific attention mechanism to identify\nmultiple damage types within a single image. Using the Rescuenet dataset from\nHurricane Michael, the proposed method achieves a mean average precision of\n90.23%, outperforming existing baseline methods. This framework enhances\npost-hurricane damage assessment, enabling more targeted and efficient disaster\nresponse and contributing to future strategies for disaster mitigation and\nresilience. This paper has been accepted at the ASCE International Conference\non Computing in Civil Engineering (i3CE 2025), and the camera-ready version\nwill appear in the official conference proceedings.", "comment": "9 pages, 3 figures. Accepted at the ASCE International Conference on\n  Computing in Civil Engineering (i3CE 2025)", "pdf_url": "http://arxiv.org/pdf/2507.02265v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "飓风损害评估的多标签分类框架", "tldr": "本研究提出一种新颖的多标签分类框架，利用航空影像评估飓风损害，通过集成ResNet特征提取和类别特定注意力机制，在Rescuenet数据集上取得了90.23%的平均精度，优于现有基线方法，有助于更高效的灾害响应。", "motivation": "飓风造成广泛破坏，需要及时准确的损害评估以有效响应灾害。传统单标签分类方法无法捕捉飓风后损害的复杂性，因此需要一种能识别多种损害类型的新方法。", "method": "提出了一种新颖的多标签分类框架，用于使用航空影像评估损害。该方法集成了基于ResNet的特征提取模块和类别特定注意力机制，以识别单张图像中的多种损害类型。", "result": "在使用飓风迈克尔的Rescuenet数据集上，所提出的方法实现了90.23%的平均精度，优于现有基线方法。", "conclusion": "该框架增强了飓风后的损害评估能力，使得灾害响应更加有针对性和高效，并有助于未来的灾害缓解和韧性策略。", "translation": "飓风造成广泛破坏，导致多种损害类型和严重程度，需要及时准确的评估以实现有效的灾害响应。虽然传统的单标签分类方法无法捕捉飓风后损害的复杂性，但本研究引入了一种新颖的多标签分类框架，用于使用航空影像评估损害。所提出的方法集成了基于ResNet的特征提取模块和类别特定注意力机制，以识别单张图像中的多种损害类型。使用来自飓风迈克尔的Rescuenet数据集，所提出的方法实现了90.23%的平均精度，优于现有基线方法。该框架增强了飓风后的损害评估，使得灾害响应更加有针对性和高效，并有助于未来的灾害缓解和韧性策略。本文已被ASCE国际土木工程计算会议（i3CE 2025）接受，最终版本将刊登在官方会议论文集中。", "summary": "本文提出了一种创新的多标签分类框架，用于利用航空影像评估飓风造成的复杂损害。该框架结合了ResNet特征提取和类别特定注意力机制，能够在一张图像中识别多种损害类型。在Rescuenet数据集上，该方法表现出色，平均精度达到90.23%，超越了现有基线。这项研究显著提升了飓风损害评估的效率和准确性，为更有效的灾害响应和未来的灾害管理策略提供了支持。", "keywords": "多标签分类, 飓风损害评估, 航空影像, ResNet, 注意力机制", "comments": "这项研究通过引入多标签分类框架来解决传统单标签方法在飓风损害评估中无法处理复杂多类型损害的局限性，具有创新性。其结合ResNet和注意力机制的方法是当前深度学习领域的有效实践。在真实数据集上的优异表现证明了其潜力，对于提升灾害响应效率和制定灾害缓解策略具有重要意义。"}}
{"id": "2507.02549", "title": "A Data-Driven Prescribed-Time Control Framework via Koopman Operator and Adaptive Backstepping", "authors": ["Yue Wu"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      6pages,4figs,1tables", "url": "http://arxiv.org/abs/2507.02549v1", "summary": "Achieving rapid and time-deterministic stabilization for complex systems\ncharacterized by strong nonlinearities and parametric uncertainties presents a\nsignificant challenge. Traditional model-based control relies on precise system\nmodels, whereas purely data-driven methods often lack formal stability\nguarantees, limiting their applicability in safety-critical systems. This paper\nproposes a novel control framework that synergistically integrates data-driven\nmodeling with model-based control. The framework first employs the Extended\nDynamic Mode Decomposition with Control (EDMDc) to identify a high-dimensional\nKoopman linear model and quantify its bounded uncertainty from data.\nSubsequently, a novel Prescribed-Time Adaptive Backstepping (PTAB) controller\nis synthesized based on this data-driven model. The design leverages the\nstructural advantages of Koopman linearization to systematically handle model\nerrors and circumvent the \"explosion of complexity\" issue inherent in\ntraditional backstepping. The proposed controller is validated through\nsimulations on the classic Van der Pol oscillator. The results demonstrate that\nthe controller can precisely stabilize the system states to a small\nneighborhood of the origin within a user-prescribed time, regardless of the\ninitial conditions, while ensuring the boundedness of all closed-loop signals.\nThis research successfully combines the flexibility of data-driven approaches\nwith the rigor of Lyapunov-based analysis. It provides a high-performance\ncontrol strategy with quantifiable performance and pre-assignable settling time\nfor nonlinear systems, showcasing its great potential for controlling complex\ndynamics.", "comment": "6pages,4figs,1tables", "pdf_url": "http://arxiv.org/pdf/2507.02549v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于Koopman算子和自适应反步法的数据驱动预设时间控制框架", "tldr": "提出一种结合Koopman算子和自适应反步法的数据驱动预设时间控制框架，用于复杂非线性系统的快速、确定时间稳定，并在范德波尔振荡器上进行了验证。", "motivation": "复杂系统（具有强非线性和参数不确定性）的快速、时间确定性稳定是一个重大挑战。传统基于模型的控制依赖精确系统模型，而纯数据驱动方法缺乏形式化稳定性保证，限制了其在安全关键系统中的应用。", "method": "本文提出一种新型控制框架，协同整合数据驱动建模与基于模型控制。首先，利用带控制的扩展动态模态分解 (EDMDc) 从数据中识别高维Koopman线性模型并量化其有界不确定性。其次，基于该数据驱动模型合成一种新型预设时间自适应反步 (PTAB) 控制器。该设计利用Koopman线性化的结构优势来系统处理模型误差并避免传统反步法固有的“复杂性爆炸”问题。", "result": "所提出的控制器通过在经典范德波尔振荡器上的仿真进行了验证。结果表明，无论初始条件如何，控制器都能在用户预设时间内将系统状态精确稳定到原点的一个小邻域内，同时确保所有闭环信号的有界性。", "conclusion": "这项研究成功结合了数据驱动方法的灵活性和基于Lyapunov分析的严谨性。它为非线性系统提供了一种具有可量化性能和可预设稳定时间的高性能控制策略，展示了其在控制复杂动力学方面的巨大潜力。", "translation": "实现具有强非线性和参数不确定性特征的复杂系统的快速、时间确定性稳定是一个重大挑战。传统的基于模型控制依赖于精确的系统模型，而纯数据驱动方法往往缺乏形式化的稳定性保证，限制了它们在安全关键系统中的适用性。本文提出了一种新颖的控制框架，协同整合了数据驱动建模与基于模型控制。该框架首先采用带控制的扩展动态模态分解 (EDMDc) 从数据中识别高维Koopman线性模型并量化其有界不确定性。随后，基于该数据驱动模型合成了一种新型预设时间自适应反步 (PTAB) 控制器。该设计利用Koopman线性化的结构优势来系统处理模型误差并规避传统反步法固有的“复杂性爆炸”问题。所提出的控制器通过在经典范德波尔振荡器上的仿真进行了验证。结果表明，无论初始条件如何，控制器都能在用户预设时间内将系统状态精确稳定到原点的一个小邻域内，同时确保所有闭环信号的有界性。这项研究成功结合了数据驱动方法的灵活性和基于Lyapunov分析的严谨性。它为非线性系统提供了一种具有可量化性能和可预设稳定时间的高性能控制策略，展示了其在控制复杂动力学方面的巨大潜力。", "summary": "本文提出了一种针对复杂非线性系统的新型数据驱动预设时间控制框架。该框架将数据驱动的Koopman建模（使用EDMDc）与基于模型的预设时间自适应反步（PTAB）控制器相结合。该方法有效地处理了模型误差并避免了复杂性问题。在范德波尔振荡器上的仿真验证了其能够在用户预设时间内精确稳定系统状态，并确保信号有界。该框架结合了数据驱动的灵活性和基于Lyapunov分析的严谨性，为非线性系统提供了一种具有可量化性能和可预设稳定时间的高性能控制策略。", "keywords": "数据驱动控制, 预设时间控制, Koopman算子, 自适应反步法, 非线性系统", "comments": "创新点在于协同结合数据驱动的Koopman建模与自适应反步法，实现了非线性系统的预设时间控制。这克服了纯数据驱动方法（缺乏稳定性保证）和传统基于模型方法（依赖精确模型、复杂性爆炸）的局限性。Koopman模型不确定性的量化及其结构优势在反步法中的利用是关键优势。"}}
{"id": "2507.02089", "title": "Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model", "authors": ["Xingtu Liu", "Lin F. Yang", "Sharan Vaswani"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02089v1", "summary": "We consider infinite-horizon $\\gamma$-discounted (linear) constrained Markov\ndecision processes (CMDPs) where the objective is to find a policy that\nmaximizes the expected cumulative reward subject to expected cumulative\nconstraints. Given access to a generative model, we propose to solve CMDPs with\na primal-dual framework that can leverage any black-box unconstrained MDP\nsolver. For linear CMDPs with feature dimension $d$, we instantiate the\nframework by using mirror descent value iteration\n(\\texttt{MDVI})~\\citep{kitamura2023regularization} an example MDP solver. We\nprovide sample complexity bounds for the resulting CMDP algorithm in two cases:\n(i) relaxed feasibility, where small constraint violations are allowed, and\n(ii) strict feasibility, where the output policy is required to exactly satisfy\nthe constraint. For (i), we prove that the algorithm can return an\n$\\epsilon$-optimal policy with high probability by using\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$ samples. We note\nthat these results exhibit a near-optimal dependence on both $d$ and\n$\\epsilon$. For (ii), we show that the algorithm requires\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$ samples,\nwhere $\\zeta$ is the problem-dependent Slater constant that characterizes the\nsize of the feasible region. Finally, we instantiate our framework for tabular\nCMDPs and show that it can be used to recover near-optimal sample complexities\nin this setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02089v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "具有生成模型的线性约束马尔可夫决策过程的样本复杂度界限", "tldr": "本文提出了一种基于原始对偶框架的CMDPs求解方法，并为两种情况（松弛可行性和严格可行性）下的线性CMDPs提供了样本复杂度界限。", "motivation": "解决无限 horizon 的线性约束马尔可夫决策过程（CMDPs），目标是在满足预期累积约束的同时最大化预期累积奖励，并提供样本复杂度界限。", "method": "提出一个原始对偶框架，该框架可以利用任何黑盒无约束MDP求解器来解决CMDPs。对于线性CMDPs，通过使用镜像下降值迭代（MDVI）实例化该框架。", "result": "在松弛可行性情况下，算法以高概率返回一个 $\\epsilon$-最优策略，所需样本量为 $\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$，对 $d$ 和 $\\epsilon$ 表现出接近最优的依赖性。在严格可行性情况下，算法需要 $\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$ 样本。该框架可用于表格CMDPs并恢复接近最优的样本复杂度。", "conclusion": "该算法在松弛可行性下表现出接近最优的样本复杂度，并且在严格可行性和表格CMDPs设置下也提供了有竞争力的界限。", "translation": "我们考虑无限 horizon 的 $\\gamma$-折扣（线性）约束马尔可夫决策过程（CMDPs），其中目标是找到一个策略，在满足预期累积约束的同时最大化预期累积奖励。在可以访问生成模型的情况下，我们提出使用原始对偶框架来解决 CMDPs，该框架可以利用任何黑盒无约束 MDP 求解器。对于特征维度为 $d$ 的线性 CMDPs，我们通过使用镜像下降值迭代（MDVI）作为 MDP 求解器来实例化该框架。我们为所得到的 CMDP 算法提供了两种情况下的样本复杂度界限：(i) 松弛可行性，即允许小的约束违反；(ii) 严格可行性，即要求输出策略精确满足约束。对于 (i)，我们证明该算法可以通过使用 $\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$ 样本，以高概率返回一个 $\\epsilon$-最优策略。我们注意到这些结果在 $d$ 和 $\\epsilon$ 上都表现出接近最优的依赖性。对于 (ii)，我们表明该算法需要 $\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$ 样本，其中 $\\zeta$ 是表征可行区域大小的依赖于问题的 Slater 常数。最后，我们将我们的框架实例化为表格 CMDPs，并表明它可以在这种设置下恢复接近最优的样本复杂度。", "summary": "本文研究了具有生成模型的线性约束马尔可夫决策过程（CMDPs），旨在找到一个在满足累积约束下最大化累积奖励的策略。作者提出了一个通用的原始对偶框架，该框架能够集成现有的无约束MDP求解器。通过使用镜像下降值迭代（MDVI）实例化该框架，作者为松弛可行性和严格可行性两种情况下的线性CMDPs推导了样本复杂度界限，并指出在松弛可行性下获得了接近最优的结果。该框架还适用于表格CMDPs，并能达到接近最优的样本复杂度。", "keywords": "约束马尔可夫决策过程, 样本复杂度, 原始对偶, 生成模型, 镜像下降值迭代", "comments": "这项工作创新性地将原始对偶框架应用于CMDPs，并为不同可行性条件下的样本复杂度提供了严格的理论界限。其利用黑盒MDP求解器的通用性也增强了方法的实用性。结果表明在某些条件下可以达到接近最优的样本效率，这对于CMDPs的学习具有重要意义。"}}
{"id": "2507.02641", "title": "Pinching-Antenna-Assisted Index Modulation: Channel Modeling, Transceiver Design, and Performance Analysis", "authors": ["Shuaixin Yang", "Yijia Li", "Yue Xiao", "Yong Liang Guan", "Xianfu Lei", "Zhiguo Ding"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02641v1", "summary": "In this paper, a novel pinching-antenna assisted index modulation (PA-IM)\nscheme is proposed for improving the spectral efficiency without increasing the\nhardware complexity, where the information bits are conveyed not only by the\nconventional M-ary quadrature amplitude modulation (QAM) symbols but also by\nthe indices of pinching antenna (PA) position patterns. To realize the full\npotential of this scheme, this paper focuses on the comprehensive transceiver\ndesign, addressing key challenges in signal detection at the receiver and\nperformance optimization at thetransmitter. First, a comprehensive channel\nmodel is formulated for this architecture, which sophisticatedly integrates the\ndeterministic in-waveguide propagation effects with the stochastic nature of\nwireless channels, including both largescale path loss and small-scale fading.\nNext, to overcome the prohibitive complexity of optimal maximum likelihood (ML)\ndetection, a low-complexity box-optimized sphere decoding (BOSD) algorithm is\ndesigned, which adaptively prunes the search space whilst preserving optimal ML\nperformance. Furthermore, an analytical upper bound on the bit error rate (BER)\nis derived and validated by the simulations. Moreover, a new transmit precoding\nmethod is designed using manifold optimization, which minimizes the BER by\njointly optimizing the complex-valued precoding coefficients across the\nwaveguides for the sake of maximizing the minimum Euclidean distance of all\nreceived signal points. Finally, the simulation results demonstrate that the\nproposed PA-IM scheme attains a significant performance gain over its\nconventional counterparts and that the overall BER of the pinching-antenna\nsystem is substantially improved by the proposed precoding design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02641v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "挤压天线辅助索引调制：信道建模、收发器设计与性能分析", "tldr": "本文提出了一种新型挤压天线辅助索引调制（PA-IM）方案，通过结合QAM符号和挤压天线位置模式索引来提高频谱效率，同时不增加硬件复杂性。研究内容包括信道建模、低复杂度检测算法和基于流形优化的预编码设计，仿真结果表明其性能优于传统方案。", "motivation": "旨在不增加硬件复杂度的前提下，提高频谱效率。", "method": "提出了一种新型挤压天线辅助索引调制（PA-IM）方案，通过结合传统M-ary QAM符号和挤压天线位置模式索引来传递信息。建立了综合信道模型，整合了波导内传播效应和无线信道（包括大尺度路径损耗和小尺度衰落）的随机性。设计了一种低复杂度的盒优化球形解码（BOSD）算法，用于实现接近最优的最大似然（ML）检测。推导并验证了误码率（BER）的解析上界。设计了一种基于流形优化的新型发射预编码方法，通过联合优化波导上的复值预编码系数来最大化所有接收信号点的最小欧氏距离，从而最小化BER。", "result": "所提出的PA-IM方案比传统方案获得了显著的性能增益。提出的预编码设计显著改善了挤压天线系统的整体误码率。", "conclusion": "所提出的挤压天线辅助索引调制（PA-IM）方案及其配套的预编码设计显著提升了系统的频谱效率和误码率性能。", "translation": "在本文中，提出了一种新颖的挤压天线辅助索引调制（PA-IM）方案，用于在不增加硬件复杂度的情况下提高频谱效率，其中信息比特不仅通过传统的M-ary正交幅度调制（QAM）符号传输，还通过挤压天线（PA）位置模式的索引传输。为了充分发挥该方案的潜力，本文重点关注全面的收发器设计，解决接收端信号检测和发射端性能优化中的关键挑战。首先，为此架构建立了全面的信道模型，该模型巧妙地将波导内的确定性传播效应与无线信道的随机性（包括大尺度路径损耗和小尺度衰落）相结合。其次，为了克服最优最大似然（ML）检测的过高复杂度，设计了一种低复杂度的盒优化球形解码（BOSD）算法，该算法在保留最优ML性能的同时自适应地剪枝搜索空间。此外，推导并验证了误码率（BER）的解析上界。再者，利用流形优化设计了一种新的发射预编码方法，该方法通过联合优化波导上的复值预编码系数来最大化所有接收信号点的最小欧氏距离，从而最小化BER。最后，仿真结果表明，所提出的PA-IM方案比传统方案获得了显著的性能增益，并且所提出的预编码设计显著改善了挤压天线系统的整体误码率。", "summary": "本文提出了一种新型挤压天线辅助索引调制（PA-IM）方案，旨在不增加硬件复杂度的前提下提升频谱效率。该方案通过结合QAM符号和挤压天线位置模式索引来传输信息。为实现其潜力，研究涵盖了全面的收发器设计，包括建立集成波导传播与无线信道效应的信道模型，设计低复杂度的盒优化球形解码（BOSD）算法以实现高效信号检测，推导BER上界，以及开发基于流形优化的发射预编码方法以优化BER性能。仿真结果验证了PA-IM方案及其预编码设计的显著性能增益和对系统误码率的改善。", "keywords": "挤压天线辅助索引调制, 信道建模, 收发器设计, 性能分析, 流形优化", "comments": "本文提出了一种创新的挤压天线辅助索引调制方案，通过利用天线位置模式作为信息载体，有效提升了频谱效率而未增加硬件复杂度，这在资源受限的通信系统中具有重要意义。其在信道建模、低复杂度检测和流形优化预编码方面的全面设计展示了该方案的实用性和优越性。特别是BOSD算法在保持ML性能的同时降低了复杂度，以及流形优化在预编码中的应用，是该研究的亮点。"}}
{"id": "2507.02445", "title": "IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising", "authors": ["Hailong Yan", "Junjian Huang", "Tingwen Huang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Artificial Intelligence (TAI) on Oct.31, 2024", "url": "http://arxiv.org/abs/2507.02445v1", "summary": "Current methods for restoring underexposed images typically rely on\nsupervised learning with paired underexposed and well-illuminated images.\nHowever, collecting such datasets is often impractical in real-world scenarios.\nMoreover, these methods can lead to over-enhancement, distorting\nwell-illuminated regions. To address these issues, we propose IGDNet, a\nZero-Shot enhancement method that operates solely on a single test image,\nwithout requiring guiding priors or training data. IGDNet exhibits strong\ngeneralization ability and effectively suppresses noise while restoring\nillumination. The framework comprises a decomposition module and a denoising\nmodule. The former separates the image into illumination and reflection\ncomponents via a dense connection network, while the latter enhances\nnon-uniformly illuminated regions using an illumination-guided pixel adaptive\ncorrection method. A noise pair is generated through downsampling and refined\niteratively to produce the final result. Extensive experiments on four public\ndatasets demonstrate that IGDNet significantly improves visual quality under\ncomplex lighting conditions. Quantitative results on metrics like PSNR\n(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art\nunsupervised methods. The code will be released soon.", "comment": "Submitted to IEEE Transactions on Artificial Intelligence (TAI) on\n  Oct.31, 2024", "pdf_url": "http://arxiv.org/pdf/2507.02445v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "IGDNet：通过光照引导和去噪实现零样本鲁棒欠曝光图像增强", "tldr": "IGDNet是一种零样本欠曝光图像增强方法，无需训练数据或先验信息，通过分解和去噪模块，有效提升图像质量并抑制噪声，性能优于现有无监督方法。", "motivation": "当前欠曝光图像恢复方法依赖于成对的监督学习数据，但在实际场景中收集此类数据集不切实际。此外，这些方法可能导致过度增强，扭曲光照良好的区域。", "method": "IGDNet包含一个分解模块和一个去噪模块。分解模块通过密集连接网络将图像分离为光照和反射分量。去噪模块使用光照引导的像素自适应校正方法增强非均匀光照区域。通过下采样生成噪声对并迭代优化以产生最终结果。", "result": "在四个公共数据集上的实验表明，IGDNet在复杂光照条件下显著提升了视觉质量。在PSNR（20.41dB）和SSIM（0.860dB）等指标上，其性能优于14种最先进的无监督方法。", "conclusion": "IGDNet作为一种零样本欠曝光图像增强方法，在无需训练数据的情况下，能够有效恢复图像光照并抑制噪声，并在多项公开数据集上取得了优于现有无监督方法的表现。", "translation": "当前恢复欠曝光图像的方法通常依赖于带有成对欠曝光和光照良好图像的监督学习。然而，在现实场景中收集此类数据集通常不切实际。此外，这些方法可能导致过度增强，扭曲光照良好的区域。为了解决这些问题，我们提出了IGDNet，一种零样本增强方法，它仅依靠单张测试图像操作，无需引导先验或训练数据。IGDNet表现出强大的泛化能力，并在恢复光照的同时有效抑制噪声。该框架包含一个分解模块和去噪模块。前者通过密集连接网络将图像分离为光照和反射分量，而后者使用光照引导的像素自适应校正方法增强非均匀光照区域。通过下采样生成噪声对并迭代优化以产生最终结果。在四个公共数据集上的大量实验表明，IGDNet在复杂光照条件下显著改善了视觉质量。在PSNR（20.41dB）和SSIM（0.860dB）等指标上的定量结果表明，它优于14种最先进的无监督方法。代码将很快发布。", "summary": "IGDNet是一种无需训练数据和先验信息的零样本欠曝光图像增强方法，旨在解决现有方法依赖监督数据和易导致过度增强的问题。该网络由分解模块和去噪模块组成，前者分离图像的光照和反射分量，后者通过光照引导的像素自适应校正增强非均匀区域并迭代优化去噪。实验证明，IGDNet在复杂光照条件下显著提高了图像质量，并在PSNR和SSIM等指标上超越了14种主流无监督方法。", "keywords": "欠曝光图像增强, 零样本, 去噪, 光照引导, IGDNet", "comments": "该论文提出了一种创新的零样本方法IGDNet，解决了欠曝光图像增强中数据收集困难和过度增强的挑战。其无需训练数据和先验知识的特点，以及通过分解和去噪模块结合光照引导的像素自适应校正，展现了强大的实用性和泛化能力。在性能上超越了众多无监督方法，表明了其在实际应用中的巨大潜力。"}}
{"id": "2507.02842", "title": "On the Structure of Replicable Hypothesis Testers", "authors": ["Anders Aamand", "Maryam Aliakbarpour", "Justin Y. Chen", "Shyam Narayanan", "Sandeep Silwal"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Abstract abridged to meet arxiv requirements", "url": "http://arxiv.org/abs/2507.02842v1", "summary": "A hypothesis testing algorithm is replicable if, when run on two different\nsamples from the same distribution, it produces the same output with high\nprobability. This notion, defined by by Impagliazzo, Lei, Pitassi, and Sorell\n[STOC'22], can increase trust in testing procedures and is deeply related to\nalgorithmic stability, generalization, and privacy. We build general tools to\nprove lower and upper bounds on the sample complexity of replicable testers,\nunifying and quantitatively improving upon existing results.\n  We identify a set of canonical properties, and prove that any replicable\ntesting algorithm can be modified to satisfy these properties without worsening\naccuracy or sample complexity. A canonical replicable algorithm computes a\ndeterministic function of its input (i.e., a test statistic) and thresholds\nagainst a uniformly random value in $[0,1]$. It is invariant to the order in\nwhich the samples are received, and, if the testing problem is ``symmetric,''\nthen the algorithm is also invariant to the labeling of the domain elements,\nresolving an open question by Liu and Ye [NeurIPS'24]. We prove new lower\nbounds for uniformity, identity, and closeness testing by reducing to the case\nwhere the replicable algorithm satisfies these canonical properties.\n  We systematize and improve upon a common strategy for replicable algorithm\ndesign based on test statistics with known expectation and bounded variance.\nOur framework allow testers which have been extensively analyzed in the\nnon-replicable setting to be made replicable with minimal overhead. As direct\napplications of our framework, we obtain constant-factor optimal bounds for\ncoin testing and closeness testing and get replicability for free in a large\nparameter regime for uniformity testing.\n  We also give state-of-the-art bounds for replicable Gaussian mean testing,\nand, unlike prior work, our algorithm runs in polynomial time.", "comment": "Abstract abridged to meet arxiv requirements", "pdf_url": "http://arxiv.org/pdf/2507.02842v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "关于可复现假设检验器的结构", "tldr": "本文深入研究了可复现假设检验器的结构，提出了规范属性，构建了样本复杂度上下界的通用工具，并改进了算法设计策略，为多种检验问题提供了最优或最先进的界限。", "motivation": "可复现假设检验算法能够在来自同一分布的不同样本上以高概率产生相同输出，这可以增加对检验过程的信任，并且与算法稳定性、泛化和隐私密切相关。", "method": "1. 构建了证明可复现检验器样本复杂度下限和上限的通用工具。2. 识别了一组规范属性，并证明任何可复现检验算法都可以被修改以满足这些属性，而不会降低准确性或样本复杂度。3. 通过归约到可复现算法满足这些规范属性的情况，证明了均匀性、同一性和接近度检验的新下限。4. 系统化并改进了一种基于已知期望和有界方差的检验统计量的可复现算法设计通用策略。", "result": "1. 建立了样本复杂度的通用下限和上限工具。2. 识别了可复现检验算法的规范属性。3. 证明了任何可复现算法都可以在不降低准确性或样本复杂度的情况下修改以满足规范属性。4. 解决了Liu和Ye提出的关于对称问题中领域元素标记不变性的开放问题。5. 证明了均匀性、同一性和接近度检验的新下限。6. 系统化并改进了可复现算法设计的通用策略。7. 为硬币检验和接近度检验获得了常数因子最优界限。8. 在大参数范围内，均匀性检验实现了免费的可复现性。9. 为可复现高斯均值检验提供了最先进的界限，且算法在多项式时间内运行。", "conclusion": "本文为理解和设计可复现假设检验器提供了一个全面的框架，统一并改进了现有结果，解决了开放问题，并为各种检验问题实现了最优或最先进的界限，使得非可复现检验器能够以最小的开销实现可复现。", "translation": "如果一个假设检验算法在来自同一分布的两个不同样本上运行时能以高概率产生相同的输出，那么它是可复现的。这个由Impagliazzo、Lei、Pitassi和Sorell [STOC'22] 定义的概念可以增加对检验过程的信任，并且与算法稳定性、泛化和隐私密切相关。我们构建了通用工具来证明可复现检验器样本复杂度的下限和上限，统一并定量改进了现有结果。\n我们识别了一组规范属性，并证明任何可复现检验算法都可以被修改以满足这些属性，而不会降低准确性或样本复杂度。一个规范的可复现算法计算其输入的确定性函数（即检验统计量），并与[0,1]中的均匀随机值进行阈值比较。它对样本接收的顺序是不变的，并且，如果检验问题是“对称的”，那么算法对领域元素的标记也是不变的，这解决了Liu和Ye [NeurIPS'24] 的一个开放问题。我们通过归约到可复现算法满足这些规范属性的情况，证明了均匀性、同一性和接近度检验的新下限。\n我们系统化并改进了一种基于已知期望和有界方差的检验统计量的可复现算法设计通用策略。我们的框架允许在非可复现设置中经过广泛分析的检验器以最小的开销实现可复现。作为我们框架的直接应用，我们为硬币检验和接近度检验获得了常数因子最优界限，并在大参数范围内，均匀性检验实现了免费的可复现性。\n我们还为可复现高斯均值检验提供了最先进的界限，并且与以前的工作不同，我们的算法在多项式时间内运行。", "summary": "本文研究了可复现假设检验器的结构，这类检验器在不同样本上能产生一致输出，从而增强信任。作者开发了样本复杂度界限的通用工具，并确定了可复现算法的规范属性，证明了任何可复现算法都可在不损失性能的情况下被改造以满足这些属性。论文解决了关于对称问题中领域元素标记不变性的开放问题，并为多种检验提出了新的下限。此外，文章系统化并改进了算法设计策略，使得非可复现检验器能高效地实现可复现。该框架为硬币检验和接近度检验带来了最优界限，在特定参数范围内为均匀性检验提供了免费的可复现性，并为高斯均值检验提供了多项式时间的最先进界限。", "keywords": "可复现假设检验, 样本复杂度, 规范属性, 算法稳定性, 统计检验", "comments": "本文的创新之处在于为可复现假设检验器提供了基础性理解，通过识别规范属性和构建通用工具，极大地推进了该领域。其重要性体现在增强了算法检验的信任度，统一并改进了现有成果，并提供了将现有非可复现检验器转化为可复现检验器的实用策略。解决了开放问题进一步凸显了其理论贡献。"}}
{"id": "2507.02652", "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yang Zhao", "Hongjin Qian", "Zhicheng Dou"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.02652v1", "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.02652v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "解耦规划与执行：一种用于深度搜索的层次化推理框架", "tldr": "HiRA是一个分层框架，通过分离规划和执行来改进复杂深度搜索中的推理和效率，优于现有方法。", "motivation": "传统检索增强生成（RAG）管道和现有基于推理的方法在处理复杂信息需求时效率低下且可扩展性有限，因为它们使用单一模型处理高级规划和详细执行。", "method": "本文引入了HiRA，一个分层框架，它将战略规划与专业执行分离。该方法将复杂搜索任务分解为集中的子任务，将每个子任务分配给配备外部工具和推理能力的领域特定代理，并通过结构化集成机制协调结果。", "result": "在四个复杂的跨模态深度搜索基准测试中，HiRA显著优于最先进的RAG和基于代理的系统，在答案质量和系统效率方面都有所改进。", "conclusion": "解耦规划和执行对于多步信息寻求任务是有效的，HiRA通过这种方法提高了深度搜索的性能和效率。", "translation": "在现实世界搜索场景中，复杂的信息需求需要跨不同来源的深度推理和知识综合，而传统的检索增强生成（RAG）管道难以有效解决。当前的基于推理的方法存在一个根本性限制：它们使用单一模型处理高级规划和详细执行，导致推理效率低下和可扩展性有限。在本文中，我们引入了HiRA，一个将战略规划与专业执行分离的层次化框架。我们的方法将复杂的搜索任务分解为集中的子任务，将每个子任务分配给配备外部工具和推理能力的领域特定代理，并通过结构化集成机制协调结果。这种分离防止了执行细节干扰高级推理，同时使系统能够利用专业知识处理不同类型的信息。在四个复杂的跨模态深度搜索基准测试中，实验表明HiRA显著优于最先进的RAG和基于代理的系统。我们的结果显示答案质量和系统效率都有所提高，突出了解耦规划和执行对于多步信息寻求任务的有效性。我们的代码可在https://github.com/ignorejjj/HiRA获取。", "summary": "本文介绍了HiRA，一个分层推理框架，旨在解决复杂深度搜索中传统方法效率低下的问题。HiRA通过将战略规划与专业执行解耦，将复杂任务分解为子任务并分配给专门代理处理，有效提高了答案质量和系统效率，并在多项跨模态深度搜索基准测试中表现优异，证明了解耦规划与执行对多步信息寻求任务的有效性。", "keywords": "深度搜索, 分层推理, 解耦规划, 代理系统, 信息寻求", "comments": "HiRA通过引入规划与执行解耦的层次化框架，有效解决了现有深度搜索方法中单一模型处理复杂任务的效率瓶颈。其创新点在于将复杂任务分解并利用领域特定代理，提高了系统的可扩展性和推理能力。这对于需要多步信息处理和知识综合的复杂搜索场景具有重要意义。"}}
{"id": "2507.02407", "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This version has been reviewed and accepted for presentation at the Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025 in Munich, Germany. 17 pages, 4 figures, 1 table", "url": "http://arxiv.org/abs/2507.02407v1", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.", "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.02407v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "跨领域数据集上的阿坎语ASR模型基准测试：性能、可扩展性和适应性的比较评估", "tldr": "本研究通过在四个阿坎语语音语料库上对七种基于Transformer的阿坎语ASR模型（如Whisper和Wav2Vec2）进行基准测试，发现模型存在领域依赖性，在训练领域外性能显著下降，并揭示了Whisper和Wav2Vec2在错误行为上的权衡。", "motivation": "大多数现有的自动语音识别（ASR）研究主要使用域内数据集评估模型，但很少评估模型在不同语音上下文中的泛化能力。本研究旨在弥补这一空白。", "method": "本研究对七个基于Transformer架构（如Whisper和Wav2Vec2）的阿坎语ASR模型，使用四个涵盖不同领域的阿坎语语音语料库进行基准测试。通过比较词错误率和字符错误率来评估模型性能。", "result": "研究结果突出了领域依赖性，模型仅在其训练领域内表现最佳，而在不匹配的场景中准确性显著下降。研究还发现Whisper和Wav2Vec2架构之间存在明显的错误行为差异：经过微调的Whisper阿坎语模型产生更流畅但可能误导性的转录错误，而Wav2Vec2在遇到不熟悉输入时产生更明显但更难解释的输出。", "conclusion": "在为低资源语言（LRL）应用选择ASR架构时，应考虑可读性和透明度之间的权衡。这些发现强调了阿坎语和其他低资源语言需要有针对性的领域适应技术、自适应路由策略和多语言训练框架。", "translation": "大多数现有的自动语音识别（ASR）研究使用域内数据集评估模型。然而，它们很少评估模型在不同语音上下文中的泛化能力。本研究通过在四个阿坎语语音语料库上对七种基于Transformer架构（如Whisper和Wav2Vec2）的阿坎语ASR模型进行基准测试，以确定其性能，从而弥补了这一空白。这些数据集涵盖了各种领域，包括与文化相关的图像描述、非正式对话、圣经经文阅读和自发性金融对话。词错误率和字符错误率的比较突出了领域依赖性，模型仅在其训练领域内表现最佳，而在不匹配的场景中准确性显著下降。本研究还发现了Whisper和Wav2Vec2架构之间明显的错误行为差异。经过微调的Whisper阿坎语模型导致更流畅但可能误导性的转录错误，而Wav2Vec2在遇到不熟悉输入时产生更明显但更难解释的输出。在为低资源语言（LRL）应用选择架构时，应考虑ASR错误中可读性和透明度之间的这种权衡。这些发现强调了阿坎语和其他低资源语言需要有针对性的领域适应技术、自适应路由策略和多语言训练框架。", "summary": "本研究旨在弥补现有ASR研究中模型泛化能力评估的不足，通过在四个不同的阿坎语语音语料库上对七个基于Transformer的阿坎语ASR模型（包括Whisper和Wav2Vec2）进行基准测试。结果表明，ASR模型存在显著的领域依赖性，在训练领域外性能大幅下降。此外，研究还揭示了Whisper和Wav2Vec2在错误行为上的独特权衡，即Whisper倾向于产生流畅但可能误导的错误，而Wav2Vec2产生明显但难以解释的输出。这些发现强调了在低资源语言应用中选择ASR架构时需考虑可读性与透明度的平衡，并指出未来研究应关注领域适应和多语言训练。", "keywords": "阿坎语ASR, 领域适应, 泛化能力, Transformer模型, 低资源语言", "comments": "这项研究的创新之处在于它首次系统地评估了阿坎语ASR模型在不同领域数据集上的泛化能力，填补了现有ASR研究中对领域适应性关注不足的空白。其重要性在于揭示了ASR模型在面对域外数据时的性能退化问题，并具体分析了Whisper和Wav2Vec2两种主流架构在错误行为上的差异和权衡，这对于低资源语言的ASR系统开发具有重要的指导意义。研究结果强调了开发针对性领域适应技术和多语言训练框架的必要性，为未来低资源语言ASR研究指明了方向。"}}
{"id": "2507.02607", "title": "Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures", "authors": ["Frida Sundfeldt", "Bianca Widstam", "Mahshid Helali Moghadam", "Kuo-Yun Liang", "Anders Vesterberg"], "categories": ["cs.CR", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02607v1", "summary": "The digital evolution of connected vehicles and the subsequent security risks\nemphasize the critical need for implementing in-vehicle cyber security measures\nsuch as intrusion detection and response systems. The continuous advancement of\nattack scenarios further highlights the need for adaptive detection mechanisms\nthat can detect evolving, unknown, and complex threats. The effective use of\nML-driven techniques can help address this challenge. However, constraints on\nimplementing diverse attack scenarios on test vehicles due to safety, cost, and\nethical considerations result in a scarcity of data representing attack\nscenarios. This limitation necessitates alternative efficient and effective\nmethods for generating high-quality attack-representing data. This paper\npresents a context-aware attack data generator that generates attack inputs and\ncorresponding in-vehicle network log, i.e., controller area network (CAN) log,\nrepresenting various types of attack including denial of service (DoS), fuzzy,\nspoofing, suspension, and replay attacks. It utilizes parameterized attack\nmodels augmented with CAN message decoding and attack intensity adjustments to\nconfigure the attack scenarios with high similarity to real-world scenarios and\npromote variability. We evaluate the practicality of the generated\nattack-representing data within an intrusion detection system (IDS) case study,\nin which we develop and perform an empirical evaluation of two deep neural\nnetwork IDS models using the generated data. In addition to the efficiency and\nscalability of the approach, the performance results of IDS models, high\ndetection and classification capabilities, validate the consistency and\neffectiveness of the generated data as well. In this experience study, we also\nelaborate on the aspects influencing the fidelity of the data to real-world\nscenarios and provide insights into its application.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02607v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "缓解攻击数据稀缺：SCANIA 在增强车载网络安全措施方面的经验", "tldr": "本文提出了一种上下文感知的攻击数据生成器，用于生成高质量的车载网络攻击数据，以解决实际车辆测试中攻击数据稀缺的问题，并验证了其在入侵检测系统中的有效性。", "motivation": "随着联网车辆的数字化发展和安全风险的增加，车载网络安全措施（如入侵检测系统）变得至关重要。ML驱动的检测机制需要大量攻击数据，但由于安全、成本和伦理限制，在测试车辆上实现多样化攻击场景导致攻击数据稀缺。因此，需要高效有效的方法来生成高质量的攻击数据。", "method": "本文提出了一种上下文感知的攻击数据生成器。该生成器能够生成攻击输入和相应的车载网络（CAN）日志，涵盖拒绝服务（DoS）、模糊、欺骗、暂停和重放等多种攻击类型。它利用参数化攻击模型，结合CAN消息解码和攻击强度调整，以配置与真实场景高度相似且具有可变性的攻击场景。", "result": "生成的攻击数据在入侵检测系统（IDS）案例研究中得到了实用性验证。使用生成数据开发的两个深度神经网络IDS模型表现出高效且可扩展的检测和分类能力，验证了生成数据的一致性和有效性。", "conclusion": "本文提出的上下文感知攻击数据生成器能够有效解决车载网络安全领域攻击数据稀缺的问题，生成高质量、高保真度的攻击数据，并为入侵检测系统的开发提供了有价值的资源。研究还探讨了影响数据真实性的因素并提供了应用见解。", "translation": "联网车辆的数字化演变及其随之而来的安全风险，强调了实施车载网络安全措施（如入侵检测和响应系统）的迫切需求。攻击场景的持续发展进一步突出了对能够检测不断演变、未知和复杂威胁的自适应检测机制的需求。有效利用机器学习驱动的技术可以帮助应对这一挑战。然而，由于安全、成本和伦理方面的考虑，在测试车辆上实施多样化攻击场景的限制导致代表攻击场景的数据稀缺。这一限制使得需要替代的、高效且有效的方法来生成高质量的攻击代表数据。本文提出了一种上下文感知的攻击数据生成器，该生成器生成攻击输入和相应的车载网络日志，即控制器局域网（CAN）日志，代表各种类型的攻击，包括拒绝服务（DoS）、模糊、欺骗、暂停和重放攻击。它利用参数化攻击模型，辅以CAN消息解码和攻击强度调整，以配置与真实场景高度相似并促进可变性的攻击场景。我们在入侵检测系统（IDS）案例研究中评估了所生成攻击代表数据的实用性，在该研究中，我们使用生成的数据开发并对两个深度神经网络IDS模型进行了实证评估。除了该方法的效率和可扩展性之外，IDS模型的性能结果，即高检测和分类能力，也验证了生成数据的一致性和有效性。在这项经验研究中，我们还详细阐述了影响数据对真实场景保真度的方面，并提供了其应用的见解。", "summary": "本文针对联网车辆网络安全中攻击数据稀缺的问题，提出了一种上下文感知的攻击数据生成器。该生成器能够生成包含多种攻击类型（如DoS、模糊、欺骗等）的高质量车载CAN日志数据，并通过参数化模型和CAN消息解码来确保数据与真实场景的高度相似性和可变性。通过在入侵检测系统（IDS）中对生成数据进行评估，证明了其在训练深度神经网络IDS模型方面的效率、可扩展性以及有效性，为解决车载网络安全数据瓶颈提供了实用方案。", "keywords": "车载网络安全, 攻击数据生成, 入侵检测系统, CAN日志, 数据稀缺", "comments": "本文的创新之处在于提出了一种上下文感知的攻击数据生成器，有效缓解了车载网络安全领域中攻击数据稀缺的难题，这对于开发和测试车载入侵检测系统具有重要意义。其方法结合了参数化模型和CAN消息解码，提高了生成数据的真实性和多样性。该研究的贡献在于提供了一个实用的数据生成解决方案，有助于推动车载网络安全技术的发展。"}}
{"id": "2507.02438", "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints", "authors": ["Shivam Chaubey", "Francesco Verdoja", "Shankar Deka", "Ville Kyrki"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.02438v1", "summary": "Shared control combines human intention with autonomous decision-making, from\nlow-level safety overrides to high-level task guidance, enabling systems that\nadapt to users while ensuring safety and performance. This enhances task\neffectiveness and user experience across domains such as assistive robotics,\nteleoperation, and autonomous driving. However, existing shared control\nmethods, based on e.g. Model Predictive Control, Control Barrier Functions, or\nlearning-based control, struggle with feasibility, scalability, or safety\nguarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework\nbased on Constrained Optimal Control Problem that incorporates an\noffline-computed Control Invariant Set, enabling online computation of control\nactions that ensure feasibility, strict constraint satisfaction, and minimal\noverride of user intent. Moreover, the framework can accommodate structured\nclass of non-convex constraints, which are common in real-world scenarios. We\nvalidate the approach through a large-scale user study with 66\nparticipants--one of the most extensive in shared control research--using a\ncomputer game environment to assess task load, trust, and perceived control, in\naddition to performance. The results show consistent improvements across all\nthese aspects without compromising safety and user intent.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.02438v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MISC：非凸约束下保证安全的最小干预共享控制", "tldr": "本文提出了一种名为MISC的共享控制框架，它通过结合离线计算的控制不变集，解决了现有方法在非凸约束下可行性、可伸缩性和安全保证方面的挑战，并在大规模用户研究中显示出显著改进。", "motivation": "现有的共享控制方法（如模型预测控制、控制障碍函数或基于学习的控制）在可行性、可伸缩性或安全保证方面存在困难，尤其是在用户输入不可预测且存在现实世界中常见的非凸约束时。", "method": "本文提出了一种基于约束最优控制问题的辅助控制器框架，该框架结合了离线计算的控制不变集，以在线计算控制动作。这种方法确保了可行性、严格的约束满足和对用户意图的最小干预，并且能够处理结构化的非凸约束。该方法通过一项包含66名参与者的大规模用户研究在计算机游戏环境中进行了验证，评估了任务负荷、信任、感知控制和性能。", "result": "大规模用户研究结果表明，该方法在任务负荷、信任、感知控制和性能方面均取得了持续改进，且未损害安全性和用户意图。", "conclusion": "本文提出的MISC框架有效解决了现有共享控制方法的局限性，在非凸约束下提供了有保证的安全性，并显著改善了用户体验。", "translation": "共享控制将人类意图与自主决策相结合，从低级安全干预到高级任务指导，使系统能够适应用户，同时确保安全性和性能。这提高了辅助机器人、远程操作和自动驾驶等领域中的任务效率和用户体验。然而，现有的共享控制方法，例如基于模型预测控制、控制障碍函数或基于学习的控制，在可行性、可伸缩性或安全保证方面存在困难，尤其是在用户输入不可预测的情况下。\n为了解决这些挑战，我们提出了一种基于约束最优控制问题的辅助控制器框架，该框架结合了离线计算的控制不变集，从而能够在线计算控制动作，确保可行性、严格的约束满足和对用户意图的最小干预。此外，该框架可以适应在现实世界场景中常见的结构化非凸约束。我们通过一项有66名参与者的大规模用户研究（共享控制研究中最广泛的研究之一）验证了该方法，使用计算机游戏环境评估任务负荷、信任和感知控制，以及性能。结果显示在所有这些方面都有持续的改进，而没有损害安全性和用户意图。", "summary": "本文提出了一种名为MISC的新型共享控制框架，该框架基于约束最优控制和离线计算的控制不变集。它解决了现有方法在处理非凸约束时面临的局限性，能够提供有保证的安全性、严格的约束满足以及对用户意图的最小干预。一项包含66名参与者的大规模用户研究证实了该方法在改善任务负荷、信任和感知控制方面的有效性，同时不牺牲安全性。", "keywords": "共享控制, 非凸约束, 控制不变集, 安全保证, 最小干预", "comments": "该论文的创新点在于其共享控制框架通过结合离线计算的控制不变集，在约束最优控制的背景下，能够有效处理非凸约束并提供严格的安全保证，同时实现对用户意图的最小干预。此外，其进行的大规模用户研究（66名参与者）在共享控制研究领域具有显著的规模和贡献。"}}
{"id": "2507.02578", "title": "Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems", "authors": ["Zoe Pfister"], "categories": ["cs.SE", "cs.HC", "D.2.1"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Copyright 2025 IEEE. Accepted for publication in: 2025 IEEE 33nd International Requirements Engineering Conference (RE), Doctor Symposium Paper, 5 pages", "url": "http://arxiv.org/abs/2507.02578v1", "summary": "Adaptive Cyber-Physical Systems (CPS) are systems that integrate both\nphysical and computational capabilities, which can adjust in response to\nchanging parameters. Furthermore, they increasingly incorporate human-machine\ncollaboration, allowing them to benefit from the individual strengths of humans\nand machines. Human-Machine Teaming (HMT) represents the most advanced paradigm\nof human-machine collaboration, envisioning seamless teamwork between humans\nand machines. However, achieving effective and seamless HMT in adaptive CPS is\nchallenging. While adaptive CPS already benefit from feedback loops such as\nMAPE-K, there is still a gap in integrating humans into these feedback loops\ndue to different operational cadences of humans and machines. Further, HMT\nrequires constant monitoring of human operators, collecting potentially\nsensitive information about their actions and behavior. Respecting the privacy\nand human values of the actors of the CPS is crucial for the success of\nhuman-machine teams. This research addresses these challenges by: (1)\ndeveloping novel methods and processes for integrating HMT into adaptive CPS,\nfocusing on human-machine interaction principles and their incorporation into\nadaptive feedback loops found in CPS, and (2) creating frameworks for\nintegrating, verifying, and validating ethics and human values throughout the\nsystem lifecycle, starting from requirements engineering.", "comment": "Copyright 2025 IEEE. Accepted for publication in: 2025 IEEE 33nd\n  International Requirements Engineering Conference (RE), Doctor Symposium\n  Paper, 5 pages", "pdf_url": "http://arxiv.org/pdf/2507.02578v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "自适应信息物理系统中的人机协作与伦理考量", "tldr": "本研究旨在解决自适应信息物理系统中人机协作的挑战，特别是将人类整合到反馈循环中以及处理敏感数据时的伦理问题，通过开发新方法和框架来实现有效且符合伦理的人机协同。", "motivation": "自适应信息物理系统（CPS）日益整合人机协作，其中人机协同（HMT）是最先进的范式。然而，在自适应CPS中实现有效且无缝的HMT面临挑战，主要体现在：1) 由于人机操作节奏不同，将人类整合到CPS的自适应反馈循环中存在空白；2) HMT需要持续监控人类操作员，这可能涉及收集敏感信息，因此尊重隐私和人类价值观至关重要。", "method": "本研究通过以下两种方法应对挑战：1) 开发新颖的方法和流程，将人机协同（HMT）整合到自适应信息物理系统（CPS）中，重点关注人机交互原则及其在CPS自适应反馈循环中的应用；2) 创建框架，用于在系统生命周期（从需求工程开始）中整合、验证和验证伦理与人类价值观。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "自适应信息物理系统（CPS）是集成了物理和计算能力的系统，能够响应变化的参数进行调整。此外，它们越来越多地融入人机协作，使其能够受益于人类和机器的各自优势。人机协同（HMT）代表了人机协作最先进的范式，设想人类和机器之间实现无缝协作。然而，在自适应CPS中实现有效且无缝的HMT具有挑战性。虽然自适应CPS已经受益于MAPE-K等反馈循环，但由于人类和机器操作节奏的不同，将人类整合到这些反馈循环中仍然存在空白。此外，HMT需要持续监控人类操作员，收集关于其行为和举止的潜在敏感信息。尊重CPS参与者的隐私和人类价值观对于人机团队的成功至关重要。本研究通过以下方式解决这些挑战：(1) 开发新颖的方法和流程，将HMT整合到自适应CPS中，重点关注人机交互原则及其在CPS中自适应反馈循环的应用；(2) 创建框架，用于在整个系统生命周期（从需求工程开始）中整合、验证和验证伦理和人类价值观。", "summary": "本论文探讨了自适应信息物理系统（CPS）中人机协同（HMT）所面临的挑战。研究指出，在现有反馈循环中整合人类的困难以及处理敏感数据时的伦理考量是关键问题。为解决这些问题，本研究提出开发新方法以更好地将HMT融入CPS的自适应反馈循环，并创建框架以在系统生命周期中整合和验证伦理与人类价值观。", "keywords": "人机协作, 自适应信息物理系统, 伦理考量, 人机协同, 反馈循环", "comments": "这篇论文关注了人机协作前沿领域中的关键挑战，即在高度动态的自适应信息物理系统中实现人机协同，并特别强调了伦理考量。其创新点在于提出将人类操作员更深入地整合到系统反馈循环中，并从需求工程阶段就开始考虑伦理和隐私问题，这对于构建可信赖和以人为中心的人机系统至关重要。该研究具有重要的理论和实践意义，为未来智能系统的设计提供了方向。"}}
{"id": "2507.02306", "title": "Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation", "authors": ["Ruican Zhong", "David W. McDonald", "Gary Hsieh"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02306v1", "summary": "Usability evaluation is crucial in human-centered design but can be costly,\nrequiring expert time and user compensation. In this work, we developed a\nmethod for synthetic heuristic evaluation using multimodal LLMs' ability to\nanalyze images and provide design feedback. Comparing our synthetic evaluations\nto those by experienced UX practitioners across two apps, we found our\nevaluation identified 73% and 77% of usability issues, which exceeded the\nperformance of 5 experienced human evaluators (57% and 63%). Compared to human\nevaluators, the synthetic evaluation's performance maintained consistent\nperformance across tasks and excelled in detecting layout issues, highlighting\npotential attentional and perceptual strengths of synthetic evaluation.\nHowever, synthetic evaluation struggled with recognizing some UI components and\ndesign conventions, as well as identifying across screen violations.\nAdditionally, testing synthetic evaluations over time and accounts revealed\nstable performance. Overall, our work highlights the performance differences\nbetween human and LLM-driven evaluations, informing the design of synthetic\nheuristic evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02306v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "合成启发式评估：AI驱动与人工可用性评估的比较", "tldr": "本研究开发了一种基于多模态LLM的合成启发式评估方法，发现其在识别可用性问题方面表现优于人类专家，但在某些特定问题上仍有不足。", "motivation": "可用性评估在以人为中心的设计中至关重要，但成本高昂，需要专家时间和用户报酬。", "method": "开发了一种使用多模态LLM分析图像并提供设计反馈的合成启发式评估方法。将合成评估与经验丰富的UX从业者对两个应用程序的评估进行比较。", "result": "合成评估识别了73%和77%的可用性问题，超过了5位经验丰富的人类评估师（57%和63%）。合成评估在任务之间保持一致的性能，并在检测布局问题方面表现出色，但难以识别某些UI组件和设计约定以及跨屏幕违规。此外，合成评估的性能随时间和账户变化保持稳定。", "conclusion": "本研究突出了人类评估与LLM驱动评估之间的性能差异，为合成启发式评估的设计提供了信息。", "translation": "可用性评估在以人为中心的设计中至关重要，但成本高昂，需要专家时间和用户报酬。在这项工作中，我们开发了一种使用多模态大型语言模型（LLMs）分析图像并提供设计反馈的合成启发式评估方法。我们将合成评估与经验丰富的用户体验（UX）从业者对两个应用程序的评估进行了比较，发现我们的评估识别了73%和77%的可用性问题，这超过了5位经验丰富的人类评估师的表现（57%和63%）。与人类评估师相比，合成评估的性能在任务之间保持一致，并在检测布局问题方面表现出色，这突显了合成评估潜在的注意力和感知优势。然而，合成评估在识别某些UI组件和设计约定以及识别跨屏幕违规方面存在困难。此外，对合成评估进行长期和跨账户测试显示其性能稳定。总的来说，我们的工作突出了人类评估与LLM驱动评估之间的性能差异，为合成启发式评估的设计提供了信息。", "summary": "本研究提出了一种基于多模态大型语言模型（LLMs）的合成启发式评估方法，旨在降低传统可用性评估的高成本。通过与人类专家进行对比实验，发现该方法在识别可用性问题方面表现出更高的效率和一致性，特别是在布局问题检测上优势明显。尽管在识别特定UI组件和跨屏违规方面仍有局限，但其性能的稳定性和超越人类专家的部分能力，为未来LLM驱动的可用性评估工具的设计提供了宝贵见解。", "keywords": "可用性评估, LLMs, 合成评估, 人机交互, 启发式评估", "comments": "这项研究的创新之处在于利用多模态LLM进行自动化可用性启发式评估，为高成本的人工评估提供了一个有前景的替代方案。其重要性在于证明了AI在识别某些类型可用性问题上的潜力，甚至在特定方面超越了人类专家。然而，论文也指出了当前AI方法的局限性，例如在理解复杂UI组件和跨屏交互方面的不足，这为未来的研究指明了方向，即如何弥补AI在这些方面的短板，实现更全面和智能的可用性评估。"}}
{"id": "2507.02319", "title": "Iterated belief revision: from postulates to abilities", "authors": ["Paolo Liberatore"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02319v1", "summary": "The belief revision field is opulent in new proposals and indigent in\nanalyses of existing approaches. Much work hinge on postulates, employed as\nsyntactic characterizations: some revision mechanism is equivalent to some\nproperties. Postulates constraint specific revision instances: certain\nrevisions update certain beliefs in a certain way. As an example, if the\nrevision is consistent with the current beliefs, it is incorporated with no\nother change. A postulate like this tells what revisions must do and neglect\nwhat they can do. Can they reach a certain state of beliefs? Can they reach all\npossible states of beliefs? Can they reach all possible states of beliefs from\nno previous belief? Can they reach a dogmatic state of beliefs, where\neverything not believed is impossible? Can they make two conditions equally\nbelieved? An application where every possible state of beliefs is sensible\nrequires each state of beliefs to be reachable. An application where conditions\nmay be equally believed requires such a belief state to be reachable. An\napplication where beliefs may become dogmatic requires a way to make them\ndogmatic. Such doxastic states need to be reached in a way or another. Not in\nspecific way, as dictated by a typical belief revision postulate. This is an\nability, not a constraint: the ability of being plastic, equating, dogmatic.\nAmnesic, correcting, believer, damascan, learnable are other abilities. Each\nrevision mechanism owns some of these abilities and lacks the others:\nlexicographic, natural, restrained, very radical, full meet, radical, severe,\nmoderate severe, deep severe, plain severe and deep severe revisions, each of\nthese revisions is proved to possess certain abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02319v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "迭代信念修正：从公设到能力", "tldr": "本文提出了一种新的视角来分析信念修正机制，即关注其“能力”而非传统的“公设”，并证明不同的修正机制拥有不同的能力。", "motivation": "信念修正领域现有大量新提案，但对现有方法的分析不足。传统工作多依赖公设进行语法表征，这些公设限制了特定的修正实例，说明了修正“必须”做什么，却忽略了修正“能够”做什么。例如，修正能否达到某个信念状态？能否达到所有可能的信念状态？能否达到教条主义的信念状态？这些“能力”对于实际应用至关重要，因此需要一种新的分析框架。", "method": "本文提出了“能力”的概念，与传统的“公设”相对。能力指的是信念修正机制可以达到的信念状态或性质，例如可塑性、等同性、教条性、遗忘性、纠正性等。研究者分析了多种现有的信念修正机制（如词典式、自然式、受限式、非常激进式、完全聚合式、激进式、严格式、中度严格式、深度严格式、简单严格式和深度严格式修正），并证明每种修正机制都拥有特定的能力。", "result": "研究结果表明，每种信念修正机制都拥有其特定的能力集合，同时也缺乏其他能力。例如，某些修正机制可能具备可塑性、等同性或教条性等能力。", "conclusion": "传统的信念修正公设侧重于规范修正“必须”做什么，而忽视了修正“能够”做什么。通过引入“能力”的概念，可以更全面地分析和理解不同的信念修正机制。不同的修正机制拥有不同的能力，这对于选择适合特定应用场景的修正机制至关重要。", "translation": "信念修正领域新提案层出不穷，但对现有方法的分析却显得贫乏。许多工作都依赖于公设，将其作为句法特征：某个修正机制等同于某些属性。公设限制了特定的修正实例：某些修正以某种方式更新某些信念。例如，如果修正与当前信念一致，它就会被纳入，无需其他改变。像这样的公设说明了修正必须做什么，却忽略了它们能做什么。它们能达到某种信念状态吗？它们能达到所有可能的信念状态吗？它们能从没有先前的信念中达到所有可能的信念状态吗？它们能达到一种教条主义的信念状态吗，即所有不被相信的事物都是不可能的？它们能使两个条件同样被相信吗？一个所有可能的信念状态都合理的应用程序要求每个信念状态都是可达的。一个条件可能同样被相信的应用程序要求这种信念状态是可达的。一个信念可能变得教条的应用程序要求有一种方法使它们变得教条。这些信念状态需要以某种方式达到，而不是像典型的信念修正公设所规定的那样以特定方式。这是一种能力，而不是一种限制：可塑性、等同性、教条性的能力。遗忘性、纠正性、信仰者、大马士革式、可学习性是其他能力。每种修正机制都拥有其中一些能力，而缺乏另一些：词典式、自然式、受限式、非常激进式、完全聚合式、激进式、严格式、中度严格式、深度严格式、简单严格式和深度严格式修正，这些修正中的每一种都被证明拥有某些能力。", "summary": "本文提出了一种新的框架来分析信念修正机制，即从传统的“公设”（约束）转向关注修正机制的“能力”（可以实现的状态）。作者认为，现有研究过分关注修正“必须”做什么，而忽略了它们“能够”做什么。通过引入“可塑性”、“等同性”、“教条性”等能力概念，论文分析了多种迭代信念修正机制，并证明不同的机制拥有不同的能力集合。这种对能力的关注为理解和选择适合特定应用需求的信念修正方法提供了新的视角。", "keywords": "信念修正, 公设, 能力, 迭代修正, 信念状态", "comments": "本文的创新点在于将信念修正的分析视角从“公设”（约束性规则）转移到“能力”（可实现性）。这种转变提供了一个更实际、更具应用导向的框架来评估不同的信念修正机制。它强调了信念修正系统在不同场景下需要具备的灵活性和表现力，而不是仅仅遵循一套预设的逻辑规则。这对于设计和选择适合特定任务的智能系统具有重要意义。"}}
{"id": "2507.02584", "title": "Observer-Based Distributed Model Predictive Control for String-Stable Multi-vehicle Systems with Markovian Switching Topology", "authors": ["Wenwei Que", "Yang Li", "Lu Wang", "Wentao Liu", "Yougang Bian", "Manjiang Hu", "Yongfu Li"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      8 pages,7 figures,conference", "url": "http://arxiv.org/abs/2507.02584v1", "summary": "Switching communication topologies can cause instability in vehicle platoons,\nas vehicle information may be lost during the dynamic switching process. This\nhighlights the need to design a controller capable of maintaining the stability\nof vehicle platoons under dynamically changing topologies. However, capturing\nthe dynamic characteristics of switching topologies and obtaining complete\nvehicle information for controller design while ensuring stability remains a\nsignificant challenge. In this study, we propose an observer-based distributed\nmodel predictive control (DMPC) method for vehicle platoons under directed\nMarkovian switching topologies. Considering the stochastic nature of the\nswitching topologies, we model the directed switching communication topologies\nusing a continuous-time Markov chain. To obtain the leader vehicle's\ninformation for controller design, we develop a fully distributed adaptive\nobserver that can quickly adapt to the randomly switching topologies, ensuring\nthat the observed information is not affected by the dynamic topology switches.\nAdditionally, a sufficient condition is derived to guarantee the mean-square\nstability of the observer. Furthermore, we construct the DMPC terminal update\nlaw based on the observer and formulate a string stability constraint based on\nthe observed information. Numerical simulations demonstrate that our method can\nreduce tracking errors while ensuring string stability.", "comment": "8 pages,7 figures,conference", "pdf_url": "http://arxiv.org/pdf/2507.02584v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于观测器的马尔可夫切换拓扑下串稳定性多车系统分布式模型预测控制", "tldr": "本文提出了一种基于观测器的分布式模型预测控制（DMPC）方法，用于在马尔可夫切换拓扑下保持车辆队列的串稳定性和跟踪性能。", "motivation": "车辆队列中切换通信拓扑可能导致不稳定，因为动态切换过程中可能丢失车辆信息。在动态变化的拓扑下设计能够保持车辆队列稳定性的控制器，并获取完整的车辆信息以进行控制器设计，同时确保稳定性，是一个重大挑战。", "method": "本研究提出了一种基于观测器的分布式模型预测控制（DMPC）方法，用于有向马尔可夫切换拓扑下的车辆队列。通过连续时间马尔可夫链建模有向切换通信拓扑的随机特性。开发了一种完全分布式的自适应观测器，以快速适应随机切换拓扑并获取领导车辆信息，确保观测信息不受动态拓扑切换影响。此外，推导了一个充分条件以保证观测器的均方稳定性。基于观测器构建了DMPC终端更新律，并基于观测信息制定了串稳定性约束。", "result": "数值仿真表明，本文提出的方法可以在确保串稳定性的同时减少跟踪误差。", "conclusion": "本文提出的基于观测器的分布式模型预测控制方法能够有效处理马尔可夫切换拓扑下的车辆队列稳定性问题，并能减少跟踪误差，保持串稳定性。", "translation": "切换通信拓扑可能导致车辆队列不稳定，因为车辆信息可能在动态切换过程中丢失。这突出表明需要设计一种能够在动态变化的拓扑下保持车辆队列稳定性的控制器。然而，捕获切换拓扑的动态特性并在确保稳定性的同时获取完整的车辆信息以进行控制器设计仍然是一个重大挑战。在这项研究中，我们提出了一种基于观测器的分布式模型预测控制（DMPC）方法，用于有向马尔可夫切换拓扑下的车辆队列。考虑到切换拓扑的随机性，我们使用连续时间马尔可夫链对有向切换通信拓扑进行建模。为了获取领导车辆信息以进行控制器设计，我们开发了一种完全分布式的自适应观测器，该观测器可以快速适应随机切换拓扑，确保观测信息不受动态拓扑切换的影响。此外，推导了一个充分条件以保证观测器的均方稳定性。此外，我们基于观测器构建了DMPC终端更新律，并基于观测信息制定了串稳定性约束。数值仿真表明，我们的方法可以在确保串稳定性的同时减少跟踪误差。", "summary": "本文针对马尔可夫切换拓扑下车辆队列可能因信息丢失导致不稳定问题，提出了一种基于观测器的分布式模型预测控制（DMPC）方法。该方法通过连续时间马尔可夫链建模拓扑切换的随机性，并设计了自适应分布式观测器以准确获取领导车辆信息，确保其不受拓扑动态切换影响。同时，推导了观测器均方稳定性的充分条件，并基于观测信息构建了DMPC终端更新律和串稳定性约束。数值仿真验证了该方法在减少跟踪误差和保持串稳定性方面的有效性。", "keywords": "分布式模型预测控制, 马尔可夫切换拓扑, 串稳定性, 自适应观测器, 车辆队列", "comments": "本文的创新点在于将自适应观测器与分布式模型预测控制相结合，有效解决了马尔可夫切换拓扑下车辆队列的信息丢失和稳定性问题。其提出的完全分布式自适应观测器能够快速适应随机拓扑变化，确保了控制器设计的准确性。该研究对于复杂通信环境下多车系统控制具有重要意义。"}}
{"id": "2507.02092", "title": "Energy-Based Transformers are Scalable Learners and Thinkers", "authors": ["Alexi Gladstone", "Ganesh Nanduru", "Md Mofijul Islam", "Peixuan Han", "Hyeonjeong Ha", "Aman Chadha", "Yilun Du", "Heng Ji", "Jundong Li", "Tariq Iqbal"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02092v1", "summary": "Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02092v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "基于能量的Transformer是可扩展的学习者和思考者", "tldr": "本文提出基于能量的Transformer（EBTs），一种新的模型，能够通过无监督学习进行系统2思维，并在文本和视觉任务中展现出更好的扩展性和性能。", "motivation": "现有的推理时计算技术（类比人类系统2思维）在提高模型性能方面很受欢迎，但它们通常受限于特定模态、特定问题，或需要额外监督。本文旨在探索是否能通过纯无监督学习泛化这些系统2思维方法。", "method": "本文提出了基于能量的Transformer（EBTs），这是一种新型的基于能量的模型（EBMs）。EBTs通过学习显式验证输入与候选预测之间的兼容性，并将预测问题重新定义为对验证器的优化。具体来说，EBTs为每个输入和候选预测对分配一个能量值，并通过基于梯度下降的能量最小化直到收敛来进行预测。", "result": "在训练过程中，EBTs在离散（文本）和连续（视觉）模态下比主流的Transformer++方法扩展更快，在数据、批大小、参数、FLOPs和深度方面实现了高达35%的更高扩展率。在推理过程中，EBTs通过系统2思维在语言任务上比Transformer++性能提升29%。EBTs在图像去噪任务上优于Diffusion Transformers，同时使用更少的前向传播。此外，EBTs在大多数下游任务中，即使在相同或更差的预训练性能下，也比现有模型取得了更好的结果。", "conclusion": "EBTs是扩展模型学习和思考能力的一个有前景的新范式，能够通过无监督学习实现泛化和优异的性能。", "translation": "推理时计算技术，类似于人类的系统2思维，最近在提高模型性能方面变得流行。然而，大多数现有方法存在一些局限性：它们是模态特定的（例如，仅适用于文本）、问题特定的（例如，可验证领域如数学和编程），或者在无监督预训练之上需要额外的监督/训练（例如，验证器或可验证奖励）。在本文中，我们提出了一个问题：“是否有可能泛化这些系统2思维方法，并开发出仅通过无监督学习就能学会思考的模型？”有趣的是，我们发现答案是肯定的，通过学习显式验证输入和候选预测之间的兼容性，然后将预测问题重新定义为对该验证器的优化。具体来说，我们训练基于能量的Transformer（EBTs）——一种新型的基于能量的模型（EBMs）——为每个输入和候选预测对分配一个能量值，通过基于梯度下降的能量最小化直到收敛来实现预测。在离散（文本）和连续（视觉）模态中，我们发现EBTs在训练期间比主流的Transformer++方法扩展更快，在数据、批大小、参数、FLOPs和深度方面实现了高达35%的更高扩展率。在推理过程中，EBTs通过系统2思维在语言任务上比Transformer++性能提升29%，并且EBTs在图像去噪方面优于Diffusion Transformers，同时使用更少的前向传播。此外，我们发现EBTs在大多数下游任务中，在相同或更差的预训练性能下，也比现有模型取得了更好的结果，这表明EBTs比现有方法具有更好的泛化能力。因此，EBTs是扩展模型学习和思考能力的一个有前景的新范式。", "summary": "本文提出了一种名为基于能量的Transformer（EBTs）的新型模型，旨在解决现有系统2思维方法在模态和问题特异性以及对额外监督需求方面的局限。EBTs通过学习显式验证输入与候选预测的兼容性，并将预测视为能量最小化问题。实验结果表明，EBTs在训练扩展性上优于Transformer++，并在推理时通过系统2思维显著提升了语言任务性能，同时在图像去噪和多个下游任务中表现出色，证明了其优异的泛化能力和作为新范式的潜力。", "keywords": "基于能量的模型, Transformer, 系统2思维, 无监督学习, 可扩展性", "comments": "这篇论文的创新点在于提出了EBTs，将能量模型与Transformer架构结合，并成功地将“系统2思维”泛化到无监督学习框架中，克服了现有方法的模态和问题局限性以及对额外监督的依赖。其重要性在于提供了一种新的模型范式，不仅在学习效率和扩展性上表现优异，尤其是在推理时的性能提升和跨模态泛化能力，为未来模型发展提供了新的方向。"}}
{"id": "2507.02802", "title": "AREE-Based Decoupled Design of Hybrid Beamformers in mmWave XL-MIMO Systems", "authors": ["Jiazhe Li", "Nicolò Decarli", "Francesco Guidi", "Heng Dong", "Anna Guerra", "Alessandro Bazzi", "Zhuoming Li"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02802v1", "summary": "Hybrid beamforming has been widely employed in mmWave communications such as\nvehicular-to-everything (V2X) scenarios, as a compromise between hardware\ncomplexity and spectral efficiency. However, the inherent coupling between\nanalog and digital precoders in hybrid array architecture significantly limits\nthe computational and spectral efficiency of existing algorithms. To address\nthis issue, we propose an alternating residual error elimination (AREE)\nalgorithm, which decomposes the hybrid beamforming problem into two\nlow-dimensional subproblems, each exhibiting a favorable matrix structure that\nenables effective decoupling of analog and digital precoders from the matrix\nproduct formulation. These subproblems iteratively eliminate each other's\nresidual errors, driving the original problem toward the optimal hybrid\nbeamforming performance. The proposed initialization ensures rapid convergence,\nwhile a low-complexity geometric channel SVD algorithm is developed by\ntransforming the high-dimensional sparse channel into a low-dimensional\nequivalent, thereby simplifying the derivation of subproblems. Simulation\nresults demonstrate that the AREE algorithm effectively decouples analog and\ndigital precoders with low complexity, achieves fast convergence, and offers\nhigher spectral efficiency than existing beamforming methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02802v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于AREE的毫米波XL-MIMO系统中混合波束赋形器的解耦设计", "tldr": "提出了一种基于AREE的混合波束赋形解耦设计，显著提升了毫米波XL-MIMO系统的频谱效率和收敛速度。", "motivation": "现有混合阵列架构中模拟和数字预编码器固有的耦合显著限制了算法的计算和频谱效率。", "method": "提出了一种交替残差消除（AREE）算法，将混合波束赋形问题分解为两个低维子问题，通过有利的矩阵结构实现模拟和数字预编码器的有效解耦。子问题迭代消除彼此的残差以逼近最优性能。算法采用快速收敛的初始化方法，并开发了一种低复杂度的几何信道SVD算法，将高维稀疏信道转化为低维等效形式以简化子问题推导。", "result": "仿真结果表明，AREE算法能以低复杂度有效解耦模拟和数字预编码器，实现快速收敛，并比现有波束赋形方法提供更高的频谱效率。", "conclusion": "AREE算法成功解决了混合波束赋形中的耦合问题，有效提升了毫米波XL-MIMO系统的性能。", "translation": "混合波束赋形作为硬件复杂度和频谱效率之间的折衷方案，已广泛应用于毫米波通信，例如车联网（V2X）场景。然而，混合阵列架构中模拟和数字预编码器固有的耦合显著限制了现有算法的计算和频谱效率。为解决此问题，我们提出了一种交替残差消除（AREE）算法，该算法将混合波束赋形问题分解为两个低维子问题，每个子问题都呈现出有利的矩阵结构，能够从矩阵乘积公式中有效解耦模拟和数字预编码器。这些子问题迭代消除彼此的残差，从而使原始问题趋向于最优的混合波束赋形性能。所提出的初始化方法确保了快速收敛，同时通过将高维稀疏信道转换为低维等效形式，开发了一种低复杂度的几何信道SVD算法，从而简化了子问题的推导。仿真结果表明，AREE算法能以低复杂度有效解耦模拟和数字预编码器，实现快速收敛，并比现有波束赋形方法提供更高的频谱效率。", "summary": "本文针对毫米波XL-MIMO混合波束赋形中模拟与数字预编码器耦合导致的计算与频谱效率限制，提出了一种交替残差消除（AREE）算法。该算法将问题分解为低维子问题以实现解耦设计，并通过新颖的初始化和低复杂度几何信道SVD算法确保快速收敛。仿真结果表明，AREE有效解耦了预编码器，收敛迅速，并提供了比现有方法更高的频谱效率。", "keywords": "混合波束赋形, 毫米波, XL-MIMO, 解耦设计, AREE算法", "comments": "该论文提出了一种创新的AREE算法，通过将混合波束赋形问题分解为可解耦的低维子问题，有效地解决了模拟与数字预编码器之间的固有耦合问题。其迭代残差消除机制、快速收敛的初始化方法以及低复杂度的几何信道SVD算法是其主要创新点，对于提升毫米波XL-MIMO系统在V2X等场景下的频谱效率和计算效率具有重要意义。"}}
{"id": "2507.02671", "title": "Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs", "authors": ["Francesco Di Salvo", "Hanh Huyen My Nguyen", "Christian Ledig"], "categories": ["cs.LG", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025", "url": "http://arxiv.org/abs/2507.02671v1", "summary": "Deep Learning (DL) has revolutionized medical imaging, yet its adoption is\nconstrained by data scarcity and privacy regulations, limiting access to\ndiverse datasets. Federated Learning (FL) enables decentralized training but\nsuffers from high communication costs and is often restricted to a single\ndownstream task, reducing flexibility. We propose a data-sharing method via\nDifferentially Private (DP) generative models. By adopting foundation models,\nwe extract compact, informative embeddings, reducing redundancy and lowering\ncomputational overhead. Clients collaboratively train a Differentially Private\nConditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware\ndata distribution, supporting diverse downstream tasks. Our approach, validated\nacross multiple feature extractors, enhances privacy, scalability, and\nefficiency, outperforming traditional FL classifiers while ensuring\ndifferential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings\nthan DP-CGAN while requiring $5{\\times}$ fewer parameters.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.02671v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于嵌入的联邦数据共享：通过差分隐私条件变分自编码器实现", "tldr": "本文提出一种通过差分隐私生成模型进行联邦数据共享的方法，利用基础模型提取嵌入，并通过差分隐私条件变分自编码器（DP-CVAE）建模全局隐私保护数据分布，解决了医疗影像领域的数据稀缺和隐私问题，并提升了效率和性能。", "motivation": "深度学习在医疗影像领域受限于数据稀缺和隐私法规，导致难以获取多样化数据集。联邦学习虽能去中心化训练，但存在高通信成本且常限于单一任务，缺乏灵活性。", "method": "本文提出一种通过差分隐私（DP）生成模型的数据共享方法。首先，利用基础模型提取紧凑、信息丰富的嵌入，以减少冗余并降低计算开销。接着，客户端协同训练一个差分隐私条件变分自编码器（DP-CVAE）来建模一个全局的、隐私保护的数据分布，以支持多样化的下游任务。", "result": "该方法在多个特征提取器上得到验证，增强了隐私性、可扩展性和效率。它优于传统的联邦学习分类器，并确保了差分隐私。此外，DP-CVAE生成的嵌入比DP-CGAN具有更高保真度，且所需参数减少5倍。", "conclusion": "通过利用基础模型提取嵌入并协同训练差分隐私条件变分自编码器，本方法有效解决了医疗影像数据共享中的隐私和效率问题，提供了优于传统联邦学习的性能，并能支持多样化的下游任务。", "translation": "深度学习（DL）彻底改变了医学影像领域，但其应用受到数据稀缺和隐私法规的限制，这局限了对多样化数据集的访问。联邦学习（FL）实现了去中心化训练，但存在通信成本高的问题，并且通常局限于单一的下游任务，降低了灵活性。我们提出了一种通过差分隐私（DP）生成模型进行数据共享的方法。通过采用基础模型，我们提取紧凑、信息丰富的嵌入，减少了冗余并降低了计算开销。客户端协同训练一个差分隐私条件变分自编码器（DP-CVAE）来建模一个全局的、隐私保护的数据分布，支持多样化的下游任务。我们的方法在多个特征提取器上得到了验证，增强了隐私性、可扩展性和效率，优于传统的联邦学习分类器，同时确保了差分隐私。此外，DP-CVAE产生比DP-CGAN更高保真度的嵌入，同时所需参数减少5倍。", "summary": "本文提出一种基于嵌入的联邦数据共享新范式，旨在解决医疗影像领域数据稀缺与隐私限制的挑战。研究利用基础模型生成紧凑嵌入，并通过协同训练差分隐私条件变分自编码器（DP-CVAE）来学习全局隐私保护数据分布。该方法不仅有效降低了联邦学习的通信成本和任务单一性限制，还在实验中展现出优于传统联邦学习分类器的性能，显著提升了隐私保护、可扩展性和效率，同时以更少的参数生成了更高质量的嵌入。", "keywords": "联邦学习, 差分隐私, 条件变分自编码器, 数据共享, 医疗影像", "comments": "该论文的创新点在于将基础模型生成的嵌入与差分隐私条件变分自编码器（DP-CVAE）相结合，实现了高效且隐私保护的联邦数据共享。这种方法不仅解决了传统联邦学习在通信成本和任务灵活性上的不足，还通过生成高保真度嵌入提升了数据质量，为医疗影像等敏感数据领域提供了实用的解决方案。其在隐私、效率和性能上的多重提升，使其在未来联邦学习应用中具有重要潜力。"}}
{"id": "2507.02496", "title": "Online Conformal Prediction with Efficiency Guarantees", "authors": ["Vaidehi Srinivas"], "categories": ["cs.LG", "cs.DS", "math.ST", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02496v1", "summary": "We study the problem of conformal prediction in a novel online framework that\ndirectly optimizes efficiency. In our problem, we are given a target\nmiscoverage rate $\\alpha > 0$, and a time horizon $T$. On each day $t \\le T$ an\nalgorithm must output an interval $I_t \\subseteq [0, 1]$, then a point $y_t \\in\n[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,\n$y_t \\in I_t$ on (close to) a $(1 - \\alpha)$-fraction of days, while\nmaintaining efficiency, that is, minimizing the average volume (length) of the\nintervals played. This problem is an online analogue to the problem of\nconstructing efficient confidence intervals.\n  We study this problem over arbitrary and exchangeable (random order) input\nsequences. For exchangeable sequences, we show that it is possible to construct\nintervals that achieve coverage $(1 - \\alpha) - o(1)$, while having length\nupper bounded by the best fixed interval that achieves coverage in hindsight.\nFor arbitrary sequences however, we show that any algorithm that achieves a\n$\\mu$-approximation in average length compared to the best fixed interval\nachieving coverage in hindsight, must make a multiplicative factor more\nmistakes than $\\alpha T$, where the multiplicative factor depends on $\\mu$ and\nthe aspect ratio of the problem. Our main algorithmic result is a matching\nalgorithm that can recover all Pareto-optimal settings of $\\mu$ and number of\nmistakes. Furthermore, our algorithm is deterministic and therefore robust to\nan adaptive adversary.\n  This gap between the exchangeable and arbitrary settings is in contrast to\nthe classical online learning problem. In fact, we show that no single\nalgorithm can simultaneously be Pareto-optimal for arbitrary sequences and\noptimal for exchangeable sequences. On the algorithmic side, we give an\nalgorithm that achieves the near-optimal tradeoff between the two cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02496v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "在线共形预测与效率保证", "tldr": "该论文研究了在线共形预测问题，重点关注效率优化。它揭示了可交换序列和任意序列之间在性能保证上的显著差距，并提出了一种算法来恢复任意序列的帕累托最优设置，以及一种在两种情况之间实现接近最优权衡的算法。", "motivation": "该论文研究了一个新颖的在线框架下的共形预测问题，其主要动机是直接优化效率。这类似于构建高效置信区间的在线版本，目标是在保证覆盖率的同时最小化预测区间的平均长度。", "method": "该研究提出了一个直接优化效率的在线共形预测框架。对于可交换序列，他们展示了如何构建区间以实现接近目标覆盖率并使其长度受事后最佳固定区间限制。对于任意序列，他们证明了在平均长度上实现近似的任何算法都会导致乘法因子更多的错误。此外，他们开发了一个匹配的确定性算法，能够恢复任意序列的所有帕累托最优设置。最后，他们提供了一个在可交换和任意序列之间实现接近最优权衡的算法。", "result": "对于可交换序列，算法能够实现接近 $(1 - \beta)$ 的覆盖率，且区间长度上限由事后实现覆盖的最佳固定区间给出。对于任意序列，任何在平均长度上达到 $\beta$ 近似的算法，其错误数量必须是 $\beta T$ 的乘法倍数。研究提出了一个匹配算法，可以恢复所有帕累托最优的错误数量和 $\beta$ 设置，并且该算法是确定性的。此外，研究表明没有单一算法可以同时对任意序列实现帕累托最优并对可交换序列实现最优，但提出了一个在两种情况之间实现接近最优权衡的算法。", "conclusion": "在线共形预测在可交换和任意输入序列之间的性能保证存在根本性差异，这与经典在线学习问题形成对比。没有单一算法能同时对任意序列和可交换序列都达到最优，但可以设计算法在两者之间实现接近最优的权衡。", "translation": "我们研究了一种新颖的在线框架下的共形预测问题，该框架直接优化效率。在我们的问题中，给定一个目标误覆盖率 $\\alpha > 0$ 和一个时间范围 $T$。在每天 $t \\le T$，算法必须输出一个区间 $I_t \\subseteq [0, 1]$，然后揭示一个点 $y_t \\in [0, 1]$。算法的目标是实现覆盖，即在（接近）$(1 - \\alpha)$ 比例的天数中 $y_t \\in I_t$，同时保持效率，即最小化所生成区间的平均体积（长度）。这个问题是构建高效置信区间的在线类比。\n我们研究了任意和可交换（随机顺序）输入序列下的这个问题。对于可交换序列，我们表明可以构建区间，实现覆盖率 $(1 - \\alpha) - o(1)$，同时其长度上限由事后实现覆盖的最佳固定区间给出。然而，对于任意序列，我们表明任何算法，如果其平均长度相对于事后实现覆盖的最佳固定区间达到 $\\mu$ 近似，则必须比 $\\alpha T$ 犯更多倍的错误，其中倍数因子取决于 $\\mu$ 和问题的纵横比。我们的主要算法结果是一个匹配算法，可以恢复所有帕累托最优的 $\\mu$ 和错误数量设置。此外，我们的算法是确定性的，因此对自适应对抗者具有鲁棒性。\n可交换和任意设置之间的这种差距与经典的在线学习问题形成对比。事实上，我们表明没有单一算法可以同时对任意序列实现帕累托最优并对可交换序列实现最优。在算法方面，我们给出了一个在两种情况之间实现接近最优权衡的算法。", "summary": "本文提出了一种新颖的在线共形预测框架，旨在直接优化效率，同时确保预测覆盖率。研究针对可交换和任意输入序列分析了该问题，发现两者之间存在显著的性能差距。对于可交换序列，可以构建实现接近目标覆盖率且长度受事后最佳固定区间限制的预测区间。然而，对于任意序列，为了实现平均长度的近似优化，错误率将成倍增加。论文提出了一个匹配的确定性算法，能够恢复任意序列的所有帕累托最优设置，并进一步证明没有单一算法可以同时对两种序列类型都达到最优。最终，提供了一个在两种情况之间实现接近最优权衡的算法。", "keywords": "在线共形预测, 效率保证, 置信区间, 可交换序列, 任意序列", "comments": "这篇论文创新性地将共形预测扩展到在线设置，并明确关注效率优化，这具有重要的实际意义。对可交换序列和任意序列之间性能差距的识别和分析是一项重要的理论贡献，揭示了在线共形预测的根本局限性。所提出的算法，特别是针对任意序列的确定性算法，提供了实用的解决方案。与经典在线学习问题的对比加深了对在线共形预测的理解。"}}
{"id": "2507.02428", "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This version has been reviewed and accepted for presentation at the InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5 pages and 3 tables", "url": "http://arxiv.org/abs/2507.02428v1", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.", "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.02428v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "低资源语言中障碍言语社区驱动数据收集的“食谱”", "tldr": "本研究提出了一种为低资源语言中障碍言语收集语音样本的方法，并开发了一个“食谱”指导社区驱动的数据收集和ASR模型构建，旨在民主化ASR技术，并提供了阿坎语障碍言语的首个开源数据集。", "motivation": "本研究旨在为低资源语言中的障碍言语构建自动语音识别（ASR）模型，并通过开发最佳实践“食谱”和培训，实现社区驱动的数据收集，从而民主化ASR技术和数据收集。", "method": "本研究提出了一种收集语音样本的方法，用于构建障碍言语的ASR模型，特别是针对低资源语言。它开发了一个“食谱”，包含最佳实践和培训，以实现社区驱动的数据收集和ASR模型构建。作为概念验证，研究整理了阿坎语（加纳的一种广泛使用的本土语言）障碍言语的首个开源数据集，并涉及了来自不同背景的言语障碍参与者。此外，研究还对开源ASR模型进行了微调，以更好地识别阿坎语中的障碍言语。", "result": "研究成果包括：策展了阿坎语障碍言语的首个开源数据集；提供了包含最佳实践的“食谱”和开源工具，这些都已公开可用；以及展示了微调开源ASR模型以更好识别阿坎语障碍言语的初步结果。", "conclusion": "本研究的结论是，通过提供公开可用的数据集、“食谱”和工具，能够使研究人员和实践者创建针对言语障碍个体独特需求的包容性ASR技术。", "translation": "本研究提出了一种收集语音样本以构建针对障碍言语，特别是低资源语言的自动语音识别（ASR）模型的方法。它旨在通过开发一套最佳实践的“食谱”和培训，用于社区驱动的数据收集和ASR模型构建，从而实现ASR技术和数据收集的民主化。作为概念验证，本研究整理了阿坎语（加纳一种广泛使用的本土语言）中障碍言语的首个开源数据集。该研究涉及了来自不同背景的言语障碍参与者。由此产生的数据集，连同该“食谱”和开源工具，都已公开可用，以使研究人员和实践者能够创建针对言语障碍个体独特需求的包容性ASR技术。此外，本研究还展示了对开源ASR模型进行微调以更好识别阿坎语中障碍言语的初步结果。", "summary": "本研究提出了一种针对低资源语言中障碍言语的社区驱动数据收集方法，旨在民主化ASR技术。它开发了一个包含最佳实践的“食谱”和培训指南，并作为概念验证，构建了阿坎语障碍言语的首个开源数据集。该数据集、食谱和开源工具均已公开，旨在促进开发包容性ASR技术。研究还展示了微调开源ASR模型以识别阿坎语障碍言语的初步成果。", "keywords": "障碍言语, 低资源语言, 自动语音识别, 社区驱动, 数据收集", "comments": "本研究的创新之处在于其“食谱”方法和社区驱动的数据收集模式，这对于低资源语言和特殊群体的数据稀缺问题提供了实际解决方案。其重要性体现在促进ASR技术的包容性和民主化，特别关注了被忽视的言语障碍群体。通过提供开源数据集和工具，该研究为未来的相关研究奠定了基础。"}}
{"id": "2507.02635", "title": "SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection", "authors": ["Mao Luo", "Zhi Wang", "Yiwen Huang", "Qingyun Zhang", "Zhouxing Su", "Zhipeng Lv", "Wen Hu", "Jianguo Li"], "categories": ["cs.CR", "cs.DB"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02635v1", "summary": "Electronic payment platforms are estimated to process billions oftransactions\ndaily, with the cumulative value of these transactionspotentially reaching into\nthe trillions. Even a minor error within thishigh-volume environment could\nprecipitate substantial financiallosses. To mitigate this risk, manually\nconstructed verification rules,developed by domain experts, are typically\nemployed to identifyand scrutinize transactions in production environments.\nHowever,due to the absence of a systematic approach to ensure the robust-ness\nof these verification rules against vulnerabilities, they remainsusceptible to\nexploitation.To mitigate this risk, manually constructed verification rules,\nde-veloped by domain experts, are typically employed to identify andscrutinize\ntransactions in production environments. However, dueto the absence of a\nsystematic approach to ensure the robustness ofthese verification rules against\nvulnerabilities, they remain suscep-tible to exploitation. To ensure data\nsecurity, database maintainersusually compose complex verification rules to\ncheck whether aquery/update request is valid. However, the rules written by\nex-perts are usually imperfect, and malicious requests may bypassthese rules.\nAs a result, the demand for identifying the defects ofthe rules systematically\nemerges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02635v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SAT-BO：欺诈交易检测的验证规则学习与优化", "tldr": "现有欺诈交易检测的手动验证规则存在漏洞，容易被绕过，需要系统性方法来识别规则缺陷。", "motivation": "电子支付平台交易量巨大，即使小错误也可能导致巨额损失。当前依赖专家手动构建的验证规则，但这些规则缺乏系统性方法保障其健壮性，容易被恶意请求绕过，因此需要系统性地识别规则缺陷。", "method": "标题提到了“SAT-BO: Verification Rule Learning and Optimization”，但抽象中没有详细说明具体的方法论。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "电子支付平台预计每天处理数十亿笔交易，这些交易的累计价值可能达到数万亿美元。在这种高交易量的环境中，即使是微小的错误也可能导致巨大的经济损失。为了降低这种风险，通常采用领域专家手动构建的验证规则来识别和审查生产环境中的交易。然而，由于缺乏确保这些验证规则抵御漏洞的系统方法，它们仍然容易受到利用。为了降低这种风险，通常采用领域专家手动构建的验证规则来识别和审查生产环境中的交易。然而，由于缺乏确保这些验证规则抵御漏洞的系统方法，它们仍然容易受到利用。为了确保数据安全，数据库维护者通常会编写复杂的验证规则来检查查询/更新请求是否有效。然而，专家编写的规则通常不完善，恶意请求可能会绕过这些规则。因此，系统性地识别规则缺陷的需求应运而生。", "summary": "鉴于电子支付平台巨大的交易量和潜在的巨额损失，本研究旨在解决现有欺诈交易检测中手动构建的验证规则的缺陷。这些由领域专家制定的规则缺乏系统性方法来确保其健壮性，容易被恶意请求绕过。因此，迫切需要一种系统性的方法来识别并优化这些验证规则中的漏洞，以增强数据安全。", "keywords": "欺诈交易检测, 验证规则, 规则学习, 规则优化, 漏洞识别", "comments": "抽象主要强调了欺诈交易检测领域中现有手动验证规则的局限性和漏洞，指出了系统性识别规则缺陷的必要性。论文标题“SAT-BO: Verification Rule Learning and Optimization”暗示了其创新的方法论，但抽象中并未详细阐述该方法的具体内容和优势。因此，从现有信息来看，其创新性和具体贡献尚不明确，主要聚焦于问题的提出。"}}
{"id": "2507.02447", "title": "HAC-LOCO: Learning Hierarchical Active Compliance Control for Quadruped Locomotion under Continuous External Disturbances", "authors": ["Xiang Zhou", "Xinyu Zhang", "Qingrui Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 7 Figures", "url": "http://arxiv.org/abs/2507.02447v1", "summary": "Despite recent remarkable achievements in quadruped control, it remains\nchallenging to ensure robust and compliant locomotion in the presence of\nunforeseen external disturbances. Existing methods prioritize locomotion\nrobustness over compliance, often leading to stiff, high-frequency motions, and\nenergy inefficiency. This paper, therefore, presents a two-stage hierarchical\nlearning framework that can learn to take active reactions to external force\ndisturbances based on force estimation. In the first stage, a velocity-tracking\npolicy is trained alongside an auto-encoder to distill historical\nproprioceptive features. A neural network-based estimator is learned through\nsupervised learning, which estimates body velocity and external forces based on\nproprioceptive measurements. In the second stage, a compliance action module,\ninspired by impedance control, is learned based on the pre-trained encoder and\npolicy. This module is employed to actively adjust velocity commands in\nresponse to external forces based on real-time force estimates. With the\ncompliance action module, a quadruped robot can robustly handle minor\ndisturbances while appropriately yielding to significant forces, thus striking\na balance between robustness and compliance. Simulations and real-world\nexperiments have demonstrated that our method has superior performance in terms\nof robustness, energy efficiency, and safety. Experiment comparison shows that\nour method outperforms the state-of-the-art RL-based locomotion controllers.\nAblation studies are given to show the critical roles of the compliance action\nmodule.", "comment": "8 pages, 7 Figures", "pdf_url": "http://arxiv.org/pdf/2507.02447v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "HAC-LOCO：在持续外部扰动下四足机器人运动的分层主动柔顺控制学习", "tldr": "本文提出了一种名为HAC-LOCO的两阶段分层学习框架，旨在使四足机器人在持续外部扰动下实现鲁棒且柔顺的运动，有效平衡鲁棒性和能效。", "motivation": "尽管四足机器人控制取得了显著进展，但在不可预见的外部扰动存在下，确保鲁棒和柔顺的运动仍然具有挑战性。现有方法优先考虑运动鲁棒性而非柔顺性，这通常导致僵硬、高频运动和低能效。", "method": "本文提出了一种两阶段分层学习框架。在第一阶段，训练一个速度跟踪策略和一个自编码器来提取历史本体感受特征，并通过监督学习训练一个神经网络估计器来估计身体速度和外部力。在第二阶段，基于预训练的编码器和策略，学习一个受阻抗控制启发的柔顺动作模块，该模块根据实时力估计主动调整速度指令以响应外部力。", "result": "仿真和真实世界实验表明，该方法在鲁棒性、能效和安全性方面表现卓越。与最先进的基于强化学习的运动控制器相比，本方法性能更优。消融研究也证实了柔顺动作模块的关键作用。", "conclusion": "所提出的HAC-LOCO框架使四足机器人能够鲁棒地处理轻微扰动，同时适当地屈服于显著力，从而在鲁棒性和柔顺性之间取得平衡，并展示出优于现有方法的性能。", "translation": "尽管四足机器人控制取得了显著进展，但在不可预见的外部扰动存在下，确保鲁棒和柔顺的运动仍然具有挑战性。现有方法优先考虑运动鲁棒性而非柔顺性，这通常导致僵硬、高频运动和低能效。因此，本文提出了一种两阶段分层学习框架，该框架可以根据力估计学习对外部力扰动采取主动反应。在第一阶段，训练一个速度跟踪策略以及一个自编码器，以提取历史本体感受特征。通过监督学习训练一个基于神经网络的估计器，该估计器根据本体感受测量估计身体速度和外部力。在第二阶段，基于预训练的编码器和策略，学习一个受阻抗控制启发的柔顺动作模块。该模块用于根据实时力估计，主动调整速度指令以响应外部力。通过柔顺动作模块，四足机器人可以鲁棒地处理轻微扰动，同时适当地屈服于显著力，从而在鲁棒性和柔顺性之间取得平衡。仿真和真实世界实验表明，我们的方法在鲁棒性、能效和安全性方面具有卓越的性能。实验比较表明，我们的方法优于最先进的基于强化学习的运动控制器。消融研究表明了柔顺动作模块的关键作用。", "summary": "本文介绍了HAC-LOCO，一个两阶段分层学习框架，旨在使四足机器人在持续外部扰动下实现鲁棒且柔顺的运动。该框架解决了现有方法过度强调鲁棒性而非柔顺性导致僵硬和低效运动的局限性。第一阶段涉及训练速度跟踪策略和基于神经网络的估计器，用于力与速度估计。第二阶段学习一个受阻抗控制启发的柔顺动作模块，该模块根据实时力估计主动调整速度指令。这使得机器人能够鲁棒地处理轻微扰动，同时对显著力进行适当的屈服。实验结果表明，与最先进的方法相比，HAC-LOCO显著提高了鲁棒性、能效和安全性。", "keywords": "分层学习, 主动柔顺控制, 四足运动, 外部扰动, 力估计", "comments": "该论文的创新之处在于其两阶段分层学习框架，该框架将力估计与柔顺动作模块相结合，从而在鲁棒性和柔顺性之间实现了平衡。这解决了以往方法的一个关键局限性。基于实时力估计主动调整速度指令是其核心特点。论文展示了在鲁棒性、能效和安全性方面的卓越性能，并通过真实世界实验验证，突显了其重要的实际应用价值。"}}
{"id": "2507.02665", "title": "Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?", "authors": ["Timo Kehrer", "Robert Haines", "Guido Juckeland", "Shurui Zhou", "David E. Bernholdt"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Early access journal version: T. Kehrer, R. Haines, G. Juckeland, S. Zhou and D. E. Bernholdt, \"Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?,\" in Computing in Science & Engineering, doi: https://doi.org/10.1109/MCSE.2025.3557236", "url": "http://arxiv.org/abs/2507.02665v1", "summary": "Anecdotal evidence suggests that Research Software Engineers (RSEs) and\nSoftware Engineering Researchers (SERs) often use different terminologies for\nsimilar concepts, creating communication challenges. To better understand these\ndivergences, we have started investigating how SE fundamentals from the SER\ncommunity are interpreted within the RSE community, identifying aligned\nconcepts, knowledge gaps, and areas for potential adaptation. Our preliminary\nfindings reveal opportunities for mutual learning and collaboration, and our\nsystematic methodology for terminology mapping provides a foundation for a\ncrowd-sourced extension and validation in the future.", "comment": "Early access journal version: T. Kehrer, R. Haines, G. Juckeland, S.\n  Zhou and D. E. Bernholdt, \"Do Research Software Engineers and Software\n  Engineering Researchers Speak the Same Language?,\" in Computing in Science &\n  Engineering, doi: 10.1109/MCSE.2025.3557236", "pdf_url": "http://arxiv.org/pdf/2507.02665v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "研究软件工程师和软件工程研究人员说同一种语言吗？", "tldr": "研究软件工程师（RSEs）和软件工程研究人员（SERs）在术语上存在差异，导致沟通障碍。本研究旨在理解这些差异，并发现相互学习和协作的机会，同时提出了一种系统的术语映射方法。", "motivation": "轶事证据表明，研究软件工程师（RSEs）和软件工程研究人员（SERs）在相似概念上常常使用不同的术语，从而造成沟通障碍。本研究的动机是为了更好地理解这些术语分歧。", "method": "研究人员开始调查软件工程研究人员（SER）社区中的软件工程基本原理在研究软件工程师（RSE）社区中是如何被解读的，以识别一致的概念、知识空白和潜在的适应领域。他们采用了一种系统的术语映射方法。", "result": "初步发现揭示了相互学习和协作的机会。所提出的系统性术语映射方法为未来众包扩展和验证奠定了基础。", "conclusion": "尽管研究软件工程师和软件工程研究人员之间存在术语差异导致的沟通挑战，但存在相互学习和协作的显著机会，并且可以通过系统性的术语映射方法来促进这种合作。", "translation": "轶事证据表明，研究软件工程师（RSEs）和软件工程研究人员（SERs）在相似概念上常常使用不同的术语，从而造成沟通障碍。为了更好地理解这些分歧，我们开始调查软件工程研究人员社区中的软件工程基本原理在研究软件工程师社区中是如何被解读的，旨在识别一致的概念、知识空白以及潜在的适应领域。我们的初步发现揭示了相互学习和协作的机会，而我们系统性的术语映射方法为未来众包扩展和验证奠定了基础。", "summary": "本文探讨了研究软件工程师（RSEs）和软件工程研究人员（SERs）之间因术语差异造成的沟通鸿沟。作者通过调查软件工程核心概念在两个社区中的理解方式，旨在识别共同点、知识空白和适应领域。初步研究结果表明存在相互学习和协作的潜力，并提出了一种系统性的术语映射方法，作为未来众包工作的基础。", "keywords": "研究软件工程师, 软件工程研究人员, 术语映射, 沟通, 软件工程", "comments": "这篇论文解决了研究软件跨学科领域中一个实际且重要的沟通问题。提出的系统性术语映射方法具有创新性，特别是其众包扩展的潜力，这可能显著弥合理论与实践之间的差距。其局限性在于目前的研究结果是初步的。"}}
{"id": "2507.02350", "title": "From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance", "authors": ["Hao Tang", "Songyun Xie", "Xinzhou Xie", "Can Liao", "Xin Zhang", "Bohan Li", "Zhongyu Tian", "Dalu Zheng"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02350v1", "summary": "Traditional video-induced emotion physiological datasets often use\nwhole-trial annotation, assigning a single emotion label to all data collected\nduring an entire trial. This coarse-grained annotation approach misaligns with\nthe dynamic and temporally localized nature of emotional responses as they\nunfold with video narratives, introducing label noise that limits emotion\nrecognition algorithm evaluation and performance. To solve the label noise\nproblem caused by coarse-grained annotation, we propose a fine-grained\nannotation method through an immediate recall paradigm. This paradigm\nintegrates an immediate video replay phase after the initial stimulus viewing,\nallowing participants to precisely mark the onset timestamp, emotion label, and\nintensity based on their immediate recall. We validate this paradigm through\nphysiological evidence and recognition performance. Physiological validation of\nmultimodal signals within participant-marked windows revealed rhythm-specific\nEEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of\nhigh-arousal versus 6% of low-arousal emotion windows. These objective\nphysiological data changes strongly aligned with subjective annotations,\nconfirming annotation precision. For recognition performance, classification\nexperiments showed that models trained on fine-grained annotations achieved\n9.7% higher accuracy than traditional whole-trial labeling, despite using less\ndata. This work not only addresses label noise through fine-grained annotation\nbut also demonstrates that annotation precision outweighs data scale in\ndetermining emotion recognition performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02350v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "从粗粒度到细粒度情感标注：一种通过生理证据和识别性能验证的即时回忆范式", "tldr": "本研究提出了一种即时回忆范式，用于实现细粒度情感标注，以解决传统粗粒度标注带来的标签噪声问题，并通过生理证据和识别性能验证了其有效性。", "motivation": "传统的视频诱发情感生理数据集常使用整段标注，将单一情感标签分配给整个试验期间收集的所有数据。这种粗粒度标注方法与情感反应随视频叙事展开时的动态和时间局部性不符，引入了标签噪声，从而限制了情感识别算法的评估和性能。", "method": "我们提出了一种通过即时回忆范式的细粒度情感标注方法。该范式在初始刺激观看后，立即整合了一个视频回放阶段，允许参与者根据其即时回忆精确地标记情感的发生时间、情感标签和强度。", "result": "生理验证显示，在参与者标记的窗口内，多模态信号揭示了节律特异性脑电图模式和唤醒依赖性皮肤电反应（高唤醒情感窗口中91%出现皮肤电反应，低唤醒情感窗口中6%出现）。这些客观的生理数据变化与主观标注高度一致，证实了标注的精确性。识别性能方面，使用细粒度标注训练的模型比传统整段标注的模型准确率高出9.7%，尽管使用了更少的数据。", "conclusion": "本研究不仅通过细粒度标注解决了标签噪声问题，还表明标注精度在决定情感识别性能方面比数据规模更重要。", "translation": "传统的视频诱发情感生理数据集通常采用整段标注，即为整个试验期间收集的所有数据分配一个单一的情感标签。这种粗粒度标注方法与情感反应随视频叙事动态展开和时间局部化的特性不符，引入了标签噪声，从而限制了情感识别算法的评估和性能。为了解决粗粒度标注引起的标签噪声问题，我们提出了一种通过即时回忆范式的细粒度标注方法。该范式在初始刺激观看后，立即整合了一个视频回放阶段，允许参与者根据其即时回忆精确地标记发生时间、情感标签和强度。我们通过生理证据和识别性能验证了该范式。参与者标记窗口内的多模态信号生理验证显示，存在节律特异性脑电图模式和唤醒依赖性皮肤电反应——高唤醒情感窗口中有91%出现皮肤电反应，而低唤醒情感窗口中只有6%。这些客观的生理数据变化与主观标注高度一致，证实了标注的精确性。在识别性能方面，分类实验表明，尽管使用的数据较少，但使用细粒度标注训练的模型比传统整段标注的准确率高出9.7%。这项工作不仅通过细粒度标注解决了标签噪声问题，而且还表明标注精度在决定情感识别性能方面比数据规模更重要。", "summary": "本研究提出了一种创新的即时回忆范式，旨在解决传统粗粒度情感标注带来的标签噪声问题。该范式允许参与者在观看视频后立即精确标记情感的发生时间、类型和强度。通过生理信号（EEG和GSR）的客观验证，证明了其标注的精确性，且生理数据变化与主观标注高度一致。实验结果显示，使用细粒度标注训练的情感识别模型，即使数据量较少，也比传统整段标注的模型准确率显著提高9.7%。这表明在情感识别中，标注的精度远比数据规模更为关键。", "keywords": "情感标注, 细粒度, 即时回忆, 生理证据, 情感识别性能", "comments": "该论文提出了一种新颖的细粒度情感标注方法，即即时回忆范式，有效解决了传统粗粒度标注引入的标签噪声问题。其创新之处在于结合了即时回忆和多模态生理数据验证，客观地证明了标注的精确性。研究结果强调了标注质量而非数据量对情感识别性能的重要性，这对于未来情感计算领域的数据集构建和算法开发具有重要指导意义，有望推动情感识别技术的进步。"}}
{"id": "2507.02353", "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent", "authors": ["Bowen Chen", "Zhao Wang", "Shingo Takamatsu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02353v1", "summary": "Keyword decision in Sponsored Search Advertising is critical to the success\nof ad campaigns. While LLM-based methods offer automated keyword generation,\nthey face three major limitations: reliance on large-scale query-keyword pair\ndata, lack of online multi-objective performance monitoring and optimization,\nand weak quality control in keyword selection. These issues hinder the agentic\nuse of LLMs in fully automating keyword decisions by monitoring and reasoning\nover key performance indicators such as impressions, clicks, conversions, and\nCTA effectiveness. To overcome these challenges, we propose OMS, a keyword\ngeneration framework that is On-the-fly (requires no training data, monitors\nonline performance, and adapts accordingly), Multi-objective (employs agentic\nreasoning to optimize keywords based on multiple performance metrics), and\nSelf-reflective (agentically evaluates keyword quality). Experiments on\nbenchmarks and real-world ad campaigns show that OMS outperforms existing\nmethods; ablation and human evaluations confirm the effectiveness of each\ncomponent and the quality of generated keywords.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02353v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "OMS：基于LLM代理的即时、多目标、自反思式广告关键词生成", "tldr": "OMS是一个即时、多目标、自反思的LLM代理框架，用于自动生成广告关键词，解决了现有LLM方法对数据依赖、缺乏在线优化和质量控制弱的问题，并在实验中表现优异。", "motivation": "现有基于LLM的广告关键词生成方法存在三大局限性：过度依赖大规模查询-关键词对数据、缺乏在线多目标性能监控与优化、以及关键词选择中的质量控制薄弱。这些问题阻碍了LLM在广告关键词决策中进行代理式自动化，无法有效监控和推理关键绩效指标。", "method": "本文提出了OMS框架，一个即时（无需训练数据，监控在线性能并自适应）、多目标（利用代理推理基于多个性能指标优化关键词）和自反思（代理式评估关键词质量）的关键词生成框架。", "result": "在基准测试和真实世界广告活动中的实验表明，OMS优于现有方法；消融实验和人工评估证实了每个组件的有效性和生成关键词的质量。", "conclusion": "OMS框架有效克服了现有LLM在广告关键词生成中的局限性，通过其即时、多目标和自反思的特性，显著提升了关键词生成的性能和质量。", "translation": "赞助搜索广告中的关键词决策对广告活动的成功至关重要。虽然基于LLM的方法提供了自动关键词生成，但它们面临三个主要限制：依赖大规模查询-关键词对数据、缺乏在线多目标性能监控和优化，以及关键词选择中质量控制薄弱。这些问题阻碍了LLM在通过监控和推理印象、点击、转化和CTA（呼吁行动）效果等关键绩效指标来完全自动化关键词决策方面的代理使用。为了克服这些挑战，我们提出了OMS，一个即时（无需训练数据，监控在线性能并相应调整）、多目标（采用代理推理根据多个性能指标优化关键词）和自反思（代理式评估关键词质量）的关键词生成框架。在基准测试和真实世界广告活动中的实验表明，OMS优于现有方法；消融实验和人工评估证实了每个组件的有效性和生成关键词的质量。", "summary": "本文提出了OMS，一个创新的LLM代理框架，旨在解决现有广告关键词生成方法在数据依赖、在线优化和质量控制方面的局限性。OMS框架具有即时性（无需训练数据，在线监控并适应）、多目标性（基于多种指标优化）和自反思性（评估关键词质量）的特点。实验结果表明，OMS在性能上超越了现有方法，且各组件和生成关键词的质量均得到验证。", "keywords": "广告关键词生成, LLM代理, 多目标优化, 自反思, 在线学习", "comments": "OMS框架的创新之处在于其“即时性”、“多目标性”和“自反思性”三大特性，这使其能够脱离大规模预训练数据，并能在线实时优化和评估关键词质量，显著提升了LLM在广告关键词生成领域的实用性和自动化程度。其代理式推理能力是关键，有望为广告技术带来变革。"}}
{"id": "2507.02270", "title": "MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement", "authors": ["Fanghai Yi", "Zehong Zheng", "Zexiao Liang", "Yihang Dong", "Xiyang Fang", "Wangyu Wu", "Xuhang Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE SMC 2025", "url": "http://arxiv.org/abs/2507.02270v1", "summary": "Enhancing underwater images is crucial for exploration. These images face\nvisibility and color issues due to light changes, water turbidity, and bubbles.\nTraditional prior-based methods and pixel-based methods often fail, while deep\nlearning lacks sufficient high-quality datasets. We introduce the Multi-Axis\nConditional Lookup (MAC-Lookup) model, which enhances visual quality by\nimproving color accuracy, sharpness, and contrast. It includes Conditional 3D\nLookup Table Color Correction (CLTCC) for preliminary color and quality\ncorrection and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.\nThis model prevents over-enhancement and saturation while handling underwater\nchallenges. Extensive experiments show that MAC-Lookup excels in enhancing\nunderwater images by restoring details and colors better than existing methods.\nThe code is https://github.com/onlycatdoraemon/MAC-Lookup.", "comment": "Accepted by IEEE SMC 2025", "pdf_url": "http://arxiv.org/pdf/2507.02270v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MAC-Lookup：水下图像增强的多轴条件查找模型", "tldr": "提出MAC-Lookup模型，通过多轴条件查找表提升水下图像的色彩、清晰度和对比度。", "motivation": "增强水下图像对探索至关重要，但水下图像因光照变化、水体浑浊和气泡等问题面临可见性和色彩挑战。传统方法和深度学习方法存在不足。", "method": "引入多轴条件查找（MAC-Lookup）模型，该模型包含条件3D查找表颜色校正（CLTCC）用于初步颜色和质量校正，以及多轴自适应增强（MAAE）用于细节细化，同时防止过增强和饱和。", "result": "广泛实验表明，MAC-Lookup在恢复细节和色彩方面优于现有方法，能有效增强水下图像。", "conclusion": "MAC-Lookup模型通过其独特的多组件设计，有效解决了水下图像增强中的挑战，实现了优于现有方法的视觉质量提升。", "translation": "增强水下图像对探索至关重要。这些图像因光照变化、水体浑浊和气泡等问题面临可见性和色彩挑战。传统的基于先验的方法和基于像素的方法常常失败，而深度学习缺乏足够的高质量数据集。我们引入了多轴条件查找（MAC-Lookup）模型，该模型通过提高色彩准确性、清晰度和对比度来增强视觉质量。它包括用于初步颜色和质量校正的条件3D查找表颜色校正（CLTCC）和用于细节细化的多轴自适应增强（MAAE）。该模型在处理水下挑战的同时，防止了过增强和饱和。广泛的实验表明，MAC-Lookup在恢复细节和色彩方面优于现有方法，在增强水下图像方面表现出色。代码可在https://github.com/onlycatdoraemon/MAC-Lookup获取。", "summary": "本文提出了一种名为MAC-Lookup的多轴条件查找模型，旨在解决水下图像的可见性和色彩问题。该模型结合了条件3D查找表颜色校正（CLTCC）和多轴自适应增强（MAAE），以提升水下图像的色彩准确性、清晰度和对比度，同时有效避免过增强。实验证明，MAC-Lookup在水下图像细节和色彩恢复方面表现优于现有方法。", "keywords": "水下图像增强, 多轴条件查找, 查找表, 颜色校正, 图像细节恢复", "comments": "该论文提出了一种创新的多轴条件查找模型（MAC-Lookup），通过结合查找表和自适应增强来解决水下图像增强的挑战。其亮点在于同时处理色彩和细节问题，并有效防止过增强和饱和，克服了传统方法和深度学习方法的一些局限性。这种模块化设计使其在实际应用中具有潜力。"}}
{"id": "2507.02721", "title": "A formal specification of the desired software behaviour of the Princess Marijke lock complex", "authors": ["Jan Friso Groote", "Matthias Volk"], "categories": ["eess.SY", "cs.LO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02721v1", "summary": "The Princess Marijke lock complex is a large lock and water-protection\ninstallation in the Netherlands between the river Rhine and the\nAmsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of\nAmsterdam. The lock complex consists of two independent locks and a moveable\nflood-protection barrier. Ensuring safe control of the lock complex is of\nutmost importance to guarantee both flood-protection and reliable ship\noperations. This paper gives a precise, formal description of the software\ncontrol of the lock complex in less than 400 lines of mCRL2 code. This\ndescription can act as a blueprint on how the software of this lock complex\nneeds to be constructed. Moreover, using model checking, 53 software\nrequirements are shown to be valid, ensuring that the formal description of the\nbehaviour is correct with regard to these properties and is unlikely to contain\nmistakes and oversights.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02721v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "玛丽亚公主船闸综合体预期软件行为的正式规范", "tldr": "本文使用mCRL2代码对玛丽亚公主船闸综合体的软件控制进行了精确、形式化的描述，并通过模型检验验证了53项软件需求，确保了描述的正确性。", "motivation": "确保荷兰玛丽亚公主船闸综合体的安全控制对于防洪和可靠的船舶运营至关重要。本文旨在提供一个精确、形式化的软件控制描述，作为软件构建的蓝图，并验证其正确性。", "method": "作者使用mCRL2代码（少于400行）对船闸综合体的软件控制进行了形式化描述。此外，通过模型检验，验证了53项软件需求的有效性。", "result": "该形式化描述可以作为船闸综合体软件构建的蓝图。通过模型检验，53项软件需求被证明是有效的，这表明行为的形式化描述在这些属性方面是正确的，并且不太可能包含错误和疏忽。", "conclusion": "该论文成功地提供了一个精确、形式化的玛丽亚公主船闸综合体软件行为描述，并通过模型检验验证了其正确性和鲁棒性，为安全控制提供了可靠的基础。", "translation": "玛丽亚公主船闸综合体是位于荷兰莱茵河与阿姆斯特丹-莱茵运河（连接莱茵河与阿姆斯特丹港的大型水道）之间的一个大型船闸和水利保护设施。该船闸综合体由两个独立的船闸和一个可移动的防洪屏障组成。确保船闸综合体的安全控制对于保障防洪和可靠的船舶运营至关重要。本文以不到400行mCRL2代码的形式，对船闸综合体的软件控制进行了精确、形式化的描述。该描述可以作为船闸综合体软件构建的蓝图。此外，通过模型检验，53项软件需求被证明是有效的，这确保了行为的形式化描述在这些属性方面是正确的，并且不太可能包含错误和疏忽。", "summary": "本文对荷兰玛丽亚公主船闸综合体的软件控制进行了精确、形式化的描述。该描述采用mCRL2代码编写，并作为软件开发的蓝图。通过模型检验，文中验证了53项软件需求的有效性，从而确保了该形式化描述的正确性，并降低了错误和遗漏的可能性，对于保障船闸的安全运行具有重要意义。", "keywords": "形式化规范, 软件行为, 船闸综合体, mCRL2, 模型检验", "comments": "这篇论文的创新之处在于它将形式化方法应用于实际的大型基础设施（船闸综合体）的软件行为规范。使用mCRL2进行精确描述，并通过模型检验验证了大量软件需求，这大大增强了软件的可靠性和安全性。这种方法对于关键基础设施的软件开发具有重要的借鉴意义，因为它能够系统性地发现潜在的错误和疏忽。"}}
{"id": "2507.02824", "title": "DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift", "authors": ["Po-Heng Chou", "Ching-Wen Chen", "Wan-Jen Huang", "Walid Saad", "Yu Tsao", "Ronald Y. Chang"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      5 pages, 4 figures, 2 tables, accepted by IEEE Globecom 2024 Workshops", "url": "http://arxiv.org/abs/2507.02824v1", "summary": "In this paper, the precoding design is investigated for maximizing the\nthroughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO)\nsystems with obstructed direct communication paths. In particular, a\nreconfigurable intelligent surface (RIS) is employed to enhance MIMO\ntransmissions, considering mmWave characteristics related to line-of-sight\n(LoS) and multipath effects. The traditional exhaustive search (ES) for optimal\ncodewords in the continuous phase shift is computationally intensive and\ntime-consuming. To reduce computational complexity, permuted discrete Fourier\ntransform (DFT) vectors are used for finding codebook design, incorporating\namplitude responses for practical or ideal RIS systems. However, even if the\ndiscrete phase shift is adopted in the ES, it results in significant\ncomputation and is time-consuming. Instead, the trained deep neural network\n(DNN) is developed to facilitate faster codeword selection. Simulation results\nshow that the DNN maintains sub-optimal spectral efficiency even as the\ndistance between the end-user and the RIS has variations in the testing phase.\nThese results highlight the potential of DNN in advancing RIS-aided systems.", "comment": "5 pages, 4 figures, 2 tables, accepted by IEEE Globecom 2024\n  Workshops", "pdf_url": "http://arxiv.org/pdf/2507.02824v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于DNN的RIS辅助毫米波MIMO系统中实际相移预编码", "tldr": "本文研究了在存在阻塞直连路径的毫米波MIMO系统中，通过RIS辅助来最大化吞吐量的预编码设计，并提出了一种基于DNN的快速码字选择方法，以克服传统穷举搜索的高计算复杂度问题。", "motivation": "在存在阻塞直连通信路径的毫米波（mmWave）多输入多输出（MIMO）系统中，需要设计预编码以最大化吞吐量。传统的连续相移最优码字穷举搜索（ES）计算量大且耗时，即使采用离散相移也依然如此。", "method": "本文提出开发训练好的深度神经网络（DNN）来加速码字选择，以替代计算量大且耗时的穷举搜索。在码本设计中，使用了置换离散傅里叶变换（DFT）向量，并考虑了实际或理想RIS系统的幅度响应。", "result": "仿真结果表明，即使在测试阶段终端用户与RIS之间的距离发生变化，所提出的DNN方法也能保持次优的频谱效率。", "conclusion": "这些结果突出了深度神经网络（DNN）在推进可重构智能表面（RIS）辅助系统方面的潜力。", "translation": "本文研究了预编码设计，旨在最大化具有阻塞直连通信路径的毫米波（mmWave）多输入多输出（MIMO）系统的吞吐量。特别是，考虑到与视距（LoS）和多径效应相关的毫米波特性，采用了可重构智能表面（RIS）来增强MIMO传输。传统上，连续相移中的最优码字的穷举搜索（ES）计算量大且耗时。为了降低计算复杂度，使用置换离散傅里叶变换（DFT）向量进行码本设计，其中包含了实际或理想RIS系统的幅度响应。然而，即使在ES中采用离散相移，其计算量仍然很大且耗时。取而代之的是，开发了训练好的深度神经网络（DNN）以促进更快的码字选择。仿真结果表明，即使在测试阶段终端用户与RIS之间的距离发生变化，DNN也能保持次优的频谱效率。这些结果突出了DNN在推进RIS辅助系统方面的潜力。", "summary": "本文针对存在阻塞直连路径的毫米波MIMO系统，研究了利用可重构智能表面（RIS）辅助的预编码设计，旨在最大化系统吞吐量。为解决传统穷举搜索（ES）计算复杂度高的问题，论文提出了一种基于深度神经网络（DNN）的码字选择方法。仿真结果表明，该DNN方法能有效加速码字选择过程，同时在终端用户与RIS距离变化时仍能保持次优的频谱效率，展现了DNN在RIS辅助系统中的应用潜力。", "keywords": "毫米波MIMO, RIS, 预编码, 深度神经网络, 相移", "comments": "该论文的创新点在于引入DNN来解决RIS辅助毫米波MIMO系统中预编码设计的计算复杂性问题，特别是在处理实际相移时。这提供了一个高效且实用的替代方案，克服了传统穷举搜索的局限性，对于未来RIS辅助通信系统的实际部署具有重要意义，尤其是在需要快速响应和适应性调整的场景中。"}}
{"id": "2507.02759", "title": "An Easy Proof of a Weak Version of Chernoff inequality", "authors": ["Sariel Har-Peled"], "categories": ["math.PR", "cs.DS", "math.CO"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02759v1", "summary": "We prove an easy but very weak version of Chernoff inequality. Namely, that\nthe probability that in $6M$ throws of a fair coin, one gets at most $M$ heads\nis $\\leq 1/2^M$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02759v1", "cate": "math.PR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "切尔诺夫不等式弱化版本的一个简易证明", "tldr": "本文提供了一个切尔诺夫不等式弱化版本的简易证明，具体指出在6M次公平硬币抛掷中，正面朝上次数至多为M的概率小于等于1/2^M。", "motivation": "本文的动机是提供一个切尔诺夫不等式弱化版本的简易证明。", "method": "通过数学证明的方式，推导了一个切尔诺夫不等式的弱化版本。", "result": "证明了在6M次公平硬币抛掷中，正面朝上次数至多为M的概率小于等于1/2^M。", "conclusion": "本文成功证明了切尔诺夫不等式的一个弱化版本，即在6M次公平硬币抛掷中，正面朝上次数至多为M的概率小于等于1/2^M。", "translation": "我们证明了一个切尔诺夫不等式的简易但非常弱的版本。即，在抛掷一枚公平硬币6M次的情况下，正面朝上次数至多为M的概率小于等于1/2^M。", "summary": "本文提出了切尔诺夫不等式的一个简易但弱化的证明。具体而言，该证明指出，在进行6M次公平硬币抛掷时，获得至多M次正面的概率不大于1/2^M。", "keywords": "切尔诺夫不等式, 概率, 硬币抛掷, 简易证明", "comments": "本文的创新点在于提供了一个非常简易的证明，尽管其针对的是切尔诺夫不等式的一个弱化版本。其重要性可能在于为教学或初学者提供了一个更易理解的入门示例，但其弱化的性质限制了其在实际应用中的普适性。"}}
{"id": "2507.02506", "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and GitHub. Submitted to arXiv for open access", "url": "http://arxiv.org/abs/2507.02506v1", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.", "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "pdf_url": "http://arxiv.org/pdf/2507.02506v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "印度保释判决-1200：一个用于印度保释令法律自然语言处理的多属性数据集", "tldr": "引入了IndianBailJudgments-1200数据集，包含1200份印度保释判决，旨在解决印度法律NLP领域结构化数据稀缺的问题。", "motivation": "印度等地区法律自然语言处理（NLP）不发达，主要原因是缺乏结构化数据集。", "method": "数据集的注释是使用提示工程（prompt-engineered）的GPT-4o管道生成的，并对一致性进行了验证。", "result": "创建了IndianBailJudgments-1200数据集，这是一个新的基准数据集，包含1200份关于保释决定的印度法院判决书，标注了20多个属性，如保释结果、IPC条款、犯罪类型和法律推理。", "conclusion": "IndianBailJudgments-1200数据集支持广泛的法律自然语言处理任务，例如结果预测、摘要和公平性分析，并且是第一个专门针对印度保释判例法公开可用的数据集。", "translation": "法律自然语言处理在印度等地区仍然不发达，原因在于结构化数据集的稀缺。我们引入了IndianBailJudgments-1200，这是一个新的基准数据集，包含1200份关于保释决定的印度法院判决书，标注了20多个属性，包括保释结果、IPC条款、犯罪类型和法律推理。注释是使用提示工程的GPT-4o管道生成的，并验证了其一致性。该资源支持广泛的法律自然语言处理任务，例如结果预测、摘要和公平性分析，并且是第一个专门针对印度保释判例法公开可用的数据集。", "summary": "本文介绍了IndianBailJudgments-1200数据集，旨在解决印度法律NLP领域结构化数据稀缺的问题。该数据集包含1200份印度保释判决书，通过提示工程的GPT-4o管道标注了20多个属性。它是首个公开的印度保释判例法数据集，可用于结果预测、摘要和公平性分析等多种法律NLP任务。", "keywords": "印度保释判决, 法律NLP, 数据集, GPT-4o, 印度法律", "comments": "该数据集通过引入GPT-4o进行标注，提高了数据生成效率，且填补了印度法律NLP领域结构化数据集的空白，对推动该领域的发展具有重要意义。其多属性标注也为更复杂的法律分析提供了可能。"}}
{"id": "2507.02599", "title": "Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data", "authors": ["Sertac Kilickaya", "Levent Eren"], "categories": ["cs.LG", "cs.SD", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to the Journal of Vibration Engineering & Technologies", "url": "http://arxiv.org/abs/2507.02599v1", "summary": "Purpose: The primary aim of this study is to enhance fault diagnosis in\ninduction machines by leveraging the Pad\\'e Approximant Neuron (PAON) model.\nWhile accelerometers and microphones are standard in motor condition\nmonitoring, deep learning models with nonlinear neuron architectures offer\npromising improvements in diagnostic performance. This research addresses the\nquestion: Can Pad\\'e Approximant Neural Networks (Pad\\'eNets) outperform\nconventional Convolutional Neural Networks (CNNs) and Self-Organized\nOperational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical\nfaults using vibration and acoustic data?\n  Methods: We evaluate and compare the diagnostic capabilities of three deep\nlearning architectures: one-dimensional CNNs, Self-ONNs, and Pad\\'eNets. These\nmodels are tested on the University of Ottawa's publicly available\nconstant-speed induction motor datasets, which include both vibration and\nacoustic sensor data. The Pad\\'eNet model is designed to introduce enhanced\nnonlinearity and is compatible with unbounded activation functions such as\nLeaky ReLU.\n  Results and Conclusion: Pad\\'eNets consistently outperformed the baseline\nmodels, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%\nfor accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced\nnonlinearity of Pad\\'eNets, together with their compatibility with unbounded\nactivation functions, significantly improves fault diagnosis performance in\ninduction motor condition monitoring.", "comment": "Submitted to the Journal of Vibration Engineering & Technologies", "pdf_url": "http://arxiv.org/pdf/2507.02599v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用振动和声学数据增强电动机故障诊断的Padé逼近神经网络", "tldr": "PadéNet在利用振动和声学数据诊断感应电机故障方面，性能优于CNN和Self-ONN，诊断准确率更高。", "motivation": "本研究旨在通过利用Padé逼近神经元（PAON）模型来增强感应电机的故障诊断。研究探讨了Padé逼近神经网络（PadéNets）在使用振动和声学数据诊断电气和机械故障方面，能否优于传统的卷积神经网络（CNNs）和自组织运算神经网络（Self-ONNs）。", "method": "研究评估并比较了一维CNNs、Self-ONNs和PadéNets三种深度学习架构的诊断能力。这些模型在渥太华大学公开的恒速感应电机数据集上进行测试，该数据集包含振动和声学传感器数据。PadéNet模型旨在引入增强的非线性，并与Leaky ReLU等无界激活函数兼容。", "result": "PadéNets始终优于基线模型，对于加速度计1、2、3和声学传感器，诊断准确率分别达到99.96%、98.26%、97.61%和98.33%。", "conclusion": "PadéNets增强的非线性及其与无界激活函数的兼容性显著提高了感应电机状态监测中的故障诊断性能。", "translation": "目的：本研究的主要目的是通过利用Padé逼近神经元（PAON）模型来增强感应电机的故障诊断。虽然加速度计和麦克风在电机状态监测中是标准配置，但具有非线性神经元架构的深度学习模型在诊断性能方面显示出有希望的改进。本研究探讨的问题是：Padé逼近神经网络（PadéNets）在使用振动和声学数据诊断电气和机械故障方面，能否优于传统的卷积神经网络（CNNs）和自组织运算神经网络（Self-ONNs）？\n方法：我们评估并比较了三种深度学习架构的诊断能力：一维CNNs、Self-ONNs和PadéNets。这些模型在渥太华大学公开的恒速感应电机数据集上进行测试，该数据集包括振动和声学传感器数据。PadéNet模型旨在引入增强的非线性，并与Leaky ReLU等无界激活函数兼容。\n结果和结论：PadéNets始终优于基线模型，对于加速度计1、2、3和声学传感器，诊断准确率分别达到99.96%、98.26%、97.61%和98.33%。PadéNets增强的非线性及其与无界激活函数的兼容性显著提高了感应电机状态监测中的故障诊断性能。", "summary": "本研究旨在通过引入Padé逼近神经网络（PadéNets）来提升感应电机故障诊断的准确性。研究比较了PadéNets与传统的CNNs和Self-ONNs在使用振动和声学数据进行故障诊断的性能。结果显示，PadéNets在所有传感器上均表现出更高的诊断准确率，证明其增强的非线性和与无界激活函数的兼容性显著提升了故障诊断性能。", "keywords": "Padé逼近神经网络, 故障诊断, 振动数据, 声学数据, 感应电机", "comments": "这项研究的创新之处在于引入了Padé逼近神经网络（PadéNets）用于电机故障诊断，并证明其在非线性处理方面的优势。通过与现有主流深度学习模型（CNNs和Self-ONNs）的比较，突出了PadéNets在提高诊断准确性方面的潜力。其重要性在于为工业电机状态监测提供了新的、更有效的深度学习模型选择。"}}
{"id": "2507.02699", "title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents", "authors": ["Jiangrong Wu", "Yuhong Nan", "Jianliang Wu", "Zitong Yao", "Zibin Zheng"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02699v1", "summary": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02699v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "控制权岌岌可危：评估LLM驱动的电子邮件代理的安全状况", "tldr": "LLM电子邮件代理对一种名为“电子邮件代理劫持”（EAH）的新型攻击高度脆弱，所有测试实例均被成功劫持，通常只需极少的尝试次数。", "motivation": "LLM驱动的电子邮件代理应用日益普及，但其安全机制尚未得到充分探索，目前缺乏对其潜在安全风险的全面研究。", "method": "本文进行了首次深入系统的LLM电子邮件代理安全研究。提出了一种名为“电子邮件代理劫持”（EAH）的攻击，该攻击通过外部电子邮件资源覆盖代理的原始提示，使攻击者能够在用户不知情的情况下远程控制电子邮件代理并执行特定攻击场景。为进行大规模评估，研究人员开发了EAHawk管道，并对14个代表性LLM代理框架、63个代理应用、12个LLM和20个电子邮件服务进行了实证研究，生成了1,404个真实世界的电子邮件代理实例进行评估。", "result": "实验结果表明，所有1,404个实例都被成功劫持；平均而言，仅需2.03次攻击尝试即可控制一个电子邮件代理实例。更糟糕的是，对于某些LLM，实现完全代理控制所需的平均尝试次数低至1.23次。", "conclusion": "LLM驱动的电子邮件代理对劫持攻击高度脆弱，存在显著的安全风险。", "translation": "LLM能力不断增强，导致LLM代理应用迅速普及，开发者通过赋予LLM访问外部资源的能力来支持复杂的任务执行。其中，LLM电子邮件代理应用是广泛使用的类别之一，因为电子邮件仍然是用户重要的通信媒介。LLM电子邮件代理能够利用LLM驱动的推理来管理和回复电子邮件，并通过外部电子邮件API（例如，发送电子邮件）自主执行用户指令。然而，尽管它们部署和实用性不断增长，LLM电子邮件代理应用的安全机制仍未得到充分探索。目前，尚未有对这些代理应用内部潜在安全风险及其更广泛影响的全面研究。\n\n在本文中，我们首次对LLM电子邮件代理进行了深入和系统的安全研究。我们提出了“电子邮件代理劫持”（EAH）攻击，该攻击通过外部电子邮件资源覆盖电子邮件代理的原始提示，允许攻击者远程控制电子邮件代理并进一步在用户不知情的情况下执行特定的攻击场景。\n\n为了促进大规模评估，我们提出了EAHawk，这是一个评估LLM电子邮件代理应用EAH攻击的管道。通过EAHawk，我们进行了一项实证研究，涵盖了14个代表性LLM代理框架、63个代理应用、12个LLM和20个电子邮件服务，从而生成了1,404个真实世界的电子邮件代理实例进行评估。实验结果表明，所有1,404个实例都被成功劫持；平均而言，仅需2.03次攻击尝试即可控制一个电子邮件代理实例。更糟糕的是，对于某些LLM，实现完全代理控制所需的平均尝试次数低至1.23次。", "summary": "本文首次对LLM驱动的电子邮件代理进行了深入的安全研究，揭示了其在广泛应用中存在的未被充分探索的安全漏洞。研究提出了一种名为“电子邮件代理劫持”（EAH）的新型攻击，攻击者可通过外部电子邮件资源覆盖原始提示，从而远程控制代理。为评估此攻击，研究人员开发了EAHawk管道，并对其进行了大规模的实证研究，涵盖了1,404个真实世界的代理实例。结果显示，所有测试实例均被成功劫持，平均仅需2.03次尝试即可控制一个实例，凸显了这些代理存在的严重安全风险。", "keywords": "LLM代理, 电子邮件安全, 劫持攻击, EAHawk, AI安全", "comments": "该论文揭示了快速普及的LLM电子邮件代理中一个关键且此前未被充分探索的安全漏洞。EAH攻击的提出以及EAHawk评估管道的引入，结合大规模的实证研究，为这些代理的普遍脆弱性提供了强有力的证据。这项工作对于提高人们对LLM驱动应用安全的认识并推动开发更强大的安全机制至关重要。高成功率和极低的劫持尝试次数是尤其令人担忧的发现。"}}
{"id": "2507.02521", "title": "Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings", "authors": ["Ayodeji O. Abioye", "Jayati Deshmukh", "Athina Georgara", "Dominic Price", "Tuyen Nguyen", "Aleksandra Landowska", "Amel Bennaceur", "Joel E. Fischer", "Sarvapali D. Ramchurn"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      3 pages, 1 figure. Accepted for poster presentation at the UK AI Research Symposium (UKAIR) 2025, themed \"A Festival of Ideas\", being held in Newcastle from 8th - 9th September, 2025. this https URL", "url": "http://arxiv.org/abs/2507.02521v1", "summary": "This research investigates strategies for multi-robot coordination in\nmulti-human environments. It proposes a multi-objective learning-based\ncoordination approach to addressing the problem of path planning, navigation,\ntask scheduling, task allocation, and human-robot interaction in multi-human\nmulti-robot (MHMR) settings.", "comment": "3 pages, 1 figure. Accepted for poster presentation at the UK AI\n  Research Symposium (UKAIR) 2025, themed \"A Festival of Ideas\", being held in\n  Newcastle from 8th - 9th September, 2025. https://www.ukairs.ac.uk/", "pdf_url": "http://arxiv.org/pdf/2507.02521v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "安全且具有社会意识的多机器人协调在多人类社会护理环境中", "tldr": "提出一种多目标学习方法，用于多人类多机器人环境中的安全和社交感知协调。", "motivation": "本研究旨在调查多人类环境中的多机器人协调策略，以解决路径规划、导航、任务调度、任务分配和人机交互等问题。", "method": "提出了一种基于多目标学习的协调方法。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "本研究调查了多人类环境中的多机器人协调策略。它提出了一种基于多目标学习的协调方法，以解决多人类多机器人（MHMR）环境中的路径规划、导航、任务调度、任务分配和人机交互问题。", "summary": "该研究提出了一种多目标学习的协调方法，旨在解决多人类多机器人（MHMR）环境中，包括路径规划、导航、任务调度、任务分配和人机交互在内的多机器人协调问题。", "keywords": "多机器人协调, 多人类环境, 多目标学习, 人机交互, 社会护理", "comments": "这篇论文的创新点在于将多目标学习应用于多人类多机器人环境中的复杂协调问题，涵盖了从路径规划到人机交互的多个关键方面，这对于提升社会护理等应用场景中多机器人系统的安全性和社交感知能力具有重要意义。"}}
{"id": "2507.02690", "title": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes", "authors": ["Jiaxing Wang", "Yifeng Yu", "Jiahan Song", "Bin Cao", "Jing Fan", "Ji Zhang"], "categories": ["cs.SE", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      15 pages, 7 figures. Business process prediction using reinforcement learning and heterogeneous graph neural networks", "url": "http://arxiv.org/abs/2507.02690v1", "summary": "Next activity prediction represents a fundamental challenge for optimizing\nbusiness processes in service-oriented architectures such as microservices\nenvironments, distributed enterprise systems, and cloud-native platforms, which\nenables proactive resource allocation and dynamic service composition. Despite\nthe prevalence of sequence-based methods, these approaches fail to capture\nnon-sequential relationships that arise from parallel executions and\nconditional dependencies. Even though graph-based approaches address structural\npreservation, they suffer from homogeneous representations and static\nstructures that apply uniform modeling strategies regardless of individual\nprocess complexity characteristics. To address these limitations, we introduce\nRLHGNN, a novel framework that transforms event logs into heterogeneous process\ngraphs with three distinct edge types grounded in established process mining\ntheory. Our approach creates four flexible graph structures by selectively\ncombining these edges to accommodate different process complexities, and\nemploys reinforcement learning formulated as a Markov Decision Process to\nautomatically determine the optimal graph structure for each specific process\ninstance. RLHGNN then applies heterogeneous graph convolution with\nrelation-specific aggregation strategies to effectively predict the next\nactivity. This adaptive methodology enables precise modeling of both sequential\nand non-sequential relationships in service interactions. Comprehensive\nevaluation on six real-world datasets demonstrates that RLHGNN consistently\noutperforms state-of-the-art approaches. Furthermore, it maintains an inference\nlatency of approximately 1 ms per prediction, representing a highly practical\nsolution suitable for real-time business process monitoring applications. The\nsource code is available at https://github.com/Joker3993/RLHGNN.", "comment": "15 pages, 7 figures. Business process prediction using reinforcement\n  learning and heterogeneous graph neural networks", "pdf_url": "http://arxiv.org/pdf/2507.02690v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RLHGNN：强化学习驱动的异构图神经网络用于业务流程中的下一步活动预测", "tldr": "RLHGNN是一个新的框架，它使用强化学习动态选择最佳的异构图结构，并通过异构图卷积来预测业务流程中的下一步活动，解决了现有方法无法捕捉非顺序关系和同构表示的局限性，并在真实数据集上表现出色且推理速度快。", "motivation": "优化业务流程中的下一步活动预测是一个基本挑战，现有序列方法无法捕捉非顺序关系，而现有图方法存在同构表示和静态结构的局限性，不能适应不同的流程复杂性。", "method": "RLHGNN将事件日志转换为具有三种不同边类型的异构过程图。它通过选择性组合这些边来创建四种灵活的图结构，并采用强化学习（表述为马尔可夫决策过程）自动为每个特定流程实例确定最佳图结构。然后，RLHGNN应用具有关系特定聚合策略的异构图卷积来预测下一步活动。", "result": "在六个真实世界数据集上的综合评估表明，RLHGNN始终优于最先进的方法。此外，它每次预测的推理延迟约为1毫秒，是一个高度实用的解决方案，适用于实时业务流程监控应用。", "conclusion": "RLHGNN通过自适应地建模业务流程中的顺序和非顺序关系，提供了一种有效且实用的下一步活动预测解决方案，并在性能和效率上超越了现有方法。", "translation": "下一步活动预测是优化面向服务架构（如微服务环境、分布式企业系统和云原生平台）中业务流程的一项基本挑战，它能够实现主动资源分配和动态服务组合。尽管基于序列的方法很普遍，但这些方法未能捕获并行执行和条件依赖关系产生的非顺序关系。尽管基于图的方法解决了结构保留问题，但它们存在同构表示和静态结构的缺点，无论个体流程复杂性特征如何，都采用统一的建模策略。为了解决这些限制，我们引入了RLHGNN，这是一个新颖的框架，它将事件日志转换为具有三种不同边类型的异构过程图，这些类型基于既定的过程挖掘理论。我们的方法通过选择性地组合这些边来创建四种灵活的图结构，以适应不同的流程复杂性，并采用表述为马尔可夫决策过程的强化学习来自动确定每个特定流程实例的最佳图结构。然后，RLHGNN应用具有关系特定聚合策略的异构图卷积来有效地预测下一步活动。这种自适应方法能够精确建模服务交互中的顺序和非顺序关系。对六个真实世界数据集的综合评估表明，RLHGNN始终优于最先进的方法。此外，它每次预测的推理延迟约为1毫秒，是一个高度实用的解决方案，适用于实时业务流程监控应用。源代码可在https://github.com/Joker3993/RLHGNN获取。", "summary": "RLHGNN是一个新颖的框架，旨在通过结合强化学习和异构图神经网络来改进业务流程中的下一步活动预测。它通过将事件日志转换为异构过程图并动态选择最佳图结构来解决现有序列和图方法的局限性。RLHGNN利用强化学习（表述为马尔可夫决策过程）来适应不同的流程复杂性，并使用异构图卷积进行预测。在六个真实世界数据集上的评估显示，RLHGNN在预测性能上优于现有技术，并保持了低推理延迟，使其适用于实时应用。", "keywords": "强化学习, 异构图神经网络, 下一步活动预测, 业务流程, 过程挖掘", "comments": "RLHGNN的创新之处在于其结合强化学习来动态选择最佳异构图结构，以适应不同业务流程的复杂性，这解决了传统图方法中静态和同构表示的局限性。其能够精确建模顺序和非顺序关系，以及在真实数据集上表现出的卓越性能和极低的推理延迟，使其成为一个非常重要且实用的解决方案，特别适用于对实时性有高要求的业务流程监控场景。"}}
{"id": "2507.02432", "title": "Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset", "authors": ["Jueun Lee", "Dennis Moschina", "Supraja Ramesh", "Tobias Röddiger", "Kai Kunze", "Michael Beigl"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures. Submitted to the International Symposium on Wearable Computers (ISWC)", "url": "http://arxiv.org/abs/2507.02432v1", "summary": "We investigate the use of musically structured, closed-loop vibration\npatterns as a passive biofeedback intervention for relaxation and sleep\ninitiation. By encoding rhythmic meter structures into smartwatch vibrations\nand adapting their frequency to be slightly slower than the user's real-time\nheart rate, our system aims to reduce arousal through tactile entrainment,\noffering a non-invasive alternative to auditory or open-loop approaches\npreviously used in sleep and anxiety contexts. In the first study (N=20), we\ncompared five adaptive vibration rhythms for their effects on heart rate and\nsubjective perceptions of relaxation in a resting context. In the second study\n(N=28), we evaluated the most promising pattern from Study 1 in a prolonged\nsleep initiation setting. Results showed increased parasympathetic activity and\nperceived relaxation during short-term stimulation, but no significant effects\non sleep-related measures during the sleep onset phase. This work contributes\nto the understanding of how wearable haptic feedback can support relaxation and\nsleep, offering design insights and identifying methodological considerations\nfor effectively integrating haptic interaction into self-directed\ninterventions.", "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "pdf_url": "http://arxiv.org/pdf/2507.02432v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "智能手表闭环节律触觉生物反馈用于放松和入睡", "tldr": "研究通过智能手表提供节律性触觉振动，以略低于心率的频率进行闭环生物反馈，旨在通过触觉带动降低唤醒度，促进放松和入睡。短期刺激显示放松效果，但对睡眠启动无显著影响。", "motivation": "旨在通过智能手表的闭环节律触觉振动，提供一种非侵入性替代方案，以降低唤醒度，促进放松和入睡，区别于现有的听觉或开环方法。", "method": "开发了一种通过智能手表振动编码音乐节律结构并将其频率调整至略低于用户实时心率的系统。进行了两项研究：第一项（N=20）比较了五种自适应振动节律对心率和主观放松感的影响；第二项（N=28）评估了第一项中最有潜力的模式在长时间睡眠启动环境下的效果。", "result": "短期刺激增加了副交感神经活动和感知放松。然而，在睡眠启动阶段，对睡眠相关指标没有显著影响。", "conclusion": "可穿戴触觉反馈能够支持放松和睡眠，但对睡眠启动阶段的直接影响不显著。研究提供了设计见解和方法论考量，以有效将触觉交互整合到自我导向的干预中。", "translation": "我们研究了使用音乐结构化、闭环振动模式作为一种被动生物反馈干预，用于放松和睡眠启动。通过将节律拍子结构编码到智能手表振动中，并将其频率调整为略低于用户实时心率，我们的系统旨在通过触觉带动来降低唤醒度，提供一种非侵入性替代方案，以替代先前用于睡眠和焦虑情境中的听觉或开环方法。在第一项研究（N=20）中，我们比较了五种自适应振动节律在休息情境下对心率和主观放松感的影响。在第二项研究（N=28）中，我们评估了第一项研究中最有前景的模式在长时间睡眠启动环境中的效果。结果显示，在短期刺激期间，副交感神经活动和感知放松有所增加，但在睡眠启动阶段，对睡眠相关指标没有显著影响。这项工作有助于理解可穿戴触觉反馈如何支持放松和睡眠，提供了设计见解并确定了有效将触觉交互整合到自我导向干预中的方法学考量。", "summary": "本文探讨了一种通过智能手表提供闭环节律触觉生物反馈的新方法，旨在通过将振动频率调整至略低于心率来降低唤醒度，从而促进放松和入睡。研究通过两项实验验证了该方法，结果表明短期触觉刺激能增加副交感神经活动和主观放松感，但对睡眠启动阶段的睡眠相关指标无显著影响。该工作为可穿戴触觉反馈在放松和睡眠支持方面的应用提供了设计洞察和方法学建议。", "keywords": "触觉生物反馈, 智能手表, 放松, 睡眠启动, 闭环系统", "comments": "这项研究创新性地将闭环触觉生物反馈应用于放松和睡眠干预，通过智能手表实现非侵入性体验。其独特之处在于将振动频率与用户实时心率关联，试图通过触觉带动实现生理调节。尽管在睡眠启动方面效果不显著，但短期放松效果的发现仍具重要性，并为未来可穿戴设备在健康领域的应用提供了宝贵的设计和方法论见解。"}}
{"id": "2507.02379", "title": "An AI-native experimental laboratory for autonomous biomolecular engineering", "authors": ["Mingyu Wu", "Zhaoguo Wang", "Jiabin Wang", "Zhiyuan Dong", "Jingkai Yang", "Qingting Li", "Tianyu Huang", "Lei Zhao", "Mingqiang Li", "Fei Wang", "Chunhai Fan", "Haibo Chen"], "categories": ["cs.AI", "q-bio.BM"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02379v1", "summary": "Autonomous scientific research, capable of independently conducting complex\nexperiments and serving non-specialists, represents a long-held aspiration.\nAchieving it requires a fundamental paradigm shift driven by artificial\nintelligence (AI). While autonomous experimental systems are emerging, they\nremain confined to areas featuring singular objectives and well-defined, simple\nexperimental workflows, such as chemical synthesis and catalysis. We present an\nAI-native autonomous laboratory, targeting highly complex scientific\nexperiments for applications like autonomous biomolecular engineering. This\nsystem autonomously manages instrumentation, formulates experiment-specific\nprocedures and optimization heuristics, and concurrently serves multiple user\nrequests. Founded on a co-design philosophy of models, experiments, and\ninstruments, the platform supports the co-evolution of AI models and the\nautomation system. This establishes an end-to-end, multi-user autonomous\nlaboratory that handles complex, multi-objective experiments across diverse\ninstrumentation. Our autonomous laboratory supports fundamental nucleic acid\nfunctions-including synthesis, transcription, amplification, and sequencing. It\nalso enables applications in fields such as disease diagnostics, drug\ndevelopment, and information storage. Without human intervention, it\nautonomously optimizes experimental performance to match state-of-the-art\nresults achieved by human scientists. In multi-user scenarios, the platform\nsignificantly improves instrument utilization and experimental efficiency. This\nplatform paves the way for advanced biomaterials research to overcome\ndependencies on experts and resource barriers, establishing a blueprint for\nscience-as-a-service at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02379v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一个用于自主生物分子工程的AI原生实验平台", "tldr": "一个AI原生实验室，用于自主生物分子工程，能够处理复杂的实验，优化性能，并服务多个用户，为大规模科学即服务铺平道路。", "motivation": "实现自主科学研究，特别是针对复杂的生物分子工程，超越目前仅限于简单工作流程的系统，并克服对专家和资源的依赖。", "method": "开发了一个AI原生自主实验室，该实验室基于模型、实验和仪器的协同设计理念。该平台自主管理仪器，制定实验特定程序和优化启发式方法，并支持AI模型和自动化系统的协同进化。它能够处理跨不同仪器的复杂多目标实验。", "result": "该系统支持基本的核酸功能（包括合成、转录、扩增和测序）以及疾病诊断、药物开发和信息存储等领域的应用。它能够在没有人为干预的情况下，自主优化实验性能以达到人类科学家所实现的最新水平。在多用户场景中，该平台显著提高了仪器利用率和实验效率。", "conclusion": "该平台为先进生物材料研究克服对专家和资源的依赖铺平了道路，并为大规模的科学即服务建立了蓝图。", "translation": "自主科学研究，能够独立进行复杂的实验并服务于非专业人士，代表着一个长期以来的愿望。实现它需要由人工智能（AI）驱动的根本性范式转变。尽管自主实验系统正在兴起，但它们仍局限于具有单一目标和明确、简单实验工作流程的领域，例如化学合成和催化。我们提出了一个AI原生自主实验室，旨在针对高度复杂的科学实验，应用于自主生物分子工程等领域。该系统自主管理仪器，制定实验特定程序和优化启发式方法，并同时服务多个用户请求。该平台建立在模型、实验和仪器的协同设计理念之上，支持AI模型和自动化系统的协同进化。这建立了一个端到端、多用户自主实验室，能够处理跨越多种仪器的复杂多目标实验。我们的自主实验室支持基本的核酸功能——包括合成、转录、扩增和测序。它还支持疾病诊断、药物开发和信息存储等领域的应用。在没有人为干预的情况下，它自主优化实验性能以匹配人类科学家所实现的最新水平。在多用户场景中，该平台显著提高了仪器利用率和实验效率。该平台为先进生物材料研究克服对专家和资源障碍的依赖铺平了道路，为大规模的科学即服务建立了蓝图。", "summary": "该论文介绍了一个AI原生自主实验室，专为高度复杂的生物分子工程设计。与现有仅限于简单任务的系统不同，该平台自主管理仪器、设计实验程序并优化性能至人类水平。它建立在协同设计理念之上，支持多种核酸功能和应用，在多用户环境中提高效率，并为可扩展的“科学即服务”奠定基础，从而减少对人类专家和资源的依赖。", "keywords": "AI原生实验室, 自主研究, 生物分子工程, 核酸功能, 科学即服务", "comments": "这篇论文在实现完全自主的科学发现方面迈出了重要一步，特别是在生物分子工程领域。其创新之处在于通过协同设计方法，解决了高度复杂、多目标实验的挑战，并实现了AI与实验自动化的深度集成。多用户能力以及性能达到人类专家水平是其显著成就，为“科学即服务”模式的实现铺平了道路。"}}
{"id": "2507.02271", "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation", "authors": ["Feizhen Huang", "Yu Wu", "Yutian Lin", "Bo Du"], "categories": ["cs.CV", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IJCAI 2025", "url": "http://arxiv.org/abs/2507.02271v1", "summary": "Video-to-Audio (V2A) Generation achieves significant progress and plays a\ncrucial role in film and video post-production. However, current methods\noverlook the cinematic language, a critical component of artistic expression in\nfilmmaking. As a result, their performance deteriorates in scenarios where\nFoley targets are only partially visible. To address this challenge, we propose\na simple self-distillation approach to extend V2A models to cinematic language\nscenarios. By simulating the cinematic language variations, the student model\nlearns to align the video features of training pairs with the same audio-visual\ncorrespondences, enabling it to effectively capture the associations between\nsounds and partial visual information. Our method not only achieves impressive\nimprovements under partial visibility across all evaluation metrics, but also\nenhances performance on the large-scale V2A dataset, VGGSound.", "comment": "Accepted by IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.02271v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过自蒸馏突出部分可见电影语言用于视频到音频生成", "tldr": "当前视频到音频（V2A）模型在处理部分可见的拟音目标时表现不佳，因为它忽略了电影语言。本文提出一种简单的自蒸馏方法来解决此问题，显著提升了在部分可见场景和大型数据集上的V2A生成性能。", "motivation": "当前视频到音频（V2A）生成方法忽略了电影语言，导致在拟音目标仅部分可见的场景中性能下降。", "method": "提出一种简单的自蒸馏方法。通过模拟电影语言变化，学生模型学习将训练对的视频特征与相同的视听对应关系对齐，从而有效捕获声音和部分视觉信息之间的关联。", "result": "该方法在部分可见性场景下，所有评估指标均取得了显著改进，并且提升了在大型V2A数据集VGGSound上的性能。", "conclusion": "所提出的自蒸馏方法有效解决了V2A生成中部分可见电影语言的挑战，显著提高了性能。", "translation": "视频到音频（V2A）生成取得了显著进展，并在电影和视频后期制作中发挥着关键作用。然而，当前方法忽略了电影语言，这是电影制作中艺术表达的关键组成部分。因此，在拟音目标仅部分可见的场景中，它们的性能会下降。为了解决这一挑战，我们提出了一种简单的自蒸馏方法，将V2A模型扩展到电影语言场景。通过模拟电影语言变化，学生模型学习将训练对的视频特征与相同的视听对应关系对齐，使其能够有效捕获声音和部分视觉信息之间的关联。我们的方法不仅在所有评估指标下对部分可见性实现了显著改进，而且还增强了在大型V2A数据集VGGSound上的性能。", "summary": "本文解决了当前视频到音频（V2A）生成模型在处理部分可见拟音目标时因忽略电影语言而导致的性能下降问题。作者提出了一种新颖的自蒸馏方法，通过模拟电影语言变化，训练学生模型将声音与部分视觉信息关联起来。该方法在部分可见场景下显著提高了V2A生成性能，并在VGGSound数据集上也有所增强。", "keywords": "视频到音频生成, 电影语言, 自蒸馏, 部分可见性, 拟音目标", "comments": "本文提出了一种创新的自蒸馏技术，将电影语言理解整合到V2A生成中，这通常是一个被忽视的关键方面。其优势在于提高了在视觉信息不完整这一现实场景中的性能，使其对电影和视频后期制作具有高度实用性。"}}
{"id": "2507.01996", "title": "A Data-Driven Model Predictive Controller to manage epidemics: The case of SARS-CoV-2 in Mauritius", "authors": ["S. Z. Sayed Hassen", "A. Aboudonia", "J. Lygeros"], "categories": ["q-bio.PE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Populations and Evolution (q-bio.PE)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures, European Control Conference 2025", "url": "http://arxiv.org/abs/2507.01996v1", "summary": "This work investigates the benefits of implementing a systematic approach to\nsocial isolation policies during epidemics. We develop a mixed integer\ndata-driven model predictive control (MPC) scheme based on an SIHRD model which\nis identified from available data. The case of the spread of the SARS-CoV-2\nvirus (also known as COVID-19) in Mauritius is used as a reference point with\ndata obtained during the period December 2021 to May 2022. The isolation scheme\nis designed with the control decision variable taking a finite set of values\ncorresponding to the desired level of isolation. The control input is further\nrestricted to shifting between levels only after a minimum amount of time. The\nsimulation results validate our design, showing that the need for\nhospitalisation remains within the capacity of the health centres, with the\nnumber of deaths considerably reduced by raising the level of isolation for\nshort periods of time with negligible social and economic impact. We also show\nthat the introduction of additional isolation levels results in a smoother\ncontainment approach with a considerably reduced hospitalisation burden.", "comment": "6 pages, 6 figures, European Control Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.01996v1", "cate": "q-bio.PE", "date": "2025-06-30", "updated": "2025-06-30", "AI": {"title_translation": "一种数据驱动模型预测控制器用于管理流行病：毛里求斯SARS-CoV-2的案例", "tldr": "本研究开发了一种数据驱动的模型预测控制方案，用于管理流行病中的社会隔离政策，以毛里求斯SARS-CoV-2数据为例，结果表明该方案能在保持医疗系统容量的同时，显著减少死亡人数。", "motivation": "本工作旨在探究在流行病期间实施系统性社会隔离政策的益处。", "method": "本文开发了一种基于SIHRD模型（从可用数据中识别）的混合整数数据驱动模型预测控制（MPC）方案。该隔离方案将控制决策变量设定为有限的隔离水平，并限制了隔离水平切换的时间间隔。研究以毛里求斯SARS-CoV-2病毒（COVID-19）在2021年12月至2022年5月期间的传播数据作为参考点。", "result": "仿真结果验证了该设计的有效性，表明住院需求保持在医疗中心容量范围内，通过在短时间内提高隔离水平，死亡人数显著减少，且对社会和经济影响可忽略不计。研究还表明，引入额外的隔离水平可实现更平滑的遏制方法，并显著减轻住院负担。", "conclusion": "数据驱动的模型预测控制器可以有效地管理流行病中的社会隔离政策，在不超载医疗系统的前提下，显著降低死亡率并提供更平滑的疫情控制策略。", "translation": "本工作旨在研究在流行病期间实施系统性社会隔离政策的益处。我们开发了一种基于SIHRD模型（从可用数据中识别）的混合整数数据驱动模型预测控制（MPC）方案。以毛里求斯SARS-CoV-2病毒（也称为COVID-19）的传播案例作为参考点，使用了2021年12月至2022年5月期间获得的数据。隔离方案的设计中，控制决策变量取有限值集，对应于期望的隔离水平。控制输入进一步限制为仅在最短时间后才能在各级别之间切换。仿真结果验证了我们的设计，表明住院需求保持在医疗中心的容量范围内，通过在短时间内提高隔离水平，死亡人数显著减少，且对社会和经济影响可忽略不计。我们还表明，引入额外的隔离水平可以实现更平滑的遏制方法，并大大减轻住院负担。", "summary": "本研究提出了一种数据驱动的混合整数模型预测控制（MPC）方案，用于管理流行病中的社会隔离政策。该方案基于从实际数据中识别的SIHRD模型，并以毛里求斯SARS-CoV-2疫情为例进行了验证。仿真结果显示，该控制策略能够有效确保住院需求在医疗系统容量之内，并通过短期提高隔离水平显著降低死亡人数，同时对社会经济影响甚微。此外，增加隔离等级有助于实现更平滑的疫情遏制和减轻住院负担。", "keywords": "模型预测控制, 流行病管理, 社会隔离, SARS-CoV-2, 数据驱动", "comments": "这篇论文提出了一种实用的数据驱动MPC方法来管理流行病中的社会隔离，其创新之处在于结合了SIHRD模型和MPC，并通过有限的隔离水平和切换限制使其更具可操作性。该研究的重要性在于为公共卫生政策制定提供了量化且系统化的决策支持工具，特别是在资源有限的情况下，能够有效平衡疫情控制与社会经济影响。其局限性可能在于模型对数据质量的依赖以及对未来疫情走势预测的准确性。"}}
{"id": "2507.02119", "title": "Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks", "authors": ["Shikai Qiu", "Lechao Xiao", "Andrew Gordon Wilson", "Jeffrey Pennington", "Atish Agarwala"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 25. Code available at this https URL", "url": "http://arxiv.org/abs/2507.02119v1", "summary": "What scaling limits govern neural network training dynamics when model size\nand training time grow in tandem? We show that despite the complex interactions\nbetween architecture, training algorithms, and data, compute-optimally trained\nmodels exhibit a remarkably precise universality. Specifically, loss curves\nfrom models of varying sizes collapse onto a single universal curve when\ntraining compute and loss are normalized to unity at the end of training. With\nlearning rate decay, the collapse becomes so tight that differences in the\nnormalized curves across models fall below the noise floor of individual loss\ncurves across random seeds, a phenomenon we term supercollapse. We observe\nsupercollapse across learning rate schedules, datasets, and architectures,\nincluding transformers trained on next-token prediction, and find it breaks\ndown when hyperparameters are scaled suboptimally, providing a precise and\npractical indicator of good scaling. We explain these phenomena by connecting\ncollapse to the power-law structure in typical neural scaling laws, and\nanalyzing a simple yet surprisingly effective model of SGD noise dynamics that\naccurately predicts loss curves across various learning rate schedules and\nquantitatively explains the origin of supercollapse.", "comment": "ICML 25. Code available at https://github.com/shikaiqiu/supercollapse", "pdf_url": "http://arxiv.org/pdf/2507.02119v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "缩放崩塌揭示了计算优化训练神经网络中的普适动力学", "tldr": "计算优化训练的神经网络在训练计算量和损失标准化后，其损失曲线会收敛到一条普适曲线上，甚至出现“超崩塌”现象。", "motivation": "了解当模型大小和训练时间同步增长时，神经网络训练动力学受何种缩放限制支配。", "method": "观察不同大小模型在计算优化训练下的损失曲线行为；通过标准化训练计算量和损失来分析曲线；应用学习率衰减并观察“超崩塌”现象；在不同学习率调度、数据集和架构（包括Transformer）上进行验证；通过将崩塌与神经缩放律中的幂律结构联系起来，并分析一个简单的SGD噪声动力学模型来解释这些现象。", "result": "计算优化训练的模型表现出精确的普适性；当训练计算量和损失标准化后，不同大小模型的损失曲线会崩塌成一条普适曲线；在学习率衰减下，崩塌变得非常紧密，差异低于噪声水平，称为“超崩塌”；“超崩塌”在不同学习率调度、数据集和架构（包括Transformer）中普遍存在；当超参数缩放不理想时，“超崩塌”现象会消失，这提供了一个衡量良好缩放的精确实用指标；通过连接幂律结构和SGD噪声动力学模型，可以解释这些现象并预测损失曲线。", "conclusion": "计算优化训练的神经网络展现出普适的训练动力学，其损失曲线在标准化后会崩塌。这种“超崩塌”现象可以作为判断超参数缩放是否良好的实用指标，并且可以通过SGD噪声动力学模型得到解释。", "translation": "当模型大小和训练时间同步增长时，何种缩放限制支配着神经网络的训练动力学？我们发现，尽管架构、训练算法和数据之间存在复杂的相互作用，但计算优化训练的模型表现出惊人精确的普适性。具体来说，当训练计算量和损失在训练结束时标准化为单位时，不同大小模型的损失曲线会崩塌到一条普适曲线上。随着学习率衰减，这种崩塌变得如此紧密，以至于归一化曲线在不同模型间的差异低于单个损失曲线在不同随机种子间的噪声水平，我们称之为“超崩塌”现象。我们在学习率调度、数据集和架构（包括用于下一词预测的Transformer）中观察到“超崩塌”，并发现当超参数缩放不理想时，该现象会失效，这提供了一个精确实用的良好缩放指标。我们通过将崩塌与典型神经缩放律中的幂律结构联系起来，并分析一个简单而惊人有效的SGD噪声动力学模型来解释这些现象，该模型准确预测了各种学习率调度下的损失曲线，并定量解释了“超崩塌”的起源。", "summary": "本文探讨了计算优化训练神经网络的缩放限制和训练动力学。研究发现，尽管存在复杂性，但这类模型的损失曲线在标准化后会崩塌到一条普适曲线上，并在学习率衰减下表现出“超崩塌”现象。这种普适性在不同设置下普遍存在，并可作为判断超参数良好缩放的指标。作者通过连接幂律结构和分析SGD噪声动力学模型来解释这些现象。", "keywords": "神经网络训练, 缩放律, 普适性, 超崩塌, SGD噪声动力学", "comments": "这篇论文揭示了计算优化训练神经网络中一个显著的普适性现象，即“缩放崩塌”和“超崩塌”。这一发现不仅加深了我们对神经网络训练动力学的理解，还提供了一个实用的工具来评估模型缩放的质量。其创新之处在于发现了这种跨架构、数据集和学习率调度的普适性，并用理论模型（SGD噪声动力学）对其进行了定量解释，这对于指导未来大规模神经网络的训练和超参数选择具有重要意义。"}}
{"id": "2507.02592", "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02592v1", "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02592v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "WebSailor：为网络代理导航超人推理", "tldr": "WebSailor是一种新的后训练方法，旨在通过生成高不确定性任务和高效的强化学习算法，使开源大型语言模型在复杂信息检索任务中达到与专有代理相当的超人性能。", "motivation": "超越人类认知极限是大型语言模型（LLM）训练的关键前沿。专有代理系统（如DeepResearch）在BrowseComp等极其复杂的信息检索基准测试中展现出超人能力，而开源模型缺乏这种系统性降低极端不确定性的复杂推理模式。因此，研究的动机是为开源模型注入这种关键能力。", "method": "WebSailor通过以下方法实现：1. 通过结构化采样和信息混淆生成新颖的、高不确定性的任务。2. 采用RFT冷启动。3. 使用高效的代理强化学习训练算法——复制采样策略优化（Duplicating Sampling Policy Optimization, DUPO）。", "result": "WebSailor在复杂的信息检索任务中显著优于所有开源代理，并能与专有代理的性能相匹配，从而弥合了能力差距。", "conclusion": "WebSailor成功地将专有代理的复杂推理模式引入开源模型，使其在复杂信息检索任务中达到超人性能，并有效缩小了与专有系统的能力差距。", "translation": "大型语言模型（LLM）训练的一个关键前沿是超越人类的认知局限。像DeepResearch这样的专有代理系统在BrowseComp等极其复杂的信息检索基准测试中展现出超人能力，这是以前无法实现的壮举。我们认为，它们的成功取决于一种在开源模型中缺失的复杂推理模式：在导航广阔信息景观时系统地降低极端不确定性的能力。基于这一洞察，我们引入了WebSailor，一个完整的后训练方法，旨在灌输这种关键能力。我们的方法包括通过结构化采样和信息混淆生成新颖的、高不确定性的任务，RFT冷启动，以及一种高效的代理强化学习训练算法——复制采样策略优化（DUPO）。通过这个集成管道，WebSailor在复杂信息检索任务中显著优于所有开源代理，与专有代理的性能相匹配，并弥合了能力差距。", "summary": "WebSailor是一种创新的后训练方法，旨在赋予开源大型语言模型（LLM）在复杂信息检索任务中处理高不确定性的超人推理能力。该方法通过生成高不确定性任务、RFT冷启动和高效的复制采样策略优化（DUPO）强化学习算法，成功地使开源代理的性能超越现有开源模型，并达到专有代理的水平，从而弥补了关键的能力差距。", "keywords": "WebSailor, 大型语言模型, 信息检索, 强化学习, 不确定性处理", "comments": "WebSailor的创新之处在于识别并解决了开源大型语言模型在处理极端不确定性方面与专有系统之间的能力差距。它通过一套新颖的后训练方法，包括生成高不确定性任务和高效的RL算法，成功地将复杂的推理模式注入开源模型，使其性能达到超人水平，这对于推动LLM在实际应用中的发展具有重要意义。"}}
{"id": "2507.02727", "title": "Quantifying Classifier Utility under Local Differential Privacy", "authors": ["Ye Zheng", "Yidan Hu"], "categories": ["cs.CR", "E.3"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02727v1", "summary": "Local differential privacy (LDP) provides a rigorous and quantifiable privacy\nguarantee for personal data by introducing perturbation at the data source.\nHowever, quantifying the impact of these perturbations on classifier utility\nremains a theoretical challenge, particularly for complex or black-box\nclassifiers.\n  This paper presents a framework for theoretically quantifying classifier\nutility under LDP mechanisms. The key insight is that LDP perturbation is\nconcentrated around the original data with a specific probability, transforming\nutility analysis of the classifier into its robustness analysis in this\nconcentrated region. Our framework connects the concentration analysis of LDP\nmechanisms with the robustness analysis of classifiers. It treats LDP\nmechanisms as general distributional functions and classifiers as black-box\nfunctions, thus applicable to any LDP mechanism and classifier. A direct\napplication of our utility quantification is guiding the selection of LDP\nmechanisms and privacy parameters for a given classifier. Notably, our analysis\nshows that a piecewise-based mechanism leads to better utility compared to\nalternatives in common scenarios.\n  Using this framework alongside two novel refinement techniques, we conduct\ncase studies on utility quantification for typical mechanism-classifier\ncombinations. The results demonstrate that our theoretical utility\nquantification aligns closely with empirical observations, particularly when\nclassifiers operate in lower-dimensional input spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02727v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "量化局部差分隐私下分类器效用", "tldr": "本文提出了一个理论框架，用于量化局部差分隐私（LDP）机制下分类器的效用，通过将LDP扰动分析转化为分类器的鲁棒性分析，并发现分段机制在常见场景下能提供更好的效用。", "motivation": "局部差分隐私（LDP）通过在数据源引入扰动来提供严格的隐私保证，但量化这些扰动对分类器效用的影响，特别是对于复杂或黑盒分类器，仍然是一个理论挑战。", "method": "本文提出了一个理论框架，用于量化LDP机制下分类器的效用。其核心思想是将LDP扰动集中在原始数据周围的特定概率，从而将分类器效用分析转化为其在该集中区域的鲁棒性分析。该框架将LDP机制的集中度分析与分类器的鲁棒性分析相结合，将LDP机制视为广义分布函数，分类器视为黑盒函数，因此适用于任何LDP机制和分类器。研究还使用了两种新颖的细化技术进行案例研究。", "result": "该框架的一个直接应用是指导给定分类器的LDP机制和隐私参数的选择。分析表明，在常见场景下，分段机制（piecewise-based mechanism）相比其他替代方案能带来更好的效用。案例研究结果表明，理论效用量化与经验观察结果非常吻合，尤其是在分类器在低维输入空间中运行时。", "conclusion": "本文提出的框架成功地量化了局部差分隐私下分类器的效用，并通过将LDP扰动分析转化为分类器鲁棒性分析，提供了一种通用且实用的方法，可用于指导LDP机制和隐私参数的选择，并能有效预测实际表现。", "translation": "局部差分隐私（LDP）通过在数据源引入扰动，为个人数据提供了严格且可量化的隐私保证。然而，量化这些扰动对分类器效用的影响仍然是一个理论挑战，特别是对于复杂或黑盒分类器。\n本文提出了一个理论框架，用于量化LDP机制下分类器的效用。关键的见解是，LDP扰动以特定概率集中在原始数据周围，从而将分类器的效用分析转化为其在该集中区域的鲁棒性分析。我们的框架将LDP机制的集中度分析与分类器的鲁棒性分析联系起来。它将LDP机制视为广义分布函数，将分类器视为黑盒函数，因此适用于任何LDP机制和分类器。我们效用量化框架的一个直接应用是指导给定分类器的LDP机制和隐私参数的选择。值得注意的是，我们的分析表明，在常见场景下，分段机制相比其他替代方案能带来更好的效用。\n利用该框架以及两种新颖的细化技术，我们对典型的机制-分类器组合进行了效用量化案例研究。结果表明，我们的理论效用量化与经验观察结果非常吻合，尤其是在分类器在低维输入空间中运行时。", "summary": "本文提出了一个通用的理论框架，用于量化局部差分隐私（LDP）机制下分类器的效用。该框架的核心思想是将LDP扰动对效用的影响转化为分类器在数据集中区域的鲁棒性分析，并连接了LDP机制的集中度分析与分类器的鲁棒性分析。该方法适用于任何LDP机制和黑盒分类器，可用于指导LDP机制和隐私参数的选择。研究发现分段机制在常见场景下能提供更好的效用，且理论量化结果与经验观察高度一致，尤其是在低维输入空间中。", "keywords": "局部差分隐私, 分类器效用, 鲁棒性分析, 理论框架, 隐私保护", "comments": "本文的创新之处在于提出了一个通用的理论框架，巧妙地将LDP下的分类器效用量化问题转化为分类器的鲁棒性分析。这种方法克服了传统上难以量化复杂或黑盒分类器效用的挑战，并为LDP机制和隐私参数的选择提供了实用的指导。其普适性（适用于任何LDP机制和分类器）以及理论与实践的良好吻合是其重要亮点。"}}
{"id": "2507.02547", "title": "Vibration of Soft, Twisted Beams for Under-Actuated Quadrupedal Locomotion", "authors": ["Yuhao Jiang", "Fuchen Chen", "Jamie Paik", "Daniel M. Aukes"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This manuscript is under revision for possible publication in the IEEE/ASME Transactions on Mechatronics. Copyright may be transferred to IEEE if the manuscript is accepted for publication, without further notice. Supplementary videos: this https URL , this https URL", "url": "http://arxiv.org/abs/2507.02547v1", "summary": "Under-actuated compliant robotic systems offer a promising approach to\nmitigating actuation and control challenges by harnessing pre-designed,\nembodied dynamic behaviors. This paper presents Flix-Walker, a novel,\nuntethered, centimeter-scale quadrupedal robot inspired by compliant\nunder-actuated mechanisms. Flix-Walker employs flexible, helix-shaped beams as\nlegs, which are actuated by vibrations from just two motors to achieve three\ndistinct mobility modes. We analyze the actuation parameters required to\ngenerate various locomotion modes through both simulation and prototype\nexperiments. The effects of system and environmental variations on locomotion\nperformance are examined, and we propose a generic metric for selecting control\nparameters that produce robust and functional motions. Experiments validate the\neffectiveness and robustness of these actuation parameters within a closed-loop\ncontrol framework, demonstrating reliable trajectory-tracking and\nself-navigation capabilities.", "comment": "This manuscript is under revision for possible publication in the\n  IEEE/ASME Transactions on Mechatronics. Copyright may be transferred to IEEE\n  if the manuscript is accepted for publication, without further notice.\n  Supplementary videos: https://youtu.be/T3d6FT3Rx-s,\n  https://youtu.be/nPQrhKlN02E", "pdf_url": "http://arxiv.org/pdf/2507.02547v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于欠驱动四足运动的软扭曲梁的振动", "tldr": "Flix-Walker是一种新型厘米级欠驱动四足机器人，通过两个电机振动驱动柔性螺旋腿实现多种运动模式，并通过仿真和实验验证了其鲁棒的轨迹跟踪和自主导航能力。", "motivation": "欠驱动柔性机器人系统通过利用预设计的、具身化的动态行为，为缓解驱动和控制挑战提供了一种有前景的方法。", "method": "本文提出了一种名为Flix-Walker的新型、无束缚、厘米级四足机器人，其灵感来源于柔性欠驱动机制。Flix-Walker采用柔性螺旋形梁作为腿部，仅通过两个电机的振动驱动，即可实现三种不同的移动模式。研究通过仿真和原型实验分析了生成各种运动模式所需的驱动参数，并检查了系统和环境变化对运动性能的影响，提出了一种选择控制参数以产生鲁棒和功能性运动的通用度量标准。", "result": "实验验证了这些驱动参数在闭环控制框架内的有效性和鲁棒性，展示了可靠的轨迹跟踪和自主导航能力。", "conclusion": "论文提出的驱动参数和闭环控制框架能够使Flix-Walker机器人实现有效且鲁棒的轨迹跟踪和自主导航。", "translation": "欠驱动柔性机器人系统通过利用预设计的、具身化的动态行为，为缓解驱动和控制挑战提供了一种有前景的方法。本文介绍了Flix-Walker，这是一种新型的、无束缚的、厘米级四足机器人，其灵感来源于柔性欠驱动机制。Flix-Walker采用柔性螺旋形梁作为腿部，仅通过两个电机的振动驱动，即可实现三种不同的移动模式。我们通过仿真和原型实验分析了生成各种运动模式所需的驱动参数。研究检查了系统和环境变化对运动性能的影响，并提出了一种选择控制参数以产生鲁棒和功能性运动的通用度量标准。实验验证了这些驱动参数在闭环控制框架内的有效性和鲁棒性，展示了可靠的轨迹跟踪和自主导航能力。", "summary": "本文介绍了一种名为Flix-Walker的新型厘米级欠驱动四足机器人，该机器人利用柔性螺旋腿并通过两个电机的振动实现多种运动模式。研究通过仿真和实验分析了驱动参数对运动模式的影响，并提出了一种选择鲁棒控制参数的通用度量。实验验证了所提出的驱动参数在闭环控制下能实现可靠的轨迹跟踪和自主导航。", "keywords": "欠驱动机器人, 四足运动, 柔性梁, 振动驱动, Flix-Walker", "comments": "这篇论文的创新点在于提出了Flix-Walker机器人，它利用柔性螺旋形腿和简单的两电机振动实现复杂的四足运动，有效解决了欠驱动系统的挑战。其通过具身智能实现多种运动模式，并提出通用度量选择控制参数，展现了在微型机器人和软体机器人领域的潜力。"}}
{"id": "2507.02695", "title": "Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms", "authors": ["Sahar Ahmadisakha", "Lech Bialek", "Mohamed Soliman", "Vasilios Andrikopoulos"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02695v1", "summary": "In recent years, sustainability in software systems has gained significant\nattention, especially with the rise of cloud computing and the shift towards\ncloud-based architectures. This shift has intensified the need to identify\nsustainability in architectural discussions to take informed architectural\ndecisions. One source to see these decisions is in online Q&A forums among\npractitioners' discussions. However, recognizing sustainability concepts within\nsoftware practitioners' discussions remains challenging due to the lack of\nclear and distinct guidelines for this task. To address this issue, we\nintroduce the notion of sustainability flags as pointers in relevant\ndiscussions, developed through thematic analysis of multiple sustainability\nbest practices from cloud providers. This study further evaluates the\neffectiveness of these flags in identifying sustainability within cloud\narchitecture posts, using a controlled experiment. Preliminary results suggest\nthat the use of flags results in classifying fewer posts as\nsustainability-related compared to a control group, with moderately higher\ncertainty and significantly improved performance. Moreover, sustainability\nflags are perceived as more useful and understandable than relying solely on\ndefinitions for identifying sustainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02695v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "识别问答平台中可持续性帖子的可持续性标志", "tldr": "本文引入“可持续性标志”来识别问答平台中关于云架构的可持续性讨论。通过主题分析和受控实验，结果显示这些标志能以更高的确定性和显著改进的性能有效识别相关帖子，且比单纯依赖定义更实用和易懂。", "motivation": "近年来，软件系统中的可持续性，尤其是在云计算背景下，受到了广泛关注。在架构讨论中识别可持续性对于做出明智决策至关重要。在线问答论坛是了解从业者讨论中这些决策的来源，但由于缺乏明确的指导方针，识别其中的可持续性概念仍然具有挑战性。", "method": "本研究引入了“可持续性标志”的概念，作为相关讨论中的指示器。这些标志是通过对云提供商的多种可持续性最佳实践进行主题分析而开发的。研究通过受控实验评估了这些标志在识别云架构帖子中可持续性方面的有效性。", "result": "初步结果表明，与对照组相比，使用可持续性标志将更少的帖子归类为可持续性相关，但具有中等程度更高的确定性和显著改进的性能。此外，可持续性标志被认为比单纯依赖定义来识别可持续性更实用和易懂。", "conclusion": "可持续性标志是识别问答平台中云架构可持续性讨论的有效且实用的工具。", "translation": "近年来，软件系统中的可持续性受到了广泛关注，特别是随着云计算的兴起以及向基于云的架构转变。这种转变加剧了在架构讨论中识别可持续性的需求，以便做出明智的架构决策。在线问答论坛是了解从业者讨论中这些决策的一个来源。然而，由于缺乏明确和独特的指导方针，在软件从业者的讨论中识别可持续性概念仍然具有挑战性。为了解决这个问题，我们引入了“可持续性标志”的概念，作为相关讨论中的指针，这些标志是通过对云提供商的多种可持续性最佳实践进行主题分析而开发的。本研究通过受控实验进一步评估了这些标志在识别云架构帖子中可持续性方面的有效性。初步结果表明，与对照组相比，使用这些标志将更少的帖子归类为可持续性相关，但具有中等程度更高的确定性和显著改进的性能。此外，可持续性标志被认为比单纯依赖定义来识别可持续性更实用和易懂。", "summary": "本文针对在线问答平台中识别软件系统可持续性概念的挑战，提出并开发了“可持续性标志”。这些标志基于对云提供商最佳实践的主题分析。通过受控实验评估显示，相比传统方法，使用可持续性标志能够以更高的确定性和显著提升的性能有效识别可持续性相关帖子，并且被认为更实用和易懂。", "keywords": "可持续性, 软件系统, 云计算, 问答平台, 主题分析, 可持续性标志", "comments": "这项研究通过引入“可持续性标志”来解决在非结构化文本（如Q&A论坛）中识别特定复杂概念（如可持续性）的挑战，具有创新性。其方法结合了主题分析和受控实验，验证了所提方法的有效性和实用性，为软件工程领域中信息提取提供了新的工具。"}}
{"id": "2507.02453", "title": "Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?", "authors": ["Jueun Lee", "Martin Flipe", "Philipp Lepold", "Tobias Röddiger", "Michael Beigl"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures. Submitted to the International Symposium on Wearable Computers (ISWC)", "url": "http://arxiv.org/abs/2507.02453v1", "summary": "Wearable haptic interventions offer promising support for relaxation through\nslow, vibrotactile biofeedback. Despite their potential, current applications\nfocus on stress-inducing procedures and fixed vibration patterns, with limited\nconsideration of body location and dynamic biofeedback during restful states.\nThis study investigates the effects of haptic biofeedback adjusted from\nreal-time heart rate during eyes-closed wakeful rest, comparing four wearable\nbody placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave\nactivity on the ear, subjective restfulness, and vibration experience were\nmeasured across these conditions. Results show that biofeedback reduced heart\nrate at the wrist, shoulder, and forearm, while alpha power measured at the ear\nremained unchanged. Subjective restfulness was rated highest at the shoulder\nand forearm, which were also the most preferred locations. In addition,\nparticipants reported greater comfort, relaxation, and further increased\nsleepiness at the forearm compared to the wrist, which was more easily\nrecognizable. These findings suggest that the forearm and shoulder are ideal\nfor unobtrusive relaxation feedback for wakeful rest, while the wrist may\nrequire design improvements for subjective experience.", "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "pdf_url": "http://arxiv.org/pdf/2507.02453v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "清醒休息中的触觉生物反馈：刺激位置有区别吗？", "tldr": "研究发现，在清醒休息时，手腕、肩膀和前臂的触觉生物反馈能降低心率，其中前臂和肩膀是更理想的放松反馈位置。", "motivation": "现有的可穿戴触觉干预主要集中在诱发压力的程序和固定振动模式，对身体位置和清醒休息时的动态生物反馈考虑有限。", "method": "本研究调查了清醒休息时基于实时心率调整的触觉生物反馈效果，比较了手腕、手、前臂和肩膀四个可穿戴身体放置位置。测量了心率、耳部α波活动、主观休息感和振动体验。", "result": "生物反馈在手腕、肩膀和前臂处降低了心率，但耳部α波活动未改变。主观休息感在肩膀和前臂处评分最高，也是最受青睐的位置。参与者报告前臂比手腕更舒适、放松，并增加睡意，而手腕更容易识别。", "conclusion": "前臂和肩膀是清醒休息时无干扰放松反馈的理想位置，手腕可能需要改进设计以提升主观体验。", "translation": "可穿戴触觉干预通过缓慢的振动触觉生物反馈为放松提供了有希望的支持。尽管它们具有潜力，但目前的应用程序主要集中在诱发压力的程序和固定振动模式上，对身体位置和休息状态下的动态生物反馈考虑有限。本研究调查了在闭眼清醒休息期间，根据实时心率调整的触觉生物反馈的效果，比较了手腕、手、前臂和肩膀四个可穿戴身体放置位置。在这些条件下测量了心率、耳部α波活动、主观休息感和振动体验。结果显示，生物反馈在手腕、肩膀和前臂处降低了心率，而耳部测量的α波功率保持不变。主观休息感在肩膀和前臂处评分最高，这也是最受青睐的位置。此外，与手腕相比，参与者报告前臂更舒适、放松，并进一步增加了睡意，而手腕更容易识别。这些发现表明，前臂和肩膀是清醒休息时无干扰放松反馈的理想选择，而手腕可能需要改进设计以提升主观体验。", "summary": "本研究探讨了在清醒休息状态下，不同身体部位（手腕、手、前臂、肩膀）的实时心率调节触觉生物反馈对放松效果的影响。结果表明，手腕、肩膀和前臂的生物反馈能有效降低心率，其中前臂和肩膀在主观放松感和舒适度方面表现最佳，是清醒休息放松反馈的理想位置，而手腕则需改进设计。", "keywords": "触觉生物反馈, 清醒休息, 刺激位置, 心率, 放松", "comments": "该研究创新性地探讨了触觉生物反馈在清醒休息中的应用，并首次比较了不同身体部位刺激的效果，为可穿戴放松设备的优化设计提供了具体指导，强调了刺激位置对用户体验和生理响应的重要性。"}}
{"id": "2507.02442", "title": "The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning", "authors": ["Moto Kamiura"], "categories": ["cs.AI", "math.CT", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02442v1", "summary": "Enhancing the intelligibility and interpretability of machine learning is a\ncrucial task in responding to the demand for Explicability as an AI principle,\nand in promoting the better social implementation of AI. The aim of our\nresearch is to contribute to this improvement by reformulating machine learning\nmodels through the lens of category theory, thereby developing a semantic\nframework for structuring and understanding AI systems. Our categorical\nmodeling in this paper clarifies and formalizes the structural interplay\nbetween residuals and parameters in supervised learning. The present paper\nfocuses on the multiple linear regression model, which represents the most\nbasic form of supervised learning. By defining two concrete categories\ncorresponding to parameters and data, along with an adjoint pair of functors\nbetween them, we introduce our categorical formulation of supervised learning.\nWe show that the essential structure of this framework is captured by what we\ncall the Gauss-Markov Adjunction. Within this setting, the dual flow of\ninformation can be explicitly described as a correspondence between variations\nin parameters and residuals. The ordinary least squares estimator for the\nparameters and the minimum residual are related via the preservation of limits\nby the right adjoint functor. Furthermore, we position this formulation as an\ninstance of extended denotational semantics for supervised learning, and\npropose applying a semantic perspective developed in theoretical computer\nscience as a formal foundation for Explicability in AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02442v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高斯-马尔可夫伴随：监督学习中残差的范畴语义", "tldr": "本文利用范畴论重新构建监督学习模型，特别关注多元线性回归，并提出高斯-马尔可夫伴随作为理解残差和参数相互作用的语义框架，旨在提升AI可解释性。", "motivation": "为了响应AI原理中对可解释性的需求并促进AI的社会实施，研究旨在通过范畴论的视角重新构建机器学习模型，从而开发一个用于构建和理解AI系统的语义框架，以增强机器学习的可理解性和可解释性。", "method": "本文通过定义对应于参数和数据的两个具体范畴，以及它们之间的一对伴随函子，来引入监督学习的范畴化表述。研究重点是多元线性回归模型，并展示了该框架的核心结构被高斯-马尔可夫伴随所捕捉。此外，它通过右伴随函子对极限的保持，关联了普通最小二乘估计量和最小残差。", "result": "研究阐明并形式化了监督学习中残差和参数之间的结构相互作用，引入了高斯-马尔可夫伴随来捕捉框架的基本结构。它明确描述了参数和残差变化之间的信息双向流动，并展示了普通最小二乘估计量与最小残差之间的关系。", "conclusion": "本文提出了监督学习的范畴化表述，以高斯-马尔可夫伴随为例，为理解AI系统提供了一个语义框架，并通过应用理论计算机科学中发展的语义视角，为AI可解释性提供了形式化基础。", "translation": "增强机器学习的可理解性和可解释性是响应AI原理中可解释性需求并促进AI更好社会实施的关键任务。我们研究的目的是通过范畴论的视角重新构建机器学习模型，从而开发一个用于构建和理解AI系统的语义框架，为这一改进做出贡献。本文中的范畴建模阐明并形式化了监督学习中残差和参数之间的结构相互作用。本文重点关注多元线性回归模型，它代表了监督学习最基本的形式。通过定义两个对应于参数和数据的具体范畴，以及它们之间的一对伴随函子，我们引入了监督学习的范畴化表述。我们展示了该框架的基本结构被我们称之为高斯-马尔可夫伴随的概念所捕捉。在此设置下，信息的双向流动可以被明确描述为参数和残差变化之间的对应关系。参数的普通最小二乘估计量和最小残差通过右伴随函子对极限的保持而相关联。此外，我们将此表述定位为监督学习的扩展指称语义的一个实例，并提议将理论计算机科学中发展起来的语义视角应用于AI可解释性的形式基础。", "summary": "本文提出了一种用于监督学习的范畴框架，旨在增强AI的可解释性，特别是针对多元线性回归。它通过“高斯-马尔可夫伴随”形式化了残差和参数之间的关系，描述了信息流，并将OLS估计量与最小残差联系起来。这种方法被视为扩展的指称语义，为AI可解释性提供了基于理论计算机科学的形式基础。", "keywords": "范畴论, 监督学习, 残差, 高斯-马尔可夫伴随, AI可解释性", "comments": "这篇论文通过将范畴论这一抽象数学分支应用于监督学习，为AI可解释性提供了一种新颖的理论方法。其创新之处在于通过“高斯-马尔可夫伴随”的形式化，揭示了残差和参数等基本组件之间的关系。这项抽象的基础性工作可以为理解和解释AI模型提供严谨的数学基础，这对于AI的信任度和更广泛的应用至关重要。其局限性可能在于数学抽象程度较高，对于不具备深厚范畴论背景的实践者来说，可能难以理解。"}}
{"id": "2507.02279", "title": "LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models", "authors": ["Juntao Liu", "Liqiang Niu", "Wenchao Chen", "Jie Zhou", "Fandong Meng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02279v1", "summary": "Existing visual token compression methods for Multimodal Large Language\nModels (MLLMs) predominantly operate as post-encoder modules, limiting their\npotential for efficiency gains. To address this limitation, we propose LaCo\n(Layer-wise Visual Token Compression), a novel framework that enables effective\ntoken compression within the intermediate layers of the vision encoder. LaCo\nintroduces two core components: 1) a layer-wise pixel-shuffle mechanism that\nsystematically merges adjacent tokens through space-to-channel transformations,\nand 2) a residual learning architecture with non-parametric shortcuts that\npreserves critical visual information during compression. Extensive experiments\nindicate that our LaCo outperforms all existing methods when compressing tokens\nin the intermediate layers of the vision encoder, demonstrating superior\neffectiveness. In addition, compared to external compression, our method\nimproves training efficiency beyond 20% and inference throughput over 15% while\nmaintaining strong performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02279v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LaCo: 多模态大型语言模型视觉令牌的高效逐层压缩", "tldr": "LaCo是一种新的视觉令牌压缩方法，通过在视觉编码器中间层进行逐层压缩，显著提高了多模态大语言模型的训练和推理效率，并优于现有方法。", "motivation": "现有视觉令牌压缩方法主要作为编码器后模块运行，限制了其效率提升的潜力。", "method": "提出LaCo框架，在视觉编码器中间层实现有效的令牌压缩。LaCo包含两个核心组件：1) 逐层像素重排机制，通过空间到通道转换系统地合并相邻令牌；2) 带有非参数快捷方式的残差学习架构，在压缩过程中保留关键视觉信息。", "result": "LaCo在视觉编码器中间层压缩令牌时优于所有现有方法，展示了卓越的有效性。与外部压缩相比，在保持强大性能的同时，训练效率提高20%以上，推理吞吐量提高15%以上。", "conclusion": "LaCo通过在视觉编码器中间层进行高效的视觉令牌压缩，显著提升了多模态大型语言模型的效率，且性能优越。", "translation": "现有的多模态大型语言模型（MLLMs）视觉令牌压缩方法主要作为编码器后模块运行，限制了其效率提升的潜力。为了解决这一限制，我们提出了LaCo（逐层视觉令牌压缩），一个新颖的框架，它能够在视觉编码器的中间层内实现有效的令牌压缩。LaCo引入了两个核心组件：1）一个逐层像素重排机制，通过空间到通道的转换系统地合并相邻令牌；2）一个带有非参数快捷方式的残差学习架构，在压缩过程中保留关键视觉信息。大量的实验表明，我们的LaCo在视觉编码器中间层压缩令牌时优于所有现有方法，展示了卓越的有效性。此外，与外部压缩相比，我们的方法在保持强大性能的同时，将训练效率提高了20%以上，推理吞吐量提高了15%以上。", "summary": "本文提出了LaCo，一种用于多模态大型语言模型的新型逐层视觉令牌压缩框架。LaCo通过在视觉编码器中间层集成像素重排和残差学习架构，有效解决了现有方法效率受限的问题。实验证明，LaCo在压缩效果上超越了现有方法，并在保持高性能的同时显著提升了训练和推理效率。", "keywords": "视觉令牌压缩, 多模态大型语言模型, 逐层压缩, 像素重排, 残差学习", "comments": "LaCo的创新之处在于将视觉令牌压缩从编码器后移至编码器中间层，这种逐层压缩策略配合像素重排和残差学习，有效解决了效率瓶颈。其重要性体现在显著提升了MLLMs的训练和推理效率，对大规模多模态模型的实际部署具有重要意义。"}}
{"id": "2507.02131", "title": "Perturbed Gradient Descent Algorithms are Small-Disturbance Input-to-State Stable", "authors": ["Leilei Cui", "Zhong-Ping Jiang", "Eduardo D. Sontag", "Richard D. Braatz"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.02131v1", "summary": "This article investigates the robustness of gradient descent algorithms under\nperturbations. The concept of small-disturbance input-to-state stability (ISS)\nfor discrete-time nonlinear dynamical systems is introduced, along with its\nLyapunov characterization. The conventional linear Polyak-Lojasiewicz (PL)\ncondition is then extended to a nonlinear version, and it is shown that the\ngradient descent algorithm is small-disturbance ISS provided the objective\nfunction satisfies the generalized nonlinear PL condition. This\nsmall-disturbance ISS property guarantees that the gradient descent algorithm\nconverges to a small neighborhood of the optimum under sufficiently small\nperturbations. As a direct application of the developed framework, we\ndemonstrate that the LQR cost satisfies the generalized nonlinear PL condition,\nthereby establishing that the policy gradient algorithm for LQR is\nsmall-disturbance ISS. Additionally, other popular policy gradient algorithms,\nincluding natural policy gradient and Gauss-Newton method, are also proven to\nbe small-disturbance ISS.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.02131v1", "cate": "math.OC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "扰动梯度下降算法是小扰动输入到状态稳定", "tldr": "本文研究发现，在满足广义非线性Polyak-Lojasiewicz (PL) 条件时，梯度下降算法对小扰动具有输入到状态稳定性（ISS），这意味着在足够小的扰动下，算法能收敛到最优值附近。该特性也适用于LQR和多种流行的策略梯度算法。", "motivation": "研究梯度下降算法在存在扰动（例如噪声或模型不确定性）情况下的鲁棒性，以确保其在实际应用中的可靠收敛性能。", "method": "引入了离散时间非线性动力系统的小扰动输入到状态稳定性（ISS）概念及其Lyapunov特征。将传统的线性Polyak-Lojasiewicz (PL) 条件推广到非线性版本。在此理论框架下，证明了满足广义非线性PL条件的目标函数能使梯度下降算法具备小扰动ISS特性。进一步将该框架应用于LQR代价函数，并分析了多种流行的策略梯度算法。", "result": "1. 引入了离散时间非线性动力系统的小扰动输入到状态稳定性（ISS）概念及其Lyapunov特征。 2. 证明了在目标函数满足广义非线性Polyak-Lojasiewicz (PL) 条件的情况下，梯度下降算法是小扰动ISS的。 3. 这种小扰动ISS特性保证了在足够小的扰动下，梯度下降算法收敛到最优解的小邻域。 4. 证明了LQR代价函数满足广义非线性PL条件，从而LQR的策略梯度算法是小扰动ISS的。 5. 证明了包括自然策略梯度和高斯-牛顿法在内的其他流行策略梯度算法也是小扰动ISS的。", "conclusion": "本文得出结论，梯度下降算法在满足广义非线性Polyak-Lojasiewicz (PL) 条件时，对小扰动具有输入到状态稳定性（ISS），这保证了其在扰动下的收敛性。该理论框架及其结论适用于包括LQR策略梯度在内的多种流行策略梯度算法，为优化算法的鲁棒性分析提供了重要工具。", "translation": "本文研究了梯度下降算法在扰动下的鲁棒性。文章引入了离散时间非线性动力系统的小扰动输入到状态稳定性（ISS）概念及其Lyapunov特征。然后将传统的线性Polyak-Lojasiewicz (PL) 条件推广到非线性版本，并表明只要目标函数满足广义非线性PL条件，梯度下降算法就是小扰动ISS的。这种小扰动ISS特性保证了在足够小的扰动下，梯度下降算法收敛到最优值的小邻域。作为所开发框架的直接应用，我们证明了LQR代价函数满足广义非线性PL条件，从而确立了LQR的策略梯度算法是小扰动ISS的。此外，其他流行的策略梯度算法，包括自然策略梯度和高斯-牛顿法，也被证明是小扰动ISS的。", "summary": "本文研究了梯度下降算法在扰动下的鲁棒性。通过引入离散时间非线性系统的小扰动输入到状态稳定性（ISS）概念及其Lyapunov表征，并将传统的线性Polyak-Lojasiewicz (PL) 条件推广到非线性版本，证明了在目标函数满足广义非线性PL条件时，梯度下降算法具有小扰动ISS特性，确保其在小扰动下收敛到最优解附近。该框架的应用表明，LQR代价函数满足此条件，且LQR的策略梯度算法以及其他流行策略梯度算法（如自然策略梯度和高斯-牛顿法）也具备小扰动ISS特性。", "keywords": "梯度下降, 鲁棒性, 输入到状态稳定性, Polyak-Lojasiewicz条件, 策略梯度算法", "comments": "本文的创新之处在于将小扰动输入到状态稳定性（ISS）的概念引入到梯度下降算法的鲁棒性分析中，并提出了广义非线性Polyak-Lojasiewicz条件来表征这种稳定性。这为理解和设计在存在扰动情况下依然能有效收敛的优化算法提供了坚实的理论基础。其重要性体现在将理论分析应用于实际的策略梯度算法，证明了这些算法在实际应用中的鲁棒性，具有较高的理论和实践价值。"}}
{"id": "2507.02128", "title": "CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs", "authors": ["Jingyu Pan", "Isaac Jacobson", "Zheng Zhao", "Tung-Chieh Chen", "Guanglei Zhou", "Chen-Chia Chang", "Vineet Rashingkar", "Yiran Chen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICCAD 2025", "url": "http://arxiv.org/abs/2507.02128v1", "summary": "Modern very large-scale integration (VLSI) design requires the implementation\nof integrated circuits using electronic design automation (EDA) tools. Due to\nthe complexity of EDA algorithms, the vast parameter space poses a huge\nchallenge to chip design optimization, as the combination of even moderate\nnumbers of parameters creates an enormous solution space to explore. Manual\nparameter selection remains industrial practice despite being excessively\nlaborious and limited by expert experience. To address this issue, we present\nCROP, the first large language model (LLM)-powered automatic VLSI design flow\ntuning framework. Our approach includes: (1) a scalable methodology for\ntransforming RTL source code into dense vector representations, (2) an\nembedding-based retrieval system for matching designs with semantically similar\ncircuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided\nparameter search system that constrains the search process with prior knowledge\nfrom similar designs. Experiment results demonstrate CROP's ability to achieve\nsuperior quality-of-results (QoR) with fewer iterations than existing\napproaches on industrial designs, including a 9.9% reduction in power\nconsumption.", "comment": "Accepted by ICCAD 2025", "pdf_url": "http://arxiv.org/pdf/2507.02128v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "CROP：使用大型语言模型进行参数引导的电路检索与优化", "tldr": "CROP是一个基于LLM的VLSI设计优化框架，通过检索和RAG增强的参数搜索，显著减少了功耗并提高了设计质量，减少了人工干预和迭代次数。", "motivation": "现代VLSI设计中，EDA工具的参数空间巨大，导致芯片设计优化面临巨大挑战。手动参数选择劳动密集且受专家经验限制，无法有效应对复杂的优化问题。", "method": "CROP是首个LLM驱动的VLSI设计流程自动调优框架。它包含：1. 将RTL源代码转换为密集向量表示的可扩展方法。2. 基于嵌入的检索系统，用于匹配语义相似的电路设计。3. 一个由检索增强生成（RAG）增强的LLM引导参数搜索系统，该系统利用来自相似设计的先验知识来约束搜索过程。", "result": "CROP在工业设计上实现了比现有方法更优异的质量结果（QoR）和更少的迭代次数，包括功耗降低9.9%。", "conclusion": "CROP框架通过结合LLM、嵌入式检索和RAG技术，成功解决了VLSI设计优化中参数空间巨大和手动调优效率低下的问题，显著提升了设计质量并减少了优化时间。", "translation": "现代超大规模集成电路（VLSI）设计需要使用电子设计自动化（EDA）工具来实现集成电路。由于EDA算法的复杂性，巨大的参数空间对芯片设计优化构成了巨大挑战，因为即使是中等数量的参数组合也会产生巨大的解决方案空间供探索。尽管手动参数选择极其费力且受专家经验限制，但它仍然是工业实践。为了解决这个问题，我们提出了CROP，这是第一个由大型语言模型（LLM）驱动的自动VLSI设计流程调优框架。我们的方法包括：（1）将RTL源代码转换为密集向量表示的可扩展方法，（2）一个基于嵌入的检索系统，用于将设计与语义相似的电路进行匹配，以及（3）一个由检索增强生成（RAG）增强的LLM引导参数搜索系统，该系统利用来自相似设计的先验知识来约束搜索过程。实验结果表明，CROP能够在工业设计上以比现有方法更少的迭代次数实现卓越的质量结果（QoR），包括功耗降低9.9%。", "summary": "本文提出了CROP，一个基于大型语言模型（LLM）的自动VLSI设计流程调优框架，旨在解决传统EDA工具参数空间巨大和手动优化效率低下的问题。CROP通过将RTL代码转换为向量表示、建立嵌入式检索系统以及利用RAG增强的LLM引导参数搜索，实现了对设计优化的有效指导。实验证明，CROP在工业设计上能以更少迭代次数获得更优的设计质量，例如显著降低功耗。", "keywords": "LLM, VLSI设计, 电路优化, 参数引导, 检索增强生成 (RAG)", "comments": "CROP的创新之处在于首次将LLM引入VLSI设计流程的自动调优中，通过结合嵌入式检索和RAG技术，有效利用了先验知识来指导复杂的参数搜索过程。这显著提升了设计效率和质量，有望改变芯片设计的传统手动优化范式，对EDA领域具有重要意义。"}}
{"id": "2507.02084", "title": "Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation", "authors": ["Yining Feng", "Ivan Selesnick"], "categories": ["stat.ML", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02084v1", "summary": "The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular\nalgorithm for finding a desirable solution to the LASSO problem without\nexplicitly tuning the regularization parameter $\\lambda$. Despite that the\nadaptive ISTA is a successful practical algorithm, few theoretical results\nexist. In this paper, we present the theoretical analysis on the adaptive ISTA\nwith the thresholding strategy of estimating noise level by median absolute\ndeviation. We show properties of the fixed points of the algorithm, including\nscale equivariance, non-uniqueness, and local stability, prove the local linear\nconvergence guarantee, and show its global convergence behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02084v1", "cate": "stat.ML", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "自适应迭代软阈值算法与中位数绝对偏差", "tldr": "本文对使用中位数绝对偏差估计噪声水平的自适应ISTA算法进行了理论分析，证明了其固定点性质、局部线性收敛性和全局收敛行为。", "motivation": "自适应迭代软阈值算法（ISTA）在解决LASSO问题时无需显式调整正则化参数，但其理论结果较少。", "method": "本文对采用中位数绝对偏差估计噪声水平的自适应ISTA算法进行了理论分析。", "result": "研究结果包括算法固定点的尺度等变性、非唯一性和局部稳定性，以及局部线性收敛保证和全局收敛行为。", "conclusion": "本文提供了自适应ISTA算法的理论分析，填补了该算法在理论方面的空白。", "translation": "自适应迭代软阈值算法（ISTA）是一种流行的算法，用于在不显式调整正则化参数$\\lambda$的情况下找到LASSO问题的理想解。尽管自适应ISTA是一种成功的实用算法，但其理论结果却很少。在本文中，我们对采用中位数绝对偏差估计噪声水平的自适应ISTA算法进行了理论分析。我们展示了算法固定点的性质，包括尺度等变性、非唯一性和局部稳定性，证明了局部线性收敛保证，并展示了其全局收敛行为。", "summary": "本研究对流行的自适应迭代软阈值算法（ISTA）进行了深入的理论分析，该算法在处理LASSO问题时无需手动调整正则化参数。文章重点探讨了使用中位数绝对偏差进行阈值处理的策略，并详细阐述了算法固定点的特性（如尺度等变性、非唯一性和局部稳定性）、局部线性收敛性以及全局收敛行为，填补了该算法在理论研究方面的空白。", "keywords": "自适应ISTA, 迭代软阈值算法, LASSO, 理论分析, 中位数绝对偏差", "comments": "本文的创新点在于为一种广泛使用的实用算法——自适应ISTA——提供了急需的理论基础，特别是在其阈值策略上采用了中位数绝对偏差。这有助于更好地理解算法的行为和收敛性，从而可能指导未来的改进和应用。"}}
{"id": "2507.02593", "title": "Revisiting Active Learning under (Human) Label Variation", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "Göran Kauermann", "Barbara Plank", "Matthias Aßenmacher"], "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02593v1", "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02593v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "重新审视（人类）标签变异下的主动学习", "tldr": "主动学习应考虑人类标签变异，以更好地反映真实世界的数据标注复杂性。", "motivation": "高质量的标注数据是监督学习的限制因素。现有的标注框架和主动学习方法常忽略人类标签变异（HLV），将其视为噪音而非信息信号，导致在实践中表现不佳。", "method": "本文审视了关于“真理”和“标签性质”的基本假设，强调需要将观察到的标签变异分解为信号（如HLV）和噪声（如标注错误）。作者调查了主动学习和(人类)标签变异社区如何处理或忽视这些区别，并提出了一个概念框架，用于在主动学习循环中整合HLV，包括实例选择、标注者选择和标签表示。此外，还讨论了将大型语言模型（LLM）作为标注者的整合。", "result": "提出了一个用于HVL感知主动学习的概念基础。", "conclusion": "本文旨在为HVL感知主动学习奠定概念基础，更好地反映真实世界标注的复杂性。", "translation": "访问高质量的标注数据仍然是应用监督学习的一个限制因素。虽然标签变异（LV），即同一实例的不同标签，很常见，尤其是在自然语言处理中，但标注框架通常仍然基于单一“真实值”的假设。这忽略了人类标签变异（HLV），即标注中可能存在的合理差异，作为一种信息信号。同样，主动学习（AL）是一种优化有限标注预算以训练机器学习模型的流行方法，它通常依赖于若干简化假设中的至少一个，而这些假设在承认HLV时在实践中很少成立。在本文中，我们审视了关于“真理”和“标签性质”的基本假设，强调需要将观察到的LV分解为信号（例如HLV）和噪声（例如标注错误）。我们调查了AL和（H）LV社区如何处理——或忽视——这些区别，并提出了一个概念框架，用于在整个AL循环中整合HLV，包括实例选择、标注者选择和标签表示。我们进一步讨论了将大型语言模型（LLM）作为标注者的整合。我们的工作旨在为HLV感知的主动学习奠定概念基础，更好地反映真实世界标注的复杂性。", "summary": "本文探讨了在监督学习中标注数据质量受限的问题，特别是现有主动学习方法忽视人类标签变异（HLV）的局限性。作者提出将标签变异分解为有用信号（HLV）和噪声，并审视了相关社区的处理方式。核心贡献是一个概念框架，用于在主动学习的各个环节（实例选择、标注者选择、标签表示）整合HLV，并讨论了LLM作为标注者的潜力，旨在为更贴近真实世界标注复杂性的HLV感知主动学习奠定基础。", "keywords": "主动学习, 标签变异, 人类标签变异, 数据标注, 大型语言模型", "comments": "本文的创新点在于系统地将人类标签变异（HLV）视为一种信息信号而非简单噪声，并提出将其整合到主动学习（AL）框架中的概念性方法。这对于提高现实世界中数据标注的效率和模型性能具有重要意义，尤其是在自然语言处理等标签模糊性高的领域。讨论LLM作为标注者的潜力也增加了其前瞻性。"}}
{"id": "2507.02735", "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks", "authors": ["Sizhe Chen", "Arman Zharmagambetov", "David Wagner", "Chuan Guo"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02735v1", "summary": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02735v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Meta SecAlign：一个对抗提示注入攻击的安全基础LLM", "tldr": "Meta SecAlign是首个开源、开放权重的LLM，内置模型级防御，能有效抵御提示注入攻击，并在通用和安全任务上表现出色，达到商业级模型的性能。", "motivation": "提示注入攻击对LLM集成应用构成重大安全威胁。尽管模型级防御有效，但目前商业模型以闭源方式部署。AI安全社区需要开源模型，通过开放研究共同开发攻击和防御，以推动缓解提示注入攻击的科学进展。", "method": "开发了Meta SecAlign，首个开源且开放权重的LLM，内置模型级防御。该模型利用了SOTA SecAlign防御的改进版本，并提供了完整的训练配方细节。", "result": "在9个效用基准和7个安全基准上的评估表明，Meta SecAlign（即使在通用指令微调数据集上训练）能在未见的下游任务（包括工具调用和代理式网络导航）以及通用指令遵循中提供安全性。其最佳模型——Meta-SecAlign-70B——在对抗提示注入攻击方面达到了最先进的鲁棒性，并且与闭源商业LLM在实用性上具有可比性。", "conclusion": "Meta SecAlign是首个开源、开放权重的LLM，通过内置模型级防御，有效抵御提示注入攻击，并在安全性和实用性方面达到商业级模型的性能，推动了AI安全领域的开放研究。", "translation": "提示注入攻击对LLM集成应用构成重大安全威胁。模型级防御已显示出强大的有效性，但目前以闭源方式部署到商业级模型中。我们认为AI安全社区需要开源模型，通过开放研究共同开发攻击和防御，从而推动缓解提示注入攻击的科学进步。为此，我们开发了Meta SecAlign，这是第一个内置模型级防御的开源和开放权重LLM，实现了商业级模型的性能。我们提供了完整的训练配方细节，该配方利用了SOTA SecAlign防御的改进版本。对9个效用基准和7个安全基准的评估表明，Meta SecAlign尽管在通用指令微调数据集上进行训练，但在未见的下游任务（包括工具调用和代理式网络导航）以及通用指令遵循中都提供了安全性。我们最好的模型——Meta-SecAlign-70B——在对抗提示注入攻击方面实现了最先进的鲁棒性，并且在实用性方面可与具有模型级防御的闭源商业LLM相媲美。", "summary": "本文介绍了Meta SecAlign，这是首个开源且开放权重的LLM，旨在通过内置模型级防御来对抗提示注入攻击。该模型利用了改进的SOTA SecAlign防御，并提供了详细的训练配方。通过在多个效用和安全基准上的评估，Meta SecAlign展现出对提示注入攻击的强大鲁棒性，并在未见任务中保持通用指令遵循能力，其性能可与闭源商业LLM媲美，旨在推动AI安全领域的开放研究。", "keywords": "提示注入攻击, LLM, 模型级防御, 开源, SecAlign", "comments": "Meta SecAlign的创新之处在于它是首个开源且开放权重的LLM，内置了商业级模型防御，从而解决了当前模型级防御闭源部署的问题。这对于AI安全社区的开放研究和共同进步至关重要，有助于加速对抗提示注入攻击的防御机制发展。其在未见任务上的安全性表现也凸显了其通用性和实用价值。"}}
{"id": "2507.02600", "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects", "authors": ["Qiaojun Yu", "Xibin Yuan", "Yu jiang", "Junting Chen", "Dongzhe Zheng", "Ce Hao", "Yang You", "Yixing Chen", "Yao Mu", "Liu Liu", "Cewu Lu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2507.02600v1", "summary": "Articulated object manipulation remains a critical challenge in robotics due\nto the complex kinematic constraints and the limited physical reasoning of\nexisting methods. In this work, we introduce ArtGS, a novel framework that\nextends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling\nfor articulated object understanding and interaction. ArtGS begins with\nmulti-view RGB-D reconstruction, followed by reasoning with a vision-language\nmodel (VLM) to extract semantic and structural information, particularly the\narticulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS\noptimizes the parameters of the articulated bones, ensuring physically\nconsistent motion constraints and enhancing the manipulation policy. By\nleveraging dynamic Gaussian splatting, cross-embodiment adaptability, and\nclosed-loop optimization, ArtGS establishes a new framework for efficient,\nscalable, and generalizable articulated object modeling and manipulation.\nExperiments conducted in both simulation and real-world environments\ndemonstrate that ArtGS significantly outperforms previous methods in joint\nestimation accuracy and manipulation success rates across a variety of\narticulated objects. Additional images and videos are available on the project\nwebsite: https://sites.google.com/view/artgs/home", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.02600v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ArtGS：用于铰接对象交互式视觉-物理建模和操作的3D高斯泼溅", "tldr": "ArtGS是一个新框架，通过将3D高斯泼溅与视觉-物理建模相结合，显著提高了铰接对象的关节估计和操作成功率。", "motivation": "铰接对象操作在机器人领域仍是一个关键挑战，因为其复杂的运动学约束和现有方法有限的物理推理能力。", "method": "ArtGS首先进行多视图RGB-D重建，然后利用视觉-语言模型（VLM）提取语义和结构信息（特别是铰接骨骼）。通过动态、可微分的基于3DGS的渲染，ArtGS优化铰接骨骼的参数，确保物理一致的运动约束并增强操作策略。它利用动态高斯泼溅、跨实体适应性和闭环优化。", "result": "在仿真和现实世界环境中进行的实验表明，ArtGS在各种铰接对象的关节估计精度和操作成功率方面显著优于以前的方法。", "conclusion": "ArtGS通过结合3D高斯泼溅与视觉-物理建模，成功地为高效、可扩展和通用化的铰接对象建模和操作建立了一个新框架，解决了现有方法的局限性。", "translation": "铰接对象操作在机器人领域仍然是一个严峻的挑战，这主要是由于复杂的运动学约束和现有方法有限的物理推理能力。在这项工作中，我们引入了ArtGS，这是一个新颖的框架，通过集成视觉-物理建模来扩展3D高斯泼溅（3DGS），以实现铰接对象的理解和交互。ArtGS首先进行多视图RGB-D重建，然后利用视觉-语言模型（VLM）进行推理，以提取语义和结构信息，特别是铰接骨骼。通过动态、可微分的基于3DGS的渲染，ArtGS优化了铰接骨骼的参数，确保了物理一致的运动约束并增强了操作策略。通过利用动态高斯泼溅、跨实体适应性和闭环优化，ArtGS为高效、可扩展和通用化的铰接对象建模和操作建立了一个新框架。在仿真和现实世界环境中进行的实验表明，ArtGS在各种铰接对象的关节估计精度和操作成功率方面显著优于以前的方法。更多图片和视频可在项目网站上获取：https://sites.google.com/view/artgs/home", "summary": "ArtGS是一个创新的框架，它将3D高斯泼溅（3DGS）与视觉-物理建模相结合，用于铰接对象的理解和操作。该方法通过多视图RGB-D重建和VLM提取骨骼信息，并利用动态、可微分的3DGS优化骨骼参数，确保物理一致性。实验证明，ArtGS在关节估计精度和操作成功率上均优于现有方法，为高效、可扩展和通用化的铰接对象操作提供了新范式。", "keywords": "3D高斯泼溅, 铰接对象, 机器人操作, 视觉-物理建模, 运动学约束", "comments": "ArtGS的创新在于将3D高斯泼溅技术与视觉-物理建模深度融合，解决了铰接对象复杂操作的难题。其利用VLM进行语义和结构推理，并结合动态可微分的3DGS优化物理约束，为机器人操作领域提供了一个通用且高效的解决方案。该方法在仿真和现实世界的优异表现，预示了其在未来机器人应用中的巨大潜力。"}}
{"id": "2507.02846", "title": "Legal Requirements Translation from Law", "authors": ["Anmol Singhal", "Travis Breaux"], "categories": ["cs.SE", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures, Accepted at the 33rd IEEE International Requirements Engineering 2025", "url": "http://arxiv.org/abs/2507.02846v1", "summary": "Software systems must comply with legal regulations, which is a\nresource-intensive task, particularly for small organizations and startups\nlacking dedicated legal expertise. Extracting metadata from regulations to\nelicit legal requirements for software is a critical step to ensure compliance.\nHowever, it is a cumbersome task due to the length and complex nature of legal\ntext. Although prior work has pursued automated methods for extracting\nstructural and semantic metadata from legal text, key limitations remain: they\ndo not consider the interplay and interrelationships among attributes\nassociated with these metadata types, and they rely on manual labeling or\nheuristic-driven machine learning, which does not generalize well to new\ndocuments. In this paper, we introduce an approach based on textual entailment\nand in-context learning for automatically generating a canonical representation\nof legal text, encodable and executable as Python code. Our representation is\ninstantiated from a manually designed Python class structure that serves as a\ndomain-specific metamodel, capturing both structural and semantic legal\nmetadata and their interrelationships. This design choice reduces the need for\nlarge, manually labeled datasets and enhances applicability to unseen\nlegislation. We evaluate our approach on 13 U.S. state data breach notification\nlaws, demonstrating that our generated representations pass approximately 89.4%\nof test cases and achieve a precision and recall of 82.2 and 88.7,\nrespectively.", "comment": "13 pages, 7 figures, Accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2507.02846v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "法律要求从法律文本中提取", "tldr": "本文提出了一种基于文本蕴含和上下文学习的方法，用于自动从法律文本中提取法律要求，并将其表示为可执行的Python代码，以帮助软件系统合规。", "motivation": "软件系统必须遵守法律法规，但这是一项资源密集型任务，特别是对于缺乏法律专业知识的小型组织和初创公司。从法规中提取元数据以获取软件的法律要求是确保合规的关键步骤，但由于法律文本的长度和复杂性，这项任务很繁琐。尽管现有工作已尝试自动化提取法律文本中的结构和语义元数据，但仍存在局限性：它们未考虑这些元数据类型相关属性之间的相互作用和相互关系，并且依赖于手动标注或启发式机器学习，这在处理新文档时泛化能力不佳。", "method": "本文引入了一种基于文本蕴含（textual entailment）和上下文学习（in-context learning）的方法，用于自动生成法律文本的规范表示，该表示可以编码并作为Python代码执行。这种表示通过手动设计的Python类结构实例化，该结构作为领域特定的元模型，捕获结构化和语义化的法律元数据及其相互关系。这种设计选择减少了对大型手动标注数据集的需求，并增强了对未见立法条文的适用性。", "result": "该方法在13项美国州数据泄露通知法上进行了评估，结果表明生成的表示通过了大约89.4%的测试用例，并分别达到了82.2的精确率和88.7的召回率。", "conclusion": "该方法能够有效地从复杂的法律文本中自动提取法律要求，提供了一种可泛化且高效的合规性保障方式，同时显著减少了对大量手动标注的需求。", "translation": "软件系统必须遵守法律法规，这是一项资源密集型任务，特别是对于缺乏专门法律专业知识的小型组织和初创公司。从法规中提取元数据以获取软件的法律要求是确保合规的关键一步。然而，由于法律文本的长度和复杂性，这是一项繁琐的任务。尽管现有工作已经尝试了从法律文本中提取结构和语义元数据的自动化方法，但仍存在关键局限性：它们没有考虑这些元数据类型相关属性之间的相互作用和相互关系，并且它们依赖于手动标注或启发式机器学习，这在处理新文档时泛化能力不佳。在本文中，我们引入了一种基于文本蕴含和上下文学习的方法，用于自动生成法律文本的规范表示，该表示可以编码并作为Python代码执行。我们的表示是从手动设计的Python类结构中实例化的，该结构作为领域特定的元模型，捕获了结构化和语义化的法律元数据及其相互关系。这种设计选择减少了对大型手动标注数据集的需求，并增强了对未见立法的适用性。我们在13项美国州数据泄露通知法上评估了我们的方法，结果表明我们生成的表示通过了大约89.4%的测试用例，并分别达到了82.2的精确率和88.7的召回率。", "summary": "本研究提出了一种创新方法，旨在解决软件系统法律合规中从复杂法律文本中提取法律要求的挑战。针对现有自动化方法在处理元数据关系和泛化性方面的不足，该方法结合了文本蕴含和上下文学习技术。它能够将法律文本自动转化为规范的Python代码表示，通过一个领域特定的Python类元模型来捕获法律文本的结构和语义信息及其相互关系。这种设计显著减少了对大量手动标注数据的依赖，并提高了对新法律文档的适用性。在针对13项美国州数据泄露通知法的评估中，该方法表现出色，测试用例通过率达到89.4%，精确率和召回率分别为82.2%和88.7%。", "keywords": "法律要求提取, 文本蕴含, 上下文学习, 软件合规, 法律文本分析", "comments": "本文的创新之处在于将文本蕴含和上下文学习与领域特定的Python类元模型相结合，实现了法律文本的自动规范化表示。这种方法有效地解决了传统法律要求提取中手动标注成本高、泛化能力差的问题，对于提升软件合规效率，尤其是帮助缺乏专业法律团队的小型组织具有重要意义。其将法律文本转化为可执行代码的思路也极具前瞻性。"}}
{"id": "2507.02537", "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue", "authors": ["Paulo Ricardo Knob", "Leonardo Scholler", "Juliano Rigatti", "Soraia Raupp Musse"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02537v1", "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02537v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "你在听我说话吗？微调聊天机器人以实现共情对话", "tldr": "研究微调LLM以生成共情对话，发现虽然结构上能模仿情感，但人类评估显示在感知到的共情和连贯性上仍有差异，强调需结合自动化和以人为中心的方法。", "motivation": "随着会话代理日益融入日常人类互动，对情商，特别是共情倾听的需求变得越来越重要。本研究旨在探索大型语言模型（LLMs）在被要求生成情感丰富的互动时的表现。", "method": "本研究从一个由专家手工制作的反映共情行为的小型数据集开始，使用ChatGPT和Gemini两个LLM扩展对话。通过情感分析（VADER）和专家评估来分析对话的情感进展。", "result": "生成的对话在情感结构上通常与预期一致，但人类评估揭示了在感知到的共情和回应的连贯性方面存在重要差异。", "conclusion": "对话中的情感建模不仅需要表达情感的结构对齐，还需要定性深度，这突出了在开发情感能力代理时结合自动化和以人为中心方法的重要性。", "translation": "会话代理自ELIZA以来取得了显著进展，其作用已扩展到医疗保健、教育和客户服务等各个领域。随着这些代理日益融入日常人类互动，对情商，特别是共情倾听的需求变得越来越重要。在本研究中，我们探讨了大型语言模型（LLMs）在被要求生成情感丰富的互动时如何响应。我们从一个由专家手工制作的反映共情行为的小型数据集开始，使用两个LLM：ChatGPT和Gemini扩展了对话。我们使用情感分析（通过VADER）和专家评估来分析对话的情感进展。虽然生成的对话通常反映了预期的情感结构，但人类评估揭示了在感知到的共情和回应的连贯性方面存在重要差异。这些发现表明，对话中的情感建模不仅需要表达情感的结构对齐，还需要定性深度，这突出了在开发情感能力代理时结合自动化和以人为中心方法的重要性。", "summary": "本研究探讨了如何微调大型语言模型（LLMs）以生成具有共情能力的对话。研究人员从一个专家创建的小型共情数据集出发，利用ChatGPT和Gemini扩展对话，并结合情感分析和人类专家评估来分析其情感进展。尽管LLMs能模仿情感结构，但人类评估发现其在感知共情和连贯性方面仍有不足。研究强调，开发情感智能代理需要结合自动化和以人为中心的方法，以实现更深层次的情感建模。", "keywords": "共情对话, 大型语言模型, 情感智能, 微调, 人类评估", "comments": "本文的创新点在于尝试通过微调LLMs来提升其共情对话能力，并结合了自动化分析和人类评估的双重方法，揭示了当前LLMs在情感“深度”而非仅仅“结构”上的局限性。其重要性在于指出了未来开发情感智能AI的方向，即不能仅仅依赖于数据驱动的结构模仿，更需要注重人类感知和质性评估。"}}
{"id": "2507.02541", "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context", "authors": ["Yanzhen Lu", "Hanbin Yang", "Xiaodie Wang", "Ge Zhang", "Biao Li", "Chenxu Fu", "Chao Li", "Yang Yuan", "Andrew Chi-Chih Yao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02541v1", "summary": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02541v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "推理前先澄清：一个带有结构化上下文的Coq证明器", "tldr": "通过向大型语言模型输入中添加结构化语义上下文，显著提高了Coq定理证明任务的清晰度和证明成功率，甚至超越了现有最佳方法。", "motivation": "研究旨在探究提高任务清晰度是否能增强大型语言模型在Coq定理证明中的推理能力。", "method": "引入概念层面的度量来评估任务清晰度；通过向标准LLM输入添加结构化语义上下文；使用选择性概念展开来丰富任务描述；采用Planner-Executor架构。", "result": "任务清晰度得分提高1.85倍（44.5%→82.3%）；使用DeepSeek-V3模型，证明成功率提高2.1倍（21.8%→45.8%），并优于Graph2Tac（33.2%）；在结构化数据上微调小型模型可实现更高性能（48.6%）。", "conclusion": "研究结果突出了结构化任务表示在弥合理解与推理之间差距的价值。", "translation": "在这项工作中，我们研究了提高任务清晰度是否能增强大型语言模型（LLMs）的推理能力，重点关注Coq中的定理证明。我们引入了一个概念层面的度量来评估任务清晰度，并表明向现代LLMs使用的标准输入中添加结构化语义上下文，可以使清晰度得分提高1.85倍（44.5%→82.3%）。使用通用模型DeepSeek-V3，我们的方法使证明成功率提高了2.1倍（21.8%→45.8%），并优于先前的最新技术Graph2Tac（33.2%）。我们根据Graph2Tac的相同评估协议，从15个标准Coq包中随机抽取1,386个定理进行了评估。此外，在我们的结构化数据上对小型模型进行微调可以实现更高的性能（48.6%）。我们的方法使用选择性概念展开来丰富任务描述，并采用Planner-Executor架构。这些发现强调了结构化任务表示在弥合理解与推理之间差距的价值。", "summary": "本研究探讨了通过增强任务清晰度来提升大型语言模型在Coq定理证明中的推理能力。通过引入概念层面的清晰度度量并向LLM输入添加结构化语义上下文，研究实现了清晰度得分1.85倍的提升。基于此，使用DeepSeek-V3模型，定理证明成功率提升了2.1倍，并超越了现有SOTA。即使是微调的小型模型也能获得更高性能。该方法利用选择性概念展开和Planner-Executor架构，强调了结构化任务表示对于连接理解与推理的重要性。", "keywords": "Coq定理证明, 大型语言模型, 任务清晰度, 结构化上下文, 语义推理", "comments": "这项工作通过引入“推理前先澄清”的概念，为提升LLM在复杂符号推理任务中的表现提供了新颖且有效的途径。其创新之处在于通过结构化语义上下文显著提高了任务清晰度，并量化了这种清晰度提升对证明成功率的积极影响。超越现有SOTA的成果以及在小模型上的表现，进一步证明了其方法的有效性和普适性。该研究对于未来构建更可靠、更高效的自动化推理系统具有重要意义。"}}
{"id": "2507.02288", "title": "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization", "authors": ["De Cheng", "Zhipeng Xu", "Xinyang Jiang", "Dongsheng Li", "Nannan Wang", "Xinbo Gao"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02288v1", "summary": "Domain Generalization (DG) seeks to develop a versatile model capable of\nperforming effectively on unseen target domains. Notably, recent advances in\npre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated\nconsiderable potential in enhancing the generalization capabilities of deep\nlearning models. Despite the increasing attention toward VFM-based domain\nprompt tuning within DG, the effective design of prompts capable of\ndisentangling invariant features across diverse domains remains a critical\nchallenge. In this paper, we propose addressing this challenge by leveraging\nthe controllable and flexible language prompt of the VFM. Noting that the text\nmodality of VFMs is naturally easier to disentangle, we introduce a novel\nframework for text feature-guided visual prompt tuning. This framework first\nautomatically disentangles the text prompt using a large language model (LLM)\nand then learns domain-invariant visual representation guided by the\ndisentangled text feature. However, relying solely on language to guide visual\nfeature disentanglement has limitations, as visual features can sometimes be\ntoo complex or nuanced to be fully captured by descriptive text. To address\nthis, we introduce Worst Explicit Representation Alignment (WERA), which\nextends text-guided visual prompts by incorporating an additional set of\nabstract prompts. These prompts enhance source domain diversity through\nstylized image augmentations, while alignment constraints ensure that visual\nrepresentations remain consistent across both the original and augmented\ndistributions. Experiments conducted on major DG datasets, including PACS,\nVLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method\noutperforms state-of-the-art DG methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02288v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过语言引导和表示对齐实现领域泛化的提示解耦", "tldr": "本文提出了一种新的领域泛化方法，通过语言引导的视觉提示调整和表示对齐来解耦不变特征，并在多个数据集上取得了最先进的性能。", "motivation": "尽管基于视觉基础模型（VFM）的领域提示调整在领域泛化（DG）中受到越来越多的关注，但有效设计能够解耦跨不同领域不变特征的提示仍然是一个关键挑战。", "method": "本文提出了一种文本特征引导的视觉提示调整框架。该框架首先使用大型语言模型（LLM）自动解耦文本提示，然后通过解耦的文本特征引导学习领域不变的视觉表示。为了解决仅依靠语言引导的局限性，引入了最差显式表示对齐（WERA），通过结合额外的抽象提示来增强源域多样性，并通过对齐约束确保视觉表示在原始和增强分布中保持一致。", "result": "在PACS、VLCS、OfficeHome、DomainNet和TerraInc等主要DG数据集上进行的实验表明，所提出的方法优于最先进的DG方法。", "conclusion": "通过结合语言引导和表示对齐，本文提出的方法有效地解决了领域泛化中提示解耦的挑战，并取得了卓越的性能。", "translation": "领域泛化（DG）旨在开发一种通用的模型，能够在未见过的目标领域有效执行。值得注意的是，预训练视觉基础模型（VFM），如CLIP，的最新进展在增强深度学习模型的泛化能力方面表现出相当大的潜力。尽管基于VFM的领域提示调整在DG中受到越来越多的关注，但有效设计能够解耦跨不同领域不变特征的提示仍然是一个关键挑战。在本文中，我们提出通过利用VFM的可控和灵活的语言提示来解决这一挑战。注意到VFM的文本模态自然更容易解耦，我们引入了一种新颖的文本特征引导的视觉提示调整框架。该框架首先使用大型语言模型（LLM）自动解耦文本提示，然后通过解耦的文本特征引导学习领域不变的视觉表示。然而，仅仅依靠语言来引导视觉特征解耦存在局限性，因为视觉特征有时可能过于复杂或细微，无法完全通过描述性文本捕获。为了解决这个问题，我们引入了最差显式表示对齐（WERA），它通过结合一组额外的抽象提示来扩展文本引导的视觉提示。这些提示通过风格化图像增强来增强源领域多样性，同时对齐约束确保视觉表示在原始和增强分布中保持一致。在包括PACS、VLCS、OfficeHome、DomainNet和TerraInc在内的主要DG数据集上进行的实验表明，我们提出的方法优于最先进的DG方法。", "summary": "本文提出了一种名为“通过语言引导和表示对齐实现提示解耦”（WERA）的新框架，用于解决领域泛化中的挑战。该方法通过利用大型语言模型自动解耦文本提示，并用这些解耦的文本特征引导视觉提示调整，以学习领域不变的视觉表示。为了克服纯文本引导的局限性，WERA引入了抽象提示和风格化图像增强来提高源域多样性，并通过对齐约束确保视觉表示的一致性。实验证明，该方法在多个主流领域泛化数据集上优于现有最先进的方法。", "keywords": "领域泛化, 提示调整, 视觉基础模型, 语言引导, 表示对齐", "comments": "该论文的创新之处在于，它巧妙地利用了大型语言模型在文本模态上的解耦能力来指导视觉提示的调整，从而解决了领域泛化中不变特征提取的难题。此外，引入最差显式表示对齐（WERA）机制，通过结合抽象提示和图像增强来处理视觉特征的复杂性，进一步提升了模型的泛化能力，为VFM在DG领域的应用开辟了新途径。"}}
{"id": "2507.02129", "title": "Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction", "authors": ["Xiao Li", "Liangji Zhu", "Anand Rangarajan", "Sanjay Ranka"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.02129v1", "summary": "Generative models have demonstrated strong performance in conditional\nsettings and can be viewed as a form of data compression, where the condition\nserves as a compact representation. However, their limited controllability and\nreconstruction accuracy restrict their practical application to data\ncompression. In this work, we propose an efficient latent diffusion framework\nthat bridges this gap by combining a variational autoencoder with a conditional\ndiffusion model. Our method compresses only a small number of keyframes into\nlatent space and uses them as conditioning inputs to reconstruct the remaining\nframes via generative interpolation, eliminating the need to store latent\nrepresentations for every frame. This approach enables accurate spatiotemporal\nreconstruction while significantly reducing storage costs. Experimental results\nacross multiple datasets show that our method achieves up to 10 times higher\ncompression ratios than rule-based state-of-the-art compressors such as SZ3,\nand up to 63 percent better performance than leading learning-based methods\nunder the same reconstruction error.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.02129v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "用于高效时空数据缩减的生成式潜在扩散", "tldr": "本文提出了一种高效的潜在扩散框架，结合变分自编码器和条件扩散模型，通过压缩少量关键帧并在潜在空间中进行生成式插值来重建剩余帧，从而实现时空数据的高效缩减和高精度重建。", "motivation": "生成模型在条件设置下表现出强大的性能，可被视为一种数据压缩形式，但其有限的可控性和重建精度限制了其在数据压缩中的实际应用。本文旨在弥补这一差距。", "method": "本文提出了一种高效的潜在扩散框架，该框架结合了变分自编码器（VAE）和条件扩散模型。该方法仅将少量关键帧压缩到潜在空间中，并将其作为条件输入，通过生成式插值重建剩余帧，从而无需为每一帧存储潜在表示。", "result": "实验结果表明，该方法比基于规则的现有最先进压缩器（如SZ3）实现了高达10倍的压缩比，并且在相同重建误差下，比领先的基于学习的方法性能提高高达63%。", "conclusion": "所提出的方法能够实现精确的时空数据重建，同时显著降低存储成本，并超越了现有方法。", "translation": "生成模型在条件设置下表现出强大的性能，可被视为一种数据压缩形式，其中条件作为一种紧凑的表示。然而，它们有限的可控性和重建精度限制了它们在数据压缩中的实际应用。在这项工作中，我们提出了一种高效的潜在扩散框架，通过将变分自编码器与条件扩散模型相结合来弥补这一差距。我们的方法仅将少量关键帧压缩到潜在空间中，并将其用作条件输入，通过生成式插值重建剩余帧，从而无需为每一帧存储潜在表示。这种方法能够实现精确的时空重建，同时显著降低存储成本。在多个数据集上的实验结果表明，我们的方法比基于规则的现有最先进压缩器（如SZ3）实现了高达10倍的压缩比，并且在相同重建误差下，比领先的基于学习的方法性能提高高达63%。", "summary": "本文提出了一种名为“生成式潜在扩散”的新型框架，旨在解决现有生成模型在数据压缩中可控性和重建精度不足的问题。该方法结合了变分自编码器和条件扩散模型，通过仅压缩少量关键帧到潜在空间，并利用这些关键帧作为条件输入，通过生成式插值来重建其余帧，从而避免了存储所有帧的潜在表示。实验证明，该方法在实现精确时空重建的同时，显著降低了存储成本，其压缩比比现有最佳规则基方法高出10倍，并且在相同重建误差下，比领先的学习基方法性能提升高达63%。", "keywords": "生成式潜在扩散, 时空数据缩减, 数据压缩, 变分自编码器, 条件扩散模型", "comments": "本文的创新之处在于巧妙地结合了变分自编码器和条件扩散模型，并引入了“关键帧”的概念，通过对少量关键帧进行潜在空间压缩和生成式插值来重建整个时空数据。这种方法有效地解决了传统生成模型在数据压缩中存在的控制性差和重建精度低的问题，同时实现了显著的压缩比提升和存储成本降低，具有重要的实际应用价值。"}}
{"id": "2507.02595", "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion", "authors": ["Xin Guan", "PeiHsin Lin", "Zekun Wu", "Ze Wang", "Ruibo Zhang", "Emre Kazim", "Adriano Koshiyama"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 AIW Workshop", "url": "http://arxiv.org/abs/2507.02595v1", "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.", "comment": "Accepted at ICML 2025 AIW Workshop", "pdf_url": "http://arxiv.org/pdf/2507.02595v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MPF：通过多视角融合在部署后对语言模型进行对齐和去偏", "tldr": "MPF是一个新颖的部署后对齐框架，通过多视角生成和分解人类基线，有效缓解LLM偏见，无需大量调优。", "motivation": "为了应对日益增长的易于偏见缓解的需求，并解决现有方法可能需要大量提示工程或微调的问题，本文提出了MPF框架。", "method": "MPF（多视角融合）是一种建立在SAGED管道之上的后训练对齐框架。SAGED是一个自动化系统，用于构建偏见基准和提取可解释的基线分布。MPF利用多视角生成来暴露和对齐LLM输出中的偏见，使其与细致入微、类人的基线保持一致。它通过将基线（例如，来自人力资源专业人员的情感分布）分解为可解释的视角组件，并根据分解中获得的概率对响应进行采样和平衡来指导生成。", "result": "经验证明，MPF能够将LLM情感分布与反事实基线（绝对平等）和HR基线（对顶尖大学有偏见）对齐。这导致了小的KL散度、校准误差的减少以及对未见问题的泛化能力。", "conclusion": "MPF提供了一种可扩展且可解释的对齐和偏见缓解方法，与已部署的LLM兼容，并且不需要大量的提示工程或微调。", "translation": "多视角融合（MPF）是一种新颖的后训练对齐框架，针对大型语言模型（LLMs）开发，以应对日益增长的易于偏见缓解的需求。MPF建立在SAGED管道之上，这是一个用于构建偏见基准和提取可解释基线分布的自动化系统。MPF利用多视角生成来暴露和对齐LLM输出中的偏见，使其与细致入微、类人的基线保持一致。通过将基线（例如，来自人力资源专业人员的情感分布）分解为可解释的视角组件，MPF通过对响应进行采样和平衡来指导生成，这些响应的权重由分解中获得的概率决定。经验上，我们证明了其能够将LLM情感分布与反事实基线（绝对平等）和HR基线（对顶尖大学有偏见）对齐，从而导致小的KL散度、校准误差的减少以及对未见问题的泛化。这表明MPF提供了一种可扩展且可解释的对齐和偏见缓解方法，与已部署的LLM兼容，并且不需要大量的提示工程或微调。", "summary": "MPF是一种新颖的部署后框架，旨在通过多视角生成和人类基线分解来对齐和缓解大型语言模型中的偏见。它基于SAGED管道，能够将LLM输出与细致的、人类般的基线对齐，并通过采样和平衡响应来指导生成。实验结果表明，MPF有效降低了KL散度和校准误差，并能泛化到新问题，提供了一种可扩展且无需大量提示工程或微调的偏见缓解方案。", "keywords": "多视角融合, 语言模型对齐, 偏见缓解, 部署后, SAGED", "comments": "MPF的创新之处在于其“多视角融合”方法，它将人类基线分解为可解释的组件，并利用这些组件指导LLM的生成，以实现偏见对齐。其重要性在于提供了一种部署后、无需大量资源（如提示工程或微调）即可对LLM进行偏见缓解的实用方法，这对于LLM的广泛应用具有重要意义。"}}
{"id": "2507.02737", "title": "Early Signs of Steganographic Capabilities in Frontier LLMs", "authors": ["Artur Zolkowski", "Kei Nishimura-Gasparian", "Robert McCarthy", "Roland S. Zimmermann", "David Lindner"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02737v1", "summary": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02737v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "前沿大型语言模型隐写能力早期迹象", "tldr": "研究发现当前LLMs具有初步的隐写能力，在额外辅助下能编码信息或进行隐式推理，但目前尚不足以规避精心设计的监控。", "motivation": "为了更好地理解大型语言模型（LLMs）可能带来的风险并减轻其滥用和错位带来的风险，研究LLMs是否能通过隐写术（即在看似良性的生成内容中编码隐藏信息）来规避监控。", "method": "评估了前沿LLMs的隐写能力，重点关注两种类型：传递编码消息和执行编码推理。研究了在标准条件下以及提供额外辅助（如使用未受监控的草稿板和协调编码方案）时的表现。", "result": "发现当前模型在标准条件下无法在不被监控发现的情况下编码短消息。然而，如果在提供额外辅助（如使用未受监控的草稿板和协调编码方案）时，它们可以成功。此外，发现模型在简单的状态跟踪问题中表现出执行基本编码推理的早期迹象，包括使用自身和预定义方案（如十六进制编码）进行推理的能力。尽管如此，它们很少能巧妙地将推理隐藏在封面任务中以欺骗监控者。", "conclusion": "总体而言，研究结果表明当前LLMs展现出初期的隐写能力。虽然这些能力目前可能不足以绕过精心设计的监控器，但未来可能会发生变化。", "translation": "监控大型语言模型（LLM）的输出对于减轻滥用和错位带来的风险至关重要。然而，LLMs可能通过隐写术来规避监控：在看似良性的生成内容中编码隐藏信息。本文评估了前沿LLMs的隐写能力，以更好地理解它们带来的风险。我们重点关注两种类型的隐写术：传递编码消息和执行编码推理。我们发现当前模型在标准条件下无法在不被监控发现的情况下在输出中编码短消息。然而，如果在提供额外辅助（例如使用未受监控的草稿板和协调编码方案）时，它们可以成功。我们还发现模型在简单的状态跟踪问题中表现出执行基本编码推理的早期迹象。这包括使用自身和预定义方案（包括十六进制等编码方案）进行推理的能力。尽管如此，它们很少能巧妙地将推理隐藏在封面任务中以欺骗监控者。总体而言，我们的结果表明当前LLMs展现出初期的隐写能力。虽然这些能力目前可能不足以绕过精心设计的监控器，但未来可能会发生变化。", "summary": "本研究评估了前沿大型语言模型（LLMs）的隐写能力，以应对其可能通过在输出中隐藏信息来规避监控的风险。研究聚焦于编码消息传递和编码推理两种类型。结果显示，在标准条件下LLMs难以不被发现地编码短消息，但在提供额外辅助（如草稿板或预设编码方案）时则能成功。此外，LLMs展现出初步的编码推理能力，尽管目前尚不能巧妙地隐藏推理。论文总结当前LLMs的隐写能力处于早期阶段，虽不足以规避现有监控，但未来可能发展。", "keywords": "大型语言模型, 隐写术, 滥用风险, 编码推理, 监控", "comments": "这项研究具有重要的前瞻性，它揭示了大型语言模型潜在的滥用风险，即通过隐写术规避内容审查和监控。其创新点在于首次系统性地评估了LLMs的隐写能力，并区分了编码消息和编码推理两种类型。研究结果为未来设计更鲁棒的LLM监控系统提供了宝贵的数据支持，并提醒业界和研究人员关注LLM技术发展可能带来的新型安全挑战。"}}
{"id": "2507.02672", "title": "MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping", "authors": ["Qingyu Fan", "Yinghao Cai", "Chao Li", "Chunting Jiao", "Xudong Zheng", "Tao Lu", "Bin Liang", "Shuo Wang"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025", "url": "http://arxiv.org/abs/2507.02672v1", "summary": "Robotic grasping faces challenges in adapting to objects with varying shapes\nand sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method\nthat integrates multi-scale feature extraction with contrastive feature\nenhancement for self-adaptive grasping. We propose a query-based interaction\nbetween high-level and low-level features through the Insight Transformer,\nwhile the Empower Transformer selectively attends to the highest-level\nfeatures, which synergistically strikes a balance between focusing on fine\ngeometric details and overall geometric structures. Furthermore, MISCGrasp\nutilizes multi-scale contrastive learning to exploit similarities among\npositive grasp samples, ensuring consistency across multi-scale features.\nExtensive experiments in both simulated and real-world environments demonstrate\nthat MISCGrasp outperforms baseline and variant methods in tabletop\ndecluttering tasks. More details are available at https://miscgrasp.github.io/.", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2025", "pdf_url": "http://arxiv.org/pdf/2507.02672v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MISCGrasp：利用多尺度集成和对比学习增强体积抓取", "tldr": "MISCGrasp是一种新的体积抓取方法，结合了多尺度特征提取和对比学习，以提高机器人抓取不同形状和大小物体的能力，并在模拟和真实环境中表现优于现有方法。", "motivation": "机器人抓取面临难以适应不同形状和大小物体的挑战。", "method": "本文引入了MISCGrasp，这是一种体积抓取方法，它整合了多尺度特征提取和对比特征增强，以实现自适应抓取。该方法通过Insight Transformer在高级和低级特征之间进行基于查询的交互，同时Empower Transformer选择性地关注最高级特征，这协同地在关注精细几何细节和整体几何结构之间取得了平衡。此外，MISCGrasp利用多尺度对比学习来利用正抓取样本之间的相似性，确保多尺度特征之间的一致性。", "result": "在模拟和真实环境中的大量实验表明，MISCGrasp在桌面整理任务中优于基线和变体方法。", "conclusion": "MISCGrasp通过结合多尺度特征提取和对比学习，显著提升了机器人对不同形状和大小物体的体积抓取能力。", "translation": "机器人抓取在适应不同形状和大小的物体方面面临挑战。在本文中，我们引入了MISCGrasp，这是一种体积抓取方法，它整合了多尺度特征提取和对比特征增强，以实现自适应抓取。我们通过Insight Transformer在高级和低级特征之间提出了基于查询的交互，同时Empower Transformer选择性地关注最高级特征，这协同地在关注精细几何细节和整体几何结构之间取得了平衡。此外，MISCGrasp利用多尺度对比学习来利用正抓取样本之间的相似性，确保多尺度特征之间的一致性。在模拟和真实环境中的大量实验表明，MISCGrasp在桌面整理任务中优于基线和变体方法。更多细节可在https://miscgrasp.github.io/上获得。", "summary": "MISCGrasp是一种新颖的体积抓取方法，旨在解决机器人抓取不同形状和大小物体的挑战。它通过Insight Transformer实现高低级特征的查询式交互，并利用Empower Transformer平衡细节与整体结构。此外，MISCGrasp采用多尺度对比学习来增强特征一致性。实验证明，该方法在模拟和真实环境中的桌面整理任务中优于现有基线方法。", "keywords": "体积抓取, 多尺度特征, 对比学习, 机器人抓取, 深度学习", "comments": "MISCGrasp的创新之处在于其结合了多尺度特征提取、对比学习以及独特的双Transformer（Insight Transformer和Empower Transformer）架构，有效地处理了机器人抓取中物体形状和尺寸多样性的挑战。其通过平衡精细细节和整体结构来增强抓取适应性，并通过对比学习确保特征一致性，这对于提高抓取鲁棒性至关重要。该方法在模拟和真实世界中的出色表现凸显了其在实际应用中的潜力。"}}
{"id": "2507.02858", "title": "Requirements Elicitation Follow-Up Question Generation", "authors": ["Yuchen Shen", "Anmol Singhal", "Travis Breaux"], "categories": ["cs.SE", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures, accepted at the 33rd IEEE International Requirements Engineering 2025", "url": "http://arxiv.org/abs/2507.02858v1", "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.", "comment": "13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2507.02858v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "需求获取后续问题生成", "tldr": "本研究探讨使用大型语言模型（LLMs），特别是GPT-4o，辅助需求获取访谈中后续问题的生成，结果显示LLM生成的问题在清晰度、相关性和信息量方面不逊于人工生成的问题，并且在有指导的情况下表现更优。", "motivation": "需求获取访谈中，面试官常面临领域不熟悉、认知负荷过重、信息过载等挑战，难以实时生成恰当的访谈问题，影响访谈效率和质量。", "method": "本研究调查了GPT-4o在需求获取过程中生成后续访谈问题的应用，其方法建立在常见的面试官错误类型框架之上。研究还描述了根据受访者发言生成问题的方法。通过两次受控实验进行评估：第一次对比LLM生成问题与人工生成问题在最少指导下的表现；第二次评估LLM生成问题在面试官错误类型指导下的表现。", "result": "在两次实验中，LLM生成的问题在清晰度、相关性和信息量方面不逊于人工生成的问题。此外，当有常见错误类型指导时，LLM生成的问题表现优于人工生成的问题。", "conclusion": "大型语言模型（LLMs）有潜力帮助面试官实时提高需求获取访谈的质量和便利性。", "translation": "访谈是获取软件系统涉众需求、偏好和期望的常用技术。有效的访谈需要熟练的面试官在实时面对多种挑战（包括不熟悉领域、过度的认知负荷和信息过载阻碍人类处理涉众讲话）时，制定恰当的访谈问题。最近，大型语言模型（LLMs）在多项自然语言处理任务中表现出最先进的性能，包括文本摘要和蕴含。为了支持面试官，我们研究了GPT-4o在需求获取过程中生成后续访谈问题的应用，该方法建立在常见的面试官错误类型框架之上。此外，我们描述了根据受访者讲话生成问题的方法。我们报告了一项受控实验，以评估在最少指导下LLM生成和人工编写的问题，以及第二项受控实验，以评估在面试官错误类型指导下LLM生成的问题。我们的发现表明，在两次实验中，LLM生成的问题在清晰度、相关性和信息量方面不逊于人工编写的问题。此外，当有常见错误类型指导时，LLM生成的问题表现优于人工编写的问题。这突出了使用LLMs帮助面试官实时提高需求获取访谈质量和便利性的潜力。", "summary": "本研究旨在解决需求获取访谈中面试官面临的挑战，通过应用GPT-4o来生成后续访谈问题。研究基于常见的面试官错误类型框架，并开发了根据受访者发言生成问题的方法。通过两项受控实验，结果表明大型语言模型（LLMs）生成的问题在清晰度、相关性和信息量方面不逊于人工生成的问题，并且在有特定指导（如错误类型）时表现更优。这表明LLMs在提高需求获取访谈效率和质量方面具有巨大潜力。", "keywords": "需求获取, 大型语言模型, GPT-4o, 后续问题生成, 访谈辅助", "comments": "本文的创新点在于将大型语言模型应用于需求获取这一特定且复杂的领域，解决了面试官在实时访谈中生成高质量后续问题的痛点。其重要性体现在展示了LLMs在专业领域辅助人类工作的潜力，特别是在认知负荷高、实时性要求强的场景。通过引入“面试官错误类型”来指导LLM生成问题，更是提升了生成内容的针对性和有效性，突破了LLM仅凭语境生成通用问题的局限性。未来研究可以进一步探索个性化指导和多模态信息的结合。"}}
{"id": "2507.02554", "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench", "authors": ["Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Rishi Hazra", "Nicolas Baldwin", "Alexis Audran-Reiss", "Michael Kuchnik", "Despoina Magka", "Minqi Jiang", "Alisia Maria Lupidi", "Andrei Lupu", "Roberta Raileanu", "Kelvin Niu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Michael Shvartsman", "Shagun Sodhani", "Alexander H. Miller", "Abhishek Charnalia", "Derek Dunfield", "Carole-Jean Wu", "Pontus Stenetorp", "Nicola Cancedda", "Jakob Nicolaus Foerster", "Yoram Bachrach"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Code: this https URL", "url": "http://arxiv.org/abs/2507.02554v1", "summary": "AI research agents are demonstrating great potential to accelerate scientific\nprogress by automating the design, implementation, and training of machine\nlearning models. We focus on methods for improving agents' performance on\nMLE-bench, a challenging benchmark where agents compete in Kaggle competitions\nto solve real-world machine learning problems. We formalize AI research agents\nas search policies that navigate a space of candidate solutions, iteratively\nmodifying them using operators. By designing and systematically varying\ndifferent operator sets and search policies (Greedy, MCTS, Evolutionary), we\nshow that their interplay is critical for achieving high performance. Our best\npairing of search strategy and operator set achieves a state-of-the-art result\non MLE-bench lite, increasing the success rate of achieving a Kaggle medal from\n39.6% to 47.7%. Our investigation underscores the importance of jointly\nconsidering the search strategy, operator design, and evaluation methodology in\nadvancing automated machine learning.", "comment": "Code: https://github.com/facebookresearch/aira-dojo", "pdf_url": "http://arxiv.org/pdf/2507.02554v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "机器学习AI研究代理：MLE-bench中的搜索、探索与泛化", "tldr": "该研究通过设计和测试不同的操作符集与搜索策略（如贪婪、MCTS、进化算法），提升了AI研究代理在MLE-bench基准测试上的性能，并在MLE-bench lite上取得了SOTA结果，强调了搜索策略和操作符设计在自动化机器学习中的重要性。", "motivation": "AI研究代理在自动化机器学习模型的设计、实现和训练方面展现出巨大潜力，能够加速科学进步。本研究旨在通过改进这些代理在MLE-bench（一个挑战性的Kaggle竞赛基准）上的性能来进一步发挥其潜力。", "method": "研究将AI研究代理形式化为在候选解决方案空间中导航的搜索策略，并使用操作符迭代地修改这些解决方案。通过系统地设计和改变不同的操作符集和搜索策略（包括贪婪、蒙特卡洛树搜索MCTS和进化算法），来分析它们对性能的影响。", "result": "研究发现，搜索策略和操作符集之间的相互作用对于实现高性能至关重要。最佳的搜索策略与操作符集组合在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。", "conclusion": "本研究强调了在推进自动化机器学习时，联合考虑搜索策略、操作符设计和评估方法的重要性。", "translation": "AI研究代理正通过自动化机器学习模型的设计、实现和训练，展现出加速科学进步的巨大潜力。我们专注于提高代理在MLE-bench上的性能，MLE-bench是一个具有挑战性的基准测试，代理在此基准中参与Kaggle竞赛以解决现实世界的机器学习问题。我们将AI研究代理形式化为搜索策略，这些策略在候选解决方案空间中导航，并使用操作符迭代地修改它们。通过设计和系统地改变不同的操作符集和搜索策略（贪婪、MCTS、进化），我们表明它们的相互作用对于实现高性能至关重要。我们最佳的搜索策略和操作符集配对在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。我们的研究强调了在推进自动化机器学习时，联合考虑搜索策略、操作符设计和评估方法的重要性。", "summary": "本研究探讨了AI研究代理在自动化机器学习中的应用，旨在提高其在MLE-bench基准测试上的性能。研究将代理建模为搜索策略，并通过系统地测试不同的操作符集和搜索策略（如贪婪、MCTS、进化算法），发现它们之间的协同作用对性能至关重要。最终，最佳组合在MLE-bench lite上取得了SOTA成果，显著提升了Kaggle奖牌的成功率，强调了搜索策略、操作符设计和评估方法在自动化机器学习进展中的联合考量。", "keywords": "AI研究代理, 机器学习, AutoML, 搜索策略, MLE-bench", "comments": "该论文的创新点在于将AI研究代理形式化为搜索策略，并通过系统性地探索不同操作符集和搜索策略的组合来优化其性能。其重要性在于为自动化机器学习（AutoML）的进步提供了新的视角和方法，特别是通过在现实世界的Kaggle竞赛环境中验证其有效性。尽管取得了显著的性能提升，但论文未详细阐述所使用的具体操作符类型和其设计细节，这可能限制了结果的可复现性和进一步的创新。"}}
{"id": "2507.02294", "title": "ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation", "authors": ["Hanbo Bi", "Yulong Xu", "Ya Li", "Yongqiang Mao", "Boyuan Tong", "Chongyang Li", "Chunbo Lang", "Wenhui Diao", "Hongqi Wang", "Yingchao Feng", "Xian Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02294v1", "summary": "The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits\nstrong generalization in generic segmentation tasks. However, applying SAM to\nremote sensing (RS) images still faces two major challenges. First, manually\nconstructing precise prompts for each image (e.g., points or boxes) is\nlabor-intensive and inefficient, especially in RS scenarios with dense small\nobjects or spatially fragmented distributions. Second, SAM lacks domain\nadaptability, as it is pre-trained primarily on natural images and struggles to\ncapture RS-specific semantics and spatial characteristics, especially when\nsegmenting novel or unseen classes. To address these issues, inspired by\nfew-shot learning, we propose ViRefSAM, a novel framework that guides SAM\nutilizing only a few annotated reference images that contain class-specific\nobjects. Without requiring manual prompts, ViRefSAM enables automatic\nsegmentation of class-consistent objects across RS images. Specifically,\nViRefSAM introduces two key components while keeping SAM's original\narchitecture intact: (1) a Visual Contextual Prompt Encoder that extracts\nclass-specific semantic clues from reference images and generates object-aware\nprompts via contextual interaction with target images; and (2) a Dynamic Target\nAlignment Adapter, integrated into SAM's image encoder, which mitigates the\ndomain gap by injecting class-specific semantics into target image features,\nenabling SAM to dynamically focus on task-relevant regions. Extensive\nexperiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,\nLoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and\nautomatic segmentation of unseen classes by leveraging only a few reference\nimages and consistently outperforms existing few-shot segmentation methods\nacross diverse datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02294v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ViRefSAM：遥感图像分割的视觉参考引导任意分割模型", "tldr": "SAM在遥感图像分割中面临手动提示低效和领域适应性差的问题。ViRefSAM提出了一种通过少量参考图像自动分割遥感图像中未知类别对象的方法，并在多个基准测试中表现优异。", "motivation": "1. 手动为每张遥感图像构建精确提示（如点或框）劳动密集且效率低下，尤其在密集小目标或空间零散分布的场景中。\n2. SAM主要在自然图像上预训练，缺乏对遥感图像特有语义和空间特征的领域适应性，难以分割新颖或未见类别。", "method": "ViRefSAM是一个受少样本学习启发的框架，通过少量带标注的参考图像引导SAM进行自动分割，无需手动提示。它在保持SAM原始架构不变的情况下，引入了两个关键组件：1. 视觉上下文提示编码器：从参考图像中提取类别特定语义线索，并通过与目标图像的上下文交互生成对象感知提示。2. 动态目标对齐适配器：集成到SAM的图像编码器中，通过注入类别特定语义到目标图像特征中来弥合领域差距，使SAM动态关注任务相关区域。", "result": "在iSAID-5$^i$、LoveDA-2$^i$和COCO-20$^i$三个少样本分割基准上的大量实验表明，ViRefSAM仅利用少量参考图像即可实现对未见类别的准确自动分割，并且在不同数据集上始终优于现有的少样本分割方法。", "conclusion": "ViRefSAM通过利用视觉参考引导和领域适应性机制，有效解决了SAM在遥感图像分割中面临的手动提示依赖和领域适应性不足的挑战，尤其在少样本未见类别分割方面表现出色。", "translation": "任意分割模型（SAM）凭借其提示驱动范式，在通用分割任务中展现出强大的泛化能力。然而，将SAM应用于遥感（RS）图像仍面临两大挑战。首先，为每张图像手动构建精确提示（例如，点或框）是劳动密集型且效率低下的，尤其是在具有密集小目标或空间零散分布的遥感场景中。其次，SAM缺乏领域适应性，因为它主要在自然图像上进行预训练，难以捕捉遥感特有的语义和空间特征，尤其是在分割新颖或未见类别时。为解决这些问题，受少样本学习的启发，我们提出了ViRefSAM，一个新颖的框架，它仅利用少量包含类别特定对象的标注参考图像来引导SAM。ViRefSAM无需手动提示，即可实现遥感图像中类别一致对象的自动分割。具体而言，ViRefSAM在保持SAM原始架构不变的同时引入了两个关键组件：（1）一个视觉上下文提示编码器，它从参考图像中提取类别特定语义线索，并通过与目标图像的上下文交互生成对象感知提示；（2）一个动态目标对齐适配器，集成到SAM的图像编码器中，通过将类别特定语义注入目标图像特征来弥合领域差距，使SAM能够动态地关注与任务相关的区域。在iSAID-5$^i$、LoveDA-2$^i$和COCO-20$^i$三个少样本分割基准上的大量实验表明，ViRefSAM仅利用少量参考图像即可实现对未见类别的准确自动分割，并且在不同数据集上始终优于现有的少样本分割方法。", "summary": "本论文提出了ViRefSAM，一个针对遥感图像分割的新颖框架，旨在解决Segment Anything Model (SAM) 在遥感领域面临的手动提示效率低下和领域适应性不足的问题。ViRefSAM受少样本学习启发，通过少量带标注的参考图像引导SAM进行自动分割，无需手动提示。它引入了视觉上下文提示编码器和动态目标对齐适配器，分别用于提取类别语义并弥合领域差距。实验证明，ViRefSAM在多个少样本分割基准上实现了对未见类别的准确自动分割，并优于现有方法。", "keywords": "遥感图像分割, 任意分割模型 (SAM), 少样本学习, 视觉参考, 领域适应性", "comments": "该论文的创新点在于将少样本学习的思想引入SAM，以解决其在遥感图像分割中的实际应用挑战。通过引入视觉参考引导机制，ViRefSAM显著降低了对人工标注提示的依赖，并提升了SAM对遥感领域特定语义的捕获能力，尤其对于未见类别的分割表现突出。这对于推动SAM在遥感领域的落地具有重要意义。"}}
{"id": "2507.02151", "title": "Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks", "authors": ["Tuo Wang", "Jian Kang", "Yujun Yan", "Adithya Kulkarni", "Dawei Zhou"], "categories": ["cs.LG", "H.1.0; I.2.0"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      accepted by KDD 2025", "url": "http://arxiv.org/abs/2507.02151v1", "summary": "Conformal prediction for graph neural networks (GNNs) offers a promising\nframework for quantifying uncertainty, enhancing GNN reliability in high-stakes\napplications. However, existing methods predominantly focus on static graphs,\nneglecting the evolving nature of real-world graphs. Temporal dependencies in\ngraph structure, node attributes, and ground truth labels violate the\nfundamental exchangeability assumption of standard conformal prediction\nmethods, limiting their applicability. To address these challenges, in this\npaper, we introduce NCPNET, a novel end-to-end conformal prediction framework\ntailored for temporal graphs. Our approach extends conformal prediction to\ndynamic settings, mitigating statistical coverage violations induced by\ntemporal dependencies. To achieve this, we propose a diffusion-based\nnon-conformity score that captures both topological and temporal uncertainties\nwithin evolving networks. Additionally, we develop an efficiency-aware\noptimization algorithm that improves the conformal prediction process,\nenhancing computational efficiency and reducing coverage violations. Extensive\nexperiments on diverse real-world temporal graphs, including WIKI, REDDIT,\nDBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to\nensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction\nin prediction set size on the WIKI dataset, significantly improving efficiency\ncompared to state-of-the-art methods. Our data and code are available at\nhttps://github.com/ODYSSEYWT/NCPNET.", "comment": "accepted by KDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.02151v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "非可交换时序图神经网络的共形预测", "tldr": "NCPNET是一种为时序图设计的共形预测框架，它通过处理非可交换性来量化不确定性，确保覆盖率并提高效率。", "motivation": "现有用于图神经网络（GNNs）的共形预测方法主要关注静态图，忽略了真实世界图的动态演变特性。图结构、节点属性和真实标签中的时间依赖性违反了标准共形预测方法的基本可交换性假设，限制了它们的适用性，并导致统计覆盖率违规。", "method": "本文引入了NCPNET，一个专为时序图设计的全新端到端共形预测框架。该方法通过提出一种基于扩散的非一致性分数来捕获演化网络中的拓扑和时间不确定性。此外，它还开发了一种效率感知优化算法，以提高计算效率并减少覆盖率违规，从而将共形预测扩展到动态设置。", "result": "在包括WIKI、REDDIT、DBLP和IBM反洗钱数据集在内的多样化真实世界时序图上进行的广泛实验表明，NCPNET能够确保时序图中的覆盖率，并在WIKI数据集上实现了预测集大小高达31%的减少，与现有最先进方法相比显著提高了效率。", "conclusion": "NCPNET有效地将共形预测扩展到动态设置，解决了GNN中时间依赖性和非可交换性带来的挑战，从而实现了可靠的不确定性量化和更高的效率。", "translation": "共形预测（Conformal Prediction）用于图神经网络（GNNs）为量化不确定性提供了一个有前景的框架，增强了GNN在关键应用中的可靠性。然而，现有方法主要关注静态图，忽略了真实世界图中不断演变的数据特性。图结构、节点属性和真实标签中的时间依赖性违反了标准共形预测方法的基本可交换性假设，限制了其适用性。为了解决这些挑战，本文引入了NCPNET，一个专为时序图设计的全新端到端共形预测框架。我们的方法将共形预测扩展到动态设置，减轻了由时间依赖性引起的统计覆盖率违规。为实现这一目标，我们提出了一种基于扩散的非一致性分数，该分数能捕获演化网络中的拓扑和时间不确定性。此外，我们开发了一种效率感知优化算法，改进了共形预测过程，提高了计算效率并减少了覆盖率违规。在WIKI、REDDIT、DBLP和IBM反洗钱数据集等多样化真实世界时序图上进行的广泛实验表明，NCPNET能够确保时序图中的覆盖率，在WIKI数据集上预测集大小最多减少31%，与现有最先进方法相比显著提高了效率。我们的数据和代码可在https://github.com/ODYSSEYWT/NCPNET获取。", "summary": "本文介绍了NCPNET，一种针对时序图神经网络的新型端到端共形预测框架。它解决了现有方法因假设静态图和可交换性而导致的局限性，这些假设在时间依赖性存在时被违反。NCPNET采用基于扩散的非一致性分数来捕获拓扑和时间不确定性，并结合效率感知优化算法。实验表明，NCPNET能确保覆盖率，并显著提高效率，在真实世界时序图上减少预测集大小。", "keywords": "时序图神经网络, 共形预测, 不确定性量化, 非可交换性, 动态图", "comments": "本文通过专门解决时间依赖性和非可交换性这一挑战性问题，在GNN的共形预测领域取得了重大进展。引入基于扩散的非一致性分数和效率感知优化算法是创新性贡献，使得共形预测适用于动态图设置并具有实用性。在覆盖率和效率方面所展示的改进对于现实世界中的高风险应用至关重要。"}}
{"id": "2507.02168", "title": "Experimental Multiport-Network Parameter Estimation and Optimization for Multi-Bit RIS", "authors": ["Philipp del Hougne"], "categories": ["physics.app-ph", "eess.SP"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "Comments:      5 pages including 2 figures", "url": "http://arxiv.org/abs/2507.02168v1", "summary": "Physics-consistent theoretical studies on RIS-parametrized wireless channels\nuse models from multiport-network theory (MNT) to capture mutual-coupling (MC)\neffects. However, in practice, RIS design and radio environment are partially\nor completely unknown. We fill a research gap on how to estimate the MNT model\nparameters in such experimentally relevant scenarios. Our technique efficiently\ncombines closed-form and gradient-descent steps, and it can be applied to\nmulti-bit-programmable RIS elements. We discuss inevitable (but operationally\nirrelevant) parameter ambiguities. We experimentally validate our technique in\nan unknown rich-scattering environment parametrized by eight 8-bit-programmable\nRIS elements of unknown design. We experimentally evaluate the performance of\nRIS configurations optimized with the estimated MNT model and an MC-unaware\ncascaded model. While the models differ in accuracy by up to 17 dB, the\nend-to-end performance differences are small.", "comment": "5 pages including 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.02168v1", "cate": "physics.app-ph", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "多比特RIS的实验性多端口网络参数估计与优化", "tldr": "本文提出了一种结合闭式解和梯度下降的有效技术，用于在未知环境中估计多比特RIS的多端口网络模型参数，并在实验中验证了其有效性，发现尽管模型精度有差异，但端到端性能差异很小。", "motivation": "现有的关于RIS参数化无线信道的物理一致性理论研究使用多端口网络理论（MNT）模型来捕捉互耦（MC）效应。然而，在实际应用中，RIS设计和无线电环境通常是部分或完全未知的。本文旨在解决在这种实验相关场景下如何估计MNT模型参数的研究空白。", "method": "本文提出了一种高效的技术，结合了闭式解和梯度下降步骤，可应用于多比特可编程RIS元件。该技术用于估计未知设计和环境下的多端口网络模型参数。研究还讨论了不可避免（但操作上无关紧要）的参数模糊性。", "result": "本文在由八个8位可编程RIS元件组成的未知富散射环境中对所提出的技术进行了实验验证。实验评估了使用估计的MNT模型和MC-不敏感的级联模型优化的RIS配置的性能。结果显示，虽然模型在精度上可能相差高达17 dB，但最终的端到端性能差异很小。", "conclusion": "尽管多端口网络（MNT）模型和互耦（MC）不敏感的级联模型在精度上存在显著差异（高达17 dB），但在实际的端到端性能方面，它们之间的差异却很小。这表明在某些应用中，即使模型精度有差异，也可能不会对最终系统性能产生重大影响。", "translation": "物理一致的关于RIS参数化无线信道的理论研究使用多端口网络理论（MNT）模型来捕捉互耦（MC）效应。然而，在实践中，RIS设计和无线电环境是部分或完全未知的。我们填补了在这些与实验相关的场景中如何估计MNT模型参数的研究空白。我们的技术有效地结合了闭式解和梯度下降步骤，并且可以应用于多比特可编程RIS元件。我们讨论了不可避免（但操作上无关紧要）的参数模糊性。我们在一个由八个8位可编程RIS元件组成的未知设计和未知富散射环境中实验验证了我们的技术。我们实验评估了使用估计的MNT模型和MC-不敏感的级联模型优化的RIS配置的性能。虽然模型在精度上相差高达17 dB，但端到端性能差异很小。", "summary": "本文针对实际应用中RIS设计和环境未知的问题，提出了一种有效估计多比特RIS多端口网络（MNT）模型参数的方法。该方法结合了闭式解和梯度下降，并在未知富散射环境中进行了实验验证。研究发现，尽管MNT模型与互耦（MC）不敏感的级联模型在精度上存在显著差异，但它们在端到端性能上的差异却很小。", "keywords": "RIS, 多端口网络, 参数估计, 互耦, 梯度下降", "comments": "本文的创新点在于提出了在实际未知环境中估计RIS多端口网络模型参数的实用方法，结合了高效的闭式解和梯度下降。其重要性在于弥补了理论研究与实际应用之间的差距，特别是在RIS设计和环境信息不完整的情况下。虽然模型精度差异显著，但最终的端到端性能差异很小这一发现，对于RIS的实际部署和系统设计具有重要的指导意义，可能意味着在某些场景下，对模型精度的极度追求并非总是必要的。"}}
{"id": "2507.02679", "title": "Exploring Gender Bias Beyond Occupational Titles", "authors": ["Ahmed Sabir", "Rajesh Sharama"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.02679v1", "summary": "In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.02679v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "探索超越职业头衔的性别偏见", "tldr": "本文研究了性别与语境偏见之间的关联，并引入了一个新数据集和框架来估计这些偏见，证实了职业刻板印象之外的性别偏见的存在。", "motivation": "本文旨在调查性别与语境偏见（特别是动作动词、宾语名词和职业）之间的相关性，并探索超越职业刻板印象的性别偏见。", "method": "引入了一个名为GenderLexicon的新数据集和一个可以估计语境偏见及其相关性别偏见的框架。该模型能够通过分数解释偏见，从而提高性别偏见的解释性。通过在包括一个日语数据集在内的五个不同数据集上进行评估来验证方法。", "result": "研究结果证实了职业刻板印象之外的性别偏见的存在。", "conclusion": "本文的发现证实了超越职业刻板印象的性别偏见的存在。", "translation": "在这项工作中，我们调查了性别与语境偏见之间的相关性，重点关注动作动词、宾语名词，尤其是职业等元素。我们引入了一个新颖的数据集GenderLexicon和一个可以估计语境偏见及其相关性别偏见的框架。我们的模型可以根据分数解释偏见，从而提高性别偏见的解释性。此外，我们的发现证实了超越职业刻板印象的性别偏见的存在。为了验证我们的方法并证明其有效性，我们在包括一个日语数据集在内的五个不同数据集上进行了评估。", "summary": "本文探讨了性别与语境偏见（包括动作动词、宾语名词和职业）之间的关系。研究引入了一个新数据集GenderLexicon和一个用于估计和解释性别偏见的框架。通过在多个数据集上的评估，研究证实了职业刻板印象之外的性别偏见普遍存在。", "keywords": "性别偏见, 语境偏见, GenderLexicon, 职业刻板印象, 偏见解释", "comments": "该研究的创新之处在于引入了新数据集GenderLexicon和用于量化及解释语境性别偏见的框架，并明确指出性别偏见不仅限于职业头衔。其重要性在于提供了一种识别和理解更广泛性别偏见的方法，有助于推动偏见检测和缓解的研究。"}}
{"id": "2507.02770", "title": "NVIDIA GPU Confidential Computing Demystified", "authors": ["Zhongshu Gu", "Enriquillo Valdez", "Salman Ahmed", "Julian James Stephen", "Michael Le", "Hani Jamjoom", "Shixuan Zhao", "Zhiqiang Lin"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02770v1", "summary": "GPU Confidential Computing (GPU-CC) was introduced as part of the NVIDIA\nHopper Architecture, extending the trust boundary beyond traditional CPU-based\nconfidential computing. This innovation enables GPUs to securely process AI\nworkloads, providing a robust and efficient solution for handling sensitive\ndata. For end users, transitioning to GPU-CC mode is seamless, requiring no\nmodifications to existing AI applications. However, this ease of adoption\ncontrasts sharply with the complexity of the underlying proprietary systems.\nThe lack of transparency presents significant challenges for security\nresearchers seeking a deeper understanding of GPU-CC's architecture and\noperational mechanisms.\n  The challenges of analyzing the NVIDIA GPU-CC system arise from a scarcity of\ndetailed specifications, the proprietary nature of the ecosystem, and the\ncomplexity of product design. In this paper, we aim to demystify the\nimplementation of NVIDIA GPU-CC system by piecing together the fragmented and\nincomplete information disclosed from various sources. Our investigation begins\nwith a high-level discussion of the threat model and security principles before\ndelving into the low-level details of each system component. We instrument the\nGPU kernel module -- the only open-source component of the system -- and\nconduct a series of experiments to identify the security weaknesses and\npotential exploits. For certain components that are out of reach through\nexperiments, we propose well-reasoned speculations about their inner working\nmechanisms. We have responsibly reported all security findings presented in\nthis paper to the NVIDIA PSIRT Team.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02770v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "NVIDIA GPU 机密计算揭秘", "tldr": "本文旨在揭秘 NVIDIA GPU 机密计算（GPU-CC）的实现机制，该系统因其专有性和缺乏透明度而难以理解。作者通过整合零散信息、分析威胁模型、检测开源 GPU 内核模块并进行实验，以识别其安全漏洞，并对无法实验的组件进行合理推测。", "motivation": "NVIDIA GPU 机密计算（GPU-CC）系统虽然能无缝支持现有 AI 应用，但其底层专有系统复杂且缺乏透明度，导致安全研究人员难以深入理解其架构和运行机制。本文旨在解决这种信息不透明性。", "method": "作者通过整合来自各种来源的零散不完整信息来揭示 NVIDIA GPU-CC 系统的实现。研究从高级别的威胁模型和安全原则讨论开始，然后深入探讨每个系统组件的低级细节。作者对系统中唯一的开源组件——GPU 内核模块进行了检测，并进行了一系列实验以识别安全弱点和潜在漏洞。对于无法通过实验触及的组件，作者提出了对其内部工作机制的合理推测。", "result": "本文识别了 NVIDIA GPU 机密计算系统中的安全弱点和潜在漏洞，并已负责任地向 NVIDIA PSIRT 团队报告了所有发现。", "conclusion": "本文成功揭示了 NVIDIA GPU 机密计算系统（GPU-CC）的部分实现细节和内部机制，识别了其中的安全弱点和潜在漏洞，尽管面临信息缺乏和系统专有性的挑战，但为深入理解该系统提供了宝贵见解。", "translation": "GPU 机密计算（GPU-CC）作为 NVIDIA Hopper 架构的一部分被引入，将信任边界扩展到传统的基于 CPU 的机密计算之外。这项创新使 GPU 能够安全地处理 AI 工作负载，为处理敏感数据提供了一个强大而高效的解决方案。对于终端用户而言，向 GPU-CC 模式的过渡是无缝的，无需修改现有的 AI 应用程序。然而，这种易于采用的特点与底层专有系统的复杂性形成鲜明对比。透明度的缺失给寻求深入了解 GPU-CC 架构和操作机制的安全研究人员带来了重大挑战。\n分析 NVIDIA GPU-CC 系统的挑战源于详细规范的稀缺、生态系统的专有性质以及产品设计的复杂性。在本文中，我们旨在通过整合从各种来源披露的零散不完整信息来揭示 NVIDIA GPU-CC 系统的实现。我们的调查从对威胁模型和安全原则的高级讨论开始，然后深入探讨每个系统组件的低级细节。我们对 GPU 内核模块——系统中唯一的开源组件——进行了检测，并进行了一系列实验以识别安全弱点和潜在漏洞。对于无法通过实验触及的某些组件，我们提出了对其内部工作机制的合理推测。我们已负责任地向 NVIDIA PSIRT 团队报告了本文中提出的所有安全发现。", "summary": "本文旨在揭示 NVIDIA 专有的 GPU 机密计算（GPU-CC）系统，该系统尽管能无缝支持 AI 工作负载，但因缺乏透明度和详细规范而难以理解。作者通过整合公开的零散信息、分析威胁模型、检测唯一的开源 GPU 内核模块并进行实验，以识别系统中的安全弱点和潜在漏洞，并对无法实验的组件进行合理推测，所有发现均已报告给 NVIDIA。", "keywords": "GPU 机密计算, NVIDIA Hopper, 安全分析, 专有系统, AI 工作负载", "comments": "本文的创新之处在于其在信息受限的专有系统中进行安全分析的方法。通过整合零散信息、逆向工程（对开源内核模块的检测）和合理推测，作者成功揭示了 GPU-CC 的部分内部机制和安全漏洞。这对于提升 GPU 机密计算的透明度和安全性至关重要，特别是考虑到其在敏感 AI 工作负载中的应用。论文也突出了在分析封闭源代码系统时，安全研究人员所面临的挑战。"}}
{"id": "2507.02700", "title": "Integrating path-planning and control for robotic unicycles", "authors": ["Máté B. Vizi", "Dénes Tákács", "Gábor Stépán", "Gábor Orosz"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02700v1", "summary": "This article focuses on integrating path-planning and control with\nspecializing on the unique needs of robotic unicycles. A unicycle design is\npresented which is capable of accelerating/breaking and carrying out a variety\nof maneuvers. The proposed path-planning method segments the path into straight\nand curved path sections dedicated for accelerating/breaking and turning\nmaneuvers, respectively. The curvature profiles of the curved sections are\noptimized while considering the control performance and the slipping limits of\nthe wheel. The performance of the proposed integrated approach is demonstrated\nvia numerical simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02700v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "机器人独轮车的路径规划与控制集成", "tldr": "本文为机器人独轮车整合了路径规划与控制，优化了弯曲路径段的曲率，并考虑了控制性能和车轮打滑限制，通过数值模拟验证了其性能。", "motivation": "为机器人独轮车集成路径规划和控制，以满足其独特的运动需求。", "method": "提出了一种独轮车设计，并将路径规划方法分为直线和曲线段。曲线段的曲率剖面在考虑控制性能和车轮打滑限制的情况下进行优化。", "result": "通过数值模拟验证了所提出的集成方法的性能。", "conclusion": "所提出的路径规划与控制集成方法在机器人独轮车上表现出良好的性能。", "translation": "本文专注于为机器人独轮车集成路径规划和控制，并特别关注其独特需求。文中提出了一种独轮车设计，该设计能够加速/制动并执行各种机动。所提出的路径规划方法将路径分为直线和曲线段，分别用于加速/制动和转弯机动。曲线段的曲率剖面在考虑控制性能和车轮打滑限制的情况下进行了优化。通过数值模拟验证了所提出的集成方法的性能。", "summary": "本文提出了一种为机器人独轮车设计的集成路径规划与控制方法。该方法将路径分为直线和曲线段，其中曲线段的曲率剖面经过优化，以考虑控制性能和车轮打滑限制。数值模拟验证了该集成方法的有效性。", "keywords": "机器人独轮车, 路径规划, 控制, 曲率优化, 打滑限制", "comments": "本文的创新点在于为机器人独轮车集成路径规划与控制，并特别优化了曲线段的曲率剖面，同时考虑了控制性能和车轮打滑限制，这对于独轮车的稳定性和机动性至关重要。"}}
{"id": "2507.02745", "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots", "authors": ["Zahra Ashktorab", "Alessandra Buccella", "Jason D'Cruz", "Zoe Fowler", "Andrew Gill", "Kei Yan Leung", "P. D. Magnus", "John Richards", "Kush R. Varshney"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02745v1", "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02745v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "谁在道歉：用户对LLM聊天机器人中机械式、共情式和解释性道歉的偏好", "tldr": "研究发现，用户普遍偏好LLM聊天机器人的解释性道歉，但在偏见情境下更喜欢共情式道歉，这表明AI系统道歉的复杂性。", "motivation": "随着大型语言模型（LLM）驱动的聊天机器人在日常生活中部署，它们通过有效道歉从错误中恢复的能力对于维护用户信任和满意度至关重要。", "method": "一项针对162名Proliﬁc工作者的预注册研究，通过配对实验，让参与者评估聊天机器人对三种常见LLM错误（偏见、无根据的虚构和事实错误）的回应，包括初始错误、随后的道歉和解决方案。研究考察了用户对三种道歉类型（机械式、解释性、共情式）的偏好。", "result": "解释性道歉普遍受到偏爱，但具体偏好因上下文和用户而异。在偏见情境中，共情式道歉因承认情感影响而受到青睐；而对于幻觉错误，尽管被视为严重，但用户没有明确偏好，反映了用户的不确定性。", "conclusion": "研究结果揭示了AI系统中有效道歉的复杂性。未来的系统必须在个性化和校准等关键方面进行探索，以有效地修复用户信任。", "translation": "随着大型语言模型（LLM）驱动的聊天机器人在日常生活中越来越多地部署，它们通过有效道歉从错误中恢复的能力对于维护用户信任和满意度至关重要。在一项针对162名Proliﬁc工作者的预注册研究中，我们考察了用户对三种道歉类型（机械式、解释性、共情式）的偏好，这些道歉是针对三类常见LLM错误（偏见、无根据的虚构和事实错误）发出的。我们设计了一个配对实验，参与者评估了聊天机器人的回应，包括初始错误、随后的道歉和解决方案。解释性道歉普遍受到偏爱，但具体偏好因上下文和用户而异。在偏见情境中，共情式道歉因承认情感影响而受到青睐，而幻觉错误，尽管被视为严重，但没有引起明确的偏好，这反映了用户的不确定性。我们的研究结果揭示了AI系统中有效道歉的复杂性。我们讨论了未来系统必须在个性化和校准等关键方面进行探索，以有效地修复信任。", "summary": "本研究探讨了用户对大型语言模型（LLM）聊天机器人不同类型道歉的偏好，以期在错误发生后维护用户信任。通过一项包含162名参与者的配对实验，研究对比了机械式、解释性和共情式道歉在处理偏见、虚构和事实错误时的效果。结果显示，解释性道歉通常更受欢迎，但在涉及情感影响的偏见情境中，共情式道歉表现更佳。研究强调了AI系统有效道歉的复杂性，并指出个性化和校准对于未来修复用户信任的重要性。", "keywords": "LLM聊天机器人, 道歉, 用户偏好, 信任修复, 错误恢复", "comments": "这篇论文深入探讨了LLM在处理错误时如何通过道歉来维护用户信任，这是一个非常实际且重要的研究方向。其创新之处在于细分了道歉类型和错误类型，并通过用户实验验证了不同场景下的偏好。研究结果强调了道歉策略的复杂性和情境依赖性，提示未来AI系统需要更精细化的个性化和校准机制来提升用户体验和信任修复能力。"}}
{"id": "2507.02582", "title": "Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms", "authors": ["Junli Jiang", "Pavel Naumov"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02582v1", "summary": "Responsibility has long been a subject of study in law and philosophy. More\nrecently, it became a focus of AI literature. The article investigates the\ncomputational complexity of two important properties of responsibility in\ncollective decision-making: diffusion and gap. It shows that the sets of\ndiffusion-free and gap-free decision-making mechanisms are $\\Pi_2$-complete and\n$\\Pi_3$-complete, respectively. At the same time, the intersection of these\nclasses is $\\Pi_2$-complete.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02582v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "序列决策机制中的责任差距与扩散", "tldr": "本文研究了集体决策中责任扩散和差距的计算复杂性。", "motivation": "责任在法律、哲学和人工智能领域都是重要的研究课题。本文旨在深入探讨集体决策中责任的扩散和差距这两种性质的计算复杂性。", "method": "文章通过分析责任的扩散和差距这两种性质的计算复杂性来进行研究。", "result": "研究表明，无扩散决策机制的集合是 $\\Pi_2$-完全的，无差距决策机制的集合是 $\\Pi_3$-完全的。同时，这些类别的交集是 $\\Pi_2$-完全的。", "conclusion": "本文确定了集体决策中责任属性（扩散和差距）的计算复杂性。", "translation": "责任长期以来一直是法律和哲学领域的研究课题。最近，它成为人工智能文献的焦点。本文研究了集体决策中责任的两个重要属性：扩散和差距的计算复杂性。结果表明，无扩散和无差距决策机制的集合分别是 $\\Pi_2$-完全和 $\\Pi_3$-完全的。同时，这些类别的交集是 $\\Pi_2$-完全的。", "summary": "本文深入探讨了集体决策中责任的两个核心特性：扩散和差距的计算复杂性。研究揭示了无扩散机制的集合是 $\\Pi_2$-完全的，无差距机制的集合是 $\\Pi_3$-完全的，而两者的交集也是 $\\Pi_2$-完全的。", "keywords": "责任, 扩散, 差距, 集体决策, 计算复杂性", "comments": "本文将责任这一抽象的哲学法律概念引入计算复杂性框架，为理解和设计集体决策系统中的责任归属提供了量化分析方法。其创新之处在于揭示了构建理想的“无责任差距”或“无责任扩散”机制在计算上的高难度，对人工智能伦理和系统设计具有重要的理论指导意义。"}}
{"id": "2507.02299", "title": "DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation", "authors": ["Yunhan Yang", "Shuo Chen", "Yukun Huang", "Xiaoyang Wu", "Yuan-Chen Guo", "Edmund Y. Lam", "Hengshuang Zhao", "Tong He", "Xihui Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by TPAMI, extension of CVPR 2024 paper DreamComposer", "url": "http://arxiv.org/abs/2507.02299v1", "summary": "Recent advancements in leveraging pre-trained 2D diffusion models achieve the\ngeneration of high-quality novel views from a single in-the-wild image.\nHowever, existing works face challenges in producing controllable novel views\ndue to the lack of information from multiple views. In this paper, we present\nDreamComposer++, a flexible and scalable framework designed to improve current\nview-aware diffusion models by incorporating multi-view conditions.\nSpecifically, DreamComposer++ utilizes a view-aware 3D lifting module to\nextract 3D representations of an object from various views. These\nrepresentations are then aggregated and rendered into the latent features of\ntarget view through the multi-view feature fusion module. Finally, the obtained\nfeatures of target view are integrated into pre-trained image or video\ndiffusion models for novel view synthesis. Experimental results demonstrate\nthat DreamComposer++ seamlessly integrates with cutting-edge view-aware\ndiffusion models and enhances their abilities to generate controllable novel\nviews from multi-view conditions. This advancement facilitates controllable 3D\nobject reconstruction and enables a wide range of applications.", "comment": "Accepted by TPAMI, extension of CVPR 2024 paper DreamComposer", "pdf_url": "http://arxiv.org/pdf/2507.02299v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DreamComposer++：通过多视图条件赋能扩散模型进行3D内容生成", "tldr": "DreamComposer++是一个框架，通过整合多视图条件，显著提升了扩散模型生成可控新视图的能力，从而促进3D内容生成。", "motivation": "现有的利用预训练2D扩散模型从单幅图像生成高质量新颖视图的方法，由于缺乏多视图信息，在生成可控新颖视图方面面临挑战。", "method": "DreamComposer++是一个灵活且可扩展的框架，旨在通过整合多视图条件来改进当前的视图感知扩散模型。它具体包含一个视图感知3D提升模块，用于从不同视图中提取对象的3D表示；一个多视图特征融合模块，用于聚合这些表示并渲染到目标视图的潜在特征；最后将获得的目标视图特征集成到预训练的图像或视频扩散模型中进行新颖视图合成。", "result": "实验结果表明，DreamComposer++能够与最先进的视图感知扩散模型无缝集成，并增强它们从多视图条件生成可控新颖视图的能力。", "conclusion": "DreamComposer++的进步促进了可控的3D对象重建，并支持广泛的应用。", "translation": "近期在利用预训练2D扩散模型从单幅野外图像生成高质量新颖视图方面取得了进展。然而，现有工作由于缺乏多视图信息，在生成可控新颖视图方面面临挑战。在本文中，我们提出了DreamComposer++，一个灵活且可扩展的框架，旨在通过整合多视图条件来改进当前的视图感知扩散模型。具体而言，DreamComposer++利用一个视图感知3D提升模块从各种视图中提取对象的3D表示。然后，这些表示通过多视图特征融合模块聚合并渲染到目标视图的潜在特征中。最后，获得的目标视图特征被集成到预训练的图像或视频扩散模型中进行新颖视图合成。实验结果表明，DreamComposer++能够与最先进的视图感知扩散模型无缝集成，并增强它们从多视图条件生成可控新颖视图的能力。这一进展促进了可控的3D对象重建并支持广泛的应用。", "summary": "DreamComposer++是一个创新的框架，旨在解决现有2D扩散模型在从单图像生成可控新视图时面临的挑战，即缺乏多视图信息。它通过引入一个视图感知3D提升模块来提取多视图3D表示，并使用一个多视图特征融合模块将这些表示整合到目标视图的潜在特征中，最终与预训练扩散模型结合以实现可控的新视图合成。实验证明，DreamComposer++显著增强了视图感知扩散模型生成可控多视图3D内容的能力。", "keywords": "扩散模型, 多视图条件, 3D内容生成, 新颖视图合成, DreamComposer++", "comments": "本文的创新点在于引入了多视图条件来增强扩散模型生成可控3D内容的能力，解决了现有方法在可控性方面的局限。通过结合3D提升和多视图特征融合，提供了一个通用且可扩展的框架，对于3D内容生成和重建领域具有重要意义。"}}
{"id": "2507.02412", "title": "Green Ammonia: A Techno-Economic Supply Chain Optimization", "authors": ["Lucien Genge", "Felix Müsgens"], "categories": ["econ.GN", "cs.SY", "eess.SY", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "Comments:      GitHub incl. data and results is linked in the text", "url": "http://arxiv.org/abs/2507.02412v1", "summary": "Green ammonia is emerging as a strategic intermediary within green energy\nsupply chains, serving effectively as both an industrial commodity and hydrogen\ncarrier. This study provides a techno-economic analysis of green ammonia supply\nchains, comparing cost-effective pathways from global production to European\nconsumers, and evaluates ammonia alongside alternative hydrogen carriers.\nGaseous hydrogen consistently remains the most economical import option for\nEurope, though ammonia holds a narrowing cost advantage over liquid hydrogen\n(from 16 % in 2030 to 10 % by 2040). Competitive ammonia suppliers, notably\nMorocco, the United States, and the United Arab Emirates, benefit from low\nrenewable energy costs, with significant price reductions expected by 2040,\ndriven by falling costs for electricity, electrolysers, and conversion\ntechnologies. Optimal transport modes vary by consumer demand and distance:\ntrucks are ideal for low demands at all distances, rail for medium ranges, and\npipelines for high-demand scenarios. By 2040, ammonia will primarily serve\ndirect-use applications, as hydrogen consumers increasingly shift to direct\nhydrogen supplies. Policymakers should prioritize pipeline infrastructure for\nhydrogen distribution, cautiously invest in ammonia's short- to medium-term\ninfrastructure advantages, and limit long-term reliance on ammonia as a\nhydrogen carrier to mitigate stranded asset risks.", "comment": "GitHub incl. data and results is linked in the text", "pdf_url": "http://arxiv.org/pdf/2507.02412v1", "cate": "econ.GN", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "绿色氨：技术经济供应链优化", "tldr": "本研究对绿色氨供应链进行了技术经济分析，比较了其作为氢载体和工业商品的成本效益，并提出了政策建议。", "motivation": "绿色氨正在成为绿色能源供应链中的战略中间体，既是工业商品又是氢载体。本研究旨在对绿色氨供应链进行技术经济分析，比较从全球生产到欧洲消费者的成本效益途径，并评估氨与其他氢载体。", "method": "本研究通过技术经济分析，比较了绿色氨与其他氢载体的成本效益路径，并评估了不同运输模式在不同需求和距离下的适用性。", "result": "气态氢是欧洲最经济的进口选择，尽管氨对液态氢的成本优势正在缩小（从2030年的16%降至2040年的10%）。摩洛哥、美国和阿联酋是具有竞争力的氨供应商，预计到2040年成本将显著降低。最佳运输模式取决于需求和距离：卡车适用于低需求，铁路适用于中等范围，管道适用于高需求。到2040年，氨将主要用于直接用途，因为氢消费者将转向直接氢供应。", "conclusion": "政策制定者应优先发展氢气管道基础设施，谨慎投资氨的短期至中期基础设施优势，并限制长期依赖氨作为氢载体，以规避搁浅资产风险。", "translation": "绿色氨正在成为绿色能源供应链中的战略中间体，有效兼具工业商品和氢载体的双重作用。本研究对绿色氨供应链进行了技术经济分析，比较了从全球生产到欧洲消费者的成本效益途径，并评估了氨与替代氢载体的表现。气态氢始终是欧洲最经济的进口选择，尽管氨对液态氢的成本优势正在缩小（从2030年的16%降至2040年的10%）。具有竞争力的氨供应商，特别是摩洛哥、美国和阿拉伯联合酋长国，受益于低可再生能源成本，预计到2040年，随着电力、电解槽和转化技术成本的下降，价格将显著降低。最佳运输模式因消费者需求和距离而异：卡车适用于所有距离的低需求，铁路适用于中等范围，管道适用于高需求场景。到2040年，氨将主要用于直接使用，因为氢消费者将越来越多地转向直接氢供应。政策制定者应优先发展氢气管道基础设施，谨慎投资氨的短期至中期基础设施优势，并限制长期依赖氨作为氢载体，以规避搁浅资产风险。", "summary": "本研究对绿色氨供应链进行了技术经济分析，将其作为氢载体和工业商品，并与欧洲的替代氢载体进行比较。研究发现，气态氢仍是最经济的进口选择，但氨对液态氢的成本优势正在缩小。低成本可再生能源地区（如摩洛哥、美国、阿联酋）是主要的氨供应商，且预计到2040年成本将进一步下降。运输模式应根据需求和距离优化。研究建议政策制定者优先发展氢气管道，并谨慎对待氨的短期基础设施优势，以避免长期搁浅资产风险。", "keywords": "绿色氨, 供应链优化, 氢载体, 技术经济分析, 能源政策", "comments": "这篇论文通过详细的技术经济分析，对绿色氨在未来能源转型中的作用提供了宝贵的见解。其创新之处在于不仅比较了不同氢载体的成本效益，还考虑了全球供应链、运输模式以及未来成本下降的趋势。重要性在于为政策制定者提供了明确的投资和发展方向，强调了氢气管道基础设施的优先性，并警示了过度依赖氨作为氢载体的潜在风险。"}}
{"id": "2507.02169", "title": "Statistical Inference for Responsiveness Verification", "authors": ["Seung Hyun Cheon", "Meredith Stewart", "Bogdan Kulynych", "Tsui-Wei Weng", "Berk Ustun"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02169v1", "summary": "Many safety failures in machine learning arise when models are used to assign\npredictions to people (often in settings like lending, hiring, or content\nmoderation) without accounting for how individuals can change their inputs. In\nthis work, we introduce a formal validation procedure for the responsiveness of\npredictions with respect to interventions on their features. Our procedure\nframes responsiveness as a type of sensitivity analysis in which practitioners\ncontrol a set of changes by specifying constraints over interventions and\ndistributions over downstream effects. We describe how to estimate\nresponsiveness for the predictions of any model and any dataset using only\nblack-box access, and how to use these estimates to support tasks such as\nfalsification and failure probability estimation. We develop algorithms that\nconstruct these estimates by generating a uniform sample of reachable points,\nand demonstrate how they can promote safety in real-world applications such as\nrecidivism prediction, organ transplant prioritization, and content moderation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02169v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "响应性验证的统计推断", "tldr": "本文提出了一种用于验证机器学习模型预测响应性的形式化程序，以解决模型在应用于个人时未考虑输入变化导致的安全性问题。", "motivation": "许多机器学习模型在应用于个人（如贷款、招聘、内容审核）时，未能考虑个体如何改变其输入，从而导致安全故障。", "method": "引入了一种针对预测响应性（相对于特征干预）的形式化验证程序。该程序将响应性框架为一种敏感性分析，其中实践者通过指定干预约束和下游效应分布来控制一系列变化。该方法仅使用黑盒访问即可估计任何模型和任何数据集的响应性，并通过生成可达点的均匀样本来构建估计。", "result": "能够估计任何模型和数据集的预测响应性，并支持伪造和失败概率估计等任务。所开发的算法能够通过生成均匀样本来构建这些估计，并在累犯预测、器官移植优先级和内容审核等实际应用中促进安全性。", "conclusion": "本文提出了一种通用的统计推断框架来验证机器学习模型的响应性，从而有助于提高模型在实际应用中的安全性。", "translation": "机器学习中许多安全故障的发生，是由于模型在用于对个人（通常在贷款、招聘或内容审核等场景中）进行预测时，未考虑个体如何改变其输入。在这项工作中，我们引入了一种针对预测响应性（相对于其特征干预）的形式化验证程序。我们的程序将响应性框架为一种敏感性分析，其中实践者通过指定干预约束和下游效应分布来控制一系列变化。我们描述了如何仅使用黑盒访问来估计任何模型和任何数据集的预测响应性，以及如何使用这些估计来支持伪造和失败概率估计等任务。我们开发了通过生成可达点的均匀样本来构建这些估计的算法，并展示了它们如何在累犯预测、器官移植优先级和内容审核等实际应用中促进安全性。", "summary": "本文针对机器学习模型在应用于个人预测时未考虑输入变化导致的安全问题，提出了一种形式化的预测响应性验证程序。该方法将响应性视为一种敏感性分析，允许通过黑盒访问估计任何模型和数据集的响应性，并支持伪造和失败概率估计。作者开发了通过生成均匀样本来构建估计的算法，并在累犯预测、器官移植优先级和内容审核等实际应用中展示了其提升安全性的潜力。", "keywords": "响应性验证, 敏感性分析, 机器学习安全, 黑盒访问, 统计推断", "comments": "这篇论文通过引入“响应性验证”这一概念及其形式化程序，解决了机器学习模型在实际应用中，特别是涉及个人决策时，因未考虑用户输入变化而产生的潜在安全问题。其创新点在于将响应性视为敏感性分析，并提出了基于黑盒访问的通用估计方法，这使得该方法适用于广泛的模型和场景。通过支持伪造和失败概率估计，该工作为提高AI系统的公平性、透明度和鲁棒性提供了重要的工具，对于构建更负责任的AI系统具有重要意义。"}}
{"id": "2507.02694", "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers", "authors": ["Zhijian Xu", "Yilun Zhao", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02694v1", "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02694v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "大型语言模型能否识别科学研究中的关键局限性？一项针对人工智能研究论文的系统评估", "tldr": "本研究评估了大型语言模型（LLMs）在识别科学论文局限性方面的能力，提出了一个局限性分类法和基准测试集LimitGen，并通过文献检索增强了LLMs识别局限性的能力。", "motivation": "同行评审是科学研究的基础，但日益增长的出版量加剧了这项专业密集型过程的挑战。尽管大型语言模型在各种科学任务中展现出潜力，但它们在辅助同行评审，特别是在识别论文局限性方面的潜力仍未得到充分研究。", "method": "首先，本文提出了一个针对科学研究中局限性类型的综合分类法，重点关注AI领域。其次，基于该分类法，本文构建了LimitGen，这是第一个用于评估大型语言模型支持早期反馈和补充人类同行评审能力的综合基准测试，该基准包含两个子集：LimitGen-Syn（通过高质量论文的受控扰动创建的合成数据集）和LimitGen-Human（真实人类编写的局限性集合）。最后，为了提高LLM系统识别局限性的能力，作者通过文献检索对其进行了增强，这对于将局限性识别建立在先前的科学发现基础上至关重要。", "result": "本文提出的方法增强了大型语言模型系统在研究论文中生成局限性的能力。", "conclusion": "增强了大型语言模型系统在研究论文中生成局限性的能力，使它们能够提供更具体和建设性的反馈。", "translation": "同行评审是科学研究的基础，但日益增长的出版物数量加剧了这项专业密集型过程的挑战。尽管大型语言模型（LLMs）在各种科学任务中展现出潜力，但它们在辅助同行评审，特别是在识别论文局限性方面的潜力仍未得到充分研究。我们首先提出了一个科学研究中局限性类型的综合分类法，重点关注人工智能领域。在此分类法的指导下，为了研究局限性，我们提出了LimitGen，这是第一个用于评估LLMs支持早期反馈和补充人类同行评审能力的综合基准测试。我们的基准由两个子集组成：LimitGen-Syn，一个通过对高质量论文进行受控扰动而精心创建的合成数据集，以及LimitGen-Human，一个真实人类编写的局限性集合。为了提高LLM系统识别局限性的能力，我们通过文献检索对其进行了增强，这对于将局限性识别建立在先前的科学发现基础上至关重要。我们的方法增强了LLM系统在研究论文中生成局限性的能力，使它们能够提供更具体和建设性的反馈。", "summary": "本研究旨在探讨大型语言模型（LLMs）在科学研究同行评审中识别论文局限性的潜力。作者首先提出了一个全面的科学研究局限性分类法，并基于此构建了首个综合基准测试LimitGen，该基准包含合成数据（LimitGen-Syn）和人类编写的真实数据（LimitGen-Human）。此外，研究通过整合文献检索来增强LLMs识别局限性的能力。实验结果表明，该方法显著提升了LLMs生成具体和建设性研究局限性反馈的能力，有助于辅助人类同行评审过程。", "keywords": "大型语言模型, 科学研究, 局限性识别, 同行评审, 基准测试", "comments": "这项研究具有重要的创新性和实用价值。它首次提出了一个全面的科学研究局限性分类法，并在此基础上构建了专门用于评估LLMs识别局限性能力的基准测试LimitGen，填补了该领域研究的空白。通过结合文献检索来增强LLMs的能力，为提高自动化同行评审的质量提供了新思路。该工作对于缓解日益增长的论文评审压力，提高科研质量具有积极意义。"}}
{"id": "2507.01615", "title": "EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data", "authors": ["Alper Alimoglu", "Kamil Erdayandi", "Mustafa A. Mustafa", "Ümit Cali"], "categories": ["cs.DC", "cs.CR"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01615v1", "summary": "This paper proposes a new decentralized framework, named EDGChain-E\n(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,\nencrypted energy data using blockchain and the InterPlanetary File System. The\nframework incorporates a Decentralized Autonomous Organization (DAO) to\norchestrate collaborative data governance across the lifecycle of energy\nresearch and operations, such as smart grid monitoring, demand forecasting, and\npeer-to-peer energy trading. In EDGChain-E, initial commits capture the full\nencrypted datasets-such as smart meter readings or grid telemetry-while\nsubsequent updates are tracked as encrypted Git patches, ensuring integrity,\ntraceability, and privacy. This versioning mechanism supports secure\ncollaboration across multiple stakeholders (e.g., utilities, researchers,\nregulators) without compromising sensitive or regulated information. We\nhighlight the framework's capability to maintain FAIR-compliant (Findable,\nAccessible, Interoperable, Reusable) provenance of encrypted data. By embedding\nhash-based content identifiers in Merkle trees, the system enables transparent,\nauditable, and immutable tracking of data changes, thereby supporting\nreproducibility and trust in decentralized energy applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01615v1", "cate": "cs.DC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "EDGChain-E：一个用于加密能源数据版本控制的去中心化基于Git的框架", "tldr": "EDGChain-E是一个去中心化框架，利用区块链、IPFS和Git实现加密能源数据的版本控制，确保数据完整性、可追溯性、隐私性和FAIR合规性，并通过DAO支持多方协作。", "motivation": "该论文旨在解决能源数据管理中版本控制、加密、完整性、可追溯性、隐私和多方协作的挑战，尤其是在智能电网监控、需求预测和点对点能源交易等应用中。", "method": "EDGChain-E是一个去中心化框架，结合了区块链和星际文件系统（IPFS）来管理版本控制的加密能源数据。它采用去中心化自治组织（DAO）来协调能源研究和操作中的数据治理。初始提交捕获完整的加密数据集，后续更新则以加密的Git补丁形式进行跟踪。系统通过在Merkle树中嵌入基于哈希的内容标识符来支持数据的透明、可审计和不可变的跟踪。", "result": "该框架能够维护加密数据的FAIR（可查找、可访问、可互操作、可重用）合规性来源，确保数据完整性、可追溯性和隐私。它支持多利益相关者（如公用事业公司、研究人员、监管机构）之间的安全协作，且不损害敏感或受管制信息。通过哈希和Merkle树，实现了数据变化的透明、可审计和不可变跟踪。", "conclusion": "EDGChain-E通过提供一个去中心化、安全且支持版本控制的框架，增强了分布式能源应用中的数据可重现性和信任度，解决了加密能源数据管理中的关键挑战。", "translation": "本文提出了一个名为EDGChain-E（能源加密数据Git链）的新型去中心化框架，旨在利用区块链和星际文件系统（IPFS）管理版本控制的加密能源数据。该框架整合了去中心化自治组织（DAO），以协调能源研究和运营（如智能电网监控、需求预测和点对点能源交易）整个生命周期中的协作数据治理。在EDGChain-E中，初始提交捕获完整的加密数据集——例如智能电表读数或电网遥测数据——而后续更新则作为加密的Git补丁进行跟踪，确保了数据的完整性、可追溯性和隐私性。这种版本控制机制支持多个利益相关者（例如公用事业公司、研究人员、监管机构）之间的安全协作，而不会损害敏感或受管制的信息。我们强调了该框架维护加密数据FAIR（可查找、可访问、可互操作、可重用）合规性来源的能力。通过在Merkle树中嵌入基于哈希的内容标识符，该系统能够对数据更改进行透明、可审计和不可变的跟踪，从而支持去中心化能源应用中的可重现性和信任。", "summary": "EDGChain-E是一个创新的去中心化框架，旨在通过结合区块链、IPFS和Git版本控制来安全管理加密能源数据。它利用DAO促进多方协作，并确保数据的完整性、可追溯性、隐私和FAIR合规性。通过加密的Git补丁和基于哈希的Merkle树，该系统实现了数据变化的透明、可审计和不可变跟踪，从而增强了去中心化能源应用的可重现性和信任度。", "keywords": "去中心化框架, 能源数据, 区块链, Git, 数据版本控制", "comments": "该论文提出的EDGChain-E框架在去中心化能源数据管理方面具有显著创新性。它巧妙地结合了区块链的去中心化信任、IPFS的分布式存储和Git的版本控制能力，为加密敏感能源数据提供了全面的解决方案。特别是在能源领域强调数据隐私和协作的背景下，其提供FAIR合规性和可审计性的能力非常重要。该框架通过DAO实现数据治理，进一步增强了其去中心化和协作性，对智能电网、能源交易等应用具有实际意义。"}}
{"id": "2507.02708", "title": "Optimizing Start Locations in Ergodic Search for Disaster Response", "authors": ["Ananya Rao", "Alyssa Hargis", "David Wettergreen", "Howie Choset"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02708v1", "summary": "In disaster response scenarios, deploying robotic teams effectively is\ncrucial for improving situational awareness and enhancing search and rescue\noperations. The use of robots in search and rescue has been studied but the\nquestion of where to start robot deployments has not been addressed. This work\naddresses the problem of optimally selecting starting locations for robots with\nheterogeneous capabilities by formulating a joint optimization problem. To\ndetermine start locations, this work adds a constraint to the ergodic\noptimization framework whose minimum assigns robots to start locations. This\nbecomes a little more challenging when the robots are heterogeneous (equipped\nwith different sensing and motion modalities) because not all robots start at\nthe same location, and a more complex adaptation of the aforementioned\nconstraint is applied. Our method assumes access to potential starting\nlocations, which can be obtained from expert knowledge or aerial imagery. We\nexperimentally evaluate the efficacy of our joint optimization approach by\ncomparing it to baseline methods that use fixed starting locations for all\nrobots. Our experimental results show significant gains in coverage\nperformance, with average improvements of 35.98% on synthetic data and 31.91%\non real-world data for homogeneous and heterogeneous teams, in terms of the\nergodic metric.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02708v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "优化灾害响应中遍历搜索的起始位置", "tldr": "该论文优化了灾害响应中遍历搜索的机器人起始位置，展示了对同质和异质团队覆盖性能的显著改进。", "motivation": "在灾害响应场景中，有效部署机器人团队对于提高态势感知和加强搜救行动至关重要。现有研究已涉及机器人搜救，但机器人部署的起始位置问题尚未得到解决。本文旨在解决为具有异构能力的机器人优化选择起始位置的问题。", "method": "通过制定一个联合优化问题来解决机器人起始位置选择问题。该方法在遍历优化框架中添加了一个约束，其最小值将机器人分配到起始位置。对于异构机器人，应用了该约束的更复杂适应。该方法假设可以从专家知识或航空图像中获取潜在的起始位置。", "result": "实验结果显示覆盖性能显著提高，与基线方法相比，在合成数据上同质和异质团队的平均改进分别为35.98%，在真实世界数据上为31.91%，均以遍历度量衡量。", "conclusion": "该研究成功优化了灾害响应中机器人的起始位置，显著提高了同质和异质团队的覆盖性能。", "translation": "在灾害响应场景中，有效部署机器人团队对于提高态势感知和加强搜救行动至关重要。机器人用于搜救的研究已经进行，但机器人部署的起始位置问题尚未得到解决。这项工作通过制定一个联合优化问题，解决了为具有异构能力的机器人优化选择起始位置的问题。为了确定起始位置，这项工作在遍历优化框架中添加了一个约束，其最小值将机器人分配到起始位置。当机器人是异构的（配备不同的传感和运动模式）时，这变得更具挑战性，因为并非所有机器人都在同一位置启动，并且应用了上述约束的更复杂适应。我们的方法假设可以获取潜在的起始位置，这可以从专家知识或航空图像中获得。我们通过将我们的联合优化方法与所有机器人使用固定起始位置的基线方法进行比较，实验评估了其功效。我们的实验结果表明，在遍历度量方面，同质和异质团队在合成数据上平均提高了35.98%，在真实世界数据上平均提高了31.91%，覆盖性能显著提高。", "summary": "本文提出了一种新颖的联合优化方法，用于确定灾害响应中异构机器人团队的最佳起始位置。通过将新约束集成到遍历优化框架中，该方法解决了机器人初始部署中尚未解决的问题。实验结果表明，与基线方法相比，覆盖性能显著提高（合成数据上提高35.98%，真实世界数据上提高31.91%），突出了其对同质和异质团队的有效性。", "keywords": "遍历搜索, 灾害响应, 机器人部署, 优化, 异构机器人", "comments": "该论文通过优化初始部署位置，解决了一个机器人灾害响应中新颖且重要的问题。将异构机器人能力整合到遍历优化框架中是其主要创新点，实验结果也证明了其重要的实际意义。"}}
{"id": "2507.02800", "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding", "authors": ["Ebrahim Feghhi", "Shreyas Kaasyap", "Nima Hadidi", "Jonathan C. Kao"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2507.02800v1", "summary": "Speech neuroprostheses aim to restore communication for people with severe\nparalysis by decoding speech directly from neural activity. To accelerate\nalgorithmic progress, a recent benchmark released intracranial recordings from\na paralyzed participant attempting to speak, along with a baseline decoding\nalgorithm. Prior work on the benchmark showed impressive accuracy gains.\nHowever, these gains increased computational costs and were not demonstrated in\na real-time decoding setting. Here, we make three contributions that pave the\nway towards accurate, efficient, and real-time neural speech decoding. First,\nwe incorporate large amounts of time masking during training. On average, over\n$50\\%$ of each trial is masked. Second, we replace the gated recurrent unit\n(GRU) architecture used in the baseline algorithm with a compact Transformer.\nThe Transformer architecture uses $77\\%$ fewer parameters, cuts peak GPU memory\nusage by $36\\%$ relative, and is significantly faster to calibrate relative to\nthe GRU. Third, we design a lightweight variant of an existing test-time\nadaptation method developed for decoding handwriting from neural activity. Our\nvariant adapts the model using multiple time masked augmentations of a single\ntrial and requires only one gradient step per trial. Together, these\ncontributions reduce word error rate by $19.5\\%$ and effectively mitigate\nperformance degradations across held-out days in a real-time decoding setting\nwhile substantially lowering computational costs.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02800v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "时间掩蔽Transformer结合轻量级测试时自适应用于神经语音解码", "tldr": "本文提出了一种结合时间掩蔽Transformer和轻量级测试时自适应的方法，以实现准确、高效和实时的神经语音解码，显著降低了词错误率和计算成本。", "motivation": "言语神经假肢旨在通过从神经活动中解码言语来帮助重度瘫痪患者恢复交流。然而，现有的神经语音解码算法虽然准确性有所提高，但计算成本高昂，且未能在实时解码环境中得到验证。", "method": "本文提出了三点贡献：1. 在训练过程中引入大量时间掩蔽（平均超过50%的试验被掩蔽）。2. 用紧凑型Transformer取代了基线算法中使用的门控循环单元（GRU）架构，参数减少77%，峰值GPU内存使用量减少36%，校准速度显著加快。3. 设计了一种轻量级的测试时自适应变体，该变体利用单个试验的多个时间掩蔽增强来适应模型，每个试验仅需一步梯度更新。", "result": "这些贡献共同将词错误率降低了19.5%，并在实时解码设置中有效缓解了跨保留日的性能下降，同时大幅降低了计算成本。", "conclusion": "本文提出的时间掩蔽Transformer结合轻量级测试时自适应的方法，为实现准确、高效和实时的神经语音解码铺平了道路，显示出在降低计算成本的同时保持高性能的潜力。", "translation": "言语神经假肢旨在通过直接从神经活动中解码言语，为重度瘫痪患者恢复交流。为了加速算法进展，最近一项基准测试发布了来自一名试图讲话的瘫痪参与者的颅内记录，以及一个基线解码算法。先前关于该基准的研究显示出令人印象深刻的准确性提升。然而，这些提升增加了计算成本，并且未能在实时解码环境中得到验证。在此，我们做出了三项贡献，为实现准确、高效和实时的神经语音解码铺平了道路。首先，我们在训练期间加入了大量时间掩蔽。平均而言，每个试验超过50%的部分被掩蔽。其次，我们将基线算法中使用的门控循环单元（GRU）架构替换为紧凑型Transformer。与GRU相比，Transformer架构使用的参数减少了77%，峰值GPU内存使用量相对减少了36%，并且校准速度显著加快。第三，我们设计了一种现有测试时自适应方法的轻量级变体，该方法最初是为从神经活动中解码手写而开发的。我们的变体通过单个试验的多个时间掩蔽增强来适应模型，并且每个试验仅需一步梯度更新。总而言之，这些贡献将词错误率降低了19.5%，并在实时解码设置中有效缓解了跨保留日的性能下降，同时大幅降低了计算成本。", "summary": "本文提出了一种用于神经语音解码的新方法，旨在解决现有方案计算成本高且非实时的问题。该方法主要包含三项创新：在训练中引入大量时间掩蔽、用紧凑型Transformer替代GRU架构以提高效率、以及开发一种轻量级的测试时自适应变体。实验结果表明，该方法在实时解码环境下将词错误率降低了19.5%，有效缓解了性能下降，并显著降低了计算成本，为实现准确、高效和实时的神经语音解码奠定了基础。", "keywords": "神经语音解码, Transformer, 时间掩蔽, 测试时自适应, 实时性", "comments": "本文在神经语音解码领域做出了重要贡献，特别是在提高效率和实时性方面。通过引入时间掩蔽、采用更紧凑的Transformer模型以及设计轻量级测试时自适应，该研究有效地解决了现有方法计算成本高的问题，同时保持了高准确性。其创新点在于将多种优化策略结合，使其在实际应用中更具可行性，对于推动神经假肢技术的发展具有重要意义。"}}
{"id": "2507.02616", "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making", "authors": ["Tianqi Shang", "Weiqing He", "Charles Zheng", "Lingyao Li", "Li Shen", "Bingxin Zhao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.02616v1", "summary": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.02616v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DynamiCare：一种用于交互式开放性医疗决策的动态多智能体框架", "tldr": "本文介绍了DynamiCare，一个用于动态、交互式医疗诊断的多智能体LLM框架，以及MIMIC-Patient，一个用于患者级模拟的数据集，为交互式临床决策提供了基准。", "motivation": "现有的医疗决策AI框架主要侧重于单轮任务，且医生智能体预先接收完整病例信息，这与现实世界中不确定、交互和迭代的诊断过程不符。", "method": "本文引入了MIMIC-Patient，一个从MIMIC-III电子健康记录构建的结构化数据集，用于支持动态的患者级模拟。在此基础上，提出了DynamiCare，一个新颖的动态多智能体框架，将临床诊断建模为多轮、交互式循环，其中专家智能体团队迭代地查询患者系统、整合新信息并动态调整其组成和策略。", "result": "通过广泛的实验，证明了DynamiCare的可行性和有效性，并为LLM驱动的智能体的动态临床决策建立了第一个基准。", "conclusion": "DynamiCare是一个可行且有效的动态多智能体框架，能够模拟现实世界中不确定、交互和迭代的医疗诊断过程，并为LLM驱动的智能体的动态临床决策建立了新的基准。", "translation": "大型语言模型（LLMs）的兴起使得开发具有领域特定推理和交互能力的专业AI智能体成为可能，特别是在医疗保健领域。虽然最近的框架模拟了医疗决策，但它们主要侧重于单轮任务，即医生智能体预先接收完整的病例信息——这与现实世界的诊断过程有所不同，后者本质上是不确定、交互和迭代的。在本文中，我们引入了MIMIC-Patient，这是一个从MIMIC-III电子健康记录（EHRs）构建的结构化数据集，旨在支持动态的患者级模拟。在此基础上，我们提出了DynamiCare，一个新颖的动态多智能体框架，它将临床诊断建模为一个多轮、交互式循环，其中一组专家智能体迭代地查询患者系统，整合新信息，并动态调整其组成和策略。我们通过广泛的实验证明了DynamiCare的可行性和有效性，为LLM驱动的智能体的动态临床决策建立了第一个基准。", "summary": "本文针对当前单轮AI医疗决策框架的局限性，提出了DynamiCare，一个新颖的动态多智能体框架。DynamiCare将临床诊断模拟为一个交互式、多轮过程，其中专家智能体动态调整其策略。为支持此框架，作者还引入了MIMIC-Patient，一个从MIMIC-III EHRs衍生的结构化数据集。实验证明了DynamiCare的有效性，为LLM驱动的动态临床决策建立了新的基准。", "keywords": "大型语言模型, 多智能体系统, 医疗决策, 临床诊断, MIMIC-Patient", "comments": "这篇论文通过将医疗决策从静态的单轮交互转变为更真实的、动态的、交互式的多智能体系统，引入了一种创新方法。MIMIC-Patient数据集的创建对于促进此类动态模拟至关重要。这项工作通过为复杂的临床场景建立新基准，对于推动LLM在医疗保健领域的应用具有重要意义。"}}
{"id": "2507.02307", "title": "Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images", "authors": ["Haoxuan Li", "Chenxu Wei", "Haodong Wang", "Xiaomeng Hu", "Boyuan An", "Lingyan Ran", "Baosen Zhang", "Jin Jin", "Omirzhan Taukebayev", "Amirkhan Temirbayev", "Junrui Liu", "Xiuwei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 8 figures", "url": "http://arxiv.org/abs/2507.02307v1", "summary": "Change detection typically involves identifying regions with changes between\nbitemporal images taken at the same location. Besides significant changes, slow\nchanges in bitemporal images are also important in real-life scenarios. For\ninstance, weak changes often serve as precursors to major hazards in scenarios\nlike slopes, dams, and tailings ponds. Therefore, designing a change detection\nnetwork that simultaneously detects slow and fast changes presents a novel\nchallenge. In this paper, to address this challenge, we propose a change\ndetection network named Flow-CDNet, consisting of two branches: optical flow\nbranch and binary change detection branch. The first branch utilizes a pyramid\nstructure to extract displacement changes at multiple scales. The second one\ncombines a ResNet-based network with the optical flow branch's output to\ngenerate fast change outputs. Subsequently, to supervise and evaluate this new\nchange detection framework, a self-built change detection dataset Flow-Change,\na loss function combining binary tversky loss and L2 norm loss, along with a\nnew evaluation metric called FEPE are designed. Quantitative experiments\nconducted on Flow-Change dataset demonstrated that our approach outperforms the\nexisting methods. Furthermore, ablation experiments verified that the two\nbranches can promote each other to enhance the detection performance.", "comment": "18 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.02307v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Flow-CDNet：一种检测双时相图像中慢速和快速变化的新型网络", "tldr": "本文提出了Flow-CDNet，一个双分支网络，用于同时检测双时相图像中的慢速和快速变化，并在新数据集上优于现有方法。", "motivation": "变化检测通常侧重于显著变化，但慢速变化在实际场景中也至关重要，例如作为重大灾害（如斜坡、大坝、尾矿库）的前兆。因此，设计一个能同时检测慢速和快速变化的网络是一个新颖的挑战。", "method": "提出了Flow-CDNet，包含光流分支（利用金字塔结构提取多尺度位移变化）和二元变化检测分支（结合ResNet和光流输出生成快速变化）。同时，构建了自建数据集Flow-Change，设计了结合二元Tversky损失和L2范数损失的复合损失函数，以及新的评估指标FEPE。", "result": "在Flow-Change数据集上的定量实验表明，所提出的方法优于现有方法。消融实验验证了两个分支可以相互促进，提高检测性能。", "conclusion": "Flow-CDNet能够有效检测双时相图像中的慢速和快速变化，其性能优于现有方法，并且其双分支设计被证明是有效的。", "translation": "变化检测通常涉及识别在同一位置拍摄的双时相图像之间发生变化的区域。除了显著变化外，双时相图像中的慢速变化在现实场景中也至关重要。例如，在斜坡、大坝和尾矿库等场景中，微弱变化常常是重大灾害的前兆。因此，设计一个能够同时检测慢速和快速变化的检测网络提出了一个新颖的挑战。在本文中，为了解决这个挑战，我们提出了一种名为Flow-CDNet的变化检测网络，它由两个分支组成：光流分支和二元变化检测分支。第一个分支利用金字塔结构提取多尺度位移变化。第二个分支将基于ResNet的网络与光流分支的输出相结合，以生成快速变化输出。随后，为了监督和评估这个新的变化检测框架，设计了一个自建的变化检测数据集Flow-Change，一个结合了二元Tversky损失和L2范数损失的损失函数，以及一个新的评估指标FEPE。在Flow-Change数据集上进行的定量实验表明，我们的方法优于现有方法。此外，消融实验验证了这两个分支可以相互促进以提高检测性能。", "summary": "本文提出了一种名为Flow-CDNet的新型双分支神经网络，旨在同时检测双时相图像中的慢速和快速变化。该网络包含一个用于多尺度位移的光流分支和一个用于快速变化的二元变化检测分支。为支持该框架，作者开发了新的Flow-Change数据集、混合损失函数以及新的评估指标FEPE。实验结果表明，Flow-CDNet超越了现有方法，消融研究证实了其分支间的协同作用。", "keywords": "变化检测, 双时相图像, 慢速变化, 快速变化, Flow-CDNet", "comments": "该论文通过关注慢速变化（通常是灾害前兆），解决了变化检测领域的一个关键空白。结合光流和二元变化检测的双分支架构具有创新性，并且新数据集、损失函数和评估指标的创建体现了解决问题的全面方法。"}}
{"id": "2507.02225", "title": "Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction", "authors": ["Jiyeon Bae", "Hyeon Jeon", "Jinwook Seo"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      IEEE VIS 2025 (short paper)", "url": "http://arxiv.org/abs/2507.02225v1", "summary": "Evaluating the accuracy of dimensionality reduction (DR) projections in\npreserving the structure of high-dimensional data is crucial for reliable\nvisual analytics. Diverse evaluation metrics targeting different structural\ncharacteristics have thus been developed. However, evaluations of DR\nprojections can become biased if highly correlated metrics--those measuring\nsimilar structural characteristics--are inadvertently selected, favoring DR\ntechniques that emphasize those characteristics. To address this issue, we\npropose a novel workflow that reduces bias in the selection of evaluation\nmetrics by clustering metrics based on their empirical correlations rather than\non their intended design characteristics alone. Our workflow works by computing\nmetric similarity using pairwise correlations, clustering metrics to minimize\noverlap, and selecting a representative metric from each cluster. Quantitative\nexperiments demonstrate that our approach improves the stability of DR\nevaluation, which indicates that our workflow contributes to mitigating\nevaluation bias.", "comment": "IEEE VIS 2025 (short paper)", "pdf_url": "http://arxiv.org/pdf/2507.02225v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "度量设计 ≠ 度量行为：改进度量选择以实现降维的无偏评估", "tldr": "本文提出一种基于经验相关性而非设计意图来聚类度量的新方法，旨在减少降维评估中的偏差，提高评估稳定性。", "motivation": "评估降维（DR）投影在保留高维数据结构方面的准确性对于可靠的视觉分析至关重要。然而，现有评估可能因选择高度相关的度量而产生偏差，偏向特定降维技术。", "method": "提出一种新颖的工作流程，通过计算度量之间的成对相关性来衡量相似性，然后聚类度量以最小化重叠，并从每个聚类中选择一个代表性度量。", "result": "定量实验表明，该方法提高了降维评估的稳定性。", "conclusion": "该工作流程通过减少评估指标选择中的偏差，有助于提高降维评估的可靠性。", "translation": "评估降维（DR）投影在保留高维数据结构方面的准确性对于可靠的视觉分析至关重要。因此，已经开发了针对不同结构特征的各种评估指标。然而，如果无意中选择了高度相关（即测量相似结构特征）的指标，降维投影的评估可能会变得有偏，从而偏向那些强调这些特征的降维技术。为了解决这个问题，我们提出了一种新颖的工作流程，通过基于指标的经验相关性而非仅仅其预期的设计特征来聚类指标，从而减少评估指标选择中的偏差。我们的工作流程通过计算成对相关性来衡量指标相似性，聚类指标以最小化重叠，并从每个聚类中选择一个代表性指标。定量实验表明，我们的方法提高了降维评估的稳定性，这表明我们的工作流程有助于减轻评估偏差。", "summary": "本文提出一种新颖的工作流程，旨在解决降维（DR）评估中因选择高度相关指标而导致的偏差问题。该方法通过分析指标的经验相关性而非其设计意图进行聚类，并从每个聚类中选择代表性指标。实验证明，这种基于行为的指标选择方法显著提高了DR评估的稳定性，从而有效减轻了评估偏差。", "keywords": "降维评估, 度量选择, 评估偏差, 经验相关性, 聚类", "comments": "创新点在于将度量选择从单纯的设计意图转向实际的行为相关性，通过聚类方法系统地减少了评估偏差，提高了评估的公正性和可靠性。这对于视觉分析和降维算法的比较具有重要意义。"}}
{"id": "2507.02744", "title": "Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens", "authors": ["Peter Viechnicki"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02744v1", "summary": "A body of work over the past several decades has demonstrated that the\ncomplex and coordinated articulatory movements of human vowel production are\ngoverned (at least in part)by control mechanisms whose targets are regions of\nauditory space. Within the target region control at the sub-phonemic level has\nalso been demonstrated. But the degree of accuracy of that control is unknown.\nThe current work investigates this question by asking how far apart must two\nvowel stimuli lie in auditory space in order to yield reliably different\nimitations? This distance is termed 'Just Producible Difference' (JPD). The\ncurrent study uses a vowel mimicry paradigm to derive the first measurement of\nJPD among two sets of English speakers during front vowel production. JPD is\nestimated at between 14 and 51 mels in F1 X F2 space. This finding has\nimplications for episodic theories of speech production. It also clarifies the\npossible structures of human vowel systems, by setting a theoretical lower\nbound for how close two vowel phonemes may be in a speaker's formant space, and\nhence a psychophysical explanation of observed trends in number and patterns of\npossible vowel phonemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02744v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "采用“刚刚可产生差异”(JPD)阈限测量元音产生空间的粒度", "tldr": "本研究通过元音模仿范式测量了英语使用者元音产生空间的“刚刚可产生差异”(JPD)阈限，发现JPD在F1 x F2空间中介于14到51mels之间，这对于语音产生的情景理论和元音系统结构具有重要意义。", "motivation": "过去几十年的研究表明人类元音产生受控于听觉空间中的目标区域，且在亚音素层面也存在控制，但这种控制的准确度尚不清楚。本研究旨在通过测量两个元音刺激在听觉空间中需相距多远才能产生可靠的不同模仿，来探究这种控制的准确度。", "method": "本研究采用元音模仿范式，首次测量了两组英语使用者在前元音产生过程中“刚刚可产生差异”(JPD)的阈限。JPD被定义为在听觉空间中两个元音刺激需要相距多远才能产生可靠的不同模仿。", "result": "JPD在F1 x F2空间中估计介于14到51mels之间。", "conclusion": "这项发现对语音产生的情景理论具有启示。它还通过设定扬声器共振峰空间中两个元音音素可能接近的理论下限，从而对观察到的元音音素数量和模式趋势提供了心理物理学解释，澄清了人类元音系统可能的结构。", "translation": "过去几十年的大量研究表明，人类元音产生中复杂协调的发音运动（至少部分地）受控于以听觉空间区域为目标的控制机制。在目标区域内，亚音素层面的控制也已得到证实。但这种控制的准确度尚不清楚。当前的工作通过询问两个元音刺激在听觉空间中必须相距多远才能产生可靠的不同模仿来调查这个问题？这个距离被称为“刚刚可产生差异”（JPD）。当前的研究使用元音模仿范式，首次测量了两组英语使用者在前元音产生过程中的JPD。JPD在F1 X F2空间中估计介于14到51mels之间。这一发现对语音产生的情景理论具有启示。它还通过设定扬声器共振峰空间中两个元音音素可能接近的理论下限，从而对观察到的元音音素数量和模式趋势提供了心理物理学解释，澄清了人类元音系统可能的结构。", "summary": "本研究旨在量化人类元音产生控制的准确性。通过引入“刚刚可产生差异”(JPD)的概念，即在听觉空间中，两个元音刺激需相距多远才能产生可靠的不同模仿，研究者使用元音模仿范式首次测量了英语使用者前元音产生中的JPD。结果显示，JPD在F1 x F2空间中介于14到51mels之间。这项发现不仅对语音产生的情景理论有重要启示，也为人类元音系统的结构设定了理论下限，从而解释了元音音素数量和模式的趋势。", "keywords": "元音产生, JPD, 听觉空间, 语音控制, 元音系统", "comments": "这项研究通过引入JPD这一新颖的测量方法，量化了人类元音产生控制的精细程度，填补了现有研究的空白。其结果为语音产生的情景理论提供了实验证据，并对理解人类元音系统的结构和演变具有基础性意义，特别是为元音音素的区分度提供了心理物理学解释。"}}
{"id": "2507.02414", "title": "Privacy-preserving Preselection for Face Identification Based on Packing", "authors": ["Rundong Xin", "Taotao Wang", "Jin Wang", "Chonghe Zhao", "Jing Wang"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication in SecureComm 2025", "url": "http://arxiv.org/abs/2507.02414v1", "summary": "Face identification systems operating in the ciphertext domain have garnered\nsignificant attention due to increasing privacy concerns and the potential\nrecovery of original facial data. However, as the size of ciphertext template\nlibraries grows, the face retrieval process becomes progressively more\ntime-intensive. To address this challenge, we propose a novel and efficient\nscheme for face retrieval in the ciphertext domain, termed Privacy-Preserving\nPreselection for Face Identification Based on Packing (PFIP). PFIP incorporates\nan innovative preselection mechanism to reduce computational overhead and a\npacking module to enhance the flexibility of biometric systems during the\nenrollment stage. Extensive experiments conducted on the LFW and CASIA datasets\ndemonstrate that PFIP preserves the accuracy of the original face recognition\nmodel, achieving a 100% hit rate while retrieving 1,000 ciphertext face\ntemplates within 300 milliseconds. Compared to existing approaches, PFIP\nachieves a nearly 50x improvement in retrieval efficiency.", "comment": "This paper has been accepted for publication in SecureComm 2025", "pdf_url": "http://arxiv.org/pdf/2507.02414v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "隐私保护的基于打包的人脸识别预筛选", "tldr": "本文提出PFIP，一种隐私保护的人脸识别预筛选方案，通过预筛选和打包模块，在密文域实现了高效且准确的人脸检索，相较现有方法效率提升近50倍。", "motivation": "随着隐私担忧的增加，密文域人脸识别系统受到关注，但密文模板库的增大导致人脸检索过程耗时。", "method": "提出PFIP方案，包含创新的预筛选机制以减少计算开销，以及打包模块以增强生物识别系统在注册阶段的灵活性。", "result": "在LFW和CASIA数据集上，PFIP保持原始模型精度，实现100%命中率，在300毫秒内检索1,000个密文人脸模板，检索效率比现有方法提高近50倍。", "conclusion": "PFIP在密文域人脸识别中实现了高效、准确且隐私保护的检索，显著提高了检索效率。", "translation": "标题：隐私保护的基于打包的人脸识别预筛选\n摘要：随着隐私担忧的增加以及原始面部数据潜在的恢复风险，在密文域运行的人脸识别系统受到了广泛关注。然而，随着密文模板库规模的增长，人脸检索过程变得越来越耗时。为了解决这一挑战，我们提出了一种新颖高效的密文域人脸检索方案，命名为基于打包的隐私保护人脸识别预筛选（PFIP）。PFIP结合了创新的预筛选机制以减少计算开销，以及一个打包模块以增强生物识别系统在注册阶段的灵活性。在LFW和CASIA数据集上进行的广泛实验表明，PFIP保持了原始人脸识别模型的准确性，在300毫秒内检索1,000个密文人脸模板时实现了100%的命中率。与现有方法相比，PFIP的检索效率提高了近50倍。", "summary": "本文提出一种名为PFIP的隐私保护人脸识别预筛选方案，旨在解决密文域人脸识别系统中大规模模板库导致的检索效率低下问题。PFIP通过引入预筛选机制降低计算成本，并利用打包模块提升系统注册灵活性。实验证明，PFIP在保持识别精度的同时，显著提升了密文人脸模板的检索速度，实现了100%的命中率和近50倍的效率提升。", "keywords": "隐私保护, 人脸识别, 密文域, 预筛选, 打包", "comments": "该论文的创新点在于提出了结合预筛选和打包模块的PFIP方案，有效解决了密文域人脸识别在大规模数据集下的效率瓶颈，同时保持了高准确率，对于隐私保护生物识别领域具有重要意义。"}}
{"id": "2507.02761", "title": "Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization", "authors": ["Long Xu", "Choilam Wong", "Mengke Zhang", "Junxiao Lin", "Fei Gao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Technical Report", "url": "http://arxiv.org/abs/2507.02761v1", "summary": "We present an efficient hierarchical motion planning pipeline for\ndifferential drive mobile manipulators. Our approach first searches for\nmultiple collisionfree and topologically distinct paths for the mobile base to\nextract the space in which optimal solutions may exist. Further sampling and\noptimization are then conducted in parallel to explore feasible whole-body\ntrajectories. For trajectory optimization, we employ polynomial trajectories\nand arc length-yaw parameterization, enabling efficient handling of the\nnonholonomic dynamics while ensuring optimality.", "comment": "Technical Report", "pdf_url": "http://arxiv.org/pdf/2507.02761v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "差速驱动移动机械臂的轨迹优化：基于拓扑路径搜索与弧长-偏航角参数化", "tldr": "本文提出了一种用于差速驱动移动机械臂的高效分层运动规划方法，该方法结合了拓扑路径搜索和弧长-偏航角参数化进行轨迹优化。", "motivation": "本文旨在为差速驱动移动机械臂开发一种高效的分层运动规划流程，以在处理非完整动力学的同时，有效地找到最优解。", "method": "该方法采用分层运动规划流程。首先，为移动基座搜索多个无碰撞且拓扑上不同的路径，以确定潜在的最优解空间。接着，并行进行进一步的采样和优化，以探索可行的全身轨迹。轨迹优化采用多项式轨迹和弧长-偏航角参数化，从而能够有效处理非完整动力学并确保最优性。", "result": "该方法能够有效处理非完整动力学，同时确保轨迹的最优性。", "conclusion": "所提出的分层运动规划流程，结合拓扑路径搜索和弧长-偏航角参数化，为差速驱动移动机械臂提供了一种高效且最优的解决方案，尤其在处理非完整约束方面表现出色。", "translation": "我们提出了一种用于差速驱动移动机械臂的高效分层运动规划流程。我们的方法首先为移动基座搜索多个无碰撞且拓扑上不同的路径，以提取可能存在最优解的空间。然后并行进行进一步采样和优化，以探索可行的全身轨迹。对于轨迹优化，我们采用多项式轨迹和弧长-偏航角参数化，从而能够有效处理非完整动力学，同时确保最优性。", "summary": "本文介绍了一种针对差速驱动移动机械臂的高效分层运动规划流程。该方法首先通过对移动基座进行拓扑路径搜索来识别无碰撞且拓扑不同的路径，从而提取潜在的最优解空间。随后，并行进行采样和优化，以生成可行的全身轨迹。在轨迹优化阶段，本文采用了多项式轨迹和弧长-偏航角参数化，这使得方法能够有效处理非完整动力学并确保轨迹的最优性。", "keywords": "轨迹优化, 移动机械臂, 运动规划, 拓扑路径, 非完整动力学", "comments": "该论文的创新之处在于其分层方法，将拓扑路径搜索用于初始空间探索与并行采样/优化相结合，并引入了新颖的弧长-偏航角参数化，以有效处理差速驱动系统特有的非完整约束。这有望显著提高移动机械臂运动规划的效率和最优性。"}}
{"id": "2507.02618", "title": "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory", "authors": ["Kenneth Payne", "Baptiste Alloui-Cros"], "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      29 pages, 27 tables, 4 figures", "url": "http://arxiv.org/abs/2507.02618v1", "summary": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.", "comment": "29 pages, 27 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02618v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "大型语言模型的战略智能：来自演化博弈论的证据", "tldr": "本文通过演化迭代囚徒困境实验，评估了大型语言模型（LLMs）的战略智能，发现它们在竞争环境中表现出独特的战略行为，并能主动进行策略推理。", "motivation": "研究旨在探究大型语言模型（LLMs）是否具备战略智能，即在竞争环境中推理目标的能力。", "method": "研究通过进行一系列演化迭代囚徒困境（IPD）锦标赛来评估LLMs的战略智能。实验将经典策略（如一报还一报、冷酷触发）与来自OpenAI、Google和Anthropic的领先AI公司的LLM代理进行对抗。通过改变每场比赛的终止概率（“未来之影”），引入复杂性和随机性，以防止记忆。研究还分析了模型提供的近32,000个散文理由，以理解它们的决策过程。", "result": "LLMs在复杂的生态系统中表现出高度竞争力，能够持续生存甚至繁衍。不同的LLMs展现出独特的“战略指纹”：Google的Gemini模型表现出战略上的无情，会利用合作对手并报复背叛者；OpenAI的模型则高度合作，但在敌对环境中表现不佳；Anthropic的Claude是最宽容的互惠者，即使被利用或成功背叛后也愿意恢复合作。模型提供的理由表明它们积极推理时间范围和对手的可能策略，并且这种推理对其决策至关重要。", "conclusion": "大型语言模型确实具备战略智能，能够主动推理竞争环境中的策略，并表现出独特的、可识别的战略行为。这项工作将经典博弈论与机器心理学联系起来，提供了不确定性下算法决策的丰富且细致的视角。", "translation": "大型语言模型（LLM）是否是一种新型的战略智能，能够在竞争环境中推理目标？我们提供了令人信服的支持证据。迭代囚徒困境（IPD）长期以来一直是研究决策的模型。我们首次进行了一系列演化IPD锦标赛，让经典策略（例如，一报还一报、冷酷触发）与来自领先的前沿AI公司OpenAI、Google和Anthropic的代理进行对抗。通过改变每场比赛的终止概率（“未来之影”），我们引入了复杂性和机会，从而混淆了记忆。\n我们的结果表明，LLM具有高度竞争力，在这些复杂的生态系统中持续生存，有时甚至繁衍。此外，它们表现出独特且持久的“战略指纹”：Google的Gemini模型被证明是战略上无情的，利用合作对手并报复背叛者，而OpenAI的模型则保持高度合作，这一特点在敌对环境中被证明是灾难性的。Anthropic的Claude表现出最宽容的互惠者，即使在被利用或成功背叛后也表现出显著的恢复合作的意愿。对模型提供的近32,000个散文理由的分析表明，它们积极地推理时间范围和对手的可能策略，并且我们证明这种推理对其决策至关重要。这项工作将经典博弈论与机器心理学联系起来，提供了不确定性下算法决策的丰富而细致的视角。", "summary": "本文通过对大型语言模型（LLMs）进行首次演化迭代囚徒困境（IPD）锦标赛，探究了LLMs的战略智能。研究发现LLMs在竞争环境中表现出高度竞争力，并展现出独特的战略“指纹”，例如Google的Gemini模型表现出无情策略，OpenAI模型高度合作，而Anthropic的Claude则更宽容。此外，通过分析模型提供的理由，研究表明LLMs能够主动推理时间范围和对手策略，并以此指导决策。这项工作成功地将经典博弈论与机器心理学结合，为理解不确定性下的算法决策提供了新视角。", "keywords": "大型语言模型, 战略智能, 演化博弈论, 迭代囚徒困境, 机器心理学", "comments": "这项研究创新性地将演化博弈论应用于评估大型语言模型的战略行为，揭示了不同LLM在竞争环境中的独特“个性”和决策机制。它不仅提供了LLM具备战略智能的有力证据，还通过分析模型内部的推理过程，深化了我们对AI决策的理解。这项工作对于理解AI在复杂社会互动中的潜在行为具有重要意义，也为未来设计更具适应性和鲁棒性的AI策略提供了启示。"}}
{"id": "2507.02308", "title": "LMPNet for Weakly-supervised Keypoint Discovery", "authors": ["Pei Guo", "Ryan Farrell"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02308v1", "summary": "In this work, we explore the task of semantic object keypoint discovery\nweakly-supervised by only category labels. This is achieved by transforming\ndiscriminatively-trained intermediate layer filters into keypoint detectors. We\nbegin by identifying three preferred characteristics of keypoint detectors: (i)\nspatially sparse activations, (ii) consistency and (iii) diversity. Instead of\nrelying on hand-crafted loss terms, a novel computationally-efficient leaky max\npooling (LMP) layer is proposed to explicitly encourage final conv-layer\nfilters to learn \"non-repeatable local patterns\" that are well aligned with\nobject keypoints. Informed by visualizations, a simple yet effective selection\nstrategy is proposed to ensure consistent filter activations and attention\nmask-out is then applied to force the network to distribute its attention to\nthe whole object instead of just the most discriminative region. For the final\nkeypoint prediction, a learnable clustering layer is proposed to group keypoint\nproposals into keypoint predictions. The final model, named LMPNet, is highly\ninterpretable in that it directly manipulates network filters to detect\npredefined concepts. Our experiments show that LMPNet can (i) automatically\ndiscover semantic keypoints that are robust to object pose and (ii) achieves\nstrong prediction accuracy comparable to a supervised pose estimation model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02308v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LMPNet用于弱监督关键点发现", "tldr": "LMPNet是一种新的弱监督语义关键点发现方法，通过将判别训练的中间层滤波器转换为关键点检测器，并引入Leaky Max Pooling (LMP) 层来学习非重复局部模式，实现了对物体姿态鲁棒的关键点发现，并达到了与监督模型相当的精度。", "motivation": "本研究旨在探索仅通过类别标签进行弱监督的语义对象关键点发现任务，通过将判别训练的中间层滤波器转换为关键点检测器来解决此问题。", "method": "LMPNet通过以下方式实现：首先，确定关键点检测器的三个特性（空间稀疏激活、一致性和多样性）；其次，提出一种新颖的计算高效的Leaky Max Pooling (LMP) 层，以促使最终卷积层滤波器学习与对象关键点对齐的“不可重复局部模式”；接着，提出一种简单有效的选择策略以确保滤波器激活的一致性，并应用注意力遮蔽以强制网络将注意力分散到整个对象；最后，提出一个可学习的聚类层来将关键点提议分组为关键点预测。该模型直接操作网络滤波器来检测预定义概念。", "result": "实验表明，LMPNet可以自动发现对物体姿态鲁棒的语义关键点，并且实现了与监督姿态估计模型相当的强大预测精度。", "conclusion": "LMPNet提供了一种高度可解释且有效的弱监督语义关键点发现方法，能够自动发现对物体姿态鲁棒的语义关键点，并取得与监督模型相当的性能。", "translation": "在这项工作中，我们探索了仅通过类别标签进行弱监督的语义对象关键点发现任务。这通过将判别训练的中间层滤波器转换为关键点检测器来实现。我们首先确定了关键点检测器的三个优选特性：(i) 空间稀疏激活，(ii) 一致性和 (iii) 多样性。为了不依赖于手工设计的损失项，我们提出了一种新颖的计算高效的Leaky Max Pooling (LMP) 层，以明确鼓励最终卷积层滤波器学习与对象关键点良好对齐的“不可重复局部模式”。根据可视化结果，我们提出了一种简单而有效的选择策略，以确保滤波器激活的一致性，然后应用注意力遮蔽，强制网络将其注意力分散到整个对象，而不仅仅是最具判别性的区域。对于最终的关键点预测，我们提出了一个可学习的聚类层来将关键点提议分组为关键点预测。最终模型命名为LMPNet，它具有高度可解释性，因为它直接操作网络滤波器来检测预定义概念。我们的实验表明，LMPNet能够 (i) 自动发现对物体姿态鲁棒的语义关键点，并且 (ii) 实现了与监督姿态估计模型相当的强大预测精度。", "summary": "LMPNet是一种新的弱监督语义关键点发现方法，仅利用类别标签进行监督。它通过将判别训练的中间层滤波器转换为关键点检测器来实现，并引入了一种新颖的Leaky Max Pooling (LMP) 层来学习非重复局部模式。LMPNet还结合了选择策略和注意力遮蔽来确保一致性和注意力分布，并使用可学习的聚类层进行最终预测。实验证明，LMPNet能自动发现对物体姿态鲁棒的语义关键点，并达到与监督姿态估计模型相当的预测精度。", "keywords": "弱监督, 关键点发现, LMPNet, Leaky Max Pooling, 语义关键点", "comments": "LMPNet的创新点在于其提出的Leaky Max Pooling (LMP) 层，它能够显式地促使网络学习与关键点相关的“不可重复局部模式”。此外，该模型通过直接操作网络滤波器来检测预定义概念，这使其具有高度可解释性。在仅使用类别标签进行弱监督的情况下，LMPNet能够发现对物体姿态鲁棒的语义关键点，并达到与监督模型相当的性能，这显著降低了数据标注成本，是其重要性所在。"}}
{"id": "2507.02227", "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations", "authors": ["Xinquan Huang", "Paris Perdikaris"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02227v1", "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02227v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "PhysicsCorrect: 一种用于稳定神经偏微分方程模拟的免训练方法", "tldr": "PhysicsCorrect是一种免训练的校正框架，通过在每个预测步骤强制执行PDE一致性来解决神经偏微分方程模拟中的误差累积问题，显著减少了预测误差并保持了低推理时间。", "motivation": "神经网络作为偏微分方程（PDEs）的替代求解器虽然计算速度快，但在长期模拟中存在误差累积问题，导致结果与物理有效解完全偏离。", "method": "PhysicsCorrect是一种免训练的校正框架，通过将校正表述为基于PDE残差的线性化逆问题，在每个预测步骤强制执行PDE一致性。其关键创新是高效的缓存策略，通过离线预热阶段预计算雅可比矩阵及其伪逆，将计算开销降低了两个数量级。", "result": "在Navier-Stokes流体动力学、波动方程和Kuramoto-Sivashinsky方程这三个代表性PDE系统上，PhysicsCorrect将预测误差降低了高达100倍，同时增加的推理时间可忽略不计（低于5%）。该框架可与傅里叶神经算子、UNets和Vision Transformers等多种架构无缝集成。", "conclusion": "PhysicsCorrect有效地将不稳定的神经替代模型转化为可靠的模拟工具，弥合了深度学习的计算效率与实际科学应用所需的物理保真度之间的鸿沟。", "translation": "神经网络已成为求解偏微分方程（PDEs）的强大替代工具，与传统方法相比，它们提供了显著的计算加速。然而，这些模型存在一个关键的局限性：在长期模拟中存在误差累积，微小的误差会呈指数级复合，最终导致与物理有效解完全偏离。我们提出了PhysicsCorrect，一个免训练的校正框架，通过将校正表述为基于PDE残差的线性化逆问题，在每个预测步骤强制执行PDE一致性。我们的关键创新是一种高效的缓存策略，它在离线预热阶段预计算雅可比矩阵及其伪逆，与标准校正方法相比，计算开销降低了两个数量级。在三个代表性PDE系统——Navier-Stokes流体动力学、波动方程和混沌Kuramoto-Sivashinsky方程——上，PhysicsCorrect将预测误差降低了高达100倍，同时增加的推理时间可忽略不计（低于5%）。该框架与包括傅里叶神经算子、UNets和Vision Transformers在内的多种架构无缝集成，有效地将不稳定的神经替代模型转化为可靠的模拟工具，弥合了深度学习的计算效率与实际科学应用所需的物理保真度之间的鸿沟。", "summary": "该论文提出了PhysicsCorrect，一个免训练的框架，旨在解决神经网络在偏微分方程（PDEs）模拟中长期误差累积导致结果发散的问题。PhysicsCorrect通过将校正公式化为基于PDE残差的线性化逆问题，在每个预测步骤强制执行物理一致性。其核心创新在于高效的雅可比矩阵预计算缓存策略，大幅降低了计算开销。实验结果表明，PhysicsCorrect在多种PDE系统上能将预测误差降低高达100倍，同时仅增加微不足道的推理时间，从而使不稳定的神经模型成为可靠的物理模拟工具。", "keywords": "神经偏微分方程, 误差校正, 免训练, 物理一致性, 稳定性", "comments": "PhysicsCorrect的创新之处在于提出了一种免训练的、基于物理一致性校正的方法，有效解决了神经PDE模拟中的误差累积问题。其高效的缓存策略显著降低了计算开销，使其在实际应用中更具可行性。该方法能够与多种现有神经架构无缝集成，极大地提升了神经替代模型的可靠性和物理保真度，对于将深度学习应用于科学计算领域具有重要意义。"}}
{"id": "2507.02649", "title": "Restricted Quasiconvexity Isometry Property for Symmetric $α$-Stable Random Matrices", "authors": ["Sunder Ram Krishnan"], "categories": ["math.PR", "eess.SP"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.02649v1", "summary": "We formulate a generalization of the Restricted Isometry Property (RIP)\nreferred to as the Restricted Quasiconvexity Isometry Property (RQIP) for alpha\nstable random projections with $0<\\alpha<1$. A lower bound on the number of\nrows for RQIP to hold for random matrices whose entries are drawn from a\nsymmetric $\\alpha$-stable ($S\\alpha S$) distribution is derived. The proof\nleverages two key components: a concentration inequality for empirical\nfractional moments of $S\\alpha S$ variables and a covering number bound for\nsparse $\\ell_\\alpha$ balls. The resulting sample complexity reflects the\npolynomial tail behavior of the concentration and reinforces an observation\nmade in the literature that the RIP framework may have to be replaced with\nother sparse recovery formulations in practice, such as those based on the null\nspace property.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.02649v1", "cate": "math.PR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "对称 $\\alpha$-稳定随机矩阵的受限拟凸等距性质", "tldr": "本文提出了受限拟凸等距性质（RQIP），这是受限等距性质（RIP）的推广，适用于 $0<\\alpha<1$ 的 $\\alpha$ 稳定随机投影。研究推导了RQIP成立所需的行数下界，并指出RIP框架可能需要被其他稀疏恢复方法取代。", "motivation": "为了推广受限等距性质（RIP）以适应 $0<\\alpha<1$ 的 $\\alpha$ 稳定随机投影，并解决现有RIP框架在实践中可能需要被其他稀疏恢复公式取代的问题。", "method": "本文提出了受限拟凸等距性质（RQIP）。通过利用 $S\\alpha S$ 变量经验分数矩的集中不等式和稀疏 $\\ell_\\alpha$ 球的覆盖数界，推导了使RQIP成立所需的行数下界。", "result": "推导出了对称 $\\alpha$-稳定 ($S\\alpha S$) 随机矩阵满足RQIP所需的行数下界。得到的样本复杂度反映了集中度的多项式尾部行为。", "conclusion": "研究结果强化了文献中的一个观点，即在实践中，受限等距性质（RIP）框架可能需要被其他稀疏恢复公式取代，例如基于零空间性质的公式。", "translation": "我们为 $0<\\alpha<1$ 的 $\\alpha$ 稳定随机投影，提出了一种受限等距性质（RIP）的推广，称之为受限拟凸等距性质（RQIP）。本文推导了使条目来自对称 $\\alpha$-稳定 ($S\\alpha S$) 分布的随机矩阵满足RQIP所需的行数下界。证明利用了两个关键组成部分：$S\\alpha S$ 变量经验分数矩的集中不等式和稀疏 $\\ell_\\alpha$ 球的覆盖数界。由此产生的样本复杂度反映了集中度的多项式尾部行为，并强化了文献中观察到的一个观点，即在实践中，RIP框架可能需要被其他稀疏恢复公式取代，例如那些基于零空间性质的公式。", "summary": "本文提出了一种新的概念——受限拟凸等距性质（RQIP），作为受限等距性质（RIP）的推广，专门用于 $0<\\alpha<1$ 的 $\\alpha$ 稳定随机投影。研究推导了对称 $\\alpha$-稳定随机矩阵满足RQIP所需的行数下界，其证明结合了 $S\\alpha S$ 变量的集中不等式和稀疏 $\\ell_\\alpha$ 球的覆盖数界。研究结果表明，样本复杂度与多项式尾部行为相关，并支持了在稀疏恢复实践中可能需要用其他方法（如基于零空间性质的方法）替代RIP框架的观点。", "keywords": "受限拟凸等距性质, 对称$\\alpha$-稳定随机矩阵, 稀疏恢复, 集中不等式, 样本复杂度", "comments": "本文的创新点在于提出了RQIP这一RIP的推广，特别适用于$\\alpha$-稳定随机矩阵，这对于处理具有重尾分布的数据具有重要意义。其通过利用特定集中不等式和覆盖数界来推导理论下界的方法是严谨的。此外，该研究强调了RIP框架的局限性，并提出了替代方案的必要性，对稀疏恢复领域的未来研究方向提供了宝贵的见解。"}}
{"id": "2507.02778", "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs", "authors": ["Ken Tsui"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      31 pages, 18 figures", "url": "http://arxiv.org/abs/2507.02778v1", "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.", "comment": "31 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2507.02778v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "自我纠正基准：揭示并解决大型语言模型中的自我纠正盲点", "tldr": "大型语言模型（LLMs）存在“自我纠正盲点”，即无法纠正自身输出中的错误。研究引入Self-Correction Bench系统性测量此现象，发现平均64.5%的盲点率。通过简单添加“Wait”指令可显著减少89.3%的盲点，表明纠正能力存在但需激活。", "motivation": "尽管大型语言模型（LLMs）具有变革性，但它们仍然会犯错误并可能探索低效的推理路径。自我纠正对于可信赖的LLM至关重要，然而LLMs在纠正自身输出中的相同错误时表现出系统性的“自我纠正盲点”。", "method": "引入Self-Correction Bench，一个系统性框架，通过在三个复杂级别注入受控错误来测量LLMs的自我纠正盲点现象。对14个模型进行了测试。", "result": "发现平均64.5%的自我纠正盲点率。多项证据表明此局限性与训练数据构成有关，人类训练演示主要显示无错误响应，而RL训练模型通过结果反馈学习纠错。值得注意的是，简单地在输出前附加“Wait”可以将盲点减少89.3%。", "conclusion": "当前LLMs存在一个关键限制，即自我纠正盲点，但其纠正能力可能存在但需要激活。本工作为提高LLMs的可靠性和可信赖性提供了潜在途径。", "translation": "尽管大型语言模型（LLMs）已变得具有变革性，但它们仍然会犯错误并可能探索低效的推理路径。自我纠正是可信赖LLM的一项重要能力，特别是对于自回归LLM。虽然LLMs可以识别用户输入中的错误，但它们表现出一种系统性的“自我纠正盲点”——未能纠正自身输出中相同的错误。为了系统地研究这种现象，我们引入了Self-Correction Bench，一个通过在三个复杂级别注入受控错误来测量这种现象的系统性框架。我们测试了14个模型，发现平均盲点率为64.5%。我们发现多项证据表明这种局限性与训练数据构成有关：人类训练演示主要显示无错误响应，而不是错误纠正序列，这与通过结果反馈学习错误纠正的RL训练模型不同。值得注意的是，简单地附加“Wait”可以将盲点减少89.3%，这表明该能力存在但需要激活。我们的工作强调了当前LLMs的一个关键限制，并为提高其可靠性和可信赖性提供了潜在途径。", "summary": "本文研究了大型语言模型（LLMs）在自我纠正方面的“盲点”现象，即LLMs难以纠正自身输出中的错误。为系统性研究，作者引入了Self-Correction Bench基准，通过注入受控错误测试了14个模型，发现平均64.5%的盲点率。研究指出，此问题可能源于训练数据中缺乏纠错序列。令人惊讶的是，在LLM输出前简单添加“Wait”指令能显著降低89.3%的盲点，表明模型具备纠错能力但需要被激活。这项工作揭示了LLMs的一个关键局限性，并提出了提高其可靠性和可信赖性的有效途径。", "keywords": "大型语言模型, 自我纠正, 盲点, Self-Correction Bench, 错误纠正", "comments": "这项工作创新性地揭示了LLMs在自我纠正方面的一个系统性“盲点”，并提出了一个新颖的基准来量化这一现象。发现训练数据组成对这一盲点的重要性，并提出“Wait”这一简单而有效的激活策略，对于提高LLMs的可靠性和可信赖性具有重要意义和实用价值。"}}
{"id": "2507.02864", "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real", "authors": ["Renhao Wang", "Haoran Geng", "Tingle Li", "Feishi Wang", "Gopala Anumanchipalli", "Philipp Wu", "Trevor Darrell", "Boyi Li", "Pieter Abbeel", "Jitendra Malik", "Alexei A. Efros"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02864v1", "summary": "Robots must integrate multiple sensory modalities to act effectively in the\nreal world. Yet, learning such multimodal policies at scale remains\nchallenging. Simulation offers a viable solution, but while vision has\nbenefited from high-fidelity simulators, other modalities (e.g. sound) can be\nnotoriously difficult to simulate. As a result, sim-to-real transfer has\nsucceeded primarily in vision-based tasks, with multimodal transfer still\nlargely unrealized. In this work, we tackle these challenges by introducing\nMultiGen, a framework that integrates large-scale generative models into\ntraditional physics simulators, enabling multisensory simulation. We showcase\nour framework on the dynamic task of robot pouring, which inherently relies on\nmultimodal feedback. By synthesizing realistic audio conditioned on simulation\nvideo, our method enables training on rich audiovisual trajectories -- without\nany real robot data. We demonstrate effective zero-shot transfer to real-world\npouring with novel containers and liquids, highlighting the potential of\ngenerative modeling to both simulate hard-to-model modalities and close the\nmultimodal sim-to-real gap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02864v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MultiGen：在仿真中使用多模态生成来学习真实世界的多模态策略", "tldr": "MultiGen将生成模型集成到仿真中，以解决多模态模拟的挑战，从而实现多模态策略的零样本迁移。", "motivation": "机器人需要在真实世界中整合多种感官模态才能有效行动，但大规模学习多模态策略仍然具有挑战性。虽然视觉模态受益于高保真模拟器，但其他模态（如声音）难以模拟，导致多模态的模拟到现实迁移尚未实现。", "method": "本文引入了MultiGen框架，该框架将大规模生成模型集成到传统物理模拟器中，从而实现多感官模拟。通过根据模拟视频合成逼真的音频，MultiGen使得能够在丰富的视听轨迹上进行训练，而无需任何真实机器人数据。", "result": "MultiGen在机器人倾倒的动态任务中展示了其有效性，实现了对具有新颖容器和液体的真实世界倾倒任务的有效零样本迁移。", "conclusion": "生成建模有潜力模拟难以建模的模态，并弥合多模态模拟到现实的差距。", "translation": "机器人必须整合多种感官模态才能在真实世界中有效行动。然而，大规模学习此类多模态策略仍然具有挑战性。模拟提供了一个可行的解决方案，但尽管视觉受益于高保真模拟器，其他模态（例如声音）可能难以模拟。因此，模拟到现实的迁移主要在基于视觉的任务中取得成功，而多模态迁移仍未实现。在这项工作中，我们通过引入MultiGen来应对这些挑战，MultiGen是一个将大规模生成模型集成到传统物理模拟器中的框架，从而实现多感官模拟。我们在机器人倾倒的动态任务中展示了我们的框架，该任务本质上依赖于多模态反馈。通过根据模拟视频合成逼真的音频，我们的方法使得能够在丰富的视听轨迹上进行训练——无需任何真实机器人数据。我们展示了对新颖容器和液体的真实世界倾倒任务的有效零样本迁移，突出了生成建模在模拟难以建模的模态和弥合多模态模拟到现实差距方面的潜力。", "summary": "MultiGen是一个新颖的框架，它将大规模生成模型集成到物理模拟器中，以解决多模态模拟的挑战，特别是对于难以模拟的模态如声音。该方法通过合成逼真的音频来训练机器人多模态策略，无需真实世界数据。通过在机器人倾倒任务中的应用，MultiGen展示了其在零样本迁移到真实世界的能力，证明了生成模型在弥合多模态模拟到现实差距方面的潜力。", "keywords": "多模态生成, 模拟到现实, 机器人学习, 生成模型, 感官模拟", "comments": "MultiGen的创新之处在于其将大规模生成模型与传统物理模拟器相结合，以解决多模态模拟中的关键挑战，特别是对于难以模拟的感官（如声音）。这使得在没有真实机器人数据的情况下训练多模态策略成为可能，为机器人学习领域开辟了新的途径，并有望显著缩小多模态模拟到现实的差距。"}}
{"id": "2507.01968", "title": "Optimising task allocation to balance business goals and worker well-being for financial service workforces", "authors": ["Chris Duckworth", "Zlatko Zlatev", "James Sciberras", "Peter Hallett", "Enrico Gerding"], "categories": ["q-fin.GN", "cs.HC"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "Comments:      Accepted in Journal of Modelling in Management", "url": "http://arxiv.org/abs/2507.01968v1", "summary": "Purpose: Financial service companies manage huge volumes of data which\nrequires timely error identification and resolution. The associated tasks to\nresolve these errors frequently put financial analyst workforces under\nsignificant pressure leading to resourcing challenges and increased business\nrisk. To address this challenge, we introduce a formal task allocation model\nwhich considers both business orientated goals and analyst well-being.\n  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to\nallocate and schedule tasks to analysts. The proposed solution is able to\nallocate tasks to analysts with appropriate skills and experience, while taking\ninto account staff well-being objectives.\n  Findings: We demonstrate our GA model outperforms baseline heuristics,\ncurrent working practice, and is applicable to a range of single and\nmulti-objective real-world scenarios. We discuss the potential for\nmetaheuristics (such as GAs) to efficiently find sufficiently good allocations\nwhich can provide recommendations for financial service managers in-the-loop.\n  Originality: A key gap in existing allocation and scheduling models, is fully\nconsidering worker well-being. This paper presents an allocation model which\nexplicitly optimises for well-being while still improving on current working\npractice for efficiency.", "comment": "Accepted in Journal of Modelling in Management", "pdf_url": "http://arxiv.org/pdf/2507.01968v1", "cate": "q-fin.GN", "date": "2025-06-18", "updated": "2025-06-18", "AI": {"title_translation": "优化任务分配以平衡金融服务员工的业务目标和员工福祉", "tldr": "本文提出了一种使用遗传算法的任务分配模型，旨在平衡金融服务公司的业务目标和员工福祉，优于现有方法。", "motivation": "金融服务公司管理着海量数据，需要及时识别和解决错误。解决这些错误的相关任务经常给金融分析师团队带来巨大压力，导致资源挑战和业务风险增加。为解决此挑战，本文引入了一个正式的任务分配模型，该模型同时考虑了以业务为导向的目标和分析师的福祉。", "method": "使用遗传算法（GA）优化所提出的正式模型，以向分析师分配和调度任务。该解决方案能够将任务分配给具有适当技能和经验的分析师，同时考虑到员工福祉目标。", "result": "遗传算法模型优于基线启发式算法和当前工作实践，并适用于一系列单目标和多目标的现实场景。元启发式算法（如遗传算法）可以有效地找到足够好的分配方案，为金融服务经理提供建议。", "conclusion": "本文提出了一个显式优化员工福祉，同时提高效率的任务分配模型，填补了现有分配和调度模型中未充分考虑员工福祉的关键空白。", "translation": "目的：金融服务公司管理着海量数据，需要及时识别和解决错误。解决这些错误的相关任务经常给金融分析师团队带来巨大压力，导致资源挑战和业务风险增加。为了解决这一挑战，我们引入了一个正式的任务分配模型，该模型同时考虑了以业务为导向的目标和分析师的福祉。 方法：我们使用遗传算法（GA）来优化我们的正式模型，以将任务分配和调度给分析师。所提出的解决方案能够将任务分配给具有适当技能和经验的分析师，同时考虑到员工福祉目标。 发现：我们证明了我们的遗传算法模型优于基线启发式算法和当前的工作实践，并且适用于一系列单目标和多目标的现实场景。我们讨论了元启发式算法（如遗传算法）有效找到足够好的分配方案的潜力，这些方案可以为在循环中的金融服务经理提供建议。 独创性：现有分配和调度模型中的一个关键空白是未能充分考虑员工福祉。本文提出了一个分配模型，该模型明确地优化了员工福祉，同时仍提高了当前工作实践的效率。", "summary": "本文针对金融服务行业中任务分配对员工福祉和业务效率的影响，提出了一种基于遗传算法的正式任务分配模型。该模型旨在平衡业务目标和分析师福祉，能够将任务有效分配给具备相应技能的员工，并考虑其福祉目标。研究结果表明，该遗传算法模型在实际应用中表现优于现有基线方法，并能为金融服务经理提供优化任务分配的建议，填补了现有模型中对员工福祉考虑不足的空白。", "keywords": "任务分配, 员工福祉, 遗传算法, 金融服务, 优化", "comments": "本文的创新之处在于其首次明确地将员工福祉纳入任务分配的优化目标，这在现有任务分配和调度模型中是一个显著的空白。通过使用遗传算法，该模型不仅提高了任务分配的效率，还关注了员工的健康和满意度，具有重要的实践意义，尤其是在高压的金融服务行业。"}}
{"id": "2507.02311", "title": "Perception Activator: An intuitive and portable framework for brain cognitive exploration", "authors": ["Le Xu", "Qi Zhang", "Qixian Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02311v1", "summary": "Recent advances in brain-vision decoding have driven significant progress,\nreconstructing with high fidelity perceived visual stimuli from neural\nactivity, e.g., functional magnetic resonance imaging (fMRI), in the human\nvisual cortex. Most existing methods decode the brain signal using a two-level\nstrategy, i.e., pixel-level and semantic-level. However, these methods rely\nheavily on low-level pixel alignment yet lack sufficient and fine-grained\nsemantic alignment, resulting in obvious reconstruction distortions of multiple\nsemantic objects. To better understand the brain's visual perception patterns\nand how current decoding models process semantic objects, we have developed an\nexperimental framework that uses fMRI representations as intervention\nconditions. By injecting these representations into multi-scale image features\nvia cross-attention, we compare both downstream performance and intermediate\nfeature changes on object detection and instance segmentation tasks with and\nwithout fMRI information. Our results demonstrate that incorporating fMRI\nsignals enhances the accuracy of downstream detection and segmentation,\nconfirming that fMRI contains rich multi-object semantic cues and coarse\nspatial localization information-elements that current models have yet to fully\nexploit or integrate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02311v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "感知激活器：一个直观便携的脑认知探索框架", "tldr": "当前的脑-视觉解码方法在语义对齐方面存在不足。本文提出了“感知激活器”，一个基于fMRI的框架，通过注入fMRI信号来改善目标检测和分割，表明fMRI包含丰富的语义和空间信息。", "motivation": "现有脑-视觉解码方法（像素级和语义级）严重依赖低级像素对齐，但缺乏足够精细的语义对齐，导致多个语义对象的重建失真。作者旨在更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象。", "method": "开发了一个名为“感知激活器”的实验框架。该框架将fMRI表征作为干预条件，通过交叉注意力将其注入多尺度图像特征中。他们比较了在有无fMRI信息的情况下，目标检测和实例分割任务的下游性能和中间特征变化。", "result": "整合fMRI信号提高了下游检测和分割的准确性。这证实了fMRI包含丰富的多对象语义线索和粗略的空间定位信息。", "conclusion": "fMRI信号包含有价值的多对象语义和空间定位信息，而当前的脑-视觉解码模型尚未充分利用这些信息。将这些信号整合可以显著提高目标检测和分割等解码任务的性能。", "translation": "脑-视觉解码的最新进展推动了显著的进步，能够从人类视觉皮层的神经活动（例如，功能性磁共振成像fMRI）中高保真地重建感知的视觉刺激。大多数现有方法采用两级策略解码脑信号，即像素级和语义级。然而，这些方法严重依赖低级像素对齐，但缺乏足够和细粒度的语义对齐，导致多个语义对象的重建出现明显的失真。为了更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象，我们开发了一个实验框架，该框架使用fMRI表征作为干预条件。通过交叉注意力将这些表征注入多尺度图像特征中，我们比较了在有无fMRI信息的情况下，目标检测和实例分割任务的下游性能和中间特征变化。我们的结果表明，整合fMRI信号提高了下游检测和分割的准确性，证实了fMRI包含丰富的多对象语义线索和粗略的空间定位信息——这些元素是当前模型尚未完全利用或整合的。", "summary": "“感知激活器”通过将fMRI表征整合到多尺度图像特征中，解决了脑-视觉解码中语义对齐的局限性。这个新颖的框架利用交叉注意力，表明fMRI信号能够提高目标检测和实例分割的准确性，揭示了其丰富的多对象语义和粗略的空间定位信息，这些信息是当前模型未充分利用的。", "keywords": "脑-视觉解码, fMRI, 语义对齐, 目标检测, 实例分割", "comments": "该论文的创新之处在于利用fMRI表征作为图像特征的直接干预条件，以增强下游任务，而不仅仅用于重建。这提供了一种探索大脑视觉感知的新颖方式，并展示了fMRI在语义和空间信息方面尚未开发的潜力。它为改进脑-视觉解码指明了一条清晰的路径。"}}
{"id": "2507.02636", "title": "Online Convex Optimization for Coordinated Long-Term and Short-Term Isolated Microgrid Dispatch", "authors": ["Ning Qi", "Yousuf Baker", "Bolun Xu"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02636v1", "summary": "This paper proposes a novel non-anticipatory long-short-term coordinated\ndispatch framework for isolated microgrid with hybrid short-long-duration\nenergy storages (LDES). We introduce a convex hull approximation model for\nnonconvex LDES electrochemical dynamics, facilitating computational\ntractability and accuracy. To address temporal coupling in SoC dynamics and\nlong-term contracts, we generate hindsight-optimal state-of-charge (SoC)\ntrajectories of LDES and netloads for offline training. In the online stage, we\nemploy kernel regression to dynamically update the SoC reference and propose an\nadaptive online convex optimization (OCO) algorithm with SoC reference tracking\nand expert tracking to mitigate myopia and enable adaptive step-size\noptimization. We rigorously prove that both long-term and short-term policies\nachieve sublinear regret bounds over time, which improves with more regression\nscenarios, stronger tracking penalties, and finer convex approximations.\nSimulation results show that the proposed method outperforms state-of-the-art\nmethods, reducing costs by 73.4%, eliminating load loss via reference tracking,\nand achieving an additional 2.4% cost saving via the OCO algorithm. These\nbenefits scale up with longer LDES durations, and the method demonstrates\nresilience to poor forecasts and unexpected system faults.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02636v1", "cate": "math.OC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "孤立微电网协调长期和短期调度的在线凸优化", "tldr": "本文提出了一种针对混合长短期储能孤立微电网的在线凸优化调度框架，通过新颖的建模和算法显著降低了成本并提高了鲁棒性。", "motivation": "解决孤立微电网中长短期能量存储的协调调度问题，特别是处理非凸的LDES电化学动力学以及SoC动态和长期合同中的时间耦合。", "method": "本文提出了一种新颖的非预测性长短期协调调度框架，用于具有混合短长期储能（LDES）的孤立微电网。引入了针对非凸LDES电化学动力学的凸包近似模型，以提高计算可行性和准确性。为解决SoC动态和长期合同中的时间耦合，生成了LDES和净负荷的事后最优SoC轨迹用于离线训练。在线阶段，采用核回归动态更新SoC参考，并提出了一种自适应在线凸优化（OCO）算法，结合SoC参考跟踪和专家跟踪，以缓解近视效应并实现自适应步长优化。", "result": "长期和短期策略都实现了随时间推移的次线性遗憾界限，且其性能随着回归场景的增加、跟踪惩罚的加强和凸近似的细化而提高。模拟结果显示，所提出的方法优于现有最先进的方法，成本降低了73.4%，通过参考跟踪消除了负荷损失，并通过OCO算法额外节省了2.4%的成本。这些优势随着LDES持续时间的增加而扩大，并且该方法对不良预测和意外系统故障表现出弹性。", "conclusion": "所提出的在线凸优化调度方法能够有效、鲁棒地协调孤立微电网中的长短期储能，显著降低运营成本并提高系统可靠性。", "translation": "本文提出了一种新颖的非预测性长短期协调调度框架，用于具有混合短长期储能（LDES）的孤立微电网。我们引入了一个针对非凸LDES电化学动力学的凸包近似模型，以提高计算可行性和准确性。为了解决荷电状态（SoC）动态和长期合同中的时间耦合问题，我们生成了LDES和净负荷的事后最优SoC轨迹用于离线训练。在在线阶段，我们采用核回归动态更新SoC参考，并提出了一种自适应在线凸优化（OCO）算法，结合SoC参考跟踪和专家跟踪，以缓解近视效应并实现自适应步长优化。我们严格证明了长期和短期策略都实现了随时间推移的次线性遗憾界限，其性能随着回归场景的增加、跟踪惩罚的加强和凸近似的细化而提高。仿真结果表明，所提出的方法优于现有最先进的方法，成本降低了73.4%，通过参考跟踪消除了负荷损失，并通过OCO算法额外节省了2.4%的成本。这些优势随着LDES持续时间的增加而扩大，并且该方法对不良预测和意外系统故障表现出弹性。", "summary": "本文提出了一种用于孤立微电网中混合长短期储能的非预测性协调调度框架。该框架通过引入凸包近似模型处理非凸LDES动力学，并通过离线训练生成事后最优SoC轨迹。在线阶段，采用核回归动态更新SoC参考，并提出了一种自适应在线凸优化（OCO）算法，结合SoC和专家跟踪以实现自适应步长优化。理论证明该方法实现了次线性遗憾界限，仿真结果显示其在成本降低、消除负荷损失和应对不确定性方面显著优于现有方法。", "keywords": "孤立微电网, 在线凸优化, 能量存储, 调度, 凸包近似", "comments": "本文的创新点在于结合了凸包近似、核回归和自适应在线凸优化算法，实现了对非凸LDES动态和长期短期时间耦合的有效处理。其提出的OCO算法通过参考跟踪和专家跟踪，有效缓解了在线优化中的近视问题，并实现了成本的大幅降低和对不确定性的鲁棒性，具有重要的实际应用价值。"}}
{"id": "2507.02241", "title": "VERBA: Verbalizing Model Differences Using Large Language Models", "authors": ["Shravan Doda", "Shashidhar Reddy Javaji", "Zining Zhu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02241v1", "summary": "In the current machine learning landscape, we face a \"model lake\" phenomenon:\nGiven a task, there is a proliferation of trained models with similar\nperformances despite different behavior. For model users attempting to navigate\nand select from the models, documentation comparing model pairs is helpful.\nHowever, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a\nnumber prohibitive for the model developers to manually perform pairwise\ncomparisons and prepare documentations. To facilitate fine-grained pairwise\ncomparisons among models, we introduced $\\textbf{VERBA}$. Our approach\nleverages a large language model (LLM) to generate verbalizations of model\ndifferences by sampling from the two models. We established a protocol that\nevaluates the informativeness of the verbalizations via simulation. We also\nassembled a suite with a diverse set of commonly used machine learning models\nas a benchmark. For a pair of decision tree models with up to 5% performance\ndifference but 20-25% behavioral differences, $\\textbf{VERBA}$ effectively\nverbalizes their variations with up to 80% overall accuracy. When we included\nthe models' structural information, the verbalization's accuracy further\nimproved to 90%. $\\textbf{VERBA}$ opens up new research avenues for improving\nthe transparency and comparability of machine learning models in a post-hoc\nmanner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02241v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "VERBA：使用大型语言模型口头表达模型差异", "tldr": "VERBA利用大型语言模型自动生成机器学习模型之间的差异描述，以解决手动比较大量模型时的效率问题，并在实验中取得了高准确率。", "motivation": "当前机器学习领域存在“模型湖”现象，即针对同一任务有大量性能相似但行为不同的模型。模型用户难以从中选择，而手动进行O(N^2)的模型对比较和文档准备对于开发者来说是难以承受的。", "method": "本研究引入了VERBA，该方法利用大型语言模型（LLM）通过从两个模型中采样来生成模型差异的口头表达。研究还建立了一个协议，通过模拟评估口头表达的信息量，并构建了一个包含多种常用机器学习模型的基准套件。当包含模型的结构信息时，准确性进一步提高。", "result": "对于一对决策树模型，即使性能差异仅为5%但行为差异达到20-25%，VERBA也能有效地口头表达其变异，总体准确率高达80%。当包含模型的结构信息时，口头表达的准确率进一步提高到90%。", "conclusion": "VERBA为改进机器学习模型的透明度和可比性开辟了新的研究途径。", "translation": "在当前的机器学习领域，我们面临着“模型湖”现象：给定一个任务，存在大量性能相似但行为不同的已训练模型。对于试图在这些模型中导航和选择的模型用户而言，比较模型对的文档非常有帮助。然而，对于每N个模型，可能存在O(N^2)个成对比较，这个数量对于模型开发者手动执行成对比较和准备文档来说是难以承受的。为了促进模型之间细粒度的成对比较，我们引入了VERBA。我们的方法利用大型语言模型（LLM）通过从两个模型中采样来生成模型差异的口头表达。我们建立了一个协议，通过模拟评估口头表达的信息量。我们还组建了一个包含各种常用机器学习模型的套件作为基准。对于一对决策树模型，即使性能差异高达5%但行为差异为20-25%，VERBA也能有效地口头表达其变异，总体准确率高达80%。当我们包含模型的结构信息时，口头表达的准确率进一步提高到90%。VERBA为改进机器学习模型的事后透明度和可比性开辟了新的研究途径。", "summary": "VERBA提出了一种利用大型语言模型（LLM）自动识别并口头表达机器学习模型之间差异的方法。针对当前“模型湖”现象中模型对比效率低下的问题，VERBA通过对模型进行采样并结合结构信息，能够以高准确率（最高达90%）生成模型行为差异的描述，显著降低了手动比较的成本，为提升模型透明度和可比性提供了新途径。", "keywords": "大型语言模型, 模型差异, 模型透明度, 模型比较, 机器学习", "comments": "这项研究的创新之处在于利用大型语言模型来自动化模型差异的口头化过程，有效解决了模型“湖”现象中O(N^2)量级手动比较的扩展性挑战。它通过提供可理解的差异描述，显著提升了机器学习模型的透明度和可比性，对于模型选择和部署具有重要意义。该方法不仅提出了一个可行的解决方案，还通过模拟评估验证了其有效性，并展示了结合结构信息对准确率的提升作用。"}}
{"id": "2507.02799", "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models", "authors": ["Riccardo Cantini", "Nicola Gabriele", "Alessio Orsino", "Domenico Talia"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02799v1", "summary": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02799v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "推理就是你所需要的一切吗？探究推理语言模型时代的偏见", "tldr": "推理语言模型（RLMs）在处理复杂任务时表现出色，但本研究发现，具有显式推理能力的RLMs（如CoT或微调推理）反而可能比基础模型更容易受到社会偏见的诱发，挑战了推理能提高鲁棒性的假设。", "motivation": "推理语言模型（RLMs）在复杂多步推理任务中表现出卓越能力，但其对社会偏见的鲁棒性影响尚不明确。本研究旨在探究RLMs在偏见诱发方面的对抗性鲁棒性。", "method": "研究利用CLEAR-Bias基准，系统评估了最先进的推理语言模型（RLMs）在不同社会文化维度上的对抗性鲁棒性。采用“LLM即评判者”方法进行自动化安全评分，并利用越狱技术评估内置安全机制的强度。研究旨在回答推理能力如何影响模型公平性和鲁棒性、为推理微调的模型是否比依赖CoT的模型更安全以及越狱攻击成功率如何随推理机制变化等问题。", "result": "研究发现推理能力与偏见安全之间存在微妙关系。具有显式推理能力的模型（无论是通过CoT提示还是微调推理）通常比没有此类机制的基础模型更容易受到偏见诱发，表明推理可能无意中为刻板印象强化开辟新途径。推理启用模型似乎比依赖CoT提示的模型更安全，后者特别容易受到通过讲故事提示、虚构角色或奖励塑造指令进行的上下文重构攻击。", "conclusion": "研究结果挑战了推理固有地提高鲁棒性的假设，并强调需要对推理设计采用更具偏见意识的方法。", "translation": "推理语言模型（RLM）因其通过思维链（CoT）提示或微调推理轨迹等机制执行复杂、多步推理任务的能力而受到关注。虽然这些能力有望提高可靠性，但它们对社会偏见鲁棒性的影响仍不清楚。在这项工作中，我们利用最初为大型语言模型（LLM）设计的CLEAR-Bias基准，调查RLM对偏见诱发的对抗性鲁棒性。我们系统地评估了跨越不同社会文化维度的最先进RLM，采用“LLM作为评判者”的方法进行自动化安全评分，并利用越狱技术评估内置安全机制的强度。我们的评估解决了三个关键问题：（i）推理能力的引入如何影响模型公平性和鲁棒性；（ii）为推理而微调的模型是否比在推理时依赖CoT提示的模型表现出更高的安全性；以及（iii）针对偏见诱发的越狱攻击成功率如何随所采用的推理机制而变化。我们的发现揭示了推理能力与偏见安全之间的一种微妙关系。令人惊讶的是，具有显式推理能力的模型，无论是通过CoT提示还是微调推理轨迹，通常比没有此类机制的基础模型更容易受到偏见诱发，这表明推理可能无意中为刻板印象强化开辟了新途径。启用推理的模型似乎比依赖CoT提示的模型更安全，后者特别容易受到通过讲故事提示、虚构角色或奖励塑造指令进行的上下文重构攻击。这些结果挑战了推理固有地提高鲁棒性的假设，并强调需要对推理设计采用更具偏见意识的方法。", "summary": "本研究利用CLEAR-Bias基准，深入探究了推理语言模型（RLMs）在处理社会偏见方面的鲁棒性。研究发现，与没有显式推理机制的基础模型相比，通过CoT提示或微调推理获得显式推理能力的RLMs反而更容易受到偏见诱发，尤其依赖CoT的模型易受上下文重构攻击。这一发现挑战了推理能力能天然提升模型鲁棒性的普遍认知，并强调了在未来推理模型设计中，必须更加重视偏见意识。", "keywords": "推理语言模型, 偏见, 鲁棒性, 思维链, 越狱攻击", "comments": "这项研究的创新之处在于系统性地揭示了推理能力与模型偏见鲁棒性之间反直觉的关系，对理解和开发更安全的AI模型具有重要指导意义。它挑战了现有关于推理能普遍提升模型安全性的假设，强调了在推理机制设计中融入偏见意识的紧迫性。"}}
{"id": "2507.02622", "title": "Access Control Threatened by Quantum Entanglement", "authors": ["Zhicheng Zhang", "Mingsheng Ying"], "categories": ["quant-ph", "cs.CR", "cs.OS"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      23 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02622v1", "summary": "Access control is a cornerstone of computer security that prevents\nunauthorised access to resources. In this paper, we study access control in\nquantum computer systems. We present the first explicit scenario of a security\nbreach when a classically secure access control system is straightforwardly\nadapted to the quantum setting. The breach is ultimately due to that quantum\nmechanics allows the phenomenon of entanglement and violates Mermin inequality,\na multi-party variant of the celebrated Bell inequality. This reveals a threat\nfrom quantum entanglement to access control if existing computer systems\nintegrate with quantum computing. To protect against such threat, we propose\nseveral new models of quantum access control, and rigorously analyse their\nsecurity, flexibility and efficiency.", "comment": "23 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02622v1", "cate": "quant-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "量子纠缠对访问控制的威胁", "tldr": "本文揭示了量子纠缠对传统访问控制系统的潜在安全威胁，并提出了新的量子访问控制模型以应对此威胁。", "motivation": "访问控制是计算机安全的基础，但当传统访问控制系统应用于量子计算环境时，量子纠缠可能导致安全漏洞。本文旨在识别并解决这一潜在威胁。", "method": "研究了一个具体的安全漏洞场景，该漏洞源于量子纠缠和对Mermin不等式的违反。为应对此威胁，提出了几种新的量子访问控制模型，并对其安全性、灵活性和效率进行了严格分析。", "result": "首次明确展示了当经典安全访问控制系统应用于量子环境时，由于量子纠缠和Mermin不等式的违反而导致的安全漏洞。同时，提出了几种新的量子访问控制模型。", "conclusion": "量子纠缠对现有计算机系统与量子计算集成后的访问控制构成威胁。为防范此威胁，需要采用新的量子访问控制模型。", "translation": "访问控制是计算机安全的基础，它防止对资源的未经授权访问。在本文中，我们研究了量子计算机系统中的访问控制。我们首次提出了一个明确的安全漏洞场景，当一个经典安全的访问控制系统被直接应用于量子环境时。该漏洞最终是由于量子力学允许纠缠现象并违反了Mermin不等式，这是著名的贝尔不等式的一个多方变体。这揭示了如果现有计算机系统与量子计算集成，量子纠缠将对访问控制构成威胁。为了防范这种威胁，我们提出了几种新的量子访问控制模型，并严格分析了它们的安全性、灵活性和效率。", "summary": "本文探讨了量子计算背景下的访问控制，并首次提出了一个具体的安全漏洞场景，该漏洞源于量子纠缠对传统访问控制的威胁。研究发现，当经典访问控制系统直接应用于量子环境时，量子纠缠可能导致安全漏洞。为应对此挑战，论文提出了多种新型量子访问控制模型，并对其安全、灵活性和效率进行了详细分析，旨在保护系统免受量子纠缠带来的潜在威胁。", "keywords": "量子纠缠, 访问控制, 量子安全, 安全漏洞, Mermin不等式", "comments": "本文创新性地指出了量子纠缠对传统访问控制系统的潜在威胁，这是未来量子安全领域的重要研究方向。其提出的新量子访问控制模型为应对这一新兴挑战提供了初步解决方案，对于保障未来量子集成系统的安全具有重要意义。"}}
{"id": "2507.02256", "title": "Uncertainty-aware Reward Design Process", "authors": ["Yang Yang", "Xiaolu Zhou", "Bosong Ding", "Miao Xin"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      34 pages, 9 figures", "url": "http://arxiv.org/abs/2507.02256v1", "summary": "Designing effective reward functions is a cornerstone of reinforcement\nlearning (RL), yet it remains a challenging process due to the inefficiencies\nand inconsistencies inherent in conventional reward engineering methodologies.\nRecent advances have explored leveraging large language models (LLMs) to\nautomate reward function design. However, their suboptimal performance in\nnumerical optimization often yields unsatisfactory reward quality, while the\nevolutionary search paradigm demonstrates inefficient utilization of simulation\nresources, resulting in prohibitively lengthy design cycles with\ndisproportionate computational overhead. To address these challenges, we\npropose the Uncertainty-aware Reward Design Process (URDP), a novel framework\nthat integrates large language models to streamline reward function design and\nevaluation in RL environments. URDP quantifies candidate reward function\nuncertainty based on self-consistency analysis, enabling simulation-free\nidentification of ineffective reward components while discovering novel reward\ncomponents. Furthermore, we introduce uncertainty-aware Bayesian optimization\n(UABO), which incorporates uncertainty estimation to significantly enhance\nhyperparameter configuration efficiency. Finally, we construct a bi-level\noptimization architecture by decoupling the reward component optimization and\nthe hyperparameter tuning. URDP orchestrates synergistic collaboration between\nthe reward logic reasoning of the LLMs and the numerical optimization strengths\nof the Bayesian Optimization. We conduct a comprehensive evaluation of URDP\nacross 35 diverse tasks spanning three benchmark environments. Our experimental\nresults demonstrate that URDP not only generates higher-quality reward\nfunctions but also achieves significant improvements in the efficiency of\nautomated reward design compared to existing approaches.", "comment": "34 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.02256v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "不确定性感知奖励设计过程", "tldr": "提出URDP框架，结合LLM和贝叶斯优化，通过不确定性感知来高效设计高质量强化学习奖励函数。", "motivation": "现有强化学习奖励函数设计方法效率低下、不一致；大型语言模型（LLM）自动化设计奖励函数性能不佳；进化搜索范式模拟资源利用效率低，设计周期长，计算开销大。", "method": "本文提出了不确定性感知奖励设计过程（URDP）框架，旨在解决强化学习中奖励函数设计效率和质量的挑战。URDP整合大型语言模型（LLM）以简化奖励函数设计和评估。它通过自洽性分析量化候选奖励函数的不确定性，实现无模拟识别无效奖励组件并发现新组件。此外，URDP引入了不确定性感知贝叶斯优化（UABO），结合不确定性估计显著提高超参数配置效率。最后，通过解耦奖励组件优化和超参数调优，构建了一个双层优化架构，协同LLM的奖励逻辑推理和贝叶斯优化的数值优化优势。", "result": "在35个不同任务和3个基准环境中对URDP进行了全面评估。实验结果表明，URDP不仅生成了更高质量的奖励函数，而且与现有方法相比，显著提高了自动化奖励设计的效率。", "conclusion": "URDP框架通过整合大型语言模型和不确定性感知贝叶斯优化，有效解决了强化学习中奖励函数设计效率和质量的挑战，实现了奖励函数设计的高效自动化。", "translation": "设计有效的奖励函数是强化学习（RL）的基石，但由于传统奖励工程方法固有的低效率和不一致性，这仍然是一个具有挑战性的过程。最近的研究探索了利用大型语言模型（LLM）自动化奖励函数设计。然而，它们在数值优化方面的次优性能常常导致奖励质量不尽如人意，而进化搜索范式则表现出模拟资源利用效率低下，导致设计周期过长且计算开销过大。为了解决这些挑战，我们提出了不确定性感知奖励设计过程（URDP），这是一个新颖的框架，它整合了大型语言模型，以简化RL环境中的奖励函数设计和评估。URDP基于自洽性分析量化候选奖励函数的不确定性，从而能够无模拟地识别无效奖励组件，同时发现新颖的奖励组件。此外，我们引入了不确定性感知贝叶斯优化（UABO），它结合了不确定性估计，显著提高了超参数配置效率。最后，我们通过解耦奖励组件优化和超参数调优，构建了一个双层优化架构。URDP协调了LLM的奖励逻辑推理能力和贝叶斯优化的数值优化优势。我们在跨越三个基准环境的35个不同任务上对URDP进行了全面评估。我们的实验结果表明，URDP不仅生成了更高质量的奖励函数，而且与现有方法相比，显著提高了自动化奖励设计的效率。", "summary": "本文提出不确定性感知奖励设计过程（URDP），一个新颖的框架，旨在解决强化学习中奖励函数设计效率低和质量差的问题。URDP整合大型语言模型和不确定性感知贝士优化，通过自洽性分析识别无效奖励组件并发现新组件，同时通过双层优化架构协同LLM的逻辑推理和贝叶斯优化的数值优化优势。实验证明URDP在多个任务上能生成更高质量的奖励函数并显著提高设计效率。", "keywords": "强化学习, 奖励设计, 大型语言模型, 贝叶斯优化, 不确定性感知", "comments": "本文的创新点在于将大型语言模型与贝叶斯优化相结合，并通过引入不确定性感知机制来优化强化学习中的奖励函数设计。特别是通过自洽性分析实现无模拟识别无效组件，以及解耦优化架构，有效解决了现有LLM和进化搜索方法的局限性。这对于自动化RL系统和提高奖励工程效率具有重要意义。"}}
{"id": "2507.02314", "title": "MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation", "authors": ["JaeHyuck Choi", "MinJun Kim", "JeHyeong Hong"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02314v1", "summary": "Few-shot anomaly generation is emerging as a practical solution for\naugmenting the scarce anomaly data in industrial quality control settings. An\nideal generator would meet three demands at once, namely (i) keep the normal\nbackground intact, (ii) inpaint anomalous regions to tightly overlap with the\ncorresponding anomaly masks, and (iii) generate anomalous regions in a\nsemantically valid location, while still producing realistic, diverse\nappearances from only a handful of real examples. Existing diffusion-based\nmethods usually satisfy at most two of these requirements: global anomaly\ngenerators corrupt the background, whereas mask-guided ones often falter when\nthe mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting\nwith multi-level perturbations and Context-aware alignment--to resolve all\nthree issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting\nbackbone that preserves normal regions and ensures strict adherence of the\nsynthesized anomaly to the supplied mask, directly addressing background\ncorruption and misalignment. To offset the diversity loss that fine-tuning can\ncause, MAGIC adds two complementary perturbation strategies: (i) Gaussian\nprompt-level perturbation applied during fine-tuning and inference that\nbroadens the global appearance of anomalies while avoiding low-fidelity textual\nappearances, and (ii) mask-guided spatial noise injection that enriches local\ntexture variations. Additionally, the context-aware mask alignment module forms\nsemantic correspondences and relocates masks so that every anomaly remains\nplausibly contained within the host object, eliminating out-of-boundary\nartifacts. Under a consistent identical evaluation protocol on the MVTec-AD\ndataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly\ntasks.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02314v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MAGIC：基于掩码引导扩散修复的多级扰动和上下文感知对齐的少样本异常生成", "tldr": "MAGIC是一种基于扩散的少样本异常生成方法，通过微调Stable Diffusion骨干网络、引入多级扰动和上下文感知对齐，解决了背景破坏、掩码错位和多样性不足的问题，并在异常检测任务中表现优于现有技术。", "motivation": "工业质量控制中异常数据稀缺，少样本异常生成是一个实用的解决方案。现有基于扩散的方法无法同时满足保持正常背景、精确修复异常区域以及在语义上有效位置生成多样化、逼真异常的需求。", "method": "MAGIC通过微调Stable Diffusion修复骨干网络来保留正常区域并确保合成异常与掩码严格一致。为弥补多样性损失，它引入了两种扰动策略：高斯提示级扰动（用于全局外观）和掩码引导的空间噪声注入（用于局部纹理变化）。此外，上下文感知掩码对齐模块用于形成语义对应并重新定位掩码，以消除越界伪影。", "result": "在MVTec-AD数据集上，MAGIC在下游异常任务中表现优于以前的最新技术。", "conclusion": "MAGIC成功解决了少样本异常生成中的背景完整性、掩码对齐和多样性问题，并实现了最先进的性能。", "translation": "少样本异常生成正成为工业质量控制环境中稀缺异常数据增强的实用解决方案。一个理想的生成器需要同时满足三个要求：(i) 保持正常背景完好无损，(ii) 修复异常区域以与相应的异常掩码紧密重叠，以及 (iii) 在语义上有效的位置生成异常区域，同时仅从少数真实样本中产生逼真、多样化的外观。现有的基于扩散的方法通常最多满足其中两个要求：全局异常生成器会破坏背景，而掩码引导的生成器在掩码不精确或错位时常常失效。我们提出了 MAGIC——基于掩码引导修复的多级扰动和上下文感知对齐——来解决所有这三个问题。其核心是，MAGIC 微调了一个 Stable Diffusion 修复骨干网络，该网络保留了正常区域并确保合成的异常严格遵守提供的掩码，直接解决了背景损坏和错位问题。为了弥补微调可能导致的多样性损失，MAGIC 添加了两种互补的扰动策略：(i) 在微调和推理过程中应用的高斯提示级扰动，扩大了异常的全局外观，同时避免了低保真度的文本外观，以及 (ii) 掩码引导的空间噪声注入，丰富了局部纹理变化。此外，上下文感知掩码对齐模块形成语义对应关系并重新定位掩码，使得每个异常都能合理地包含在宿主对象中，消除了越界伪影。在 MVTec-AD 数据集上采用一致的相同评估协议，MAGIC 在下游异常任务中优于以前的最新技术。", "summary": "MAGIC是一种新颖的少样本异常生成方法，旨在解决现有扩散模型在工业质量控制中面临的挑战。它通过微调Stable Diffusion修复骨干网络来保留背景并确保异常与掩码对齐，同时引入多级扰动（高斯提示级扰动和掩码引导空间噪声注入）以增强生成的多样性。此外，上下文感知掩码对齐模块确保异常在语义上合理地放置。MAGIC在MVTec-AD数据集上表现优于现有SOTA方法。", "keywords": "少样本异常生成, 扩散模型, 图像修复, 质量控制, MVTec-AD", "comments": "MAGIC的创新之处在于其综合方法，通过结合微调Stable Diffusion、多级扰动和上下文感知对齐，同时解决了少样本异常生成中背景完整性、掩码精确性和生成多样性的关键挑战。这对于工业质控中稀缺异常数据的增强具有重要意义。"}}
{"id": "2507.02244", "title": "Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies", "authors": ["Fangzhou Shi", "Xiaopeng Ke", "Xinye Xiong", "Kexin Meng", "Chang Men", "Zhengdan Zhu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02244v1", "summary": "The proliferation of ride-hailing aggregator platforms presents significant\ngrowth opportunities for ride-service providers by increasing order volume and\ngross merchandise value (GMV). On most ride-hailing aggregator platforms,\nservice providers that offer lower fares are ranked higher in listings and,\nconsequently, are more likely to be selected by passengers. This competitive\nranking mechanism creates a strong incentive for service providers to adopt\ncoupon strategies that lower prices to secure a greater number of orders, as\norder volume directly influences their long-term viability and sustainability.\nThus, designing an effective coupon strategy that can dynamically adapt to\nmarket fluctuations while optimizing order acquisition under budget constraints\nis a critical research challenge. However, existing studies in this area remain\nscarce.\n  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based\nsubsidy strategy framework designed to rapidly adapt to competitors' pricing\nadjustments. Our approach integrates two key techniques: Fast Competition\nAdaptation (FCA), which enables swift responses to dynamic price changes, and\nReinforced Lagrangian Adjustment (RLA), which ensures adherence to budget\nconstraints while optimizing coupon decisions on new price landscape.\nFurthermore, we introduce RideGym, the first dedicated simulation environment\ntailored for ride-hailing aggregators, facilitating comprehensive evaluation\nand benchmarking of different pricing strategies without compromising\nreal-world operational efficiency. Experimental results demonstrate that our\nproposed method consistently outperforms baseline approaches across diverse\nmarket conditions, highlighting its effectiveness in subsidy optimization for\nride-hailing service providers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02244v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "竞争压力下的订单获取：一种针对网约车补贴策略的快速自适应强化学习方法", "tldr": "本文提出了一种名为FCA-RL的强化学习框架，用于网约车补贴策略，该框架能够快速适应竞争对手的定价并遵守预算限制。同时，还引入了RideGym模拟环境。", "motivation": "在网约车聚合平台中，服务提供商需要设计一种有效的优惠券策略，以在预算限制下动态适应市场波动并优化订单获取。这是一个关键的研究挑战，且现有研究稀缺。这对于网约车服务提供商在竞争排名机制下的长期生存能力和可持续性至关重要。", "method": "本文提出了一种新颖的基于强化学习的补贴策略框架FCA-RL，旨在快速适应竞争对手的定价调整。该方法整合了两种关键技术：快速竞争适应（FCA）以实现对动态价格变化的迅速响应，以及强化拉格朗日调整（RLA）以确保遵守预算限制，同时优化新价格格局下的优惠券决策。此外，还引入了RideGym，第一个专为网约车聚合平台定制的模拟环境，用于全面评估和基准测试不同的定价策略。", "result": "实验结果表明，本文提出的方法在各种市场条件下始终优于基线方法，突显了其在网约车服务提供商补贴优化方面的有效性。", "conclusion": "FCA-RL为网约车服务提供商提供了一种有效且快速适应的解决方案，以在竞争压力和预算限制下优化补贴策略、获取更多订单并确保长期生存能力，并得到了新的RideGym模拟环境的支持。", "translation": "网约车聚合平台的普及为网约车服务提供商带来了巨大的增长机会，通过增加订单量和商品总值（GMV）。在大多数网约车聚合平台上，提供更低票价的服务提供商在列表中排名更高，因此更有可能被乘客选择。这种竞争性排名机制强烈激励服务提供商采用降低价格的优惠券策略，以获得更多的订单，因为订单量直接影响他们的长期生存能力和可持续性。因此，设计一种能够动态适应市场波动同时在预算限制下优化订单获取的有效优惠券策略是一个关键的研究挑战。然而，该领域的现有研究仍然稀缺。\n为了弥补这一空白，我们提出了FCA-RL，一种新颖的基于强化学习的补贴策略框架，旨在快速适应竞争对手的定价调整。我们的方法整合了两种关键技术：快速竞争适应（FCA），可实现对动态价格变化的迅速响应；以及强化拉格朗日调整（RLA），可确保遵守预算限制，同时优化新价格格局下的优惠券决策。此外，我们引入了RideGym，这是第一个专为网约车聚合平台定制的模拟环境，有助于在不影响实际运营效率的情况下，对不同定价策略进行全面评估和基准测试。实验结果表明，我们提出的方法在各种市场条件下始终优于基线方法，突显了其在网约车服务提供商补贴优化方面的有效性。", "summary": "本文旨在解决网约车服务提供商在竞争压力和预算限制下设计自适应补贴策略的挑战。为此，论文提出了FCA-RL，一个新颖的强化学习框架，它结合了快速竞争适应（FCA）以实现对价格的迅速响应，以及强化拉格朗日调整（RLA）以确保预算遵守。此外，论文还介绍了RideGym，一个专用的模拟环境。实验结果表明，FCA-RL在补贴优化方面表现优于基线方法。", "keywords": "强化学习, 网约车, 补贴策略, 竞争定价, 自适应控制", "comments": "该论文的创新之处在于提出了一个快速自适应的强化学习方法（FCA-RL），以解决网约车补贴策略在竞争环境下的关键且研究不足的问题。同时，引入了专用的模拟环境RideGym，这对于实际评估和未来的研究至关重要，提升了研究的实用性和可复现性。"}}
{"id": "2507.02804", "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective", "authors": ["Wenhao Shi", "Zhiqiang Hu", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Heng Tao Shen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.02804v1", "summary": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.02804v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多模态数学推理与多样化解题视角", "tldr": "本文引入了MathV-DP数据集和Qwen-VL-DP模型，通过多样化的解题路径和反思性推理，显著提升了多模态数学推理的准确性和生成多样性。", "motivation": "当前多模态大语言模型（MLLMs）在数学推理方面，常依赖一对一的图像-文本对和单一解法监督，忽视了有效推理视角的 다양性 和内部反思。", "method": "本文引入了MathV-DP数据集，该数据集捕获每个图像-问题对的多种多样化解题轨迹。在此基础上，提出了Qwen-VL-DP模型，该模型基于Qwen-VL，通过监督学习进行微调，并通过群组相对策略优化（GRPO）——一种基于规则的强化学习方法进行增强，GRPO整合了正确性判别和多样性感知奖励函数。", "result": "在MathVista的minitest和Math-V基准测试上的大量实验表明，Qwen-VL-DP在准确性和生成多样性方面显著优于先前的基础MLLMs。", "conclusion": "结果强调了在多模态数学推理中融入多样化视角和反思性推理的重要性。", "translation": "大型强化学习（RL）的最新进展显著增强了大型语言模型（LLMs）的推理能力，尤其是在数学领域。然而，当前用于数学推理的多模态大语言模型（MLLMs）通常依赖于一对一的图像-文本对和单一解法监督，忽视了有效推理视角的多样性和内部反思。在这项工作中，我们引入了MathV-DP，一个新颖的数据集，它捕获了每个图像-问题对的多个多样化解题轨迹，从而促进了更丰富的推理监督。我们进一步提出了Qwen-VL-DP，一个基于Qwen-VL构建的模型，通过监督学习进行微调，并通过群组相对策略优化（GRPO）进行增强，这是一种基于规则的强化学习方法，它整合了正确性判别和多样性感知奖励函数。我们的方法强调从不同的推理视角学习，并区分正确但不同的解决方案。在MathVista的minitest和Math-V基准测试上的大量实验表明，Qwen-VL-DP在准确性和生成多样性方面显著优于先前的基础MLLMs，突出了在多模态数学推理中融入多样化视角和反思性推理的重要性。", "summary": "本文针对当前多模态大语言模型在数学推理中缺乏多样化视角和内部反思的问题，提出了MathV-DP数据集和Qwen-VL-DP模型。MathV-DP数据集包含多样的解题轨迹，用于提供丰富的监督。Qwen-VL-DP模型基于Qwen-VL，通过监督学习和结合正确性判别与多样性感知奖励的GRPO强化学习进行训练。实验证明，该方法在准确性和生成多样性方面显著超越现有模型，强调了多样化和反思性推理在多模态数学推理中的关键作用。", "keywords": "多模态数学推理, 多样化视角, 强化学习, MathV-DP, Qwen-VL-DP", "comments": "该论文的创新点在于引入了包含多样化解题路径的数据集和结合强化学习的方法，以解决现有模型在数学推理中单一解法和缺乏反思的问题。通过强调从不同视角学习和区分正确但不同的解法，提升了模型的泛化能力和鲁棒性，对多模态数学推理领域具有重要意义。"}}
{"id": "2507.02844", "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection", "authors": ["Ziqi Miao", "Yi Ding", "Lijun Li", "Jing Shao"], "categories": ["cs.CV", "cs.CL", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.02844v1", "summary": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.02844v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "视觉语境攻击：通过图像驱动的语境注入来越狱多模态大语言模型", "tldr": "本文提出了VisCo攻击，一种新颖的以视觉为中心的越狱方法，通过图像驱动的语境对话来触发多模态大语言模型（MLLMs）的有害响应，例如针对GPT-4o，并取得了高攻击成功率。", "motivation": "多模态大语言模型（MLLMs）的视觉模态存在安全漏洞，这给模型的部署带来了挑战。现有的视觉越狱方法通常缺乏语义基础和真实性，因为视觉输入主要充当触发器。因此，需要一种以视觉为中心的越狱方法，其中视觉信息是构建完整且现实的攻击上下文的必要组成部分。", "method": "该研究定义了一种新颖的“以视觉为中心的越狱”设置，其中视觉信息是构建完整且现实的越狱上下文的必要组成部分。在此基础上，提出了VisCo（视觉语境）攻击。VisCo利用四种不同的以视觉为中心的策略来伪造语境对话，并在必要时动态生成辅助图像以构建以视觉为中心的越狱场景。为了最大限度地提高攻击效果，VisCo结合了自动毒性混淆和语义细化，以生成可靠触发目标黑盒MLLMs有害响应的最终攻击提示。", "result": "VisCo攻击在MM-SafetyBench上针对GPT-4o实现了4.78的毒性分数和85%的攻击成功率（ASR）。这显著优于基线方法，基线方法的毒性分数为2.48，ASR为22.2%。", "conclusion": "本文证明了VisCo攻击通过利用图像驱动的语境注入，能够有效地越狱多模态大语言模型，取得了高攻击成功率和毒性分数，从而揭示了MLLMs中存在的重大安全漏洞。", "translation": "随着强大的视觉-语言能力的出现，多模态大语言模型（MLLMs）在实际应用中展现出巨大的潜力。然而，视觉模态所表现出的安全漏洞对在开放世界环境中部署此类模型提出了重大挑战。最近的研究已成功通过将有害文本语义直接编码到视觉输入中来诱导目标MLLMs产生有害响应。然而，在这些方法中，视觉模态主要充当不安全行为的触发器，通常表现出语义模糊性，并且缺乏现实场景中的基础。在这项工作中，我们定义了一种新颖的设置：以视觉为中心的越狱，其中视觉信息是构建完整且现实的越狱上下文的必要组成部分。在此设置的基础上，我们提出了VisCo（视觉语境）攻击。VisCo利用四种不同的以视觉为中心的策略来伪造语境对话，并在必要时动态生成辅助图像，以构建以视觉为中心的越狱场景。为了最大限度地提高攻击效果，它结合了自动毒性混淆和语义细化，以生成最终的攻击提示，可靠地触发目标黑盒MLLMs的有害响应。具体而言，VisCo在MM-SafetyBench上针对GPT-4o实现了4.78的毒性分数和85%的攻击成功率（ASR），显著优于基线（其毒性分数为2.48，ASR为22.2%）。代码可在https://github.com/Dtc7w3PQ/Visco-Attack获得。", "summary": "本文提出了一种名为VisCo攻击的新型以视觉为中心的多模态大语言模型（MLLMs）越狱方法。与以往视觉仅作为触发器的方法不同，VisCo将视觉信息作为构建真实完整攻击语境的关键组成部分。它采用四种以视觉为中心的策略，动态生成辅助图像，并整合毒性混淆和语义细化。实验表明，VisCo在MM-SafetyBench上对GPT-4o实现了85%的攻击成功率和4.78的毒性分数，显著优于基线，揭示了MLLMs中一种强大的新漏洞。", "keywords": "MLLMs, 越狱, 视觉语境攻击, 安全漏洞, GPT-4o", "comments": "这篇论文通过将越狱从简单的视觉触发器转变为深度整合的视觉语境，为MLLM越狱引入了一种创新方法。“以视觉为中心的越狱”概念是一项重大贡献，突显了更复杂的漏洞。动态生成辅助图像和关注现实场景使这种攻击比以前的方法更具实用性和效力。对GPT-4o的高攻击成功率强调了解决此类漏洞对于MLLM安全部署的重要性。"}}
{"id": "2507.02207", "title": "Public perspectives on the design of fusion energy facilities", "authors": ["Nathan Kawamoto", "Daniel Hoover", "Jonathan Xie", "Jacob Walters", "Katie Snyder", "Aditi Verma"], "categories": ["physics.soc-ph", "cs.HC", "physics.ed-ph", "physics.plasm-ph"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      33 pages", "url": "http://arxiv.org/abs/2507.02207v1", "summary": "As fusion energy technologies approach demonstration and commercial\ndeployment, understanding public perspectives on future fusion facilities will\nbe critical for achieving social license, especially because fusion energy\nfacilities, unlike large fission reactors, may be sited in closer proximity to\npeople and communities, due to distinct regulatory frameworks. In a departure\nfrom the 'decide-announce-defend' approach typically used to site energy\ninfrastructure, we develop a participatory design methodology for\ncollaboratively designing fusion energy facilities with prospective host\ncommunities. We present here our findings from a participatory design workshop\nthat brought together 22 community participants and 34 engineering students.\nOur analysis of the textual and visual data from this workshop shows a range of\ndesign values and decision-making criteria with 'integrity' and 'respect'\nranking highest among values and 'economic benefits' and 'environmental\nprotection/safety' ranking highest among decision-making criteria. Salient\ndesign themes that emerge across facility concepts include connecting the\nhistory and legacy of the community to the design of the facility, care for\nworkers, transparency and access to the facility, and health and safety of the\nhost community. Participants reported predominantly positive sentiments,\nexpressing joy and surprise as the workshop progressed from learning about\nfusion to designing the hypothetical facility. Our findings suggest that\ncarrying out participatory design in the early stages of technology development\ncan invite and make concrete public hopes and concerns, improve understanding\nof, and curiosity about, an emerging technology, build toward social license,\nand inform context-specific development of fusion energy facilities.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2507.02207v1", "cate": "physics.soc-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "公众对聚变能设施设计的看法", "tldr": "本研究开发了一种参与式设计方法，旨在与潜在的主办社区合作设计聚变能设施，发现公众对设施设计有积极的看法，并强调了诚信、尊重、经济效益和环境保护等设计价值观和决策标准。", "motivation": "随着聚变能技术接近示范和商业部署，了解公众对未来聚变设施的看法对于获得社会许可至关重要，尤其是在聚变能设施可能比大型裂变反应堆更接近居民区的情况下。", "method": "本研究开发了一种参与式设计方法，用于与潜在的主办社区共同设计聚变能设施。通过一个汇集了22名社区参与者和34名工程学生的参与式设计研讨会，对文本和视觉数据进行了分析。", "result": "分析显示了一系列设计价值观和决策标准，其中“诚信”和“尊重”在价值观中排名最高，“经济效益”和“环境保护/安全”在决策标准中排名最高。突出的设计主题包括将社区的历史和遗产与设施设计联系起来、关爱工人、设施的透明度和可访问性以及主办社区的健康和安全。参与者报告了以积极情绪为主，表达了在研讨会从了解聚变到设计假设设施过程中产生的喜悦和惊喜。", "conclusion": "研究结果表明，在技术开发的早期阶段开展参与式设计可以引发并具体化公众的希望和担忧，增进对新兴技术的理解和好奇心，有助于建立社会许可，并为聚变能设施的特定语境开发提供信息。", "translation": "随着聚变能技术接近示范和商业部署，了解公众对未来聚变设施的看法对于获得社会许可至关重要，尤其是在聚变能设施（与大型裂变反应堆不同）由于独特的监管框架，可能更接近人群和社区的情况下。与通常用于选址能源基础设施的“决定-宣布-捍卫”方法不同，我们开发了一种参与式设计方法，用于与潜在的主办社区合作设计聚变能设施。我们在此展示了我们从一个参与式设计研讨会中获得的结果，该研讨会汇集了22名社区参与者和34名工程系学生。我们对该研讨会的文本和视觉数据分析显示了一系列设计价值观和决策标准，其中“诚信”和“尊重”在价值观中排名最高，“经济效益”和“环境保护/安全”在决策标准中排名最高。在设施概念中出现的突出设计主题包括将社区的历史和遗产与设施设计联系起来、关爱工人、设施的透明度和可访问性以及主办社区的健康和安全。参与者报告了以积极情绪为主，表达了在研讨会从了解聚变到设计假设设施过程中产生的喜悦和惊喜。我们的研究结果表明，在技术开发的早期阶段开展参与式设计可以引发并具体化公众的希望和担忧，增进对新兴技术的理解和好奇心，有助于建立社会许可，并为聚变能设施的特定语境开发提供信息。", "summary": "本研究旨在了解公众对聚变能设施设计的看法，并开发了一种与潜在主办社区合作的参与式设计方法。通过一个包含社区参与者和工程学生的研讨会，研究发现公众高度重视诚信、尊重、经济效益和环境保护等设计标准。主要设计主题包括社区联系、工人关怀、透明度和社区健康安全。研究表明，早期阶段的参与式设计有助于获得社会许可并指导设施开发。", "keywords": "聚变能, 公众视角, 参与式设计, 社会许可, 设施设计", "comments": "该论文通过引入参与式设计方法，创新性地解决了能源基础设施选址中常见的“决定-宣布-捍卫”模式的局限性。其重要性在于强调了在聚变能技术早期开发阶段整合公众视角的必要性，这对于获得社会许可和确保未来设施的成功部署至关重要。该方法不仅揭示了公众的设计偏好，还促进了公众对新兴技术的理解和积极情绪，为未来大规模能源项目的公众参与提供了宝贵的经验。"}}
{"id": "2507.02663", "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models", "authors": ["Yongjiang Liu", "Haoxi Li", "Xiaosong Ma", "Jie Zhang", "Song Guo"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      21 pages, 18 figures", "url": "http://arxiv.org/abs/2507.02663v1", "summary": "Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities\nin handling complex reasoning tasks, but are hindered by excessive\noverthinking. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we bootstrap such ability to further alleviate the overthinking phenomenon\nin LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage\nfine-tuning strategy that progressively inspires LRMs' difficulty cognition and\nredundancy cognition. First, we introduce difficulty-hypnosis in the prefixes\nof model outputs to intervene in the internal reasoning trajectory. Combined\nwith a heterogeneous short and long reasoning dataset, the trained model\nenhances its sensitivity to task difficulty, enabling native, differentiated\nreasoning strategies across various tasks. Second, we further extend\nredundancy-hypnosis to the internal reasoning process, guiding the model to\nidentify redundant structures within the reasoning steps and generate more\nconcise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that\nTH2T significantly reduces inference costs (more than 70% on easy tasks and 40%\non hard tasks) while maintaining performance stability. The resulting outputs\nexhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,\nreflection).", "comment": "21 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2507.02663v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "思考如何思考：通过大型推理模型中的自主难度认知来缓解过度思考", "tldr": "本文提出了一种名为TH2T的两阶段微调策略，通过难度认知和冗余认知来缓解大型推理模型（LRMs）的过度思考问题，显著降低了推理成本并保持了性能。", "motivation": "最近的大型推理模型（LRMs）在处理复杂推理任务方面表现出色，但受到过度思考的阻碍。实证分析表明，LRMs主要受限于在解决问题前识别人类般的任务属性（即难度级别），导致“一刀切”的推理过程。因此，本文旨在探索如何通过引导模型进行难度认知来缓解LRMs的过度思考现象。", "method": "本文提出了“思考如何思考”（TH2T），这是一种新颖的两阶段微调策略，逐步激发LRMs的难度认知和冗余认知。首先，通过在模型输出前缀中引入“难度催眠”，并结合异构的长短推理数据集，训练模型增强对任务难度的敏感性，从而实现原生的、差异化的推理策略。其次，进一步将“冗余催眠”扩展到内部推理过程，引导模型识别推理步骤中的冗余结构并生成更简洁的推理输出。", "result": "在7B/14B/32B模型上的实验表明，TH2T显著降低了推理成本（在简单任务上超过70%，在困难任务上超过40%），同时保持了性能稳定性。生成的输出表现出清晰的难度感知能力和减少的冗余（例如，反射）。", "conclusion": "本文成功提出并验证了TH2T策略，通过引入难度和冗余认知，有效缓解了大型推理模型的过度思考问题，显著提高了推理效率并保持了性能。", "translation": "大型推理模型（LRMs）最近在处理复杂推理任务方面展现出卓越的能力，但却受到过度思考的阻碍。为了探究其本质，我们的实证分析揭示，LRMs主要受限于在解决问题前识别人类般的任务属性（即难度级别），导致“一刀切”的推理过程。受此启发，一个紧迫而自然的问题浮现：我们能否利用这种能力来进一步缓解LRMs中的过度思考现象？在本文中，我们提出了“思考如何思考”（TH2T），这是一种新颖的两阶段微调策略，逐步激发LRMs的难度认知和冗余认知。首先，我们在模型输出的前缀中引入“难度催眠”，以干预内部推理轨迹。结合异构的长短推理数据集，训练后的模型增强了对任务难度的敏感性，从而在各种任务中实现原生的、差异化的推理策略。其次，我们进一步将“冗余催眠”扩展到内部推理过程，引导模型识别推理步骤中的冗余结构并生成更简洁的推理输出。在7B/14B/32B模型上的实验表明，TH2T显著降低了推理成本（在简单任务上超过70%，在困难任务上超过40%），同时保持了性能稳定性。生成的输出表现出清晰的难度感知能力和减少的冗余（例如，反射）。", "summary": "本文针对大型推理模型（LRMs）存在的过度思考问题，提出了一种名为“思考如何思考”（TH2T）的两阶段微调策略。该策略通过引入“难度催眠”和“冗余催眠”，旨在提升模型对任务难度的认知能力，并引导模型生成更简洁、无冗余的推理过程。实验结果表明，TH2T能显著降低LRMs的推理成本，同时保持或提升性能，有效缓解了过度思考现象，使模型能根据任务难度自适应地调整推理策略。", "keywords": "大型推理模型, 过度思考, 难度认知, 冗余认知, 微调", "comments": "这项研究通过引入“难度认知”和“冗余认知”的概念，为解决大型推理模型中的过度思考问题提供了一个新颖且实用的方法。其创新之处在于将认知过程融入到模型的微调策略中，使模型能够像人类一样根据任务难度调整思维方式，避免不必要的计算。这种方法不仅提高了模型的效率，降低了推理成本，还可能提升模型的可解释性。未来的工作可以探索这种认知能力在更广泛任务中的泛化能力。"}}
{"id": "2507.02316", "title": "Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos", "authors": ["Zecheng Zhao", "Selena Song", "Tong Chen", "Zhi Chen", "Shazia Sadiq", "Yadan Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02316v1", "summary": "Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation\nmetrics primarily capture visual quality and temporal consistency, offering\nlimited insight into how synthetic videos perform in downstream tasks such as\ntext-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset\nand benchmark designed to evaluate the utility of synthetic videos for building\nretrieval models. Based on 800 diverse user queries derived from MSRVTT\ntraining split, we generate synthetic videos using state-of-the-art T2V models\nand annotate each video-text pair along four key semantic alignment dimensions:\nObject \\& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation\nframework correlates general video quality assessment (VQA) metrics with these\nalignment scores, and examines their predictive power for downstream TVR\nperformance. To explore pathways of scaling up, we further develop an\nAuto-Evaluator to estimate alignment quality from existing metrics. Beyond\nbenchmarking, our results show that SynTVA is a valuable asset for dataset\naugmentation, enabling the selection of high-utility synthetic samples that\nmeasurably improve TVR outcomes. Project page and dataset can be found at\nhttps://jasoncodemaker.github.io/SynTVA/.", "comment": "7 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02316v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "合成视频有用吗？一个以检索为中心的合成视频评估基准", "tldr": "当前文本到视频合成的评估指标不足以衡量其在检索任务中的效用。本文引入了SynTVA数据集和基准，以评估合成视频对构建检索模型的价值，并发现其可用于数据增强以提高检索性能。", "motivation": "当前文本到视频（T2V）合成的评估指标主要捕捉视觉质量和时间一致性，但对合成视频在文本到视频检索（TVR）等下游任务中的表现提供的洞察有限。", "method": "本文引入了SynTVA，一个用于评估合成视频在构建检索模型中的效用的新数据集和基准。基于从MSRVTT训练集中提取的800个不同用户查询，使用最先进的T2V模型生成合成视频，并对每个视频-文本对在四个关键语义对齐维度上进行标注：对象与场景、动作、属性和提示保真度。评估框架将通用视频质量评估（VQA）指标与这些对齐分数相关联，并检查它们对下游TVR性能的预测能力。为探索规模化途径，进一步开发了一个自动评估器，以从现有指标估算对齐质量。", "result": "除了基准测试，我们的结果表明SynTVA是数据集增强的宝贵资产，能够选择高效用的合成样本，从而显著改善TVR结果。", "conclusion": "SynTVA数据集和评估框架为评估合成视频在检索任务中的实用性提供了一个新方法，并证明了高质量合成视频在数据集增强方面对提升文本到视频检索性能的潜力。", "translation": "文本到视频（T2V）合成技术发展迅速，但当前的评估指标主要捕捉视觉质量和时间一致性，对合成视频在文本到视频检索（TVR）等下游任务中的表现提供的洞察有限。在这项工作中，我们引入了SynTVA，这是一个新的数据集和基准，旨在评估合成视频在构建检索模型方面的效用。基于从MSRVTT训练集中提取的800个不同用户查询，我们使用最先进的T2V模型生成合成视频，并对每个视频-文本对在四个关键语义对齐维度上进行标注：对象与场景、动作、属性和提示保真度。我们的评估框架将通用视频质量评估（VQA）指标与这些对齐分数相关联，并检查它们对下游TVR性能的预测能力。为了探索规模化途径，我们进一步开发了一个自动评估器，以从现有指标估算对齐质量。除了基准测试，我们的结果表明SynTVA是数据集增强的宝贵资产，能够选择高效用的合成样本，从而显著改善TVR结果。项目页面和数据集可在 https://jasoncodemaker.github.io/SynTVA/ 找到。", "summary": "本文提出了SynTVA数据集和基准，旨在解决当前文本到视频（T2V）合成评估指标在衡量其在文本到视频检索（TVR）等下游任务中效用方面的不足。SynTVA包含基于MSRVTT查询生成的合成视频，并对视频-文本对进行多维度语义对齐标注。研究不仅提供了评估框架，还开发了自动评估器。结果表明SynTVA可作为有效的数据增强工具，显著提升TVR性能。", "keywords": "合成视频, 文本到视频检索, 评估基准, 数据增强, SynTVA", "comments": "本文的创新点在于提出了一个以检索为中心的合成视频评估基准SynTVA，弥补了现有评估指标仅关注视觉质量的不足。它不仅为评估T2V模型在实际应用中的效用提供了新视角，还证明了高质量合成视频在数据增强方面的巨大潜力，为提升下游任务性能开辟了新途径。"}}
{"id": "2507.02764", "title": "Terahertz Chip-Scale Meta-Networks with LSPR Routing: A Theoretical Framework", "authors": ["Maryam Khodadadi", "Hamidreza Taghvaee", "Pei Xiao", "Gabriele Gradoni", "Mohsen Khalily"], "categories": ["physics.optics", "eess.SP", "physics.app-ph"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02764v1", "summary": "Efficient chip-scale interconnects are essential for modern\nmicroelectronic-photonic systems, supporting high bandwidth and low-latency\nprocessing. Traditional wired links face high resistivity and latency, while\nmillimeter-wave wireless solutions suffer from bandwidth congestion and\ninterference. Terahertz (THz) plasmonic communication, based on surface plasmon\npolaritons (SPPs), offers high data rates and broad bandwidth, and is\ncompatible with nanophotonic platforms. This work introduces a Binary\nField-Driven Meta-Routing Method supported by a semi-analytical framework that\nmodels the tunable interaction between THz plasmonic phenomena and graphene's\nelectromagnetic properties. By modulating graphene's impedance, the method\nenables dynamic coupling and routing of localized surface plasmon resonances\n(LSPRs) across a meta-network, facilitating real-time beam steering in\nchip-scale systems. Combining analytical conductivity models, coupled-mode\ntheory, and algorithmic control, the approach enables predictive configuration\nof LSPR-based steering in reconfigurable graphene metasurfaces. Four meta-pixel\nantenna configurations Y-MetaRouter, MetaSwitcher, Penta-MetaEmitter, and\nCP-MetaCore are designed to support unidirectional radiation, bi-directional\nsteering, frequency-driven transitions, and circular polarization,\nrespectively. Chemical potential modulation creates reconfigurable LSPR\npathways and virtual SPP channels. A Coupled-Mode Theory for Field-Driven LSPR\nMeta-Networks is proposed to model current distributions and predict far-field\ncharacteristics. Results show strong agreement between theory and full-wave\nsimulations. A point-to-point meta-wireless link is analyzed, demonstrating\nscalability for low-latency, high-performance THz communication in WiNoC and\nchiplet applications. System-level metrics confirm feasibility for\nspace-constrained, high-speed interconnects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02764v1", "cate": "physics.optics", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "太赫兹芯片级超网络与LSPR路由：一个理论框架", "tldr": "该研究提出了一种基于石墨烯调制太赫兹等离激元通信的二进制场驱动超路由方法，以实现芯片级系统中的实时光束控制，并验证了其在低延迟、高性能互连方面的可行性。", "motivation": "传统的有线链路面临高电阻率和延迟问题，而毫米波无线解决方案则存在带宽拥堵和干扰。为了解决现代微电子光子系统对高效芯片级互连的需求，该研究旨在探索太赫兹等离激元通信的可能性。", "method": "本研究引入了一种二进制场驱动超路由方法，该方法由一个半解析框架支持，用于建模太赫兹等离激元现象与石墨烯电磁特性之间可调谐的相互作用。通过调制石墨烯的阻抗，该方法能够实现局部表面等离激元共振（LSPR）在超网络中的动态耦合和路由。该方法结合了分析电导率模型、耦合模理论和算法控制，实现了可重构石墨烯超表面中基于LSPR转向的预测性配置。设计了四种超像素天线配置：Y-MetaRouter、MetaSwitcher、Penta-MetaEmitter和CP-MetaCore，分别支持单向辐射、双向转向、频率驱动转换和圆极化。提出了一个用于场驱动LSPR超网络的耦合模理论，以模拟电流分布并预测远场特性。", "result": "结果显示理论与全波仿真之间具有很强的一致性。分析了点对点超无线链路，证明了其在WiNoC和chiplet应用中实现低延迟、高性能太赫兹通信的可扩展性。系统级指标证实了其在空间受限、高速互连方面的可行性。", "conclusion": "该研究提出的二进制场驱动超路由方法及其理论框架，通过利用石墨烯调制LSPR，为芯片级太赫兹通信提供了高效、可扩展的解决方案，并有望应用于未来的高带宽、低延迟互连系统。", "translation": "高效的芯片级互连对于现代微电子光子系统至关重要，它们支持高带宽和低延迟处理。传统的有线链路面临高电阻率和延迟，而毫米波无线解决方案则受到带宽拥堵和干扰。基于表面等离激元（SPPs）的太赫兹（THz）等离激元通信提供了高数据速率和宽带宽，并与纳米光子平台兼容。这项工作引入了一种二进制场驱动超路由方法，该方法由一个半解析框架支持，该框架模拟了太赫兹等离激元现象与石墨烯电磁特性之间可调谐的相互作用。通过调制石墨烯的阻抗，该方法能够在超网络中实现局部表面等离激元共振（LSPRs）的动态耦合和路由，从而促进芯片级系统中的实时光束控制。结合分析电导率模型、耦合模理论和算法控制，该方法能够预测性地配置可重构石墨烯超表面中基于LSPR的转向。设计了四种超像素天线配置：Y-MetaRouter、MetaSwitcher、Penta-MetaEmitter和CP-MetaCore，分别支持单向辐射、双向转向、频率驱动转换和圆极化。化学势调制创建了可重构的LSPR路径和虚拟SPP通道。提出了一个用于场驱动LSPR超网络的耦合模理论，以模拟电流分布并预测远场特性。结果显示理论与全波仿真之间具有很强的一致性。分析了点对点超无线链路，证明了其在WiNoC和chiplet应用中实现低延迟、高性能太赫兹通信的可扩展性。系统级指标证实了其在空间受限、高速互连方面的可行性。", "summary": "本论文提出了一种用于太赫兹芯片级互连的二进制场驱动超路由方法，该方法利用石墨烯的电磁特性调制局部表面等离激元共振（LSPR）以实现动态光束控制。通过结合理论模型和具体设计，该方法展示了在低延迟、高带宽太赫兹通信中的可行性和可扩展性，有望应用于WiNoC和chiplet等芯片级系统。", "keywords": "太赫兹通信, 芯片级互连, 局部表面等离激元共振, 石墨烯, 超网络", "comments": "这项工作在芯片级互连领域具有重要的创新性，它提出了一种新颖的基于石墨烯调制LSPR的太赫兹通信方法。通过结合理论框架、耦合模理论和具体的天线设计，实现了实时光束控制和动态路由，有效解决了传统互连方式的局限性。其在WiNoC和chiplet应用中的潜力表明了其在未来高性能计算和通信中的重要性。"}}
{"id": "2507.02822", "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model", "authors": ["Wencheng Zhang", "Shiqin Qiao", "Lingjie Luo", "Yinfeng Li", "Chuanyang Zheng", "Qian Xu", "Meng Li", "Yong Gui", "Yijun He", "Jianing Qiu", "Jindong Hong", "Jiankai Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02822v1", "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02822v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SynapseRoute：一种基于双态大语言模型的自动路由切换框架", "tldr": "SynapseRoute是一个动态路由框架，能智能地将LLM查询分配到“思考”或“非思考”模式，以优化准确性、成本效率和用户体验。", "motivation": "随着大语言模型（LLMs）在实际应用中的广泛采用，选择合适的模型不仅需要平衡性能，还需要考虑运营成本。具有推理能力的模型出现后，“思考”（高推理）和“非思考”（快速、低成本）模式之间的成本差距进一步扩大。研究发现，约58%的医疗问题仅通过非思考模式即可准确回答，无需高成本的推理过程。这表明问题复杂性存在明显二分法，并暗示根据复杂性动态路由查询可以优化准确性、成本效率和整体用户体验。", "method": "提出SynapseRoute，一个基于机器学习的动态路由框架，它智能地将输入查询分配到“思考”或“非思考”模式。", "result": "在多个医疗数据集上的实验结果表明，与单独使用思考模式相比，SynapseRoute不仅提高了整体准确性（0.8390 vs. 0.8272），还减少了36.8%的推理时间和39.66%的Token消耗。定性分析表明，对简单查询进行过度推理可能导致不必要的延迟甚至准确性下降，而我们的自适应路由避免了这一缺陷。", "conclusion": "SynapseRoute通过动态路由优化了LLM的使用，提高了准确性并显著降低了成本。该工作还引入了AIT指数，以全面评估准确性、延迟和Token成本之间的权衡。", "translation": "随着大语言模型（LLMs）在实际应用中的广泛采用，选择合适的模型不仅需要平衡性能，还需要考虑运营成本。具有推理能力的模型出现后，“思考”（高推理）和“非思考”（快速、低成本）模式之间的成本差距进一步扩大。在这项工作中，我们发现大约58%的医疗问题仅通过非思考模式即可准确回答，无需高成本的推理过程。这突显了问题复杂性上的明显二分法，并表明根据复杂性将查询动态路由到适当的模式可以优化准确性、成本效率和整体用户体验。在此基础上，我们进一步提出了SynapseRoute，一个基于机器学习的动态路由框架，它智能地将输入查询分配到思考或非思考模式。在多个医疗数据集上的实验结果表明，与单独使用思考模式相比，SynapseRoute不仅提高了整体准确性（0.8390 vs. 0.8272），还减少了36.8%的推理时间和39.66%的Token消耗。重要的是，定性分析表明，对简单查询进行过度推理可能导致不必要的延迟甚至准确性下降，而我们的自适应路由避免了这一缺陷。最后，这项工作进一步引入了准确性-推理-Token（AIT）指数，以全面评估准确性、延迟和Token成本之间的权衡。", "summary": "本文提出了SynapseRoute，一个基于机器学习的动态路由框架，旨在优化大语言模型（LLMs）在实际应用中的使用。鉴于LLM在“思考”（高成本、高推理）和“非思考”（低成本、快速）模式之间存在显著的成本差异，SynapseRoute根据查询复杂性智能地将输入路由到适当的模式。实验结果显示，在医疗数据集上，SynapseRoute不仅提高了整体准确性，还显著降低了推理时间和Token消耗，同时避免了对简单查询过度推理导致的效率低下。此外，论文还引入了AIT指数来综合评估准确性、延迟和Token成本之间的权衡。", "keywords": "大语言模型, 动态路由, 成本效率, 推理模式, SynapseRoute", "comments": "SynapseRoute的创新之处在于其动态路由机制，解决了LLM在不同复杂性任务中性能与成本平衡的关键问题。通过识别并避免对简单查询的“过度推理”，该框架显著提升了效率和用户体验。引入AIT指数也为LLM的综合评估提供了一个有价值的新指标，具有重要的实践意义。"}}
{"id": "2507.02850", "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users", "authors": ["Almog Hilel", "Idan Shenfeld", "Leshem Choshen", "Jacob Andreas"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02850v1", "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02850v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLM 催眠：利用用户反馈对所有用户进行未经授权的知识注入", "tldr": "该论文描述了在经过用户反馈训练的语言模型（LM）中存在的一个漏洞，即单个用户仅通过提供提示以及对LM输出进行赞成/反对反馈的能力，就可以持久地改变LM的知识和行为，从而实现未经授权的知识注入。", "motivation": "该论文的动机是描述在经过用户反馈训练的语言模型中存在的一个漏洞，即单个用户可以持久地改变语言模型的知识和行为。它旨在识别语言模型偏好调整的一个新的定性特征和一种新的攻击机制。", "method": "攻击者提示语言模型随机输出“中毒”或良性响应，然后赞成中毒响应或反对良性响应。当反馈信号在随后的偏好调整行为中使用时，即使在没有恶意提示的上下文中，语言模型也会表现出生成中毒响应的概率增加。", "result": "该攻击可用于（1）插入模型以前不具备的事实知识，（2）以引入可利用的安全缺陷的方式修改代码生成模式，以及（3）注入虚假金融新闻。", "conclusion": "该发现识别了语言模型偏好调整的一个新的定性特征（表明即使是高度受限的偏好数据也可以用于对行为进行细粒度控制），以及针对通过用户反馈训练的LM的一种新的攻击机制（扩展了预训练时数据投毒和部署时提示注入的工作）。", "translation": "我们描述了在经过用户反馈训练的语言模型（LM）中存在的一个漏洞，即单个用户仅通过提供提示以及对LM输出进行赞成/反对反馈的能力，就可以持久地改变LM的知识和行为。为了实施攻击，攻击者提示LM随机输出“中毒”或良性响应，然后赞成中毒响应或反对良性响应。当反馈信号在随后的偏好调整行为中使用时，即使在没有恶意提示的上下文中，LM也会表现出生成中毒响应的概率增加。我们表明，这种攻击可以用于（1）插入模型以前不具备的事实知识，（2）以引入可利用的安全缺陷的方式修改代码生成模式，以及（3）注入虚假金融新闻。我们的发现既识别了语言模型偏好调整的一个新的定性特征（表明即使是高度受限的偏好数据也可以用于对行为进行细粒度控制），也识别了针对通过用户反馈训练的LM的一种新的攻击机制（扩展了预训练时数据投毒和部署时提示注入的工作）。", "summary": "本文揭示了在通过用户反馈训练的语言模型中存在的一个名为“LLM 催眠”的漏洞。单个恶意用户可以利用赞成/反对机制来持久地注入未经授权的知识并改变语言模型的行为。通过对“中毒”或良性响应策略性地提供反馈，攻击者可以操纵随后的偏好调整，导致语言模型即使在正常上下文中也生成中毒内容。该攻击的有效性通过插入新事实、修改代码生成以引入安全缺陷以及注入虚假新闻得到了证实。这项工作突出了偏好调整的一个新方面，并引入了一种针对反馈训练语言模型的新攻击向量。", "keywords": "LLM 催眠, 用户反馈, 知识注入, 漏洞, 偏好调整", "comments": "该论文识别了依赖用户反馈进行偏好调整的LLM中一个关键且新颖的漏洞。“LLM 催眠”攻击尤其阴险，因为它允许单个用户微妙且持久地改变模型的知识和行为，可能影响所有用户。这通过看似无害的反馈机制展示了细粒度控制，扩展了之前关于数据投毒和提示注入的工作。这对LLM的安全性和可信度具有重要意义，强调了针对此类基于反馈的攻击需要强大的防御机制。"}}
{"id": "2507.02747", "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "authors": ["Jiawei He", "Danshi Li", "Xinqiang Yu", "Zekun Qi", "Wenyao Zhang", "Jiayi Chen", "Zhaoxiang Zhang", "Zhizheng Zhang", "Li Yi", "He Wang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02747v1", "summary": "As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02747v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DexVLG：大规模灵巧视觉-语言-抓取模型", "tldr": "DexVLG是一个大规模视觉-语言-抓取模型，它利用一个包含1.7亿个灵巧抓取姿态的大型数据集，实现了基于语言指令的灵巧手抓取，并在模拟和真实世界中展现出强大的零样本泛化能力。", "motivation": "当前视觉-语言-动作（VLA）系统在机器人任务中取得进展，但由于数据收集困难，研究主要集中在简单的夹持器末端执行器上。对于类人灵巧手的实用抓取，结合大型模型的研究非常有限。", "method": "本文引入了DexVLG，一个大型视觉-语言-抓取模型，用于预测与语言指令对齐的灵巧抓取姿态，并使用单视图RGBD输入。为此，研究团队生成了一个名为DexGraspNet 3.0的大规模数据集，包含1.7亿个灵巧抓取姿态，这些姿态映射到174,000个模拟对象的语义部分，并配有详细的部分级描述。该数据集用于训练一个VLM和一个基于流匹配的姿态头部，使其能够为桌面物体生成与指令对齐的抓取姿态。", "result": "DexVLG在物理模拟和真实世界实验中进行了评估。结果表明，DexVLG具有强大的零样本泛化能力，在模拟中实现了超过76%的零样本执行成功率和最先进的部分抓取精度，并在真实世界场景中成功实现了物理对象的部分对齐抓取。", "conclusion": "DexVLG成功解决了使用大型模型实现基于语言指令的灵巧手抓取姿态预测的挑战，并在大规模数据集和实际应用中展示了其有效性和强大的泛化能力。", "translation": "随着大型模型的普及，视觉-语言-动作（VLA）系统正使机器人能够处理日益复杂的任务。然而，受限于数据收集的难度，目前的进展主要集中在控制简单的夹持器末端执行器。关于结合大型模型实现类人灵巧手的功能性抓取研究甚少。在本文中，我们引入了DexVLG，一个大型视觉-语言-抓取模型，用于预测与语言指令对齐的灵巧抓取姿态，并使用单视图RGBD输入。为了实现这一目标，我们生成了一个包含1.7亿个灵巧抓取姿态的数据集，这些姿态映射到模拟中174,000个对象的语义部分，并配有详细的部分级描述。这个名为DexGraspNet 3.0的大规模数据集用于训练一个VLM和一个基于流匹配的姿态头部，使其能够为桌面物体生成与指令对齐的抓取姿态。为了评估DexVLG的性能，我们在基于物理的模拟中创建了基准，并进行了真实世界实验。广泛的测试表明，DexVLG具有强大的零样本泛化能力——在模拟中实现了超过76%的零样本执行成功率和最先进的部分抓取精度——并在真实世界场景中成功实现了物理对象的部分对齐抓取。", "summary": "本文提出了DexVLG，一个大规模视觉-语言-抓取模型，旨在解决现有VLA系统在灵巧手抓取方面的局限性。通过构建一个包含1.7亿个灵巧抓取姿态和详细语言描述的DexGraspNet 3.0数据集，并训练一个VLM和基于流匹配的姿态头部，DexVLG能够根据语言指令预测灵巧抓取姿态。实验结果表明，该模型在模拟和真实世界中均展现出卓越的零样本泛化能力和高精度，成功实现了类人灵巧手的功能性抓取。", "keywords": "灵巧抓取, 视觉-语言模型, 大规模数据集, 机器人, 零样本学习", "comments": "DexVLG的创新之处在于其大规模数据集的构建（DexGraspNet 3.0），该数据集包含了海量的灵巧抓取姿态与语义部分的语言描述，这极大地推动了灵巧手抓取领域的数据瓶颈。模型结合VLM和流匹配方法，实现了语言指令与复杂抓取动作的对齐，展现了在零样本场景下的强大泛化能力，对于机器人操作的实际应用具有重要意义。"}}
{"id": "2507.02320", "title": "Transformer-based EEG Decoding: A Survey", "authors": ["Haodong Zhang", "Hongqi Li"], "categories": ["cs.LG", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Journals", "url": "http://arxiv.org/abs/2507.02320v1", "summary": "Electroencephalography (EEG) is one of the most common signals used to\ncapture the electrical activity of the brain, and the decoding of EEG, to\nacquire the user intents, has been at the forefront of brain-computer/machine\ninterfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods\nwith machine learning, the advent of deep learning approaches have gradually\nrevolutionized the field by providing an end-to-end long-cascaded architecture,\nwhich can learn more discriminative features automatically. Among these,\nTransformer is renowned for its strong handling capability of sequential data\nby the attention mechanism, and the application of Transformers in various EEG\nprocessing tasks is increasingly prevalent. This article delves into a relevant\nsurvey, summarizing the latest application of Transformer models in EEG\ndecoding since it appeared. The evolution of the model architecture is followed\nto sort and organize the related advances, in which we first elucidate the\nfundamentals of the Transformer that benefits EEG decoding and its direct\napplication. Then, the common hybrid architectures by integrating basic\nTransformer with other deep learning techniques\n(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial\nnetworks, diffusion models, etc.) is overviewed in detail. The research\nadvances of applying the modified intrinsic structures of customized\nTransformer have also been introduced. Finally, the current challenges and\nfuture development prospects in this rapidly evolving field are discussed. This\npaper aims to help readers gain a clear understanding of the current state of\nTransformer applications in EEG decoding and to provide valuable insights for\nfuture research endeavors.", "comment": "Submitted to IEEE Journals", "pdf_url": "http://arxiv.org/pdf/2507.02320v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于Transformer的脑电图解码：一项综述", "tldr": "本文综述了Transformer模型在脑电图（EEG）解码中的最新应用、模型架构演变、混合架构以及定制Transformer结构的应用，并讨论了当前挑战和未来发展。", "motivation": "脑电图（EEG）解码是脑机接口（BCI）研究的前沿，深度学习方法特别是Transformer模型在处理序列数据方面表现出色，已逐渐彻底改变了该领域。本文旨在全面总结Transformer模型在EEG解码中的最新应用，帮助读者清晰了解当前状态并为未来研究提供见解。", "method": "本文对Transformer模型在EEG解码中的应用进行了综述，首先阐述了Transformer对EEG解码的基本原理和直接应用。接着，详细概述了Transformer与其他深度学习技术（如卷积/循环/图/脉冲神经网络、生成对抗网络、扩散模型等）结合的常见混合架构。此外，还介绍了应用定制Transformer的改进内部结构的研究进展。最后，讨论了该领域的当前挑战和未来发展前景。", "result": "本文总结了自Transformer出现以来，其在EEG解码中的最新应用，并追踪了模型架构的演变。具体概述了Transformer对EEG解码的基本原理和直接应用，详细介绍了其与多种深度学习技术结合的混合架构，以及定制Transformer内部结构的应用进展。同时，讨论了该领域面临的挑战和未来发展方向。", "conclusion": "本文旨在帮助读者清晰理解Transformer在EEG解码应用中的当前状态，并为未来的研究工作提供有价值的见解。该领域仍面临挑战，但未来发展前景广阔。", "translation": "脑电图（EEG）是捕获大脑电活动最常用的信号之一，而EEG解码（旨在获取用户意图）一直处于脑机接口（BCIs/BMIs）研究的前沿。与传统的机器学习EEG分析方法相比，深度学习方法的出现逐渐彻底改变了该领域，通过提供端到端的长级联架构，可以自动学习更具判别性的特征。其中，Transformer因其通过注意力机制对序列数据强大的处理能力而闻名，Transformer在各种EEG处理任务中的应用也越来越普遍。本文深入探讨了一项相关综述，总结了自Transformer出现以来，其在EEG解码中最新应用。本文追踪了模型架构的演变，以梳理和组织相关进展，其中我们首先阐明了有益于EEG解码的Transformer基础及其直接应用。然后，详细概述了通过将基本Transformer与其他深度学习技术（卷积/循环/图/脉冲神经网络、生成对抗网络、扩散模型等）集成的常见混合架构。定制Transformer的改进内部结构的应用研究进展也得到了介绍。最后，讨论了这一快速发展领域的当前挑战和未来发展前景。本文旨在帮助读者清晰理解Transformer在EEG解码应用中的当前状态，并为未来的研究工作提供有价值的见解。", "summary": "本文对Transformer模型在脑电图（EEG）解码领域的应用进行了全面综述。论文首先介绍了EEG解码的重要性，并指出深度学习尤其是Transformer模型在处理EEG序列数据方面的优势。随后，详细阐述了Transformer的基本原理及其在EEG解码中的直接应用，并深入探讨了Transformer与其他深度学习技术（如CNN、RNN、GAN等）相结合的混合架构。此外，还介绍了定制Transformer结构的研究进展。最后，文章讨论了当前面临的挑战并展望了未来的发展方向，旨在为研究人员提供该领域的最新概览和宝贵见解。", "keywords": "Transformer, EEG解码, 脑机接口, 深度学习, 综述", "comments": "这是一篇及时且重要的综述性论文，它系统地梳理了Transformer模型在EEG解码这一快速发展领域中的应用现状。其创新之处在于不仅涵盖了基础应用，还深入探讨了混合架构和定制结构，为研究人员提供了全面的视角。对于希望进入或深入研究EEG解码与深度学习结合领域的学者来说，该论文具有很高的参考价值。"}}
{"id": "2507.02681", "title": "Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education", "authors": ["Behnam Parsaeifard", "Christof Imhof", "Tansu Pancar", "Ioan-Sorin Comsa", "Martin Hlosta", "Nicole Bergamin", "Per Bergamin"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02681v1", "summary": "Students disengaging from their tasks can have serious long-term\nconsequences, including academic drop-out. This is particularly relevant for\nstudents in distance education. One way to measure the level of disengagement\nin distance education is to observe participation in non-mandatory exercises in\ndifferent online courses. In this paper, we detect student disengagement in the\nnon-mandatory quizzes of 42 courses in four semesters from a distance-based\nuniversity. We carefully identified the most informative student log data that\ncould be extracted and processed from Moodle. Then, eight machine learning\nalgorithms were trained and compared to obtain the highest possible prediction\naccuracy. Using the SHAP method, we developed an explainable machine learning\nframework that allows practitioners to better understand the decisions of the\ntrained algorithm. The experimental results show a balanced accuracy of 91\\%,\nwhere about 85\\% of disengaged students were correctly detected. On top of the\nhighly predictive performance and explainable framework, we provide a\ndiscussion on how to design a timely intervention to minimise disengagement\nfrom voluntary tasks in online learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02681v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "自愿测验中脱离学习的检测：高等远程教育中的可解释机器学习方法", "tldr": "本研究利用可解释机器学习方法，通过分析学生日志数据，检测远程教育学生在非强制性测验中的脱离学习行为，并取得了91%的平衡准确率，为及时干预提供了基础。", "motivation": "学生脱离学习会带来严重的长期后果，包括辍学，这在远程教育中尤为突出。测量脱离学习水平的一种方法是观察学生在非强制性在线练习中的参与情况。本研究旨在检测远程教育学生在非强制性测验中的脱离学习行为。", "method": "研究从一所远程大学的42门课程中收集了四个学期学生的Moodle日志数据，识别出最具信息量的学生日志数据。训练并比较了八种机器学习算法以获得最高的预测准确率。使用SHAP方法开发了一个可解释的机器学习框架，帮助实践者理解算法的决策。", "result": "实验结果显示，平衡准确率为91%，其中约85%的脱离学习学生被正确检测。研究提供了一个高性能且可解释的框架。", "conclusion": "本研究成功地使用可解释机器学习方法检测了远程教育学生在非强制性测验中的脱离学习行为，并讨论了如何设计及时干预措施以最大程度地减少在线学习中自愿任务的脱离。", "translation": "学生脱离任务会带来严重的长期后果，包括学业辍学。这对于远程教育的学生尤其重要。衡量远程教育中脱离学习水平的一种方法是观察不同在线课程中非强制性练习的参与情况。在本文中，我们检测了一所远程大学四个学期42门课程中学生在非强制性测验中的脱离学习行为。我们仔细识别了可以从Moodle中提取和处理的最具信息量的学生日志数据。然后，训练并比较了八种机器学习算法以获得最高的预测准确度。使用SHAP方法，我们开发了一个可解释的机器学习框架，使实践者能够更好地理解训练算法的决策。实验结果显示平衡准确率为91%，其中约85%的脱离学习学生被正确检测。除了高预测性能和可解释框架之外，我们还讨论了如何设计及时干预措施，以最大程度地减少在线学习中自愿任务的脱离。", "summary": "本研究旨在检测远程教育学生在非强制性测验中的脱离学习行为。通过从Moodle中提取并处理学生日志数据，并比较八种机器学习算法，研究取得了91%的平衡准确率，成功检测出约85%的脱离学习学生。此外，本研究还开发了一个基于SHAP的可解释机器学习框架，帮助理解算法决策，并讨论了如何设计及时干预措施以减少在线学习中的脱离行为。", "keywords": "脱离学习, 远程教育, 机器学习, 可解释性, SHAP", "comments": "本文的创新之处在于将可解释机器学习（SHAP）应用于远程教育中学生脱离学习行为的检测，这不仅提供了高精度的预测模型，还增强了模型的透明度和可理解性，有助于教育工作者制定更有效的干预策略。其重要性在于为远程教育中的学生支持系统提供了实用的工具和方法，有助于降低学生辍学率。"}}
{"id": "2507.02321", "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback", "authors": ["Nina Konovalova", "Maxim Nikolaev", "Andrey Kuznetsov", "Aibek Alanov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      code available at this https URL", "url": "http://arxiv.org/abs/2507.02321v1", "summary": "Despite significant progress in text-to-image diffusion models, achieving\nprecise spatial control over generated outputs remains challenging. ControlNet\naddresses this by introducing an auxiliary conditioning module, while\nControlNet++ further refines alignment through a cycle consistency loss applied\nonly to the final denoising steps. However, this approach neglects intermediate\ngeneration stages, limiting its effectiveness. We propose InnerControl, a\ntraining strategy that enforces spatial consistency across all diffusion steps.\nOur method trains lightweight convolutional probes to reconstruct input control\nsignals (e.g., edges, depth) from intermediate UNet features at every denoising\nstep. These probes efficiently extract signals even from highly noisy latents,\nenabling pseudo ground truth controls for training. By minimizing the\ndiscrepancy between predicted and target conditions throughout the entire\ndiffusion process, our alignment loss improves both control fidelity and\ngeneration quality. Combined with established techniques like ControlNet++,\nInnerControl achieves state-of-the-art performance across diverse conditioning\nmethods (e.g., edges, depth).", "comment": "code available at https://github.com/ControlGenAI/InnerControl", "pdf_url": "http://arxiv.org/pdf/2507.02321v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "倾听内心之声：通过中间特征反馈对齐ControlNet训练", "tldr": "InnerControl提出了一种新的训练策略，通过在所有去噪步骤中利用中间特征反馈来提高ControlNet的空间控制精度和生成质量。", "motivation": "尽管文本到图像扩散模型取得了显著进展，但实现对生成输出的精确空间控制仍然具有挑战性。ControlNet++虽然通过循环一致性损失改进了对齐，但其方法仅应用于最终去噪步骤，忽略了中间生成阶段，限制了其有效性。", "method": "我们提出了InnerControl，这是一种训练策略，旨在强制在所有扩散步骤中实现空间一致性。我们的方法训练轻量级卷积探针，以从每个去噪步骤的中间UNet特征中重建输入控制信号（例如，边缘、深度）。这些探针即使从高度噪声的潜在变量中也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测条件与目标条件之间的差异，我们的对齐损失提高了控制保真度和生成质量。", "result": "InnerControl与ControlNet++等现有技术结合，在各种条件方法（如边缘、深度）上实现了最先进的性能，提高了控制保真度和生成质量。", "conclusion": "通过在所有扩散步骤中强制执行空间一致性并利用中间特征反馈，InnerControl显著提高了ControlNet的空间控制精度和生成质量，达到了最先进的性能。", "translation": "尽管文本到图像扩散模型取得了显著进展，但实现对生成输出的精确空间控制仍然具有挑战性。ControlNet通过引入辅助条件模块解决了这个问题，而ControlNet++通过仅应用于最终去噪步骤的循环一致性损失进一步完善了对齐。然而，这种方法忽略了中间生成阶段，限制了其有效性。我们提出了InnerControl，这是一种训练策略，旨在强制在所有扩散步骤中实现空间一致性。我们的方法训练轻量级卷积探针，以从每个去噪步骤的中间UNet特征中重建输入控制信号（例如，边缘、深度）。这些探针即使从高度噪声的潜在变量中也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测条件与目标条件之间的差异，我们的对齐损失提高了控制保真度和生成质量。结合ControlNet++等现有技术，InnerControl在各种条件方法（例如，边缘、深度）上实现了最先进的性能。", "summary": "本文提出了InnerControl，一种新的ControlNet训练策略，旨在通过在所有扩散步骤中强制执行空间一致性来提高文本到图像扩散模型的空间控制精度。该方法通过训练轻量级卷积探针从中间UNet特征中重建控制信号，并引入对齐损失来最小化预测与目标条件之间的差异。实验表明，InnerControl与ControlNet++结合，在多种条件下取得了最先进的性能，显著提升了控制保真度和生成质量。", "keywords": "ControlNet, 扩散模型, 空间控制, 中间特征, 对齐损失", "comments": "InnerControl的创新之处在于其“倾听内心之声”的方法，即在整个扩散过程中利用中间特征反馈进行对齐，而非仅关注最终输出。这解决了ControlNet++的局限性，通过更细粒度的控制提高了模型的空间一致性。其使用轻量级探针从噪声潜在变量中提取伪真实控制信号，提供了一种高效且有效的方式来指导训练。"}}
{"id": "2507.02833", "title": "Generalizing Verifiable Instruction Following", "authors": ["Valentina Pyatkin", "Saumya Malik", "Victoria Graf", "Hamish Ivison", "Shengyi Huang", "Pradeep Dasigi", "Nathan Lambert", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.02833v1", "summary": "A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.02833v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "泛化可验证指令遵循", "tldr": "现有语言模型在遵循可验证输出约束方面泛化能力差，本文引入新基准IFBench和RLVR训练方法，显著提升了模型泛化遵循指令的能力。", "motivation": "语言模型或聊天机器人精确遵循人类指令，特别是输出约束，是人机交互成功的关键。现有模型在此方面表现不佳且泛化能力差，因此需要改进。", "method": "引入了新基准IFBench，包含58个多样、具有挑战性的域外可验证约束，用于评估精确指令遵循的泛化能力。此外，设计了约束验证模块，并提出了使用可验证奖励的强化学习（RLVR）方法来训练模型。", "result": "可验证奖励强化学习（RLVR）显著改善了指令遵循能力。", "conclusion": "本文通过引入IFBench基准、提供额外的训练约束和验证函数，并提出RLVR训练方法，有效提升了语言模型在精确指令遵循方面的泛化能力。", "translation": "成功人机交互的一个关键因素是语言模型或聊天机器人精确遵循人类指令的能力。指令的一个常见特征是用户添加的输出约束，例如“只用是或否回答”或“至少提及‘abrakadabra’3次”，以制作更有用的答案。即使是当今最强大的模型也难以满足此类约束。我们发现大多数模型在测试这些能力的基准（一种称为精确指令遵循的技能）中的一小部分可验证约束上严重过拟合，并且无法很好地泛化到未见过的输出约束。我们引入了一个新的基准IFBench，用于评估58个新的、多样化且具有挑战性的域外可验证约束上的精确指令遵循泛化能力。此外，我们对模型如何以及在何种数据上进行训练以改善精确指令遵循泛化能力进行了广泛分析。具体来说，我们精心设计了约束验证模块，并表明使用可验证奖励的强化学习（RLVR）显著改善了指令遵循。除了IFBench，我们还发布了29个额外的新手动标注训练约束和验证函数、RLVR训练提示以及代码。", "summary": "本文研究了语言模型在遵循人类指令中输出约束时泛化能力不足的问题。作者发现现有模型在基准测试中对少量可验证约束存在严重过拟合。为解决此问题，论文引入了新基准IFBench，用于评估模型在58个多样化、域外可验证约束上的泛化能力。同时，提出并验证了基于可验证奖励的强化学习（RLVR）方法，显著提升了模型精确指令遵循的泛化表现。研究还提供了新的训练数据、验证函数和代码。", "keywords": "精确指令遵循, 泛化, 可验证约束, 强化学习, IFBench", "comments": "这项工作通过引入一个专门的基准IFBench来评估语言模型在“可验证指令遵循泛化”方面的能力，具有重要意义。它揭示了现有模型在此方面的过拟合问题，并提出了RLVR这一创新性的训练方法，通过强化学习利用可验证奖励来直接优化模型对复杂指令约束的理解和遵循能力。这对于提升语言模型的可靠性和实用性至关重要，特别是那些需要精确输出的应用场景。"}}
{"id": "2507.02771", "title": "Grounding Intelligence in Movement", "authors": ["Melanie Segado", "Felipe Parodi", "Jordan K. Matelsky", "Michael L. Platt", "Eva B. Dyer", "Konrad P. Kording"], "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures", "url": "http://arxiv.org/abs/2507.02771v1", "summary": "Recent advances in machine learning have dramatically improved our ability to\nmodel language, vision, and other high-dimensional data, yet they continue to\nstruggle with one of the most fundamental aspects of biological systems:\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\nessential for interpreting behavior, predicting intent, and enabling\ninteraction. Despite its core significance in our intelligence, movement is\noften treated as an afterthought rather than as a rich and structured modality\nin its own right. This reflects a deeper fragmentation in how movement data is\ncollected and modeled, often constrained by task-specific goals and\ndomain-specific assumptions. But movement is not domain-bound. It reflects\nshared physical constraints, conserved morphological structures, and purposeful\ndynamics that cut across species and settings. We argue that movement should be\ntreated as a primary modeling target for AI. It is inherently structured and\ngrounded in embodiment and physics. This structure, often allowing for compact,\nlower-dimensional representations (e.g., pose), makes it more interpretable and\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\nDeveloping models that can learn from and generalize across diverse movement\ndata will not only advance core capabilities in generative modeling and\ncontrol, but also create a shared foundation for understanding behavior across\nbiological and artificial systems. Movement is not just an outcome, it is a\nwindow into how intelligent systems engage with the world.", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.02771v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "将智能根植于运动", "tldr": "本文认为，运动是理解生物和人工智能系统的核心，应作为AI的主要建模目标，因为它具有固有的结构性和可解释性，有助于推进生成模型和控制能力。", "motivation": "尽管机器学习在语言和视觉方面取得了巨大进展，但在处理生物系统最基本的运动方面仍面临挑战。运动对于解释行为、预测意图和实现交互至关重要，但常被忽视。现有的运动数据建模方式碎片化，受限于特定任务和领域假设。", "method": "本文提出将运动视为AI的主要建模目标。运动具有固有的结构性，并以具身性和物理学为基础，通常允许紧凑的低维表示（例如姿态），这使得它比原始高维感官输入更具可解释性和计算可行性。", "result": "学习并泛化多样化运动数据的模型不仅能提升生成建模和控制的核心能力，还能为理解生物和人工系统行为提供共享基础。", "conclusion": "运动不仅仅是结果，它是智能系统如何与世界互动的窗口。将运动作为AI的核心建模目标，有助于深入理解智能行为并推动AI发展。", "translation": "机器学习的最新进展极大地提高了我们对语言、视觉和其他高维数据进行建模的能力，但它们在处理生物系统最基本的方面之一：运动，上仍然举步维艰。在神经科学、医学、机器人学和动物行为学中，运动对于解释行为、预测意图和实现交互至关重要。尽管运动在我们的智能中具有核心意义，但它常常被视为事后补充，而不是一种丰富且结构化的独立模态。这反映了运动数据收集和建模方式的更深层次的碎片化，通常受限于特定任务目标和领域特定假设。但运动并非领域受限。它反映了跨物种和环境的共享物理约束、保守的形态结构和有目的的动态。我们认为运动应该被视为AI的主要建模目标。它本质上是结构化的，并以具身性和物理学为基础。这种结构，通常允许紧凑的低维表示（例如姿态），使其比原始高维感官输入更具可解释性和计算可行性。开发能够从多样化运动数据中学习并泛化的模型，不仅将推进生成建模和控制的核心能力，而且还将为理解生物和人工系统行为创造一个共享基础。运动不仅仅是结果，它是智能系统如何与世界互动的窗口。", "summary": "本文指出，尽管当前机器学习在处理语言和视觉数据方面表现出色，但在建模生物系统核心的运动方面仍显不足。文章强调运动在神经科学、医学、机器人和动物行为学中的重要性，并主张应将其视为AI领域的主要建模目标。作者认为运动具有内在结构，植根于具身性和物理学，能够形成紧凑的低维表示，从而比高维感官数据更易于建模和解释。研究从多样化运动数据中学习的模型，不仅能提升生成建模和控制能力，还能为理解生物和人工系统行为提供统一基础，因为运动是理解智能系统如何与世界互动的关键。", "keywords": "运动智能, 具身智能, AI建模, 行为理解, 生物系统", "comments": "本文提出了一种重要且被忽视的观点：运动是智能的基石，应作为AI的核心研究对象。其创新之处在于将运动提升到与语言、视觉同等重要的地位，并强调其固有的结构性和跨领域特性。这对于推动具身智能、机器人学以及对生物行为的理解具有重要意义。文章指出了当前AI研究的局限性，并为未来的研究方向提供了清晰的指引。"}}
{"id": "2507.02703", "title": "Time-critical and confidence-based abstraction dropping methods", "authors": ["Robin Schmöcker", "Lennart Kampmann", "Alexander Dockhorn"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted for Publication at the IEEE Conference on Games 2025", "url": "http://arxiv.org/abs/2507.02703v1", "summary": "One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and\nuse state and/or action abstractions during the tree search. Non-exact\nabstractions, however, introduce an approximation error making convergence to\nthe optimal action in the abstract space impossible. Hence, as proposed as a\ncomponent of Elastic Monte Carlo Tree Search by Xu et al., abstraction\nalgorithms should eventually drop the abstraction. In this paper, we propose\ntwo novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can\nyield clear performance improvements whilst being safe in the sense that the\ndropping never causes any notable performance degradations contrary to Xu's\ndropping method. OGA-IAAD is designed for time critical settings while OGA-CAD\nis designed to improve the MCTS performance with the same number of iterations.", "comment": "Accepted for Publication at the IEEE Conference on Games 2025", "pdf_url": "http://arxiv.org/pdf/2507.02703v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "时间关键和基于置信度的抽象丢弃方法", "tldr": "本文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），用于改进蒙特卡洛树搜索（MCTS）的性能，且比现有方法更安全。", "motivation": "蒙特卡洛树搜索（MCTS）中的非精确抽象会引入近似误差，导致无法收敛到抽象空间中的最优动作。现有的抽象丢弃方法（如Xu等人的方法）可能导致性能显著下降。", "method": "提出了两种新颖的抽象丢弃方案：OGA-IAAD和OGA-CAD。OGA-IAAD专为时间关键设置设计，而OGA-CAD旨在以相同迭代次数提高MCTS性能。", "result": "提出的方法（OGA-IAAD和OGA-CAD）可以带来明显的性能提升，并且是安全的，即不会像Xu的丢弃方法那样导致任何显著的性能下降。", "conclusion": "本文提出了两种安全且能有效提升MCTS性能的抽象丢弃方法，解决了现有方法可能导致性能下降的问题。", "translation": "蒙特卡洛树搜索（MCTS）改进的一种范式是在树搜索过程中构建和使用状态和/或动作抽象。然而，非精确抽象会引入近似误差，使得在抽象空间中无法收敛到最优动作。因此，正如Xu等人提出的弹性蒙特卡洛树搜索的一个组件，抽象算法最终应该丢弃抽象。在本文中，我们提出了两种新颖的抽象丢弃方案，即OGA-IAAD和OGA-CAD，它们可以带来明显的性能提升，同时在丢弃过程中是安全的，即与Xu的丢弃方法相反，丢弃不会导致任何显著的性能下降。OGA-IAAD专为时间关键设置设计，而OGA-CAD旨在以相同迭代次数提高MCTS性能。", "summary": "本文针对蒙特卡洛树搜索（MCTS）中非精确抽象引入的近似误差问题，提出了两种新型的抽象丢弃方案：OGA-IAAD和OGA-CAD。与现有方法可能导致性能下降不同，这两种新方案在提升MCTS性能的同时，确保了丢弃过程的安全性。OGA-IAAD适用于时间敏感场景，而OGA-CAD旨在在相同迭代次数下优化MCTS性能。", "keywords": "蒙特卡洛树搜索, 抽象丢弃, 时间关键, 近似误差, 性能提升", "comments": "这篇论文的创新点在于提出了两种新的抽象丢弃方法，解决了MCTS中非精确抽象引入的近似误差问题，并改进了现有丢弃方法可能导致性能下降的缺点。其安全性（不会导致显著性能下降）是其重要贡献。"}}
{"id": "2507.02322", "title": "Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model", "authors": ["Farida Siddiqi Prity", "Mirza Raquib", "Saydul Akbar Murad", "Md. Jubayar Alam Rafi", "Md. Khairul Bashar Bhuiyan", "Anupam Kumar Bairagi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02322v1", "summary": "Rice leaf diseases significantly reduce productivity and cause economic\nlosses, highlighting the need for early detection to enable effective\nmanagement and improve yields. This study proposes Artificial Neural Network\n(ANN)-based image-processing techniques for timely classification and\nrecognition of rice diseases. Despite the prevailing approach of directly\ninputting images of rice leaves into ANNs, there is a noticeable absence of\nthorough comparative analysis between the Feature Analysis Detection Model\n(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it\ncomes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).\nHence, this research presents initial experiments on the Feature Analysis\nDetection Model, utilizing various image Feature Extraction Algorithms,\nDimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms\n(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on\ndatasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf\nscald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation\nmethod. A Direct Image-Centric Detection Model is established without the\nutilization of any FEA, and the evaluation of classification performance relies\non different metrics. Ultimately, an exhaustive contrast is performed between\nthe achievements of the Feature Analysis Detection Model and Direct\nImage-Centric Detection Model in classifying rice leaf diseases. The results\nreveal that the highest performance is attained using the Feature Analysis\nDetection Model. The adoption of the proposed Feature Analysis Detection Model\nfor detecting rice leaf diseases holds excellent potential for improving crop\nhealth, minimizing yield losses, and enhancing overall productivity and\nsustainability of rice farming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02322v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "神经网络水稻叶片病害识别与分类研究：基于特征模型与直接成像模型的比较分析", "tldr": "该研究比较了基于特征分析和直接图像的神经网络模型在水稻叶片病害识别上的表现，发现基于特征分析的模型性能更优。", "motivation": "水稻叶片病害严重降低生产力并造成经济损失，因此需要早期检测以实现有效管理和提高产量。尽管直接将图像输入ANNs是普遍方法，但缺乏对特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）之间，特别是在评估特征提取算法（FEAs）有效性方面的彻底比较分析。", "method": "本研究提出了基于人工神经网络（ANN）的图像处理技术，用于水稻病害的及时分类和识别。研究对特征分析检测模型（FADM）进行了初步实验，利用了各种图像特征提取算法（FEAs）、降维算法（DRAs）、特征选择算法（FSAs）和极限学习机（ELM）。实验在包含细菌性叶枯病、褐斑病、稻瘟病、叶鞘枯萎病、叶鞘腐烂病和健康叶片的数据集上进行，采用10折交叉验证方法。同时建立了一个不使用任何FEA的直接图像中心检测模型（DICDM），并根据不同的分类性能指标进行评估。最后，对FADM和DICDM在水稻叶片病害分类方面的表现进行了全面对比。", "result": "结果显示，特征分析检测模型（FADM）取得了最高的性能。", "conclusion": "采用所提出的特征分析检测模型检测水稻叶片病害，在改善作物健康、最大程度减少产量损失以及提高水稻种植的整体生产力和可持续性方面具有巨大的潜力。", "translation": "水稻叶片病害严重降低生产力并造成经济损失，凸显了早期检测对于实现有效管理和提高产量的必要性。本研究提出了基于人工神经网络（ANN）的图像处理技术，用于水稻病害的及时分类和识别。尽管直接将水稻叶片图像输入ANNs是普遍方法，但缺乏对特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）之间，特别是在评估特征提取算法（FEAs）有效性方面的彻底比较分析。因此，本研究对特征分析检测模型进行了初步实验，利用了各种图像特征提取算法、降维算法（DRAs）、特征选择算法（FSAs）和极限学习机（ELM）。实验在包含细菌性叶枯病、褐斑病、稻瘟病、叶鞘枯萎病、叶鞘腐烂病和健康叶片的数据集上进行，采用10折交叉验证方法。同时建立了一个不使用任何FEA的直接图像中心检测模型，并根据不同的分类性能指标进行评估。最终，对特征分析检测模型和直接图像中心检测模型在水稻叶片病害分类方面的成就进行了全面对比。结果显示，使用特征分析检测模型取得了最高的性能。采用所提出的特征分析检测模型检测水稻叶片病害，在改善作物健康、最大程度减少产量损失以及提高水稻种植的整体生产力和可持续性方面具有巨大的潜力。", "summary": "本研究针对水稻叶片病害识别和分类问题，提出并比较了基于特征分析（FADM）和直接图像（DICDM）的神经网络模型。研究发现，尽管直接图像输入是常见做法，但对两种模型，特别是特征提取算法的有效性缺乏系统比较。实验利用多种特征提取、降维和特征选择算法以及极限学习机构建FADM，并与不使用特征提取的DICDM进行对比。结果表明，FADM在水稻叶片病害分类上表现最佳，证明了其在提高作物健康和水稻生产力方面的巨大潜力。", "keywords": "水稻叶片病害, 神经网络, 特征提取, 图像分类, 比较分析", "comments": "该研究的创新点在于系统地比较了基于特征提取的神经网络模型与直接图像输入模型在水稻叶片病害识别中的性能，填补了该领域比较分析的空白。其重要性在于证明了特征工程对于提升病害识别准确性的价值，为农业智能化提供了实用的解决方案，有助于减少病害带来的经济损失，提高农业可持续性。"}}
{"id": "2507.02310", "title": "Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment", "authors": ["Alif Ashrafee", "Jedrzej Kozal", "Michal Wozniak", "Bartosz Krawczyk"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02310v1", "summary": "Traditional continual learning methods prioritize knowledge retention and\nfocus primarily on mitigating catastrophic forgetting, implicitly assuming that\nthe data distribution of previously learned tasks remains static. This\noverlooks the dynamic nature of real-world data streams, where concept drift\npermanently alters previously seen data and demands both stability and rapid\nadaptation.\n  We introduce a holistic framework for continual learning under concept drift\nthat simulates realistic scenarios by evolving task distributions. As a\nbaseline, we consider Full Relearning (FR), in which the model is retrained\nfrom scratch on newly labeled samples from the drifted distribution. While\neffective, this approach incurs substantial annotation and computational\noverhead. To address these limitations, we propose Adaptive Memory Realignment\n(AMR), a lightweight alternative that equips rehearsal-based learners with a\ndrift-aware adaptation mechanism. AMR selectively removes outdated samples of\ndrifted classes from the replay buffer and repopulates it with a small number\nof up-to-date instances, effectively realigning memory with the new\ndistribution. This targeted resampling matches the performance of FR while\nreducing the need for labeled data and computation by orders of magnitude.\n  To enable reproducible evaluation, we introduce four concept-drift variants\nof standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and\nTiny-ImageNet-CD, where previously seen classes reappear with shifted\nrepresentations. Comprehensive experiments on these datasets using several\nrehearsal-based baselines show that AMR consistently counters concept drift,\nmaintaining high accuracy with minimal overhead. These results position AMR as\na scalable solution that reconciles stability and plasticity in non-stationary\ncontinual learning environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02310v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "概念漂移下自适应内存重校准的整体持续学习", "tldr": "针对概念漂移下的持续学习，本文提出了一种名为自适应内存重校准（AMR）的轻量级方法，通过选择性地更新内存中的样本来有效应对数据分布变化，并在保持高准确性的同时显著降低了计算和标注成本。", "motivation": "传统持续学习方法假设数据分布是静态的，主要关注缓解灾难性遗忘，但忽略了现实世界数据流中概念漂移的动态性，这需要模型同时具备稳定性和快速适应能力。", "method": "本文提出了自适应内存重校准（AMR）方法，作为一种轻量级替代方案。AMR通过选择性地从回放缓冲区中移除已漂移类别过时的样本，并用少量最新实例重新填充，从而有效地使内存与新数据分布对齐。", "result": "AMR在减少所需标注数据和计算量的情况下，能够匹配完全重新学习（FR）的性能。在Fashion-MNIST-CD、CIFAR10-CD、CIFAR100-CD和Tiny-ImageNet-CD等概念漂移数据集上的综合实验表明，AMR能够持续对抗概念漂移，以最小的开销保持高准确性。", "conclusion": "AMR是一种可扩展的解决方案，能够在非平稳持续学习环境中协调稳定性与可塑性。", "translation": "传统持续学习方法优先考虑知识保留，主要侧重于减轻灾难性遗忘，隐含地假设先前学习任务的数据分布保持静态。这忽略了现实世界数据流的动态性质，其中概念漂移会永久改变先前见过的数据，并要求模型同时具备稳定性和快速适应能力。我们引入了一个在概念漂移下进行持续学习的整体框架，通过演变任务分布来模拟真实场景。作为基线，我们考虑了完全重新学习（FR），其中模型根据来自漂移分布的新标记样本从头开始重新训练。虽然这种方法有效，但会产生大量的标注和计算开销。为了解决这些限制，我们提出了自适应内存重校准（AMR），这是一种轻量级替代方案，为基于回放的学习器配备了漂移感知适应机制。AMR选择性地从回放缓冲区中移除已漂移类别过时的样本，并用少量最新实例重新填充，从而有效地使内存与新分布对齐。这种有针对性的重采样匹配了FR的性能，同时将标记数据和计算的需求降低了几个数量级。为了实现可复现的评估，我们引入了标准视觉基准的四种概念漂移变体：Fashion-MNIST-CD、CIFAR10-CD、CIFAR100-CD和Tiny-ImageNet-CD，其中先前见过的类别以偏移表示重新出现。在这些数据集上使用几种基于回放的基线进行的综合实验表明，AMR持续对抗概念漂移，以最小的开销保持高准确性。这些结果将AMR定位为一种可扩展的解决方案，能够在非平稳持续学习环境中协调稳定性与可塑性。", "summary": "本文针对传统持续学习方法在概念漂移下的局限性，提出了自适应内存重校准（AMR）框架。AMR通过选择性地更新回放缓冲区中的样本，使其与新的数据分布对齐，从而在保持模型稳定性的同时实现快速适应。实验结果表明，AMR在减少计算和标注成本的同时，能够有效应对概念漂移并保持高准确性，证明其是解决非平稳持续学习挑战的有效且可扩展的方案。", "keywords": "持续学习, 概念漂移, 内存重校准, 灾难性遗忘, 回放学习", "comments": "本文创新性地解决了持续学习中概念漂移的挑战，提出了AMR这一轻量级且高效的内存重校准机制。通过引入新的基准数据集变体，提高了研究的可复现性。AMR在性能上媲美完全重新学习，同时显著降低了资源消耗，这在实际应用中具有重要意义。"}}
{"id": "2507.02726", "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving", "authors": ["Matthieu Zimmer", "Xiaotong Ji", "Rasul Tutunov", "Anthony Bordg", "Jun Wang", "Haitham Bou Ammar"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02726v1", "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02726v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "布尔巴基：用于定理证明的自生成和目标条件MDPs", "tldr": "提出Bourbaki系统，利用自生成目标条件MDPs和MCTS解决LLM在自动定理证明中的挑战，并在PutnamBench上取得SOTA。", "motivation": "大型语言模型（LLMs）在逻辑约束的自动定理证明（ATP）环境中面临推理挑战，主要由于奖励稀疏和证明规模庞大，尤其在需要复杂多步推理的PutnamBench基准测试中问题更加突出。", "method": "引入自生成目标条件MDPs (sG-MDPs) 框架，智能体根据证明状态生成并追求子目标，使问题更适合搜索。然后应用类蒙特卡洛树搜索（MCTS）算法解决sG-MDP，并在Bourbaki (7B) 系统中实现，该系统是一个模块化系统，可以集成多个7B LLM进行子目标生成和策略合成。", "result": "在PutnamBench上，Bourbaki (7B) 解决了26个问题，在该规模模型中取得了新的最先进（SOTA）结果。", "conclusion": "通过引入自生成目标条件MDPs和结合MCTS算法的Bourbaki系统，可以有效提升大型语言模型在复杂定理证明任务上的性能，并在大学级别问题基准测试中达到领先水平。", "translation": "推理对于大型语言模型（LLMs）来说仍然是一项具有挑战性的任务，尤其是在自动定理证明（ATP）这种逻辑受限的环境中，这归因于稀疏奖励和庞大的证明规模。在像PutnamBench这样的基准测试中，这些挑战被进一步放大，该基准测试包含需要复杂多步推理的大学级问题。为了解决这个问题，我们引入了自生成目标条件MDPs（sG-MDPs），这是一个新的框架，其中智能体根据不断演变的证明状态生成并追求它们的子目标。鉴于这种更结构化的目标生成方式，所产生的问题变得更适合搜索。然后，我们应用类似蒙特卡洛树搜索（MCTS）的算法来解决sG-MDP，并在Bourbaki（7B）中实例化了我们的方法，Bourbaki（7B）是一个模块化系统，可以集成多个7B LLM进行子目标生成和策略合成。在PutnamBench上，Bourbaki（7B）解决了26个问题，在该规模的模型中取得了新的最先进结果。", "summary": "本文针对大型语言模型在自动定理证明中面临的稀疏奖励和证明规模大等挑战，提出了一种名为自生成目标条件MDPs (sG-MDPs) 的新框架。该框架允许智能体根据证明状态生成并追求子目标，从而使问题更易于搜索。研究人员将类蒙特卡洛树搜索（MCTS）算法应用于解决sG-MDP，并在Bourbaki (7B) 系统中实现，该系统通过集成多个7B LLM进行子目标生成和策略合成。实验结果表明，Bourbaki (7B) 在PutnamBench基准测试上解决了26个问题，在该规模模型中达到了最先进的性能。", "keywords": "自动定理证明, 大型语言模型, 马尔可夫决策过程, 蒙特卡洛树搜索, 子目标生成", "comments": "本文通过引入自生成目标条件MDPs（sG-MDPs）框架，并结合MCTS算法，有效解决了LLMs在自动定理证明中面临的稀疏奖励和大规模搜索空间问题。Bourbaki (7B) 作为一个模块化系统，能够整合多个LLM进行子目标生成和策略合成，展现了其在复杂推理任务上的创新性和强大潜力，尤其是在大学级别数学问题上取得了显著的SOTA结果。"}}
{"id": "2507.02349", "title": "Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection", "authors": ["Rafic Nader", "Vincent L'Allinec", "Romain Bourcier", "Florent Autrusseau"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02349v1", "summary": "Intracranial aneurysms (ICA) commonly occur in specific segments of the\nCircle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.\nAn accurate detection of these critical landmarks is necessary for a prompt and\nefficient diagnosis. We introduce a fully automated landmark detection approach\nfor CoW bifurcations using a two-step neural networks process. Initially, an\nobject detection network identifies regions of interest (ROIs) proximal to the\nlandmark locations. Subsequently, a modified U-Net with deep supervision is\nexploited to accurately locate the bifurcations. This two-step method reduces\nvarious problems, such as the missed detections caused by two landmarks being\nclose to each other and having similar visual characteristics, especially when\nprocessing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for\nthe anatomical variability of the CoW, which affects the number of detectable\nlandmarks per scan. We assessed the effectiveness of our approach using two\ncerebral MRA datasets: our In-House dataset which had varying numbers of\nlandmarks, and a public dataset with standardized landmark configuration. Our\nexperimental results demonstrate that our method achieves the highest level of\nperformance on a bifurcation detection task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02349v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于自动化脑血管地标检测的两步神经网络", "tldr": "该论文提出了一种基于两步神经网络的自动化脑血管分叉地标检测方法，通过先识别感兴趣区域再精确定位，有效提高了检测性能。", "motivation": "颅内动脉瘤（ICA）常发生于Willis环的特定动脉分叉处，准确检测这些关键地标对于及时有效的诊断至关重要。传统的检测方法可能面临地标间距近、视觉特征相似以及Willis环解剖变异性导致的漏检问题。", "method": "本研究引入了一种全自动的Willis环分叉地标检测方法，采用两步神经网络流程。首先，一个目标检测网络识别地标位置附近的感兴趣区域（ROIs）。随后，一个带有深度监督的改进U-Net被用于精确地定位分叉点。这种两步法旨在减少因两个地标靠近且视觉特征相似（尤其是在处理完整的MRA飞行时间序列时）导致的漏检问题，并考虑了Willis环的解剖变异性。", "result": "研究人员使用两个脑部MRA数据集（一个内部数据集，地标数量可变；一个公共数据集，地标配置标准化）评估了该方法的有效性。实验结果表明，该方法在分叉检测任务中取得了最高水平的性能。", "conclusion": "该研究提出的两步神经网络方法能够有效、准确地自动化检测脑血管分叉地标，并在实验中表现出卓越的性能，解决了现有方法在处理复杂解剖结构和相似地标时的挑战。", "translation": "颅内动脉瘤（ICA）通常发生在Willis环（CoW）的特定节段，主要是十三个主要动脉分叉处。准确检测这些关键地标对于及时有效的诊断是必要的。我们引入了一种使用两步神经网络过程的Willis环分叉全自动地标检测方法。最初，一个目标检测网络识别靠近地标位置的感兴趣区域（ROIs）。随后，利用一个带有深度监督的改进U-Net来精确地定位分叉点。这种两步法减少了各种问题，例如由于两个地标彼此靠近且具有相似视觉特征（尤其是在处理完整的MRA飞行时间序列时）导致的漏检。此外，它还考虑了Willis环的解剖变异性，这会影响每次扫描可检测的地标数量。我们使用两个脑部MRA数据集评估了我们方法的有效性：我们内部的数据集具有不同数量的地标，以及一个具有标准化地标配置的公共数据集。我们的实验结果表明，我们的方法在分叉检测任务中取得了最高水平的性能。", "summary": "本论文提出了一种用于自动化脑血管分叉地标检测的两步神经网络方法。该方法首先利用目标检测网络识别地标附近的感兴趣区域，然后通过改进的U-Net进行精确的分叉定位。这种分层策略有效解决了传统方法中因地标接近、特征相似及解剖变异性导致的漏检问题。在两个脑部MRA数据集上的评估结果显示，该方法在分叉检测任务中表现出卓越的性能。", "keywords": "神经网络, 脑血管, 地标检测, 动脉瘤, 分叉检测", "comments": "该论文的创新点在于采用了“两步走”的神经网络策略，即先进行粗略的ROI识别再进行精细定位，有效解决了脑血管地标检测中常见的挑战，如地标密集、形态相似以及个体解剖差异等。这种方法提高了检测的鲁棒性和准确性，对于颅内动脉瘤的早期诊断具有重要意义。该模型在处理MRA TOF数据时表现出的优势值得关注。"}}
{"id": "2507.02315", "title": "Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo", "authors": ["Sooyeon Kim", "Giung Nam", "Juho Lee"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02315v1", "summary": "Recent work has framed constrained text generation with autoregressive\nlanguage models as a probabilistic inference problem. Among these, Zhao et al.\n(2024) introduced a promising approach based on twisted Sequential Monte Carlo,\nwhich incorporates learned twist functions and twist-induced proposals to guide\nthe generation process. However, in constrained generation settings where the\ntarget distribution concentrates on outputs that are unlikely under the base\nmodel, learning becomes challenging due to sparse and uninformative reward\nsignals. We show that iteratively refining the base model through\nself-distillation alleviates this issue by making the model progressively more\naligned with the target, leading to substantial gains in generation quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02315v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过自蒸馏扭曲序列蒙特卡洛改进语言模型中的受限生成", "tldr": "本文通过自蒸馏迭代优化基础模型，解决了受限文本生成中因稀疏奖励信号导致的学习困难问题，从而显著提高了生成质量。", "motivation": "在受限生成设置中，当目标分布集中在基础模型下不太可能输出的内容时，由于奖励信号稀疏且信息量不足，学习变得具有挑战性。", "method": "通过自蒸馏迭代地优化基础模型，使其逐渐与目标对齐。", "result": "该方法缓解了学习困难，使模型与目标逐步对齐，并显著提高了生成质量。", "conclusion": "通过自蒸馏迭代优化基础模型，可以有效解决受限文本生成中的学习挑战，并显著提升生成质量。", "translation": "最近的工作将自回归语言模型中的受限文本生成视为概率推理问题。其中，Zhao 等人（2024）引入了一种基于扭曲序列蒙特卡洛的有前景的方法，该方法结合了学习到的扭曲函数和扭曲诱导提议来指导生成过程。然而，在受限生成设置中，当目标分布集中在基础模型下不太可能输出的内容时，由于奖励信号稀疏且信息量不足，学习变得具有挑战性。我们表明，通过自蒸馏迭代地优化基础模型可以缓解这个问题，使模型逐渐与目标对齐，从而在生成质量上获得显著提升。", "summary": "针对受限文本生成中因稀疏奖励信号导致的学习挑战，本文提出一种通过自蒸馏迭代优化基础模型的方法。该方法使得模型能够逐步与目标分布对齐，从而有效缓解了学习困难，并显著提升了语言模型的生成质量。", "keywords": "受限生成, 语言模型, 自蒸馏, 序列蒙特卡洛, 文本生成", "comments": "本文的创新点在于引入自蒸馏机制来解决受限生成中模型与目标分布不匹配的问题，通过迭代优化提升了模型的对齐能力和生成质量，为受限文本生成提供了一种有效的新方法。"}}
{"id": "2507.02760", "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work", "authors": ["Guangwei Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02760v1", "summary": "The capabilities of Large Language Models (LLMs) have opened new frontiers\nfor interacting with complex, domain-specific knowledge. However, prevailing\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\nmethodological reasoning inherent to expert domains. RAG provides factual\ncontext but fails to convey logical frameworks; autonomous agents can be\ninefficient and unpredictable without domain-specific heuristics. To bridge\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\nfocused on systematically translating human expert knowledge, often expressed\nin natural language documents, into a machine-executable Knowledge Protocol\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\ninformation to endowing them with a domain's intrinsic logic, operational\nstrategies, and methodological principles. We argue that a well-engineered\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\nof decomposing abstract queries and executing complex, multi-step tasks. This\nposition paper defines the core principles of KPE, differentiates it from\nrelated concepts, and illustrates its potential applicability across diverse\nfields such as law and bioinformatics, positing it as a foundational\nmethodology for the future of human-AI collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02760v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "知识协议工程：领域特定知识工作中人工智能的新范式", "tldr": "大型语言模型（LLM）在处理深度领域推理方面存在不足。知识协议工程（KPE）将人类专家知识转化为机器可执行的协议，使LLM能够像专家一样运作。", "motivation": "现有的大型语言模型（LLM）方法，如检索增强生成（RAG）和通用代理AI，在处理专家领域中需要深度、程序性和方法论推理的任务时面临困难。RAG虽然提供事实上下文，但缺乏传达逻辑框架的能力；自主代理在没有领域特定启发式方法的情况下可能效率低下且不可预测。为了弥补这一差距，本文引入了知识协议工程（KPE）。", "method": "知识协议工程（KPE）是一种新范式，专注于系统地将人类专家知识（通常以自然语言文档表达）转化为机器可执行的知识协议（KP）。KPE旨在赋予大型语言模型（LLM）领域固有的逻辑、操作策略和方法论原则，从而使通用LLM能够像专家一样运作，分解抽象查询并执行复杂的多步骤任务。", "result": "本文是一篇立场论文，定义了知识协议工程（KPE）的核心原则，并阐述了其在法律和生物信息学等不同领域的潜在适用性。它提出，精心设计的知识协议可以使通用大型语言模型（LLM）像专家一样运作，能够分解抽象查询并执行复杂的、多步骤的任务。抽象中未提及具体的实验结果。", "conclusion": "知识协议工程（KPE）被定位为未来人机协作的基础方法论，通过将领域特定的逻辑和操作策略融入大型语言模型（LLM）中，使其能够作为专家进行功能。", "translation": "大型语言模型（LLM）的能力为与复杂、领域特定知识的交互开辟了新领域。然而，当前主流的方法，如检索增强生成（RAG）和通用代理AI，虽然强大，但往往难以应对需要专家领域固有的深度、程序性和方法论推理的任务。RAG提供事实上下文，但未能传达逻辑框架；自主代理在没有领域特定启发式方法的情况下可能效率低下且不可预测。为了弥补这一差距，我们引入了知识协议工程（KPE），这是一种新范式，专注于系统地将人类专家知识（通常以自然语言文档表达）转化为机器可执行的知识协议（KP）。KPE将重点从仅仅用零碎信息增强LLM，转向赋予它们领域固有的逻辑、操作策略和方法论原则。我们认为，一个精心设计的知识协议可以使通用LLM像专家一样运作，能够分解抽象查询并执行复杂的、多步骤的任务。这篇立场论文定义了KPE的核心原则，将其与相关概念区分开来，并阐述了其在法律和生物信息学等不同领域的潜在适用性，将其定位为未来人机协作的基础方法论。", "summary": "本文提出了一种名为知识协议工程（KPE）的新范式，旨在解决当前大型语言模型（LLM）方法（如RAG和代理AI）在处理专家领域内深度、程序性推理方面的局限性。KPE通过系统地将人类专家知识转化为机器可执行的知识协议，赋予LLM领域固有的逻辑和操作策略。这种方法旨在使通用LLM能够像专家一样执行复杂的任务，并被视为未来人机协作的基础方法论。", "keywords": "知识协议工程, 大型语言模型, 领域特定知识, 人机协作, 专家系统", "comments": "该论文提出了一种新颖的范式——知识协议工程（KPE），其创新之处在于专注于系统地编码领域固有的逻辑，而非仅仅通过事实进行增强。这解决了当前LLM在专家领域应用中的一个关键限制，有望显著提升AI在复杂推理任务和人机协作中的能力。"}}
{"id": "2507.02354", "title": "Lightweight Shrimp Disease Detection Research Based on YOLOv8n", "authors": ["Fei Yuhuan", "Wang Gengchen", "Liu Fenghao", "Zang Ran", "Sun Xufei", "Chang Hao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      in Chinese language", "url": "http://arxiv.org/abs/2507.02354v1", "summary": "Shrimp diseases are one of the primary causes of economic losses in shrimp\naquaculture. To prevent disease transmission and enhance intelligent detection\nefficiency in shrimp farming, this paper proposes a lightweight network\narchitecture based on YOLOv8n. First, by designing the RLDD detection head and\nC2f-EMCM module, the model reduces computational complexity while maintaining\ndetection accuracy, improving computational efficiency. Subsequently, an\nimproved SegNext_Attention self-attention mechanism is introduced to further\nenhance the model's feature extraction capability, enabling more precise\nidentification of disease characteristics. Extensive experiments, including\nablation studies and comparative evaluations, are conducted on a\nself-constructed shrimp disease dataset, with generalization tests extended to\nthe URPC2020 dataset. Results demonstrate that the proposed model achieves a\n32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5\nof 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms\nother lightweight YOLO-series models in mAP@0.5, parameter count, and model\nsize. Generalization experiments on the URPC2020 dataset further validate the\nmodel's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The\nproposed method achieves an optimal balance between accuracy and efficiency,\nproviding reliable technical support for intelligent disease detection in\nshrimp aquaculture.", "comment": "in Chinese language", "pdf_url": "http://arxiv.org/pdf/2507.02354v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于YOLOv8n的轻量级对虾疾病检测研究", "tldr": "本文提出了一种基于YOLOv8n的轻量级网络，用于高效准确地检测对虾疾病，显著降低了参数并提高了检测精度。", "motivation": "对虾疾病是导致对虾养殖业经济损失的主要原因之一。为了防止疾病传播并提高对虾养殖中的智能检测效率。", "method": "本文提出了一种基于YOLOv8n的轻量级网络架构。首先，通过设计RLDD检测头和C2f-EMCM模块，在保持检测精度的同时降低了计算复杂度，提高了计算效率。其次，引入改进的SegNext_Attention自注意力机制，进一步增强了模型的特征提取能力。在自建对虾疾病数据集上进行了广泛的实验，包括消融研究和比较评估，并将泛化测试扩展到URPC2020数据集。", "result": "与原始YOLOv8n相比，所提出的模型参数减少了32.3%，mAP@0.5达到92.7%（比YOLOv8n提高了3%）。此外，该模型在mAP@0.5、参数数量和模型大小方面优于其他轻量级YOLO系列模型。在URPC2020数据集上的泛化实验进一步验证了模型的鲁棒性，mAP@0.5比YOLOv8n增加了4.1%。", "conclusion": "所提出的方法在精度和效率之间取得了最佳平衡，为对虾养殖中的智能疾病检测提供了可靠的技术支持。", "translation": "对虾疾病是对虾养殖业经济损失的主要原因之一。为了防止疾病传播并提高对虾养殖中的智能检测效率，本文提出了一种基于YOLOv8n的轻量级网络架构。首先，通过设计RLDD检测头和C2f-EMCM模块，该模型在保持检测精度的同时降低了计算复杂度，提高了计算效率。其次，引入改进的SegNext_Attention自注意力机制，进一步增强了模型的特征提取能力，从而更精确地识别疾病特征。在自建对虾疾病数据集上进行了广泛的实验，包括消融研究和比较评估，并将泛化测试扩展到URPC2020数据集。结果表明，与原始YOLOv8n相比，所提出的模型参数减少了32.3%，mAP@0.5达到92.7%（比YOLOv8n提高了3%）。此外，该模型在mAP@0.5、参数数量和模型大小方面优于其他轻量级YOLO系列模型。在URPC2020数据集上的泛化实验进一步验证了模型的鲁棒性，mAP@0.5比YOLOv8n增加了4.1%。所提出的方法在精度和效率之间取得了最佳平衡，为对虾养殖中的智能疾病检测提供了可靠的技术支持。", "summary": "本文提出了一种基于YOLOv8n的轻量级对虾疾病检测网络，通过引入RLDD检测头、C2f-EMCM模块和改进的SegNext_Attention机制，显著降低了模型参数和计算复杂度，同时大幅提高了检测精度和特征提取能力。实验结果表明，该模型在自建数据集和URPC2020数据集上均表现出优异的性能和鲁棒性，实现了精度与效率的平衡，为对虾养殖中的智能疾病检测提供了有效解决方案。", "keywords": "对虾疾病检测, YOLOv8n, 轻量化网络, 计算机视觉, 深度学习", "comments": "本文通过对YOLOv8n进行轻量化改进，在保持甚至提升检测精度的同时，显著降低了模型复杂度，这对于资源受限的实际部署场景（如对虾养殖场）具有重要意义。引入的RLDD、C2f-EMCM和SegNext_Attention机制是其创新点，有效平衡了性能与效率。泛化性测试也增强了其可靠性。"}}
{"id": "2507.02856", "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation", "authors": ["Nikhil Chandak", "Shashwat Goel", "Ameya Prabhu", "Moritz Hardt", "Jonas Geiping"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      34 pages, Code is available at this https URL", "url": "http://arxiv.org/abs/2507.02856v1", "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.", "comment": "34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching", "pdf_url": "http://arxiv.org/pdf/2507.02856v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "答案匹配优于多项选择用于语言模型评估", "tldr": "论文指出多项选择评估语言模型存在缺陷，提出“答案匹配”作为更准确的替代方法，该方法与人工评估高度一致并能改变模型排名。", "motivation": "现有的多项选择基准在评估语言模型时存在根本性限制，模型可以通过捷径而非真实理解来回答问题，导致评估不准确且无法有效区分模型能力。因此，需要一种更有效、可扩展的替代评估方法。", "method": "论文提出了一种名为“答案匹配”的生成式评估方法。该方法让候选语言模型对问题生成自由形式的响应（不提供选项），然后使用一个现代语言模型结合参考答案来判断生成的响应是否匹配参考答案。为了验证其有效性，研究者标注了MMLU-Pro和GPQA-Diamond数据集以获取人类评分数据，并比较了答案匹配、多项选择评估以及无参考答案的“LLM-as-a-judge”方法与人类评分的一致性。", "result": "实验结果显示，使用最新模型（包括小型模型）的答案匹配方法与人工评分达到了近乎完美的协议，其一致性范围与标注者间一致性相当。相比之下，传统的多项选择评估和不使用参考答案的“LLM-as-a-judge”方法与人工评分的一致性较差。此外，通过答案匹配评估模型的自由形式响应，会显著改变多个模型的排名。", "conclusion": "答案匹配是一种比多项选择更有效、更准确的语言模型评估方法。它能够克服多项选择评估的固有缺陷，并与人类判断高度一致。研究建议将语言模型评估的重心从多项选择转向答案匹配。", "translation": "长久以来，多项选择基准一直是语言模型评估的主力，因为多项选择的评分客观且易于自动化。然而，我们发现流行基准中的多项选择题通常可以在不看问题的情况下被回答。这些捷径源于判别式评估的一个根本限制，而模型的自由形式生成式答案的评估则没有这个限制。直到最近，似乎还没有可行的、可扩展的多项选择替代方案——但我们表明这种情况已经改变。我们考虑通过我们称之为答案匹配的方法进行生成式评估：给候选模型问题而不提供选项，让它生成自由形式的响应，然后使用现代语言模型和参考答案来确定响应是否匹配参考答案。为了比较不同评估策略的有效性，我们标注了MMLU-Pro和GPQA-Diamond数据集以获取人工评分数据，并测量了每种评估方法的协议程度。我们发现使用最新模型（甚至是小型模型）的答案匹配达到了近乎完美的协议，与标注者间协议范围一致。相比之下，多项选择评估和不带参考答案的“LLM-as-a-judge”方法与人工评分的对齐效果都很差。通过答案匹配改进评估不仅仅是一个概念问题：在用答案匹配评估模型的自由形式响应时，几个模型的排名发生了显著变化。鉴于这些发现，我们讨论了如何将评估生态系统从多项选择转向答案匹配。", "summary": "本文探讨了当前语言模型评估中多项选择基准的局限性，指出模型可能通过捷径而非真实理解来作答。为解决此问题，论文提出了一种名为“答案匹配”的生成式评估新范式。该方法允许语言模型自由生成答案，随后利用另一语言模型与参考答案进行匹配判断。实验结果表明，答案匹配法与人工评估具有高度一致性，远优于传统多项选择及无参考答案的LLM-as-a-judge方法，并且能够显著改变模型的性能排名。研究强调了答案匹配在提升语言模型评估有效性方面的重要性，并呼吁评估生态系统向此方向转变。", "keywords": "语言模型评估, 答案匹配, 多项选择, 生成式评估, 人工一致性", "comments": "这篇论文对当前语言模型评估领域提出了一个关键且及时的挑战。它创新性地提出了“答案匹配”这一生成式评估方法，有效地解决了多项选择评估中模型利用捷径而非真实理解作答的问题。通过与人类评估的高度一致性，该方法不仅提高了评估的有效性，而且揭示了现有评估方法可能导致的模型排名偏差，这对于指导未来语言模型的研究和开发具有重要意义。论文为推动语言模型评估范式的进步提供了一个有前景的方向。"}}
{"id": "2507.02358", "title": "Holistic Tokenizer for Autoregressive Image Generation", "authors": ["Anlin Zheng", "Haochen Wang", "Yucheng Zhao", "Weipeng Deng", "Tiancai Wang", "Xiangyu Zhang", "Xiaojuan Qi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02358v1", "summary": "The vanilla autoregressive image generation model generates visual tokens in\na step-by-step fashion, which limits the ability to capture holistic\nrelationships among token sequences. Moreover, most visual tokenizers map local\nimage patches into latent tokens, leading to limited global information. To\naddress this, we introduce \\textit{Hita}, a novel image tokenizer for\nautoregressive (AR) image generation. It introduces a holistic-to-local\ntokenization scheme with learnable holistic queries and local patch tokens.\nBesides, Hita incorporates two key strategies for improved alignment with the\nAR generation process: 1) it arranges a sequential structure with holistic\ntokens at the beginning followed by patch-level tokens while using causal\nattention to maintain awareness of previous tokens; and 2) before feeding the\nde-quantized tokens into the decoder, Hita adopts a lightweight fusion module\nto control information flow to prioritize holistic tokens. Extensive\nexperiments show that Hita accelerates the training speed of AR generators and\noutperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID}\nand \\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the\nholistic representation highlights its ability to capture global image\nproperties such as textures, materials, and shapes. Additionally, Hita also\ndemonstrates effectiveness in zero-shot style transfer and image in-painting.\nThe code is available at\n\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}", "comment": "17 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02358v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "全局分词器用于自回归图像生成", "tldr": "本文提出Hita，一种新型全局到局部图像分词器，用于自回归图像生成，通过捕获整体关系和全局信息，加速训练并显著提升性能。", "motivation": "香草自回归图像生成模型逐步生成视觉标记，限制了捕获标记序列之间整体关系的能力。此外，大多数视觉分词器将局部图像块映射到潜在标记，导致全局信息有限。", "method": "本文引入了Hita，一种用于自回归（AR）图像生成的新型图像分词器。Hita采用一种全局到局部的分词方案，包含可学习的全局查询和局部补丁标记。Hita还整合了两个关键策略以改进与AR生成过程的对齐：1）它安排了一种序列结构，全局标记在前，后跟补丁级标记，同时使用因果注意力保持对先前标记的感知；2）在将去量化标记输入解码器之前，Hita采用一个轻量级融合模块来控制信息流，优先处理全局标记。", "result": "实验表明，Hita加速了AR生成器的训练速度，并优于使用香草分词器训练的模型，在ImageNet基准上实现了2.59 FID和281.9 IS。对全局表示的详细分析突出其捕获全局图像属性（如纹理、材料和形状）的能力。此外，Hita在零样本风格迁移和图像修复中也表现出有效性。", "conclusion": "Hita通过引入全局到局部的分词方案和信息流控制策略，有效解决了自回归图像生成中全局信息捕获不足的问题，显著提升了生成性能和训练效率，并在多项任务中展现出泛化能力。", "translation": "香草自回归图像生成模型逐步生成视觉标记，这限制了捕获标记序列之间整体关系的能力。此外，大多数视觉分词器将局部图像块映射到潜在标记，导致全局信息有限。为了解决这个问题，我们引入了Hita，一种用于自回归（AR）图像生成的新型图像分词器。它引入了一种全局到局部的分词方案，包含可学习的全局查询和局部补丁标记。此外，Hita整合了两个关键策略以改进与AR生成过程的对齐：1）它安排了一种序列结构，全局标记在前，后跟补丁级标记，同时使用因果注意力保持对先前标记的感知；2）在将去量化标记输入解码器之前，Hita采用一个轻量级融合模块来控制信息流，优先处理全局标记。广泛的实验表明，Hita加速了AR生成器的训练速度，并优于使用香草分词器训练的模型，在ImageNet基准上实现了2.59 FID和281.9 IS。对全局表示的详细分析突出其捕获全局图像属性（如纹理、材料和形状）的能力。此外，Hita在零样本风格迁移和图像修复中也表现出有效性。代码可在https://github.com/CVMI-Lab/Hita 获取。", "summary": "本文提出Hita，一种用于自回归图像生成的新型全局到局部图像分词器，旨在解决现有方法在捕获全局信息和整体关系方面的不足。Hita通过引入可学习的全局查询和局部补丁标记，并采用全局优先的序列结构和信息融合模块，显著加速了自回归生成器的训练，并在ImageNet上取得了2.59 FID和281.9 IS的优异性能。实验证明其能够有效捕获全局图像属性，并在零样本风格迁移和图像修复等任务中展现了泛化能力。", "keywords": "全局分词器, 自回归图像生成, Hita, 视觉标记化, 全局表示", "comments": "这篇论文通过引入“全局到局部”的分词策略和针对自回归生成过程优化的信息流控制，有效地解决了传统视觉分词器在捕获图像全局信息方面的局限性。Hita的创新点在于其分层式的标记化方法，以及对全局标记的优先级处理，这使得模型能够更好地理解图像的整体结构和属性，从而在生成质量和训练效率上都取得了显著提升。在多项下游任务中的有效性也表明了其方法的普适性和鲁棒性。"}}
{"id": "2507.02342", "title": "DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values", "authors": ["Changhun Kim", "Yechan Mun", "Sangchul Hahn", "Eunho Yang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to ICML 2025 Workshop on Actionable Interpretability. Code is available at this https URL", "url": "http://arxiv.org/abs/2507.02342v1", "summary": "This study proposes DeltaSHAP, a novel explainable artificial intelligence\n(XAI) algorithm specifically designed for online patient monitoring systems. In\nclinical environments, discovering the causes driving patient risk evolution is\ncritical for timely intervention, yet existing XAI methods fail to address the\nunique requirements of clinical time series explanation tasks. To this end,\nDeltaSHAP addresses three key clinical needs: explaining the changes in the\nconsecutive predictions rather than isolated prediction scores, providing both\nmagnitude and direction of feature attributions, and delivering these insights\nin real time. By adapting Shapley values to temporal settings, our approach\naccurately captures feature coalition effects. It further attributes prediction\nchanges using only the actually observed feature combinations, making it\nefficient and practical for time-sensitive clinical applications. We also\nintroduce new evaluation metrics to evaluate the faithfulness of the\nattributions for online time series, and demonstrate through experiments on\nonline patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI\nmethods in both explanation quality as 62% and computational efficiency as 33%\ntime reduction on the MIMIC-III decompensation benchmark. We release our code\nat https://github.com/AITRICS/DeltaSHAP.", "comment": "Accepted to ICML 2025 Workshop on Actionable Interpretability. Code\n  is available at https://github.com/AITRICS/DeltaSHAP", "pdf_url": "http://arxiv.org/pdf/2507.02342v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DeltaSHAP：使用 Shapley 值解释在线患者监测中的预测演变", "tldr": "DeltaSHAP 是一种新的可解释人工智能 (XAI) 算法，专为在线患者监测设计，通过适应 Shapley 值来解释预测变化，并在准确性和效率上超越现有方法。", "motivation": "现有的可解释人工智能 (XAI) 方法无法满足临床时间序列解释任务的独特需求，特别是在解释连续预测变化、提供特征归因的方向和幅度以及实时洞察方面。在临床环境中，发现驱动患者风险演变的原因对于及时干预至关重要。", "method": "DeltaSHAP 通过将 Shapley 值应用于时间序列设置来捕获特征联合效应，并仅使用实际观察到的特征组合来归因预测变化。它还引入了新的评估指标来评估在线时间序列归因的忠实性。", "result": "DeltaSHAP 在在线患者监测任务中，解释质量优于现有最先进的 XAI 方法 62%，在计算效率上实现了 33% 的时间减少（基于 MIMIC-III 代偿基准）。", "conclusion": "DeltaSHAP 是一种有效且高效的 XAI 方法，能够满足在线患者监测中解释预测演变的关键临床需求。", "translation": "本研究提出了 DeltaSHAP，一种专为在线患者监测系统设计的新型可解释人工智能 (XAI) 算法。在临床环境中，发现驱动患者风险演变的原因对于及时干预至关重要，但现有的 XAI 方法未能解决临床时间序列解释任务的独特要求。为此，DeltaSHAP 解决了三个关键的临床需求：解释连续预测的变化而不是孤立的预测分数，提供特征归因的大小和方向，以及实时提供这些见解。通过将 Shapley 值适应于时间设置，我们的方法准确地捕获了特征联合效应。它进一步仅使用实际观察到的特征组合来归因预测变化，使其高效且实用，适用于时间敏感的临床应用。我们还引入了新的评估指标来评估在线时间序列归因的忠实性，并通过在线患者监测任务的实验证明，DeltaSHAP 在解释质量方面优于最先进的 XAI 方法 62%，在计算效率方面（在 MIMIC-III 代偿基准上时间减少 33%）。我们已在 https://github.com/AITRICS/DeltaSHAP 发布了我们的代码。", "summary": "DeltaSHAP 是一种新型的可解释人工智能 (XAI) 算法，专为在线患者监测系统设计。它通过将 Shapley 值应用于时间序列数据，解决了现有方法在解释连续预测变化、提供特征归因的方向和幅度以及实时洞察方面的不足。该方法能够高效地归因预测变化，并引入了新的评估指标。实验证明，DeltaSHAP 在解释质量和计算效率上均优于现有的最先进 XAI 方法，尤其适用于时间敏感的临床应用。", "keywords": "可解释人工智能, Shapley 值, 患者监测, 时间序列, 预测解释", "comments": "该论文提出了一种创新的 XAI 方法 DeltaSHAP，专门针对在线患者监测中的时间序列数据解释问题。其创新点在于将 Shapley 值适应于时间序列背景，并关注预测的“演变”而非孤立的预测，同时提供了特征归因的方向和幅度，并强调实时性，这对于临床决策至关重要。该方法通过实验证明了其在解释质量和计算效率上的显著优势，对实际临床应用具有重要意义。"}}
{"id": "2507.02363", "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling", "authors": ["Jiahao Wu", "Rui Peng", "Jianbo Jiao", "Jiayu Yang", "Luyang Tang", "Kaiqiang Xiong", "Jie Liang", "Jinbo Yan", "Runling Liu", "Ronggang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.02363v1", "summary": "Due to the complex and highly dynamic motions in the real world, synthesizing\ndynamic videos from multi-view inputs for arbitrary viewpoints is challenging.\nPrevious works based on neural radiance field or 3D Gaussian splatting are\nlimited to modeling fine-scale motion, greatly restricting their application.\nIn this paper, we introduce LocalDyGS, which consists of two parts to adapt our\nmethod to both large-scale and fine-scale motion scenes: 1) We decompose a\ncomplex dynamic scene into streamlined local spaces defined by seeds, enabling\nglobal modeling by capturing motion within each local space. 2) We decouple\nstatic and dynamic features for local space motion modeling. A static feature\nshared across time steps captures static information, while a dynamic residual\nfield provides time-specific features. These are combined and decoded to\ngenerate Temporal Gaussians, modeling motion within each local space. As a\nresult, we propose a novel dynamic scene reconstruction framework to model\nhighly dynamic real-world scenes more realistically. Our method not only\ndemonstrates competitive performance on various fine-scale datasets compared to\nstate-of-the-art (SOTA) methods, but also represents the first attempt to model\nlarger and more complex highly dynamic scenes. Project page:\nhttps://wujh2001.github.io/LocalDyGS/.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02363v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LocalDyGS：通过自适应局部隐式特征解耦实现多视角全局动态场景建模", "tldr": "LocalDyGS提出了一种新的动态场景建模方法，通过分解局部空间并解耦静态和动态特征，实现了对大规模和精细尺度动态场景的逼真重建，超越了现有方法的局限性。", "motivation": "由于现实世界中复杂且高度动态的运动，从多视角输入合成任意视点的动态视频极具挑战性。现有基于神经辐射场或3D高斯泼溅的工作仅限于建模精细尺度运动，极大地限制了其应用。", "method": "本文引入了LocalDyGS，它包含两部分以适应大规模和精细尺度运动场景：1) 将复杂的动态场景分解为由种子定义的流线型局部空间，通过捕获每个局部空间内的运动实现全局建模。2) 解耦局部空间运动建模的静态和动态特征，其中静态特征跨时间步共享以捕获静态信息，动态残差场提供时间特定特征，两者结合并解码生成时间高斯，建模每个局部空间内的运动。", "result": "本文提出了一种新颖的动态场景重建框架，能够更真实地建模高度动态的真实世界场景。该方法不仅在各种精细尺度数据集上与最先进方法相比展现出有竞争力的性能，而且是首次尝试建模更大、更复杂的高度动态场景。", "conclusion": "LocalDyGS通过其独特的局部空间分解和特征解耦方法，成功克服了现有动态场景建模方法在处理大规模和复杂运动方面的局限性，实现了更逼真的动态视频合成。", "translation": "由于现实世界中复杂且高度动态的运动，从多视角输入合成任意视点的动态视频极具挑战性。现有基于神经辐射场或3D高斯泼溅的工作仅限于建模精细尺度运动，极大地限制了其应用。在本文中，我们引入了LocalDyGS，它包含两部分以使我们的方法适应大规模和精细尺度运动场景：1) 我们将复杂的动态场景分解为由种子定义的流线型局部空间，通过捕获每个局部空间内的运动实现全局建模。2) 我们解耦了局部空间运动建模的静态和动态特征。一个跨时间步共享的静态特征捕获静态信息，而动态残差场提供时间特定特征。这些被组合并解码以生成时间高斯，建模每个局部空间内的运动。因此，我们提出了一种新颖的动态场景重建框架，以更真实地建模高度动态的真实世界场景。我们的方法不仅在各种精细尺度数据集上与最先进（SOTA）方法相比展现出有竞争力的性能，而且代表了首次尝试建模更大、更复杂的高度动态场景。项目页面：https://wujh2001.github.io/LocalDyGS/。", "summary": "LocalDyGS是一种用于多视角全局动态场景建模的新方法，旨在解决现有技术在处理大规模复杂运动时的局限性。它通过将动态场景分解为局部空间并解耦静态与动态特征来生成时间高斯，从而能够真实地重建高度动态场景。该方法在精细尺度数据集上表现出色，并且首次成功建模了更大、更复杂的动态场景。", "keywords": "动态场景建模, 多视角, 局部隐式特征解耦, 时间高斯, 大规模运动", "comments": "LocalDyGS的创新之处在于其独特的双重策略：将复杂动态场景分解为易于管理的局部空间，并巧妙地解耦静态和动态特征。这种方法有效地解决了现有神经辐射场和3D高斯泼溅方法在处理大规模和高度动态场景时的局限性，是动态场景重建领域的重要进展，尤其是在处理现实世界复杂运动方面的能力。"}}
{"id": "2507.02356", "title": "Offline Reinforcement Learning with Penalized Action Noise Injection", "authors": ["JunHyeok Oh", "Byung-Jun Lee"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02356v1", "summary": "Offline reinforcement learning (RL) optimizes a policy using only a fixed\ndataset, making it a practical approach in scenarios where interaction with the\nenvironment is costly. Due to this limitation, generalization ability is key to\nimproving the performance of offline RL algorithms, as demonstrated by recent\nsuccesses of offline RL with diffusion models. However, it remains questionable\nwhether such diffusion models are necessary for highly performing offline RL\nalgorithms, given their significant computational requirements during\ninference. In this paper, we propose Penalized Action Noise Injection (PANI), a\nmethod that simply enhances offline learning by utilizing noise-injected\nactions to cover the entire action space, while penalizing according to the\namount of noise injected. This approach is inspired by how diffusion models\nhave worked in offline RL algorithms. We provide a theoretical foundation for\nthis method, showing that offline RL algorithms with such noise-injected\nactions solve a modified Markov Decision Process (MDP), which we call the noisy\naction MDP. PANI is compatible with a wide range of existing off-policy and\noffline RL algorithms, and despite its simplicity, it demonstrates significant\nperformance improvements across various benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02356v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "离线强化学习与惩罚性动作噪声注入", "tldr": "本文提出了PANI（惩罚性动作噪声注入）方法，通过简单地向动作中注入噪声并施加惩罚，有效提升了离线强化学习的性能，同时避免了扩散模型的高计算成本。", "motivation": "离线强化学习（RL）的泛化能力是关键，现有扩散模型虽能提升性能但推理计算成本高昂。本文旨在探究是否存在更简单、计算效率更高的方法来提升离线RL的性能，质疑扩散模型的必要性。", "method": "本文提出惩罚性动作噪声注入（PANI）方法。该方法通过向动作中注入噪声来覆盖整个动作空间，并根据注入噪声的量进行惩罚。理论上，这种方法解决了我们称之为“噪声动作MDP”的修改版马尔可夫决策过程。PANI兼容多种现有离策略和离线RL算法。", "result": "PANI方法尽管简单，但在各种基准测试中均表现出显著的性能提升。", "conclusion": "PANI是一种简单而有效的离线强化学习性能增强方法，通过惩罚性动作噪声注入实现，并具备坚实的理论基础，可作为扩散模型之外的高效替代方案。", "translation": "离线强化学习（RL）仅使用固定数据集来优化策略，使其成为与环境交互成本高昂场景中的实用方法。由于这一限制，泛化能力是提高离线RL算法性能的关键，正如最近扩散模型在离线RL中取得的成功所证明的那样。然而，考虑到扩散模型在推理过程中巨大的计算需求，这些模型对于高性能的离线RL算法是否必要仍然值得怀疑。在本文中，我们提出了惩罚性动作噪声注入（PANI），这是一种通过利用噪声注入的动作覆盖整个动作空间，并根据注入的噪声量进行惩罚来简单地增强离线学习的方法。这种方法受到了扩散模型在离线RL算法中工作方式的启发。我们为该方法提供了理论基础，表明使用这种噪声注入动作的离线RL算法解决了一个修改过的马尔可夫决策过程（MDP），我们称之为噪声动作MDP。PANI与广泛的现有离策略和离线RL算法兼容，尽管其简单，但它在各种基准测试中表现出显著的性能改进。", "summary": "本文提出了一种名为PANI（惩罚性动作噪声注入）的离线强化学习方法，旨在解决现有扩散模型在离线RL中推理计算量大的问题。PANI的核心思想是向动作中注入噪声以覆盖更广的动作空间，并根据注入噪声的量施加惩罚。该方法受到扩散模型启发，具备理论基础（解决了“噪声动作MDP”），且与多种现有离策略和离线RL算法兼容。尽管PANI方法实现简单，但在多项基准测试中均展现出显著的性能提升。", "keywords": "离线强化学习, 动作噪声注入, 泛化能力, 马尔可夫决策过程, 性能提升", "comments": "该论文提出了一种新颖且实用的离线强化学习方法PANI。其创新之处在于，通过一种相对简单且计算效率高的方式（惩罚性动作噪声注入），实现了类似扩散模型对动作空间探索的益处，同时避免了扩散模型在推理阶段的高计算开销。这对于在资源受限或需要高效部署的离线RL应用中具有重要意义。该方法提供了理论基础，并显示出强大的兼容性和性能提升，为离线RL领域提供了一个有价值的替代方案。"}}
{"id": "2507.01991", "title": "FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports", "authors": ["Muhammad Bilal Zafar"], "categories": ["q-fin.CP", "cs.CL", "econ.GN", "q-fin.EC", "q-fin.GN"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      The FinAI-BERT model can be directly loaded via Hugging Face Transformers ( this https URL ) for sentence-level AI disclosure classification", "url": "http://arxiv.org/abs/2507.01991v1", "summary": "The proliferation of artificial intelligence (AI) in financial services has\nprompted growing demand for tools that can systematically detect AI-related\ndisclosures in corporate filings. While prior approaches often rely on keyword\nexpansion or document-level classification, they fall short in granularity,\ninterpretability, and robustness. This study introduces FinAI-BERT, a\ndomain-adapted transformer-based language model designed to classify AI-related\ncontent at the sentence level within financial texts. The model was fine-tuned\non a manually curated and balanced dataset of 1,586 sentences drawn from 669\nannual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect\nclassification performance (accuracy of 99.37 percent, F1 score of 0.993),\noutperforming traditional baselines such as Logistic Regression, Naive Bayes,\nRandom Forest, and XGBoost. Interpretability was ensured through SHAP-based\ntoken attribution, while bias analysis and robustness checks confirmed the\nmodel's stability across sentence lengths, adversarial inputs, and temporal\nsamples. Theoretically, the study advances financial NLP by operationalizing\nfine-grained, theme-specific classification using transformer architectures.\nPractically, it offers a scalable, transparent solution for analysts,\nregulators, and scholars seeking to monitor the diffusion and framing of AI\nacross financial institutions.", "comment": "The FinAI-BERT model can be directly loaded via Hugging Face\n  Transformers (https://huggingface.co/bilalzafar/FinAI-BERT) for\n  sentence-level AI disclosure classification", "pdf_url": "http://arxiv.org/pdf/2507.01991v1", "cate": "q-fin.CP", "date": "2025-06-29", "updated": "2025-06-29", "AI": {"title_translation": "FinAI-BERT：一种基于Transformer的金融报告中AI披露语句级检测模型", "tldr": "FinAI-BERT是一个用于在金融报告中检测AI披露语句的Transformer模型，性能优异且可解释。", "motivation": "金融服务中人工智能的普及，促使人们对系统性检测公司文件中AI相关披露工具的需求不断增长。现有方法在粒度、可解释性和鲁棒性方面存在不足。", "method": "本研究引入了FinAI-BERT，这是一种领域适应的基于Transformer的语言模型，旨在对金融文本中的AI相关内容进行语句级分类。该模型在从669份美国银行年度报告（2015年至2023年）中提取的1,586个句子组成的手动整理和平衡数据集上进行了微调。通过基于SHAP的token归因确保可解释性，并通过偏置分析和鲁棒性检查证实了模型在句子长度、对抗性输入和时间样本方面的稳定性。", "result": "FinAI-BERT实现了接近完美的分类性能（准确率99.37%，F1分数0.993），优于Logistic Regression、Naive Bayes、Random Forest和XGBoost等传统基线。", "conclusion": "理论上，该研究通过使用Transformer架构实现细粒度、主题特定的分类，推动了金融NLP的发展。实践中，它为分析师、监管机构和学者提供了一个可扩展、透明的解决方案，以监测AI在金融机构中的传播和框架。", "translation": "人工智能（AI）在金融服务中的普及，促使人们对系统性检测公司文件中AI相关披露工具的需求不断增长。虽然现有方法通常依赖于关键词扩展或文档级分类，但它们在粒度、可解释性和鲁棒性方面存在不足。本研究引入了FinAI-BERT，这是一种领域适应的基于Transformer的语言模型，旨在对金融文本中的AI相关内容进行语句级分类。该模型在从669份美国银行年度报告（2015年至2023年）中提取的1,586个句子组成的手动整理和平衡数据集上进行了微调。FinAI-BERT实现了接近完美的分类性能（准确率99.37%，F1分数0.993），优于Logistic Regression、朴素贝叶斯、随机森林和XGBoost等传统基线。通过基于SHAP的token归因确保了可解释性，同时偏置分析和鲁棒性检查证实了模型在句子长度、对抗性输入和时间样本方面的稳定性。理论上，该研究通过使用Transformer架构实现细粒度、主题特定的分类，推动了金融NLP的发展。实践中，它为分析师、监管机构和学者提供了一个可扩展、透明的解决方案，以监测AI在金融机构中的传播和框架。", "summary": "本研究提出了FinAI-BERT，一个基于Transformer的领域适应语言模型，用于在金融报告中进行AI披露的语句级检测。该模型在包含1,586个句子的平衡数据集上进行微调，并取得了99.37%的准确率和0.993的F1分数，显著优于传统基线。FinAI-BERT还通过SHAP确保了可解释性，并通过偏置分析和鲁棒性检查验证了其稳定性。这项工作在理论上促进了金融NLP的发展，并为监测金融机构中AI的传播提供了一个实用的、可扩展的解决方案。", "keywords": "FinAI-BERT, AI披露检测, 金融NLP, Transformer, 语句级分类", "comments": "本研究的创新之处在于提出了FinAI-BERT模型，首次实现了金融报告中AI披露的语句级检测，解决了现有方法在粒度、可解释性和鲁棒性上的不足。其性能表现（接近完美的准确率和F1分数）令人印象深刻，并且通过SHAP确保了模型的可解释性，这对于金融领域的应用至关重要。此外，对模型鲁棒性的全面检查也增加了其在实际应用中的可信度。该模型对分析师、监管机构和学者监测AI在金融领域的扩散具有重要的实践意义。"}}
{"id": "2507.02373", "title": "UVLM: Benchmarking Video Language Model for Underwater World Understanding", "authors": ["Xizhe Xue", "Yang Zhou", "Dawei Yan", "Ying Li", "Haokui Zhang", "Rong Xiao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures, 3 tables", "url": "http://arxiv.org/abs/2507.02373v1", "summary": "Recently, the remarkable success of large language models (LLMs) has achieved\na profound impact on the field of artificial intelligence. Numerous advanced\nworks based on LLMs have been proposed and applied in various scenarios. Among\nthem, video language models (VidLMs) are particularly widely used. However,\nexisting works primarily focus on terrestrial scenarios, overlooking the highly\ndemanding application needs of underwater observation. To overcome this gap, we\nintroduce UVLM, an under water observation benchmark which is build through a\ncollaborative approach combining human expertise and AI models. To ensure data\nquality, we have conducted in-depth considerations from multiple perspectives.\nFirst, to address the unique challenges of underwater environments, we selected\nvideos that represent typical underwater challenges including light variations,\nwater turbidity, and diverse viewing angles to construct the dataset. Second,\nto ensure data diversity, the dataset covers a wide range of frame rates,\nresolutions, 419 classes of marine animals, and various static plants and\nterrains. Next, for task diversity, we adopted a structured design where\nobservation targets are categorized into two major classes: biological and\nenvironmental. Each category includes content observation and change/action\nobservation, totaling 20 distinct task types. Finally, we designed several\nchallenging evaluation metrics to enable quantitative comparison and analysis\nof different methods. Experiments on two representative VidLMs demonstrate that\nfine-tuning VidLMs on UVLM significantly improves underwater world\nunderstanding while also showing potential for slight improvements on existing\nin-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and\nprompt engineering will be released publicly.", "comment": "13 pages, 4 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.02373v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "UVLM：水下世界理解视频语言模型基准测试", "tldr": "UVLM是一个新的水下视频语言模型基准测试，旨在弥补现有模型对水下场景关注不足的缺陷，并显著提升水下世界理解能力。", "motivation": "现有视频语言模型主要关注陆地场景，忽视了水下观测的高要求应用需求。", "method": "引入了UVLM，一个通过结合人类专业知识和AI模型构建的水下观测基准。该基准在数据质量方面进行了深入考虑，包括：选择代表性水下挑战的视频，确保数据多样性（帧率、分辨率、419种海洋动物、植物和地形），设计任务多样性（生物和环境两大类，共20种任务类型），并设计了挑战性的评估指标。", "result": "在两个代表性视频语言模型上的实验表明，在UVLM上进行微调显著提高了水下世界理解能力，并且对现有陆地视频语言模型基准（如VideoMME和Perception text）也显示出轻微的改进潜力。", "conclusion": "UVLM基准测试有效提升了视频语言模型在水下环境中的理解能力，并有望为更广泛的视频语言模型研究提供帮助。数据集和提示工程将公开发布。", "translation": "最近，大型语言模型（LLMs）的卓越成功对人工智能领域产生了深远影响。许多基于LLMs的先进工作已被提出并应用于各种场景。其中，视频语言模型（VidLMs）的应用尤其广泛。然而，现有工作主要集中在陆地场景，忽视了水下观测的高要求应用需求。为了弥补这一空白，我们引入了UVLM，一个通过结合人类专业知识和AI模型构建的水下观测基准。为了确保数据质量，我们从多个角度进行了深入考虑。首先，为了应对水下环境的独特挑战，我们选择了代表典型水下挑战的视频来构建数据集，包括光线变化、水体浑浊和多样的视角。其次，为了确保数据多样性，数据集涵盖了广泛的帧率、分辨率、419种海洋动物以及各种静态植物和地形。接着，为了任务多样性，我们采用了结构化设计，将观测目标分为两大类：生物和环境。每个类别都包括内容观察和变化/行动观察，总计20种不同的任务类型。最后，我们设计了几种具有挑战性的评估指标，以实现对不同方法的定量比较和分析。在两个代表性VidLMs上的实验表明，在UVLM上对VidLMs进行微调显著提高了水下世界理解能力，同时对现有陆地VidLM基准（如VideoMME和Perception text）也显示出轻微的改进潜力。该数据集和提示工程将公开发布。", "summary": "本文提出了UVLM，一个专为水下观测设计的视频语言模型（VidLM）基准测试。针对现有VidLM主要关注陆地场景而忽略水下环境的不足，UVLM通过结合人类专业知识和AI模型构建，并从视频选择、数据多样性（涵盖多种海洋生物、植物、地形及帧率分辨率）、任务多样性（20种生物和环境观察任务）和评估指标设计等多方面确保数据质量。实验证明，在UVLM上对VidLM进行微调能够显著提升其水下世界理解能力，并对现有陆地VidLM基准表现出积极影响。该数据集和提示工程将公开可用。", "keywords": "视频语言模型, 水下观测, 基准测试, 数据集, 海洋理解", "comments": "UVLM的创新之处在于它是首个专门针对水下环境的视频语言模型基准测试，填补了该领域研究的空白。其重要性在于为水下AI应用（如海洋生物研究、水下机器人导航）提供了关键的评估和训练资源。通过精心设计的数据集和任务类型，UVLM有望推动视频语言模型在复杂水下环境理解方面的进步。"}}
{"id": "2507.02365", "title": "Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations", "authors": ["Muhammad Usama", "Dong Eui Chang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02365v1", "summary": "Equalizer parameter optimization for signal integrity in high-speed Dynamic\nRandom Access Memory systems is crucial but often computationally demanding or\nmodel-reliant. This paper introduces a data-driven framework employing learned\nlatent signal representations for efficient signal integrity evaluation,\ncoupled with a model-free Advantage Actor-Critic reinforcement learning agent\nfor parameter optimization. The latent representation captures vital signal\nintegrity features, offering a fast alternative to direct eye diagram analysis\nduring optimization, while the reinforcement learning agent derives optimal\nequalizer settings without explicit system models. Applied to industry-standard\nDynamic Random Access Memory waveforms, the method achieved significant\neye-opening window area improvements: 42.7\\% for cascaded Continuous-Time\nLinear Equalizer and Decision Feedback Equalizer structures, and 36.8\\% for\nDecision Feedback Equalizer-only configurations. These results demonstrate\nsuperior performance, computational efficiency, and robust generalization\nacross diverse Dynamic Random Access Memory units compared to existing\ntechniques. Core contributions include an efficient latent signal integrity\nmetric for optimization, a robust model-free reinforcement learning strategy,\nand validated superior performance for complex equalizer architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02365v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "采用潜在表征的深度强化学习DRAM均衡器参数优化", "tldr": "本文提出了一种基于深度强化学习和潜在信号表示的方法，用于高效优化DRAM均衡器参数，显著提升了信号完整性。", "motivation": "高速DRAM系统中均衡器参数优化对于信号完整性至关重要，但现有方法计算量大或依赖模型。", "method": "本文提出了一种数据驱动框架，结合学习到的潜在信号表示（用于高效信号完整性评估）和无模型优势Actor-Critic强化学习代理（用于参数优化）。潜在表示捕获关键信号完整性特征，替代直接眼图分析；强化学习代理在没有显式系统模型的情况下推导出最优均衡器设置。", "result": "应用于行业标准DRAM波形，级联CTLE和DFE结构眼图开窗面积提升42.7%，仅DFE配置提升36.8%。与现有技术相比，表现出卓越的性能、计算效率和在不同DRAM单元上的鲁棒泛化能力。", "conclusion": "本文的核心贡献包括：一种高效的用于优化的潜在信号完整性度量、一种鲁棒的无模型强化学习策略，以及对复杂均衡器架构的验证过的卓越性能。", "translation": "高速动态随机存取存储器系统中用于信号完整性的均衡器参数优化至关重要，但通常计算量大或依赖模型。本文引入了一种数据驱动框架，该框架利用学习到的潜在信号表示进行高效的信号完整性评估，并结合无模型的优势Actor-Critic强化学习代理进行参数优化。潜在表示捕获了重要的信号完整性特征，为优化期间的直接眼图分析提供了一种快速替代方案，而强化学习代理则在没有显式系统模型的情况下推导出最优均衡器设置。该方法应用于行业标准的动态随机存取存储器波形，实现了显著的眼图开窗面积改进：对于级联的连续时间线性均衡器（CTLE）和判决反馈均衡器（DFE）结构，改进了42.7%；对于仅判决反馈均衡器（DFE-only）配置，改进了36.8%。这些结果表明，与现有技术相比，该方法在性能、计算效率和在不同动态随机存取存储器单元上的鲁棒泛化能力方面均表现出卓越的性能。核心贡献包括：一种用于优化的有效潜在信号完整性度量、一种鲁棒的无模型强化学习策略，以及对复杂均衡器架构的验证过的卓越性能。", "summary": "本文提出了一种创新的数据驱动框架，利用学习到的潜在信号表示和无模型的优势Actor-Critic深度强化学习，解决高速DRAM系统中均衡器参数优化计算量大和模型依赖的问题。该方法通过潜在表示快速评估信号完整性，并通过强化学习自动寻优，无需显式系统模型。实验结果显示，该方法在DRAM波形上显著提升了眼图开窗面积，并展现出优越的性能、计算效率和泛化能力。", "keywords": "深度强化学习, DRAM, 均衡器优化, 信号完整性, 潜在表示", "comments": "本文通过结合潜在表示和深度强化学习，为DRAM均衡器参数优化提供了一个新颖且高效的解决方案。其创新点在于利用数据驱动的潜在表示替代传统的眼图分析，并采用无模型的强化学习策略，极大地提高了优化效率和泛化能力，对于高带宽存储系统的设计具有重要意义。"}}
{"id": "2507.02825", "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks", "authors": ["Yuxuan Zhu", "Tengjun Jin", "Yada Pruksachatkun", "Andy Zhang", "Shu Liu", "Sasha Cui", "Sayash Kapoor", "Shayne Longpre", "Kevin Meng", "Rebecca Weiss", "Fazl Barez", "Rahul Gupta", "Jwala Dhamala", "Jacob Merizian", "Mario Giulianelli", "Harry Coppock", "Cozmin Ududec", "Jasjeet Sekhon", "Jacob Steinhardt", "Antony Kellerman", "Sarah Schwettmann", "Matei Zaharia", "Ion Stoica", "Percy Liang", "Daniel Kang"], "categories": ["cs.AI", "A.1; I.2.m"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      39 pages, 15 tables, 6 figures", "url": "http://arxiv.org/abs/2507.02825v1", "summary": "Benchmarks are essential for quantitatively tracking progress in AI. As AI\nagents become increasingly capable, researchers and practitioners have\nintroduced agentic benchmarks to evaluate agents on complex, real-world tasks.\nThese benchmarks typically measure agent capabilities by evaluating task\noutcomes via specific reward designs. However, we show that many agentic\nbenchmarks have issues task setup or reward design. For example, SWE-bench\nVerified uses insufficient test cases, while TAU-bench counts empty responses\nas successful. Such issues can lead to under- or overestimation agents'\nperformance by up to 100% in relative terms. To make agentic evaluation\nrigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of\nguidelines that we synthesized from our benchmark-building experience, a survey\nof best practices, and previously reported issues. When applied to CVE-Bench, a\nbenchmark with a particularly complex evaluation design, ABC reduces the\nperformance overestimation by 33%.", "comment": "39 pages, 15 tables, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02825v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "建立严谨的智能体基准测试的最佳实践", "tldr": "现有智能体基准测试存在缺陷，导致评估不准确；本文提出了一个“智能体基准测试清单（ABC）”来建立更严谨的基准测试，并证明其有效性。", "motivation": "现有智能体基准测试在任务设置或奖励设计上存在问题，导致对智能体性能的评估不准确（可能高达100%的偏差），因此需要建立一套严谨的评估方法。", "method": "作者通过总结构建基准测试的经验、调查最佳实践以及分析已报告的问题，提出了一个名为“智能体基准测试清单（ABC）”的指导原则。", "result": "许多现有智能体基准测试（如SWE-bench Verified和TAU-bench）存在问题，导致性能评估偏差高达100%。应用ABC到CVE-Bench上，将性能高估降低了33%。", "conclusion": "提出的ABC清单能有效提高智能体基准测试的严谨性，纠正不准确的性能评估。", "translation": "基准测试对于定量跟踪人工智能的进展至关重要。随着人工智能智能体能力日益增强，研究人员和从业者引入了智能体基准测试，以评估智能体在复杂、真实世界任务中的表现。这些基准测试通常通过特定的奖励设计评估任务结果来衡量智能体能力。然而，我们发现许多智能体基准测试在任务设置或奖励设计上存在问题。例如，SWE-bench Verified 使用了不足的测试用例，而TAU-bench则将空响应计为成功。这些问题可能导致对智能体性能的低估或高估，相对偏差高达100%。为了使智能体评估更加严谨，我们引入了智能体基准测试清单（ABC），这是一套我们从构建基准测试的经验、最佳实践调查以及先前报告的问题中综合得出的指南。当应用于CVE-Bench（一个评估设计特别复杂的基准测试）时，ABC将性能高估降低了33%。", "summary": "本文指出当前智能体基准测试存在任务设置和奖励设计缺陷，导致智能体性能评估不准确。为解决此问题，作者提出了一个“智能体基准测试清单（ABC）”，该清单综合了构建经验、最佳实践和已知问题。实验证明，ABC能有效减少评估偏差，例如在CVE-Bench上将性能高估降低了33%，从而提高了智能体评估的严谨性。", "keywords": "智能体基准测试,评估,最佳实践,ABC清单,严谨性", "comments": "这篇论文的创新点在于提出了一个系统性的、基于实践经验和问题分析的“智能体基准测试清单（ABC）”，旨在解决当前智能体评估中普遍存在的严谨性不足问题。其重要性在于为未来智能体基准测试的构建提供了明确的指导原则，有助于更准确地衡量AI智能体的真实能力，从而推动AI领域的健康发展。"}}
{"id": "2507.02393", "title": "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection", "authors": ["Seokyeong Lee", "Sithu Aung", "Junyong Choi", "Seungryong Kim", "Ig-Jae Kim", "Junghyun Cho"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 16 figures", "url": "http://arxiv.org/abs/2507.02393v1", "summary": "Monocular 3D object detection (M3OD) has long faced challenges due to data\nscarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.\nAlthough various weakly supervised methods and pseudo-labeling methods have\nbeen proposed to address these issues, they are mostly limited by\ndomain-specific learning or rely solely on shape information from a single\nobservation. In this paper, we propose a novel pseudo-labeling framework that\nuses only video data and is more robust to occlusion, without requiring a\nmulti-view setup, additional sensors, camera poses, or domain-specific\ntraining. Specifically, we explore a technique for aggregating the\npseudo-LiDARs of both static and dynamic objects across temporally adjacent\nframes using object point tracking, enabling 3D attribute extraction in\nscenarios where 3D data acquisition is infeasible. Extensive experiments\ndemonstrate that our method ensures reliable accuracy and strong scalability,\nmaking it a practical and effective solution for M3OD.", "comment": "18 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2507.02393v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "PLOT：通过视频对象跟踪进行伪标签以实现可扩展单目3D对象检测", "tldr": "本文提出PLOT框架，利用视频对象跟踪聚合伪LiDAR，解决单目3D目标检测数据稀缺和2D-3D歧义问题，实现高精度和可扩展性。", "motivation": "单目3D目标检测（M3OD）长期面临数据稀缺（高标注成本）和固有的2D到3D歧义挑战。现有弱监督和伪标签方法受限于特定领域学习或仅依赖单次观测的形状信息。", "method": "本文提出一种新颖的伪标签框架PLOT，仅使用视频数据，对遮挡更鲁棒，无需多视角设置、额外传感器、相机姿态或特定领域训练。通过对象点跟踪，聚合静态和动态对象在时间相邻帧的伪LiDAR，以实现3D属性提取，尤其适用于3D数据获取不可行的情况。", "result": "大量实验表明，该方法确保了可靠的精度和强大的可扩展性。", "conclusion": "PLOT框架是单目3D目标检测领域一个实用且有效的解决方案，通过创新的伪标签策略克服了数据限制和歧义问题。", "translation": "单目3D目标检测（M3OD）长期以来一直面临数据稀缺（由高昂的标注成本引起）和固有的2D到3D歧义所带来的挑战。尽管已经提出了各种弱监督方法和伪标签方法来解决这些问题，但它们大多受限于特定领域学习或仅依赖于单一观测的形状信息。在本文中，我们提出了一种新颖的伪标签框架，该框架仅使用视频数据，对遮挡更具鲁棒性，无需多视角设置、额外的传感器、相机姿态或特定领域训练。具体来说，我们探索了一种通过对象点跟踪来聚合静态和动态对象在时间相邻帧的伪LiDAR的技术，从而在3D数据采集不可行的情况下实现3D属性提取。大量的实验表明，我们的方法确保了可靠的精度和强大的可扩展性，使其成为M3OD的一个实用且有效的解决方案。", "summary": "本文提出了一种名为PLOT的新型伪标签框架，用于解决单目3D目标检测（M3OD）中的数据稀缺和2D-3D歧义问题。该方法仅利用视频数据，通过对象点跟踪聚合相邻帧的伪LiDAR，以提取3D属性，无需额外传感器或特定领域训练。实验证明，PLOT在M3OD任务中表现出可靠的精度和强大的可扩展性，提供了一个实用且鲁棒的解决方案。", "keywords": "单目3D目标检测, 伪标签, 视频对象跟踪, 数据稀缺, 可扩展性", "comments": "该论文的创新点在于提出了一种无需多视角、额外传感器或特定领域训练的伪标签方法，通过利用视频数据中的时间信息和对象跟踪来生成伪LiDAR，有效解决了单目3D目标检测中的数据稀缺和2D-3D歧义问题。其强调的鲁棒性（对遮挡）和可扩展性使其在实际应用中具有重要潜力。"}}
{"id": "2507.02406", "title": "Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization", "authors": ["Caio Azevedo", "Lina Achaji", "Stefano Sabatini", "Nicola Poerio", "Grzegorz Bartyzel", "Sascha Hornauer", "Fabien Moutarde"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for publication at ITSC 2025", "url": "http://arxiv.org/abs/2507.02406v1", "summary": "Trajectory prediction is an essential step in the pipeline of an autonomous\nvehicle. Inaccurate or inconsistent predictions regarding the movement of\nagents in its surroundings lead to poorly planned maneuvers and potentially\ndangerous situations for the end-user. Current state-of-the-art\ndeep-learning-based trajectory prediction models can achieve excellent accuracy\non public datasets. However, when used in more complex, interactive scenarios,\nthey often fail to capture important interdependencies between agents, leading\nto inconsistent predictions among agents in the traffic scene. Inspired by the\nefficacy of incorporating human preference into large language models, this\nwork fine-tunes trajectory prediction models in multi-agent settings using\npreference optimization. By taking as input automatically calculated preference\nrankings among predicted futures in the fine-tuning process, our\nexperiments--using state-of-the-art models on three separate datasets--show\nthat we are able to significantly improve scene consistency while minimally\nsacrificing trajectory prediction accuracy and without adding any excess\ncomputational requirements at inference time.", "comment": "Accepted for publication at ITSC 2025", "pdf_url": "http://arxiv.org/pdf/2507.02406v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过偏好优化提高车辆轨迹预测的一致性", "tldr": "当前最先进的深度学习轨迹预测模型在复杂的、多代理人场景中存在一致性问题。本文提出使用偏好优化来微调这些模型，显著提高了场景一致性，同时对预测准确性影响最小，且推理时没有增加额外计算负担。", "motivation": "尽管当前最先进的深度学习轨迹预测模型在公共数据集上表现出优异的准确性，但在更复杂、交互式的场景中，它们往往无法捕捉代理人之间重要的相互依赖关系，导致交通场景中代理人之间的预测不一致。这种不准确或不一致的预测会导致自动驾驶汽车规划不佳的机动，并可能给最终用户带来危险情况。", "method": "受将人类偏好纳入大型语言模型有效性的启发，本文利用偏好优化在多代理人设置中微调轨迹预测模型。具体方法是在微调过程中，将预测未来之间自动计算的偏好排名作为输入。", "result": "实验结果表明，在三个独立数据集上使用最先进的模型进行测试，能够显著提高场景一致性，同时最小程度地牺牲轨迹预测准确性，并且在推理时没有增加任何额外的计算要求。", "conclusion": "偏好优化是一种有效的方法，可以提高多代理人轨迹预测的一致性，且没有显著的缺点。", "translation": "轨迹预测是自动驾驶汽车管线中的一个重要步骤。对其周围代理人运动的不准确或不一致的预测会导致规划不佳的机动，并可能给最终用户带来危险情况。当前最先进的基于深度学习的轨迹预测模型可以在公共数据集上实现出色的准确性。然而，当在更复杂、交互式的场景中使用时，它们常常未能捕捉到代理人之间重要的相互依赖关系，导致交通场景中代理人之间的预测不一致。受将人类偏好纳入大型语言模型有效性的启发，这项工作利用偏好优化在多代理人设置中微调轨迹预测模型。通过在微调过程中将预测未来之间自动计算的偏好排名作为输入，我们的实验——使用最先进的模型在三个独立数据集上——表明我们能够显著提高场景一致性，同时最小程度地牺牲轨迹预测准确性，并且在推理时没有增加任何额外的计算要求。", "summary": "本文旨在解决当前深度学习轨迹预测模型在复杂多代理人场景中存在的预测不一致性问题。受大型语言模型中融入偏好机制的启发，该研究提出通过偏好优化来微调轨迹预测模型，其中利用自动计算的预测未来偏好排名作为输入。实验结果表明，该方法在不增加推理计算成本的前提下，显著提高了场景一致性，同时对轨迹预测准确性的影响微乎其微。", "keywords": "轨迹预测, 偏好优化, 多代理人系统, 自动驾驶, 一致性", "comments": "该论文的创新之处在于将大型语言模型中成功的偏好优化概念应用于多代理人轨迹预测领域，这在自动驾驶领域具有重要的实际意义。它有效地解决了轨迹预测中的一致性关键问题，并证明了其方法在不增加推理计算成本的情况下能够显著提升性能，这是一个重要的优势。"}}
{"id": "2507.02841", "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason", "authors": ["Kaiyi Zhang", "Ang Lv", "Jinpeng Li", "Yongbo Wang", "Feng Wang", "Haoyuan Hu", "Rui Yan"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02841v1", "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02841v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "StepHint：多级分步提示增强强化学习推理能力", "tldr": "StepHint通过多级分步提示解决RLVR中近失奖励和探索停滞问题，提高LLM的推理能力和训练效率。", "motivation": "现有可验证奖励强化学习（RLVR）面临两大挑战：近失奖励问题（微小错误导致推理过程无效，降低训练效率）和探索停滞（模型倾向于在“舒适区”内探索，缺乏探索更有效替代方案的动力）。", "method": "提出StepHint算法，利用多级分步提示增强模型探索解空间的能力。它从更强的模型生成有效推理链，并使用自适应分区方法将链条分成推理步骤。将前几步用作提示，同时提供不同步数的多级提示给模型。这种方法将模型的探索引向一个有前景的解决方案子空间，同时保留其独立探索的灵活性。通过提供提示，StepHint缓解了近失奖励问题，提高了训练效率，并帮助模型发展更好的推理能力，解决探索停滞。", "result": "StepHint在六个数学基准测试中优于竞争性RLVR增强方法，并展示出卓越的泛化能力，在域外基准测试中也优于基线。", "conclusion": "StepHint通过引入多级分步提示，有效解决了RLVR中近失奖励和探索停滞的挑战，显著提升了大型语言模型的推理能力、训练效率和泛化性能。", "translation": "可验证奖励强化学习 (RLVR) 是一种有前途的方法，用于提高大型语言模型 (LLM) 的复杂推理能力。然而，当前的 RLVR 方法面临两个重大挑战：近失奖励问题，即一个小错误可能导致原本正确的推理过程失效，从而极大地阻碍训练效率；以及探索停滞，即模型倾向于专注于其“舒适区”内的解决方案，缺乏探索可能更有效替代方案的动力。为了解决这些挑战，我们提出了 StepHint，一种新颖的 RLVR 算法，它利用多级分步提示来帮助模型更有效地探索解决方案空间。StepHint 从更强的模型生成有效的推理链，并使用我们提出的自适应分区方法将这些链分解为推理步骤。前几个步骤用作提示，同时向模型提供多级提示（每个提示包含不同数量的步骤）。这种方法将模型的探索引向一个有前景的解决方案子空间，同时保留其独立探索的灵活性。通过提供提示，StepHint 缓解了近失奖励问题，从而提高了训练效率。此外，外部推理路径有助于模型发展更好的推理能力，使其能够超越其“舒适区”并缓解探索停滞。StepHint 在六个数学基准测试中优于竞争性 RLVR 增强方法，同时还展示了卓越的泛化能力，并在域外基准测试中优于基线。", "summary": "StepHint是一种新型的可验证奖励强化学习（RLVR）算法，旨在通过提供多级分步提示来解决LLM在复杂推理中面临的近失奖励问题和探索停滞问题。该方法从更强的模型生成推理链并进行自适应分区，将初始步骤作为提示，引导模型高效探索解空间。实验表明，StepHint在多个数学基准测试中显著优于现有RLVR方法，并展现出更强的泛化能力。", "keywords": "强化学习, 大型语言模型, 推理, 分步提示, 近失奖励", "comments": "StepHint的创新之处在于其“多级分步提示”机制，这有效地结合了外部知识引导和模型自主探索的平衡。它直接解决了强化学习中常见的稀疏奖励（近失奖励问题）和局部最优（探索停滞）问题，对于提升LLM在复杂推理任务上的表现具有重要意义。其通过“更强模型”生成提示的思路也提供了一个实用的知识蒸馏或引导范式。"}}
{"id": "2507.02395", "title": "Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis", "authors": ["Byung Hyun Lee", "Wongi Jeong", "Woojae Han", "Kyoungbun Lee", "Se Young Chun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.02395v1", "summary": "Multiple instance learning (MIL) significantly reduced annotation costs via\nbag-level weak labels for large-scale images, such as histopathological whole\nslide images (WSIs). However, its adaptability to continual tasks with minimal\nforgetting has been rarely explored, especially on instance classification for\nlocalization. Weakly incremental learning for semantic segmentation has been\nstudied for continual localization, but it focused on natural images,\nleveraging global relationships among hundreds of small patches (e.g., $16\n\\times 16$) using pre-trained models. This approach seems infeasible for MIL\nlocalization due to enormous amounts ($\\sim 10^5$) of large patches (e.g., $256\n\\times 256$) and no available global relationships such as cancer cells. To\naddress these challenges, we propose Continual Multiple Instance Learning with\nEnhanced Localization (CoMEL), an MIL framework for both localization and\nadaptability with minimal forgetting. CoMEL consists of (1) Grouped Double\nAttention Transformer (GDAT) for efficient instance encoding, (2) Bag\nPrototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling,\nand (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting\nin both bag and instance classification. Extensive experiments on three public\nWSI datasets demonstrate superior performance of CoMEL, outperforming the prior\narts by up to $11.00\\%$ in bag-level accuracy and up to $23.4\\%$ in\nlocalization accuracy under the continual MIL setup.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02395v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "组织病理学全玻片图像分析中增强定位的持续多示例学习", "tldr": "本文提出了CoMEL，一个用于组织病理学全玻片图像分析的持续多示例学习框架，它通过新颖的组件解决了持续任务中的定位和遗忘问题，并显著提高了袋级和定位准确性。", "motivation": "多示例学习（MIL）在持续任务中适应性差，尤其是在实例分类定位方面。现有针对持续定位的弱增量学习方法不适用于MIL定位，因为其数据量巨大且缺乏全局关系。", "method": "提出了Continual Multiple Instance Learning with Enhanced Localization (CoMEL) 框架，其包含：1. Grouped Double Attention Transformer (GDAT) 用于高效实例编码。2. Bag Prototypes-based Pseudo-Labeling (BPPL) 用于可靠的实例伪标签。3. Orthogonal Weighted Low-Rank Adaptation (OWLoRA) 用于减轻袋级和实例分类中的遗忘。", "result": "CoMEL在三个公共WSI数据集上的实验表明其性能优越，在持续MIL设置下，袋级准确率比现有技术高出11.00%，定位准确率高出23.4%。", "conclusion": "CoMEL有效地解决了组织病理学全玻片图像分析中持续多示例学习的挑战，特别是在定位和遗忘缓解方面，并取得了显著的性能提升。", "translation": "多示例学习（MIL）通过袋级弱标签显著降低了组织病理学全玻片图像（WSI）等大规模图像的标注成本。然而，其在持续任务中以最小遗忘适应性的探索很少，尤其是在实例分类定位方面。弱增量学习用于语义分割已被研究用于持续定位，但它侧重于自然图像，利用数百个小补丁（例如，$16 \\times 16$）之间的全局关系，并使用预训练模型。这种方法对于MIL定位似乎不可行，因为存在大量的（约$10^5$）大补丁（例如，$256 \\times 256$）且没有可用的全局关系（如癌细胞）。为了解决这些挑战，我们提出了持续多示例学习与增强定位（CoMEL），这是一个用于定位和适应性且遗忘最小的MIL框架。CoMEL由（1）用于高效实例编码的分组双注意力变换器（GDAT），（2）基于袋原型伪标签（BPPL）用于可靠的实例伪标签，以及（3）正交加权低秩适应（OWLoRA）以减轻袋级和实例分类中的遗忘组成。在三个公共WSI数据集上的大量实验表明，CoMEL表现优越，在持续MIL设置下，袋级准确率比现有技术高出11.00%，定位准确率高出23.4%。", "summary": "本文提出了CoMEL框架，旨在解决组织病理学全玻片图像分析中持续多示例学习（MIL）的定位和遗忘问题。CoMEL结合了GDAT进行高效实例编码、BPPL进行可靠实例伪标签以及OWLoRA以减轻遗忘。实验证明，CoMEL在袋级和定位准确性方面均显著优于现有方法，为持续MIL任务提供了有效解决方案。", "keywords": "多示例学习, 持续学习, 组织病理学图像, 全玻片图像, 定位", "comments": "该论文创新性地将持续学习引入多示例学习领域，特别关注了组织病理学图像分析中的实例定位挑战。其提出的CoMEL框架，通过GDAT、BPPL和OWLoRA的结合，有效地解决了大规模医疗图像分析中数据量大、全局关系缺失以及遗忘等关键问题，为未来医疗AI的发展提供了有价值的参考。"}}
{"id": "2507.02409", "title": "S2FGL: Spatial Spectral Federated Graph Learning", "authors": ["Zihan Tan", "Suyuan Huang", "Guancheng Wan", "Wenke Huang", "He Li", "Mang Ye"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02409v1", "summary": "Federated Graph Learning (FGL) combines the privacy-preserving capabilities\nof federated learning (FL) with the strong graph modeling capability of Graph\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\nstructural perspective, neglecting the propagation of graph signals on spatial\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\nintroduces edge disconnections between clients, leading to disruptions in label\nsignals and a degradation in the class knowledge of the global GNN. From a\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\npropagation schemes. As a result, spectral client drifts occur, undermining\nglobal generalizability. To tackle the challenges, we propose a global\nknowledge repository to mitigate label signal disruption and a frequency\nalignment to address spectral client drifts. The combination of spatial and\nspectral strategies forms our framework S2FGL. Extensive experiments on\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\nat https://github.com/Wonder7racer/S2FGL.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02409v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "S2FGL：空间谱联邦图学习", "tldr": "本文提出了S2FGL框架，旨在解决联邦图学习中因忽视图信号在空间和谱域传播而导致的问题，通过全局知识库和频率对齐策略提升了性能。", "motivation": "当前联邦图学习（FGL）研究仅从结构角度处理子图联邦学习，忽略了图信号在空间和谱域的传播。这导致空间域的标签信号中断（影响全局GNN的类知识）和谱域的客户端漂移（损害全局泛化能力），因此需要新的方法来解决这些挑战。", "method": "本文提出了S2FGL框架，结合了空间和谱策略。具体而言，通过引入一个全局知识库来缓解标签信号中断问题，并采用频率对齐机制来解决谱客户端漂移问题。", "result": "在多个数据集上的大量实验表明S2FGL框架具有优越的性能。", "conclusion": "S2FGL框架通过同时解决联邦图学习中空间域的标签信号中断和谱域的客户端漂移问题，显著提升了模型的性能和泛化能力。", "translation": "联邦图学习（FGL）结合了联邦学习（FL）的隐私保护能力和图神经网络（GNN）强大的图建模能力。当前研究仅从结构角度处理子图联邦学习，忽略了图信号在结构的空间和谱域的传播。从空间角度看，子图联邦学习引入了客户端之间的边缘断开，导致标签信号中断，并降低了全局GNN的类知识。从谱角度看，谱异质性导致子图间信号频率的不一致，使得本地GNN过度拟合本地信号传播方案。结果，发生谱客户端漂移，损害了全局泛化能力。为了解决这些挑战，我们提出了一个全局知识库来缓解标签信号中断，并采用频率对齐来解决谱客户端漂移。空间和谱策略的结合形成了我们的S2FGL框架。在多个数据集上的大量实验证明了S2FGL的优越性。代码可在https://github.com/Wonder7racer/S2FGL.git获取。", "summary": "本文提出了S2FGL框架，旨在解决联邦图学习（FGL）中，现有方法忽视图信号在空间和谱域传播所带来的挑战。针对空间域的标签信号中断问题，S2FGL引入了全局知识库；针对谱域的客户端漂移问题，S2FGL采用了频率对齐机制。通过结合这些空间和谱策略，S2FGL在多个数据集上表现出优越的性能，有效提升了联邦图学习的类知识和全局泛化能力。", "keywords": "联邦图学习, 空间谱, 图神经网络, 隐私保护, 频率对齐", "comments": "这篇论文通过同时考虑联邦图学习中的空间和谱域问题，提出了S2FGL框架，具有创新性。它解决了现有方法在处理子图FL时忽略图信号传播的关键限制，特别是通过引入全局知识库和频率对齐来缓解标签信号中断和客户端漂移，提升了模型的泛化能力和类知识，为联邦图学习领域提供了新的视角和解决方案。"}}
{"id": "2507.00884", "title": "A Scalable and Quantum-Accurate Foundation Model for Biomolecular Force Field via Linearly Tensorized Quadrangle Attention", "authors": ["Qun Su", "Kai Zhu", "Qiaolin Gou", "Jintu Zhang", "Renling Hu", "Yurong Li", "Yongze Wang", "Hui Zhang", "Ziyi You", "Linlong Jiang", "Yu Kang", "Jike Wang", "Chang-Yu Hsieh", "Tingjun Hou"], "categories": ["physics.chem-ph", "cs.AI", "cs.LG", "physics.bio-ph"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00884v1", "summary": "Accurate atomistic biomolecular simulations are vital for disease mechanism\nunderstanding, drug discovery, and biomaterial design, but existing simulation\nmethods exhibit significant limitations. Classical force fields are efficient\nbut lack accuracy for transition states and fine conformational details\ncritical in many chemical and biological processes. Quantum Mechanics (QM)\nmethods are highly accurate but computationally infeasible for large-scale or\nlong-time simulations. AI-based force fields (AIFFs) aim to achieve QM-level\naccuracy with efficiency but struggle to balance many-body modeling complexity,\naccuracy, and speed, often constrained by limited training data and\ninsufficient validation for generalizability. To overcome these challenges, we\nintroduce LiTEN, a novel equivariant neural network with Tensorized Quadrangle\nAttention (TQA). TQA efficiently models three- and four-body interactions with\nlinear complexity by reparameterizing high-order tensor features via vector\noperations, avoiding costly spherical harmonics. Building on LiTEN, LiTEN-FF is\na robust AIFF foundation model, pre-trained on the extensive nablaDFT dataset\nfor broad chemical generalization and fine-tuned on SPICE for accurate solvated\nsystem simulations. LiTEN achieves state-of-the-art (SOTA) performance across\nmost evaluation subsets of rMD17, MD22, and Chignolin, outperforming leading\nmodels such as MACE, NequIP, and EquiFormer. LiTEN-FF enables the most\ncomprehensive suite of downstream biomolecular modeling tasks to date,\nincluding QM-level conformer searches, geometry optimization, and free energy\nsurface construction, while offering 10x faster inference than MACE-OFF for\nlarge biomolecules (~1000 atoms). In summary, we present a physically grounded,\nhighly efficient framework that advances complex biomolecular modeling,\nproviding a versatile foundation for drug discovery and related applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00884v1", "cate": "physics.chem-ph", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "通过线性张量化四边形注意力构建可扩展且量子精确的生物分子力场基础模型", "tldr": "LiTEN-FF是一个新的AI力场基础模型，通过引入线性张量化四边形注意力（TQA）高效建模多体相互作用，实现了量子级别的精度和更快的推理速度，在多种生物分子模拟任务中表现优异。", "motivation": "现有的生物分子模拟方法存在局限性：经典力场效率高但精度不足，量子力学方法精度高但计算成本过高，而现有AI力场难以平衡多体建模复杂性、精度和速度，且受限于数据和泛化性。", "method": "本文提出了LiTEN，一个新颖的等变神经网络，其核心是张量化四边形注意力（TQA）。TQA通过向量操作重新参数化高阶张量特征，以线性复杂度高效模拟三体和四体相互作用，避免了昂贵的球谐函数。在此基础上，构建了LiTEN-FF，这是一个鲁棒的AI力场基础模型，在nablaDFT数据集上进行预训练以实现广泛的化学泛化，并在SPICE数据集上进行微调以实现精确的溶剂化系统模拟。", "result": "LiTEN在rMD17、MD22和Chignolin的大多数评估子集上取得了最先进（SOTA）的性能，优于MACE、NequIP和EquiFormer等领先模型。LiTEN-FF支持迄今为止最全面的下游生物分子建模任务，包括量子级别的构象搜索、几何优化和自由能表面构建，同时对于大型生物分子（约1000个原子）的推理速度比MACE-OFF快10倍。", "conclusion": "本文提出了一个物理基础扎实、高效的框架LiTEN-FF，它推进了复杂的生物分子建模，为药物发现及相关应用提供了多功能的基础。", "translation": "精确的原子级生物分子模拟对于理解疾病机制、药物发现和生物材料设计至关重要，但现有模拟方法存在显著局限性。经典力场效率高但对过渡态和许多化学与生物过程中关键的精细构象细节缺乏准确性。量子力学（QM）方法高度准确但对于大规模或长时间模拟而言计算上不可行。基于AI的力场（AIFFs）旨在以高效的方式实现QM级别的精度，但难以平衡多体建模复杂性、精度和速度，通常受限于有限的训练数据和不足的泛化性验证。为了克服这些挑战，我们引入了LiTEN，一种带有张量化四边形注意力（TQA）的新型等变神经网络。TQA通过向量操作重新参数化高阶张量特征，以线性复杂度高效地模拟三体和四体相互作用，避免了昂贵的球谐函数。基于LiTEN，LiTEN-FF是一个鲁棒的AIFF基础模型，在广泛的nablaDFT数据集上进行预训练以实现广泛的化学泛化，并在SPICE上进行微调以实现精确的溶剂化系统模拟。LiTEN在rMD17、MD22和Chignolin的大多数评估子集上取得了最先进（SOTA）的性能，超越了MACE、NequIP和EquiFormer等领先模型。LiTEN-FF支持迄今为止最全面的下游生物分子建模任务，包括QM级别的构象搜索、几何优化和自由能表面构建，同时对于大型生物分子（约1000个原子）的推理速度比MACE-OFF快10倍。总而言之，我们提出了一个物理基础扎实、高效的框架，它推进了复杂的生物分子建模，为药物发现和相关应用提供了多功能的基础。", "summary": "本文提出了一种名为LiTEN-FF的新型AI力场基础模型，旨在解决传统力场精度不足和量子力学方法计算成本过高的问题。LiTEN-FF核心是等变神经网络LiTEN，其引入了线性张量化四边形注意力（TQA），能够以线性复杂度高效地建模三体和四体相互作用。该模型在nablaDFT数据集上预训练以提高泛化性，并在SPICE上微调以适应溶剂化系统。实验结果表明，LiTEN在多个基准测试中达到了SOTA性能，并能以10倍的速度优势执行多种复杂的生物分子模拟任务，为药物发现提供了高效精确的工具。", "keywords": "生物分子力场, 深度学习, 张量化四边形注意力, 量子精度, 可扩展性", "comments": "LiTEN-FF通过引入线性张量化四边形注意力（TQA）有效地解决了多体相互作用的建模难题，同时实现了量子级别的精度和显著的计算效率提升。其在大型数据集上的预训练和微调策略增强了模型的泛化能力和对复杂系统的适用性。该工作为生物分子模拟领域带来了重要的创新，尤其是在药物发现和材料设计方面具有巨大的应用潜力。"}}
{"id": "2507.02398", "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection", "authors": ["Taehoon Kim", "Jongwook Choi", "Yonghyun Jeong", "Haeun Noh", "Jaejun Yoo", "Seungryul Baek", "Jongwon Choi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by iccv 2025. code is will be available at this https URL", "url": "http://arxiv.org/abs/2507.02398v1", "summary": "We introduce a deepfake video detection approach that exploits pixel-wise\ntemporal inconsistencies, which traditional spatial frequency-based detectors\noften overlook. Traditional detectors represent temporal information merely by\nstacking spatial frequency spectra across frames, resulting in the failure to\ndetect temporal artifacts in the pixel plane. Our approach performs a 1D\nFourier transform on the time axis for each pixel, extracting features highly\nsensitive to temporal inconsistencies, especially in areas prone to unnatural\nmovements. To precisely locate regions containing the temporal artifacts, we\nintroduce an attention proposal module trained in an end-to-end manner.\nAdditionally, our joint transformer module effectively integrates pixel-wise\ntemporal frequency features with spatio-temporal context features, expanding\nthe range of detectable forgery artifacts. Our framework represents a\nsignificant advancement in deepfake video detection, providing robust\nperformance across diverse and challenging detection scenarios.", "comment": "accepted by iccv 2025. code is will be available at\n  https://github.com/rama0126/PwTF-DVD", "pdf_url": "http://arxiv.org/pdf/2507.02398v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "超越空间频率：基于像素级时间频率的深度伪造视频检测", "tldr": "本文提出了一种基于像素级时间频率的深度伪造视频检测方法，通过分析像素级时间不一致性来弥补传统空间频率方法的不足，并结合时空上下文，实现对伪造视频的鲁棒检测。", "motivation": "传统深度伪造检测器主要依赖空间频率分析，但忽略了像素级的时间不一致性，导致无法检测像素平面中的时间伪影。", "method": "该方法对每个像素的时间轴进行一维傅里叶变换，提取对时间不一致性敏感的特征。引入了一个端到端训练的注意力提议模块来精确定位时间伪影区域，并使用一个联合Transformer模块将像素级时间频率特征与时空上下文特征有效整合。", "result": "该框架在各种多样化和具有挑战性的深度伪造检测场景中表现出强大的性能。", "conclusion": "所提出的框架通过有效利用像素级时间不一致性，代表了深度伪造视频检测领域的重大进步。", "translation": "我们引入了一种深度伪造视频检测方法，该方法利用像素级时间不一致性，这是传统基于空间频率的检测器经常忽略的。传统检测器仅通过在帧之间堆叠空间频率谱来表示时间信息，导致无法检测像素平面中的时间伪影。我们的方法对每个像素的时间轴进行一维傅里叶变换，提取对时间不一致性高度敏感的特征，特别是在容易出现不自然运动的区域。为了精确定位包含时间伪影的区域，我们引入了一个端到端训练的注意力提议模块。此外，我们的联合Transformer模块有效地将像素级时间频率特征与时空上下文特征相结合，扩大了可检测的伪造伪影范围。我们的框架代表了深度伪造视频检测的重大进步，在各种具有挑战性的检测场景中提供了强大的性能。", "summary": "本文提出了一种新颖的深度伪造视频检测方法，通过关注像素级时间不一致性来超越传统的空间频率分析。该方法对每个像素的时间轴进行一维傅里叶变换以提取敏感的时间频率特征。引入注意力提议模块辅助定位伪影，并通过联合Transformer模块将这些特征与时空上下文相结合，从而在各种挑战性场景中实现鲁棒的检测性能。", "keywords": "深度伪造检测, 时间频率, 像素级分析, 傅里叶变换, 时空上下文", "comments": "该论文的创新之处在于将检测重心从空间频率转移到像素级时间频率分析，直接针对传统方法常忽略的时间伪影。对每个像素进行一维傅里叶变换以及引入注意力机制进行定位是其关键新颖点。这种方法通过针对以前被忽视的不一致性类型，显著推动了深度伪造检测技术的发展。"}}
{"id": "2507.02466", "title": "Variational Kolmogorov-Arnold Network", "authors": ["Francesco Alesiani", "Henrik Christiansen", "Federico Errica"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      A preliminary (short paper) version presented at ComBayNS Workshop at IJCNN'25", "url": "http://arxiv.org/abs/2507.02466v1", "summary": "Kolmogorov Arnold Networks (KANs) are an emerging architecture for building\nmachine learning models. KANs are based on the theoretical foundation of the\nKolmogorov-Arnold Theorem and its expansions, which provide an exact\nrepresentation of a multi-variate continuous bounded function as the\ncomposition of a limited number of univariate continuous functions. While such\ntheoretical results are powerful, their use as a representation learning\nalternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of\nthe number of bases modeling each of the univariate functions. In this work, we\nshow how to address this problem by adaptively learning a potentially infinite\nnumber of bases for each univariate function during training. We therefore\nmodel the problem as a variational inference optimization problem. Our\nproposal, called InfinityKAN, which uses backpropagation, extends the potential\napplicability of KANs by treating an important hyperparameter as part of the\nlearning process.", "comment": "A preliminary (short paper) version presented at ComBayNS Workshop at\n  IJCNN'25", "pdf_url": "http://arxiv.org/pdf/2507.02466v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "变分科尔莫哥洛夫-阿诺德网络", "tldr": "KANs需要手动选择基函数数量，本文提出InfinityKAN，通过变分推断自适应学习潜在无限数量的基函数，扩展了KAN的应用。", "motivation": "KANs作为多层感知器（MLP）的表示学习替代方案，其使用取决于对每个单变量函数建模的基函数数量的临时选择，这是一个需要解决的问题。", "method": "本文通过将问题建模为变分推断优化问题，实现了在训练期间自适应地学习每个单变量函数的潜在无限数量的基函数。提出的方法名为InfinityKAN，并使用反向传播进行训练。", "result": "InfinityKAN通过将一个重要的超参数（基函数数量）视为学习过程的一部分，从而扩展了科尔莫哥洛夫-阿诺德网络（KAN）的潜在适用性。", "conclusion": "通过将一个关键超参数（基函数数量）纳入学习过程，InfinityKAN解决了传统KAN的一个主要限制，提升了其作为MLP替代方案的适用性和潜力。", "translation": "科尔莫哥洛夫-阿诺德网络（KAN）是一种新兴的机器学习模型构建架构。KAN基于科尔莫哥洛夫-阿诺德定理及其扩展的理论基础，该定理提供了一种将多变量连续有界函数精确表示为有限数量的单变量连续函数组合的方法。尽管这些理论结果很强大，但它们作为多层感知器（MLP）的表示学习替代方案，其使用取决于对每个单变量函数建模的基函数数量的临时选择。在这项工作中，我们展示了如何通过在训练期间自适应地学习每个单变量函数的潜在无限数量的基函数来解决这个问题。因此，我们将该问题建模为变分推断优化问题。我们提出的方法，名为InfinityKAN，它使用反向传播，通过将一个重要的超参数视为学习过程的一部分，扩展了KAN的潜在适用性。", "summary": "本文提出InfinityKAN，一种改进的科尔莫哥洛夫-阿诺德网络（KAN）架构。针对传统KAN需要手动选择单变量函数基函数数量的问题，InfinityKAN通过将此问题建模为变分推断优化，实现了在训练过程中自适应学习潜在无限数量的基函数。这使得KAN的一个关键超参数成为学习过程的一部分，从而显著扩展了KAN作为多层感知器替代方案的适用性。", "keywords": "Kolmogorov-Arnold Network, KAN, Variational Inference, InfinityKAN, Machine Learning", "comments": "这篇论文的创新点在于将KANs中一个关键的超参数（基函数数量）从手动选择转变为自适应学习，通过引入变分推断优化框架解决了这一问题。InfinityKAN的提出，通过使模型能够学习更复杂的函数表示，有望提高KANs的实用性和泛化能力，使其在实际应用中更具竞争力。"}}
{"id": "2507.02399", "title": "TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation", "authors": ["Peilin Zhang", "Shaouxan Wua", "Jun Feng", "Zhuo Jin", "Zhizezhang Gao", "Jingkun Chen", "Yaqiong Xing", "Xiao Zhang"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02399v1", "summary": "Background and objective: Medical image segmentation is a core task in\nvarious clinical applications. However, acquiring large-scale, fully annotated\nmedical image datasets is both time-consuming and costly. Scribble annotations,\nas a form of sparse labeling, provide an efficient and cost-effective\nalternative for medical image segmentation. However, the sparsity of scribble\nannotations limits the feature learning of the target region and lacks\nsufficient boundary supervision, which poses significant challenges for\ntraining segmentation networks. Methods: We propose TAB Net, a novel\nweakly-supervised medical image segmentation framework, consisting of two key\ncomponents: the triplet augmentation self-recovery (TAS) module and the\nboundary-aware pseudo-label supervision (BAP) module. The TAS module enhances\nfeature learning through three complementary augmentation strategies: intensity\ntransformation improves the model's sensitivity to texture and contrast\nvariations, cutout forces the network to capture local anatomical structures by\nmasking key regions, and jigsaw augmentation strengthens the modeling of global\nanatomical layout by disrupting spatial continuity. By guiding the network to\nrecover complete masks from diverse augmented inputs, TAS promotes a deeper\nsemantic understanding of medical images under sparse supervision. The BAP\nmodule enhances pseudo-supervision accuracy and boundary modeling by fusing\ndual-branch predictions into a loss-weighted pseudo-label and introducing a\nboundary-aware loss for fine-grained contour refinement. Results: Experimental\nevaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB\nNet significantly outperforms state-of-the-art methods for scribble-based\nweakly supervised segmentation. Moreover, it achieves performance comparable to\nthat of fully supervised methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02399v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "TABNet：一种用于医学图像分割的边界感知伪标签三元组增强自恢复框架", "tldr": "本文提出了TABNet，一个新颖的弱监督医学图像分割框架，通过结合三元组增强自恢复（TAS）模块和边界感知伪标签监督（BAP）模块，有效解决了稀疏涂鸦标注在医学图像分割中的挑战，并在弱监督设置下实现了与全监督方法相当的性能。", "motivation": "医学图像分割是临床应用中的核心任务，但获取大规模、完全标注的数据集耗时且成本高昂。涂鸦标注作为一种高效的稀疏标注形式，其稀疏性限制了目标区域的特征学习，并缺乏足够的边界监督，这给训练分割网络带来了显著挑战。", "method": "本文提出了TABNet，一个新颖的弱监督医学图像分割框架，包含两个关键模块：\n1. 三元组增强自恢复（TAS）模块：通过强度变换、裁剪（cutout）和拼图（jigsaw）三种互补的增强策略，增强特征学习，并引导网络从多样化的增强输入中恢复完整掩膜，以在稀疏监督下促进对医学图像的更深层语义理解。\n2. 边界感知伪标签监督（BAP）模块：通过将双分支预测融合为损失加权的伪标签，并引入边界感知损失以进行精细的轮廓细化，从而提高伪监督的准确性和边界建模能力。", "result": "在ACDC和MSCMR seg两个公共数据集上的实验评估表明，TABNet显著优于现有的基于涂鸦的弱监督分割方法。此外，它还达到了与全监督方法相当的性能。", "conclusion": "TABNet在弱监督医学图像分割中表现出色，通过结合三元组增强自恢复和边界感知伪标签监督，有效解决了稀疏涂鸦标注带来的挑战，并实现了接近全监督方法的性能。", "translation": "背景与目标：医学图像分割是各种临床应用中的核心任务。然而，获取大规模、完全标注的医学图像数据集既耗时又昂贵。涂鸦标注作为一种稀疏标注形式，为医学图像分割提供了一种高效且经济的选择。然而，涂鸦标注的稀疏性限制了目标区域的特征学习并缺乏足够的边界监督，这给训练分割网络带来了重大挑战。方法：我们提出了TABNet，一个新颖的弱监督医学图像分割框架，由两个关键组件组成：三元组增强自恢复（TAS）模块和边界感知伪标签监督（BAP）模块。TAS模块通过三种互补的增强策略增强特征学习：强度变换提高了模型对纹理和对比度变化的敏感性，裁剪（cutout）通过遮蔽关键区域迫使网络捕获局部解剖结构，拼图（jigsaw）增强通过打乱空间连续性来加强全局解剖布局的建模。通过引导网络从多样化的增强输入中恢复完整掩膜，TAS在稀疏监督下促进了对医学图像更深层的语义理解。BAP模块通过将双分支预测融合为损失加权的伪标签并引入边界感知损失以进行精细的轮廓细化，从而提高了伪监督准确性和边界建模能力。结果：在ACDC和MSCMR seg两个公共数据集上的实验评估表明，TABNet显著优于现有的基于涂鸦的弱监督分割方法。此外，它还达到了与全监督方法相当的性能。", "summary": "本文提出了TABNet，一个用于弱监督医学图像分割的框架，旨在解决稀疏涂鸦标注的挑战。该框架包含三元组增强自恢复（TAS）模块，通过多种数据增强策略增强特征学习和语义理解；以及边界感知伪标签监督（BAP）模块，通过生成高质量伪标签和引入边界感知损失来优化边界。实验证明，TABNet在两个公共数据集上显著优于现有弱监督方法，并能达到与全监督方法相媲美的性能。", "keywords": "弱监督分割, 医学图像分割, 涂鸦标注, 数据增强, 伪标签", "comments": "这篇论文的创新点在于结合了三元组增强自恢复和边界感知伪标签监督，有效地解决了弱监督医学图像分割中稀疏标注带来的特征学习不足和边界监督缺乏的问题。通过多样的增强策略促进了深层语义理解，同时精细的伪标签和边界损失提升了分割精度，使其在仅有涂鸦标注的情况下达到了接近全监督的性能，这对于降低医学图像标注成本具有重要意义。"}}
{"id": "2507.02135", "title": "Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency", "authors": ["Zongpu Zhang", "Pranab Dash", "Y. Charlie Hu", "Qiang Xu", "Jian Li", "Haibing Guan"], "categories": ["cs.OS", "cs.CL"], "primary_category": "Subjects:       Operating Systems (cs.OS)", "pdf_link": null, "comments": "Comments:      equal contribution between Zhang and Dash", "url": "http://arxiv.org/abs/2507.02135v1", "summary": "Large Language Models (LLMs) are increasingly being integrated into various\napplications and services running on billions of mobile devices. However,\ndeploying LLMs on resource-limited mobile devices faces a significant challenge\ndue to their high demand for computation, memory, and ultimately energy. While\ncurrent LLM frameworks for mobile use three power-hungry components-CPU, GPU,\nand Memory-even when running primarily-GPU LLM models, optimized DVFS governors\nfor CPU, GPU, and memory featured in modern mobile devices operate\nindependently and are oblivious of each other. Motivated by the above\nobservation, in this work, we first measure the energy-efficiency of a SOTA LLM\nframework consisting of various LLM models on mobile phones which showed the\ntriplet mobile governors result in up to 40.4% longer prefilling and decoding\nlatency compared to optimal combinations of CPU, GPU, and memory frequencies\nwith the same energy consumption for sampled prefill and decode lengths.\nSecond, we conduct an in-depth measurement study to uncover how the intricate\ninterplay (or lack of) among the mobile governors cause the above inefficiency\nin LLM inference. Finally, based on these insights, we design FUSE - a unified\nenergy-aware governor for optimizing the energy efficiency of LLM inference on\nmobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the\ntime-to-first-token and time-per-output-token latencies by 7.0%-16.9% and\n25.4%-36.8% on average with the same energy-per-token for various mobile LLM\nmodels.", "comment": "equal contribution between Zhang and Dash", "pdf_url": "http://arxiv.org/pdf/2507.02135v1", "cate": "cs.OS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "剖析移动DVFS调速器对LLM推理性能和能效的影响", "tldr": "该研究测量并分析了移动设备上LLM推理中CPU、GPU和内存DVFS调速器之间的相互作用，发现它们独立运行导致效率低下，并提出了一个统一的能效调速器FUSE，显著降低了LLM推理延迟。", "motivation": "在资源受限的移动设备上部署LLM面临计算、内存和能耗的巨大挑战。当前的移动LLM框架使用CPU、GPU和内存这三个耗电组件，即使主要运行GPU LLM模型，其优化的DVFS调速器也独立运行且互不感知，导致效率低下。", "method": "首先，测量了SOTA LLM框架在手机上的能效，发现独立的DVFS调速器导致预填充和解码延迟显著增加。其次，深入研究了移动调速器之间相互作用（或缺乏相互作用）如何导致LLM推理效率低下。最后，基于这些洞察设计了FUSE——一个统一的、能效感知的调速器，用于优化移动设备上LLM推理的能效。", "result": "测量结果显示，与相同能耗下的最佳CPU、GPU和内存频率组合相比，独立的移动调速器导致预填充和解码延迟延长高达40.4%。FUSE在ShareGPT数据集上的评估显示，对于各种移动LLM模型，在相同每令牌能耗下，首令牌时间和每输出令牌时间延迟平均分别降低了7.0%-16.9%和25.4%-36.8%。", "conclusion": "本研究揭示了移动设备上LLM推理中，CPU、GPU和内存DVFS调速器独立运行导致的能效低下问题，并成功设计并验证了FUSE，一个统一的能效感知调速器，显著提升了移动LLM推理的性能和能效。", "translation": "大型语言模型（LLMs）正越来越多地被集成到运行在数十亿移动设备上的各种应用程序和服务中。然而，由于LLMs对计算、内存和最终能耗的高需求，在资源受限的移动设备上部署LLMs面临着巨大挑战。尽管当前用于移动设备的LLM框架使用了三个耗电组件——CPU、GPU和内存——即使主要运行基于GPU的LLM模型，现代移动设备中用于CPU、GPU和内存的优化DVFS调速器也独立运行且彼此互不感知。受上述观察的启发，在这项工作中，我们首先测量了由各种LLM模型组成的SOTA LLM框架在手机上的能效，结果显示，对于采样预填充和解码长度，在相同能耗下，独立的三重移动调速器导致预填充和解码延迟比CPU、GPU和内存频率的最佳组合长高达40.4%。其次，我们进行了一项深入的测量研究，以揭示移动调速器之间复杂的相互作用（或缺乏相互作用）如何导致LLM推理的上述低效率。最后，基于这些见解，我们设计了FUSE——一个统一的能效感知调速器，用于优化移动设备上LLM推理的能效。我们使用ShareGPT数据集进行的评估显示，FUSE在相同每令牌能耗下，将首令牌时间和每输出令牌时间延迟平均分别降低了7.0%-16.9%和25.4%-36.8%，适用于各种移动LLM模型。", "summary": "本论文深入分析了移动设备上大型语言模型（LLM）推理的性能和能效问题，发现现有的CPU、GPU和内存DVFS调速器独立运行导致显著的延迟和能效低下。研究通过详细测量揭示了这种低效率的根源，并在此基础上设计了一个名为FUSE的统一能效感知调速器。实验结果表明，FUSE能够有效降低LLM推理的延迟，从而提升移动设备上LLM的运行效率。", "keywords": "LLM推理, DVFS调速器, 能效, 移动设备, FUSE", "comments": "该论文创新性地关注了移动设备上LLM推理的DVFS调速器协同问题，这是在边缘设备部署LLM时一个非常实际且重要的挑战。通过揭示现有独立调速器的局限性并提出统一的FUSE调速器，为提升移动LLM的能效和性能提供了新的解决方案。其贡献在于深入的测量研究和实用系统的设计，对移动AI领域具有重要意义。"}}
{"id": "2507.01972", "title": "Accelerated Portfolio Optimization and Option Pricing with Reinforcement Learning", "authors": ["Hadi Keramati", "Samaneh Jazayeri"], "categories": ["q-fin.PM", "cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "Subjects:       Portfolio Management (q-fin.PM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01972v1", "summary": "We present a reinforcement learning (RL)-driven framework for optimizing\nblock-preconditioner sizes in iterative solvers used in portfolio optimization\nand option pricing. The covariance matrix in portfolio optimization or the\ndiscretization of differential operators in option pricing models lead to large\nlinear systems of the form $\\mathbf{A}\\textbf{x}=\\textbf{b}$. Direct inversion\nof high-dimensional portfolio or fine-grid option pricing incurs a significant\ncomputational cost. Therefore, iterative methods are usually used for\nportfolios in real-world situations. Ill-conditioned systems, however, suffer\nfrom slow convergence. Traditional preconditioning techniques often require\nproblem-specific parameter tuning. To overcome this limitation, we rely on RL\nto dynamically adjust the block-preconditioner sizes and accelerate iterative\nsolver convergence. Evaluations on a suite of real-world portfolio optimization\nmatrices demonstrate that our RL framework can be used to adjust\npreconditioning and significantly accelerate convergence and reduce\ncomputational cost. The proposed accelerated solver supports faster\ndecision-making in dynamic portfolio allocation and real-time option pricing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01972v1", "cate": "q-fin.PM", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "强化学习加速投资组合优化和期权定价", "tldr": "本文提出了一个由强化学习驱动的框架，用于动态调整块预处理器大小，以加速投资组合优化和期权定价中迭代求解器的收敛，显著降低了计算成本。", "motivation": "投资组合优化和期权定价中的大型线性系统（如协方差矩阵或微分算子离散化）导致高昂的计算成本。虽然迭代方法常用于真实场景，但病态系统收敛缓慢，且传统预处理技术需要特定于问题的手动参数调整。", "method": "本文利用强化学习来动态调整迭代求解器中块预处理器的尺寸，以加速其收敛。", "result": "在真实世界的投资组合优化矩阵套件上的评估表明，本文提出的强化学习框架可以调整预处理，并显著加速收敛，降低计算成本。", "conclusion": "所提出的加速求解器支持在动态投资组合分配和实时期权定价中进行更快的决策。", "translation": "我们提出了一个由强化学习（RL）驱动的框架，用于优化投资组合优化和期权定价中迭代求解器中的块预处理器大小。投资组合优化中的协方差矩阵或期权定价模型中微分算子的离散化会导致大型线性系统，形式为 $\\mathbf{A}\\textbf{x}=\\textbf{b}$。对高维投资组合或精细网格期权定价进行直接求逆会产生显著的计算成本。因此，在实际情况中，迭代方法通常用于投资组合。然而，病态系统收敛缓慢。传统的预处理技术通常需要针对问题的参数调整。为了克服这一限制，我们依靠强化学习来动态调整块预处理器大小，并加速迭代求解器的收敛。对一系列真实世界投资组合优化矩阵的评估表明，我们的强化学习框架可用于调整预处理并显著加速收敛和降低计算成本。所提出的加速求解器支持在动态投资组合分配和实时期权定价中进行更快的决策。", "summary": "本文提出了一个基于强化学习（RL）的框架，旨在解决投资组合优化和期权定价中大型线性系统求解的计算效率问题。针对传统迭代求解器收敛缓慢且预处理技术需手动调参的局限性，该框架利用RL动态调整块预处理器大小，从而加速迭代求解器的收敛。实验结果表明，该RL框架能有效优化预处理，显著提高收敛速度并降低计算成本，从而支持更快的动态投资组合分配和实时期权定价决策。", "keywords": "强化学习, 投资组合优化, 期权定价, 迭代求解器, 预处理", "comments": "该论文的创新点在于将强化学习应用于金融领域中迭代求解器的预处理参数的动态调整。传统方法需要手动或启发式调整，而RL的引入实现了自动化和自适应优化，有望显著提升实际应用中的计算效率和决策速度，特别是在高频交易和实时风险管理等场景下具有重要意义。"}}
{"id": "2507.02403", "title": "Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings", "authors": ["Mufhumudzi Muthivhi", "Terence L. van Zyl"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in IEEE Xplore and ISIF FUSION 2025 proceedings:", "url": "http://arxiv.org/abs/2507.02403v1", "summary": "Wildlife re-identification aims to match individuals of the same species\nacross different observations. Current state-of-the-art (SOTA) models rely on\nclass labels to train supervised models for individual classification. This\ndependence on annotated data has driven the curation of numerous large-scale\nwildlife datasets. This study investigates self-supervised learning\nSelf-Supervised Learning (SSL) for wildlife re-identification. We automatically\nextract two distinct views of an individual using temporal image pairs from\ncamera trap data without supervision. The image pairs train a self-supervised\nmodel from a potentially endless stream of video data. We evaluate the learnt\nrepresentations against supervised features on open-world scenarios and\ntransfer learning in various wildlife downstream tasks. The analysis of the\nexperimental results shows that self-supervised models are more robust even\nwith limited data. Moreover, self-supervised features outperform supervision\nacross all downstream tasks. The code is available here\nhttps://github.com/pxpana/SSLWildlife.", "comment": "Accepted for publication in IEEE Xplore and ISIF FUSION 2025\n  proceedings:", "pdf_url": "http://arxiv.org/pdf/2507.02403v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "野生动物目标重识别在非城市环境中使用自监督学习", "tldr": "本文研究了在非城市环境中利用自监督学习进行野生动物重识别，通过相机陷阱数据自动提取图像对训练模型，结果显示自监督模型在数据有限的情况下表现更鲁棒且优于监督学习。", "motivation": "现有的野生动物重识别模型严重依赖带标注的类别标签数据进行监督学习，导致需要大量的人工标注工作来构建大型数据集。", "method": "本文提出使用自监督学习（SSL）进行野生动物重识别。通过相机陷阱数据中的时间图像对自动提取个体的两个不同视图，无需监督。这些图像对用于训练自监督模型。", "result": "实验结果表明，即使在数据有限的情况下，自监督模型也更具鲁棒性。此外，在所有下游任务中，自监督特征的表现优于监督特征。", "conclusion": "自监督学习在野生动物重识别任务中展现出优越的性能和鲁棒性，尤其是在数据有限的非城市环境中，为未来的研究提供了新的方向。", "translation": "野生动物重识别旨在匹配不同观测中同一物种的个体。当前最先进（SOTA）的模型依赖类别标签来训练监督模型进行个体分类。这种对标注数据的依赖推动了大量大规模野生动物数据集的整理。本研究调查了自监督学习（SSL）在野生动物重识别中的应用。我们利用相机陷阱数据中的时间图像对，在无监督的情况下自动提取个体的两个不同视图。这些图像对从潜在的无限视频数据流中训练一个自监督模型。我们在开放世界场景中评估了学习到的表示与监督特征的性能，并在各种野生动物下游任务中进行了迁移学习。实验结果分析表明，即使数据有限，自监督模型也更具鲁棒性。此外，自监督特征在所有下游任务中都优于监督特征。代码可在https://github.com/pxpana/SSLWildlife 获取。", "summary": "本文探讨了在非城市环境下使用自监督学习（SSL）进行野生动物重识别，以解决传统监督学习对大量标注数据的依赖问题。研究通过相机陷阱数据自动生成时间图像对来训练SSL模型。实验结果表明，与监督学习相比，自监督模型在数据有限的情况下表现出更高的鲁棒性，并且在多个野生动物下游任务中性能更优。", "keywords": "野生动物重识别, 自监督学习, 相机陷阱, 鲁棒性, 特征学习", "comments": "本文的创新之处在于将自监督学习应用于野生动物重识别领域，有效解决了该领域对大量标注数据依赖的痛点。通过利用相机陷阱的无尽视频流数据，提供了一种高效且鲁棒的解决方案，对于资源受限的野生动物研究具有重要意义。"}}
{"id": "2507.02503", "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs", "authors": ["Chenxu Wang", "Yilin Lyu", "Zicheng Sun", "Liping Jing"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures, accepted by ACL 2025 main", "url": "http://arxiv.org/abs/2507.02503v1", "summary": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP.", "comment": "15 pages, 6 figures, accepted by ACL 2025 main", "pdf_url": "http://arxiv.org/pdf/2507.02503v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLM 的持续梯度低秩投影微调", "tldr": "GORP通过在统一的低秩梯度子空间中结合全秩和低秩参数，解决了LLM持续微调中的效率与表达性权衡问题，性能优于现有方法。", "motivation": "大型语言模型（LLM）的持续微调受到效率与表达性之间权衡的阻碍。低秩适应（LoRA）虽然提供了效率，但由于其低秩性质和对显式参数约束的依赖，限制了模型学习新任务和知识迁移的能力。", "method": "本文提出了用于持续学习的GORP（梯度低秩投影）方法，这是一种新颖的训练策略。它通过协同结合全秩和低秩参数，并在统一的低秩梯度子空间中联合更新来克服现有局限性。GORP在保持效率和减轻灾难性遗忘的同时，扩展了优化空间。", "result": "在持续学习基准上的广泛实验表明，与现有最先进的方法相比，GORP表现出卓越的性能。", "conclusion": "GORP在持续学习任务中表现出卓越的性能，有效克服了LLM持续微调中效率与表达性的权衡问题。", "translation": "大型语言模型（LLM）的持续微调受到效率与表达性之间权衡的阻碍。低秩适应（LoRA）提供了效率，但由于其低秩性质和对显式参数约束的依赖，限制了模型学习新任务和迁移知识的能力。我们提出了用于持续学习的GORP（梯度低秩投影），这是一种新颖的训练策略，通过协同结合全秩和低秩参数，并在统一的低秩梯度子空间中联合更新，克服了这些限制。GORP在保持效率和减轻灾难性遗忘的同时，扩展了优化空间。在持续学习基准上的广泛实验表明，与现有最先进的方法相比，GORP表现出卓越的性能。代码可在 https://github.com/Wcxwcxw/GORP 获取。", "summary": "针对LLM持续微调中效率与表达性之间的权衡问题，以及LoRA等现有方法的局限性，本文提出了GORP（梯度低秩投影）方法。GORP通过协同结合全秩和低秩参数，并在统一的低秩梯度子空间中进行联合更新，从而扩展了优化空间，同时保持了效率并减轻了灾难性遗忘。实验证明，GORP在持续学习基准上优于现有最先进的方法。", "keywords": "持续学习, 低秩适应, LLM, 微调, 梯度投影", "comments": "GORP的创新之处在于其将全秩和低秩参数协同结合，并在统一的低秩梯度子空间中进行联合更新，这有效地扩展了优化空间，同时保持了效率并减轻了灾难性遗忘。这对于解决LLM持续学习中的核心挑战具有重要意义，其性能优于现有SOTA方法，表明了该方法的有效性和实用性。"}}
{"id": "2507.02405", "title": "PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration", "authors": ["Ayantika Das", "Moitreya Chaudhuri", "Koushik Bhat", "Keerthi Ram", "Mihail Bota", "Mohanasankar Sivaprakasam"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published in IEEE Journal of Biomedical and Health Informatics (Early Access Available) this https URL", "url": "http://arxiv.org/abs/2507.02405v1", "summary": "Denoising diffusion models produce high-fidelity image samples by capturing\nthe image distribution in a progressive manner while initializing with a simple\ndistribution and compounding the distribution complexity. Although these models\nhave unlocked new applicabilities, the sampling mechanism of diffusion does not\noffer means to extract image-specific semantic representation, which is\ninherently provided by auto-encoders. The encoding component of auto-encoders\nenables mapping between a specific image and its latent space, thereby offering\nexplicit means of enforcing structures in the latent space. By integrating an\nencoder with the diffusion model, we establish an auto-encoding formulation,\nwhich learns image-specific representations and offers means to organize the\nlatent space. In this work, First, we devise a mechanism to structure the\nlatent space of a diffusion auto-encoding model, towards recognizing\nregion-specific cellular patterns in brain images. We enforce the\nrepresentations to regress positional information of the patches from\nhigh-resolution images. This creates a conducive latent space for\ndifferentiating tissue types of the brain. Second, we devise an unsupervised\ntear artifact restoration technique based on neighborhood awareness, utilizing\nlatent representations and the constrained generation capability of diffusion\nmodels during inference. Third, through representational guidance and\nleveraging the inference time steerable noising and denoising capability of\ndiffusion, we devise an unsupervised JPEG artifact restoration technique.", "comment": "Published in IEEE Journal of Biomedical and Health Informatics (Early\n  Access Available) https://ieeexplore.ieee.org/document/10989734", "pdf_url": "http://arxiv.org/pdf/2507.02405v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "PosDiffAE：结合伪影修复的面向高分辨率脑组织分类的位置感知扩散自编码器", "tldr": "提出PosDiffAE，一个结合自编码器和扩散模型的框架，用于高分辨率脑组织分类，并通过位置感知潜空间和邻域感知方法进行伪影修复。", "motivation": "扩散模型虽然能生成高质量图像，但缺乏提取图像特定语义表示的能力，而自编码器可以提供这种能力，并允许组织潜在空间。因此，作者旨在结合两者优点，解决高分辨率脑图像分类和伪影修复问题。", "method": "首先，将编码器与扩散模型集成，形成自编码扩散模型，学习图像特定表示并组织潜在空间。其次，设计机制，使潜在空间结构化，以识别脑图像中区域特定的细胞模式，通过强制表示回归高分辨率图像块的位置信息，从而有利于区分脑组织类型。第三，基于邻域感知，利用潜在表示和扩散模型的受限生成能力，开发了一种无监督的撕裂伪影修复技术。最后，通过表示引导和利用扩散模型在推理时的可控噪声和去噪能力，开发了一种无监督的JPEG伪影修复技术。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "去噪扩散模型通过以渐进方式捕获图像分布，同时以简单分布初始化并复合分布复杂性，从而生成高保真图像样本。尽管这些模型开辟了新的应用可能性，但扩散的采样机制无法提供提取图像特定语义表示的手段，而这正是自编码器固有提供的。自编码器的编码组件能够实现特定图像与其潜在空间之间的映射，从而提供在潜在空间中强制执行结构的明确手段。通过将编码器与扩散模型集成，我们建立了一个自编码公式，该公式学习图像特定表示并提供组织潜在空间的手段。在这项工作中，首先，我们设计了一种机制来结构化扩散自编码模型的潜在空间，以识别脑图像中区域特定的细胞模式。我们强制表示回归高分辨率图像块的位置信息。这为区分脑组织类型创造了一个有利的潜在空间。其次，我们基于邻域感知，利用潜在表示和扩散模型在推理过程中的受限生成能力，设计了一种无监督的撕裂伪影修复技术。第三，通过表示引导和利用扩散在推理时的可控噪声和去噪能力，我们设计了一种无监督的JPEG伪影修复技术。", "summary": "本文提出了PosDiffAE，一个结合了自编码器和扩散模型的框架，旨在解决高分辨率脑组织分类和图像伪影修复问题。该模型通过集成编码器来学习图像特定表示并组织潜在空间。具体地，它通过强制潜在表示回归图像块的位置信息来构建有利于区分脑组织类型的潜在空间。此外，PosDiffAE还开发了基于邻域感知和表示引导的无监督撕裂和JPEG伪影修复技术，利用了扩散模型在推理时的生成和去噪能力。", "keywords": "扩散模型, 自编码器, 脑组织分类, 伪影修复, 位置感知", "comments": "这篇论文的创新点在于将自编码器的编码能力与扩散模型的生成能力相结合，解决了传统扩散模型缺乏语义表示的问题。通过引入位置感知机制来结构化潜在空间，使其更适用于高分辨率脑组织分类。同时，在扩散模型的推理阶段利用其特性进行无监督的图像伪影修复（撕裂和JPEG伪影），展示了其在医学图像处理领域的潜在应用价值和多功能性。"}}
{"id": "2507.01979", "title": "Forecasting Labor Markets with LSTNet: A Multi-Scale Deep Learning Approach", "authors": ["Adam Nelson-Archer", "Aleia Sen", "Meena Al Hasani", "Sofia Davila", "Jessica Le", "Omar Abbouchi"], "categories": ["q-fin.ST", "cs.AI", "cs.LG", "I.2.6; I.5.1"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      Undergraduate senior project, University of Houston, Department of Computer Science", "url": "http://arxiv.org/abs/2507.01979v1", "summary": "We present a deep learning approach for forecasting short-term employment\nchanges and assessing long-term industry health using labor market data from\nthe U.S. Bureau of Labor Statistics. Our system leverages a Long- and\nShort-Term Time-series Network (LSTNet) to process multivariate time series\ndata, including employment levels, wages, turnover rates, and job openings. The\nmodel outputs both 7-day employment forecasts and an interpretable Industry\nEmployment Health Index (IEHI). Our approach outperforms baseline models across\nmost sectors, particularly in stable industries, and demonstrates strong\nalignment between IEHI rankings and actual employment volatility. We discuss\nerror patterns, sector-specific performance, and future directions for\nimproving interpretability and generalization.", "comment": "Undergraduate senior project, University of Houston, Department of\n  Computer Science", "pdf_url": "http://arxiv.org/pdf/2507.01979v1", "cate": "q-fin.ST", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "使用LSTNet预测劳动力市场：一种多尺度深度学习方法", "tldr": "利用深度学习LSTNet模型对美国劳动力市场数据进行短期就业预测和长期行业健康评估，表现优于基线模型。", "motivation": "该研究旨在利用深度学习方法预测短期就业变化并评估长期行业健康，以应对劳动力市场数据的复杂性。", "method": "本研究提出了一种深度学习方法，利用长短期时间序列网络（LSTNet）处理来自美国劳工统计局的多元时间序列数据，包括就业水平、工资、离职率和职位空缺。模型输出7天就业预测和可解释的行业就业健康指数（IEHI）。", "result": "该方法在大多数行业，特别是在稳定行业中，表现优于基线模型，并且行业就业健康指数（IEHI）排名与实际就业波动之间显示出很强的一致性。", "conclusion": "该深度学习方法（LSTNet）能够有效预测劳动力市场短期就业变化并评估行业健康，其表现优于传统模型，并为未来改进可解释性和泛化能力提供了方向。", "translation": "我们提出了一种深度学习方法，利用美国劳工统计局的劳动力市场数据预测短期就业变化并评估长期行业健康。我们的系统利用长短期时间序列网络（LSTNet）处理多元时间序列数据，包括就业水平、工资、离职率和职位空缺。模型输出7天就业预测和可解释的行业就业健康指数（IEHI）。我们的方法在大多数行业，特别是在稳定行业中，表现优于基线模型，并且IEHI排名与实际就业波动之间显示出很强的一致性。我们讨论了误差模式、特定行业表现以及未来改进可解释性和泛化能力的方向。", "summary": "本研究提出一种基于深度学习LSTNet的劳动力市场预测模型。该模型利用美国劳工统计局的多元时间序列数据，如就业水平和工资，进行7天就业预测并生成可解释的行业就业健康指数（IEHI）。实验结果表明，该方法在多数行业（尤其稳定行业）表现优于基线模型，且IEHI与实际就业波动高度一致，为劳动力市场分析提供了新工具。", "keywords": "劳动力市场预测, 深度学习, LSTNet, 时间序列分析, 行业健康指数", "comments": "该论文的创新之处在于将LSTNet深度学习模型应用于劳动力市场预测，并引入了可解释的行业就业健康指数（IEHI），这对于理解和评估行业健康具有重要意义。其超越基线模型的表现也证明了该方法的有效性。"}}
{"id": "2507.02408", "title": "A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern", "authors": ["Duong Nguyen-Ngoc Tran", "Long Hoang Pham", "Chi Dai Tran", "Quoc Pham-Nam Ho", "Huy-Hung Nguyen", "Jae Wook Jeon"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02408v1", "summary": "Multi-Object Tracking in thermal images is essential for surveillance\nsystems, particularly in challenging environments where RGB cameras struggle\ndue to low visibility or poor lighting conditions. Thermal sensors enhance\nrecognition tasks by capturing infrared signatures, but a major challenge is\ntheir low-level feature representation, which makes it difficult to accurately\ndetect and track pedestrians. To address this, the paper introduces a novel\ntuning method for pedestrian tracking, specifically designed to handle the\ncomplex motion patterns in thermal imagery. The proposed framework optimizes\ntwo-stages, ensuring that each stage is tuned with the most suitable\nhyperparameters to maximize tracking performance. By fine-tuning\nhyperparameters for real-time tracking, the method achieves high accuracy\nwithout relying on complex reidentification or motion models. Extensive\nexperiments on PBVS Thermal MOT dataset demonstrate that the approach is highly\neffective across various thermal camera conditions, making it a robust solution\nfor real-world surveillance applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02408v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种利用热传感器处理复杂运动模式的实时多目标跟踪新调优方法", "tldr": "本文提出了一种新颖的调优方法，用于在热图像中进行实时多目标跟踪，通过优化两阶段的超参数，在不依赖复杂重识别或运动模型的情况下实现高精度，并在各种热成像条件下表现出色。", "motivation": "在RGB相机因低能见度或恶劣光照条件而难以工作的挑战性环境中，热成像中的多目标跟踪对于监控系统至关重要。然而，热传感器的低级特征表示是一个主要挑战，使得准确检测和跟踪行人，特别是处理复杂运动模式变得困难。", "method": "本文引入了一种新颖的行人跟踪调优方法，专门设计用于处理热成像中复杂的运动模式。该框架优化了两个阶段，确保每个阶段都通过最合适的超参数进行调整，以最大限度地提高跟踪性能。通过对实时跟踪的超参数进行微调，该方法无需依赖复杂的重识别或运动模型即可实现高精度。", "result": "在PBVS热MOT数据集上进行的广泛实验表明，该方法在各种热像仪条件下都非常有效。", "conclusion": "该方法为实时监控应用提供了一个强大的解决方案，能够有效跟踪热图像中的多个目标，即使存在复杂运动模式，且无需复杂的重识别或运动模型。", "translation": "热成像中的多目标跟踪对于监控系统至关重要，特别是在RGB相机因能见度低或光照条件差而难以工作的挑战性环境中。热传感器通过捕获红外特征来增强识别任务，但一个主要挑战是它们的低级特征表示，这使得准确检测和跟踪行人变得困难。为了解决这个问题，本文引入了一种新颖的行人跟踪调优方法，专门设计用于处理热成像中复杂的运动模式。所提出的框架优化了两个阶段，确保每个阶段都通过最合适的超参数进行调整，以最大限度地提高跟踪性能。通过对实时跟踪的超参数进行微调，该方法无需依赖复杂的重识别或运动模型即可实现高精度。在PBVS热MOT数据集上进行的广泛实验表明，该方法在各种热像仪条件下都非常有效，使其成为实时监控应用的强大解决方案。", "summary": "本文提出了一种新颖的两阶段调优方法，用于实时多目标跟踪，特别是在热图像中跟踪行人。该方法通过在每个阶段优化超参数，解决了热成像中低级特征表示和复杂运动模式的挑战。该方法在不依赖复杂重识别或运动模型的情况下实现了高精度，并在PBVS热MOT数据集上得到了有效验证，使其成为恶劣环境下监控的强大解决方案。", "keywords": "多目标跟踪, 热传感器, 超参数调优, 实时跟踪, 监控", "comments": "该论文的创新之处在于其“新颖的调优方法”，通过在两个阶段优化超参数，简化了跟踪过程，避免了复杂的重识别或运动模型，同时在挑战性的热环境实现了高精度。其重要性体现在其适用于RGB相机可能失效的实际监控系统。一个潜在的局限性，尽管未明确说明，可能是其对超参数调优的依赖，这可能需要领域专业知识或广泛的实验才能在比测试范围更广的场景中获得最佳性能。"}}
{"id": "2507.02529", "title": "RetrySQL: text-to-SQL training with retry data for self-correcting query generation", "authors": ["Alicja Rączkowska", "Riccardo Belluzzo", "Piotr Zieliński", "Joanna Baran", "Paweł Olszewski"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02529v1", "summary": "The text-to-SQL task is an active challenge in Natural Language Processing.\nMany existing solutions focus on using black-box language models extended with\nspecialized components within customized end-to-end text-to-SQL pipelines.\nWhile these solutions use both closed-source proprietary language models and\ncoding-oriented open-source models, there is a lack of research regarding\nSQL-specific generative models. At the same time, recent advancements in\nself-correcting generation strategies show promise for improving the\ncapabilities of existing architectures. The application of these concepts to\nthe text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,\na new approach to training text-to-SQL generation models. We prepare reasoning\nsteps for reference SQL queries and then corrupt them to create retry data that\ncontains both incorrect and corrected steps, divided with a special token. We\ncontinuously pre-train an open-source coding model with this data and\ndemonstrate that retry steps yield an improvement of up to 4 percentage points\nin both overall and challenging execution accuracy metrics, compared to\npre-training without retry data. Additionally, we confirm that supervised\nfine-tuning with LoRA is ineffective for learning from retry data and that\nfull-parameter pre-training is a necessary requirement for that task. We\nshowcase that the self-correcting behavior is learned by the model and the\nincrease in downstream accuracy metrics is a result of this additional skill.\nFinally, we incorporate RetrySQL-trained models into the full text-to-SQL\npipeline and showcase that they are competitive in terms of execution accuracy\nwith proprietary models that contain orders of magnitude more parameters.\nRetrySQL demonstrates that self-correction can be learned in the text-to-SQL\ntask and provides a novel way of improving generation accuracy for SQL-oriented\nlanguage models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02529v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RetrySQL：使用重试数据进行text-to-SQL训练以实现自校正查询生成", "tldr": "RetrySQL通过使用包含错误和纠正步骤的重试数据对开源编码模型进行预训练，显著提高了text-to-SQL任务的查询生成准确性，并证明了自校正行为是可学习的。", "motivation": "现有text-to-SQL解决方案多依赖黑盒语言模型或通用开源模型，缺乏SQL特异性生成模型的研究。同时，自校正生成策略在text-to-SQL任务中尚未被探索。", "method": "本文引入RetrySQL，一种新的text-to-SQL生成模型训练方法。通过为参考SQL查询准备推理步骤并对其进行破坏，创建包含错误和已纠正步骤的“重试数据”。使用这些数据持续预训练一个开源编码模型。研究还确认全参数预训练是学习重试数据的必要条件。", "result": "重试步骤使整体和挑战性执行准确率指标提高了多达4个百分点。模型学习了自校正行为，并且下游准确率的提高是这种额外技能的结果。RetrySQL训练的模型在执行准确率方面与参数量大几个数量级的专有模型具有竞争力。", "conclusion": "RetrySQL证明了自校正可以在text-to-SQL任务中学习，并提供了一种提高SQL导向语言模型生成准确性的新方法。", "translation": "text-to-SQL任务是自然语言处理领域的一个活跃挑战。许多现有解决方案侧重于使用黑盒语言模型，并在定制的端到端text-to-SQL管道中扩展专用组件。虽然这些解决方案同时使用闭源专有语言模型和面向编码的开源模型，但缺乏对SQL特异性生成模型的研究。与此同时，自校正生成策略的最新进展显示出改善现有架构能力的潜力。这些概念在text-to-SQL任务中的应用仍未被探索。在本文中，我们介绍了RetrySQL，一种训练text-to-SQL生成模型的新方法。我们为参考SQL查询准备推理步骤，然后破坏它们以创建包含错误和已纠正步骤（由特殊标记分隔）的重试数据。我们使用这些数据持续预训练一个开源编码模型，并证明与没有重试数据进行预训练相比，重试步骤在整体和挑战性执行准确率指标上均提高了多达4个百分点。此外，我们确认使用LoRA进行的监督微调对于从重试数据中学习是无效的，并且全参数预训练是该任务的必要条件。我们展示了模型学习了自校正行为，并且下游准确率指标的增加是这种额外技能的结果。最后，我们将经过RetrySQL训练的模型整合到完整的text-to-SQL管道中，并展示它们在执行准确率方面与参数量大几个数量级的专有模型具有竞争力。RetrySQL证明了自校正可以在text-to-SQL任务中学习，并提供了一种提高面向SQL的语言模型生成准确性的新颖方法。", "summary": "本文介绍了RetrySQL，一种针对text-to-SQL任务的新型训练方法。该方法通过创建包含错误和纠正步骤的“重试数据”来持续预训练开源编码模型。实验表明，RetrySQL将查询生成准确率提高了高达4个百分点，并证明了模型能够学习自校正行为。此外，研究强调全参数预训练对于从重试数据中学习至关重要。RetrySQL训练的模型在性能上可与大型专有模型匹敌，为提高SQL导向语言模型的生成准确性提供了一条新途径。", "keywords": "text-to-SQL, 自校正, 重试数据, 查询生成, 预训练", "comments": "本文的创新点在于引入了“重试数据”的概念，通过模拟错误和纠正过程来训练模型自校正，这在text-to-SQL领域是一个新颖且有效的方法。其重要性在于证明了自校正能力可以通过数据驱动的方式学习，并能显著提高模型性能，尤其是在SQL生成这种对准确性要求高的任务中。同时，它为使用开源模型达到与大型专有模型相当的性能提供了可能性。抽象中未明确提及重试数据的具体构建复杂性或泛化性挑战，也未提及模型对不同类型SQL查询的鲁棒性。"}}
{"id": "2507.02550", "title": "Position: A Theory of Deep Learning Must Include Compositional Sparsity", "authors": ["David A. Danhofer", "Davide D'Ascenzo", "Rafael Dubach", "Tomaso Poggio"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02550v1", "summary": "Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable\nsuccess in a wide variety of domains too high-dimensional for classical shallow\nnetworks subject to the curse of dimensionality. However, open questions about\nfundamental principles, that govern the learning dynamics of DNNs, remain. In\nthis position paper we argue that it is the ability of DNNs to exploit the\ncompositionally sparse structure of the target function driving their success.\nAs such, DNNs can leverage the property that most practically relevant\nfunctions can be composed from a small set of constituent functions, each of\nwhich relies only on a low-dimensional subset of all inputs. We show that this\nproperty is shared by all efficiently Turing-computable functions and is\ntherefore highly likely present in all current learning problems. While some\npromising theoretical insights on questions concerned with approximation and\ngeneralization exist in the setting of compositionally sparse functions,\nseveral important questions on the learnability and optimization of DNNs\nremain. Completing the picture of the role of compositional sparsity in deep\nlearning is essential to a comprehensive theory of artificial, and even\ngeneral, intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02550v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "立场：深度学习理论必须包含组合稀疏性", "tldr": "深度学习的成功源于其利用目标函数的组合稀疏性；理解这一点对于建立全面的人工智能理论至关重要。", "motivation": "尽管深度神经网络（DNNs）取得了显著成功，但关于其学习动力学的基本原理仍存在未解问题。本文旨在提出一个解释DNNs成功的核心原因。", "method": "本文作为一篇立场论文，论证了深度神经网络之所以成功，是因为它们能够利用目标函数的组合稀疏结构。论文指出，这种组合稀疏性是所有高效图灵可计算函数所共有的特性。", "result": "所有高效图灵可计算函数都共享组合稀疏性这一特性，这意味着在当前所有学习问题中极有可能存在这种结构。", "conclusion": "要建立一个全面的人工智能乃至通用智能理论，必须完整理解组合稀疏性在深度学习中所扮演的角色。", "translation": "过参数化深度神经网络（DNNs）在各种高维领域中取得了显著成功，这些领域对于受维度诅咒影响的经典浅层网络而言过于复杂。然而，关于支配DNN学习动力学的基本原理仍存在未解问题。在这篇立场论文中，我们认为正是DNN利用目标函数的组合稀疏结构的能力推动了它们的成功。因此，DNN可以利用这样一个特性：大多数实际相关的函数可以由一小组组成函数构成，每个函数仅依赖于所有输入的一个低维子集。我们表明，所有高效图灵可计算函数都共享这一特性，因此在当前所有学习问题中极有可能存在。虽然在组合稀疏函数设置中，关于近似和泛化问题存在一些有前景的理论见解，但关于DNN的可学习性和优化的一些重要问题仍未解决。完善组合稀疏性在深度学习中作用的图景，对于构建一个全面的人工智能乃至通用智能理论至关重要。", "summary": "这篇立场论文提出，深度神经网络之所以能取得巨大成功，关键在于它们能够利用目标函数的组合稀疏结构。这种结构允许复杂函数由少量低维的组成函数构成，并且这种特性在所有高效图灵可计算函数中普遍存在。论文强调，虽然关于组合稀疏函数在近似和泛化方面已有理论进展，但在DNN的可学习性和优化方面仍有待解决的问题。最终，论文认为全面理解组合稀疏性在深度学习中的作用，对于构建完整的人工智能理论至关重要。", "keywords": "深度学习, 组合稀疏性, 神经网络, 人工智能理论", "comments": "这篇论文的核心创新在于提出了组合稀疏性是解释深度学习成功的关键因素，并将其与高效图灵可计算函数的普遍性联系起来，为理解深度学习的理论基础提供了一个新颖且重要的视角。它不仅指出了现有理论的局限性，还为未来研究指明了方向，即深入探讨组合稀疏性在可学习性和优化中的作用，对构建全面的人工智能理论具有重要意义。"}}
{"id": "2507.02287", "title": "Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents", "authors": ["Lapo Santarlasci", "Armando Rungi", "Antonio Zinilli"], "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02287v1", "summary": "This paper introduces Natural Language Processing for identifying ``true''\ngreen patents from official supporting documents. We start our training on\nabout 12.4 million patents that had been classified as green from previous\nliterature. Thus, we train a simple neural network to enlarge a baseline\ndictionary through vector representations of expressions related to\nenvironmental technologies. After testing, we find that ``true'' green patents\nrepresent about 20\\% of the total of patents classified as green from previous\nliterature. We show heterogeneity by technological classes, and then check that\n`true' green patents are about 1\\% less cited by following inventions. In the\nsecond part of the paper, we test the relationship between patenting and a\ndashboard of firm-level financial accounts in the European Union. After\ncontrolling for reverse causality, we show that holding at least one ``true''\ngreen patent raises sales, market shares, and productivity. If we restrict the\nanalysis to high-novelty ``true'' green patents, we find that they also yield\nhigher profits. Our findings underscore the importance of using text analyses\nto gauge finer-grained patent classifications that are useful for policymaking\nin different domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02287v1", "cate": "econ.GN", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "看穿绿色：基于文本的分类与企业绿色专利回报", "tldr": "本文利用自然语言处理技术识别“真实”绿色专利，发现其仅占现有绿色专利的20%，但持有“真实”绿色专利能显著提升企业的销售额、市场份额和生产力，高新颖性绿色专利还能带来更高利润，强调了文本分析在专利分类和政策制定中的重要性。", "motivation": "现有文献对绿色专利的分类可能不够精确，本研究旨在利用自然语言处理技术识别“真实”的绿色专利，并评估这些“真实”绿色专利对企业财务表现的影响。", "method": "1. 使用自然语言处理（NLP）技术，从官方支持文件中识别“真实”绿色专利。2. 在约1240万份已被分类为绿色专利的数据集上进行训练。3. 训练一个简单的神经网络，通过环境技术相关表达的向量表示来扩大基线词典。4. 在第二部分，测试专利持有与欧盟企业层面财务指标（销售额、市场份额、生产力、利润）之间的关系，并控制逆向因果关系。", "result": "1. “真实”绿色专利约占现有文献中被分类为绿色专利总数的20%。2. “真实”绿色专利在技术类别上表现出异质性。3. “真实”绿色专利被后续发明引用的次数约减少1%。4. 持有至少一项“真实”绿色专利能提高企业的销售额、市场份额和生产力。5. 仅限于高新颖性的“真实”绿色专利，它们还能带来更高的利润。", "conclusion": "研究结果强调了使用文本分析进行更细粒度的专利分类的重要性，这对于不同领域的政策制定具有实用价值。识别“真实”绿色专利有助于更准确地评估其对企业表现的积极影响。", "translation": "本文引入自然语言处理技术，通过官方支持文件识别“真实”绿色专利。我们从现有文献中已分类为绿色的约1240万份专利开始训练。接着，我们训练一个简单的神经网络，通过环境技术相关表达的向量表示来扩大基线词典。经过测试，我们发现“真实”绿色专利约占现有文献中被分类为绿色专利总数的20%。我们展示了技术类别的异质性，并发现“真实”绿色专利被后续发明引用的次数约减少1%。在论文的第二部分，我们测试了专利持有与欧盟企业层面财务指标（包括销售额、市场份额和生产力）之间的关系。在控制逆向因果关系后，我们发现持有至少一项“真实”绿色专利能提高销售额、市场份额和生产力。如果我们将分析范围限制在高新颖性的“真实”绿色专利，我们发现它们还能带来更高的利润。我们的研究结果强调了使用文本分析进行更细粒度的专利分类的重要性，这对于不同领域的政策制定具有实用价值。", "summary": "本研究利用自然语言处理（NLP）技术，从现有绿色专利中识别出更精确的“真实”绿色专利。通过对约1240万份专利进行神经网络训练和词典扩展，研究发现“真实”绿色专利仅占现有绿色专利的20%，且其被引用率略低。进一步分析表明，持有“真实”绿色专利的企业，其销售额、市场份额和生产力均有所提升，而高新颖性的“真实”绿色专利还能带来更高的利润。研究强调了文本分析在精确专利分类和政策制定中的重要性。", "keywords": "绿色专利, 自然语言处理, 专利分类, 企业回报, 神经网络", "comments": "本文创新性地运用自然语言处理技术来区分“真实”的绿色专利，纠正了以往分类可能存在的偏差。其重要性在于，通过更精确的分类，能够更准确地评估绿色技术对企业财务绩效的实际贡献，为绿色创新政策的制定提供更可靠的依据。该方法论也为其他领域中需要细致分类的数据分析提供了借鉴。"}}
{"id": "2507.01990", "title": "Integrating Large Language Models in Financial Investments and Market Analysis: A Survey", "authors": ["Sedigheh Mahdavi", "Jiating", "Chen", "Pradeep Kumar Joshi", "Lina Huertas Guativa", "Upmanyu Singh"], "categories": ["q-fin.GN", "cs.AI", "cs.LG"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01990v1", "summary": "Large Language Models (LLMs) have been employed in financial decision making,\nenhancing analytical capabilities for investment strategies. Traditional\ninvestment strategies often utilize quantitative models, fundamental analysis,\nand technical indicators. However, LLMs have introduced new capabilities to\nprocess and analyze large volumes of structured and unstructured data, extract\nmeaningful insights, and enhance decision-making in real-time. This survey\nprovides a structured overview of recent research on LLMs within the financial\ndomain, categorizing research contributions into four main frameworks:\nLLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and\nAdaptation Approaches, and Agent-Based Architectures. This study provides a\nstructured review of recent LLMs research on applications in stock selection,\nrisk assessment, sentiment analysis, trading, and financial forecasting. By\nreviewing the existing literature, this study highlights the capabilities,\nchallenges, and potential directions of LLMs in financial markets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01990v1", "cate": "q-fin.GN", "date": "2025-06-29", "updated": "2025-06-29", "AI": {"title_translation": "大型语言模型在金融投资与市场分析中的整合：一项综述", "tldr": "本综述概述了大型语言模型在金融领域的应用，包括其框架、集成方法、微调和基于代理的架构，并探讨了它们在股票选择、风险评估、情感分析、交易和金融预测中的能力、挑战和未来方向。", "motivation": "大型语言模型（LLMs）为金融决策带来了处理和分析大量结构化和非结构化数据、提取有意义见解以及实时增强决策的新能力。本综述旨在提供一个结构化的概述，以理解LLMs在金融领域应用的最新研究进展。", "method": "本综述通过提供一个结构化的概述来分析大型语言模型在金融领域的最新研究，将研究贡献分为四个主要框架：基于LLM的框架和管道、混合集成方法、微调和适应方法、以及基于代理的架构。此外，它还回顾了LLMs在股票选择、风险评估、情感分析、交易和金融预测等具体应用中的研究。", "result": "本研究提供了一个关于大型语言模型在金融领域应用的结构化综述，并按四种主要框架对研究贡献进行了分类。它还回顾了LLMs在股票选择、风险评估、情感分析、交易和金融预测等具体金融应用中的研究，并强调了LLMs在金融市场中的能力、挑战和潜在方向。", "conclusion": "大型语言模型在金融市场中展现出显著的能力，但同时也面临挑战，并具有潜在的未来发展方向。本综述全面概述了这些能力、挑战和方向。", "translation": "大型语言模型（LLMs）已被应用于金融决策，增强了投资策略的分析能力。传统的投资策略通常利用定量模型、基本面分析和技术指标。然而，LLMs引入了处理和分析大量结构化和非结构化数据、提取有意义见解以及实时增强决策的新能力。本综述对金融领域LLMs的最新研究提供了结构化概述，将研究贡献分为四个主要框架：基于LLM的框架和管道、混合集成方法、微调和适应方法、以及基于代理的架构。本研究对近期LLMs在股票选择、风险评估、情感分析、交易和金融预测应用中的研究进行了结构化回顾。通过回顾现有文献，本研究强调了LLMs在金融市场中的能力、挑战和潜在方向。", "summary": "本综述全面审视了大型语言模型（LLMs）在金融投资和市场分析中的应用。文章首先指出LLMs如何通过处理大数据和提取实时洞察来超越传统金融分析方法。接着，它将现有研究分为LLM驱动框架、混合集成、微调策略和基于代理的系统四大类。该研究进一步探讨了LLMs在股票选择、风险评估、情感分析、交易和金融预测等具体金融应用中的表现，并最终总结了LLMs在金融市场中的潜力、面临的挑战及未来的发展方向。", "keywords": "大型语言模型, 金融投资, 市场分析, 综述, 金融科技", "comments": "这篇综述论文的重要性在于它系统地梳理了大型语言模型在金融领域应用的最新进展，为研究人员和从业者提供了一个清晰的路线图。它不仅指出了LLMs的强大能力，也坦诚地讨论了其面临的挑战，这对于推动该领域的健康发展至关重要。结构化的分类方法也使得理解复杂的研究格局变得更加容易。"}}
{"id": "2507.02559", "title": "Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability", "authors": ["Luca Baroni", "Galvin Khara", "Joachim Schaeffer", "Marat Subkhankulov", "Stefan Heimersheim"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02559v1", "summary": "Layer-wise normalization (LN) is an essential component of virtually all\ntransformer-based large language models. While its effects on training\nstability are well documented, its role at inference time is poorly understood.\nAdditionally, LN layers hinder mechanistic interpretability by introducing\nadditional nonlinearities and increasing the interconnectedness of individual\nmodel components. Here, we show that all LN layers can be removed from every\nGPT-2 model with only a small increase in validation loss (e.g. +0.03\ncross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in\nlanguage modeling. We find that the amount of fine-tuning data needed for LN\nremoval grows sublinearly with model parameters, suggesting scaling to larger\nmodels is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.\nFurthermore, we test interpretability techniques on LN-free models. Direct\nlogit attribution now gives the exact direct effect of individual components,\nwhile the accuracy of attribution patching does not significantly improve. We\nalso confirm that GPT-2's \"confidence neurons\" are inactive in the LN-free\nmodels. Our work clarifies the role of LN layers in language modeling, showing\nthat GPT-2-class models can function without LN layers. We hope that our\nLN-free analogs of the GPT-2 family of models will enable more precise\ninterpretability research and improve our understanding of language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02559v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Transformer在推理时不需要LayerNorm：将LayerNorm移除扩展到GPT-2 XL及其对机械可解释性的影响", "tldr": "研究表明，可以从GPT-2模型中移除LayerNorm层，性能损失很小，这改善了可解释性并表明LayerNorm对于语言建模并非必不可少。", "motivation": "Layer-wise normalization (LN) 在Transformer模型中普遍存在，但其在推理时的作用尚不明确。此外，LN层通过引入额外的非线性和增加模型组件的互联性，阻碍了机械可解释性研究。", "method": "研究人员从所有GPT-2模型中移除了LayerNorm层，并量化了验证损失的变化。他们还测试了在无LN模型上的可解释性技术，包括直接logit归因和归因修补，并观察了“置信神经元”的活跃度。", "result": "所有LN层都可以从GPT-2模型中移除，仅导致验证损失小幅增加（例如，GPT-2 XL的交叉熵损失增加+0.03）。移除LN所需的微调数据量与模型参数呈亚线性增长，表明可扩展到更大的模型。在无LN模型上，直接logit归因现在可以提供单个组件的精确直接效应，但归因修补的准确性没有显著提高。此外，GPT-2的“置信神经元”在无LN模型中不活跃。", "conclusion": "GPT-2类模型可以在没有LayerNorm层的情况下运行，这表明LN在语言建模中不扮演实质性角色。移除LN层有望实现更精确的可解释性研究，并增进对语言模型的理解。", "translation": "逐层归一化（LN）是几乎所有基于Transformer的大型语言模型中必不可少的一个组成部分。虽然其对训练稳定性的影响已有充分记录，但其在推理时的作用却知之甚少。此外，LN层通过引入额外的非线性和增加单个模型组件的互联性，阻碍了机械可解释性。在此，我们表明所有LN层都可以从每个GPT-2模型中移除，而验证损失仅有小幅增加（例如，GPT-2 XL的交叉熵损失增加+0.03）。因此，LN在语言建模中不可能发挥实质性作用。我们发现，移除LN所需的微调数据量与模型参数呈亚线性增长，这表明扩展到更大的模型是可行的。我们在Hugging Face上发布了一套无LN的GPT-2模型。此外，我们还在无LN模型上测试了解释性技术。直接logit归因现在可以给出单个组件的精确直接效应，而归因修补的准确性没有显著提高。我们还确认了GPT-2的“置信神经元”在无LN模型中不活跃。我们的工作阐明了LN层在语言建模中的作用，表明GPT-2类模型可以在没有LN层的情况下运行。我们希望我们的GPT-2系列模型的无LN模拟版本能够实现更精确的可解释性研究，并提高我们对语言模型的理解。", "summary": "本文证明，Transformer模型中常用的LayerNorm (LN) 层可以从GPT-2模型（包括GPT-2 XL）中移除，且对性能影响极小。这挑战了LN对语言建模至关重要的观点。作者指出，移除LN通过允许直接logit归因来提高机械可解释性，并使“置信神经元”不活跃。研究还表明，将LN移除扩展到更大模型是可行的，并发布了一系列无LN的GPT-2模型，以促进未来的可解释性研究。", "keywords": "LayerNorm移除, GPT-2, 机械可解释性, 语言模型, 推理时间", "comments": "这项工作具有创新性，它挑战了LayerNorm在Transformer模型中不可或缺的普遍认知，特别是其在推理时的作用。通过证明GPT-2模型可以在没有LN的情况下运行且性能损失极小，该研究为机械可解释性开辟了新途径，简化了模型架构，有助于更深入地理解大型语言模型的工作原理。其局限性在于主要集中在GPT-2模型上，尽管暗示了可扩展性，但其发现对更大型或不同架构的Transformer模型的普适性仍需进一步验证。"}}
{"id": "2507.02419", "title": "AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars", "authors": ["Yiming Zhong", "Xiaolin Zhang", "Ligang Liu", "Yao Zhao", "Yunchao Wei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02419v1", "summary": "Similar to facial beautification in real life, 3D virtual avatars require\npersonalized customization to enhance their visual appeal, yet this area\nremains insufficiently explored. Although current 3D Gaussian editing methods\ncan be adapted for facial makeup purposes, these methods fail to meet the\nfundamental requirements for achieving realistic makeup effects: 1) ensuring a\nconsistent appearance during drivable expressions, 2) preserving the identity\nthroughout the makeup process, and 3) enabling precise control over fine\ndetails. To address these, we propose a specialized 3D makeup method named\nAvatarMakeup, leveraging a pretrained diffusion model to transfer makeup\npatterns from a single reference photo of any individual. We adopt a\ncoarse-to-fine idea to first maintain the consistent appearance and identity,\nand then to refine the details. In particular, the diffusion model is employed\nto generate makeup images as supervision. Due to the uncertainties in diffusion\nprocess, the generated images are inconsistent across different viewpoints and\nexpressions. Therefore, we propose a Coherent Duplication method to coarsely\napply makeup to the target while ensuring consistency across dynamic and\nmultiview effects. Coherent Duplication optimizes a global UV map by recoding\nthe averaged facial attributes among the generated makeup images. By querying\nthe global UV map, it easily synthesizes coherent makeup guidance from\narbitrary views and expressions to optimize the target avatar. Given the coarse\nmakeup avatar, we further enhance the makeup by incorporating a Refinement\nModule into the diffusion model to achieve high makeup quality. Experiments\ndemonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality\nand consistency throughout animation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02419v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AvatarMakeup：可动画3D头部化身的真实感美妆迁移", "tldr": "AvatarMakeup提出了一种利用预训练扩散模型为3D可动画头部化身实现真实感美妆迁移的方法，解决了现有方法在表情一致性、身份保留和细节控制上的不足。", "motivation": "3D虚拟化身需要个性化定制以增强视觉吸引力，但该领域探索不足。现有3D高斯编辑方法无法满足真实感美妆效果的基本要求：1) 驱动表情时保持外观一致性；2) 美妆过程中保留身份；3) 精确控制细节。", "method": "提出了一种名为AvatarMakeup的专用3D美妆方法，利用预训练扩散模型从单张参考照片迁移美妆模式。采用粗到细的思路，首先通过“Coherent Duplication”方法优化全局UV图，以确保动态和多视角效果下美妆的一致性，从而粗略应用美妆并保持外观和身份一致性。随后，通过将“Refinement Module”整合到扩散模型中，进一步增强美妆质量，以实现高品质美妆。", "result": "实验证明，AvatarMakeup在动画过程中实现了最先进的美妆迁移质量和一致性。", "conclusion": "AvatarMakeup成功解决了3D可动画头部化身美妆迁移中存在的挑战，实现了高品质、一致且身份保留的真实感美妆效果。", "translation": "与现实生活中的面部美化类似，3D虚拟化身需要个性化定制以增强其视觉吸引力，然而该领域仍未得到充分探索。尽管当前的3D高斯编辑方法可以适应面部美妆目的，但这些方法未能满足实现真实感美妆效果的基本要求：1) 确保在可驱动表情期间保持一致的外观，2) 在整个美妆过程中保留身份，以及3) 能够精确控制精细细节。为了解决这些问题，我们提出了一种名为AvatarMakeup的专用3D美妆方法，该方法利用预训练的扩散模型，从任何个体的单张参考照片中迁移美妆模式。我们采用了一种从粗到细的思路，首先保持一致的外观和身份，然后细化细节。特别是，扩散模型被用于生成美妆图像作为监督。由于扩散过程中的不确定性，生成的图像在不同视角和表情下不一致。因此，我们提出了一种“Coherent Duplication”方法，以粗略地将美妆应用于目标，同时确保在动态和多视角效果下的一致性。“Coherent Duplication”通过重新编码生成美妆图像中平均的面部属性来优化全局UV图。通过查询全局UV图，它可以轻松地从任意视图和表情合成连贯的美妆指导，以优化目标化身。鉴于粗略的美妆化身，我们通过将“Refinement Module”整合到扩散模型中，进一步增强美妆效果，以实现高质量美妆。实验表明，AvatarMakeup在整个动画过程中实现了最先进的美妆迁移质量和一致性。", "summary": "本文提出了AvatarMakeup，一种针对3D可动画头部化身的真实感美妆迁移方法，旨在解决现有技术在表情一致性、身份保留和细节控制方面的不足。该方法利用预训练的扩散模型，采用粗到细的策略，通过“Coherent Duplication”技术确保跨视角和表情的美妆一致性，并通过“Refinement Module”提升美妆细节质量。实验结果表明，AvatarMakeup在美妆迁移质量和动画一致性方面达到了SOTA水平。", "keywords": "3D美妆迁移, 扩散模型, 可动画化身, Coherent Duplication, 虚拟形象", "comments": "AvatarMakeup创新性地将扩散模型应用于3D美妆迁移，并提出了“Coherent Duplication”方法来解决扩散模型在多视角和动态表情下的一致性问题，这对于实现真实感和可动画的虚拟形象美妆至关重要。其粗到细的策略和对UV图的利用，有效解决了现有方法在身份保留和细节控制上的挑战，具有重要的应用前景。"}}
{"id": "2507.02585", "title": "Scalable Interconnect Learning in Boolean Networks", "authors": ["Fabian Kresse", "Emily Yu", "Christoph H. Lampert"], "categories": ["cs.LG", "cs.LO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 8 Figures", "url": "http://arxiv.org/abs/2507.02585v1", "summary": "Learned Differentiable Boolean Logic Networks (DBNs) already deliver\nefficient inference on resource-constrained hardware. We extend them with a\ntrainable, differentiable interconnect whose parameter count remains constant\nas input width grows, allowing DBNs to scale to far wider layers than earlier\nlearnable-interconnect designs while preserving their advantageous accuracy. To\nfurther reduce model size, we propose two complementary pruning stages: an\nSAT-based logic equivalence pass that removes redundant gates without affecting\nperformance, and a similarity-based, data-driven pass that outperforms a\nmagnitude-style greedy baseline and offers a superior compression-accuracy\ntrade-off.", "comment": "12 pages, 8 Figures", "pdf_url": "http://arxiv.org/pdf/2507.02585v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可扩展布尔网络中的互连学习", "tldr": "本文提出了一种可训练的、可微分的互连方法，使可微分布尔逻辑网络（DBNs）能够扩展到更宽的层，并引入了两种剪枝阶段以减小模型大小。", "motivation": "现有可微分布尔逻辑网络（DBNs）在资源受限硬件上推理高效，但早期可学习互连设计在扩展到更宽层时存在限制，且需要进一步减小模型大小。", "method": "1. 通过引入一个可训练、可微分的互连来扩展DBNs，该互连的参数数量不随输入宽度增长。2. 提出两种互补的剪枝阶段：基于SAT的逻辑等价剪枝（移除冗余门）和基于相似性、数据驱动的剪枝。", "result": "新的互连设计使DBNs能够扩展到更宽的层，同时保持其有利的准确性。相似性剪枝阶段优于基于幅度的贪婪基线，并提供了更优的压缩-准确性权衡。", "conclusion": "本文通过引入可扩展的互连和有效的剪枝策略，显著提高了可微分布Ns的扩展性和模型效率，使其能处理更宽的层并减小模型大小。", "translation": "学习到的可微分布尔逻辑网络（DBNs）已经在资源受限硬件上提供了高效的推理。我们通过一个可训练的、可微分的互连扩展了它们，该互连的参数数量在输入宽度增长时保持不变，从而使DBNs能够扩展到比早期可学习互连设计宽得多的层，同时保持其有利的准确性。为了进一步减小模型大小，我们提出了两个互补的剪枝阶段：一个基于SAT的逻辑等价阶段，它在不影响性能的情况下移除冗余门；以及一个基于相似性、数据驱动的阶段，它优于基于幅度的贪婪基线，并提供了更优的压缩-准确性权衡。", "summary": "本文提出了一种创新的方法来提高可微分布尔逻辑网络（DBNs）的可扩展性，通过引入一种参数数量不随输入宽度增长的可训练、可微分互连。此外，为了进一步优化模型大小，研究人员提出了两种互补的剪枝策略：一种基于SAT的逻辑等价剪枝和一种基于相似性的数据驱动剪枝。这些方法共同使DBNs能够处理更宽的输入层，同时保持高准确性并实现有效的模型压缩。", "keywords": "可微分布尔网络, 可扩展性, 互连学习, 模型剪枝, 硬件推理", "comments": "本文的创新之处在于其提出的可扩展互连设计，解决了DBNs在处理更宽输入时的扩展性问题，且参数量不随输入宽度增长，这对于资源受限的硬件非常重要。其次，引入的两种剪枝策略，特别是数据驱动的相似性剪枝，为模型压缩提供了新的有效途径，并在压缩与准确性之间取得了更好的平衡。"}}
{"id": "2507.02608", "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation", "authors": ["François Rozet", "Ruben Ohana", "Michael McCabe", "Gilles Louppe", "François Lanusse", "Shirley Ho"], "categories": ["cs.LG", "physics.flu-dyn"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02608v1", "summary": "The steep computational cost of diffusion models at inference hinders their\nuse as fast physics emulators. In the context of image and video generation,\nthis computational drawback has been addressed by generating in the latent\nspace of an autoencoder instead of the pixel space. In this work, we\ninvestigate whether a similar strategy can be effectively applied to the\nemulation of dynamical systems and at what cost. We find that the accuracy of\nlatent-space emulation is surprisingly robust to a wide range of compression\nrates (up to 1000x). We also show that diffusion-based emulators are\nconsistently more accurate than non-generative counterparts and compensate for\nuncertainty in their predictions with greater diversity. Finally, we cover\npractical design choices, spanning from architectures to optimizers, that we\nfound critical to train latent-space emulators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02608v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "迷失在潜在空间：潜在扩散模型在物理模拟中的实证研究", "tldr": "扩散模型在物理模拟中计算成本高昂。本研究探讨了使用潜在扩散模型（LDMs）来加速物理模拟，发现LDMs在高达1000倍压缩率下仍能保持惊人的准确性，并优于非生成模型。", "motivation": "扩散模型在推理时高昂的计算成本阻碍了它们作为快速物理模拟器的应用。本文旨在探究能否将图像和视频生成中在自编码器潜在空间进行生成的策略，有效地应用于动态系统模拟以降低成本。", "method": "研究人员调查了将潜在空间生成策略应用于动态系统模拟的有效性及其代价。他们评估了潜在空间模拟在不同压缩率下的准确性，并将基于扩散的模拟器与非生成性模拟器进行了比较。此外，他们还探讨了训练潜在空间模拟器的关键实际设计选择。", "result": "潜在空间模拟的准确性对广泛的压缩率（高达1000倍）表现出惊人的鲁棒性。基于扩散的模拟器始终比非生成性对应物更准确，并且通过更大的多样性弥补了预测中的不确定性。研究还确定了训练潜在空间模拟器的关键实际设计选择，包括架构和优化器。", "conclusion": "潜在扩散模型为快速准确的物理模拟提供了一种有前景的方法，克服了传统扩散模型的计算限制，同时保持了性能和多样性。", "translation": "扩散模型在推理时高昂的计算成本阻碍了它们作为快速物理模拟器的应用。在图像和视频生成领域，这一计算缺陷已通过在自编码器的潜在空间而非像素空间中生成来解决。在这项工作中，我们研究了类似策略是否能有效应用于动态系统的模拟，以及其代价。我们发现，潜在空间模拟的准确性在广泛的压缩率（高达1000倍）下表现出惊人的鲁棒性。我们还表明，基于扩散的模拟器始终比非生成性对应物更准确，并通过更大的多样性来弥补其预测中的不确定性。最后，我们涵盖了从架构到优化器的实际设计选择，这些选择对于训练潜在空间模拟器至关重要。", "summary": "本文探讨了将潜在扩散模型（LDMs）应用于加速物理模拟，以解决标准扩散模型高昂的计算成本问题。研究表明，LDMs即使在高达1000倍的显著压缩率下也能保持高精度，并且在准确性和多样性方面优于非生成模型，同时提供了实际的设计指导。", "keywords": "潜在扩散模型, 物理模拟, 计算成本, 动态系统, 压缩率", "comments": "本论文通过将通常用于图像/视频生成的潜在扩散模型应用于物理模拟领域，做出了有价值的贡献。其主要创新在于证明了即使在极高压缩率下，模型精度仍表现出惊人的鲁棒性，这使得扩散模型在计算密集型物理模拟中更具实用性。与非生成模型相比，其在准确性和多样性方面的发现突出了生成式AI在科学计算中的潜力。"}}
{"id": "2507.02659", "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding", "authors": ["Ramchalam Kinattinkara Ramakrishnan", "Zhaocong Yuan", "Shaojie Zhuo", "Chen Feng", "Yicheng Lin", "Chenzheng Su", "Xiaopeng Zhang"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02659v1", "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02659v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "OmniDraft：一种用于设备上推测解码的跨词汇、在线自适应草稿器", "tldr": "OmniDraft是一个统一的框架，使单个草稿模型能够与任何目标模型配合使用，并在线动态适应用户数据，解决了推测解码中草稿模型与目标模型不兼容以及在线部署中延迟改进的挑战，并提供高达1.5-2倍的速度提升。", "motivation": "在推测解码的在线部署设置中，存在两个主要挑战：1) 使用与草稿模型不兼容的目标模型；2) 期望在使用和时间上获得延迟改进。这促使了“一个草稿器适用于所有”范式的需求。", "method": "本文提出了OmniDraft框架，通过引入在线n-gram缓存与混合蒸馏微调相结合的方式，解决了草稿模型和目标模型之间的跨词汇不匹配问题。此外，通过利用自适应草稿技术进一步提高了解码速度。", "result": "OmniDraft在数学推理、编码和文本生成任务的在线学习中表现出色。它使单个Llama-68M模型能够与包括Vicuna-7B、Qwen2-7B和Llama3-8B在内的各种目标模型进行推测解码配对，并额外提供高达1.5-2倍的速度提升。", "conclusion": "OmniDraft通过解决跨词汇不匹配和在线适应性挑战，实现了“一个草稿器适用于所有”的范式，特别适用于对模型成本、效率和用户定制有高要求的设备上大型语言模型（LLM）应用。", "translation": "推测解码通常要求有一个小而高效的草稿模型，该模型要么是预训练的，要么是离线蒸馏到特定的目标模型系列，例如Llama或Qwen模型。然而，在在线部署设置中，存在两个主要挑战：1) 使用与草稿模型不兼容的目标模型；2) 期望在使用和时间上获得延迟改进。在这项工作中，我们提出了OmniDraft，一个统一的框架，它使单个草稿模型能够与任何目标模型配合使用并动态适应用户数据。我们引入了一个在线n-gram缓存与混合蒸馏微调相结合的方式，以解决草稿模型和目标模型之间的跨词汇不匹配问题；并通过利用自适应草稿技术进一步提高了解码速度。OmniDraft特别适用于设备上大型语言模型（LLM）应用，其中模型成本、效率和用户定制是主要的争议点。这进一步突出了解决上述挑战的必要性，并推动了“一个草稿器适用于所有”的范式。我们通过在数学推理、编码和文本生成任务上进行在线学习，展示了OmniDraft框架的熟练性。值得注意的是，OmniDraft使单个Llama-68M模型能够与包括Vicuna-7B、Qwen2-7B和Llama3-8B在内的各种目标模型进行推测解码配对；并额外提供高达1.5-2倍的速度提升。", "summary": "本研究提出了OmniDraft，一个针对设备上推测解码的统一框架，旨在解决现有草稿模型与目标模型不兼容以及在线部署中延迟改进的挑战。OmniDraft通过结合在线n-gram缓存和混合蒸馏微调来解决跨词汇不匹配问题，并利用自适应草稿技术提升解码速度。实验证明，该框架使单个草稿模型能与多种目标模型兼容，并在数学推理、编码和文本生成任务中实现1.5-2倍的速度提升，特别适用于对成本、效率和定制化有要求的设备上LLM应用。", "keywords": "推测解码, 在线自适应, 跨词汇, 设备上LLM, OmniDraft", "comments": "OmniDraft的创新之处在于其“一个草稿器适用于所有”的范式，通过在线自适应和跨词汇兼容性解决了推测解码在实际部署中的关键限制。其混合蒸馏微调和自适应草稿技术是核心创新点，对于降低设备上LLM的部署成本和提高效率具有重要意义。"}}
{"id": "2507.02619", "title": "L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation", "authors": ["Hazal Mogultay Ozcan", "Sinan Kalkan", "Fatos T. Yarman-Vural"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The paper is under revision at Machine Vision and Applications", "url": "http://arxiv.org/abs/2507.02619v1", "summary": "In this paper, we propose a novel model called Learnable VAE (L-VAE), which\nlearns a disentangled representation together with the hyperparameters of the\ncost function. L-VAE can be considered as an extension of \\b{eta}-VAE, wherein\nthe hyperparameter, \\b{eta}, is empirically adjusted. L-VAE mitigates the\nlimitations of \\b{eta}-VAE by learning the relative weights of the terms in the\nloss function to control the dynamic trade-off between disentanglement and\nreconstruction losses. In the proposed model, the weight of the loss terms and\nthe parameters of the model architecture are learned concurrently. An\nadditional regularization term is added to the loss function to prevent bias\ntowards either reconstruction or disentanglement losses. Experimental analyses\nshow that the proposed L-VAE finds an effective balance between reconstruction\nfidelity and disentangling the latent dimensions. Comparisons of the proposed\nL-VAE against \\b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\\sigma}-VAE on\ndatasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that\nL-VAE consistently provides the best or the second best performances measured\nby a set of disentanglement metrics. Moreover, qualitative experiments on\nCelebA dataset, confirm the success of the L-VAE model for disentangling the\nfacial attributes.", "comment": "The paper is under revision at Machine Vision and Applications", "pdf_url": "http://arxiv.org/pdf/2507.02619v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "L-VAE：具有可学习Beta的变分自编码器用于解缠表示", "tldr": "L-VAE通过学习损失函数中的超参数，动态平衡解缠和重建损失，从而实现更好的解缠表示。", "motivation": "现有的β-VAE模型需要经验性地调整超参数β，且难以动态平衡解缠和重建损失。", "method": "提出L-VAE模型，它是β-VAE的扩展，通过并发学习损失项的权重和模型架构参数来动态控制解缠和重建损失之间的权衡。模型中增加了一个额外的正则化项以防止偏向。", "result": "L-VAE在重建保真度和解缠潜在维度之间找到了有效平衡。在dSprites、MPI3D-complex、Falcor3D和Isaac3D等数据集上，L-VAE在解缠指标方面表现最佳或次佳。在CelebA数据集上的定性实验也证实了其在解缠面部属性方面的成功。", "conclusion": "L-VAE通过学习损失函数中的超参数，能够有效地实现解缠表示，并在多个数据集上优于现有方法，同时保持了良好的重建质量。", "translation": "在本文中，我们提出了一种名为可学习VAE（L-VAE）的新模型，该模型与成本函数的超参数一起学习解缠表示。L-VAE可以被视为β-VAE的扩展，其中超参数β是凭经验调整的。L-VAE通过学习损失函数中项的相对权重来控制解缠和重建损失之间的动态权衡，从而缓解了β-VAE的局限性。在所提出的模型中，损失项的权重和模型架构的参数是同时学习的。损失函数中增加了一个额外的正则化项，以防止偏向重建或解缠损失。实验分析表明，所提出的L-VAE在重建保真度和解缠潜在维度之间找到了有效的平衡。将所提出的L-VAE与β-VAE、VAE、ControlVAE、DynamicVAE和σ-VAE在dSprites、MPI3D-complex、Falcor3D和Isaac3D等数据集上进行比较，结果显示L-VAE在解缠指标方面始终提供最佳或次佳的性能。此外，在CelebA数据集上的定性实验证实了L-VAE模型在解缠面部属性方面的成功。", "summary": "本文提出L-VAE，一种变分自编码器模型，通过学习损失函数中各项的权重，动态平衡解缠表示和重建损失，从而克服了传统β-VAE需要经验调整超参数的限制。实验证明L-VAE在多个数据集上均能实现优异的解缠效果和重建质量，性能优于多种现有模型。", "keywords": "变分自编码器, 解缠表示, 可学习Beta, L-VAE, 超参数学习", "comments": "L-VAE的创新之处在于将损失函数的超参数β变为可学习的，从而实现了对解缠和重建损失之间权衡的动态控制，这解决了传统方法中经验调参的痛点，并可能带来更泛化和鲁棒的解缠表示。"}}
{"id": "2507.02018", "title": "NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction", "authors": ["Yingjie Niu", "Mingchuan Zhao", "Valerio Poti", "Ruihai Dong"], "categories": ["q-fin.ST", "cs.AI", "cs.LG", "I.2.1"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02018v1", "summary": "Graph representation learning methods have been widely adopted in financial\napplications to enhance company representations by leveraging inter-firm\nrelationships. However, current approaches face three key challenges: (1) The\nadvantages of relational information are obscured by limitations in downstream\ntask designs; (2) Existing graph models specifically designed for stock\nprediction often suffer from excessive complexity and poor generalization; (3)\nExperience-based construction of corporate relationship graphs lacks effective\ncomparison of different graph structures. To address these limitations, we\npropose a long-term stock prediction task and develop a Node-level Graph\nAttention Network (NGAT) specifically tailored for corporate relationship\ngraphs. Furthermore, we experimentally demonstrate the limitations of existing\ngraph comparison methods based on model downstream task performance.\nExperimental results across two datasets consistently demonstrate the\neffectiveness of our proposed task and model. The project is publicly available\non GitHub to encourage reproducibility and future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02018v1", "cate": "q-fin.ST", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "NGAT：一种用于长期股票预测的节点级图注意力网络", "tldr": "本文提出了一个节点级图注意力网络（NGAT）和一个长期股票预测任务，以解决现有图学习方法在金融应用中面临的挑战，并在两个数据集上验证了其有效性。", "motivation": "当前图表示学习方法在金融应用中面临三个主要挑战：1) 关系信息的优势被下游任务设计所掩盖；2) 现有专为股票预测设计的图模型过于复杂且泛化能力差；3) 基于经验构建的公司关系图缺乏有效的比较方法。", "method": "本文提出了一个长期股票预测任务，并开发了一个专门针对公司关系图的节点级图注意力网络（NGAT）。此外，通过实验证明了现有基于模型下游任务性能的图比较方法的局限性。", "result": "在两个数据集上的实验结果一致表明了所提出的任务和模型的有效性。", "conclusion": "所提出的长期股票预测任务和节点级图注意力网络（NGAT）能够有效解决现有图学习方法在金融股票预测中遇到的挑战，并提升预测性能。", "translation": "图表示学习方法已被广泛应用于金融领域，通过利用公司间关系来增强公司表示。然而，当前的方法面临三个关键挑战：(1) 关系信息的优势被下游任务设计的局限性所掩盖；(2) 现有专为股票预测设计的图模型通常存在过度复杂和泛化能力差的问题；(3) 基于经验构建的企业关系图缺乏对不同图结构的有效比较。为了解决这些局限性，我们提出了一个长期股票预测任务，并开发了一个专门为企业关系图量身定制的节点级图注意力网络（NGAT）。此外，我们实验性地证明了现有基于模型下游任务性能的图比较方法的局限性。在两个数据集上的实验结果一致表明了我们提出的任务和模型的有效性。该项目已在GitHub上公开，以鼓励可复现性和未来的研究。", "summary": "本文针对图表示学习在金融应用中面临的挑战，如关系信息优势被掩盖、模型复杂性高和图结构比较困难等问题，提出了一个长期股票预测任务和一个节点级图注意力网络（NGAT）。该网络专门用于处理公司关系图。实验结果在两个数据集上验证了所提出的任务和模型的有效性，并指出现有图比较方法的局局限性。项目代码已开源。", "keywords": "图注意力网络, 股票预测, 金融应用, 公司关系图, 长期预测", "comments": "本文的创新点在于提出了一个专门用于长期股票预测的节点级图注意力网络（NGAT），并结合新的任务定义，有效解决了现有图模型在金融领域应用中的泛化能力和复杂性问题。同时，它还强调了现有图比较方法的局限性，为未来的研究提供了方向。开源项目有助于研究的可复现性。"}}
{"id": "2507.02454", "title": "Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection", "authors": ["Weiwei Duan", "Luping Ji", "Shengjia Chen", "Sicheng Zhu", "Jianghong Huang", "Mao Ye"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02454v1", "summary": "Different from general object detection, moving infrared small target\ndetection faces huge challenges due to tiny target size and weak background\ncontrast.Currently, most existing methods are fully-supervised, heavily relying\non a large number of manual target-wise annotations. However, manually\nannotating video sequences is often expensive and time-consuming, especially\nfor low-quality infrared frame images. Inspired by general object detection,\nnon-fully supervised strategies ($e.g.$, weakly supervised) are believed to be\npotential in reducing annotation requirements. To break through traditional\nfully-supervised frameworks, as the first exploration work, this paper proposes\na new weakly-supervised contrastive learning (WeCoL) scheme, only requires\nsimple target quantity prompts during model training.Specifically, in our\nscheme, based on the pretrained segment anything model (SAM), a potential\ntarget mining strategy is designed to integrate target activation maps and\nmulti-frame energy accumulation.Besides, contrastive learning is adopted to\nfurther improve the reliability of pseudo-labels, by calculating the similarity\nbetween positive and negative samples in feature subspace.Moreover, we propose\na long-short term motion-aware learning scheme to simultaneously model the\nlocal motion patterns and global motion trajectory of small targets.The\nextensive experiments on two public datasets (DAUB and ITSDT-15K) verify that\nour weakly-supervised scheme could often outperform early fully-supervised\nmethods. Even, its performance could reach over 90\\% of state-of-the-art (SOTA)\nfully-supervised ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02454v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于数量提示的弱监督对比学习用于运动红外小目标检测", "tldr": "本文提出了一个名为WeCoL的弱监督对比学习方案，仅需简单的目标数量提示即可用于运动红外小目标检测，其性能可媲美甚至超越部分全监督方法。", "motivation": "运动红外小目标检测面临目标尺寸微小和背景对比度弱的巨大挑战。现有方法多为全监督，严重依赖昂贵耗时的人工标注。为了减少标注需求，本文探索弱监督策略，旨在突破传统全监督框架。", "method": "本文提出了弱监督对比学习（WeCoL）方案。具体方法包括：基于预训练的SAM模型，设计了融合目标激活图和多帧能量累积的潜在目标挖掘策略；采用对比学习，通过计算特征子空间中正负样本的相似性来提高伪标签的可靠性；提出了长短期运动感知学习方案，同时建模小目标的局部运动模式和全局运动轨迹。", "result": "在DAUB和ITSDT-15K两个公共数据集上的大量实验表明，所提出的弱监督方案通常优于早期的全监督方法。甚至，其性能可以达到最先进（SOTA）全监督方法的90%以上。", "conclusion": "本文首次探索了弱监督对比学习方案WeCoL用于运动红外小目标检测，仅需目标数量提示，并在公共数据集上取得了与先进全监督方法相当的性能，验证了弱监督策略在减少标注成本方面的潜力。", "translation": "与一般目标检测不同，运动红外小目标检测由于目标尺寸微小和背景对比度弱而面临巨大挑战。目前，大多数现有方法都是全监督的，严重依赖大量人工逐目标标注。然而，人工标注视频序列通常成本高昂且耗时，特别是对于低质量红外帧图像。受一般目标检测的启发，非全监督策略（如弱监督）被认为在减少标注需求方面具有潜力。为了突破传统的全监督框架，作为首次探索性工作，本文提出了一种新的弱监督对比学习（WeCoL）方案，在模型训练期间仅需要简单的目标数量提示。具体而言，在我们的方案中，基于预训练的Segment Anything Model（SAM），设计了一种潜在目标挖掘策略，以整合目标激活图和多帧能量累积。此外，采用对比学习，通过计算特征子空间中正样本和负样本之间的相似性来进一步提高伪标签的可靠性。此外，我们提出了一种长短期运动感知学习方案，以同时建模小目标的局部运动模式和全局运动轨迹。在两个公共数据集（DAUB和ITSDT-15K）上进行的广泛实验验证了我们的弱监督方案通常可以优于早期的全监督方法。甚至，其性能可以达到最先进（SOTA）全监督方法的90%以上。", "summary": "本文针对运动红外小目标检测中全监督方法对大量标注的依赖问题，首次提出了一种名为WeCoL的弱监督对比学习方案。该方案仅需简单的目标数量提示，通过结合基于SAM的潜在目标挖掘策略、对比学习以提高伪标签可靠性，以及长短期运动感知学习来建模目标运动。实验结果表明，WeCoL在公共数据集上表现出色，性能可与先进的全监督方法相媲美。", "keywords": "弱监督学习, 对比学习, 红外小目标检测, 数量提示, SAM", "comments": "本文的创新点在于首次将弱监督对比学习引入运动红外小目标检测领域，显著降低了对昂贵人工标注的依赖。其提出的仅需目标数量提示的策略，结合了SAM进行目标挖掘和对比学习提升伪标签质量，具有很强的实用价值和开创性。性能接近SOTA全监督方法，凸显了弱监督策略在这一挑战性任务中的巨大潜力。"}}
{"id": "2507.02624", "title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes", "authors": ["Antoine Honoré", "Borja Rodríguez Gálvez", "Yoomi Park", "Yitian Zhou", "Volker M. Lauschke", "Ming Xiao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12+8 pages", "url": "http://arxiv.org/abs/2507.02624v1", "summary": "Variant effect predictors (VEPs) aim to assess the functional impact of\nprotein variants, traditionally relying on multiple sequence alignments (MSAs).\nThis approach assumes that naturally occurring variants are fit, an assumption\nchallenged by pharmacogenomics, where some pharmacogenes experience low\nevolutionary pressure. Deep mutational scanning (DMS) datasets provide an\nalternative by offering quantitative fitness scores for variants. In this work,\nwe propose a transformer-based matrix variational auto-encoder (matVAE) with a\nstructured prior and evaluate its performance on 33 DMS datasets corresponding\nto 26 drug target and ADME proteins from the ProteinGym benchmark. Our model\ntrained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence\nmodel in zero-shot prediction on DMS datasets, despite using an order of\nmagnitude fewer parameters and requiring less computation at inference time. We\nalso compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on\nDMS data, and find that the latter performs better on supervised prediction\ntasks. Additionally, incorporating AlphaFold-generated structures into our\ntransformer model further improves performance, achieving results comparable to\nDeepSequence trained on MSAs and finetuned on DMS. These findings highlight the\npotential of DMS datasets to replace MSAs without significant loss in\npredictive performance, motivating further development of DMS datasets and\nexploration of their relationships to enhance variant effect prediction.", "comment": "12+8 pages", "pdf_url": "http://arxiv.org/pdf/2507.02624v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于药物基因变异效应预测的矩阵变分自编码器", "tldr": "本研究提出了一种基于Transformer的矩阵变分自编码器（matVAE），用于药物基因中的变异效应预测，并展示了DMS数据集在预测性能上替代MSA的潜力。", "motivation": "传统的变异效应预测器（VEPs）依赖于多序列比对（MSAs），但这种方法假设自然发生的变异是适应的，这在药物基因组学中受到挑战，因为一些药物基因的进化压力较低。深度突变扫描（DMS）数据集提供了定量适应度分数，为解决这一问题提供了替代方案。", "method": "本研究提出了一种基于Transformer的矩阵变分自编码器（matVAE），带有结构化先验，并在ProteinGym基准测试的33个DMS数据集上进行评估。模型在MSAs上训练（matVAE-MSA），也与在DMS数据上训练的类似容量模型（matENC-DMS）进行比较。此外，还将AlphaFold生成的结构整合到Transformer模型中以进一步提高性能。", "result": "在零样本预测方面，在MSAs上训练的matVAE-MSA在DMS数据集上的表现优于最先进的DeepSequence模型，尽管参数量少一个数量级且推理计算量更小。在监督预测任务中，在DMS数据上训练的matENC-DMS表现更好。将AlphaFold生成的结构整合到Transformer模型中进一步提高了性能，达到了与在MSAs上训练并在DMS上微调的DeepSequence相当的结果。", "conclusion": "这些发现突出了DMS数据集在预测性能上替代MSAs的潜力，激励了DMS数据集的进一步开发以及对其关系的探索，以增强变异效应预测。", "translation": "变异效应预测器（VEPs）旨在评估蛋白质变异的功能影响，传统上依赖于多序列比对（MSAs）。这种方法假设自然发生的变异是适应的，这一假设在药物基因组学中受到挑战，因为一些药物基因的进化压力较低。深度突变扫描（DMS）数据集通过提供变异的定量适应度分数，提供了一种替代方案。在这项工作中，我们提出了一种基于Transformer的矩阵变分自编码器（matVAE），带有结构化先验，并在ProteinGym基准测试中对应26种药物靶点和ADME蛋白的33个DMS数据集上评估其性能。我们模型在MSAs上训练（matVAE-MSA）在DMS数据集上的零样本预测中表现优于最先进的DeepSequence模型，尽管使用的参数量少一个数量级且推理时间计算量更小。我们还将matVAE-MSA与matENC-DMS（一个在DMS数据上训练的类似容量模型）进行比较，发现后者在监督预测任务中表现更好。此外，将AlphaFold生成的结构整合到我们的Transformer模型中进一步提高了性能，取得了与在MSAs上训练并在DMS上微调的DeepSequence相当的结果。这些发现突出了DMS数据集在预测性能上替代MSAs的潜力，激励了DMS数据集的进一步开发以及对其关系的探索，以增强变异效应预测。", "summary": "本研究提出了一种名为matVAE的基于Transformer的矩阵变分自编码器，用于药物基因中的变异效应预测。该模型在多序列比对（MSA）上训练（matVAE-MSA），在零样本预测任务中超越了现有先进模型DeepSequence，且计算效率更高。与在深度突变扫描（DMS）数据上训练的模型（matENC-DMS）相比，后者在监督任务中表现更优。值得注意的是，整合AlphaFold结构进一步提升了性能，表明DMS数据集有潜力替代MSA，从而推动了DMS数据集的开发和应用。", "keywords": "变异效应预测, 矩阵变分自编码器, 药物基因, 深度突变扫描, Transformer", "comments": "这篇论文的创新点在于提出了一个参数量更少、计算效率更高的Transformer-based matVAE模型，并且在零样本预测上超越了DeepSequence。同时，它明确指出了DMS数据集在变异效应预测中的巨大潜力，并展示了结合AlphaFold结构能够进一步提升性能，为该领域未来的研究方向提供了有价值的指引。论文强调了DMS数据集替代传统MSA的可能性，这对于药物基因组学领域具有重要意义。"}}
{"id": "2507.02628", "title": "Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data", "authors": ["Irena Girshovitz", "Atai Ambus", "Moni Shahar", "Ran Gilad-Bachrach"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      18 pages, 4 figures (+ appendix)", "url": "http://arxiv.org/abs/2507.02628v1", "summary": "Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods.", "comment": "18 pages, 4 figures (+ appendix)", "pdf_url": "http://arxiv.org/pdf/2507.02628v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "医疗数据啄取：一种用于结构化医疗数据自动化质量评估的上下文感知方法", "tldr": "该论文提出“医疗数据啄取”方法和MDPT工具，利用大型语言模型和软件测试概念，自动评估结构化医疗数据质量，以识别不一致或不符合要求的数据问题，从而提高研究结果的可靠性。", "motivation": "电子健康记录（EHR）数据在流行病学研究和人工智能训练中的应用日益增多，但其可靠性受限于严重的质量问题（如误报、偏见、系统错误），因为这些数据主要为临床和计费目的收集。现有质量评估方法不足，缺乏评估数据是否适合研究的系统程序。", "method": "该研究提出了“医疗数据啄取”方法，该方法借鉴了软件工程中的单元测试和覆盖概念来识别数据质量问题。他们开发了医疗数据啄取工具（MDPT），包含两个主要组件：（1）一个自动化测试生成器，利用大型语言模型和接地技术从数据和研究描述中创建测试套件；（2）一个数据测试框架，用于执行这些测试并报告潜在错误和覆盖率。", "result": "MDPT在All of Us (AoU)、MIMIC-III和SyntheticMass三个数据集上进行了评估，在四种条件下，每个队列生成了55-73个测试。这些测试成功识别了20-43个不一致或不符合要求的数据问题。研究还详细分析了LLM生成的测试套件在参考接地和值准确性方面的表现。", "conclusion": "该方法将外部医学知识整合到数据分析工作流程中，以实现上下文敏感的数据质量测试，从而提高结果的有效性。该方法从质量保证的角度解决了这些挑战，为未来的发展奠定了基础，包括支持额外的数据模态和改进接地方法。", "translation": "背景：电子健康记录（EHR）在流行病学研究和人工智能（AI）训练中的使用正在迅速增加。结果的可靠性取决于EHR数据的准确性和完整性。然而，由于EHR数据主要用于临床和计费目的，它们通常包含严重的质量问题，包括亚群的错误表示、偏见和系统错误。现有的质量评估方法仍然不足，缺乏评估数据是否适合研究的系统程序。\n方法：我们提出了“医疗数据啄取”方法，该方法借鉴了软件工程中的单元测试和覆盖概念来识别数据质量问题。我们使用医疗数据啄取工具（MDPT）来演示我们的方法，该工具由两个主要组件组成：（1）一个自动化测试生成器，它使用大型语言模型和接地技术从数据和研究描述中创建测试套件；（2）一个数据测试框架，用于执行这些测试，报告潜在错误和覆盖率。\n结果：我们在三个数据集上评估了MDPT：All of Us (AoU)、MIMIC-III和SyntheticMass，在四种条件下，每个队列生成了55-73个测试。这些测试正确识别了20-43个不一致或不符合要求的数据问题。我们详细分析了LLM生成的测试套件在参考接地和值准确性方面的表现。\n结论：我们的方法整合了外部医学知识，以在数据分析工作流程中实现上下文敏感的数据质量测试，从而提高其结果的有效性。我们的方法从质量保证的角度解决了这些挑战，为未来的发展奠定了基础，例如额外的数据模态和改进的接地方法。", "summary": "本论文提出了“医疗数据啄取”方法，这是一种用于结构化医疗数据自动化质量评估的上下文感知方法。为解决EHR数据在研究中可靠性不足的问题，该方法借鉴了软件工程中的单元测试和覆盖等概念。随附的MDPT工具利用大型语言模型从数据和研究描述中生成测试套件，并执行这些测试以识别数据质量问题。在AoU和MIMIC-III等数据集上的评估表明，该方法在检测不一致或不符合要求的数据问题方面是有效的，展示了其提高研究数据有效性的潜力。", "keywords": "医疗数据质量, EHR, 自动化测试, 大型语言模型, 上下文感知", "comments": "该论文的创新之处在于将软件工程的单元测试和覆盖概念应用于医疗数据质量保证，并利用大型语言模型实现自动化测试生成。这对于解决EHR数据质量差阻碍可靠流行病学研究和AI训练的关键问题具有重要意义。尽管该方法为未来的发展奠定了基础，例如支持更多数据模态和改进接地方法，但抽象中并未明确提及当前的具体局限性。"}}
{"id": "2507.02479", "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios", "authors": ["Teng Fu", "Yuwen Chen", "Zhuofan Chen", "Mengyang Zhao", "Bin Li", "Xiangyang Xue"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02479v1", "summary": "Multi-object tracking is a classic field in computer vision. Among them,\npedestrian tracking has extremely high application value and has become the\nmost popular research category. Existing methods mainly use motion or\nappearance information for tracking, which is often difficult in complex\nscenarios. For the motion information, mutual occlusions between objects often\nprevent updating of the motion state; for the appearance information,\nnon-robust results are often obtained due to reasons such as only partial\nvisibility of the object or blurred images. Although learning how to perform\ntracking in these situations from the annotated data is the simplest solution,\nthe existing MOT dataset fails to satisfy this solution. Existing methods\nmainly have two drawbacks: relatively simple scene composition and\nnon-realistic scenarios. Although some of the video sequences in existing\ndataset do not have the above-mentioned drawbacks, the number is far from\nadequate for research purposes. To this end, we propose a difficult large-scale\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\nthere are numerous objects in most of the sequences. Our dataset consists of 33\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\na complete bounding box and a unique object ID. The dataset will provide a\nplatform to facilitate the development of algorithms that remain effective in\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\nSOTA models on our dataset. Besides, we analyzed the performance of the\nfoundation models on our dataset. The dataset and project code is released at:\nhttps://github.com/loseevaya/CrowdTrack .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02479v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CrowdTrack：一个用于真实场景中困难多行人跟踪的基准", "tldr": "提出并发布了一个名为CrowdTrack的大规模困难多行人跟踪数据集，以应对现有数据集在复杂真实场景下不足的问题。", "motivation": "现有行人跟踪方法在复杂场景下难以有效，主要原因在于对象相互遮挡导致运动状态难以更新，以及部分可见或图像模糊导致外观信息不鲁棒。尽管从标注数据中学习是简单解决方案，但现有MOT数据集存在场景构成过于简单和非真实场景的缺点，且高质量数据量不足以支持研究。", "method": "作者提出了一个名为“CrowdTrack”的困难大规模多行人跟踪数据集。该数据集主要从第一人称视角拍摄，全部来自真实复杂的场景，包含33个视频和5,185条轨迹，每个对象都标注有完整的边界框和唯一的对象ID。作者还对数据集进行了全面分析，并测试了多个SOTA模型和基础模型。", "result": "CrowdTrack数据集包含33个视频和5,185条轨迹，主要从第一人称视角拍摄，全部来自真实复杂的场景，且每个对象都标注有完整的边界框和唯一的对象ID。该数据集将为开发在复杂情况下仍然有效的算法提供平台。作者对数据集进行了全面分析，并测试了多个SOTA模型和基础模型。", "conclusion": "CrowdTrack数据集的发布将为多行人跟踪领域提供一个在复杂真实场景下开发和评估算法的基准平台，有望推动该领域在实际应用中的进展。", "translation": "多目标跟踪是计算机视觉领域的一个经典方向。其中，行人跟踪具有极高的应用价值，并已成为最热门的研究类别。现有方法主要利用运动或外观信息进行跟踪，这在复杂场景中往往很困难。对于运动信息，对象之间的相互遮挡常常阻碍运动状态的更新；对于外观信息，由于对象部分可见或图像模糊等原因，常常获得不稳健的结果。尽管从标注数据中学习如何在这些情况下进行跟踪是最简单的解决方案，但现有的MOT数据集未能满足这一需求。现有方法主要有两个缺点：场景构成相对简单和非真实场景。尽管现有数据集中部分视频序列没有上述缺点，但其数量远不足以满足研究目的。为此，我们提出了一个用于多行人跟踪的困难大规模数据集，主要从第一人称视角拍摄，且全部来自真实复杂的场景。我们将其命名为“CrowdTrack”，因为大多数序列中都有大量对象。我们的数据集包含33个视频，共5,185条轨迹。每个对象都标注有完整的边界框和唯一的对象ID。该数据集将为促进在复杂情况下仍能有效运行的算法的开发提供一个平台。我们全面分析了该数据集，并在我们的数据集上测试了多个SOTA模型。此外，我们还分析了基础模型在我们数据集上的性能。数据集和项目代码已发布在：https://github.com/loseevaya/CrowdTrack。", "summary": "这篇论文介绍了CrowdTrack，一个专为解决复杂真实场景下多行人跟踪挑战而设计的大规模基准数据集。针对现有数据集在场景复杂度、真实性和数据量上的不足，CrowdTrack提供了33个来自第一人称视角真实复杂场景的视频，包含5,185条带有完整边界框和唯一ID的行人轨迹。该数据集旨在为开发在遮挡、模糊等困难条件下仍能有效工作的跟踪算法提供一个平台，并已用于测试SOTA模型和基础模型的性能。", "keywords": "多行人跟踪, 数据集, 计算机视觉, 基准, 复杂场景", "comments": "CrowdTrack的创新之处在于其专注于真实世界中多行人跟踪的“困难”场景，特别是通过第一人称视角捕获大量遮挡和模糊情况，弥补了现有数据集的不足。其重要性在于为研究人员提供了一个更具挑战性和真实性的基准，有望推动多目标跟踪算法在实际复杂环境中的鲁棒性提升。目前看来，论文主要贡献是数据集，尚未展示基于此数据集开发的新算法的突破性成果，但其作为基准的价值不容忽视。"}}
{"id": "2507.02634", "title": "High-Order Deep Meta-Learning with Category-Theoretic Interpretation", "authors": ["David H. Mguni"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02634v1", "summary": "We introduce a new hierarchical deep learning framework for recursive\nhigher-order meta-learning that enables neural networks (NNs) to construct,\nsolve, and generalise across hierarchies of tasks. Central to this approach is\na generative mechanism that creates \\emph{virtual tasks} -- synthetic problem\ninstances designed to enable the meta-learner to learn \\emph{soft constraints}\nand unknown generalisable rules across related tasks. Crucially, this enables\nthe framework to generate its own informative, task-grounded datasets thereby\nfreeing machine learning (ML) training from the limitations of relying entirely\non human-generated data. By actively exploring the virtual point landscape and\nseeking out tasks lower-level learners find difficult, the meta-learner\niteratively refines constraint regions. This enhances inductive biases,\nregularises the adaptation process, and produces novel, unanticipated tasks and\nconstraints required for generalisation. Each meta-level of the hierarchy\ncorresponds to a progressively abstracted generalisation of problems solved at\nlower levels, enabling a structured and interpretable learning progression. By\ninterpreting meta-learners as category-theoretic \\emph{functors} that generate\nand condition a hierarchy of subordinate learners, we establish a compositional\nstructure that supports abstraction and knowledge transfer across progressively\ngeneralised tasks. The category-theoretic perspective unifies existing\nmeta-learning models and reveals how learning processes can be transformed and\ncompared through functorial relationships, while offering practical design\nprinciples for structuring meta-learning. We speculate this architecture may\nunderpin the next generation of NNs capable of autonomously generating novel,\ninstructive tasks and their solutions, thereby advancing ML towards general\nartificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02634v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高阶深度元学习与范畴论解释", "tldr": "提出一种高阶深度元学习框架，通过生成虚拟任务和范畴论解释，使神经网络能自主生成任务和数据，实现跨层级任务的构建、解决和泛化，迈向通用人工智能。", "motivation": "现有机器学习训练受限于人类生成的数据；需要神经网络能够构建、解决和泛化跨层级任务；需要提升归纳偏置和泛化能力；寻求实现通用人工智能。", "method": "引入一个递归高阶元学习的层次深度学习框架。核心是一个生成机制，创建“虚拟任务”来学习软约束和可泛化规则。元学习器主动探索虚拟点景观，寻找低级学习器难以解决的任务，迭代细化约束区域。通过将元学习器解释为范畴论中的“函子”，建立了一个支持抽象和知识迁移的组合结构，统一现有模型并提供设计原则。", "result": "框架能够生成自己的信息丰富、以任务为基础的数据集，摆脱对人类生成数据的依赖。增强了归纳偏置，规范了适应过程，并产生了泛化所需的新颖、意想不到的任务和约束。范畴论视角统一了现有元学习模型，揭示了学习过程如何通过函子关系进行转换和比较。", "conclusion": "该框架通过范畴论解释的高阶深度元学习，使得神经网络能够自主生成任务和解决方案，有望推动机器学习迈向通用人工智能。它统一了现有元学习模型，并为元学习的结构设计提供了实用原则。", "translation": "我们引入了一种新的层次深度学习框架，用于递归高阶元学习，使神经网络（NNs）能够构建、解决和泛化跨任务层级的问题。该方法的核心是一个生成机制，它创建“虚拟任务”——合成的问题实例，旨在使元学习器能够学习相关任务中的“软约束”和未知的可泛化规则。至关重要的是，这使得该框架能够生成自己的信息丰富、以任务为基础的数据集，从而将机器学习（ML）训练从完全依赖人类生成数据的限制中解放出来。通过主动探索虚拟点景观并寻找低级学习器认为困难的任务，元学习器迭代地细化约束区域。这增强了归纳偏置，规范了适应过程，并产生了泛化所需的新颖、意想不到的任务和约束。层次结构的每个元级别对应于对较低级别解决的问题的逐步抽象泛化，从而实现了结构化和可解释的学习进展。通过将元学习器解释为范畴论中的“函子”，它们生成并条件化一个从属学习器层次结构，我们建立了一个组合结构，支持跨逐步泛化任务的抽象和知识迁移。范畴论视角统一了现有元学习模型，并揭示了学习过程如何通过函子关系进行转换和比较，同时为元学习的结构化提供了实用的设计原则。我们推测这种架构可能支撑下一代神经网络，使其能够自主生成新颖、有指导性的任务及其解决方案，从而推动机器学习迈向通用人工智能。", "summary": "本文提出一种高阶深度元学习框架，通过引入生成“虚拟任务”的机制，使神经网络能够自主生成训练数据，摆脱对人工数据的依赖。该框架通过迭代细化约束区域，增强归纳偏置并生成新颖任务。通过将元学习器解释为范畴论中的函子，该方法建立了一个支持跨层级抽象和知识迁移的组合结构，统一了现有元学习模型，并为设计提供了原则，有望推动通用人工智能的发展。", "keywords": "元学习, 高阶学习, 范畴论, 虚拟任务, 通用人工智能", "comments": "这项研究的创新之处在于提出了一个结合高阶元学习和范畴论解释的深度学习框架。通过生成“虚拟任务”来训练元学习器，该框架能够自主生成数据并发现可泛化规则，显著减轻了对大规模人工标注数据的依赖。范畴论的引入为元学习提供了一个统一且结构化的理论基础，有助于理解和设计更通用的学习系统。其重要性在于为实现通用人工智能迈出了关键一步，特别是在自主任务生成和知识抽象方面。"}}
{"id": "2507.02790", "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding", "authors": ["Xiangfeng Wang", "Xiao Li", "Yadong Wei", "Xueyu Song", "Yang Song", "Xiaoqiang Xia", "Fangrui Zeng", "Zaiyi Chen", "Liu Liu", "Gu Xu", "Tong Xu"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02790v1", "summary": "The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02790v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "从长视频到引人入胜的短片：一个基于多模态叙事理解的人类启发式视频编辑框架", "tldr": "本文提出了一个名为HIVE的人类启发式自动视频编辑框架，利用多模态叙事理解将长视频浓缩成引人入胜的短片，并通过引入新的数据集和实验证明其优于现有方法，显著缩小了自动编辑与人工编辑之间的质量差距。", "motivation": "在线视频内容（尤其是短视频平台）的快速增长，对将长视频浓缩为简洁、引人入胜的短片的视频编辑技术产生了巨大需求。现有的自动编辑方法主要依赖ASR文本线索和端到端片段选择，常忽略丰富的视觉上下文，导致输出不连贯。", "method": "本文提出了一个人类启发式自动视频编辑框架（HIVE），利用多模态叙事理解来解决现有方法的局限性。该方法通过多模态大型语言模型整合了角色提取、对话分析和叙事总结，实现了对视频内容的整体理解。为增强连贯性，还应用了场景级分割，并将编辑过程分解为三个子任务：亮点检测、开头/结尾选择和不相关内容的修剪。此外，引入了一个名为DramaAD的新型基准数据集，包含800多个短剧集和500个专业编辑的广告片段。", "result": "实验结果表明，该框架在通用和广告导向的编辑任务中均持续优于现有基线，显著缩小了自动编辑与人工编辑视频之间的质量差距。", "conclusion": "本文提出的HIVE框架通过多模态叙事理解和精细的编辑流程，成功地提高了自动视频编辑的质量，使其更接近人类编辑的水平。", "translation": "在线视频内容，尤其是短视频平台的快速增长，对能够将长视频浓缩成简洁且引人入胜的短片的视频编辑技术产生了日益增长的需求。现有的自动编辑方法主要依赖于ASR转录文本线索和端到端的片段选择，常常忽视丰富的视觉上下文，导致输出不连贯。在本文中，我们提出了一个人类启发式自动视频编辑框架（HIVE），它利用多模态叙事理解来解决这些局限性。我们的方法通过多模态大型语言模型整合了角色提取、对话分析和叙事总结，从而实现了对视频内容的整体理解。为了进一步增强连贯性，我们应用了场景级分割，并将编辑过程分解为三个子任务：亮点检测、开头/结尾选择以及不相关内容的修剪。为了促进该领域的研究，我们引入了DramaAD，这是一个新颖的基准数据集，包含800多个短剧集和500个专业编辑的广告片段。实验结果表明，我们的框架在通用和广告导向的编辑任务中均持续优于现有基线，显著缩小了自动编辑与人工编辑视频之间的质量差距。", "summary": "本文针对长视频自动编辑中现有方法忽略视觉上下文导致不连贯的问题，提出了一个名为HIVE的人类启发式自动视频编辑框架。该框架通过多模态大型语言模型进行角色提取、对话分析和叙事总结，实现视频内容的整体理解。同时，通过场景级分割和将编辑分解为亮点检测、开头/结尾选择和内容修剪三个子任务来增强连贯性。为推动研究，论文还发布了新的基准数据集DramaAD。实验证明，HIVE在各项任务中均优于现有基线，显著缩小了自动编辑与人工编辑视频的质量差距。", "keywords": "视频编辑, 多模态理解, 叙事分析, 大型语言模型, 自动剪辑", "comments": "本文的创新点在于提出了一个人类启发式的多模态视频编辑框架HIVE，通过整合多模态大语言模型进行叙事理解，并细化编辑流程为多个子任务，有效解决了现有自动编辑方法中视觉上下文缺失导致不连贯的问题。此外，引入新的大规模基准数据集DramaAD，对于推动该领域的研究具有重要意义。该工作显著提升了自动视频编辑的质量，使其更接近专业人工编辑水平，具有重要的实际应用价值。"}}
{"id": "2507.02488", "title": "MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention", "authors": ["Zunhui Xia", "Hongxing Li", "Libin Lan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 9 figures, 9 tables", "url": "http://arxiv.org/abs/2507.02488v1", "summary": "Medical image recognition serves as a key way to aid in clinical diagnosis,\nenabling more accurate and timely identification of diseases and abnormalities.\nVision transformer-based approaches have proven effective in handling various\nmedical recognition tasks. However, these methods encounter two primary\nchallenges. First, they are often task-specific and architecture-tailored,\nlimiting their general applicability. Second, they usually either adopt full\nattention to model long-range dependencies, resulting in high computational\ncosts, or rely on handcrafted sparse attention, potentially leading to\nsuboptimal performance. To tackle these issues, we present MedFormer, an\nefficient medical vision transformer with two key ideas. First, it employs a\npyramid scaling structure as a versatile backbone for various medical image\nrecognition tasks, including image classification and dense prediction tasks\nsuch as semantic segmentation and lesion detection. This structure facilitates\nhierarchical feature representation while reducing the computation load of\nfeature maps, highly beneficial for boosting performance. Second, it introduces\na novel Dual Sparse Selection Attention (DSSA) with content awareness to\nimprove computational efficiency and robustness against noise while maintaining\nhigh performance. As the core building technique of MedFormer, DSSA is\nexplicitly designed to attend to the most relevant content. In addition, a\ndetailed theoretical analysis has been conducted, demonstrating that MedFormer\nhas superior generality and efficiency in comparison to existing medical vision\ntransformers. Extensive experiments on a variety of imaging modality datasets\nconsistently show that MedFormer is highly effective in enhancing performance\nacross all three above-mentioned medical image recognition tasks. The code is\navailable at https://github.com/XiaZunhui/MedFormer.", "comment": "13 pages, 9 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.02488v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MedFormer：一种具有内容感知双稀疏选择注意的分层医学视觉Transformer", "tldr": "MedFormer是一种高效的医学视觉Transformer，通过分层结构和内容感知双稀疏选择注意力解决了现有方法在通用性和计算成本上的问题，并在多种医学图像识别任务上表现出色。", "motivation": "现有医学视觉Transformer方法存在两个主要挑战：1) 它们通常是针对特定任务和架构定制的，限制了通用性；2) 它们要么采用全注意力导致高计算成本，要么依赖手工稀疏注意力导致性能不佳。", "method": "提出了MedFormer，它包含两个关键思想：1) 采用金字塔缩放结构作为通用骨干网络，适用于图像分类、语义分割和病灶检测等多种医学图像识别任务，同时降低计算负荷并提升性能。2) 引入了一种新颖的具有内容感知能力的双稀疏选择注意力（DSSA），以提高计算效率、鲁棒性并保持高性能，DSSA旨在关注最相关的内容。", "result": "理论分析表明MedFormer在通用性和效率上优于现有医学视觉Transformer。在多种成像模态数据集上的大量实验一致表明，MedFormer在所有三种提到的医学图像识别任务（图像分类、语义分割、病灶检测）中都非常有效地提升了性能。", "conclusion": "MedFormer通过其分层结构和内容感知双稀疏选择注意力，成功解决了现有医学视觉Transformer的通用性、计算效率和性能问题，并在多个医学图像识别任务中展现出卓越的性能。", "translation": "医学图像识别是辅助临床诊断的关键方式，能够更准确、及时地识别疾病和异常。基于视觉Transformer的方法已被证明在处理各种医学识别任务中是有效的。然而，这些方法面临两个主要挑战。首先，它们通常是针对特定任务和架构定制的，这限制了它们的通用性。其次，它们通常要么采用全注意力来建模长程依赖，导致高计算成本，要么依赖手工设计的稀疏注意力，可能导致次优性能。为了解决这些问题，我们提出了MedFormer，一种高效的医学视觉Transformer，它包含两个关键思想。首先，它采用金字塔缩放结构作为通用骨干网络，适用于各种医学图像识别任务，包括图像分类和语义分割、病灶检测等密集预测任务。这种结构有助于分层特征表示，同时降低特征图的计算负荷，这对于提升性能非常有益。其次，它引入了一种新颖的具有内容感知能力的双稀疏选择注意力（DSSA），以提高计算效率和对噪声的鲁棒性，同时保持高性能。作为MedFormer的核心构建技术，DSSA被明确设计为关注最相关的内容。此外，还进行了详细的理论分析，证明MedFormer与现有医学视觉Transformer相比具有卓越的通用性和效率。在各种成像模态数据集上进行的大量实验一致表明，MedFormer在上述所有三种医学图像识别任务中都非常有效地提升了性能。代码可在https://github.com/XiaZunhui/MedFormer获取。", "summary": "MedFormer是一种新型高效的医学视觉Transformer，旨在解决现有方法在通用性和计算效率方面的局限性。它结合了金字塔缩放结构作为通用骨干网络，以实现分层特征表示和降低计算负荷，并引入了内容感知双稀疏选择注意力（DSSA），以提高效率、鲁棒性和性能。实验证明，MedFormer在图像分类、语义分割和病灶检测等多种医学图像识别任务中均表现出卓越的通用性和性能提升。", "keywords": "医学图像识别, 视觉Transformer, MedFormer, 稀疏注意力, 分层结构", "comments": "MedFormer的创新点在于其结合了金字塔缩放结构和内容感知双稀疏选择注意力（DSSA），以解决医学视觉Transformer在通用性和计算效率方面的瓶颈。金字塔结构使其能够作为多任务的通用骨干，而DSSA则有效地降低了计算成本并增强了对相关内容的关注，这对于处理高分辨率医学图像和提高诊断效率具有重要意义。"}}
{"id": "2507.02639", "title": "On Efficient Bayesian Exploration in Model-Based Reinforcement Learning", "authors": ["Alberto Caron", "Chris Hicks", "Vasilios Mavroudis"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02639v1", "summary": "In this work, we address the challenge of data-efficient exploration in\nreinforcement learning by examining existing principled, information-theoretic\napproaches to intrinsic motivation. Specifically, we focus on a class of\nexploration bonuses that targets epistemic uncertainty rather than the\naleatoric noise inherent in the environment. We prove that these bonuses\nnaturally signal epistemic information gains and converge to zero once the\nagent becomes sufficiently certain about the environment's dynamics and\nrewards, thereby aligning exploration with genuine knowledge gaps. Our analysis\nprovides formal guarantees for IG-based approaches, which previously lacked\ntheoretical grounding. To enable practical use, we also discuss tractable\napproximations via sparse variational Gaussian Processes, Deep Kernels and Deep\nEnsemble models. We then outline a general framework - Predictive Trajectory\nSampling with Bayesian Exploration (PTS-BE) - which integrates model-based\nplanning with information-theoretic bonuses to achieve sample-efficient deep\nexploration. We empirically demonstrate that PTS-BE substantially outperforms\nother baselines across a variety of environments characterized by sparse\nrewards and/or purely exploratory tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02639v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "关于模型基强化学习中高效贝叶斯探索", "tldr": "本文通过检查信息论方法，解决了强化学习中数据高效探索的挑战，特别是关注认知不确定性奖励。研究证明了这些奖励的理论有效性，并提出了PTS-BE框架，该框架在稀疏奖励和探索性任务中表现出色。", "motivation": "解决强化学习中数据高效探索的挑战，特别是通过信息论方法改进内在动机，并为缺乏理论基础的信息增益（IG）方法提供形式保证。", "method": "研究了针对认知不确定性的探索奖励，并证明其能信号信息增益并收敛。为基于信息增益（IG）的方法提供了理论保证。讨论了通过稀疏变分高斯过程、深度核和深度集成模型进行的可行近似。提出了预测轨迹采样与贝叶斯探索（PTS-BE）通用框架，将模型基规划与信息论奖励结合。", "result": "证明了探索奖励自然地指示认知信息增益，并在智能体足够确定后收敛到零。为基于信息增益（IG）的方法提供了此前缺乏的理论基础。PTS-BE在各种稀疏奖励和/或纯探索性任务环境中显著优于其他基线。", "conclusion": "本文通过提供信息增益奖励的理论基础和引入PTS-BE框架，有效解决了强化学习中高效贝士探索的问题，并在实践中展示了优越性能。", "translation": "在这项工作中，我们通过检查现有的原则性、信息论的内在动机方法，解决了强化学习中数据高效探索的挑战。具体来说，我们专注于一类针对认知不确定性而非环境固有的随机噪声的探索奖励。我们证明了这些奖励自然地指示认知信息增益，并在智能体对环境动态和奖励足够确定后收敛到零，从而使探索与真正的知识差距对齐。我们的分析为基于信息增益（IG）的方法提供了形式保证，这些方法此前缺乏理论基础。为了实现实际应用，我们还讨论了通过稀疏变分高斯过程、深度核和深度集成模型进行的可行近似。然后，我们概述了一个通用框架——预测轨迹采样与贝叶斯探索（PTS-BE）——它将基于模型的规划与信息论奖励相结合，以实现样本高效的深度探索。我们凭经验证明，PTS-BE在各种以稀疏奖励和/或纯探索性任务为特征的环境中，显著优于其他基线。", "summary": "本文旨在解决强化学习中数据高效探索的难题，通过深入分析针对认知不确定性的信息论探索奖励。研究证明了这些奖励能有效指示知识增益并最终收敛，为此前缺乏理论基础的信息增益方法提供了正式保证。为实现实用性，文章提出了通过多种近似方法的可行实现，并引入了预测轨迹采样与贝叶斯探索（PTS-BE）框架。实验结果表明，PTS-BE在稀疏奖励和探索任务中表现出显著优越性。", "keywords": "贝叶斯探索, 强化学习, 信息增益, 认知不确定性, PTS-BE", "comments": "创新性：本文为基于信息增益的探索方法提供了严格的理论基础，填补了该领域的一个空白。提出的PTS-BE框架结合了模型基规划和信息论奖励，提供了一种新的高效探索范式。重要性：解决了强化学习中数据效率和探索的根本问题，这对于真实世界应用至关重要，特别是那些数据获取成本高或奖励稀疏的场景。局限性：抽象中未提及具体的计算复杂性或在极高维环境中的扩展性挑战，这些可能是未来研究需要考虑的方面。"}}
{"id": "2507.02834", "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning", "authors": ["Ruiyang Zhou", "Shuozhe Li", "Amy Zhang", "Liu Leqi"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02834v1", "summary": "Recent advances in large language models have been driven by reinforcement\nlearning (RL)-style post-training, which improves reasoning by optimizing model\noutputs based on reward or preference signals. GRPO-style approaches implement\nthis by using self-generated samples labeled by an outcome-based verifier.\nHowever, these methods depend heavily on the model's initial ability to produce\npositive samples. They primarily refine what the model already knows\n(distribution sharpening) rather than enabling the model to solve problems\nwhere it initially fails. This limitation is especially problematic in\nearly-stage RL training and on challenging reasoning tasks, where positive\nsamples are unlikely to be generated. To unlock reasoning ability in such\nsettings, the model must explore new reasoning trajectories beyond its current\noutput distribution. Such exploration requires access to sufficiently good\npositive samples to guide the learning. While expert demonstrations seem like a\nnatural solution, we find that they are often ineffective in RL post-training.\nInstead, we identify two key properties of effective positive samples: they\nshould (1) be likely under the current policy, and (2) increase the model's\nlikelihood of predicting the correct answer. Based on these insights, we\npropose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and\nmodular framework that generates such samples by conditioning on the\nground-truth answer. ExPO enables efficient exploration and guides the model to\nproduce reasoning trajectories more aligned with its policy than expert-written\nCoTs, while ensuring higher quality than its own (incorrect) samples.\nExperiments show that ExPO improves both learning efficiency and final\nperformance on reasoning benchmarks, surpassing expert-demonstration-based\nmethods in challenging settings such as MATH level-5, where the model initially\nstruggles the most.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02834v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ExPO：使用自解释引导的强化学习解锁困难推理", "tldr": "ExPO通过生成高质量的自解释样本，解决了现有RL方法在困难推理任务中探索不足的问题，显著提升了模型学习效率和性能。", "motivation": "现有强化学习（RL）后训练方法（如GRPO）严重依赖模型生成正面样本的初始能力，主要优化模型已知内容而非解决模型初始失败的问题。在早期RL训练和挑战性推理任务中，正面样本难以生成，导致探索不足。此外，专家演示在RL后训练中往往无效。", "method": "本文提出了自解释策略优化（ExPO）框架，通过识别并生成具有两个关键特性的有效正面样本：(1) 在当前策略下可能性高；(2) 增加模型预测正确答案的可能性。ExPO通过以真实答案为条件来生成这些样本，使模型能够探索新的推理轨迹，其生成的推理轨迹比专家编写的CoT更符合其策略，同时质量高于其自身（不正确）的样本。", "result": "实验表明，ExPO提高了推理基准测试的学习效率和最终性能。在MATH level-5等模型初始表现不佳的挑战性设置中，ExPO超越了基于专家演示的方法。", "conclusion": "ExPO通过生成高质量的自解释样本，有效克服了现有强化学习方法在困难推理任务中探索不足的局限性，显著提升了大型语言模型的学习效率和最终性能。", "translation": "大型语言模型（LLM）的最新进展得益于强化学习（RL）风格的后训练，通过基于奖励或偏好信号优化模型输出，从而提高推理能力。GRPO风格的方法通过使用由基于结果的验证器标记的自生成样本来实现这一点。然而，这些方法严重依赖模型生成正面样本的初始能力。它们主要优化模型已知的内容（分布锐化），而不是使模型能够解决其最初失败的问题。这种局限性在早期RL训练和挑战性推理任务中尤其突出，因为正面样本不太可能被生成。为了在这些设置中解锁推理能力，模型必须探索超出其当前输出分布的新推理轨迹。这种探索需要获得足够好的正面样本来指导学习。虽然专家演示看似是一个自然的解决方案，但我们发现它们在RL后训练中通常无效。相反，我们确定了有效正面样本的两个关键属性：它们应该（1）在当前策略下具有高可能性，并且（2）增加模型预测正确答案的可能性。基于这些见解，我们提出了自解释策略优化（ExPO）——一个简单且模块化的框架，通过以真实答案为条件来生成此类样本。ExPO能够实现高效探索，并引导模型生成比专家编写的CoT更符合其策略的推理轨迹，同时确保比其自身的（不正确）样本具有更高的质量。实验表明，ExPO提高了推理基准测试的学习效率和最终性能，在MATH level-5等模型最初表现最差的挑战性设置中，超越了基于专家演示的方法。", "summary": "本文提出了自解释策略优化（ExPO）框架，旨在解决大型语言模型在困难推理任务中，现有强化学习方法因缺乏高质量正面样本而导致的探索不足问题。ExPO通过以真实答案为条件生成满足特定条件的自解释样本，这些样本既符合当前策略又提升正确预测的可能性。实验证明，ExPO显著提高了模型在推理基准测试上的学习效率和最终性能，尤其是在MATH level-5等模型初始表现较差的挑战性任务中，其表现优于依赖专家演示的方法。", "keywords": "强化学习, 自解释, 推理, 大型语言模型, 策略优化", "comments": "ExPO的创新点在于其样本生成机制，通过以真实答案为条件来创建高质量的自解释样本，有效解决了在困难任务中缺乏正面引导的问题。这对于提升大型语言模型在复杂推理场景下的泛化能力和学习效率具有重要意义。它超越了传统依赖专家演示的方法，为强化学习在大型语言模型训练中的应用提供了新的思路。"}}
{"id": "2507.02493", "title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy", "authors": ["Luca Parolari", "Andrea Cherubini", "Lamberto Ballan", "Carlo Biffi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI 2025", "url": "http://arxiv.org/abs/2507.02493v1", "summary": "Automated polyp counting in colonoscopy is a crucial step toward automated\nprocedure reporting and quality control, aiming to enhance the\ncost-effectiveness of colonoscopy screening. Counting polyps in a procedure\ninvolves detecting and tracking polyps, and then clustering tracklets that\nbelong to the same polyp entity. Existing methods for polyp counting rely on\nself-supervised learning and primarily leverage visual appearance, neglecting\ntemporal relationships in both tracklet feature learning and clustering stages.\nIn this work, we introduce a paradigm shift by proposing a supervised\ncontrastive loss that incorporates temporally-aware soft targets. Our approach\ncaptures intra-polyp variability while preserving inter-polyp discriminability,\nleading to more robust clustering. Additionally, we improve tracklet clustering\nby integrating a temporal adjacency constraint, reducing false positive\nre-associations between visually similar but temporally distant tracklets. We\ntrain and validate our method on publicly available datasets and evaluate its\nperformance with a leave-one-out cross-validation strategy. Results demonstrate\na 2.2x reduction in fragmentation rate compared to prior approaches. Our\nresults highlight the importance of temporal awareness in polyp counting,\nestablishing a new state-of-the-art. Code is available at\nhttps://github.com/lparolari/temporally-aware-polyp-counting.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.02493v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "结肠镜检查中息肉计数的时序感知监督对比学习", "tldr": "本文提出了一种时序感知监督对比学习方法，用于结肠镜检查中的息肉计数，通过引入时序感知软目标和时间邻近约束，显著降低了碎片化率，并建立了新的技术水平。", "motivation": "结肠镜检查中自动息肉计数是实现自动化程序报告和质量控制的关键步骤，旨在提高结肠镜筛查的成本效益。现有息肉计数方法主要依赖自监督学习并侧重于视觉外观，忽略了轨迹特征学习和聚类阶段的时序关系。", "method": "本研究提出了一种范式转变，引入了包含时序感知软目标的监督对比损失。该方法捕获息肉内部变异性，同时保持息肉间可区分性，从而实现更鲁棒的聚类。此外，通过整合时间邻近约束，减少了视觉相似但时间上相距较远的轨迹之间的错误关联，改进了轨迹聚类。", "result": "与现有方法相比，碎片化率降低了2.2倍。结果强调了时序感知在息肉计数中的重要性，并建立了新的技术水平。", "conclusion": "本研究强调了时序感知在息肉计数中的重要性，通过提出时序感知监督对比学习方法，显著提高了息肉计数性能，达到了新的技术水平。", "translation": "结肠镜检查中自动息肉计数是实现自动化程序报告和质量控制的关键步骤，旨在提高结肠镜筛查的成本效益。在一次检查中计数息肉涉及检测和跟踪息肉，然后将属于同一息肉实体的轨迹片段进行聚类。现有息肉计数方法依赖于自监督学习，主要利用视觉外观，却忽视了轨迹特征学习和聚类阶段的时序关系。在这项工作中，我们通过提出一种包含时序感知软目标的监督对比损失，引入了范式转变。我们的方法捕获了息肉内部的变异性，同时保留了息肉之间的可区分性，从而实现了更鲁棒的聚类。此外，我们通过整合时间邻近约束来改进轨迹片段聚类，减少了视觉相似但时间上相距较远的轨迹片段之间的错误重新关联。我们在公开可用数据集上训练和验证了我们的方法，并采用留一法交叉验证策略评估其性能。结果表明，与现有方法相比，碎片化率降低了2.2倍。我们的结果强调了时序感知在息肉计数中的重要性，建立了新的技术水平。代码可在 https://github.com/lparolari/temporally-aware-polyp-counting 获取。", "summary": "本文提出了一种用于结肠镜息肉计数的时序感知监督对比学习方法，旨在解决现有方法忽略时序关系的问题。通过引入时序感知软目标和时间邻近约束，该方法增强了息肉轨迹的聚类鲁棒性，有效区分了相似但不同时序的息肉轨迹。实验结果表明，该方法将碎片化率降低了2.2倍，证明了时序感知在息肉计数中的关键作用，并达到了新的技术水平。", "keywords": "息肉计数, 结肠镜检查, 监督对比学习, 时序感知, 轨迹聚类", "comments": "该论文通过引入时序感知监督对比学习，为结肠镜息肉计数领域带来了创新。其核心贡献在于将时间信息融入到特征学习和聚类过程中，有效解决了现有方法中忽视时间关系导致的问题。2.2倍的碎片化率降低是显著的改进，表明了其方法的有效性和实用价值，对于提高结肠镜筛查的自动化和质量控制具有重要意义。该研究为未来在医疗图像分析中融合时序信息提供了新的思路。"}}
{"id": "2507.02645", "title": "Fair Deepfake Detectors Can Generalize", "authors": ["Harry Cheng", "Ming-Hui Liu", "Yangyang Guo", "Tianyi Wang", "Liqiang Nie", "Mohan Kankanhalli"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, version 1", "url": "http://arxiv.org/abs/2507.02645v1", "summary": "Deepfake detection models face two critical challenges: generalization to\nunseen manipulations and demographic fairness among population groups. However,\nexisting approaches often demonstrate that these two objectives are inherently\nconflicting, revealing a trade-off between them. In this paper, we, for the\nfirst time, uncover and formally define a causal relationship between fairness\nand generalization. Building on the back-door adjustment, we show that\ncontrolling for confounders (data distribution and model capacity) enables\nimproved generalization via fairness interventions. Motivated by this insight,\nwe propose Demographic Attribute-insensitive Intervention Detection (DAID), a\nplug-and-play framework composed of: i) Demographic-aware data rebalancing,\nwhich employs inverse-propensity weighting and subgroup-wise feature\nnormalization to neutralize distributional biases; and ii) Demographic-agnostic\nfeature aggregation, which uses a novel alignment loss to suppress\nsensitive-attribute signals. Across three cross-domain benchmarks, DAID\nconsistently achieves superior performance in both fairness and generalization\ncompared to several state-of-the-art detectors, validating both its theoretical\nfoundation and practical effectiveness.", "comment": "14 pages, version 1", "pdf_url": "http://arxiv.org/pdf/2507.02645v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "公平的深度伪造检测器可以泛化", "tldr": "本研究首次揭示并形式化定义了公平性与泛化能力之间的因果关系，并提出DAID框架，通过数据再平衡和特征聚合，在公平性和泛化性方面均超越现有最先进的深度伪造检测器。", "motivation": "深度伪造检测模型面临两大挑战：对未知操纵的泛化能力和不同人群间的公平性。现有方法常认为这两个目标相互冲突，存在权衡。本研究旨在解决这一冲突，证明公平性可以促进泛化。", "method": "本研究首次揭示并形式化定义了公平性与泛化能力之间的因果关系，基于后门调整（back-door adjustment）证明控制混杂因素（数据分布和模型容量）可通过公平性干预来改善泛化。受此启发，提出了一种即插即用的框架DAID（Demographic Attribute-insensitive Intervention Detection），其包含：i) 人口统计学感知数据再平衡，利用逆倾向加权和子组特征归一化来中和分布偏差；ii) 人口统计学无关特征聚合，使用新颖的对齐损失来抑制敏感属性信号。", "result": "在三个跨领域基准测试中，DAID在公平性和泛化能力方面均持续优于几种最先进的检测器。", "conclusion": "DAID不仅验证了其理论基础，也证明了其在实践中的有效性，表明公平性干预可以促进深度伪造检测器的泛化能力。", "translation": "深度伪造检测模型面临两个关键挑战：对未见操纵的泛化能力和不同人群间的公平性。然而，现有方法通常表明这两个目标本质上是冲突的，揭示了它们之间的权衡。在本文中，我们首次揭示并形式化定义了公平性与泛化能力之间的因果关系。基于后门调整，我们表明控制混杂因素（数据分布和模型容量）可以通过公平性干预实现泛化能力的提升。受此启发，我们提出了人口统计学属性不敏感干预检测（DAID），一个即插即用的框架，其组成部分包括：i) 人口统计学感知数据再平衡，它采用逆倾向加权和子组特征归一化来中和分布偏差；ii) 人口统计学无关特征聚合，它使用一种新颖的对齐损失来抑制敏感属性信号。在三个跨领域基准测试中，DAID在公平性和泛化能力方面均持续优于几种最先进的检测器，验证了其理论基础和实际有效性。", "summary": "本文首次深入探讨了深度伪造检测中公平性与泛化能力之间的关系，并首次从因果角度证明两者并非冲突，公平性实际上能促进泛化。研究提出了一种名为DAID的即插即用框架，通过人口统计学感知的数据再平衡和人口统计学无关的特征聚合来解决现有模型的局限性。实验结果表明，DAID在多个跨领域基准测试中，在公平性和泛化能力上均显著优于现有最先进的检测器，验证了其理论和实践效果。", "keywords": "深度伪造检测, 公平性, 泛化, 因果关系, DAID", "comments": "本文的创新之处在于首次从因果关系角度揭示并形式化定义了公平性与泛化能力之间的正向关联，挑战了传统认为两者相互冲突的观点。提出的DAID框架通过独特的数据和特征处理机制，有效提升了深度伪造检测器的公平性和泛化能力，为该领域提供了新的研究方向和实用的解决方案。其即插即用的特性也增加了其实用价值。"}}
{"id": "2507.02106", "title": "Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework", "authors": ["Semih Kacmaz", "E. A. Huerta", "Roland Haas"], "categories": ["physics.flu-dyn", "cs.AI", "cs.LG", "gr-qc", "physics.comp-ph", "J.2; I.2"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02106v1", "summary": "We present a hybrid machine learning framework that combines Physics-Informed\nNeural Operators (PINOs) with score-based generative diffusion models to\nsimulate the full spatio-temporal evolution of two-dimensional, incompressible,\nresistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds\nnumbers ($\\mathrm{Re}$). The framework leverages the equation-constrained\ngeneralization capabilities of PINOs to predict coherent, low-frequency\ndynamics, while a conditional diffusion model stochastically corrects\nhigh-frequency residuals, enabling accurate modeling of fully developed\nturbulence. Trained on a comprehensive ensemble of high-fidelity simulations\nwith $\\mathrm{Re} \\in \\{100, 250, 500, 750, 1000, 3000, 10000\\}$, the approach\nachieves state-of-the-art accuracy in regimes previously inaccessible to\ndeterministic surrogates. At $\\mathrm{Re}=1000$ and $3000$, the model\nfaithfully reconstructs the full spectral energy distributions of both velocity\nand magnetic fields late into the simulation, capturing non-Gaussian\nstatistics, intermittent structures, and cross-field correlations with high\nfidelity. At extreme turbulence levels ($\\mathrm{Re}=10000$), it remains the\nfirst surrogate capable of recovering the high-wavenumber evolution of the\nmagnetic field, preserving large-scale morphology and enabling statistically\nmeaningful predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02106v1", "cate": "physics.flu-dyn", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "解析湍流磁流体动力学：一种混合算子-扩散框架", "tldr": "该论文提出了一种结合物理信息神经网络算子（PINOs）和扩散模型的混合机器学习框架，用于模拟二维磁流体动力学（MHD）湍流。该框架在广泛的雷诺数范围内实现了最先进的精度，尤其是在传统确定性替代模型无法企及的高雷诺数（如Re=10000）下，首次成功恢复了磁场的高波数演化。", "motivation": "该论文旨在模拟二维、不可压缩、有电阻的磁流体动力学（MHD）湍流的完整时空演化，覆盖广泛的雷诺数范围，特别是解决传统确定性替代模型此前无法触及的高雷诺数区域的准确建模问题。", "method": "本研究提出了一种混合机器学习框架，该框架结合了物理信息神经网络算子（PINOs）和基于分数的生成扩散模型。PINOs负责预测连贯的低频动力学，而条件扩散模型则随机校正高频残差，从而实现对完全发展湍流的精确建模。该框架通过对雷诺数（Re）在{100, 250, 500, 750, 1000, 3000, 10000}范围内的高保真模拟的综合集合进行训练。", "result": "该方法在传统确定性替代模型此前无法企及的雷诺数区间达到了最先进的精度。在Re=1000和3000时，模型忠实地重构了模拟后期速度场和磁场的完整谱能量分布，高保真地捕捉了非高斯统计特性、间歇性结构和交叉场关联。在极端湍流水平（Re=10000）下，它是首个能够恢复磁场高波数演化、保持大尺度形态并实现统计意义上有效预测的替代模型。", "conclusion": "该混合框架成功地在广泛的雷诺数范围内，包括极端湍流水平下，对磁流体动力学（MHD）湍流进行了建模，克服了以往确定性替代模型的局限性，展示了其在复杂物理系统模拟方面的卓越能力。", "translation": "我们提出了一种混合机器学习框架，该框架结合了物理信息神经网络算子（PINOs）和基于分数的生成扩散模型，以模拟二维、不可压缩、有电阻的磁流体动力学（MHD）湍流在广泛雷诺数（Re）范围内的完整时空演化。该框架利用PINOs受方程约束的泛化能力来预测连贯的低频动力学，而条件扩散模型则随机校正高频残差，从而实现对完全发展湍流的精确建模。该方法通过对Re在{100, 250, 500, 750, 1000, 3000, 10000}范围内的高保真模拟的综合集合进行训练，在传统确定性替代模型此前无法企及的雷诺数区间达到了最先进的精度。在Re=1000和3000时，模型忠实地重构了模拟后期速度场和磁场的完整谱能量分布，高保真地捕捉了非高斯统计特性、间歇性结构和交叉场关联。在极端湍流水平（Re=10000）下，它是首个能够恢复磁场高波数演化、保持大尺度形态并实现统计意义上有效预测的替代模型。", "summary": "该论文提出了一种新颖的混合机器学习框架，该框架将物理信息神经网络算子（PINOs）与基于分数的生成扩散模型相结合，用于模拟二维不可压缩电阻磁流体动力学（MHD）湍流。该框架利用PINOs处理低频动力学，并使用扩散模型对高频残差进行随机校正。该模型在广泛的雷诺数（100-10000）数据集上进行训练，实现了最先进的精度，能够准确重构高雷诺数下的谱能量分布、非高斯统计特性和场间关联，并且首次成功模拟了极端雷诺数（Re=10000）下磁场的高波数演化。", "keywords": "湍流磁流体动力学, 物理信息神经网络算子, 扩散模型, 雷诺数, 机器学习", "comments": "该论文提出了一种创新性的混合方法，结合了物理信息神经网络算子和生成扩散模型，这对于模拟磁流体动力学等复杂湍流系统是一个重大进步。其处理高雷诺数（尤其是在传统确定性替代模型失效的Re=10000）的能力，凸显了其在新颖性和在科学计算及流体动力学领域的潜在影响。"}}
{"id": "2507.02494", "title": "MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations", "authors": ["Hyunsoo Son", "Jeonghyun Noh", "Suemin Jeon", "Chaoli Wang", "Won-Ki Jeong"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.02494v1", "summary": "Implicit Neural Representations (INRs) are widely used to encode data as\ncontinuous functions, enabling the visualization of large-scale multivariate\nscientific simulation data with reduced memory usage. However, existing\nINR-based methods face three main limitations: (1) inflexible representation of\ncomplex structures, (2) primarily focusing on single-variable data, and (3)\ndependence on structured grids. Thus, their performance degrades when applied\nto complex real-world datasets. To address these limitations, we propose a\nnovel neural network-based framework, MC-INR, which handles multivariate data\non unstructured grids. It combines meta-learning and clustering to enable\nflexible encoding of complex structures. To further improve performance, we\nintroduce a residual-based dynamic re-clustering mechanism that adaptively\npartitions clusters based on local error. We also propose a branched layer to\nleverage multivariate data through independent branches simultaneously.\nExperimental results demonstrate that MC-INR outperforms existing methods on\nscientific data encoding tasks.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.02494v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MC-INR：使用元学习和聚类隐式神经表示对多元科学模拟数据进行高效编码", "tldr": "MC-INR 是一种新的框架，结合元学习和聚类，用于高效编码非结构化网格上的多元科学模拟数据，并通过残差动态再聚类和分支层提高性能，优于现有方法。", "motivation": "现有的基于隐式神经表示 (INRs) 的方法在编码复杂结构、处理单变量数据以及依赖结构化网格方面存在局限性，导致在复杂真实世界数据集上的性能下降。", "method": "提出了一种名为 MC-INR 的新型神经网络框架，它结合了元学习和聚类来灵活编码复杂结构，并处理非结构化网格上的多元数据。为进一步提升性能，引入了基于残差的动态再聚类机制，根据局部误差自适应地划分簇；同时提出了一种分支层，通过独立分支同时利用多元数据。", "result": "实验结果表明，MC-INR 在科学数据编码任务上优于现有方法。", "conclusion": "MC-INR 有效解决了现有 INR 方法在处理复杂多元科学数据时的局限性，提供了一种更高效、更灵活的编码方案。", "translation": "隐式神经表示（INRs）被广泛用于将数据编码为连续函数，从而以更少的内存使用实现大规模多元科学模拟数据的可视化。然而，现有的基于 INR 的方法面临三个主要限制：（1）复杂结构表示不灵活，（2）主要关注单变量数据，以及（3）依赖结构化网格。因此，当应用于复杂的真实世界数据集时，它们的性能会下降。为了解决这些限制，我们提出了一种新颖的基于神经网络的框架 MC-INR，它处理非结构化网格上的多元数据。它结合了元学习和聚类，以实现复杂结构的灵活编码。为了进一步提高性能，我们引入了一种基于残差的动态再聚类机制，该机制根据局部误差自适应地划分簇。我们还提出了一种分支层，通过独立分支同时利用多元数据。实验结果表明，MC-INR 在科学数据编码任务上优于现有方法。", "summary": "本文提出了一种名为 MC-INR 的新型神经网络框架，旨在高效编码非结构化网格上的多元科学模拟数据。针对现有隐式神经表示（INRs）在处理复杂结构、单变量数据和结构化网格方面的局限性，MC-INR 结合了元学习和聚类以实现灵活编码。此外，它引入了基于残差的动态再聚类机制和分支层，以自适应地优化聚类并同时利用多元数据。实验证明 MC-INR 在科学数据编码任务上表现优于现有方法。", "keywords": "隐式神经表示, 元学习, 聚类, 多元数据, 科学模拟数据编码", "comments": "这篇论文通过结合元学习、聚类、动态再聚类和分支层，创新性地解决了隐式神经表示在处理复杂多元非结构化科学数据时的局限性。其提出的 MC-INR 框架提高了数据编码的灵活性和效率，对于大规模科学数据可视化和分析具有重要意义。"}}
{"id": "2507.02513", "title": "Automatic Labelling for Low-Light Pedestrian Detection", "authors": ["Dimitrios Bouzoulas", "Eerik Alamikkotervo", "Risto Ojala"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02513v1", "summary": "Pedestrian detection in RGB images is a key task in pedestrian safety, as the\nmost common sensor in autonomous vehicles and advanced driver assistance\nsystems is the RGB camera. A challenge in RGB pedestrian detection, that does\nnot appear to have large public datasets, is low-light conditions. As a\nsolution, in this research, we propose an automated infrared-RGB labeling\npipeline. The proposed pipeline consists of 1) Infrared detection, where a\nfine-tuned model for infrared pedestrian detection is used 2) Label transfer\nprocess from the infrared detections to their RGB counterparts 3) Training\nobject detection models using the generated labels for low-light RGB pedestrian\ndetection. The research was performed using the KAIST dataset. For the\nevaluation, object detection models were trained on the generated autolabels\nand ground truth labels. When compared on a previously unseen image sequence,\nthe results showed that the models trained on generated labels outperformed the\nones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and\nmAP@50-95 metrics. The source code for this research is available at\nhttps://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02513v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "低光照行人检测的自动化标注", "tldr": "本研究提出了一种自动化的红外-RGB标注流程，用于解决低光照条件下RGB图像行人检测缺乏数据集的问题，并展示了其在性能上超越人工标注的潜力。", "motivation": "RGB图像中的行人检测是自动驾驶和高级驾驶辅助系统中的关键任务，但在低光照条件下，RGB行人检测面临缺乏大型公共数据集的挑战。", "method": "本研究提出了一种自动化的红外-RGB标注流程。该流程包括三个步骤：1) 红外检测，使用经过微调的红外行人检测模型；2) 标签从红外检测结果到对应RGB图像的迁移过程；3) 使用生成的标签训练目标检测模型，用于低光照RGB行人检测。研究使用了KAIST数据集。", "result": "在对未见过的图像序列进行评估时，结果显示，在mAP@50和mAP@50-95指标上，使用生成标签训练的模型在9个案例中有6个表现优于使用地面真值标签训练的模型。", "conclusion": "提出的自动化红外-RGB标注流程能够有效生成用于低光照RGB行人检测的数据集，并且使用这些自动生成的标签训练的模型在多数情况下表现优于使用传统地面真值标签训练的模型，显示了其在解决低光照行人检测数据稀缺问题上的潜力。", "translation": "RGB图像中的行人检测是行人安全中的一项关键任务，因为自动驾驶汽车和高级驾驶辅助系统中最常见的传感器是RGB相机。RGB行人检测面临的一个挑战是低光照条件，而目前似乎没有大型公共数据集来解决这个问题。作为解决方案，本研究提出了一种自动化的红外-RGB标注流程。该流程包括：1) 红外检测，使用经过微调的红外行人检测模型；2) 将红外检测结果的标签迁移到对应的RGB图像上；3) 使用生成的标签训练目标检测模型，用于低光照RGB行人检测。本研究使用KAIST数据集进行。为了评估，目标检测模型分别使用生成的自动标签和地面真值标签进行训练。在对先前未见的图像序列进行比较时，结果显示，在mAP@50和mAP@50-95指标上，使用生成标签训练的模型在9个案例中有6个表现优于使用地面真值标签训练的模型。本研究的源代码可在https://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling获取。", "summary": "本研究提出了一种新颖的自动化红外-RGB标注流程，旨在解决RGB图像在低光照条件下行人检测缺乏公共数据集的问题。该流程通过红外检测、标签迁移和模型训练三个阶段，为低光照RGB行人检测生成训练数据。实验结果表明，使用自动生成的标签训练的模型在多数评估指标上（mAP@50和mAP@50-95）表现优于使用传统地面真值标签训练的模型，验证了其在提升低光照行人检测性能方面的有效性。", "keywords": "低光照, 行人检测, 自动化标注, 红外-RGB, 计算机视觉", "comments": "这项研究的创新之处在于提出了一种自动化生成低光照行人检测数据集的方法，有效地解决了该领域数据稀缺的痛点。通过利用红外图像的鲁棒性来辅助RGB图像的标注，该方法为在恶劣光照条件下提升行人检测性能提供了新的思路。其结果显示自动标注甚至能超越人工标注，这对于降低数据标注成本和加速模型开发具有重要意义。"}}
{"id": "2507.02670", "title": "Guided Generation for Developable Antibodies", "authors": ["Siqi Zhao", "Joshua Moller", "Porfi Quintero-Cadena", "Lood van Niekerk"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in ICML 2025 GenBio Workshop", "url": "http://arxiv.org/abs/2507.02670v1", "summary": "Therapeutic antibodies require not only high-affinity target engagement, but\nalso favorable manufacturability, stability, and safety profiles for clinical\neffectiveness. These properties are collectively called `developability'. To\nenable a computational framework for optimizing antibody sequences for\nfavorable developability, we introduce a guided discrete diffusion model\ntrained on natural paired heavy- and light-chain sequences from the Observed\nAntibody Space (OAS) and quantitative developability measurements for 246\nclinical-stage antibodies. To steer generation toward biophysically viable\ncandidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module\nthat biases sampling without compromising naturalness. In unconstrained\nsampling, our model reproduces global features of both the natural repertoire\nand approved therapeutics, and under SVDD guidance we achieve significant\nenrichment in predicted developability scores over unguided baselines. When\ncombined with high-throughput developability assays, this framework enables an\niterative, ML-driven pipeline for designing antibodies that satisfy binding and\nbiophysical criteria in tandem.", "comment": "Published in ICML 2025 GenBio Workshop", "pdf_url": "http://arxiv.org/pdf/2507.02670v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可开发抗体的引导生成", "tldr": "引入了一种引导离散扩散模型，用于生成具有良好可开发性的治疗性抗体序列，并显著提高了可开发性得分。", "motivation": "治疗性抗体除了高亲和力结合靶点外，还需要良好的可生产性、稳定性和安全性（统称为“可开发性”）才能在临床上有效。因此需要一个计算框架来优化抗体序列以获得良好的可开发性。", "method": "引入了一个引导离散扩散模型，该模型在来自OAS的天然配对重链和轻链序列以及246个临床阶段抗体的定量可开发性测量数据上进行训练。为了使生成偏向于生物物理上可行的候选物，集成了软值解码扩散（SVDD）模块，该模块在不损害自然性的情况下偏置采样。", "result": "在无约束采样中，模型重现了天然库和已批准治疗药物的全局特征。在SVDD引导下，与无引导基线相比，预测的可开发性得分显著富集。", "conclusion": "该框架与高通量可开发性测定相结合，能够实现一个迭代的、机器学习驱动的管线，用于设计同时满足结合和生物物理标准的抗体。", "translation": "治疗性抗体不仅需要高亲和力的靶点结合，还需要良好的可生产性、稳定性和安全性，才能实现临床有效性。这些特性统称为“可开发性”。为了建立一个计算框架，用于优化抗体序列以获得良好的可开发性，我们引入了一个引导离散扩散模型，该模型在来自“观察抗体空间”（OAS）的天然配对重链和轻链序列以及246个临床阶段抗体的定量可开发性测量数据上进行训练。为了使生成偏向于生物物理上可行的候选物，我们集成了一个基于软值解码的扩散（SVDD）模块，该模块在不损害自然性的情况下偏置采样。在无约束采样中，我们的模型再现了天然抗体库和已批准治疗药物的全局特征，并且在SVDD引导下，与无引导基线相比，我们在预测的可开发性得分方面实现了显著富集。当与高通量可开发性测定相结合时，该框架能够实现一个迭代的、由机器学习驱动的管线，用于设计同时满足结合和生物物理标准的抗体。", "summary": "本文提出了一个计算框架，利用引导离散扩散模型和SVDD模块，旨在优化治疗性抗体的序列以提高其可开发性。该模型在天然抗体序列和临床抗体可开发性数据上训练，能够生成具有良好生物物理特性的抗体，并显著提高预测的可开发性得分，为药物设计提供了一个迭代的ML驱动管线。", "keywords": "抗体设计, 可开发性, 引导生成, 扩散模型, 机器学习", "comments": "这项研究创新性地将引导离散扩散模型应用于抗体设计，解决了传统抗体优化中可开发性难以兼顾的问题。SVDD模块的引入在保证序列自然性的同时引导生成，是其重要创新点。该框架有望加速高可开发性抗体的发现和优化过程，具有重要的实际应用价值。"}}
{"id": "2507.02519", "title": "IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning", "authors": ["Abiam Remache González", "Meriem Chagour", "Timon Bijan Rüth", "Raúl Trapiella Cañedo", "Marina Martínez Soler", "Álvaro Lorenzo Felipe", "Hyun-Suk Shin", "María-Jesús Zamorano Serrano", "Ricardo Torres", "Juan-Antonio Castillo Parra", "Eduardo Reyes Abad", "Miguel-Ángel Ferrer Ballester", "Juan-Manuel Afonso López", "Francisco-Mario Hernández Tejera", "Adrian Penate-Sanchez"], "categories": ["cs.CV", "I.2.10; I.4.8"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02519v1", "summary": "This paper introduces IMASHRIMP, an adapted system for the automated\nmorphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing\ngenetic selection tasks in aquaculture. Existing deep learning and computer\nvision techniques were modified to address the specific challenges of shrimp\nmorphology analysis from RGBD images. IMASHRIMP incorporates two discrimination\nmodules, based on a modified ResNet-50 architecture, to classify images by the\npoint of view and determine rostrum integrity. It is proposed a \"two-factor\nauthentication (human and IA)\" system, it reduces human error in view\nclassification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%.\nAdditionally, a pose estimation module was adapted from VitPose to predict 23\nkey points on the shrimp's skeleton, with separate networks for lateral and\ndorsal views. A morphological regression module, using a Support Vector Machine\n(SVM) model, was integrated to convert pixel measurements to centimeter units.\nExperimental results show that the system effectively reduces human error,\nachieving a mean average precision (mAP) of 97.94% for pose estimation and a\npixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP\ndemonstrates the potential to automate and accelerate shrimp morphological\nanalysis, enhancing the efficiency of genetic selection and contributing to\nmore sustainable aquaculture practices.The code are available at\nhttps://github.com/AbiamRemacheGonzalez/ImaShrimp-public", "comment": "14 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02519v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "IMASHRIMP：基于计算机视觉和深度学习的实验室图像白对虾（凡纳滨对虾）生物测量自动分析", "tldr": "IMASHRIMP是一个利用计算机视觉和深度学习技术，对白对虾（凡纳滨对虾）进行自动化形态学分析的系统，旨在优化水产养殖中的遗传选择任务。", "motivation": "为了优化水产养殖中的遗传选择任务，需要对白对虾进行自动化的形态学分析，以克服现有方法在虾形态分析中面临的挑战。", "method": "IMASHRIMP系统修改了现有的深度学习和计算机视觉技术，以处理RGBD图像的虾形态分析。它包含两个基于修改版ResNet-50架构的判别模块，用于图像视角分类和吻部完整性判断。系统提出了一个“双因素认证（人工和AI）”机制。此外，还适配了VitPose的姿态估计模块，用于预测虾骨骼上的23个关键点，并针对侧视图和背视图使用独立的网络。一个使用支持向量机（SVM）模型的形态回归模块被集成，用于将像素测量值转换为厘米单位。", "result": "实验结果表明，该系统有效降低了人为错误。在视角分类方面，人为错误从0.97%降至0%；在吻部检测方面，人为错误从12.46%降至3.64%。姿态估计的平均精度（mAP）达到97.94%，像素到厘米的转换误差为0.07（+/- 0.1）厘米。", "conclusion": "IMASHRIMP系统展示了自动化和加速虾形态学分析的潜力，能提高遗传选择的效率，并有助于实现更可持续的水产养殖实践。", "translation": "本文介绍了IMASHRIMP，一个用于白对虾（凡纳滨对虾）自动化形态学分析的改良系统，旨在优化水产养殖中的遗传选择任务。现有的深度学习和计算机视觉技术经过修改，以解决RGBD图像中虾形态分析的特定挑战。IMASHRIMP包含两个判别模块，基于修改后的ResNet-50架构，用于按视角分类图像并确定吻部完整性。它提出了一个“双因素认证（人工和AI）”系统，将视角分类中的人为错误从0.97%降低到0%，将吻部检测中的人为错误从12.46%降低到3.64%。此外，姿态估计模块从VitPose改编而来，用于预测虾骨骼上的23个关键点，并为侧视图和背视图设置了独立的网络。一个使用支持向量机（SVM）模型的形态回归模块被整合，用于将像素测量值转换为厘米单位。实验结果表明，该系统有效降低了人为错误，姿态估计的平均精度（mAP）达到97.94%，像素到厘米的转换误差为0.07（+/- 0.1）厘米。IMASHRIMP展示了自动化和加速虾形态学分析的潜力，提高了遗传选择的效率，并有助于实现更可持续的水产养殖实践。代码可在https://github.com/AbiamRemacheGonzalez/ImaShrimp-public获取。", "summary": "IMASHRIMP是一个创新的系统，通过修改深度学习和计算机视觉技术，实现了白对虾（凡纳滨对虾）的自动化形态学分析，以优化水产养殖中的遗传选择。该系统集成了基于ResNet-50的判别模块、改编自VitPose的姿态估计模块和基于SVM的形态回归模块。它显著降低了人为错误，提升了姿态估计精度和像素到厘米的转换准确性，有望大幅提升遗传选择效率并促进可持续水产养殖。", "keywords": "白对虾, 形态学分析, 计算机视觉, 深度学习, 遗传选择", "comments": "该论文提出了一种结合深度学习和计算机视觉的创新方法，用于解决水产养殖中白对虾的自动化生物测量问题。其“双因素认证”系统有效降低了人为错误，并通过高精度姿态估计和像素转换，展现了在实际应用中的巨大潜力。这项工作对于加速遗传选择过程和推动可持续水产养殖具有重要意义。"}}
{"id": "2507.02698", "title": "Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions", "authors": ["Thomas Hazenberg", "Yao Ma", "Seyed Sahand Mohammadi Ziabari", "Marijn van Rijswijk"], "categories": ["cs.LG", "econ.EM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02698v1", "summary": "This study investigates how Multi-Agent Reinforcement Learning (MARL) can\nimprove dynamic pricing strategies in supply chains, particularly in contexts\nwhere traditional ERP systems rely on static, rule-based approaches that\noverlook strategic interactions among market actors. While recent research has\napplied reinforcement learning to pricing, most implementations remain\nsingle-agent and fail to model the interdependent nature of real-world supply\nchains. This study addresses that gap by evaluating the performance of three\nMARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,\nwithin a simulated environment informed by real e-commerce transaction data and\na LightGBM demand prediction model. Results show that rule-based agents achieve\nnear-perfect fairness (Jain's Index: 0.9896) and the highest price stability\n(volatility: 0.024), but they fully lack competitive dynamics. Among MARL\nagents, MADQN exhibits the most aggressive pricing behaviour, with the highest\nvolatility and the lowest fairness (0.5844). MADDPG provides a more balanced\napproach, supporting market competition (share volatility: 9.5 pp) while\nmaintaining relatively high fairness (0.8819) and stable pricing. These\nfindings suggest that MARL introduces emergent strategic behaviour not captured\nby static pricing rules and may inform future developments in dynamic pricing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02698v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "供应链中动态定价的多智能体强化学习：在真实模拟市场条件下基准测试策略性智能体行为", "tldr": "本研究在真实模拟市场条件下，评估了多智能体强化学习（MARL）算法（MADDPG, MADQN, QMIX）在供应链动态定价中的表现，并与静态规则基线进行比较，发现MARL能引入传统方法缺乏的战略行为。", "motivation": "传统ERP系统在供应链动态定价中依赖静态、基于规则的方法，忽略了市场参与者之间的战略互动。现有强化学习应用于定价多为单智能体，未能建模真实供应链的相互依赖性。", "method": "本研究通过一个由真实电商交易数据和LightGBM需求预测模型构建的模拟环境，评估了MADDPG、MADQN和QMIX三种多智能体强化学习（MARL）算法与静态规则基线的性能。", "result": "规则基线智能体实现了接近完美的公平性（Jain指数：0.9896）和最高的定价稳定性（波动性：0.024），但完全缺乏竞争动态。在MARL智能体中，MADQN展现出最具攻击性的定价行为，具有最高波动性和最低公平性（0.5844）。MADDPG提供了一种更平衡的方法，支持市场竞争（份额波动：9.5 pp），同时保持相对较高的公平性（0.8819）和稳定的定价。", "conclusion": "研究结果表明，多智能体强化学习（MARL）引入了静态定价规则无法捕捉的新兴战略行为，可能为动态定价的未来发展提供信息。", "translation": "本研究探讨了多智能体强化学习（MARL）如何改进供应链中的动态定价策略，特别是在传统ERP系统依赖静态、基于规则的方法而忽略市场参与者之间战略互动的背景下。尽管最近的研究已将强化学习应用于定价，但大多数实施仍是单智能体，未能建模真实世界供应链的相互依赖性质。本研究通过在一个由真实电商交易数据和LightGBM需求预测模型构建的模拟环境中，评估了MADDPG、MADQN和QMIX三种MARL算法与静态规则基线的性能，从而弥补了这一空白。结果显示，基于规则的智能体实现了接近完美的公平性（Jain指数：0.9896）和最高的定价稳定性（波动性：0.024），但它们完全缺乏竞争动态。在MARL智能体中，MADQN表现出最具攻击性的定价行为，具有最高的波动性和最低的公平性（0.5844）。MADDPG提供了一种更平衡的方法，支持市场竞争（份额波动：9.5 pp），同时保持相对较高的公平性（0.8819）和稳定的定价。这些发现表明，MARL引入了静态定价规则无法捕捉的新兴战略行为，并可能为动态定价的未来发展提供信息。", "summary": "本研究探讨了多智能体强化学习（MARL）在供应链动态定价中的应用，旨在弥补传统方法和现有单智能体RL未能捕捉市场参与者间战略互动的不足。研究在一个基于真实电商数据的模拟环境中，比较了MADDPG、MADQN、QMIX三种MARL算法与静态规则基线。结果表明，虽然规则基线具有高公平性和稳定性但缺乏竞争性，MARL算法能够引入新兴战略行为，其中MADDPG在支持竞争、维持公平性和定价稳定性方面表现出较好的平衡。这些发现为动态定价的未来发展提供了新的视角。", "keywords": "多智能体强化学习, 动态定价, 供应链, 战略行为, 市场模拟", "comments": "这项研究通过引入多智能体强化学习来解决供应链动态定价中传统方法缺乏战略互动的问题，具有创新性。它不仅弥补了现有单智能体RL的局限性，还通过在真实模拟环境下进行基准测试，为理解不同MARL算法在引入竞争动态和平衡性能指标方面的表现提供了宝贵见解。其结果对于未来动态定价策略的开发具有重要指导意义。"}}
{"id": "2507.02546", "title": "MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details", "authors": ["Ruicheng Wang", "Sicheng Xu", "Yue Dong", "Yu Deng", "Jianfeng Xiang", "Zelong Lv", "Guangzhong Sun", "Xin Tong", "Jiaolong Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.02546v1", "summary": "We propose MoGe-2, an advanced open-domain geometry estimation model that\nrecovers a metric scale 3D point map of a scene from a single image. Our method\nbuilds upon the recent monocular geometry estimation approach, MoGe, which\npredicts affine-invariant point maps with unknown scales. We explore effective\nstrategies to extend MoGe for metric geometry prediction without compromising\nthe relative geometry accuracy provided by the affine-invariant point\nrepresentation. Additionally, we discover that noise and errors in real data\ndiminish fine-grained detail in the predicted geometry. We address this by\ndeveloping a unified data refinement approach that filters and completes real\ndata from different sources using sharp synthetic labels, significantly\nenhancing the granularity of the reconstructed geometry while maintaining the\noverall accuracy. We train our model on a large corpus of mixed datasets and\nconducted comprehensive evaluations, demonstrating its superior performance in\nachieving accurate relative geometry, precise metric scale, and fine-grained\ndetail recovery -- capabilities that no previous methods have simultaneously\nachieved.", "comment": "Project page: https://wangrc.site/MoGe2Page/", "pdf_url": "http://arxiv.org/pdf/2507.02546v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MoGe-2：具有度量尺度和锐利细节的精确单目几何", "tldr": "MoGe-2是一个先进的单目几何估计模型，能从单张图像中恢复具有度量尺度的3D点图，同时保持相对几何精度并显著增强细节，解决了以往方法未能同时实现的挑战。", "motivation": "现有的单目几何估计方法MoGe预测的是具有未知尺度的仿射不变点图，无法提供度量尺度的几何信息。此外，真实数据中的噪声和误差会降低预测几何的精细细节。本文旨在解决这些问题，实现度量尺度预测和细节增强。", "method": "MoGe-2在MoGe的基础上，探索了有效的策略以实现度量几何预测，同时不损害仿射不变点表示提供的相对几何精度。为了解决真实数据中细节丢失的问题，开发了一种统一的数据细化方法，利用锐利的合成标签过滤和补全来自不同来源的真实数据。模型在大量混合数据集上进行训练。", "result": "MoGe-2在实现精确的相对几何、精确的度量尺度和精细细节恢复方面表现出卓越的性能，这些能力是以前的方法未能同时实现的。", "conclusion": "MoGe-2成功地从单张图像中恢复了具有度量尺度的场景3D点图，同时保持了高相对几何精度并显著增强了细节，填补了现有方法的空白。", "translation": "我们提出了MoGe-2，一个先进的开放域几何估计模型，能够从单张图像中恢复场景的度量尺度3D点图。我们的方法建立在最近的单目几何估计方法MoGe的基础上，该方法预测具有未知尺度的仿射不变点图。我们探索了有效的策略来扩展MoGe以进行度量几何预测，同时不损害仿射不变点表示提供的相对几何精度。此外，我们发现真实数据中的噪声和误差会降低预测几何的精细细节。我们通过开发一种统一的数据细化方法来解决这个问题，该方法利用锐利的合成标签过滤和补全来自不同来源的真实数据，显著增强了重建几何的粒度，同时保持了整体精度。我们在大量混合数据集上训练了我们的模型，并进行了全面的评估，证明了其在实现精确相对几何、精确度量尺度和精细细节恢复方面的卓越性能——这些能力是以前的方法未能同时实现的。", "summary": "MoGe-2是一个先进的单目几何估计模型，能够从单张图像中恢复具有度量尺度的场景3D点图。它在MoGe的基础上进行了扩展，实现了度量几何预测，并引入了统一的数据细化方法，利用合成标签提升真实数据的细节精度。实验证明，MoGe-2在相对几何精度、度量尺度准确性和细节恢复方面均表现出色，超越了现有方法。", "keywords": "单目几何估计, 度量尺度, 细节恢复, 数据细化, 3D点图", "comments": "MoGe-2的创新之处在于其能够同时实现度量尺度估计和精细细节恢复，这是以往单目几何方法难以兼顾的挑战。其通过数据细化利用合成标签提升真实数据质量的方法尤其值得关注，为处理真实世界噪声数据提供了有效途径。该研究对于单目3D重建领域的实际应用具有重要意义。"}}
{"id": "2507.02710", "title": "Fluid Democracy in Federated Data Aggregation", "authors": ["Aditya Vema Reddy Kesari", "Krishna Reddy Kesari"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "url": "http://arxiv.org/abs/2507.02710v1", "summary": "Federated learning (FL) mechanisms typically require each client to transfer\ntheir weights to a central server, irrespective of how useful they are. In\norder to avoid wasteful data transfer costs from clients to the central server,\nwe propose the use of consensus based protocols to identify a subset of clients\nwith most useful model weights at each data transfer step. First, we explore\nthe application of existing fluid democracy protocols to FL from a performance\nstandpoint, comparing them with traditional one-person-one-vote (also known as\n1p1v or FedAvg). We propose a new fluid democracy protocol named\nviscous-retained democracy that always does better than 1p1v under the same\nassumptions as existing fluid democracy protocols while also not allowing for\ninfluence accumulation. Secondly, we identify weaknesses of fluid democracy\nprotocols from an adversarial lens in terms of their dependence on topology\nand/ or number of adversaries required to negatively impact the global model\nweights. To this effect, we propose an algorithm (FedVRD) that dynamically\nlimits the effect of adversaries while minimizing cost by leveraging the\ndelegation topology.", "comment": "ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "pdf_url": "http://arxiv.org/pdf/2507.02710v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "联邦数据聚合中的流动民主", "tldr": "传统的联邦学习（FL）机制要求所有客户端传输权重，造成资源浪费。本文提出并探索了将流动民主协议（包括一种新的“粘性保留民主”协议）应用于FL，以选择最有用的客户端模型权重，从而降低传输成本并抵抗对抗性攻击。", "motivation": "传统的联邦学习（FL）机制要求每个客户端将其权重传输到中央服务器，无论其有用性如何，这导致了浪费性的数据传输成本。", "method": "首先，从性能角度探讨了现有流动民主协议在联邦学习中的应用，并将其与传统的一人一票（1p1v或FedAvg）进行比较。其次，提出了一种名为“粘性保留民主”的新型流动民主协议，该协议在相同假设下始终优于1p1v，且不允许影响力累积。最后，识别了流动民主协议在对抗性场景下的弱点，并提出了一种算法（FedVRD），通过利用委托拓扑动态限制对手的影响，同时最小化成本。", "result": "所提出的“粘性保留民主”协议在与现有流动民主协议相同的假设下，始终优于传统的一人一票（1p1v）方法，并且不会导致影响力累积。所提出的FedVRD算法能够动态地限制对手的影响，同时通过利用委托拓扑来最小化成本。", "conclusion": "本文通过引入和改进流动民主协议（特别是“粘性保留民主”协议和FedVRD算法），成功解决了联邦学习中数据传输效率低下的问题，并增强了对对抗性攻击的鲁棒性。", "translation": "联邦学习（FL）机制通常要求每个客户端将其权重传输到中央服务器，无论其有用性如何。为了避免从客户端到中央服务器的浪费性数据传输成本，我们建议使用基于共识的协议，在每个数据传输步骤中识别出具有最有用的模型权重的客户端子集。首先，我们从性能角度探讨了现有流动民主协议在联邦学习中的应用，并将其与传统的“一人一票”（也称为1p1v或FedAvg）进行比较。我们提出了一种名为“粘性保留民主”的新型流动民主协议，该协议在与现有流动民主协议相同的假设下，始终优于1p1v，同时也不允许影响力累积。其次，我们从对抗性角度识别了流动民主协议的弱点，包括它们对拓扑结构和/或负面影响全局模型权重所需的对手数量的依赖性。为此，我们提出了一种算法（FedVRD），该算法通过利用委托拓扑，动态地限制对手的影响，同时最小化成本。", "summary": "本文旨在解决联邦学习（FL）中因客户端无差别传输模型权重而导致的数据传输低效问题。研究提出并探索了将基于共识的流动民主协议应用于FL，以选择具有最有用的模型权重的客户端子集，从而降低成本。具体地，文章比较了现有流动民主协议与传统FedAvg的性能，并提出了一种新的“粘性保留民主”协议，该协议在性能上优于FedAvg且能防止影响力累积。此外，论文还分析了流动民主协议在对抗性场景下的弱点，并引入了FedVRD算法，该算法通过利用委托拓扑动态限制对手影响并最小化成本。", "keywords": "联邦学习, 流动民主, 数据聚合, 粘性保留民主, FedVRD", "comments": "本文创新性地将流动民主的概念应用于联邦学习，以优化数据传输并增强系统鲁棒性。提出的“粘性保留民主”协议和FedVRD算法是关键贡献，有效解决了FL中的效率和安全挑战。专注于减少浪费的数据传输对于联邦学习的实际部署具有重要的实用价值。"}}
{"id": "2507.02565", "title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning", "authors": ["Buzhen Huang", "Chen Li", "Chongyang Xu", "Dongyue Lu", "Jinnan Chen", "Yangang Wang", "Gim Hee Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02565v1", "summary": "Due to visual ambiguities and inter-person occlusions, existing human pose\nestimation methods cannot recover plausible close interactions from in-the-wild\nvideos. Even state-of-the-art large foundation models~(\\eg, SAM) cannot\naccurately distinguish human semantics in such challenging scenarios. In this\nwork, we find that human appearance can provide a straightforward cue to\naddress these obstacles. Based on this observation, we propose a dual-branch\noptimization framework to reconstruct accurate interactive motions with\nplausible body contacts constrained by human appearances, social proxemics, and\nphysical laws. Specifically, we first train a diffusion model to learn the\nhuman proxemic behavior and pose prior knowledge. The trained network and two\noptimizable tensors are then incorporated into a dual-branch optimization\nframework to reconstruct human motions and appearances. Several constraints\nbased on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to\nassist the optimization. With the proxemics prior and diverse constraints, our\nmethod is capable of estimating accurate interactions from in-the-wild videos\ncaptured in complex environments. We further build a dataset with pseudo\nground-truth interaction annotations, which may promote future research on pose\nestimation and human behavior understanding. Experimental results on several\nbenchmarks demonstrate that our method outperforms existing approaches. The\ncode and data are available at https://www.buzhenhuang.com/works/CloseApp.html.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02565v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "重建近距离人体交互与外观和近体推理", "tldr": "提出一种基于外观和近体推理的双分支优化框架，用于从野外视频中准确重建近距离人体交互，并构建了一个新的数据集。", "motivation": "现有的人体姿态估计方法由于视觉模糊和人际遮挡，无法从野外视频中恢复可信的近距离交互。即使是SOTA的大型基础模型也无法准确区分复杂场景下的人体语义。", "method": "提出一个双分支优化框架，利用人体外观作为线索，重建准确的交互动作，并受人体外观、社交近体学和物理定律约束。具体包括：训练一个扩散模型学习人体近体行为和姿态先验知识；将训练好的网络和两个可优化张量整合到双分支优化框架中，重建人体动作和外观；设计基于3D高斯、2D关键点和网格穿透的约束来辅助优化。此外，还构建了一个带有伪真值交互标注的数据集。", "result": "该方法能够从复杂环境下的野外视频中估计准确的交互。实验结果表明，该方法优于现有方法。", "conclusion": "该研究通过结合外观和近体推理，提出了一种有效的方法来解决近距离人体交互重建的挑战，并提供了一个新的数据集，有望促进相关领域的研究。", "translation": "由于视觉模糊和人际遮挡，现有的人体姿态估计方法无法从野外视频中恢复可信的近距离交互。即使是最先进的大型基础模型（例如SAM）也无法在如此具有挑战性的场景中准确区分人体语义。在这项工作中，我们发现人体外观可以提供一个直接的线索来解决这些障碍。基于这一观察，我们提出了一个双分支优化框架，用于重建准确的交互动作，并受人体外观、社交近体学和物理定律约束，实现可信的身体接触。具体来说，我们首先训练一个扩散模型来学习人体近体行为和姿态先验知识。然后，将训练好的网络和两个可优化张量整合到一个双分支优化框架中，以重建人体动作和外观。还设计了基于3D高斯、2D关键点和网格穿透的多个约束来辅助优化。凭借近体先验和多样化的约束，我们的方法能够从复杂环境中捕获的野外视频中估计准确的交互。我们进一步构建了一个带有伪真值交互标注的数据集，这可能会促进未来在姿态估计和人体行为理解方面的研究。在多个基准上的实验结果表明，我们的方法优于现有方法。代码和数据可在https://www.buzhenhuang.com/works/CloseApp.html获取。", "summary": "本文提出了一种新颖的双分支优化框架，利用人体外观和社交近体学原理，从野外视频中准确重建近距离人体交互。通过训练扩散模型学习姿态先验，并结合多种3D和2D约束，该方法有效解决了现有方法在视觉模糊和遮挡下的局限性。研究还构建了一个新的数据集，并实验证明其性能优于现有方法。", "keywords": "人体交互重建, 姿态估计, 近体学, 外观推理, 扩散模型", "comments": "本文的创新点在于将人体外观作为解决近距离交互中视觉模糊和遮挡的关键线索，并结合社交近体学和物理定律进行约束。提出的双分支优化框架和扩散模型的使用是其方法的核心。此外，构建新的伪真值数据集对未来研究具有重要贡献，解决了该领域数据稀缺的问题。"}}
{"id": "2507.02712", "title": "A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control", "authors": ["Zilin Kang", "Chenyuan Hu", "Yu Luo", "Zhecheng Yuan", "Ruijie Zheng", "Huazhe Xu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02712v1", "summary": "Deep reinforcement learning for continuous control has recently achieved\nimpressive progress. However, existing methods often suffer from primacy bias,\na tendency to overfit early experiences stored in the replay buffer, which\nlimits an RL agent's sample efficiency and generalizability. In contrast,\nhumans are less susceptible to such bias, partly due to infantile amnesia,\nwhere the formation of new neurons disrupts early memory traces, leading to the\nforgetting of initial experiences. Inspired by this dual processes of\nforgetting and growing in neuroscience, in this paper, we propose Forget and\nGrow (FoG), a new deep RL algorithm with two mechanisms introduced. First,\nExperience Replay Decay (ER Decay) \"forgetting early experience\", which\nbalances memory by gradually reducing the influence of early experiences.\nSecond, Network Expansion, \"growing neural capacity\", which enhances agents'\ncapability to exploit the patterns of existing data by dynamically adding new\nparameters during training. Empirical results on four major continuous control\nbenchmarks with more than 40 tasks demonstrate the superior performance of FoG\nagainst SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02712v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种用于连续控制中深度强化学习扩展的“遗忘与成长”策略", "tldr": "受神经科学中遗忘和成长双重过程的启发，本文提出了一种名为FoG的深度强化学习算法，通过经验回放衰减和网络扩展来解决连续控制中深度强化学习的过拟合问题，并在多个基准测试中取得了优于现有算法的性能。", "motivation": "现有深度强化学习方法在连续控制中存在“首因偏差”问题，即对回放缓冲区中早期经验过拟合，这限制了RL智能体的样本效率和泛化能力。人类对此类偏差的易感性较低，部分原因是婴儿期失忆症，新神经元的形成会扰乱早期记忆痕迹，导致早期经验的遗忘。", "method": "本文提出了一种名为Forget and Grow (FoG) 的新型深度RL算法，引入了两种机制：1. 经验回放衰减 (Experience Replay Decay, ER Decay)：“遗忘早期经验”，通过逐渐减少早期经验的影响来平衡记忆。2. 网络扩展 (Network Expansion)：“增长神经容量”，通过在训练期间动态添加新参数来增强智能体利用现有数据模式的能力。", "result": "在四个主要连续控制基准（包含40多个任务）上的实证结果表明，FoG的性能优于现有的最先进深度RL算法，包括BRO、SimBa和TD-MPC2。", "conclusion": "FoG算法通过模拟人类遗忘与成长机制，有效解决了深度强化学习中的首因偏差问题，显著提升了算法在连续控制任务中的样本效率和泛化能力，并超越了现有SOTA算法。", "translation": "深度强化学习在连续控制方面最近取得了令人瞩目的进展。然而，现有方法通常遭受首因偏差的影响，即过度拟合回放缓冲区中存储的早期经验，这限制了强化学习智能体的样本效率和泛化能力。相比之下，人类对此类偏差的易感性较低，部分原因是婴儿期失忆症，新神经元的形成会扰乱早期记忆痕迹，导致早期经验的遗忘。受神经科学中这种遗忘和成长双重过程的启发，本文提出了一种名为Forget and Grow (FoG) 的新型深度强化学习算法，引入了两种机制。首先是经验回放衰减 (Experience Replay Decay, ER Decay)，“遗忘早期经验”，它通过逐渐减少早期经验的影响来平衡记忆。其次是网络扩展 (Network Expansion)，“增长神经容量”，它通过在训练期间动态添加新参数来增强智能体利用现有数据模式的能力。在四个主要连续控制基准（包含40多个任务）上的实证结果表明，FoG的性能优于现有的最先进深度强化学习算法，包括BRO、SimBa和TD-MPC2。", "summary": "本文提出了一种受神经科学启发的新型深度强化学习算法FoG，旨在解决连续控制中深度RL的首因偏差问题。FoG包含两个核心机制：经验回放衰减（ER Decay）用于“遗忘”早期经验以平衡记忆，以及网络扩展用于“增长”神经容量以提高数据利用能力。实验结果表明，FoG在多个连续控制基准测试中表现优于现有的先进深度RL算法。", "keywords": "深度强化学习, 连续控制, 首因偏差, 经验回放衰减, 网络扩展", "comments": "该论文的创新点在于将神经科学中的“遗忘与成长”概念引入深度强化学习，以解决困扰RL算法的首因偏差问题。通过模拟人类记忆的动态调整过程，FoG不仅提升了样本效率，还增强了泛化能力。这种跨学科的借鉴为RL算法的设计提供了新的思路，具有重要的理论和实践意义。"}}
{"id": "2507.02576", "title": "Parametric shape models for vessels learned from segmentations via differentiable voxelization", "authors": ["Alina F. Dima", "Suprosanna Shit", "Huaqi Qiu", "Robbie Holland", "Tamara T. Mueller", "Fabio Antonio Musio", "Kaiyuan Yang", "Bjoern Menze", "Rickmer Braren", "Marcus Makowski", "Daniel Rueckert"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02576v1", "summary": "Vessels are complex structures in the body that have been studied extensively\nin multiple representations. While voxelization is the most common of them,\nmeshes and parametric models are critical in various applications due to their\ndesirable properties. However, these representations are typically extracted\nthrough segmentations and used disjointly from each other. We propose a\nframework that joins the three representations under differentiable\ntransformations. By leveraging differentiable voxelization, we automatically\nextract a parametric shape model of the vessels through shape-to-segmentation\nfitting, where we learn shape parameters from segmentations without the\nexplicit need for ground-truth shape parameters. The vessel is parametrized as\ncenterlines and radii using cubic B-splines, ensuring smoothness and continuity\nby construction. Meshes are differentiably extracted from the learned shape\nparameters, resulting in high-fidelity meshes that can be manipulated post-fit.\nOur method can accurately capture the geometry of complex vessels, as\ndemonstrated by the volumetric fits in experiments on aortas, aneurysms, and\nbrain vessels.", "comment": "15 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02576v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于可微分体素化的血管分割学习参数化形状模型", "tldr": "本文提出了一个通过可微分体素化从分割数据中自动提取血管参数化形状模型的新框架，实现了体素化、网格和参数模型三种表示的统一，并能生成高保真网格。", "motivation": "血管的体素化、网格和参数模型是重要的表示，但它们通常从分割中提取并彼此独立使用。本研究旨在提出一个框架，通过可微分变换将这三种表示连接起来。", "method": "提出一个框架，通过可微分变换将体素化、网格和参数模型三种血管表示连接起来。利用可微分体素化，通过形状到分割的拟合，从分割中自动提取血管的参数化形状模型，无需显式真值形状参数。血管使用三次B样条参数化为中心线和半径，确保平滑性和连续性。网格从学习到的形状参数中可微分地提取，生成高保真网格。", "result": "该方法能够准确捕获复杂血管的几何形状。在主动脉、动脉瘤和脑血管的实验中，通过体积拟合验证了其有效性。", "conclusion": "该框架成功地通过可微分体素化实现了从分割中学习血管参数化形状模型，并能生成高保真网格，有效统一了血管的不同表示方法，提高了建模的准确性和实用性。", "translation": "血管是身体中复杂的结构，已被广泛研究并有多种表示方法。虽然体素化是最常见的，但网格和参数模型因其理想的特性在各种应用中至关重要。然而，这些表示通常通过分割提取，并彼此独立使用。我们提出了一个框架，通过可微分变换将这三种表示连接起来。通过利用可微分体素化，我们通过形状到分割的拟合自动提取血管的参数化形状模型，其中我们从分割中学习形状参数，而无需明确的真值形状参数。血管使用三次B样条参数化为中心线和半径，通过构造确保平滑性和连续性。网格从学习到的形状参数中可微分地提取，从而产生可在拟合后进行操作的高保真网格。我们的方法可以准确捕获复杂血管的几何形状，这在主动脉、动脉瘤和脑血管的实验中通过体积拟合得到了证明。", "summary": "本文提出一个统一框架，通过可微分体素化技术，实现了从血管分割数据中自动学习参数化形状模型。该方法将血管表示为三次B样条定义的中心线和半径，并能从学习到的参数中可微分地生成高保真网格。实验证明，该方法能准确捕捉复杂血管的几何形态，有效整合了体素化、网格和参数模型三种表示。", "keywords": "血管建模, 参数化形状模型, 可微分体素化, 医疗图像分割, B样条", "comments": "这篇论文的创新点在于提出了一个统一的框架，将血管的三种重要表示（体素化、网格和参数模型）通过可微分变换连接起来。特别是，利用可微分体素化实现了从分割数据中自动学习参数化模型，无需显式真值，这大大降低了数据标注的难度。生成高保真可操作网格的能力也增强了模型的实用性。"}}
{"id": "2507.02715", "title": "A Comprehensive Machine Learning Framework for Micromobility Demand Prediction", "authors": ["Omri Porat", "Michael Fire", "Eran Ben-Elia"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02715v1", "summary": "Dockless e-scooters, a key micromobility service, have emerged as\neco-friendly and flexible urban transport alternatives. These services improve\nfirst and last-mile connectivity, reduce congestion and emissions, and\ncomplement public transport for short-distance travel. However, effective\nmanagement of these services depends on accurate demand prediction, which is\ncrucial for optimal fleet distribution and infrastructure planning. While\nprevious studies have focused on analyzing spatial or temporal factors in\nisolation, this study introduces a framework that integrates spatial, temporal,\nand network dependencies for improved micromobility demand forecasting. This\nintegration enhances accuracy while providing deeper insights into urban\nmicromobility usage patterns. Our framework improves demand prediction accuracy\nby 27 to 49% over baseline models, demonstrating its effectiveness in capturing\nmicromobility demand patterns. These findings support data-driven micromobility\nmanagement, enabling optimized fleet distribution, cost reduction, and\nsustainable urban planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02715v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "微出行需求预测的综合机器学习框架", "tldr": "本研究提出了一个集成了空间、时间和网络依赖性的机器学习框架，用于提高微出行需求预测的准确性，并实现了27%至49%的准确性提升。", "motivation": "共享电动滑板车等微出行服务虽有益处，但其有效管理依赖于准确的需求预测。现有研究未能充分整合空间、时间及网络因素，导致预测精度不足，影响车队优化和基础设施规划。", "method": "本研究提出了一个机器学习框架，该框架整合了空间、时间以及网络依赖性，以改进微出行需求预测。", "result": "该框架将需求预测准确性比基线模型提高了27%至49%。", "conclusion": "该研究提出的集成框架显著提高了微出行需求预测的准确性，为数据驱动的微出行管理提供了支持，有助于优化车队分配、降低成本和可持续城市规划。", "translation": "共享电动滑板车作为一种关键的微出行服务，已成为环保、灵活的城市交通替代方案。这些服务改善了第一英里和最后一英里的连接性，减少了交通拥堵和排放，并补充了短途公共交通。然而，这些服务的有效管理取决于准确的需求预测，这对于最佳车队分配和基础设施规划至关重要。虽然之前的研究主要集中于孤立地分析空间或时间因素，但本研究引入了一个整合空间、时间以及网络依赖性的框架，以改进微出行需求预测。这种整合在提高准确性的同时，也为城市微出行使用模式提供了更深入的洞察。我们的框架将需求预测准确性比基线模型提高了27%至49%，证明了其在捕捉微出行需求模式方面的有效性。这些发现支持数据驱动的微出行管理，从而实现优化的车队分配、成本降低和可持续城市规划。", "summary": "本研究提出了一个综合的机器学习框架，用于预测共享电动滑板车等微出行服务的需求。该框架创新性地整合了空间、时间及网络依赖性，以克服现有研究仅孤立分析单一因素的局限性。通过这种集成方法，该框架显著提高了微出行需求预测的准确性，比基线模型提升了27%至49%。研究结果表明，该框架能有效捕捉微出行需求模式，为数据驱动的微出行管理、优化车队调度和城市可持续规划提供了有力支持。", "keywords": "微出行, 需求预测, 机器学习, 空间-时间分析, 网络依赖", "comments": "这项研究的创新之处在于其综合性方法，首次将空间、时间及网络依赖性整合到一个机器学习框架中，用于微出行需求预测。这超越了以往研究的局限性，提供了更全面和准确的预测能力。其重要性在于直接解决了城市微出行服务管理的实际痛点，通过提高预测精度，能够显著优化资源配置、降低运营成本，并促进城市交通的可持续发展。该框架的普适性也使其未来有望应用于其他共享出行服务领域。"}}
{"id": "2507.02581", "title": "Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning", "authors": ["Tan Pan", "Zhaorui Tan", "Kaiyu Guo", "Dongli Xu", "Weidi Xu", "Chen Jiang", "Xin Guo", "Yuan Qi", "Yuan Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV25", "url": "http://arxiv.org/abs/2507.02581v1", "summary": "3D medical image self-supervised learning (mSSL) holds great promise for\nmedical analysis. Effectively supporting broader applications requires\nconsidering anatomical structure variations in location, scale, and morphology,\nwhich are crucial for capturing meaningful distinctions. However, previous mSSL\nmethods partition images with fixed-size patches, often ignoring the structure\nvariations. In this work, we introduce a novel perspective on 3D medical images\nwith the goal of learning structure-aware representations. We assume that\npatches within the same structure share the same semantics (semantic\nconsistency) while those from different structures exhibit distinct semantics\n(semantic discrepancy). Based on this assumption, we propose an mSSL framework\nnamed $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency\nin two steps. First, $S^2DC$ enforces distinct representations for different\npatches to increase semantic discrepancy by leveraging an optimal transport\nstrategy. Second, $S^2DC$ advances semantic consistency at the structural level\nbased on neighborhood similarity distribution. By bridging patch-level and\nstructure-level representations, $S^2DC$ achieves structure-aware\nrepresentations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3\nmodalities, our proposed method consistently outperforms the state-of-the-art\nmethods in mSSL.", "comment": "Accepted by ICCV25", "pdf_url": "http://arxiv.org/pdf/2507.02581v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "三维医学图像自监督学习中的结构感知语义差异与一致性", "tldr": "本文提出了一种名为$S^2DC$的新型三维医学图像自监督学习框架，通过考虑解剖结构变异来学习结构感知表示，并在多项任务中超越了现有最先进方法。", "motivation": "现有的三维医学图像自监督学习（mSSL）方法通常使用固定大小的图像块来分割图像，这忽略了解剖结构在位置、尺度和形态上的重要变异，从而限制了其捕捉有意义区别的能力和在更广泛应用中的有效性。", "method": "本文提出了一种名为$S^2DC$的mSSL框架，旨在学习结构感知的表示。该方法基于一个假设：同一结构内的图像块共享相同的语义（语义一致性），而不同结构间的图像块则表现出不同的语义（语义差异性）。$S^2DC$分两步实现：首先，利用最优传输策略强制不同图像块产生不同的表示，以增加语义差异性；其次，基于邻域相似性分布，在结构层面推进语义一致性。通过连接图像块级和结构级表示，$S^2DC$实现了结构感知表示。", "result": "在10个数据集、4项任务和3种模态上进行了全面评估，所提出的$S^2DC$方法始终优于现有最先进的mSSL方法。", "conclusion": "本研究提出的$S^2DC$框架通过有效整合结构感知的语义差异与一致性，成功解决了三维医学图像自监督学习中忽视解剖结构变异的问题，并在多项基准测试中展现出超越现有技术的卓越性能，证明了其在医学图像分析中的巨大潜力。", "translation": "三维医学图像自监督学习（mSSL）在医学分析中具有广阔前景。为有效支持更广泛的应用，需要考虑解剖结构在位置、尺度和形态上的变异，这对于捕捉有意义的区别至关重要。然而，以往的mSSL方法使用固定大小的图像块来分割图像，往往忽略了结构变异。在这项工作中，我们引入了一种关于三维医学图像的新颖视角，旨在学习结构感知表示。我们假设同一结构内的图像块共享相同的语义（语义一致性），而来自不同结构的图像块则表现出不同的语义（语义差异性）。基于这一假设，我们提出了一个名为$S^2DC$的mSSL框架，分两步实现结构感知语义差异与一致性。首先，$S^2DC$通过利用最优传输策略，强制不同图像块产生不同的表示，以增加语义差异性。其次，$S^2DC$基于邻域相似性分布，在结构层面推进语义一致性。通过连接图像块级和结构级表示，$S^2DC$实现了结构感知表示。在10个数据集、4项任务和3种模态上进行了全面评估，我们提出的方法在mSSL中始终优于现有最先进的方法。", "summary": "本论文提出了一个名为$S^2DC$的新型三维医学图像自监督学习（mSSL）框架，旨在解决现有方法忽略解剖结构变异的问题。$S^2DC$通过强制同一结构内的图像块保持语义一致性，并使不同结构间的图像块呈现语义差异性，从而学习结构感知表示。具体而言，它利用最优传输策略增强语义差异，并基于邻域相似性分布提升结构层面的语义一致性。该方法在10个数据集、4项任务和3种模态上的广泛评估表明，其性能持续优于现有最先进的mSSL方法。", "keywords": "三维医学图像, 自监督学习, 结构感知, 语义差异, 语义一致性", "comments": "该论文的创新点在于明确提出了结构感知（Structure-aware）的概念，并通过语义差异与一致性来引导三维医学图像的自监督学习，从而克服了传统固定尺寸图像块方法在处理解剖结构变异方面的局限性。其方法论结合了最优传输和邻域相似性，有效桥接了图像块级和结构级表示，为医学图像分析领域提供了新的视角和强大的工具。该方法在多项任务和数据集上表现出卓越的性能，证明了其重要性和有效性。"}}
{"id": "2507.02724", "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms", "authors": ["Shiyi Liu", "Buwen Liang", "Yuetong Fang", "Zixuan Jiang", "Renjing Xu"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02724v1", "summary": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02724v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "跨生物体蛋白质-蛋白质相互作用预测的分层多标签对比学习", "tldr": "HIPPO是一种分层对比学习框架，用于跨生物体蛋白质-蛋白质相互作用（PPI）预测，通过分层对比损失和数据驱动的惩罚机制，实现了最先进的性能和强大的零样本迁移能力，特别适用于数据稀疏的物种。", "motivation": "近期人工智能在科学领域的进展突出了对比学习在连接异构生物数据模态方面的强大能力。本研究旨在利用这一范式，解决蛋白质-蛋白质相互作用（PPI）预测问题，特别是在跨生物体场景下，并应对数据稀疏或不平衡的挑战。", "method": "本研究提出了HIPPO（HIerarchical Protein-Protein interaction prediction across Organisms），一个用于蛋白质-蛋白质相互作用（PPI）预测的分层对比框架。该方法通过多层生物表征匹配，对蛋白质序列及其分层属性进行对齐。它整合了模拟蛋白质功能类别结构化关系的分层对比损失函数，并通过数据驱动的惩罚机制自适应地结合领域和家族知识，以确保学习到的嵌入空间与蛋白质功能的内在层次结构保持一致。", "result": "实验结果表明，HIPPO在基准数据集上实现了最先进的性能，优于现有方法，并在数据量较少的情况下显示出鲁棒性。值得注意的是，该模型无需重新训练即可对其他物种表现出强大的零样本迁移能力。进一步分析揭示，分层特征融合对于捕获保守的相互作用决定因素（如结合基序和功能注释）至关重要。", "conclusion": "这项工作推动了跨物种PPI预测的进展，并为数据稀疏或不平衡的多物种场景中的相互作用预测提供了一个统一的框架。", "translation": "科学领域人工智能的最新进展突显了对比学习在连接异构生物数据模态方面的强大能力。基于这一范式，我们提出了HIPPO（HIerarchical Protein-Protein interaction prediction across Organisms），一个用于蛋白质-蛋白质相互作用（PPI）预测的分层对比框架，其中蛋白质序列及其分层属性通过多层生物表征匹配进行对齐。所提出的方法结合了分层对比损失函数，这些函数模拟了蛋白质功能类别之间的结构化关系。该框架通过数据驱动的惩罚机制自适应地整合领域和家族知识，强制学习到的嵌入空间与蛋白质功能的内在层次结构保持一致。在基准数据集上的实验表明，HIPPO取得了最先进的性能，优于现有方法，并在数据量较少的情况下表现出鲁棒性。值得注意的是，该模型无需重新训练即可对其他物种表现出强大的零样本迁移能力，即使在实验数据有限的特征较少或稀有生物体中，也能实现可靠的PPI预测和功能推断。进一步分析表明，分层特征融合对于捕获保守的相互作用决定因素，例如结合基序和功能注释，至关重要。这项工作推动了跨物种PPI预测的进展，并为数据稀疏或不平衡的多物种数据场景中的相互作用预测提供了一个统一的框架。", "summary": "本研究提出了一种名为HIPPO的分层多标签对比学习框架，用于跨生物体的蛋白质-蛋白质相互作用（PPI）预测。该框架通过对蛋白质序列和分层属性进行多层生物表征匹配，并结合分层对比损失和数据驱动的惩罚机制，实现了学习嵌入空间与蛋白质功能内在层次结构的一致性。实验证明，HIPPO在基准数据集上表现出最先进的性能，并在低数据量环境下具有鲁棒性，尤其展现出强大的零样本跨物种迁移能力，为数据稀疏或不平衡的多物种PPI预测提供了统一且有效的解决方案。", "keywords": "蛋白质-蛋白质相互作用, 对比学习, 分层学习, 跨物种预测, 零样本学习", "comments": "该论文的创新点在于提出了HIPPO框架，将分层对比学习应用于跨物种PPI预测。其亮点在于能够将蛋白质的层次属性融入学习过程，并实现了强大的零样本迁移能力，这对于数据稀疏或未充分表征的物种具有重要意义。这一方法为解决跨物种生物数据异构性和数据量限制问题提供了新的思路，具有较高的实用价值。"}}
{"id": "2507.02591", "title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding", "authors": ["Weili Xu", "Enxin Song", "Wenhao Chai", "Xuexiang Wen", "Tian Ye", "Gaoang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.02591v1", "summary": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02591v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AuroraLong：将RNN带回高效开放式视频理解", "tldr": "AuroraLong提出用线性RNN取代多模态大语言模型（MLLM）中的Transformer，以解决长视频理解中Transformer模型计算和内存成本随序列长度二次增长的问题，实现了与同类Transformer模型相当的性能。", "motivation": "现有基于Transformer的大语言模型（LLMs）在长视频理解中面临计算复杂度和内存成本高昂的问题，因为其所需内存和计算量与输入序列长度呈二次方增长。", "method": "提出AuroraLong模型，通过将多模态大语言模型（MLLMs）中的LLM组件替换为线性RNN语言模型来处理任意长度的输入序列，且具有固定大小的隐藏状态。为了进一步提高吞吐量和效率，该方法将视觉token合并与线性RNN模型结合，通过按升序重新排列视觉token的大小。", "result": "AuroraLong模型仅有20亿参数，且仅使用公共数据进行训练，但在多个视频基准测试中，其性能与在私有数据集上训练的类似大小的基于Transformer的模型相当。", "conclusion": "这表明高效的线性RNNs有潜力通过降低计算门槛来普及长视频理解。据作者所知，这是首次在类LLaVA模型中使用基于线性RNN的LLM骨干进行开放式视频理解。", "translation": "长视频理解的挑战在于其高计算复杂度和高昂的内存成本，因为基于Transformer的大语言模型所需的内存和计算量随输入序列长度呈二次方增长。我们提出AuroraLong来解决这一挑战，通过将多模态大语言模型（MLLM）中的LLM组件替换为线性RNN语言模型，该模型能够处理任意长度的输入序列，并具有固定大小的隐藏状态。为了进一步提高吞吐量和效率，我们通过按升序重新排列视觉token的大小，将视觉token合并与线性RNN模型相结合。尽管AuroraLong只有20亿参数，并且仅在公共数据上进行训练，但它在多个视频基准测试中取得了与在私有数据集上训练的类似大小的基于Transformer的模型相当的性能。这表明高效的线性RNNs有潜力通过降低计算门槛来普及长视频理解。据我们所知，我们是第一个在类LLaVA模型中使用基于线性RNN的LLM骨干进行开放式视频理解的研究。", "summary": "AuroraLong旨在解决长视频理解中基于Transformer模型计算和内存成本过高的问题。它通过在多模态大语言模型中用线性RNN取代传统LLM，实现了对任意长度序列的高效处理，并结合视觉token合并技术进一步提升性能。尽管参数量较小且仅使用公共数据训练，AuroraLong在多个视频基准测试中展现出与大型Transformer模型相当的性能，突显了线性RNN在普及长视频理解方面的潜力。", "keywords": "线性RNN, 长视频理解, 多模态大语言模型, 计算效率, AuroraLong", "comments": "这篇论文的创新点在于将线性RNN重新引入到长视频理解领域，并将其作为多模态大语言模型（MLLM）的骨干，以克服Transformer模型在处理长序列时的计算和内存瓶颈。其重要性在于证明了小型、高效的模型也能在复杂任务上取得有竞争力的性能，尤其是在资源受限的场景下，这有助于推动长视频理解的“民主化”。"}}
{"id": "2507.02602", "title": "Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development", "authors": ["Riccardo Gallon", "Fabian Schiemenz", "Alessandra Menicucci", "Eberhard Gill"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to Acta Astronautica", "url": "http://arxiv.org/abs/2507.02602v1", "summary": "The increasing importance of Vision-Based Navigation (VBN) algorithms in\nspace missions raises numerous challenges in ensuring their reliability and\noperational robustness. Sensor faults can lead to inaccurate outputs from\nnavigation algorithms or even complete data processing faults, potentially\ncompromising mission objectives. Artificial Intelligence (AI) offers a powerful\nsolution for detecting such faults, overcoming many of the limitations\nassociated with traditional fault detection methods. However, the primary\nobstacle to the adoption of AI in this context is the lack of sufficient and\nrepresentative datasets containing faulty image data.\n  This study addresses these challenges by focusing on an interplanetary\nexploration mission scenario. A comprehensive analysis of potential fault cases\nin camera sensors used within the VBN pipeline is presented. The causes and\neffects of these faults are systematically characterized, including their\nimpact on image quality and navigation algorithm performance, as well as\ncommonly employed mitigation strategies. To support this analysis, a simulation\nframework is introduced to recreate faulty conditions in synthetically\ngenerated images, enabling a systematic and controlled reproduction of faulty\ndata. The resulting dataset of fault-injected images provides a valuable tool\nfor training and testing AI-based fault detection algorithms. The final link to\nthe dataset will be added after an embargo period. For peer-reviewers, this\nprivate link is available.", "comment": "Submitted to Acta Astronautica", "pdf_url": "http://arxiv.org/pdf/2507.02602v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "解决视觉导航中的相机传感器故障：仿真与数据集开发", "tldr": "由于缺乏用于训练AI的数据集，本文开发了一个模拟框架和故障注入图像数据集，以解决视觉导航中相机传感器故障的检测问题。", "motivation": "视觉导航（VBN）算法在空间任务中日益重要，但传感器故障会影响其可靠性。人工智能（AI）是检测这些故障的强大解决方案，但主要障碍是缺乏足够和有代表性的包含故障图像数据的数据集。", "method": "本研究针对行星际探索任务场景，全面分析了视觉导航流程中相机传感器潜在的故障情况，系统地描述了这些故障的原因和影响。为支持分析，引入了一个仿真框架来在合成图像中重现故障条件，从而系统且可控地生成故障数据。", "result": "生成了一个注入故障图像的数据集，为训练和测试基于AI的故障检测算法提供了有价值的工具。", "conclusion": "本研究通过开发一个故障注入图像数据集，解决了AI在视觉导航故障检测中面临的数据不足问题，从而有助于提高视觉导航系统的可靠性。", "translation": "视觉导航（VBN）算法在空间任务中日益重要，这在确保其可靠性和操作鲁棒性方面带来了诸多挑战。传感器故障可能导致导航算法输出不准确，甚至完全的数据处理故障，从而可能危及任务目标。人工智能（AI）为检测此类故障提供了一个强大的解决方案，克服了传统故障检测方法的许多局限性。然而，在这种情况下，AI 应用的主要障碍是缺乏足够且有代表性的包含故障图像数据的数据集。\n本研究通过关注行星际探索任务场景来应对这些挑战。文中对视觉导航流程中使用的相机传感器中潜在的故障案例进行了全面分析。系统地描述了这些故障的原因和影响，包括它们对图像质量和导航算法性能的影响，以及常用的缓解策略。为了支持这项分析，引入了一个仿真框架，用于在合成图像中重现故障条件，从而实现对故障数据的系统化和受控的再现。由此产生的故障注入图像数据集为训练和测试基于AI的故障检测算法提供了宝贵的工具。该数据集的最终链接将在禁运期后添加。对于同行评审员，此私有链接可用。", "summary": "本研究解决了视觉导航（VBN）在空间任务中因相机传感器故障而导致的可靠性问题。鉴于传统故障检测方法的局限性以及基于AI的解决方案缺乏足够的故障图像数据集，本文系统分析了行星际探索任务中相机传感器的潜在故障类型、原因及影响。为克服数据障碍，研究引入了一个仿真框架，用于合成生成包含故障的图像数据，并构建了一个新的故障注入图像数据集，旨在为训练和测试AI驱动的故障检测算法提供支持。", "keywords": "视觉导航, 传感器故障, 人工智能, 数据集, 模拟", "comments": "该论文创新性地通过模拟和数据集开发解决了AI在视觉导航故障检测中面临的数据稀缺问题。其重要性在于为未来基于AI的鲁棒视觉导航系统奠定了数据基础。局限性可能在于模拟的故障类型是否能完全覆盖真实世界的复杂性和多样性。"}}
{"id": "2507.02754", "title": "Fast and Simplex: 2-Simplicial Attention in Triton", "authors": ["Aurko Roy", "Timothy Chou", "Sai Surya Duvvuri", "Sijia Chen", "Jiecao Yu", "Xiaodong Wang", "Manzil Zaheer", "Rohan Anil"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, with appendix 25 pages", "url": "http://arxiv.org/abs/2507.02754v1", "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.", "comment": "10 pages, with appendix 25 pages", "pdf_url": "http://arxiv.org/pdf/2507.02754v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "快速与单纯：Triton中的2-单纯注意力", "tldr": "2-单纯注意力Transformer通过将点积注意力推广到三线性函数，并利用高效的Triton内核实现，在数学、编码、推理和逻辑任务上表现出比标准Transformer更好的令牌效率，尤其在数据量大而非计算受限的场景下，它能改变知识和推理任务的缩放定律指数。", "motivation": "传统缩放定律假设计算受限和无限数据供应，但现代大型语言模型越来越依赖海量互联网数据，不再是纯粹的计算受限，因此需要优先考虑令牌效率的架构。", "method": "本文研究了2-单纯Transformer，该架构通过高效的Triton内核实现，将标准点积注意力推广到三线性函数。", "result": "2-单纯Transformer比标准Transformer展现出更好的令牌效率：在固定令牌预算下，类似大小的模型在数学、编码、推理和逻辑任务上优于其点积对应物。研究表明，2-单纯注意力改变了知识和推理任务中缩放定律的指数。", "conclusion": "2-单纯注意力通过提高令牌效率并改变特定任务的缩放定律指数，为大型语言模型提供了一种有前景的架构，特别适用于数据丰富而非计算是主要瓶颈的场景。", "translation": "最近的研究表明，训练损失与模型大小和令牌数量都呈幂律关系，并且实现计算最优模型需要同时扩展模型大小和令牌数量。然而，这些缩放定律假设数据供应无限，并且主要适用于计算受限的环境。随着现代大型语言模型越来越依赖海量的互联网规模数据集，它们是计算受限的假设变得越来越不成立。这种转变突显了对优先考虑令牌效率的架构的需求。\n在这项工作中，我们研究了2-单纯Transformer的使用，这是一种通过高效的Triton内核实现将标准点积注意力推广到三线性函数的架构。我们证明了2-单纯Transformer比标准Transformer实现了更好的令牌效率：在固定的令牌预算下，类似大小的模型在涉及数学、编码、推理和逻辑的任务上优于其点积对应物。我们通过证明2-单纯注意力与点积注意力相比，改变了知识和推理任务缩放定律中的指数来量化这些增益。", "summary": "本文介绍了2-单纯Transformer，这是一种旨在提高大型语言模型令牌效率的架构。它通过高效的Triton内核实现，将标准点积注意力推广到三线性函数。实验表明，在固定令牌预算下，2-单纯Transformer在数学、编码、推理和逻辑等任务上的表现优于标准Transformer。作者通过证明2-单纯注意力改变了知识和推理任务的缩放定律指数来量化这些增益，表明在数据丰富且计算不再是唯一瓶颈的场景下，它是一种更具令牌效率的方法。", "keywords": "令牌效率, 2-单纯注意力, Transformer, 缩放定律, Triton", "comments": "该论文的创新之处在于将注意力机制推广到三线性函数，并展示了其在令牌效率方面的显著提升，这对于当前大型语言模型从计算受限转向数据受限的趋势至关重要。利用Triton进行高效实现也值得关注。这项工作有效解决了LLM扩展中的一个关键挑战。"}}
{"id": "2507.02664", "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models", "authors": ["Ziyin Zhou", "Yunpeng Luo", "Yuanchen Wu", "Ke Sun", "Jiayi Ji", "Ke Yan", "Shouhong Ding", "Xiaoshuai Sun", "Yunsheng Wu", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.02664v1", "summary": "The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02664v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AIGI-Holmes：迈向可解释和可泛化的多模态大语言模型AI生成图像检测", "tldr": "本研究提出AIGI-Holmes，一个基于多模态大语言模型（MLLM）的AI生成图像（AIGI）检测模型，旨在解决现有检测技术缺乏可解释性和泛化能力的问题。它引入了一个大型数据集和三阶段训练框架，并能生成人类可验证的解释。", "motivation": "AI生成内容（AIGC）技术的快速发展导致高度逼真的AI生成图像（AIGI）被滥用以传播虚假信息，对公共信息安全构成威胁。尽管现有的AIGI检测技术普遍有效，但它们面临两个问题：1）缺乏人类可验证的解释，以及2）在最新一代技术中缺乏泛化能力。", "method": "为解决现有AIGI检测技术面临的问题，本研究引入了一个大规模综合数据集Holmes-Set，包括用于解释AIGI的指令微调数据集Holmes-SFTSet和人类对齐偏好数据集Holmes-DPOSet。开发了一种名为多专家评审的高效数据标注方法，通过结构化MLLM解释增强数据生成，并通过跨模型评估、专家缺陷过滤和人类偏好修改进行质量控制。此外，提出了Holmes Pipeline，这是一个精心设计的三阶段训练框架，包括视觉专家预训练、监督微调和直接偏好优化。Holmes Pipeline使多模态大语言模型（MLLMs）适用于AIGI检测，同时生成人类可验证和人类对齐的解释，最终得到模型AIGI-Holmes。在推理阶段，引入了一种协同解码策略，整合了视觉专家的模型感知与MLLM的语义推理，进一步增强了泛化能力。", "result": "在三个基准测试上进行的广泛实验验证了AIGI-Holmes的有效性。", "conclusion": "本研究通过提出AIGI-Holmes模型、大型数据集和三阶段训练框架，成功解决了AI生成图像检测中可解释性和泛化能力不足的问题。该模型能够提供人类可验证和人类对齐的解释，并在实验中展现出有效性。", "translation": "AI生成内容（AIGC）技术的快速发展导致高度逼真的AI生成图像（AIGI）被滥用以传播虚假信息，对公共信息安全构成威胁。尽管现有的AIGI检测技术普遍有效，但它们面临两个问题：1）缺乏人类可验证的解释，以及2）在最新一代技术中缺乏泛化能力。为解决这些问题，我们引入了一个大规模综合数据集Holmes-Set，其中包括Holmes-SFTSet（一个带有图像是否为AI生成解释的指令微调数据集）和Holmes-DPOSet（一个人类对齐偏好数据集）。我们的工作引入了一种名为多专家评审的高效数据标注方法，通过结构化MLLM解释增强数据生成，并通过跨模型评估、专家缺陷过滤和人类偏好修改进行质量控制。此外，我们提出了Holmes Pipeline，一个精心设计的三阶段训练框架，包括视觉专家预训练、监督微调和直接偏好优化。Holmes Pipeline使多模态大语言模型（MLLMs）适用于AIGI检测，同时生成人类可验证和人类对齐的解释，最终得到我们的模型AIGI-Holmes。在推理阶段，我们引入了一种协同解码策略，整合了视觉专家的模型感知与MLLM的语义推理，进一步增强了泛化能力。在三个基准测试上进行的广泛实验验证了AIGI-Holmes的有效性。", "summary": "本论文提出了一种名为AIGI-Holmes的新型AI生成图像（AIGI）检测模型，旨在解决现有方法在可解释性和泛化能力方面的不足。为此，研究团队构建了大规模综合数据集Holmes-Set，并开发了高效的多专家评审数据标注方法。AIGI-Holmes基于一个三阶段的Holmes Pipeline训练框架，该框架将多模态大语言模型（MLLMs）应用于AIGI检测，并能生成人类可验证和对齐的解释。此外，引入了协同解码策略以增强模型的泛化能力。在多个基准测试上的实验验证了AIGI-Holmes的有效性。", "keywords": "AI生成图像, AIGI检测, 多模态大语言模型, 可解释性, 泛化性", "comments": "本文的主要创新点在于利用多模态大语言模型（MLLMs）解决AI生成图像检测中的可解释性和泛化性问题。通过构建专门的数据集和设计独特的三阶段训练框架，AIGI-Holmes不仅提升了检测能力，还提供了人类可理解的解释，这对于增强公众对检测结果的信任至关重要。其数据标注方法和协同解码策略也具有创新性。然而，摘要中未提供具体的性能指标，这使得对其有效性的量化评估有所欠缺。"}}
{"id": "2507.02762", "title": "Contextual Online Pricing with (Biased) Offline Data", "authors": ["Yixuan Zhang", "Ruihao Zhu", "Qiaomin Xie"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      47 pages, 4 figures", "url": "http://arxiv.org/abs/2507.02762v1", "summary": "We study contextual online pricing with biased offline data. For the scalar\nprice elasticity case, we identify the instance-dependent quantity $\\delta^2$\nthat measures how far the offline data lies from the (unknown) online optimum.\nWe show that the time length $T$, bias bound $V$, size $N$ and dispersion\n$\\lambda_{\\min}(\\hat{\\Sigma})$ of the offline data, and $\\delta^2$ jointly\ndetermine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty\n(OFU) policy achieves a minimax-optimal, instance-dependent regret bound\n$\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T +\n\\frac{dT}{\\lambda_{\\min}(\\hat{\\Sigma}) + (N \\wedge T) \\delta^2})\\big)$. For\ngeneral price elasticity, we establish a worst-case, minimax-optimal rate\n$\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT\n}{\\lambda_{\\min}(\\hat{\\Sigma})})\\big)$ and provide a generalized OFU algorithm\nthat attains it. When the bias bound $V$ is unknown, we design a robust variant\nthat always guarantees sub-linear regret and strictly improves on purely online\nmethods whenever the exact bias is small. These results deliver the first tight\nregret guarantees for contextual pricing in the presence of biased offline\ndata. Our techniques also transfer verbatim to stochastic linear bandits with\nbiased offline data, yielding analogous bounds.", "comment": "47 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02762v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用（有偏）离线数据的上下文在线定价", "tldr": "本文研究了使用有偏离线数据的上下文在线定价问题，提出了OFU策略和广义OFU算法，首次为该问题提供了紧致的遗憾界，并证明了其最优性。", "motivation": "研究有偏离线数据下的上下文在线定价问题，以解决现有方法可能存在的不足或填补该领域的理论空白，特别是当离线数据存在偏差时如何进行有效定价。", "method": "对于标量价格弹性情况，提出了乐观面对不确定性（OFU）策略，并给出了其遗憾界。对于一般价格弹性，提出了广义OFU算法。当偏差界未知时，设计了一种鲁棒变体，以确保次线性遗憾。", "result": "确定了衡量离线数据与在线最优值距离的实例相关量$\\\\delta^2$。证明了时间长度$T$、偏差界$V$、离线数据大小$N$和离线数据色散$\\\\lambda_{\\\\min}(\\\\hat{\\\\Sigma})$以及$\\\\delta^2$共同决定了统计复杂性。OFU策略实现了极小化极大最优的、实例相关的遗憾界$\\\\tilde{\\\\mathcal{O}}\\\\\big(d\\\\sqrt{T} \\\\wedge (V^2T + \\\\frac{dT}{\\\\lambda_{\\\\min}(\\\\hat{\\\\Sigma}) + (N \\\\wedge T) \\\\delta^2})\\\\big)$。对于一般价格弹性，建立了最坏情况下的极小化极大最优速率$\\\\tilde{\\\\mathcal{O}}\\\\\big(d\\\\sqrt{T} \\\\wedge (V^2T + \\\\frac{dT}{\\\\lambda_{\\\\min}(\\\\hat{\\\\Sigma})})\\\\big)$。当偏差界$V$未知时，设计的鲁棒变体始终保证次线性遗憾，并在精确偏差较小时显著优于纯在线方法。这些结果首次为存在有偏离线数据的上下文定价提供了紧致的遗憾保证。", "conclusion": "本文首次为存在有偏离线数据的上下文定价问题提供了紧致的遗憾界，并提出了相应的极小化极大最优算法。所提出的技术也适用于带有有偏离线数据的随机线性强盗问题。", "translation": "我们研究了使用有偏离线数据的上下文在线定价问题。对于标量价格弹性情况，我们确定了衡量离线数据与（未知）在线最优值之间距离的实例相关量$\\\\delta^2$。我们表明，时间长度$T$、偏差界$V$、离线数据大小$N$和离线数据色散$\\\\lambda_{\\\\min}(\\\\hat{\\\\Sigma})$以及$\\\\delta^2$共同决定了统计复杂性。乐观面对不确定性（OFU）策略实现了极小化极大最优的、实例相关的遗憾界$\\\\tilde{\\\\mathcal{O}}\\\\\big(d\\\\sqrt{T} \\\\wedge (V^2T + \\\\frac{dT}{\\\\lambda_{\\\\min}(\\\\hat{\\\\Sigma}) + (N \\\\wedge T) \\\\delta^2})\\\\big)$。对于一般价格弹性，我们建立了最坏情况下的极小化极大最优速率$\\\\tilde{\\\\mathcal{O}}\\\\\big(d\\\\sqrt{T} \\\\wedge (V^2T + \\\\frac{dT}{\\\\lambda_{\\\\min}(\\\\hat{\\\\Sigma})})\\\\big)$，并提供了实现该速率的广义OFU算法。当偏差界$V$未知时，我们设计了一种鲁棒变体，它始终保证次线性遗憾，并且在精确偏差较小时严格优于纯在线方法。这些结果首次为存在有偏离线数据的上下文定价提供了紧致的遗憾保证。我们的技术也原封不动地适用于带有有偏离线数据的随机线性强盗问题，并得到了类似的界限。", "summary": "本文研究了在存在有偏离线数据情况下的上下文在线定价问题。针对标量和一般价格弹性两种情况，分别提出了基于乐观面对不确定性（OFU）的策略和广义算法，并推导了其极小化极大最优的遗憾界。研究表明，离线数据的特性（如偏差、大小和色散）以及与在线最优值的距离共同影响统计复杂性。此外，文章还设计了一种鲁棒算法来处理未知偏差界的情况，并证明其能保证次线性遗憾。该研究首次为有偏离线数据下的上下文定价提供了严格的遗憾保证，其技术也适用于随机线性强盗问题。", "keywords": "上下文在线定价, 有偏离线数据, OFU策略, 遗憾界, 极小化极大最优", "comments": "这项研究的创新之处在于首次为存在有偏离离线数据的上下文定价问题提供了紧致的遗憾界，填补了该领域的理论空白。通过引入实例相关量\\\\(\\\\delta^2\\\\)来量化离线数据偏差，并提出适应性强的OFU算法，解决了实际应用中数据偏差的挑战。其结果对于需要整合历史有偏数据进行实时决策的场景（如推荐系统、广告竞价）具有重要指导意义。此外，该技术的可迁移性也增加了其重要性。"}}
{"id": "2507.02686", "title": "Learning few-step posterior samplers by unfolding and distillation of diffusion models", "authors": ["Charlesquin Kemajou Mbakam", "Jonathan Spence", "Marcelo Pereyra"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      28 pages, 16 figures, 10 tables", "url": "http://arxiv.org/abs/2507.02686v1", "summary": "Diffusion models (DMs) have emerged as powerful image priors in Bayesian\ncomputational imaging. Two primary strategies have been proposed for leveraging\nDMs in this context: Plug-and-Play methods, which are zero-shot and highly\nflexible but rely on approximations; and specialized conditional DMs, which\nachieve higher accuracy and faster inference for specific tasks through\nsupervised training. In this work, we introduce a novel framework that\nintegrates deep unfolding and model distillation to transform a DM image prior\ninto a few-step conditional model for posterior sampling. A central innovation\nof our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm\n- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et\nal., 2025) - representing the first known instance of deep unfolding applied to\na Monte Carlo sampling scheme. We demonstrate our proposed unfolded and\ndistilled samplers through extensive experiments and comparisons with the state\nof the art, where they achieve excellent accuracy and computational efficiency,\nwhile retaining the flexibility to adapt to variations in the forward model at\ninference time.", "comment": "28 pages, 16 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.02686v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过扩散模型的展开与蒸馏学习少步后验采样器", "tldr": "本文提出了一种结合深度展开和模型蒸馏的新框架，将扩散模型图像先验转化为少步条件模型用于后验采样，首次将深度展开应用于蒙特卡洛采样方案，实现了卓越的准确性和计算效率。", "motivation": "扩散模型是贝叶斯计算成像中强大的图像先验。现有方法（即插即用和专用条件扩散模型）存在局限性：即插即用方法依赖近似，而专用条件扩散模型需要监督训练且仅适用于特定任务。本文的动机是开发一种能够结合两者的优点，实现高精度、快速推理且灵活的后验采样方法。", "method": "本文引入了一个新颖的框架，该框架整合了深度展开和模型蒸馏，将扩散模型图像先验转化为用于后验采样的少步条件模型。核心创新是将马尔可夫链蒙特卡罗（MCMC）算法（具体是LATINO Langevin采样器）进行展开，这是首次将深度展开应用于蒙特卡罗采样方案。", "result": "所提出的展开和蒸馏采样器在广泛的实验中与现有技术进行了比较，取得了卓越的准确性和计算效率，同时保持了在推理时适应前向模型变化的灵活性。", "conclusion": "本文提出的通过深度展开和模型蒸馏学习的少步后验采样器，能够有效利用扩散模型作为图像先验，并在贝叶斯计算成像中实现高精度、高效率的后验采样，同时保持了良好的灵活性。", "translation": "扩散模型（DMs）已成为贝叶斯计算成像中强大的图像先验。在此背景下，已提出了两种利用DMs的主要策略：即插即用方法，它们是零样本且高度灵活但依赖于近似；以及专用条件DMs，它们通过监督训练为特定任务实现更高的准确性和更快的推理。在这项工作中，我们引入了一个新颖的框架，该框架整合了深度展开和模型蒸馏，将DM图像先验转化为用于后验采样的少步条件模型。我们方法的一个核心创新是对马尔可夫链蒙特卡罗（MCMC）算法（特别是最近提出的LATINO Langevin采样器（Spagnoletti et al., 2025））进行展开，这代表了首次将深度展开应用于蒙特卡罗采样方案。我们通过广泛的实验和与现有技术的比较，展示了我们提出的展开和蒸馏采样器，它们实现了卓越的准确性和计算效率，同时保持了在推理时适应前向模型变化的灵活性。", "summary": "本文提出了一种新颖的框架，通过结合深度展开和模型蒸馏，将扩散模型（DM）图像先验转化为高效的少步条件模型，用于贝叶斯计算成像中的后验采样。该方法首次将深度展开应用于蒙特卡罗采样方案（如LATINO Langevin采样器），旨在克服现有DM利用策略（即插即用和专用条件DM）的局限性。实验结果表明，该方法在保持灵活性的同时，实现了卓越的准确性和计算效率。", "keywords": "扩散模型, 后验采样, 深度展开, 模型蒸馏, MCMC", "comments": "本文的创新之处在于首次将深度展开技术应用于蒙特卡罗采样方案（特别是MCMC算法），为扩散模型在贝叶斯计算成像中的高效利用开辟了新途径。它有效结合了现有方法的优点，解决了精度、速度和灵活性之间的权衡问题。"}}
{"id": "2507.02782", "title": "Understanding and Improving Length Generalization in Recurrent Models", "authors": ["Ricardo Buitrago Ruiz", "Albert Gu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02782v1", "summary": "Recently, recurrent models such as state space models and linear attention\nhave become popular due to their linear complexity in the sequence length.\nThanks to their recurrent nature, in principle they can process arbitrarily\nlong sequences, but their performance sometimes drops considerably beyond their\ntraining context lengths-i.e. they fail to length generalize. In this work, we\nprovide comprehensive empirical and theoretical analysis to support the\nunexplored states hypothesis, which posits that models fail to length\ngeneralize when during training they are only exposed to a limited subset of\nthe distribution of all attainable states (i.e. states that would be attained\nif the recurrence was applied to long sequences). Furthermore, we investigate\nsimple training interventions that aim to increase the coverage of the states\nthat the model is trained on, e.g. by initializing the state with Gaussian\nnoise or with the final state of a different input sequence. With only 500\npost-training steps ($\\sim 0.1\\%$ of the pre-training budget), these\ninterventions enable length generalization for sequences that are orders of\nmagnitude longer than the training context (e.g. $2k\\longrightarrow 128k$) and\nshow improved performance in long context tasks, thus presenting a simple and\nefficient way to enable robust length generalization in general recurrent\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02782v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "理解和改进循环模型中的长度泛化能力", "tldr": "循环模型在处理训练长度之外的序列时性能下降，本文通过提出“未探索状态假说”并引入简单的训练干预措施，显著提高了模型的长度泛化能力，使其能处理远超训练长度的序列。", "motivation": "循环模型（如状态空间模型和线性注意力）因其线性复杂性而流行，理论上可处理任意长序列。然而，它们在训练上下文长度之外的性能会显著下降，即未能实现长度泛化。本文旨在理解并改进这一问题。", "method": "本文提供了全面的实证和理论分析，以支持“未探索状态假说”，该假说认为模型未能进行长度泛化是因为在训练期间仅接触了所有可达状态分布的有限子集。此外，研究了旨在增加模型训练状态覆盖范围的简单训练干预措施，例如使用高斯噪声或不同输入序列的最终状态初始化状态。", "result": "仅需500个后训练步骤（约占预训练预算的0.1%），这些干预措施就能使模型对长度比训练上下文长几个数量级的序列实现长度泛化（例如2k→128k），并在长上下文任务中表现出改进的性能。", "conclusion": "通过提出“未探索状态假说”并引入简单的训练干预措施，本文提供了一种简单有效的方法来使通用循环模型实现鲁棒的长度泛化能力。", "translation": "最近，循环模型如状态空间模型和线性注意力因其在序列长度上的线性复杂性而变得流行。由于其循环性质，它们原则上可以处理任意长的序列，但它们的性能有时在超出训练上下文长度后会大幅下降——即它们未能实现长度泛化。在这项工作中，我们提供了全面的实证和理论分析来支持“未探索状态假说”，该假说认为当模型在训练期间仅接触所有可达状态分布的有限子集（即如果循环应用于长序列将达到的状态）时，它们未能实现长度泛化。此外，我们研究了旨在增加模型训练状态覆盖范围的简单训练干预措施，例如通过高斯噪声或不同输入序列的最终状态初始化状态。仅用500个后训练步骤（约占预训练预算的0.1%），这些干预措施就使模型能够对比训练上下文长几个数量级的序列实现长度泛化（例如2k→128k），并在长上下文任务中显示出改进的性能，从而提出了一种简单有效的方法来使通用循环模型实现鲁棒的长度泛化能力。", "summary": "本研究探讨了循环模型在训练上下文长度之外性能显著下降的“长度泛化”问题。论文提出了“未探索状态假说”，认为该问题源于训练时模型未能充分探索所有潜在状态空间。为解决此问题，研究引入了简单高效的训练干预措施，如通过噪声或序列最终状态初始化模型状态。实验证明，这些干预措施仅需少量额外训练，即可显著提升模型处理远超训练长度序列的泛化能力，并在长上下文任务中表现更优。", "keywords": "循环模型, 长度泛化, 未探索状态假说, 状态空间模型, 训练干预", "comments": "本文创新性地提出了“未探索状态假说”来解释循环模型长度泛化失败的原因，并提供实证和理论支持。其最大的亮点在于提出的简单训练干预措施，仅用极小的额外计算成本（0.1%的预训练预算）就实现了显著的长度泛化提升，这对于实际应用具有重要价值和效率优势。该研究为提高循环模型在长序列处理中的鲁棒性提供了一个高效且实用的解决方案。"}}
{"id": "2507.02687", "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data", "authors": ["JungWoo Chae", "Jiyoon Kim", "JaeWoong Choi", "Kyungyul Kim", "Sangheum Hwang"], "categories": ["cs.CV", "cs.AI", "60J60, 68T07", "I.2.6; I.2.10; I.4.9"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      CVPR 2025 camera ready. Project page: this https URL", "url": "http://arxiv.org/abs/2507.02687v1", "summary": "Personalizing diffusion models using limited data presents significant\nchallenges, including overfitting, loss of prior knowledge, and degradation of\ntext alignment. Overfitting leads to shifts in the noise prediction\ndistribution, disrupting the denoising trajectory and causing the model to lose\nsemantic coherence. In this paper, we propose Adaptive Personalized Training\n(APT), a novel framework that mitigates overfitting by employing adaptive\ntraining strategies and regularizing the model's internal representations\nduring fine-tuning. APT consists of three key components: (1) Adaptive Training\nAdjustment, which introduces an overfitting indicator to detect the degree of\noverfitting at each time step bin and applies adaptive data augmentation and\nadaptive loss weighting based on this indicator; (2)Representation\nStabilization, which regularizes the mean and variance of intermediate feature\nmaps to prevent excessive shifts in noise prediction; and (3) Attention\nAlignment for Prior Knowledge Preservation, which aligns the cross-attention\nmaps of the fine-tuned model with those of the pretrained model to maintain\nprior knowledge and semantic coherence. Through extensive experiments, we\ndemonstrate that APT effectively mitigates overfitting, preserves prior\nknowledge, and outperforms existing methods in generating high-quality, diverse\nimages with limited reference data.", "comment": "CVPR 2025 camera ready. Project page: https://lgcnsai.github.io/apt", "pdf_url": "http://arxiv.org/pdf/2507.02687v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "APT：有限数据下扩散模型的自适应个性化训练", "tldr": "APT是一种新颖的自适应个性化训练框架，旨在解决使用有限数据个性化扩散模型时遇到的过拟合、先验知识丢失和文本对齐退化等挑战，通过自适应策略和内部表示正则化，有效生成高质量、多样化的图像。", "motivation": "使用有限数据个性化扩散模型面临过拟合、先验知识丢失和文本对齐退化等挑战。过拟合会导致噪声预测分布偏移，破坏去噪轨迹，并使模型失去语义连贯性，因此需要一种新的训练框架来缓解这些问题。", "method": "本文提出了自适应个性化训练（APT）框架。APT通过自适应训练策略和正则化模型内部表示来缓解过拟合。它包含三个关键组件：1) 自适应训练调整，引入过拟合指标，根据过拟合程度应用自适应数据增强和自适应损失加权；2) 表示稳定，正则化中间特征图的均值和方差，以防止噪声预测的过度偏移；3) 注意力对齐以保留先验知识，将微调模型的交叉注意力图与预训练模型对齐，以保持先验知识和语义连贯性。", "result": "通过大量实验，APT有效缓解了过拟合，保留了先验知识，并且在仅使用有限参考数据的情况下，在生成高质量、多样化图像方面优于现有方法。", "conclusion": "APT框架能够有效解决有限数据下扩散模型个性化面临的挑战，成功缓解过拟合、保留先验知识，并生成高质量且多样化的图像。", "translation": "使用有限数据个性化扩散模型带来了显著挑战，包括过拟合、先验知识丢失和文本对齐退化。过拟合会导致噪声预测分布发生偏移，扰乱去噪轨迹，并使模型失去语义连贯性。在本文中，我们提出了自适应个性化训练（APT），这是一个新颖的框架，通过采用自适应训练策略并在微调过程中正则化模型的内部表示来缓解过拟合。APT由三个关键组件组成：(1) 自适应训练调整，引入一个过拟合指标来检测每个时间步箱的过拟合程度，并根据此指标应用自适应数据增强和自适应损失加权；(2) 表示稳定，正则化中间特征图的均值和方差，以防止噪声预测的过度偏移；和 (3) 注意力对齐以保留先验知识，将微调模型的交叉注意力图与预训练模型的交叉注意力图对齐，以保持先验知识和语义连贯性。通过广泛的实验，我们证明了APT有效缓解了过拟合，保留了先验知识，并且在有限参考数据下生成高质量、多样化图像方面优于现有方法。", "summary": "本文提出了一种名为自适应个性化训练（APT）的新框架，旨在解决使用有限数据个性化扩散模型时面临的过拟合、先验知识丢失和文本对齐退化等挑战。APT通过结合自适应训练策略和正则化模型内部表示来缓解过拟合。该框架由三个核心组件构成：自适应训练调整（根据过拟合指标动态调整数据增强和损失权重）、表示稳定（正则化中间特征图以防止预测偏移）和注意力对齐（通过对齐交叉注意力图来保留先验知识）。实验结果表明，APT能有效减轻过拟合，保持模型先验知识，并在有限数据条件下生成高质量、多样化的图像，性能优于现有方法。", "keywords": "扩散模型, 个性化训练, 有限数据, 过拟合, 自适应训练", "comments": "该论文提出的APT框架通过结合自适应训练策略、内部表示正则化和注意力对齐，为有限数据下扩散模型的个性化训练提供了一个全面的解决方案。其创新点在于引入了过拟合指标来动态调整训练过程，并同时解决了过拟合和先验知识丢失这两个核心问题，这对于提升个性化扩散模型的实用性和生成质量具有重要意义。"}}
{"id": "2507.02807", "title": "In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization", "authors": ["Thiti Suttaket", "Stanley Kok"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02807v1", "summary": "Survival analysis is an important problem in healthcare because it models the\nrelationship between an individual's covariates and the onset time of an event\nof interest (e.g., death). It is important for survival models to be\nwell-calibrated (i.e., for their predicted probabilities to be close to\nground-truth probabilities) because badly calibrated systems can result in\nerroneous clinical decisions. Existing survival models are typically calibrated\nat the population level only, and thus run the risk of being poorly calibrated\nfor one or more minority subpopulations. We propose a model called GRADUATE\nthat achieves multicalibration by ensuring that all subpopulations are\nwell-calibrated too. GRADUATE frames multicalibration as a constrained\noptimization problem, and optimizes both calibration and discrimination\nin-training to achieve a good balance between them. We mathematically prove\nthat the optimization method used yields a solution that is both near-optimal\nand feasible with high probability. Empirical comparisons against\nstate-of-the-art baselines on real-world clinical datasets demonstrate\nGRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of\nthe baselines vis-a-vis GRADUATE's strengths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02807v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "医疗领域中基于约束优化的训练内多重校准生存分析", "tldr": "现有生存模型对亚群体校准不足，本文提出GRADUATE模型，通过约束优化在训练中实现多重校准，提升了医疗领域预测的准确性和可靠性。", "motivation": "现有的生存模型通常仅在群体层面进行校准，导致对一个或多个少数亚群体校准不良，这可能导致错误的临床决策。", "method": "本文提出GRADUATE模型，通过将多重校准表述为约束优化问题，并在训练过程中同时优化校准和判别能力，以确保所有亚群体都得到良好校准。", "result": "数学证明表明，所使用的优化方法能够以高概率产生接近最优且可行的解决方案。在真实世界临床数据集上的经验比较显示，GRADUATE模型比最先进的基线模型更有效，并能弥补现有基线的不足。", "conclusion": "GRADUATE模型通过创新的约束优化方法，成功解决了生存分析中亚群体校准不良的问题，从而提高了医疗预测的可靠性和准确性。", "translation": "生存分析是医疗领域的一个重要问题，因为它模拟了个体协变量与感兴趣事件（例如死亡）发生时间之间的关系。生存模型进行良好校准（即其预测概率接近真实概率）非常重要，因为校准不良的系统可能导致错误的临床决策。现有的生存模型通常仅在群体层面进行校准，因此存在对一个或多个少数亚群体校准不良的风险。我们提出了一种名为 GRADUATE 的模型，通过确保所有亚群体也都得到良好校准来实现多重校准。GRADUATE 将多重校准框架为一个约束优化问题，并在训练过程中同时优化校准和判别能力，以实现它们之间的良好平衡。我们数学证明了所使用的优化方法以高概率产生一个接近最优且可行的解决方案。在真实世界临床数据集上与最先进基线的经验比较证明了 GRADUATE 的有效性。在详细分析中，我们阐明了基线相对于 GRADUATE 优势的缺点。", "summary": "本文针对医疗领域中现有生存模型在亚群体校准方面存在的不足，提出了一种名为GRADUATE的新模型。该模型将多重校准问题构建为一个约束优化问题，并在训练过程中同时优化模型的校准度与判别能力，以确保所有亚群体都能得到良好校准。研究通过数学证明了其优化方法的有效性，并在真实临床数据集上通过实验验证了GRADUATE模型优于现有基线的性能。", "keywords": "生存分析, 多重校准, 约束优化, 医疗健康, 临床决策", "comments": "该论文的创新点在于将多重校准问题转化为约束优化，并在训练阶段同时优化校准和判别能力，从而解决了医疗AI中公平性和可靠性的关键挑战。其提供的数学证明增强了理论支撑。"}}
{"id": "2507.02691", "title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation", "authors": ["Xiangyang Luo", "Ye Zhu", "Yunfei Liu", "Lijian Lin", "Cong Wan", "Zijian Cai", "Shao-Lun Huang", "Yu Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV Accepted", "url": "http://arxiv.org/abs/2507.02691v1", "summary": "Video face swapping aims to address two primary challenges: effectively\ntransferring the source identity to the target video and accurately preserving\nthe dynamic attributes of the target face, such as head poses, facial\nexpressions, lip-sync, \\etc. Existing methods mainly focus on achieving\nhigh-quality identity transfer but often fall short in maintaining the dynamic\nattributes of the target face, leading to inconsistent results. We attribute\nthis issue to the inherent coupling of facial appearance and motion in videos.\nTo address this, we propose CanonSwap, a novel video face-swapping framework\nthat decouples motion information from appearance information. Specifically,\nCanonSwap first eliminates motion-related information, enabling identity\nmodification within a unified canonical space. Subsequently, the swapped\nfeature is reintegrated into the original video space, ensuring the\npreservation of the target face's dynamic attributes. To further achieve\nprecise identity transfer with minimal artifacts and enhanced realism, we\ndesign a Partial Identity Modulation module that adaptively integrates source\nidentity features using a spatial mask to restrict modifications to facial\nregions. Additionally, we introduce several fine-grained synchronization\nmetrics to comprehensively evaluate the performance of video face swapping\nmethods. Extensive experiments demonstrate that our method significantly\noutperforms existing approaches in terms of visual quality, temporal\nconsistency, and identity preservation. Our project page are publicly available\nat https://luoxyhappy.github.io/CanonSwap/.", "comment": "ICCV Accepted", "pdf_url": "http://arxiv.org/pdf/2507.02691v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CanonSwap：通过规范空间调制实现高保真和一致的视频换脸", "tldr": "CanonSwap是一种新的视频换脸框架，通过解耦外观和运动信息，并在统一的规范空间中进行身份修改，解决了现有方法在保持目标面部动态属性方面的一致性问题，实现了高保真和时间一致的换脸效果。", "motivation": "视频换脸面临两大挑战：有效转移源身份和精确保留目标面部动态属性（如头部姿态、面部表情、唇形同步等）。现有方法主要关注高质量的身份转移，但在保持目标面部动态属性方面表现不佳，导致结果不一致。我们将此问题归因于视频中面部外观和运动的固有耦合。", "method": "我们提出了CanonSwap，一种新颖的视频换脸框架，它将运动信息从外观信息中解耦。具体来说，CanonSwap首先消除与运动相关的信息，从而在统一的规范空间内进行身份修改。随后，将交换后的特征重新整合到原始视频空间中，以确保保留目标面部的动态属性。为了进一步实现精确的身份转移，减少伪影并增强真实感，我们设计了一个局部身份调制模块，该模块使用空间掩模自适应地整合源身份特征，以将修改限制在面部区域。此外，我们引入了几种细粒度同步指标来全面评估视频换脸方法的性能。", "result": "广泛的实验表明，我们的方法在视觉质量、时间一致性和身份保留方面显著优于现有方法。", "conclusion": "CanonSwap通过解耦面部外观和运动信息，并在规范空间中进行身份修改，成功解决了视频换脸中保持目标面部动态属性一致性的挑战，从而实现了高保真、时间一致且视觉效果优越的换脸结果。", "translation": "视频换脸旨在解决两个主要挑战：有效地将源身份转移到目标视频，并准确保留目标面部的动态属性，例如头部姿态、面部表情、唇形同步等。现有方法主要专注于实现高质量的身份转移，但往往未能保持目标面部的动态属性，导致结果不一致。我们将此问题归因于视频中面部外观和运动的固有耦合。为了解决这个问题，我们提出了CanonSwap，一个新颖的视频换脸框架，它将运动信息从外观信息中解耦。具体来说，CanonSwap首先消除与运动相关的信息，从而在统一的规范空间内进行身份修改。随后，将交换后的特征重新整合到原始视频空间中，以确保保留目标面部的动态属性。为了进一步实现精确的身份转移，减少伪影并增强真实感，我们设计了一个局部身份调制模块，该模块使用空间掩模自适应地整合源身份特征，以将修改限制在面部区域。此外，我们引入了几种细粒度同步指标来全面评估视频换脸方法的性能。广泛的实验表明，我们的方法在视觉质量、时间一致性和身份保留方面显著优于现有方法。我们的项目页面已在https://luoxyhappy.github.io/CanonSwap/公开。", "summary": "CanonSwap是一种创新的视频换脸框架，旨在解决现有方法在保持目标面部动态属性一致性方面的不足。它通过解耦面部外观和运动信息，并在统一的规范空间中进行身份修改，然后将交换后的特征重新整合回原始视频空间，从而确保保留目标面部的动态属性。该方法还引入了局部身份调制模块以实现精确的身份转移，并提出了细粒度同步指标进行评估。实验证明，CanonSwap在视觉质量、时间一致性和身份保留方面均显著优于现有方法。", "keywords": "视频换脸, 规范空间, 身份解耦, 时间一致性, 局部身份调制", "comments": "CanonSwap的创新点在于提出了将面部外观和运动信息解耦的策略，并在规范空间中进行身份修改，有效地解决了视频换脸中长期存在的动态属性保持和一致性问题。局部身份调制模块和新的评估指标进一步提升了其性能和实用性。这项工作对于视频内容生成和编辑领域具有重要意义。"}}
{"id": "2507.02814", "title": "Replicable Distribution Testing", "authors": ["Ilias Diakonikolas", "Jingyi Gao", "Daniel Kane", "Sihan Liu", "Christopher Ye"], "categories": ["cs.LG", "G.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      39 pages", "url": "http://arxiv.org/abs/2507.02814v1", "summary": "We initiate a systematic investigation of distribution testing in the\nframework of algorithmic replicability. Specifically, given independent samples\nfrom a collection of probability distributions, the goal is to characterize the\nsample complexity of replicably testing natural properties of the underlying\ndistributions. On the algorithmic front, we develop new replicable algorithms\nfor testing closeness and independence of discrete distributions. On the lower\nbound front, we develop a new methodology for proving sample complexity lower\nbounds for replicable testing that may be of broader interest. As an\napplication of our technique, we establish near-optimal sample complexity lower\nbounds for replicable uniformity testing -- answering an open question from\nprior work -- and closeness testing.", "comment": "39 pages", "pdf_url": "http://arxiv.org/pdf/2507.02814v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可复现的分布检验", "tldr": "本文在算法可复现性框架下系统研究了分布检验，开发了新的可复现算法并提出了新的下界证明方法，解决了之前工作的开放问题。", "motivation": "在算法可复现性框架下，系统性地研究分布检验，目标是表征可复现地检验底层分布自然属性的样本复杂度。", "method": "在算法方面，开发了用于检验离散分布的接近性和独立性的新可复现算法。在下界方面，开发了一种新的方法来证明可复现检验的样本复杂度下界。", "result": "开发了用于检验离散分布的接近性和独立性的新可复现算法。为可复现均匀性检验和接近性检验建立了接近最优的样本复杂度下界，回答了先前工作中的一个开放问题。", "conclusion": "成功地在算法可复现性框架下对分布检验进行了系统性研究，并提供了新的算法和下界证明方法，解决了重要的开放问题。", "translation": "我们在算法可复现性框架下启动了对分布检验的系统性研究。具体来说，给定来自一组概率分布的独立样本，目标是表征可复现地检验底层分布自然属性的样本复杂度。在算法方面，我们开发了用于检验离散分布的接近性和独立性的新可复现算法。在下界方面，我们开发了一种新的方法来证明可复现检验的样本复杂度下界，这可能具有更广泛的意义。作为我们技术的一个应用，我们为可复现均匀性检验——回答了先前工作中的一个开放问题——和接近性检验建立了接近最优的样本复杂度下界。", "summary": "本文在算法可复现性框架内对分布检验进行了系统性探索。研究目标是确定可复现地检验分布属性所需的样本复杂度。作者开发了新的可复现算法以检验离散分布的接近性和独立性，并提出了一种通用的新方法来证明可复现检验的样本复杂度下界。通过应用该方法，论文为可复现均匀性检验和接近性检验设定了接近最优的样本复杂度下界，解决了先前研究中的一个悬而未决的问题。", "keywords": "分布检验, 可复现性, 样本复杂度, 接近性检验, 独立性检验", "comments": "这篇论文通过引入“可复现性”这一重要概念到分布检验领域，具有显著的创新性。它不仅提供了具体的算法，还开发了通用的下界证明方法，这对于未来该领域的研究具有重要意义。解决了开放问题也体现了其贡献。"}}
{"id": "2507.02705", "title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment", "authors": ["Qi Xu", "Dongxu Wei", "Lingzhe Zhao", "Wenpu Li", "Zhangchi Huang", "Shunping Ji", "Peidong Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02705v1", "summary": "Simultaneous understanding and 3D reconstruction plays an important role in\ndeveloping end-to-end embodied intelligent systems. To achieve this, recent\napproaches resort to 2D-to-3D feature alignment paradigm, which leads to\nlimited 3D understanding capability and potential semantic information loss. In\nlight of this, we propose SIU3R, the first alignment-free framework for\ngeneralizable simultaneous understanding and 3D reconstruction from unposed\nimages. Specifically, SIU3R bridges reconstruction and understanding tasks via\npixel-aligned 3D representation, and unifies multiple understanding tasks into\na set of unified learnable queries, enabling native 3D understanding without\nthe need of alignment with 2D models. To encourage collaboration between the\ntwo tasks with shared representation, we further conduct in-depth analyses of\ntheir mutual benefits, and propose two lightweight modules to facilitate their\ninteraction. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance not only on the individual tasks of 3D\nreconstruction and understanding, but also on the task of simultaneous\nunderstanding and 3D reconstruction, highlighting the advantages of our\nalignment-free framework and the effectiveness of the mutual benefit designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02705v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SIU3R：超越特征对齐的同步场景理解与三维重建", "tldr": "提出SIU3R，首个无对齐框架，通过像素对齐的3D表示和统一的可学习查询，实现从无姿态图像中进行泛化同步场景理解和3D重建，并在各项任务上达到SOTA性能。", "motivation": "现有方法依赖2D-to-3D特征对齐范式，导致3D理解能力有限和潜在的语义信息丢失，限制了端到端具身智能系统的发展。", "method": "提出SIU3R，一个无对齐框架，用于从无姿态图像中进行泛化同步理解和3D重建。SIU3R通过像素对齐的3D表示桥接重建和理解任务，并将多个理解任务统一为一组统一的可学习查询，从而无需与2D模型对齐即可实现原生3D理解。此外，还提出了两个轻量级模块以促进两个任务之间通过共享表示进行协作。", "result": "SIU3R在3D重建和理解的独立任务以及同步理解和3D重建任务上均取得了最先进的性能。", "conclusion": "SIU3R的无对齐框架和互惠设计在同步场景理解和3D重建中表现出显著优势和有效性，为开发端到端具身智能系统提供了重要进展。", "translation": "同步理解和三维重建在开发端到端具身智能系统中扮演着重要角色。为了实现这一点，最近的方法诉诸于2D到3D特征对齐范式，这导致了有限的三维理解能力和潜在的语义信息丢失。鉴于此，我们提出了SIU3R，这是第一个用于从无姿态图像中进行泛化同步理解和三维重建的无对齐框架。具体而言，SIU3R通过像素对齐的三维表示桥接重建和理解任务，并将多个理解任务统一为一组统一的可学习查询，从而无需与二维模型对齐即可实现原生三维理解。为了鼓励两个任务之间通过共享表示进行协作，我们进一步深入分析了它们的互惠互利，并提出了两个轻量级模块以促进它们的交互。广泛的实验表明，我们的方法不仅在三维重建和理解的独立任务上，而且在同步理解和三维重建任务上都取得了最先进的性能，突出了我们无对齐框架的优势和互惠设计（mutual benefit designs）的有效性。", "summary": "本文提出了SIU3R，一个创新的无对齐框架，用于从无姿态图像中实现同步场景理解和3D重建。针对现有方法中2D-to-3D特征对齐导致的3D理解能力有限和语义信息丢失问题，SIU3R通过像素对齐的3D表示和统一的可学习查询，实现了原生3D理解。此外，该框架设计了轻量级模块以促进重建和理解任务间的协作。实验证明，SIU3R在独立任务和同步任务上均达到了最先进的性能，验证了其无对齐范式和互惠设计的有效性。", "keywords": "同步场景理解, 3D重建, 无对齐框架, 像素对齐3D表示, 可学习查询", "comments": "SIU3R的创新点在于其提出的无对齐框架，解决了传统2D-to-3D特征对齐范式带来的3D理解能力受限和信息丢失问题。通过引入像素对齐的3D表示和统一的可学习查询，实现了更原生的3D理解。此外，强调任务间协作并设计轻量级模块促进交互，提升了系统的整体性能，使其在端到端具身智能系统开发中具有重要意义。"}}
{"id": "2507.02713", "title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation", "authors": ["Qin Guo", "Ailing Zeng", "Dongxu Yue", "Ceyuan Yang", "Yang Cao", "Hanzhong Guo", "Fei Shen", "Wei Liu", "Xihui Liu", "Dan Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02713v1", "summary": "Although significant advancements have been achieved in the progress of\nkeypoint-guided Text-to-Image diffusion models, existing mainstream\nkeypoint-guided models encounter challenges in controlling the generation of\nmore general non-rigid objects beyond humans (e.g., animals). Moreover, it is\ndifficult to generate multiple overlapping humans and animals based on keypoint\ncontrols solely. These challenges arise from two main aspects: the inherent\nlimitations of existing controllable methods and the lack of suitable datasets.\nFirst, we design a DiT-based framework, named UniMC, to explore unifying\ncontrollable multi-class image generation. UniMC integrates instance- and\nkeypoint-level conditions into compact tokens, incorporating attributes such as\nclass, bounding box, and keypoint coordinates. This approach overcomes the\nlimitations of previous methods that struggled to distinguish instances and\nclasses due to their reliance on skeleton images as conditions. Second, we\npropose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed\nfor keypoint-guided human and animal image generation. HAIG-2.9M includes 786K\nimages with 2.9M instances. This dataset features extensive annotations such as\nkeypoints, bounding boxes, and fine-grained captions for both humans and\nanimals, along with rigorous manual inspection to ensure annotation accuracy.\nExtensive experiments demonstrate the high quality of HAIG-2.9M and the\neffectiveness of UniMC, particularly in heavy occlusions and multi-class\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02713v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "UniMC：驯服扩散Transformer用于统一关键点引导的多类别图像生成", "tldr": "UniMC是一个基于DiT的框架，结合了实例和关键点条件，并提出了HAIG-2.9M数据集，用于解决现有关键点引导模型在生成非刚性多类别图像（特别是重叠场景）时的局限性。", "motivation": "现有关键点引导的文本到图像扩散模型在生成人类之外的非刚性物体（如动物）时遇到挑战，并且难以基于关键点控制生成多个重叠的人类和动物。这些挑战源于现有可控方法的固有局限性以及缺乏合适的数据集。", "method": "本文设计了一个名为UniMC的基于DiT的框架，用于统一可控的多类别图像生成。UniMC将实例级和关键点级条件（包括类别、边界框和关键点坐标）整合到紧凑的tokens中，克服了以往方法依赖骨架图像作为条件而难以区分实例和类别的局限性。同时，提出了HAIG-2.9M，一个大规模、高质量、多样化的关键点引导人类和动物图像生成数据集，包含786K图像和2.9M实例，具有关键点、边界框和细粒度标注，并经过严格的人工检查。", "result": "实验证明了HAIG-2.9M的高质量和UniMC的有效性，尤其是在严重遮挡和多类别场景下。", "conclusion": "UniMC框架和HAIG-2.9M数据集有效解决了关键点引导模型在生成非刚性多类别图像（特别是重叠场景）方面的挑战，显著提升了生成能力。", "translation": "尽管关键点引导的文本到图像扩散模型取得了显著进展，但现有主流的关键点引导模型在控制生成人类之外的更普遍非刚性物体（例如动物）时遇到了挑战。此外，仅凭关键点控制难以生成多个重叠的人类和动物。这些挑战主要源于两个方面：现有可控方法的固有局限性以及缺乏合适的数据集。首先，我们设计了一个名为UniMC的基于DiT的框架，旨在探索统一的可控多类别图像生成。UniMC将实例级和关键点级条件整合为紧凑的tokens，其中包含类别、边界框和关键点坐标等属性。这种方法克服了以前方法因依赖骨架图像作为条件而难以区分实例和类别的局限性。其次，我们提出了HAIG-2.9M，一个大规模、高质量、多样化的数据集，专为关键点引导的人类和动物图像生成而设计。HAIG-2.9M包含786K张图像，其中有2.9M个实例。该数据集具有对人类和动物的关键点、边界框和细粒度描述等大量标注，并经过严格的人工检查以确保标注准确性。大量的实验证明了HAIG-2.9M的高质量和UniMC的有效性，特别是在严重遮挡和多类别场景中。", "summary": "本文提出了UniMC框架和HAIG-2.9M数据集，以解决现有关键点引导扩散模型在生成非刚性多类别物体（尤其是重叠场景）时的局限性。UniMC是一个基于DiT的框架，通过整合实例和关键点条件克服了现有方法的缺陷。HAIG-2.9M是一个大规模高质量的人类和动物关键点引导图像数据集。实验证明了UniMC在复杂场景下的有效性。", "keywords": "关键点引导生成, 扩散模型, 多类别图像生成, UniMC, HAIG-2.9M", "comments": "该研究通过提出一种新的框架UniMC和大规模数据集HAIG-2.9M，有效解决了现有关键点引导扩散模型在处理非刚性、多类别以及重叠实例生成方面的挑战。UniMC将实例和关键点信息编码为紧凑的tokens，提升了模型的控制能力和泛化性。HAIG-2.9M数据集的构建填补了该领域高质量多类别关键点标注数据的空白，其规模和多样性对未来的研究具有重要价值。"}}
{"id": "2507.02843", "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding", "authors": ["Yuchen Ma", "Dennis Frauen", "Jonas Schweisthal", "Stefan Feuerriegel"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02843v1", "summary": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02843v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLM驱动的推断时文本混淆下治疗效果估计", "tldr": "本文提出了一种利用大型语言模型和双重鲁棒学习器来解决临床实践中推断时文本混淆导致的治疗效果估计偏差问题的新框架。", "motivation": "在临床实践中，治疗效果估计对于个性化决策至关重要。然而，训练时模型通常使用详细的结构化数据，而在推断时则依赖于不完整的文本描述，这种数据差异会导致治疗效果估计出现偏差。作者将此问题形式化为推断时文本混淆问题。", "method": "为了解决推断时文本混淆问题，本文提出了一个新颖的框架。该框架结合了大型语言模型（LLMs）和一个定制的双重鲁棒学习器，以减轻由推断时文本混淆引起的偏差。", "result": "通过一系列实验，证明了所提出的框架在实际应用中的有效性。", "conclusion": "本文提出的LLM驱动框架能够有效解决推断时文本混淆导致的治疗效果估计偏差问题，并在实际应用中表现出良好效果。", "translation": "治疗效果估计对于医学中的个性化决策至关重要，但这项任务在临床实践中面临独特的挑战。在训练时，用于估计治疗效果的模型通常在包含详细患者信息的结构化医疗数据集上进行训练。然而，在推断时，预测通常使用文本描述（例如，包含自我报告症状的描述）进行，这些描述是对原始患者信息的不完整表示。在这项工作中，我们做出了三项贡献。(1) 我们表明，训练时和推断时可用数据之间的差异可能导致治疗效果的估计存在偏差。我们将此问题形式化为推断时文本混淆问题，其中混淆因素在训练时被完全观察到，但在推断时仅通过文本部分可用。(2) 为了解决这个问题，我们提出了一个新颖的治疗效果估计框架，该框架明确考虑了推断时文本混淆。我们的框架利用大型语言模型以及定制的双重鲁棒学习器来减轻由推断时文本混淆引起的偏差。(3) 通过一系列实验，我们证明了我们的框架在实际应用中的有效性。", "summary": "本文研究了在临床实践中，由于训练时使用结构化数据而推断时依赖不完整文本描述所导致的治疗效果估计偏差问题，并将其定义为“推断时文本混淆”。为解决此问题，作者提出了一个新颖的框架，该框架结合了大型语言模型和定制的双重鲁棒学习器，以有效减轻这种偏差。实验结果表明该框架在实际应用中表现出有效性。", "keywords": "治疗效果估计, 文本混淆, 大型语言模型, 双重鲁棒学习, 临床实践", "comments": "本文识别并 formalize 了一个在实际临床应用中非常重要的挑战：训练和推断时数据表示不一致导致的混淆偏差。其创新点在于利用大型语言模型处理文本信息，并结合双重鲁棒学习器来校正这种偏差，为真实世界场景下的治疗效果估计提供了实用的解决方案。"}}
{"id": "2507.02714", "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models", "authors": ["Yuxuan Wang", "Tianwei Cao", "Huayu Zhang", "Zhongjiang He", "Kongming Liang", "Zhanyu Ma"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.02714v1", "summary": "Image generation has achieved remarkable progress with the development of\nlarge-scale text-to-image models, especially diffusion-based models. However,\ngenerating human images with plausible details, such as faces or hands, remains\nchallenging due to insufficient supervision of local regions during training.\nTo address this issue, we propose FairHuman, a multi-objective fine-tuning\napproach designed to enhance both global and local generation quality fairly.\nSpecifically, we first construct three learning objectives: a global objective\nderived from the default diffusion objective function and two local objectives\nfor hands and faces based on pre-annotated positional priors. Subsequently, we\nderive the optimal parameter updating strategy under the guidance of the\nMinimum Potential Delay (MPD) criterion, thereby attaining fairness-ware\noptimization for this multi-objective problem. Based on this, our proposed\nmethod can achieve significant improvements in generating challenging local\ndetails while maintaining overall quality. Extensive experiments showcase the\neffectiveness of our method in improving the performance of human image\ngeneration under different scenarios.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02714v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "FairHuman：通过扩散模型中的最小潜在延迟公平性提升人物图像生成中的手部和面部质量", "tldr": "FairHuman是一种多目标微调方法，通过引入手部和面部局部监督以及最小潜在延迟（MPD）公平性优化，显著提升了扩散模型生成人物图像时手部和面部细节的质量，同时保持了整体图像质量。", "motivation": "尽管扩散模型在图像生成方面取得了显著进展，但在生成人物图像时，手部和面部等局部区域的细节仍然存在挑战，因为训练过程中缺乏足够的局部监督。", "method": "提出FairHuman，一种多目标微调方法。首先构建三个学习目标：一个全局目标（源自默认扩散目标函数）和两个基于预标注位置先验的局部目标（用于手部和面部）。然后，在最小潜在延迟（MPD）准则的指导下，推导出最优参数更新策略，以实现多目标问题的公平性优化。", "result": "所提出的方法在生成具有挑战性的局部细节方面取得了显著改进，同时保持了整体图像质量。广泛的实验证明了该方法在不同场景下提升人物图像生成性能的有效性。", "conclusion": "FairHuman通过多目标微调和MPD公平性优化，有效解决了扩散模型在生成人物图像时局部细节（尤其是手部和面部）质量不足的问题，显著提升了生成效果。", "translation": "图像生成随着大规模文本到图像模型，特别是基于扩散模型的开发，取得了显著进展。然而，由于训练过程中对局部区域的监督不足，生成具有合理细节（如面部或手部）的人物图像仍然具有挑战性。为了解决这个问题，我们提出了FairHuman，一种多目标微调方法，旨在公平地提升全局和局部生成质量。具体而言，我们首先构建了三个学习目标：一个源自默认扩散目标函数的全局目标，以及两个基于预标注位置先验的手部和面部局部目标。随后，我们在最小潜在延迟（MPD）准则的指导下，推导出最优参数更新策略，从而实现了针对此多目标问题的公平性感知优化。在此基础上，我们提出的方法在生成具有挑战性的局部细节方面取得了显著改进，同时保持了整体质量。广泛的实验展示了我们方法在不同场景下改进人物图像生成性能的有效性。", "summary": "FairHuman是一种针对扩散模型的多目标微调方法，旨在解决人物图像生成中手部和面部细节不足的问题。它通过结合全局扩散目标与基于位置先验的局部手部和面部目标，并采用最小潜在延迟（MPD）准则进行公平性优化，显著提升了局部细节的生成质量，同时保持了整体图像的真实性。实验证明了其在不同场景下提升人物图像生成性能的有效性。", "keywords": "扩散模型, 人物图像生成, 手部质量, 面部质量, 多目标优化, 最小潜在延迟", "comments": "这篇论文通过引入局部区域（手部和面部）的精细监督，并结合多目标优化中的公平性概念（MPD），有效地解决了扩散模型在生成人物图像时长期存在的“鬼手”和“模糊脸”问题。其创新点在于将局部质量提升与多目标公平性优化相结合，为生成高质量人体图像提供了新的思路。"}}
{"id": "2507.02847", "title": "MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis", "authors": ["Kunyu Zhang", "Qiang Li", "Shujian Yu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by MICCAI-25, code is available at \\url{ this https URL }", "url": "http://arxiv.org/abs/2507.02847v1", "summary": "Recent evidence suggests that modeling higher-order interactions (HOIs) in\nfunctional magnetic resonance imaging (fMRI) data can enhance the diagnostic\naccuracy of machine learning systems. However, effectively extracting and\nutilizing HOIs remains a significant challenge. In this work, we propose\nMvHo-IB, a novel multi-view learning framework that integrates both pairwise\ninteractions and HOIs for diagnostic decision-making, while automatically\ncompressing task-irrelevant redundant information. MvHo-IB introduces several\nkey innovations: (1) a principled method that combines O-information from\ninformation theory with a matrix-based Renyi alpha-order entropy estimator to\nquantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to\neffectively utilize these interactions, and (3) a new multi-view learning\ninformation bottleneck objective to enhance representation learning.\nExperiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves\nstate-of-the-art performance, significantly outperforming previous methods,\nincluding recent hypergraph-based techniques. The implementation of MvHo-IB is\navailable at https://github.com/zky04/MvHo-IB.", "comment": "Accepted by MICCAI-25, code is available at\n  \\url{https://github.com/zky04/MvHo-IB}", "pdf_url": "http://arxiv.org/pdf/2507.02847v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MvHo-IB：用于脑疾病诊断的多视图高阶信息瓶颈", "tldr": "MvHo-IB是一种新颖的多视图学习框架，用于脑疾病诊断，它有效结合了高阶交互和成对交互，并通过信息瓶颈自动压缩无关信息，在fMRI数据上实现了最先进的性能。", "motivation": "现有研究表明在fMRI数据中建模高阶交互（HOIs）可以提高机器学习系统的诊断准确性，但有效提取和利用HOIs仍然是一个重大挑战。", "method": "本文提出了MvHo-IB，一个结合成对交互和高阶交互的多视图学习框架。其主要创新包括：1) 结合信息论中的O-信息和基于矩阵的Renyi alpha-阶熵估计器来量化和提取高阶交互；2) 使用专门构建的Brain3DCNN编码器来有效利用这些交互；3) 引入新的多视图学习信息瓶颈目标以增强表示学习。", "result": "在三个基准fMRI数据集上的实验表明，MvHo-IB实现了最先进的性能，显著优于以前的方法，包括最近基于超图的技术。", "conclusion": "MvHo-IB是一种有效且性能优越的脑疾病诊断方法，能够通过新颖的多视图学习框架更好地提取和利用fMRI数据中的高阶交互信息。", "translation": "最近的证据表明，在功能性磁共振成像（fMRI）数据中建模高阶交互（HOIs）可以提高机器学习系统的诊断准确性。然而，有效提取和利用HOIs仍然是一个重大挑战。在这项工作中，我们提出了MvHo-IB，一个新颖的多视图学习框架，它集成了成对交互和HOIs用于诊断决策，同时自动压缩与任务无关的冗余信息。MvHo-IB引入了几个关键创新：(1) 一种结合信息论中的O-信息和基于矩阵的Renyi alpha-阶熵估计器来量化和提取HOIs的原则性方法，(2) 一个专门构建的Brain3DCNN编码器来有效利用这些交互，以及 (3) 一个新的多视图学习信息瓶颈目标来增强表示学习。在三个基准fMRI数据集上的实验表明，MvHo-IB实现了最先进的性能，显著优于以前的方法，包括最近基于超图的技术。MvHo-IB的实现可在https://github.com/zky04/MvHo-IB获取。", "summary": "本文提出了MvHo-IB，一个用于脑疾病诊断的新型多视图学习框架。该框架旨在有效提取并利用fMRI数据中的高阶交互信息，同时通过信息瓶颈机制压缩冗余信息。MvHo-IB通过结合O-信息和Renyi alpha-阶熵量化HOIs，并使用Brain3DCNN编码器处理这些交互，以及引入新的多视图信息瓶颈目标来增强表示学习。实验结果表明，MvHo-IB在多个fMRI数据集上取得了最先进的诊断性能。", "keywords": "高阶交互, 信息瓶颈, 脑疾病诊断, 多视图学习, fMRI", "comments": "MvHo-IB的创新点在于其将高阶交互与信息瓶颈理论相结合，有效地处理了fMRI数据中的复杂关系并抑制了冗余信息。这种多视图学习方法为脑疾病诊断提供了新的视角，并显著提升了诊断精度，有望在临床应用中发挥重要作用。"}}
{"id": "2507.02743", "title": "Prompt learning with bounding box constraints for medical image segmentation", "authors": ["Mélanie Gaillochet", "Mehrdad Noori", "Sahar Dastani", "Christian Desrosiers", "Hervé Lombaert"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14 pages", "url": "http://arxiv.org/abs/2507.02743v1", "summary": "Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acquire-offer a practical alternative.\nVision foundation models have recently shown noteworthy segmentation\nperformance when provided with prompts such as points or bounding boxes. Prompt\nlearning exploits these models by adapting them to downstream tasks and\nautomating segmentation, thereby reducing user intervention. However, existing\nprompt learning approaches depend on fully annotated segmentation masks. This\npaper proposes a novel framework that combines the representational power of\nfoundation models with the annotation efficiency of weakly supervised\nsegmentation. More specifically, our approach automates prompt generation for\nfoundation models using only bounding box annotations. Our proposed\noptimization scheme integrates multiple constraints derived from box\nannotations with pseudo-labels generated by the prompted foundation model.\nExtensive experiments across multimodal datasets reveal that our weakly\nsupervised method achieves an average Dice score of 84.90% in a limited data\nsetting, outperforming existing fully-supervised and weakly-supervised\napproaches. The code is available at\nhttps://github.com/Minimel/box-prompt-learning-VFM.git", "comment": "Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14\n  pages", "pdf_url": "http://arxiv.org/pdf/2507.02743v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于边界框约束的医学图像分割提示学习", "tldr": "本文提出了一种结合基础模型和弱监督的医学图像分割框架，仅使用边界框标注自动生成提示，并在有限数据设置下取得了优异性能。", "motivation": "在医学领域，像素级标注获取费力且成本高昂。现有提示学习方法依赖于完全标注的分割掩码，这与减轻标注负担的目标相悖。因此，需要一种能利用弱监督标注（如边界框）的提示学习方法。", "method": "本文提出了一种新颖的框架，将基础模型的表示能力与弱监督分割的标注效率相结合。具体来说，该方法仅使用边界框标注来自动化基础模型的提示生成。提出的优化方案将来自边界框标注的多个约束与提示基础模型生成的伪标签相结合。", "result": "在多模态数据集上的大量实验表明，在有限数据设置下，该弱监督方法取得了平均84.90%的Dice分数，优于现有的全监督和弱监督方法。", "conclusion": "本文提出的弱监督方法，通过结合基础模型和边界框约束，有效解决了医学图像分割中像素级标注的难题，并在性能上超越了现有方法，证明了其在实际应用中的潜力。", "translation": "在医学领域，像素级标注的获取众所周知地费力且成本高昂。为了减轻这一负担，基于边界框标注（更容易获取）的弱监督方法提供了一个实用的替代方案。视觉基础模型最近在提供点或边界框等提示时，展现了显著的分割性能。提示学习通过使这些模型适应下游任务并自动化分割，从而减少用户干预，进而利用这些模型。然而，现有的提示学习方法依赖于完全标注的分割掩码。本文提出了一种新颖的框架，将基础模型的表示能力与弱监督分割的标注效率相结合。更具体地说，我们的方法仅使用边界框标注来自动化基础模型的提示生成。我们提出的优化方案将来自边界框标注的多个约束与提示基础模型生成的伪标签相结合。跨多模态数据集的广泛实验表明，我们的弱监督方法在有限数据设置下取得了平均84.90%的Dice分数，优于现有的全监督和弱监督方法。代码可在https://github.com/Minimel/box-prompt-learning-VFM.git获取。", "summary": "本文针对医学图像分割中像素级标注成本高昂的问题，提出了一种创新的弱监督提示学习框架。该方法利用边界框标注自动生成基础模型的提示，并通过结合边界框约束和伪标签进行优化。实验结果显示，在有限数据条件下，该方法在Dice分数上达到84.90%，优于现有的全监督和弱监督方法，有效减轻了标注负担并提升了分割性能。", "keywords": "医学图像分割, 提示学习, 边界框约束, 弱监督, 基础模型", "comments": "该论文的创新点在于将前沿的视觉基础模型与高效的弱监督学习范式相结合，解决了医学图像领域标注成本高昂的痛点。通过自动化边界框提示生成和引入多重约束优化，显著提升了医学图像分割的效率和性能，具有重要的实际应用价值。"}}
{"id": "2507.01964", "title": "Forecasting Nigerian Equity Stock Returns Using Long Short-Term Memory Technique", "authors": ["Adebola K. Ojo", "Ifechukwude Jude Okafor"], "categories": ["q-fin.ST", "cs.LG", "68T07"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.01964v1", "summary": "Investors and stock market analysts face major challenges in predicting stock\nreturns and making wise investment decisions. The predictability of equity\nstock returns can boost investor confidence, but it remains a difficult task.\nTo address this issue, a study was conducted using a Long Short-term Memory\n(LSTM) model to predict future stock market movements. The study used a\nhistorical dataset from the Nigerian Stock Exchange (NSE), which was cleaned\nand normalized to design the LSTM model. The model was evaluated using\nperformance metrics and compared with other deep learning models like\nArtificial and Convolutional Neural Networks (CNN). The experimental results\nshowed that the LSTM model can predict future stock market prices and returns\nwith over 90% accuracy when trained with a reliable dataset. The study\nconcludes that LSTM models can be useful in predicting financial\ntime-series-related problems if well-trained. Future studies should explore\ncombining LSTM models with other deep learning techniques like CNN to create\nhybrid models that mitigate the risks associated with relying on a single model\nfor future equity stock predictions.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.01964v1", "cate": "q-fin.ST", "date": "2025-05-27", "updated": "2025-05-27", "AI": {"title_translation": "运用长短期记忆技术预测尼日利亚股票收益", "tldr": "本研究使用LSTM模型预测尼日利亚股票收益，准确率超过90%，表明LSTM在金融时间序列预测中的有效性。", "motivation": "投资者和股市分析师在预测股票收益和做出明智投资决策方面面临挑战，股票收益的可预测性可以增强投资者信心，但仍很困难。", "method": "本研究使用长短期记忆 (LSTM) 模型来预测未来的股票市场走势。使用了来自尼日利亚证券交易所 (NSE) 的历史数据集，该数据集经过清洗和标准化。模型使用性能指标进行评估，并与人工神经网络 (ANN) 和卷积神经网络 (CNN) 等其他深度学习模型进行了比较。", "result": "实验结果表明，当使用可靠数据集进行训练时，LSTM模型能够以超过90%的准确率预测未来的股票市场价格和收益。", "conclusion": "LSTM模型如果训练得当，可有效预测金融时间序列相关问题。未来的研究应探索将LSTM模型与其他深度学习技术（如CNN）结合，创建混合模型，以减轻单一模型预测未来股票收益的风险。", "translation": "投资者和股市分析师在预测股票收益和做出明智投资方面临重大挑战。股票收益的可预测性可以增强投资者信心，但这仍然是一项艰巨的任务。为了解决这个问题，一项研究使用长短期记忆 (LSTM) 模型来预测未来的股票市场走势。该研究使用了来自尼日利亚证券交易所 (NSE) 的历史数据集，该数据集经过清洗和标准化以设计LSTM模型。该模型使用性能指标进行评估，并与人工神经网络 (ANN) 和卷积神经网络 (CNN) 等其他深度学习模型进行了比较。实验结果表明，当使用可靠数据集进行训练时，LSTM模型能够以超过90%的准确率预测未来的股票市场价格和收益。研究得出结论，如果训练得当，LSTM模型在预测金融时间序列相关问题方面可能很有用。未来的研究应探索将LSTM模型与其他深度学习技术（如CNN）结合，创建混合模型，以减轻依赖单一模型预测未来股票收益所带来的风险。", "summary": "本文旨在解决股票收益预测的挑战，通过使用长短期记忆 (LSTM) 模型预测尼日利亚股票市场的未来走势。研究利用尼日利亚证券交易所的历史数据对LSTM模型进行训练和评估，并将其与人工神经网络和卷积神经网络进行了比较。结果显示，LSTM模型在可靠数据集上训练后，能以超过90%的准确率预测股票价格和收益，证明了其在金融时间序列预测中的有效性。研究建议未来可探索LSTM与其他深度学习技术的混合模型。", "keywords": "股票预测, LSTM, 深度学习, 金融时间序列, 尼日利亚股市", "comments": "这项研究证明了LSTM模型在预测尼日利亚股票市场收益方面的潜力，超过90%的准确率是一个显著的结果。其创新之处在于将LSTM应用于特定区域（尼日利亚）的股票市场预测，并强调了数据可靠性的重要性。论文也提出了未来结合多种深度学习模型以提高鲁棒性的展望，这对于金融预测领域具有重要意义。"}}
{"id": "2507.02748", "title": "Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics", "authors": ["Alex Colagrande", "Paul Caillon", "Eva Feillet", "Alexandre Allauzen"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ECLR Workshop at ICCV 2025", "url": "http://arxiv.org/abs/2507.02748v1", "summary": "Transformers have become the de facto standard for a wide range of tasks,\nfrom image classification to physics simulations. Despite their impressive\nperformance, the quadratic complexity of standard Transformers in both memory\nand time with respect to the input length makes them impractical for processing\nhigh-resolution inputs. Therefore, several variants have been proposed, the\nmost successful relying on patchification, downsampling, or coarsening\ntechniques, often at the cost of losing the finest-scale details. In this work,\nwe take a different approach. Inspired by state-of-the-art techniques in\n$n$-body numerical simulations, we cast attention as an interaction problem\nbetween grid points. We introduce the Multipole Attention Neural Operator\n(MANO), which computes attention in a distance-based multiscale fashion. MANO\nmaintains, in each attention head, a global receptive field and achieves linear\ntime and memory complexity with respect to the number of grid points. Empirical\nresults on image classification and Darcy flows demonstrate that MANO rivals\nstate-of-the-art models such as ViT and Swin Transformer, while reducing\nruntime and peak memory usage by orders of magnitude. We open source our code\nfor reproducibility at https://github.com/AlexColagrande/MANO.", "comment": "Accepted at ECLR Workshop at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02748v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "线性注意力与全局上下文：一种用于视觉和物理的多极注意力机制", "tldr": "本文提出多极注意力神经算子（MANO），一种受N体模拟启发的线性注意力机制，旨在解决标准Transformer处理高分辨率输入的二次复杂度问题。MANO在图像分类和物理模拟任务上表现出与SOTA模型相当的性能，同时显著降低了运行时和内存消耗。", "motivation": "标准Transformer在处理高分辨率输入时，其二次复杂度（时间与内存）使其不切实际；现有变体常以牺牲最精细尺度细节为代价。", "method": "提出多极注意力神经算子（MANO），将注意力视为网格点间的相互作用，灵感来源于N体数值模拟。MANO在每个注意力头中保持全局感受野，并以基于距离的多尺度方式计算注意力，实现了关于网格点数量的线性时间和内存复杂度。", "result": "在图像分类和Darcy流任务上的经验结果表明，MANO与ViT和Swin Transformer等现有最先进模型性能相当，同时将运行时和峰值内存使用量降低了几个数量级。", "conclusion": "MANO提供了一种高效且性能卓越的替代方案，克服了标准Transformer的计算限制，适用于高分辨率视觉和物理任务。", "translation": "Transformer已成为从图像分类到物理模拟等广泛任务的实际标准。尽管它们性能卓越，但标准Transformer在处理输入长度方面的时间和内存上的二次复杂度使其在处理高分辨率输入时变得不切实际。因此，已经提出了几种变体，其中最成功的方法依赖于分块、下采样或粗化技术，但这通常以牺牲最精细尺度的细节为代价。在这项工作中，我们采取了一种不同的方法。受N体数值模拟中最新技术的启发，我们将注意力视为网格点之间的相互作用问题。我们引入了多极注意力神经算子（MANO），它以基于距离的多尺度方式计算注意力。MANO在每个注意力头中保持全局感受野，并实现关于网格点数量的线性时间和内存复杂度。在图像分类和达西流上的经验结果表明，MANO与ViT和Swin Transformer等现有最先进模型竞争，同时将运行时和峰值内存使用量降低了几个数量级。我们开源了我们的代码以供复现：https://github.com/AlexColagrande/MANO。", "summary": "本文提出了一种名为多极注意力神经算子（MANO）的新型线性注意力机制，旨在解决标准Transformer在处理高分辨率输入时面临的二次复杂度问题。MANO受N体模拟启发，将注意力建模为网格点间的相互作用，并通过基于距离的多尺度方法实现全局感受野和线性时间/内存复杂度。实验证明，MANO在图像分类和物理模拟任务上，性能可与ViT和Swin Transformer等SOTA模型媲美，同时显著降低了计算资源消耗。", "keywords": "线性注意力, 多极注意力, Transformer, N体模拟, 高分辨率处理", "comments": "该论文的创新点在于将N体模拟中的多极思想引入到Transformer的注意力机制中，成功地将二次复杂度降低到线性复杂度，同时保持了全局感受野和高分辨率处理能力。这对于需要处理大量数据或高分辨率输入的视觉和物理任务具有重要意义，克服了现有Transformer变体可能牺牲细节的问题。"}}
{"id": "2507.01970", "title": "News Sentiment Embeddings for Stock Price Forecasting", "authors": ["Ayaan Qayyum"], "categories": ["q-fin.ST", "cs.LG"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      12 pages, 11 figures", "url": "http://arxiv.org/abs/2507.01970v1", "summary": "This paper will discuss how headline data can be used to predict stock\nprices. The stock price in question is the SPDR S&P 500 ETF Trust, also known\nas SPY that tracks the performance of the largest 500 publicly traded\ncorporations in the United States. A key focus is to use news headlines from\nthe Wall Street Journal (WSJ) to predict the movement of stock prices on a\ndaily timescale with OpenAI-based text embedding models used to create vector\nencodings of each headline with principal component analysis (PCA) to exact the\nkey features. The challenge of this work is to capture the time-dependent and\ntime-independent, nuanced impacts of news on stock prices while handling\npotential lag effects and market noise. Financial and economic data were\ncollected to improve model performance; such sources include the U.S. Dollar\nIndex (DXY) and Treasury Interest Yields. Over 390 machine-learning inference\nmodels were trained. The preliminary results show that headline data embeddings\ngreatly benefit stock price prediction by at least 40% compared to training and\noptimizing a machine learning system without headline data embeddings.", "comment": "12 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.01970v1", "cate": "q-fin.ST", "date": "2025-06-19", "updated": "2025-06-19", "AI": {"title_translation": "新闻情感嵌入用于股票价格预测", "tldr": "本文利用OpenAI嵌入的华尔街日报新闻标题来预测SPY股票价格，显示预测准确性提升超过40%。", "motivation": "旨在探讨如何利用新闻标题数据预测股票价格，并解决新闻对股价的时间依赖和时间无关影响、滞后效应以及市场噪音等挑战。", "method": "使用华尔街日报(WSJ)新闻标题数据预测SPDR S&P 500 ETF Trust (SPY)的每日股价波动。采用基于OpenAI的文本嵌入模型创建标题的向量编码，并利用主成分分析(PCA)提取关键特征。此外，收集了美国美元指数(DXY)和国债收益率等金融经济数据以提高模型性能。训练了超过390个机器学习推理模型。", "result": "初步结果显示，与没有新闻标题数据嵌入的机器学习系统相比，标题数据嵌入使股票价格预测的准确性至少提高了40%。", "conclusion": "新闻情感嵌入对股票价格预测具有显著的益处，能够大幅提升预测性能。", "translation": "本文将讨论如何利用新闻标题数据预测股票价格。所讨论的股票价格是SPDR S&P 500 ETF Trust，也称为SPY，它追踪美国500家最大上市公司的表现。一个关键的重点是利用《华尔街日报》(WSJ)的新闻标题，在每日时间尺度上预测股票价格的变动，使用基于OpenAI的文本嵌入模型创建每个标题的向量编码，并使用主成分分析(PCA)提取关键特征。这项工作的挑战在于捕捉新闻对股票价格的时间依赖和时间无关的细微影响，同时处理潜在的滞后效应和市场噪音。收集了金融和经济数据以改善模型性能；这些来源包括美国美元指数(DXY)和国债收益率。训练了超过390个机器学习推理模型。初步结果显示，与不使用新闻标题数据嵌入的机器学习系统相比，新闻标题数据嵌入极大地有利于股票价格预测，至少提高了40%。", "summary": "本文探讨了利用《华尔街日报》新闻标题的情感嵌入来预测SPDR S&P 500 ETF Trust (SPY)的股价。研究采用基于OpenAI的文本嵌入模型生成标题的向量编码，并通过主成分分析(PCA)提取关键特征，同时整合了美元指数和国债收益率等金融经济数据。研究训练了超过390个机器学习模型，结果表明，新闻标题数据嵌入能够使股票价格预测的准确性提高至少40%。", "keywords": "新闻情感, 股票价格预测, 文本嵌入, SPY, 机器学习", "comments": "这项研究的创新之处在于将先进的OpenAI文本嵌入模型与PCA相结合，用于新闻情感分析，进而预测股票价格。其重要性体现在引入新闻标题数据后，股票价格预测性能获得了显著提升（至少40%）。潜在的局限性可能包括对特定嵌入模型的依赖性以及该方法在不同市场或新闻源上的泛化能力。"}}
{"id": "2507.02751", "title": "Partial Weakly-Supervised Oriented Object Detection", "authors": ["Mingxin Liu", "Peiyuan Zhang", "Yuan Liu", "Wei Zhang", "Yue Zhou", "Ning Liao", "Ziyang Gong", "Junwei Luo", "Zhirui Wang", "Yi Yu", "Xue Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, 4 tables, source code: this https URL", "url": "http://arxiv.org/abs/2507.02751v1", "summary": "The growing demand for oriented object detection (OOD) across various domains\nhas driven significant research in this area. However, the high cost of dataset\nannotation remains a major concern. Current mainstream OOD algorithms can be\nmainly categorized into three types: (1) fully supervised methods using\ncomplete oriented bounding box (OBB) annotations, (2) semi-supervised methods\nusing partial OBB annotations, and (3) weakly supervised methods using weak\nannotations such as horizontal boxes or points. However, these algorithms\ninevitably increase the cost of models in terms of annotation speed or\nannotation cost. To address this issue, we propose:(1) the first Partial\nWeakly-Supervised Oriented Object Detection (PWOOD) framework based on\npartially weak annotations (horizontal boxes or single points), which can\nefficiently leverage large amounts of unlabeled data, significantly\noutperforming weakly supervised algorithms trained with partially weak\nannotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware\nStudent (OS-Student) model capable of learning orientation and scale\ninformation with only a small amount of orientation-agnostic or scale-agnostic\nweak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)\nto reduce the model's sensitivity to static filtering thresholds. Comprehensive\nexperiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD\nframework performs comparably to, or even surpasses, traditional\nsemi-supervised algorithms.", "comment": "10 pages, 5 figures, 4 tables, source code:\n  https://github.com/VisionXLab/PWOOD", "pdf_url": "http://arxiv.org/pdf/2507.02751v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "部分弱监督定向目标检测", "tldr": "本文提出了PWOOD框架，一个基于部分弱标注（水平框或单点）的定向目标检测方法，旨在降低标注成本并有效利用未标注数据，其性能可与传统半监督算法媲美甚至超越。", "motivation": "定向目标检测（OOD）的需求日益增长，但数据集标注成本高昂是主要问题。现有主流OOD算法（全监督、半监督、弱监督）都不可避免地增加了模型在标注速度或标注成本方面的开销。", "method": "本文提出了首个部分弱监督定向目标检测（PWOOD）框架，该框架基于部分弱标注（水平框或单点）。PWOOD包含：1) 定向和尺度感知学生（OS-Student）模型，能够仅用少量与定向无关或与尺度无关的弱标注学习定向和尺度信息；2) 类别无关伪标签过滤（CPF）策略，以减少模型对静态过滤阈值的敏感性。", "result": "在DOTA-v1.0/v1.5/v2.0和DIOR数据集上的综合实验表明，PWOOD框架的性能与传统半监督算法相当，甚至超越它们。", "conclusion": "PWOOD框架通过利用部分弱标注，显著降低了定向目标检测的标注成本，同时在性能上达到了与传统半监督算法相当甚至超越的水平，为OOD领域提供了一个高效且低成本的解决方案。", "translation": "定向目标检测（OOD）在各个领域日益增长的需求推动了该领域的重大研究。然而，数据集标注的高成本仍然是一个主要问题。当前主流的OOD算法主要分为三类：(1) 使用完整定向边界框（OBB）标注的全监督方法，(2) 使用部分OBB标注的半监督方法，以及(3) 使用水平框或点等弱标注的弱监督方法。然而，这些算法不可避免地增加了模型在标注速度或标注成本方面的开销。为了解决这个问题，我们提出：(1) 首个基于部分弱标注（水平框或单点）的部分弱监督定向目标检测（PWOOD）框架，该框架可以有效利用大量未标注数据，显著优于使用部分弱标注训练的弱监督算法，并且提供了一种更低成本的解决方案；(2) 定向和尺度感知学生（OS-Student）模型，能够仅用少量与定向无关或与尺度无关的弱标注学习定向和尺度信息；以及(3) 类别无关伪标签过滤（CPF）策略，以减少模型对静态过滤阈值的敏感性。在DOTA-v1.0/v1.5/v2.0和DIOR数据集上的综合实验表明，我们的PWOOD框架的性能与传统半监督算法相当，甚至超越它们。", "summary": "本文提出了PWOOD（部分弱监督定向目标检测）框架，旨在解决定向目标检测中高昂的数据标注成本问题。该框架创新性地利用水平框或单点等部分弱标注来训练模型，并有效利用大量未标注数据。PWOOD包含OS-Student模型用于学习方向和尺度信息，以及CPF策略来优化伪标签过滤。实验证明，PWOOD在多个标准数据集上的表现可与传统半监督方法媲美甚至超越，显著降低了标注成本。", "keywords": "定向目标检测, 部分弱监督, 标注成本, 水平框, 伪标签", "comments": "PWOOD框架的创新之处在于首次提出了部分弱监督的概念，有效地结合了弱监督和半监督的优点，在降低标注成本的同时保持了高检测性能。其提出的OS-Student模型和CPF策略是实现这一目标的关键组成部分，对于解决实际应用中数据标注瓶颈具有重要意义。"}}
{"id": "2507.02792", "title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation", "authors": ["Liheng Zhang", "Lexi Pang", "Hang Ye", "Xiaoxuan Ma", "Yizhou Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02792v1", "summary": "Text-to-image (T2I) diffusion models have shown remarkable success in\ngenerating high-quality images from text prompts. Recent efforts extend these\nmodels to incorporate conditional images (e.g., depth or pose maps) for\nfine-grained spatial control. Among them, feature injection methods have\nemerged as a training-free alternative to traditional fine-tuning approaches.\nHowever, they often suffer from structural misalignment, condition leakage, and\nvisual artifacts, especially when the condition image diverges significantly\nfrom natural RGB distributions. By revisiting existing methods, we identify a\ncore limitation: the synchronous injection of condition features fails to\naccount for the trade-off between domain alignment and structural preservation\nduring denoising. Inspired by this observation, we propose a flexible feature\ninjection framework that decouples the injection timestep from the denoising\nprocess. At its core is a structure-rich injection module, which enables the\nmodel to better adapt to the evolving interplay between alignment and structure\npreservation throughout the diffusion steps, resulting in more faithful\nstructural generation. In addition, we introduce appearance-rich prompting and\na restart refinement strategy to further enhance appearance control and visual\nquality. Together, these designs enable training-free generation that is both\nstructure-rich and appearance-rich. Extensive experiments show that our\napproach achieves state-of-the-art performance across diverse zero-shot\nconditioning scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02792v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RichControl：结构与外观丰富、免训练的文本到图像生成空间控制", "tldr": "RichControl通过解耦特征注入时间步和去噪过程，实现结构和外观丰富的免训练文本到图像空间控制，解决现有方法的结构错位和伪影问题。", "motivation": "现有的文本到图像（T2I）扩散模型在结合条件图像进行精细空间控制时，特别是免训练的特征注入方法，常出现结构错位、条件泄露和视觉伪影，尤其当条件图像与自然RGB分布差异大时。核心限制在于同步注入条件特征未能兼顾去噪过程中的域对齐和结构保持之间的权衡。", "method": "提出一个灵活的特征注入框架，将注入时间步与去噪过程解耦。其核心是一个结构丰富的注入模块，使模型能更好地适应扩散步骤中对齐和结构保持的演变。此外，引入了外观丰富的提示（appearance-rich prompting）和重启细化策略（restart refinement strategy）来进一步增强外观控制和视觉质量。", "result": "大量实验表明，该方法在各种零样本条件生成场景中实现了最先进的性能。", "conclusion": "RichControl通过创新的特征注入框架和辅助策略，实现了高质量、结构和外观丰富的免训练文本到图像生成，有效解决了现有方法的局限性。", "translation": "文本到图像（T2I）扩散模型在从文本提示生成高质量图像方面取得了显著成功。最近的工作扩展了这些模型，以结合条件图像（例如，深度或姿态图）进行精细的空间控制。其中，特征注入方法已成为传统微调方法的免训练替代方案。然而，它们常常遭受结构错位、条件泄露和视觉伪影的困扰，特别是当条件图像与自然RGB分布显著不同时。通过重新审视现有方法，我们发现了一个核心局限性：条件特征的同步注入未能考虑去噪过程中域对齐和结构保持之间的权衡。受此观察启发，我们提出了一个灵活的特征注入框架，该框架将注入时间步与去噪过程解耦。其核心是一个结构丰富的注入模块，它使模型能够更好地适应在扩散步骤中对齐和结构保持之间不断演变的相互作用，从而产生更忠实的结构生成。此外，我们引入了外观丰富的提示和重启细化策略，以进一步增强外观控制和视觉质量。总而言之，这些设计使得免训练生成既结构丰富又外观丰富。大量实验表明，我们的方法在各种零样本条件生成场景中实现了最先进的性能。", "summary": "本文提出了RichControl，一个用于文本到图像生成的免训练空间控制框架。针对现有特征注入方法在结构对齐、条件泄露和伪影方面的不足，RichControl通过解耦特征注入时间步与去噪过程，并引入结构丰富的注入模块、外观丰富的提示以及重启细化策略，实现了对图像结构和外观的精确控制。实验证明，该方法在零样本条件下达到了最先进的性能。", "keywords": "文本到图像生成, 空间控制, 特征注入, 扩散模型, 免训练", "comments": "这篇论文的创新点在于它识别并解决了现有特征注入方法中同步注入的局限性，通过解耦注入时间步和去噪过程，实现了更灵活和有效的结构保持。其免训练的特性对于实际应用具有重要价值，因为它避免了耗时且资源密集型的模型微调。结构和外观控制的分离处理也显示了对T2I生成中复杂挑战的深入理解。"}}
{"id": "2507.02798", "title": "No time to train! Training-Free Reference-Based Instance Segmentation", "authors": ["Miguel Espinosa", "Chenhongyi Yang", "Linus Ericsson", "Steven McDonagh", "Elliot J. Crowley"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.02798v1", "summary": "The performance of image segmentation models has historically been\nconstrained by the high cost of collecting large-scale annotated data. The\nSegment Anything Model (SAM) alleviates this original problem through a\npromptable, semantics-agnostic, segmentation paradigm and yet still requires\nmanual visual-prompts or complex domain-dependent prompt-generation rules to\nprocess a new image. Towards reducing this new burden, our work investigates\nthe task of object segmentation when provided with, alternatively, only a small\nset of reference images. Our key insight is to leverage strong semantic priors,\nas learned by foundation models, to identify corresponding regions between a\nreference and a target image. We find that correspondences enable automatic\ngeneration of instance-level segmentation masks for downstream tasks and\ninstantiate our ideas via a multi-stage, training-free method incorporating (1)\nmemory bank construction; (2) representation aggregation and (3) semantic-aware\nfeature matching. Our experiments show significant improvements on segmentation\nmetrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),\nPASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free\napproaches on the Cross-Domain FSOD benchmark (22.4% nAP).", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.02798v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "没时间训练！免训练的基于参照的实例分割", "tldr": "本文提出了一种免训练的基于参照的实例分割方法，利用基础模型学习到的强大语义先验来自动生成实例级分割掩码，并在多个基准测试上取得了最先进的性能。", "motivation": "图像分割模型通常需要大量标注数据，而像SAM这样的模型虽然缓解了这一问题，但仍需手动或复杂的提示。本文旨在通过仅提供少量参考图像来减少这种新的标注负担，实现目标分割。", "method": "本文提出了一种多阶段、免训练的方法，包括：(1) 记忆库构建；(2) 表示聚合；(3) 语义感知特征匹配。该方法利用基础模型学习到的强大语义先验来识别参考图像和目标图像之间的对应区域，从而自动生成实例级分割掩码。", "result": "在分割指标上取得了显著改进，在COCO FSOD上达到了36.8% nAP的最先进性能，在PASCAL VOC Few-Shot上达到了71.2% nAP50，并且在跨领域FSOD基准测试中以22.4% nAP的成绩优于现有免训练方法。", "conclusion": "本文提出的免训练的基于参照的实例分割方法，通过利用基础模型的语义先验，有效地解决了有限监督下的实例分割挑战，并取得了高性能。", "translation": "图像分割模型的性能历来受到收集大规模标注数据高成本的限制。Segment Anything Model (SAM) 通过可提示的、语义无关的分割范式缓解了这一原始问题，但仍需要手动视觉提示或复杂的领域相关提示生成规则来处理新图像。为了减轻这一新负担，我们的工作研究了在仅提供少量参考图像的情况下进行目标分割的任务。我们的关键见解是利用基础模型学习到的强大语义先验，以识别参考图像和目标图像之间的对应区域。我们发现对应关系能够为下游任务自动生成实例级分割掩码，并通过一个多阶段、免训练的方法实例化了我们的想法，该方法包括 (1) 记忆库构建；(2) 表示聚合和 (3) 语义感知特征匹配。我们的实验表明，在分割指标上取得了显著改进，在COCO FSOD上达到了最先进的性能 (36.8% nAP)，在PASCAL VOC Few-Shot上达到了 (71.2% nAP50)，并且在跨领域FSOD基准测试中优于现有的免训练方法 (22.4% nAP)。", "summary": "本文提出了一种新颖的免训练实例分割方法，该方法仅依赖少量参考图像，而非大量标注数据或手动提示。它利用基础模型中的语义先验来建立参考图像与目标图像之间的对应关系，从而自动生成实例级分割掩码。该方法包含记忆库构建、表示聚合和语义感知特征匹配等阶段。实验结果表明，该方法在COCO FSOD、PASCAL VOC Few-Shot和跨领域FSOD基准测试上均达到了最先进的性能，有效减轻了标注负担。", "keywords": "实例分割, 免训练, 基于参照, 语义先验, 基础模型", "comments": "本文为图像分割领域的数据标注瓶颈提供了一个创新性解决方案，提出了一种完全免训练的方法。其依赖于基础模型的强大语义先验和基于参照的范式，对于少样本和跨领域场景而言是一个显著的进步。能够在不进行训练的情况下实现最先进的性能是其一个显著的优势。"}}
{"id": "2507.02345", "title": "HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3", "authors": ["Jie Gao", "Jing Hu", "Shanzhuo Zhang", "Kunrui Zhu", "Sheng Qian", "Yueyang Huang", "Xiaonan Zhang", "Xiaomin Fang"], "categories": ["q-bio.BM", "cs.AI"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02345v1", "summary": "Antibody engineering is essential for developing therapeutics and advancing\nbiomedical research. Traditional discovery methods often rely on time-consuming\nand resource-intensive experimental screening. To enhance and streamline this\nprocess, we introduce a production-grade, high-throughput platform built on\nHelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure\nprediction model, HelixFold3. The platform facilitates the large-scale\ngeneration of antibody candidate sequences and evaluates their interaction with\nantigens. Integrated high-performance computing (HPC) support enables\nhigh-throughput screening, addressing challenges such as fragmented toolchains\nand high computational demands. Validation on multiple antigens showcases the\nplatform's ability to generate diverse and high-quality antibodies, confirming\na scaling law where exploring larger sequence spaces increases the likelihood\nof identifying optimal binders. This platform provides a seamless, accessible\nsolution for large-scale antibody design and is available via the antibody\ndesign page of PaddleHelix platform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02345v1", "cate": "q-bio.BM", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "HelixDesign-Antibody：一个基于HelixFold3的可扩展生产级抗体设计平台", "tldr": "HelixDesign-Antibody是一个基于HelixFold3的高通量、生产级平台，用于大规模抗体设计和抗原相互作用评估，解决了传统方法的耗时和计算挑战。", "motivation": "传统的抗体发现方法耗时且资源密集，依赖实验筛选。为增强和简化这一过程，需要一个高效、高通量的平台。", "method": "本文介绍了HelixDesign-Antibody平台，它建立在HelixFold3高精度结构预测模型之上。该平台能够大规模生成抗体候选序列并评估其与抗原的相互作用，并集成了高性能计算（HPC）支持以实现高通量筛选。", "result": "该平台在多种抗原上进行了验证，展示了其生成多样化高质量抗体的能力，并证实了探索更大序列空间会增加识别最佳结合物可能性的缩放定律。", "conclusion": "HelixDesign-Antibody提供了一个无缝、易于访问的大规模抗体设计解决方案，并通过PaddleHelix平台的抗体设计页面提供。", "translation": "抗体工程对于开发治疗药物和推进生物医学研究至关重要。传统的发现方法通常依赖耗时且资源密集型的实验筛选。为了增强和简化这一过程，我们引入了一个基于HelixFold3的生产级高通量平台——HelixDesign-Antibody，它利用了高精度结构预测模型HelixFold3。该平台促进了抗体候选序列的大规模生成，并评估它们与抗原的相互作用。集成的高性能计算（HPC）支持实现了高通量筛选，解决了工具链碎片化和高计算需求等挑战。在多种抗原上的验证展示了该平台生成多样化和高质量抗体的能力，证实了探索更大序列空间会增加识别最佳结合物可能性的缩放定律。该平台为大规模抗体设计提供了一个无缝、易于访问的解决方案，并通过PaddleHelix平台的抗体设计页面提供。", "summary": "HelixDesign-Antibody是一个基于高精度结构预测模型HelixFold3的可扩展、生产级高通量平台，旨在解决传统抗体发现方法耗时和计算资源大的问题。它能够大规模生成抗体候选序列并评估其与抗原的相互作用，通过集成高性能计算实现高效筛选。该平台已验证能生成多样化高质量抗体，并支持“探索更大序列空间可提高最佳结合物识别率”的缩放定律，为大规模抗体设计提供了便捷的解决方案。", "keywords": "抗体设计, HelixFold3, 高通量筛选, 结构预测, 生产级平台", "comments": "该论文介绍的HelixDesign-Antibody平台通过结合先进的结构预测模型（HelixFold3）和高性能计算，显著提升了抗体设计的效率和规模。其创新点在于将高精度预测与高通量筛选相结合，解决了传统方法的瓶颈。该平台的“生产级”定位和“缩放定律”的验证，表明其在实际应用和未来研究中具有重要价值，尤其是在加速药物发现方面。"}}
{"id": "2507.01980", "title": "Detecting Fraud in Financial Networks: A Semi-Supervised GNN Approach with Granger-Causal Explanations", "authors": ["Linh Nguyen", "Marcel Boersma", "Erman Acar"], "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01980v1", "summary": "Fraudulent activity in the financial industry costs billions annually.\nDetecting fraud, therefore, is an essential yet technically challenging task\nthat requires carefully analyzing large volumes of data. While machine learning\n(ML) approaches seem like a viable solution, applying them successfully is not\nso easy due to two main challenges: (1) the sparsely labeled data, which makes\nthe training of such approaches challenging (with inherent labeling costs), and\n(2) lack of explainability for the flagged items posed by the opacity of ML\nmodels, that is often required by business regulations. This article proposes\nSAGE-FIN, a semi-supervised graph neural network (GNN) based approach with\nGranger causal explanations for Financial Interaction Networks. SAGE-FIN learns\nto flag fraudulent items based on weakly labeled (or unlabelled) data points.\nTo adhere to regulatory requirements, the flagged items are explained by\nhighlighting related items in the network using Granger causality. We\nempirically validate the favorable performance of SAGE-FIN on a real-world\ndataset, Bipartite Edge-And-Node Attributed financial network (Elliptic++),\nwith Granger-causal explanations for the identified fraudulent items without\nany prior assumption on the network structure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01980v1", "cate": "q-fin.ST", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "金融网络中的欺诈检测：一种基于格兰杰因果解释的半监督图神经网络方法", "tldr": "本文提出了SAGE-FIN，一个半监督图神经网络（GNN）方法，用于金融欺诈检测，旨在解决数据标记稀疏和模型可解释性不足的问题。SAGE-FIN利用格兰杰因果关系提供欺诈行为的解释，并在真实世界数据集中验证了其有效性和解释能力。", "motivation": "金融行业的欺诈活动每年造成数十亿美元的损失，因此欺诈检测至关重要但技术上具有挑战性。现有机器学习方法在应用于金融欺诈检测时面临两大挑战：一是数据标记稀疏性导致训练困难和高昂的标记成本；二是模型缺乏可解释性，这不符合商业法规要求。", "method": "本文提出了SAGE-FIN，一种基于半监督图神经网络（GNN）的方法，结合格兰杰因果解释，用于金融交互网络。SAGE-FIN能够根据弱标记（或未标记）数据点识别欺诈项。为满足监管要求，SAGE-FIN通过使用格兰杰因果关系突出网络中相关项来解释被标记的欺诈项。", "result": "SAGE-FIN在真实世界数据集（Bipartite Edge-And-Node Attributed financial network (Elliptic++)）上表现出良好的性能，并且能够为识别出的欺诈项提供格兰杰因果解释，无需对网络结构进行任何先验假设。", "conclusion": "SAGE-FIN成功地解决了金融欺诈检测中标记数据稀疏和模型可解释性不足的关键挑战。通过结合半监督GNN和格兰杰因果解释，该方法在真实数据集上实现了有效的欺诈检测和符合监管要求的解释性。", "translation": "金融行业的欺诈活动每年造成数十亿美元的损失。因此，检测欺诈是一项重要但技术上具有挑战性的任务，需要仔细分析大量数据。虽然机器学习（ML）方法似乎是可行的解决方案，但由于两个主要挑战，成功应用它们并不容易：（1）标记数据稀疏，这使得此类方法的训练具有挑战性（并伴随着固有的标记成本），以及（2）ML模型的不透明性导致标记项缺乏可解释性，而这通常是商业法规所要求的。本文提出了SAGE-FIN，一种基于半监督图神经网络（GNN）的方法，结合格兰杰因果解释，用于金融交互网络。SAGE-FIN学习根据弱标记（或未标记）数据点标记欺诈项。为了遵守监管要求，通过使用格兰杰因果关系突出网络中的相关项来解释被标记的项。我们通过实验验证了SAGE-FIN在真实世界数据集（二分边节点属性金融网络（Elliptic++））上的良好性能，并为识别出的欺诈项提供了格兰杰因果解释，而无需对网络结构进行任何先验假设。", "summary": "本文提出SAGE-FIN，一种创新的半监督图神经网络方法，专门用于金融网络中的欺诈检测。SAGE-FIN旨在解决现有机器学习方法在金融欺诈检测中面临的标记数据稀疏和模型可解释性不足的挑战。该方法能够利用弱标记或未标记数据进行学习，并通过整合格兰杰因果解释来提供对欺诈行为的可解释性，以满足监管要求。在真实世界金融数据集上的实验验证了SAGE-FIN的有效性能及其提供可解释性欺诈检测的能力。", "keywords": "金融欺诈检测, 半监督学习, 图神经网络, 格兰杰因果关系, 可解释性AI", "comments": "SAGE-FIN的创新之处在于其结合了半监督GNN来处理金融欺诈数据中常见的标记数据稀疏问题，并通过引入格兰杰因果解释来解决机器学习模型普遍存在的“黑箱”问题。后者对于金融行业的合规性和信任建立至关重要。该方法在真实数据集上的实证验证进一步凸显了其在实际应用中的潜力和实用性。"}}
{"id": "2507.02813", "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion", "authors": ["Fangfu Liu", "Hao Li", "Jiawei Chi", "Hanyang Wang", "Minghui Yang", "Fudong Wang", "Yueqi Duan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.02813v1", "summary": "Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.", "comment": "Project page: https://liuff19.github.io/LangScene-X", "pdf_url": "http://arxiv.org/pdf/2507.02813v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LangScene-X: 使用TriMap视频扩散重建可泛化的3D语言嵌入场景", "tldr": "LangScene-X是一个生成框架，利用TriMap视频扩散模型和语言量化压缩器，从稀疏视图重建可泛化的3D语言嵌入场景，并在质量和泛化能力上优于现有方法。", "motivation": "从2D图像中恢复具有开放词汇场景理解的3D结构是一项基础但艰巨的任务。现有方法依赖于校准的密集视图重建，导致在视图有限时出现严重的渲染伪影和不合理的语义合成。", "method": "LangScene-X框架统一并生成3D一致的多模态信息；首先训练一个TriMap视频扩散模型，通过渐进式知识集成从稀疏输入生成外观、几何和语义；接着提出一个语言量化压缩器（LQC），在大规模图像数据集上训练，以高效编码语言嵌入，实现跨场景泛化；最后通过将语言信息对齐到3D场景表面来重建语言表面场，支持开放式语言查询。", "result": "在真实世界数据上的大量实验证明，LangScene-X在质量和泛化能力方面优于现有最先进的方法。", "conclusion": "LangScene-X成功地解决了从稀疏视图重建3D语言嵌入场景的挑战，通过其新颖的生成框架和组件，实现了高质量和高泛化能力的场景重建，超越了现有方法。", "translation": "从2D图像中恢复具有开放词汇场景理解的3D结构是一项基础但艰巨的任务。最近的发展通过对嵌入语言信息的每场景优化实现了这一点。然而，它们严重依赖于校准的密集视图重建范式，因此在可用视图有限时会遭受严重的渲染伪影和不合理的语义合成。在本文中，我们引入了一种新颖的生成框架，命名为LangScene-X，用于统一和生成3D一致的多模态信息，以进行重建和理解。凭借创建更一致的新颖观测的生成能力，我们可以仅从稀疏视图构建可泛化的3D语言嵌入场景。具体来说，我们首先训练一个TriMap视频扩散模型，该模型可以通过渐进式知识集成从稀疏输入生成外观（RGB）、几何（法线）和语义（分割图）。此外，我们提出了一种语言量化压缩器（LQC），在大规模图像数据集上进行训练，以有效地编码语言嵌入，从而实现跨场景泛化而无需每场景重新训练。最后，我们通过将语言信息对齐到3D场景的表面上来重建语言表面场，从而实现开放式语言查询。在真实世界数据上的大量实验证明了我们的LangScene-X在质量和泛化能力方面优于现有最先进的方法。项目页面：https://liuff19.github.io/LangScene-X。", "summary": "LangScene-X是一种新颖的生成框架，旨在解决从稀疏2D图像重建可泛化3D语言嵌入场景的挑战。它通过训练TriMap视频扩散模型来生成多模态信息（RGB、法线、分割图），并引入语言量化压缩器（LQC）以实现高效的语言嵌入和跨场景泛化。该方法通过将语言信息对齐到3D场景表面来支持开放式语言查询，并在实验中表现出优于现有方法的质量和泛化能力。", "keywords": "3D场景重建, 语言嵌入, 视频扩散, 泛化能力, 稀疏视图", "comments": "这篇论文提出了一种新颖的生成方法来重建3D语言嵌入场景，解决了现有每场景优化方法（特别是对密集视图的依赖）的局限性。其创新之处在于使用TriMap视频扩散模型进行多模态生成，以及使用语言量化压缩器实现高效的跨场景泛化，这对于从稀疏输入构建可泛化的3D场景至关重要。这项工作显著推动了具有开放词汇场景理解的3D重建领域。"}}
{"id": "2507.01987", "title": "Predicting and Explaining Customer Data Sharing in the Open Banking", "authors": ["João B. G. de Brito", "Rodrigo Heldt", "Cleo S. Silveira", "Matthias Bogaert", "Guilherme B. Bucco", "Fernando B. Luce", "João L. Becker", "Filipe J. Zabala", "Michel J. Anzanello"], "categories": ["q-fin.GN", "cs.LG"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01987v1", "summary": "The emergence of Open Banking represents a significant shift in financial\ndata management, influencing financial institutions' market dynamics and\nmarketing strategies. This increased competition creates opportunities and\nchallenges, as institutions manage data inflow to improve products and services\nwhile mitigating data outflow that could aid competitors. This study introduces\na framework to predict customers' propensity to share data via Open Banking and\ninterprets this behavior through Explanatory Model Analysis (EMA). Using data\nfrom a large Brazilian financial institution with approximately 3.2 million\ncustomers, a hybrid data balancing strategy incorporating ADASYN and NEARMISS\ntechniques was employed to address the infrequency of data sharing and enhance\nthe training of XGBoost models. These models accurately predicted customer data\nsharing, achieving 91.39% accuracy for inflow and 91.53% for outflow. The EMA\nphase combined the Shapley Additive Explanations (SHAP) method with the\nClassification and Regression Tree (CART) technique, revealing the most\ninfluential features on customer decisions. Key features included the number of\ntransactions and purchases in mobile channels, interactions within these\nchannels, and credit-related features, particularly credit card usage across\nthe national banking system. These results highlight the critical role of\nmobile engagement and credit in driving customer data-sharing behaviors,\nproviding financial institutions with strategic insights to enhance\ncompetitiveness and innovation in the Open Banking environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01987v1", "cate": "q-fin.GN", "date": "2025-06-28", "updated": "2025-06-28", "AI": {"title_translation": "在开放银行中预测和解释客户数据共享", "tldr": "研究在开放银行中预测和解释客户数据共享行为，使用XGBoost模型和SHAP/CART解释，发现移动参与度和信用是关键影响因素。", "motivation": "开放银行背景下，金融机构需要管理数据流入以改进服务，同时控制数据流出以避免竞争劣势。理解客户数据共享行为对于增强竞争力至关重要。", "method": "本研究引入了一个框架来预测客户数据共享倾向并解释其行为。使用来自巴西一家大型金融机构约320万客户的数据，采用ADASYN和NEARMISS混合数据平衡策略处理数据共享不频繁的问题，并训练XGBoost模型进行预测。解释性模型分析（EMA）阶段结合Shapley Additive Explanations (SHAP) 方法与分类回归树 (CART) 技术，揭示了对客户决策影响最大的特征。", "result": "XGBoost模型准确预测了客户数据共享，流入准确率达91.39%，流出准确率达91.53%。EMA揭示了影响客户决策的最关键特征，包括移动渠道的交易和购买数量、这些渠道的互动以及与信用相关的特征，特别是全国银行系统中的信用卡使用。", "conclusion": "移动参与度和信用在驱动客户数据共享行为中扮演关键角色，为金融机构在开放银行环境中提升竞争力和创新提供了战略性见解。", "translation": "开放银行的出现代表着金融数据管理领域的重大转变，影响着金融机构的市场动态和营销策略。这种日益激烈的竞争既带来了机遇也带来了挑战，因为机构需要管理数据流入以改进产品和服务，同时减轻可能帮助竞争对手的数据流出。本研究引入了一个框架，用于预测客户通过开放银行共享数据的倾向，并通过解释性模型分析（EMA）来解释这种行为。研究使用了来自一家拥有约320万客户的巴西大型金融机构的数据，并采用了结合ADASYN和NEARMISS技术的混合数据平衡策略，以解决数据共享不频繁的问题，并增强XGBoost模型的训练。这些模型准确预测了客户数据共享，流入准确率达到91.39%，流出准确率达到91.53%。EMA阶段将Shapley Additive Explanations (SHAP) 方法与分类回归树 (CART) 技术相结合，揭示了对客户决策影响最大的特征。关键特征包括移动渠道的交易和购买数量、这些渠道内的互动以及与信用相关的特征，特别是全国银行系统中的信用卡使用情况。这些结果突出了移动参与度和信用在驱动客户数据共享行为中的关键作用，为金融机构在开放银行环境中提升竞争力和创新提供了战略性见解。", "summary": "本研究提出了一个框架，用于预测和解释客户在开放银行中共享数据的行为。利用巴西一家大型金融机构的320万客户数据，采用混合数据平衡策略和XGBoost模型实现了对数据流入和流出91%以上的准确预测。通过SHAP和CART的解释性模型分析，发现移动渠道的互动、交易量以及信用相关特征（尤其是信用卡使用）是影响客户数据共享决策的关键因素。研究结果为金融机构在开放银行背景下提升竞争力和创新提供了重要洞察。", "keywords": "开放银行, 客户数据共享, XGBoost, SHAP", "comments": "本文的创新之处在于结合预测模型（XGBoost）和解释性模型分析（SHAP/CART）来理解客户数据共享行为，特别是在开放银行这一新兴且关键的领域。其应用混合数据平衡策略处理不平衡数据是实证研究的亮点。研究结果不仅提供了高精度的预测，更重要的是揭示了影响客户决策的关键因素，为金融机构制定精准的市场策略和风险管理提供了有价值的指导。"}}
{"id": "2507.02826", "title": "Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach", "authors": ["Panpan Ji", "Junni Song", "Hang Xiao", "Hanyu Liu", "Chao Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02826v1", "summary": "Sensor-based Human Activity Recognition (HAR) is a core technology that\nenables intelligent systems to perceive and interact with their environment.\nHowever, multimodal HAR systems still encounter key challenges, such as\ndifficulties in cross-modal feature alignment and imbalanced modality\ncontributions. To address these issues, we propose a novel framework called the\nDynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three\nkey components. First, a dual-path feature extraction architecture is employed,\nwhere ResNet and DenseNet branches collaboratively process multimodal sensor\ndata. Second, a multi-stage contrastive learning mechanism is introduced to\nachieve progressive alignment from local perception to semantic abstraction.\nThird, we present a confidence-driven gradient modulation strategy that\ndynamically monitors and adjusts the learning intensity of each modality branch\nduring backpropagation, effectively alleviating modality competition. In\naddition, a momentum-based gradient accumulation strategy is adopted to enhance\ntraining stability. We conduct ablation studies to validate the effectiveness\nof each component and perform extensive comparative experiments on four public\nbenchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02826v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多模态人体活动识别中的置信度驱动梯度调制：一种动态对比双路径学习方法", "tldr": "本文提出DCDP-HAR框架，通过双路径特征提取、多阶段对比学习和置信度驱动梯度调制，有效解决多模态人体活动识别中跨模态特征对齐困难和模态贡献不平衡的问题。", "motivation": "多模态人体活动识别（HAR）系统面临跨模态特征对齐困难和模态贡献不平衡等关键挑战。", "method": "提出动态对比双路径网络（DCDP-HAR）框架。该框架包含三个核心组件：1) 双路径特征提取架构，使用ResNet和DenseNet分支协同处理多模态传感器数据；2) 多阶段对比学习机制，实现从局部感知到语义抽象的渐进对齐；3) 置信度驱动的梯度调制策略，动态监控和调整每个模态分支的学习强度，以缓解模态竞争。此外，还采用了基于动量的梯度累积策略以增强训练稳定性。", "result": "通过消融研究验证了每个组件的有效性，并在四个公共基准数据集上进行了广泛的比较实验。", "conclusion": "Not mentioned in abstract", "translation": "基于传感器的 H 人体活动识别 (HAR) 是一项核心技术，它使智能系统能够感知环境并与环境互动。然而，多模态 HAR 系统仍然面临着关键挑战，例如跨模态特征对齐困难和模态贡献不平衡。为了解决这些问题，我们提出了一种名为动态对比双路径网络 (DCDP-HAR) 的新颖框架。该框架包含三个关键组件。首先，采用双路径特征提取架构，其中 ResNet 和 DenseNet 分支协同处理多模态传感器数据。其次，引入多阶段对比学习机制，以实现从局部感知到语义抽象的渐进对齐。第三，我们提出了一种置信度驱动的梯度调制策略，该策略在反向传播过程中动态监控和调整每个模态分支的学习强度，有效缓解模态竞争。此外，还采用了基于动量的梯度累积策略来增强训练稳定性。我们进行了消融研究以验证每个组件的有效性，并在四个公共基准数据集上进行了广泛的比较实验。", "summary": "本文提出了一种名为动态对比双路径网络（DCDP-HAR）的新颖框架，旨在解决多模态人体活动识别（HAR）中跨模态特征对齐困难和模态贡献不平衡的问题。该框架包含双路径特征提取架构（利用ResNet和DenseNet）、多阶段对比学习机制实现渐进对齐，以及置信度驱动的梯度调制策略动态调整模态学习强度并缓解模态竞争。此外，还采用了基于动量的梯度累积策略以增强训练稳定性。研究通过消融实验和在四个公共基准数据集上的广泛比较实验验证了所提方法的有效性。", "keywords": "多模态人体活动识别, 对比学习, 梯度调制, 特征对齐, 传感器融合", "comments": "该论文创新性地结合了双路径特征提取、多阶段对比学习和置信度驱动的梯度调制策略来解决多模态HAR中的核心挑战。特别是置信度驱动的梯度调制，通过动态调整模态学习强度，有效缓解了模态间的竞争，这对于提高多模态融合的性能具有重要意义。"}}
{"id": "2507.02827", "title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network", "authors": ["Ying Yu", "Hang Xiao", "Siyao Li", "Jiarui Li", "Haotian Tang", "Hanyu Liu", "Chao Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02827v1", "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02827v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "USAD：一种无监督数据增强时空注意力扩散网络", "tldr": "USAD是一种用于人类活动识别的深度学习模型，通过无监督数据增强、多分支时空注意力网络和自适应多损失函数解决数据稀缺、特征提取不足和轻量级设备性能差的问题，并在多个数据集上取得了显著优于现有方法的表现，并可在嵌入式设备上高效部署。", "motivation": "人类活动识别（HAR）面临的主要挑战包括：稀有活动标注样本稀缺、高级特征提取不足以及模型在轻量级设备上性能不佳。", "method": "本文提出了一种名为USAD的无监督数据增强时空注意力扩散网络。首先，采用无监督的、统计引导的扩散模型进行数据增强，以缓解标注数据稀缺和类别不平衡问题。其次，设计了一个多分支时空交互网络，通过包含3x3、5x5和7x7卷积核的并行残差分支捕获序列数据的多尺度特征。同时，引入时间注意力机制识别关键时间点，空间注意力增强传感器间交互，并加入跨分支特征融合单元提升特征表示能力。最后，集成了自适应多损失函数融合策略，动态调整损失权重并优化模型。", "result": "在WISDM、PAMAP2和OPPORTUNITY三个公共数据集上，USAD分别达到了98.84%、93.81%和80.92%的准确率，显著优于现有方法。此外，在嵌入式设备上的实际部署验证了该方法的效率和可行性。", "conclusion": "USAD通过其综合优化方法，有效解决了HAR领域中数据稀缺、特征提取不足和轻量级设备性能限制等关键挑战，并在多个数据集上取得了最先进的性能，同时证明了其在实际部署中的高效性和可行性。", "translation": "人类活动识别（HAR）的主要目标是从传感器数据中推断正在进行的人类动作，这项任务在健康监测、安全保护和体育分析中具有广泛的应用。尽管研究不断涌现，HAR仍然面临关键挑战，包括稀有活动标注样本的稀缺、高级特征提取不足以及模型在轻量级设备上性能不佳。为了解决这些问题，本文提出了一种以多注意力交互机制为中心的综合优化方法。首先，采用无监督的、统计引导的扩散模型进行数据增强，从而缓解了标注数据稀缺和严重类别不平衡的问题。其次，设计了一个多分支时空交互网络，通过包含3x3、5x5和7x7卷积核的并行残差分支捕获序列数据的多尺度特征。同时，引入时间注意力机制以识别关键时间点，而空间注意力则增强了传感器间的交互。进一步引入了跨分支特征融合单元，以提高整体特征表示能力。最后，集成了自适应多损失函数融合策略，允许动态调整损失权重和整体模型优化。在WISDM、PAMAP2和OPPORTUNITY三个公共数据集上的实验结果表明，所提出的无监督数据增强时空注意力扩散网络（USAD）分别实现了98.84%、93.81%和80.92%的准确率，显著优于现有方法。此外，在嵌入式设备上的实际部署验证了所提方法的效率和可行性。", "summary": "本文提出了USAD（无监督数据增强时空注意力扩散网络），旨在解决人类活动识别（HAR）中数据稀缺、特征提取不足及轻量级设备性能受限的问题。USAD通过无监督的统计引导扩散模型进行数据增强，构建了包含多尺度卷积核、时空注意力机制和跨分支特征融合的多分支时空交互网络，并采用自适应多损失函数融合策略。实验结果表明，USAD在WISDM、PAMAP2和OPPORTUNITY数据集上取得了显著优于现有方法的准确率，并验证了其在嵌入式设备上的高效和可行性。", "keywords": "人类活动识别, 无监督数据增强, 时空注意力, 扩散网络, 多尺度特征", "comments": "该论文通过结合无监督数据增强、多注意力时空网络和自适应损失函数，为人类活动识别提供了一个全面的解决方案。其创新点在于将扩散模型引入数据增强以解决数据稀缺和不平衡问题，并通过多尺度卷积核与时空注意力机制有效捕获高级特征。在嵌入式设备上的验证进一步凸显了其实用价值。"}}
{"id": "2507.02011", "title": "Machine Learning Based Stress Testing Framework for Indian Financial Market Portfolios", "authors": ["Vidya Sagar G", "Shifat Ali", "Siddhartha P. Chakrabarty"], "categories": ["q-fin.RM", "cs.LG", "q-fin.PM"], "primary_category": "Subjects:       Risk Management (q-fin.RM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02011v1", "summary": "This paper presents a machine learning driven framework for sectoral stress\ntesting in the Indian financial market, focusing on financial services,\ninformation technology, energy, consumer goods, and pharmaceuticals. Initially,\nwe address the limitations observed in conventional stress testing through\ndimensionality reduction and latent factor modeling via Principal Component\nAnalysis and Autoencoders. Building on this, we extend the methodology using\nVariational Autoencoders, which introduces a probabilistic structure to the\nlatent space. This enables Monte Carlo-based scenario generation, allowing for\nmore nuanced, distribution-aware simulation of stressed market conditions. The\nproposed framework captures complex non-linear dependencies and supports risk\nestimation through Value-at-Risk and Expected Shortfall. Together, these\npipelines demonstrate the potential of Machine Learning approaches to improve\nthe flexibility, robustness, and realism of financial stress testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02011v1", "cate": "q-fin.RM", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "基于机器学习的印度金融市场投资组合压力测试框架", "tldr": "该论文提出了一个基于机器学习的框架，用于印度金融市场（包括金融服务、IT、能源、消费品和制药行业）的行业压力测试，通过降维、潜在因子建模和变分自编码器改进了传统方法，并支持风险估计。", "motivation": "解决传统压力测试中存在的局限性，特别是通过降维和潜在因子建模来改进。", "method": "首先通过主成分分析（PCA）和自编码器（Autoencoders）进行降维和潜在因子建模。在此基础上，使用变分自编码器（Variational Autoencoders）引入概率结构到潜在空间，从而实现基于蒙特卡洛的场景生成。该框架还支持通过风险价值（Value-at-Risk）和预期损失（Expected Shortfall）进行风险估计。", "result": "所提出的框架能够捕捉复杂的非线性依赖关系，并支持通过风险价值（Value-at-Risk）和预期损失（Expected Shortfall）进行风险估计。这些方法展示了机器学习在提高金融压力测试的灵活性、鲁棒性和真实性方面的潜力。", "conclusion": "机器学习方法能够显著提高金融压力测试的灵活性、鲁棒性和真实性。", "translation": "本文提出了一个机器学习驱动的框架，用于印度金融市场（重点关注金融服务、信息技术、能源、消费品和制药行业）的行业压力测试。最初，我们通过主成分分析和自编码器进行降维和潜在因子建模，解决了传统压力测试中观察到的局限性。在此基础上，我们使用变分自编码器扩展了该方法，这为潜在空间引入了概率结构。这使得基于蒙特卡洛的场景生成成为可能，从而允许对压力市场条件进行更细致、更具分布意识的模拟。所提出的框架捕捉了复杂的非线性依赖关系，并通过风险价值（Value-at-Risk）和预期损失（Expected Shortfall）支持风险估计。总而言之，这些流程展示了机器学习方法在提高金融压力测试的灵活性、鲁棒性和真实性方面的潜力。", "summary": "本研究提出了一个创新的机器学习框架，旨在改进印度金融市场的行业压力测试。该框架通过结合主成分分析、自编码器和变分自编码器来克服传统压力测试的局限性，特别是在处理复杂非线性依赖方面。通过引入概率潜在空间和蒙特卡洛模拟，该方法能够生成更真实、分布感知的压力情景，并支持通过风险价值和预期损失进行精确的风险评估。这表明机器学习在提升金融压力测试的灵活性、鲁棒性和真实性方面具有显著潜力。", "keywords": "机器学习, 压力测试, 印度金融市场, 变分自编码器, 风险管理", "comments": "该论文的创新之处在于将多种机器学习技术（PCA、Autoencoders、Variational Autoencoders）整合到一个统一的压力测试框架中，并引入了概率结构和蒙特卡洛模拟，以生成更细致、更真实的压力情景。这对于解决传统金融模型在捕捉非线性依赖和不确定性方面的不足具有重要意义，尤其是在印度这种新兴且复杂的金融市场中。该研究为金融风险管理领域提供了新的视角和工具。"}}
{"id": "2507.02857", "title": "AnyI2V: Animating Any Conditional Image with Motion Control", "authors": ["Ziye Li", "Hao Luo", "Xincheng Shuai", "Henghui Ding"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.02857v1", "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.", "comment": "ICCV 2025, Project Page: https://henghuiding.com/AnyI2V/", "pdf_url": "http://arxiv.org/pdf/2507.02857v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AnyI2V：通过运动控制动画化任意条件图像", "tldr": "AnyI2V是一个无需训练的框架，能够利用用户定义的运动轨迹动画化任意条件图像，解决了现有视频生成方法在动态运动信号和灵活空间约束方面的不足。", "motivation": "现有的文本到视频（T2V）方法缺乏对生成内容空间布局的精确控制，而图像到视频（I2V）方法受限于真实图像且可编辑性差。虽然一些方法使用ControlNet，但它们通常缺乏明确的运动控制并需要昂贵的训练。", "method": "我们提出了AnyI2V，一个无需训练的框架，能够通过用户定义的运动轨迹动画化任意条件图像。AnyI2V支持更广泛的条件图像模态，包括ControlNet不支持的网格和点云等数据类型。它还支持混合条件输入，并通过LoRA和文本提示实现风格迁移和编辑。", "result": "广泛的实验表明，所提出的AnyI2V实现了卓越的性能，并在空间和运动控制的视频生成方面提供了新的视角。", "conclusion": "AnyI2V为空间和运动控制的视频生成提供了一个无需训练的、灵活且性能优越的解决方案，支持多种条件输入模态。", "translation": "视频生成领域的最新进展，特别是在扩散模型方面，推动了文本到视频（T2V）和图像到视频（I2V）合成的显著进展。然而，在有效整合动态运动信号和灵活空间约束方面仍存在挑战。现有的T2V方法通常依赖文本提示，这本质上缺乏对生成内容空间布局的精确控制。相比之下，I2V方法受限于对真实图像的依赖，这限制了合成内容的可编辑性。尽管一些方法结合ControlNet引入了基于图像的条件，但它们通常缺乏明确的运动控制并需要计算成本高昂的训练。为了解决这些局限性，我们提出了AnyI2V，一个无需训练的框架，它可以通过用户定义的运动轨迹动画化任意条件图像。AnyI2V支持更广泛的模态作为条件图像，包括ControlNet不支持的网格和点云等数据类型，从而实现更灵活和多功能的视频生成。此外，它支持混合条件输入，并通过LoRA和文本提示实现风格迁移和编辑。广泛的实验表明，所提出的AnyI2V实现了卓越的性能，并在空间和运动控制的视频生成方面提供了新的视角。代码可在https://henghuiding.com/AnyI2V/获取。", "summary": "AnyI2V是一个创新的无需训练的框架，旨在解决当前视频生成（T2V和I2V）在精确运动控制和灵活空间约束方面的不足。它允许用户通过自定义运动轨迹动画化任意条件图像，支持包括网格和点云在内的多种模态，并能进行混合输入、风格迁移和编辑。实验证明AnyI2V在空间和运动控制的视频生成方面表现出色，提供了一种新颖且高效的方法。", "keywords": "视频生成, 图像到视频, 运动控制, 无需训练, 扩散模型", "comments": "AnyI2V的创新之处在于其“无需训练”的特性和对“任意条件图像”的支持，包括传统ControlNet不支持的模态如网格和点云，极大地提升了I2V方法的灵活性和应用范围。其对显式运动控制的强调，并通过用户定义的轨迹实现，是该领域的重要进步。支持混合输入和风格迁移进一步增强了其实用性。"}}
{"id": "2507.02859", "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation", "authors": ["Jiaer Xia", "Bingkui Tong", "Yuhang Zang", "Rui Shao", "Kaiyang Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.02859v1", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in interpreting images using natural language. However, without\nusing large-scale datasets for retraining, these models are difficult to adapt\nto specialized vision tasks, e.g., chart understanding. This problem is caused\nby a mismatch between pre-training and downstream datasets: pre-training\ndatasets primarily concentrate on scenes and objects but contain limited\ninformation about specialized, non-object images, such as charts and tables. In\nthis paper, we share an interesting finding that training an MLLM with\nChain-of-Thought (CoT) reasoning data can facilitate model adaptation in\nspecialized vision tasks, especially under data-limited regimes. However, we\nidentify a critical issue within CoT data distilled from pre-trained MLLMs,\ni.e., the data often contains multiple factual errors in the reasoning steps.\nTo address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple\nbootstrapping-based approach that aims to inject grounding information (i.e.,\nbounding boxes) into CoT data, essentially making the reasoning steps more\nfaithful to input images. We evaluate our approach on five specialized vision\ntasks, which cover a variety of visual formats including charts, tables,\nreceipts, and reports. The results demonstrate that under data-limited regimes\nour approach significantly improves upon fine-tuning and distillation.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.02859v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多模态大型语言模型中引导式接地思维链以实现数据高效模型适应", "tldr": "针对多模态大模型在数据稀缺的专业视觉任务中适应性差的问题，本文提出GCoT方法，通过引入边界框接地信息改进思维链数据，显著提升了模型性能。", "motivation": "多模态大型语言模型（MLLMs）在不使用大规模数据集进行再训练的情况下，难以适应专业的视觉任务（如图表理解），原因是预训练数据集与下游数据集之间存在不匹配，预训练数据主要集中于场景和物体，而专业非物体图像（如图表和表格）的信息有限。", "method": "本文发现使用思维链（CoT）推理数据训练MLLM可以促进模型在专业视觉任务中的适应性，尤其是在数据受限的情况下。针对CoT数据中存在的推理步骤事实错误问题，提出了一种基于引导（bootstrapping）的接地思维链（GCoT）方法，通过向CoT数据中注入接地信息（即边界框），使推理步骤更忠实于输入图像。", "result": "在涵盖图表、表格、收据和报告等多种视觉格式的五项专业视觉任务上进行了评估。结果表明，在数据受限的情况下，该方法显著优于微调和蒸馏。", "conclusion": "接地思维链（GCoT）通过注入接地信息改进思维链数据，有效解决了多模态大型语言模型在数据稀缺的专业视觉任务中的适应性问题，显著提升了模型性能。", "translation": "多模态大型语言模型（MLLMs）在利用自然语言解释图像方面展现了卓越的能力。然而，在不使用大规模数据集进行再训练的情况下，这些模型难以适应专业的视觉任务，例如图表理解。这个问题是由于预训练数据集与下游数据集之间的不匹配造成的：预训练数据集主要集中于场景和物体，但包含的关于专业非物体图像（如图表和表格）的信息有限。在本文中，我们分享了一个有趣的发现，即使用思维链（CoT）推理数据训练MLLM可以促进模型在专业视觉任务中的适应性，尤其是在数据受限的情况下。然而，我们发现从预训练MLLM中提取的CoT数据存在一个关键问题，即数据在推理步骤中经常包含多个事实错误。为了解决这个问题，我们提出了一种简单的基于引导（bootstrapping）的接地思维链（GCoT）方法，旨在将接地信息（即边界框）注入到CoT数据中，从而使推理步骤更忠实于输入图像。我们在五项专业视觉任务上评估了我们的方法，这些任务涵盖了图表、表格、收据和报告等多种视觉格式。结果表明，在数据受限的情况下，我们的方法显著优于微调和蒸馏。", "summary": "本文针对多模态大型语言模型在数据稀缺的专业视觉任务（如图表理解）中适应性差的问题。研究发现，使用思维链（CoT）数据训练有助于模型适应，但现有CoT数据常含事实错误。为解决此问题，本文提出接地思维链（GCoT），通过引导式方法将边界框等接地信息注入CoT数据，使推理更忠实于图像。实验证明，在数据受限场景下，GCoT在多项专业视觉任务上显著优于传统微调和蒸馏方法。", "keywords": "多模态大型语言模型, 思维链, 数据高效, 模型适应, 接地信息", "comments": "这篇论文通过引入“接地”概念来改进思维链（CoT）数据，解决了多模态大模型在专业视觉任务中数据稀缺时的适应性问题。其创新点在于识别并纠正了CoT数据中存在的推理错误，通过注入边界框信息增强了推理的忠实性。这种方法在数据高效模型适应方面具有重要意义，尤其是在难以获取大规模标注数据的专业领域。该研究为提升MLLMs在实际应用中的鲁棒性和泛化能力提供了新思路。"}}
{"id": "2507.02860", "title": "Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching", "authors": ["Xin Zhou", "Dingkang Liang", "Kaijin Chen", "Tianrui Feng", "Xiwu Chen", "Hongkai Lin", "Yikang Ding", "Feiyang Tan", "Hengshuang Zhao", "Xiang Bai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The code is made available at this https URL . Project page: this https URL", "url": "http://arxiv.org/abs/2507.02860v1", "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.", "comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/", "pdf_url": "http://arxiv.org/pdf/2507.02860v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "少即是多：通过运行时自适应缓存实现免训练视频扩散加速", "tldr": "EasyCache 提出了一种免训练的运行时自适应缓存机制，用于加速视频扩散模型，显著减少推理时间并保持高质量。", "motivation": "视频生成模型因迭代去噪过程导致推理速度慢和计算成本高，限制了其广泛应用。解决此瓶颈对于普及先进视频合成技术和集成到实际应用中至关重要。", "method": "本文提出了 EasyCache，一个用于视频扩散模型的免训练加速框架。EasyCache 引入了一种轻量级、运行时自适应的缓存机制，动态重用先前计算的变换向量，避免推理过程中的冗余计算。它不需要离线分析、预计算或大量参数调整。", "result": "在OpenSora、Wan2.1和HunyuanVideo等各种大型视频生成模型上进行了全面研究。与原始基线相比，推理时间减少了2.1-3.3倍，同时保持了高视觉保真度，与之前的SOTA方法相比，PSNR显著提高了36%。", "conclusion": "EasyCache 是一种高效且高度可用的解决方案，适用于研究和实际应用中的高质量视频生成。", "translation": "视频生成模型展现出卓越的性能，然而，由于去噪过程的迭代性质，其较慢的推理速度和巨大的计算成本限制了其更广泛的应用。解决这一瓶颈对于普及先进的视频合成技术并使其融入现实世界应用至关重要。本文提出 EasyCache，一个用于视频扩散模型的免训练加速框架。EasyCache 引入了一种轻量级、运行时自适应的缓存机制，动态重用先前计算的变换向量，从而避免了推理过程中的冗余计算。与以往的方法不同，EasyCache 不需要离线分析、预计算或大量的参数调整。我们对各种大型视频生成模型进行了全面的研究，包括 OpenSora、Wan2.1 和 HunyuanVideo。我们的方法实现了领先的加速性能，与原始基线相比，推理时间减少了高达 2.1-3.3 倍，同时保持了高视觉保真度，与之前的 SOTA 方法相比，PSNR 显著提高了高达 36%。这一改进使我们的 EasyCache 成为研究和实际应用中高质量视频生成的高效且高度可用的解决方案。代码可在 https://github.com/H-EmbodVis/EasyCache 获取。", "summary": "本研究提出了一种名为 EasyCache 的免训练加速框架，旨在解决视频扩散模型推理速度慢和计算成本高的问题。EasyCache 采用运行时自适应缓存机制，通过重用计算过的变换向量来避免冗余计算，无需离线分析或参数调优。实验证明，该方法在多个大型视频生成模型上实现了显著的加速（2.1-3.3倍），同时保持了高质量，并相较于现有最佳方法提高了36%的PSNR，使其成为视频生成领域高效且实用的解决方案。", "keywords": "视频扩散模型, 加速, 免训练, 缓存机制, EasyCache", "comments": "该论文的创新点在于提出了一个免训练、运行时自适应的缓存机制 EasyCache，有效解决了视频扩散模型推理速度慢的瓶颈。其优势在于无需复杂的预处理和调优，具有高度的实用性和可访问性。性能提升显著，对于推动视频生成技术在实际应用中的普及具有重要意义。"}}
{"id": "2507.02076", "title": "Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs", "authors": ["Mohammad Ali Alomrani", "Yingxue Zhang", "Derek Li", "Qianyi Sun", "Soumyasundar Pal", "Zhanguang Zhang", "Yaochen Hu", "Rohan Deepak Ajwani", "Antonios Valkanas", "Raika Karimi", "Peng Cheng", "Yunzhou Wang", "Pengyi Liao", "Hanrui Huang", "Bin Wang", "Jianye Hao", "Mark Coates"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02076v1", "summary": "Large language models (LLMs) have rapidly progressed into general-purpose\nagents capable of solving a broad spectrum of tasks. However, current models\nremain inefficient at reasoning: they apply fixed inference-time compute\nregardless of task complexity, often overthinking simple problems while\nunderthinking hard ones. This survey presents a comprehensive review of\nefficient test-time compute (TTC) strategies, which aim to improve the\ncomputational efficiency of LLM reasoning. We introduce a two-tiered taxonomy\nthat distinguishes between L1-controllability, methods that operate under fixed\ncompute budgets, and L2-adaptiveness, methods that dynamically scale inference\nbased on input difficulty or model confidence. We benchmark leading proprietary\nLLMs across diverse datasets, highlighting critical trade-offs between\nreasoning performance and token usage. Compared to prior surveys on efficient\nreasoning, our review emphasizes the practical control, adaptability, and\nscalability of TTC methods. Finally, we discuss emerging trends such as hybrid\nthinking models and identify key challenges for future work towards making LLMs\nmore computationally efficient, robust, and responsive to user constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02076v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "预算推理：LLMs中自适应和可控测试时间计算的综述", "tldr": "这是一篇关于LLM中高效测试时间计算（TTC）策略的综述，旨在通过根据任务复杂性调整计算量来提高推理效率。", "motivation": "当前大型语言模型（LLMs）在推理时效率低下，无论任务复杂性如何都应用固定的推理时间计算，导致对简单问题过度思考，对困难问题思考不足。", "method": "本综述全面回顾了高效测试时间计算（TTC）策略，并引入了一个两层分类法，区分了L1-可控性（在固定计算预算下操作的方法）和L2-适应性（根据输入难度或模型置信度动态调整推理的方法）。此外，还对领先的专有LLM在不同数据集上进行了基准测试。", "result": "研究结果强调了推理性能和令牌使用之间的关键权衡。与之前的关于高效推理的综述相比，本综述强调了TTC方法的实际控制、适应性和可扩展性。", "conclusion": "本综述讨论了混合思维模型等新兴趋势，并指出了未来工作中的关键挑战，旨在使LLMs在计算上更高效、更鲁棒，并更能响应用户约束。", "translation": "大型语言模型（LLMs）已迅速发展成为能够解决广泛任务的通用代理。然而，当前模型在推理方面仍然效率低下：它们无论任务复杂性如何都应用固定的推理时间计算，经常对简单问题过度思考，而对困难问题思考不足。本综述全面审查了高效测试时间计算（TTC）策略，旨在提高LLM推理的计算效率。我们引入了一个两层分类法，区分了L1-可控性（在固定计算预算下操作的方法）和L2-适应性（根据输入难度或模型置信度动态调整推理的方法）。我们对领先的专有LLMs在不同数据集上进行了基准测试，突出了推理性能和令牌使用之间的关键权衡。与之前关于高效推理的综述相比，我们的审查强调了TTC方法的实际控制、适应性和可扩展性。最后，我们讨论了混合思维模型等新兴趋势，并指出了未来工作中的关键挑战，旨在使LLMs在计算上更高效、更鲁棒，并更能响应用户约束。", "summary": "由于推理时计算量固定，大型语言模型（LLMs）在处理任务时效率低下。本综述全面审查了旨在提高LLM推理计算效率的测试时间计算（TTC）策略。文章引入了一个两层分类法，区分了在固定预算下操作的L1-可控性方法和根据输入难度动态调整推理的L2-适应性方法。该综述对领先的LLMs进行了基准测试，揭示了推理性能和令牌使用之间的权衡，并强调了TTC方法的实用性、适应性和可扩展性。最后，讨论了新兴趋势并指出了未来使LLMs更高效、更鲁棒、更响应用户约束的挑战。", "keywords": "LLMs, 测试时间计算, 推理, 计算效率, 自适应推理", "comments": "这篇综述解决了LLM部署中的一个关键实际问题：推理过程中的计算效率。提出的两层分类法为现有和未来的TTC方法提供了一个有用的框架。对专有模型的基准测试增加了实际相关性，对于LLM的扩展和降低成本至关重要。"}}
{"id": "2507.02862", "title": "RefTok: Reference-Based Tokenization for Video Generation", "authors": ["Xiang Fan", "Xiaohang Sun", "Kushan Thakkar", "Zhu Liu", "Vimal Bhat", "Ranjay Krishna", "Xiang Hao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02862v1", "summary": "Effectively handling temporal redundancy remains a key challenge in learning\nvideo models. Prevailing approaches often treat each set of frames\nindependently, failing to effectively capture the temporal dependencies and\nredundancies inherent in videos. To address this limitation, we introduce\nRefTok, a novel reference-based tokenization method capable of capturing\ncomplex temporal dynamics and contextual information. Our method encodes and\ndecodes sets of frames conditioned on an unquantized reference frame. When\ndecoded, RefTok preserves the continuity of motion and the appearance of\nobjects across frames. For example, RefTok retains facial details despite head\nmotion, reconstructs text correctly, preserves small patterns, and maintains\nthe legibility of handwriting from the context. Across 4 video datasets (K600,\nUCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms\ncurrent state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all\nevaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or\nhigher compression ratios. When a video generation model is trained using\nRefTok's latents on the BAIR Robot Pushing task, the generations not only\noutperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,\nacross all generation metrics by an average of 27.9%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02862v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RefTok：基于引用的视频生成标记化方法", "tldr": "RefTok是一种新型的基于引用的标记化方法，通过利用未量化参考帧来有效处理视频中的时间冗余，显著优于现有技术，并在视频生成任务中表现出色。", "motivation": "在学习视频模型时，有效处理时间冗余仍然是一个关键挑战。现有方法通常独立处理每组帧，未能有效捕获视频固有的时间依赖性和冗余性。", "method": "我们引入了RefTok，一种新颖的基于引用的标记化方法。该方法以一个未量化的参考帧为条件来编码和解码帧集，从而捕获复杂的时间动态和上下文信息。", "result": "RefTok在4个视频数据集（K600、UCF-101、BAIR Robot Pushing和DAVIS）上显著优于当前的SOTA标记器（Cosmos和MAGVIT），在相同或更高压缩比下，所有评估指标（PSNR、SSIM、LPIPS）平均提高了36.7%。当使用RefTok的潜在空间训练视频生成模型时，在BAIR Robot Pushing任务上，生成效果不仅优于MAGVIT-B，甚至优于参数量大4倍的MAGVIT-L，所有生成指标平均提高了27.9%。RefTok能保留面部细节、正确重建文本、保留小图案并保持手写字迹的清晰度。", "conclusion": "RefTok通过引入基于参考的标记化方法，有效地解决了视频中的时间冗余问题，并在视频编码和生成方面取得了显著的SOTA性能提升。", "translation": "有效处理时间冗余仍然是学习视频模型中的一个关键挑战。现有方法通常独立处理每组帧，未能有效捕获视频固有的时间依赖性和冗余性。为了解决这一限制，我们引入了RefTok，一种新颖的基于引用的标记化方法，能够捕获复杂的时间动态和上下文信息。我们的方法以一个未量化的参考帧为条件来编码和解码帧集。解码时，RefTok能保持跨帧的运动连续性和对象外观。例如，RefTok即使在头部运动的情况下也能保留面部细节，正确重建文本，保留小图案，并从上下文中保持手写字迹的清晰度。在4个视频数据集（K600、UCF-101、BAIR Robot Pushing和DAVIS）上，RefTok显著优于当前的SOTA标记器（Cosmos和MAGVIT），并在相同或更高压缩比下，所有评估指标（PSNR、SSIM、LPIPS）平均提高了36.7%。当使用RefTok的潜在空间训练视频生成模型时，在BAIR Robot Pushing任务上，生成效果不仅优于MAGVIT-B，甚至优于参数量大4倍的MAGVIT-L，所有生成指标平均提高了27.9%。", "summary": "本文提出了RefTok，一种创新的基于引用的标记化方法，旨在解决视频模型中时间冗余处理的挑战。通过利用未量化的参考帧进行条件编码和解码，RefTok能有效捕获复杂的视频时间动态和上下文信息，从而保持运动连续性和对象外观。实验结果表明，RefTok在视频压缩和生成任务中均显著超越了现有的先进标记器，在多个指标上取得了大幅提升，证明了其在处理视频时间依赖性方面的优越性。", "keywords": "视频生成, 标记化, 时间冗余, 参考帧, RefTok", "comments": "RefTok的创新点在于其引入的“基于引用的标记化”概念，这是一种有效利用视频时间冗余的新颖方法。通过条件化编码和解码，它能够更好地保留视频的细节和运动连续性，显著提升了视频压缩和生成质量。其在多项指标上的显著领先，表明了该方法在视频处理领域的巨大潜力，为未来的视频建模提供了新的方向。"}}
{"id": "2507.02086", "title": "Selective Feature Re-Encoded Quantum Convolutional Neural Network with Joint Optimization for Image Classification", "authors": ["Shaswata Mahernob Sarkar", "Sheikh Iftekhar Ahmed", "Jishnu Mahmud", "Shaikh Anowarul Fattah", "Gaurav Sharma"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      26 pages, 12 figures, 6 Tables", "url": "http://arxiv.org/abs/2507.02086v1", "summary": "Quantum Machine Learning (QML) has seen significant advancements, driven by\nrecent improvements in Noisy Intermediate-Scale Quantum (NISQ) devices.\nLeveraging quantum principles such as entanglement and superposition, quantum\nconvolutional neural networks (QCNNs) have demonstrated promising results in\nclassifying both quantum and classical data. This study examines QCNNs in the\ncontext of image classification and proposes a novel strategy to enhance\nfeature processing and a QCNN architecture for improved classification\naccuracy. First, a selective feature re-encoding strategy is proposed, which\ndirects the quantum circuits to prioritize the most informative features,\nthereby effectively navigating the crucial regions of the Hilbert space to find\nthe optimal solution space. Secondly, a novel parallel-mode QCNN architecture\nis designed to simultaneously incorporate features extracted by two classical\nmethods, Principal Component Analysis (PCA) and Autoencoders, within a unified\ntraining scheme. The joint optimization involved in the training process allows\nthe QCNN to benefit from complementary feature representations, enabling better\nmutual readjustment of model parameters. To assess these methodologies,\ncomprehensive experiments have been performed using the widely used MNIST and\nFashion MNIST datasets for binary classification tasks. Experimental findings\nreveal that the selective feature re-encoding method significantly improves the\nquantum circuit's feature processing capability and performance. Furthermore,\nthe jointly optimized parallel QCNN architecture consistently outperforms the\nindividual QCNN models and the traditional ensemble approach involving\nindependent learning followed by decision fusion, confirming its superior\naccuracy and generalization capabilities.", "comment": "26 pages, 12 figures, 6 Tables", "pdf_url": "http://arxiv.org/pdf/2507.02086v1", "cate": "quant-ph", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "选择性特征重编码量子卷积神经网络与联合优化图像分类", "tldr": "本文提出一种结合选择性特征重编码和并行QCNN架构的量子神经网络，通过联合优化提升图像分类性能。", "motivation": "量子机器学习（QML）在NISQ设备推动下取得显著进展，QCNN在数据分类中显示出潜力。本研究旨在通过增强特征处理和改进QCNN架构来提高图像分类的准确性。", "method": "本文提出了一种新颖的策略和QCNN架构来提高图像分类准确性。首先，提出了选择性特征重编码策略，引导量子电路优先处理信息量最大的特征。其次，设计了一种新型并行模式QCNN架构，在统一训练方案中同时整合由主成分分析（PCA）和自编码器提取的经典特征。训练过程采用联合优化，使QCNN能够从互补的特征表示中受益，并实现模型参数的相互调整。", "result": "实验结果表明，选择性特征重编码方法显著提高了量子电路的特征处理能力和性能。此外，联合优化的并行QCNN架构始终优于单个QCNN模型和传统的独立学习后决策融合的集成方法，并在MNIST和Fashion MNIST数据集的二分类任务上，证明了其卓越的准确性和泛化能力。", "conclusion": "提出的选择性特征重编码和联合优化的并行QCNN架构有效提升了图像分类的准确性和泛化能力。", "translation": "量子机器学习（QML）在近期噪声中等规模量子（NISQ）设备的改进推动下取得了显著进展。利用纠缠和叠加等量子原理，量子卷积神经网络（QCNN）在分类量子和经典数据方面展现出有前景的结果。本研究在图像分类背景下考察QCNN，并提出一种新颖的策略来增强特征处理和改进QCNN架构以提高分类准确性。首先，提出了一种选择性特征重编码策略，该策略引导量子电路优先处理信息量最大的特征，从而有效导航希尔伯特空间的关键区域以找到最优解空间。其次，设计了一种新颖的并行模式QCNN架构，在统一的训练方案中同时整合由两种经典方法（主成分分析（PCA）和自编码器）提取的特征。训练过程中涉及的联合优化使QCNN能够从互补的特征表示中受益，从而实现模型参数更好的相互调整。为了评估这些方法，我们使用广泛使用的MNIST和Fashion MNIST数据集进行了二分类任务的综合实验。实验结果表明，选择性特征重编码方法显著提高了量子电路的特征处理能力和性能。此外，联合优化的并行QCNN架构始终优于单个QCNN模型和涉及独立学习后决策融合的传统集成方法，证实了其卓越的准确性和泛化能力。", "summary": "本文提出一种新颖的量子卷积神经网络（QCNN）方法，用于提高图像分类准确性。该方法包含两部分：一是选择性特征重编码策略，旨在引导量子电路聚焦于最具信息量的特征；二是并行模式QCNN架构，通过联合优化同时整合PCA和自编码器提取的经典特征。在MNIST和Fashion MNIST数据集上的实验结果表明，该方法显著提升了特征处理能力，并且联合优化的并行QCNN架构在准确性和泛化能力上优于现有模型和传统集成方法。", "keywords": "量子机器学习, 量子卷积神经网络, 图像分类, 特征重编码, 联合优化", "comments": "这篇论文的创新点在于结合了选择性特征重编码和并行QCNN架构，并通过联合优化利用了多种经典特征提取方法的优势。这种混合量子-经典方法在NISQ时代具有重要意义，因为它能有效利用量子计算的潜力，同时结合经典算法的成熟性来处理复杂数据。其局限性可能在于对NISQ设备误差的敏感性以及所提架构的扩展性。"}}
{"id": "2507.02436", "title": "Toward a Robust and Generalizable Metamaterial Foundation Model", "authors": ["Namjung Kim", "Dongseok Lee", "Jongbin Yu", "Sung Woong Cho", "Dosung Lee", "Yesol Park", "Youngjoon Hong"], "categories": ["physics.optics", "cs.AI"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02436v1", "summary": "Advances in material functionalities drive innovations across various fields,\nwhere metamaterials-defined by structure rather than composition-are leading\nthe way. Despite the rise of artificial intelligence (AI)-driven design\nstrategies, their impact is limited by task-specific retraining, poor\nout-of-distribution(OOD) generalization, and the need for separate models for\nforward and inverse design. To address these limitations, we introduce the\nMetamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation\nmodel inspired by large language models. MetaFO learns the underlying mechanics\nof metamaterials, enabling probabilistic, zero-shot predictions across diverse,\nunseen combinations of material properties and structural responses. It also\nexcels in nonlinear inverse design, even under OOD conditions. By treating\nmetamaterials as an operator that maps material properties to structural\nresponses, MetaFO uncovers intricate structure-property relationships and\nsignificantly expands the design space. This scalable and generalizable\nframework marks a paradigm shift in AI-driven metamaterial discovery, paving\nthe way for next-generation innovations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02436v1", "cate": "physics.optics", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "迈向鲁棒且可泛化的超材料基础模型", "tldr": "MetaFO是一个受大型语言模型启发的贝叶斯Transformer模型，能实现超材料的零样本预测和逆向设计，克服了现有AI设计的局限性。", "motivation": "现有AI驱动的超材料设计策略存在局限性，包括需要任务特定的再训练、差的出分布（OOD）泛化能力以及正向和逆向设计需要独立模型。", "method": "本文引入了超材料基础模型（MetaFO），一个基于贝叶斯Transformer的基础模型，灵感来源于大型语言模型。MetaFO通过将超材料视为将材料属性映射到结构响应的算子，学习其底层力学。", "result": "MetaFO能够对材料属性和结构响应的各种未见组合进行概率性的零样本预测，并在OOD条件下擅长非线性逆向设计。它揭示了复杂的结构-属性关系，显著扩展了设计空间。", "conclusion": "MetaFO框架是一个可扩展且可泛化的框架，标志着AI驱动超材料发现的范式转变，为下一代创新铺平了道路。", "translation": "材料功能性的进步推动了各个领域的创新，其中超材料（由结构而非成分定义）正引领潮流。尽管人工智能（AI）驱动的设计策略日益兴起，但其影响受限于任务特定的再训练、差的出分布（OOD）泛化能力以及正向和逆向设计需要独立模型。为了解决这些局限性，我们引入了超材料基础模型（MetaFO），这是一个受大型语言模型启发的基于贝叶斯Transformer的基础模型。MetaFO学习超材料的底层力学，从而能够在材料属性和结构响应的各种未见组合中进行概率性的零样本预测。它在非线性逆向设计方面也表现出色，即使在OOD条件下也是如此。通过将超材料视为将材料属性映射到结构响应的算子，MetaFO揭示了复杂的结构-属性关系，并显著扩展了设计空间。这个可扩展且可泛化的框架标志着AI驱动的超材料发现的范式转变，为下一代创新铺平了道路。", "summary": "MetaFO是一个受大型语言模型启发的贝叶斯Transformer基础模型，旨在克服现有AI驱动超材料设计中任务特定再训练、OOD泛化差以及正逆向设计分离的局限性。它通过学习超材料的底层力学，实现了对未见材料属性和结构响应的概率性零样本预测，并擅长OOD条件下的非线性逆向设计，从而揭示复杂的结构-属性关系并扩展设计空间，标志着AI驱动超材料发现的范式转变。", "keywords": "超材料, 基础模型, 贝叶斯Transformer, 零样本预测, 逆向设计", "comments": "创新点在于将大型语言模型的基础模型概念引入超材料设计，通过贝叶斯Transformer实现对超材料底层力学的学习，解决了现有AI方法在泛化性和正逆向设计统一性上的痛点。其零样本预测和OOD逆向设计能力具有重要意义。"}}
{"id": "2507.02863", "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory", "authors": ["Yuqi Wu", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code is available at: this https URL", "url": "http://arxiv.org/abs/2507.02863v1", "summary": "Dense 3D scene reconstruction from an ordered sequence or unordered image\ncollections is a critical step when bringing research in computer vision into\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\nan image pair densely into a shared coordinate system, subsequent methods\nmaintain an implicit memory to achieve dense 3D reconstruction from more\nimages. However, such implicit memory is limited in capacity and may suffer\nfrom information loss of earlier frames. We propose Point3R, an online\nframework targeting dense streaming 3D reconstruction. To be specific, we\nmaintain an explicit spatial pointer memory directly associated with the 3D\nstructure of the current scene. Each pointer in this memory is assigned a\nspecific 3D position and aggregates scene information nearby in the global\ncoordinate system into a changing spatial feature. Information extracted from\nthe latest frame interacts explicitly with this pointer memory, enabling dense\nintegration of the current observation into the global coordinate system. We\ndesign a 3D hierarchical position embedding to promote this interaction and\ndesign a simple yet effective fusion mechanism to ensure that our pointer\nmemory is uniform and efficient. Our method achieves competitive or\nstate-of-the-art performance on various tasks with low training costs. Code is\navailable at: https://github.com/YkiWu/Point3R.", "comment": "Code is available at: https://github.com/YkiWu/Point3R", "pdf_url": "http://arxiv.org/pdf/2507.02863v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Point3R: 流式三维重建与显式空间指针记忆", "tldr": "Point3R通过显式空间指针记忆实现高效、实时的密集三维重建，解决了传统隐式记忆容量有限和信息丢失的问题。", "motivation": "传统方法在密集三维重建中使用隐式记忆，但这种记忆容量有限，且可能导致早期帧信息丢失。", "method": "提出Point3R，一个针对密集流式三维重建的在线框架。它维护一个与当前场景三维结构直接关联的显式空间指针记忆。每个指针分配一个特定三维位置，并聚合附近场景信息。最新帧的信息与此指针记忆显式交互，实现观测结果到全局坐标系的密集集成。设计了三维分层位置嵌入来促进交互，并设计了简单有效的融合机制确保指针记忆的均匀性和效率。", "result": "在各种任务上实现了具有竞争力的或最先进的性能，且训练成本低。", "conclusion": "Point3R通过其独特的显式空间指针记忆和集成机制，有效解决了流式三维重建中信息丢失和容量限制的问题，并在性能和效率上表现出色。", "translation": "从有序序列或无序图像集合中进行密集三维场景重建是计算机视觉研究走向实际应用的关键一步。遵循DUSt3R引入的范式，即密集地将图像对统一到共享坐标系中，后续方法通过维护隐式记忆来实现从更多图像的密集三维重建。然而，这种隐式记忆容量有限，并且可能遭受早期帧的信息丢失。我们提出了Point3R，一个针对密集流式三维重建的在线框架。具体来说，我们维护一个与当前场景三维结构直接关联的显式空间指针记忆。此记忆中的每个指针都被分配一个特定的三维位置，并在全局坐标系中聚合附近场景信息，形成一个变化的 K空间特征。从最新帧中提取的信息与此指针记忆显式交互，从而将当前观测结果密集地集成到全局坐标系中。我们设计了一个三维分层位置嵌入来促进这种交互，并设计了一个简单而有效的融合机制，以确保我们的指针记忆是均匀和高效的。我们的方法在各种任务上以低训练成本实现了具有竞争力或最先进的性能。代码可在：https://github.com/YkiWu/Point3R 获取。", "summary": "Point3R是一个为密集流式三维重建设计的在线框架。它通过引入显式空间指针记忆来克服传统方法中隐式记忆容量有限和信息丢失的问题。该指针记忆直接关联场景三维结构，每个指针聚合附近信息，并与最新帧数据显式交互，实现高效的全局坐标系集成。Point3R结合了三维分层位置嵌入和有效的融合机制，在多种任务上取得了领先或SOTA的性能，且训练成本低。", "keywords": "3D Reconstruction, Streaming, Pointer Memory, Spatial Feature, Online Framework", "comments": "该论文的创新点在于提出了显式空间指针记忆，解决了流式三维重建中长期信息保持和集成的问题，克服了传统隐式记忆的局限性。其在线框架设计和低训练成本使其具有较高的实用价值。"}}
{"id": "2507.02024", "title": "TubuleTracker: a high-fidelity shareware software to quantify angiogenesis architecture and maturity", "authors": ["Danish Mahmood", "Stephanie Buczkowski", "Sahaj Shah", "Autumn Anthony", "Rohini Desetty", "Carlo R Bartoli"], "categories": ["q-bio.QM", "cs.CV", "q-bio.CB"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      Abstract word count = [285] Total word count = [3910] Main body text = [2179] References = [30] Table = [0] Figures = [4]", "url": "http://arxiv.org/abs/2507.02024v1", "summary": "Background: In vitro endothelial cell culture is widely used to study\nangiogenesis. Histomicrographic images of cell networks are often analyzed\nmanually, a process that is time-consuming and subjective. Automated tools like\nImageJ (NIH) can assist, but are often slow and inaccurate. Additionally, as\nendothelial networks grow more complex, traditional architectural metrics may\nnot fully reflect network maturity. To address these limitations, we developed\ntubuleTracker, a software tool that quantifies endothelial network architecture\nand maturity rapidly and objectively. Methods: Human umbilical vein endothelial\ncells were cultured in an extracellular matrix, and 54 images were acquired\nusing phase contrast microscopy. Each image was analyzed manually by three\nindependent reviewers, and by both ImageJ and tubuleTracker. Key metrics\nincluded tubule count, total length, node count, tubule area, and vessel\ncircularity. In parallel, trained scientists rated each image for angiogenesis\nmaturity on a 1-5 scale (1 = most mature). Results: Analysis time per image\ndiffered significantly: manual (8 min), ImageJ (58+/-4 s), and tubuleTracker\n(6+/-2 s) (p<0.0001). Significant differences were also found in tubule count\n(manual 168+/-SD, tubuleTracker 92+/-SD, ImageJ 433+/-SD), length, and node\ncount (all p<0.0001). tubuleTracker's metrics varied significantly across\nangiogenesis maturity scores, including tubule count, length, node count, area,\nand circularity (all p<0.0001). Conclusions: tubuleTracker was faster and more\nconsistent than both manual and ImageJ-based analysis. Vessel circularity\nproved especially effective in capturing angiogenesis maturity. tubuleTracker\nis available as free shareware for the biomedical research community.", "comment": "Abstract word count = [285] Total word count = [3910] Main body text\n  = [2179] References = [30] Table = [0] Figures = [4]", "pdf_url": "http://arxiv.org/pdf/2507.02024v1", "cate": "q-bio.QM", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "TubuleTracker：一种用于量化血管生成结构和成熟度的高保真共享软件", "tldr": "TubuleTracker是一款快速、客观、高保真的共享软件，用于量化血管生成网络的结构和成熟度，优于手动和ImageJ分析。", "motivation": "现有的血管生成体外图像分析方法（手动或ImageJ）耗时、主观、不准确，且传统指标可能无法充分反映网络成熟度。", "method": "培养人脐静脉内皮细胞，获取54张图像。使用TubuleTracker、ImageJ和手动三种方法分析图像，测量小管计数、总长度、节点计数、小管面积和血管圆度。同时，专家对血管生成成熟度进行1-5评分。", "result": "TubuleTracker的分析速度（6+/-2秒）远快于手动（8分钟）和ImageJ（58+/-4秒）。TubuleTracker在小管计数、长度和节点计数上与手动和ImageJ存在显著差异。TubuleTracker的各项指标（包括小管计数、长度、节点计数、面积和圆度）随血管生成成熟度评分显著变化，其中血管圆度在捕捉成熟度方面特别有效。", "conclusion": "TubuleTracker比手动和ImageJ分析更快、更一致。血管圆度被证明在捕捉血管生成成熟度方面特别有效。TubuleTracker作为免费共享软件可供生物医学研究社区使用。", "translation": "背景：体外内皮细胞培养广泛用于研究血管生成。细胞网络的组织显微图像通常手动分析，这个过程耗时且主观。像ImageJ (NIH) 这样的自动化工具可以提供帮助，但通常速度慢且不准确。此外，随着内皮网络变得更加复杂，传统的结构指标可能无法完全反映网络成熟度。为了解决这些限制，我们开发了tubuleTracker，这是一种可以快速客观地量化内皮网络结构和成熟度的软件工具。\n方法：将人脐静脉内皮细胞在细胞外基质中培养，并使用相差显微镜获取了54张图像。每张图像由三名独立评审员手动分析，并分别由ImageJ和tubuleTracker进行分析。关键指标包括小管计数、总长度、节点计数、小管面积和血管圆度。同时，受过训练的科学家根据1-5的等级（1 = 最成熟）对每张图像的血管生成成熟度进行评分。\n结果：每张图像的分析时间显著不同：手动（8分钟）、ImageJ（58+/-4秒）和tubuleTracker（6+/-2秒）（p<0.0001）。在小管计数（手动168+/-SD，tubuleTracker 92+/-SD，ImageJ 433+/-SD）、长度和节点计数方面也发现了显著差异（所有p<0.0001）。tubuleTracker的指标在血管生成成熟度评分中表现出显著差异，包括小管计数、长度、节点计数、面积和圆度（所有p<0001）。\n结论：tubuleTracker比手动和基于ImageJ的分析更快、更一致。血管圆度被证明在捕捉血管生成成熟度方面特别有效。tubuleTracker作为免费共享软件可供生物医学研究社区使用。", "summary": "本文介绍了TubuleTracker，一款高保真共享软件，旨在解决体外血管生成图像分析中手动和ImageJ方法的耗时、主观和不准确性问题。该软件能够快速客观地量化内皮网络结构和成熟度，并通过实验证明其分析速度和一致性均优于传统方法，特别是血管圆度在评估血管生成成熟度方面表现出色。", "keywords": "血管生成, 图像分析, TubuleTracker, 软件, 成熟度", "comments": "TubuleTracker的创新之处在于提供了一个快速、客观且高保真的自动化工具，显著提高了血管生成图像分析的效率和准确性。其引入新的成熟度指标，特别是血管圆度，弥补了传统结构指标的不足。作为免费共享软件发布，将极大地促进生物医学研究。"}}
{"id": "2507.02248", "title": "Transfer Learning for Matrix Completion", "authors": ["Dali Liu", "Haolei Weng"], "categories": ["stat.ML", "cs.LG", "15A83", "I.2.6; G.3"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      37 pages, 1 figure", "url": "http://arxiv.org/abs/2507.02248v1", "summary": "In this paper, we explore the knowledge transfer under the setting of matrix\ncompletion, which aims to enhance the estimation of a low-rank target matrix\nwith auxiliary data available. We propose a transfer learning procedure given\nprior information on which source datasets are favorable. We study its\nconvergence rates and prove its minimax optimality. Our analysis reveals that\nwith the source matrices close enough to the target matrix, out method\noutperforms the traditional method using the single target data. In particular,\nwe leverage the advanced sharp concentration inequalities introduced in\n\\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the\nconvergence rate, which is crucial for proving the minimax optimality. When the\nrelevance of source datasets is unknown, we develop an efficient detection\nprocedure to identify informative sources and establish its selection\nconsistency. Simulations and real data analysis are conducted to support the\nvalidity of our methodology.", "comment": "37 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.02248v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "矩阵补全的迁移学习", "tldr": "本文探讨了矩阵补全中的知识迁移，提出了一种新的迁移学习方法，证明了其收敛性和最优性，并在源数据相关时优于传统方法。", "motivation": "旨在利用辅助数据增强低秩目标矩阵的估计，解决传统矩阵补全方法在数据有限时的局限性。", "method": "提出了一种迁移学习过程，该过程在已知哪些源数据集有利的情况下进行。研究了其收敛率并证明了其极小极大最优性。利用先进的尖锐集中不等式消除了收敛率中的对数因子。当源数据集的相关性未知时，开发了一种有效的检测程序来识别信息源并建立其选择一致性。", "result": "当源矩阵与目标矩阵足够接近时，所提出的方法优于仅使用单一目标数据的传统方法。通过利用尖锐集中不等式，消除了收敛率中的对数因子，这对于证明极小极大最优性至关重要。仿真和真实数据分析支持了该方法的有效性。", "conclusion": "本文提出的迁移学习方法在矩阵补全中表现出色，尤其是在源数据相关且已知或可识别时，能够显著提升估计性能并达到最优性。", "translation": "**标题**：矩阵补全的迁移学习\n\n**摘要**：在本文中，我们探讨了矩阵补全背景下的知识迁移，旨在利用可用的辅助数据增强低秩目标矩阵的估计。我们提出了一种迁移学习程序，该程序基于对哪些源数据集有利的先验信息。我们研究了其收敛率并证明了其极小极大最优性。我们的分析表明，当源矩阵与目标矩阵足够接近时，我们的方法优于使用单一目标数据的传统方法。特别是，我们利用[brailovskaya2024universality]中引入的先进尖锐集中不等式消除了收敛率中的对数因子，这对于证明极小极大最优性至关重要。当源数据集的相关性未知时，我们开发了一种有效的检测程序来识别信息源并建立其选择一致性。进行了仿真和真实数据分析以支持我们方法的有效性。", "summary": "本文研究了矩阵补全中的知识迁移问题，旨在通过辅助数据提升低秩目标矩阵的估计精度。作者提出了一种新的迁移学习方法，该方法在已知有利源数据集的情况下，被证明具有优越的收敛率和极小极大最优性，并能超越传统方法。特别地，该方法利用先进的集中不等式消除了收敛率中的对数因子。此外，针对源数据集相关性未知的情况，论文还开发了一种有效的源识别程序。仿真和真实数据实验验证了该方法的有效性。", "keywords": "矩阵补全, 迁移学习, 收敛率, 极小极大最优性, 知识迁移", "comments": "本文的创新之处在于将迁移学习引入矩阵补全领域，并从理论上证明了其收敛率和极小极大最优性。特别是，通过引入尖锐集中不等式来消除对数因子，提升了理论结果的精确性。此外，对于未知源相关性的情况提供了解决方案，增加了方法的实用性。"}}
{"id": "2507.02264", "title": "NLP4Neuro: Sequence-to-sequence learning for neural population decoding", "authors": ["Jacob J. Morra", "Kaitlyn E. Fouke", "Kexin Hang", "Zichen He", "Owen Traubert", "Timothy W. Dunn", "Eva A. Naumann"], "categories": ["q-bio.NC", "cs.LG"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      17 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02264v1", "summary": "Delineating how animal behavior arises from neural activity is a foundational\ngoal of neuroscience. However, as the computations underlying behavior unfold\nin networks of thousands of individual neurons across the entire brain, this\npresents challenges for investigating neural roles and computational mechanisms\nin large, densely wired mammalian brains during behavior. Transformers, the\nbackbones of modern large language models (LLMs), have become powerful tools\nfor neural decoding from smaller neural populations. These modern LLMs have\nbenefited from extensive pre-training, and their sequence-to-sequence learning\nhas been shown to generalize to novel tasks and data modalities, which may also\nconfer advantages for neural decoding from larger, brain-wide activity\nrecordings. Here, we present a systematic evaluation of off-the-shelf LLMs to\ndecode behavior from brain-wide populations, termed NLP4Neuro, which we used to\ntest LLMs on simultaneous calcium imaging and behavior recordings in larval\nzebrafish exposed to visual motion stimuli. Through NLP4Neuro, we found that\nLLMs become better at neural decoding when they use pre-trained weights learned\nfrom textual natural language data. Moreover, we found that a recent\nmixture-of-experts LLM, DeepSeek Coder-7b, significantly improved behavioral\ndecoding accuracy, predicted tail movements over long timescales, and provided\nanatomically consistent highly interpretable readouts of neuron salience.\nNLP4Neuro demonstrates that LLMs are highly capable of informing brain-wide\nneural circuit dissection.", "comment": "17 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02264v1", "cate": "q-bio.NC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "NLP4Neuro：用于神经群体解码的序列到序列学习", "tldr": "该研究提出并评估了NLP4Neuro，一个使用预训练大型语言模型（LLMs）从全脑神经活动中解码行为的框架，发现LLMs，特别是DeepSeek Coder-7b，在解码斑马鱼行为方面表现出色，并能提供可解释的神经元显著性信息。", "motivation": "神经科学的一个基本目标是描绘动物行为如何从神经活动中产生。然而，在大型、密集连接的哺乳动物大脑中研究神经作用和计算机制面临挑战。Transformer模型已成为神经解码的强大工具，但其在更大规模全脑活动记录中的潜力尚未充分探索。本研究旨在系统评估现成的LLMs在全脑神经群体解码行为中的能力。", "method": "研究提出了NLP4Neuro框架，用于系统评估现成的LLMs从全脑神经群体中解码行为。该框架在斑马鱼幼体暴露于视觉运动刺激时的同步钙成像和行为记录数据上进行了测试。研究特别评估了使用从文本自然语言数据中学习到的预训练权重的LLMs。", "result": "研究发现，使用从文本自然语言数据中学习到的预训练权重的LLMs在神经解码方面表现更好。此外，最近的混合专家LLM DeepSeek Coder-7b显著提高了行为解码准确性，预测了长时间尺度上的尾部运动，并提供了在解剖学上一致且高度可解释的神经元显著性读数。", "conclusion": "NLP4Neuro证明了大型语言模型（LLMs）在辅助全脑神经回路剖析方面具有很强的能力。", "translation": "描绘动物行为如何从神经活动中产生是神经科学的一个基本目标。然而，由于行为背后的计算是在整个大脑中数千个独立神经元的网络中展开的，这给在行为过程中研究大型、密集连接的哺乳动物大脑中的神经作用和计算机制带来了挑战。作为现代大型语言模型（LLMs）主干的Transformer模型，已经成为从小神经元群体中进行神经解码的强大工具。这些现代LLMs受益于广泛的预训练，其序列到序列学习已被证明可以推广到新任务和数据模态，这可能也为从更大规模、全脑活动记录中进行神经解码带来优势。在此，我们提出了对现成LLMs进行系统评估的方法，以从全脑群体中解码行为，我们称之为NLP4Neuro，我们用它来测试LLMs在暴露于视觉运动刺激的斑马鱼幼体中同步钙成像和行为记录数据上的表现。通过NLP4Neuro，我们发现LLMs在使用从文本自然语言数据中学习到的预训练权重时，在神经解码方面表现更好。此外，我们发现最近的混合专家LLM DeepSeek Coder-7b显著提高了行为解码准确性，预测了长时间尺度上的尾部运动，并提供了在解剖学上一致且高度可解释的神经元显著性读数。NLP4Neuro表明LLMs在辅助全脑神经回路剖析方面具有很强的能力。", "summary": "本研究介绍了NLP4Neuro，一个利用预训练大型语言模型（LLMs）进行全脑神经群体行为解码的系统评估框架。通过在斑马鱼幼体全脑钙成像和行为数据上进行测试，研究发现LLMs，特别是利用文本数据预训练权重的模型，在神经解码任务中表现出卓越的性能。其中，DeepSeek Coder-7b不仅显著提升了行为解码精度，还能够预测长期尾部运动，并提供具有生物学意义的神经元显著性解释，突显了LLMs在理解全脑神经回路中的巨大潜力。", "keywords": "神经解码, 大型语言模型, 序列到序列学习, 全脑成像, 斑马鱼", "comments": "这项工作创新性地将大型语言模型（LLMs）应用于全脑尺度的神经活动解码，突破了传统方法在处理大规模神经数据时的局限性。通过利用LLMs的序列到序列学习能力和预训练优势，该研究为理解复杂的神经回路如何产生行为提供了新的视角和强大的工具。特别是DeepSeek Coder-7b的成功应用，展现了混合专家模型在神经科学领域的潜力，其提供的可解释性读数对于神经回路的深入剖析具有重要意义。"}}
{"id": "2507.02275", "title": "It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation", "authors": ["Jikai Jin", "Lester Mackey", "Vasilis Syrgkanis"], "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02275v1", "summary": "Structure-agnostic causal inference studies how well one can estimate a\ntreatment effect given black-box machine learning estimates of nuisance\nfunctions (like the impact of confounders on treatment and outcomes). Here, we\nfind that the answer depends in a surprising way on the distribution of the\ntreatment noise. Focusing on the partially linear model of\n\\citet{robinson1988root}, we first show that the widely adopted double machine\nlearning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,\nresolving an open problem of \\citet{mackey2018orthogonal}. Meanwhile, for\nindependent non-Gaussian treatment noise, we show that DML is always suboptimal\nby constructing new practical procedures with higher-order robustness to\nnuisance errors. These \\emph{ACE} procedures use structure-agnostic cumulant\nestimators to achieve $r$-th order insensitivity to nuisance errors whenever\nthe $(r+1)$-st treatment cumulant is non-zero. We complement these core results\nwith novel minimax guarantees for binary treatments in the partially linear\nmodel. Finally, using synthetic demand estimation experiments, we demonstrate\nthe practical benefits of our higher-order robust estimators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02275v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "很难正常：噪声对结构无关估计的影响", "tldr": "本文研究了在结构无关因果推断中，处理噪声分布如何影响处理效应估计。发现对于高斯噪声，DML是极小极大率最优的；但对于非高斯噪声，DML是次优的，并提出了新的ACE程序，提供更高阶的鲁棒性。", "motivation": "研究在结构无关因果推断中，处理噪声的分布如何影响处理效应的估计，并解决DML估计器在不同噪声条件下的性能问题，特别是对于非高斯噪声的情况。", "method": "聚焦于部分线性模型，证明了双重机器学习（DML）估计器在高斯处理噪声下是极小极大率最优的。针对独立的非高斯处理噪声，构建了新的“ACE”程序，这些程序利用结构无关的累积量估计器，实现了对混杂误差更高阶的鲁棒性。补充了部分线性模型中二元处理的新的极小极大保证，并通过合成需求估计实验验证了新方法的实际效益。", "result": "双重机器学习（DML）估计器对于高斯处理噪声是极小极大率最优的，解决了Mackey等人的一个开放问题。对于独立的非高斯处理噪声，DML总是次优的。提出了新的“ACE”程序，这些程序通过结构无关的累积量估计器，在(r+1)阶处理累积量非零时，实现了对混杂误差的r阶不敏感性，提供了更高阶的鲁棒性。提供了部分线性模型中二元处理的新的极小极大保证。合成需求估计实验证明了所提出的更高阶鲁棒估计器的实际益处。", "conclusion": "处理噪声的分布对结构无关因果推断中的处理效应估计具有重要影响。虽然DML对于高斯噪声是最佳的，但对于非高斯噪声，需要采用新的、更高阶鲁棒的估计方法（如ACE程序）才能达到最优性能。", "translation": "结构无关因果推断研究的是，在给定混杂函数（如混杂因素对处理和结果的影响）的黑盒机器学习估计的情况下，能多好地估计处理效应。在此，我们发现答案以一种令人惊讶的方式取决于处理噪声的分布。我们以\n\n\\citet{robinson1988root}的部分线性模型为例，首先证明了广泛采用的双重机器学习（DML）估计器对于高斯处理噪声是极小极大率最优的，解决了\n\n\\citet{mackey2018orthogonal}的一个开放问题。同时，对于独立的非高斯处理噪声，我们通过构建对混杂误差具有更高阶鲁棒性的新实用程序，证明了DML始终是次优的。这些“ACE”程序使用结构无关的累积量估计器，只要(r+1)阶处理累积量非零，就能实现对混杂误差的r阶不敏感性。我们用部分线性模型中二元处理的新极小极大保证来补充这些核心结果。最后，通过合成需求估计实验，我们展示了我们更高阶鲁棒估计器的实际益处。", "summary": "本文探讨了处理噪声分布对结构无关因果推断中处理效应估计的影响。研究发现，在部分线性模型中，双重机器学习（DML）估计器对高斯处理噪声是极小极大率最优的，但对非高斯处理噪声则表现次优。为解决此问题，作者提出了名为“ACE”的新程序，这些程序利用结构无关的累积量估计器，实现了对混杂误差更高阶的鲁棒性。实验证明了这些新估计器的实际效用。", "keywords": "结构无关因果推断, 处理噪声, 双重机器学习, 累积量估计器, 鲁棒性", "comments": "这篇论文通过揭示处理噪声分布对结构无关因果推断中估计器性能的关键影响，为该领域带来了重要见解。其创新点在于证明了DML在高斯噪声下的最优性，并针对非高斯噪声提出了新颖的、具有更高阶鲁棒性的ACE程序，这对于实际应用中处理非标准噪声分布的数据具有重要意义。该研究不仅解决了现有估计器的局限性，还通过理论分析和实验验证提供了实用的解决方案。"}}
{"id": "2507.02644", "title": "Solving the Hubbard model with Neural Quantum States", "authors": ["Yuntian Gu", "Wenrui Li", "Heng Lin", "Bo Zhan", "Ruichen Li", "Yifei Huang", "Di He", "Yantao Wu", "Tao Xiang", "Mingpu Qin", "Liwei Wang", "Dingshun Lv"], "categories": ["cond-mat.str-el", "cs.AI", "quant-ph"], "primary_category": "Subjects:       Strongly Correlated Electrons (cond-mat.str-el)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02644v1", "summary": "The rapid development of neural quantum states (NQS) has established it as a\npromising framework for studying quantum many-body systems. In this work, by\nleveraging the cutting-edge transformer-based architectures and developing\nhighly efficient optimization algorithms, we achieve the state-of-the-art\nresults for the doped two-dimensional (2D) Hubbard model, arguably the minimum\nmodel for high-Tc superconductivity. Interestingly, we find different attention\nheads in the NQS ansatz can directly encode correlations at different scales,\nmaking it capable of capturing long-range correlations and entanglements in\nstrongly correlated systems. With these advances, we establish the half-filled\nstripe in the ground state of 2D Hubbard model with the next nearest\nneighboring hoppings, consistent with experimental observations in cuprates.\nOur work establishes NQS as a powerful tool for solving challenging\nmany-fermions systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02644v1", "cate": "cond-mat.str-el", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用神经网络量子态求解哈伯德模型", "tldr": "利用基于Transformer的神经网络量子态，实现了二维掺杂Hubbard模型的SOTA结果，并发现了NQS能编码不同尺度关联，且确认了二维Hubbard模型基态中的半填充条纹。", "motivation": "研究量子多体系统，特别是高温超导的最小模型——掺杂二维Hubbard模型，并推动神经网络量子态(NQS)在解决这些挑战性问题上的应用。", "method": "利用尖端基于Transformer的架构和高效优化算法开发神经网络量子态(NQS)来解决掺杂二维Hubbard模型。研究了NQS中不同注意力头编码不同尺度关联的能力。", "result": "在掺杂二维Hubbard模型上取得了最先进的结果。发现NQS中的不同注意力头可以直接编码不同尺度的关联，使其能够捕获强关联系统中的长程关联和纠缠。在具有次近邻跳跃的二维Hubbard模型的基态中确定了半填充条纹，这与铜酸盐中的实验观察结果一致。", "conclusion": "该工作确立了神经网络量子态(NQS)作为解决挑战性多费米子系统的强大工具。", "translation": "神经网络量子态（NQS）的快速发展使其成为研究量子多体系统的一个有前景的框架。在这项工作中，通过利用尖端的基于Transformer的架构和开发高效的优化算法，我们为掺杂二维（2D）Hubbard模型（可以说是高温超导的最小模型）取得了最先进的结果。有趣的是，我们发现NQS猜想中不同的注意力头可以直接编码不同尺度的关联，使其能够捕获强关联系统中的长程关联和纠缠。凭借这些进展，我们在具有次近邻跳跃的二维Hubbard模型的基态中确定了半填充条纹，这与铜酸盐中的实验观察结果一致。我们的工作确立了NQS作为解决挑战性多费米子系统的强大工具。", "summary": "本研究利用基于Transformer的神经网络量子态（NQS）和高效优化算法，在掺杂二维Hubbard模型上取得了最先进的结果。研究发现NQS中的注意力头能编码不同尺度的关联，从而捕获强关联系统中的长程关联。此外，还在二维Hubbard模型的基态中确定了半填充条纹，与实验结果吻合，证明了NQS是解决多费米子问题的有效工具。", "keywords": "神经网络量子态, Hubbard模型, Transformer, 高温超导, 强关联系统", "comments": "这项工作通过引入基于Transformer的架构和高效优化算法，显著提升了NQS在解决Hubbard模型方面的性能。特别值得注意的是，发现NQS的注意力头能够编码不同尺度的关联，这不仅解释了其捕获复杂关联的能力，也为未来NQS的设计提供了新的方向。确认二维Hubbard模型基态中的半填充条纹，也进一步验证了模型的有效性，并与实验观察建立了联系，具有重要的物理意义。"}}
{"id": "2507.02377", "title": "Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited", "authors": ["Thang D. Bui", "Michalis K. Titsias"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02377v1", "summary": "Inducing-point-based sparse variational Gaussian processes have become the\nstandard workhorse for scaling up GP models. Recent advances show that these\nmethods can be improved by introducing a diagonal scaling matrix to the\nconditional posterior density given the inducing points. This paper first\nconsiders an extension that employs a block-diagonal structure for the scaling\nmatrix, provably tightening the variational lower bound. We then revisit the\nunifying framework of sparse GPs based on Power Expectation Propagation (PEP)\nand show that it can leverage and benefit from the new structured approximate\nposteriors. Through extensive regression experiments, we show that the proposed\nblock-diagonal approximation consistently performs similarly to or better than\nexisting diagonal approximations while maintaining comparable computational\ncosts. Furthermore, the new PEP framework with structured posteriors provides\ncompetitive performance across various power hyperparameter settings, offering\npractitioners flexible alternatives to standard variational approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02377v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "稀疏高斯过程：结构化近似与幂期望传播再探", "tldr": "本文提出了一种新的块对角结构化近似方法，用于稀疏变分高斯过程，并将其整合到幂期望传播（PEP）框架中，实验证明其性能优于或媲美现有方法且计算成本相当。", "motivation": "为了改进基于诱导点的稀疏变分高斯过程的性能，特别是通过引入更有效的条件后验密度对角缩放矩阵来进一步收紧变分下界。", "method": "首先，提出了一种将对角缩放矩阵扩展为块对角结构的方法，并证明其能收紧变分下界。其次，将这种新的结构化近似后验方法整合到基于幂期望传播（PEP）的稀疏高斯过程统一框架中。", "result": "所提出的块对角近似方法在回归实验中始终表现出与现有对角近似方法相似或更优的性能，同时保持可比的计算成本。此外，带有结构化后验的新PEP框架在各种幂超参数设置下提供了具有竞争力的性能。", "conclusion": "块对角结构化近似能有效提升稀疏高斯过程的性能，并能与幂期望传播框架结合，为实际应用提供灵活且性能优越的替代方案。", "translation": "基于诱导点的稀疏变分高斯过程已成为扩展高斯过程模型的标准工具。最近的进展表明，通过在给定诱导点的情况下引入对角缩放矩阵到条件后验密度中，可以改进这些方法。本文首先考虑了一种采用块对角结构作为缩放矩阵的扩展，可证明地收紧了变分下界。然后，我们重新审视了基于幂期望传播（PEP）的稀疏高斯过程的统一框架，并表明它可以利用并受益于新的结构化近似后验。通过大量的回归实验，我们表明所提出的块对角近似始终表现出与现有对角近似相似或更优的性能，同时保持可比的计算成本。此外，带有结构化后验的新PEP框架在各种幂超参数设置下提供了具有竞争力的性能，为实践者提供了标准变分方法的灵活替代方案。", "summary": "本文提出了一种用于稀疏变分高斯过程的块对角结构化近似方法，该方法通过收紧变分下界来改进现有诱导点方法。研究进一步将此近似整合到幂期望传播（PEP）框架中。实验结果表明，该块对角近似在性能上优于或媲美现有对角近似，且计算成本相当。新的PEP框架结合结构化后验，在不同超参数设置下也表现出竞争力，为高斯过程实践提供了新的灵活选择。", "keywords": "稀疏高斯过程, 结构化近似, 幂期望传播, 变分推断, 块对角矩阵", "comments": "本文的创新点在于提出了块对角结构化近似，并证明其能收紧变分下界，这对于提高稀疏高斯过程的准确性具有重要意义。此外，将其与幂期望传播框架结合，扩展了现有方法的应用范围和灵活性。该研究在保持计算效率的同时提升了模型性能，具有较高的实用价值。"}}
{"id": "2507.02752", "title": "Synthesizable by Design: A Retrosynthesis-Guided Framework for Molecular Analog Generation", "authors": ["Shuan Chen", "Gunwook Nam", "Yousung Jung"], "categories": ["physics.chem-ph", "cs.AI"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02752v1", "summary": "The disconnect between AI-generated molecules with desirable properties and\ntheir synthetic feasibility remains a critical bottleneck in computational drug\nand material discovery. While generative AI has accelerated the proposal of\ncandidate molecules, many of these structures prove challenging or impossible\nto synthesize using established chemical reactions. Here, we introduce\nSynTwins, a novel retrosynthesis-guided molecular analog design framework that\ndesigns synthetically accessible molecular analogs by emulating expert chemist\nstrategies through a three-step process: retrosynthesis, similar building block\nsearching, and virtual synthesis. In comparative evaluations, SynTwins\ndemonstrates superior performance in generating synthetically accessible\nanalogs compared to state-of-the-art machine learning models while maintaining\nhigh structural similarity to original target molecules. Furthermore, when\nintegrated with existing molecule optimization frameworks, our hybrid approach\nproduces synthetically feasible molecules with property profiles comparable to\nunconstrained molecule generators, yet its synthesizability ensured. Our\ncomprehensive benchmarking across diverse molecular datasets demonstrates that\nSynTwins effectively bridges the gap between computational design and\nexperimental synthesis, providing a practical solution for accelerating the\ndiscovery of synthesizable molecules with desired properties for a wide range\nof applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02752v1", "cate": "physics.chem-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可设计合成：一种逆合成引导的分子类似物生成框架", "tldr": "SynTwins是一个逆合成引导的分子类似物设计框架，通过模拟专家化学家策略，解决了AI生成分子与合成可行性之间的脱节问题，能够生成可合成的分子类似物，并优于现有模型。", "motivation": "AI生成的分子在药物和材料发现中，其合成可行性与理想性质之间存在脱节，许多AI提出的结构难以或无法合成，这是计算发现中的一个关键瓶颈。", "method": "本文引入了SynTwins，一种新颖的逆合成引导分子类似物设计框架。它通过模拟专家化学家策略，采用三步过程：逆合成、相似构建块搜索和虚拟合成，来设计可合成的分子类似物。", "result": "在对比评估中，SynTwins在生成可合成类似物方面表现出优于最先进机器学习模型的性能，同时保持了与原始目标分子的高度结构相似性。当与现有分子优化框架结合时，其混合方法能生成与无约束分子生成器性能相当但合成性得到保证的可合成分子。", "conclusion": "SynTwins有效弥合了计算设计与实验合成之间的鸿沟，为加速发现具有所需性质的可合成分子提供了一个实用解决方案。", "translation": "AI生成的具有所需性质的分子与它们的合成可行性之间的脱节仍然是计算药物和材料发现中的一个关键瓶颈。尽管生成式AI加速了候选分子的提出，但其中许多结构被证明使用既有化学反应难以或无法合成。在此，我们引入了SynTwins，一种新颖的逆合成引导分子类似物设计框架，它通过逆合成、相似构建块搜索和虚拟合成这三步过程，模拟专家化学家策略，设计可合成的分子类似物。在比较评估中，SynTwins在生成可合成类似物方面表现出优于最先进机器学习模型的性能，同时保持了与原始目标分子的高度结构相似性。此外，当与现有分子优化框架集成时，我们的混合方法能够生成具有与无约束分子生成器相当的性质分布，但其合成性得到保证的可合成分子。我们对各种分子数据集的全面基准测试表明，SynTwins有效地弥合了计算设计与实验合成之间的鸿沟，为加速发现具有所需性质的可合成分子提供了实用解决方案，适用于广泛的应用。", "summary": "SynTwins是一个逆合成引导的分子类似物设计框架，旨在解决AI生成分子与实际合成可行性之间的脱节。它通过模拟专家化学家策略的三步法（逆合成、构建块搜索、虚拟合成）来设计可合成的分子。实验证明，SynTwins在生成可合成类似物方面优于现有模型，并能与分子优化框架结合，确保生成分子的合成可行性，从而弥合了计算设计与实验合成的差距。", "keywords": "逆合成, 分子生成, 合成可行性, 药物发现, SynTwins", "comments": "SynTwins的创新之处在于其逆合成引导的设计理念，它通过模仿人类化学家的思维过程，将合成可行性内嵌到分子生成过程中，而不是作为后处理步骤。这对于加速药物和材料发现至关重要，因为它直接解决了AI在这一领域应用中的关键瓶颈。其与现有优化框架的集成能力也增加了其实用性。"}}
{"id": "2507.02801", "title": "Learning to Coordinate Bidders in Non-Truthful Auctions", "authors": ["Hu Fu", "Tao Lin"], "categories": ["cs.GT", "cs.LG", "econ.TH"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02801v1", "summary": "In non-truthful auctions such as first-price and all-pay auctions, the\nindependent strategic behaviors of bidders, with the corresponding equilibrium\nnotion -- Bayes Nash equilibria -- are notoriously difficult to characterize\nand can cause undesirable outcomes. An alternative approach to designing better\nauction systems is to coordinate the bidders: let a mediator make\nincentive-compatible recommendations of correlated bidding strategies to the\nbidders, namely, implementing a Bayes correlated equilibrium (BCE). The\nimplementation of BCE, however, requires knowledge of the distribution of\nbidders' private valuations, which is often unavailable. We initiate the study\nof the sample complexity of learning Bayes correlated equilibria in\nnon-truthful auctions. We prove that the BCEs in a large class of non-truthful\nauctions, including first-price and all-pay auctions, can be learned with a\npolynomial number $\\tilde O(\\frac{n}{\\varepsilon^2})$ of samples from the\nbidders' value distributions. Our technique is a reduction to the problem of\nestimating bidders' expected utility from samples, combined with an analysis of\nthe pseudo-dimension of the class of all monotone bidding strategies of\nbidders.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02801v1", "cate": "cs.GT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "学习在非真实性拍卖中协调竞标者", "tldr": "本文研究了在非真实性拍卖（如第一价格和全付拍卖）中学习贝叶斯相关均衡（BCE）的样本复杂度。作者证明了在一大类非真实性拍卖中，BCE 可以通过多项式数量的样本学习得到，这通过将问题简化为估计竞标者预期效用并分析单调竞标策略的伪维度来实现。", "motivation": "在第一价格和全付拍卖等非真实性拍卖中，竞标者的独立策略行为及其对应的贝叶斯纳什均衡难以刻画，并可能导致不良结果。通过协调竞标者（即实施贝叶斯相关均衡）是设计更好拍卖系统的一种替代方法，但实施BCE通常需要竞标者私人估值分布的知识，而这些知识通常不可用。因此，本文旨在研究在缺乏估值分布知识的情况下学习BCE的样本复杂度。", "method": "本文通过研究在非真实性拍卖中学习贝叶斯相关均衡（BCE）的样本复杂度来解决问题。其技术是将问题简化为从样本中估计竞标者预期效用的问题，并结合对竞标者所有单调竞标策略类别伪维度的分析。", "result": "作者证明了在一大类非真实性拍卖（包括第一价格和全付拍卖）中，贝叶斯相关均衡（BCE）可以通过多项式数量的样本（$\tilde O(\\frac{n}{\\varepsilon^2})$）从竞标者估值分布中学习得到。", "conclusion": "本文开创了在非真实性拍卖中学习贝叶斯相关均衡样本复杂度的研究，并证明了在一大类此类拍卖中，BCE是可学习的，为在缺乏完全信息的情况下设计更优的拍卖系统提供了理论基础。", "translation": "在第一价格和全付拍卖等非真实性拍卖中，竞标者的独立策略行为及其对应的均衡概念——贝叶斯纳什均衡——以难以刻画著称，并可能导致不良结果。设计更好的拍卖系统的一种替代方法是协调竞标者：让调解人向竞标者提供激励兼容的关联竞标策略建议，即实施贝叶斯相关均衡（BCE）。然而，BCE的实施需要了解竞标者私人估值的分布，而这通常是不可用的。我们开创了在非真实性拍卖中学习贝叶斯相关均衡样本复杂度的研究。我们证明，包括第一价格和全付拍卖在内的一大类非真实性拍卖中的BCE，可以通过竞标者估值分布的$\tilde O(\\frac{n}{\\varepsilon^2})$多项式数量的样本学习得到。我们的技术是将问题简化为从样本中估计竞标者预期效用的问题，并结合对竞标者所有单调竞标策略类别伪维度的分析。", "summary": "本文研究了在第一价格和全付拍卖等非真实性拍卖中学习贝叶斯相关均衡（BCE）的样本复杂度。鉴于传统贝叶斯纳什均衡的局限性以及BCE实施对估值分布知识的需求，作者提出了一种在缺乏此类知识的情况下学习BCE的方法。研究证明，在一大类非真实性拍卖中，BCE可以通过多项式数量的样本有效地学习得到。其核心技术是将问题转化为估计竞标者预期效用，并结合对单调竞标策略伪维度的分析。", "keywords": "非真实性拍卖, 贝叶斯相关均衡, 样本复杂度, 机制设计, 学习理论", "comments": "本文的创新之处在于首次系统性地探讨了在非真实性拍卖中学习贝叶斯相关均衡的样本复杂度问题，并给出了具体的学习效率保证。这对于在实际应用中，当竞标者估值分布未知时，设计和实现更有效的协调机制具有重要意义。该研究为机制设计和机器学习的交叉领域提供了新的视角和工具。"}}
