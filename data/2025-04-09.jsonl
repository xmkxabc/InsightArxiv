{"id": "2504.05325", "pdf": "https://arxiv.org/pdf/2504.05325", "abs": "https://arxiv.org/abs/2504.05325", "authors": ["Shiran Dudy", "Thulasi Tholeti", "Resmi Ramachandranpillai", "Muhammad Ali", "Toby Jia-Jun Li", "Ricardo Baeza-Yates"], "title": "Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have made them a popular\ninformation-seeking tool among end users. However, the statistical training\nmethods for LLMs have raised concerns about their representation of\nunder-represented topics, potentially leading to biases that could influence\nreal-world decisions and opportunities. These biases could have significant\neconomic, social, and cultural impacts as LLMs become more prevalent, whether\nthrough direct interactions--such as when users engage with chatbots or\nautomated assistants--or through their integration into third-party\napplications (as agents), where the models influence decision-making processes\nand functionalities behind the scenes. Our study examines the biases present in\nLLMs recommendations of U.S. cities and towns across three domains: relocation,\ntourism, and starting a business. We explore two key research questions: (i)\nHow similar LLMs responses are, and (ii) How this similarity might favor areas\nwith certain characteristics over others, introducing biases. We focus on the\nconsistency of LLMs responses and their tendency to over-represent or\nunder-represent specific locations. Our findings point to consistent\ndemographic biases in these recommendations, which could perpetuate a\n``rich-get-richer'' effect that widens existing economic disparities."}
{"id": "2504.05410", "pdf": "https://arxiv.org/pdf/2504.05410", "abs": "https://arxiv.org/abs/2504.05410", "authors": ["Benjamin Lipkin", "Benjamin LeBrun", "Jacob Hoover Vigly", "João Loula", "David R. MacIver", "Li Du", "Jason Eisner", "Ryan Cotterell", "Vikash Mansinghka", "Timothy J. O'Donnell", "Alexander K. Lew", "Tim Vieira"], "title": "Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The dominant approach to generating from language models subject to some\nconstraint is locally constrained decoding (LCD), incrementally sampling tokens\nat each time step such that the constraint is never violated. Typically, this\nis achieved through token masking: looping over the vocabulary and excluding\nnon-conforming tokens. There are two important problems with this approach. (i)\nEvaluating the constraint on every token can be prohibitively expensive -- LM\nvocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global\ndistribution over strings, sampling tokens based only on local information,\neven if they lead down dead-end paths. This work introduces a new algorithm\nthat addresses both these problems. First, to avoid evaluating a constraint on\nthe full vocabulary at each step of generation, we propose an adaptive\nrejection sampling algorithm that typically requires orders of magnitude fewer\nconstraint evaluations. Second, we show how this algorithm can be extended to\nproduce low-variance, unbiased estimates of importance weights at a very small\nadditional cost -- estimates that can be soundly used within previously\nproposed sequential Monte Carlo algorithms to correct for the myopic behavior\nof local constraint enforcement. Through extensive empirical evaluation in\ntext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON\ndomains, we show that our approach is superior to state-of-the-art baselines,\nsupporting a broader class of constraints and improving both runtime and\nperformance. Additional theoretical and empirical analyses show that our\nmethod's runtime efficiency is driven by its dynamic use of computation,\nscaling with the divergence between the unconstrained and constrained LM, and\nas a consequence, runtime improvements are greater for better models."}
{"id": "2504.05411", "pdf": "https://arxiv.org/pdf/2504.05411", "abs": "https://arxiv.org/abs/2504.05411", "authors": ["Lingzhi Shen", "Yunfei Long", "Xiaohao Cai", "Guanming Chen", "Imran Razzak", "Shoaib Jameel"], "title": "Less but Better: Parameter-Efficient Fine-Tuning of Large Language Models for Personality Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Personality detection automatically identifies an individual's personality\nfrom various data sources, such as social media texts. However, as the\nparameter scale of language models continues to grow, the computational cost\nbecomes increasingly difficult to manage. Fine-tuning also grows more complex,\nmaking it harder to justify the effort and reliably predict outcomes. We\nintroduce a novel parameter-efficient fine-tuning framework, PersLLM, to\naddress these challenges. In PersLLM, a large language model (LLM) extracts\nhigh-dimensional representations from raw data and stores them in a dynamic\nmemory layer. PersLLM then updates the downstream layers with a replaceable\noutput network, enabling flexible adaptation to various personality detection\nscenarios. By storing the features in the memory layer, we eliminate the need\nfor repeated complex computations by the LLM. Meanwhile, the lightweight output\nnetwork serves as a proxy for evaluating the overall effectiveness of the\nframework, improving the predictability of results. Experimental results on key\nbenchmark datasets like Kaggle and Pandora show that PersLLM significantly\nreduces computational cost while maintaining competitive performance and strong\nadaptability."}
{"id": "2504.05420", "pdf": "https://arxiv.org/pdf/2504.05420", "abs": "https://arxiv.org/abs/2504.05420", "authors": ["Steven Koniaev", "Ori Ernst", "Jackie Chi Kit Cheung"], "title": "PreSumm: Predicting Summarization Performance Without Summarizing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite recent advancements in automatic summarization, state-of-the-art\nmodels do not summarize all documents equally well, raising the question: why?\nWhile prior research has extensively analyzed summarization models, little\nattention has been given to the role of document characteristics in influencing\nsummarization performance. In this work, we explore two key research questions.\nFirst, do documents exhibit consistent summarization quality across multiple\nsystems? If so, can we predict a document's summarization performance without\ngenerating a summary? We answer both questions affirmatively and introduce\nPreSumm, a novel task in which a system predicts summarization performance\nbased solely on the source document. Our analysis sheds light on common\nproperties of documents with low PreSumm scores, revealing that they often\nsuffer from coherence issues, complex content, or a lack of a clear main theme.\nIn addition, we demonstrate PreSumm's practical utility in two key\napplications: improving hybrid summarization workflows by identifying documents\nthat require manual summarization and enhancing dataset quality by filtering\noutliers and noisy documents. Overall, our findings highlight the critical role\nof document properties in summarization performance and offer insights into the\nlimitations of current systems that could serve as the basis for future\nimprovements."}
{"id": "2504.05400", "pdf": "https://arxiv.org/pdf/2504.05400", "abs": "https://arxiv.org/abs/2504.05400", "authors": ["Sihang Li", "Zeyu Jiang", "Grace Chen", "Chenyang Xu", "Siqi Tan", "Xue Wang", "Irving Fang", "Kristof Zyskowski", "Shannon P. McPherron", "Radu Iovita", "Chen Feng", "Jing Zhang"], "title": "GARF: Learning Generalizable 3D Reassembly for Real-World Fractures", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 11 figures. Project Page https://ai4ce.github.io/GARF/", "summary": "3D reassembly is a challenging spatial intelligence task with broad\napplications across scientific domains. While large-scale synthetic datasets\nhave fueled promising learning-based approaches, their generalizability to\ndifferent domains is limited. Critically, it remains uncertain whether models\ntrained on synthetic datasets can generalize to real-world fractures where\nbreakage patterns are more complex. To bridge this gap, we propose GARF, a\ngeneralizable 3D reassembly framework for real-world fractures. GARF leverages\nfracture-aware pretraining to learn fracture features from individual\nfragments, with flow matching enabling precise 6-DoF alignments. At inference\ntime, we introduce one-step preassembly, improving robustness to unseen objects\nand varying numbers of fractures. In collaboration with archaeologists,\npaleoanthropologists, and ornithologists, we curate Fractura, a diverse dataset\nfor vision and learning communities, featuring real-world fracture types across\nceramics, bones, eggshells, and lithics. Comprehensive experiments have shown\nour approach consistently outperforms state-of-the-art methods on both\nsynthetic and real-world datasets, achieving 82.87\\% lower rotation error and\n25.15\\% higher part accuracy. This sheds light on training on synthetic data to\nadvance real-world 3D puzzle solving, demonstrating its strong generalization\nacross unseen object shapes and diverse fracture types."}
{"id": "2504.05496", "pdf": "https://arxiv.org/pdf/2504.05496", "abs": "https://arxiv.org/abs/2504.05496", "authors": ["Atilla Kaan Alkan", "Shashwat Sourav", "Maja Jablonska", "Simone Astarita", "Rishabh Chakrabarty", "Nikhil Garuda", "Pranav Khetarpal", "Maciej Pióro", "Dimitrios Tanoglidis", "Kartheik G. Iyer", "Mugdha S. Polimera", "Michael J. Smith", "Tirthankar Ghosal", "Marc Huertas-Company", "Sandor Kruk", "Kevin Schawinski", "Ioana Ciucă"], "title": "A Survey on Hypothesis Generation for Scientific Discovery in the Era of Large Language Models", "categories": ["cs.CL", "68T50"], "comment": "9 pages (+2 pages of references), 2 figures", "summary": "Hypothesis generation is a fundamental step in scientific discovery, yet it\nis increasingly challenged by information overload and disciplinary\nfragmentation. Recent advances in Large Language Models (LLMs) have sparked\ngrowing interest in their potential to enhance and automate this process. This\npaper presents a comprehensive survey of hypothesis generation with LLMs by (i)\nreviewing existing methods, from simple prompting techniques to more complex\nframeworks, and proposing a taxonomy that categorizes these approaches; (ii)\nanalyzing techniques for improving hypothesis quality, such as novelty boosting\nand structured reasoning; (iii) providing an overview of evaluation strategies;\nand (iv) discussing key challenges and future directions, including multimodal\nintegration and human-AI collaboration. Our survey aims to serve as a reference\nfor researchers exploring LLMs for hypothesis generation."}
{"id": "2504.05402", "pdf": "https://arxiv.org/pdf/2504.05402", "abs": "https://arxiv.org/abs/2504.05402", "authors": ["Victor Fonte Chavez", "Claudia Esteves", "Jean-Bernard Hayet"], "title": "Time-adaptive Video Frame Interpolation based on Residual Diffusion", "categories": ["cs.CV"], "comment": "17 pages", "summary": "In this work, we propose a new diffusion-based method for video frame\ninterpolation (VFI), in the context of traditional hand-made animation. We\nintroduce three main contributions: The first is that we explicitly handle the\ninterpolation time in our model, which we also re-estimate during the training\nprocess, to cope with the particularly large variations observed in the\nanimation domain, compared to natural videos; The second is that we adapt and\ngeneralize a diffusion scheme called ResShift recently proposed in the\nsuper-resolution community to VFI, which allows us to perform a very low number\nof diffusion steps (in the order of 10) to produce our estimates; The third is\nthat we leverage the stochastic nature of the diffusion process to provide a\npixel-wise estimate of the uncertainty on the interpolated frame, which could\nbe useful to anticipate where the model may be wrong. We provide extensive\ncomparisons with respect to state-of-the-art models and show that our model\noutperforms these models on animation videos."}
{"id": "2504.05506", "pdf": "https://arxiv.org/pdf/2504.05506", "abs": "https://arxiv.org/abs/2504.05506", "authors": ["Ahmed Masry", "Mohammed Saidul Islam", "Mahir Ahmed", "Aayush Bajaj", "Firoz Kabir", "Aaryaman Kartha", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Shadikur Rahman", "Mehrad Shahmohammadi", "Megh Thakkar", "Md Rizwan Parvez", "Enamul Hoque", "Shafiq Joty"], "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Charts are ubiquitous, as people often use them to analyze data, answer\nquestions, and discover critical insights. However, performing complex\nanalytical tasks with charts requires significant perceptual and cognitive\neffort. Chart Question Answering (CQA) systems automate this process by\nenabling models to interpret and reason with visual representations of data.\nHowever, existing benchmarks like ChartQA lack real-world diversity and have\nrecently shown performance saturation with modern large vision-language models\n(LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark\nthat includes 1,341 charts from 157 diverse sources, spanning various chart\ntypes, including infographics and dashboards, and featuring 1,948 questions in\nvarious types, such as multiple-choice, conversational, hypothetical, and\nunanswerable questions, to better reflect real-world challenges. Our\nevaluations with 21 models show a substantial performance drop for LVLMs on\nChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on\nChartQAPro, underscoring the complexity of chart reasoning. We complement our\nfindings with detailed error analyses and ablation studies, identifying key\nchallenges and opportunities for advancing LVLMs in chart understanding and\nreasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro."}
{"id": "2504.05422", "pdf": "https://arxiv.org/pdf/2504.05422", "abs": "https://arxiv.org/abs/2504.05422", "authors": ["Yue Yao", "Mohamed-Khalil Bouzidi", "Daniel Goehring", "Joerg Reichardt"], "title": "EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "As the prediction horizon increases, predicting the future evolution of\ntraffic scenes becomes increasingly difficult due to the multi-modal nature of\nagent motion. Most state-of-the-art (SotA) prediction models primarily focus on\nforecasting the most likely future. However, for the safe operation of\nautonomous vehicles, it is equally important to cover the distribution for\nplausible motion alternatives. To address this, we introduce EP-Diffuser, a\nnovel parameter-efficient diffusion-based generative model designed to capture\nthe distribution of possible traffic scene evolutions. Conditioned on road\nlayout and agent history, our model acts as a predictor and generates diverse,\nplausible scene continuations. We benchmark EP-Diffuser against two SotA models\nin terms of accuracy and plausibility of predictions on the Argoverse 2\ndataset. Despite its significantly smaller model size, our approach achieves\nboth highly accurate and plausible traffic scene predictions. We further\nevaluate model generalization ability in an out-of-distribution (OoD) test\nsetting using Waymo Open dataset and show superior robustness of our approach.\nThe code and model checkpoints can be found here:\nhttps://github.com/continental/EP-Diffuser."}
{"id": "2504.05523", "pdf": "https://arxiv.org/pdf/2504.05523", "abs": "https://arxiv.org/abs/2504.05523", "authors": ["Elisabeth Fittschen", "Sabrina Li", "Tom Lippincott", "Leshem Choshsem", "Craig Messner"], "title": "Pretraining Language Models for Diachronic Linguistic Change Discovery", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation."}
{"id": "2504.05444", "pdf": "https://arxiv.org/pdf/2504.05444", "abs": "https://arxiv.org/abs/2504.05444", "authors": ["Ziad Kheil", "Soleakhena Ken", "Laurent Risser"], "title": "Biomechanical Constraints Assimilation in Deep-Learning Image Registration: Application to sliding and locally rigid deformations", "categories": ["cs.CV", "2008 (Primary) 68U10, 68T99, 62P10, 74L15 (Secondary)"], "comment": null, "summary": "Regularization strategies in medical image registration often take a\none-size-fits-all approach by imposing uniform constraints across the entire\nimage domain. Yet biological structures are anything but regular. Lacking\nstructural awareness, these strategies may fail to consider a panoply of\nspatially inhomogeneous deformation properties, which would faithfully account\nfor the biomechanics of soft and hard tissues, especially in poorly contrasted\nstructures.\n  To bridge this gap, we propose a learning-based image registration approach\nin which the inferred deformation properties can locally adapt themselves to\ntrained biomechanical characteristics. Specifically, we first enforce in the\ntraining process local rigid displacements, shearing motions or pseudo-elastic\ndeformations using regularization losses inspired from the field of\nsolid-mechanics. We then show on synthetic and real 3D thoracic and abdominal\nimages that these mechanical properties of different nature are well\ngeneralized when inferring the deformations between new image pairs. Our\napproach enables neural-networks to infer tissue-specific deformation patterns\ndirectly from input images, ensuring mechanically plausible motion. These\nnetworks preserve rigidity within hard tissues while allowing controlled\nsliding in regions where tissues naturally separate, more faithfully capturing\nphysiological motion. The code is publicly available at\nhttps://github.com/Kheil-Z/biomechanical_DLIR ."}
{"id": "2504.05527", "pdf": "https://arxiv.org/pdf/2504.05527", "abs": "https://arxiv.org/abs/2504.05527", "authors": ["Despina Tomkou", "George Fatouros", "Andreas Andreou", "Georgios Makridis", "Fotis Liarokapis", "Dimitrios Dardanis", "Athanasios Kiourtis", "John Soldatos", "Dimosthenis Kyriazis"], "title": "Bridging Industrial Expertise and XR with LLM-Powered Conversational Agents", "categories": ["cs.CL", "cs.AI", "68T50, 68T40, 68U20, 68U35", "H.5.1; I.2.7; I.2.11; H.3.3; H.5.2; C.3"], "comment": "7 pages, 7 figures", "summary": "This paper introduces a novel integration of Retrieval-Augmented Generation\n(RAG) enhanced Large Language Models (LLMs) with Extended Reality (XR)\ntechnologies to address knowledge transfer challenges in industrial\nenvironments. The proposed system embeds domain-specific industrial knowledge\ninto XR environments through a natural language interface, enabling hands-free,\ncontext-aware expert guidance for workers. We present the architecture of the\nproposed system consisting of an LLM Chat Engine with dynamic tool\norchestration and an XR application featuring voice-driven interaction.\nPerformance evaluation of various chunking strategies, embedding models, and\nvector databases reveals that semantic chunking, balanced embedding models, and\nefficient vector stores deliver optimal performance for industrial knowledge\nretrieval. The system's potential is demonstrated through early implementation\nin multiple industrial use cases, including robotic assembly, smart\ninfrastructure maintenance, and aerospace component servicing. Results indicate\npotential for enhancing training efficiency, remote assistance capabilities,\nand operational guidance in alignment with Industry 5.0's human-centric and\nresilient approach to industrial development."}
{"id": "2504.05451", "pdf": "https://arxiv.org/pdf/2504.05451", "abs": "https://arxiv.org/abs/2504.05451", "authors": ["Arjun Somayazulu", "Efi Mavroudi", "Changan Chen", "Lorenzo Torresani", "Kristen Grauman"], "title": "Learning Activity View-invariance Under Extreme Viewpoint Changes via Curriculum Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Traditional methods for view-invariant learning from video rely on controlled\nmulti-view settings with minimal scene clutter. However, they struggle with\nin-the-wild videos that exhibit extreme viewpoint differences and share little\nvisual content. We introduce a method for learning rich video representations\nin the presence of such severe view-occlusions. We first define a\ngeometry-based metric that ranks views at a fine-grained temporal scale by\ntheir likely occlusion level. Then, using those rankings, we formulate a\nknowledge distillation objective that preserves action-centric semantics with a\nnovel curriculum learning procedure that pairs incrementally more challenging\nviews over time, thereby allowing smooth adaptation to extreme viewpoint\ndifferences. We evaluate our approach on two tasks, outperforming SOTA models\non both temporal keystep grounding and fine-grained keystep recognition\nbenchmarks - particularly on views that exhibit severe occlusion."}
{"id": "2504.05535", "pdf": "https://arxiv.org/pdf/2504.05535", "abs": "https://arxiv.org/abs/2504.05535", "authors": ["M-A-P Team", "Siwei Wu", "Jincheng Ren", "Xinrun Du", "Shuyue Guo", "Xingwei Qu", "Yiming Liang", "Jie Liu", "Yunwen Li", "Tianyu Zheng", "Boyu Feng", "Huaqing Yuan", "Zenith Wang", "Jiaheng Liu", "Wenhao Huang", "Chenglin Cai", "Haoran Que", "Jian Yang", "Yuelin Bai", "Zekun Moore Wang", "Zhouliang Yu", "Qunshu Lin", "Ding Pan", "Yuchen Jiang", "Tiannan Wang", "Wangchunshu Zhou", "Shenzhi Wang", "Xingyuan Bu", "Minghao Liu", "Guoyin Wang", "Ge Zhang", "Chenghua Lin"], "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has achieved\nremarkable success. However, existing Chinese preference datasets are limited\nby small scale, narrow domain coverage, and lack of rigorous data validation.\nAdditionally, the reliance on human annotators for instruction and response\nlabeling significantly constrains the scalability of human preference datasets.\nTo address these challenges, we design an LLM-based Chinese preference dataset\nannotation pipeline with no human intervention. Specifically, we crawled and\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstream\nLLMs to generate and score chosen-rejected response pairs. Based on it, we\nintroduce COIG-P (Chinese Open Instruction Generalist - Preference), a\nhigh-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese\npreference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel,\nand Role. Building upon COIG-P, to reduce the overhead of using LLMs for\nscoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously\nconstructed a Chinese Reward Benchmark (CRBench). Evaluation results based on\nAlignBench \\citep{liu2024alignbenchbenchmarkingchinesealignment} show that that\nCOIG-P significantly outperforms other Chinese preference datasets, and it\nbrings significant performance improvements ranging from 2% to 12% for the\nQwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results\non CRBench demonstrate that our CRM has a strong and robust scoring ability. We\napply it to filter chosen-rejected response pairs in a test split of COIG-P,\nand our experiments show that it is comparable to GPT-4o in identifying\nlow-quality samples while maintaining efficiency and cost-effectiveness. Our\ncodes and data are released in\nhttps://github.com/multimodal-art-projection/COIG-P."}
{"id": "2504.05456", "pdf": "https://arxiv.org/pdf/2504.05456", "abs": "https://arxiv.org/abs/2504.05456", "authors": ["Omar De Mitri", "Ruyu Wang", "Marco F. Huber"], "title": "Generative Adversarial Networks with Limited Data: A Survey and Benchmarking", "categories": ["cs.CV"], "comment": null, "summary": "Generative Adversarial Networks (GANs) have shown impressive results in\nvarious image synthesis tasks. Vast studies have demonstrated that GANs are\nmore powerful in feature and expression learning compared to other generative\nmodels and their latent space encodes rich semantic information. However, the\ntremendous performance of GANs heavily relies on the access to large-scale\ntraining data and deteriorates rapidly when the amount of data is limited. This\npaper aims to provide an overview of GANs, its variants and applications in\nvarious vision tasks, focusing on addressing the limited data issue. We analyze\nstate-of-the-art GANs in limited data regime with designed experiments, along\nwith presenting various methods attempt to tackle this problem from different\nperspectives. Finally, we further elaborate on remaining challenges and trends\nfor future research."}
{"id": "2504.05570", "pdf": "https://arxiv.org/pdf/2504.05570", "abs": "https://arxiv.org/abs/2504.05570", "authors": ["Conrad Borchers", "Tianze Shou"], "title": "Can Large Language Models Match Tutoring System Adaptivity? A Benchmarking Study", "categories": ["cs.CL"], "comment": "Accepted as full paper to the 26th International Conference on\n  Artificial Intelligence in Education (AIED 2025)", "summary": "Large Language Models (LLMs) hold promise as dynamic instructional aids. Yet,\nit remains unclear whether LLMs can replicate the adaptivity of intelligent\ntutoring systems (ITS)--where student knowledge and pedagogical strategies are\nexplicitly modeled. We propose a prompt variation framework to assess\nLLM-generated instructional moves' adaptivity and pedagogical soundness across\n75 real-world tutoring scenarios from an ITS. We systematically remove key\ncontext components (e.g., student errors and knowledge components) from prompts\nto create variations of each scenario. Three representative LLMs (Llama3-8B,\nLlama3-70B, and GPT-4o) generate 1,350 instructional moves. We use text\nembeddings and randomization tests to measure how the omission of each context\nfeature impacts the LLMs' outputs (adaptivity) and a validated tutor-training\nclassifier to evaluate response quality (pedagogical soundness). Surprisingly,\neven the best-performing model only marginally mimics the adaptivity of ITS.\nSpecifically, Llama3-70B demonstrates statistically significant adaptivity to\nstudent errors. Although Llama3-8B's recommendations receive higher pedagogical\nsoundness scores than the other models, it struggles with instruction-following\nbehaviors, including output formatting. By contrast, GPT-4o reliably adheres to\ninstructions but tends to provide overly direct feedback that diverges from\neffective tutoring, prompting learners with open-ended questions to gauge\nknowledge. Given these results, we discuss how current LLM-based tutoring is\nunlikely to produce learning benefits rivaling known-to-be-effective ITS\ntutoring. Through our open-source benchmarking code, we contribute a\nreproducible method for evaluating LLMs' instructional adaptivity and fidelity."}
{"id": "2504.05457", "pdf": "https://arxiv.org/pdf/2504.05457", "abs": "https://arxiv.org/abs/2504.05457", "authors": ["Vésteinn Snæbjarnarson", "Kevin Du", "Niklas Stoehr", "Serge Belongie", "Ryan Cotterell", "Nico Lang", "Stella Frank"], "title": "Taxonomy-Aware Evaluation of Vision-Language Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "When a vision-language model (VLM) is prompted to identify an entity depicted\nin an image, it may answer 'I see a conifer,' rather than the specific label\n'norway spruce'. This raises two issues for evaluation: First, the\nunconstrained generated text needs to be mapped to the evaluation label space\n(i.e., 'conifer'). Second, a useful classification measure should give partial\ncredit to less-specific, but not incorrect, answers ('norway spruce' being a\ntype of 'conifer'). To meet these requirements, we propose a framework for\nevaluating unconstrained text predictions, such as those generated from a\nvision-language model, against a taxonomy. Specifically, we propose the use of\nhierarchical precision and recall measures to assess the level of correctness\nand specificity of predictions with regard to a taxonomy. Experimentally, we\nfirst show that existing text similarity measures do not capture taxonomic\nsimilarity well. We then develop and compare different methods to map textual\nVLM predictions onto a taxonomy. This allows us to compute hierarchical\nsimilarity measures between the generated text and the ground truth labels.\nFinally, we analyze modern VLMs on fine-grained visual classification tasks\nbased on our proposed taxonomic evaluation scheme."}
{"id": "2504.05571", "pdf": "https://arxiv.org/pdf/2504.05571", "abs": "https://arxiv.org/abs/2504.05571", "authors": ["Oded Ovadia", "Meni Brief", "Rachel Lemberg", "Eitam Sheetrit"], "title": "Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) acquire vast knowledge during\npre-training, they often lack domain-specific, new, or niche information.\nContinual pre-training (CPT) attempts to address this gap but suffers from\ncatastrophic forgetting and inefficiencies in low-data regimes. We introduce\nKnowledge-Instruct, a novel approach to efficiently inject knowledge from\nlimited corpora through pure instruction-tuning. By generating\ninformation-dense synthetic instruction data, it effectively integrates new\nknowledge while preserving general reasoning and instruction-following\nabilities. Knowledge-Instruct demonstrates superior factual memorization,\nminimizes catastrophic forgetting, and remains scalable by leveraging synthetic\ndata from relatively small language models. Additionally, it enhances\ncontextual understanding, including complex multi-hop reasoning, facilitating\nintegration with retrieval systems. We validate its effectiveness across\ndiverse benchmarks, including Companies, a new dataset that we release to\nmeasure knowledge injection capabilities."}
{"id": "2504.05458", "pdf": "https://arxiv.org/pdf/2504.05458", "abs": "https://arxiv.org/abs/2504.05458", "authors": ["In-Hwan Jin", "Haesoo Choo", "Seong-Hun Jeong", "Heemoon Park", "Junghwan Kim", "Oh-joon Kwon", "Kyeongbo Kong"], "title": "Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape Images", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "To achieve realistic immersion in landscape images, fluids such as water and\nclouds need to move within the image while revealing new scenes from various\ncamera perspectives. Recently, a field called dynamic scene video has emerged,\nwhich combines single image animation with 3D photography. These methods use\npseudo 3D space, implicitly represented with Layered Depth Images (LDIs). LDIs\nseparate a single image into depth-based layers, which enables elements like\nwater and clouds to move within the image while revealing new scenes from\ndifferent camera perspectives. However, as landscapes typically consist of\ncontinuous elements, including fluids, the representation of a 3D space\nseparates a landscape image into discrete layers, and it can lead to diminished\ndepth perception and potential distortions depending on camera movement.\nFurthermore, due to its implicit modeling of 3D space, the output may be\nlimited to videos in the 2D domain, potentially reducing their versatility. In\nthis paper, we propose representing a complete 3D space for dynamic scene video\nby modeling explicit representations, specifically 4D Gaussians, from a single\nimage. The framework is focused on optimizing 3D Gaussians by generating\nmulti-view images from a single image and creating 3D motion to optimize 4D\nGaussians. The most important part of proposed framework is consistent 3D\nmotion estimation, which estimates common motion among multi-view images to\nbring the motion in 3D space closer to actual motions. As far as we know, this\nis the first attempt that considers animation while representing a complete 3D\nspace from a single landscape image. Our model demonstrates the ability to\nprovide realistic immersion in various landscape images through diverse\nexperiments and metrics. Extensive experimental results are\nhttps://cvsp-lab.github.io/ICLR2025_3D-MOM/."}
{"id": "2504.05598", "pdf": "https://arxiv.org/pdf/2504.05598", "abs": "https://arxiv.org/abs/2504.05598", "authors": ["Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang", "Murali Annavaram"], "title": "DEL: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Speculative Decoding (SD) is a widely used approach to accelerate the\ninference of large language models (LLMs) without reducing generation quality.\nIt operates by first using a compact model to draft multiple tokens\nefficiently, followed by parallel verification using the target LLM. This\napproach leads to faster inference compared to auto-regressive decoding. While\nthere are multiple approaches to create a draft model, one promising approach\nis to use early-exit methods. These methods draft candidate tokens by using a\nsubset of layers of the primary model and applying the remaining layers for\nverification, allowing a single model to handle both drafting and verification.\nWhile this technique reduces memory usage and computational cost, its\nperformance relies on the choice of the exit layer for drafting and the number\nof tokens drafted (speculation length) in each SD round. Prior works use\nhyperparameter exploration to statically select these values. However, our\nevaluations show that these hyperparameter values are task-specific, and even\nwithin a task they are dependent on the current sequence context. We introduce\nDEL, a plug-and-play method that adaptively selects the exit layer and\nspeculation length during inference. DEL dynamically tracks the token\nacceptance rate if the tokens are drafted at each layer of an LLM and uses that\nknowledge to heuristically select the optimal exit layer and speculation\nlength. Our experiments across a broad range of models and downstream tasks\nshow that DEL achieves overall speedups of $2.16\\times$$\\sim$$2.50\\times$ over\nvanilla auto-regressive decoding and improves upon the state-of-the-art SD\nmethods by up to $0.27\\times$."}
{"id": "2504.05463", "pdf": "https://arxiv.org/pdf/2504.05463", "abs": "https://arxiv.org/abs/2504.05463", "authors": ["Sofian Chaybouti", "Walid Bousselham", "Moritz Wolter", "Hilde Kuehne"], "title": "REVEAL: Relation-based Video Representation Learning for Video-Question-Answering", "categories": ["cs.CV"], "comment": "18 pages, 7 figures", "summary": "Video-Question-Answering (VideoQA) comprises the capturing of complex visual\nrelation changes over time, remaining a challenge even for advanced Video\nLanguage Models (VLM), i.a., because of the need to represent the visual\ncontent to a reasonably sized input for those models. To address this problem,\nwe propose\n  RElation-based Video rEpresentAtion Learning (REVEAL), a framework designed\nto capture visual relation information by encoding them into structured,\ndecomposed representations. Specifically, inspired by spatiotemporal scene\ngraphs, we propose to encode video sequences as sets of relation triplets in\nthe form of (\\textit{subject-predicate-object}) over time via their language\nembeddings. To this end, we extract explicit relations from video captions and\nintroduce a Many-to-Many Noise Contrastive Estimation (MM-NCE) together with a\nQ-Former architecture to align an unordered set of video-derived queries with\ncorresponding text-based relation descriptions. At inference, the resulting\nQ-former produces an efficient token representation that can serve as input to\na VLM for VideoQA.\n  We evaluate the proposed framework on five challenging benchmarks: NeXT-QA,\nIntent-QA, STAR, VLEP, and TVQA. It shows that the resulting query-based video\nrepresentation is able to outperform global alignment-based CLS or patch token\nrepresentations and achieves competitive results against state-of-the-art\nmodels, particularly on tasks requiring temporal reasoning and relation\ncomprehension. The code and models will be publicly released."}
{"id": "2504.05603", "pdf": "https://arxiv.org/pdf/2504.05603", "abs": "https://arxiv.org/abs/2504.05603", "authors": ["Naman Bhargava", "Mohammed I. Radaideh", "O Hwang Kwon", "Aditi Verma", "Majdi I. Radaideh"], "title": "On the Impact of Language Nuances on Sentiment Analysis with Large Language Models: Paraphrasing, Sarcasm, and Emojis", "categories": ["cs.CL", "cs.LG"], "comment": "21 pages, 10 Tables, 5 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, including sentiment analysis. However, data\nquality--particularly when sourced from social media--can significantly impact\ntheir accuracy. This research explores how textual nuances, including emojis\nand sarcasm, affect sentiment analysis, with a particular focus on improving\ndata quality through text paraphrasing techniques. To address the lack of\nlabeled sarcasm data, the authors created a human-labeled dataset of 5929\ntweets that enabled the assessment of LLM in various sarcasm contexts. The\nresults show that when topic-specific datasets, such as those related to\nnuclear power, are used to finetune LLMs these models are not able to\ncomprehend accurate sentiment in presence of sarcasm due to less diverse text,\nrequiring external interventions like sarcasm removal to boost model accuracy.\nSarcasm removal led to up to 21% improvement in sentiment accuracy, as LLMs\ntrained on nuclear power-related content struggled with sarcastic tweets,\nachieving only 30% accuracy. In contrast, LLMs trained on general tweet\ndatasets, covering a broader range of topics, showed considerable improvements\nin predicting sentiment for sarcastic tweets (60% accuracy), indicating that\nincorporating general text data can enhance sarcasm detection. The study also\nutilized adversarial text augmentation, showing that creating synthetic text\nvariants by making minor changes significantly increased model robustness and\naccuracy for sarcastic tweets (approximately 85%). Additionally, text\nparaphrasing of tweets with fragmented language transformed around 40% of the\ntweets with low-confidence labels into high-confidence ones, improving LLMs\nsentiment analysis accuracy by 6%."}
{"id": "2504.05468", "pdf": "https://arxiv.org/pdf/2504.05468", "abs": "https://arxiv.org/abs/2504.05468", "authors": ["Thanos Delatolas", "Vicky Kalogeiton", "Dim P. Papadopoulos"], "title": "Studying Image Diffusion Features for Zero-Shot Video Object Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPRW2025", "summary": "This paper investigates the use of large-scale diffusion models for Zero-Shot\nVideo Object Segmentation (ZS-VOS) without fine-tuning on video data or\ntraining on any image segmentation data. While diffusion models have\ndemonstrated strong visual representations across various tasks, their direct\napplication to ZS-VOS remains underexplored. Our goal is to find the optimal\nfeature extraction process for ZS-VOS by identifying the most suitable time\nstep and layer from which to extract features. We further analyze the affinity\nof these features and observe a strong correlation with point correspondences.\nThrough extensive experiments on DAVIS-17 and MOSE, we find that diffusion\nmodels trained on ImageNet outperform those trained on larger, more diverse\ndatasets for ZS-VOS. Additionally, we highlight the importance of point\ncorrespondences in achieving high segmentation accuracy, and we yield\nstate-of-the-art results in ZS-VOS. Finally, our approach performs on par with\nmodels trained on expensive image segmentation datasets."}
{"id": "2504.05607", "pdf": "https://arxiv.org/pdf/2504.05607", "abs": "https://arxiv.org/abs/2504.05607", "authors": ["Qian-Wen Zhang", "Fang Li", "Jie Wang", "Lingfeng Qiao", "Yifei Yu", "Di Yin", "Xing Sun"], "title": "FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extractive reading comprehension systems are designed to locate the correct\nanswer to a question within a given text. However, a persistent challenge lies\nin ensuring these models maintain high accuracy in answering questions while\nreliably recognizing unanswerable queries. Despite significant advances in\nlarge language models (LLMs) for reading comprehension, this issue remains\ncritical, particularly as the length of supported contexts continues to expand.\nTo address this challenge, we propose an innovative data augmentation\nmethodology grounded in a multi-agent collaborative framework. Unlike\ntraditional methods, such as the costly human annotation process required for\ndatasets like SQuAD 2.0, our method autonomously generates evidence-based\nquestion-answer pairs and systematically constructs unanswerable questions.\nUsing this methodology, we developed the FactGuard-Bench dataset, which\ncomprises 25,220 examples of both answerable and unanswerable question\nscenarios, with context lengths ranging from 8K to 128K. Experimental\nevaluations conducted on seven popular LLMs reveal that even the most advanced\nmodels achieve only 61.79% overall accuracy. Furthermore, we emphasize the\nimportance of a model's ability to reason about unanswerable questions to avoid\ngenerating plausible but incorrect answers. By implementing efficient data\nselection and generation within the multi-agent collaborative framework, our\nmethod significantly reduces the traditionally high costs associated with\nmanual annotation and provides valuable insights for the training and\noptimization of LLMs."}
{"id": "2504.05483", "pdf": "https://arxiv.org/pdf/2504.05483", "abs": "https://arxiv.org/abs/2504.05483", "authors": ["Mohammad Hossein Najafi", "Mohammad Morsali", "Mohammadreza Pashanejad", "Saman Soleimani Roudi", "Mohammad Norouzi", "Saeed Bagheri Shouraki"], "title": "Secure Diagnostics: Adversarial Robustness Meets Clinical Interpretability", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks for medical image classification often fail to\ngeneralize consistently in clinical practice due to violations of the i.i.d.\nassumption and opaque decision-making. This paper examines interpretability in\ndeep neural networks fine-tuned for fracture detection by evaluating model\nperformance against adversarial attack and comparing interpretability methods\nto fracture regions annotated by an orthopedic surgeon. Our findings prove that\nrobust models yield explanations more aligned with clinically meaningful areas,\nindicating that robustness encourages anatomically relevant feature\nprioritization. We emphasize the value of interpretability for facilitating\nhuman-AI collaboration, in which models serve as assistants under a\nhuman-in-the-loop paradigm: clinically plausible explanations foster trust,\nenable error correction, and discourage reliance on AI for high-stakes\ndecisions. This paper investigates robustness and interpretability as\ncomplementary benchmarks for bridging the gap between benchmark performance and\nsafe, actionable clinical deployment."}
{"id": "2504.05614", "pdf": "https://arxiv.org/pdf/2504.05614", "abs": "https://arxiv.org/abs/2504.05614", "authors": ["Yichen Dong", "Xinglin Lyu", "Junhui Li", "Daimeng Wei", "Min Zhang", "Shimin Tao", "Hao Yang"], "title": "Two Intermediate Translations Are Better Than One: Fine-tuning LLMs for Document-level Translation Refinement", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Recent research has shown that large language models (LLMs) can enhance\ntranslation quality through self-refinement. In this paper, we build on this\nidea by extending the refinement from sentence-level to document-level\ntranslation, specifically focusing on document-to-document (Doc2Doc)\ntranslation refinement. Since sentence-to-sentence (Sent2Sent) and Doc2Doc\ntranslation address different aspects of the translation process, we propose\nfine-tuning LLMs for translation refinement using two intermediate\ntranslations, combining the strengths of both Sent2Sent and Doc2Doc.\nAdditionally, recognizing that the quality of intermediate translations varies,\nwe introduce an enhanced fine-tuning method with quality awareness that assigns\nlower weights to easier translations and higher weights to more difficult ones,\nenabling the model to focus on challenging translation cases. Experimental\nresults across ten translation tasks with LLaMA-3-8B-Instruct and\nMistral-Nemo-Instruct demonstrate the effectiveness of our approach."}
{"id": "2504.05491", "pdf": "https://arxiv.org/pdf/2504.05491", "abs": "https://arxiv.org/abs/2504.05491", "authors": ["Sakib Reza", "Xiyun Song", "Heather Yu", "Zongfang Lin", "Mohsen Moghaddam", "Octavia Camps"], "title": "REEF: Relevance-Aware and Efficient LLM Adapter for Video Understanding", "categories": ["cs.CV"], "comment": "Accepted at CVPRW'25", "summary": "Integrating vision models into large language models (LLMs) has sparked\nsignificant interest in creating vision-language foundation models, especially\nfor video understanding. Recent methods often utilize memory banks to handle\nuntrimmed videos for video-level understanding. However, they typically\ncompress visual memory using similarity-based greedy approaches, which can\noverlook the contextual importance of individual tokens. To address this, we\nintroduce an efficient LLM adapter designed for video-level understanding of\nuntrimmed videos that prioritizes the contextual relevance of spatio-temporal\ntokens. Our framework leverages scorer networks to selectively compress the\nvisual memory bank and filter spatial tokens based on relevance, using a\ndifferentiable Top-K operator for end-to-end training. Across three key\nvideo-level understanding tasks$\\unicode{x2013}$ untrimmed video\nclassification, video question answering, and video\ncaptioning$\\unicode{x2013}$our method achieves competitive or superior results\non four large-scale datasets while reducing computational overhead by up to\n34%. The code will be available soon on GitHub."}
{"id": "2504.05632", "pdf": "https://arxiv.org/pdf/2504.05632", "abs": "https://arxiv.org/abs/2504.05632", "authors": ["Sanchit Kabra", "Akshita Jha", "Chandan Reddy"], "title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages", "summary": "Recent advances in large-scale generative language models have shown that\nreasoning capabilities can significantly improve model performance across a\nvariety of tasks. However, the impact of reasoning on a model's ability to\nmitigate stereotypical responses remains largely underexplored. In this work,\nwe investigate the crucial relationship between a model's reasoning ability and\nfairness, and ask whether improved reasoning capabilities can mitigate harmful\nstereotypical responses, especially those arising due to shallow or flawed\nreasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,\nand find that larger models with stronger reasoning abilities exhibit\nsubstantially lower stereotypical bias on existing fairness benchmarks.\nBuilding on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,\na novel approach that extracts structured reasoning traces from advanced\nreasoning models and infuses them into models that lack such capabilities. We\nuse only general-purpose reasoning and do not require any fairness-specific\nsupervision for bias mitigation. Notably, we see that models fine-tuned using\nReGiFT not only improve fairness relative to their non-reasoning counterparts\nbut also outperform advanced reasoning models on fairness benchmarks. We also\nanalyze how variations in the correctness of the reasoning traces and their\nlength influence model fairness and their overall performance. Our findings\nhighlight that enhancing reasoning capabilities is an effective,\nfairness-agnostic strategy for mitigating stereotypical bias caused by\nreasoning flaws."}
{"id": "2504.05499", "pdf": "https://arxiv.org/pdf/2504.05499", "abs": "https://arxiv.org/abs/2504.05499", "authors": ["Ruoyu Xue", "Jingyi Xu", "Sounak Mondal", "Hieu Le", "Gregory Zelinsky", "Minh Hoai", "Dimitris Samaras"], "title": "Few-shot Personalized Scanpath Prediction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025,20 pages, 10 figures", "summary": "A personalized model for scanpath prediction provides insights into the\nvisual preferences and attention patterns of individual subjects. However,\nexisting methods for training scanpath prediction models are data-intensive and\ncannot be effectively personalized to new individuals with only a few available\nexamples. In this paper, we propose few-shot personalized scanpath prediction\ntask (FS-PSP) and a novel method to address it, which aims to predict scanpaths\nfor an unseen subject using minimal support data of that subject's scanpath\nbehavior. The key to our method's adaptability is the Subject-Embedding Network\n(SE-Net), specifically designed to capture unique, individualized\nrepresentations for each subject's scanpaths. SE-Net generates subject\nembeddings that effectively distinguish between subjects while minimizing\nvariability among scanpaths from the same individual. The personalized scanpath\nprediction model is then conditioned on these subject embeddings to produce\naccurate, personalized results. Experiments on multiple eye-tracking datasets\ndemonstrate that our method excels in FS-PSP settings and does not require any\nfine-tuning steps at test time. Code is available at:\nhttps://github.com/cvlab-stonybrook/few-shot-scanpath"}
{"id": "2504.05639", "pdf": "https://arxiv.org/pdf/2504.05639", "abs": "https://arxiv.org/abs/2504.05639", "authors": ["Vasant Dhar", "João Sedoc"], "title": "DBOT: Artificial Intelligence for Systematic Long-Term Investing", "categories": ["cs.CL", "cs.AI", "q-fin.PR"], "comment": null, "summary": "Long-term investing was previously seen as requiring human judgment. With the\nadvent of generative artificial intelligence (AI) systems, automated systematic\nlong-term investing is now feasible. In this paper, we present DBOT, a system\nwhose goal is to reason about valuation like Aswath Damodaran, who is a unique\nexpert in the investment arena in terms of having published thousands of\nvaluations on companies in addition to his numerous writings on the topic,\nwhich provide ready training data for an AI system. DBOT can value any publicly\ntraded company. DBOT can also be back-tested, making its behavior and\nperformance amenable to scientific inquiry. We compare DBOT to its analytic\nparent, Damodaran, and highlight the research challenges involved in raising\nits current capability to that of Damodaran's. Finally, we examine the\nimplications of DBOT-like AI agents for the financial industry, especially how\nthey will impact the role of human analysts in valuation."}
{"id": "2504.05504", "pdf": "https://arxiv.org/pdf/2504.05504", "abs": "https://arxiv.org/abs/2504.05504", "authors": ["Marija Ivanovska", "Leon Todorov", "Naser Damer", "Deepak Kumar Jain", "Peter Peer", "Vitomir Štruc"], "title": "SelfMAD: Enhancing Generalization and Robustness in Morphing Attack Detection via Self-Supervised Learning", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2025)", "summary": "With the continuous advancement of generative models, face morphing attacks\nhave become a significant challenge for existing face verification systems due\nto their potential use in identity fraud and other malicious activities.\nContemporary Morphing Attack Detection (MAD) approaches frequently rely on\nsupervised, discriminative models trained on examples of bona fide and morphed\nimages. These models typically perform well with morphs generated with\ntechniques seen during training, but often lead to sub-optimal performance when\nsubjected to novel unseen morphing techniques. While unsupervised models have\nbeen shown to perform better in terms of generalizability, they typically\nresult in higher error rates, as they struggle to effectively capture features\nof subtle artifacts. To address these shortcomings, we present SelfMAD, a novel\nself-supervised approach that simulates general morphing attack artifacts,\nallowing classifiers to learn generic and robust decision boundaries without\noverfitting to the specific artifacts induced by particular face morphing\nmethods. Through extensive experiments on widely used datasets, we demonstrate\nthat SelfMAD significantly outperforms current state-of-the-art MADs, reducing\nthe detection error by more than 64% in terms of EER when compared to the\nstrongest unsupervised competitor, and by more than 66%, when compared to the\nbest performing discriminative MAD model, tested in cross-morph settings. The\nsource code for SelfMAD is available at https://github.com/LeonTodorov/SelfMAD."}
{"id": "2504.05642", "pdf": "https://arxiv.org/pdf/2504.05642", "abs": "https://arxiv.org/abs/2504.05642", "authors": ["Subhankar Maity", "Aniket Deroy"], "title": "Leveraging Prompt-Tuning for Bengali Grammatical Error Explanation Using Large Language Models", "categories": ["cs.CL"], "comment": "9 pages, 2 figures", "summary": "We propose a novel three-step prompt-tuning method for Bengali Grammatical\nError Explanation (BGEE) using state-of-the-art large language models (LLMs)\nsuch as GPT-4, GPT-3.5 Turbo, and Llama-2-70b. Our approach involves\nidentifying and categorizing grammatical errors in Bengali sentences,\ngenerating corrected versions of the sentences, and providing natural language\nexplanations for each identified error. We evaluate the performance of our BGEE\nsystem using both automated evaluation metrics and human evaluation conducted\nby experienced Bengali language experts. Our proposed prompt-tuning approach\nshows that GPT-4, the best performing LLM, surpasses the baseline model in\nautomated evaluation metrics, with a 5.26% improvement in F1 score and a 6.95%\nimprovement in exact match. Furthermore, compared to the previous baseline,\nGPT-4 demonstrates a decrease of 25.51% in wrong error type and a decrease of\n26.27% in wrong error explanation. However, the results still lag behind the\nhuman baseline."}
{"id": "2504.05508", "pdf": "https://arxiv.org/pdf/2504.05508", "abs": "https://arxiv.org/abs/2504.05508", "authors": ["Mo Zhou", "Josh Myers-Dean", "Danna Gurari"], "title": "PartStickers: Generating Parts of Objects for Rapid Prototyping", "categories": ["cs.CV"], "comment": "Accepted to CVPR CVEU workshop 2025", "summary": "Design prototyping involves creating mockups of products or concepts to\ngather feedback and iterate on ideas. While prototyping often requires specific\nparts of objects, such as when constructing a novel creature for a video game,\nexisting text-to-image methods tend to only generate entire objects. To address\nthis, we propose a novel task and method of ``part sticker generation\", which\nentails generating an isolated part of an object on a neutral background.\nExperiments demonstrate our method outperforms state-of-the-art baselines with\nrespect to realism and text alignment, while preserving object-level generation\ncapabilities. We publicly share our code and models to encourage community-wide\nprogress on this new task: https://partsticker.github.io."}
{"id": "2504.05683", "pdf": "https://arxiv.org/pdf/2504.05683", "abs": "https://arxiv.org/abs/2504.05683", "authors": ["Subhankar Maity", "Aniket Deroy", "Sudeshna Sarkar"], "title": "Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs Ready for HR Spoken Interview Transcript Analysis?", "categories": ["cs.CL", "cs.AI"], "comment": "32 pages, 24 figures", "summary": "This research paper presents a comprehensive analysis of the performance of\nprominent pre-trained large language models (LLMs), including GPT-4 Turbo,\nGPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001,\ntext-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in\ncomparison to expert human evaluators in providing scores, identifying errors,\nand offering feedback and improvement suggestions to candidates during mock HR\n(Human Resources) interviews. We introduce a dataset called HURIT (Human\nResource Interview Transcripts), which comprises 3,890 HR interview transcripts\nsourced from real-world HR interview scenarios. Our findings reveal that\npre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit\ncommendable performance and are capable of producing evaluations comparable to\nthose of expert human evaluators. Although these LLMs demonstrate proficiency\nin providing scores comparable to human experts in terms of human evaluation\nmetrics, they frequently fail to identify errors and offer specific actionable\nadvice for candidate performance improvement in HR interviews. Our research\nsuggests that the current state-of-the-art pre-trained LLMs are not fully\nconducive for automatic deployment in an HR interview assessment. Instead, our\nfindings advocate for a human-in-the-loop approach, to incorporate manual\nchecks for inconsistencies and provisions for improving feedback quality as a\nmore suitable strategy."}
{"id": "2504.05537", "pdf": "https://arxiv.org/pdf/2504.05537", "abs": "https://arxiv.org/abs/2504.05537", "authors": ["Tasmiah Haque", "Md. Asif Bin Syed", "Byungheon Jeong", "Xue Bai", "Sumit Mohan", "Somdyuti Paul", "Imtiaz Ahmed", "Srinjoy Das"], "title": "Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a deep learning framework designed to significantly optimize\nbandwidth for motion-transfer-enabled video applications, including video\nconferencing, virtual reality interactions, health monitoring systems, and\nvision-based real-time anomaly detection. To capture complex motion\neffectively, we utilize the First Order Motion Model (FOMM), which encodes\ndynamic objects by detecting keypoints and their associated local affine\ntransformations. These keypoints are identified using a self-supervised\nkeypoint detector and arranged into a time series corresponding to the\nsuccessive frames. Forecasting is performed on these keypoints by integrating\ntwo advanced generative time series models into the motion transfer pipeline,\nnamely the Variational Recurrent Neural Network (VRNN) and the Gated Recurrent\nUnit with Normalizing Flow (GRU-NF). The predicted keypoints are subsequently\nsynthesized into realistic video frames using an optical flow estimator paired\nwith a generator network, thereby facilitating accurate video forecasting and\nenabling efficient, low-frame-rate video transmission. We validate our results\nacross three datasets for video animation and reconstruction using the\nfollowing metrics: Mean Absolute Error, Joint Embedding Predictive Architecture\nEmbedding Distance, Structural Similarity Index, and Average Pair-wise\nDisplacement. Our results confirm that by utilizing the superior reconstruction\nproperty of the Variational Autoencoder, the VRNN integrated FOMM excels in\napplications involving multi-step ahead forecasts such as video conferencing.\nOn the other hand, by leveraging the Normalizing Flow architecture for exact\nlikelihood estimation, and enabling efficient latent space sampling, the GRU-NF\nbased FOMM exhibits superior capabilities for producing diverse future samples\nwhile maintaining high visual quality for tasks like real-time video-based\nanomaly detection."}
{"id": "2504.05689", "pdf": "https://arxiv.org/pdf/2504.05689", "abs": "https://arxiv.org/abs/2504.05689", "authors": ["Xitao Li", "Haijun Wang", "Jiang Wu", "Ting Liu"], "title": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Conversational large language models (LLMs) have gained widespread attention\ndue to their instruction-following capabilities. To ensure conversational LLMs\nfollow instructions, role separators are employed to distinguish between\ndifferent participants in a conversation. However, incorporating role\nseparators introduces potential vulnerabilities. Misusing roles can lead to\nprompt injection attacks, which can easily misalign the model's behavior with\nthe user's intentions, raising significant security concerns. Although various\nprompt injection attacks have been proposed, recent research has largely\noverlooked the impact of role separators on safety. This highlights the\ncritical need to thoroughly understand the systemic weaknesses in dialogue\nsystems caused by role separators. This paper identifies modeling weaknesses\ncaused by role separators. Specifically, we observe a strong positional bias\nassociated with role separators, which is inherent in the format of dialogue\nmodeling and can be triggered by the insertion of role separators. We further\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\non role separators. The experiment results show that SIA is efficient and\nextensive in manipulating model behavior with an average gain of 18.2% for\nmanual methods and enhances the attack success rate to 100% with automatic\nmethods."}
{"id": "2504.05541", "pdf": "https://arxiv.org/pdf/2504.05541", "abs": "https://arxiv.org/abs/2504.05541", "authors": ["Yunlong Tang", "Jing Bi", "Chao Huang", "Susan Liang", "Daiki Shimada", "Hang Hua", "Yunzhong Xiao", "Yizhi Song", "Pinxin Liu", "Mingqian Feng", "Junjia Guo", "Zhuo Liu", "Luchuan Song", "Ali Vosoughi", "Jinxi He", "Liu He", "Zeliang Zhang", "Jiebo Luo", "Chenliang Xu"], "title": "Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting", "categories": ["cs.CV"], "comment": null, "summary": "We present CAT-V (Caption AnyThing in Video), a training-free framework for\nfine-grained object-centric video captioning that enables detailed descriptions\nof user-selected objects through time. CAT-V integrates three key components: a\nSegmenter based on SAMURAI for precise object segmentation across frames, a\nTemporal Analyzer powered by TRACE-Uni for accurate event boundary detection\nand temporal analysis, and a Captioner using InternVL-2.5 for generating\ndetailed object-centric descriptions. Through spatiotemporal visual prompts and\nchain-of-thought reasoning, our framework generates detailed, temporally-aware\ndescriptions of objects' attributes, actions, statuses, interactions, and\nenvironmental contexts without requiring additional training data. CAT-V\nsupports flexible user interactions through various visual prompts (points,\nbounding boxes, and irregular regions) and maintains temporal sensitivity by\ntracking object states and interactions across different time segments. Our\napproach addresses limitations of existing video captioning methods, which\neither produce overly abstract descriptions or lack object-level precision,\nenabling fine-grained, object-specific descriptions while maintaining temporal\ncoherence and spatial accuracy. The GitHub repository for this project is\navailable at https://github.com/yunlong10/CAT-V"}
{"id": "2504.05693", "pdf": "https://arxiv.org/pdf/2504.05693", "abs": "https://arxiv.org/abs/2504.05693", "authors": ["Aniket Deroy", "Subhankar Maity"], "title": "STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 6 figures", "summary": "Automatically assessing question quality is crucial for educators as it saves\ntime, ensures consistency, and provides immediate feedback for refining\nteaching materials. We propose a novel methodology called STRIVE (Structured\nThinking and Refinement with multiLLMs for Improving Verified Question\nEstimation) using a series of Large Language Models (LLMs) for automatic\nquestion evaluation. This approach aims to improve the accuracy and depth of\nquestion quality assessment, ultimately supporting diverse learners and\nenhancing educational practices. The method estimates question quality in an\nautomated manner by generating multiple evaluations based on the strengths and\nweaknesses of the provided question and then choosing the best solution\ngenerated by the LLM. Then the process is improved by iterative review and\nresponse with another LLM until the evaluation metric values converge. This\nsophisticated method of evaluating question quality improves the estimation of\nquestion quality by automating the task of question quality evaluation.\nCorrelation scores show that using this proposed method helps to improve\ncorrelation with human judgments compared to the baseline method. Error\nanalysis shows that metrics like relevance and appropriateness improve\nsignificantly relative to human judgments by using STRIVE."}
{"id": "2504.05575", "pdf": "https://arxiv.org/pdf/2504.05575", "abs": "https://arxiv.org/abs/2504.05575", "authors": ["Belal Alsinglawi", "Chris McCarthy", "Sara Webb", "Christopher Fluke", "Navid Toosy Saidy"], "title": "A Lightweight Large Vision-language Model for Multimodal Medical Images", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 4 figures", "summary": "Medical Visual Question Answering (VQA) enhances clinical decision-making by\nenabling systems to interpret medical images and answer clinical queries.\nHowever, developing efficient, high-performance VQA models is challenging due\nto the complexity of medical imagery and diverse modalities. In this paper, we\nintroduce a lightweight, multimodal VQA model integrating BiomedCLIP for image\nfeature extraction and LLaMA-3 for text processing. Designed for medical VQA\ntasks, our model achieves state-of-the-art performance on the OmniMedVQA\ndataset. With approximately 8 billion parameters, it requires only two NVIDIA\n40 GB A100 GPUs, demonstrating superior efficiency over larger models. Our\nresults show 73.4% accuracy for open-end questions, surpassing existing models\nand validating its potential for real-world medical applications. Key\ncontributions include a specialized multimodal VQA model, a resource-efficient\narchitecture, and strong performance in answering open-ended clinical\nquestions."}
{"id": "2504.05702", "pdf": "https://arxiv.org/pdf/2504.05702", "abs": "https://arxiv.org/abs/2504.05702", "authors": ["Jonathan Wright", "Mark Liberman", "Neville Ryant", "James Fiumara"], "title": "Evaluating Speech-to-Text Systems with PennSound", "categories": ["cs.CL"], "comment": null, "summary": "A random sample of nearly 10 hours of speech from PennSound, the world's\nlargest online collection of poetry readings and discussions, was used as a\nbenchmark to evaluate several commercial and open-source speech-to-text\nsystems. PennSound's wide variation in recording conditions and speech styles\nmakes it a good representative for many other untranscribed audio collections.\nReference transcripts were created by trained annotators, and system\ntranscripts were produced from AWS, Azure, Google, IBM, NeMo, Rev.ai, Whisper,\nand Whisper.cpp. Based on word error rate, Rev.ai was the top performer, and\nWhisper was the top open source performer (as long as hallucinations were\navoided). AWS had the best diarization error rates among three systems.\nHowever, WER and DER differences were slim, and various tradeoffs may motivate\nchoosing different systems for different end users. We also examine the issue\nof hallucinations in Whisper. Users of Whisper should be cautioned to be aware\nof runtime options, and whether the speed vs accuracy trade off is acceptable."}
{"id": "2504.05579", "pdf": "https://arxiv.org/pdf/2504.05579", "abs": "https://arxiv.org/abs/2504.05579", "authors": ["Artem Zholus", "Carl Doersch", "Yi Yang", "Skanda Koppula", "Viorica Patraucean", "Xu Owen He", "Ignacio Rocco", "Mehdi S. M. Sajjadi", "Sarath Chandar", "Ross Goroshin"], "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Tracking Any Point (TAP) in a video is a challenging computer vision problem\nwith many demonstrated applications in robotics, video editing, and 3D\nreconstruction. Existing methods for TAP rely heavily on complex\ntracking-specific inductive biases and heuristics, limiting their generality\nand potential for scaling. To address these challenges, we present TAPNext, a\nnew approach that casts TAP as sequential masked token decoding. Our model is\ncausal, tracks in a purely online fashion, and removes tracking-specific\ninductive biases. This enables TAPNext to run with minimal latency, and removes\nthe temporal windowing required by many existing state of art trackers. Despite\nits simplicity, TAPNext achieves a new state-of-the-art tracking performance\namong both online and offline trackers. Finally, we present evidence that many\nwidely used tracking heuristics emerge naturally in TAPNext through end-to-end\ntraining."}
{"id": "2504.05732", "pdf": "https://arxiv.org/pdf/2504.05732", "abs": "https://arxiv.org/abs/2504.05732", "authors": ["Haoyu Wang", "Yujia Fu", "Zhu Zhang", "Shuo Wang", "Zirui Ren", "Xiaorong Wang", "Zhili Li", "Chaoqun He", "Bo An", "Zhiyuan Liu", "Maosong Sun"], "title": "LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources", "categories": ["cs.CL"], "comment": null, "summary": "Long-form generation is crucial for a wide range of practical applications,\ntypically categorized into short-to-long and long-to-long generation. While\nshort-to-long generations have received considerable attention, generating long\ntexts from extremely long resources remains relatively underexplored. The\nprimary challenge in long-to-long generation lies in effectively integrating\nand analyzing relevant information from extensive inputs, which remains\ndifficult for current large language models (LLMs). In this paper, we propose\nLLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance\nthe ability of LLMs to process extremely long inputs. Drawing inspiration from\nconvolutional neural networks, which iteratively integrate local features into\nhigher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked\nconvolutional scaling layers to progressively expand the understanding of input\nmaterials. Both quantitative and qualitative experimental results demonstrate\nthat our approach substantially enhances the ability of LLMs to process long\ninputs and generate coherent, informative long-form articles, outperforming\nseveral representative baselines."}
{"id": "2504.05583", "pdf": "https://arxiv.org/pdf/2504.05583", "abs": "https://arxiv.org/abs/2504.05583", "authors": ["Jiahang Li", "Shibo Xue", "Yong Su"], "title": "Gaze-Guided Learning: Avoiding Shortcut Bias in Visual Classification", "categories": ["cs.CV", "I.4.9; I.5.1"], "comment": "10 pages, 5 figures, 3 tables, URL:\n  https://szyyjl.github.io/eye_tracking_data.github.io/", "summary": "Inspired by human visual attention, deep neural networks have widely adopted\nattention mechanisms to learn locally discriminative attributes for challenging\nvisual classification tasks. However, existing approaches primarily emphasize\nthe representation of such features while neglecting their precise\nlocalization, which often leads to misclassification caused by shortcut biases.\nThis limitation becomes even more pronounced when models are evaluated on\ntransfer or out-of-distribution datasets. In contrast, humans are capable of\nleveraging prior object knowledge to quickly localize and compare fine-grained\nattributes, a capability that is especially crucial in complex and\nhigh-variance classification scenarios. Motivated by this, we introduce\nGaze-CIFAR-10, a human gaze time-series dataset, along with a dual-sequence\ngaze encoder that models the precise sequential localization of human attention\non distinct local attributes. In parallel, a Vision Transformer (ViT) is\nemployed to learn the sequential representation of image content. Through\ncross-modal fusion, our framework integrates human gaze priors with\nmachine-derived visual sequences, effectively correcting inaccurate\nlocalization in image feature representations. Extensive qualitative and\nquantitative experiments demonstrate that gaze-guided cognitive cues\nsignificantly enhance classification accuracy."}
{"id": "2504.05736", "pdf": "https://arxiv.org/pdf/2504.05736", "abs": "https://arxiv.org/abs/2504.05736", "authors": ["Yida Cai", "Kun Liang", "Sanwoo Lee", "Qinghan Wang", "Yunfang Wu"], "title": "Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "In recent years, large language models (LLMs) achieve remarkable success\nacross a variety of tasks. However, their potential in the domain of Automated\nEssay Scoring (AES) remains largely underexplored. Moreover, compared to\nEnglish data, the methods for Chinese AES is not well developed. In this paper,\nwe propose Rank-Then-Score (RTS), a fine-tuning framework based on large\nlanguage models to enhance their essay scoring capabilities. Specifically, we\nfine-tune the ranking model (Ranker) with feature-enriched data, and then feed\nthe output of the ranking model, in the form of a candidate score set, with the\nessay content into the scoring model (Scorer) to produce the final score.\nExperimental results on two benchmark datasets, HSK and ASAP, demonstrate that\nRTS consistently outperforms the direct prompting (Vanilla) method in terms of\naverage QWK across all LLMs and datasets, and achieves the best performance on\nChinese essay scoring using the HSK dataset."}
{"id": "2504.05590", "pdf": "https://arxiv.org/pdf/2504.05590", "abs": "https://arxiv.org/abs/2504.05590", "authors": ["Long Ma", "Yuxin Feng", "Yan Zhang", "Jinyuan Liu", "Weimin Wang", "Guang-Yong Chen", "Chengpei Xu", "Zhuo Su"], "title": "CoA: Towards Real Image Dehazing via Compression-and-Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Learning-based image dehazing algorithms have shown remarkable success in\nsynthetic domains. However, real image dehazing is still in suspense due to\ncomputational resource constraints and the diversity of real-world scenes.\nTherefore, there is an urgent need for an algorithm that excels in both\nefficiency and adaptability to address real image dehazing effectively. This\nwork proposes a Compression-and-Adaptation (CoA) computational flow to tackle\nthese challenges from a divide-and-conquer perspective. First, model\ncompression is performed in the synthetic domain to develop a compact dehazing\nparameter space, satisfying efficiency demands. Then, a bilevel adaptation in\nthe real domain is introduced to be fearless in unknown real environments by\naggregating the synthetic dehazing capabilities during the learning process.\nLeveraging a succinct design free from additional constraints, our CoA exhibits\ndomain-irrelevant stability and model-agnostic flexibility, effectively\nbridging the model chasm between synthetic and real domains to further improve\nits practical utility. Extensive evaluations and analyses underscore the\napproach's superiority and effectiveness. The code is publicly available at\nhttps://github.com/fyxnl/COA."}
{"id": "2504.05747", "pdf": "https://arxiv.org/pdf/2504.05747", "abs": "https://arxiv.org/abs/2504.05747", "authors": ["Raymond Ng", "Thanh Ngan Nguyen", "Yuli Huang", "Ngee Chia Tai", "Wai Yi Leong", "Wei Qi Leong", "Xianbin Yong", "Jian Gang Ngui", "Yosephine Susanto", "Nicholas Cheng", "Hamsawardhini Rengarajan", "Peerat Limkonchotiwat", "Adithya Venkatadri Hulagadri", "Kok Wai Teng", "Yeo Yeow Tong", "Bryan Siow", "Wei Yi Teo", "Wayne Lau", "Choon Meng Tan", "Brandon Ong", "Zhi Hao Ong", "Jann Railey Montalan", "Adwin Chan", "Sajeban Antonyrex", "Ren Lee", "Esther Choa", "David Ong Tat-Wee", "Bing Jie Darius Liu", "William Chandra Tjhi", "Erik Cambria", "Leslie Teo"], "title": "SEA-LION: Southeast Asian Languages in One Network", "categories": ["cs.CL"], "comment": "We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581", "summary": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity."}
{"id": "2504.05594", "pdf": "https://arxiv.org/pdf/2504.05594", "abs": "https://arxiv.org/abs/2504.05594", "authors": ["Qi Mao", "Lan Chen", "Yuchao Gu", "Mike Zheng Shou", "Ming-Hsuan Yang"], "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model", "categories": ["cs.CV"], "comment": "under review", "summary": "Balancing fidelity and editability is essential in text-based image editing\n(TIE), where failures commonly lead to over- or under-editing issues. Existing\nmethods typically rely on attention injections for structure preservation and\nleverage the inherent text alignment capabilities of pre-trained text-to-image\n(T2I) models for editability, but they lack explicit and unified mechanisms to\nproperly balance these two objectives. In this work, we introduce UnifyEdit, a\ntuning-free method that performs diffusion latent optimization to enable a\nbalanced integration of fidelity and editability within a unified framework.\nUnlike direct attention injections, we develop two attention-based constraints:\na self-attention (SA) preservation constraint for structural fidelity, and a\ncross-attention (CA) alignment constraint to enhance text alignment for\nimproved editability. However, simultaneously applying both constraints can\nlead to gradient conflicts, where the dominance of one constraint results in\nover- or under-editing. To address this challenge, we introduce an adaptive\ntime-step scheduler that dynamically adjusts the influence of these\nconstraints, guiding the diffusion latent toward an optimal balance. Extensive\nquantitative and qualitative experiments validate the effectiveness of our\napproach, demonstrating its superiority in achieving a robust balance between\nstructure preservation and text alignment across various editing tasks,\noutperforming other state-of-the-art methods. The source code will be available\nat https://github.com/CUC-MIPG/UnifyEdit."}
{"id": "2504.05759", "pdf": "https://arxiv.org/pdf/2504.05759", "abs": "https://arxiv.org/abs/2504.05759", "authors": ["Nathanaël Beau", "Benoît Crabbé"], "title": "RETROcode: Leveraging a Code Database for Improved Natural Language to Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "As text and code resources have expanded, large-scale pre-trained models have\nshown promising capabilities in code generation tasks, typically employing\nsupervised fine-tuning with problem statement-program pairs. However,\nincreasing model size and data volume for performance gains also raises\ncomputational demands and risks of overfitting. Addressing these challenges, we\npresent RETROcode, a novel adaptation of the RETRO architecture \\cite{RETRO}\nfor sequence-to-sequence models, utilizing a large code database as an\nauxiliary scaling method. This approach, diverging from simply enlarging model\nand dataset sizes, allows RETROcode to leverage a vast code database for\nprediction, enhancing the model's efficiency by integrating extensive memory.\nOur findings indicate that RETROcode not only outperforms similar-sized\ntraditional architectures on test sets but also approaches the effectiveness of\nthe much larger Codex model, despite being trained from scratch on a\nsubstantially smaller dataset."}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility."}
{"id": "2504.05764", "pdf": "https://arxiv.org/pdf/2504.05764", "abs": "https://arxiv.org/abs/2504.05764", "authors": ["Jiho Gwak", "Yuchul Jung"], "title": "Layer-Aware Embedding Fusion for LLMs in Text Classifications", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "11 pages, 3 figures, Preprint", "summary": "Embedding fusion has emerged as an effective approach for enhancing\nperformance across various NLP tasks. However, systematic guidelines for\nselecting optimal layers and developing effective fusion strategies for the\nintegration of LLMs remain underexplored. In this study, we propose a\nlayer-aware embedding selection method and investigate how to quantitatively\nevaluate different layers to identify the most important ones for downstream\nNLP tasks, showing that the critical layers vary depending on the dataset. We\nalso explore how combining embeddings from multiple LLMs, without requiring\nmodel fine-tuning, can improve performance. Experiments on four English text\nclassification datasets (SST-2, MR, R8, and R52) demonstrate that different\nlayers in LLMs exhibit varying degrees of representational strength for\nclassification, and that combining embeddings from different models can enhance\nperformance if the models exhibit complementary characteristics. Additionally,\nwe discuss resources overhead (memory and inference time) to provide a balanced\nperspective on the real world feasibility of embedding fusion. Future work will\nexplore multilingual and domain specific datasets, as well as techniques for\nautomating layer selection, to improve both performance and scalability."}
{"id": "2504.05601", "pdf": "https://arxiv.org/pdf/2504.05601", "abs": "https://arxiv.org/abs/2504.05601", "authors": ["Zhenteng Li", "Sheng Lian", "Dengfeng Pan", "Youlin Wang", "Wei Liu"], "title": "AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes", "categories": ["cs.CV"], "comment": null, "summary": "Object detection in Unmanned Aerial Vehicle (UAV) images poses significant\nchallenges due to complex scale variations and class imbalance among objects.\nExisting methods often address these challenges separately, overlooking the\nintricate nature of UAV images and the potential synergy between them. In\nresponse, this paper proposes AD-Det, a novel framework employing a coherent\ncoarse-to-fine strategy that seamlessly integrates two pivotal components:\nAdaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste\n(DCC). ASOE utilizes a high-resolution feature map to identify and cluster\nregions containing small objects. These regions are subsequently enlarged and\nprocessed by a fine-grained detector. On the other hand, DCC conducts\nobject-level resampling by dynamically pasting tail classes around the cluster\ncenters obtained by ASOE, main-taining a dynamic memory bank for each tail\nclass. This approach enables AD-Det to not only extract regions with small\nobjects for precise detection but also dynamically perform reasonable\nresampling for tail-class objects. Consequently, AD-Det enhances the overall\ndetection performance by addressing the challenges of scale variations and\nclass imbalance in UAV images through a synergistic and adaptive framework. We\nextensively evaluate our approach on two public datasets, i.e., VisDrone and\nUAVDT, and demonstrate that AD-Det significantly outperforms existing\ncompetitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision\n(AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%."}
{"id": "2504.05765", "pdf": "https://arxiv.org/pdf/2504.05765", "abs": "https://arxiv.org/abs/2504.05765", "authors": ["András Horváth", "Paolo Ballarini", "Pierre Cry"], "title": "Probabilistic Process Discovery with Stochastic Process Trees", "categories": ["cs.CL"], "comment": "EAI VALUESTOOLS 2024, Dec 2024, Milan, Italy", "summary": "In order to obtain a stochastic model that accounts for the stochastic\naspects of the dynamics of a business process, usually the following steps are\ntaken. Given an event log, a process tree is obtained through a process\ndiscovery algorithm, i.e., a process tree that is aimed at reproducing, as\naccurately as possible, the language of the log. The process tree is then\ntransformed into a Petri net that generates the same set of sequences as the\nprocess tree. In order to capture the frequency of the sequences in the event\nlog, weights are assigned to the transitions of the Petri net, resulting in a\nstochastic Petri net with a stochastic language in which each sequence is\nassociated with a probability. In this paper we show that this procedure has\nunfavorable properties. First, the weights assigned to the transitions of the\nPetri net have an unclear role in the resulting stochastic language. We will\nshow that a weight can have multiple, ambiguous impact on the probability of\nthe sequences generated by the Petri net. Second, a number of different Petri\nnets with different number of transitions can correspond to the same process\ntree. This means that the number of parameters (the number of weights) that\ndetermines the stochastic language is not well-defined. In order to avoid these\nambiguities, in this paper, we propose to add stochasticity directly to process\ntrees. The result is a new formalism, called stochastic process trees, in which\nthe number of parameters and their role in the associated stochastic language\nis clear and well-defined."}
{"id": "2504.05613", "pdf": "https://arxiv.org/pdf/2504.05613", "abs": "https://arxiv.org/abs/2504.05613", "authors": ["Xiao Zhang", "Xiangyu Han", "Xiwen Lai", "Yao Sun", "Pei Zhang", "Konrad Kording"], "title": "Falcon: Fractional Alternating Cut with Overcoming Minima in Unsupervised Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Today's unsupervised image segmentation algorithms often segment\nsuboptimally. Modern graph-cut based approaches rely on high-dimensional\nattention maps from Transformer-based foundation models, typically employing a\nrelaxed Normalized Cut solved recursively via the Fiedler vector (the\neigenvector of the second smallest eigenvalue). Consequently, they still lag\nbehind supervised methods in both mask generation speed and segmentation\naccuracy. We present a regularized fractional alternating cut (Falcon), an\noptimization-based K-way Normalized Cut without relying on recursive\neigenvector computations, achieving substantially improved speed and accuracy.\nFalcon operates in two stages: (1) a fast K-way Normalized Cut solved by\nextending into a fractional quadratic transformation, with an alternating\niterative procedure and regularization to avoid local minima; and (2)\nrefinement of the resulting masks using complementary low-level information,\nproducing high-quality pixel-level segmentations. Experiments show that Falcon\nnot only surpasses existing state-of-the-art methods by an average of 2.5%\nacross six widely recognized benchmarks (reaching up to 4.3\\% improvement on\nCityscapes), but also reduces runtime by around 30% compared to prior\ngraph-based approaches. These findings demonstrate that the semantic\ninformation within foundation-model attention can be effectively harnessed by a\nhighly parallelizable graph cut framework. Consequently, Falcon can narrow the\ngap between unsupervised and supervised segmentation, enhancing scalability in\nreal-world applications and paving the way for dense prediction-based vision\npre-training in various downstream tasks. The code is released in\nhttps://github.com/KordingLab/Falcon."}
{"id": "2504.05767", "pdf": "https://arxiv.org/pdf/2504.05767", "abs": "https://arxiv.org/abs/2504.05767", "authors": ["Zhang Dong", "Mingbang Wang", "Songhang deng", "Le Dai", "Jiyuan Li", "Xingzu Liu", "Ruilin Nong"], "title": "Cross-Document Contextual Coreference Resolution in Knowledge Graphs", "categories": ["cs.CL", "cs.MA"], "comment": "ACL 2025 Submission Version", "summary": "Coreference resolution across multiple documents poses a significant\nchallenge in natural language processing, particularly within the domain of\nknowledge graphs. This study introduces an innovative method aimed at\nidentifying and resolving references to the same entities that appear across\ndiffering texts, thus enhancing the coherence and collaboration of information.\nOur method employs a dynamic linking mechanism that associates entities in the\nknowledge graph with their corresponding textual mentions. By utilizing\ncontextual embeddings along with graph-based inference strategies, we\neffectively capture the relationships and interactions among entities, thereby\nimproving the accuracy of coreference resolution. Rigorous evaluations on\nvarious benchmark datasets highlight notable advancements in our approach over\ntraditional methodologies. The results showcase how the contextual information\nderived from knowledge graphs enhances the understanding of complex\nrelationships across documents, leading to better entity linking and\ninformation extraction capabilities in applications driven by knowledge. Our\ntechnique demonstrates substantial improvements in both precision and recall,\nunderscoring its effectiveness in the area of cross-document coreference\nresolution."}
{"id": "2504.05623", "pdf": "https://arxiv.org/pdf/2504.05623", "abs": "https://arxiv.org/abs/2504.05623", "authors": ["Mahmoud Afifi", "Luxi Zhao", "Abhijith Punnappurath", "Mohammed A. Abdelsalam", "Ran Zhang", "Michael S. Brown"], "title": "Time-Aware Auto White Balance in Mobile Photography", "categories": ["cs.CV"], "comment": null, "summary": "Cameras rely on auto white balance (AWB) to correct undesirable color casts\ncaused by scene illumination and the camera's spectral sensitivity. This is\ntypically achieved using an illuminant estimator that determines the global\ncolor cast solely from the color information in the camera's raw sensor image.\nMobile devices provide valuable additional metadata-such as capture timestamp\nand geolocation-that offers strong contextual clues to help narrow down the\npossible illumination solutions. This paper proposes a lightweight illuminant\nestimation method that incorporates such contextual metadata, along with\nadditional capture information and image colors, into a compact model (~5K\nparameters), achieving promising results, matching or surpassing larger models.\nTo validate our method, we introduce a dataset of 3,224 smartphone images with\ncontextual metadata collected at various times of day and under diverse\nlighting conditions. The dataset includes ground-truth illuminant colors,\ndetermined using a color chart, and user-preferred illuminants validated\nthrough a user study, providing a comprehensive benchmark for AWB evaluation."}
{"id": "2504.05824", "pdf": "https://arxiv.org/pdf/2504.05824", "abs": "https://arxiv.org/abs/2504.05824", "authors": ["Zhang Dong", "Songhang deng", "Mingbang Wang", "Le Dai", "Jiyuan Li", "Xingzu Liu", "Ruilin Nong"], "title": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency and Accuracy in Large-Scale Systems", "categories": ["cs.CL"], "comment": "submission of acl 2025", "summary": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield."}
{"id": "2504.05640", "pdf": "https://arxiv.org/pdf/2504.05640", "abs": "https://arxiv.org/abs/2504.05640", "authors": ["Mingyang Zhu", "Yuqiu Liang", "Jiacheng Wang"], "title": "CTI-Unet: Cascaded Threshold Integration for Improved U-Net Segmentation of Pathology Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Chronic kidney disease (CKD) is a growing global health concern,\nnecessitating precise and efficient image analysis to aid diagnosis and\ntreatment planning. Automated segmentation of kidney pathology images plays a\ncentral role in facilitating clinical workflows, yet conventional segmentation\nmodels often require delicate threshold tuning. This paper proposes a novel\n\\textit{Cascaded Threshold-Integrated U-Net (CTI-Unet)} to overcome the\nlimitations of single-threshold segmentation. By sequentially integrating\nmultiple thresholded outputs, our approach can reconcile noise suppression with\nthe preservation of finer structural details. Experiments on the challenging\nKPIs2024 dataset demonstrate that CTI-Unet outperforms state-of-the-art\narchitectures such as nnU-Net, Swin-Unet, and CE-Net, offering a robust and\nflexible framework for kidney pathology image segmentation."}
{"id": "2504.05831", "pdf": "https://arxiv.org/pdf/2504.05831", "abs": "https://arxiv.org/abs/2504.05831", "authors": ["Mingye Zhu", "Yi Liu", "Junbo Guo", "Quan Wang", "Yongdong Zhang", "Zhendong Mao"], "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly rely on preference alignment\nmethods to steer outputs toward human values, yet these methods are often\nconstrained by the scarcity of high-quality human-annotated data. To tackle\nthis, recent approaches have turned to synthetic data generated by LLMs as a\nscalable alternative. However, synthetic data can introduce distribution\nshifts, compromising the nuanced human preferences that are essential for\ndesirable outputs. In this paper, we propose a novel distribution-aware\noptimization framework that improves preference alignment in the presence of\nsuch shifts. Our approach first estimates the likelihood ratios between the\ntarget and training distributions leveraging a learned classifier, then it\nminimizes the worst-case loss over data regions that reflect the target\nhuman-preferred distribution. By explicitly prioritizing the target\ndistribution during optimization, our method mitigates the adverse effects of\ndistributional variation and enhances the generation of responses that\nfaithfully reflect human values."}
{"id": "2504.05644", "pdf": "https://arxiv.org/pdf/2504.05644", "abs": "https://arxiv.org/abs/2504.05644", "authors": ["Yan Zhang", "Zhong Ji", "Changxu Meng", "Yanwei Pang", "Jungong Han"], "title": "iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via Eliminate Before Align and Keyword Explicit Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR),\nwhich aims at searching for the corresponding targets based on the given query.\nAmong these efforts, the application of Foundation Models (FMs), such as CLIP,\nto the domain of remote sensing has yielded encouraging outcomes. However,\nexisting FM based methodologies neglect the negative impact of weakly\ncorrelated sample pairs and fail to account for the key distinctions among\nremote sensing texts, leading to biased and superficial exploration of sample\npairs. To address these challenges, we propose an approach named iEBAKER (an\nImproved Eliminate Before Align strategy with Keyword Explicit Reasoning\nframework) for RSITR. Specifically, we propose an innovative Eliminate Before\nAlign (EBA) strategy to filter out the weakly correlated sample pairs, thereby\nmitigating their deviations from optimal embedding space during\nalignment.Further, two specific schemes are introduced from the perspective of\nwhether local similarity and global similarity affect each other. On this\nbasis, we introduce an alternative Sort After Reversed Retrieval (SAR)\nstrategy, aims at optimizing the similarity matrix via reverse retrieval.\nAdditionally, we incorporate a Keyword Explicit Reasoning (KER) module to\nfacilitate the beneficial impact of subtle key concept distinctions. Without\nbells and whistles, our approach enables a direct transition from FM to RSITR\ntask, eliminating the need for additional pretraining on remote sensing data.\nExtensive experiments conducted on three popular benchmark datasets demonstrate\nthat our proposed iEBAKER method surpasses the state-of-the-art models while\nrequiring less training data. Our source code will be released at\nhttps://github.com/zhangy0822/iEBAKER."}
{"id": "2504.05855", "pdf": "https://arxiv.org/pdf/2504.05855", "abs": "https://arxiv.org/abs/2504.05855", "authors": ["Xingzu Liu", "Songhang deng", "Mingbang Wang", "Zhang Dong", "Le Dai", "Jiyuan Li", "Ruilin Nong"], "title": "Enhancing Coreference Resolution with Pretrained Language Models: Bridging the Gap Between Syntax and Semantics", "categories": ["cs.CL", "cs.AI"], "comment": "acl submission", "summary": "Large language models have made significant advancements in various natural\nlanguage processing tasks, including coreference resolution. However,\ntraditional methods often fall short in effectively distinguishing referential\nrelationships due to a lack of integration between syntactic and semantic\ninformation. This study introduces an innovative framework aimed at enhancing\ncoreference resolution by utilizing pretrained language models. Our approach\ncombines syntax parsing with semantic role labeling to accurately capture finer\ndistinctions in referential relationships. By employing state-of-the-art\npretrained models to gather contextual embeddings and applying an attention\nmechanism for fine-tuning, we improve the performance of coreference tasks.\nExperimental results across diverse datasets show that our method surpasses\nconventional coreference resolution systems, achieving notable accuracy in\ndisambiguating references. This development not only improves coreference\nresolution outcomes but also positively impacts other natural language\nprocessing tasks that depend on precise referential understanding."}
{"id": "2504.05649", "pdf": "https://arxiv.org/pdf/2504.05649", "abs": "https://arxiv.org/abs/2504.05649", "authors": ["Yining Shi", "Kun Jiang", "Xin Zhao", "Kangan Qian", "Chuchu Xie", "Tuopu Wen", "Mengmeng Yang", "Diange Yang"], "title": "POD: Predictive Object Detection with Single-Frame FMCW LiDAR Point Cloud", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based 3D object detection is a fundamental task in the field of\nautonomous driving. This paper explores the unique advantage of Frequency\nModulated Continuous Wave (FMCW) LiDAR in autonomous perception. Given a single\nframe FMCW point cloud with radial velocity measurements, we expect that our\nobject detector can detect the short-term future locations of objects using\nonly the current frame sensor data and demonstrate a fast ability to respond to\nintermediate danger. To achieve this, we extend the standard object detection\ntask to a novel task named predictive object detection (POD), which aims to\npredict the short-term future location and dimensions of objects based solely\non current observations. Typically, a motion prediction task requires\nhistorical sensor information to process the temporal contexts of each object,\nwhile our detector's avoidance of multi-frame historical information enables a\nmuch faster response time to potential dangers. The core advantage of FMCW\nLiDAR lies in the radial velocity associated with every reflected point. We\npropose a novel POD framework, the core idea of which is to generate a virtual\nfuture point using a ray casting mechanism, create virtual two-frame point\nclouds with the current and virtual future frames, and encode these two-frame\nvoxel features with a sparse 4D encoder. Subsequently, the 4D voxel features\nare separated by temporal indices and remapped into two Bird's Eye View (BEV)\nfeatures: one decoded for standard current frame object detection and the other\nfor future predictive object detection. Extensive experiments on our in-house\ndataset demonstrate the state-of-the-art standard and predictive detection\nperformance of the proposed POD framework."}
{"id": "2504.05898", "pdf": "https://arxiv.org/pdf/2504.05898", "abs": "https://arxiv.org/abs/2504.05898", "authors": ["Peerat Limkonchotiwat", "Kanruethai Masuk", "Surapon Nonesung", "Chalermpun Mai-On", "Sarana Nutanong", "Wuttikorn Ponwitayarat", "Potsawee Manakul"], "title": "Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and Human Evaluation", "categories": ["cs.CL"], "comment": "Datasets and codes are available at\n  https://github.com/mrpeerat/Thai_local_benchmark", "summary": "Large language models show promising results in various NLP tasks. Despite\nthese successes, the robustness and consistency of LLMs in underrepresented\nlanguages remain largely unexplored, especially concerning local dialects.\nExisting benchmarks also focus on main dialects, neglecting LLMs' ability on\nlocal dialect texts. In this paper, we introduce a Thai local dialect benchmark\ncovering Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai,\nevaluating LLMs on five NLP tasks: summarization, question answering,\ntranslation, conversation, and food-related tasks. Furthermore, we propose a\nhuman evaluation guideline and metric for Thai local dialects to assess\ngeneration fluency and dialect-specific accuracy. Results show that LLM\nperformance declines significantly in local Thai dialects compared to standard\nThai, with only proprietary models like GPT-4o and Gemini2 demonstrating some\nfluency"}
{"id": "2504.05662", "pdf": "https://arxiv.org/pdf/2504.05662", "abs": "https://arxiv.org/abs/2504.05662", "authors": ["Shunsuke Sakai", "Tatsuhito Hasegawa"], "title": "Reconstruction-Free Anomaly Detection with Diffusion Models via Direct Latent Likelihood Evaluation", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/SkyShunsuke/InversionAD", "summary": "Diffusion models, with their robust distribution approximation capabilities,\nhave demonstrated excellent performance in anomaly detection. However,\nconventional reconstruction-based approaches rely on computing the\nreconstruction error between the original and denoised images, which requires\ncareful noise-strength tuning and over ten network evaluations per\ninput-leading to significantly slower detection speeds. To address these\nlimitations, we propose a novel diffusion-based anomaly detection method that\ncircumvents the need for resource-intensive reconstruction. Instead of\nreconstructing the input image, we directly infer its corresponding latent\nvariables and measure their density under the Gaussian prior distribution.\nRemarkably, the prior density proves effective as an anomaly score even when\nusing a short partial diffusion process of only 2-5 steps. We evaluate our\nmethod on the MVTecAD dataset, achieving an AUC of 0.991 at 15 FPS, thereby\nsetting a new state-of-the-art speed-AUC anomaly detection trade-off."}
{"id": "2504.05914", "pdf": "https://arxiv.org/pdf/2504.05914", "abs": "https://arxiv.org/abs/2504.05914", "authors": ["Abhiram Reddy Yanampally"], "title": "High-Resource Translation:Turning Abundance into Accessibility", "categories": ["cs.CL"], "comment": "6 pages, 2 figures", "summary": "This paper presents a novel approach to constructing an English-to-Telugu\ntranslation model by leveraging transfer learning techniques and addressing the\nchallenges associated with low-resource languages. Utilizing the Bharat\nParallel Corpus Collection (BPCC) as the primary dataset, the model\nincorporates iterative backtranslation to generate synthetic parallel data,\neffectively augmenting the training dataset and enhancing the model's\ntranslation capabilities. The research focuses on a comprehensive strategy for\nimproving model performance through data augmentation, optimization of training\nparameters, and the effective use of pre-trained models. These methodologies\naim to create a robust translation system that can handle diverse sentence\nstructures and linguistic nuances in both English and Telugu. This work\nhighlights the significance of innovative data handling techniques and the\npotential of transfer learning in overcoming limitations posed by sparse\ndatasets in low-resource languages. The study contributes to the field of\nmachine translation and seeks to improve communication between English and\nTelugu speakers in practical contexts."}
{"id": "2504.05672", "pdf": "https://arxiv.org/pdf/2504.05672", "abs": "https://arxiv.org/abs/2504.05672", "authors": ["Tianshui Chen", "Jianman Lin", "Zhijing Yang", "Chumei Qing", "Yukai Shi", "Liang Lin"], "title": "Contrastive Decoupled Representation Learning and Regularization for Speech-Preserving Facial Expression Manipulation", "categories": ["cs.CV", "cs.SD"], "comment": null, "summary": "Speech-preserving facial expression manipulation (SPFEM) aims to modify a\ntalking head to display a specific reference emotion while preserving the mouth\nanimation of source spoken contents. Thus, emotion and content information\nexisting in reference and source inputs can provide direct and accurate\nsupervision signals for SPFEM models. However, the intrinsic intertwining of\nthese elements during the talking process poses challenges to their\neffectiveness as supervisory signals. In this work, we propose to learn content\nand emotion priors as guidance augmented with contrastive learning to learn\ndecoupled content and emotion representation via an innovative Contrastive\nDecoupled Representation Learning (CDRL) algorithm. Specifically, a Contrastive\nContent Representation Learning (CCRL) module is designed to learn audio\nfeature, which primarily contains content information, as content priors to\nguide learning content representation from the source input. Meanwhile, a\nContrastive Emotion Representation Learning (CERL) module is proposed to make\nuse of a pre-trained visual-language model to learn emotion prior, which is\nthen used to guide learning emotion representation from the reference input. We\nfurther introduce emotion-aware and emotion-augmented contrastive learning to\ntrain CCRL and CERL modules, respectively, ensuring learning\nemotion-independent content representation and content-independent emotion\nrepresentation. During SPFEM model training, the decoupled content and emotion\nrepresentations are used to supervise the generation process, ensuring more\naccurate emotion manipulation together with audio-lip synchronization.\nExtensive experiments and evaluations on various benchmarks show the\neffectiveness of the proposed algorithm."}
{"id": "2504.05954", "pdf": "https://arxiv.org/pdf/2504.05954", "abs": "https://arxiv.org/abs/2504.05954", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "title": "Unsupervised Location Mapping for Narrative Corpora", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work presents the task of unsupervised location mapping, which seeks to\nmap the trajectory of an individual narrative on a spatial map of locations in\nwhich a large set of narratives take place. Despite the fundamentality and\ngenerality of the task, very little work addressed the spatial mapping of\nnarrative texts. The task consists of two parts: (1) inducing a ``map'' with\nthe locations mentioned in a set of texts, and (2) extracting a trajectory from\na single narrative and positioning it on the map. Following recent advances in\nincreasing the context length of large language models, we propose a pipeline\nfor this task in a completely unsupervised manner without predefining the set\nof labels. We test our method on two different domains: (1) Holocaust\ntestimonies and (2) Lake District writing, namely multi-century literature on\ntravels in the English Lake District. We perform both intrinsic and extrinsic\nevaluations for the task, with encouraging results, thereby setting a benchmark\nand evaluation practices for the task, as well as highlighting challenges."}
{"id": "2504.05673", "pdf": "https://arxiv.org/pdf/2504.05673", "abs": "https://arxiv.org/abs/2504.05673", "authors": ["Dongjun Qian", "Kai Su", "Yiming Tan", "Qishuai Diao", "Xian Wu", "Chang Liu", "Bingyue Peng", "Zehuan Yuan"], "title": "VC-LLM: Automated Advertisement Video Creation from Raw Footage using Multi-modal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "As short videos have risen in popularity, the role of video content in\nadvertising has become increasingly significant. Typically, advertisers record\na large amount of raw footage about the product and then create numerous\ndifferent short-form advertisement videos based on this raw footage. Creating\nsuch videos mainly involves editing raw footage and writing advertisement\nscripts, which requires a certain level of creative ability. It is usually\nchallenging to create many different video contents for the same product, and\nmanual efficiency is often low. In this paper, we present VC-LLM, a framework\npowered by Large Language Models for the automatic creation of high-quality\nshort-form advertisement videos. Our approach leverages high-resolution spatial\ninput and low-resolution temporal input to represent video clips more\neffectively, capturing both fine-grained visual details and broader temporal\ndynamics. In addition, during training, we incorporate supplementary\ninformation generated by rewriting the ground truth text, ensuring that all key\noutput information can be directly traced back to the input, thereby reducing\nmodel hallucinations. We also designed a benchmark to evaluate the quality of\nthe created videos. Experiments show that VC-LLM based on GPT-4o can produce\nvideos comparable to those created by humans. Furthermore, we collected\nnumerous high-quality short advertisement videos to create a pre-training\ndataset and manually cleaned a portion of the data to construct a high-quality\nfine-tuning dataset. Experiments indicate that, on the benchmark, the VC-LLM\nbased on fine-tuned LLM can produce videos with superior narrative logic\ncompared to those created by the VC-LLM based on GPT-4o."}
{"id": "2504.05995", "pdf": "https://arxiv.org/pdf/2504.05995", "abs": "https://arxiv.org/abs/2504.05995", "authors": ["Firoj Alam", "Md Arid Hasan", "Sahinur Rahman Laskar", "Mucahid Kutlu", "Shammur Absar Chowdhury"], "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday Knowledge", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework)."}
{"id": "2504.05677", "pdf": "https://arxiv.org/pdf/2504.05677", "abs": "https://arxiv.org/abs/2504.05677", "authors": ["Shunsuke Sakai", "Shunsuke Tsuge", "Tatsuhito Hasegawa"], "title": "Noisy Deep Ensemble: Accelerating Deep Ensemble Learning via Noise Injection", "categories": ["cs.CV"], "comment": null, "summary": "Neural network ensembles is a simple yet effective approach for enhancing\ngeneralization capabilities. The most common method involves independently\ntraining multiple neural networks initialized with different weights and then\naveraging their predictions during inference. However, this approach increases\ntraining time linearly with the number of ensemble members. To address this\nissue, we propose the novel ``\\textbf{Noisy Deep Ensemble}'' method,\nsignificantly reducing the training time required for neural network ensembles.\nIn this method, a \\textit{parent model} is trained until convergence, and then\nthe weights of the \\textit{parent model} are perturbed in various ways to\nconstruct multiple \\textit{child models}. This perturbation of the\n\\textit{parent model} weights facilitates the exploration of different local\nminima while significantly reducing the training time for each ensemble member.\nWe evaluated our method using diverse CNN architectures on CIFAR-10 and\nCIFAR-100 datasets, surpassing conventional efficient ensemble methods and\nachieving test accuracy comparable to standard ensembles. Code is available at\n\\href{https://github.com/TSTB-dev/NoisyDeepEnsemble}{https://github.com/TSTB-dev/NoisyDeepEnsemble}"}
{"id": "2504.06011", "pdf": "https://arxiv.org/pdf/2504.06011", "abs": "https://arxiv.org/abs/2504.06011", "authors": ["Monojit Choudhury", "Shivam Chauhan", "Rocktim Jyoti Das", "Dhruv Sahnan", "Xudong Han", "Haonan Li", "Aaryamonvikram Singh", "Alok Anil Jadhav", "Utkarsh Agarwal", "Mukund Choudhary", "Debopriyo Banerjee", "Fajri Koto", "Junaid Bhat", "Awantika Shukla", "Samujjwal Ghosh", "Samta Kamboj", "Onkar Pandit", "Lalit Pradhan", "Rahul Pal", "Sunil Sahu", "Soundar Doraiswamy", "Parvez Mullah", "Ali El Filali", "Neha Sengupta", "Gokul Ramakrishnan", "Rituraj Joshi", "Gurpreet Gosal", "Avraham Sheinin", "Natalia Vassilieva", "Preslav Nakov"], "title": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for Hindi", "categories": ["cs.CL"], "comment": null, "summary": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services."}
{"id": "2504.05679", "pdf": "https://arxiv.org/pdf/2504.05679", "abs": "https://arxiv.org/abs/2504.05679", "authors": ["Udayanga G. W. K. N. Gamage", "Xuanni Huo", "Luca Zanatta", "T Delbruck", "Cesar Cadena", "Matteo Fumagalli", "Silvia Tolu"], "title": "Event-based Civil Infrastructure Visual Defect Detection: ev-CIVIL Dataset and Benchmark", "categories": ["cs.CV"], "comment": "A journal paper which submitted to Sage SHM journa and it is under\n  review currently. consist of 25 pages. It has 19 figures and 5 tables.\n  Keywords Event-based vision, civil structural health monitoring, defect\n  detection, crack, spalling, DVS, dataset, YOLOv6, SSD, 2D event histograms", "summary": "Small Unmanned Aerial Vehicle (UAV) based visual inspections are a more\nefficient alternative to manual methods for examining civil structural defects,\noffering safe access to hazardous areas and significant cost savings by\nreducing labor requirements. However, traditional frame-based cameras, widely\nused in UAV-based inspections, often struggle to capture defects under low or\ndynamic lighting conditions. In contrast, Dynamic Vision Sensors (DVS), or\nevent-based cameras, excel in such scenarios by minimizing motion blur,\nenhancing power efficiency, and maintaining high-quality imaging across diverse\nlighting conditions without saturation or information loss. Despite these\nadvantages, existing research lacks studies exploring the feasibility of using\nDVS for detecting civil structural defects.Moreover, there is no dedicated\nevent-based dataset tailored for this purpose. Addressing this gap, this study\nintroduces the first event-based civil infrastructure defect detection dataset,\ncapturing defective surfaces as a spatio-temporal event stream using DVS.In\naddition to event-based data, the dataset includes grayscale intensity image\nframes captured simultaneously using an Active Pixel Sensor (APS). Both data\ntypes were collected using the DAVIS346 camera, which integrates DVS and APS\nsensors.The dataset focuses on two types of defects: cracks and spalling, and\nincludes data from both field and laboratory environments. The field dataset\ncomprises 318 recording sequences,documenting 458 distinct cracks and 121\ndistinct spalling instances.The laboratory dataset includes 362 recording\nsequences, covering 220 distinct cracks and 308 spalling instances.Four\nrealtime object detection models were evaluated on it to validate the dataset\neffectiveness.The results demonstrate the dataset robustness in enabling\naccurate defect detection and classification,even under challenging lighting\nconditions."}
{"id": "2504.06036", "pdf": "https://arxiv.org/pdf/2504.06036", "abs": "https://arxiv.org/abs/2504.06036", "authors": ["Qitong Wang", "Mohammed J. Zaki", "Georgios Kollias", "Vasileios Kalantzis"], "title": "Multi-Sense Embeddings for Language Models and Knowledge Distillation", "categories": ["cs.CL"], "comment": "16 pages, 4 figures", "summary": "Transformer-based large language models (LLMs) rely on contextual embeddings\nwhich generate different (continuous) representations for the same token\ndepending on its surrounding context. Nonetheless, words and tokens typically\nhave a limited number of senses (or meanings). We propose multi-sense\nembeddings as a drop-in replacement for each token in order to capture the\nrange of their uses in a language. To construct a sense embedding dictionary,\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\nthe cluster centers as representative sense embeddings. In addition, we propose\na novel knowledge distillation method that leverages the sense dictionary to\nlearn a smaller student model that mimics the senses from the much larger base\nLLM model, offering significant space and inference time savings, while\nmaintaining competitive performance. Via thorough experiments on various\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\ndistillation approach. We share our code at\nhttps://github.com/Qitong-Wang/SenseDict"}
{"id": "2504.05682", "pdf": "https://arxiv.org/pdf/2504.05682", "abs": "https://arxiv.org/abs/2504.05682", "authors": ["Xiaxu Chen", "Wei Li", "Chunxu Liu", "Chi Xie", "Xiaoyan Hu", "Chengqian Ma", "Feng Zhu", "Rui Zhao"], "title": "On the Suitability of Reinforcement Fine-Tuning to Visual Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) is proved to be greatly valuable for\nenhancing the reasoning ability of LLMs. Researchers have been starting to\napply RFT to MLLMs, hoping it will also enhance the capabilities of visual\nunderstanding. However, these works are at a very early stage and have not\nexamined how suitable RFT actually is for visual tasks. In this work, we\nendeavor to understand the suitabilities and limitations of RFT for visual\ntasks, through experimental analysis and observations. We start by quantitative\ncomparisons on various tasks, which shows RFT is generally better than SFT on\nvisual tasks. %especially when the number of training samples are limited. To\ncheck whether such advantages are brought up by the reasoning process, we\ndesign a new reward that encourages the model to ``think'' more, whose results\nshow more thinking can be beneficial for complicated tasks but harmful for\nsimple tasks. We hope this study can provide more insight for the rapid\nadvancements on this topic."}
{"id": "2504.06037", "pdf": "https://arxiv.org/pdf/2504.06037", "abs": "https://arxiv.org/abs/2504.06037", "authors": ["Seunghyun Ji", "Soowon Lee"], "title": "Confidence Regularized Masked Language Modeling using Text Length", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 1 figure", "summary": "Masked language modeling, which is a task to predict a randomly masked word\nin the input text, is an efficient language representation learning method.\nMasked language modeling ignores various words which people can think of for\nfilling in the masked position and calculates the loss with a single word.\nEspecially when the input text is short, the entropy of the word distribution\nthat can fill in the masked position can be high. This may cause the model to\nbe overconfident in the single answer. To address this issue, we propose a\nnovel confidence regularizer that controls regularizing strength dynamically by\nthe input text length. Experiments with GLUE and SQuAD datasets showed that our\nmethod achieves better accuracy and lower expected calibration error."}
{"id": "2504.05698", "pdf": "https://arxiv.org/pdf/2504.05698", "abs": "https://arxiv.org/abs/2504.05698", "authors": ["Wesley Khademi", "Li Fuxin"], "title": "Point-based Instance Completion with Scene Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Recent point-based object completion methods have demonstrated the ability to\naccurately recover the missing geometry of partially observed objects. However,\nthese approaches are not well-suited for completing objects within a scene, as\nthey do not consider known scene constraints (e.g., other observed surfaces) in\ntheir completions and further expect the partial input to be in a canonical\ncoordinate system, which does not hold for objects within scenes. While\ninstance scene completion methods have been proposed for completing objects\nwithin a scene, they lag behind point-based object completion methods in terms\nof object completion quality and still do not consider known scene constraints\nduring completion. To overcome these limitations, we propose a point\ncloud-based instance completion model that can robustly complete objects at\narbitrary scales and pose in the scene. To enable reasoning at the scene level,\nwe introduce a sparse set of scene constraints represented as point clouds and\nintegrate them into our completion model via a cross-attention mechanism. To\nevaluate the instance scene completion task on indoor scenes, we further build\na new dataset called ScanWCF, which contains labeled partial scans as well as\naligned ground truth scene completions that are watertight and collision-free.\nThrough several experiments, we demonstrate that our method achieves improved\nfidelity to partial scans, higher completion quality, and greater plausibility\nover existing state-of-the-art methods."}
{"id": "2504.06136", "pdf": "https://arxiv.org/pdf/2504.06136", "abs": "https://arxiv.org/abs/2504.06136", "authors": ["Movina Moses", "Mohab Elkaref", "James Barry", "Shinnosuke Tanaka", "Vishnudev Kuruvanthodi", "Nathan Herr", "Campbell D Watson", "Geeth De Mel"], "title": "QGen Studio: An Adaptive Question-Answer Generation, Training and Evaluation Platform", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally."}
{"id": "2504.05700", "pdf": "https://arxiv.org/pdf/2504.05700", "abs": "https://arxiv.org/abs/2504.05700", "authors": ["Seth Z. Zhao", "Reza Ghoddoosian", "Isht Dwivedi", "Nakul Agarwal", "Behzad Dariush"], "title": "Pose-Aware Weakly-Supervised Action Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Understanding human behavior is an important problem in the pursuit of visual\nintelligence. A challenge in this endeavor is the extensive and costly effort\nrequired to accurately label action segments. To address this issue, we\nconsider learning methods that demand minimal supervision for segmentation of\nhuman actions in long instructional videos. Specifically, we introduce a\nweakly-supervised framework that uniquely incorporates pose knowledge during\ntraining while omitting its use during inference, thereby distilling pose\nknowledge pertinent to each action component. We propose a pose-inspired\ncontrastive loss as a part of the whole weakly-supervised framework which is\ntrained to distinguish action boundaries more effectively. Our approach,\nvalidated through extensive experiments on representative datasets, outperforms\nprevious state-of-the-art (SOTA) in segmenting long instructional videos under\nboth online and offline settings. Additionally, we demonstrate the framework's\nadaptability to various segmentation backbones and pose extractors across\ndifferent datasets."}
{"id": "2504.06160", "pdf": "https://arxiv.org/pdf/2504.06160", "abs": "https://arxiv.org/abs/2504.06160", "authors": ["Rijul Magu", "Arka Dutta", "Sean Kim", "Ashiqur R. KhudaBukhsh", "Munmun De Choudhury"], "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI", "J.4; K.4.1; K.4.2"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation."}
{"id": "2504.05706", "pdf": "https://arxiv.org/pdf/2504.05706", "abs": "https://arxiv.org/abs/2504.05706", "authors": ["Fida Mohammad Thoker", "Letian Jiang", "Chen Zhao", "Piyush Bagad", "Hazel Doughty", "Bernard Ghanem", "Cees G. M. Snoek"], "title": "SEVERE++: Evaluating Benchmark Sensitivity in Generalization of Video Representation Learning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Continued advances in self-supervised learning have led to significant\nprogress in video representation learning, offering a scalable alternative to\nsupervised approaches by removing the need for manual annotations. Despite\nstrong performance on standard action recognition benchmarks, video\nself-supervised learning methods are largely evaluated under narrow protocols,\ntypically pretraining on Kinetics-400 and fine-tuning on similar datasets,\nlimiting our understanding of their generalization in real world scenarios. In\nthis work, we present a comprehensive evaluation of modern video\nself-supervised models, focusing on generalization across four key downstream\nfactors: domain shift, sample efficiency, action granularity, and task\ndiversity. Building on our prior work analyzing benchmark sensitivity in\nCNN-based contrastive learning, we extend the study to cover state-of-the-art\ntransformer-based video-only and video-text models. Specifically, we benchmark\n12 transformer-based methods (7 video-only, 5 video-text) and compare them to\n10 CNN-based methods, totaling over 1100 experiments across 8 datasets and 7\ndownstream tasks. Our analysis shows that, despite architectural advances,\ntransformer-based models remain sensitive to downstream conditions. No method\ngeneralizes consistently across all factors, video-only transformers perform\nbetter under domain shifts, CNNs outperform for fine-grained tasks, and\nvideo-text models often underperform despite large scale pretraining. We also\nfind that recent transformer models do not consistently outperform earlier\napproaches. Our findings provide a detailed view of the strengths and\nlimitations of current video SSL methods and offer a unified benchmark for\nevaluating generalization in video representation learning."}
{"id": "2504.06166", "pdf": "https://arxiv.org/pdf/2504.06166", "abs": "https://arxiv.org/abs/2504.06166", "authors": ["Montgomery Gole", "Andriy Miranskyy"], "title": "Assessing how hyperparameters impact Large Language Models' sarcasm detection performance", "categories": ["cs.CL"], "comment": null, "summary": "Sarcasm detection is challenging for both humans and machines. This work\nexplores how model characteristics impact sarcasm detection in OpenAI's GPT,\nand Meta's Llama-2 models, given their strong natural language understanding,\nand popularity. We evaluate fine-tuned and zero-shot models across various\nsizes, releases, and hyperparameters. Experiments were conducted on the\npolitical and balanced (pol-bal) portion of the popular Self-Annotated Reddit\nCorpus (SARC2.0) sarcasm dataset. Fine-tuned performance improves monotonically\nwith model size within a model family, while hyperparameter tuning also impacts\nperformance. In the fine-tuning scenario, full precision Llama-2-13b achieves\nstate-of-the-art accuracy and $F_1$-score, both measured at 0.83, comparable to\naverage human performance. In the zero-shot setting, one GPT-4 model achieves\ncompetitive performance to prior attempts, yielding an accuracy of 0.70 and an\n$F_1$-score of 0.75. Furthermore, a model's performance may increase or decline\nwith each release, highlighting the need to reassess performance after each\nrelease."}
{"id": "2504.05720", "pdf": "https://arxiv.org/pdf/2504.05720", "abs": "https://arxiv.org/abs/2504.05720", "authors": ["Jiaqi Li", "Ruowei Wang", "Yu Liu", "Qijun Zhao"], "title": "QEMesh: Employing A Quadric Error Metrics-Based Representation for Mesh Generation", "categories": ["cs.CV"], "comment": "Accepted by International Conference on Multimedia and Expo", "summary": "Mesh generation plays a crucial role in 3D content creation, as mesh is\nwidely used in various industrial applications. Recent works have achieved\nimpressive results but still face several issues, such as unrealistic patterns\nor pits on surfaces, thin parts missing, and incomplete structures. Most of\nthese problems stem from the choice of shape representation or the capabilities\nof the generative network. To alleviate these, we extend PoNQ, a Quadric Error\nMetrics (QEM)-based representation, and propose a novel model, QEMesh, for\nhigh-quality mesh generation. PoNQ divides the shape surface into tiny patches,\neach represented by a point with its normal and QEM matrix, which preserves\nfine local geometry information. In our QEMesh, we regard these elements as\ngenerable parameters and design a unique latent diffusion model containing a\nnovel multi-decoder VAE for PoNQ parameters generation. Given the latent code\ngenerated by the diffusion model, three parameter decoders produce several PoNQ\nparameters within each voxel cell, and an occupancy decoder predicts which\nvoxel cells containing parameters to form the final shape. Extensive\nevaluations demonstrate that our method generates results with watertight\nsurfaces and is comparable to state-of-the-art methods in several main metrics."}
{"id": "2504.06214", "pdf": "https://arxiv.org/pdf/2504.06214", "abs": "https://arxiv.org/abs/2504.06214", "authors": ["Chejian Xu", "Wei Ping", "Peng Xu", "Zihan Liu", "Boxin Wang", "Mohammad Shoeybi", "Bo Li", "Bryan Catanzaro"], "title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capabilities are essential for a wide range of applications,\nincluding document and video understanding, in-context learning, and\ninference-time scaling, all of which require models to process and reason over\nlong sequences of text and multimodal data. In this work, we introduce a\nefficient training recipe for building ultra-long context LLMs from aligned\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\nand 4M tokens. Our approach leverages efficient continued pretraining\nstrategies to extend the context window and employs effective instruction\ntuning to maintain the instruction-following and reasoning abilities. Our\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\nstate-of-the-art performance across a diverse set of long-context benchmarks.\nImportantly, models trained with our approach maintain competitive performance\non standard benchmarks, demonstrating balanced improvements for both long and\nshort context tasks. We further provide an in-depth analysis of key design\nchoices, highlighting the impacts of scaling strategies and data composition.\nOur findings establish a robust framework for efficiently scaling context\nlengths while preserving general model capabilities. We release all model\nweights at: https://ultralong.github.io/."}
{"id": "2504.05741", "pdf": "https://arxiv.org/pdf/2504.05741", "abs": "https://arxiv.org/abs/2504.05741", "authors": ["Shuai Wang", "Zhi Tian", "Weilin Huang", "Limin Wang"], "title": "DDT: Decoupled Diffusion Transformer", "categories": ["cs.CV", "cs.AI"], "comment": "sota on ImageNet256 and ImageNet512", "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\textbf{\\color{ddt}D}ecoupled \\textbf{\\color{ddt}D}iffusion\n\\textbf{\\color{ddt}T}ransformer~(\\textbf{\\color{ddt}DDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n$256\\times256$, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly $4\\times$ faster training convergence compared to previous\ndiffusion transformers). For ImageNet $512\\times512$, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies."}
{"id": "2504.06219", "pdf": "https://arxiv.org/pdf/2504.06219", "abs": "https://arxiv.org/abs/2504.06219", "authors": ["Dongyang Fan", "Vinko Sabolčec", "Matin Ansaripour", "Ayush Kumar Tarun", "Martin Jaggi", "Antoine Bosselut", "Imanol Schlag"], "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions."}
{"id": "2504.05746", "pdf": "https://arxiv.org/pdf/2504.05746", "abs": "https://arxiv.org/abs/2504.05746", "authors": ["Zhihua Xu", "Tianshui Chen", "Zhijing Yang", "Siyuan Peng", "Keze Wang", "Liang Lin"], "title": "Exploiting Temporal Audio-Visual Correlation Embedding for Audio-Driven One-Shot Talking Head Animation", "categories": ["cs.CV"], "comment": "Accepted at TMM 2025", "summary": "The paramount challenge in audio-driven One-shot Talking Head Animation\n(ADOS-THA) lies in capturing subtle imperceptible changes between adjacent\nvideo frames. Inherently, the temporal relationship of adjacent audio clips is\nhighly correlated with that of the corresponding adjacent video frames,\noffering supplementary information that can be pivotal for guiding and\nsupervising talking head animations. In this work, we propose to learn\naudio-visual correlations and integrate the correlations to help enhance\nfeature representation and regularize final generation by a novel Temporal\nAudio-Visual Correlation Embedding (TAVCE) framework. Specifically, it first\nlearns an audio-visual temporal correlation metric, ensuring the temporal audio\nrelationships of adjacent clips are aligned with the temporal visual\nrelationships of corresponding adjacent video frames. Since the temporal audio\nrelationship contains aligned information about the visual frame, we first\nintegrate it to guide learning more representative features via a simple yet\neffective channel attention mechanism. During training, we also use the\nalignment correlations as an additional objective to supervise generating\nvisual frames. We conduct extensive experiments on several publicly available\nbenchmarks (i.e., HDTF, LRW, VoxCeleb1, and VoxCeleb2) to demonstrate its\nsuperiority over existing leading algorithms."}
{"id": "2504.06225", "pdf": "https://arxiv.org/pdf/2504.06225", "abs": "https://arxiv.org/abs/2504.06225", "authors": ["Biao Zhang", "Fedor Moiseev", "Joshua Ainslie", "Paul Suganthan", "Min Ma", "Surya Bhupatiraju", "Fede Lebron", "Orhan Firat", "Armand Joulin", "Zhe Dong"], "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While decoder-only large language models (LLMs) have shown impressive\nresults, encoder-decoder models are still widely adopted in real-world\napplications for their inference efficiency and richer encoder representation.\nIn this paper, we study a novel problem: adapting pretrained decoder-only LLMs\nto encoder-decoder, with the goal of leveraging the strengths of both\napproaches to achieve a more favorable quality-efficiency trade-off. We argue\nthat adaptation not only enables inheriting the capability of decoder-only LLMs\nbut also reduces the demand for computation compared to pretraining from\nscratch. We rigorously explore different pretraining objectives and parameter\ninitialization/optimization techniques. Through extensive experiments based on\nGemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to\n1.6B), we demonstrate the effectiveness of adaptation and the advantage of\nencoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs\nachieve comparable (often better) pretraining performance but substantially\nbetter finetuning performance than their decoder-only counterpart. For example,\nGemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning.\nEncoder-decoder adaptation also allows for flexible combination of\ndifferent-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B\nby $>$3\\%. The adapted encoder representation also yields better results on\nSuperGLUE. We will release our checkpoints to facilitate future research."}
{"id": "2504.05748", "pdf": "https://arxiv.org/pdf/2504.05748", "abs": "https://arxiv.org/abs/2504.05748", "authors": ["Tri Tung Nguyen Nguyen", "Quang Tien Dam", "Dinh Tuan Tran", "Joo-Ho Lee"], "title": "When Less Is More: A Sparse Facial Motion Structure For Listening Motion Learning", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Effective human behavior modeling is critical for successful human-robot\ninteraction. Current state-of-the-art approaches for predicting listening head\nbehavior during dyadic conversations employ continuous-to-discrete\nrepresentations, where continuous facial motion sequence is converted into\ndiscrete latent tokens. However, non-verbal facial motion presents unique\nchallenges owing to its temporal variance and multi-modal nature.\nState-of-the-art discrete motion token representation struggles to capture\nunderlying non-verbal facial patterns making training the listening head\ninefficient with low-fidelity generated motion. This study proposes a novel\nmethod for representing and predicting non-verbal facial motion by encoding\nlong sequences into a sparse sequence of keyframes and transition frames. By\nidentifying crucial motion steps and interpolating intermediate frames, our\nmethod preserves the temporal structure of motion while enhancing instance-wise\ndiversity during the learning process. Additionally, we apply this novel sparse\nrepresentation to the task of listening head prediction, demonstrating its\ncontribution to improving the explanation of facial motion patterns."}
{"id": "2504.06227", "pdf": "https://arxiv.org/pdf/2504.06227", "abs": "https://arxiv.org/abs/2504.06227", "authors": ["Krithi Shailya", "Shreya Rajpal", "Gokul S Krishnan", "Balaraman Ravindran"], "title": "LExT: Towards Evaluating Trustworthiness of Natural Language Explanations", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into\nhigh-stakes domains, there have been several approaches proposed toward\ngenerating natural language explanations. These explanations are crucial for\nenhancing the interpretability of a model, especially in sensitive domains like\nhealthcare, where transparency and reliability are key. In light of such\nexplanations being generated by LLMs and its known concerns, there is a growing\nneed for robust evaluation frameworks to assess model-generated explanations.\nNatural Language Generation metrics like BLEU and ROUGE capture syntactic and\nsemantic accuracies but overlook other crucial aspects such as factual\naccuracy, consistency, and faithfulness. To address this gap, we propose a\ngeneral framework for quantifying trustworthiness of natural language\nexplanations, balancing Plausibility and Faithfulness, to derive a\ncomprehensive Language Explanation Trustworthiness Score (LExT) (The code and\nset up to reproduce our experiments are publicly available at\nhttps://github.com/cerai-iitm/LExT). Applying our domain-agnostic framework to\nthe healthcare domain using public medical datasets, we evaluate six models,\nincluding domain-specific and general-purpose models. Our findings demonstrate\nsignificant differences in their ability to generate trustworthy explanations.\nOn comparing these explanations, we make interesting observations such as\ninconsistencies in Faithfulness demonstrated by general-purpose models and\ntheir tendency to outperform domain-specific fine-tuned models. This work\nfurther highlights the importance of using a tailored evaluation framework to\nassess natural language explanations in sensitive fields, providing a\nfoundation for improving the trustworthiness and transparency of language\nmodels in healthcare and beyond."}
{"id": "2504.05751", "pdf": "https://arxiv.org/pdf/2504.05751", "abs": "https://arxiv.org/abs/2504.05751", "authors": ["Jiangsan Zhao", "Jakob Geipel", "Krzysztof Kusnierek", "Xuean Cui"], "title": "InvNeRF-Seg: Fine-Tuning a Pre-Trained NeRF for 3D Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have been widely adopted for reconstructing\nhigh quality 3D point clouds from 2D RGB images. However, the segmentation of\nthese reconstructed 3D scenes is more essential for downstream tasks such as\nobject counting, size estimation, and scene understanding. While segmentation\non raw 3D point clouds using deep learning requires labor intensive and\ntime-consuming manual annotation, directly training NeRF on binary masks also\nfails due to the absence of color and shading cues essential for geometry\nlearning. We propose Invariant NeRF for Segmentation (InvNeRFSeg), a two step,\nzero change fine tuning strategy for 3D segmentation. We first train a standard\nNeRF on RGB images and then fine tune it using 2D segmentation masks without\naltering either the model architecture or loss function. This approach produces\nhigher quality, cleaner segmented point clouds directly from the refined\nradiance field with minimal computational overhead or complexity. Field density\nanalysis reveals consistent semantic refinement: densities of object regions\nincrease while background densities are suppressed, ensuring clean and\ninterpretable segmentations. We demonstrate InvNeRFSegs superior performance\nover both SA3D and FruitNeRF on both synthetic fruit and real world soybean\ndatasets. This approach effectively extends 2D segmentation to high quality 3D\nsegmentation."}
{"id": "2504.05311", "pdf": "https://arxiv.org/pdf/2504.05311", "abs": "https://arxiv.org/abs/2504.05311", "authors": ["Ylli Prifti", "Alessandro Provetti", "Pasquale de Meo"], "title": "Dr Web: a modern, query-based web data retrieval engine", "categories": ["cs.DB", "cs.CL", "cs.IR"], "comment": "10 pages, 1 figure, 1 table, 7 listings", "summary": "This article introduces the Data Retrieval Web Engine (also referred to as\ndoctor web), a flexible and modular tool for extracting structured data from\nweb pages using a simple query language. We discuss the engineering challenges\naddressed during its development, such as dynamic content handling and messy\ndata extraction. Furthermore, we cover the steps for making the DR Web Engine\npublic, highlighting its open source potential."}
{"id": "2504.05770", "pdf": "https://arxiv.org/pdf/2504.05770", "abs": "https://arxiv.org/abs/2504.05770", "authors": ["Inho Jake Park", "Jaehoon Jay Jeong", "Ho-Sang Jo"], "title": "A Lightweight Multi-Module Fusion Approach for Korean Character Recognition", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10"], "comment": "12 pages, 5 figures, 5 tables", "summary": "Optical Character Recognition (OCR) is essential in applications such as\ndocument processing, license plate recognition, and intelligent surveillance.\nHowever, existing OCR models often underperform in real-world scenarios due to\nirregular text layouts, poor image quality, character variability, and high\ncomputational costs.\n  This paper introduces SDA-Net (Stroke-Sensitive Attention and Dynamic Context\nEncoding Network), a lightweight and efficient architecture designed for robust\nsingle-character recognition. SDA-Net incorporates: (1) a Dual Attention\nMechanism to enhance stroke-level and spatial feature extraction; (2) a Dynamic\nContext Encoding module that adaptively refines semantic information using a\nlearnable gating mechanism; (3) a U-Net-inspired Feature Fusion Strategy for\ncombining low-level and high-level features; and (4) a highly optimized\nlightweight backbone that reduces memory and computational demands.\n  Experimental results show that SDA-Net achieves state-of-the-art accuracy on\nchallenging OCR benchmarks, with significantly faster inference, making it\nwell-suited for deployment in real-time and edge-based OCR systems."}
{"id": "2504.05314", "pdf": "https://arxiv.org/pdf/2504.05314", "abs": "https://arxiv.org/abs/2504.05314", "authors": ["Jianyang Zhai", "Zi-Feng Mai", "Chang-Dong Wang", "Feidiao Yang", "Xiawu Zheng", "Hui Li", "Yonghong Tian"], "title": "Multimodal Quantitative Language for Generative Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Generative recommendation has emerged as a promising paradigm aiming at\ndirectly generating the identifiers of the target candidates. Most existing\nmethods attempt to leverage prior knowledge embedded in Pre-trained Language\nModels (PLMs) to improve the recommendation performance. However, they often\nfail to accommodate the differences between the general linguistic knowledge of\nPLMs and the specific needs of recommendation systems. Moreover, they rarely\nconsider the complementary knowledge between the multimodal information of\nitems, which represents the multi-faceted preferences of users. To facilitate\nefficient recommendation knowledge transfer, we propose a novel approach called\nMultimodal Quantitative Language for Generative Recommendation (MQL4GRec). Our\nkey idea is to transform items from different domains and modalities into a\nunified language, which can serve as a bridge for transferring recommendation\nknowledge. Specifically, we first introduce quantitative translators to convert\nthe text and image content of items from various domains into a new and concise\nlanguage, known as quantitative language, with all items sharing the same\nvocabulary. Then, we design a series of quantitative language generation tasks\nto enrich quantitative language with semantic information and prior knowledge.\nFinally, we achieve the transfer of recommendation knowledge from different\ndomains and modalities to the recommendation task through pre-training and\nfine-tuning. We evaluate the effectiveness of MQL4GRec through extensive\nexperiments and comparisons with existing methods, achieving improvements over\nthe baseline by 11.18\\%, 14.82\\%, and 7.95\\% on the NDCG metric across three\ndifferent datasets, respectively."}
{"id": "2504.05774", "pdf": "https://arxiv.org/pdf/2504.05774", "abs": "https://arxiv.org/abs/2504.05774", "authors": ["Enming Zhang", "Zhengyu Li", "Yanru Wu", "Jingge Wang", "Yang Tan", "Ruizhe Zhao", "Guan Wang", "Yang Li"], "title": "Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Vision Transformers (ViTs) have set new benchmarks in\nsemantic segmentation. However, when adapting pretrained ViTs to new target\ndomains, significant performance degradation often occurs due to distribution\nshifts, resulting in suboptimal global attention. Since self-attention\nmechanisms are inherently data-driven, they may fail to effectively attend to\nkey objects when source and target domains exhibit differences in texture,\nscale, or object co-occurrence patterns. While global and patch-level domain\nadaptation methods provide partial solutions, region-level adaptation with\ndynamically shaped regions is crucial due to spatial heterogeneity in\ntransferability across different image areas. We present Transferable Mask\nTransformer (TMT), a novel region-level adaptation framework for semantic\nsegmentation that aligns cross-domain representations through spatial\ntransferability analysis. TMT consists of two key components: (1) An Adaptive\nCluster-based Transferability Estimator (ACTE) that dynamically segments images\ninto structurally and semantically coherent regions for localized\ntransferability assessment, and (2) A Transferable Masked Attention (TMA)\nmodule that integrates region-specific transferability maps into ViTs'\nattention mechanisms, prioritizing adaptation in regions with low\ntransferability and high semantic uncertainty. Comprehensive evaluations across\n20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%\nMIoU improvement over vanilla fine-tuning and a 1.28% increase compared to\nstate-of-the-art baselines. The source code will be publicly available."}
{"id": "2504.05317", "pdf": "https://arxiv.org/pdf/2504.05317", "abs": "https://arxiv.org/abs/2504.05317", "authors": ["Gorjan Radevski", "Kiril Gashteovski", "Shahbaz Syed", "Christopher Malon", "Sebastien Nicolas", "Chia-Chien Hung", "Timo Sztyler", "Verena Heußer", "Wiem Ben Rim", "Masafumi Enomoto", "Kunihiro Takeoka", "Masafumi Oyamada", "Goran Glavaš", "Carolin Lawrence"], "title": "On Synthesizing Data for Context Attribution in Question Answering", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA."}
{"id": "2504.05779", "pdf": "https://arxiv.org/pdf/2504.05779", "abs": "https://arxiv.org/abs/2504.05779", "authors": ["Tao Lin", "Qingwang Wang", "Qiwei Liang", "Minghua Tang", "Yuxuan Sun"], "title": "FASR-Net: Unsupervised Shadow Removal Leveraging Inherent Frequency Priors", "categories": ["cs.CV"], "comment": null, "summary": "Shadow removal is challenging due to the complex interaction of geometry,\nlighting, and environmental factors. Existing unsupervised methods often\noverlook shadow-specific priors, leading to incomplete shadow recovery. To\naddress this issue, we propose a novel unsupervised Frequency Aware Shadow\nRemoval Network (FASR-Net), which leverages the inherent frequency\ncharacteristics of shadow regions. Specifically, the proposed Wavelet Attention\nDownsampling Module (WADM) integrates wavelet-based image decomposition and\ndeformable attention, effectively breaking down the image into frequency\ncomponents to enhance shadow details within specific frequency bands. We also\nintroduce several new loss functions for precise shadow-free image\nreproduction: a frequency loss to capture image component details, a\nbrightness-chromaticity loss that references the chromaticity of shadow-free\nregions, and an alignment loss to ensure smooth transitions between shadowed\nand shadow-free regions. Experimental results on the AISTD and SRD datasets\ndemonstrate that our method achieves superior shadow removal performance."}
{"id": "2504.05323", "pdf": "https://arxiv.org/pdf/2504.05323", "abs": "https://arxiv.org/abs/2504.05323", "authors": ["Mingjian Fu", "Hengsheng Chen", "Dongchun Jiang", "Yanchao Tan"], "title": "Multi-Perspective Attention Mechanism for Bias-Aware Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2"], "comment": "30 pages,10 figures,4 tables", "summary": "In the era of advancing information technology, recommender systems have\nemerged as crucial tools for dealing with information overload. However,\ntraditional recommender systems still have limitations in capturing the dynamic\nevolution of user behavior. To better understand and predict user behavior,\nespecially taking into account the complexity of temporal evolution, sequential\nrecommender systems have gradually become the focus of research. Currently,\nmany sequential recommendation algorithms ignore the amplification effects of\nprevalent biases, which leads to recommendation results being susceptible to\nthe Matthew Effect. Additionally, it will impose limitations on the recommender\nsystem's ability to deeply perceive and capture the dynamic shifts in user\npreferences, thereby diminishing the extent of its recommendation reach. To\naddress this issue effectively, we propose a recommendation system based on\nsequential information and attention mechanism called Multi-Perspective\nAttention Bias Sequential Recommendation (MABSRec). Firstly, we reconstruct\nuser sequences into three short types and utilize graph neural networks for\nitem weighting. Subsequently, an adaptive multi-bias perspective attention\nmodule is proposed to enhance the accuracy of recommendations. Experimental\nresults show that the MABSRec model exhibits significant advantages in all\nevaluation metrics, demonstrating its excellent performance in the sequence\nrecommendation task."}
{"id": "2504.05782", "pdf": "https://arxiv.org/pdf/2504.05782", "abs": "https://arxiv.org/abs/2504.05782", "authors": ["Pengfei Zhou", "Fanrui Zhang", "Xiaopeng Peng", "Zhaopan Xu", "Jiaxin Ai", "Yansheng Qiu", "Chuanhao Li", "Zhen Li", "Ming Li", "Yukang Feng", "Jianwen Sun", "Haoquan Zhang", "Zizhen Li", "Xiaofeng Mao", "Wangbo Zhao", "Kai Wang", "Xiaojun Chang", "Wenqi Shao", "Yang You", "Kaipeng Zhang"], "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 8 figures", "summary": "Multimodal reasoning, which integrates language and visual cues into problem\nsolving and decision making, is a fundamental aspect of human intelligence and\na crucial step toward artificial general intelligence. However, the evaluation\nof multimodal reasoning capabilities in Multimodal Large Language Models\n(MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained\nby limited data size, narrow domain coverage, and unstructured knowledge\ndistribution. To close these gaps, we introduce MDK12-Bench, a\nmulti-disciplinary benchmark assessing the reasoning capabilities of MLLMs via\nreal-world K-12 examinations. Spanning six disciplines (math, physics,\nchemistry, biology, geography, and information science), our benchmark\ncomprises 140K reasoning instances across diverse difficulty levels from\nprimary school to 12th grade. It features 6,827 instance-level knowledge point\nannotations based on a well-organized knowledge structure, detailed answer\nexplanations, difficulty labels and cross-year partitions, providing a robust\nplatform for comprehensive evaluation. Additionally, we present a novel dynamic\nevaluation framework to mitigate data contamination issues by bootstrapping\nquestion forms, question types, and image styles during evaluation. Extensive\nexperiment on MDK12-Bench reveals the significant limitation of current MLLMs\nin multimodal reasoning. The findings on our benchmark provide insights into\nthe development of the next-generation models. Our data and codes are available\nat https://github.com/LanceZPF/MDK12."}
{"id": "2504.05324", "pdf": "https://arxiv.org/pdf/2504.05324", "abs": "https://arxiv.org/abs/2504.05324", "authors": ["Chandana Sree Mala", "Gizem Gezici", "Fosca Giannotti"], "title": "Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in language comprehension and generation\nbut are prone to hallucinations, producing factually incorrect or unsupported\noutputs. Retrieval Augmented Generation (RAG) systems address this issue by\ngrounding LLM responses with external knowledge. This study evaluates the\nrelationship between retriever effectiveness and hallucination reduction in\nLLMs using three retrieval approaches: sparse retrieval based on BM25 keyword\nsearch, dense retrieval using semantic search with Sentence Transformers, and a\nproposed hybrid retrieval module. The hybrid module incorporates query\nexpansion and combines the results of sparse and dense retrievers through a\ndynamically weighted Reciprocal Rank Fusion score. Using the HaluBench dataset,\na benchmark for hallucinations in question answering tasks, we assess retrieval\nperformance with metrics such as mean average precision and normalised\ndiscounted cumulative gain, focusing on the relevance of the top three\nretrieved documents. Results show that the hybrid retriever achieves better\nrelevance scores, outperforming both sparse and dense retrievers. Further\nevaluation of LLM-generated answers against ground truth using metrics such as\naccuracy, hallucination rate, and rejection rate reveals that the hybrid\nretriever achieves the highest accuracy on fails, the lowest hallucination\nrate, and the lowest rejection rate. These findings highlight the hybrid\nretriever's ability to enhance retrieval relevance, reduce hallucination rates,\nand improve LLM reliability, emphasising the importance of advanced retrieval\ntechniques in mitigating hallucinations and improving response accuracy."}
{"id": "2504.05783", "pdf": "https://arxiv.org/pdf/2504.05783", "abs": "https://arxiv.org/abs/2504.05783", "authors": ["Zijie Song", "Zhenzhen Hu", "Yixiao Ma", "Jia Li", "Richang Hong"], "title": "Video Flow as Time Series: Discovering Temporal Consistency and Variability for VideoQA", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Question Answering (VideoQA) is a complex video-language task that\ndemands a sophisticated understanding of both visual content and temporal\ndynamics. Traditional Transformer-style architectures, while effective in\nintegrating multimodal data, often simplify temporal dynamics through\npositional encoding and fail to capture non-linear interactions within video\nsequences. In this paper, we introduce the Temporal Trio Transformer (T3T), a\nnovel architecture that models time consistency and time variability. The T3T\nintegrates three key components: Temporal Smoothing (TS), Temporal Difference\n(TD), and Temporal Fusion (TF). The TS module employs Brownian Bridge for\ncapturing smooth, continuous temporal transitions, while the TD module\nidentifies and encodes significant temporal variations and abrupt changes\nwithin the video content. Subsequently, the TF module synthesizes these\ntemporal features with textual cues, facilitating a deeper contextual\nunderstanding and response accuracy. The efficacy of the T3T is demonstrated\nthrough extensive testing on multiple VideoQA benchmark datasets. Our results\nunderscore the importance of a nuanced approach to temporal modeling in\nimproving the accuracy and depth of video-based question answering."}
{"id": "2504.05346", "pdf": "https://arxiv.org/pdf/2504.05346", "abs": "https://arxiv.org/abs/2504.05346", "authors": ["Ivan Ilin", "Peter Richtarik"], "title": "Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PF", "68T07, 68Q32"], "comment": "8 pages, 3 Figures, 3 Tables, 2 Algorithms, paper comes with Appendix", "summary": "This paper presents Thanos, a novel weight-pruning algorithm designed to\nreduce the memory footprint and enhance the computational efficiency of large\nlanguage models (LLMs) by removing redundant weights while maintaining\naccuracy. Thanos introduces a block-wise pruning strategy with adaptive masks\nthat dynamically adjust to weight importance, enabling flexible sparsity\npatterns and structured formats, such as $n:m$ sparsity, optimized for hardware\nacceleration. Experimental evaluations demonstrate that Thanos achieves\nstate-of-the-art performance in structured pruning and outperforms existing\nmethods in unstructured pruning. By providing an efficient and adaptable\napproach to model compression, Thanos offers a practical solution for deploying\nlarge models in resource-constrained environments."}
{"id": "2504.05786", "pdf": "https://arxiv.org/pdf/2504.05786", "abs": "https://arxiv.org/abs/2504.05786", "authors": ["Jirong Zha", "Yuxuan Fan", "Xiao Yang", "Chen Gao", "Xinlei Chen"], "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 5 figures", "summary": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications."}
{"id": "2504.05419", "pdf": "https://arxiv.org/pdf/2504.05419", "abs": "https://arxiv.org/abs/2504.05419", "authors": ["Anqi Zhang", "Yulin Chen", "Jane Pan", "Chen Zhao", "Aurojit Panda", "Jinyang Li", "He He"], "title": "Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning models have achieved remarkable performance on tasks like math and\nlogical reasoning thanks to their ability to search during reasoning. However,\nthey still suffer from overthinking, often performing unnecessary reasoning\nsteps even after reaching the correct answer. This raises the question: can\nmodels evaluate the correctness of their intermediate answers during reasoning?\nIn this work, we study whether reasoning models encode information about answer\ncorrectness through probing the model's hidden states. The resulting probe can\nverify intermediate answers with high accuracy and produces highly calibrated\nscores. Additionally, we find models' hidden states encode correctness of\nfuture answers, enabling early prediction of the correctness before the\nintermediate answer is fully formulated. We then use the probe as a verifier to\ndecide whether to exit reasoning at intermediate answers during inference,\nreducing the number of inference tokens by 24\\% without compromising\nperformance. These findings confirm that reasoning models do encode a notion of\ncorrectness yet fail to exploit it, revealing substantial untapped potential to\nenhance their efficiency."}
{"id": "2504.05789", "pdf": "https://arxiv.org/pdf/2504.05789", "abs": "https://arxiv.org/abs/2504.05789", "authors": ["Sarosij Bose", "Hannah Dela Cruz", "Arindam Dutta", "Elena Kokkoni", "Konstantinos Karydis", "Amit K. Roy-Chowdhury"], "title": "Leveraging Synthetic Adult Datasets for Unsupervised Infant Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at ABAW@CVPR 2025", "summary": "Human pose estimation is a critical tool across a variety of healthcare\napplications. Despite significant progress in pose estimation algorithms\ntargeting adults, such developments for infants remain limited. Existing\nalgorithms for infant pose estimation, despite achieving commendable\nperformance, depend on fully supervised approaches that require large amounts\nof labeled data. These algorithms also struggle with poor generalizability\nunder distribution shifts. To address these challenges, we introduce SHIFT:\nLeveraging SyntHetic Adult Datasets for Unsupervised InFanT Pose Estimation,\nwhich leverages the pseudo-labeling-based Mean-Teacher framework to compensate\nfor the lack of labeled data and addresses distribution shifts by enforcing\nconsistency between the student and the teacher pseudo-labels. Additionally, to\npenalize implausible predictions obtained from the mean-teacher framework, we\nincorporate an infant manifold pose prior. To enhance SHIFT's self-occlusion\nperception ability, we propose a novel visibility consistency module for\nimproved alignment of the predicted poses with the original image. Extensive\nexperiments on multiple benchmarks show that SHIFT significantly outperforms\nexisting state-of-the-art unsupervised domain adaptation (UDA) pose estimation\nmethods by 5% and supervised infant pose estimation methods by a margin of 16%.\nThe project page is available at: https://sarosijbose.github.io/SHIFT."}
{"id": "2504.05478", "pdf": "https://arxiv.org/pdf/2504.05478", "abs": "https://arxiv.org/abs/2504.05478", "authors": ["Alfred Clemedtson", "Borun Shi"], "title": "GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases", "categories": ["cs.LG", "cs.CL", "cs.IR"], "comment": null, "summary": "Large language models have shown remarkable language processing and reasoning\nability but are prone to hallucinate when asked about private data.\nRetrieval-augmented generation (RAG) retrieves relevant data that fit into an\nLLM's context window and prompts the LLM for an answer. GraphRAG extends this\napproach to structured Knowledge Graphs (KGs) and questions regarding entities\nmultiple hops away. The majority of recent GraphRAG methods either overlook the\nretrieval step or have ad hoc retrieval processes that are abstract or\ninefficient. This prevents them from being adopted when the KGs are stored in\ngraph databases supporting graph query languages. In this work, we present\nGraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate\nprovably correct Cypher queries to retrieve high-quality subgraph contexts and\nproduce accurate answers. Our method is the first such solution that can be\ntaken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks\nsuggest that our method is sample-efficient and scales with the availability of\ntraining data. Our method achieves significantly better results than all\nstate-of-the-art models across all four standard metrics on two challenging\nQ\\&As on large text-attributed KGs."}
{"id": "2504.05794", "pdf": "https://arxiv.org/pdf/2504.05794", "abs": "https://arxiv.org/abs/2504.05794", "authors": ["Leiye Liu", "Miao Zhang", "Jihao Yin", "Tingwei Liu", "Wei Ji", "Yongri Piao", "Huchuan Lu"], "title": "DefMamba: Deformable Visual State Space Model", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Recently, state space models (SSM), particularly Mamba, have attracted\nsignificant attention from scholars due to their ability to effectively balance\ncomputational efficiency and performance. However, most existing visual Mamba\nmethods flatten images into 1D sequences using predefined scan orders, which\nresults the model being less capable of utilizing the spatial structural\ninformation of the image during the feature extraction process. To address this\nissue, we proposed a novel visual foundation model called DefMamba. This model\nincludes a multi-scale backbone structure and deformable mamba (DM) blocks,\nwhich dynamically adjust the scanning path to prioritize important information,\nthus enhancing the capture and processing of relevant input features. By\ncombining a deformable scanning(DS) strategy, this model significantly improves\nits ability to learn image structures and detects changes in object details.\nNumerous experiments have shown that DefMamba achieves state-of-the-art\nperformance in various visual tasks, including image classification, object\ndetection, instance segmentation, and semantic segmentation. The code is open\nsource on DefMamba."}
{"id": "2504.05518", "pdf": "https://arxiv.org/pdf/2504.05518", "abs": "https://arxiv.org/abs/2504.05518", "authors": ["Rem Yang", "Julian Dai", "Nikos Vasilakis", "Martin Rinard"], "title": "Evaluating the Generalization Capabilities of Large Language Models on Code Reasoning", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "We assess how the code reasoning abilities of large language models (LLMs)\ngeneralize to different kinds of programs. We present techniques for obtaining\nin- and out-of-distribution programs with different characteristics: code\nsampled from a domain-specific language, code automatically generated by an\nLLM, code collected from competitive programming contests, and mutated versions\nof these programs. We also present an experimental methodology for evaluating\nLLM generalization by comparing their performance on these programs. We perform\nan extensive evaluation across 10 state-of-the-art models from the past year,\nobtaining insights into their generalization capabilities over time and across\ndifferent classes of programs. Our results highlight that while earlier models\nexhibit behavior consistent with pattern matching, the latest models exhibit\nstrong generalization abilities on code reasoning."}
{"id": "2504.05795", "pdf": "https://arxiv.org/pdf/2504.05795", "abs": "https://arxiv.org/abs/2504.05795", "authors": ["Hao Zhang", "Yanping Zha", "Qingwei Zhuang", "Zhenfeng Shao", "Jiayi Ma"], "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions", "categories": ["cs.CV"], "comment": null, "summary": "Current image fusion methods struggle to adapt to real-world environments\nencompassing diverse degradations with spatially varying characteristics. To\naddress this challenge, we propose a robust fusion controller (RFC) capable of\nachieving degradation-aware image fusion through fine-grained language\ninstructions, ensuring its reliable application in adverse environments.\nSpecifically, RFC first parses language instructions to innovatively derive the\nfunctional condition and the spatial condition, where the former specifies the\ndegradation type to remove, while the latter defines its spatial coverage.\nThen, a composite control priori is generated through a multi-condition\ncoupling network, achieving a seamless transition from abstract language\ninstructions to latent control variables. Subsequently, we design a hybrid\nattention-based fusion network to aggregate multi-modal information, in which\nthe obtained composite control priori is deeply embedded to linearly modulate\nthe intermediate fused features. To ensure the alignment between language\ninstructions and control outcomes, we introduce a novel language-feature\nalignment loss, which constrains the consistency between feature-level gains\nand the composite control priori. Extensive experiments on publicly available\ndatasets demonstrate that our RFC is robust against various composite\ndegradations, particularly in highly challenging flare scenarios."}
{"id": "2504.05520", "pdf": "https://arxiv.org/pdf/2504.05520", "abs": "https://arxiv.org/abs/2504.05520", "authors": ["Taiwei Shi", "Yiyang Wu", "Linxin Song", "Tianyi Zhou", "Jieyu Zhao"], "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning", "categories": ["cs.LG", "cs.CL"], "comment": "18 pages, 4 figures, 2 tables", "summary": "Reinforcement finetuning (RFT) has shown great potential for enhancing the\nmathematical reasoning capabilities of large language models (LLMs), but it is\noften sample- and compute-inefficient, requiring extensive training. In this\nwork, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a\nmethod that significantly improves both the efficiency and final accuracy of\nRFT through adaptive curriculum learning. AdaRFT dynamically adjusts the\ndifficulty of training problems based on the model's recent reward signals,\nensuring that the model consistently trains on tasks that are challenging but\nsolvable. This adaptive sampling strategy accelerates learning by maintaining\nan optimal difficulty range, avoiding wasted computation on problems that are\ntoo easy or too hard. AdaRFT requires only a lightweight extension to standard\nRFT algorithms like Proximal Policy Optimization (PPO), without modifying the\nreward function or model architecture. Experiments on competition-level math\ndatasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT\nsignificantly improves both training efficiency and reasoning performance. We\nevaluate AdaRFT across multiple data distributions and model sizes, showing\nthat it reduces the number of training steps by up to 2x and improves accuracy\nby a considerable margin, offering a more scalable and effective RFT framework."}
{"id": "2504.05800", "pdf": "https://arxiv.org/pdf/2504.05800", "abs": "https://arxiv.org/abs/2504.05800", "authors": ["Jaskirat Singh", "Junshen Kevin Chen", "Jonas Kohler", "Michael Cohen"], "title": "Storybooth: Training-free Multi-Subject Consistency for Improved Visual Storytelling", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Training-free consistent text-to-image generation depicting the same subjects\nacross different images is a topic of widespread recent interest. Existing\nworks in this direction predominantly rely on cross-frame self-attention; which\nimproves subject-consistency by allowing tokens in each frame to pay attention\nto tokens in other frames during self-attention computation. While useful for\nsingle subjects, we find that it struggles when scaling to multiple characters.\nIn this work, we first analyze the reason for these limitations. Our\nexploration reveals that the primary-issue stems from self-attention-leakage,\nwhich is exacerbated when trying to ensure consistency across\nmultiple-characters. This happens when tokens from one subject pay attention to\nother characters, causing them to appear like each other (e.g., a dog appearing\nlike a duck). Motivated by these findings, we propose StoryBooth: a\ntraining-free approach for improving multi-character consistency. In\nparticular, we first leverage multi-modal chain-of-thought reasoning and\nregion-based generation to apriori localize the different subjects across the\ndesired story outputs. The final outputs are then generated using a modified\ndiffusion model which consists of two novel layers: 1) a bounded cross-frame\nself-attention layer for reducing inter-character attention leakage, and 2)\ntoken-merging layer for improving consistency of fine-grain subject details.\nThrough both qualitative and quantitative results we find that the proposed\napproach surpasses prior state-of-the-art, exhibiting improved consistency\nacross both multiple-characters and fine-grain subject details."}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility."}
{"id": "2504.05808", "pdf": "https://arxiv.org/pdf/2504.05808", "abs": "https://arxiv.org/abs/2504.05808", "authors": ["Pawel Tomasz Pieta", "Peter Winkel Rasumssen", "Anders Bjorholm Dahl", "Anders Nymark Christensen"], "title": "Fast Sphericity and Roundness approximation in 2D and 3D using Local Thickness", "categories": ["cs.CV"], "comment": "Accepted at CVMI (CVPR 2025 Workshop)", "summary": "Sphericity and roundness are fundamental measures used for assessing object\nuniformity in 2D and 3D images. However, using their strict definition makes\ncomputation costly. As both 2D and 3D microscopy imaging datasets grow larger,\nthere is an increased demand for efficient algorithms that can quantify\nmultiple objects in large volumes. We propose a novel approach for extracting\nsphericity and roundness based on the output of a local thickness algorithm.\nFor sphericity, we simplify the surface area computation by modeling objects as\nspheroids/ellipses of varying lengths and widths of mean local thickness. For\nroundness, we avoid a complex corner curvature determination process by\napproximating it with local thickness values on the contour/surface of the\nobject. The resulting methods provide an accurate representation of the exact\nmeasures while being significantly faster than their existing implementations."}
{"id": "2504.05605", "pdf": "https://arxiv.org/pdf/2504.05605", "abs": "https://arxiv.org/abs/2504.05605", "authors": ["Gejian Zhao", "Hanzhou Wu", "Xinpeng Zhang", "Athanasios V. Vasilakos"], "title": "ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": "Zhao et al., 16 pages, 2025, uploaded by Hanzhou Wu, Shanghai\n  University", "summary": "Chain-of-Thought (CoT) enhances an LLM's ability to perform complex reasoning\ntasks, but it also introduces new security issues. In this work, we present\nShadowCoT, a novel backdoor attack framework that targets the internal\nreasoning mechanism of LLMs. Unlike prior token-level or prompt-based attacks,\nShadowCoT directly manipulates the model's cognitive reasoning path, enabling\nit to hijack multi-step reasoning chains and produce logically coherent but\nadversarial outcomes. By conditioning on internal reasoning states, ShadowCoT\nlearns to recognize and selectively disrupt key reasoning steps, effectively\nmounting a self-reflective cognitive attack within the target model. Our\napproach introduces a lightweight yet effective multi-stage injection pipeline,\nwhich selectively rewires attention pathways and perturbs intermediate\nrepresentations with minimal parameter overhead (only 0.15% updated). ShadowCoT\nfurther leverages reinforcement learning and reasoning chain pollution (RCP) to\nautonomously synthesize stealthy adversarial CoTs that remain undetectable to\nadvanced defenses. Extensive experiments across diverse reasoning benchmarks\nand LLMs show that ShadowCoT consistently achieves high Attack Success Rate\n(94.4%) and Hijacking Success Rate (88.4%) while preserving benign performance.\nThese results reveal an emergent class of cognition-level threats and highlight\nthe urgent need for defenses beyond shallow surface-level consistency."}
{"id": "2504.05810", "pdf": "https://arxiv.org/pdf/2504.05810", "abs": "https://arxiv.org/abs/2504.05810", "authors": ["Xinpeng Ding", "Kui Zhang", "Jinahua Han", "Lanqing Hong", "Hang Xu", "Xiaomeng Li"], "title": "PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware Multi-Instance Video Preference Learning", "categories": ["cs.CV"], "comment": null, "summary": "Direct Preference Optimization (DPO) helps reduce hallucinations in Video\nMultimodal Large Language Models (VLLMs), but its reliance on offline\npreference data limits adaptability and fails to capture true video-response\nmisalignment. We propose Video Direct Preference Optimization (VDPO), an online\npreference learning framework that eliminates the need for preference\nannotation by leveraging video augmentations to generate rejected samples while\nkeeping responses fixed. However, selecting effective augmentations is\nnon-trivial, as some clips may be semantically identical to the original under\nspecific prompts, leading to false rejections and disrupting alignment. To\naddress this, we introduce Prompt-aware Multi-instance Learning VDPO\n(PaMi-VDPO), which selects augmentations based on prompt context. Instead of a\nsingle rejection, we construct a candidate set of augmented clips and apply a\nclose-to-far selection strategy, initially ensuring all clips are semantically\nrelevant while then prioritizing the most prompt-aware distinct clip. This\nallows the model to better capture meaningful visual differences, mitigating\nhallucinations, while avoiding false rejections, and improving alignment.\nPaMi-VDPOseamlessly integrates into existing VLLMs without additional\nparameters, GPT-4/human supervision. With only 10k SFT data, it improves the\nbase model by 5.3% on VideoHallucer, surpassing GPT-4o, while maintaining\nstable performance on general video benchmarks."}
{"id": "2504.05652", "pdf": "https://arxiv.org/pdf/2504.05652", "abs": "https://arxiv.org/abs/2504.05652", "authors": ["Yu-Hang Wu", "Yu-Jie Xiong", "Jie-Zhang"], "title": "Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become increasingly integral to a wide\nrange of applications. However, they still remain the threat of jailbreak\nattacks, where attackers manipulate designed prompts to make the models elicit\nmalicious outputs. Analyzing jailbreak methods can help us delve into the\nweakness of LLMs and improve it. In this paper, We reveal a vulnerability in\nlarge language models (LLMs), which we term Defense Threshold Decay (DTD), by\nanalyzing the attention weights of the model's output on input and subsequent\noutput on prior output: as the model generates substantial benign content, its\nattention weights shift from the input to prior output, making it more\nsusceptible to jailbreak attacks. To demonstrate the exploitability of DTD, we\npropose a novel jailbreak attack method, Sugar-Coated Poison (SCP), which\ninduces the model to generate substantial benign content through benign input\nand adversarial reasoning, subsequently producing malicious content. To\nmitigate such attacks, we introduce a simple yet effective defense strategy,\nPOSD, which significantly reduces jailbreak success rates while preserving the\nmodel's generalization capabilities."}
{"id": "2504.05815", "pdf": "https://arxiv.org/pdf/2504.05815", "abs": "https://arxiv.org/abs/2504.05815", "authors": ["Jiahao Chen", "Yu Pan", "Yi Du", "Chunkai Wu", "Lin Wang"], "title": "Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, the diffusion model has gained significant attention as one of the\nmost successful image generation models, which can generate high-quality images\nby iteratively sampling noise. However, recent studies have shown that\ndiffusion models are vulnerable to backdoor attacks, allowing attackers to\nenter input data containing triggers to activate the backdoor and generate\ntheir desired output. Existing backdoor attack methods primarily focused on\ntarget noise-to-image and text-to-image tasks, with limited work on backdoor\nattacks in image-to-image tasks. Furthermore, traditional backdoor attacks\noften rely on a single, conspicuous trigger to generate a fixed target image,\nlacking concealability and flexibility. To address these limitations, we\npropose a novel backdoor attack method called \"Parasite\" for image-to-image\ntasks in diffusion models, which not only is the first to leverage\nsteganography for triggers hiding, but also allows attackers to embed the\ntarget content as a backdoor trigger to achieve a more flexible attack.\n\"Parasite\" as a novel attack method effectively bypasses existing detection\nframeworks to execute backdoor attacks. In our experiments, \"Parasite\" achieved\na 0 percent backdoor detection rate against the mainstream defense frameworks.\nIn addition, in the ablation study, we discuss the influence of different\nhiding coefficients on the attack results. You can find our code at\nhttps://anonymous.4open.science/r/Parasite-1715/."}
{"id": "2504.05731", "pdf": "https://arxiv.org/pdf/2504.05731", "abs": "https://arxiv.org/abs/2504.05731", "authors": ["Teng Shi", "Jun Xu", "Xiao Zhang", "Xiaoxue Zang", "Kai Zheng", "Yang Song", "Han Li"], "title": "Retrieval Augmented Generation with Collaborative Filtering for Personalized Text Generation", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Recently, the personalization of Large Language Models (LLMs) to generate\ncontent that aligns with individual user preferences has garnered widespread\nattention. Personalized Retrieval-Augmented Generation (RAG), which retrieves\nrelevant documents from the user's history to reflect their preferences and\nenhance LLM generation, is one commonly used approach for personalization.\nHowever, existing personalized RAG methods do not consider that the histories\nof similar users can also assist in personalized generation for the current\nuser, meaning that collaborative information between users can also benefit\npersonalized generation. Inspired by the application of collaborative filtering\nin recommender systems, we propose a method called CFRAG, which adapts\nCollaborative Filtering to RAG for personalized text generation. However, this\npresents two challenges: (1)~how to incorporate collaborative information\nwithout explicit user similarity labels? (2)~how to retrieve documents that\nsupport personalized LLM generation? For Challenge 1, we use contrastive\nlearning to train user embeddings to retrieve similar users and introduce\ncollaborative information. For Challenge 2, we design a personalized retriever\nand reranker to retrieve the top-$k$ documents from these users' histories. We\ntake into account the user's preference during retrieval and reranking. Then we\nleverage feedback from the LLM to fine-tune the personalized retriever and\nreranker, enabling them to retrieve documents that meet the personalized\ngeneration needs of the LLM. Experimental results on the Language Model\nPersonalization (LaMP) benchmark validate the effectiveness of CFRAG. Further\nanalysis confirms the importance of incorporating collaborative information."}
{"id": "2504.05830", "pdf": "https://arxiv.org/pdf/2504.05830", "abs": "https://arxiv.org/abs/2504.05830", "authors": ["Shiao Wang", "Xiao Wang", "Bo Jiang", "Lin Zhu", "Guoqi Li", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "Human Activity Recognition using RGB-Event based Sensors: A Multi-modal Heat Conduction Model and A Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Journal Extension of HARDVS (AAAI 2024)", "summary": "Human Activity Recognition (HAR) primarily relied on traditional RGB cameras\nto achieve high-performance activity recognition. However, the challenging\nfactors in real-world scenarios, such as insufficient lighting and rapid\nmovements, inevitably degrade the performance of RGB cameras. To address these\nchallenges, biologically inspired event cameras offer a promising solution to\novercome the limitations of traditional RGB cameras. In this work, we rethink\nhuman activity recognition by combining the RGB and event cameras. The first\ncontribution is the proposed large-scale multi-modal RGB-Event human activity\nrecognition benchmark dataset, termed HARDVS 2.0, which bridges the dataset\ngaps. It contains 300 categories of everyday real-world actions with a total of\n107,646 paired videos covering various challenging scenarios. Inspired by the\nphysics-informed heat conduction model, we propose a novel multi-modal heat\nconduction operation framework for effective activity recognition, termed\nMMHCO-HAR. More in detail, given the RGB frames and event streams, we first\nextract the feature embeddings using a stem network. Then, multi-modal Heat\nConduction blocks are designed to fuse the dual features, the key module of\nwhich is the multi-modal Heat Conduction Operation layer. We integrate RGB and\nevent embeddings through a multi-modal DCT-IDCT layer while adaptively\nincorporating the thermal conductivity coefficient via FVEs into this module.\nAfter that, we propose an adaptive fusion module based on a policy routing\nstrategy for high-performance classification. Comprehensive experiments\ndemonstrate that our method consistently performs well, validating its\neffectiveness and robustness. The source code and benchmark dataset will be\nreleased on https://github.com/Event-AHU/HARDVS/tree/HARDVSv2"}
{"id": "2504.05862", "pdf": "https://arxiv.org/pdf/2504.05862", "abs": "https://arxiv.org/abs/2504.05862", "authors": ["Takehiro Takayanagi", "Kiyoshi Izumi", "Javier Sanz-Cruzado", "Richard McCreadie", "Iadh Ounis"], "title": "Are Generative AI Agents Effective Personalized Financial Advisors?", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "q-fin.CP"], "comment": null, "summary": "Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice."}
{"id": "2504.05838", "pdf": "https://arxiv.org/pdf/2504.05838", "abs": "https://arxiv.org/abs/2504.05838", "authors": ["Junxi Chen", "Junhao Dong", "Xiaohua Xie"], "title": "Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Accepted by CVPR2025 as Highlight", "summary": "Recently, the Image Prompt Adapter (IP-Adapter) has been increasingly\nintegrated into text-to-image diffusion models (T2I-DMs) to improve\ncontrollability. However, in this paper, we reveal that T2I-DMs equipped with\nthe IP-Adapter (T2I-IP-DMs) enable a new jailbreak attack named the hijacking\nattack. We demonstrate that, by uploading imperceptible image-space adversarial\nexamples (AEs), the adversary can hijack massive benign users to jailbreak an\nImage Generation Service (IGS) driven by T2I-IP-DMs and mislead the public to\ndiscredit the service provider. Worse still, the IP-Adapter's dependency on\nopen-source image encoders reduces the knowledge required to craft AEs.\nExtensive experiments verify the technical feasibility of the hijacking attack.\nIn light of the revealed threat, we investigate several existing defenses and\nexplore combining the IP-Adapter with adversarially trained models to overcome\nexisting defenses' limitations. Our code is available at\nhttps://github.com/fhdnskfbeuv/attackIPA."}
{"id": "2504.05902", "pdf": "https://arxiv.org/pdf/2504.05902", "abs": "https://arxiv.org/abs/2504.05902", "authors": ["Weijun Li", "Ansh Arora", "Xuanli He", "Mark Dras", "Qiongkai Xu"], "title": "Defending Deep Neural Networks against Backdoor Attacks via Module Switching", "categories": ["cs.CR", "cs.CL", "I.2.7; I.2.10"], "comment": "20 pages, 12 figures", "summary": "The exponential increase in the parameters of Deep Neural Networks (DNNs) has\nsignificantly raised the cost of independent training, particularly for\nresource-constrained entities. As a result, there is a growing reliance on\nopen-source models. However, the opacity of training processes exacerbates\nsecurity risks, making these models more vulnerable to malicious threats, such\nas backdoor attacks, while simultaneously complicating defense mechanisms.\nMerging homogeneous models has gained attention as a cost-effective\npost-training defense. However, we notice that existing strategies, such as\nweight averaging, only partially mitigate the influence of poisoned parameters\nand remain ineffective in disrupting the pervasive spurious correlations\nembedded across model parameters. We propose a novel module-switching strategy\nto break such spurious correlations within the model's propagation path. By\nleveraging evolutionary algorithms to optimize fusion strategies, we validate\nour approach against backdoor attacks targeting text and vision domains. Our\nmethod achieves effective backdoor mitigation even when incorporating a couple\nof compromised models, e.g., reducing the average attack success rate (ASR) to\n22% compared to 31.9% with the best-performing baseline on SST-2."}
{"id": "2504.05849", "pdf": "https://arxiv.org/pdf/2504.05849", "abs": "https://arxiv.org/abs/2504.05849", "authors": ["Julian Lorenz", "Katja Ludwig", "Valentin Haug", "Rainer Lienhart"], "title": "On the Importance of Conditioning for Privacy-Preserving Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models can be used as a powerful augmentation method to\nartificially extend datasets for enhanced training. To the human eye, these\naugmented images look very different to the originals. Previous work has\nsuggested to use this data augmentation technique for data anonymization.\nHowever, we show that latent diffusion models that are conditioned on features\nlike depth maps or edges to guide the diffusion process are not suitable as a\nprivacy preserving method. We use a contrastive learning approach to train a\nmodel that can correctly identify people out of a pool of candidates. Moreover,\nwe demonstrate that anonymization using conditioned diffusion models is\nsusceptible to black box attacks. We attribute the success of the described\nmethods to the conditioning of the latent diffusion model in the anonymization\nprocess. The diffusion model is instructed to produce similar edges for the\nanonymized images. Hence, a model can learn to recognize these patterns for\nidentification."}
{"id": "2504.06188", "pdf": "https://arxiv.org/pdf/2504.06188", "abs": "https://arxiv.org/abs/2504.06188", "authors": ["Pagkratios Tagkopoulos", "Fangzhou Li", "Ilias Tagkopoulos"], "title": "SkillFlow: Efficient Skill and Code Transfer Through Communication in Adapting AI Agents", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "AI agents are autonomous systems that can execute specific tasks based on\npredefined programming. Here, we present SkillFlow, a modular,\ntechnology-agnostic framework that allows agents to expand their functionality\nin an ad-hoc fashion by acquiring new skills from their environment or other\nagents. We present a theoretical model that examines under which conditions\nthis framework would be beneficial, and we then explore SkillFlow's ability to\naccelerate task completion and lead to lower cumulative costs in a real-world\napplication, namely scheduling agents for calendar events. We demonstrate that\nwithin a few iterations, SkillFlow leads to considerable (24.8%, p-value =\n$6.4\\times10^{-3}$) gains in time and cost, especially when the communication\ncost is high. Finally, we draw analogies from well-studied biological systems\nand compare this framework to that of lateral gene transfer, a significant\nprocess of adaptation and evolution in novel environments."}
{"id": "2504.05882", "pdf": "https://arxiv.org/pdf/2504.05882", "abs": "https://arxiv.org/abs/2504.05882", "authors": ["Luca Barco", "Giacomo Blanco", "Gaetano Chiriaco", "Alessia Intini", "Luigi La Riccia", "Vittorio Scolamiero", "Piero Boccardo", "Paolo Garza", "Fabrizio Dominici"], "title": "Turin3D: Evaluating Adaptation Strategies under Label Scarcity in Urban LiDAR Segmentation with Semi-Supervised Techniques", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW2025 - USM3D", "summary": "3D semantic segmentation plays a critical role in urban modelling, enabling\ndetailed understanding and mapping of city environments. In this paper, we\nintroduce Turin3D: a new aerial LiDAR dataset for point cloud semantic\nsegmentation covering an area of around 1.43 km2 in the city centre of Turin\nwith almost 70M points. We describe the data collection process and compare\nTurin3D with others previously proposed in the literature. We did not fully\nannotate the dataset due to the complexity and time-consuming nature of the\nprocess; however, a manual annotation process was performed on the validation\nand test sets, to enable a reliable evaluation of the proposed techniques. We\nfirst benchmark the performances of several point cloud semantic segmentation\nmodels, trained on the existing datasets, when tested on Turin3D, and then\nimprove their performances by applying a semi-supervised learning technique\nleveraging the unlabelled training set. The dataset will be publicly available\nto support research in outdoor point cloud segmentation, with particular\nrelevance for self-supervised and semi-supervised learning approaches given the\nabsence of ground truth annotations for the training set."}
{"id": "2504.06196", "pdf": "https://arxiv.org/pdf/2504.06196", "abs": "https://arxiv.org/abs/2504.06196", "authors": ["Eric Wang", "Samuel Schmidgall", "Paul F. Jaeger", "Fan Zhang", "Rory Pilgrim", "Yossi Matias", "Joelle Barral", "David Fleet", "Shekoofeh Azizi"], "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high)."}
{"id": "2504.05904", "pdf": "https://arxiv.org/pdf/2504.05904", "abs": "https://arxiv.org/abs/2504.05904", "authors": ["Xiangyu Zheng", "Wanyun Li", "Songcheng He", "Xiaoqiang Li", "We Zhang"], "title": "Intrinsic Saliency Guided Trunk-Collateral Network for Unsupervised Video Object Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent unsupervised video object segmentation (UVOS) methods predominantly\nadopt the motion-appearance paradigm. Mainstream motion-appearance approaches\nuse either the two-encoder structure to separately encode motion and appearance\nfeatures, or the single-encoder structure for joint encoding. However, these\nmethods fail to properly balance the motion-appearance relationship.\nConsequently, even with complex fusion modules for motion-appearance\nintegration, the extracted suboptimal features degrade the models' overall\nperformance. Moreover, the quality of optical flow varies across scenarios,\nmaking it insufficient to rely solely on optical flow to achieve high-quality\nsegmentation results. To address these challenges, we propose the Intrinsic\nSaliency guided Trunk-Collateral Net}work (ISTC-Net), which better balances the\nmotion-appearance relationship and incorporates model's intrinsic saliency\ninformation to enhance segmentation performance. Specifically, considering that\noptical flow maps are derived from RGB images, they share both commonalities\nand differences. We propose a novel Trunk-Collateral structure. The shared\ntrunk backbone captures the motion-appearance commonality, while the collateral\nbranch learns the uniqueness of motion features. Furthermore, an Intrinsic\nSaliency guided Refinement Module (ISRM) is devised to efficiently leverage the\nmodel's intrinsic saliency information to refine high-level features, and\nprovide pixel-level guidance for motion-appearance fusion, thereby enhancing\nperformance without additional input. Experimental results show that ISTC-Net\nachieved state-of-the-art performance on three UVOS datasets (89.2% J&F on\nDAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS) and four standard video\nsalient object detection (VSOD) benchmarks with the notable increase,\ndemonstrating its effectiveness and superiority over previous methods."}
{"id": "2504.06260", "pdf": "https://arxiv.org/pdf/2504.06260", "abs": "https://arxiv.org/abs/2504.06260", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "categories": ["cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": "39 pages. Accepted at the NeurIPS 2024 Workshops on Mathematical\n  Reasoning and AI and Open-World Agents", "summary": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench"}
{"id": "2504.05908", "pdf": "https://arxiv.org/pdf/2504.05908", "abs": "https://arxiv.org/abs/2504.05908", "authors": ["Sriram Mandalika", "Lalitha V", "Athira Nambiar"], "title": "PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2025 - CVPRW", "summary": "Driving scene understanding is a critical real-world problem that involves\ninterpreting and associating various elements of a driving environment, such as\nvehicles, pedestrians, and traffic signals. Despite advancements in autonomous\ndriving, traditional pipelines rely on deterministic models that fail to\ncapture the probabilistic nature and inherent uncertainty of real-world\ndriving. To address this, we propose PRIMEDrive-CoT, a novel uncertainty-aware\nmodel for object interaction and Chain-of-Thought (CoT) reasoning in driving\nscenarios. In particular, our approach combines LiDAR-based 3D object detection\nwith multi-view RGB references to ensure interpretable and reliable scene\nunderstanding. Uncertainty and risk assessment, along with object interactions,\nare modelled using Bayesian Graph Neural Networks (BGNNs) for probabilistic\nreasoning under ambiguous conditions. Interpretable decisions are facilitated\nthrough CoT reasoning, leveraging object dynamics and contextual cues, while\nGrad-CAM visualizations highlight attention regions. Extensive evaluations on\nthe DriveCoT dataset demonstrate that PRIMEDrive-CoT outperforms\nstate-of-the-art CoT and risk-aware models."}
{"id": "2504.06261", "pdf": "https://arxiv.org/pdf/2504.06261", "abs": "https://arxiv.org/abs/2504.06261", "authors": ["Gleb Rodionov", "Roman Garipov", "Alina Shutova", "George Yakushev", "Vage Egiazarian", "Anton Sinitsin", "Denis Kuznedelev", "Dan Alistarh"], "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "categories": ["cs.LG", "cs.CL"], "comment": "Preprint, work in progress", "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."}
{"id": "2504.05913", "pdf": "https://arxiv.org/pdf/2504.05913", "abs": "https://arxiv.org/abs/2504.05913", "authors": ["Theodor Wulff", "Fares Abawi", "Philipp Allgeuer", "Stefan Wermter"], "title": "Balancing long- and short-term dynamics for the modeling of saliency in videos", "categories": ["cs.CV"], "comment": null, "summary": "The role of long- and short-term dynamics towards salient object detection in\nvideos is under-researched. We present a Transformer-based approach to learn a\njoint representation of video frames and past saliency information. Our model\nembeds long- and short-term information to detect dynamically shifting saliency\nin video. We provide our model with a stream of video frames and past saliency\nmaps, which acts as a prior for the next prediction, and extract spatiotemporal\ntokens from both modalities. The decomposition of the frame sequence into\ntokens lets the model incorporate short-term information from within the token,\nwhile being able to make long-term connections between tokens throughout the\nsequence. The core of the system consists of a dual-stream Transformer\narchitecture to process the extracted sequences independently before fusing the\ntwo modalities. Additionally, we apply a saliency-based masking scheme to the\ninput frames to learn an embedding that facilitates the recognition of\ndeviations from previous outputs. We observe that the additional prior\ninformation aids in the first detection of the salient location. Our findings\nindicate that the ratio of spatiotemporal long- and short-term features\ndirectly impacts the model's performance. While increasing the short-term\ncontext is beneficial up to a certain threshold, the model's performance\ngreatly benefits from an expansion of the long-term context."}
{"id": "2504.05925", "pdf": "https://arxiv.org/pdf/2504.05925", "abs": "https://arxiv.org/abs/2504.05925", "authors": ["Hao Du", "Bo Wu", "Yan Lu", "Zhendong Mao"], "title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation", "categories": ["cs.CV"], "comment": "CVPR 2025. The first two authors contributed equally", "summary": "Vision-language temporal alignment is a crucial capability for human dynamic\nrecognition and cognition in real-world scenarios. While existing research\nfocuses on capturing vision-language relevance, it faces limitations due to\nbiased temporal distributions, imprecise annotations, and insufficient\ncompositionally. To achieve fair evaluation and comprehensive exploration, our\nobjective is to investigate and evaluate the ability of models to achieve\nalignment from a temporal perspective, specifically focusing on their capacity\nto synchronize visual scenarios with linguistic context in a temporally\ncoherent manner. As a preliminary step, we present the statistical analysis of\nexisting benchmarks and reveal the existing challenges from a decomposed\nperspective. To this end, we introduce SVLTA, the Synthetic Vision-Language\nTemporal Alignment derived via a well-designed and feasible control generation\nmethod within a simulation environment. The approach considers commonsense\nknowledge, manipulable action, and constrained filtering, which generates\nreasonable, diverse, and balanced data distributions for diagnostic\nevaluations. Our experiments reveal diagnostic insights through the evaluations\nin temporal question answering, distributional shift sensitiveness, and\ntemporal alignment adaptation."}
{"id": "2504.05956", "pdf": "https://arxiv.org/pdf/2504.05956", "abs": "https://arxiv.org/abs/2504.05956", "authors": ["SuBeen Lee", "WonJun Moon", "Hyun Seok Seong", "Jae-Pil Heo"], "title": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures, 6 tables, Accepted to CVPR 2025 as Oral\n  Presentation", "summary": "Few-Shot Action Recognition (FSAR) aims to train a model with only a few\nlabeled video instances. A key challenge in FSAR is handling divergent\nnarrative trajectories for precise video matching. While the frame- and\ntuple-level alignment approaches have been promising, their methods heavily\nrely on pre-defined and length-dependent alignment units (e.g., frames or\ntuples), which limits flexibility for actions of varying lengths and speeds. In\nthis work, we introduce a novel TEmporal Alignment-free Matching (TEAM)\napproach, which eliminates the need for temporal units in action representation\nand brute-force alignment during matching. Specifically, TEAM represents each\nvideo with a fixed set of pattern tokens that capture globally discriminative\nclues within the video instance regardless of action length or speed, ensuring\nits flexibility. Furthermore, TEAM is inherently efficient, using token-wise\ncomparisons to measure similarity between videos, unlike existing methods that\nrely on pairwise comparisons for temporal alignment. Additionally, we propose\nan adaptation process that identifies and removes common information across\nclasses, establishing clear boundaries even between novel categories. Extensive\nexperiments demonstrate the effectiveness of TEAM. Codes are available at\ngithub.com/leesb7426/TEAM."}
{"id": "2504.05966", "pdf": "https://arxiv.org/pdf/2504.05966", "abs": "https://arxiv.org/abs/2504.05966", "authors": ["Xiaolin Fan", "Yan Wang", "Yingying Zhang", "Mingkun Bao", "Bosen Jia", "Dong Lu", "Yifan Gu", "Jian Cheng", "Haogang Zhu"], "title": "AVP-AP: Self-supervised Automatic View Positioning in 3D cardiac CT via Atlas Prompting", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 8 figures, published to TMI", "summary": "Automatic view positioning is crucial for cardiac computed tomography (CT)\nexaminations, including disease diagnosis and surgical planning. However, it is\nhighly challenging due to individual variability and large 3D search space.\nExisting work needs labor-intensive and time-consuming manual annotations to\ntrain view-specific models, which are limited to predicting only a fixed set of\nplanes. However, in real clinical scenarios, the challenge of positioning\nsemantic 2D slices with any orientation into varying coordinate space in\narbitrary 3D volume remains unsolved. We thus introduce a novel framework,\nAVP-AP, the first to use Atlas Prompting for self-supervised Automatic View\nPositioning in the 3D CT volume. Specifically, this paper first proposes an\natlas prompting method, which generates a 3D canonical atlas and trains a\nnetwork to map slices into their corresponding positions in the atlas space via\na self-supervised manner. Then, guided by atlas prompts corresponding to the\ngiven query images in a reference CT, we identify the coarse positions of\nslices in the target CT volume using rigid transformation between the 3D atlas\nand target CT volume, effectively reducing the search space. Finally, we refine\nthe coarse positions by maximizing the similarity between the predicted slices\nand the query images in the feature space of a given foundation model. Our\nframework is flexible and efficient compared to other methods, outperforming\nother methods by 19.8% average structural similarity (SSIM) in arbitrary view\npositioning and achieving 9% SSIM in two-chamber view compared to four\nradiologists. Meanwhile, experiments on a public dataset validate our\nframework's generalizability."}
{"id": "2504.05977", "pdf": "https://arxiv.org/pdf/2504.05977", "abs": "https://arxiv.org/abs/2504.05977", "authors": ["Jakob Lønborg Christensen", "Morten Rieger Hannemose", "Anders Bjorholm Dahl", "Vedrana Andersen Dahl"], "title": "Diffusion Based Ambiguous Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted at SCIA25", "summary": "Medical image segmentation often involves inherent uncertainty due to\nvariations in expert annotations. Capturing this uncertainty is an important\ngoal and previous works have used various generative image models for the\npurpose of representing the full distribution of plausible expert ground\ntruths. In this work, we explore the design space of diffusion models for\ngenerative segmentation, investigating the impact of noise schedules,\nprediction types, and loss weightings. Notably, we find that making the noise\nschedule harder with input scaling significantly improves performance. We\nconclude that x- and v-prediction outperform epsilon-prediction, likely because\nthe diffusion process is in the discrete segmentation domain. Many loss\nweightings achieve similar performance as long as they give enough weight to\nthe end of the diffusion process. We base our experiments on the LIDC-IDRI lung\nlesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we\nintroduce a randomly cropped variant of the LIDC-IDRI dataset that is better\nsuited for uncertainty in image segmentation. Our model also achieves SOTA in\nthis harder setting."}
{"id": "2504.05979", "pdf": "https://arxiv.org/pdf/2504.05979", "abs": "https://arxiv.org/abs/2504.05979", "authors": ["Sixiang Chen", "Jinbin Bai", "Zhuoran Zhao", "Tian Ye", "Qingyu Shi", "Donghao Zhou", "Wenhao Chai", "Xin Lin", "Jianzong Wu", "Chao Tang", "Shilin Xu", "Tao Zhang", "Haobo Yuan", "Yikang Zhou", "Wei Chow", "Linfeng Li", "Xiangtai Li", "Lei Zhu", "Lu Qi"], "title": "An Empirical Study of GPT-4o Image Generation Capabilities", "categories": ["cs.CV"], "comment": null, "summary": "The landscape of image generation has rapidly evolved, from early GAN-based\napproaches to diffusion models and, most recently, to unified generative\narchitectures that seek to bridge understanding and generation tasks. Recent\nadvances, especially the GPT-4o, have demonstrated the feasibility of\nhigh-fidelity multimodal generation, their architectural design remains\nmysterious and unpublished. This prompts the question of whether image and text\ngeneration have already been successfully integrated into a unified framework\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\nimage generation capabilities, benchmarking it against leading open-source and\ncommercial models. Our evaluation covers four main categories, including\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\nGPT-4o under various settings, and situates it within the broader evolution of\ngenerative modeling. Through this investigation, we identify promising\ndirections for future unified generative models, emphasizing the role of\narchitectural design and data scaling."}
{"id": "2504.05992", "pdf": "https://arxiv.org/pdf/2504.05992", "abs": "https://arxiv.org/abs/2504.05992", "authors": ["Jie Yang", "Chang Su", "Yuhan Zhang", "Jianjun Zhu", "Jianli Wang"], "title": "Under-Sampled High-Dimensional Data Recovery via Symbiotic Multi-Prior Tensor Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The advancement of sensing technology has driven the widespread application\nof high-dimensional data. However, issues such as missing entries during\nacquisition and transmission negatively impact the accuracy of subsequent\ntasks. Tensor reconstruction aims to recover the underlying complete data from\nunder-sampled observed data by exploring prior information in high-dimensional\ndata. However, due to insufficient exploration, reconstruction methods still\nface challenges when sampling rate is extremely low. This work proposes a\ntensor reconstruction method integrating multiple priors to comprehensively\nexploit the inherent structure of the data. Specifically, the method combines\nlearnable tensor decomposition to enforce low-rank constraints of the\nreconstructed data, a pre-trained convolutional neural network for smoothing\nand denoising, and block-matching and 3D filtering regularization to enhance\nthe non-local similarity in the reconstructed data. An alternating direction\nmethod of the multipliers algorithm is designed to decompose the resulting\noptimization problem into three subproblems for efficient resolution. Extensive\nexperiments on color images, hyperspectral images, and grayscale videos\ndatasets demonstrate the superiority of our method in extreme cases as compared\nwith state-of-the-art methods."}
{"id": "2504.06003", "pdf": "https://arxiv.org/pdf/2504.06003", "abs": "https://arxiv.org/abs/2504.06003", "authors": ["Can Zhang", "Gim Hee Lee"], "title": "econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "The primary focus of most recent works on open-vocabulary neural fields is\nextracting precise semantic features from the VLMs and then consolidating them\nefficiently into a multi-view consistent 3D neural fields representation.\nHowever, most existing works over-trusted SAM to regularize image-level CLIP\nwithout any further refinement. Moreover, several existing works improved\nefficiency by dimensionality reduction of semantic features from 2D VLMs before\nfusing with 3DGS semantic fields, which inevitably leads to multi-view\ninconsistency. In this work, we propose econSG for open-vocabulary semantic\nsegmentation with 3DGS. Our econSG consists of: 1) A Confidence-region Guided\nRegularization (CRR) that mutually refines SAM and CLIP to get the best of both\nworlds for precise semantic features with complete and precise boundaries. 2) A\nlow dimensional contextual space to enforce 3D multi-view consistency while\nimproving computational efficiency by fusing backprojected multi-view 2D\nfeatures and follow by dimensional reduction directly on the fused 3D features\ninstead of operating on each 2D view separately. Our econSG shows\nstate-of-the-art performance on four benchmark datasets compared to the\nexisting methods. Furthermore, we are also the most efficient training among\nall the methods."}
{"id": "2504.06004", "pdf": "https://arxiv.org/pdf/2504.06004", "abs": "https://arxiv.org/abs/2504.06004", "authors": ["Mrityunjoy Gain", "Kitae Kim", "Avi Deb Raha", "Apurba Adhikary", "Eui-Nam Huh", "Zhu Han", "Choong Seon Hong"], "title": "FedFeat+: A Robust Federated Learning Framework Through Federated Aggregation and Differentially Private Feature-Based Classifier Retraining", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose the FedFeat+ framework, which distinctively\nseparates feature extraction from classification. We develop a two-tiered model\ntraining process: following local training, clients transmit their weights and\nsome features extracted from the feature extractor from the final local epochs\nto the server. The server aggregates these models using the FedAvg method and\nsubsequently retrains the global classifier utilizing the shared features. The\nclassifier retraining process enhances the model's understanding of the\nholistic view of the data distribution, ensuring better generalization across\ndiverse datasets. This improved generalization enables the classifier to\nadaptively influence the feature extractor during subsequent local training\nepochs. We establish a balance between enhancing model accuracy and\nsafeguarding individual privacy through the implementation of differential\nprivacy mechanisms. By incorporating noise into the feature vectors shared with\nthe server, we ensure that sensitive data remains confidential. We present a\ncomprehensive convergence analysis, along with theoretical reasoning regarding\nperformance enhancement and privacy preservation. We validate our approach\nthrough empirical evaluations conducted on benchmark datasets, including\nCIFAR-10, CIFAR-100, MNIST, and FMNIST, achieving high accuracy while adhering\nto stringent privacy guarantees. The experimental results demonstrate that the\nFedFeat+ framework, despite using only a lightweight two-layer CNN classifier,\noutperforms the FedAvg method in both IID and non-IID scenarios, achieving\naccuracy improvements ranging from 3.92 % to 12.34 % across CIFAR-10,\nCIFAR-100, and Fashion-MNIST datasets."}
{"id": "2504.06010", "pdf": "https://arxiv.org/pdf/2504.06010", "abs": "https://arxiv.org/abs/2504.06010", "authors": ["Stefanos-Iordanis Papadopoulos", "Christos Koutlis", "Symeon Papadopoulos", "Panagiotis C. Petrantonakis"], "title": "Latent Multimodal Reconstruction for Misinformation Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Multimodal misinformation, such as miscaptioned images, where captions\nmisrepresent an image's origin, context, or meaning, poses a growing challenge\nin the digital age. To support fact-checkers, researchers have been focusing on\ncreating datasets and developing methods for multimodal misinformation\ndetection (MMD). Due to the scarcity of large-scale annotated MMD datasets,\nrecent studies leverage synthetic training data via out-of-context\nimage-caption pairs or named entity manipulations; altering names, dates, and\nlocations. However, these approaches often produce simplistic misinformation\nthat fails to reflect real-world complexity, limiting the robustness of\ndetection models trained on them. Meanwhile, despite recent advancements, Large\nVision-Language Models (LVLMs) remain underutilized for generating diverse,\nrealistic synthetic training data for MMD. To address this gap, we introduce\n\"MisCaption This!\", a training dataset comprising LVLM-generated miscaptioned\nimages. Additionally, we introduce \"Latent Multimodal Reconstruction\" (LAMAR),\na network trained to reconstruct the embeddings of truthful captions, providing\na strong auxiliary signal to the detection process. To optimize LAMAR, we\nexplore different training strategies (end-to-end training and large-scale\npre-training) and integration approaches (direct, mask, gate, and attention).\nExtensive experiments show that models trained on \"MisCaption This!\" generalize\nbetter on real-world misinformation, while LAMAR sets new state-of-the-art on\nboth NewsCLIPpings and VERITE benchmarks; highlighting the potential of\nLVLM-generated data and reconstruction-based approaches for advancing MMD. We\nrelease our code at:\nhttps://github.com/stevejpapad/miscaptioned-image-reconstruction"}
{"id": "2504.06021", "pdf": "https://arxiv.org/pdf/2504.06021", "abs": "https://arxiv.org/abs/2504.06021", "authors": ["Dahyun Kang", "Ahmet Iscen", "Eunchan Jo", "Sua Choi", "Minsu Cho", "Cordelia Schmid"], "title": "Memory-Modular Classification: Learning to Generalize with Memory Replacement", "categories": ["cs.CV"], "comment": "Accepted to TMLR. Code available: https://github.com/dahyun-kang/mml", "summary": "We propose a novel memory-modular learner for image classification that\nseparates knowledge memorization from reasoning. Our model enables effective\ngeneralization to new classes by simply replacing the memory contents, without\nthe need for model retraining. Unlike traditional models that encode both world\nknowledge and task-specific skills into their weights during training, our\nmodel stores knowledge in the external memory of web-crawled image and text\ndata. At inference time, the model dynamically selects relevant content from\nthe memory based on the input image, allowing it to adapt to arbitrary classes\nby simply replacing the memory contents. The key differentiator that our\nlearner meta-learns to perform classification tasks with noisy web data from\nunseen classes, resulting in robust performance across various classification\nscenarios. Experimental results demonstrate the promising performance and\nversatility of our approach in handling diverse classification tasks, including\nzero-shot/few-shot classification of unseen classes, fine-grained\nclassification, and class-incremental classification."}
{"id": "2504.06022", "pdf": "https://arxiv.org/pdf/2504.06022", "abs": "https://arxiv.org/abs/2504.06022", "authors": ["Luis Denninger", "Sina Mokhtarzadeh Azar", "Juergen Gall"], "title": "CamContextI2V: Context-aware Controllable Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, image-to-video (I2V) diffusion models have demonstrated impressive\nscene understanding and generative quality, incorporating image conditions to\nguide generation. However, these models primarily animate static images without\nextending beyond their provided context. Introducing additional constraints,\nsuch as camera trajectories, can enhance diversity but often degrades visual\nquality, limiting their applicability for tasks requiring faithful scene\nrepresentation. We propose CamContextI2V, an I2V model that integrates multiple\nimage conditions with 3D constraints alongside camera control to enrich both\nglobal semantics and fine-grained visual details. This enables more coherent\nand context-aware video generation. Moreover, we motivate the necessity of\ntemporal awareness for an effective context representation. Our comprehensive\nstudy on the RealEstate10K dataset demonstrates improvements in visual quality\nand camera controllability. We make our code and models publicly available at:\nhttps://github.com/LDenninger/CamContextI2V."}
{"id": "2504.06039", "pdf": "https://arxiv.org/pdf/2504.06039", "abs": "https://arxiv.org/abs/2504.06039", "authors": ["Julia Werner", "Christoph Gerum", "Jorg Nick", "Maxime Le Floch", "Franz Brinkmann", "Jochen Hampe", "Oliver Bringmann"], "title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning Strategies", "categories": ["cs.CV"], "comment": "Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBS EMBC)", "summary": "Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies."}
{"id": "2504.06088", "pdf": "https://arxiv.org/pdf/2504.06088", "abs": "https://arxiv.org/abs/2504.06088", "authors": ["Divyanshu Mishra", "Pramit Saha", "He Zhao", "Netzahualcoyotl Hernandez-Cruz", "Olga Patey", "Aris Papageorghiou", "J. Alison Noble"], "title": "MCAT: Visual Query-Based Localization of Standard Anatomical Clips in Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted in AAAI 2025", "summary": "Accurate standard plane acquisition in fetal ultrasound (US) videos is\ncrucial for fetal growth assessment, anomaly detection, and adherence to\nclinical guidelines. However, manually selecting standard frames is\ntime-consuming and prone to intra- and inter-sonographer variability. Existing\nmethods primarily rely on image-based approaches that capture standard frames\nand then classify the input frames across different anatomies. This ignores the\ndynamic nature of video acquisition and its interpretation. To address these\nchallenges, we introduce Multi-Tier Class-Aware Token Transformer (MCAT), a\nvisual query-based video clip localization (VQ-VCL) method, to assist\nsonographers by enabling them to capture a quick US sweep. By then providing a\nvisual query of the anatomy they wish to analyze, MCAT returns the video clip\ncontaining the standard frames for that anatomy, facilitating thorough\nscreening for potential anomalies. We evaluate MCAT on two ultrasound video\ndatasets and a natural image VQ-VCL dataset based on Ego4D. Our model\noutperforms state-of-the-art methods by 10% and 13% mIoU on the ultrasound\ndatasets and by 5.35% mIoU on the Ego4D dataset, using 96% fewer tokens. MCAT's\nefficiency and accuracy have significant potential implications for public\nhealth, especially in low- and middle-income countries (LMICs), where it may\nenhance prenatal care by streamlining standard plane acquisition, simplifying\nUS-based screening, diagnosis and allowing sonographers to examine more\npatients."}
{"id": "2504.06099", "pdf": "https://arxiv.org/pdf/2504.06099", "abs": "https://arxiv.org/abs/2504.06099", "authors": ["Samuel Bielik", "Simon Bilik"], "title": "Towards Varroa destructor mite detection using a narrow spectra illumination", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper focuses on the development and modification of a beehive\nmonitoring device and Varroa destructor detection on the bees with the help of\nhyperspectral imagery while utilizing a U-net, semantic segmentation\narchitecture, and conventional computer vision methods. The main objectives\nwere to collect a dataset of bees and mites, and propose the computer vision\nmodel which can achieve the detection between bees and mites."}
{"id": "2504.06116", "pdf": "https://arxiv.org/pdf/2504.06116", "abs": "https://arxiv.org/abs/2504.06116", "authors": ["Davide Sferrazza", "Gabriele Berton", "Gabriele Trivigno", "Carlo Masone"], "title": "To Match or Not to Match: Revisiting Image Matching for Reliable Visual Place Recognition", "categories": ["cs.CV"], "comment": "CVPRW 2025", "summary": "Visual Place Recognition (VPR) is a critical task in computer vision,\ntraditionally enhanced by re-ranking retrieval results with image matching.\nHowever, recent advancements in VPR methods have significantly improved\nperformance, challenging the necessity of re-ranking. In this work, we show\nthat modern retrieval systems often reach a point where re-ranking can degrade\nresults, as current VPR datasets are largely saturated. We propose using image\nmatching as a verification step to assess retrieval confidence, demonstrating\nthat inlier counts can reliably predict when re-ranking is beneficial. Our\nfindings shift the paradigm of retrieval pipelines, offering insights for more\nrobust and adaptive VPR systems."}
{"id": "2504.06120", "pdf": "https://arxiv.org/pdf/2504.06120", "abs": "https://arxiv.org/abs/2504.06120", "authors": ["Yuanpei Liu", "Zhenqi He", "Kai Han"], "title": "Hyperbolic Category Discovery", "categories": ["cs.CV"], "comment": "Accepted as a conference paper at CVPR 2025", "summary": "Generalized Category Discovery (GCD) is an intriguing open-world problem that\nhas garnered increasing attention. Given a dataset that includes both labelled\nand unlabelled images, GCD aims to categorize all images in the unlabelled\nsubset, regardless of whether they belong to known or unknown classes. In GCD,\nthe common practice typically involves applying a spherical projection operator\nat the end of the self-supervised pretrained backbone, operating within\nEuclidean or spherical space. However, both of these spaces have been shown to\nbe suboptimal for encoding samples that possesses hierarchical structures. In\ncontrast, hyperbolic space exhibits exponential volume growth relative to\nradius, making it inherently strong at capturing the hierarchical structure of\nsamples from both seen and unseen categories. Therefore, we propose to tackle\nthe category discovery challenge in the hyperbolic space. We introduce HypCD, a\nsimple \\underline{Hyp}erbolic framework for learning hierarchy-aware\nrepresentations and classifiers for generalized \\underline{C}ategory\n\\underline{D}iscovery. HypCD first transforms the Euclidean embedding space of\nthe backbone network into hyperbolic space, facilitating subsequent\nrepresentation and classification learning by considering both hyperbolic\ndistance and the angle between samples. This approach is particularly helpful\nfor knowledge transfer from known to unknown categories in GCD. We thoroughly\nevaluate HypCD on public GCD benchmarks, by applying it to various baseline and\nstate-of-the-art methods, consistently achieving significant improvements."}
{"id": "2504.06121", "pdf": "https://arxiv.org/pdf/2504.06121", "abs": "https://arxiv.org/abs/2504.06121", "authors": ["Ronghui Zhang", "Yuhang Ma", "Tengfei Li", "Ziyu Lin", "Yueying Wu", "Junzhou Chen", "Lin Zhang", "Jia Hu", "Tony Z. Qiu", "Konghui Guo"], "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments."}
{"id": "2504.06131", "pdf": "https://arxiv.org/pdf/2504.06131", "abs": "https://arxiv.org/abs/2504.06131", "authors": ["Sudipta Banerjee", "Anubhav Jain", "Chinmay Hegde", "Nasir Memon"], "title": "FaceCloak: Learning to Protect Face Templates", "categories": ["cs.CV"], "comment": "Accepted in IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2025)", "summary": "Generative models can reconstruct face images from encoded representations\n(templates) bearing remarkable likeness to the original face raising security\nand privacy concerns. We present FaceCloak, a neural network framework that\nprotects face templates by generating smart, renewable binary cloaks. Our\nmethod proactively thwarts inversion attacks by cloaking face templates with\nunique disruptors synthesized from a single face template on the fly while\nprovably retaining biometric utility and unlinkability. Our cloaked templates\ncan suppress sensitive attributes while generalizing to novel feature\nextraction schemes and outperforms leading baselines in terms of biometric\nmatching and resiliency to reconstruction attacks. FaceCloak-based matching is\nextremely fast (inference time cost=0.28ms) and light-weight (0.57MB)."}
{"id": "2504.06144", "pdf": "https://arxiv.org/pdf/2504.06144", "abs": "https://arxiv.org/abs/2504.06144", "authors": ["Jihun Park", "Jongmin Gim", "Kyoungmin Lee", "Minseok Oh", "Minwoo Choi", "Jaeyeul Kim", "Woo Chool Park", "Sunghoon Im"], "title": "A Training-Free Style-aligned Image Generation with Scale-wise Autoregressive Model", "categories": ["cs.CV"], "comment": "17 pages, 15 figures", "summary": "We present a training-free style-aligned image generation method that\nleverages a scale-wise autoregressive model. While large-scale text-to-image\n(T2I) models, particularly diffusion-based methods, have demonstrated\nimpressive generation quality, they often suffer from style misalignment across\ngenerated image sets and slow inference speeds, limiting their practical\nusability. To address these issues, we propose three key components: initial\nfeature replacement to ensure consistent background appearance, pivotal feature\ninterpolation to align object placement, and dynamic style injection, which\nreinforces style consistency using a schedule function. Unlike previous methods\nrequiring fine-tuning or additional training, our approach maintains fast\ninference while preserving individual content details. Extensive experiments\nshow that our method achieves generation quality comparable to competing\napproaches, significantly improves style alignment, and delivers inference\nspeeds over six times faster than the fastest model."}
{"id": "2504.06148", "pdf": "https://arxiv.org/pdf/2504.06148", "abs": "https://arxiv.org/abs/2504.06148", "authors": ["Xiangxi Zheng", "Linjie Li", "Zhengyuan Yang", "Ping Yu", "Alex Jinpeng Wang", "Rui Yan", "Yuan Yao", "Lijuan Wang"], "title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to\nsignificant improvements across various multimodal benchmarks. However, as\nevaluations shift from static datasets to open-world, dynamic environments,\ncurrent game-based benchmarks remain inadequate because they lack\nvisual-centric tasks and fail to assess the diverse reasoning skills required\nfor real-world decision-making. To address this, we introduce Visual-centric\nMultiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework\ndesigned to assess visual reasoning capabilities of MLLMs. V-MAGE features five\ndiverse games with 30+ handcrafted levels, testing models on core visual skills\nsuch as positioning, trajectory tracking, timing, and visual memory, alongside\nhigher-level reasoning like long-term planning and deliberation. We use V-MAGE\nto evaluate leading MLLMs, revealing significant challenges in their visual\nperception and reasoning. In all game environments, the top-performing MLLMs,\nas determined by Elo rating comparisons, exhibit a substantial performance gap\ncompared to humans. Our findings highlight critical limitations, including\nvarious types of perceptual errors made by the models, and suggest potential\navenues for improvement from an agent-centric perspective, such as refining\nagent strategies and addressing perceptual inaccuracies. Code is available at\nhttps://github.com/CSU-JPG/V-MAGE."}
{"id": "2504.06153", "pdf": "https://arxiv.org/pdf/2504.06153", "abs": "https://arxiv.org/abs/2504.06153", "authors": ["Akash Kumar", "Ashlesha Kumar", "Vibhav Vineet", "Yogesh S Rawat"], "title": "A Large-Scale Analysis on Contextual Self-Supervised Video Representation Learning", "categories": ["cs.CV"], "comment": "CVPR'25 Workshop: 6th Data-Efficient Workshop", "summary": "Self-supervised learning has emerged as a powerful paradigm for label-free\nmodel pretraining, particularly in the video domain, where manual annotation is\ncostly and time-intensive. However, existing self-supervised approaches employ\ndiverse experimental setups, making direct comparisons challenging due to the\nabsence of a standardized benchmark. In this work, we establish a unified\nbenchmark that enables fair comparisons across different methods. Additionally,\nwe systematically investigate five critical aspects of self-supervised learning\nin videos: (1) dataset size, (2) model complexity, (3) data distribution, (4)\ndata noise, and (5) feature representations. To facilitate this study, we\nevaluate six self-supervised learning methods across six network architectures,\nconducting extensive experiments on five benchmark datasets and assessing\nperformance on two distinct downstream tasks. Our analysis reveals key insights\ninto the interplay between pretraining strategies, dataset characteristics,\npretext tasks, and model architectures. Furthermore, we extend these findings\nto Video Foundation Models (ViFMs), demonstrating their relevance in\nlarge-scale video representation learning. Finally, leveraging these insights,\nwe propose a novel approach that significantly reduces training data\nrequirements while surpassing state-of-the-art methods that rely on 10% more\npretraining data. We believe this work will guide future research toward a\ndeeper understanding of self-supervised video representation learning and its\nbroader implications."}
{"id": "2504.06158", "pdf": "https://arxiv.org/pdf/2504.06158", "abs": "https://arxiv.org/abs/2504.06158", "authors": ["Saad Wazir", "Daeyoung Kim"], "title": "Rethinking the Nested U-Net Approach: Enhancing Biomarker Segmentation with Attention Mechanisms and Multiscale Feature Fusion", "categories": ["eess.IV", "cs.CV"], "comment": "Published in the Proceedings of the 2024 International Conference on\n  Medical Imaging and Computer-Aided Diagnosis (MICAD 2024), Lecture Notes in\n  Electrical Engineering (LNEE), Volume 1372, Springer Nature, Singapore", "summary": "Identifying biomarkers in medical images is vital for a wide range of biotech\napplications. However, recent Transformer and CNN based methods often struggle\nwith variations in morphology and staining, which limits their feature\nextraction capabilities. In medical image segmentation, where data samples are\noften limited, state-of-the-art (SOTA) methods improve accuracy by using\npre-trained encoders, while end-to-end approaches typically fall short due to\ndifficulties in transferring multiscale features effectively between encoders\nand decoders. To handle these challenges, we introduce a nested UNet\narchitecture that captures both local and global context through Multiscale\nFeature Fusion and Attention Mechanisms. This design improves feature\nintegration from encoders, highlights key channels and regions, and restores\nspatial details to enhance segmentation performance. Our method surpasses SOTA\napproaches, as evidenced by experiments across four datasets and detailed\nablation studies. Code: https://github.com/saadwazir/ReN-UNet"}
{"id": "2504.06163", "pdf": "https://arxiv.org/pdf/2504.06163", "abs": "https://arxiv.org/abs/2504.06163", "authors": ["Artur Xarles", "Sergio Escalera", "Thomas B. Moeslund", "Albert Clapés"], "title": "Action Valuation in Sports: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Action Valuation (AV) has emerged as a key topic in Sports Analytics,\noffering valuable insights by assigning scores to individual actions based on\ntheir contribution to desired outcomes. Despite a few surveys addressing\nrelated concepts such as Player Valuation, there is no comprehensive review\ndedicated to an in-depth analysis of AV across different sports. In this\nsurvey, we introduce a taxonomy with nine dimensions related to the AV task,\nencompassing data, methodological approaches, evaluation techniques, and\npractical applications. Through this analysis, we aim to identify the essential\ncharacteristics of effective AV methods, highlight existing gaps in research,\nand propose future directions for advancing the field."}
{"id": "2504.06178", "pdf": "https://arxiv.org/pdf/2504.06178", "abs": "https://arxiv.org/abs/2504.06178", "authors": ["Yujia Hu", "Songhua Liu", "Xingyi Yang", "Xinchao Wang"], "title": "Flash Sculptor: Modular 3D Worlds from Objects", "categories": ["cs.CV"], "comment": null, "summary": "Existing text-to-3D and image-to-3D models often struggle with complex scenes\ninvolving multiple objects and intricate interactions. Although some recent\nattempts have explored such compositional scenarios, they still require an\nextensive process of optimizing the entire layout, which is highly cumbersome\nif not infeasible at all. To overcome these challenges, we propose Flash\nSculptor in this paper, a simple yet effective framework for compositional 3D\nscene/object reconstruction from a single image. At the heart of Flash Sculptor\nlies a divide-and-conquer strategy, which decouples compositional scene\nreconstruction into a sequence of sub-tasks, including handling the appearance,\nrotation, scale, and translation of each individual instance. Specifically, for\nrotation, we introduce a coarse-to-fine scheme that brings the best of both\nworlds--efficiency and accuracy--while for translation, we develop an\noutlier-removal-based algorithm that ensures robust and precise parameters in a\nsingle step, without any iterative optimization. Extensive experiments\ndemonstrate that Flash Sculptor achieves at least a 3 times speedup over\nexisting compositional 3D methods, while setting new benchmarks in\ncompositional 3D reconstruction performance. Codes are available at\nhttps://github.com/YujiaHu1109/Flash-Sculptor."}
{"id": "2504.06185", "pdf": "https://arxiv.org/pdf/2504.06185", "abs": "https://arxiv.org/abs/2504.06185", "authors": ["Vanessa Borst", "Timo Dittus", "Tassilo Dege", "Astrid Schmieder", "Samuel Kounev"], "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care", "categories": ["cs.CV", "cs.AI"], "comment": "Main paper: 17 pages; supplementary material: 16 pages; paper\n  submitted to the application track of the European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases\n  (ECML PKDD 2025)", "summary": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication."}
{"id": "2504.06205", "pdf": "https://arxiv.org/pdf/2504.06205", "abs": "https://arxiv.org/abs/2504.06205", "authors": ["Qing Xu", "Zhenye Lou", "Chenxin Li", "Xiangjian He", "Rong Qu", "Tesema Fiseha Berhanu", "Yi Wang", "Wenting Duan", "Zhen Chen"], "title": "HRMedSeg: Unlocking High-resolution Medical Image segmentation via Memory-efficient Attention Modeling", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review", "summary": "High-resolution segmentation is critical for precise disease diagnosis by\nextracting micro-imaging information from medical images. Existing\ntransformer-based encoder-decoder frameworks have demonstrated remarkable\nversatility and zero-shot performance in medical segmentation. While\nbeneficial, they usually require huge memory costs when handling large-size\nsegmentation mask predictions, which are expensive to apply to real-world\nscenarios. To address this limitation, we propose a memory-efficient framework\nfor high-resolution medical image segmentation, called HRMedSeg. Specifically,\nwe first devise a lightweight gated vision transformer (LGViT) as our image\nencoder to model long-range dependencies with linear complexity. Then, we\ndesign an efficient cross-multiscale decoder (ECM-Decoder) to generate\nhigh-resolution segmentation masks. Moreover, we utilize feature distillation\nduring pretraining to unleash the potential of our proposed model. Extensive\nexperiments reveal that HRMedSeg outperforms state-of-the-arts in diverse\nhigh-resolution medical image segmentation tasks. In particular, HRMedSeg uses\nonly 0.59GB GPU memory per batch during fine-tuning, demonstrating low training\ncosts. Besides, when HRMedSeg meets the Segment Anything Model (SAM), our\nHRMedSegSAM takes 0.61% parameters of SAM-H. The code is available at\nhttps://github.com/xq141839/HRMedSeg."}
{"id": "2504.06210", "pdf": "https://arxiv.org/pdf/2504.06210", "abs": "https://arxiv.org/abs/2504.06210", "authors": ["Yiming Liang", "Tianhan Xu", "Yuta Kikuchi"], "title": "HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation", "categories": ["cs.CV"], "comment": "CVPR 2025. Project Page: https://pfnet-research.github.io/himor", "summary": "We present Hierarchical Motion Representation (HiMoR), a novel deformation\nrepresentation for 3D Gaussian primitives capable of achieving high-quality\nmonocular dynamic 3D reconstruction. The insight behind HiMoR is that motions\nin everyday scenes can be decomposed into coarser motions that serve as the\nfoundation for finer details. Using a tree structure, HiMoR's nodes represent\ndifferent levels of motion detail, with shallower nodes modeling coarse motion\nfor temporal smoothness and deeper nodes capturing finer motion. Additionally,\nour model uses a few shared motion bases to represent motions of different sets\nof nodes, aligning with the assumption that motion tends to be smooth and\nsimple. This motion representation design provides Gaussians with a more\nstructured deformation, maximizing the use of temporal relationships to tackle\nthe challenging task of monocular dynamic 3D reconstruction. We also propose\nusing a more reliable perceptual metric as an alternative, given that\npixel-level metrics for evaluating monocular dynamic 3D reconstruction can\nsometimes fail to accurately reflect the true quality of reconstruction.\nExtensive experiments demonstrate our method's efficacy in achieving superior\nnovel view synthesis from challenging monocular videos with complex motions."}
{"id": "2504.06220", "pdf": "https://arxiv.org/pdf/2504.06220", "abs": "https://arxiv.org/abs/2504.06220", "authors": ["Xiaoxing Hu", "Ziyang Gong", "Yupei Wang", "Yuru Jia", "Gen Luo", "Xue Yang"], "title": "Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt\npowerful Foundation Models (FMs) to diverse downstream tasks while preserving\nand unleashing their inherent capabilities. However, we have observed that\nexisting PEFT methods, which are often designed with natural imagery in mind,\nstruggle when applied to Remote Sensing (RS) scenarios. This is primarily due\nto their inability to handle artifact influences, a problem particularly severe\nin RS image features. To tackle this challenge, we introduce Earth-Adapter, the\nfirst PEFT method specifically designed for RS artifacts conquering.\nEarth-Adapter introduces a novel Mixture of Frequency Adaptation process that\ncombines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT).\nBy utilizing DFT, Earth-Adapter can decompose features into different frequency\ncomponents, precisely separating artifacts from original features. The MoA then\ndynamically assigns weights to each adapter expert, allowing for the\ncombination of features across various frequency domains. These\nsimple-yet-effective approaches enable Earth-Adapter to more efficiently\novercome the disturbances caused by artifacts than previous PEFT methods,\nsignificantly enhancing the FMs' performance on RS scenarios. Experiments on\nDomain Adaptation (DA), and Domain Generalization (DG) semantic segmentation\nbenchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline\nRein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG\nbenchmarks. Our code will be released at\nhttps://github.com/VisionXLab/Earth-Adapter."}
{"id": "2504.06232", "pdf": "https://arxiv.org/pdf/2504.06232", "abs": "https://arxiv.org/abs/2504.06232", "authors": ["Jiazi Bu", "Pengyang Ling", "Yujie Zhou", "Pan Zhang", "Tong Wu", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention\nrecently due to their remarkable ability to deliver flexible visual creations.\nStill, high-resolution image synthesis presents formidable challenges due to\nthe scarcity and complexity of high-resolution content. To this end, we present\nHiFlow, a training-free and model-agnostic framework to unlock the resolution\npotential of pre-trained flow models. Specifically, HiFlow establishes a\nvirtual reference flow within the high-resolution space that effectively\ncaptures the characteristics of low-resolution flow information, offering\nguidance for high-resolution generation through three key aspects:\ninitialization alignment for low-frequency consistency, direction alignment for\nstructure preservation, and acceleration alignment for detail fidelity. By\nleveraging this flow-aligned guidance, HiFlow substantially elevates the\nquality of high-resolution image synthesis of T2I models and demonstrates\nversatility across their personalized variants. Extensive experiments validate\nHiFlow's superiority in achieving superior high-resolution image quality over\ncurrent state-of-the-art methods."}
{"id": "2504.06237", "pdf": "https://arxiv.org/pdf/2504.06237", "abs": "https://arxiv.org/abs/2504.06237", "authors": ["Mina Bishay", "Graham Page", "Waleed Emad", "Mohammad Mavadati"], "title": "Monitoring Viewer Attention During Online Ads", "categories": ["cs.CV"], "comment": "Presented at the ECCV 2024 Workshops", "summary": "Nowadays, video ads spread through numerous online platforms, and are being\nwatched by millions of viewers worldwide. Big brands gauge the liking and\npurchase intent of their new ads, by analyzing the facial responses of viewers\nrecruited online to watch the ads from home or work. Although this approach\ncaptures naturalistic responses, it is susceptible to distractions inherent in\nthe participants' environments, such as a movie playing on TV, a colleague\nspeaking, or mobile notifications. Inattentive participants should get flagged\nand eliminated to avoid skewing the ad-testing process. In this paper we\nintroduce an architecture for monitoring viewer attention during online ads.\nLeveraging two behavior analysis toolkits; AFFDEX 2.0 and SmartEye SDK, we\nextract low-level facial features encompassing facial expressions, head pose,\nand gaze direction. These features are then combined to extract high-level\nfeatures that include estimated gaze on the screen plane, yawning, speaking,\netc -- this enables the identification of four primary distractors; off-screen\ngaze, drowsiness, speaking, and unattended screen. Our architecture tailors the\ngaze settings according to the device type (desktop or mobile). We validate our\narchitecture first on datasets annotated for specific distractors, and then on\na real-world ad testing dataset with various distractors. The proposed\narchitecture shows promising results in detecting distraction across both\ndesktop and mobile devices."}
{"id": "2504.06256", "pdf": "https://arxiv.org/pdf/2504.06256", "abs": "https://arxiv.org/abs/2504.06256", "authors": ["Xichen Pan", "Satya Narayan Shukla", "Aashu Singh", "Zhuokai Zhao", "Shlok Kumar Mishra", "Jialiang Wang", "Zhiyang Xu", "Jiuhai Chen", "Kunpeng Li", "Felix Juefei-Xu", "Ji Hou", "Saining Xie"], "title": "Transfer between Modalities with MetaQueries", "categories": ["cs.CV"], "comment": "Project Page: https://xichenpan.com/metaquery", "summary": "Unified multimodal models aim to integrate understanding (text output) and\ngeneration (pixel output), but aligning these different modalities within a\nsingle architecture often demands complex training recipes and careful data\nbalancing. We introduce MetaQueries, a set of learnable queries that act as an\nefficient interface between autoregressive multimodal LLMs (MLLMs) and\ndiffusion models. MetaQueries connects the MLLM's latents to the diffusion\ndecoder, enabling knowledge-augmented image generation by leveraging the MLLM's\ndeep understanding and reasoning capabilities. Our method simplifies training,\nrequiring only paired image-caption data and standard diffusion objectives.\nNotably, this transfer is effective even when the MLLM backbone remains frozen,\nthereby preserving its state-of-the-art multimodal understanding capabilities\nwhile achieving strong generative performance. Additionally, our method is\nflexible and can be easily instruction-tuned for advanced applications such as\nimage editing and subject-driven generation."}
{"id": "2504.06257", "pdf": "https://arxiv.org/pdf/2504.06257", "abs": "https://arxiv.org/abs/2504.06257", "authors": ["Mina Bishay", "Graham Page", "Mohammad Mavadati"], "title": "PainNet: Statistical Relation Network with Episode-Based Training for Pain Estimation", "categories": ["cs.CV"], "comment": "Presented at the ACII 2024 Workshops", "summary": "Despite the span in estimating pain from facial expressions, limited works\nhave focused on estimating the sequence-level pain, which is reported by\npatients and used commonly in clinics. In this paper, we introduce a novel\nStatistical Relation Network, referred to as PainNet, designed for the\nestimation of the sequence-level pain. PainNet employs two key modules, the\nembedding and the relation modules, for comparing pairs of pain videos, and\nproducing relation scores indicating if each pair belongs to the same pain\ncategory or not. At the core of the embedding module is a statistical layer\nmounted on the top of a RNN for extracting compact video-level features. The\nstatistical layer is implemented as part of the deep architecture. Doing so,\nallows combining multiple training stages used in previous research, into a\nsingle end-to-end training stage. PainNet is trained using the episode-based\ntraining scheme, which involves comparing a query video with a set of videos\nrepresenting the different pain categories. Experimental results show the\nbenefit of using the statistical layer and the episode-based training in the\nproposed model. Furthermore, PainNet outperforms the state-of-the-art results\non self-reported pain estimation."}
{"id": "2504.06263", "pdf": "https://arxiv.org/pdf/2504.06263", "abs": "https://arxiv.org/abs/2504.06263", "authors": ["Yiying Yang", "Wei Cheng", "Sijin Chen", "Xianfang Zeng", "Jiaxu Zhang", "Liao Wang", "Gang Yu", "Xingjun Ma", "Yu-Gang Jiang"], "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model", "categories": ["cs.CV"], "comment": "18 pages; Project Page: https://omnisvg.github.io/", "summary": "Scalable Vector Graphics (SVG) is an important image format widely adopted in\ngraphic design because of their resolution independence and editability. The\nstudy of generating high-quality SVG has continuously drawn attention from both\ndesigners and researchers in the AIGC community. However, existing methods\neither produces unstructured outputs with huge computational cost or is limited\nto generating monochrome icons of over-simplified structures. To produce\nhigh-quality and complex SVG, we propose OmniSVG, a unified framework that\nleverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal\nSVG generation. By parameterizing SVG commands and coordinates into discrete\ntokens, OmniSVG decouples structural logic from low-level geometry for\nefficient training while maintaining the expressiveness of complex SVG\nstructure. To further advance the development of SVG synthesis, we introduce\nMMSVG-2M, a multimodal dataset with two million richly annotated SVG assets,\nalong with a standardized evaluation protocol for conditional SVG generation\ntasks. Extensive experiments show that OmniSVG outperforms existing methods and\ndemonstrates its potential for integration into professional SVG design\nworkflows."}
{"id": "2504.06264", "pdf": "https://arxiv.org/pdf/2504.06264", "abs": "https://arxiv.org/abs/2504.06264", "authors": ["Jisang Han", "Honggyu An", "Jaewoo Jung", "Takuya Narihira", "Junyoung Seo", "Kazumi Fukuda", "Chaehyun Kim", "Sunghwan Hong", "Yuki Mitsufuji", "Seungryong Kim"], "title": "D^2USt3R: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes", "categories": ["cs.CV"], "comment": "project page: https://cvlab-kaist.github.io/DDUSt3R/", "summary": "We address the task of 3D reconstruction in dynamic scenes, where object\nmotions degrade the quality of previous 3D pointmap regression methods, such as\nDUSt3R, originally designed for static 3D scene reconstruction. Although these\nmethods provide an elegant and powerful solution in static settings, they\nstruggle in the presence of dynamic motions that disrupt alignment based solely\non camera poses. To overcome this, we propose D^2USt3R that regresses 4D\npointmaps that simultaneiously capture both static and dynamic 3D scene\ngeometry in a feed-forward manner. By explicitly incorporating both spatial and\ntemporal aspects, our approach successfully encapsulates spatio-temporal dense\ncorrespondence to the proposed 4D pointmaps, enhancing downstream tasks.\nExtensive experimental evaluations demonstrate that our proposed approach\nconsistently achieves superior reconstruction performance across various\ndatasets featuring complex motions."}
{"id": "2504.05316", "pdf": "https://arxiv.org/pdf/2504.05316", "abs": "https://arxiv.org/abs/2504.05316", "authors": ["Yinan Zhou", "Yaxiong Wang", "Haokun Lin", "Chen Ma", "Li Zhu", "Zhedong Zheng"], "title": "Scale Up Composed Image Retrieval Learning via Modification Text Generation", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "12 pages, 8 figures", "summary": "Composed Image Retrieval (CIR) aims to search an image of interest using a\ncombination of a reference image and modification text as the query. Despite\nrecent advancements, this task remains challenging due to limited training data\nand laborious triplet annotation processes. To address this issue, this paper\nproposes to synthesize the training triplets to augment the training resource\nfor the CIR problem. Specifically, we commence by training a modification text\ngenerator exploiting large-scale multimodal models and scale up the CIR\nlearning throughout both the pretraining and fine-tuning stages. During\npretraining, we leverage the trained generator to directly create Modification\nText-oriented Synthetic Triplets(MTST) conditioned on pairs of images. For\nfine-tuning, we first synthesize reverse modification text to connect the\ntarget image back to the reference image. Subsequently, we devise a two-hop\nalignment strategy to incrementally close the semantic gap between the\nmultimodal pair and the target image. We initially learn an implicit prototype\nutilizing both the original triplet and its reversed version in a cycle manner,\nfollowed by combining the implicit prototype feature with the modification text\nto facilitate accurate alignment with the target image. Extensive experiments\nvalidate the efficacy of the generated triplets and confirm that our proposed\nmethodology attains competitive recall on both the CIRR and FashionIQ\nbenchmarks."}
{"id": "2504.05342", "pdf": "https://arxiv.org/pdf/2504.05342", "abs": "https://arxiv.org/abs/2504.05342", "authors": ["Donato Crisostomi", "Alessandro Zirilli", "Antonio Andrea Gargiulo", "Maria Sofia Bucarelli", "Simone Scardapane", "Fabrizio Silvestri", "Iacopo Masi", "Emanuele Rodolà"], "title": "MASS: MoErging through Adaptive Subspace Selection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Model merging has recently emerged as a lightweight alternative to\nensembling, combining multiple fine-tuned models into a single set of\nparameters with no additional training overhead. Yet, existing merging methods\nfall short of matching the full accuracy of separately fine-tuned endpoints. We\npresent MASS (MoErging through Adaptive Subspace Selection), a new approach\nthat closes this gap by unifying multiple fine-tuned models while retaining\nnear state-of-the-art performance across tasks. Building on the low-rank\ndecomposition of per-task updates, MASS stores only the most salient singular\ncomponents for each task and merges them into a shared model. At inference\ntime, a non-parametric, data-free router identifies which subspace (or\ncombination thereof) best explains an input's intermediate features and\nactivates the corresponding task-specific block. This procedure is fully\ntraining-free and introduces only a two-pass inference overhead plus a ~2\nstorage factor compared to a single pretrained model, irrespective of the\nnumber of tasks. We evaluate MASS on CLIP-based image classification using\nViT-B-16, ViT-B-32 and ViT-L-14 for benchmarks of 8, 14 and 20 tasks\nrespectively, establishing a new state-of-the-art. Most notably, MASS recovers\nup to ~98% of the average accuracy of individual fine-tuned models, making it a\npractical alternative to ensembling at a fraction of the storage cost."}
{"id": "2504.05365", "pdf": "https://arxiv.org/pdf/2504.05365", "abs": "https://arxiv.org/abs/2504.05365", "authors": ["Shan Suthaharan"], "title": "A Nature-Inspired Colony of Artificial Intelligence System with Fast, Detailed, and Organized Learner Agents for Enhancing Diversity and Quality", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "12 pages, 8 figures", "summary": "The concepts of convolutional neural networks (CNNs) and multi-agent systems\nare two important areas of research in artificial intelligence (AI). In this\npaper, we present an approach that builds a CNN-based colony of AI agents to\nserve as a single system and perform multiple tasks (e.g., predictions or\nclassifications) in an environment. The proposed system impersonates the\nnatural environment of a biological system, like an ant colony or a human\ncolony. The proposed colony of AI that is defined as a role-based system\nuniquely contributes to accomplish tasks in an environment by incorporating AI\nagents that are fast learners, detailed learners, and organized learners. These\nlearners can enhance their localized learning and their collective decisions as\na single system of colony of AI agents. This approach also enhances the\ndiversity and quality of the colony of AI with the help of Genetic Algorithms\nand their crossover and mutation mechanisms. The evolution of fast, detailed,\nand organized learners in the colony of AI is achieved by introducing a unique\none-to-one mapping between these learners and the pretrained VGG16, VGG19, and\nResNet50 models, respectively. This role-based approach creates two parent-AI\nagents using the AI models through the processes, called the intra- and\ninter-marriage of AI, so that they can share their learned knowledge (weights\nand biases) based on a probabilistic rule and produce diversified child-AI\nagents to perform new tasks. This process will form a colony of AI that\nconsists of families of multi-model and mixture-model AI agents to improve\ndiversity and quality. Simulations show that the colony of AI, built using the\nVGG16, VGG19, and ResNet50 models, can provide a single system that generates\nchild-AI agents of excellent predictive performance, ranging between 82% and\n95% of F1-scores, to make diversified collective and quality decisions on a\ntask."}
{"id": "2504.05403", "pdf": "https://arxiv.org/pdf/2504.05403", "abs": "https://arxiv.org/abs/2504.05403", "authors": ["Manahil Raza", "Muhammad Dawood", "Talha Qaiser", "Nasir M. Rajpoot"], "title": "A Novel Approach to Linking Histology Images with DNA Methylation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "DNA methylation is an epigenetic mechanism that regulates gene expression by\nadding methyl groups to DNA. Abnormal methylation patterns can disrupt gene\nexpression and have been linked to cancer development. To quantify DNA\nmethylation, specialized assays are typically used. However, these assays are\noften costly and have lengthy processing times, which limits their widespread\navailability in routine clinical practice. In contrast, whole slide images\n(WSIs) for the majority of cancer patients can be more readily available. As\nsuch, given the ready availability of WSIs, there is a compelling need to\nexplore the potential relationship between WSIs and DNA methylation patterns.\nTo address this, we propose an end-to-end graph neural network based weakly\nsupervised learning framework to predict the methylation state of gene groups\nexhibiting coherent patterns across samples. Using data from three cohorts from\nThe Cancer Genome Atlas (TCGA) - TCGA-LGG (Brain Lower Grade Glioma), TCGA-GBM\n(Glioblastoma Multiforme) ($n$=729) and TCGA-KIRC (Kidney Renal Clear Cell\nCarcinoma) ($n$=511) - we demonstrate that the proposed approach achieves\nsignificantly higher AUROC scores than the state-of-the-art (SOTA) methods, by\nmore than $20\\%$. We conduct gene set enrichment analyses on the gene groups\nand show that majority of the gene groups are significantly enriched in\nimportant hallmarks and pathways. We also generate spatially enriched heatmaps\nto further investigate links between histological patterns and DNA methylation\nstates. To the best of our knowledge, this is the first study that explores\nassociation of spatially resolved histological patterns with gene group\nmethylation states across multiple cancer types using weakly supervised deep\nlearning."}
{"id": "2504.05562", "pdf": "https://arxiv.org/pdf/2504.05562", "abs": "https://arxiv.org/abs/2504.05562", "authors": ["Bartlomiej Wronski", "Matt Pharr", "Tomas Akenine-Möller"], "title": "Improved Stochastic Texture Filtering Through Sample Reuse", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to 2025 ACM SIGGRAPH Symposium on Interactive 3D Graphics\n  and Games (I3D 2025)", "summary": "Stochastic texture filtering (STF) has re-emerged as a technique that can\nbring down the cost of texture filtering of advanced texture compression\nmethods, e.g., neural texture compression. However, during texture\nmagnification, the swapped order of filtering and shading with STF can result\nin aliasing. The inability to smoothly interpolate material properties stored\nin textures, such as surface normals, leads to potentially undesirable\nappearance changes. We present a novel method to improve the quality of\nstochastically-filtered magnified textures and reduce the image difference\ncompared to traditional texture filtering. When textures are magnified, nearby\npixels filter similar sets of texels and we introduce techniques for sharing\ntexel values among pixels with only a small increase in cost (0.04--0.14~ms per\nframe). We propose an improvement to weighted importance sampling that\nguarantees that our method never increases error beyond single-sample\nstochastic texture filtering. Under high magnification, our method has >10 dB\nhigher PSNR than single-sample STF. Our results show greatly improved image\nquality both with and without spatiotemporal denoising."}
{"id": "2504.05576", "pdf": "https://arxiv.org/pdf/2504.05576", "abs": "https://arxiv.org/abs/2504.05576", "authors": ["Mingfei Chen", "Israel D. Gebru", "Ishwarya Ananthabhotla", "Christian Richardt", "Dejan Markovic", "Jake Sandakly", "Steven Krenn", "Todd Keebler", "Eli Shlizerman", "Alexander Richard"], "title": "SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "comment": "Highlight Accepted to CVPR 2025", "summary": "We introduce SoundVista, a method to generate the ambient sound of an\narbitrary scene at novel viewpoints. Given a pre-acquired recording of the\nscene from sparsely distributed microphones, SoundVista can synthesize the\nsound of that scene from an unseen target viewpoint. The method learns the\nunderlying acoustic transfer function that relates the signals acquired at the\ndistributed microphones to the signal at the target viewpoint, using a limited\nnumber of known recordings. Unlike existing works, our method does not require\nconstraints or prior knowledge of sound source details. Moreover, our method\nefficiently adapts to diverse room layouts, reference microphone configurations\nand unseen environments. To enable this, we introduce a visual-acoustic binding\nmodule that learns visual embeddings linked with local acoustic properties from\npanoramic RGB and depth data. We first leverage these embeddings to optimize\nthe placement of reference microphones in any given scene. During synthesis, we\nleverage multiple embeddings extracted from reference locations to get adaptive\nweights for their contribution, conditioned on target viewpoint. We benchmark\nthe task on both publicly available data and real-world settings. We\ndemonstrate significant improvements over existing methods."}
{"id": "2504.05591", "pdf": "https://arxiv.org/pdf/2504.05591", "abs": "https://arxiv.org/abs/2504.05591", "authors": ["Peter D. Erickson", "Tejas Sudharshan Mathai", "Ronald M. Summers"], "title": "Class Imbalance Correction for Improved Universal Lesion Detection and Tagging in CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at MICCAI MILLAND Workshop 2022", "summary": "Radiologists routinely detect and size lesions in CT to stage cancer and\nassess tumor burden. To potentially aid their efforts, multiple lesion\ndetection algorithms have been developed with a large public dataset called\nDeepLesion (32,735 lesions, 32,120 CT slices, 10,594 studies, 4,427 patients, 8\nbody part labels). However, this dataset contains missing measurements and\nlesion tags, and exhibits a severe imbalance in the number of lesions per label\ncategory. In this work, we utilize a limited subset of DeepLesion (6\\%, 1331\nlesions, 1309 slices) containing lesion annotations and body part label tags to\ntrain a VFNet model to detect lesions and tag them. We address the class\nimbalance by conducting three experiments: 1) Balancing data by the body part\nlabels, 2) Balancing data by the number of lesions per patient, and 3)\nBalancing data by the lesion size. In contrast to a randomly sampled\n(unbalanced) data subset, our results indicated that balancing the body part\nlabels always increased sensitivity for lesions >= 1cm for classes with low\ndata quantities (Bone: 80\\% vs. 46\\%, Kidney: 77\\% vs. 61\\%, Soft Tissue: 70\\%\nvs. 60\\%, Pelvis: 83\\% vs. 76\\%). Similar trends were seen for three other\nmodels tested (FasterRCNN, RetinaNet, FoveaBox). Balancing data by lesion size\nalso helped the VFNet model improve recalls for all classes in contrast to an\nunbalanced dataset. We also provide a structured reporting guideline for a\n``Lesions'' subsection to be entered into the ``Findings'' section of a\nradiology report. To our knowledge, we are the first to report the class\nimbalance in DeepLesion, and have taken data-driven steps to address it in the\ncontext of joint lesion detection and tagging."}
{"id": "2504.05604", "pdf": "https://arxiv.org/pdf/2504.05604", "abs": "https://arxiv.org/abs/2504.05604", "authors": ["Jihoon Kim", "Namwoo Kang"], "title": "PyTopo3D: A Python Framework for 3D SIMP-based Topology Optimization", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Three-dimensional topology optimization (TO) is a powerful technique in\nengineering design, but readily usable, open-source implementations remain\nlimited within the popular Python scientific environment. This paper introduces\nPyTopo3D, a software framework developed to address this gap. PyTopo3D provides\na feature-rich tool for 3D TO by implementing the well-established Solid\nIsotropic Material with Penalization (SIMP) method and an Optimality Criteria\n(OC) update scheme, adapted and significantly enhanced from the efficient\nMATLAB code by Liu and Tovar (2014). While building on proven methodology,\nPyTopo3D's primary contribution is its integration and extension within Python,\nleveraging sparse matrix operations, optional parallel solvers, and accelerated\nKD-Tree sensitivity filtering for performance. Crucially, it incorporates\nfunctionalities vital for practical engineering workflows, including the direct\nimport of complex design domains and non-design obstacles via STL files,\nintegrated 3D visualization of the optimization process, and direct STL export\nof optimized geometries for manufacturing or further analysis. PyTopo3D is\npresented as an accessible, performance-aware tool and citable reference\ndesigned to empower engineers, students, and researchers to more easily utilize\n3D TO within their existing Python-based workflows."}
{"id": "2504.05618", "pdf": "https://arxiv.org/pdf/2504.05618", "abs": "https://arxiv.org/abs/2504.05618", "authors": ["Jiawei Duan", "Haibo Hu", "Qingqing Ye", "Xinyue Sun"], "title": "Technical Report: Full Version of Analyzing and Optimizing Perturbation of DP-SGD Geometrically", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DB"], "comment": "This is the full version of our paper \"Analyzing and Optimizing\n  Perturbation of DP-SGD Geometrically\", which will appear in ICDE 2025 as a\n  regular research paper", "summary": "Differential privacy (DP) has become a prevalent privacy model in a wide\nrange of machine learning tasks, especially after the debut of DP-SGD. However,\nDP-SGD, which directly perturbs gradients in the training iterations, fails to\nmitigate the negative impacts of noise on gradient direction. As a result,\nDP-SGD is often inefficient. Although various solutions (e.g., clipping to\nreduce the sensitivity of gradients and amplifying privacy bounds to save\nprivacy budgets) are proposed to trade privacy for model efficiency, the root\ncause of its inefficiency is yet unveiled.\n  In this work, we first generalize DP-SGD and theoretically derive the impact\nof DP noise on the training process. Our analysis reveals that, in terms of a\nperturbed gradient, only the noise on direction has eminent impact on the model\nefficiency while that on magnitude can be mitigated by optimization techniques,\ni.e., fine-tuning gradient clipping and learning rate. Besides, we confirm that\ntraditional DP introduces biased noise on the direction when adding unbiased\nnoise to the gradient itself. Overall, the perturbation of DP-SGD is actually\nsub-optimal from a geometric perspective. Motivated by this, we design a\ngeometric perturbation strategy GeoDP within the DP framework, which perturbs\nthe direction and the magnitude of a gradient, respectively. By directly\nreducing the noise on the direction, GeoDP mitigates the negative impact of DP\nnoise on model efficiency with the same DP guarantee. Extensive experiments on\ntwo public datasets (i.e., MNIST and CIFAR-10), one synthetic dataset and three\nprevalent models (i.e., Logistic Regression, CNN and ResNet) confirm the\neffectiveness and generality of our strategy."}
{"id": "2504.05636", "pdf": "https://arxiv.org/pdf/2504.05636", "abs": "https://arxiv.org/abs/2504.05636", "authors": ["Jungkyu Park", "Jan Witowski", "Yanqi Xu", "Hari Trivedi", "Judy Gichoya", "Beatrice Brown-Mulry", "Malte Westerhoff", "Linda Moy", "Laura Heacock", "Alana Lewin", "Krzysztof J. Geras"], "title": "A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Although digital breast tomosynthesis (DBT) improves diagnostic performance\nover full-field digital mammography (FFDM), false-positive recalls remain a\nconcern in breast cancer screening. We developed a multi-modal artificial\nintelligence system integrating FFDM, synthetic mammography, and DBT to provide\nbreast-level predictions and bounding-box localizations of suspicious findings.\nOur AI system, trained on approximately 500,000 mammography exams, achieved\n0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls\nby 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity,\nunderscoring its potential to improve clinical workflows. External validation\nconfirmed strong generalizability, reducing the gap to a perfect AUROC by\n35.31%-69.14% relative to strong baselines. In prospective deployment across 18\nsites, the system reduced recall rates for low-risk cases. An improved version,\ntrained on over 750,000 exams with additional labels, further reduced the gap\nby 18.86%-56.62% across large external datasets. Overall, these results\nunderscore the importance of utilizing all available imaging modalities,\ndemonstrate the potential for clinical impact, and indicate feasibility of\nfurther reduction of the test error with increased training set when using\nlarge-capacity neural networks."}
{"id": "2504.05651", "pdf": "https://arxiv.org/pdf/2504.05651", "abs": "https://arxiv.org/abs/2504.05651", "authors": ["Narine Kokhlikyan", "Bargav Jayaraman", "Florian Bordes", "Chuan Guo", "Kamalika Chaudhuri"], "title": "Measuring Déjà vu Memorization Efficiently", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent research has shown that representation learning models may\naccidentally memorize their training data. For example, the d\\'ej\\`a vu method\nshows that for certain representation learning models and training images, it\nis sometimes possible to correctly predict the foreground label given only the\nrepresentation of the background - better than through dataset-level\ncorrelations. However, their measurement method requires training two models -\none to estimate dataset-level correlations and the other to estimate\nmemorization. This multiple model setup becomes infeasible for large\nopen-source models. In this work, we propose alternative simple methods to\nestimate dataset-level correlations, and show that these can be used to\napproximate an off-the-shelf model's memorization ability without any\nretraining. This enables, for the first time, the measurement of memorization\nin pre-trained open-source image representation and vision-language\nrepresentation models. Our results show that different ways of measuring\nmemorization yield very similar aggregate results. We also find that\nopen-source models typically have lower aggregate memorization than similar\nmodels trained on a subset of the data. The code is available both for vision\nand vision language models."}
{"id": "2504.05684", "pdf": "https://arxiv.org/pdf/2504.05684", "abs": "https://arxiv.org/abs/2504.05684", "authors": ["Tri Ton", "Ji Woo Hong", "Chang D. Yoo"], "title": "TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV"], "comment": "10 pages, 6 figures", "summary": "This paper introduces Timestep-Adaptive Representation Alignment with\nOnset-Aware Conditioning (TARO), a novel framework for high-fidelity and\ntemporally coherent video-to-audio synthesis. Built upon flow-based\ntransformers, which offer stable training and continuous transformations for\nenhanced synchronization and audio quality, TARO introduces two key\ninnovations: (1) Timestep-Adaptive Representation Alignment (TRA), which\ndynamically aligns latent representations by adjusting alignment strength based\non the noise schedule, ensuring smooth evolution and improved fidelity, and (2)\nOnset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp\nevent-driven markers of audio-relevant visual moments to enhance\nsynchronization with dynamic visual events. Extensive experiments on the\nVGGSound and Landscape datasets demonstrate that TARO outperforms prior\nmethods, achieving relatively 53\\% lower Frechet Distance (FD), 29% lower\nFrechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its\nsuperior audio quality and synchronization precision."}
{"id": "2504.05692", "pdf": "https://arxiv.org/pdf/2504.05692", "abs": "https://arxiv.org/abs/2504.05692", "authors": ["Songyan Zhang", "Yongtao Ge", "Jinyuan Tian", "Guangkai Xu", "Hao Chen", "Chen Lv", "Chunhua Shen"], "title": "POMATO: Marrying Pointmap Matching with Temporal Motion for Dynamic 3D Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "code: https://github.com/wyddmw/POMATO", "summary": "3D reconstruction in dynamic scenes primarily relies on the combination of\ngeometry estimation and matching modules where the latter task is pivotal for\ndistinguishing dynamic regions which can help to mitigate the interference\nintroduced by camera and object motion. Furthermore, the matching module\nexplicitly models object motion, enabling the tracking of specific targets and\nadvancing motion understanding in complex scenarios. Recently, the proposed\nrepresentation of pointmap in DUSt3R suggests a potential solution to unify\nboth geometry estimation and matching in 3D space, but it still struggles with\nambiguous matching in dynamic regions, which may hamper further improvement. In\nthis work, we present POMATO, a unified framework for dynamic 3D reconstruction\nby marrying pointmap matching with temporal motion. Specifically, our method\nfirst learns an explicit matching relationship by mapping RGB pixels from both\ndynamic and static regions across different views to 3D pointmaps within a\nunified coordinate system. Furthermore, we introduce a temporal motion module\nfor dynamic motions that ensures scale consistency across different frames and\nenhances performance in tasks requiring both precise geometry and reliable\nmatching, most notably 3D point tracking. We show the effectiveness of the\nproposed pointmap matching and temporal fusion paradigm by demonstrating the\nremarkable performance across multiple downstream tasks, including video depth\nestimation, 3D point tracking, and pose estimation. Code and models are\npublicly available at https://github.com/wyddmw/POMATO."}
{"id": "2504.05696", "pdf": "https://arxiv.org/pdf/2504.05696", "abs": "https://arxiv.org/abs/2504.05696", "authors": ["Sidhiq Mardianta", "Affandy", "Catur Supriyanto", "Catur Supriyanto", "Adi Wijaya"], "title": "Diabetic Retinopathy Detection Based on Convolutional Neural Networks with SMOTE and CLAHE Techniques Applied to Fundus Images", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.NC"], "comment": "6 pages, 6 figures, 2 tables", "summary": "Diabetic retinopathy (DR) is one of the major complications in diabetic\npatients' eyes, potentially leading to permanent blindness if not detected\ntimely. This study aims to evaluate the accuracy of artificial intelligence\n(AI) in diagnosing DR. The method employed is the Synthetic Minority\nOver-sampling Technique (SMOTE) algorithm, applied to identify DR and its\nseverity stages from fundus images using the public dataset \"APTOS 2019\nBlindness Detection.\" Literature was reviewed via ScienceDirect, ResearchGate,\nGoogle Scholar, and IEEE Xplore. Classification results using Convolutional\nNeural Network (CNN) showed the best performance for the binary classes normal\n(0) and DR (1) with an accuracy of 99.55%, precision of 99.54%, recall of\n99.54%, and F1-score of 99.54%. For the multiclass classification No_DR (0),\nMild (1), Moderate (2), Severe (3), Proliferate_DR (4), the accuracy was\n95.26%, precision 95.26%, recall 95.17%, and F1-score 95.23%. Evaluation using\nthe confusion matrix yielded results of 99.68% for binary classification and\n96.65% for multiclass. This study highlights the significant potential in\nenhancing the accuracy of DR diagnosis compared to traditional human analysis"}
{"id": "2504.05740", "pdf": "https://arxiv.org/pdf/2504.05740", "abs": "https://arxiv.org/abs/2504.05740", "authors": ["Jee Won Lee", "Hansol Lim", "Sooyeun Yang", "Jongseong Choi"], "title": "Micro-splatting: Maximizing Isotropic Constraints for Refined Optimization in 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting have achieved impressive\nscalability and real-time rendering for large-scale scenes but often fall short\nin capturing fine-grained details. Conventional approaches that rely on\nrelatively large covariance parameters tend to produce blurred representations,\nwhile directly reducing covariance sizes leads to sparsity. In this work, we\nintroduce Micro-splatting (Maximizing Isotropic Constraints for Refined\nOptimization in 3D Gaussian Splatting), a novel framework designed to overcome\nthese limitations. Our approach leverages a covariance regularization term to\npenalize excessively large Gaussians to ensure each splat remains compact and\nisotropic. This work implements an adaptive densification strategy that\ndynamically refines regions with high image gradients by lowering the splitting\nthreshold, followed by loss function enhancement. This strategy results in a\ndenser and more detailed gaussian means where needed, without sacrificing\nrendering efficiency. Quantitative evaluations using metrics such as L1, L2,\nPSNR, SSIM, and LPIPS, alongside qualitative comparisons demonstrate that our\nmethod significantly enhances fine-details in 3D reconstructions."}
{"id": "2504.05803", "pdf": "https://arxiv.org/pdf/2504.05803", "abs": "https://arxiv.org/abs/2504.05803", "authors": ["Yihuan Huang", "Jiajun Liu", "Yanzhen Ren", "Wuyang Liu", "Juhua Tang"], "title": "SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve Phoneme-Viseme Alignment Ambiguity", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Speech-driven talking head synthesis tasks commonly use general acoustic\nfeatures (such as HuBERT and DeepSpeech) as guided speech features. However, we\ndiscovered that these features suffer from phoneme-viseme alignment ambiguity,\nwhich refers to the uncertainty and imprecision in matching phonemes (speech)\nwith visemes (lip). To address this issue, we propose the Speech Encoder for\nLip (SE4Lip) to encode lip features from speech directly, aligning speech and\nlip features in the joint embedding space by a cross-modal alignment framework.\nThe STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve\nthe fine-grained speech features. Experimental results show that SE4Lip\nachieves state-of-the-art performance in both NeRF and 3DGS rendering models.\nIts lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline\nand produces results close to the ground truth videos."}
{"id": "2504.05878", "pdf": "https://arxiv.org/pdf/2504.05878", "abs": "https://arxiv.org/abs/2504.05878", "authors": ["Xingyuan Li", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "title": "KAN-SAM: Kolmogorov-Arnold Network Guided Segment Anything Model for RGB-T Salient Object Detection", "categories": ["cs.MM", "cs.CV"], "comment": "This paper is accepted by ICME2025", "summary": "Existing RGB-thermal salient object detection (RGB-T SOD) methods aim to\nidentify visually significant objects by leveraging both RGB and thermal\nmodalities to enable robust performance in complex scenarios, but they often\nsuffer from limited generalization due to the constrained diversity of\navailable datasets and the inefficiencies in constructing multi-modal\nrepresentations. In this paper, we propose a novel prompt learning-based RGB-T\nSOD method, named KAN-SAM, which reveals the potential of visual foundational\nmodels for RGB-T SOD tasks. Specifically, we extend Segment Anything Model 2\n(SAM2) for RGB-T SOD by introducing thermal features as guiding prompts through\nefficient and accurate Kolmogorov-Arnold Network (KAN) adapters, which\neffectively enhance RGB representations and improve robustness. Furthermore, we\nintroduce a mutually exclusive random masking strategy to reduce reliance on\nRGB data and improve generalization. Experimental results on benchmarks\ndemonstrate superior performance over the state-of-the-art methods."}
{"id": "2504.05888", "pdf": "https://arxiv.org/pdf/2504.05888", "abs": "https://arxiv.org/abs/2504.05888", "authors": ["Guillaume Gautier", "Alexandre Mercat", "Louis Fréneau", "Mikko Pitkänen", "Jarno Vanne"], "title": "UVG-VPC: Voxelized Point Cloud Dataset for Visual Volumetric Video-based Coding", "categories": ["cs.MM", "cs.CV"], "comment": "Point cloud compression;Geometry;Visualization;Three-dimensional\n  displays;Video sequences;Transform coding;Media;Open dataset;point\n  cloud;Visual Volumetric Video-based Coding (V3C);Video-based Point Cloud\n  Compression (V-PCC);Extended Reality (XR)", "summary": "Point cloud compression has become a crucial factor in immersive visual media\nprocessing and streaming. This paper presents a new open dataset called UVG-VPC\nfor the development, evaluation, and validation of MPEG Visual Volumetric\nVideo-based Coding (V3C) technology. The dataset is distributed under its own\nnon-commercial license. It consists of 12 point cloud test video sequences of\ndiverse characteristics with respect to the motion, RGB texture, 3D geometry,\nand surface occlusion of the points. Each sequence is 10 seconds long and\ncomprises 250 frames captured at 25 frames per second. The sequences are\nvoxelized with a geometry precision of 9 to 12 bits, and the voxel color\nattributes are represented as 8-bit RGB values. The dataset also includes\nassociated normals that make it more suitable for evaluating point cloud\ncompression solutions. The main objective of releasing the UVG-VPC dataset is\nto foster the development of V3C technologies and thereby shape the future in\nthis field."}
{"id": "2504.05945", "pdf": "https://arxiv.org/pdf/2504.05945", "abs": "https://arxiv.org/abs/2504.05945", "authors": ["Kuntian Zhang", "Simin Yu", "Yaoshu Wang", "Makoto Onizuka", "Chuan Xiao"], "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Source codes are available at https://github.com/chuanxiao1983/CKGAN/", "summary": "In this paper, we propose CKGAN, a novel generative adversarial network (GAN)\nvariant based on an integral probability metrics framework with characteristic\nkernel (CKIPM). CKIPM, as a distance between two probability distributions, is\ndesigned to optimize the lowerbound of the maximum mean discrepancy (MMD) in a\nreproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN\nmitigates the notorious problem of mode collapse by mapping the generated\nimages back to random noise. To save the effort of selecting the kernel\nfunction manually, we propose a soft selection method to automatically learn a\ncharacteristic kernel function. The experimental evaluation conducted on a set\nof synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that\nCKGAN generally outperforms other MMD-based GANs. The results also show that at\nthe cost of moderately more training time, the automatically selected kernel\nfunction delivers very close performance to the best of manually fine-tuned one\non real image benchmarks and is able to improve the performances of other\nMMD-based GANs."}
{"id": "2504.05990", "pdf": "https://arxiv.org/pdf/2504.05990", "abs": "https://arxiv.org/abs/2504.05990", "authors": ["Heather M. Whitney", "Hui Li", "Karen Drukker", "Elbert Huang", "Maryellen L. Giger"], "title": "AI analysis of medical images at scale as a health disparities probe: a feasibility demonstration using chest radiographs", "categories": ["physics.med-ph", "cs.CV"], "comment": "21 pages, 4 figures", "summary": "Health disparities (differences in non-genetic conditions that influence\nhealth) can be associated with differences in burden of disease by groups\nwithin a population. Social determinants of health (SDOH) are domains such as\nhealth care access, dietary access, and economics frequently studied for\npotential association with health disparities. Evaluating SDOH-related\nphenotypes using routine medical images as data sources may enhance health\ndisparities research. We developed a pipeline for using quantitative measures\nautomatically extracted from medical images as inputs into health disparities\nindex calculations. Our study focused on the use case of two SDOH demographic\ncorrelates (sex and race) and data extracted from chest radiographs of 1,571\nunique patients. The likelihood of severe disease within the lung parenchyma\nfrom each image type, measured using an established deep learning model, was\nmerged into a single numerical image-based phenotype for each patient. Patients\nwere then separated into phenogroups by unsupervised clustering of the\nimage-based phenotypes. The health rate for each phenogroup was defined as the\nmedian image-based phenotype for each SDOH used as inputs to four\nimaging-derived health disparities indices (iHDIs): one absolute measure\n(between-group variance) and three relative measures (index of disparity, Theil\nindex, and mean log deviation). The iHDI measures demonstrated feasible values\nfor each SDOH demographic correlate, showing potential for medical images to\nserve as a novel probe for health disparities. Large-scale AI analysis of\nmedical images can serve as a probe for a novel data source for health\ndisparities research."}
{"id": "2504.06027", "pdf": "https://arxiv.org/pdf/2504.06027", "abs": "https://arxiv.org/abs/2504.06027", "authors": ["Xiaochen Wei", "Weiwei Guo", "Wenxian Yu", "Feiming Wei", "Dongying Li"], "title": "OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Multimodal remote sensing image registration aligns images from different\nsensors for data fusion and analysis. However, current methods often fail to\nextract modality-invariant features when aligning image pairs with large\nnonlinear radiometric differences. To address this issues, we propose\nOSDM-MReg, a novel multimodal image registration framework based image-to-image\ntranslation to eliminate the gap of multimodal images. Firstly, we propose a\nnovel one-step unaligned target-guided conditional denoising diffusion\nprobabilistic models(UTGOS-CDDPM)to translate multimodal images into a unified\ndomain. In the inference stage, traditional conditional DDPM generate\ntranslated source image by a large number of iterations, which severely slows\ndown the image registration task. To address this issues, we use the unaligned\ntraget image as a condition to promote the generation of low-frequency features\nof the translated source image. Furthermore, during the training stage, we add\nthe inverse process of directly predicting the translated image to ensure that\nthe translated source image can be generated in one step during the testing\nstage. Additionally, to supervised the detail features of translated source\nimage, we propose a new perceptual loss that focuses on the high-frequency\nfeature differences between the translated and ground-truth images. Finally, a\nmultimodal multiscale image registration network (MM-Reg) fuse the multimodal\nfeature of the unimodal images and multimodal images by proposed multimodal\nfeature fusion strategy. Experiments demonstrate superior accuracy and\nefficiency across various multimodal registration tasks, particularly for\nSAR-optical image pairs."}
{"id": "2504.06084", "pdf": "https://arxiv.org/pdf/2504.06084", "abs": "https://arxiv.org/abs/2504.06084", "authors": ["Alexey Gavryushin", "Xi Wang", "Robert J. S. Malate", "Chenyu Yang", "Xiangyi Jia", "Shubh Goel", "Davide Liconti", "René Zurbrügg", "Robert K. Katzschmann", "Marc Pollefeys"], "title": "MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Large-scale egocentric video datasets capture diverse human activities across\na wide range of scenarios, offering rich and detailed insights into how humans\ninteract with objects, especially those that require fine-grained dexterous\ncontrol. Such complex, dexterous skills with precise controls are crucial for\nmany robotic manipulation tasks, yet are often insufficiently addressed by\ntraditional data-driven approaches to robotic manipulation. To address this\ngap, we leverage manipulation priors learned from large-scale egocentric video\ndatasets to improve policy learning for dexterous robotic manipulation tasks.\nWe present MAPLE, a novel method for dexterous robotic manipulation that\nexploits rich manipulation priors to enable efficient policy learning and\nbetter performance on diverse, complex manipulation tasks. Specifically, we\npredict hand-object contact points and detailed hand poses at the moment of\nhand-object contact and use the learned features to train policies for\ndownstream manipulation tasks. Experimental results demonstrate the\neffectiveness of MAPLE across existing simulation benchmarks, as well as a\nnewly designed set of challenging simulation tasks, which require fine-grained\nobject control and complex dexterous skills. The benefits of MAPLE are further\nhighlighted in real-world experiments using a dexterous robotic hand, whereas\nsimultaneous evaluation across both simulation and real-world experiments has\nremained underexplored in prior work."}
