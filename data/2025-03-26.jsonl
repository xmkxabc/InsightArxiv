{"id": "2503.18991", "pdf": "https://arxiv.org/pdf/2503.18991", "abs": "https://arxiv.org/abs/2503.18991", "authors": ["Ruoxi Cheng", "Shuirong Cao"], "title": "SRMIR: Shadow Reward Models Based on Introspective Reasoning for LLM Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences and values is\nvital for application. However, current alignment methods face three main\nlimitations: (1) reliance on costly human annotation; (2) alignment tax; (3)\nshallow alignment vulnerable to jailbreak attacks. Additionally, current\nalignment datasets often suffer from uneven distributions, leading to\noverrepresentation of some topics and neglect of others. To address these\nissues, we propose SRMIR (Shadow Reward Models Based on Introspective\nReasoning), inspired by shadow models in membership inference attacks. We first\nconstruct a balanced safety Chain of Draft (CoD) dataset across $7$ harmful\ntypes with structured prompt leveraging the introspective reasoning\ncapabilities of LLMs, then train a set of specialized reward models to guide\npolicy optimization through Group Relative Policy Optimization (GRPO). We apply\ntwo strategies, linear combination and categorized approach, to integrate\nshadow reward models for policy optimization. By comparison, we find that the\nlatter achieves superior alignment despite higher computational costs.\nExperiments across several LLMs demonstrate SRMIR significantly outperforms\nexisting methods."}
{"id": "2503.19041", "pdf": "https://arxiv.org/pdf/2503.19041", "abs": "https://arxiv.org/abs/2503.19041", "authors": ["Kangwei Liu", "Mengru Wang", "Yujie Luo", "Lin Yuan", "Mengshu Sun", "Ningyu Zhang", "Lei Liang", "Zhiqiang Zhang", "Jun Zhou", "Huajun Chen"], "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning."}
{"id": "2503.19090", "pdf": "https://arxiv.org/pdf/2503.19090", "abs": "https://arxiv.org/abs/2503.19090", "authors": ["Varsha Embar", "Ritvik Shrivastava", "Vinay Damodaran", "Travis Mehlinger", "Yu-Chung Hsiao", "Karthik Raghunathan"], "title": "LLM-Based Insight Extraction for Contact Center Analytics and Cost-Efficient Deployment", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have transformed the Contact Center industry,\nmanifesting in enhanced self-service tools, streamlined administrative\nprocesses, and augmented agent productivity. This paper delineates our system\nthat automates call driver generation, which serves as the foundation for tasks\nsuch as topic modeling, incoming call classification, trend detection, and FAQ\ngeneration, delivering actionable insights for contact center agents and\nadministrators to consume. We present a cost-efficient LLM system design, with\n1) a comprehensive evaluation of proprietary, open-weight, and fine-tuned\nmodels and 2) cost-efficient strategies, and 3) the corresponding cost analysis\nwhen deployed in production environments."}
{"id": "2503.19099", "pdf": "https://arxiv.org/pdf/2503.19099", "abs": "https://arxiv.org/abs/2503.19099", "authors": ["Kenneth Alperin", "Rohan Leekha", "Adaku Uchendu", "Trang Nguyen", "Srilakshmi Medarametla", "Carlos Levya Capote", "Seth Aycock", "Charlie Dagli"], "title": "Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on Authorship Verification", "categories": ["cs.CL"], "comment": "Accepted at NLP4DH Workshop @ NAACL 2025", "summary": "The increasing use of Artificial Intelligence (AI) technologies, such as\nLarge Language Models (LLMs) has led to nontrivial improvements in various\ntasks, including accurate authorship identification of documents. However,\nwhile LLMs improve such defense techniques, they also simultaneously provide a\nvehicle for malicious actors to launch new attack vectors. To combat this\nsecurity risk, we evaluate the adversarial robustness of authorship models\n(specifically an authorship verification model) to potent LLM-based attacks.\nThese attacks include untargeted methods - \\textit{authorship obfuscation} and\ntargeted methods - \\textit{authorship impersonation}. For both attacks, the\nobjective is to mask or mimic the writing style of an author while preserving\nthe original texts' semantics, respectively. Thus, we perturb an accurate\nauthorship verification model, and achieve maximum attack success rates of 92\\%\nand 78\\% for both obfuscation and impersonation attacks, respectively."}
{"id": "2503.19114", "pdf": "https://arxiv.org/pdf/2503.19114", "abs": "https://arxiv.org/abs/2503.19114", "authors": ["Weronika Łajewska", "Momchil Hardalov", "Laura Aina", "Neha Anna John", "Hang Su", "Lluís Màrquez"], "title": "Understanding and Improving Information Preservation in Prompt Compression for LLMs", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "21 pages, 6 figures, 23 tables", "summary": "Recent advancements in large language models (LLMs) have enabled their\nsuccessful application to a broad range of tasks. However, in\ninformation-intensive tasks, the prompt length can grow fast, leading to\nincreased computational requirements, performance degradation, and induced\nbiases from irrelevant or redundant information. Recently, various prompt\ncompression techniques have been introduced to optimize the trade-off between\nreducing input length and retaining performance. We propose a holistic\nevaluation framework that allows for in-depth analysis of prompt compression\nmethods. We focus on three key aspects, besides compression ratio: (i)\ndownstream task performance, (ii) grounding in the input context, and (iii)\ninformation preservation. Through this framework, we investigate\nstate-of-the-art soft and hard compression methods, showing that they struggle\nto preserve key details from the original prompt, limiting their performance on\ncomplex tasks. We demonstrate that modifying soft prompting methods to control\nbetter the granularity of the compressed information can significantly improve\ntheir effectiveness -- up to +23\\% in downstream task performance, more than +8\nBERTScore points in grounding, and 2.7x more entities preserved in compression."}
{"id": "2503.19120", "pdf": "https://arxiv.org/pdf/2503.19120", "abs": "https://arxiv.org/abs/2503.19120", "authors": ["Armineh Nourbakhsh", "Siddharth Parekh", "Pranav Shetty", "Zhao Jin", "Sameena Shah", "Carolyn Rose"], "title": "Where is this coming from? Making groundedness count in the evaluation of Document VQA models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL Findings 2025", "summary": "Document Visual Question Answering (VQA) models have evolved at an impressive\nrate over the past few years, coming close to or matching human performance on\nsome benchmarks. We argue that common evaluation metrics used by popular\nbenchmarks do not account for the semantic and multimodal groundedness of a\nmodel's outputs. As a result, hallucinations and major semantic errors are\ntreated the same way as well-grounded outputs, and the evaluation scores do not\nreflect the reasoning capabilities of the model. In response, we propose a new\nevaluation methodology that accounts for the groundedness of predictions with\nregard to the semantic characteristics of the output as well as the multimodal\nplacement of the output within the input document. Our proposed methodology is\nparameterized in such a way that users can configure the score according to\ntheir preferences. We validate our scoring methodology using human judgment and\nshow its potential impact on existing popular leaderboards. Through extensive\nanalyses, we demonstrate that our proposed method produces scores that are a\nbetter indicator of a model's robustness and tends to give higher rewards to\nbetter-calibrated answers."}
{"id": "2503.19123", "pdf": "https://arxiv.org/pdf/2503.19123", "abs": "https://arxiv.org/abs/2503.19123", "authors": ["Haebin Shin", "Lei Ji", "Xiao Liu", "Yeyun Gong"], "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Using large teacher models to guide the training of smaller student models\nhas become the prevailing paradigm for efficient and effective learning.\nHowever, vocabulary mismatches between teacher and student language models pose\nsignificant challenges in language modeling, resulting in divergent token\nsequences and output distributions. To overcome these limitations, we propose\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\napproach that bridges the gap caused by vocabulary mismatch through two key\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\nof teacher model to guide effective student training. We demonstrate its\neffectiveness in language modeling with 1B student model using various 7B\nteacher models with different vocabularies. Notably, with\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\nconsistently benefits from stronger teacher models, providing a robust solution\nto vocabulary mismatches in language modeling."}
{"id": "2503.19134", "pdf": "https://arxiv.org/pdf/2503.19134", "abs": "https://arxiv.org/abs/2503.19134", "authors": ["Wenhao You", "Bryan Hooi", "Yiwei Wang", "Youke Wang", "Zong Ke", "Ming-Hsuan Yang", "Zi Huang", "Yujun Cai"], "title": "MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "While safety mechanisms have significantly progressed in filtering harmful\ntext inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit\ntheir cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal\njailbreak framework that exploits narrative-driven context and role immersion\nto circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By\nsystematically decomposing the toxic query into environment, role, and action\ntriplets, MIRAGE constructs a multi-turn visual storytelling sequence of images\nand text using Stable Diffusion, guiding the target model through an engaging\ndetective narrative. This process progressively lowers the model's defences and\nsubtly guides its reasoning through structured contextual cues, ultimately\neliciting harmful responses. In extensive experiments on the selected datasets\nwith six mainstream MLLMs, MIRAGE achieves state-of-the-art performance,\nimproving attack success rates by up to 17.5% over the best baselines.\nMoreover, we demonstrate that role immersion and structured semantic\nreconstruction can activate inherent model biases, facilitating the model's\nspontaneous violation of ethical safeguards. These results highlight critical\nweaknesses in current multimodal safety mechanisms and underscore the urgent\nneed for more robust defences against cross-modal threats."}
{"id": "2503.19168", "pdf": "https://arxiv.org/pdf/2503.19168", "abs": "https://arxiv.org/abs/2503.19168", "authors": ["Yinghao Li", "Rushi Qiang", "Lama Moukheiber", "Chao Zhang"], "title": "Language Model Uncertainty Quantification with Attention Chain", "categories": ["cs.CL"], "comment": "33 pages, 7 figures, 30 tables", "summary": "Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. Similarity filtering and probability thresholding further refine the\nresulting chain, allowing us to approximate the marginal probabilities of the\nanswer tokens, which serve as the LLM's confidence. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency."}
{"id": "2503.19182", "pdf": "https://arxiv.org/pdf/2503.19182", "abs": "https://arxiv.org/abs/2503.19182", "authors": ["Hayate Iso", "Pouya Pezeshkpour", "Nikita Bhutani", "Estevam Hruschka"], "title": "Evaluating Bias in LLMs for Job-Resume Matching: Gender, Race, and Education", "categories": ["cs.CL"], "comment": "NAACL 2025: Industry Track", "summary": "Large Language Models (LLMs) offer the potential to automate hiring by\nmatching job descriptions with candidate resumes, streamlining recruitment\nprocesses, and reducing operational costs. However, biases inherent in these\nmodels may lead to unfair hiring practices, reinforcing societal prejudices and\nundermining workplace diversity. This study examines the performance and\nfairness of LLMs in job-resume matching tasks within the English language and\nU.S. context. It evaluates how factors such as gender, race, and educational\nbackground influence model decisions, providing critical insights into the\nfairness and reliability of LLMs in HR applications. Our findings indicate that\nwhile recent models have reduced biases related to explicit attributes like\ngender and race, implicit biases concerning educational background remain\nsignificant. These results highlight the need for ongoing evaluation and the\ndevelopment of advanced bias mitigation strategies to ensure equitable hiring\npractices when using LLMs in industry settings."}
{"id": "2503.19186", "pdf": "https://arxiv.org/pdf/2503.19186", "abs": "https://arxiv.org/abs/2503.19186", "authors": ["Parisa Mollaei", "Amir Barati Farimani"], "title": "Protein Structure-Function Relationship: A Kernel-PCA Approach for Reaction Coordinate Identification", "categories": ["cs.CL", "q-bio.QM"], "comment": "28 pages, 10 figures", "summary": "In this study, we propose a Kernel-PCA model designed to capture\nstructure-function relationships in a protein. This model also enables ranking\nof reaction coordinates according to their impact on protein properties. By\nleveraging machine learning techniques, including Kernel and principal\ncomponent analysis (PCA), our model uncovers meaningful patterns in\nhigh-dimensional protein data obtained from molecular dynamics (MD)\nsimulations. The effectiveness of our model in accurately identifying reaction\ncoordinates has been demonstrated through its application to a G\nprotein-coupled receptor. Furthermore, this model utilizes a network-based\napproach to uncover correlations in the dynamic behavior of residues associated\nwith a specific protein property. These findings underscore the potential of\nour model as a powerful tool for protein structure-function analysis and\nvisualization."}
{"id": "2503.19206", "pdf": "https://arxiv.org/pdf/2503.19206", "abs": "https://arxiv.org/abs/2503.19206", "authors": ["Jacob Mitchell Springer", "Sachin Goyal", "Kaiyue Wen", "Tanishq Kumar", "Xiang Yue", "Sadhika Malladi", "Graham Neubig", "Aditi Raghunathan"], "title": "Overtrained Language Models Are Harder to Fine-Tune", "categories": ["cs.CL", "cs.AI"], "comment": "72 pages, 65 figures, 6 tables", "summary": "Large language models are pre-trained on ever-growing token budgets under the\nassumption that better pre-training performance translates to improved\ndownstream models. In this work, we challenge this assumption and show that\nextended pre-training can make models harder to fine-tune, leading to degraded\nfinal performance. We term this phenomenon catastrophic overtraining. For\nexample, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to\nover 2% worse performance on multiple standard LLM benchmarks than its 2.3T\ntoken counterpart. Through controlled experiments and theoretical analysis, we\nshow that catastrophic overtraining arises from a systematic increase in the\nbroad sensitivity of pre-trained parameters to modifications, including but not\nlimited to fine-tuning. Our findings call for a critical reassessment of\npre-training design that considers the downstream adaptability of the model."}
{"id": "2503.19211", "pdf": "https://arxiv.org/pdf/2503.19211", "abs": "https://arxiv.org/abs/2503.19211", "authors": ["Mahdi Nasser", "Laura Sayyah", "Fadi A. Zaraket"], "title": "Towards Terminology Management Automation for Arabic", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a method and supporting tools for automation of\nterminology management for Arabic. The tools extract lists of parallel\nterminology matching terms in foreign languages to their Arabic counterparts\nfrom field specific texts. This has significant implications as it can be used\nto improve consistent translation and use of terms in specialized Arabic\nacademic books, and provides automated aid for enhancing cross lingual text\nprocessing. This automation of terminology management aims to reduce processing\ntime, and ensure use of consistent and correct terminology. The extraction\ntakes advantage of naturally occurring term translations. It considers several\ncandidate phrases of varying lengths that co-occur next to the foreign terms.\nThen it computes several similarity metrics, including lexicographic, phonetic,\nmorphological, and semantic ones to decide the problem. We experiment with\nheuristic, machine learning, and ML with post processing approaches. This paper\nreports on a novel curated dataset for the task, an existing expert reviewed\nindustry parallel corpora, and on the performance of the three approaches. The\nbest approach achieved 94.9% precision and 92.4% recall."}
{"id": "2503.19213", "pdf": "https://arxiv.org/pdf/2503.19213", "abs": "https://arxiv.org/abs/2503.19213", "authors": ["Murong Yue"], "title": "A Survey of Large Language Model Agents for Question Answering", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "This paper surveys the development of large language model (LLM)-based agents\nfor question answering (QA). Traditional agents face significant limitations,\nincluding substantial data requirements and difficulty in generalizing to new\nenvironments. LLM-based agents address these challenges by leveraging LLMs as\ntheir core reasoning engine. These agents achieve superior QA results compared\nto traditional QA pipelines and naive LLM QA systems by enabling interaction\nwith external environments. We systematically review the design of LLM agents\nin the context of QA tasks, organizing our discussion across key stages:\nplanning, question understanding, information retrieval, and answer generation.\nAdditionally, this paper identifies ongoing challenges and explores future\nresearch directions to enhance the performance of LLM agent QA systems."}
{"id": "2503.19257", "pdf": "https://arxiv.org/pdf/2503.19257", "abs": "https://arxiv.org/abs/2503.19257", "authors": ["Farhana Keya", "Gollam Rabby", "Prasenjit Mitra", "Sahar Vahdati", "Sören Auer", "Yaser Jaradeh"], "title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "Every scientific discovery starts with an idea inspired by prior work,\ninterdisciplinary concepts, and emerging challenges. Recent advancements in\nlarge language models (LLMs) trained on scientific corpora have driven interest\nin AI-supported idea generation. However, generating context-aware,\nhigh-quality, and innovative ideas remains challenging. We introduce SCI-IDEA,\na framework that uses LLM prompting strategies and Aha Moment detection for\niterative idea refinement. SCI-IDEA extracts essential facets from research\npublications, assessing generated ideas on novelty, excitement, feasibility,\nand effectiveness. Comprehensive experiments validate SCI-IDEA's effectiveness,\nachieving average scores of 6.84, 6.86, 6.89, and 6.84 (on a 1-10 scale) across\nnovelty, excitement, feasibility, and effectiveness, respectively. Evaluations\nemployed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and\nDeepSeek-70B (3-shot prompting), with token-level embeddings used for Aha\nMoment detection. Similarly, it achieves scores of 6.87, 6.86, 6.83, and 6.87\nusing GPT-4o under 5-shot prompting, GPT-4.5 under 3-shot prompting,\nDeepSeek-32B under zero-shot chain-of-thought prompting, and DeepSeek-70B under\n5-shot prompting with sentence-level embeddings. We also address ethical\nconsiderations such as intellectual credit, potential misuse, and balancing\nhuman creativity with AI-driven ideation. Our results highlight SCI-IDEA's\npotential to facilitate the structured and flexible exploration of\ncontext-aware scientific ideas, supporting innovation while maintaining ethical\nstandards."}
{"id": "2503.19260", "pdf": "https://arxiv.org/pdf/2503.19260", "abs": "https://arxiv.org/abs/2503.19260", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "Linguistic Blind Spots of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "NAACL 2025 Cognitive Modeling and Computational Linguistics Workshop", "summary": "Large language models (LLMs) are the foundation of many AI applications\ntoday. However, despite their remarkable proficiency in generating coherent\ntext, questions linger regarding their ability to perform fine-grained\nlinguistic annotation tasks, such as detecting nouns or verbs, or identifying\nmore complex syntactic structures like clauses in input texts. These tasks\nrequire precise syntactic and semantic understanding of input text, and when\nLLMs underperform on specific linguistic structures, it raises concerns about\ntheir reliability for detailed linguistic analysis and whether their (even\ncorrect) outputs truly reflect an understanding of the inputs. In this paper,\nwe empirically study the performance of recent LLMs on fine-grained linguistic\nannotation tasks. Through a series of experiments, we find that recent LLMs\nshow limited efficacy in addressing linguistic queries and often struggle with\nlinguistically complex inputs. We show that the most capable LLM (Llama3-70b)\nmakes notable errors in detecting linguistic structures, such as misidentifying\nembedded clauses, failing to recognize verb phrases, and confusing complex\nnominals with clauses. Our results provide insights to inform future\nadvancements in LLM design and development."}
{"id": "2503.19265", "pdf": "https://arxiv.org/pdf/2503.19265", "abs": "https://arxiv.org/abs/2503.19265", "authors": ["Sarah Pungitore", "Shashank Yadav", "Vignesh Subbian"], "title": "PHEONA: An Evaluation Framework for Large Language Model-based Approaches to Computational Phenotyping", "categories": ["cs.CL"], "comment": "2 figures, 5 tables, submitted to 2025 AMIA Annual Symposium", "summary": "Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes."}
{"id": "2503.19271", "pdf": "https://arxiv.org/pdf/2503.19271", "abs": "https://arxiv.org/abs/2503.19271", "authors": ["Xuechen Liang", "Meiling Tao", "Yinghui Xia", "Jianhui Wang", "Kun Li", "Yijin Wang", "Jingsong Yang", "Tianyu Shi", "Yuantao Wang", "Miao Zhang", "Xueqian Wang"], "title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information."}
{"id": "2503.19274", "pdf": "https://arxiv.org/pdf/2503.19274", "abs": "https://arxiv.org/abs/2503.19274", "authors": ["Junfeng Liu", "Christopher T. Symons", "Ranga Raju Vatsavai"], "title": "CoMAC: Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "The 29th Pacific-Asia Conference on Knowledge Discovery and Data\n  Mining (PAKDD2025)", "summary": "Recent advancements in AI-driven conversational agents have exhibited immense\npotential of AI applications. Effective response generation is crucial to the\nsuccess of these agents. While extensive research has focused on leveraging\nmultiple auxiliary data sources (e.g., knowledge bases and personas) to enhance\nresponse generation, existing methods often struggle to efficiently extract\nrelevant information from these sources. There are still clear limitations in\nthe ability to combine versatile conversational capabilities with adherence to\nknown facts and adaptation to large variations in user preferences and belief\nsystems, which continues to hinder the wide adoption of conversational AI\ntools. This paper introduces a novel method, Conversational Agent for\nMulti-Source Auxiliary Context with Sparse and Symmetric Latent Interactions\n(CoMAC), for conversation generation, which employs specialized encoding\nstreams and post-fusion grounding networks for multiple data sources to\nidentify relevant persona and knowledge information for the conversation. CoMAC\nalso leverages a novel text similarity metric that allows bi-directional\ninformation sharing among multiple sources and focuses on a selective subset of\nmeaningful words. Our experiments show that CoMAC improves the relevant persona\nand knowledge prediction accuracies and response generation quality\nsignificantly over two state-of-the-art methods."}
{"id": "2503.19279", "pdf": "https://arxiv.org/pdf/2503.19279", "abs": "https://arxiv.org/abs/2503.19279", "authors": ["Wenjuan Qin", "Weiran Wang", "Yuming Yang", "Tao Gui"], "title": "Machine-assisted writing evaluation: Exploring pre-trained language models in analyzing argumentative moves", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The study investigates the efficacy of pre-trained language models (PLMs) in\nanalyzing argumentative moves in a longitudinal learner corpus. Prior studies\non argumentative moves often rely on qualitative analysis and manual coding,\nlimiting their efficiency and generalizability. The study aims to: 1) to assess\nthe reliability of PLMs in analyzing argumentative moves; 2) to utilize\nPLM-generated annotations to illustrate developmental patterns and predict\nwriting quality. A longitudinal corpus of 1643 argumentative texts from 235\nEnglish learners in China is collected and annotated into six move types:\nclaim, data, counter-claim, counter-data, rebuttal, and non-argument. The\ncorpus is divided into training, validation, and application sets annotated by\nhuman experts and PLMs. We use BERT as one of the implementations of PLMs. The\nresults indicate a robust reliability of PLMs in analyzing argumentative moves,\nwith an overall F1 score of 0.743, surpassing existing models in the field.\nAdditionally, PLM-labeled argumentative moves effectively capture developmental\npatterns and predict writing quality. Over time, students exhibit an increase\nin the use of data and counter-claims and a decrease in non-argument moves.\nWhile low-quality texts are characterized by a predominant use of claims and\ndata supporting only oneside position, mid- and high-quality texts demonstrate\nan integrative perspective with a higher ratio of counter-claims, counter-data,\nand rebuttals. This study underscores the transformative potential of\nintegrating artificial intelligence into language education, enhancing the\nefficiency and accuracy of evaluating students' writing. The successful\napplication of PLMs can catalyze the development of educational technology,\npromoting a more data-driven and personalized learning environment that\nsupports diverse educational needs."}
{"id": "2503.19309", "pdf": "https://arxiv.org/pdf/2503.19309", "abs": "https://arxiv.org/abs/2503.19309", "authors": ["Gollam Rabby", "Diyana Muhammed", "Prasenjit Mitra", "Sören Auer"], "title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees", "categories": ["cs.CL"], "comment": null, "summary": "Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation."}
{"id": "2503.19328", "pdf": "https://arxiv.org/pdf/2503.19328", "abs": "https://arxiv.org/abs/2503.19328", "authors": ["Vidya Srinivas", "Xuhai Xu", "Xin Liu", "Kumar Ayush", "Isaac Galatzer-Levy", "Shwetak Patel", "Daniel McDuff", "Tim Althoff"], "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications."}
{"id": "2503.19426", "pdf": "https://arxiv.org/pdf/2503.19426", "abs": "https://arxiv.org/abs/2503.19426", "authors": ["Suyoung Bae", "YunSeok Choi", "Jee-Hyong Lee"], "title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to NAACL 2025 main. 20 pages, 3 figures", "summary": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings."}
{"id": "2503.18957", "pdf": "https://arxiv.org/pdf/2503.18957", "abs": "https://arxiv.org/abs/2503.18957", "authors": ["Yixuan Wang", "Paul Stynes", "Pramod Pathak", "Cristina Muntean"], "title": "A Real-Time Human Action Recognition Model for Assisted Living", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Ensuring the safety and well-being of elderly and vulnerable populations in\nassisted living environments is a critical concern. Computer vision presents an\ninnovative and powerful approach to predicting health risks through video\nmonitoring, employing human action recognition (HAR) technology. However,\nreal-time prediction of human actions with high performance and efficiency is a\nchallenge. This research proposes a real-time human action recognition model\nthat combines a deep learning model and a live video prediction and alert\nsystem, in order to predict falls, staggering and chest pain for residents in\nassisted living. Six thousand RGB video samples from the NTU RGB+D 60 dataset\nwere selected to create a dataset with four classes: Falling, Staggering, Chest\nPain, and Normal, with the Normal class comprising 40 daily activities.\nTransfer learning technique was applied to train four state-of-the-art HAR\nmodels on a GPU server, namely, UniFormerV2, TimeSformer, I3D, and SlowFast.\nResults of the four models are presented in this paper based on class-wise and\nmacro performance metrics, inference efficiency, model complexity and\ncomputational costs. TimeSformer is proposed for developing the real-time human\naction recognition model, leveraging its leading macro F1 score (95.33%),\nrecall (95.49%), and precision (95.19%) along with significantly higher\ninference throughput compared to the others. This research provides insights to\nenhance safety and health of the elderly and people with chronic illnesses in\nassisted living environments, fostering sustainable care, smarter communities\nand industry innovation."}
{"id": "2503.19469", "pdf": "https://arxiv.org/pdf/2503.19469", "abs": "https://arxiv.org/abs/2503.19469", "authors": ["Fred Philippy", "Siwen Guo", "Cedric Lothritz", "Jacques Klein", "Tegawendé F. Bissyandé"], "title": "Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "Workshop on Language Models for Underserved Communities (co-located\n  with NAACL 2025)", "summary": "In NLP, Zero-Shot Classification (ZSC) has become essential for enabling\nmodels to classify text into categories unseen during training, particularly in\nlow-resource languages and domains where labeled data is scarce. While\npretrained language models (PLMs) have shown promise in ZSC, they often rely on\nlarge training datasets or external knowledge, limiting their applicability in\nmultilingual and low-resource scenarios. Recent approaches leveraging natural\nlanguage prompts reduce the dependence on large training datasets but struggle\nto effectively incorporate available labeled data from related classification\ntasks, especially when these datasets originate from different languages or\ndistributions. Moreover, existing prompt-based methods typically rely on\nmanually crafted prompts in a specific language, limiting their adaptability\nand effectiveness in cross-lingual settings. To address these challenges, we\nintroduce RoSPrompt, a lightweight and data-efficient approach for training\nsoft prompts that enhance cross-lingual ZSC while ensuring robust\ngeneralization across data distribution shifts. RoSPrompt is designed for small\nmultilingual PLMs, enabling them to leverage high-resource languages to improve\nperformance in low-resource settings without requiring extensive fine-tuning or\nhigh computational costs. We evaluate our approach on multiple multilingual\nPLMs across datasets covering 106 languages, demonstrating strong cross-lingual\ntransfer performance and robust generalization capabilities over unseen\nclasses."}
{"id": "2503.18988", "pdf": "https://arxiv.org/pdf/2503.18988", "abs": "https://arxiv.org/abs/2503.18988", "authors": ["Haoliang Shang", "Hanyu Wu", "Guangyao Zhai", "Boyang Sun", "Fangjinhua Wang", "Federico Tombari", "Marc Pollefeys"], "title": "SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene Graph Manipulation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "The code will be available at https://github.com/josef5838/SG-Tailor", "summary": "Scene graphs capture complex relationships among objects, serving as strong\npriors for content generation and manipulation. Yet, reasonably manipulating\nscene graphs -- whether by adding nodes or modifying edges -- remains a\nchallenging and untouched task. Tasks such as adding a node to the graph or\nreasoning about a node's relationships with all others are computationally\nintractable, as even a single edge modification can trigger conflicts due to\nthe intricate interdependencies within the graph. To address these challenges,\nwe introduce SG-Tailor, an autoregressive model that predicts the conflict-free\nrelationship between any two nodes. SG-Tailor not only infers inter-object\nrelationships, including generating commonsense edges for newly added nodes but\nalso resolves conflicts arising from edge modifications to produce coherent,\nmanipulated graphs for downstream tasks. For node addition, the model queries\nthe target node and other nodes from the graph to predict the appropriate\nrelationships. For edge modification, SG-Tailor employs a Cut-And-Stitch\nstrategy to solve the conflicts and globally adjust the graph. Extensive\nexperiments demonstrate that SG-Tailor outperforms competing methods by a large\nmargin and can be seamlessly integrated as a plug-in module for scene\ngeneration and robotic manipulation tasks."}
{"id": "2503.19482", "pdf": "https://arxiv.org/pdf/2503.19482", "abs": "https://arxiv.org/abs/2503.19482", "authors": ["Zhiwei Wang", "Zhongxin Liu", "Ying Li", "Hongyu Sun", "Meng Xu", "Yuqing Zhang"], "title": "KSHSeek: Data-Driven Approaches to Mitigating and Detecting Knowledge-Shortcut Hallucinations in Generative Models", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "16 pages, 34 figures", "summary": "The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications."}
{"id": "2503.18997", "pdf": "https://arxiv.org/pdf/2503.18997", "abs": "https://arxiv.org/abs/2503.18997", "authors": ["Tonmoy Ghosh", "Edward Sazonov"], "title": "Improving Food Image Recognition with Noisy Vision Transformer", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Food image recognition is a challenging task in computer vision due to the\nhigh variability and complexity of food images. In this study, we investigate\nthe potential of Noisy Vision Transformers (NoisyViT) for improving food\nclassification performance. By introducing noise into the learning process,\nNoisyViT reduces task complexity and adjusts the entropy of the system, leading\nto enhanced model accuracy. We fine-tune NoisyViT on three benchmark datasets:\nFood2K (2,000 categories, ~1M images), Food-101 (101 categories, ~100K images),\nand CNFOOD-241 (241 categories, ~190K images). The performance of NoisyViT is\nevaluated against state-of-the-art food recognition models. Our results\ndemonstrate that NoisyViT achieves Top-1 accuracies of 95%, 99.5%, and 96.6% on\nFood2K, Food-101, and CNFOOD-241, respectively, significantly outperforming\nexisting approaches. This study underscores the potential of NoisyViT for\ndietary assessment, nutritional monitoring, and healthcare applications, paving\nthe way for future advancements in vision-based food computing. Code for\nreproducing NoisyViT for food recognition is available at NoisyViT_Food."}
{"id": "2503.19498", "pdf": "https://arxiv.org/pdf/2503.19498", "abs": "https://arxiv.org/abs/2503.19498", "authors": ["Ling Zhong", "Yujing Lu", "Jing Yang", "Weiming Li", "Peng Wei", "Yongheng Wang", "Manni Duan", "Qing Zhang"], "title": "DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts", "categories": ["cs.CL"], "comment": "11 pages, 6 figures", "summary": "Chart Question Answering (CQA) benchmarks are essential for evaluating the\ncapability of Multimodal Large Language Models (MLLMs) to interpret visual\ndata. However, current benchmarks focus primarily on the evaluation of\ngeneral-purpose CQA but fail to adequately capture domain-specific challenges.\nWe introduce DomainCQA, a systematic methodology for constructing\ndomain-specific CQA benchmarks, and demonstrate its effectiveness by developing\nAstroChart, a CQA benchmark in the field of astronomy. Our evaluation shows\nthat chart reasoning and combining chart information with domain knowledge for\ndeeper analysis and summarization, rather than domain-specific knowledge, pose\nthe primary challenge for existing MLLMs, highlighting a critical gap in\ncurrent benchmarks. By providing a scalable and rigorous framework, DomainCQA\nenables more precise assessment and improvement of MLLMs for domain-specific\napplications."}
{"id": "2503.19001", "pdf": "https://arxiv.org/pdf/2503.19001", "abs": "https://arxiv.org/abs/2503.19001", "authors": ["Kangwei Liu", "Junwu Liu", "Yun Cao", "Jinlin Guo", "Xiaowei Yi"], "title": "DisentTalk: Cross-lingual Talking Face Generation via Semantic Disentangled Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in talking face generation have significantly improved facial\nanimation synthesis. However, existing approaches face fundamental limitations:\n3DMM-based methods maintain temporal consistency but lack fine-grained regional\ncontrol, while Stable Diffusion-based methods enable spatial manipulation but\nsuffer from temporal inconsistencies. The integration of these approaches is\nhindered by incompatible control mechanisms and semantic entanglement of facial\nrepresentations. This paper presents DisentTalk, introducing a data-driven\nsemantic disentanglement framework that decomposes 3DMM expression parameters\ninto meaningful subspaces for fine-grained facial control. Building upon this\ndisentangled representation, we develop a hierarchical latent diffusion\narchitecture that operates in 3DMM parameter space, integrating region-aware\nattention mechanisms to ensure both spatial precision and temporal coherence.\nTo address the scarcity of high-quality Chinese training data, we introduce\nCHDTF, a Chinese high-definition talking face dataset. Extensive experiments\nshow superior performance over existing methods across multiple metrics,\nincluding lip synchronization, expression quality, and temporal consistency.\nProject Page: https://kangweiiliu.github.io/DisentTalk."}
{"id": "2503.19540", "pdf": "https://arxiv.org/pdf/2503.19540", "abs": "https://arxiv.org/abs/2503.19540", "authors": ["Dahyun Jung", "Seungyoon Lee", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 findings", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness."}
{"id": "2503.19009", "pdf": "https://arxiv.org/pdf/2503.19009", "abs": "https://arxiv.org/abs/2503.19009", "authors": ["Arun Reddy", "Alexander Martin", "Eugene Yang", "Andrew Yates", "Kate Sanders", "Kenton Murray", "Reno Kriz", "Celso M. de Melo", "Benjamin Van Durme", "Rama Chellappa"], "title": "Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. 13 pages, 4 figures. Approved for public\n  release: distribution unlimited", "summary": "In this work, we tackle the problem of text-to-video retrieval (T2VR).\nInspired by the success of late interaction techniques in text-document,\ntext-image, and text-video retrieval, our approach, Video-ColBERT, introduces a\nsimple and efficient mechanism for fine-grained similarity assessment between\nqueries and videos. Video-ColBERT is built upon 3 main components: a\nfine-grained spatial and temporal token-wise interaction, query and visual\nexpansions, and a dual sigmoid loss during training. We find that this\ninteraction and training paradigm leads to strong individual, yet compatible,\nrepresentations for encoding video content. These representations lead to\nincreases in performance on common text-to-video retrieval benchmarks compared\nto other bi-encoder methods."}
{"id": "2503.19551", "pdf": "https://arxiv.org/pdf/2503.19551", "abs": "https://arxiv.org/abs/2503.19551", "authors": ["Zeyu Qin", "Qingxiu Dong", "Xingxing Zhang", "Li Dong", "Xiaolong Huang", "Ziyi Yang", "Mahmoud Khademi", "Dongdong Zhang", "Hany Hassan Awadalla", "Yi R. Fung", "Weizhu Chen", "Minhao Cheng", "Furu Wei"], "title": "Scaling Laws of Synthetic Data for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the \\emph{rectified scaling law} across various model\nsizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger\nmodels approach optimal performance with fewer training tokens. For instance,\nan 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover,\ncomparisons with existing synthetic data generation and augmentation methods\ndemonstrate that SynthLLM achieves superior performance and scalability. Our\nfindings highlight synthetic data as a scalable and reliable alternative to\norganic pre-training corpora, offering a viable path toward continued\nimprovement in model performance."}
{"id": "2503.19011", "pdf": "https://arxiv.org/pdf/2503.19011", "abs": "https://arxiv.org/abs/2503.19011", "authors": ["Yifei Feng", "Mingxin Yang", "Shuhui Yang", "Sheng Zhang", "Jiaao Yu", "Zibo Zhao", "Yuhong Liu", "Jie Jiang", "Chunchao Guo"], "title": "RomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Painting textures for existing geometries is a critical yet labor-intensive\nprocess in 3D asset generation. Recent advancements in text-to-image (T2I)\nmodels have led to significant progress in texture generation. Most existing\nresearch approaches this task by first generating images in 2D spaces using\nimage diffusion models, followed by a texture baking process to achieve UV\ntexture. However, these methods often struggle to produce high-quality textures\ndue to inconsistencies among the generated multi-view images, resulting in\nseams and ghosting artifacts. In contrast, 3D-based texture synthesis methods\naim to address these inconsistencies, but they often neglect 2D diffusion model\npriors, making them challenging to apply to real-world objects To overcome\nthese limitations, we propose RomanTex, a multiview-based texture generation\nframework that integrates a multi-attention network with an underlying 3D\nrepresentation, facilitated by our novel 3D-aware Rotary Positional Embedding.\nAdditionally, we incorporate a decoupling characteristic in the multi-attention\nblock to enhance the model's robustness in image-to-texture task, enabling\nsemantically-correct back-view synthesis. Furthermore, we introduce a\ngeometry-related Classifier-Free Guidance (CFG) mechanism to further improve\nthe alignment with both geometries and images. Quantitative and qualitative\nevaluations, along with comprehensive user studies, demonstrate that our method\nachieves state-of-the-art results in texture quality and consistency."}
{"id": "2503.19574", "pdf": "https://arxiv.org/pdf/2503.19574", "abs": "https://arxiv.org/abs/2503.19574", "authors": ["Yanhong Li", "David Yunis", "David McAllester", "Jiawei Zhou"], "title": "Context-Efficient Retrieval with Factual Decomposition", "categories": ["cs.CL", "cs.IR"], "comment": "NAACL 2025 Main Conference", "summary": "There has recently been considerable interest in incorporating information\nretrieval into large language models (LLMs). Retrieval from a dynamically\nexpanding external corpus of text allows a model to incorporate current events\nand can be viewed as a form of episodic memory. Here we demonstrate that\npre-processing the external corpus into semi-structured ''atomic facts'' makes\nretrieval more efficient. More specifically, we demonstrate that our particular\nform of atomic facts improves performance on various question answering tasks\nwhen the amount of retrieved text is limited. Limiting the amount of retrieval\nreduces the size of the context and improves inference efficiency."}
{"id": "2503.19012", "pdf": "https://arxiv.org/pdf/2503.19012", "abs": "https://arxiv.org/abs/2503.19012", "authors": ["Lingyan Ran", "Lidong Wang", "Guangcong Wang", "Peng Wang", "Yanning Zhang"], "title": "DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language Understanding", "categories": ["cs.CV"], "comment": "Project page: https://diffv2ir.github.io/", "summary": "The task of translating visible-to-infrared images (V2IR) is inherently\nchallenging due to three main obstacles: 1) achieving semantic-aware\ntranslation, 2) managing the diverse wavelength spectrum in infrared imagery,\nand 3) the scarcity of comprehensive infrared datasets. Current leading methods\ntend to treat V2IR as a conventional image-to-image synthesis challenge, often\noverlooking these specific issues. To address this, we introduce DiffV2IR, a\nnovel framework for image translation comprising two key elements: a\nProgressive Learning Module (PLM) and a Vision-Language Understanding Module\n(VLUM). PLM features an adaptive diffusion model architecture that leverages\nmulti-stage knowledge learning to infrared transition from full-range to target\nwavelength. To improve V2IR translation, VLUM incorporates unified\nVision-Language Understanding. We also collected a large infrared dataset,\nIR-500K, which includes 500,000 infrared images compiled by various scenes and\nobjects under various environmental conditions. Through the combination of PLM,\nVLUM, and the extensive IR-500K dataset, DiffV2IR markedly improves the\nperformance of V2IR. Experiments validate DiffV2IR's excellence in producing\nhigh-quality translations, establishing its efficacy and broad applicability.\nThe code, dataset, and DiffV2IR model will be available at\nhttps://github.com/LidongWang-26/DiffV2IR."}
{"id": "2503.19586", "pdf": "https://arxiv.org/pdf/2503.19586", "abs": "https://arxiv.org/abs/2503.19586", "authors": ["Hanlin Wu", "Xufeng Duan", "Zhenguang Cai"], "title": "Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment", "categories": ["cs.CL", "q-bio.NC"], "comment": "Accepted by the 14th edition of the Workshop on Cognitive Modeling\n  and Computational Linguistics (CMCL 2025)", "summary": "Voice-based AI development faces unique challenges in processing both\nlinguistic and paralinguistic information. This study compares how large\naudio-language models (LALMs) and humans integrate speaker characteristics\nduring speech comprehension, asking whether LALMs process\nspeaker-contextualized language in ways that parallel human cognitive\nmechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing\npatterns with human EEG responses. Using surprisal and entropy metrics from the\nmodels, we analyzed their sensitivity to speaker-content incongruency across\nsocial stereotype violations (e.g., a man claiming to regularly get manicures)\nand biological knowledge violations (e.g., a man claiming to be pregnant).\nResults revealed that Qwen2-Audio exhibited increased surprisal for\nspeaker-incongruent content and its surprisal values significantly predicted\nhuman N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker\ncharacteristics. Importantly, neither model replicated the human-like\nprocessing distinction between social violations (eliciting N400 effects) and\nbiological violations (eliciting P600 effects). These findings reveal both the\npotential and limitations of current LALMs in processing speaker-contextualized\nlanguage, and suggest differences in social-linguistic processing mechanisms\nbetween humans and LALMs."}
{"id": "2503.19034", "pdf": "https://arxiv.org/pdf/2503.19034", "abs": "https://arxiv.org/abs/2503.19034", "authors": ["Alexander Lobashev", "Maria Larchenko", "Dmitry Guskov"], "title": "Color Conditional Generation with Sliced Wasserstein Guidance", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We propose SW-Guidance, a training-free approach for image generation\nconditioned on the color distribution of a reference image. While it is\npossible to generate an image with fixed colors by first creating an image from\na text prompt and then applying a color style transfer method, this approach\noften results in semantically meaningless colors in the generated image. Our\nmethod solves this problem by modifying the sampling process of a diffusion\nmodel to incorporate the differentiable Sliced 1-Wasserstein distance between\nthe color distribution of the generated image and the reference palette. Our\nmethod outperforms state-of-the-art techniques for color-conditional generation\nin terms of color similarity to the reference, producing images that not only\nmatch the reference colors but also maintain semantic coherence with the\noriginal text prompt. Our source code is available at\nhttps://github.com/alobashev/sw-guidance/."}
{"id": "2503.19598", "pdf": "https://arxiv.org/pdf/2503.19598", "abs": "https://arxiv.org/abs/2503.19598", "authors": ["Giovanni Franco Gabriel Marraffini", "Andrés Cotton", "Noe Fabian Hsueh", "Axel Fridman", "Juan Wisznia", "Luciano Del Corro"], "title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas", "categories": ["cs.CL"], "comment": null, "summary": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment."}
{"id": "2503.19062", "pdf": "https://arxiv.org/pdf/2503.19062", "abs": "https://arxiv.org/abs/2503.19062", "authors": ["Maria Larchenko", "Alexander Lobashev", "Dmitry Guskov", "Vladimir Vladimirovich Palyulin"], "title": "Color Transfer with Modulated Flows", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "In this work, we introduce Modulated Flows (ModFlows), a novel approach for\ncolor transfer between images based on rectified flows. The primary goal of the\ncolor transfer is to adjust the colors of a target image to match the color\ndistribution of a reference image. Our technique is based on optimal transport\nand executes color transfer as an invertible transformation within the RGB\ncolor space. The ModFlows utilizes the bijective property of flows, enabling us\nto introduce a common intermediate color distribution and build a dataset of\nrectified flows. We train an encoder on this dataset to predict the weights of\na rectified model for new images. After training on a set of optimal transport\nplans, our approach can generate plans for new pairs of distributions without\nadditional fine-tuning. We additionally show that the trained encoder provides\nan image embedding, associated only with its color style. The presented method\nis capable of processing 4K images and achieves the state-of-the-art\nperformance in terms of content and style similarity. Our source code is\navailable at https://github.com/maria-larchenko/modflows"}
{"id": "2503.19633", "pdf": "https://arxiv.org/pdf/2503.19633", "abs": "https://arxiv.org/abs/2503.19633", "authors": ["Han Zhao", "Haotian Wang", "Yiping Peng", "Sitong Zhao", "Xiaoyu Tian", "Shuaiting Chen", "Yunjie Ji", "Xiangang Li"], "title": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training", "categories": ["cs.CL"], "comment": null, "summary": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}."}
{"id": "2503.19065", "pdf": "https://arxiv.org/pdf/2503.19065", "abs": "https://arxiv.org/abs/2503.19065", "authors": ["Zhongyu Yang", "Jun Chen", "Dannong Xu", "Junjie Fei", "Xiaoqian Shen", "Liangbing Zhao", "Chun-Mei Feng", "Mohamed Elhoseiny"], "title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation", "categories": ["cs.CV"], "comment": "Project in https://wikiautogen.github.io/", "summary": "Knowledge discovery and collection are intelligence-intensive tasks that\ntraditionally require significant human effort to ensure high-quality outputs.\nRecent research has explored multi-agent frameworks for automating\nWikipedia-style article generation by retrieving and synthesizing information\nfrom the internet. However, these methods primarily focus on text-only\ngeneration, overlooking the importance of multimodal content in enhancing\ninformativeness and engagement. In this work, we introduce WikiAutoGen, a novel\nsystem for automated multimodal Wikipedia-style article generation. Unlike\nprior approaches, WikiAutoGen retrieves and integrates relevant images\nalongside text, enriching both the depth and visual appeal of generated\ncontent. To further improve factual accuracy and comprehensiveness, we propose\na multi-perspective self-reflection mechanism, which critically assesses\nretrieved content from diverse viewpoints to enhance reliability, breadth, and\ncoherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising\nWikipedia articles with topics paired with both textual and image-based\nrepresentations, designed to evaluate multimodal knowledge generation on more\nchallenging topics. Experimental results show that WikiAutoGen outperforms\nprevious methods by 8%-29% on our WikiSeek benchmark, producing more accurate,\ncoherent, and visually enriched Wikipedia-style articles. We show some of our\ngenerated examples in https://wikiautogen.github.io/ ."}
{"id": "2503.19642", "pdf": "https://arxiv.org/pdf/2503.19642", "abs": "https://arxiv.org/abs/2503.19642", "authors": ["Ibrahim Said Ahmad", "Shiran Dudy", "Tadesse Destaw Belay", "Idris Abdulmumin", "Seid Muhie Yimam", "Shamsuddeen Hassan Muhammad", "Kenneth Church"], "title": "Exploring Cultural Nuances in Emotion Perception Across 15 African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how emotions are expressed across languages is vital for\nbuilding culturally-aware and inclusive NLP systems. However, emotion\nexpression in African languages is understudied, limiting the development of\neffective emotion detection tools in these languages. In this work, we present\na cross-linguistic analysis of emotion expression in 15 African languages. We\nexamine four key dimensions of emotion representation: text length, sentiment\npolarity, emotion co-occurrence, and intensity variations. Our findings reveal\ndiverse language-specific patterns in emotional expression -- with Somali texts\ntypically longer, while others like IsiZulu and Algerian Arabic show more\nconcise emotional expression. We observe a higher prevalence of negative\nsentiment in several Nigerian languages compared to lower negativity in\nlanguages like IsiXhosa. Further, emotion co-occurrence analysis demonstrates\nstrong cross-linguistic associations between specific emotion pairs\n(anger-disgust, sadness-fear), suggesting universal psychological connections.\nIntensity distributions show multimodal patterns with significant variations\nbetween language families; Bantu languages display similar yet distinct\nprofiles, while Afroasiatic languages and Nigerian Pidgin demonstrate wider\nintensity ranges. These findings highlight the need for language-specific\napproaches to emotion detection while identifying opportunities for transfer\nlearning across related languages."}
{"id": "2503.19067", "pdf": "https://arxiv.org/pdf/2503.19067", "abs": "https://arxiv.org/abs/2503.19067", "authors": ["Axel Descamps", "Sélène Forget", "Aliénor Lahlou", "Claire Lavergne", "Camille Berthelot", "Guillaume Stirnemann", "Rodolphe Vuilleumier", "Nicolas Chéron"], "title": "Clustering data by reordering them", "categories": ["cs.CV", "q-bio.BM"], "comment": "60 pages, 21 figures", "summary": "Grouping elements into families to analyse them separately is a standard\nanalysis procedure in many areas of sciences. We propose herein a new algorithm\nbased on the simple idea that members from a family look like each other, and\ndon't resemble elements foreign to the family. After reordering the data\naccording to the distance between elements, the analysis is automatically\nperformed with easily-understandable parameters. Noise is explicitly taken into\naccount to deal with the variety of problems of a data-driven world. We applied\nthe algorithm to sort biomolecules conformations, gene sequences, cells,\nimages, and experimental conditions."}
{"id": "2503.19650", "pdf": "https://arxiv.org/pdf/2503.19650", "abs": "https://arxiv.org/abs/2503.19650", "authors": ["Maryam Bala", "Amina Imam Abubakar", "Abdulhamid Abubakar", "Abdulkadir Shehu Bichi", "Hafsa Kabir Ahmad", "Sani Abdullahi Sani", "Idris Abdulmumin", "Shamsuddeen Hassan Muhamad", "Ibrahim Said Ahmad"], "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable."}
{"id": "2503.19096", "pdf": "https://arxiv.org/pdf/2503.19096", "abs": "https://arxiv.org/abs/2503.19096", "authors": ["Sina Ditzel", "Achref Jaziri", "Iuliia Pliushch", "Visvanathan Ramesh"], "title": "Uncertainty-Aware Decomposed Hybrid Networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The robustness of image recognition algorithms remains a critical challenge,\nas current models often depend on large quantities of labeled data. In this\npaper, we propose a hybrid approach that combines the adaptability of neural\nnetworks with the interpretability, transparency, and robustness of\ndomain-specific quasi-invariant operators. Our method decomposes the\nrecognition into multiple task-specific operators that focus on different\ncharacteristics, supported by a novel confidence measurement tailored to these\noperators. This measurement enables the network to prioritize reliable features\nand accounts for noise. We argue that our design enhances transparency and\nrobustness, leading to improved performance, particularly in low-data regimes.\nExperimental results in traffic sign detection highlight the effectiveness of\nthe proposed method, especially in semi-supervised and unsupervised scenarios,\nunderscoring its potential for data-constrained applications."}
{"id": "2503.19668", "pdf": "https://arxiv.org/pdf/2503.19668", "abs": "https://arxiv.org/abs/2503.19668", "authors": ["Fredy Alejandro Mendoza López", "Jefferson Rodriguez", "Fabio Martínez"], "title": "A multitask transformer to sign language translation using motion gesture primitives", "categories": ["cs.CL"], "comment": "32 pages, 10 tables, 13 figures", "summary": "The absence of effective communication the deaf population represents the\nmain social gap in this community. Furthermore, the sign language, main deaf\ncommunication tool, is unlettered, i.e., there is no formal written\nrepresentation. In consequence, main challenge today is the automatic\ntranslation among spatiotemporal sign representation and natural text language.\nRecent approaches are based on encoder-decoder architectures, where the most\nrelevant strategies integrate attention modules to enhance non-linear\ncorrespondences, besides, many of these approximations require complex training\nand architectural schemes to achieve reasonable predictions, because of the\nabsence of intermediate text projections. However, they are still limited by\nthe redundant background information of the video sequences. This work\nintroduces a multitask transformer architecture that includes a gloss learning\nrepresentation to achieve a more suitable translation. The proposed approach\nalso includes a dense motion representation that enhances gestures and includes\nkinematic information, a key component in sign language. From this\nrepresentation it is possible to avoid background information and exploit the\ngeometry of the signs, in addition, it includes spatiotemporal representations\nthat facilitate the alignment between gestures and glosses as an intermediate\ntextual representation. The proposed approach outperforms the state-of-the-art\nevaluated on the CoL-SLTD dataset, achieving a BLEU-4 of 72,64% in split 1, and\na BLEU-4 of 14,64% in split 2. Additionally, the strategy was validated on the\nRWTH-PHOENIX-Weather 2014 T dataset, achieving a competitive BLEU-4 of 11,58%."}
{"id": "2503.19100", "pdf": "https://arxiv.org/pdf/2503.19100", "abs": "https://arxiv.org/abs/2503.19100", "authors": ["Md. Barkat Ullah Tusher", "Shartaz Khan Akash", "Amirul Islam Showmik"], "title": "Anomaly Detection Using Computer Vision: A Comparative Analysis of Class Distinction and Performance Metrics", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "This paper showcases an experimental study on anomaly detection using\ncomputer vision. The study focuses on class distinction and performance\nevaluation, combining OpenCV with deep learning techniques while employing a\nTensorFlow-based convolutional neural network for real-time face recognition\nand classification. The system effectively distinguishes among three classes:\nauthorized personnel (admin), intruders, and non-human entities. A\nMobileNetV2-based deep learning model is utilized to optimize real-time\nperformance, ensuring high computational efficiency without compromising\naccuracy. Extensive dataset preprocessing, including image augmentation and\nnormalization, enhances the models generalization capabilities. Our analysis\ndemonstrates classification accuracies of 90.20% for admin, 98.60% for\nintruders, and 75.80% for non-human detection, while maintaining an average\nprocessing rate of 30 frames per second. The study leverages transfer learning,\nbatch normalization, and Adam optimization to achieve stable and robust\nlearning, and a comparative analysis of class differentiation strategies\nhighlights the impact of feature extraction techniques and training\nmethodologies. The results indicate that advanced feature selection and data\naugmentation significantly enhance detection performance, particularly in\ndistinguishing human from non-human scenes. As an experimental study, this\nresearch provides critical insights into optimizing deep learning-based\nsurveillance systems for high-security environments and improving the accuracy\nand efficiency of real-time anomaly detection."}
{"id": "2503.19693", "pdf": "https://arxiv.org/pdf/2503.19693", "abs": "https://arxiv.org/abs/2503.19693", "authors": ["Itay Nakash", "Nitay Calderon", "Eyal Ben David", "Elad Hoffer", "Roi Reichart"], "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance"}
{"id": "2503.19108", "pdf": "https://arxiv.org/pdf/2503.19108", "abs": "https://arxiv.org/abs/2503.19108", "authors": ["Tommie Kerssies", "Niccolò Cavagnero", "Alexander Hermans", "Narges Norouzi", "Giuseppe Averta", "Bastian Leibe", "Gijs Dubbelman", "Daan de Geus"], "title": "Your ViT is Secretly an Image Segmentation Model", "categories": ["cs.CV"], "comment": "CVPR 2025. Code: https://www.tue-mps.org/eomt/", "summary": "Vision Transformers (ViTs) have shown remarkable performance and scalability\nacross various computer vision tasks. To apply single-scale ViTs to image\nsegmentation, existing methods adopt a convolutional adapter to generate\nmulti-scale features, a pixel decoder to fuse these features, and a Transformer\ndecoder that uses the fused features to make predictions. In this paper, we\nshow that the inductive biases introduced by these task-specific components can\ninstead be learned by the ViT itself, given sufficiently large models and\nextensive pre-training. Based on these findings, we introduce the Encoder-only\nMask Transformer (EoMT), which repurposes the plain ViT architecture to conduct\nimage segmentation. With large-scale models and pre-training, EoMT obtains a\nsegmentation accuracy similar to state-of-the-art models that use task-specific\ncomponents. At the same time, EoMT is significantly faster than these methods\ndue to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a\nrange of model sizes, EoMT demonstrates an optimal balance between segmentation\naccuracy and prediction speed, suggesting that compute resources are better\nspent on scaling the ViT itself rather than adding architectural complexity.\nCode: https://www.tue-mps.org/eomt/."}
{"id": "2503.19702", "pdf": "https://arxiv.org/pdf/2503.19702", "abs": "https://arxiv.org/abs/2503.19702", "authors": ["Abdulhamid Abubakar", "Hamidatu Abdulkadir", "Ibrahim Rabiu Abdullahi", "Abubakar Auwal Khalid", "Ahmad Mustapha Wali", "Amina Aminu Umar", "Maryam Bala", "Sani Abdullahi Sani", "Ibrahim Said Ahmad", "Shamsuddeen Hassan Muhammad", "Idris Abdulmumin", "Vukosi Marivate"], "title": "HausaNLP at SemEval-2025 Task 2: Entity-Aware Fine-tuning vs. Prompt Engineering in Entity-Aware Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our findings for SemEval 2025 Task 2, a shared task on\nentity-aware machine translation (EA-MT). The goal of this task is to develop\ntranslation models that can accurately translate English sentences into target\nlanguages, with a particular focus on handling named entities, which often pose\nchallenges for MT systems. The task covers 10 target languages with English as\nthe source. In this paper, we describe the different systems we employed,\ndetail our results, and discuss insights gained from our experiments."}
{"id": "2503.19145", "pdf": "https://arxiv.org/pdf/2503.19145", "abs": "https://arxiv.org/abs/2503.19145", "authors": ["Marco Garosi", "Alessandro Conti", "Gaowen Liu", "Elisa Ricci", "Massimiliano Mancini"], "title": "Compositional Caching for Training-free Open-vocabulary Attribute Detection", "categories": ["cs.CV"], "comment": "CVPR 2025. Project website at https://comca-attributes.github.io/", "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."}
{"id": "2503.19711", "pdf": "https://arxiv.org/pdf/2503.19711", "abs": "https://arxiv.org/abs/2503.19711", "authors": ["Sian Gooding", "Lucia Lopez-Rivilla", "Edward Grefenstette"], "title": "Writing as a testbed for open ended agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains."}
{"id": "2503.19157", "pdf": "https://arxiv.org/pdf/2503.19157", "abs": "https://arxiv.org/abs/2503.19157", "authors": ["Mingzhen Huang", "Fu-Jen Chu", "Bugra Tekin", "Kevin J Liang", "Haoyu Ma", "Weiyao Wang", "Xingyu Chen", "Pierre Gleize", "Hongfei Xue", "Siwei Lyu", "Kris Kitani", "Matt Feiszli", "Hao Tang"], "title": "HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce HOIGPT, a token-based generative method that unifies 3D\nhand-object interactions (HOI) perception and generation, offering the first\ncomprehensive solution for captioning and generating high-quality 3D HOI\nsequences from a diverse range of conditional signals (\\eg text, objects,\npartial sequences). At its core, HOIGPT utilizes a large language model to\npredict the bidrectional transformation between HOI sequences and natural\nlanguage descriptions. Given text inputs, HOIGPT generates a sequence of hand\nand object meshes; given (partial) HOI sequences, HOIGPT generates text\ndescriptions and completes the sequences. To facilitate HOI understanding with\na large language model, this paper introduces two key innovations: (1) a novel\nphysically grounded HOI tokenizer, the hand-object decomposed VQ-VAE, for\ndiscretizing HOI sequences, and (2) a motion-aware language model trained to\nprocess and generate both text and HOI tokens. Extensive experiments\ndemonstrate that HOIGPT sets new state-of-the-art performance on both text\ngeneration (+2.01% R Precision) and HOI generation (-2.56 FID) across multiple\ntasks and benchmarks."}
{"id": "2503.19786", "pdf": "https://arxiv.org/pdf/2503.19786", "abs": "https://arxiv.org/abs/2503.19786", "authors": ["Gemma Team", "Aishwarya Kamath", "Johan Ferret", "Shreya Pathak", "Nino Vieillard", "Ramona Merhej", "Sarah Perrin", "Tatiana Matejovicova", "Alexandre Ramé", "Morgane Rivière", "Louis Rouillard", "Thomas Mesnard", "Geoffrey Cideron", "Jean-bastien Grill", "Sabela Ramos", "Edouard Yvinec", "Michelle Casbon", "Etienne Pot", "Ivo Penchev", "Gaël Liu", "Francesco Visin", "Kathleen Kenealy", "Lucas Beyer", "Xiaohai Zhai", "Anton Tsitsulin", "Robert Busa-Fekete", "Alex Feng", "Noveen Sachdeva", "Benjamin Coleman", "Yi Gao", "Basil Mustafa", "Iain Barr", "Emilio Parisotto", "David Tian", "Matan Eyal", "Colin Cherry", "Jan-Thorsten Peter", "Danila Sinopalnikov", "Surya Bhupatiraju", "Rishabh Agarwal", "Mehran Kazemi", "Dan Malkin", "Ravin Kumar", "David Vilar", "Idan Brusilovsky", "Jiaming Luo", "Andreas Steiner", "Abe Friesen", "Abhanshu Sharma", "Abheesht Sharma", "Adi Mayrav Gilady", "Adrian Goedeckemeyer", "Alaa Saade", "Alex Feng", "Alexander Kolesnikov", "Alexei Bendebury", "Alvin Abdagic", "Amit Vadi", "András György", "André Susano Pinto", "Anil Das", "Ankur Bapna", "Antoine Miech", "Antoine Yang", "Antonia Paterson", "Ashish Shenoy", "Ayan Chakrabarti", "Bilal Piot", "Bo Wu", "Bobak Shahriari", "Bryce Petrini", "Charlie Chen", "Charline Le Lan", "Christopher A. Choquette-Choo", "CJ Carey", "Cormac Brick", "Daniel Deutsch", "Danielle Eisenbud", "Dee Cattle", "Derek Cheng", "Dimitris Paparas", "Divyashree Shivakumar Sreepathihalli", "Doug Reid", "Dustin Tran", "Dustin Zelle", "Eric Noland", "Erwin Huizenga", "Eugene Kharitonov", "Frederick Liu", "Gagik Amirkhanyan", "Glenn Cameron", "Hadi Hashemi", "Hanna Klimczak-Plucińska", "Harman Singh", "Harsh Mehta", "Harshal Tushar Lehri", "Hussein Hazimeh", "Ian Ballantyne", "Idan Szpektor", "Ivan Nardini", "Jean Pouget-Abadie", "Jetha Chan", "Joe Stanton", "John Wieting", "Jonathan Lai", "Jordi Orbay", "Joseph Fernandez", "Josh Newlan", "Ju-yeong Ji", "Jyotinder Singh", "Kat Black", "Kathy Yu", "Kevin Hui", "Kiran Vodrahalli", "Klaus Greff", "Linhai Qiu", "Marcella Valentine", "Marina Coelho", "Marvin Ritter", "Matt Hoffman", "Matthew Watson", "Mayank Chaturvedi", "Michael Moynihan", "Min Ma", "Nabila Babar", "Natasha Noy", "Nathan Byrd", "Nick Roy", "Nikola Momchev", "Nilay Chauhan", "Noveen Sachdeva", "Oskar Bunyan", "Pankil Botarda", "Paul Caron", "Paul Kishan Rubenstein", "Phil Culliton", "Philipp Schmid", "Pier Giuseppe Sessa", "Pingmei Xu", "Piotr Stanczyk", "Pouya Tafti", "Rakesh Shivanna", "Renjie Wu", "Renke Pan", "Reza Rokni", "Rob Willoughby", "Rohith Vallu", "Ryan Mullins", "Sammy Jerome", "Sara Smoot", "Sertan Girgin", "Shariq Iqbal", "Shashir Reddy", "Shruti Sheth", "Siim Põder", "Sijal Bhatnagar", "Sindhu Raghuram Panyam", "Sivan Eiger", "Susan Zhang", "Tianqi Liu", "Trevor Yacovone", "Tyler Liechty", "Uday Kalra", "Utku Evci", "Vedant Misra", "Vincent Roseberry", "Vlad Feinberg", "Vlad Kolesnikov", "Woohyun Han", "Woosuk Kwon", "Xi Chen", "Yinlam Chow", "Yuvein Zhu", "Zichuan Wei", "Zoltan Egyed", "Victor Cotruta", "Minh Giang", "Phoebe Kirk", "Anand Rao", "Kat Black", "Nabila Babar", "Jessica Lo", "Erica Moreira", "Luiz Gustavo Martins", "Omar Sanseviero", "Lucas Gonzalez", "Zach Gleicher", "Tris Warkentin", "Vahab Mirrokni", "Evan Senter", "Eli Collins", "Joelle Barral", "Zoubin Ghahramani", "Raia Hadsell", "Yossi Matias", "D. Sculley", "Slav Petrov", "Noah Fiedel", "Noam Shazeer", "Oriol Vinyals", "Jeff Dean", "Demis Hassabis", "Koray Kavukcuoglu", "Clement Farabet", "Elena Buchatskaya", "Jean-Baptiste Alayrac", "Rohan Anil", "Dmitry", "Lepikhin", "Sebastian Borgeaud", "Olivier Bachem", "Armand Joulin", "Alek Andreev", "Cassidy Hardin", "Robert Dadashi", "Léonard Hussenot"], "title": "Gemma 3 Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."}
{"id": "2503.19191", "pdf": "https://arxiv.org/pdf/2503.19191", "abs": "https://arxiv.org/abs/2503.19191", "authors": ["Yufan Ren", "Zicong Jiang", "Tong Zhang", "Søren Forchhammer", "Sabine Süsstrunk"], "title": "FDS: Frequency-Aware Denoising Score for Text-Guided Latent Diffusion Image Editing", "categories": ["cs.CV"], "comment": "8 pages (main paper)", "summary": "Text-guided image editing using Text-to-Image (T2I) models often fails to\nyield satisfactory results, frequently introducing unintended modifications,\nsuch as the loss of local detail and color changes. In this paper, we analyze\nthese failure cases and attribute them to the indiscriminate optimization\nacross all frequency bands, even though only specific frequencies may require\nadjustment. To address this, we introduce a simple yet effective approach that\nenables the selective optimization of specific frequency bands within localized\nspatial regions for precise edits. Our method leverages wavelets to decompose\nimages into different spatial resolutions across multiple frequency bands,\nenabling precise modifications at various levels of detail. To extend the\napplicability of our approach, we provide a comparative analysis of different\nfrequency-domain techniques. Additionally, we extend our method to 3D texture\nediting by performing frequency decomposition on the triplane representation,\nenabling frequency-aware adjustments for 3D textures. Quantitative evaluations\nand user studies demonstrate the effectiveness of our method in producing\nhigh-quality and precise edits."}
{"id": "2503.19800", "pdf": "https://arxiv.org/pdf/2503.19800", "abs": "https://arxiv.org/abs/2503.19800", "authors": ["Korbinian Randl", "John Pavlopoulos", "Aron Henriksson", "Tony Lindgren", "Juli Bakagianni"], "title": "SemEval-2025 Task 9: The Food Hazard Detection Challenge", "categories": ["cs.CL"], "comment": "Under review for SemEval 2025", "summary": "In this challenge, we explored text-based food hazard prediction with long\ntail distributed classes. The task was divided into two subtasks: (1)\npredicting whether a web text implies one of ten food-hazard categories and\nidentifying the associated food category, and (2) providing a more fine-grained\nclassification by assigning a specific label to both the hazard and the\nproduct. Our findings highlight that large language model-generated synthetic\ndata can be highly effective for oversampling long-tail distributions.\nFurthermore, we find that fine-tuned encoder-only, encoder-decoder, and\ndecoder-only systems achieve comparable maximum performance across both\nsubtasks. During this challenge, we gradually released (under CC BY-NC-SA 4.0)\na novel set of 6,644 manually labeled food-incident reports."}
{"id": "2503.19199", "pdf": "https://arxiv.org/pdf/2503.19199", "abs": "https://arxiv.org/abs/2503.19199", "authors": ["Chenyangguang Zhang", "Alexandros Delitzas", "Fangjinhua Wang", "Ruida Zhang", "Xiangyang Ji", "Marc Pollefeys", "Francis Engelmann"], "title": "Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "We introduce the task of predicting functional 3D scene graphs for real-world\nindoor environments from posed RGB-D images. Unlike traditional 3D scene graphs\nthat focus on spatial relationships of objects, functional 3D scene graphs\ncapture objects, interactive elements, and their functional relationships. Due\nto the lack of training data, we leverage foundation models, including visual\nlanguage models (VLMs) and large language models (LLMs), to encode functional\nknowledge. We evaluate our approach on an extended SceneFun3D dataset and a\nnewly collected dataset, FunGraph3D, both annotated with functional 3D scene\ngraphs. Our method significantly outperforms adapted baselines, including\nOpen3DSG and ConceptGraph, demonstrating its effectiveness in modeling complex\nscene functionalities. We also demonstrate downstream applications such as 3D\nquestion answering and robotic manipulation using functional 3D scene graphs.\nSee our project page at https://openfungraph.github.io"}
{"id": "2503.19828", "pdf": "https://arxiv.org/pdf/2503.19828", "abs": "https://arxiv.org/abs/2503.19828", "authors": ["Athiya Deviyani", "Fernando Diaz"], "title": "Contextual Metric Meta-Evaluation by Measuring Local Metric Accuracy", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (Findings)", "summary": "Meta-evaluation of automatic evaluation metrics -- assessing evaluation\nmetrics themselves -- is crucial for accurately benchmarking natural language\nprocessing systems and has implications for scientific inquiry, production\nmodel development, and policy enforcement. While existing approaches to metric\nmeta-evaluation focus on general statements about the absolute and relative\nquality of metrics across arbitrary system outputs, in practice, metrics are\napplied in highly contextual settings, often measuring the performance for a\nhighly constrained set of system outputs. For example, we may only be\ninterested in evaluating a specific model or class of models. We introduce a\nmethod for contextual metric meta-evaluation by comparing the local metric\naccuracy of evaluation metrics. Across translation, speech recognition, and\nranking tasks, we demonstrate that the local metric accuracies vary both in\nabsolute value and relative effectiveness as we shift across evaluation\ncontexts. This observed variation highlights the importance of adopting\ncontext-specific metric evaluations over global ones."}
{"id": "2503.19202", "pdf": "https://arxiv.org/pdf/2503.19202", "abs": "https://arxiv.org/abs/2503.19202", "authors": ["Sara Al-Emadi", "Yin Yang", "Ferda Ofli"], "title": "Benchmarking Object Detectors under Real-World Distribution Shifts in Satellite Imagery", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Object detectors have achieved remarkable performance in many applications;\nhowever, these deep learning models are typically designed under the i.i.d.\nassumption, meaning they are trained and evaluated on data sampled from the\nsame (source) distribution. In real-world deployment, however, target\ndistributions often differ from source data, leading to substantial performance\ndegradation. Domain Generalisation (DG) seeks to bridge this gap by enabling\nmodels to generalise to Out-Of-Distribution (OOD) data without access to target\ndistributions during training, enhancing robustness to unseen conditions. In\nthis work, we examine the generalisability and robustness of state-of-the-art\nobject detectors under real-world distribution shifts, focusing particularly on\nspatial domain shifts. Despite the need, a standardised benchmark dataset\nspecifically designed for assessing object detection under realistic DG\nscenarios is currently lacking. To address this, we introduce Real-World\nDistribution Shifts (RWDS), a suite of three novel DG benchmarking datasets\nthat focus on humanitarian and climate change applications. These datasets\nenable the investigation of domain shifts across (i) climate zones and (ii)\nvarious disasters and geographic regions. To our knowledge, these are the first\nDG benchmarking datasets tailored for object detection in real-world,\nhigh-impact contexts. We aim for these datasets to serve as valuable resources\nfor evaluating the robustness and generalisation of future object detection\nmodels. Our datasets and code are available at https://github.com/RWGAI/RWDS."}
{"id": "2503.19844", "pdf": "https://arxiv.org/pdf/2503.19844", "abs": "https://arxiv.org/abs/2503.19844", "authors": ["Zhao Fang", "Liang-Chun Wu", "Xuening Kong", "Spencer Dean Stewart"], "title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NLP4DH 2025 at NAACL 2025", "summary": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data."}
{"id": "2503.19207", "pdf": "https://arxiv.org/pdf/2503.19207", "abs": "https://arxiv.org/abs/2503.19207", "authors": ["Rong Wang", "Fabian Prada", "Ziyan Wang", "Zhongshi Jiang", "Chengxiang Yin", "Junxuan Li", "Shunsuke Saito", "Igor Santesteban", "Javier Romero", "Rohan Joshi", "Hongdong Li", "Jason Saragih", "Yaser Sheikh"], "title": "FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from Few Images", "categories": ["cs.CV"], "comment": "Published in CVPR 2025", "summary": "We present a novel method for reconstructing personalized 3D human avatars\nwith realistic animation from only a few images. Due to the large variations in\nbody shapes, poses, and cloth types, existing methods mostly require hours of\nper-subject optimization during inference, which limits their practical\napplications. In contrast, we learn a universal prior from over a thousand\nclothed humans to achieve instant feedforward generation and zero-shot\ngeneralization. Specifically, instead of rigging the avatar with shared\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\nand pose-dependent deformations, which effectively improves overall geometric\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\nvariations and resolve coupled ambiguity between canonical shapes and skinning\nweights, we design a 3D canonicalization process to produce pixel-aligned\ninitial conditions, which helps to reconstruct fine-grained geometric details.\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\nintroduced in canonicalization and fuse a plausible avatar preserving\nperson-specific identities. Finally, we train the model in an end-to-end\nframework on a large-scale capture dataset, which contains diverse human\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\nmethod generates more authentic reconstruction and animation than\nstate-of-the-arts, and can be directly generalized to inputs from casually\ntaken phone photos. Project page and code is available at\nhttps://github.com/rongakowang/FRESA."}
{"id": "2503.19855", "pdf": "https://arxiv.org/pdf/2503.19855", "abs": "https://arxiv.org/abs/2503.19855", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yunjie Ji", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer."}
{"id": "2503.19215", "pdf": "https://arxiv.org/pdf/2503.19215", "abs": "https://arxiv.org/abs/2503.19215", "authors": ["Bilal Alsallakh", "Timothy Wroge", "Vivek Miglani", "Narine Kokhlikyan"], "title": "On Symmetries in Convolutional Weights", "categories": ["cs.CV"], "comment": "Accepted to the ICLR 2025 Workshop on Weight Space Learning (WSL)", "summary": "We explore the symmetry of the mean k x k weight kernel in each layer of\nvarious convolutional neural networks. Unlike individual neurons, the mean\nkernels in internal layers tend to be symmetric about their centers instead of\nfavoring specific directions. We investigate why this symmetry emerges in\nvarious datasets and models, and how it is impacted by certain architectural\nchoices. We show how symmetry correlates with desirable properties such as\nshift and flip consistency, and might constitute an inherent inductive bias in\nconvolutional neural networks."}
{"id": "2503.19877", "pdf": "https://arxiv.org/pdf/2503.19877", "abs": "https://arxiv.org/abs/2503.19877", "authors": ["Seungone Kim", "Ian Wu", "Jinu Lee", "Xiang Yue", "Seongyun Lee", "Mingyeong Moon", "Kiril Gashteovski", "Carolin Lawrence", "Julia Hockenmaier", "Graham Neubig", "Sean Welleck"], "title": "Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "As language model (LM) outputs get more and more natural, it is becoming more\ndifficult than ever to evaluate their quality. Simultaneously, increasing LMs'\n\"thinking\" time through scaling test-time compute has proven an effective\ntechnique to solve challenging problems in domains such as math and code. This\nraises a natural question: can an LM's evaluation capability also be improved\nby spending more test-time compute? To answer this, we investigate employing\nreasoning models-LMs that natively generate long chain-of-thought reasoning-as\nevaluators. Specifically, we examine methods to leverage more test-time compute\nby (1) using reasoning models, and (2) prompting these models to evaluate not\nonly the response as a whole (i.e., outcome evaluation) but also assess each\nstep in the response separately (i.e., process evaluation). In experiments, we\nobserve that the evaluator's performance improves monotonically when generating\nmore reasoning tokens, similar to the trends observed in LM-based generation.\nFurthermore, we use these more accurate evaluators to rerank multiple\ngenerations, and demonstrate that spending more compute at evaluation time can\nbe as effective as using more compute at generation time in improving an LM's\nproblem-solving capability."}
{"id": "2503.19223", "pdf": "https://arxiv.org/pdf/2503.19223", "abs": "https://arxiv.org/abs/2503.19223", "authors": ["Najeebullah", "Maaz Salman", "Zar Nawab Khan Swati"], "title": "Face Spoofing Detection using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages, 9 figures,3 tables", "summary": "Digital image spoofing has emerged as a significant security threat in\nbiometric authentication systems, particularly those relying on facial\nrecognition. This study evaluates the performance of three vision based models,\nMobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in\nimage classification, utilizing a dataset of 150,986 images divided into\ntraining , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof\ndetection is critical for enhancing the security of image recognition systems,\nand this research compares the models effectiveness through accuracy,\nprecision, recall, and F1 score metrics. Results reveal that MobileNetV2\noutperforms other architectures on the test dataset, achieving an accuracy of\n91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared\nto ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation\ndataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17%\naccuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during\ntraining and superior generalization to unseen data, despite both models\nshowing signs of overfitting. These findings highlight MobileNetV2 balanced\nperformance and robustness, making it the preferred choice for spoof detection\napplications where reliability on new data is essential. The study underscores\nthe importance of model selection in security sensitive contexts and suggests\nMobileNetV2 as a practical solution for real world deployment."}
{"id": "2503.19878", "pdf": "https://arxiv.org/pdf/2503.19878", "abs": "https://arxiv.org/abs/2503.19878", "authors": ["Nengbo Wang", "Xiaotian Han", "Jagdip Singh", "Jing Ma", "Vipin Chaudhary"], "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks."}
{"id": "2503.19240", "pdf": "https://arxiv.org/pdf/2503.19240", "abs": "https://arxiv.org/abs/2503.19240", "authors": ["Hao Guo", "Jianfei Zhu", "Wei Fan", "Chunzhi Yi", "Feng Jiang"], "title": "Beyond Object Categories: Multi-Attribute Reference Understanding for Visual Grounding", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Referring expression comprehension (REC) aims at achieving object\nlocalization based on natural language descriptions. However, existing REC\napproaches are constrained by object category descriptions and single-attribute\nintention descriptions, hindering their application in real-world scenarios. In\nnatural human-robot interactions, users often express their desires through\nindividual states and intentions, accompanied by guiding gestures, rather than\ndetailed object descriptions. To address this challenge, we propose Multi-ref\nEC, a novel task framework that integrates state descriptions, derived\nintentions, and embodied gestures to locate target objects. We introduce the\nState-Intention-Gesture Attributes Reference (SIGAR) dataset, which combines\nstate and intention expressions with embodied references. Through extensive\nexperiments with various baseline models on SIGAR, we demonstrate that properly\nordered multi-attribute references contribute to improved localization\nperformance, revealing that single-attribute reference is insufficient for\nnatural human-robot interaction scenarios. Our findings underscore the\nimportance of multi-attribute reference expressions in advancing\nvisual-language understanding."}
{"id": "2503.19092", "pdf": "https://arxiv.org/pdf/2503.19092", "abs": "https://arxiv.org/abs/2503.19092", "authors": ["Krisztian Balog", "Donald Metzler", "Zhen Qin"], "title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integral to information\nretrieval (IR), powering ranking, evaluation, and AI-assisted content creation.\nThis widespread adoption necessitates a critical examination of potential\nbiases arising from the interplay between these LLM-based components. This\npaper synthesizes existing research and presents novel experiment designs that\nexplore how LLM-based rankers and assistants influence LLM-based judges. We\nprovide the first empirical evidence of LLM judges exhibiting significant bias\ntowards LLM-based rankers. Furthermore, we observe limitations in LLM judges'\nability to discern subtle system performance differences. Contrary to some\nprevious findings, our preliminary study does not find evidence of bias against\nAI-generated content. These results highlight the need for a more holistic view\nof the LLM-driven information ecosystem. To this end, we offer initial\nguidelines and a research agenda to ensure the reliable use of LLMs in IR\nevaluation."}
{"id": "2503.19258", "pdf": "https://arxiv.org/pdf/2503.19258", "abs": "https://arxiv.org/abs/2503.19258", "authors": ["Hui Chen", "Liangyu Liu", "Xianchao Xiu", "Wanquan Liu"], "title": "Adaptive Multi-Order Graph Regularized NMF with Dual Sparsity for Hyperspectral Unmixing", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Hyperspectral unmixing (HU) is a critical yet challenging task in remote\nsensing. However, existing nonnegative matrix factorization (NMF) methods with\ngraph learning mostly focus on first-order or second-order nearest neighbor\nrelationships and usually require manual parameter tuning, which fails to\ncharacterize intrinsic data structures. To address the above issues, we propose\na novel adaptive multi-order graph regularized NMF method (MOGNMF) with three\nkey features. First, multi-order graph regularization is introduced into the\nNMF framework to exploit global and local information comprehensively. Second,\nthese parameters associated with the multi-order graph are learned adaptively\nthrough a data-driven approach. Third, dual sparsity is embedded to obtain\nbetter robustness, i.e., $\\ell_{1/2}$-norm on the abundance matrix and\n$\\ell_{2,1}$-norm on the noise matrix. To solve the proposed model, we develop\nan alternating minimization algorithm whose subproblems have explicit\nsolutions, thus ensuring effectiveness. Experiments on simulated and real\nhyperspectral data indicate that the proposed method delivers better unmixing\nresults."}
{"id": "2503.19193", "pdf": "https://arxiv.org/pdf/2503.19193", "abs": "https://arxiv.org/abs/2503.19193", "authors": ["Sky CH-Wang", "Darshan Deshpande", "Smaranda Muresan", "Anand Kannappan", "Rebecca Qian"], "title": "Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue Search and Reasoning", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.MA"], "comment": null, "summary": "We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue\nknown-item search and reasoning benchmark for general AI assistants. BLUR\nintroduces a set of 573 real-world validated questions that demand searching\nand reasoning across multi-modal and multilingual inputs, as well as proficient\ntool use, in order to excel on. Humans easily ace these questions (scoring on\naverage 98%), while the best-performing system scores around 56%. To facilitate\nprogress toward addressing this challenging and aspirational use case for\ngeneral AI assistants, we release 350 questions through a public leaderboard,\nretain the answers to 250 of them, and have the rest as a private test set."}
{"id": "2503.19262", "pdf": "https://arxiv.org/pdf/2503.19262", "abs": "https://arxiv.org/abs/2503.19262", "authors": ["Ruiyi Wang", "Yushuo Zheng", "Zicheng Zhang", "Chunyi Li", "Shuaicheng Liu", "Guangtao Zhai", "Xiaohong Liu"], "title": "Learning Hazing to Dehazing: Towards Realistic Haze Generation for Real-World Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Existing real-world image dehazing methods primarily attempt to fine-tune\npre-trained models or adapt their inference procedures, thus heavily relying on\nthe pre-trained models and associated training data. Moreover, restoring\nheavily distorted information under dense haze requires generative diffusion\nmodels, whose potential in dehazing remains underutilized partly due to their\nlengthy sampling processes. To address these limitations, we introduce a novel\nhazing-dehazing pipeline consisting of a Realistic Hazy Image Generation\nframework (HazeGen) and a Diffusion-based Dehazing framework (DiffDehaze).\nSpecifically, HazeGen harnesses robust generative diffusion priors of\nreal-world hazy images embedded in a pre-trained text-to-image diffusion model.\nBy employing specialized hybrid training and blended sampling strategies,\nHazeGen produces realistic and diverse hazy images as high-quality training\ndata for DiffDehaze. To alleviate the inefficiency and fidelity concerns\nassociated with diffusion-based methods, DiffDehaze adopts an Accelerated\nFidelity-Preserving Sampling process (AccSamp). The core of AccSamp is the\nTiled Statistical Alignment Operation (AlignOp), which can provide a clean and\nfaithful dehazing estimate within a small fraction of sampling steps to reduce\ncomplexity and enable effective fidelity guidance. Extensive experiments\ndemonstrate the superior dehazing performance and visual quality of our\napproach over existing methods. The code is available at\nhttps://github.com/ruiyi-w/Learning-Hazing-to-Dehazing."}
{"id": "2503.19353", "pdf": "https://arxiv.org/pdf/2503.19353", "abs": "https://arxiv.org/abs/2503.19353", "authors": ["Yuxuan Hu", "Xiaodong Chen", "Cuiping Li", "Hong Chen", "Jing Zhang"], "title": "QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition", "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "18 pages, 8 figures, 8 tables", "summary": "Large Language Models (LLMs) excel in diverse applications but suffer\ninefficiency due to massive scale. While quantization reduces computational\ncosts, existing methods degrade accuracy in medium-sized LLMs (e.g.,\nLlama-3-8B) due to activation outliers. To address this, we propose QUAD\n(Quantization with Activation Decomposition), a framework leveraging Singular\nValue Decomposition (SVD) to suppress activation outliers for effective 4-bit\nquantization. QUAD estimates activation singular vectors offline using\ncalibration data to construct an orthogonal transformation matrix P, shifting\noutliers to additional dimensions in full precision while quantizing rest\ncomponents to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning\nvia adaptable full-precision outlier weights, narrowing the accuracy gap\nbetween quantized and full-precision models. Experiments demonstrate that QUAD\nachieves 94% ~ 96% accuracy under W4A4 quantization and 98% accuracy with\nW4A4/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.\nOur code is available at \\href{https://github.com/hyx1999/Quad}{repository}."}
{"id": "2503.19263", "pdf": "https://arxiv.org/pdf/2503.19263", "abs": "https://arxiv.org/abs/2503.19263", "authors": ["Fucai Ke", "Vijay Kumar B G", "Xingjian Leng", "Zhixi Cai", "Zaid Khan", "Weiqing Wang", "Pari Delir Haghighi", "Hamid Rezatofighi", "Manmohan Chandraker"], "title": "DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow Generation & Instruct-Masking Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning (VR), which is crucial in many fields for enabling\nhuman-like visual understanding, remains highly challenging. Recently,\ncompositional visual reasoning approaches, which leverage the reasoning\nabilities of large language models (LLMs) with integrated tools to solve\nproblems, have shown promise as more effective strategies than end-to-end VR\nmethods. However, these approaches face limitations, as frozen LLMs lack tool\nawareness in VR, leading to performance bottlenecks. While leveraging LLMs for\nreasoning is widely used in other domains, they are not directly applicable to\nVR due to limited training data, imperfect tools that introduce errors and\nreduce data collection efficiency in VR, and challenging in fine-tuning on\nnoisy workflows. To address these challenges, we propose DWIM: i)\nDiscrepancy-aware training Workflow generation, which assesses tool usage and\nextracts more viable workflows for training; and ii) Instruct-Masking\nfine-tuning, which guides the model to only clone effective actions, enabling\nthe generation of more practical solutions. Our experiments demonstrate that\nDWIM achieves state-of-the-art performance across various VR tasks, exhibiting\nstrong generalization on multiple widely-used datasets."}
{"id": "2503.19470", "pdf": "https://arxiv.org/pdf/2503.19470", "abs": "https://arxiv.org/abs/2503.19470", "authors": ["Mingyang Chen", "Tianpeng Li", "Haoze Sun", "Yijie Zhou", "Chenzheng Zhu", "Fan Yang", "Zenan Zhou", "Weipeng Chen", "Haofen Wang", "Jeff Z. Pan", "Wen Zhang", "Huajun Chen"], "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process."}
{"id": "2503.19276", "pdf": "https://arxiv.org/pdf/2503.19276", "abs": "https://arxiv.org/abs/2503.19276", "authors": ["Ben Rahman"], "title": "Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation has made significant strides in pixel-level image\nunderstanding, yet it remains limited in capturing contextual and semantic\nrelationships between objects. Current models, such as CNN and\nTransformer-based architectures, excel at identifying pixel-level features but\nfail to distinguish semantically similar objects (e.g., \"doctor\" vs. \"nurse\" in\na hospital scene) or understand complex contextual scenarios (e.g.,\ndifferentiating a running child from a regular pedestrian in autonomous\ndriving). To address these limitations, we proposed a novel Context-Aware\nSemantic Segmentation framework that integrates Large Language Models (LLMs)\nwith state-of-the-art vision backbones. Our hybrid model leverages the Swin\nTransformer for robust visual feature extraction and GPT-4 for enriching\nsemantic understanding through text embeddings. A Cross-Attention Mechanism is\nintroduced to align vision and language features, enabling the model to reason\nabout context more effectively. Additionally, Graph Neural Networks (GNNs) are\nemployed to model object relationships within the scene, capturing dependencies\nthat are overlooked by traditional models. Experimental results on benchmark\ndatasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the\nexisting methods in both pixel-level accuracy (mIoU) and contextual\nunderstanding (mAP). This work bridges the gap between vision and language,\npaving the path for more intelligent and context-aware vision systems in\napplications including autonomous driving, medical imaging, and robotics."}
{"id": "2503.19584", "pdf": "https://arxiv.org/pdf/2503.19584", "abs": "https://arxiv.org/abs/2503.19584", "authors": ["Songtao Sun", "Jingyi Li", "Yuanfei Dong", "Haoguang Liu", "Chenxin Xu", "Fuyang Li", "Qiang Liu"], "title": "Multi-agent Application System in Office Collaboration Scenarios", "categories": ["cs.AI", "cs.CL", "cs.SE"], "comment": "Technical report", "summary": "This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems."}
{"id": "2503.19278", "pdf": "https://arxiv.org/pdf/2503.19278", "abs": "https://arxiv.org/abs/2503.19278", "authors": ["Junle Liu", "Yun Zhang", "Zixi Guo"], "title": "Multiscale Feature Importance-based Bit Allocation for End-to-End Feature Coding for Machines", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Feature Coding for Machines (FCM) aims to compress intermediate features\neffectively for remote intelligent analytics, which is crucial for future\nintelligent visual applications. In this paper, we propose a Multiscale Feature\nImportance-based Bit Allocation (MFIBA) for end-to-end FCM. First, we find that\nthe importance of features for machine vision tasks varies with the scales,\nobject size, and image instances. Based on this finding, we propose a\nMultiscale Feature Importance Prediction (MFIP) module to predict the\nimportance weight for each scale of features. Secondly, we propose a task\nloss-rate model to establish the relationship between the task accuracy losses\nof using compressed features and the bitrate of encoding these features.\nFinally, we develop a MFIBA for end-to-end FCM, which is able to assign coding\nbits of multiscale features more reasonably based on their importance.\nExperimental results demonstrate that when combined with a retained Efficient\nLearned Image Compression (ELIC), the proposed MFIBA achieves an average of\n38.202% bitrate savings in object detection compared to the anchor ELIC.\nMoreover, the proposed MFIBA achieves an average of 17.212% and 36.492% feature\nbitrate savings for instance segmentation and keypoint detection, respectively.\nWhen the proposed MFIBA is applied to the LIC-TCM, it achieves an average of\n18.103%, 19.866% and 19.597% bit rate savings on three machine vision tasks,\nrespectively, which validates the proposed MFIBA has good generalizability and\nadaptability to different machine vision tasks and FCM base codecs."}
{"id": "2503.19605", "pdf": "https://arxiv.org/pdf/2503.19605", "abs": "https://arxiv.org/abs/2503.19605", "authors": ["Sho Sonoda", "Kazumi Kasaura", "Yuma Mizuno", "Kei Tsukamoto", "Naoto Onda"], "title": "Lean Formalization of Generalization Error Bound by Rademacher Complexity", "categories": ["cs.LG", "cs.CL", "math.ST", "stat.TH"], "comment": null, "summary": "We formalize the generalization error bound using Rademacher complexity in\nthe Lean 4 theorem prover. Generalization error quantifies the gap between a\nlearning machine's performance on given training data versus unseen test data,\nand Rademacher complexity serves as an estimate of this error based on the\ncomplexity of learning machines, or hypothesis class. Unlike traditional\nmethods such as PAC learning and VC dimension, Rademacher complexity is\napplicable across diverse machine learning scenarios including deep learning\nand kernel methods. We formalize key concepts and theorems, including the\nempirical and population Rademacher complexities, and establish generalization\nerror bounds through formal proofs of McDiarmid's inequality, Hoeffding's\nlemma, and symmetrization arguments."}
{"id": "2503.19283", "pdf": "https://arxiv.org/pdf/2503.19283", "abs": "https://arxiv.org/abs/2503.19283", "authors": ["Yang Ren", "Hai Jiang", "Menglong Yang", "Wei Li", "Shuaicheng Liu"], "title": "ISPDiffuser: Learning RAW-to-sRGB Mappings with Texture-Aware Diffusion Models and Histogram-Guided Color Consistency", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "RAW-to-sRGB mapping, or the simulation of the traditional camera image signal\nprocessor (ISP), aims to generate DSLR-quality sRGB images from raw data\ncaptured by smartphone sensors. Despite achieving comparable results to\nsophisticated handcrafted camera ISP solutions, existing learning-based methods\nstill struggle with detail disparity and color distortion. In this paper, we\npresent ISPDiffuser, a diffusion-based decoupled framework that separates the\nRAW-to-sRGB mapping into detail reconstruction in grayscale space and color\nconsistency mapping from grayscale to sRGB. Specifically, we propose a\ntexture-aware diffusion model that leverages the generative ability of\ndiffusion models to focus on local detail recovery, in which a texture\nenrichment loss is further proposed to prompt the diffusion model to generate\nmore intricate texture details. Subsequently, we introduce a histogram-guided\ncolor consistency module that utilizes color histogram as guidance to learn\nprecise color information for grayscale to sRGB color consistency mapping, with\na color consistency loss designed to constrain the learned color information.\nExtensive experimental results show that the proposed ISPDiffuser outperforms\nstate-of-the-art competitors both quantitatively and visually. The code is\navailable at https://github.com/RenYangSCU/ISPDiffuser."}
{"id": "2503.19707", "pdf": "https://arxiv.org/pdf/2503.19707", "abs": "https://arxiv.org/abs/2503.19707", "authors": ["Ilias Stogiannidis", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "8 main pages, 4 pages Appendix, 5 figures", "summary": "Vision-Language Models (VLMs) have recently emerged as powerful tools,\nexcelling in tasks that integrate visual and textual comprehension, such as\nimage captioning, visual question answering, and image-text retrieval. However,\nexisting benchmarks for VLMs include spatial components, which often fail to\nisolate spatial reasoning from related tasks such as object detection or\nsemantic comprehension. In this paper, we address these deficiencies with a\nmulti-faceted approach towards understanding spatial reasoning. Informed by the\ndiverse and multi-dimensional nature of human spatial reasoning abilities, we\npresent a detailed analysis that first delineates the core elements of spatial\nreasoning: spatial relations, orientation and navigation, mental rotation, and\nspatial visualization, and then assesses the performance of these models in\nboth synthetic and real-world images, bridging controlled and naturalistic\ncontexts. We analyze 13 state-of-the-art Vision-Language Models, uncovering\npivotal insights into their spatial reasoning performance. Our results reveal\nprofound shortcomings in current VLMs, with average accuracy across the 13\nmodels approximating random chance, highlighting spatial reasoning as a\npersistent obstacle. This work not only exposes the pressing need to advance\nspatial reasoning within VLMs but also establishes a solid platform for future\nexploration. Code available on GitHub (https://github.com/stogiannidis/srbench)\nand dataset available on HuggingFace\n(https://huggingface.co/datasets/stogiannidis/srbench)."}
{"id": "2503.19295", "pdf": "https://arxiv.org/pdf/2503.19295", "abs": "https://arxiv.org/abs/2503.19295", "authors": ["Guanglu Dong", "Xiangyu Liao", "Mingyang Li", "Guihuan Guo", "Chao Ren"], "title": "Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to CVPR2025", "summary": "Generative Adversarial Networks (GANs) have been widely applied to image\nsuper-resolution (SR) to enhance the perceptual quality. However, most existing\nGAN-based SR methods typically perform coarse-grained discrimination directly\non images and ignore the semantic information of images, making it challenging\nfor the super resolution networks (SRN) to learn fine-grained and\nsemantic-related texture details. To alleviate this issue, we propose a\nsemantic feature discrimination method, SFD, for perceptual SR. Specifically,\nwe first design a feature discriminator (Feat-D), to discriminate the\npixel-wise middle semantic features from CLIP, aligning the feature\ndistributions of SR images with that of high-quality images. Additionally, we\npropose a text-guided discrimination method (TG-D) by introducing learnable\nprompt pairs (LPP) in an adversarial manner to perform discrimination on the\nmore abstract output feature of CLIP, further enhancing the discriminative\nability of our method. With both Feat-D and TG-D, our SFD can effectively\ndistinguish between the semantic feature distributions of low-quality and\nhigh-quality images, encouraging SRN to generate more realistic and\nsemantic-relevant textures. Furthermore, based on the trained Feat-D and LPP,\nwe propose a novel opinion-unaware no-reference image quality assessment (OU\nNR-IQA) method, SFD-IQA, greatly improving OU NR-IQA performance without any\nadditional targeted training. Extensive experiments on classical SISR,\nreal-world SISR, and OU NR-IQA tasks demonstrate the effectiveness of our\nproposed methods."}
{"id": "2503.19900", "pdf": "https://arxiv.org/pdf/2503.19900", "abs": "https://arxiv.org/abs/2503.19900", "authors": ["Hao Yu", "Zhuokai Zhao", "Shen Yan", "Lukasz Korycki", "Jianyu Wang", "Baosheng He", "Jiayi Liu", "Lizhu Zhang", "Xiangjun Fan", "Hanchao Yu"], "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation."}
{"id": "2503.19296", "pdf": "https://arxiv.org/pdf/2503.19296", "abs": "https://arxiv.org/abs/2503.19296", "authors": ["Haoqiang Lin", "Haokun Wen", "Xuemeng Song", "Meng Liu", "Yupeng Hu", "Liqiang Nie"], "title": "Fine-grained Textual Inversion Network for Zero-Shot Composed Image Retrieval", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Composed Image Retrieval (CIR) allows users to search target images with a\nmultimodal query, comprising a reference image and a modification text that\ndescribes the user's modification demand over the reference image.\nNevertheless, due to the expensive labor cost of training data annotation,\nrecent researchers have shifted to the challenging task of zero-shot CIR\n(ZS-CIR), which targets fulfilling CIR without annotated triplets. The pioneer\nZS-CIR studies focus on converting the CIR task into a standard text-to-image\nretrieval task by pre-training a textual inversion network that can map a given\nimage into a single pseudo-word token. Despite their significant progress,\ntheir coarse-grained textual inversion may be insufficient to capture the full\ncontent of the image accurately. To overcome this issue, in this work, we\npropose a novel Fine-grained Textual Inversion Network for ZS-CIR, named\nFTI4CIR. In particular, FTI4CIR comprises two main components: fine-grained\npseudo-word token mapping and tri-wise caption-based semantic regularization.\nThe former maps the image into a subject-oriented pseudo-word token and several\nattribute-oriented pseudo-word tokens to comprehensively express the image in\nthe textual form, while the latter works on jointly aligning the fine-grained\npseudo-word tokens to the real-word token embedding space based on a\nBLIP-generated image caption template. Extensive experiments conducted on three\nbenchmark datasets demonstrate the superiority of our proposed method."}
{"id": "2503.19303", "pdf": "https://arxiv.org/pdf/2503.19303", "abs": "https://arxiv.org/abs/2503.19303", "authors": ["Hanshuo Qiu", "Jie Jiang", "Ruoli Yang", "Lixin Zhan", "Jizhao Liu"], "title": "BIMII-Net: Brain-Inspired Multi-Iterative Interactive Network for RGB-T Road Scene Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "RGB-T road scene semantic segmentation enhances visual scene understanding in\ncomplex environments characterized by inadequate illumination or occlusion by\nfusing information from RGB and thermal images. Nevertheless, existing RGB-T\nsemantic segmentation models typically depend on simple addition or\nconcatenation strategies or ignore the differences between information at\ndifferent levels. To address these issues, we proposed a novel RGB-T road scene\nsemantic segmentation network called Brain-Inspired Multi-Iteration Interaction\nNetwork (BIMII-Net). First, to meet the requirements of accurate texture and\nlocal information extraction in road scenarios like autonomous driving, we\nproposed a deep continuous-coupled neural network (DCCNN) architecture based on\na brain-inspired model. Second, to enhance the interaction and expression\ncapabilities among multi-modal information, we designed a cross explicit\nattention-enhanced fusion module (CEAEF-Module) in the feature fusion stage of\nBIMII-Net to effectively integrate features at different levels. Finally, we\nconstructed a complementary interactive multi-layer decoder structure,\nincorporating the shallow-level feature iteration module (SFI-Module), the\ndeep-level feature iteration module (DFI-Module), and the multi-feature\nenhancement module (MFE-Module) to collaboratively extract texture details and\nglobal skeleton information, with multi-module joint supervision further\noptimizing the segmentation results. Experimental results demonstrate that\nBIMII-Net achieves state-of-the-art (SOTA) performance in the brain-inspired\ncomputing domain and outperforms most existing RGB-T semantic segmentation\nmethods. It also exhibits strong generalization capabilities on multiple RGB-T\ndatasets, proving the effectiveness of brain-inspired computer models in\nmulti-modal image segmentation tasks."}
{"id": "2503.19307", "pdf": "https://arxiv.org/pdf/2503.19307", "abs": "https://arxiv.org/abs/2503.19307", "authors": ["Zhuoran Zhao", "Linlin Yang", "Pengzhan Sun", "Pan Hui", "Angela Yao"], "title": "Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent synthetic 3D human datasets for the face, body, and hands have pushed\nthe limits on photorealism. Face recognition and body pose estimation have\nachieved state-of-the-art performance using synthetic training data alone, but\nfor the hand, there is still a large synthetic-to-real gap. This paper presents\nthe first systematic study of the synthetic-to-real gap of 3D hand pose\nestimation. We analyze the gap and identify key components such as the forearm,\nimage frequency statistics, hand pose, and object occlusions. To facilitate our\nanalysis, we propose a data synthesis pipeline to synthesize high-quality data.\nWe demonstrate that synthetic hand data can achieve the same level of accuracy\nas real data when integrating our identified components, paving the path to use\nsynthetic data alone for hand pose estimation. Code and data are available at:\nhttps://github.com/delaprada/HandSynthesis.git."}
{"id": "2503.19308", "pdf": "https://arxiv.org/pdf/2503.19308", "abs": "https://arxiv.org/abs/2503.19308", "authors": ["Chaohan Wang", "Yutong Xie", "Qi Chen", "Yuyin Zhou", "Qi Wu"], "title": "A Comprehensive Analysis of Mamba for 3D Volumetric Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Mamba, with its selective State Space Models (SSMs), offers a more\ncomputationally efficient solution than Transformers for long-range dependency\nmodeling. However, there is still a debate about its effectiveness in\nhigh-resolution 3D medical image segmentation. In this study, we present a\ncomprehensive investigation into Mamba's capabilities in 3D medical image\nsegmentation by tackling three pivotal questions: Can Mamba replace\nTransformers? Can it elevate multi-scale representation learning? Is complex\nscanning necessary to unlock its full potential? We evaluate Mamba's\nperformance across three large public benchmarks-AMOS, TotalSegmentator, and\nBraTS. Our findings reveal that UlikeMamba, a U-shape Mamba-based network,\nconsistently surpasses UlikeTrans, a U-shape Transformer-based network,\nparticularly when enhanced with custom-designed 3D depthwise convolutions,\nboosting accuracy and computational efficiency. Further, our proposed\nmulti-scale Mamba block demonstrates superior performance in capturing both\nfine-grained details and global context, especially in complex segmentation\ntasks, surpassing Transformer-based counterparts. We also critically assess\ncomplex scanning strategies, finding that simpler methods often suffice, while\nour Tri-scan approach delivers notable advantages in the most challenging\nscenarios. By integrating these advancements, we introduce a new network for 3D\nmedical image segmentation, positioning Mamba as a transformative force that\noutperforms leading models such as nnUNet, CoTr, and U-Mamba, offering\ncompetitive accuracy with superior computational efficiency. This study\nprovides key insights into Mamba's unique advantages, paving the way for more\nefficient and accurate approaches to 3D medical imaging."}
{"id": "2503.19311", "pdf": "https://arxiv.org/pdf/2503.19311", "abs": "https://arxiv.org/abs/2503.19311", "authors": ["Weizhi Chen", "Jingbo Chen", "Yupeng Deng", "Jiansheng Chen", "Yuman Feng", "Zhihao Xi", "Diyou Liu", "Kai Li", "Yu Meng"], "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 12 figures", "summary": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP."}
{"id": "2503.19312", "pdf": "https://arxiv.org/pdf/2503.19312", "abs": "https://arxiv.org/abs/2503.19312", "authors": ["Jiaqi Liao", "Zhengyuan Yang", "Linjie Li", "Dianqi Li", "Kevin Lin", "Yu Cheng", "Lijuan Wang"], "title": "ImageGen-CoT: Enhancing Text-to-Image In-context Learning with Chain-of-Thought Reasoning", "categories": ["cs.CV"], "comment": "Project Page: https://ImageGen-CoT.github.io/", "summary": "In this work, we study the problem of Text-to-Image In-Context Learning\n(T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in\nrecent years, they struggle with contextual reasoning in T2I-ICL scenarios. To\naddress this limitation, we propose a novel framework that incorporates a\nthought process called ImageGen-CoT prior to image generation. To avoid\ngenerating unstructured ineffective reasoning steps, we develop an automatic\npipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs\nusing this dataset to enhance their contextual reasoning capabilities. To\nfurther enhance performance, we explore test-time scale-up strategies and\npropose a novel hybrid scaling approach. This approach first generates multiple\nImageGen-CoT chains and then produces multiple images for each chain via\nsampling. Extensive experiments demonstrate the effectiveness of our proposed\nmethod. Notably, fine-tuning with the ImageGen-CoT dataset leads to a\nsubstantial 80\\% performance gain for SEED-X on T2I-ICL tasks. See our project\npage at https://ImageGen-CoT.github.io/. Code and model weights will be\nopen-sourced."}
{"id": "2503.19325", "pdf": "https://arxiv.org/pdf/2503.19325", "abs": "https://arxiv.org/abs/2503.19325", "authors": ["Yuchao Gu", "Weijia Mao", "Mike Zheng Shou"], "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction", "categories": ["cs.CV"], "comment": "Project page at https://farlongctx.github.io/", "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling."}
{"id": "2503.19331", "pdf": "https://arxiv.org/pdf/2503.19331", "abs": "https://arxiv.org/abs/2503.19331", "authors": ["Chau Pham", "Juan C. Caicedo", "Bryan A. Plummer"], "title": "ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel Vision Transformers for Improved Cross-Channel Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Prior work using Masked Autoencoders (MAEs) typically relies on random patch\nmasking based on the assumption that images have significant redundancies\nacross different channels, allowing for the reconstruction of masked content\nusing cross-channel correlations. However, this assumption does not hold in\nMulti-Channel Imaging (MCI), where channels may provide complementary\ninformation with minimal feature overlap. Thus, these MAEs primarily learn\nlocal structures within individual channels from patch reconstruction, failing\nto fully leverage cross-channel interactions and limiting their MCI\neffectiveness. In this paper, we present ChA-MAEViT, an MAE-based method that\nenhances feature learning across MCI channels via four key strategies: (1)\ndynamic channel-patch masking, which compels the model to reconstruct missing\nchannels in addition to masked patches, thereby enhancing cross-channel\ndependencies and improving robustness to varying channel configurations; (2)\nmemory tokens, which serve as long-term memory aids to promote information\nsharing across channels, addressing the challenges of reconstructing\nstructurally diverse channels; (3) hybrid token fusion module, which merges\nfine-grained patch tokens with a global class token to capture richer\nrepresentations; and (4) Channel-Aware Decoder, a lightweight decoder utilizes\nchannel tokens to effectively reconstruct image patches. Experiments on\nsatellite and microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, show that\nChA-MAEViT significantly outperforms state-of-the-art MCI-ViTs by 3.0-21.5%,\nhighlighting the importance of cross-channel interactions in MCI."}
{"id": "2503.19332", "pdf": "https://arxiv.org/pdf/2503.19332", "abs": "https://arxiv.org/abs/2503.19332", "authors": ["Zhiying Yan", "Yiyuan Liang", "Shilv Cai", "Tao Zhang", "Sheng Zhong", "Luxin Yan", "Xu Zou"], "title": "Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D Gaussian Spatting", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "Semantic 4D Gaussians can be used for reconstructing and understanding\ndynamic scenes, with temporal variations than static scenes. Directly applying\nstatic methods to understand dynamic scenes will fail to capture the temporal\nfeatures. Few works focus on dynamic scene understanding based on Gaussian\nSplatting, since once the same update strategy is employed for both dynamic and\nstatic parts, regardless of the distinction and interaction between Gaussians,\nsignificant artifacts and noise appear. We propose Dual-Hierarchical\nOptimization (DHO), which consists of Hierarchical Gaussian Flow and\nHierarchical Gaussian Guidance in a divide-and-conquer manner. The former\nimplements effective division of static and dynamic rendering and features. The\nlatter helps to mitigate the issue of dynamic foreground rendering distortion\nin textured complex scenes. Extensive experiments show that our method\nconsistently outperforms the baselines on both synthetic and real-world\ndatasets, and supports various downstream tasks. Project Page:\nhttps://sweety-yan.github.io/DHO."}
{"id": "2503.19340", "pdf": "https://arxiv.org/pdf/2503.19340", "abs": "https://arxiv.org/abs/2503.19340", "authors": ["Yuguang Li", "Ivaylo Boyadzhiev", "Zixuan Liu", "Linda Shapiro", "Alex Colburn"], "title": "BADGR: Bundle Adjustment Diffusion Conditioned by GRadients for Wide-Baseline Floor Plan Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing precise camera poses and floor plan layouts from wide-baseline\nRGB panoramas is a difficult and unsolved problem. We introduce BADGR, a novel\ndiffusion model that jointly performs reconstruction and bundle adjustment (BA)\nto refine poses and layouts from a coarse state, using 1D floor boundary\npredictions from dozens of images of varying input densities. Unlike a guided\ndiffusion model, BADGR is conditioned on dense per-entity outputs from a\nsingle-step Levenberg Marquardt (LM) optimizer and is trained to predict camera\nand wall positions while minimizing reprojection errors for view-consistency.\nThe objective of layout generation from denoising diffusion process complements\nBA optimization by providing additional learned layout-structural constraints\non top of the co-visible features across images. These constraints help BADGR\nto make plausible guesses on spatial relations which help constrain pose graph,\nsuch as wall adjacency, collinearity, and learn to mitigate errors from dense\nboundary observations with global contexts. BADGR trains exclusively on 2D\nfloor plans, simplifying data acquisition, enabling robust augmentation, and\nsupporting variety of input densities. Our experiments and analysis validate\nour method, which significantly outperforms the state-of-the-art pose and floor\nplan layout reconstruction with different input densities."}
{"id": "2503.19347", "pdf": "https://arxiv.org/pdf/2503.19347", "abs": "https://arxiv.org/abs/2503.19347", "authors": ["Philip Doldo", "Derek Everett", "Amol Khanna", "Andre T Nguyen", "Edward Raff"], "title": "Stop Walking in Circles! Bailing Out Early in Projected Gradient Descent", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": "To appear in the 2025 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR)", "summary": "Projected Gradient Descent (PGD) under the $L_\\infty$ ball has become one of\nthe defacto methods used in adversarial robustness evaluation for computer\nvision (CV) due to its reliability and efficacy, making a strong and\neasy-to-implement iterative baseline. However, PGD is computationally demanding\nto apply, especially when using thousands of iterations is the current\nbest-practice recommendation to generate an adversarial example for a single\nimage. In this work, we introduce a simple novel method for early termination\nof PGD based on cycle detection by exploiting the geometry of how PGD is\nimplemented in practice and show that it can produce large speedup factors\nwhile providing the \\emph{exact} same estimate of model robustness as standard\nPGD. This method substantially speeds up PGD without sacrificing any attack\nstrength, enabling evaluations of robustness that were previously\ncomputationally intractable."}
{"id": "2503.19351", "pdf": "https://arxiv.org/pdf/2503.19351", "abs": "https://arxiv.org/abs/2503.19351", "authors": ["Jingyu Liu", "Zijie Xin", "Yuhan Fu", "Ruixiang Zhao", "Bangxiang Lan", "Xirong Li"], "title": "Multi-Object Sketch Animation by Scene Decomposition and Motion Planning", "categories": ["cs.CV"], "comment": "16 pages, 17 figures", "summary": "Sketch animation, which brings static sketches to life by generating dynamic\nvideo sequences, has found widespread applications in GIF design, cartoon\nproduction, and daily entertainment. While current sketch animation methods\nperform well in single-object sketch animation, they struggle in multi-object\nscenarios. By analyzing their failures, we summarize two challenges of\ntransitioning from single-object to multi-object sketch animation: object-aware\nmotion modeling and complex motion optimization. For multi-object sketch\nanimation, we propose MoSketch based on iterative optimization through Score\nDistillation Sampling (SDS), without any other data for training. We propose\nfour modules: LLM-based scene decomposition, LLM-based motion planning, motion\nrefinement network and compositional SDS, to tackle the two challenges in a\ndivide-and-conquer strategy. Extensive qualitative and quantitative experiments\ndemonstrate the superiority of our method over existing sketch animation\napproaches. MoSketch takes a pioneering step towards multi-object sketch\nanimation, opening new avenues for future research and applications. The code\nwill be released."}
{"id": "2503.19355", "pdf": "https://arxiv.org/pdf/2503.19355", "abs": "https://arxiv.org/abs/2503.19355", "authors": ["Dohwan Ko", "Sihyeon Kim", "Yumin Suh", "Vijay Kumar B. G", "Minseo Yoon", "Manmohan Chandraker", "Hyunwoo J. Kim"], "title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Spatio-temporal reasoning is essential in understanding real-world\nenvironments in various fields, eg, autonomous driving and sports analytics.\nRecent advances have improved the spatial reasoning ability of Vision-Language\nModels (VLMs) by introducing large-scale data, but these models still struggle\nto analyze kinematic elements like traveled distance and speed of moving\nobjects. To bridge this gap, we construct a spatio-temporal reasoning dataset\nand benchmark involving kinematic instruction tuning, referred to as STKit and\nSTKit-Bench. They consist of real-world videos with 3D annotations, detailing\nobject motion dynamics: traveled distance, speed, movement direction,\ninter-object distance comparisons, and relative movement direction. To further\nscale such data construction to videos without 3D labels, we propose an\nautomatic pipeline to generate pseudo-labels using 4D reconstruction in\nreal-world scale. With our kinematic instruction tuning data for\nspatio-temporal reasoning, we present ST-VLM, a VLM enhanced for\nspatio-temporal reasoning, which exhibits outstanding performance on\nSTKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across\ndiverse domains and tasks, outperforming baselines on other spatio-temporal\nbenchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned\nspatio-temporal reasoning with existing abilities, ST-VLM enables complex\nmulti-step reasoning. Project page: https://ikodoh.github.io/ST-VLM."}
{"id": "2503.19356", "pdf": "https://arxiv.org/pdf/2503.19356", "abs": "https://arxiv.org/abs/2503.19356", "authors": ["Reza Pourreza", "Rishit Dagli", "Apratim Bhattacharyya", "Sunny Panchal", "Guillaume Berger", "Roland Memisevic"], "title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?", "categories": ["cs.CV"], "comment": null, "summary": "AI models have made significant strides in recent years in their ability to\ndescribe and answer questions about real-world images. They have also made\nprogress in the ability to converse with users in real-time using audio input.\nThis raises the question: have we reached the point where AI models, connected\nto a camera and microphone, can converse with users in real-time about scenes\nand events that are unfolding live in front of the camera? This has been a\nlong-standing goal in AI and is a prerequisite for real-world AI assistants and\nhumanoid robots to interact with humans in everyday situations. In this work,\nwe introduce a new dataset and benchmark, the Qualcomm Interactive Video\nDataset (IVD), which allows us to assess the extent to which existing models\ncan support these abilities, and to what degree these capabilities can be\ninstilled through fine-tuning. The dataset is based on a simple\nquestion-answering setup, where users ask questions that the system has to\nanswer, in real-time, based on the camera and audio input. We show that\nexisting models fall far behind human performance on this task, and we identify\nthe main sources for the performance gap. However, we also show that for many\nof the required perceptual skills, fine-tuning on this form of data can\nsignificantly reduce this gap."}
{"id": "2503.19357", "pdf": "https://arxiv.org/pdf/2503.19357", "abs": "https://arxiv.org/abs/2503.19357", "authors": ["Farzad Beizaee", "Gregory A. Lodygensky", "Christian Desrosiers", "Jose Dolz"], "title": "Correcting Deviations from Normality: A Reformulated Diffusion Model for Multi-Class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have spurred research into their\napplication for Reconstruction-based unsupervised anomaly detection. However,\nthese methods may struggle with maintaining structural integrity and recovering\nthe anomaly-free content of abnormal regions, especially in multi-class\nscenarios. Furthermore, diffusion models are inherently designed to generate\nimages from pure noise and struggle to selectively alter anomalous regions of\nan image while preserving normal ones. This leads to potential degradation of\nnormal regions during reconstruction, hampering the effectiveness of anomaly\ndetection. This paper introduces a reformulation of the standard diffusion\nmodel geared toward selective region alteration, allowing the accurate\nidentification of anomalies. By modeling anomalies as noise in the latent\nspace, our proposed Deviation correction diffusion (DeCo-Diff) model preserves\nthe normal regions and encourages transformations exclusively on anomalous\nareas. This selective approach enhances the reconstruction quality,\nfacilitating effective unsupervised detection and localization of anomaly\nregions. Comprehensive evaluations demonstrate the superiority of our method in\naccurately identifying and localizing anomalies in complex images, with\npixel-level AUPRC improvements of 11-14% over state-of-the-art models on well\nknown anomaly detection datasets. The code is available at\nhttps://github.com/farzad-bz/DeCo-Diff"}
{"id": "2503.19358", "pdf": "https://arxiv.org/pdf/2503.19358", "abs": "https://arxiv.org/abs/2503.19358", "authors": ["Zhiwei Huang", "Hailin Yu", "Yichun Shentu", "Jin Yuan", "Guofeng Zhang"], "title": "From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting", "categories": ["cs.CV"], "comment": "15 pages, 12 figures, CVPR 2025", "summary": "This paper presents a novel camera relocalization method, STDLoc, which\nleverages Feature Gaussian as scene representation. STDLoc is a full\nrelocalization pipeline that can achieve accurate relocalization without\nrelying on any pose prior. Unlike previous coarse-to-fine localization methods\nthat require image retrieval first and then feature matching, we propose a\nnovel sparse-to-dense localization paradigm. Based on this scene\nrepresentation, we introduce a novel matching-oriented Gaussian sampling\nstrategy and a scene-specific detector to achieve efficient and robust initial\npose estimation. Furthermore, based on the initial localization results, we\nalign the query feature map to the Gaussian feature field by dense feature\nmatching to enable accurate localization. The experiments on indoor and outdoor\ndatasets show that STDLoc outperforms current state-of-the-art localization\nmethods in terms of localization accuracy and recall."}
{"id": "2503.19359", "pdf": "https://arxiv.org/pdf/2503.19359", "abs": "https://arxiv.org/abs/2503.19359", "authors": ["Yunhe Gao", "Di Liu", "Zhuowei Li", "Yunsheng Li", "Dongdong Chen", "Mu Zhou", "Dimitris N. Metaxas"], "title": "Show and Segment: Universal Medical Image Segmentation via In-Context Learning", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Medical image segmentation remains challenging due to the vast diversity of\nanatomical structures, imaging modalities, and segmentation tasks. While deep\nlearning has made significant advances, current approaches struggle to\ngeneralize as they require task-specific training or fine-tuning on unseen\nclasses. We present Iris, a novel In-context Reference Image guided\nSegmentation framework that enables flexible adaptation to novel tasks through\nthe use of reference examples without fine-tuning. At its core, Iris features a\nlightweight context task encoding module that distills task-specific\ninformation from reference context image-label pairs. This rich context\nembedding information is used to guide the segmentation of target objects. By\ndecoupling task encoding from inference, Iris supports diverse strategies from\none-shot inference and context example ensemble to object-level context example\nretrieval and in-context tuning. Through comprehensive evaluation across twelve\ndatasets, we demonstrate that Iris performs strongly compared to task-specific\nmodels on in-distribution tasks. On seven held-out datasets, Iris shows\nsuperior generalization to out-of-distribution data and unseen classes.\nFurther, Iris's task encoding module can automatically discover anatomical\nrelationships across datasets and modalities, offering insights into medical\nobjects without explicit anatomical supervision."}
{"id": "2503.19361", "pdf": "https://arxiv.org/pdf/2503.19361", "abs": "https://arxiv.org/abs/2503.19361", "authors": ["Piera Riccio", "Francesco Galati", "Kajetan Schweighofer", "Noa Garcia", "Nuria Oliver"], "title": "ImageSet2Text: Describing Sets of Images through Text", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ImageSet2Text, a novel approach that leverages vision-language\nfoundation models to automatically create natural language descriptions of\nimage sets. Inspired by concept bottleneck models (CBMs) and based on\nvisual-question answering (VQA) chains, ImageSet2Text iteratively extracts key\nconcepts from image subsets, encodes them into a structured graph, and refines\ninsights using an external knowledge graph and CLIP-based validation. This\niterative process enhances interpretability and enables accurate and detailed\nset-level summarization. Through extensive experiments, we evaluate\nImageSet2Text's descriptions on accuracy, completeness, readability and overall\nquality, benchmarking it against existing vision-language models and\nintroducing new datasets for large-scale group image captioning."}
{"id": "2503.19367", "pdf": "https://arxiv.org/pdf/2503.19367", "abs": "https://arxiv.org/abs/2503.19367", "authors": ["Zizhi Chen", "Minghao Han", "Xukun Zhang", "Shuwei Ma", "Tao Liu", "Xing Wei", "Lihua Zhang"], "title": "VGAT: A Cancer Survival Analysis Framework Transitioning from Generative Visual Question Answering to Genomic Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal learning combining pathology images and genomic sequences enhances\ncancer survival analysis but faces clinical implementation barriers due to\nlimited access to genomic sequencing in under-resourced regions. To enable\nsurvival prediction using only whole-slide images (WSI), we propose the\nVisual-Genomic Answering-Guided Transformer (VGAT), a framework integrating\nVisual Question Answering (VQA) techniques for genomic modality reconstruction.\nBy adapting VQA's text feature extraction approach, we derive stable genomic\nrepresentations that circumvent dimensionality challenges in raw genomic data.\nSimultaneously, a cluster-based visual prompt module selectively enhances\ndiscriminative WSI patches, addressing noise from unfiltered image regions.\nEvaluated across five TCGA datasets, VGAT outperforms existing WSI-only\nmethods, demonstrating the viability of genomic-informed inference without\nsequencing. This approach bridges multimodal research and clinical feasibility\nin resource-constrained settings. The code link is\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VGAT."}
{"id": "2503.19369", "pdf": "https://arxiv.org/pdf/2503.19369", "abs": "https://arxiv.org/abs/2503.19369", "authors": ["Yufei Cai", "Hu Han", "Yuxiang Wei", "Shiguang Shan", "Xilin Chen"], "title": "EfficientMT: Efficient Temporal Adaptation for Motion Transfer in Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "The progress on generative models has led to significant advances on\ntext-to-video (T2V) generation, yet the motion controllability of generated\nvideos remains limited. Existing motion transfer methods explored the motion\nrepresentations of reference videos to guide generation. Nevertheless, these\nmethods typically rely on sample-specific optimization strategy, resulting in\nhigh computational burdens. In this paper, we propose \\textbf{EfficientMT}, a\nnovel and efficient end-to-end framework for video motion transfer. By\nleveraging a small set of synthetic paired motion transfer samples, EfficientMT\neffectively adapts a pretrained T2V model into a general motion transfer\nframework that can accurately capture and reproduce diverse motion patterns.\nSpecifically, we repurpose the backbone of the T2V model to extract temporal\ninformation from reference videos, and further propose a scaler module to\ndistill motion-related information. Subsequently, we introduce a temporal\nintegration mechanism that seamlessly incorporates reference motion features\ninto the video generation process. After training on our self-collected\nsynthetic paired samples, EfficientMT enables general video motion transfer\nwithout requiring test-time optimization. Extensive experiments demonstrate\nthat our EfficientMT outperforms existing methods in efficiency while\nmaintaining flexible motion controllability. Our code will be available\nhttps://github.com/PrototypeNx/EfficientMT."}
{"id": "2503.19373", "pdf": "https://arxiv.org/pdf/2503.19373", "abs": "https://arxiv.org/abs/2503.19373", "authors": ["Hyeongjin Nam", "Donghwan Kim", "Jeongtaek Oh", "Kyoung Mu Lee"], "title": "DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image", "categories": ["cs.CV", "cs.AI"], "comment": "Published at CVPR 2025, 17 pages including the supplementary material", "summary": "Most existing methods of 3D clothed human reconstruction from a single image\ntreat the clothed human as a single object without distinguishing between cloth\nand human body. In this regard, we present DeClotH, which separately\nreconstructs 3D cloth and human body from a single image. This task remains\nlargely unexplored due to the extreme occlusion between cloth and the human\nbody, making it challenging to infer accurate geometries and textures.\nMoreover, while recent 3D human reconstruction methods have achieved impressive\nresults using text-to-image diffusion models, directly applying such an\napproach to this problem often leads to incorrect guidance, particularly in\nreconstructing 3D cloth. To address these challenges, we propose two core\ndesigns in our framework. First, to alleviate the occlusion issue, we leverage\n3D template models of cloth and human body as regularizations, which provide\nstrong geometric priors to prevent erroneous reconstruction by the occlusion.\nSecond, we introduce a cloth diffusion model specifically designed to provide\ncontextual information about cloth appearance, thereby enhancing the\nreconstruction of 3D cloth. Qualitative and quantitative experiments\ndemonstrate that our proposed approach is highly effective in reconstructing\nboth 3D cloth and the human body. More qualitative results are provided at\nhttps://hygenie1228.github.io/DeClotH/."}
{"id": "2503.19377", "pdf": "https://arxiv.org/pdf/2503.19377", "abs": "https://arxiv.org/abs/2503.19377", "authors": ["Akshay Kulkarni", "Ge Yan", "Chung-En Sun", "Tuomas Oikarinen", "Tsui-Wei Weng"], "title": "Interpretable Generative Models through Post-hoc Concept Bottlenecks", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025. Project Page:\n  https://lilywenglab.github.io/posthoc-generative-cbm/", "summary": "Concept bottleneck models (CBM) aim to produce inherently interpretable\nmodels that rely on human-understandable concepts for their predictions.\nHowever, existing approaches to design interpretable generative models based on\nCBMs are not yet efficient and scalable, as they require expensive generative\nmodel training from scratch as well as real images with labor-intensive concept\nsupervision. To address these challenges, we present two novel and low-cost\nmethods to build interpretable generative models through post-hoc techniques\nand we name our approaches: concept-bottleneck autoencoder (CB-AE) and concept\ncontroller (CC). Our proposed approaches enable efficient and scalable training\nwithout the need of real data and require only minimal to no concept\nsupervision. Additionally, our methods generalize across modern generative\nmodel families including generative adversarial networks and diffusion models.\nWe demonstrate the superior interpretability and steerability of our methods on\nnumerous standard datasets like CelebA, CelebA-HQ, and CUB with large\nimprovements (average ~25%) over the prior work, while being 4-15x faster to\ntrain. Finally, a large-scale user study is performed to validate the\ninterpretability and steerability of our methods."}
{"id": "2503.19383", "pdf": "https://arxiv.org/pdf/2503.19383", "abs": "https://arxiv.org/abs/2503.19383", "authors": ["Yukang Lin", "Hokit Fung", "Jianjin Xu", "Zeping Ren", "Adela S. M. Lau", "Guosheng Yin", "Xiu Li"], "title": "MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid Portrait Animation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent portrait animation methods have made significant strides in generating\nrealistic lip synchronization. However, they often lack explicit control over\nhead movements and facial expressions, and cannot produce videos from multiple\nviewpoints, resulting in less controllable and expressive animations. Moreover,\ntext-guided portrait animation remains underexplored, despite its user-friendly\nnature. We present a novel two-stage text-guided framework, MVPortrait\n(Multi-view Vivid Portrait), to generate expressive multi-view portrait\nanimations that faithfully capture the described motion and emotion. MVPortrait\nis the first to introduce FLAME as an intermediate representation, effectively\nembedding facial movements, expressions, and view transformations within its\nparameter space. In the first stage, we separately train the FLAME motion and\nemotion diffusion models based on text input. In the second stage, we train a\nmulti-view video generation model conditioned on a reference portrait image and\nmulti-view FLAME rendering sequences from the first stage. Experimental results\nexhibit that MVPortrait outperforms existing methods in terms of motion and\nemotion control, as well as view consistency. Furthermore, by leveraging FLAME\nas a bridge, MVPortrait becomes the first controllable portrait animation\nframework that is compatible with text, speech, and video as driving signals."}
{"id": "2503.19385", "pdf": "https://arxiv.org/pdf/2503.19385", "abs": "https://arxiv.org/abs/2503.19385", "authors": ["Jaihoon Kim", "Taehoon Yoon", "Jisung Hwang", "Minhyuk Sung"], "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://flow-inference-time-scaling.github.io/", "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches."}
{"id": "2503.19386", "pdf": "https://arxiv.org/pdf/2503.19386", "abs": "https://arxiv.org/abs/2503.19386", "authors": ["Peishan Huang", "Dong Li"], "title": "Exploring Textual Semantics Diversity for Image Transmission in Semantic Communication Systems using Visual Language Model", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "In recent years, the rapid development of machine learning has brought\nreforms and challenges to traditional communication systems. Semantic\ncommunication has appeared as an effective strategy to effectively extract\nrelevant semantic signals semantic segmentation labels and image features for\nimage transmission. However, the insufficient number of extracted semantic\nfeatures of images will potentially result in a low reconstruction accuracy,\nwhich hinders the practical applications and still remains challenging for\nsolving. In order to fill this gap, this letter proposes a multi-text\ntransmission semantic communication (Multi-SC) system, which uses the visual\nlanguage model (VLM) to assist in the transmission of image semantic signals.\nUnlike previous image transmission semantic communication systems, the proposed\nsystem divides the image into multiple blocks and extracts multiple text\ninformation from the image using a modified large language and visual assistant\n(LLaVA), and combines semantic segmentation tags with semantic text for image\nrecovery. Simulation results show that the proposed text semantics diversity\nscheme can significantly improve the reconstruction accuracy compared with\nrelated works."}
{"id": "2503.19391", "pdf": "https://arxiv.org/pdf/2503.19391", "abs": "https://arxiv.org/abs/2503.19391", "authors": ["Zhiying Song", "Lei Yang", "Fuxi Wen", "Jun Li"], "title": "TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception", "categories": ["cs.CV", "cs.MA"], "comment": "Accepted to CVPR 2025", "summary": "Cooperative perception presents significant potential for enhancing the\nsensing capabilities of individual vehicles, however, inter-agent latency\nremains a critical challenge. Latencies cause misalignments in both spatial and\nsemantic features, complicating the fusion of real-time observations from the\nego vehicle with delayed data from others. To address these issues, we propose\nTraF-Align, a novel framework that learns the flow path of features by\npredicting the feature-level trajectory of objects from past observations up to\nthe ego vehicle's current time. By generating temporally ordered sampling\npoints along these paths, TraF-Align directs attention from the current-time\nquery to relevant historical features along each trajectory, supporting the\nreconstruction of current-time features and promoting semantic interaction\nacross multiple frames. This approach corrects spatial misalignment and ensures\nsemantic consistency across agents, effectively compensating for motion and\nachieving coherent feature fusion. Experiments on two real-world datasets,\nV2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for\nasynchronous cooperative perception."}
{"id": "2503.19404", "pdf": "https://arxiv.org/pdf/2503.19404", "abs": "https://arxiv.org/abs/2503.19404", "authors": ["Jiaqi Liao", "Yuwei Niu", "Fanqing Meng", "Hao Li", "Changyao Tian", "Yinuo Du", "Yuwen Xiong", "Dianqi Li", "Xizhou Zhu", "Li Yuan", "Jifeng Dai", "Yu Cheng"], "title": "LangBridge: Interpreting Image as a Combination of Language Embeddings", "categories": ["cs.CV"], "comment": "The code and weights will be open-sourced. Project page:\n  https://LangBridge.github.io/", "summary": "Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https://LangBridge.github.io/"}
{"id": "2503.19405", "pdf": "https://arxiv.org/pdf/2503.19405", "abs": "https://arxiv.org/abs/2503.19405", "authors": ["Mingxiao Tu", "Hoijoon Jung", "Alireza Moghadam", "Jineel Raythatha", "Lachlan Allan", "Jeremy Hsu", "Andre Kyme", "Jinman Kim"], "title": "Multi-modal 3D Pose and Shape Estimation with Computed Tomography", "categories": ["cs.CV"], "comment": null, "summary": "In perioperative care, precise in-bed 3D patient pose and shape estimation\n(PSE) can be vital in optimizing patient positioning in preoperative planning,\nenabling accurate overlay of medical images for augmented reality-based\nsurgical navigation, and mitigating risks of prolonged immobility during\nrecovery. Conventional PSE methods relying on modalities such as RGB-D,\ninfrared, or pressure maps often struggle with occlusions caused by bedding and\ncomplex patient positioning, leading to inaccurate estimation that can affect\nclinical outcomes. To address these challenges, we present the first\nmulti-modal in-bed patient 3D PSE network that fuses detailed geometric\nfeatures extracted from routinely acquired computed tomography (CT) scans with\ndepth maps (mPSE-CT). mPSE-CT incorporates a shape estimation module that\nutilizes probabilistic correspondence alignment, a pose estimation module with\na refined neural network, and a final parameters mixing module. This\nmulti-modal network robustly reconstructs occluded body regions and enhances\nthe accuracy of the estimated 3D human mesh model. We validated mPSE-CT using\nproprietary whole-body rigid phantom and volunteer datasets in clinical\nscenarios. mPSE-CT outperformed the best-performing prior method by 23% and\n49.16% in pose and shape estimation respectively, demonstrating its potential\nfor improving clinical outcomes in challenging perioperative environments."}
{"id": "2503.19406", "pdf": "https://arxiv.org/pdf/2503.19406", "abs": "https://arxiv.org/abs/2503.19406", "authors": ["Ziyuan Liu", "Jiawei Zhang", "Wenyu Wang", "Yuantao Gu"], "title": "M$^2$CD: A Unified MultiModal Framework for Optical-SAR Change Detection with Mixture of Experts and Self-Distillation", "categories": ["cs.CV"], "comment": "5 pages, 2 figures", "summary": "Most existing change detection (CD) methods focus on optical images captured\nat different times, and deep learning (DL) has achieved remarkable success in\nthis domain. However, in extreme scenarios such as disaster response, synthetic\naperture radar (SAR), with its active imaging capability, is more suitable for\nproviding post-event data. This introduces new challenges for CD methods, as\nexisting weight-sharing Siamese networks struggle to effectively learn the\ncross-modal data distribution between optical and SAR images. To address this\nchallenge, we propose a unified MultiModal CD framework, M$^2$CD. We integrate\nMixture of Experts (MoE) modules into the backbone to explicitly handle diverse\nmodalities, thereby enhancing the model's ability to learn multimodal data\ndistributions. Additionally, we innovatively propose an Optical-to-SAR guided\npath (O2SP) and implement self-distillation during training to reduce the\nfeature space discrepancy between different modalities, further alleviating the\nmodel's learning burden. We design multiple variants of M$^2$CD based on both\nCNN and Transformer backbones. Extensive experiments validate the effectiveness\nof the proposed framework, with the MiT-b1 version of M$^2$CD outperforming all\nstate-of-the-art (SOTA) methods in optical-SAR CD tasks."}
{"id": "2503.19407", "pdf": "https://arxiv.org/pdf/2503.19407", "abs": "https://arxiv.org/abs/2503.19407", "authors": ["Bingjian Yao", "Weiping Lin", "Yan He", "Zheng Wang", "Liangsheng Wang"], "title": "A Prototype-Guided Coarse Annotations Refining Approach for Whole Slide Images", "categories": ["cs.CV"], "comment": "10 pages", "summary": "The fine-grained annotations in whole slide images (WSIs) show the boundaries\nof various pathological regions. However, generating such detailed annotation\nis often costly, whereas the coarse annotations are relatively simpler to\nproduce. Existing methods for refining coarse annotations often rely on\nextensive training samples or clean datasets, and fail to capture both\nintra-slide and inter-slide latent sematic patterns, limiting their precision.\nIn this paper, we propose a prototype-guided approach. Specifically, we\nintroduce a local-to-global approach to construct non-redundant representative\nprototypes by jointly modeling intra-slide local semantics and inter-slide\ncontextual relationships. Then a prototype-guided pseudo-labeling module is\nproposed for refining coarse annotations. Finally, we employ dynamic data\nsampling and re-finetuning strategy to train a patch classifier. Extensive\nexperiments on three publicly available WSI datasets, covering lymph, liver,\nand colorectal cancers, demonstrate that our method significantly outperforms\nexisting state-of-the-art (SOTA) methods. The code will be available."}
{"id": "2503.19416", "pdf": "https://arxiv.org/pdf/2503.19416", "abs": "https://arxiv.org/abs/2503.19416", "authors": ["Xuli Shen", "Hua Cai", "Dingding Yu", "Weilin Shen", "Qing Xu", "Xiangyang Xue"], "title": "EmoHead: Emotional Talking Head via Manipulating Semantic Expression Parameters", "categories": ["cs.CV"], "comment": null, "summary": "Generating emotion-specific talking head videos from audio input is an\nimportant and complex challenge for human-machine interaction. However, emotion\nis highly abstract concept with ambiguous boundaries, and it necessitates\ndisentangled expression parameters to generate emotionally expressive talking\nhead videos. In this work, we present EmoHead to synthesize talking head videos\nvia semantic expression parameters. To predict expression parameter for\narbitrary audio input, we apply an audio-expression module that can be\nspecified by an emotion tag. This module aims to enhance correlation from audio\ninput across various emotions. Furthermore, we leverage pre-trained hyperplane\nto refine facial movements by probing along the vertical direction. Finally,\nthe refined expression parameters regularize neural radiance fields and\nfacilitate the emotion-consistent generation of talking head videos.\nExperimental results demonstrate that semantic expression parameters lead to\nbetter reconstruction quality and controllability."}
{"id": "2503.19443", "pdf": "https://arxiv.org/pdf/2503.19443", "abs": "https://arxiv.org/abs/2503.19443", "authors": ["Jiaxin Zhang", "Junjun Jiang", "Youyu Chen", "Kui Jiang", "Xianming Liu"], "title": "COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on Boundary-Adaptive Gaussian Splitting", "categories": ["cs.CV"], "comment": null, "summary": "Accurate object segmentation is crucial for high-quality scene understanding\nin the 3D vision domain. However, 3D segmentation based on 3D Gaussian\nSplatting (3DGS) struggles with accurately delineating object boundaries, as\nGaussian primitives often span across object edges due to their inherent volume\nand the lack of semantic guidance during training. In order to tackle these\nchallenges, we introduce Clear Object Boundaries for 3DGS Segmentation\n(COB-GS), which aims to improve segmentation accuracy by clearly delineating\nblurry boundaries of interwoven Gaussian primitives within the scene. Unlike\nexisting approaches that remove ambiguous Gaussians and sacrifice visual\nquality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and\nvisual information, allowing the two different levels to cooperate with each\nother effectively. Specifically, for the semantic guidance, we introduce a\nboundary-adaptive Gaussian splitting technique that leverages semantic gradient\nstatistics to identify and split ambiguous Gaussians, aligning them closely\nwith object boundaries. For the visual optimization, we rectify the degraded\nsuboptimal texture of the 3DGS scene, particularly along the refined boundary\nstructures. Experimental results show that COB-GS substantially improves\nsegmentation accuracy and robustness against inaccurate masks from pre-trained\nmodel, yielding clear boundaries while preserving high visual quality. Code is\navailable at https://github.com/ZestfulJX/COB-GS."}
{"id": "2503.19448", "pdf": "https://arxiv.org/pdf/2503.19448", "abs": "https://arxiv.org/abs/2503.19448", "authors": ["Changyong He", "Jin Zeng", "Jiawei Zhang", "Jiajie Guo"], "title": "Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Time-of-Flight (ToF) sensors efficiently capture scene depth, but the\nnonlinear depth construction procedure often results in extremely large noise\nvariance or even invalid areas. Recent methods based on deep neural networks\n(DNNs) achieve enhanced ToF denoising accuracy but tend to struggle when\npresented with severe noise corruption due to limited prior knowledge of ToF\ndata distribution. In this paper, we propose DepthCAD, a novel ToF denoising\napproach that ensures global structural smoothness by leveraging the rich prior\nknowledge in Stable Diffusion and maintains local metric accuracy by steering\nthe diffusion process with confidence guidance. To adopt the pretrained image\ndiffusion model to ToF depth denoising, we apply the diffusion on raw ToF\ncorrelation measurements with dynamic range normalization before converting to\ndepth maps. Experimental results validate the state-of-the-art performance of\nthe proposed scheme, and the evaluation on real data further verifies its\nrobustness against real-world ToF noise."}
{"id": "2503.19452", "pdf": "https://arxiv.org/pdf/2503.19452", "abs": "https://arxiv.org/abs/2503.19452", "authors": ["Yiqing Li", "Xuan Wang", "Jiawei Wu", "Yikun Ma", "Zhi Jin"], "title": "SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with Generative Priors", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing novel views of large-scale scenes from unconstrained in-the-wild\nimages is an important but challenging task in computer vision. Existing\nmethods, which optimize per-image appearance and transient occlusion through\nimplicit neural networks from dense training views (approximately 1000 images),\nstruggle to perform effectively under sparse input conditions, resulting in\nnoticeable artifacts. To this end, we propose SparseGS-W, a novel framework\nbased on 3D Gaussian Splatting that enables the reconstruction of complex\noutdoor scenes and handles occlusions and appearance changes with as few as\nfive training images. We leverage geometric priors and constrained diffusion\npriors to compensate for the lack of multi-view information from extremely\nsparse input. Specifically, we propose a plug-and-play Constrained Novel-View\nEnhancement module to iteratively improve the quality of rendered novel views\nduring the Gaussian optimization process. Furthermore, we propose an Occlusion\nHandling module, which flexibly removes occlusions utilizing the inherent\nhigh-quality inpainting capability of constrained diffusion priors. Both\nmodules are capable of extracting appearance features from any user-provided\nreference image, enabling flexible modeling of illumination-consistent scenes.\nExtensive experiments on the PhotoTourism and Tanks and Temples datasets\ndemonstrate that SparseGS-W achieves state-of-the-art performance not only in\nfull-reference metrics, but also in commonly used non-reference metrics such as\nFID, ClipIQA, and MUSIQ."}
{"id": "2503.19457", "pdf": "https://arxiv.org/pdf/2503.19457", "abs": "https://arxiv.org/abs/2503.19457", "authors": ["Juntao Jian", "Xiuping Liu", "Zixuan Chen", "Manyi Li", "Jian Liu", "Ruizhen Hu"], "title": "G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation", "categories": ["cs.CV", "cs.RO"], "comment": "11 pages, 5 figures", "summary": "Recent advances in dexterous grasping synthesis have demonstrated significant\nprogress in producing reasonable and plausible grasps for many task purposes.\nBut it remains challenging to generalize to unseen object categories and\ndiverse task instructions. In this paper, we propose G-DexGrasp, a\nretrieval-augmented generation approach that can produce high-quality dexterous\nhand configurations for unseen object categories and language-based task\ninstructions. The key is to retrieve generalizable grasping priors, including\nthe fine-grained contact part and the affordance-related distribution of\nrelevant grasping instances, for the following synthesis pipeline.\nSpecifically, the fine-grained contact part and affordance act as generalizable\nguidance to infer reasonable grasping configurations for unseen objects with a\ngenerative model, while the relevant grasping distribution plays as\nregularization to guarantee the plausibility of synthesized grasps during the\nsubsequent refinement optimization. Our comparison experiments validate the\neffectiveness of our key designs for generalization and demonstrate the\nremarkable performance against the existing approaches. Project page:\nhttps://g-dexgrasp.github.io/"}
{"id": "2503.19458", "pdf": "https://arxiv.org/pdf/2503.19458", "abs": "https://arxiv.org/abs/2503.19458", "authors": ["Shujuan Li", "Yu-Shen Liu", "Zhizhong Han"], "title": "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing open surfaces from multi-view images is vital in digitalizing\ncomplex objects in daily life. A widely used strategy is to learn unsigned\ndistance functions (UDFs) by checking if their appearance conforms to the image\nobservations through neural rendering. However, it is still hard to learn\ncontinuous and implicit UDF representations through 3D Gaussians splatting\n(3DGS) due to the discrete and explicit scene representation, i.e., 3D\nGaussians. To resolve this issue, we propose a novel approach to bridge the gap\nbetween 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D\nGaussian planes on surfaces, and then, leverage the self-supervision and\ngradient-based inference to supervise unsigned distances in both near and far\narea to surfaces. To this end, we introduce novel constraints and strategies to\nconstrain the learning of 2D Gaussians to pursue more stable optimization and\nmore reliable self-supervision, addressing the challenges brought by\ncomplicated gradient field on or near the zero level set of UDFs. We report\nnumerical and visual comparisons with the state-of-the-art on widely used\nbenchmarks and real data to show our advantages in terms of accuracy,\nefficiency, completeness, and sharpness of reconstructed open surfaces with\nboundaries. Project page: https://lisj575.github.io/GaussianUDF/"}
{"id": "2503.19462", "pdf": "https://arxiv.org/pdf/2503.19462", "abs": "https://arxiv.org/abs/2503.19462", "authors": ["Haiyu Zhang", "Xinyuan Chen", "Yaohui Wang", "Xihui Liu", "Yunhong Wang", "Yu Qiao"], "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset", "categories": ["cs.CV"], "comment": "Project Page: https://aejion.github.io/accvideo/", "summary": "Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps."}
{"id": "2503.19468", "pdf": "https://arxiv.org/pdf/2503.19468", "abs": "https://arxiv.org/abs/2503.19468", "authors": ["Nadja Gruber", "Johannes Schwab", "Markus Haltmeier", "Ander Biguri", "Clemens Dlaska", "Gyeongha Hwang"], "title": "Noisier2Inverse: Self-Supervised Learning for Image Reconstruction with Correlated Noise", "categories": ["cs.CV", "eess.IV", "math.OC", "94A08, 92C55", "I.2.10; I.4.5; I.4.4; G.3"], "comment": null, "summary": "We propose Noisier2Inverse, a correction-free self-supervised deep learning\napproach for general inverse prob- lems. The proposed method learns a\nreconstruction function without the need for ground truth samples and is ap-\nplicable in cases where measurement noise is statistically correlated. This\nincludes computed tomography, where detector imperfections or photon scattering\ncreate correlated noise patterns, as well as microscopy and seismic imaging,\nwhere physical interactions during measurement introduce dependencies in the\nnoise structure. Similar to Noisier2Noise, a key step in our approach is the\ngeneration of noisier data from which the reconstruction net- work learns.\nHowever, unlike Noisier2Noise, the proposed loss function operates in\nmeasurement space and is trained to recover an extrapolated image instead of\nthe original noisy one. This eliminates the need for an extrap- olation step\nduring inference, which would otherwise suffer from ill-posedness. We\nnumerically demonstrate that our method clearly outperforms previous\nself-supervised approaches that account for correlated noise."}
{"id": "2503.19474", "pdf": "https://arxiv.org/pdf/2503.19474", "abs": "https://arxiv.org/abs/2503.19474", "authors": ["Yaomin Shen", "Xiaojian Lin", "Wei Fan"], "title": "A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accept by ICME2025", "summary": "In the domain of multimodal intent recognition (MIR), the objective is to\nrecognize human intent by integrating a variety of modalities, such as language\ntext, body gestures, and tones. However, existing approaches face difficulties\nadequately capturing the intrinsic connections between the modalities and\noverlooking the corresponding semantic representations of intent. To address\nthese limitations, we present the Anchor-based Mul- timodal Embedding with\nSemantic Synchronization (A-MESS) framework. We first design an Anchor-based\nMultimodal Embed- ding (A-ME) module that employs an anchor-based embedding\nfusion mechanism to integrate multimodal inputs. Furthermore, we develop a\nSemantic Synchronization (SS) strategy with the Triplet Contrastive Learning\npipeline, which optimizes the pro- cess by synchronizing multimodal\nrepresentation with label de- scriptions produced by the large language model.\nComprehensive experiments indicate that our A-MESS achieves state-of-the-art\nand provides substantial insight into multimodal representation and downstream\ntasks."}
{"id": "2503.19478", "pdf": "https://arxiv.org/pdf/2503.19478", "abs": "https://arxiv.org/abs/2503.19478", "authors": ["Saverio Cavasin", "Pietro Biasetton", "Mattia Tamiazzo", "Mauro Conti", "Simone Milani"], "title": "TeLL Me what you cant see", "categories": ["cs.CV", "cs.CY", "I.2.1"], "comment": "16 pages, 58 images", "summary": "During criminal investigations, images of persons of interest directly\ninfluence the success of identification procedures. However, law enforcement\nagencies often face challenges related to the scarcity of high-quality images\nor their obsolescence, which can affect the accuracy and success of people\nsearching processes. This paper introduces a novel forensic mugshot\naugmentation framework aimed at addressing these limitations. Our approach\nenhances the identification probability of individuals by generating\nadditional, high-quality images through customizable data augmentation\ntechniques, while maintaining the biometric integrity and consistency of the\noriginal data. Several experimental results show that our method significantly\nimproves identification accuracy and robustness across various forensic\nscenarios, demonstrating its effectiveness as a trustworthy tool law\nenforcement applications. Index Terms: Digital Forensics, Person\nre-identification, Feature extraction, Data augmentation, Visual-Language\nmodels."}
{"id": "2503.19480", "pdf": "https://arxiv.org/pdf/2503.19480", "abs": "https://arxiv.org/abs/2503.19480", "authors": ["Shijie Ma", "Yuying Ge", "Teng Wang", "Yuxin Guo", "Yixiao Ge", "Ying Shan"], "title": "GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers", "categories": ["cs.CV"], "comment": "Project released at: https://mashijie1028.github.io/GenHancer/", "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available."}
{"id": "2503.19486", "pdf": "https://arxiv.org/pdf/2503.19486", "abs": "https://arxiv.org/abs/2503.19486", "authors": ["Zhengwentai Sun", "Heyuan Li", "Xihe Yang", "Keru Zheng", "Shuliang Ning", "Yihao Zhi", "Hongjie Liao", "Chenghong Li", "Shuguang Cui", "Xiaoguang Han"], "title": "Exploring Disentangled and Controllable Human Image Synthesis: From End-to-End to Stage-by-Stage", "categories": ["cs.CV"], "comment": null, "summary": "Achieving fine-grained controllability in human image synthesis is a\nlong-standing challenge in computer vision. Existing methods primarily focus on\neither facial synthesis or near-frontal body generation, with limited ability\nto simultaneously control key factors such as viewpoint, pose, clothing, and\nidentity in a disentangled manner. In this paper, we introduce a new\ndisentangled and controllable human synthesis task, which explicitly separates\nand manipulates these four factors within a unified framework. We first develop\nan end-to-end generative model trained on MVHumanNet for factor\ndisentanglement. However, the domain gap between MVHumanNet and in-the-wild\ndata produce unsatisfacotry results, motivating the exploration of virtual\ntry-on (VTON) dataset as a potential solution. Through experiments, we observe\nthat simply incorporating the VTON dataset as additional data to train the\nend-to-end model degrades performance, primarily due to the inconsistency in\ndata forms between the two datasets, which disrupts the disentanglement\nprocess. To better leverage both datasets, we propose a stage-by-stage\nframework that decomposes human image generation into three sequential steps:\nclothed A-pose generation, back-view synthesis, and pose and view control. This\nstructured pipeline enables better dataset utilization at different stages,\nsignificantly improving controllability and generalization, especially for\nin-the-wild scenarios. Extensive experiments demonstrate that our\nstage-by-stage approach outperforms end-to-end models in both visual fidelity\nand disentanglement quality, offering a scalable solution for real-world tasks.\nAdditional demos are available on the project page:\nhttps://taited.github.io/discohuman-project/."}
{"id": "2503.19501", "pdf": "https://arxiv.org/pdf/2503.19501", "abs": "https://arxiv.org/abs/2503.19501", "authors": ["Vinayak Mali", "Saurabh Jaiswal"], "title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs", "categories": ["cs.CV", "cs.AI"], "comment": "4 Pages, 2 figures, 2 code block, 1 flow chart", "summary": "Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources."}
{"id": "2503.19503", "pdf": "https://arxiv.org/pdf/2503.19503", "abs": "https://arxiv.org/abs/2503.19503", "authors": ["Juncen Guo", "Xiaoguang Zhu", "Liangyu Teng", "Hao Yang", "Jing Liu", "Yang Liu", "Liang Song"], "title": "Adaptive Weighted Parameter Fusion with CLIP for Class-Incremental Learning", "categories": ["cs.CV"], "comment": "Accepted by ICME2025", "summary": "Class-incremental Learning (CIL) enables the model to incrementally absorb\nknowledge from new classes and build a generic classifier across all previously\nencountered classes. When the model optimizes with new classes, the knowledge\nof previous classes is inevitably erased, leading to catastrophic forgetting.\nAddressing this challenge requires making a trade-off between retaining old\nknowledge and accommodating new information. However, this balancing process\noften requires sacrificing some information, which can lead to a partial loss\nin the model's ability to discriminate between classes. To tackle this issue,\nwe design the adaptive weighted parameter fusion with Contrastive\nLanguage-Image Pre-training (CLIP), which not only takes into account the\nvariability of the data distribution of different tasks, but also retains all\nthe effective information of the parameter matrix to the greatest extent. In\naddition, we introduce a balance factor that can balance the data distribution\nalignment and distinguishability of adjacent tasks. Experimental results on\nseveral traditional benchmarks validate the superiority of the proposed method."}
{"id": "2503.19508", "pdf": "https://arxiv.org/pdf/2503.19508", "abs": "https://arxiv.org/abs/2503.19508", "authors": ["Kartik Jangra", "Aman Kumar Singh", "Yashwani Mann", "Geetanjali Rathee"], "title": "Improved Alignment of Modalities in Large Vision Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in vision-language models have achieved remarkable\nresults in making language models understand vision inputs. However, a unified\napproach to align these models across diverse tasks such as image captioning\nand visual question answering remains a challenge. Existing methods either\nrequire very big language models or very big datasets which is not efficient in\nutilizing existing models. This paper addresses this gap and devises a training\nstrategy of auto-regressive vision-language models, to unify vision-language\ntasks like image-captioning and visual question answering. We propose four\ntraining stages for aligning the vision model with the language model, in other\nwords, the language model is given an ability to process visual inputs. We also\ndevise different attention masks for training transformer-based language models\nthat improve the quality of visual features. Further, we introduce some\nfindings, 1) the attention mask should not be applied on visual inputs, 2) the\nLanguage model converges faster on AI- generated data, 3) More work should be\ndone in the alignment stage during the pre-training of the model, 4) the model\ncan easily adapt to any downstream tasks like visual question answering on\nhealthcare datasets like PathVQA. After training the model for one epoch for\nall the stages, it outperforms large models like VILA-13 billion models on\ncommon benchmarks like CIDEr scores on COCO and Flickr30k datasets and achieves\nvery close scores to GIT-2 on the same dataset despite being a much smaller\nmodel trained on a much smaller dataset. All of the training is done using best\npractices available like multi- GPU parallel training, lower-precision training\nwith 16-bit float numbers, faster attention (SDPA), and gradient accumulation,\nand completed the training within 12 hours."}
{"id": "2503.19543", "pdf": "https://arxiv.org/pdf/2503.19543", "abs": "https://arxiv.org/abs/2503.19543", "authors": ["Junwei Zheng", "Ruiping Liu", "Yufan Chen", "Zhenfang Chen", "Kailun Yang", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "Scene-agnostic Pose Regression for Visual Localization", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project page:\n  https://junweizheng93.github.io/publications/SPR/SPR.html", "summary": "Absolute Pose Regression (APR) predicts 6D camera poses but lacks the\nadaptability to unknown environments without retraining, while Relative Pose\nRegression (RPR) generalizes better yet requires a large image retrieval\ndatabase. Visual Odometry (VO) generalizes well in unseen environments but\nsuffers from accumulated error in open trajectories. To address this dilemma,\nwe introduce a new task, Scene-agnostic Pose Regression (SPR), which can\nachieve accurate pose regression in a flexible way while eliminating the need\nfor retraining or databases. To benchmark SPR, we created a large-scale\ndataset, 360SPR, with over 200K photorealistic panoramas, 3.6M pinhole images\nand camera poses in 270 scenes at three different sensor heights. Furthermore,\na SPR-Mamba model is initially proposed to address SPR in a dual-branch manner.\nExtensive experiments and studies demonstrate the effectiveness of our SPR\nparadigm, dataset, and model. In the unknown scenes of both 360SPR and 360Loc\ndatasets, our method consistently outperforms APR, RPR and VO. The dataset and\ncode are available at\nhttps://junweizheng93.github.io/publications/SPR/SPR.html."}
{"id": "2503.19545", "pdf": "https://arxiv.org/pdf/2503.19545", "abs": "https://arxiv.org/abs/2503.19545", "authors": ["Elena Buglakova", "Anwai Archit", "Edoardo D'Imprima", "Julia Mahamid", "Constantin Pape", "Anna Kreshuk"], "title": "Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images", "categories": ["cs.CV"], "comment": null, "summary": "Segmentation of very large images is a common problem in microscopy, medical\nimaging or remote sensing. The problem is usually addressed by sliding window\ninference, which can theoretically lead to seamlessly stitched predictions.\nHowever, in practice many of the popular pipelines still suffer from tiling\nartifacts. We investigate the root cause of these issues and show that they\nstem from the normalization layers within the neural networks. We propose\nindicators to detect normalization issues and further explore the trade-offs\nbetween artifact-free and high-quality predictions, using three diverse\nmicroscopy datasets as examples. Finally, we propose to use BatchRenorm as the\nmost suitable normalization strategy, which effectively removes tiling\nartifacts and enhances transfer performance, thereby improving the reusability\nof trained networks for new datasets."}
{"id": "2503.19546", "pdf": "https://arxiv.org/pdf/2503.19546", "abs": "https://arxiv.org/abs/2503.19546", "authors": ["Jan Kohút", "Michal Hradiš"], "title": "Practical Fine-Tuning of Autoregressive Models on Limited Handwritten Texts", "categories": ["cs.CV"], "comment": "Submitted to ICDAR2025 conference", "summary": "A common use case for OCR applications involves users uploading documents and\nprogressively correcting automatic recognition to obtain the final transcript.\nThis correction phase presents an opportunity for progressive adaptation of the\nOCR model, making it crucial to adapt early, while ensuring stability and\nreliability. We demonstrate that state-of-the-art transformer-based models can\neffectively support this adaptation, gradually reducing the annotator's\nworkload. Our results show that fine-tuning can reliably start with just 16\nlines, yielding a 10% relative improvement in CER, and scale up to 40% with 256\nlines. We further investigate the impact of model components, clarifying the\nroles of the encoder and decoder in the fine-tuning process. To guide\nadaptation, we propose reliable stopping criteria, considering both direct\napproaches and global trend analysis. Additionally, we show that OCR models can\nbe leveraged to cut annotation costs by half through confidence-based selection\nof informative lines, achieving the same performance with fewer annotations."}
{"id": "2503.19557", "pdf": "https://arxiv.org/pdf/2503.19557", "abs": "https://arxiv.org/abs/2503.19557", "authors": ["Haim Sawdayee", "Chuan Guo", "Guy Tevet", "Bing Zhou", "Jian Wang", "Amit H. Bermano"], "title": "Dance Like a Chicken: Low-Rank Stylization for Human Motion Diffusion", "categories": ["cs.CV"], "comment": "Project page at https://haimsaw.github.io/LoRA-MDM/", "summary": "Text-to-motion generative models span a wide range of 3D human actions but\nstruggle with nuanced stylistic attributes such as a \"Chicken\" style. Due to\nthe scarcity of style-specific data, existing approaches pull the generative\nprior towards a reference style, which often results in out-of-distribution low\nquality generations. In this work, we introduce LoRA-MDM, a lightweight\nframework for motion stylization that generalizes to complex actions while\nmaintaining editability. Our key insight is that adapting the generative prior\nto include the style, while preserving its overall distribution, is more\neffective than modifying each individual motion during generation. Building on\nthis idea, LoRA-MDM learns to adapt the prior to include the reference style\nusing only a few samples. The style can then be used in the context of\ndifferent textual prompts for generation. The low-rank adaptation shifts the\nmotion manifold in a semantically meaningful way, enabling realistic style\ninfusion even for actions not present in the reference samples. Moreover,\npreserving the distribution structure enables advanced operations such as style\nblending and motion editing. We compare LoRA-MDM to state-of-the-art stylized\nmotion generation methods and demonstrate a favorable balance between text\nfidelity and style consistency."}
{"id": "2503.19570", "pdf": "https://arxiv.org/pdf/2503.19570", "abs": "https://arxiv.org/abs/2503.19570", "authors": ["Olgica Zaric", "Carmen Leser", "Vladimir Juras", "Alex Farr", "Malina Gologan", "Stanislas Rapacchi", "Laura Villazan Garcia", "Christian Singer", "Siegfried Trattnig", "Christian Licht", "Ramona Woitek"], "title": "Improved tissue sodium concentration quantification in breast cancer by reducing partial volume effects: a preliminary study", "categories": ["cs.CV"], "comment": null, "summary": "Introduction: In sodium (23Na) MRI, partial volume effects (PVE) are one of\nthe most common causes of errors in the quantification of tissue sodium\nconcentration (TSC) in vivo. Advanced image reconstruction algorithms, such as\ncompressed sensing (CS), have been shown to potentially reduce PVE. Therefore,\nwe investigated the feasibility of CS-based methods for image quality and TSC\nquantification accuracy improvement in patients with breast cancer (BC).\nSubjects and Methods: Three healthy participants and 12 female participants\nwith BC were examined on a 7T MRI scanner in this study. We reconstructed\n23Na-MRI images using the weighted total variation (wTV) and directional total\nvariation (dTV), anatomically guided total variation (AG-TV), and adaptive\ncombine (ADC) reconstruction and performed image quality assessment. We\nevaluated agreement in tumor volumes delineated on sodium data using the Dice\nscore and performed TSC quantification for different image reconstruction\napproaches. Results: All methods provided sodium images of the breast with good\nquality. The mean Dice scores for wTV, dTV, and AG-TV were 65%, 72%, and 75%,\nrespectively. In the breast tumors, average TSC values were 83.0, 72.0, 80.0,\nand 84.0 mmol/L, respectively. There was a significant difference between dTV\nand wTV (p<0.001), as well as between dTV and AG-TV (p<0.001) and dTV and ADC\nalgorithm (p<0.001). Conclusion: The results of this study showed that there\nare differences in tumor appearance and TSC estimations that might be depending\non the type of image reconstruction and parameters used, most likely due to\ndifferences in their robustness in reducing PVE."}
{"id": "2503.19588", "pdf": "https://arxiv.org/pdf/2503.19588", "abs": "https://arxiv.org/abs/2503.19588", "authors": ["Mia Siemon", "Ivan Nikolov", "Thomas B. Moeslund", "Kamal Nasrollahi"], "title": "Video Anomaly Detection with Contours - A Study", "categories": ["cs.CV"], "comment": null, "summary": "In Pose-based Video Anomaly Detection prior art is rooted on the assumption\nthat abnormal events can be mostly regarded as a result of uncommon human\nbehavior. Opposed to utilizing skeleton representations of humans, however, we\ninvestigate the potential of learning recurrent motion patterns of normal human\nbehavior using 2D contours. Keeping all advantages of pose-based methods, such\nas increased object anonymization, the shift from human skeletons to contours\nis hypothesized to leave the opportunity to cover more object categories open\nfor future research. We propose formulating the problem as a regression and a\nclassification task, and additionally explore two distinct data representation\ntechniques for contours. To further reduce the computational complexity of\nPose-based Video Anomaly Detection solutions, all methods in this study are\nbased on shallow Neural Networks from the field of Deep Learning, and evaluated\non the three most prominent benchmark datasets within Video Anomaly Detection\nand their human-related counterparts, totaling six datasets. Our results\nindicate that this novel perspective on Pose-based Video Anomaly Detection\nmarks a promising direction for future research."}
{"id": "2503.19592", "pdf": "https://arxiv.org/pdf/2503.19592", "abs": "https://arxiv.org/abs/2503.19592", "authors": ["Xinxing Cheng", "Tianyang Zhang", "Wenqi Lu", "Qingjie Meng", "Alejandro F. Frangi", "Jinming Duan"], "title": "SACB-Net: Spatial-awareness Convolutions for Medical Image Registration", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Deep learning-based image registration methods have shown state-of-the-art\nperformance and rapid inference speeds. Despite these advances, many existing\napproaches fall short in capturing spatially varying information in non-local\nregions of feature maps due to the reliance on spatially-shared convolution\nkernels. This limitation leads to suboptimal estimation of deformation fields.\nIn this paper, we propose a 3D Spatial-Awareness Convolution Block (SACB) to\nenhance the spatial information within feature representations. Our SACB\nestimates the spatial clusters within feature maps by leveraging feature\nsimilarity and subsequently parameterizes the adaptive convolution kernels\nacross diverse regions. This adaptive mechanism generates the convolution\nkernels (weights and biases) tailored to spatial variations, thereby enabling\nthe network to effectively capture spatially varying information. Building on\nSACB, we introduce a pyramid flow estimator (named SACB-Net) that integrates\nSACBs to facilitate multi-scale flow composition, particularly addressing large\ndeformations. Experimental results on the brain IXI and LPBA datasets as well\nas Abdomen CT datasets demonstrate the effectiveness of SACB and the\nsuperiority of SACB-Net over the state-of-the-art learning-based registration\nmethods. The code is available at https://github.com/x-xc/SACB_Net ."}
{"id": "2503.19622", "pdf": "https://arxiv.org/pdf/2503.19622", "abs": "https://arxiv.org/abs/2503.19622", "authors": ["Hongcheng Gao", "Jiashu Qu", "Jingyi Tang", "Baolong Bi", "Yue Liu", "Hongyu Chen", "Li Liang", "Li Su", "Qingming Huang"], "title": "Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN."}
{"id": "2503.19625", "pdf": "https://arxiv.org/pdf/2503.19625", "abs": "https://arxiv.org/abs/2503.19625", "authors": ["Xiangting Meng", "Jiaqi Yang", "Mingshu Chen", "Chenxin Yan", "Yujiao Shi", "Wenchao Ding", "Laurent Kneip"], "title": "DynOPETs: A Versatile Benchmark for Dynamic Object Pose Estimation and Tracking in Moving Camera Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "In the realm of object pose estimation, scenarios involving both dynamic\nobjects and moving cameras are prevalent. However, the scarcity of\ncorresponding real-world datasets significantly hinders the development and\nevaluation of robust pose estimation models. This is largely attributed to the\ninherent challenges in accurately annotating object poses in dynamic scenes\ncaptured by moving cameras. To bridge this gap, this paper presents a novel\ndataset DynOPETs and a dedicated data acquisition and annotation pipeline\ntailored for object pose estimation and tracking in such unconstrained\nenvironments. Our efficient annotation method innovatively integrates pose\nestimation and pose tracking techniques to generate pseudo-labels, which are\nsubsequently refined through pose graph optimization. The resulting dataset\noffers accurate pose annotations for dynamic objects observed from moving\ncameras. To validate the effectiveness and value of our dataset, we perform\ncomprehensive evaluations using 18 state-of-the-art methods, demonstrating its\npotential to accelerate research in this challenging domain. The dataset will\nbe made publicly available to facilitate further exploration and advancement in\nthe field."}
{"id": "2503.19634", "pdf": "https://arxiv.org/pdf/2503.19634", "abs": "https://arxiv.org/abs/2503.19634", "authors": ["Ozan Unal", "Steven Marty", "Dengxin Dai"], "title": "Burst Image Super-Resolution with Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Burst image super-resolution (BISR) aims to enhance the resolution of a\nkeyframe by leveraging information from multiple low-resolution images captured\nin quick succession. In the deep learning era, BISR methods have evolved from\nfully convolutional networks to transformer-based architectures, which, despite\ntheir effectiveness, suffer from the quadratic complexity of self-attention. We\nsee Mamba as the next natural step in the evolution of this field, offering a\ncomparable global receptive field and selective information routing with only\nlinear time complexity. In this work, we introduce BurstMamba, a Mamba-based\narchitecture for BISR. Our approach decouples the task into two specialized\nbranches: a spatial module for keyframe super-resolution and a temporal module\nfor subpixel prior extraction, striking a balance between computational\nefficiency and burst information integration. To further enhance burst\nprocessing with Mamba, we propose two novel strategies: (i) optical flow-based\nserialization, which aligns burst sequences only during state updates to\npreserve subpixel details, and (ii) a wavelet-based reparameterization of the\nstate-space update rules, prioritizing high-frequency features for improved\nburst-to-keyframe information passing. Our framework achieves SOTA performance\non public benchmarks of SyntheticSR, RealBSR-RGB, and RealBSR-RAW."}
{"id": "2503.19647", "pdf": "https://arxiv.org/pdf/2503.19647", "abs": "https://arxiv.org/abs/2503.19647", "authors": ["Niccolo Avogaro", "Thomas Frick", "Mattia Rigotti", "Andrea Bartezzaghi", "Filip Janicki", "Cristiano Malossi", "Konrad Schindler", "Roy Assaf"], "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (VLMs) are increasingly being regarded as\nfoundation models that can be instructed to solve diverse tasks by prompting,\nwithout task-specific training. We examine the seemingly obvious question: how\nto effectively prompt VLMs for semantic segmentation. To that end, we\nsystematically evaluate the segmentation performance of several recent models\nguided by either text or visual prompts on the out-of-distribution MESS dataset\ncollection. We introduce a scalable prompting scheme, few-shot prompted\nsemantic segmentation, inspired by open-vocabulary segmentation and few-shot\nlearning. It turns out that VLMs lag far behind specialist models trained for a\nspecific segmentation task, by about 30% on average on the\nIntersection-over-Union metric. Moreover, we find that text prompts and visual\nprompts are complementary: each one of the two modes fails on many examples\nthat the other one can solve. Our analysis suggests that being able to\nanticipate the most effective prompt modality can lead to a 11% improvement in\nperformance. Motivated by our findings, we propose PromptMatcher, a remarkably\nsimple training-free baseline that combines both text and visual prompts,\nachieving state-of-the-art results outperforming the best text-prompted VLM by\n2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic\nsegmentation."}
{"id": "2503.19653", "pdf": "https://arxiv.org/pdf/2503.19653", "abs": "https://arxiv.org/abs/2503.19653", "authors": ["Yabin Wang", "Zhiwu Huang", "Xiaopeng Hong"], "title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps://github.com/iamwangyabin/OpenSDI."}
{"id": "2503.19654", "pdf": "https://arxiv.org/pdf/2503.19654", "abs": "https://arxiv.org/abs/2503.19654", "authors": ["Mehdi Moshtaghi", "Siavash H. Khajavi", "Joni Pajarinen"], "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available."}
{"id": "2503.19658", "pdf": "https://arxiv.org/pdf/2503.19658", "abs": "https://arxiv.org/abs/2503.19658", "authors": ["Jan Kohút", "Martin Dočekal", "Michal Hradiš", "Marek Vaško"], "title": "BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Submitted to ICDAR2025 conference", "summary": "Manual digitization of bibliographic metadata is time consuming and labor\nintensive, especially for historical and real-world archives with highly\nvariable formatting across documents. Despite advances in machine learning, the\nabsence of dedicated datasets for metadata extraction hinders automation. To\naddress this gap, we introduce BiblioPage, a dataset of scanned title pages\nannotated with structured bibliographic metadata. The dataset consists of\napproximately 2,000 monograph title pages collected from 14 Czech libraries,\nspanning a wide range of publication periods, typographic styles, and layout\nstructures. Each title page is annotated with 16 bibliographic attributes,\nincluding title, contributors, and publication metadata, along with precise\npositional information in the form of bounding boxes. To extract structured\ninformation from this dataset, we valuated object detection models such as YOLO\nand DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and\nan F1 score of 59. Additionally, we assess the performance of various visual\nlarge language models, including LlamA 3.2-Vision and GPT-4o, with the best\nmodel reaching an F1 score of 67. BiblioPage serves as a real-world benchmark\nfor bibliographic metadata extraction, contributing to document understanding,\ndocument question answering, and document information extraction. Dataset and\nevaluation scripts are availible at: https://github.com/DCGM/biblio-dataset"}
{"id": "2503.19661", "pdf": "https://arxiv.org/pdf/2503.19661", "abs": "https://arxiv.org/abs/2503.19661", "authors": ["Rupak Bose", "Chinedu Innocent Nwoye", "Aditya Bhat", "Nicolas Padoy"], "title": "CoSimGen: Controllable Diffusion Model for Simultaneous Image and Mask Generation", "categories": ["cs.CV"], "comment": "15 pages, 14 figure, 2 tables, project page at\n  https://camma-public.github.io/endogen/cosimgen", "summary": "The acquisition of annotated datasets with paired images and segmentation\nmasks is a critical challenge in domains such as medical imaging, remote\nsensing, and computer vision. Manual annotation demands significant resources,\nfaces ethical constraints, and depends heavily on domain expertise. Existing\ngenerative models often target single-modality outputs, either images or\nsegmentation masks, failing to address the need for high-quality, simultaneous\nimage-mask generation. Additionally, these models frequently lack adaptable\nconditioning mechanisms, restricting control over the generated outputs and\nlimiting their applicability for dataset augmentation and rare scenario\nsimulation. We propose CoSimGen, a diffusion-based framework for controllable\nsimultaneous image and mask generation. Conditioning is intuitively achieved\nthrough (1) text prompts grounded in class semantics, (2) spatial embedding of\ncontext prompts to provide spatial coherence, and (3) spectral embedding of\ntimestep information to model noise levels during diffusion. To enhance\ncontrollability and training efficiency, the framework incorporates contrastive\ntriplet loss between text and class embeddings, alongside diffusion and\nadversarial losses. Initial low-resolution outputs 128 x 128 are super-resolved\nto 512 x 512, producing high-fidelity images and masks with strict adherence to\nconditions. We evaluate CoSimGen on metrics such as FID, KID, LPIPS, Class FID,\nPositive predicted value for image fidelity and semantic alignment of generated\nsamples over 4 diverse datasets. CoSimGen achieves state-of-the-art performance\nacross all datasets, achieving the lowest KID of 0.11 and LPIPS of 0.53 across\ndatasets."}
{"id": "2503.19670", "pdf": "https://arxiv.org/pdf/2503.19670", "abs": "https://arxiv.org/abs/2503.19670", "authors": ["Saurav Sharma", "Didier Mutter", "Nicolas Padoy"], "title": "fine-CLIP: Enhancing Zero-Shot Fine-Grained Surgical Action Recognition with Vision-Language Models", "categories": ["cs.CV"], "comment": "6 pages, 3 tables, 3 figures", "summary": "While vision-language models like CLIP have advanced zero-shot surgical phase\nrecognition, they struggle with fine-grained surgical activities, especially\naction triplets. This limitation arises because current CLIP formulations rely\non global image features, which overlook the fine-grained semantics and\ncontextual details crucial for complex tasks like zero-shot triplet\nrecognition. Furthermore, these models do not explore the hierarchical\nstructure inherent in triplets, reducing their ability to generalize to novel\ntriplets. To address these challenges, we propose fine-CLIP, which learns\nobject-centric features and lever- ages the hierarchy in triplet formulation.\nOur approach integrates three components: hierarchical prompt modeling to\ncapture shared semantics, LoRA-based vision backbone adaptation for enhanced\nfeature extraction, and a graph-based condensation strategy that groups similar\npatch features into meaningful object clusters. Since triplet classification is\na challenging task, we introduce an alternative yet meaningful base-to-novel\ngeneralization benchmark with two settings on the CholecT50 dataset:\nUnseen-Target, assessing adaptability to triplets with novel anatomical\nstructures, and Unseen-Instrument-Verb, where models need to generalize to\nnovel instrument-verb interactions. fine-CLIP shows significant improvements in\nF1 and mAP, enhancing zero-shot recognition of novel surgical triplets."}
{"id": "2503.19683", "pdf": "https://arxiv.org/pdf/2503.19683", "abs": "https://arxiv.org/abs/2503.19683", "authors": ["Andrii Yermakov", "Jan Cech", "Jiri Matas"], "title": "Unlocking the Hidden Potential of CLIP in Generalizable Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "This paper tackles the challenge of detecting partially manipulated facial\ndeepfakes, which involve subtle alterations to specific facial features while\nretaining the overall context, posing a greater detection difficulty than fully\nsynthetic faces. We leverage the Contrastive Language-Image Pre-training (CLIP)\nmodel, specifically its ViT-L/14 visual encoder, to develop a generalizable\ndetection method that performs robustly across diverse datasets and unknown\nforgery techniques with minimal modifications to the original model. The\nproposed approach utilizes parameter-efficient fine-tuning (PEFT) techniques,\nsuch as LN-tuning, to adjust a small subset of the model's parameters,\npreserving CLIP's pre-trained knowledge and reducing overfitting. A tailored\npreprocessing pipeline optimizes the method for facial images, while\nregularization strategies, including L2 normalization and metric learning on a\nhyperspherical manifold, enhance generalization. Trained on the FaceForensics++\ndataset and evaluated in a cross-dataset fashion on Celeb-DF-v2, DFDC, FFIW,\nand others, the proposed method achieves competitive detection accuracy\ncomparable to or outperforming much more complex state-of-the-art techniques.\nThis work highlights the efficacy of CLIP's visual encoder in facial deepfake\ndetection and establishes a simple, powerful baseline for future research,\nadvancing the field of generalizable deepfake detection. The code is available\nat: https://github.com/yermandy/deepfake-detection"}
{"id": "2503.19700", "pdf": "https://arxiv.org/pdf/2503.19700", "abs": "https://arxiv.org/abs/2503.19700", "authors": ["Boyi Li", "Ye Yuan", "Wenjun Tan"], "title": "Optimization of MedSAM model based on bounding box adaptive perturbation algorithm", "categories": ["cs.CV"], "comment": "6 pages, 6 figures, 3 Tables", "summary": "The MedSAM model, built upon the SAM framework, enhances medical image\nsegmentation through generalizable training but still exhibits notable\nlimitations. First, constraints in the perturbation window settings during\ntraining can cause MedSAM to incorrectly segment small tissues or organs\ntogether with adjacent structures, leading to segmentation errors. Second, when\ndealing with medical image targets characterized by irregular shapes and\ncomplex structures, segmentation often relies on narrowing the bounding box to\nrefine segmentation intent. However, MedSAM's performance under reduced\nbounding box prompts remains suboptimal. To address these challenges, this\nstudy proposes a bounding box adaptive perturbation algorithm to optimize the\ntraining process. The proposed approach aims to reduce segmentation errors for\nsmall targets and enhance the model's accuracy when processing reduced bounding\nbox prompts, ultimately improving the robustness and reliability of the MedSAM\nmodel for complex medical imaging tasks."}
{"id": "2503.19703", "pdf": "https://arxiv.org/pdf/2503.19703", "abs": "https://arxiv.org/abs/2503.19703", "authors": ["Qian Wang", "Zhihao Zhan", "Jialei He", "Zhituo Tu", "Xiang Zhu", "Jie Yuan"], "title": "High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Highly accurate geometric precision and dense image features characterize\nTrue Digital Orthophoto Maps (TDOMs), which are in great demand for\napplications such as urban planning, infrastructure management, and\nenvironmental monitoring. Traditional TDOM generation methods need\nsophisticated processes, such as Digital Surface Models (DSM) and occlusion\ndetection, which are computationally expensive and prone to errors. This work\npresents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free\nof explicit DSM and occlusion detection. With depth map generation, spatial\ninformation for every pixel within the TDOM is retrieved and can reconstruct\nthe scene with high precision. Divide-and-conquer strategy achieves excellent\nGS training and rendering with high-resolution TDOMs at a lower resource cost,\nwhich preserves higher quality of rendering on complex terrain and thin\nstructure without a decrease in efficiency. Experimental results demonstrate\nthe efficiency of large-scale scene reconstruction and high-precision terrain\nmodeling. This approach provides accurate spatial data, which assists users in\nbetter planning and decision-making based on maps."}
{"id": "2503.19706", "pdf": "https://arxiv.org/pdf/2503.19706", "abs": "https://arxiv.org/abs/2503.19706", "authors": ["Jungin Park", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025 Camera-ready", "summary": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov."}
{"id": "2503.19707", "pdf": "https://arxiv.org/pdf/2503.19707", "abs": "https://arxiv.org/abs/2503.19707", "authors": ["Ilias Stogiannidis", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "8 main pages, 4 pages Appendix, 5 figures", "summary": "Vision-Language Models (VLMs) have recently emerged as powerful tools,\nexcelling in tasks that integrate visual and textual comprehension, such as\nimage captioning, visual question answering, and image-text retrieval. However,\nexisting benchmarks for VLMs include spatial components, which often fail to\nisolate spatial reasoning from related tasks such as object detection or\nsemantic comprehension. In this paper, we address these deficiencies with a\nmulti-faceted approach towards understanding spatial reasoning. Informed by the\ndiverse and multi-dimensional nature of human spatial reasoning abilities, we\npresent a detailed analysis that first delineates the core elements of spatial\nreasoning: spatial relations, orientation and navigation, mental rotation, and\nspatial visualization, and then assesses the performance of these models in\nboth synthetic and real-world images, bridging controlled and naturalistic\ncontexts. We analyze 13 state-of-the-art Vision-Language Models, uncovering\npivotal insights into their spatial reasoning performance. Our results reveal\nprofound shortcomings in current VLMs, with average accuracy across the 13\nmodels approximating random chance, highlighting spatial reasoning as a\npersistent obstacle. This work not only exposes the pressing need to advance\nspatial reasoning within VLMs but also establishes a solid platform for future\nexploration. Code available on GitHub (https://github.com/stogiannidis/srbench)\nand dataset available on HuggingFace\n(https://huggingface.co/datasets/stogiannidis/srbench)."}
{"id": "2503.19721", "pdf": "https://arxiv.org/pdf/2503.19721", "abs": "https://arxiv.org/abs/2503.19721", "authors": ["Chengjie Ge", "Xueyang Fu", "Peng He", "Kunyu Wang", "Chengzhi Cao", "Zheng-Jun Zha"], "title": "EventMamba: Enhancing Spatio-Temporal Locality with State Space Models for Event-Based Video Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging its robust linear global modeling capability, Mamba has notably\nexcelled in computer vision. Despite its success, existing Mamba-based vision\nmodels have overlooked the nuances of event-driven tasks, especially in video\nreconstruction. Event-based video reconstruction (EBVR) demands spatial\ntranslation invariance and close attention to local event relationships in the\nspatio-temporal domain. Unfortunately, conventional Mamba algorithms apply\nstatic window partitions and standard reshape scanning methods, leading to\nsignificant losses in local connectivity. To overcome these limitations, we\nintroduce EventMamba--a specialized model designed for EBVR tasks. EventMamba\ninnovates by incorporating random window offset (RWO) in the spatial domain,\nmoving away from the restrictive fixed partitioning. Additionally, it features\na new consistent traversal serialization approach in the spatio-temporal\ndomain, which maintains the proximity of adjacent events both spatially and\ntemporally. These enhancements enable EventMamba to retain Mamba's robust\nmodeling capabilities while significantly preserving the spatio-temporal\nlocality of event data. Comprehensive testing on multiple datasets shows that\nEventMamba markedly enhances video reconstruction, drastically improving\ncomputation speed while delivering superior visual quality compared to\nTransformer-based methods."}
{"id": "2503.19730", "pdf": "https://arxiv.org/pdf/2503.19730", "abs": "https://arxiv.org/abs/2503.19730", "authors": ["Yuli Zhou", "Guolei Sun", "Yawei Li", "Yuqian Fu", "Luca Benini", "Ender Konukoglu"], "title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video camouflaged object segmentation (VCOS), aiming at segmenting\ncamouflaged objects that seamlessly blend into their environment, is a\nfundamental vision task with various real-world applications. With the release\nof SAM2, video segmentation has witnessed significant progress. However, SAM2's\ncapability of segmenting camouflaged videos is suboptimal, especially when\ngiven simple prompts such as point and box. To address the problem, we propose\nCamouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged\nscenes without modifying SAM2's parameters. Specifically, we introduce a\ndecamouflaged token to provide the flexibility of feature adjustment for VCOS.\nTo make full use of fine-grained and high-resolution features from the current\nframe and previous frames, we propose implicit object-aware fusion (IOF) and\nexplicit object-aware fusion (EOF) modules, respectively. Object prototype\ngeneration (OPG) is introduced to abstract and memorize object prototypes with\ninformative details using high-quality features from previous frames. Extensive\nexperiments are conducted to validate the effectiveness of our approach. While\nCamSAM2 only adds negligible learnable parameters to SAM2, it substantially\noutperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains\nwith click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on\nSUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at\n\\href{https://github.com/zhoustan/CamSAM2}{github.com/zhoustan/CamSAM2}."}
{"id": "2503.19731", "pdf": "https://arxiv.org/pdf/2503.19731", "abs": "https://arxiv.org/abs/2503.19731", "authors": ["Junhyuk So", "Jiwoong Shin", "Chaeyeon Jang", "Eunhyeok Park"], "title": "PCM : Picard Consistency Model for Fast Parallel Sampling of Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to the CVPR 2025", "summary": "Recently, diffusion models have achieved significant advances in vision,\ntext, and robotics. However, they still face slow generation speeds due to\nsequential denoising processes. To address this, a parallel sampling method\nbased on Picard iteration was introduced, effectively reducing sequential steps\nwhile ensuring exact convergence to the original output. Nonetheless, Picard\niteration does not guarantee faster convergence, which can still result in slow\ngeneration in practice. In this work, we propose a new parallelization scheme,\nthe Picard Consistency Model (PCM), which significantly reduces the number of\ngeneration steps in Picard iteration. Inspired by the consistency model, PCM is\ndirectly trained to predict the fixed-point solution, or the final output, at\nany stage of the convergence trajectory. Additionally, we introduce a new\nconcept called model switching, which addresses PCM's limitations and ensures\nexact convergence. Extensive experiments demonstrate that PCM achieves up to a\n2.71x speedup over sequential sampling and a 1.77x speedup over Picard\niteration across various tasks, including image generation and robotic control."}
{"id": "2503.19739", "pdf": "https://arxiv.org/pdf/2503.19739", "abs": "https://arxiv.org/abs/2503.19739", "authors": ["Pihai Sun", "Junjun Jiang", "Yuanqi Yao", "Youyu Chen", "Wenbo Zhao", "Kui Jiang", "Xianming Liu"], "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via Frequency-Decoupled Alignment and Degradation-Robust Fusion", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth.Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE"}
{"id": "2503.19740", "pdf": "https://arxiv.org/pdf/2503.19740", "abs": "https://arxiv.org/abs/2503.19740", "authors": ["Chengan Che", "Chao Wang", "Tom Vercauteren", "Sophia Tsoka", "Luis C. Garcia-Peraza-Herrera"], "title": "Surg-3M: A Dataset and Foundation Model for Perception in Surgical Settings", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Advancements in computer-assisted surgical procedures heavily rely on\naccurate visual data interpretation from camera systems used during surgeries.\nTraditional open-access datasets focusing on surgical procedures are often\nlimited by their small size, typically consisting of fewer than 100 videos with\nless than 100K images. To address these constraints, a new dataset called\nSurg-3M has been compiled using a novel aggregation pipeline that collects\nhigh-resolution videos from online sources. Featuring an extensive collection\nof over 4K surgical videos and more than 3 million high-quality images from\nmultiple procedure types, Surg-3M offers a comprehensive resource surpassing\nexisting alternatives in size and scope, including two novel tasks. To\ndemonstrate the effectiveness of this dataset, we present SurgFM, a\nself-supervised foundation model pretrained on Surg-3M that achieves impressive\nresults in downstream tasks such as surgical phase recognition, action\nrecognition, and tool presence detection. Combining key components from\nConvNeXt, DINO, and an innovative augmented distillation method, SurgFM\nexhibits exceptional performance compared to specialist architectures across\nvarious benchmarks. Our experimental results show that SurgFM outperforms\nstate-of-the-art models in multiple downstream tasks, including significant\ngains in surgical phase recognition (+8.9pp, +4.7pp, and +3.9pp of Jaccard in\nAutoLaparo, M2CAI16, and Cholec80), action recognition (+3.1pp of mAP in\nCholecT50) and tool presence detection (+4.6pp of mAP in Cholec80). Moreover,\neven when using only half of the data, SurgFM outperforms state-of-the-art\nmodels in AutoLaparo and achieves state-of-the-art performance in Cholec80.\nBoth Surg-3M and SurgFM have significant potential to accelerate progress\ntowards developing autonomous robotic surgery systems."}
{"id": "2503.19755", "pdf": "https://arxiv.org/pdf/2503.19755", "abs": "https://arxiv.org/abs/2503.19755", "authors": ["Haoyu Fu", "Diankun Zhang", "Zongchuang Zhao", "Jianfeng Cui", "Dingkang Liang", "Chong Zhang", "Dingyuan Zhang", "Hongwei Xie", "Bing Wang", "Xiang Bai"], "title": "ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end (E2E) autonomous driving methods still struggle to make correct\ndecisions in interactive closed-loop evaluation due to limited causal reasoning\ncapability. Current methods attempt to leverage the powerful understanding and\nreasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma.\nHowever, the problem is still open that few VLMs for E2E methods perform well\nin the closed-loop evaluation due to the gap between the semantic reasoning\nspace and the purely numerical trajectory output in the action space. To tackle\nthis issue, we propose ORION, a holistic E2E autonomous driving framework by\nvision-language instructed action generation. ORION uniquely combines a\nQT-Former to aggregate long-term history context, a Large Language Model (LLM)\nfor driving scenario reasoning, and a generative planner for precision\ntrajectory prediction. ORION further aligns the reasoning space and the action\nspace to implement a unified E2E optimization for both visual\nquestion-answering (VQA) and planning tasks. Our method achieves an impressive\nclosed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate\n(SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art\n(SOTA) methods by a large margin of 14.28 DS and 19.61% SR."}
{"id": "2503.19764", "pdf": "https://arxiv.org/pdf/2503.19764", "abs": "https://arxiv.org/abs/2503.19764", "authors": ["Christina Kassab", "Sacha Morin", "Martin Büchner", "Matías Mattamala", "Kumaraditya Gupta", "Abhinav Valada", "Liam Paull", "Maurice Fallon"], "title": "OpenLex3D: A New Evaluation Benchmark for Open-Vocabulary 3D Scene Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D scene understanding has been transformed by open-vocabulary language\nmodels that enable interaction via natural language. However, the evaluation of\nthese representations is limited to closed-set semantics that do not capture\nthe richness of language. This work presents OpenLex3D, a dedicated benchmark\nto evaluate 3D open-vocabulary scene representations. OpenLex3D provides\nentirely new label annotations for 23 scenes from Replica, ScanNet++, and HM3D,\nwhich capture real-world linguistic variability by introducing synonymical\nobject categories and additional nuanced descriptions. By introducing an\nopen-set 3D semantic segmentation task and an object retrieval task, we provide\ninsights on feature precision, segmentation, and downstream capabilities. We\nevaluate various existing 3D open-vocabulary methods on OpenLex3D, showcasing\nfailure cases, and avenues for improvement. The benchmark is publicly available\nat: https://openlex3d.github.io/."}
{"id": "2503.19769", "pdf": "https://arxiv.org/pdf/2503.19769", "abs": "https://arxiv.org/abs/2503.19769", "authors": ["Suzhe Xu", "Jialin Peng", "Chengyuan Zhang"], "title": "BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Segmentation is a fundamental task in computer vision, with prompt-driven\nmethods gaining prominence due to their flexibility. The recent Segment\nAnything Model (SAM) has demonstrated powerful point-prompt segmentation\ncapabilities, while text-based segmentation models offer rich semantic\nunderstanding. However, existing approaches rarely explore how to effectively\ncombine these complementary modalities for optimal segmentation performance.\nThis paper presents BiPrompt-SAM, a novel dual-modal prompt segmentation\nframework that fuses the advantages of point and text prompts through an\nexplicit selection mechanism. Specifically, we leverage SAM's inherent ability\nto generate multiple mask candidates, combined with a semantic guidance mask\nfrom text prompts, and explicitly select the most suitable candidate based on\nsimilarity metrics. This approach can be viewed as a simplified Mixture of\nExperts (MoE) system, where the point and text modules act as distinct\n\"experts,\" and the similarity scoring serves as a rudimentary \"gating network.\"\nWe conducted extensive evaluations on both the Endovis17 medical dataset and\nRefCOCO series natural image datasets. On Endovis17, BiPrompt-SAM achieved\n89.55\\% mDice and 81.46\\% mIoU, comparable to state-of-the-art specialized\nmedical segmentation models. On the RefCOCO series datasets, our method\nattained 87.1\\%, 86.5\\%, and 85.8\\% IoU, significantly outperforming existing\napproaches. Experiments demonstrate that our explicit dual-selection method\neffectively combines the spatial precision of point prompts with the semantic\nrichness of text prompts, particularly excelling in scenarios involving\nsemantically complex objects, multiple similar objects, and partial occlusions.\nBiPrompt-SAM not only provides a simple yet effective implementation but also\noffers a new perspective on multi-modal prompt fusion."}
{"id": "2503.19776", "pdf": "https://arxiv.org/pdf/2503.19776", "abs": "https://arxiv.org/abs/2503.19776", "authors": ["Konyul Park", "Yecheol Kim", "Daehun Kim", "Jun Won Choi"], "title": "Resilient Sensor Fusion under Adverse Sensor Failures via Multi-Modal Expert Fusion", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Modern autonomous driving perception systems utilize complementary\nmulti-modal sensors, such as LiDAR and cameras. Although sensor fusion\narchitectures enhance performance in challenging environments, they still\nsuffer significant performance drops under severe sensor failures, such as\nLiDAR beam reduction, LiDAR drop, limited field of view, camera drop, and\nocclusion. This limitation stems from inter-modality dependencies in current\nsensor fusion frameworks. In this study, we introduce an efficient and robust\nLiDAR-camera 3D object detector, referred to as MoME, which can achieve robust\nperformance through a mixture of experts approach. Our MoME fully decouples\nmodality dependencies using three parallel expert decoders, which use camera\nfeatures, LiDAR features, or a combination of both to decode object queries,\nrespectively. We propose Multi-Expert Decoding (MED) framework, where each\nquery is decoded selectively using one of three expert decoders. MoME utilizes\nan Adaptive Query Router (AQR) to select the most appropriate expert decoder\nfor each query based on the quality of camera and LiDAR features. This ensures\nthat each query is processed by the best-suited expert, resulting in robust\nperformance across diverse sensor failure scenarios. We evaluated the\nperformance of MoME on the nuScenes-R benchmark. Our MoME achieved\nstate-of-the-art performance in extreme weather and sensor failure conditions,\nsignificantly outperforming the existing models across various sensor failure\nscenarios."}
{"id": "2503.19777", "pdf": "https://arxiv.org/pdf/2503.19777", "abs": "https://arxiv.org/abs/2503.19777", "authors": ["Vladan Stojnić", "Yannis Kalantidis", "Jiří Matas", "Giorgos Tolias"], "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS"}
{"id": "2503.19783", "pdf": "https://arxiv.org/pdf/2503.19783", "abs": "https://arxiv.org/abs/2503.19783", "authors": ["Kartik Thakral", "Tamar Glaser", "Tal Hassner", "Mayank Vatsa", "Richa Singh"], "title": "Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models", "categories": ["cs.CV"], "comment": "Published in CVPR 2025", "summary": "Existing unlearning algorithms in text-to-image generative models often fail\nto preserve the knowledge of semantically related concepts when removing\nspecific target concepts: a challenge known as adjacency. To address this, we\npropose FADE (Fine grained Attenuation for Diffusion Erasure), introducing\nadjacency aware unlearning in diffusion models. FADE comprises two components:\n(1) the Concept Neighborhood, which identifies an adjacency set of related\nconcepts, and (2) Mesh Modules, employing a structured combination of\nExpungement, Adjacency, and Guidance loss components. These enable precise\nerasure of target concepts while preserving fidelity across related and\nunrelated concepts. Evaluated on datasets like Stanford Dogs, Oxford Flowers,\nCUB, I2P, Imagenette, and ImageNet1k, FADE effectively removes target concepts\nwith minimal impact on correlated concepts, achieving atleast a 12% improvement\nin retention performance over state-of-the-art methods."}
{"id": "2503.19791", "pdf": "https://arxiv.org/pdf/2503.19791", "abs": "https://arxiv.org/abs/2503.19791", "authors": ["Jingdan Kang", "Haoxin Yang", "Yan Cai", "Huaidong Zhang", "Xuemiao Xu", "Yong Du", "Shengfeng He"], "title": "SITA: Structurally Imperceptible and Transferable Adversarial Attacks for Stylized Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Image generation technology has brought significant advancements across\nvarious fields but has also raised concerns about data misuse and potential\nrights infringements, particularly with respect to creating visual artworks.\nCurrent methods aimed at safeguarding artworks often employ adversarial\nattacks. However, these methods face challenges such as poor transferability,\nhigh computational costs, and the introduction of noticeable noise, which\ncompromises the aesthetic quality of the original artwork. To address these\nlimitations, we propose a Structurally Imperceptible and Transferable\nAdversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss,\nwhich decouples and disrupts the robust style representation of the image. This\ndisruption hinders style extraction during stylized image generation, thereby\nimpairing the overall stylization process. Importantly, SITA eliminates the\nneed for a surrogate diffusion model, leading to significantly reduced\ncomputational overhead. The method's robust style feature disruption ensures\nhigh transferability across diverse models. Moreover, SITA introduces\nperturbations by embedding noise within the imperceptible structural details of\nthe image. This approach effectively protects against style extraction without\ncompromising the visual quality of the artwork. Extensive experiments\ndemonstrate that SITA offers superior protection for artworks against\nunauthorized use in stylized generation. It significantly outperforms existing\nmethods in terms of transferability, computational efficiency, and noise\nimperceptibility. Code is available at https://github.com/A-raniy-day/SITA."}
{"id": "2503.19793", "pdf": "https://arxiv.org/pdf/2503.19793", "abs": "https://arxiv.org/abs/2503.19793", "authors": ["Vitaly Gnatyuk", "Valeriia Koriukina Ilya Levoshevich", "Pavel Nurminskiy", "Guenter Wallner"], "title": "In the Blink of an Eye: Instant Game Map Editing using a Generative-AI Smart Brush", "categories": ["cs.CV", "I.4"], "comment": null, "summary": "With video games steadily increasing in complexity, automated generation of\ngame content has found widespread interest. However, the task of 3D gaming map\nart creation remains underexplored to date due to its unique complexity and\ndomain-specific challenges. While recent works have addressed related topics\nsuch as retro-style level generation and procedural terrain creation, these\nworks primarily focus on simpler data distributions. To the best of our\nknowledge, we are the first to demonstrate the application of modern AI\ntechniques for high-resolution texture manipulation in complex, highly detailed\nAAA 3D game environments. We introduce a novel Smart Brush for map editing,\ndesigned to assist artists in seamlessly modifying selected areas of a game map\nwith minimal effort. By leveraging generative adversarial networks and\ndiffusion models we propose two variants of the brush that enable efficient and\ncontext-aware generation. Our hybrid workflow aims to enhance both artistic\nflexibility and production efficiency, enabling the refinement of environments\nwithout manually reworking every detail, thus helping to bridge the gap between\nautomation and creative control in game development. A comparative evaluation\nof our two methods with adapted versions of several state-of-the art models\nshows that our GAN-based brush produces the sharpest and most detailed outputs\nwhile preserving image context while the evaluated state-of-the-art models tend\ntowards blurrier results and exhibit difficulties in maintaining contextual\nconsistency."}
{"id": "2503.19794", "pdf": "https://arxiv.org/pdf/2503.19794", "abs": "https://arxiv.org/abs/2503.19794", "authors": ["Zhuoming Liu", "Yiquan Li", "Khoi Duc Nguyen", "Yiwu Zhong", "Yin Li"], "title": "PAVE: Patching and Adapting Video Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR2025 Camera Ready", "summary": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE."}
{"id": "2503.19798", "pdf": "https://arxiv.org/pdf/2503.19798", "abs": "https://arxiv.org/abs/2503.19798", "authors": ["Ruixi You", "Hecheng Jia", "Feng Xu"], "title": "Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with Keypoints-Guided Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Synthetic Aperture Radar (SAR) imagery provides all-weather, all-day, and\nhigh-resolution imaging capabilities but its unique imaging mechanism makes\ninterpretation heavily reliant on expert knowledge, limiting interpretability,\nespecially in complex target tasks. Translating SAR images into optical images\nis a promising solution to enhance interpretation and support downstream tasks.\nMost existing research focuses on scene-level translation, with limited work on\nobject-level translation due to the scarcity of paired data and the challenge\nof accurately preserving contour and texture details. To address these issues,\nthis study proposes a keypoint-guided diffusion model (KeypointDiff) for\nSAR-to-optical image translation of unpaired aircraft targets. This framework\nintroduces supervision on target class and azimuth angle via keypoints, along\nwith a training strategy for unpaired data. Based on the classifier-free\nguidance diffusion architecture, a class-angle guidance module (CAGM) is\ndesigned to integrate class and angle information into the diffusion generation\nprocess. Furthermore, adversarial loss and consistency loss are employed to\nimprove image fidelity and detail quality, tailored for aircraft targets.\nDuring sampling, aided by a pre-trained keypoint detector, the model eliminates\nthe requirement for manually labeled class and azimuth information, enabling\nautomated SAR-to-optical translation. Experimental results demonstrate that the\nproposed method outperforms existing approaches across multiple metrics,\nproviding an efficient and effective solution for object-level SAR-to-optical\ntranslation and downstream tasks. Moreover, the method exhibits strong\nzero-shot generalization to untrained aircraft types with the assistance of the\nkeypoint detector."}
{"id": "2503.19801", "pdf": "https://arxiv.org/pdf/2503.19801", "abs": "https://arxiv.org/abs/2503.19801", "authors": ["Zhiyang Liu", "Dong Yang", "Minghao Zhang", "Hanyu Sun", "Hong Wu", "Huiying Wang", "Wen Shen", "Chao Chai", "Shuang Xia"], "title": "SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite that deep learning (DL) methods have presented tremendous potential\nin many medical image analysis tasks, the practical applications of medical DL\nmodels are limited due to the lack of enough data samples with manual\nannotations. By noting that the clinical radiology examinations are associated\nwith radiology reports that describe the images, we propose to develop a\nfoundation model for multi-model head MRI by using contrastive learning on the\nimages and the corresponding radiology findings. In particular, a contrastive\nlearning framework is proposed, where a mixed syntax and semantic similarity\nmatching metric is integrated to reduce the thirst of extreme large dataset in\nconventional contrastive learning framework. Our proposed similarity enhanced\ncontrastive language image pretraining (SeLIP) is able to effectively extract\nmore useful features. Experiments revealed that our proposed SeLIP performs\nwell in many downstream tasks including image-text retrieval task,\nclassification task, and image segmentation, which highlights the importance of\nconsidering the similarities among texts describing different images in\ndeveloping medical image foundation models."}
{"id": "2503.19804", "pdf": "https://arxiv.org/pdf/2503.19804", "abs": "https://arxiv.org/abs/2503.19804", "authors": ["Manjushree Aithal", "Rosaura G. VidalMata", "Manikandtan Kartha", "Gong Chen", "Eashan Adhikarla", "Lucas N. Kirsten", "Zhicheng Fu", "Nikhil A. Madhusudhana", "Joe Nasti"], "title": "LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Dataset will be released upon publication", "summary": "Low-light image enhancement is crucial for a myriad of applications, from\nnight vision and surveillance, to autonomous driving. However, due to the\ninherent limitations that come in hand with capturing images in\nlow-illumination environments, the task of enhancing such scenes still presents\na formidable challenge. To advance research in this field, we introduce our Low\nExposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure\nbenchmark dataset for low-light image enhancement comprising of over 230K\nframes showcasing 24K real-world indoor and outdoor, with-and without human,\nscenes. Captured using 3 different camera sensors, LENVIZ offers a wide range\nof lighting conditions, noise levels, and scene complexities, making it the\nlargest publicly available up-to 4K resolution benchmark in the field. LENVIZ\nincludes high quality human-generated ground truth, for which each\nmulti-exposure low-light scene has been meticulously curated and edited by\nexpert photographers to ensure optimal image quality. Furthermore, we also\nconduct a comprehensive analysis of current state-of-the-art low-light image\nenhancement techniques on our dataset and highlight potential areas of\nimprovement."}
{"id": "2503.19839", "pdf": "https://arxiv.org/pdf/2503.19839", "abs": "https://arxiv.org/abs/2503.19839", "authors": ["Jun Zhou", "Jiahao Li", "Zunnan Xu", "Hanhui Li", "Yiji Cheng", "Fa-Ting Hong", "Qin Lin", "Qinglin Lu", "Xiaodan Liang"], "title": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Currently, instruction-based image editing methods have made significant\nprogress by leveraging the powerful cross-modal understanding capabilities of\nvision language models (VLMs). However, they still face challenges in three key\nareas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained\nediting. To address these issues, we propose FireEdit, an innovative\nFine-grained Instruction-based image editing framework that exploits a\nREgion-aware VLM. FireEdit is designed to accurately comprehend user\ninstructions and ensure effective control over the editing process.\nSpecifically, we enhance the fine-grained visual perception capabilities of the\nVLM by introducing additional region tokens. Relying solely on the output of\nthe LLM to guide the diffusion model may lead to suboptimal editing results.\nTherefore, we propose a Time-Aware Target Injection module and a Hybrid Visual\nCross Attention module. The former dynamically adjusts the guidance strength at\nvarious denoising stages by integrating timestep embeddings with the text\nembeddings. The latter enhances visual details for image editing, thereby\npreserving semantic consistency between the edited result and the source image.\nBy combining the VLM enhanced with fine-grained region tokens and the\ntime-dependent diffusion model, FireEdit demonstrates significant advantages in\ncomprehending editing instructions and maintaining high semantic consistency.\nExtensive experiments indicate that our approach surpasses the state-of-the-art\ninstruction-based image editing methods. Our project is available at\nhttps://zjgans.github.io/fireedit.github.io."}
{"id": "2503.19846", "pdf": "https://arxiv.org/pdf/2503.19846", "abs": "https://arxiv.org/abs/2503.19846", "authors": ["Aaron Serianni", "Tyler Zhu", "Vikram V. Ramaswamy", "Olga Russakovsky"], "title": "Attention IoU: Examining Biases in CelebA using Attention Maps", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in CVPR 2025. Code and data is available at\n  https://github.com/aaronserianni/attention-iou . 15 pages, 14 figures,\n  including appendix", "summary": "Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels."}
{"id": "2503.19850", "pdf": "https://arxiv.org/pdf/2503.19850", "abs": "https://arxiv.org/abs/2503.19850", "authors": ["Carlos Plou", "Cesar Borja", "Ruben Martinez-Cantin", "Ana C. Murillo"], "title": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Information retrieval in hour-long videos presents a significant challenge,\neven for state-of-the-art Vision-Language Models (VLMs), particularly when the\ndesired information is localized within a small subset of frames. Long video\ndata presents challenges for VLMs due to context window limitations and the\ndifficulty of pinpointing frames containing the answer. Our novel video agent,\nFALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant\ninformation along the video, and locate the frames with the answer. FALCONEye\nnovelty relies on 1) the proposed meta-architecture, which is better suited to\ntackle hour-long videos compared to short video approaches in the\nstate-of-the-art; 2) a new efficient exploration algorithm to locate the\ninformation using short clips, captions and answer confidence; and 3) our\nstate-of-the-art VLMs calibration analysis for the answer confidence. Our agent\nis built over a small-size VLM and a medium-size LLM being accessible to run on\nstandard computational resources. We also release FALCON-Bench, a benchmark to\nevaluate long (average > 1 hour) Video Answer Search challenges, highlighting\nthe need for open-ended question evaluation. Our experiments show FALCONEye's\nsuperior performance than the state-of-the-art in FALCON-Bench, and similar or\nbetter performance in related benchmarks."}
{"id": "2503.19851", "pdf": "https://arxiv.org/pdf/2503.19851", "abs": "https://arxiv.org/abs/2503.19851", "authors": ["Xinpeng Li", "Shijian Deng", "Bolin Lai", "Weiguo Pian", "James M. Rehg", "Yapeng Tian"], "title": "Towards Online Multi-Modal Social Interaction Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal social interaction understanding (MMSI) is critical in human-robot\ninteraction systems. In real-world scenarios, AI agents are required to provide\nreal-time feedback. However, existing models often depend on both past and\nfuture contexts, which hinders them from applying to real-world problems. To\nbridge this gap, we propose an online MMSI setting, where the model must\nresolve MMSI tasks using only historical information, such as recorded\ndialogues and video streams. To address the challenges of missing the useful\nfuture context, we develop a novel framework, named Online-MMSI-VLM, that\nleverages two complementary strategies: multi-party conversation forecasting\nand social-aware visual prompting with multi-modal large language models.\nFirst, to enrich linguistic context, the multi-party conversation forecasting\nsimulates potential future utterances in a coarse-to-fine manner, anticipating\nupcoming speaker turns and then generating fine-grained conversational details.\nSecond, to effectively incorporate visual social cues like gaze and gesture,\nsocial-aware visual prompting highlights the social dynamics in video with\nbounding boxes and body keypoints for each person and frame. Extensive\nexperiments on three tasks and two datasets demonstrate that our method\nachieves state-of-the-art performance and significantly outperforms baseline\nmodels, indicating its effectiveness on Online-MMSI. The code and pre-trained\nmodels will be publicly released at: https://github.com/Sampson-Lee/OnlineMMSI."}
{"id": "2503.19881", "pdf": "https://arxiv.org/pdf/2503.19881", "abs": "https://arxiv.org/abs/2503.19881", "authors": ["Tianhao Qi", "Jianlong Yuan", "Wanquan Feng", "Shancheng Fang", "Jiawei Liu", "SiYu Zhou", "Qian He", "Hongtao Xie", "Yongdong Zhang"], "title": "Mask$^2$DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Sora has unveiled the immense potential of the Diffusion Transformer (DiT)\narchitecture in single-scene video generation. However, the more challenging\ntask of multi-scene video generation, which offers broader applications,\nremains relatively underexplored. To bridge this gap, we propose Mask$^2$DiT, a\nnovel approach that establishes fine-grained, one-to-one alignment between\nvideo segments and their corresponding text annotations. Specifically, we\nintroduce a symmetric binary mask at each attention layer within the DiT\narchitecture, ensuring that each text annotation applies exclusively to its\nrespective video segment while preserving temporal coherence across visual\ntokens. This attention mechanism enables precise segment-level\ntextual-to-visual alignment, allowing the DiT architecture to effectively\nhandle video generation tasks with a fixed number of scenes. To further equip\nthe DiT architecture with the ability to generate additional scenes based on\nexisting ones, we incorporate a segment-level conditional mask, which\nconditions each newly generated segment on the preceding video segments,\nthereby enabling auto-regressive scene extension. Both qualitative and\nquantitative experiments confirm that Mask$^2$DiT excels in maintaining visual\nconsistency across segments while ensuring semantic alignment between each\nsegment and its corresponding text description. Our project page is\nhttps://tianhao-qi.github.io/Mask2DiTProject."}
{"id": "2503.19897", "pdf": "https://arxiv.org/pdf/2503.19897", "abs": "https://arxiv.org/abs/2503.19897", "authors": ["Lifu Wang", "Daqing Liu", "Xinchen Liu", "Xiaodong He"], "title": "Scaling Down Text Encoders of Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "Text encoders in diffusion models have rapidly evolved, transitioning from\nCLIP to T5-XXL. Although this evolution has significantly enhanced the models'\nability to understand complex prompts and generate text, it also leads to a\nsubstantial increase in the number of parameters. Despite T5 series encoders\nbeing trained on the C4 natural language corpus, which includes a significant\namount of non-visual data, diffusion models with T5 encoder do not respond to\nthose non-visual prompts, indicating redundancy in representational power.\nTherefore, it raises an important question: \"Do we really need such a large\ntext encoder?\" In pursuit of an answer, we employ vision-based knowledge\ndistillation to train a series of T5 encoder models. To fully inherit its\ncapabilities, we constructed our dataset based on three criteria: image\nquality, semantic understanding, and text-rendering. Our results demonstrate\nthe scaling down pattern that the distilled T5-base model can generate images\nof comparable quality to those produced by T5-XXL, while being 50 times smaller\nin size. This reduction in model size significantly lowers the GPU requirements\nfor running state-of-the-art models such as FLUX and SD3, making high-quality\ntext-to-image generation more accessible."}
{"id": "2503.19900", "pdf": "https://arxiv.org/pdf/2503.19900", "abs": "https://arxiv.org/abs/2503.19900", "authors": ["Hao Yu", "Zhuokai Zhao", "Shen Yan", "Lukasz Korycki", "Jianyu Wang", "Baosheng He", "Jiayi Liu", "Lizhu Zhang", "Xiangjun Fan", "Hanchao Yu"], "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation."}
{"id": "2503.19901", "pdf": "https://arxiv.org/pdf/2503.19901", "abs": "https://arxiv.org/abs/2503.19901", "authors": ["Liang Pan", "Zeshi Yang", "Zhiyang Dou", "Wenjia Wang", "Buzhen Huang", "Bo Dai", "Taku Komura", "Jingbo Wang"], "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps://liangpan99.github.io/TokenHSI/"}
{"id": "2503.19902", "pdf": "https://arxiv.org/pdf/2503.19902", "abs": "https://arxiv.org/abs/2503.19902", "authors": ["Fernando Julio Cendra", "Kai Han"], "title": "ICE: Intrinsic Concept Extraction from a Single Image via Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://visual-ai.github.io/ice", "summary": "The inherent ambiguity in defining visual concepts poses significant\nchallenges for modern generative models, such as the diffusion-based\nText-to-Image (T2I) models, in accurately learning concepts from a single\nimage. Existing methods lack a systematic way to reliably extract the\ninterpretable underlying intrinsic concepts. To address this challenge, we\npresent ICE, short for Intrinsic Concept Extraction, a novel framework that\nexclusively utilizes a T2I model to automatically and systematically extract\nintrinsic concepts from a single image. ICE consists of two pivotal stages. In\nthe first stage, ICE devises an automatic concept localization module to\npinpoint relevant text-based concepts and their corresponding masks within the\nimage. This critical stage streamlines concept initialization and provides\nprecise guidance for subsequent analysis. The second stage delves deeper into\neach identified mask, decomposing the object-level concepts into intrinsic\nconcepts and general concepts. This decomposition allows for a more granular\nand interpretable breakdown of visual elements. Our framework demonstrates\nsuperior performance on intrinsic concept extraction from a single image in an\nunsupervised manner. Project page: https://visual-ai.github.io/ice"}
{"id": "2503.19903", "pdf": "https://arxiv.org/pdf/2503.19903", "abs": "https://arxiv.org/abs/2503.19903", "authors": ["Baifeng Shi", "Boyi Li", "Han Cai", "Yao Lu", "Sifei Liu", "Marco Pavone", "Jan Kautz", "Song Han", "Trevor Darrell", "Pavlo Molchanov", "Hongxu Yin"], "title": "Scaling Vision Pre-Training to 4K Resolution", "categories": ["cs.CV"], "comment": "CVPR 2025. Project Page: https://nvlabs.github.io/PS3", "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL."}
{"id": "2503.19904", "pdf": "https://arxiv.org/pdf/2503.19904", "abs": "https://arxiv.org/abs/2503.19904", "authors": ["Zihang Lai", "Andrea Vedaldi"], "title": "Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025. Project website: zlai0.github.io/TrackTention", "summary": "Temporal consistency is critical in video prediction to ensure that outputs\nare coherent and free of artifacts. Traditional methods, such as temporal\nattention and 3D convolution, may struggle with significant object motion and\nmay not capture long-range temporal dependencies in dynamic scenes. To address\nthis gap, we propose the Tracktention Layer, a novel architectural component\nthat explicitly integrates motion information using point tracks, i.e.,\nsequences of corresponding points across frames. By incorporating these motion\ncues, the Tracktention Layer enhances temporal alignment and effectively\nhandles complex object motions, maintaining consistent feature representations\nover time. Our approach is computationally efficient and can be seamlessly\nintegrated into existing models, such as Vision Transformers, with minimal\nmodification. It can be used to upgrade image-only models to state-of-the-art\nvideo ones, sometimes outperforming models natively designed for video\nprediction. We demonstrate this on video depth prediction and video\ncolorization, where models augmented with the Tracktention Layer exhibit\nsignificantly improved temporal consistency compared to baselines."}
{"id": "2503.19906", "pdf": "https://arxiv.org/pdf/2503.19906", "abs": "https://arxiv.org/abs/2503.19906", "authors": ["Hongyu Liu", "Xuan Wang", "Ziyu Wan", "Yue Ma", "Jingye Chen", "Yanbo Fan", "Yujun Shen", "Yibing Song", "Qifeng Chen"], "title": "AvatarArtist: Open-Domain 4D Avatarization", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "This work focuses on open-domain 4D avatarization, with the purpose of\ncreating a 4D avatar from a portrait image in an arbitrary style. We select\nparametric triplanes as the intermediate 4D representation and propose a\npractical training paradigm that takes advantage of both generative adversarial\nnetworks (GANs) and diffusion models. Our design stems from the observation\nthat 4D GANs excel at bridging images and triplanes without supervision yet\nusually face challenges in handling diverse data distributions. A robust 2D\ndiffusion prior emerges as the solution, assisting the GAN in transferring its\nexpertise across various domains. The synergy between these experts permits the\nconstruction of a multi-domain image-triplane dataset, which drives the\ndevelopment of a general 4D avatar creator. Extensive experiments suggest that\nour model, AvatarArtist, is capable of producing high-quality 4D avatars with\nstrong robustness to various source image domains. The code, the data, and the\nmodels will be made publicly available to facilitate future studies.."}
{"id": "2503.19907", "pdf": "https://arxiv.org/pdf/2503.19907", "abs": "https://arxiv.org/abs/2503.19907", "authors": ["Xuan Ju", "Weicai Ye", "Quande Liu", "Qiulin Wang", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Qiang Xu"], "title": "FullDiT: Multi-Task Video Generative Foundation Model with Full Attention", "categories": ["cs.CV"], "comment": "Project Page: https://fulldit.github.io/", "summary": "Current video generative foundation models primarily focus on text-to-video\ntasks, providing limited control for fine-grained video content creation.\nAlthough adapter-based approaches (e.g., ControlNet) enable additional controls\nwith minimal fine-tuning, they encounter challenges when integrating multiple\nconditions, including: branch conflicts between independently trained adapters,\nparameter redundancy leading to increased computational cost, and suboptimal\nperformance compared to full fine-tuning. To address these challenges, we\nintroduce FullDiT, a unified foundation model for video generation that\nseamlessly integrates multiple conditions via unified full-attention\nmechanisms. By fusing multi-task conditions into a unified sequence\nrepresentation and leveraging the long-context learning ability of full\nself-attention to capture condition dynamics, FullDiT reduces parameter\noverhead, avoids conditions conflict, and shows scalability and emergent\nability. We further introduce FullBench for multi-task video generation\nevaluation. Experiments demonstrate that FullDiT achieves state-of-the-art\nresults, highlighting the efficacy of full-attention in complex multi-task\nvideo generation."}
{"id": "2503.19910", "pdf": "https://arxiv.org/pdf/2503.19910", "abs": "https://arxiv.org/abs/2503.19910", "authors": ["Chuong Huynh", "Jinyu Yang", "Ashish Tawari", "Mubarak Shah", "Son Tran", "Raffay Hamid", "Trishul Chilimbi", "Abhinav Shrivastava"], "title": "CoLLM: A Large Language Model for Composed Image Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": "CVPR 2025. Project page: https://collm-cvpr25.github.io/", "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field."}
{"id": "2503.19912", "pdf": "https://arxiv.org/pdf/2503.19912", "abs": "https://arxiv.org/abs/2503.19912", "authors": ["Xiang Xu", "Lingdong Kong", "Hui Shuai", "Wenwei Zhang", "Liang Pan", "Kai Chen", "Ziwei Liu", "Qingshan Liu"], "title": "SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data Pretraining", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Preprint; 15 pages, 6 figures, 10 tables; Code at\n  https://github.com/Xiangxu-0103/SuperFlow", "summary": "LiDAR representation learning has emerged as a promising approach to reducing\nreliance on costly and labor-intensive human annotations. While existing\nmethods primarily focus on spatial alignment between LiDAR and camera sensors,\nthey often overlook the temporal dynamics critical for capturing motion and\nscene continuity in driving scenarios. To address this limitation, we propose\nSuperFlow++, a novel framework that integrates spatiotemporal cues in both\npretraining and downstream tasks using consecutive LiDAR-camera pairs.\nSuperFlow++ introduces four key components: (1) a view consistency alignment\nmodule to unify semantic information across camera views, (2) a dense-to-sparse\nconsistency regularization mechanism to enhance feature robustness across\nvarying point cloud densities, (3) a flow-based contrastive learning approach\nthat models temporal relationships for improved scene understanding, and (4) a\ntemporal voting strategy that propagates semantic information across LiDAR\nscans to improve prediction consistency. Extensive evaluations on 11\nheterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms\nstate-of-the-art methods across diverse tasks and driving conditions.\nFurthermore, by scaling both 2D and 3D backbones during pretraining, we uncover\nemergent properties that provide deeper insights into developing scalable 3D\nfoundation models. With strong generalizability and computational efficiency,\nSuperFlow++ establishes a new benchmark for data-efficient LiDAR-based\nperception in autonomous driving. The code is publicly available at\nhttps://github.com/Xiangxu-0103/SuperFlow"}
{"id": "2503.19913", "pdf": "https://arxiv.org/pdf/2503.19913", "abs": "https://arxiv.org/abs/2503.19913", "authors": ["Mingju Gao", "Yike Pan", "Huan-ang Gao", "Zongzheng Zhang", "Wenyi Li", "Hao Dong", "Hao Tang", "Li Yi", "Hao Zhao"], "title": "PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page: https://partrm.c7w.tech/", "summary": "As interest grows in world models that predict future states from current\nobservations and actions, accurately modeling part-level dynamics has become\nincreasingly relevant for various applications. Existing approaches, such as\nPuppet-Master, rely on fine-tuning large-scale pre-trained video diffusion\nmodels, which are impractical for real-world use due to the limitations of 2D\nvideo representation and slow processing times. To overcome these challenges,\nwe present PartRM, a novel 4D reconstruction framework that simultaneously\nmodels appearance, geometry, and part-level motion from multi-view images of a\nstatic object. PartRM builds upon large 3D Gaussian reconstruction models,\nleveraging their extensive knowledge of appearance and geometry in static\nobjects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset,\nproviding multi-view observations of part-level dynamics across over 20,000\nstates. We enhance the model's understanding of interaction conditions with a\nmulti-scale drag embedding module that captures dynamics at varying\ngranularities. To prevent catastrophic forgetting during fine-tuning, we\nimplement a two-stage training process that focuses sequentially on motion and\nappearance learning. Experimental results show that PartRM establishes a new\nstate-of-the-art in part-level motion learning and can be applied in\nmanipulation tasks in robotics. Our code, data, and models are publicly\navailable to facilitate future research."}
{"id": "2503.19914", "pdf": "https://arxiv.org/pdf/2503.19914", "abs": "https://arxiv.org/abs/2503.19914", "authors": ["Sangwon Beak", "Hyeonwoo Kim", "Hanbyul Joo"], "title": "Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion Models", "categories": ["cs.CV"], "comment": "Project Page: https://tlb-miss.github.io/oor/", "summary": "We present a method for learning 3D spatial relationships between object\npairs, referred to as object-object spatial relationships (OOR), by leveraging\nsynthetically generated 3D samples from pre-trained 2D diffusion models. We\nhypothesize that images synthesized by 2D diffusion models inherently capture\nplausible and realistic OOR cues, enabling efficient ways to collect a 3D\ndataset to learn OOR for various unbounded object categories. Our approach\nbegins by synthesizing diverse images that capture plausible OOR cues, which we\nthen uplift into 3D samples. Leveraging our diverse collection of plausible 3D\nsamples for the object pairs, we train a score-based OOR diffusion model to\nlearn the distribution of their relative spatial relationships. Additionally,\nwe extend our pairwise OOR to multi-object OOR by enforcing consistency across\npairwise relations and preventing object collisions. Extensive experiments\ndemonstrate the robustness of our method across various object-object spatial\nrelationships, along with its applicability to real-world 3D scene arrangement\ntasks using the OOR diffusion model."}
{"id": "2503.19916", "pdf": "https://arxiv.org/pdf/2503.19916", "abs": "https://arxiv.org/abs/2503.19916", "authors": ["Lingdong Kong", "Dongyue Lu", "Xiang Xu", "Lai Xing Ng", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "EventFly: Event Camera Perception from Ground to the Sky", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025; 30 pages, 8 figures, 16 tables; Project Page at\n  https://event-fly.github.io/", "summary": "Cross-platform adaptation in event-based dense perception is crucial for\ndeploying event cameras across diverse settings, such as vehicles, drones, and\nquadrupeds, each with unique motion dynamics, viewpoints, and class\ndistributions. In this work, we introduce EventFly, a framework for robust\ncross-platform adaptation in event camera perception. Our approach comprises\nthree key components: i) Event Activation Prior (EAP), which identifies\nhigh-activation regions in the target domain to minimize prediction entropy,\nfostering confident, domain-adaptive predictions; ii) EventBlend, a data-mixing\nstrategy that integrates source and target event voxel grids based on\nEAP-driven similarity and density maps, enhancing feature alignment; and iii)\nEventMatch, a dual-discriminator technique that aligns features from source,\ntarget, and blended domains for better domain-invariant learning. To\nholistically assess cross-platform adaptation abilities, we introduce EXPo, a\nlarge-scale benchmark with diverse samples across vehicle, drone, and quadruped\nplatforms. Extensive experiments validate our effectiveness, demonstrating\nsubstantial gains over popular adaptation methods. We hope this work can pave\nthe way for more adaptive, high-performing event perception across diverse and\ncomplex environments."}
{"id": "2503.18955", "pdf": "https://arxiv.org/pdf/2503.18955", "abs": "https://arxiv.org/abs/2503.18955", "authors": ["Vincent C. Müller"], "title": "Is there a future for AI without representation?", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This paper investigates the prospects of AI without representation in\ngeneral, and the proposals of Rodney Brooks in particular. What turns out to be\ncharacteristic of Brooks' proposal is the rejection of central control in\nintelligent agents; his systems has as much or as little representation as\ntraditional AI. The traditional view that representation is necessary for\nintelligence presupposes that intelligence requires central control. However,\nmuch of recent cognitive science suggests that we should dispose of the image\nof intelligent agents as central representation processors. If this paradigm\nshift is achieved, Brooks' proposal for non-centralized cognition without\nrepresentation appears promising for full-blown intelligent agents - though not\nfor conscious agents and thus not for human-like AI."}
{"id": "2503.18973", "pdf": "https://arxiv.org/pdf/2503.18973", "abs": "https://arxiv.org/abs/2503.18973", "authors": ["Muhammad Ahmad", "Sardar Usman", "Ildar Batyrshin", "Muhammad Muzammil", "K. Sajid", "M. Hasnain", "Muhammad Jalal", "Grigori Sidorov"], "title": "Automated diagnosis of lung diseases using vision transformer: a comparative study on chest x-ray classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Background: Lung disease is a significant health issue, particularly in\nchildren and elderly individuals. It often results from lung infections and is\none of the leading causes of mortality in children. Globally, lung-related\ndiseases claim many lives each year, making early and accurate diagnoses\ncrucial. Radiographs are valuable tools for the diagnosis of such conditions.\nThe most prevalent lung diseases, including pneumonia, asthma, allergies,\nchronic obstructive pulmonary disease (COPD), bronchitis, emphysema, and lung\ncancer, represent significant public health challenges. Early prediction of\nthese conditions is critical, as it allows for the identification of risk\nfactors and implementation of preventive measures to reduce the likelihood of\ndisease onset\n  Methods: In this study, we utilized a dataset comprising 3,475 chest X-ray\nimages sourced from from Mendeley Data provided by Talukder, M. A. (2023) [14],\ncategorized into three classes: normal, lung opacity, and pneumonia. We applied\nfive pre-trained deep learning models, including CNN, ResNet50, DenseNet,\nCheXNet, and U-Net, as well as two transfer learning algorithms such as Vision\nTransformer (ViT) and Shifted Window (Swin) to classify these images. This\napproach aims to address diagnostic issues in lung abnormalities by reducing\nreliance on human intervention through automated classification systems. Our\nanalysis was conducted in both binary and multiclass settings. Results: In the\nbinary classification, we focused on distinguishing between normal and viral\npneumonia cases, whereas in the multi-class classification, all three classes\n(normal, lung opacity, and viral pneumonia) were included. Our proposed\nmethodology (ViT) achieved remarkable performance, with accuracy rates of 99%\nfor binary classification and 95.25% for multiclass classification."}
{"id": "2503.18985", "pdf": "https://arxiv.org/pdf/2503.18985", "abs": "https://arxiv.org/abs/2503.18985", "authors": ["Xuan Liu", "Xiaobin Chang"], "title": "LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "In continual learning (CL), catastrophic forgetting often arises due to\nfeature drift. This challenge is particularly prominent in the exemplar-free\ncontinual learning (EFCL) setting, where samples from previous tasks cannot be\nretained, making it difficult to preserve prior knowledge. To address this\nissue, some EFCL methods aim to identify feature spaces that minimize the\nimpact on previous tasks while accommodating new ones. However, they rely on\nstatic features or outdated statistics stored from old tasks, which prevents\nthem from capturing the dynamic evolution of the feature space in CL, leading\nto performance degradation over time. In this paper, we introduce the\nDrift-Resistant Space (DRS), which effectively handles feature drifts without\nrequiring explicit feature modeling or the storage of previous tasks. A novel\nparameter-efficient fine-tuning approach called Low-Rank Adaptation Subtraction\n(LoRA-) is proposed to develop the DRS. This method subtracts the LoRA weights\nof old tasks from the initial pre-trained weight before processing new task\ndata to establish the DRS for model training. Therefore, LoRA- enhances\nstability, improves efficiency, and simplifies implementation. Furthermore,\nstabilizing feature drifts allows for better plasticity by learning with a\ntriplet loss. Our method consistently achieves state-of-the-art results,\nespecially for long task sequences, across multiple datasets."}
{"id": "2503.18998", "pdf": "https://arxiv.org/pdf/2503.18998", "abs": "https://arxiv.org/abs/2503.18998", "authors": ["Haiqi Liu", "C. L. Philip Chen", "Tong Zhang"], "title": "FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG Emotion Recognition", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Under Review", "summary": "Cross-subject EEG emotion recognition is challenged by significant\ninter-subject variability and intricately entangled intra-subject variability.\nExisting works have primarily addressed these challenges through domain\nadaptation or generalization strategies. However, they typically require\nextensive target subject data or demonstrate limited generalization performance\nto unseen subjects. Recent few-shot learning paradigms attempt to address these\nlimitations but often encounter catastrophic overfitting during\nsubject-specific adaptation with limited samples. This article introduces the\nfew-shot adapter with a cross-view fusion method called FACE for cross-subject\nEEG emotion recognition, which leverages dynamic multi-view fusion and\neffective subject-specific adaptation. Specifically, FACE incorporates a\ncross-view fusion module that dynamically integrates global brain connectivity\nwith localized patterns via subject-specific fusion weights to provide\ncomplementary emotional information. Moreover, the few-shot adapter module is\nproposed to enable rapid adaptation for unseen subjects while reducing\noverfitting by enhancing adapter structures with meta-learning. Experimental\nresults on three public EEG emotion recognition benchmarks demonstrate FACE's\nsuperior generalization performance over state-of-the-art methods. FACE\nprovides a practical solution for cross-subject scenarios with limited labeled\ndata."}
{"id": "2503.19005", "pdf": "https://arxiv.org/pdf/2503.19005", "abs": "https://arxiv.org/abs/2503.19005", "authors": ["Abdul Qayyum", "Moona Mazher", "Devran Ugurlu", "Jose Alonso Solis Lemus", "Cristobal Rodero", "Steven A Niederer"], "title": "Foundation Model for Whole-Heart Segmentation: Leveraging Student-Teacher Learning in Multi-Modal Medical Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Whole-heart segmentation from CT and MRI scans is crucial for cardiovascular\ndisease analysis, yet existing methods struggle with modality-specific biases\nand the need for extensive labeled datasets. To address these challenges, we\npropose a foundation model for whole-heart segmentation using a self-supervised\nlearning (SSL) framework based on a student-teacher architecture. Our model is\npretrained on a large, unlabeled dataset of CT and MRI scans, leveraging the\nxLSTM backbone to capture long-range spatial dependencies and complex\nanatomical structures in 3D medical images. By incorporating multi-modal\npretraining, our approach ensures strong generalization across both CT and MRI\nmodalities, mitigating modality-specific variations and improving segmentation\naccuracy in diverse clinical settings. The use of large-scale unlabeled data\nsignificantly reduces the dependency on manual annotations, enabling robust\nperformance even with limited labeled data. We further introduce an\nxLSTM-UNet-based architecture for downstream whole-heart segmentation tasks,\ndemonstrating its effectiveness on few-label CT and MRI datasets. Our results\nvalidate the robustness and adaptability of the proposed model, highlighting\nits potential for advancing automated whole-heart segmentation in medical\nimaging."}
{"id": "2503.19041", "pdf": "https://arxiv.org/pdf/2503.19041", "abs": "https://arxiv.org/abs/2503.19041", "authors": ["Kangwei Liu", "Mengru Wang", "Yujie Luo", "Lin Yuan", "Mengshu Sun", "Ningyu Zhang", "Lei Liang", "Zhiqiang Zhang", "Jun Zhou", "Huajun Chen"], "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning."}
{"id": "2503.19083", "pdf": "https://arxiv.org/pdf/2503.19083", "abs": "https://arxiv.org/abs/2503.19083", "authors": ["Swati Sharma", "Fabian A. Braeu", "Thanadet Chuangsuwanich", "Tin A. Tun", "Quan V Hoang", "Rachel Chong", "Shamira Perera", "Ching-Lin Ho", "Rahat Husain", "Martin L. Buist", "Tin Aung", "Michaël J. A. Girard"], "title": "3D Structural Phenotype of the Optic Nerve Head at the Intersection of Glaucoma and Myopia - A Key to Improving Glaucoma Diagnosis in Myopic Populations", "categories": ["eess.IV", "cs.CV"], "comment": "27 Pages, 2 Tables, 6 Figures, 1 Appendix", "summary": "Purpose: To characterize the 3D structural phenotypes of the optic nerve head\n(ONH) in patients with glaucoma, high myopia, and concurrent high myopia and\nglaucoma, and to evaluate their variations across these conditions.\nParticipants: A total of 685 optical coherence tomography (OCT) scans from 754\nsubjects of Singapore-Chinese ethnicity, including 256 healthy (H), 94 highly\nmyopic (HM), 227 glaucomatous (G), and 108 highly myopic with glaucoma (HMG)\ncases. Methods: We segmented the retinal and connective tissues from OCT\nvolumes and their boundary edges were converted into 3D point clouds. To\nclassify the 3D point clouds into four ONH conditions, i.e., H, HM, G, and HMG,\na specialized ensemble network was developed, consisting of an encoder to\ntransform high-dimensional input data into a compressed latent vector, a\ndecoder to reconstruct point clouds from the latent vector, and a classifier to\ncategorize the point clouds into the four ONH conditions. Results: The\nclassification network achieved high accuracy, distinguishing H, HM, G, and HMG\nclasses with a micro-average AUC of 0.92 $\\pm$ 0.03 on an independent test set.\nThe decoder effectively reconstructed point clouds, achieving a Chamfer loss of\n0.013 $\\pm$ 0.002. Dimensionality reduction clustered ONHs into four distinct\ngroups, revealing structural variations such as changes in retinal and\nconnective tissue thickness, tilting and stretching of the disc and scleral\ncanal opening, and alterations in optic cup morphology, including shallow or\ndeep excavation, across the four conditions. Conclusions: This study\ndemonstrated that ONHs exhibit distinct structural signatures across H, HM, G,\nand HMG conditions. The findings further indicate that ONH morphology provides\nsufficient information for classification into distinct clusters, with\nprincipal components capturing unique structural patterns within each group."}
{"id": "2503.19119", "pdf": "https://arxiv.org/pdf/2503.19119", "abs": "https://arxiv.org/abs/2503.19119", "authors": ["Yiling Wang", "Elia Lombardo", "Adrian Thummerer", "Tom Blöcker", "Yu Fan", "Yue Zhao", "Christianna Iris Papadopoulou", "Coen Hurkmans", "Rob H. N. Tijssen", "Pia A. W. Görts", "Shyama U. Tetar", "Davide Cusumano", "Martijn P. W. Intven", "Pim Borman", "Marco Riboldi", "Denis Dudáš", "Hilary Byrne", "Lorenzo Placidi", "Marco Fusella", "Michael Jameson", "Miguel Palacios", "Paul Cobussen", "Tobias Finazzi", "Cornelis J. A. Haasbeek", "Paul Keall", "Christopher Kurz", "Guillaume Landry", "Matteo Maspero"], "title": "TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided radiotherapy", "categories": ["physics.med-ph", "cs.CV"], "comment": "10 pages, 5 figures, 2 tables; submitted to Medical Physics", "summary": "Purpose: Magnetic resonance imaging (MRI) to visualize anatomical motion is\nbecoming increasingly important when treating cancer patients with\nradiotherapy. Hybrid MRI-linear accelerator (MRI-linac) systems allow real-time\nmotion management during irradiation. This paper presents a multi-institutional\nreal-time MRI time series dataset from different MRI-linac vendors. The dataset\nis designed to support developing and evaluating real-time tumor localization\n(tracking) algorithms for MRI-guided radiotherapy within the TrackRAD2025\nchallenge (https://trackrad2025.grand-challenge.org/).\n  Acquisition and validation methods: The dataset consists of sagittal 2D cine\nMRIs in 585 patients from six centers (3 Dutch, 1 German, 1 Australian, and 1\nChinese). Tumors in the thorax, abdomen, and pelvis acquired on two\ncommercially available MRI-linacs (0.35 T and 1.5 T) were included. For 108\ncases, irradiation targets or tracking surrogates were manually segmented on\neach temporal frame. The dataset was randomly split into a public training set\nof 527 cases (477 unlabeled and 50 labeled) and a private testing set of 58\ncases (all labeled).\n  Data Format and Usage Notes: The data is publicly available under the\nTrackRAD2025 collection: https://doi.org/10.57967/hf/4539. Both the images and\nsegmentations for each patient are available in metadata format.\n  Potential Applications: This novel clinical dataset will enable the\ndevelopment and evaluation of real-time tumor localization algorithms for\nMRI-guided radiotherapy. By enabling more accurate motion management and\nadaptive treatment strategies, this dataset has the potential to advance the\nfield of radiotherapy significantly."}
{"id": "2503.19136", "pdf": "https://arxiv.org/pdf/2503.19136", "abs": "https://arxiv.org/abs/2503.19136", "authors": ["Sidhanth Holalkere", "David S. Bindel", "Silvia Sellán", "Alexander Terenin"], "title": "Stochastic Poisson Surface Reconstruction with One Solve using Geometric Gaussian Processes", "categories": ["cs.GR", "cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Poisson Surface Reconstruction is a widely-used algorithm for reconstructing\na surface from an oriented point cloud. To facilitate applications where only\npartial surface information is available, or scanning is performed\nsequentially, a recent line of work proposes to incorporate uncertainty into\nthe reconstructed surface via Gaussian process models. The resulting algorithms\nfirst perform Gaussian process interpolation, then solve a set of volumetric\npartial differential equations globally in space, resulting in a\ncomputationally expensive two-stage procedure. In this work, we apply\nrecently-developed techniques from geometric Gaussian processes to combine\ninterpolation and surface reconstruction into a single stage, requiring only\none linear solve per sample. The resulting reconstructed surface samples can be\nqueried locally in space, without the use of problem-dependent volumetric\nmeshes or grids. These capabilities enable one to (a) perform probabilistic\ncollision detection locally around the region of interest, (b) perform ray\ncasting without evaluating points not on the ray's trajectory, and (c) perform\nnext-view planning on a per-slice basis. They also improve reconstruction\nquality, by not requiring one to approximate kernel matrix inverses with\ndiagonal matrices as part of intermediate computations. Results show that our\napproach provides a cleaner, more-principled, and more-flexible stochastic\nsurface reconstruction pipeline."}
{"id": "2503.19146", "pdf": "https://arxiv.org/pdf/2503.19146", "abs": "https://arxiv.org/abs/2503.19146", "authors": ["Yorick Estievenart", "Sukanya Patra", "Souhaib Ben Taieb"], "title": "Risk-Based Thresholding for Reliable Anomaly Detection in Concentrated Solar Power Plants", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Efficient and reliable operation of Concentrated Solar Power (CSP) plants is\nessential for meeting the growing demand for sustainable energy. However,\nhigh-temperature solar receivers face severe operational risks, such as\nfreezing, deformation, and corrosion, resulting in costly downtime and\nmaintenance. To monitor CSP plants, cameras mounted on solar receivers record\ninfrared images at irregular intervals ranging from one to five minutes\nthroughout the day. Anomalous images can be detected by thresholding an anomaly\nscore, where the threshold is chosen to optimize metrics such as the F1-score\non a validation set. This work proposes a framework for generating more\nreliable decision thresholds with finite-sample coverage guarantees on any\nchosen risk function. Our framework also incorporates an abstention mechanism,\nallowing high-risk predictions to be deferred to domain experts. Second, we\npropose a density forecasting method to estimate the likelihood of an observed\nimage given a sequence of previously observed images, using this likelihood as\nits anomaly score. Third, we analyze the deployment results of our framework\nacross multiple training scenarios over several months for two CSP plants. This\nanalysis provides valuable insights to our industry partner for optimizing\nmaintenance operations. Finally, given the confidential nature of our dataset,\nwe provide an extended simulated dataset, leveraging recent advancements in\ngenerative modeling to create diverse thermal images that simulate multiple CSP\nplants. Our code is publicly available."}
{"id": "2503.19149", "pdf": "https://arxiv.org/pdf/2503.19149", "abs": "https://arxiv.org/abs/2503.19149", "authors": ["Christian John Hurry", "Jinjie Zhang", "Olubukola Ishola", "Emma Slade", "Cuong Q. Nguyen"], "title": "Out-of-distribution evaluations of channel agnostic masked autoencoders in fluorescence microscopy", "categories": ["cs.LG", "cs.CV"], "comment": "13 pages, 5 figures", "summary": "Developing computer vision for high-content screening is challenging due to\nvarious sources of distribution-shift caused by changes in experimental\nconditions, perturbagens, and fluorescent markers. The impact of different\nsources of distribution-shift are confounded in typical evaluations of models\nbased on transfer learning, which limits interpretations of how changes to\nmodel design and training affect generalisation. We propose an evaluation\nscheme that isolates sources of distribution-shift using the JUMP-CP dataset,\nallowing researchers to evaluate generalisation with respect to specific\nsources of distribution-shift. We then present a channel-agnostic masked\nautoencoder $\\mathbf{Campfire}$ which, via a shared decoder for all channels,\nscales effectively to datasets containing many different fluorescent markers,\nand show that it generalises to out-of-distribution experimental batches,\nperturbagens, and fluorescent markers, and also demonstrates successful\ntransfer learning from one cell type to another."}
{"id": "2503.19152", "pdf": "https://arxiv.org/pdf/2503.19152", "abs": "https://arxiv.org/abs/2503.19152", "authors": ["Shoffan Saifullah", "Rafał Dreżewski"], "title": "PSO-UNet: Particle Swarm-Optimized U-Net Framework for Precise Multimodal Brain Tumor Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "68Q07", "I.4.6; I.2"], "comment": "9 pages, 6 figures, 4 tables, Gecco 2025 Conference", "summary": "Medical image segmentation, particularly for brain tumor analysis, demands\nprecise and computationally efficient models due to the complexity of\nmultimodal MRI datasets and diverse tumor morphologies. This study introduces\nPSO-UNet, which integrates Particle Swarm Optimization (PSO) with the U-Net\narchitecture for dynamic hyperparameter optimization. Unlike traditional manual\ntuning or alternative optimization approaches, PSO effectively navigates\ncomplex hyperparameter search spaces, explicitly optimizing the number of\nfilters, kernel size, and learning rate. PSO-UNet substantially enhances\nsegmentation performance, achieving Dice Similarity Coefficients (DSC) of\n0.9578 and 0.9523 and Intersection over Union (IoU) scores of 0.9194 and 0.9097\non the BraTS 2021 and Figshare datasets, respectively. Moreover, the method\nreduces computational complexity significantly, utilizing only 7.8 million\nparameters and executing in approximately 906 seconds, markedly faster than\ncomparable U-Net-based frameworks. These outcomes underscore PSO-UNet's robust\ngeneralization capabilities across diverse MRI modalities and tumor\nclassifications, emphasizing its clinical potential and clear advantages over\nconventional hyperparameter tuning methods. Future research will explore hybrid\noptimization strategies and validate the framework against other bio-inspired\nalgorithms to enhance its robustness and scalability."}
{"id": "2503.19232", "pdf": "https://arxiv.org/pdf/2503.19232", "abs": "https://arxiv.org/abs/2503.19232", "authors": ["Xinpeng Liu", "Zeyi Huang", "Fumio Okura", "Yasuyuki Matsushita"], "title": "HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": "Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR'25)", "summary": "Novel view synthesis has demonstrated impressive progress recently, with 3D\nGaussian splatting (3DGS) offering efficient training time and photorealistic\nreal-time rendering. However, reliance on Cartesian coordinates limits 3DGS's\nperformance on distant objects, which is important for reconstructing unbounded\noutdoor environments. We found that, despite its ultimate simplicity, using\nhomogeneous coordinates, a concept on the projective geometry, for the 3DGS\npipeline remarkably improves the rendering accuracies of distant objects. We\ntherefore propose Homogeneous Gaussian Splatting (HoGS) incorporating\nhomogeneous coordinates into the 3DGS framework, providing a unified\nrepresentation for enhancing near and distant objects. HoGS effectively manages\nboth expansive spatial positions and scales particularly in outdoor unbounded\nenvironments by adopting projective geometry principles. Experiments show that\nHoGS significantly enhances accuracy in reconstructing distant objects while\nmaintaining high-quality rendering of nearby objects, along with fast training\nspeed and real-time rendering capability. Our implementations are available on\nour project page https://kh129.github.io/hogs/."}
{"id": "2503.19248", "pdf": "https://arxiv.org/pdf/2503.19248", "abs": "https://arxiv.org/abs/2503.19248", "authors": ["Chonghang Zhao", "Mingyuan Ge", "Xiaogang Yang", "Yong S. Chu", "Hanfei Yan"], "title": "Limited-angle x-ray nano-tomography with machine-learning enabled iterative reconstruction engine", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": null, "summary": "A long-standing challenge in tomography is the 'missing wedge' problem, which\narises when the acquisition of projection images within a certain angular range\nis restricted due to geometrical constraints. This incomplete dataset results\nin significant artifacts and poor resolution in the reconstructed image. To\ntackle this challenge, we propose an approach dubbed Perception Fused Iterative\nTomography Reconstruction Engine, which integrates a convolutional neural\nnetwork (CNN) with perceptional knowledge as a smart regularizer into an\niterative solving engine. We employ the Alternating Direction Method of\nMultipliers to optimize the solution in both physics and image domains, thereby\nachieving a physically coherent and visually enhanced result. We demonstrate\nthe effectiveness of the proposed approach using various experimental datasets\nobtained with different x-ray microscopy techniques. All show significantly\nimproved reconstruction even with a missing wedge of over 100 degrees - a\nscenario where conventional methods fail. Notably, it also improves the\nreconstruction in case of sparse projections, despite the network not being\nspecifically trained for that. This demonstrates the robustness and generality\nof our method of addressing commonly occurring challenges in 3D x-ray imaging\napplications for real-world problems."}
{"id": "2503.19253", "pdf": "https://arxiv.org/pdf/2503.19253", "abs": "https://arxiv.org/abs/2503.19253", "authors": ["Zeqiang Wei", "Kai Jin", "Zeyi Hou", "Kuan Song", "Xiuzhuang Zhou"], "title": "$L^2$FMamba: Lightweight Light Field Image Super-Resolution with State Space Model", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Transformers bring significantly improved performance to the light field\nimage super-resolution task due to their long-range dependency modeling\ncapability. However, the inherently high computational complexity of their core\nself-attention mechanism has increasingly hindered their advancement in this\ntask. To address this issue, we first introduce the LF-VSSM block, a novel\nmodule inspired by progressive feature extraction, to efficiently capture\ncritical long-range spatial-angular dependencies in light field images. LF-VSSM\nsuccessively extracts spatial features within sub-aperture images,\nspatial-angular features between sub-aperture images, and spatial-angular\nfeatures between light field image pixels. On this basis, we propose a\nlightweight network, $L^2$FMamba (Lightweight Light Field Mamba), which\nintegrates the LF-VSSM block to leverage light field features for\nsuper-resolution tasks while overcoming the computational challenges of\nTransformer-based approaches. Extensive experiments on multiple light field\ndatasets demonstrate that our method reduces the number of parameters and\ncomplexity while achieving superior super-resolution performance with faster\ninference speed."}
{"id": "2503.19271", "pdf": "https://arxiv.org/pdf/2503.19271", "abs": "https://arxiv.org/abs/2503.19271", "authors": ["Xuechen Liang", "Meiling Tao", "Yinghui Xia", "Jianhui Wang", "Kun Li", "Yijin Wang", "Jingsong Yang", "Tianyu Shi", "Yuantao Wang", "Miao Zhang", "Xueqian Wang"], "title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information."}
{"id": "2503.19292", "pdf": "https://arxiv.org/pdf/2503.19292", "abs": "https://arxiv.org/abs/2503.19292", "authors": ["Xiaoqing Zhang", "Hanfeng Shi", "Xiangyu Li", "Haili Ye", "Tao Xu", "Na Li", "Yan Hu", "Fan Lv", "Jiangfan Chen", "Jiang Liu"], "title": "Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for Parkinson's Disease Screening in OCT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally.\nThe eye's retina is an extension of the brain and has great potential in PD\nscreening. Recent studies have suggested that texture features extracted from\nretinal layers can be adopted as biomarkers for PD diagnosis under optical\ncoherence tomography (OCT) images. Frequency domain learning techniques can\nenhance the feature representations of deep neural networks (DNNs) by\ndecomposing frequency components involving rich texture features. Additionally,\nprevious works have not exploited texture features for automated PD screening\nin OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet\nFilter (AWF) that serves as the Practical Texture Feature Amplifier to fully\nleverage the merits of texture features to boost the PD screening performance\nof DNNs with the aid of frequency domain learning. Specifically, AWF first\nenhances texture feature representation diversities via channel mixer, then\nemphasizes informative texture feature representations with the well-designed\nadaptive wavelet filtering token mixer. By combining the AWFs with the DNN\nstem, AWFNet is constructed for automated PD screening. Additionally, we\nintroduce a novel Balanced Confidence (BC) Loss by mining the potential of\nsample-wise predicted probabilities of all classes and class frequency prior,\nto further boost the PD screening performance and trustworthiness of AWFNet.\nThe extensive experiments manifest the superiority of our AWFNet and BC over\nstate-of-the-art methods in terms of PD screening performance and\ntrustworthiness."}
{"id": "2503.19329", "pdf": "https://arxiv.org/pdf/2503.19329", "abs": "https://arxiv.org/abs/2503.19329", "authors": ["Yongting Hu", "Yuxin Lin", "Chengliang Liu", "Xiaoling Luo", "Xiaoyan Dou", "Qihao Xu", "Yong Xu"], "title": "Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025", "summary": "Multi-view diabetic retinopathy (DR) detection has recently emerged as a\npromising method to address the issue of incomplete lesions faced by\nsingle-view DR. However, it is still challenging due to the variable sizes and\nscattered locations of lesions. Furthermore, existing multi-view DR methods\ntypically merge multiple views without considering the correlations and\nredundancies of lesion information across them. Therefore, we propose a novel\nmethod to overcome the challenges of difficult lesion information learning and\ninadequate multi-view fusion. Specifically, we introduce a two-branch network\nto obtain both local lesion features and their global dependencies. The\nhigh-frequency component of the wavelet transform is used to exploit lesion\nedge information, which is then enhanced by global semantic to facilitate\ndifficult lesion learning. Additionally, we present a cross-view fusion module\nto improve multi-view fusion and reduce redundancy. Experimental results on\nlarge public datasets demonstrate the effectiveness of our method. The code is\nopen sourced on https://github.com/HuYongting/WGLIN."}
{"id": "2503.19330", "pdf": "https://arxiv.org/pdf/2503.19330", "abs": "https://arxiv.org/abs/2503.19330", "authors": ["Jee Won Lee", "Hansol Lim", "SooYeun Yang", "Jongseong Brad Choi"], "title": "MATT-GS: Masked Attention-based 3DGS for Robot Perception and Object Detection", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": "This work has been submitted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) for possible publication", "summary": "This paper presents a novel masked attention-based 3D Gaussian Splatting\n(3DGS) approach to enhance robotic perception and object detection in\nindustrial and smart factory environments. U2-Net is employed for background\nremoval to isolate target objects from raw images, thereby minimizing clutter\nand ensuring that the model processes only relevant data. Additionally, a Sobel\nfilter-based attention mechanism is integrated into the 3DGS framework to\nenhance fine details - capturing critical features such as screws, wires, and\nintricate textures essential for high-precision tasks. We validate our approach\nusing quantitative metrics, including L1 loss, SSIM, PSNR, comparing the\nperformance of the background-removed and attention-incorporated 3DGS model\nagainst the ground truth images and the original 3DGS training baseline. The\nresults demonstrate significant improves in visual fidelity and detail\npreservation, highlighting the effectiveness of our method in enhancing robotic\nvision for object recognition and manipulation in complex industrial settings."}
{"id": "2503.19427", "pdf": "https://arxiv.org/pdf/2503.19427", "abs": "https://arxiv.org/abs/2503.19427", "authors": ["Muyi Bao", "Shuchang Lyu", "Zhaoyang Xu", "Qi Zhao", "Changyu Zeng", "Wenpei Bai", "Guangliang Cheng"], "title": "ASP-VMUNet: Atrous Shifted Parallel Vision Mamba U-Net for Skin Lesion Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Skin lesion segmentation is a critical challenge in computer vision, and it\nis essential to separate pathological features from healthy skin for\ndiagnostics accurately. Traditional Convolutional Neural Networks (CNNs) are\nlimited by narrow receptive fields, and Transformers face significant\ncomputational burdens. This paper presents a novel skin lesion segmentation\nframework, the Atrous Shifted Parallel Vision Mamba UNet (ASP-VMUNet), which\nintegrates the efficient and scalable Mamba architecture to overcome\nlimitations in traditional CNNs and computationally demanding Transformers. The\nframework introduces an atrous scan technique that minimizes background\ninterference and expands the receptive field, enhancing Mamba's scanning\ncapabilities. Additionally, the inclusion of a Parallel Vision Mamba (PVM)\nlayer and a shift round operation optimizes feature segmentation and fosters\nrich inter-segment information exchange. A supplementary CNN branch with a\nSelective-Kernel (SK) Block further refines the segmentation by blending local\nand global contextual information. Tested on four benchmark datasets\n(ISIC16/17/18 and PH2), ASP-VMUNet demonstrates superior performance in skin\nlesion segmentation, validated by comprehensive ablation studies. This approach\nnot only advances medical image segmentation but also highlights the benefits\nof hybrid architectures in medical imaging technology. Our code is available at\nhttps://github.com/BaoBao0926/ASP-VMUNet/tree/main."}
{"id": "2503.19429", "pdf": "https://arxiv.org/pdf/2503.19429", "abs": "https://arxiv.org/abs/2503.19429", "authors": ["Masaya Hasegawa", "Koji Yasuda"], "title": "Quantifying the Ease of Reproducing Training Data in Unconditional Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models, which have been advancing rapidly in recent years, may\ngenerate samples that closely resemble the training data. This phenomenon,\nknown as memorization, may lead to copyright issues. In this study, we propose\na method to quantify the ease of reproducing training data in unconditional\ndiffusion models. The average of a sample population following the Langevin\nequation in the reverse diffusion process moves according to a first-order\nordinary differential equation (ODE). This ODE establishes a 1-to-1\ncorrespondence between images and their noisy counterparts in the latent space.\nSince the ODE is reversible and the initial noisy images are sampled randomly,\nthe volume of an image's projected area represents the probability of\ngenerating those images. We examined the ODE, which projects images to latent\nspace, and succeeded in quantifying the ease of reproducing training data by\nmeasuring the volume growth rate in this process. Given the relatively low\ncomputational complexity of this method, it allows us to enhance the quality of\ntraining data by detecting and modifying the easily memorized training samples."}
{"id": "2503.19495", "pdf": "https://arxiv.org/pdf/2503.19495", "abs": "https://arxiv.org/abs/2503.19495", "authors": ["Stefano Della Fiore", "Alessandro Gnutti", "Marco Dalai", "Pierangelo Migliorati", "Riccardo Leonardi"], "title": "TFIC: End-to-End Text-Focused Image Compression for Coding for Machines", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Traditional image compression methods aim to faithfully reconstruct images\nfor human perception. In contrast, Coding for Machines focuses on compressing\nimages to preserve information relevant to a specific machine task. In this\npaper, we present an image compression system designed to retain text-specific\nfeatures for subsequent Optical Character Recognition (OCR). Our encoding\nprocess requires half the time needed by the OCR module, making it especially\nsuitable for devices with limited computational capacity. In scenarios where\non-device OCR is computationally prohibitive, images are compressed and later\nprocessed to recover the text content. Experimental results demonstrate that\nour method achieves significant improvements in text extraction accuracy at low\nbitrates, even improving over the accuracy of OCR performed on uncompressed\nimages, thus acting as a local pre-processing step."}
{"id": "2503.19505", "pdf": "https://arxiv.org/pdf/2503.19505", "abs": "https://arxiv.org/abs/2503.19505", "authors": ["Xiaohui Sun", "Jiangwei Mo", "Hanlin Wu", "Jie Ma"], "title": "Single-Step Latent Consistency Model for Remote Sensing Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Recent advancements in diffusion models (DMs) have greatly advanced remote\nsensing image super-resolution (RSISR). However, their iterative sampling\nprocesses often result in slow inference speeds, limiting their application in\nreal-time tasks. To address this challenge, we propose the latent consistency\nmodel for super-resolution (LCMSR), a novel single-step diffusion approach\ndesigned to enhance both efficiency and visual quality in RSISR tasks. Our\nproposal is structured into two distinct stages. In the first stage, we\npretrain a residual autoencoder to encode the differential information between\nhigh-resolution (HR) and low-resolution (LR) images, transitioning the\ndiffusion process into a latent space to reduce computational costs. The second\nstage focuses on consistency diffusion learning, which aims to learn the\ndistribution of residual encodings in the latent space, conditioned on LR\nimages. The consistency constraint enforces that predictions at any two\ntimesteps along the reverse diffusion trajectory remain consistent, enabling\ndirect mapping from noise to data. As a result, the proposed LCMSR reduces the\niterative steps of traditional diffusion models from 50-1000 or more to just a\nsingle step, significantly improving efficiency. Experimental results\ndemonstrate that LCMSR effectively balances efficiency and performance,\nachieving inference times comparable to non-diffusion models while maintaining\nhigh-quality output."}
{"id": "2503.19510", "pdf": "https://arxiv.org/pdf/2503.19510", "abs": "https://arxiv.org/abs/2503.19510", "authors": ["Sheng Wang"], "title": "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus."}
{"id": "2503.19523", "pdf": "https://arxiv.org/pdf/2503.19523", "abs": "https://arxiv.org/abs/2503.19523", "authors": ["Xin Cai"], "title": "One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in RLHF", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In this article, we primarily examine a variety of RL-based and RL-free\nmethods designed to address Reinforcement Learning from Human Feedback (RLHF)\nand Large Reasoning Models (LRMs). We begin with a concise overview of the\ntypical steps involved in RLHF and LRMs. Next, we reinterpret several RL-based\nand RL-free algorithms through the perspective of neural structured bandit\nprediction, providing a clear conceptual framework that uncovers a deeper\nconnection between these seemingly distinct approaches. Following this, we\nbriefly review some core principles of reinforcement learning, drawing\nattention to an often-overlooked aspect in existing RLHF studies. This leads to\na detailed derivation of the standard RLHF objective within a full RL context,\ndemonstrating its equivalence to neural structured bandit prediction. Finally,\nby reinvestigating the principles behind Proximal Policy Optimization (PPO), we\npinpoint areas needing adjustment, which culminates in the introduction of the\nGeneralized Reinforce Optimization (GRO) framework, seamlessly integrating\nRL-based and RL-free methods in RLHF. We look forward to the community's\nefforts to empirically validate GRO and invite constructive feedback."}
{"id": "2503.19576", "pdf": "https://arxiv.org/pdf/2503.19576", "abs": "https://arxiv.org/abs/2503.19576", "authors": ["Dhananjaya Jayasundara", "Sudarshan Rajagopalan", "Yasiru Ranasinghe", "Trac D. Tran", "Vishal M. Patel"], "title": "SINR: Sparsity Driven Compressed Implicit Neural Representations", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) are increasingly recognized as a\nversatile data modality for representing discretized signals, offering benefits\nsuch as infinite query resolution and reduced storage requirements. Existing\nsignal compression approaches for INRs typically employ one of two strategies:\n1. direct quantization with entropy coding of the trained INR; 2. deriving a\nlatent code on top of the INR through a learnable transformation. Thus, their\nperformance is heavily dependent on the quantization and entropy coding schemes\nemployed. In this paper, we introduce SINR, an innovative compression algorithm\nthat leverages the patterns in the vector spaces formed by weights of INRs. We\ncompress these vector spaces using a high-dimensional sparse code within a\ndictionary. Further analysis reveals that the atoms of the dictionary used to\ngenerate the sparse code do not need to be learned or transmitted to\nsuccessfully recover the INR weights. We demonstrate that the proposed approach\ncan be integrated with any existing INR-based signal compression technique. Our\nresults indicate that SINR achieves substantial reductions in storage\nrequirements for INRs across various configurations, outperforming conventional\nINR-based compression baselines. Furthermore, SINR maintains high-quality\ndecoding across diverse data modalities, including images, occupancy fields,\nand Neural Radiance Fields."}
{"id": "2503.19589", "pdf": "https://arxiv.org/pdf/2503.19589", "abs": "https://arxiv.org/abs/2503.19589", "authors": ["Shaolei Zhang", "Jinyan Liu", "Tianyi Qian", "Xuesong Li"], "title": "Prompt-Guided Dual-Path UNet with Mamba for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) and transformers are widely employed in\nconstructing UNet architectures for medical image segmentation tasks. However,\nCNNs struggle to model long-range dependencies, while transformers suffer from\nquadratic computational complexity. Recently, Mamba, a type of State Space\nModels, has gained attention for its exceptional ability to model long-range\ninteractions while maintaining linear computational complexity. Despite the\nemergence of several Mamba-based methods, they still present the following\nlimitations: first, their network designs generally lack perceptual\ncapabilities for the original input data; second, they primarily focus on\ncapturing global information, while often neglecting local details. To address\nthese challenges, we propose a prompt-guided CNN-Mamba dual-path UNet, termed\nPGM-UNet, for medical image segmentation. Specifically, we introduce a\nprompt-guided residual Mamba module that adaptively extracts dynamic visual\nprompts from the original input data, effectively guiding Mamba in capturing\nglobal information. Additionally, we design a local-global information fusion\nnetwork, comprising a local information extraction module, a prompt-guided\nresidual Mamba module, and a multi-focus attention fusion module, which\neffectively integrates local and global information. Furthermore, inspired by\nKolmogorov-Arnold Networks (KANs), we develop a multi-scale information\nextraction module to capture richer contextual information without altering the\nresolution. We conduct extensive experiments on the ISIC-2017, ISIC-2018, DIAS,\nand DRIVE. The results demonstrate that the proposed method significantly\noutperforms state-of-the-art approaches in multiple medical image segmentation\ntasks."}
{"id": "2503.19604", "pdf": "https://arxiv.org/pdf/2503.19604", "abs": "https://arxiv.org/abs/2503.19604", "authors": ["Ge Gao", "Siyue Teng", "Tianhao Peng", "Fan Zhang", "David Bull"], "title": "GIViC: Generative Implicit Video Compression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "While video compression based on implicit neural representations (INRs) has\nrecently demonstrated great potential, existing INR-based video codecs still\ncannot achieve state-of-the-art (SOTA) performance compared to their\nconventional or autoencoder-based counterparts given the same coding\nconfiguration. In this context, we propose a Generative Implicit Video\nCompression framework, GIViC, aiming at advancing the performance limits of\nthis type of coding methods. GIViC is inspired by the characteristics that INRs\nshare with large language and diffusion models in exploiting long-term\ndependencies. Through the newly designed implicit diffusion process, GIViC\nperforms diffusive sampling across coarse-to-fine spatiotemporal\ndecompositions, gradually progressing from coarser-grained full-sequence\ndiffusion to finer-grained per-token diffusion. A novel Hierarchical Gated\nLinear Attention-based transformer (HGLA), is also integrated into the\nframework, which dual-factorizes global dependency modeling along scale and\nsequential axes. The proposed GIViC model has been benchmarked against SOTA\nconventional and neural codecs using a Random Access (RA) configuration (YUV\n4:2:0, GOPSize=32), and yields BD-rate savings of 15.94%, 22.46% and 8.52% over\nVVC VTM, DCVC-FM and NVRC, respectively. As far as we are aware, GIViC is the\nfirst INR-based video codec that outperforms VTM based on the RA coding\nconfiguration. The source code will be made available."}
{"id": "2503.19606", "pdf": "https://arxiv.org/pdf/2503.19606", "abs": "https://arxiv.org/abs/2503.19606", "authors": ["Deepti Madurai Muthu", "Priyanka S", "Lalitha Rani N", "P. G. Kubendran Amos"], "title": "Single Shot AI-assisted quantification of KI-67 proliferation index in breast cancer", "categories": ["eess.IV", "cs.CV", "q-bio.QM", "q-bio.TO"], "comment": null, "summary": "Reliable quantification of Ki-67, a key proliferation marker in breast\ncancer, is essential for molecular subtyping and informed treatment planning.\nConventional approaches, including visual estimation and manual counting,\nsuffer from interobserver variability and limited reproducibility. This study\nintroduces an AI-assisted method using the YOLOv8 object detection framework\nfor automated Ki-67 scoring. High-resolution digital images (40x magnification)\nof immunohistochemically stained tumor sections were captured from Ki-67\nhotspot regions and manually annotated by a domain expert to distinguish\nKi-67-positive and negative tumor cells. The dataset was augmented and divided\ninto training (80%), validation (10%), and testing (10%) subsets. Among the\nYOLOv8 variants tested, the Medium model achieved the highest performance, with\na mean Average Precision at 50% Intersection over Union (mAP50) exceeding 85%\nfor Ki-67-positive cells. The proposed approach offers an efficient, scalable,\nand objective alternative to conventional scoring methods, supporting greater\nconsistency in Ki-67 evaluation. Future directions include developing\nuser-friendly clinical interfaces and expanding to multi-institutional datasets\nto enhance generalizability and facilitate broader adoption in diagnostic\npractice."}
{"id": "2503.19673", "pdf": "https://arxiv.org/pdf/2503.19673", "abs": "https://arxiv.org/abs/2503.19673", "authors": ["Federico Lincetto", "Gianluca Agresti", "Mattia Rossi", "Pietro Zanuttigh"], "title": "MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for Neural Rendering across Multiple Imaging Modalities", "categories": ["cs.GR", "cs.CV", "I.3.7; I.3.2; I.4.1"], "comment": "Accepted at CVPR 2025", "summary": "Neural Radiance Fields (NeRF) have shown impressive performances in the\nrendering of 3D scenes from arbitrary viewpoints. While RGB images are widely\npreferred for training volume rendering models, the interest in other radiance\nmodalities is also growing. However, the capability of the underlying implicit\nneural models to learn and transfer information across heterogeneous imaging\nmodalities has seldom been explored, mostly due to the limited training data\navailability. For this purpose, we present MultimodalStudio (MMS): it\nencompasses MMS-DATA and MMS-FW. MMS-DATA is a multimodal multi-view dataset\ncontaining 32 scenes acquired with 5 different imaging modalities: RGB,\nmonochrome, near-infrared, polarization and multispectral. MMS-FW is a novel\nmodular multimodal NeRF framework designed to handle multimodal raw data and\nable to support an arbitrary number of multi-channel devices. Through extensive\nexperiments, we demonstrate that MMS-FW trained on MMS-DATA can transfer\ninformation between different imaging modalities and produce higher quality\nrenderings than using single modalities alone. We publicly release the dataset\nand the framework, to promote the research on multimodal volume rendering and\nbeyond."}
{"id": "2503.19713", "pdf": "https://arxiv.org/pdf/2503.19713", "abs": "https://arxiv.org/abs/2503.19713", "authors": ["Yusen Xie", "Zhengmin Huang", "Shaojie Shen", "Jun Ma"], "title": "Semi-SD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In this paper, we introduce Semi-SD, a novel metric depth estimation\nframework tailored for surrounding cameras equipment in autonomous driving. In\nthis work, the input data consists of adjacent surrounding frames and camera\nparameters. We propose a unified spatial-temporal-semantic fusion module to\nconstruct the visual fused features. Cross-attention components for surrounding\ncameras and adjacent frames are utilized to focus on metric scale information\nrefinement and temporal feature matching. Building on this, we propose a pose\nestimation framework using surrounding cameras, their corresponding estimated\ndepths, and extrinsic parameters, which effectively address the scale ambiguity\nin multi-camera setups. Moreover, semantic world model and monocular depth\nestimation world model are integrated to supervised the depth estimation, which\nimprove the quality of depth estimation. We evaluate our algorithm on DDAD and\nnuScenes datasets, and the results demonstrate that our method achieves\nstate-of-the-art performance in terms of surrounding camera based depth\nestimation quality. The source code will be available on\nhttps://github.com/xieyuser/Semi-SD."}
{"id": "2503.19719", "pdf": "https://arxiv.org/pdf/2503.19719", "abs": "https://arxiv.org/abs/2503.19719", "authors": ["Francisco Mena", "Diego Arenas", "Miro Miranda", "Andreas Dengel"], "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2025", "summary": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications."}
{"id": "2503.19735", "pdf": "https://arxiv.org/pdf/2503.19735", "abs": "https://arxiv.org/abs/2503.19735", "authors": ["Zixue Zeng", "Matthew Cartier", "Xiaoyan Zhao", "Pengyu Chen", "Xin Meng", "Zhiyu Sheng", "Maryam Satarpour", "John M Cormack", "Allison C. Bean", "Ryan P. Nussbaum", "Maya Maurer", "Emily Landis-Walkenhorst", "Kang Kim", "Ajay D. Wasan", "Jiantao Pu"], "title": "InterSliceBoost: Identifying Tissue Layers in Three-dimensional Ultrasound Images for Chronic Lower Back Pain (cLBP) Assessment", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Available studies on chronic lower back pain (cLBP) typically focus on one or\na few specific tissues rather than conducting a comprehensive layer-by-layer\nanalysis. Since three-dimensional (3-D) images often contain hundreds of\nslices, manual annotation of these anatomical structures is both time-consuming\nand error-prone. We aim to develop and validate a novel approach called\nInterSliceBoost to enable the training of a segmentation model on a partially\nannotated dataset without compromising segmentation performance. The\narchitecture of InterSliceBoost includes two components: an inter-slice\ngenerator and a segmentation model. The generator utilizes residual block-based\nencoders to extract features from adjacent image-mask pairs (IMPs).\nDifferential features are calculated and input into a decoder to generate\ninter-slice IMPs. The segmentation model is trained on partially annotated\ndatasets (e.g., skipping 1, 2, 3, or 7 images) and the generated inter-slice\nIMPs. To validate the performance of InterSliceBoost, we utilized a dataset of\n76 B-mode ultrasound scans acquired on 29 subjects enrolled in an ongoing cLBP\nstudy. InterSliceBoost, trained on only 33% of the image slices, achieved a\nmean Dice coefficient of 80.84% across all six layers on the independent test\nset, with Dice coefficients of 73.48%, 61.11%, 81.87%, 95.74%, 83.52% and\n88.74% for segmenting dermis, superficial fat, superficial fascial membrane,\ndeep fat, deep fascial membrane, and muscle. This performance is significantly\nhigher than the conventional model trained on fully annotated images (p<0.05).\nInterSliceBoost can effectively segment the six tissue layers depicted on 3-D\nB-model ultrasound images in settings with partial annotations."}
{"id": "2503.19736", "pdf": "https://arxiv.org/pdf/2503.19736", "abs": "https://arxiv.org/abs/2503.19736", "authors": ["Zixue Zeng", "Xiaoyan Zhao", "Matthew Cartier", "Xin Meng", "Jiantao Pu"], "title": "GRN+: A Simplified Generative Reinforcement Network for Tissue Layer Analysis in 3D Ultrasound Images for Chronic Low-back Pain", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "3D ultrasound delivers high-resolution, real-time images of soft tissues,\nwhich is essential for pain research. However, manually distinguishing various\ntissues for quantitative analysis is labor-intensive. To streamline this\nprocess, we developed and validated GRN+, a novel multi-model framework that\nautomates layer segmentation with minimal annotated data. GRN+ combines a\nResNet-based generator and a U-Net segmentation model. Through a method called\nSegmentation-guided Enhancement (SGE), the generator produces new images and\nmatching masks under the guidance of the segmentation model, with its weights\nadjusted according to the segmentation loss gradient. To prevent gradient\nexplosion and secure stable training, a two-stage backpropagation strategy was\nimplemented: the first stage propagates the segmentation loss through both the\ngenerator and segmentation model, while the second stage concentrates on\noptimizing the segmentation model alone, thereby refining mask prediction using\nthe generated images. Tested on 69 fully annotated 3D ultrasound scans from 29\nsubjects with six manually labeled tissue layers, GRN+ outperformed all other\nsemi-supervised methods in terms of the Dice coefficient using only 5% labeled\ndata, despite not using unlabeled data for unsupervised training. Additionally,\nwhen applied to fully annotated datasets, GRN+ with SGE achieved a 2.16% higher\nDice coefficient while incurring lower computational costs compared to other\nmodels. Overall, GRN+ provides accurate tissue segmentation while reducing both\ncomputational expenses and the dependency on extensive annotations, making it\nan effective tool for 3D ultrasound analysis in cLBP patients."}
{"id": "2503.19753", "pdf": "https://arxiv.org/pdf/2503.19753", "abs": "https://arxiv.org/abs/2503.19753", "authors": ["Chuanzhi Xu", "Haoxian Zhou", "Haodong Chen", "Vera Chung", "Qiang Qu"], "title": "A Survey on Event-driven 3D Reconstruction: Development under Different Categories", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "6 pages, 1 figure, 6 tables", "summary": "Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc."}
{"id": "2503.19757", "pdf": "https://arxiv.org/pdf/2503.19757", "abs": "https://arxiv.org/abs/2503.19757", "authors": ["Zhi Hou", "Tianyi Zhang", "Yuwen Xiong", "Haonan Duan", "Hengjun Pu", "Ronglei Tong", "Chengyang Zhao", "Xizhou Zhu", "Yu Qiao", "Jifeng Dai", "Yuntao Chen"], "title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy", "categories": ["cs.RO", "cs.CV"], "comment": "Preprint; https://robodita.github.io;", "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io."}
{"id": "2503.19819", "pdf": "https://arxiv.org/pdf/2503.19819", "abs": "https://arxiv.org/abs/2503.19819", "authors": ["Pratibha Kumari", "Afshin Bozorgpour", "Daniel Reisenbüchler", "Edgar Jost", "Martina Crysandt", "Christian Matek", "Dorit Merhof"], "title": "Domain-incremental White Blood Cell Classification with Privacy-aware Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "White blood cell (WBC) classification plays a vital role in hematology for\ndiagnosing various medical conditions. However, it faces significant challenges\ndue to domain shifts caused by variations in sample sources (e.g., blood or\nbone marrow) and differing imaging conditions across hospitals. Traditional\ndeep learning models often suffer from catastrophic forgetting in such dynamic\nenvironments, while foundation models, though generally robust, experience\nperformance degradation when the distribution of inference data differs from\nthat of the training data. To address these challenges, we propose a generative\nreplay-based Continual Learning (CL) strategy designed to prevent forgetting in\nfoundation models for WBC classification. Our method employs lightweight\ngenerators to mimic past data with a synthetic latent representation to enable\nprivacy-preserving replay. To showcase the effectiveness, we carry out\nextensive experiments with a total of four datasets with different task\nordering and four backbone models including ResNet50, RetCCL, CTransPath, and\nUNI. Experimental results demonstrate that conventional fine-tuning methods\ndegrade performance on previously learned tasks and struggle with domain\nshifts. In contrast, our continual learning strategy effectively mitigates\ncatastrophic forgetting, preserving model performance across varying domains.\nThis work presents a practical solution for maintaining reliable WBC\nclassification in real-world clinical settings, where data distributions\nfrequently evolve."}
{"id": "2503.19823", "pdf": "https://arxiv.org/pdf/2503.19823", "abs": "https://arxiv.org/abs/2503.19823", "authors": ["Yan Zhuang", "Minheng Chen", "Chao Cao", "Tong Chen", "Jing Zhang", "Xiaowei Yu", "Yanjun Lyu", "Lu Zhang", "Tianming Liu", "Dajiang Zhu"], "title": "GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": "10 pages, 3 figures", "summary": "Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity."}
{"id": "2503.19824", "pdf": "https://arxiv.org/pdf/2503.19824", "abs": "https://arxiv.org/abs/2503.19824", "authors": ["Jiazhi Guan", "Kaisiyuan Wang", "Zhiliang Xu", "Quanwei Yang", "Yasheng Sun", "Shengyi He", "Borong Liang", "Yukang Cao", "Yingying Li", "Haocheng Feng", "Errui Ding", "Jingdong Wang", "Youjian Zhao", "Hang Zhou", "Ziwei Liu"], "title": "AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2025. Project page:\n  https://guanjz20.github.io/projects/AudCast", "summary": "Despite the recent progress of audio-driven video generation, existing\nmethods mostly focus on driving facial movements, leading to non-coherent head\nand body dynamics. Moving forward, it is desirable yet challenging to generate\nholistic human videos with both accurate lip-sync and delicate co-speech\ngestures w.r.t. given audio. In this work, we propose AudCast, a generalized\naudio-driven human video generation framework adopting a cascade\nDiffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos\nbased on a reference image and a given audio. 1) Firstly, an audio-conditioned\nHolistic Human DiT architecture is proposed to directly drive the movements of\nany human body with vivid gesture dynamics. 2) Then to enhance hand and face\ndetails that are well-knownly difficult to handle, a Regional Refinement DiT\nleverages regional 3D fitting as the bridge to reform the signals, producing\nthe final results. Extensive experiments demonstrate that our framework\ngenerates high-fidelity audio-driven holistic human videos with temporal\ncoherence and fine facial and hand details. Resources can be found at\nhttps://guanjz20.github.io/projects/AudCast."}
{"id": "2503.19860", "pdf": "https://arxiv.org/pdf/2503.19860", "abs": "https://arxiv.org/abs/2503.19860", "authors": ["Junzhi Ning", "Dominic Marshall", "Yijian Gao", "Xiaodan Xing Yang Nan", "Yingying Fang", "Sheng Zhang", "Matthieu Komorowski", "Guang Yang"], "title": "Unpaired Translation of Chest X-ray Images for Lung Opacity Diagnosis via Adaptive Activation Masks and Cross-Domain Alignment", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Chest X-ray radiographs (CXRs) play a pivotal role in diagnosing and\nmonitoring cardiopulmonary diseases. However, lung opac- ities in CXRs\nfrequently obscure anatomical structures, impeding clear identification of lung\nborders and complicating the localization of pathology. This challenge\nsignificantly hampers segmentation accuracy and precise lesion identification,\nwhich are crucial for diagnosis. To tackle these issues, our study proposes an\nunpaired CXR translation framework that converts CXRs with lung opacities into\ncounterparts without lung opacities while preserving semantic features. Central\nto our approach is the use of adaptive activation masks to selectively modify\nopacity regions in lung CXRs. Cross-domain alignment ensures translated CXRs\nwithout opacity issues align with feature maps and prediction labels from a\npre-trained CXR lesion classifier, facilitating the interpretability of the\ntranslation process. We validate our method using RSNA, MIMIC-CXR-JPG and JSRT\ndatasets, demonstrating superior translation quality through lower Frechet\nInception Distance (FID) and Kernel Inception Distance (KID) scores compared to\nexisting meth- ods (FID: 67.18 vs. 210.4, KID: 0.01604 vs. 0.225). Evaluation\non RSNA opacity, MIMIC acute respiratory distress syndrome (ARDS) patient CXRs\nand JSRT CXRs show our method enhances segmentation accuracy of lung borders\nand improves lesion classification, further underscoring its potential in\nclinical settings (RSNA: mIoU: 76.58% vs. 62.58%, Sensitivity: 85.58% vs.\n77.03%; MIMIC ARDS: mIoU: 86.20% vs. 72.07%, Sensitivity: 92.68% vs. 86.85%;\nJSRT: mIoU: 91.08% vs. 85.6%, Sensitivity: 97.62% vs. 95.04%). Our approach\nadvances CXR imaging analysis, especially in investigating segmentation impacts\nthrough image translation techniques."}
{"id": "2503.19868", "pdf": "https://arxiv.org/pdf/2503.19868", "abs": "https://arxiv.org/abs/2503.19868", "authors": ["Sungyeon Kim", "Xinliang Zhu", "Xiaofan Lin", "Muhammet Bastan", "Douglas Gray", "Suha Kwak"], "title": "GENIUS: A Generative Framework for Universal Multimodal Search", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency."}
{"id": "2503.19893", "pdf": "https://arxiv.org/pdf/2503.19893", "abs": "https://arxiv.org/abs/2503.19893", "authors": ["Lukas Mack", "Felix Grüninger", "Benjamin A. Richardson", "Regine Lendway", "Katherine J. Kuchenbecker", "Joerg Stueckler"], "title": "Visuo-Tactile Object Pose Estimation for a Multi-Finger Robot Hand with Low-Resolution In-Hand Tactile Sensing", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted for publication at the IEEE International Conference on\n  Robotics and Automation (ICRA), 2025", "summary": "Accurate 3D pose estimation of grasped objects is an important prerequisite\nfor robots to perform assembly or in-hand manipulation tasks, but object\nocclusion by the robot's own hand greatly increases the difficulty of this\nperceptual task. Here, we propose that combining visual information and\nproprioception with binary, low-resolution tactile contact measurements from\nacross the interior surface of an articulated robotic hand can mitigate this\nissue. The visuo-tactile object-pose-estimation problem is formulated\nprobabilistically in a factor graph. The pose of the object is optimized to\nalign with the three kinds of measurements using a robust cost function to\nreduce the influence of visual or tactile outlier readings. The advantages of\nthe proposed approach are first demonstrated in simulation: a custom 15-DoF\nrobot hand with one binary tactile sensor per link grasps 17 YCB objects while\nobserved by an RGB-D camera. This low-resolution in-hand tactile sensing\nsignificantly improves object-pose estimates under high occlusion and also high\nvisual noise. We also show these benefits through grasping tests with a\npreliminary real version of our tactile hand, obtaining reasonable\nvisuo-tactile estimates of object pose at approximately 13.3 Hz on average."}
