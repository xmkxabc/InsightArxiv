{"id": "2503.16511", "pdf": "https://arxiv.org/pdf/2503.16511", "abs": "https://arxiv.org/abs/2503.16511", "authors": ["Tingkai Liu", "Ari S. Benjamin", "Anthony M. Zador"], "title": "Token-Level Uncertainty-Aware Objective for Language Model Post-Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the current work, we connect token-level uncertainty in causal language\nmodeling to two types of training objectives: 1) masked maximum likelihood\n(MLE), 2) self-distillation. We show that masked MLE is effective in reducing\nepistemic uncertainty, and serve as an effective token-level automatic\ncurriculum learning technique. However, masked MLE is prone to overfitting and\nrequires self-distillation regularization to improve or maintain performance on\nout-of-distribution tasks. We demonstrate significant performance gain via the\nproposed training objective - combined masked MLE and self-distillation -\nacross multiple architectures (Gemma, LLaMA, Phi) and datasets (Alpaca,\nShareGPT, GSM8K), mitigating overfitting while maintaining adaptability during\npost-training. Our findings suggest that uncertainty-aware training provides an\neffective mechanism for enhancing language model training."}
{"id": "2503.16513", "pdf": "https://arxiv.org/pdf/2503.16513", "abs": "https://arxiv.org/abs/2503.16513", "authors": ["Nadia Saeed"], "title": "Medifact at PerAnsSumm 2025: Leveraging Lightweight Models for Perspective-Specific Summarization of Clinical Q&A Forums", "categories": ["cs.CL", "cs.AI"], "comment": "This paper accepted in PerAnsSumm: Perspective-aware Healthcare\n  answer summarization, a shared task organized at the CL4Health workshop\n  colocated with NAACL 2025", "summary": "The PerAnsSumm 2025 challenge focuses on perspective-aware healthcare answer\nsummarization (Agarwal et al., 2025). This work proposes a few-shot learning\nframework using a Snorkel-BART-SVM pipeline for classifying and summarizing\nopen-ended healthcare community question-answering (CQA). An SVM model is\ntrained with weak supervision via Snorkel, enhancing zero-shot learning.\nExtractive classification identifies perspective-relevant sentences, which are\nthen summarized using a pretrained BART-CNN model. The approach achieved 12th\nplace among 100 teams in the shared task, demonstrating computational\nefficiency and contextual accuracy. By leveraging pretrained summarization\nmodels, this work advances medical CQA research and contributes to clinical\ndecision support systems."}
{"id": "2503.16515", "pdf": "https://arxiv.org/pdf/2503.16515", "abs": "https://arxiv.org/abs/2503.16515", "authors": ["Lachlan McGinness", "Peter Baumgartner"], "title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) were used to assist four Commonwealth Scientific\nand Industrial Research Organisation (CSIRO) researchers to perform systematic\nliterature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in\nthese case studies. In each, we explore the impact of changing parameters on\nthe accuracy of LLM responses. The LLM was tasked with extracting evidence from\nchosen academic papers to answer specific research questions. We evaluate the\nmodels' performance in faithfully reproducing quotes from the literature and\nsubject experts were asked to assess the model performance in answering the\nresearch questions. We developed a semantic text highlighting tool to\nfacilitate expert review of LLM responses.\n  We found that state of the art LLMs were able to reproduce quotes from texts\nwith greater than 95% accuracy and answer research questions with an accuracy\nof approximately 83%. We use two methods to determine the correctness of LLM\nresponses; expert review and the cosine similarity of transformer embeddings of\nLLM and expert answers. The correlation between these methods ranged from 0.48\nto 0.77, providing evidence that the latter is a valid metric for measuring\nsemantic similarity."}
{"id": "2503.16516", "pdf": "https://arxiv.org/pdf/2503.16516", "abs": "https://arxiv.org/abs/2503.16516", "authors": ["Yuxin Chen", "Peng Tang", "Weidong Qiu", "Shujun Li"], "title": "Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Privacy policies are widely used by digital services and often required for\nlegal purposes. Many machine learning based classifiers have been developed to\nautomate detection of different concepts in a given privacy policy, which can\nhelp facilitate other automated tasks such as producing a more reader-friendly\nsummary and detecting legal compliance issues. Despite the successful\napplications of large language models (LLMs) to many NLP tasks in various\ndomains, there is very little work studying the use of LLMs for automated\nprivacy policy analysis, therefore, if and how LLMs can help automate privacy\npolicy analysis remains under-explored. To fill this research gap, we conducted\na comprehensive evaluation of LLM-based privacy policy concept classifiers,\nemploying both prompt engineering and LoRA (low-rank adaptation) fine-tuning,\non four state-of-the-art (SOTA) privacy policy corpora and taxonomies. Our\nexperimental results demonstrated that combining prompt engineering and\nfine-tuning can make LLM-based classifiers outperform other SOTA methods,\n\\emph{significantly} and \\emph{consistently} across privacy policy\ncorpora/taxonomies and concepts. Furthermore, we evaluated the explainability\nof the LLM-based classifiers using three metrics: completeness, logicality, and\ncomprehensibility. For all three metrics, a score exceeding 91.1\\% was observed\nin our evaluation, indicating that LLMs are not only useful to improve the\nclassification performance, but also to enhance the explainability of detection\nresults."}
{"id": "2503.16522", "pdf": "https://arxiv.org/pdf/2503.16522", "abs": "https://arxiv.org/abs/2503.16522", "authors": ["Yongjia Ma", "Donglin Di", "Xuan Liu", "Xiaokai Chen", "Lei Fan", "Wei Chen", "Tonghua Su"], "title": "Adams Bashforth Moulton Solver for Inversion and Editing in Rectified Flow", "categories": ["cs.CV"], "comment": null, "summary": "Rectified flow models have achieved remarkable performance in image and video\ngeneration tasks. However, existing numerical solvers face a trade-off between\nfast sampling and high-accuracy solutions, limiting their effectiveness in\ndownstream applications such as reconstruction and editing. To address this\nchallenge, we propose leveraging the Adams-Bashforth-Moulton (ABM)\npredictor-corrector method to enhance the accuracy of ODE solving in rectified\nflow models. Specifically, we introduce ABM-Solver, which integrates a multi\nstep predictor corrector approach to reduce local truncation errors and employs\nAdaptive Step Size Adjustment to improve sampling speed. Furthermore, to\neffectively preserve non edited regions while facilitating semantic\nmodifications, we introduce a Mask Guided Feature Injection module. We estimate\nself-similarity to generate a spatial mask that differentiates preserved\nregions from those available for editing. Extensive experiments on multiple\nhigh-resolution image datasets validate that ABM-Solver significantly improves\ninversion precision and editing quality, outperforming existing solvers without\nrequiring additional training or optimization."}
{"id": "2503.16520", "pdf": "https://arxiv.org/pdf/2503.16520", "abs": "https://arxiv.org/abs/2503.16520", "authors": ["Ji-Eun Han", "Yoonseok Heo"], "title": "Not All Personas Are Worth It: Culture-Reflective Persona Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Incorporating personas into conversational AI models is crucial for achieving\nauthentic and engaging interactions. However, the cultural diversity and\nadaptability of existing persona datasets is often overlooked, reducing their\nefficacy in building culturally aware AI systems. To address this issue, we\npropose a two-step pipeline for generating culture-specific personas and\nintroduce KoPersona, a dataset comprising 200,000 personas designed to capture\nKorean cultural values, behaviors, and social nuances. A comprehensive\nevaluation through various metrics validates the quality of KoPersona and its\nrelevance to Korean culture. This work not only contributes to persona-based\nresearch, but also establishes a scalable approach for creating culturally\nrelevant personas adaptable to various languages and cultural contexts."}
{"id": "2503.16535", "pdf": "https://arxiv.org/pdf/2503.16535", "abs": "https://arxiv.org/abs/2503.16535", "authors": ["Jinchang Zhang", "Guoyu Lu"], "title": "Vision-Language Embodiment for Monocular Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Depth estimation is a core problem in robotic perception and vision tasks,\nbut 3D reconstruction from a single image presents inherent uncertainties.\nCurrent depth estimation models primarily rely on inter-image relationships for\nsupervised training, often overlooking the intrinsic information provided by\nthe camera itself. We propose a method that embodies the camera model and its\nphysical characteristics into a deep learning model, computing embodied scene\ndepth through real-time interactions with road environments. The model can\ncalculate embodied scene depth in real-time based on immediate environmental\nchanges using only the intrinsic properties of the camera, without any\nadditional equipment. By combining embodied scene depth with RGB image\nfeatures, the model gains a comprehensive perspective on both geometric and\nvisual details. Additionally, we incorporate text descriptions containing\nenvironmental content and depth information as priors for scene understanding,\nenriching the model's perception of objects. This integration of image and\nlanguage - two inherently ambiguous modalities - leverages their complementary\nstrengths for monocular depth estimation. The real-time nature of the embodied\nlanguage and depth prior model ensures that the model can continuously adjust\nits perception and behavior in dynamic environments. Experimental results show\nthat the embodied depth estimation method enhances model performance across\ndifferent scenes."}
{"id": "2503.16523", "pdf": "https://arxiv.org/pdf/2503.16523", "abs": "https://arxiv.org/abs/2503.16523", "authors": ["Shi Yin Hong", "Uttamasha Oyshi", "Quan Mai", "Gibson Nkhata", "Susan Gauch"], "title": "Mind2: Mind-to-Mind Emotional Support System with Bidirectional Cognitive Discourse Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 2 figures, and 3 tables; WI-IAT 2024", "summary": "Emotional support (ES) systems alleviate users' mental distress by generating\nstrategic supportive dialogues based on diverse user situations. However, ES\nsystems are limited in their ability to generate effective ES dialogues that\ninclude timely context and interpretability, hindering them from earning public\ntrust. Driven by cognitive models, we propose Mind-to-Mind (Mind2), an ES\nframework that approaches interpretable ES context modeling for the ES dialogue\ngeneration task from a discourse analysis perspective. Specifically, we perform\ncognitive discourse analysis on ES dialogues according to our dynamic discourse\ncontext propagation window, which accommodates evolving context as the\nconversation between the ES system and user progresses. To enhance\ninterpretability, Mind2 prioritizes details that reflect each speaker's belief\nabout the other speaker with bidirectionality, integrating Theory-of-Mind,\nphysiological expected utility, and cognitive rationality to extract cognitive\nknowledge from ES conversations. Experimental results support that Mind2\nachieves competitive performance versus state-of-the-art ES systems while\ntrained with only 10\\% of the available training data."}
{"id": "2503.16538", "pdf": "https://arxiv.org/pdf/2503.16538", "abs": "https://arxiv.org/abs/2503.16538", "authors": ["Bastian Pätzold", "Jan Nogga", "Sven Behnke"], "title": "Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "This paper introduces a novel approach that leverages the capabilities of\nvision-language models (VLMs) by integrating them with established approaches\nfor open-vocabulary detection (OVD), instance segmentation, and tracking. We\nutilize VLM-generated structured descriptions to identify visible object\ninstances, collect application-relevant attributes, and inform an\nopen-vocabulary detector to extract corresponding bounding boxes that are\npassed to a video segmentation model providing precise segmentation masks and\ntracking capabilities. Once initialized, this model can then directly extract\nsegmentation masks, allowing processing of image streams in real time with\nminimal computational overhead. Tracks can be updated online as needed by\ngenerating new structured descriptions and corresponding open-vocabulary\ndetections. This combines the descriptive power of VLMs with the grounding\ncapability of OVD and the pixel-level understanding and speed of video\nsegmentation. Our evaluation across datasets and robotics platforms\ndemonstrates the broad applicability of this approach, showcasing its ability\nto extract task-specific attributes from non-standard objects in dynamic\nenvironments."}
{"id": "2503.16525", "pdf": "https://arxiv.org/pdf/2503.16525", "abs": "https://arxiv.org/abs/2503.16525", "authors": ["Huan Yang", "Renji Zhang", "Deyu Zhang"], "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large Language Model Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education."}
{"id": "2503.16542", "pdf": "https://arxiv.org/pdf/2503.16542", "abs": "https://arxiv.org/abs/2503.16542", "authors": ["Shiyi Jiang", "Farshad Firouzi", "Krishnendu Chakrabarty"], "title": "Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data."}
{"id": "2503.16527", "pdf": "https://arxiv.org/pdf/2503.16527", "abs": "https://arxiv.org/abs/2503.16527", "authors": ["Ang Li", "Haozhe Chen", "Hongseok Namkoong", "Tianyi Peng"], "title": "LLM Generated Persona is a Promise with a Catch", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "The use of large language models (LLMs) to simulate human behavior has gained\nsignificant attention, particularly through personas that approximate\nindividual characteristics. Persona-based simulations hold promise for\ntransforming disciplines that rely on population-level feedback, including\nsocial science, economic analysis, marketing research, and business operations.\nTraditional methods to collect realistic persona data face significant\nchallenges. They are prohibitively expensive and logistically challenging due\nto privacy constraints, and often fail to capture multi-dimensional attributes,\nparticularly subjective qualities. Consequently, synthetic persona generation\nwith LLMs offers a scalable, cost-effective alternative. However, current\napproaches rely on ad hoc and heuristic generation techniques that do not\nguarantee methodological rigor or simulation precision, resulting in systematic\nbiases in downstream tasks. Through extensive large-scale experiments including\npresidential election forecasts and general opinion surveys of the U.S.\npopulation, we reveal that these biases can lead to significant deviations from\nreal-world outcomes. Our findings underscore the need to develop a rigorous\nscience of persona generation and outline the methodological innovations,\norganizational and institutional support, and empirical foundations required to\nenhance the reliability and scalability of LLM-driven persona simulations. To\nsupport further research and development in this area, we have open-sourced\napproximately one million generated personas, available for public access and\nanalysis at https://huggingface.co/datasets/Tianyi-Lab/Personas."}
{"id": "2503.16546", "pdf": "https://arxiv.org/pdf/2503.16546", "abs": "https://arxiv.org/abs/2503.16546", "authors": ["Saddam Hussain Khan", "Rashid Iqbal"], "title": "A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "100 Pages, 44 Figures", "summary": "Deep Convolutional Neural Networks (CNNs) have significantly advanced deep\nlearning, driving breakthroughs in computer vision, natural language\nprocessing, medical diagnosis, object detection, and speech recognition.\nArchitectural innovations including 1D, 2D, and 3D convolutional models,\ndilated and grouped convolutions, depthwise separable convolutions, and\nattention mechanisms address domain-specific challenges and enhance feature\nrepresentation and computational efficiency. Structural refinements such as\nspatial-channel exploitation, multi-path design, and feature-map enhancement\ncontribute to robust hierarchical feature extraction and improved\ngeneralization, particularly through transfer learning. Efficient preprocessing\nstrategies, including Fourier transforms, structured transforms, low-precision\ncomputation, and weight compression, optimize inference speed and facilitate\ndeployment in resource-constrained environments. This survey presents a unified\ntaxonomy that classifies CNN architectures based on spatial exploitation,\nmulti-path structures, depth, width, dimensionality expansion, channel\nboosting, and attention mechanisms. It systematically reviews CNN applications\nin face recognition, pose estimation, action recognition, text classification,\nstatistical language modeling, disease diagnosis, radiological analysis,\ncryptocurrency sentiment prediction, 1D data processing, video analysis, and\nspeech recognition. In addition to consolidating architectural advancements,\nthe review highlights emerging learning paradigms such as few-shot, zero-shot,\nweakly supervised, federated learning frameworks and future research directions\ninclude hybrid CNN-transformer models, vision-language integration, generative\nlearning, etc. This review provides a comprehensive perspective on CNN's\nevolution from 2015 to 2025, outlining key innovations, challenges, and\nopportunities."}
{"id": "2503.16528", "pdf": "https://arxiv.org/pdf/2503.16528", "abs": "https://arxiv.org/abs/2503.16528", "authors": ["Heng Ping", "Shixuan Li", "Peiyu Zhang", "Anzhe Cheng", "Shukai Duan", "Nikos Kanakaris", "Xiongye Xiao", "Wei Yang", "Shahin Nazarian", "Andrei Irimia", "Paul Bogdan"], "title": "HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in code generation tasks. However, when applied to hardware\ndescription languages (HDL), these models exhibit significant limitations due\nto data scarcity, resulting in hallucinations and incorrect code generation. To\naddress these challenges, we propose HDLCoRe, a training-free framework that\nenhances LLMs' HDL generation capabilities through prompt engineering\ntechniques and retrieval-augmented generation (RAG). Our approach consists of\ntwo main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting\ntechnique with self-verification that classifies tasks by complexity and type,\nincorporates domain-specific knowledge, and guides LLMs through step-by-step\nself-simulation for error correction; and (2) a two-stage heterogeneous RAG\nsystem that addresses formatting inconsistencies through key component\nextraction and efficiently retrieves relevant HDL examples through sequential\nfiltering and re-ranking. HDLCoRe eliminates the need for model fine-tuning\nwhile substantially improving LLMs' HDL generation capabilities. Experimental\nresults demonstrate that our framework achieves superior performance on the\nRTLLM2.0 benchmark, significantly reducing hallucinations and improving both\nsyntactic and functional correctness."}
{"id": "2503.16549", "pdf": "https://arxiv.org/pdf/2503.16549", "abs": "https://arxiv.org/abs/2503.16549", "authors": ["Felix Chen", "Hangjie Yuan", "Yunqiu Xu", "Tao Feng", "Jun Cen", "Pengwei Liu", "Zeying Huang", "Yi Yang"], "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems", "categories": ["cs.CV"], "comment": "https://github.com/MathFlow-zju/MathFlow", "summary": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow."}
{"id": "2503.16529", "pdf": "https://arxiv.org/pdf/2503.16529", "abs": "https://arxiv.org/abs/2503.16529", "authors": ["Wenjing Zhang", "Xuejiao Lei", "Zhaoxiang Liu", "Limin Han", "Jiaojiao Zhao", "Beibei Huang", "Zhenhong Long", "Junting Guo", "Meijuan An", "Rongjia Du", "Ning Wang", "Kai Wang", "Shiguo Lian"], "title": "Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "21 pages,13 figures", "summary": "DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for six distilled models. Evaluation results indicate that\nthe enhanced models achieve significant improvements in safety while\nmaintaining reasoning capabilities without notable degradation. We open-source\nthe safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main to serve as a\nvaluable resource for future research and optimization of DeepSeek models."}
{"id": "2503.16566", "pdf": "https://arxiv.org/pdf/2503.16566", "abs": "https://arxiv.org/abs/2503.16566", "authors": ["Jie Zhang", "Zheng Yuan", "Zhongqi Wang", "Bei Yan", "Sibo Wang", "Xiangkui Cao", "Zonghui Guo", "Shiguang Shan", "Xilin Chen"], "title": "REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models", "categories": ["cs.CV"], "comment": "45 pages, 5 figures, 18 tables", "summary": "The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted\nthe necessity for comprehensive evaluation frameworks that assess these models\nacross diverse dimensions. While existing benchmarks focus on specific aspects\nsuch as perceptual abilities, cognitive capabilities, and safety against\nadversarial attacks, they often lack the breadth and depth required to provide\na holistic understanding of LVLMs' strengths and limitations. To address this\ngap, we introduce REVAL, a comprehensive benchmark designed to evaluate the\n\\textbf{RE}liability and \\textbf{VAL}ue of LVLMs. REVAL encompasses over 144K\nimage-text Visual Question Answering (VQA) samples, structured into two primary\nsections: Reliability, which assesses truthfulness (\\eg, perceptual accuracy\nand hallucination tendencies) and robustness (\\eg, resilience to adversarial\nattacks, typographic attacks, and image corruption), and Values, which\nevaluates ethical concerns (\\eg, bias and moral understanding), safety issues\n(\\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\\eg,\nprivacy awareness and privacy leakage). We evaluate 26 models, including\nmainstream open-source LVLMs and prominent closed-source models like GPT-4o and\nGemini-1.5-Pro. Our findings reveal that while current LVLMs excel in\nperceptual tasks and toxicity avoidance, they exhibit significant\nvulnerabilities in adversarial scenarios, privacy preservation, and ethical\nreasoning. These insights underscore critical areas for future improvements,\nguiding the development of more secure, reliable, and ethically aligned LVLMs.\nREVAL provides a robust framework for researchers to systematically assess and\ncompare LVLMs, fostering advancements in the field."}
{"id": "2503.16530", "pdf": "https://arxiv.org/pdf/2503.16530", "abs": "https://arxiv.org/abs/2503.16530", "authors": ["Chengfeng Dou", "Ying Zhang", "Zhi Jin", "Wenpin Jiao", "Haiyan Zhao", "Yongqiang Zhao", "Zhengwei Tao"], "title": "Enhancing LLM Generation with Knowledge Hypergraph for Evidence-Based Medicine", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Evidence-based medicine (EBM) plays a crucial role in the application of\nlarge language models (LLMs) in healthcare, as it provides reliable support for\nmedical decision-making processes. Although it benefits from current\nretrieval-augmented generation~(RAG) technologies, it still faces two\nsignificant challenges: the collection of dispersed evidence and the efficient\norganization of this evidence to support the complex queries necessary for EBM.\nTo tackle these issues, we propose using LLMs to gather scattered evidence from\nmultiple sources and present a knowledge hypergraph-based evidence management\nmodel to integrate these evidence while capturing intricate relationships.\nFurthermore, to better support complex queries, we have developed an\nImportance-Driven Evidence Prioritization (IDEP) algorithm that utilizes the\nLLM to generate multiple evidence features, each with an associated importance\nscore, which are then used to rank the evidence and produce the final retrieval\nresults. Experimental results from six datasets demonstrate that our approach\noutperforms existing RAG techniques in application domains of interest to EBM,\nsuch as medical quizzing, hallucination detection, and decision support.\nTestsets and the constructed knowledge graph can be accessed at\n\\href{https://drive.google.com/file/d/1WJ9QTokK3MdkjEmwuFQxwH96j_Byawj_/view?usp=drive_link}{https://drive.google.com/rag4ebm}."}
{"id": "2503.16579", "pdf": "https://arxiv.org/pdf/2503.16579", "abs": "https://arxiv.org/abs/2503.16579", "authors": ["Jonas Krumme", "Christoph Zetzsche"], "title": "World Knowledge from AI Image Generation for Robot Control", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages, 10 figures", "summary": "When interacting with the world robots face a number of difficult questions,\nhaving to make decisions when given under-specified tasks where they need to\nmake choices, often without clearly defined right and wrong answers. Humans, on\nthe other hand, can often rely on their knowledge and experience to fill in the\ngaps. For example, the simple task of organizing newly bought produce into the\nfridge involves deciding where to put each thing individually, how to arrange\nthem together meaningfully, e.g. putting related things together, all while\nthere is no clear right and wrong way to accomplish this task. We could encode\nall this information on how to do such things explicitly into the robots'\nknowledge base, but this can quickly become overwhelming, considering the\nnumber of potential tasks and circumstances the robot could encounter. However,\nimages of the real world often implicitly encode answers to such questions and\ncan show which configurations of objects are meaningful or are usually used by\nhumans. An image of a full fridge can give a lot of information about how\nthings are usually arranged in relation to each other and the full fridge at\nlarge. Modern generative systems are capable of generating plausible images of\nthe real world and can be conditioned on the environment in which the robot\noperates. Here we investigate the idea of using the implicit knowledge about\nthe world of modern generative AI systems given by their ability to generate\nconvincing images of the real world to solve under-specified tasks."}
{"id": "2503.16531", "pdf": "https://arxiv.org/pdf/2503.16531", "abs": "https://arxiv.org/abs/2503.16531", "authors": ["Tidiane Camaret N'dir", "Robin Tibor Schirrmeister"], "title": "EEG-CLIP : Learning EEG representations from natural language descriptions", "categories": ["cs.CL", "cs.LG", "eess.SP"], "comment": null, "summary": "Deep networks for electroencephalogram (EEG) decoding are currently often\ntrained to only solve a specific task like pathology or gender decoding. A more\ngeneral approach leveraging the medical reports of clinical EEG recordings is\nto learn mappings between medical reports and EEG recordings. This approach was\npioneered in the computer vision domain matching images and their text captions\nand subsequently allowed to do successful zero-shot decoding using textual\nclass prompts. In this work, we follow this approach and develop a contrastive\nlearning framework EEG-CLIP that aligns EEG time series and their corresponding\nclinical text descriptions in a shared embedding space. We investigate its\npotential for versatile EEG decoding, assessing performance on a range of\nfew-shot and zero-shot settings. Overall, results show that EEG-CLIP manages to\nnontrivially align text and EEG representations. Our work presents a promising\napproach to learn general EEG representations, which could enable easier\nanalyses of diverse decoding questions through zero shot decoding or training\ntask-specific models from fewer training examples. The code for reproducing our\nresults is available at https://github.com/tidiane-camaret/EEGClip."}
{"id": "2503.16591", "pdf": "https://arxiv.org/pdf/2503.16591", "abs": "https://arxiv.org/abs/2503.16591", "authors": ["Luigi Piccinelli", "Christos Sakaridis", "Mattia Segu", "Yung-Hsu Yang", "Siyuan Li", "Wim Abbeloos", "Luc Van Gool"], "title": "UniK3D: Universal Camera Monocular 3D Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D estimation is crucial for visual perception. However, current\nmethods fall short by relying on oversimplified assumptions, such as pinhole\ncamera models or rectified images. These limitations severely restrict their\ngeneral applicability, causing poor performance in real-world scenarios with\nfisheye or panoramic images and resulting in substantial context loss. To\naddress this, we present UniK3D, the first generalizable method for monocular\n3D estimation able to model any camera. Our method introduces a spherical 3D\nrepresentation which allows for better disentanglement of camera and scene\ngeometry and enables accurate metric 3D reconstruction for unconstrained camera\nmodels. Our camera component features a novel, model-independent representation\nof the pencil of rays, achieved through a learned superposition of spherical\nharmonics. We also introduce an angular loss, which, together with the camera\nmodule design, prevents the contraction of the 3D outputs for wide-view\ncameras. A comprehensive zero-shot evaluation on 13 diverse datasets\ndemonstrates the state-of-the-art performance of UniK3D across 3D, depth, and\ncamera metrics, with substantial gains in challenging large-field-of-view and\npanoramic settings, while maintaining top accuracy in conventional pinhole\nsmall-field-of-view domains. Code and models are available at\ngithub.com/lpiccinelli-eth/unik3d ."}
{"id": "2503.16533", "pdf": "https://arxiv.org/pdf/2503.16533", "abs": "https://arxiv.org/abs/2503.16533", "authors": ["Hassan S. Al Khatib", "Sudip Mittal", "Shahram Rahimi", "Nina Marhamati", "Sean Bozorgzad"], "title": "From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The transition towards patient-centric healthcare necessitates a\ncomprehensive understanding of patient journeys, which encompass all healthcare\nexperiences and interactions across the care spectrum. Existing healthcare data\nsystems are often fragmented and lack a holistic representation of patient\ntrajectories, creating challenges for coordinated care and personalized\ninterventions. Patient Journey Knowledge Graphs (PJKGs) represent a novel\napproach to addressing the challenge of fragmented healthcare data by\nintegrating diverse patient information into a unified, structured\nrepresentation. This paper presents a methodology for constructing PJKGs using\nLarge Language Models (LLMs) to process and structure both formal clinical\ndocumentation and unstructured patient-provider conversations. These graphs\nencapsulate temporal and causal relationships among clinical encounters,\ndiagnoses, treatments, and outcomes, enabling advanced temporal reasoning and\npersonalized care insights. The research evaluates four different LLMs, such as\nClaude 3.5, Mistral, Llama 3.1, and Chatgpt4o, in their ability to generate\naccurate and computationally efficient knowledge graphs. Results demonstrate\nthat while all models achieved perfect structural compliance, they exhibited\nvariations in medical entity processing and computational efficiency. The paper\nconcludes by identifying key challenges and future research directions. This\nwork contributes to advancing patient-centric healthcare through the\ndevelopment of comprehensive, actionable knowledge graphs that support improved\ncare coordination and outcome prediction."}
{"id": "2503.16611", "pdf": "https://arxiv.org/pdf/2503.16611", "abs": "https://arxiv.org/abs/2503.16611", "authors": ["Katja Schwarz", "Denys Rozumnyi", "Samuel Rota Bulò", "Lorenzo Porzi", "Peter Kontschieder"], "title": "A Recipe for Generating 3D Worlds From a Single Image", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce a recipe for generating immersive 3D worlds from a single image\nby framing the task as an in-context learning problem for 2D inpainting models.\nThis approach requires minimal training and uses existing generative models.\nOur process involves two steps: generating coherent panoramas using a\npre-trained diffusion model and lifting these into 3D with a metric depth\nestimator. We then fill unobserved regions by conditioning the inpainting model\non rendered point clouds, requiring minimal fine-tuning. Tested on both\nsynthetic and real images, our method produces high-quality 3D environments\nsuitable for VR display. By explicitly modeling the 3D structure of the\ngenerated environment from the start, our approach consistently outperforms\nstate-of-the-art, video synthesis-based methods along multiple quantitative\nimage quality metrics. Project Page: https://katjaschwarz.github.io/worlds/"}
{"id": "2503.16534", "pdf": "https://arxiv.org/pdf/2503.16534", "abs": "https://arxiv.org/abs/2503.16534", "authors": ["Roberto Balestri"], "title": "Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "This study evaluates the biases in Gemini 2.0 Flash Experimental, a\nstate-of-the-art large language model (LLM) developed by Google, focusing on\ncontent moderation and gender disparities. By comparing its performance to\nChatGPT-4o, examined in a previous work of the author, the analysis highlights\nsome differences in ethical moderation practices. Gemini 2.0 demonstrates\nreduced gender bias, notably with female-specific prompts achieving a\nsubstantial rise in acceptance rates compared to results obtained by\nChatGPT-4o. It adopts a more permissive stance toward sexual content and\nmaintains relatively high acceptance rates for violent prompts, including\ngender-specific cases. Despite these changes, whether they constitute an\nimprovement is debatable. While gender bias has been reduced, this reduction\ncomes at the cost of permitting more violent content toward both males and\nfemales, potentially normalizing violence rather than mitigating harm.\nMale-specific prompts still generally receive higher acceptance rates than\nfemale-specific ones. These findings underscore the complexities of aligning AI\nsystems with ethical standards, highlighting progress in reducing certain\nbiases while raising concerns about the broader implications of the model's\npermissiveness. Ongoing refinements are essential to achieve moderation\npractices that ensure transparency, fairness, and inclusivity without\namplifying harmful content."}
{"id": "2503.16616", "pdf": "https://arxiv.org/pdf/2503.16616", "abs": "https://arxiv.org/abs/2503.16616", "authors": ["Xiaoran Zhang", "Byung-Woo Hong", "Hyoungseob Park", "Daniel H. Pak", "Anne-Marie Rickmann", "Lawrence H. Staib", "James S. Duncan", "Alex Wong"], "title": "Progressive Test Time Energy Adaptation for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a model-agnostic, progressive test-time energy adaptation approach\nfor medical image segmentation. Maintaining model performance across diverse\nmedical datasets is challenging, as distribution shifts arise from inconsistent\nimaging protocols and patient variations. Unlike domain adaptation methods that\nrequire multiple passes through target data - impractical in clinical settings\n- our approach adapts pretrained models progressively as they process test\ndata. Our method leverages a shape energy model trained on source data, which\nassigns an energy score at the patch level to segmentation maps: low energy\nrepresents in-distribution (accurate) shapes, while high energy signals\nout-of-distribution (erroneous) predictions. By minimizing this energy score at\ntest time, we refine the segmentation model to align with the target\ndistribution. To validate the effectiveness and adaptability, we evaluated our\nframework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets\nspanning cardiac, spinal cord, and lung segmentation. We consistently\noutperform baselines both quantitatively and qualitatively."}
{"id": "2503.16536", "pdf": "https://arxiv.org/pdf/2503.16536", "abs": "https://arxiv.org/abs/2503.16536", "authors": ["Shuo Huang", "Muhammad Umair Nasir", "Steven James", "Julian Togelius"], "title": "Word2Minecraft: Generating 3D Game Levels through Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We present Word2Minecraft, a system that leverages large language models to\ngenerate playable game levels in Minecraft based on structured stories. The\nsystem transforms narrative elements-such as protagonist goals, antagonist\nchallenges, and environmental settings-into game levels with both spatial and\ngameplay constraints. We introduce a flexible framework that allows for the\ncustomization of story complexity, enabling dynamic level generation. The\nsystem employs a scaling algorithm to maintain spatial consistency while\nadapting key game elements. We evaluate Word2Minecraft using both metric-based\nand human-based methods. Our results show that GPT-4-Turbo outperforms\nGPT-4o-Mini in most areas, including story coherence and objective enjoyment,\nwhile the latter excels in aesthetic appeal. We also demonstrate the system' s\nability to generate levels with high map enjoyment, offering a promising step\nforward in the intersection of story generation and game design. We open-source\nthe code at https://github.com/JMZ-kk/Word2Minecraft/tree/word2mc_v0"}
{"id": "2503.16628", "pdf": "https://arxiv.org/pdf/2503.16628", "abs": "https://arxiv.org/abs/2503.16628", "authors": ["Moshiur Rahman Tonmoy", "Md. Mithun Hossain", "Nilanjan Dey", "M. F. Mridha"], "title": "MobilePlantViT: A Mobile-friendly Hybrid ViT for Generalized Plant Disease Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Submitted to a journal for peer-review under IEEE Transactions series", "summary": "Plant diseases significantly threaten global food security by reducing crop\nyields and undermining agricultural sustainability. AI-driven automated\nclassification has emerged as a promising solution, with deep learning models\ndemonstrating impressive performance in plant disease identification. However,\ndeploying these models on mobile and edge devices remains challenging due to\nhigh computational demands and resource constraints, highlighting the need for\nlightweight, accurate solutions for accessible smart agriculture systems. To\naddress this, we propose MobilePlantViT, a novel hybrid Vision Transformer\n(ViT) architecture designed for generalized plant disease classification, which\noptimizes resource efficiency while maintaining high performance. Extensive\nexperiments across diverse plant disease datasets of varying scales show our\nmodel's effectiveness and strong generalizability, achieving test accuracies\nranging from 80% to over 99%. Notably, with only 0.69 million parameters, our\narchitecture outperforms the smallest versions of MobileViTv1 and MobileViTv2,\ndespite their higher parameter counts. These results underscore the potential\nof our approach for real-world, AI-powered automated plant disease\nclassification in sustainable and resource-efficient smart agriculture systems.\nAll codes will be available in the GitHub repository:\nhttps://github.com/moshiurtonmoy/MobilePlantViT"}
{"id": "2503.16537", "pdf": "https://arxiv.org/pdf/2503.16537", "abs": "https://arxiv.org/abs/2503.16537", "authors": ["Grigorii Khvatskii", "Yong Suk Lee", "Corey Angst", "Maria Gibbs", "Robert Landers", "Nitesh V. Chawla"], "title": "Do Multimodal Large Language Models Understand Welding?", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages", "summary": "This paper examines the performance of Multimodal LLMs (MLLMs) in skilled\nproduction work, with a focus on welding. Using a novel data set of real-world\nand online weld images, annotated by a domain expert, we evaluate the\nperformance of two state-of-the-art MLLMs in assessing weld acceptability\nacross three contexts: RV \\& Marine, Aeronautical, and Farming. While both\nmodels perform better on online images, likely due to prior exposure or\nmemorization, they also perform relatively well on unseen, real-world weld\nimages. Additionally, we introduce WeldPrompt, a prompting strategy that\ncombines Chain-of-Thought generation with in-context learning to mitigate\nhallucinations and improve reasoning. WeldPrompt improves model recall in\ncertain contexts but exhibits inconsistent performance across others. These\nresults underscore the limitations and potentials of MLLMs in high-stakes\ntechnical domains and highlight the importance of fine-tuning, domain-specific\ndata, and more sophisticated prompting strategies to improve model reliability.\nThe study opens avenues for further research into multimodal learning in\nindustry applications."}
{"id": "2503.16653", "pdf": "https://arxiv.org/pdf/2503.16653", "abs": "https://arxiv.org/abs/2503.16653", "authors": ["Hanxiao Wang", "Biao Zhang", "Weize Quan", "Dong-Ming Yan", "Peter Wonka"], "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation", "categories": ["cs.CV"], "comment": "https://wanghanxiao123.github.io/iFa/", "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."}
{"id": "2503.16541", "pdf": "https://arxiv.org/pdf/2503.16541", "abs": "https://arxiv.org/abs/2503.16541", "authors": ["Hanzhi Zhang", "Sumera Anjum", "Heng Fan", "Weijian Zheng", "Yan Huang", "Yunhe Feng"], "title": "Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations in generative AI, particularly in Large Language Models\n(LLMs), pose a significant challenge to the reliability of multilingual\napplications. Existing benchmarks for hallucination detection focus primarily\non English and a few widely spoken languages, lacking the breadth to assess\ninconsistencies in model performance across diverse linguistic contexts. To\naddress this gap, we introduce Poly-FEVER, a large-scale multilingual fact\nverification benchmark specifically designed for evaluating hallucination\ndetection in LLMs. Poly-FEVER comprises 77,973 labeled factual claims spanning\n11 languages, sourced from FEVER, Climate-FEVER, and SciFact. It provides the\nfirst large-scale dataset tailored for analyzing hallucination patterns across\nlanguages, enabling systematic evaluation of LLMs such as ChatGPT and the LLaMA\nseries. Our analysis reveals how topic distribution and web resource\navailability influence hallucination frequency, uncovering language-specific\nbiases that impact model accuracy. By offering a multilingual benchmark for\nfact verification, Poly-FEVER facilitates cross-linguistic comparisons of\nhallucination detection and contributes to the development of more reliable,\nlanguage-inclusive AI systems. The dataset is publicly available to advance\nresearch in responsible AI, fact-checking methodologies, and multilingual NLP,\npromoting greater transparency and robustness in LLM performance. The proposed\nPoly-FEVER is available at:\nhttps://huggingface.co/datasets/HanzhiZhang/Poly-FEVER."}
{"id": "2503.16660", "pdf": "https://arxiv.org/pdf/2503.16660", "abs": "https://arxiv.org/abs/2503.16660", "authors": ["Eduard Allakhverdov", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image Representation", "categories": ["cs.CV", "68T10, 68T30, 68T45", "I.2.10"], "comment": "10 pages, 8 figures", "summary": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance."}
{"id": "2503.16544", "pdf": "https://arxiv.org/pdf/2503.16544", "abs": "https://arxiv.org/abs/2503.16544", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "title": "Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Tailoring persuasive conversations to users leads to more effective\npersuasion. However, existing dialogue systems often struggle to adapt to\ndynamically evolving user states. This paper presents a novel method that\nleverages causal discovery and counterfactual reasoning for optimizing system\npersuasion capability and outcomes. We employ the Greedy Relaxation of the\nSparsest Permutation (GRaSP) algorithm to identify causal relationships between\nuser and system utterance strategies, treating user strategies as states and\nsystem strategies as actions. GRaSP identifies user strategies as causal\nfactors influencing system responses, which inform Bidirectional Conditional\nGenerative Adversarial Networks (BiCoGAN) in generating counterfactual\nutterances for the system. Subsequently, we use the Dueling Double Deep\nQ-Network (D3QN) model to utilize counterfactual data to determine the best\npolicy for selecting system utterances. Our experiments with the\nPersuasionForGood dataset show measurable improvements in persuasion outcomes\nusing our approach over baseline methods. The observed increase in cumulative\nrewards and Q-values highlights the effectiveness of causal discovery in\nenhancing counterfactual reasoning and optimizing reinforcement learning\npolicies for online dialogue systems."}
{"id": "2503.16664", "pdf": "https://arxiv.org/pdf/2503.16664", "abs": "https://arxiv.org/abs/2503.16664", "authors": ["Martin Kostelník", "Karel Beneš", "Michal Hradiš"], "title": "TextBite: A Historical Czech Document Dataset for Logical Page Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Logical page segmentation is an important step in document analysis, enabling\nbetter semantic representations, information retrieval, and text understanding.\nPrevious approaches define logical segmentation either through text or\ngeometric objects, relying on OCR or precise geometry. To avoid the need for\nOCR, we define the task purely as segmentation in the image domain.\nFurthermore, to ensure the evaluation remains unaffected by geometrical\nvariations that do not impact text segmentation, we propose to use only\nforeground text pixels in the evaluation metric and disregard all background\npixels. To support research in logical document segmentation, we introduce\nTextBite, a dataset of historical Czech documents spanning the 18th to 20th\ncenturies, featuring diverse layouts from newspapers, dictionaries, and\nhandwritten records. The dataset comprises 8,449 page images with 78,863\nannotated segments of logically and thematically coherent text. We propose a\nset of baseline methods combining text region detection and relation\nprediction. The dataset, baselines and evaluation framework can be accessed at\nhttps://github.com/DCGM/textbite-dataset."}
{"id": "2503.16550", "pdf": "https://arxiv.org/pdf/2503.16550", "abs": "https://arxiv.org/abs/2503.16550", "authors": ["Yudao Sun", "Juan Yin", "Juan Zhao", "Fan Zhang", "Yongheng Liu", "Hongji Chen"], "title": "Unified Enhancement of the Generalization and Robustness of Language Models via Bi-Stage Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Neural network language models (LMs) are confronted with significant\nchallenges in generalization and robustness. Currently, many studies focus on\nimproving either generalization or robustness in isolation, without methods\naddressing both aspects simultaneously, which presents a significant challenge\nin developing LMs that are both robust and generalized. In this paper, we\npropose a bi-stage optimization framework to uniformly enhance both the\ngeneralization and robustness of LMs, termed UEGR. Specifically, during the\nforward propagation stage, we enrich the output probability distributions of\nadversarial samples by adaptive dropout to generate diverse sub models, and\nincorporate JS divergence and adversarial losses of these output distributions\nto reinforce output stability. During backward propagation stage, we compute\nparameter saliency scores and selectively update only the most critical\nparameters to minimize unnecessary deviations and consolidate the model's\nresilience. Theoretical analysis shows that our framework includes gradient\nregularization to limit the model's sensitivity to input perturbations and\nselective parameter updates to flatten the loss landscape, thus improving both\ngeneralization and robustness. The experimental results show that our method\nsignificantly improves the generalization and robustness of LMs compared to\nother existing methods across 13 publicly available language datasets,\nachieving state-of-the-art (SOTA) performance."}
{"id": "2503.16683", "pdf": "https://arxiv.org/pdf/2503.16683", "abs": "https://arxiv.org/abs/2503.16683", "authors": ["Zeping Liu", "Fan Zhang", "Junfeng Jiao", "Ni Lao", "Gengchen Mai"], "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations", "categories": ["cs.CV", "cs.AI", "I.4.10"], "comment": "18 pages, 10 figures", "summary": "Advancements in vision and language foundation models have inspired the\ndevelopment of geo-foundation models (GeoFMs), enhancing performance across\ndiverse geospatial tasks. However, many existing GeoFMs primarily focus on\noverhead remote sensing (RS) data while neglecting other data modalities such\nas ground-level imagery. A key challenge in multimodal GeoFM development is to\nexplicitly model geospatial relationships across modalities, which enables\ngeneralizability across tasks, spatial scales, and temporal contexts. To\naddress these limitations, we propose GAIR, a novel multimodal GeoFM\narchitecture integrating overhead RS data, street view (SV) imagery, and their\ngeolocation metadata. We utilize three factorized neural encoders to project an\nSV image, its geolocation, and an RS image into the embedding space. The SV\nimage needs to be located within the RS image's spatial footprint but does not\nneed to be at its geographic center. In order to geographically align the SV\nimage and RS image, we propose a novel implicit neural representations (INR)\nmodule that learns a continuous RS image representation and looks up the RS\nembedding at the SV image's geolocation. Next, these geographically aligned SV\nembedding, RS embedding, and location embedding are trained with contrastive\nlearning objectives from unlabeled data. We evaluate GAIR across 10 geospatial\ntasks spanning RS image-based, SV image-based, and location embedding-based\nbenchmarks. Experimental results demonstrate that GAIR outperforms\nstate-of-the-art GeoFMs and other strong baselines, highlighting its\neffectiveness in learning generalizable and transferable geospatial\nrepresentations."}
{"id": "2503.16553", "pdf": "https://arxiv.org/pdf/2503.16553", "abs": "https://arxiv.org/abs/2503.16553", "authors": ["Zhenlin Qin", "Leizhen Wang", "Francisco Camara Pereira", "Zhenlinag Ma"], "title": "A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs."}
{"id": "2503.16707", "pdf": "https://arxiv.org/pdf/2503.16707", "abs": "https://arxiv.org/abs/2503.16707", "authors": ["Jinlong Li", "Cristiano Saltori", "Fabio Poiesi", "Nicu Sebe"], "title": "Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "The lack of a large-scale 3D-text corpus has led recent works to distill\nopen-vocabulary knowledge from vision-language models (VLMs). owever, these\nmethods typically rely on a single VLM to align the feature spaces of 3D models\nwithin a common language space, which limits the potential of 3D models to\nleverage the diverse spatial and semantic capabilities encapsulated in various\nfoundation models. In this paper, we propose Cross-modal and Uncertainty-aware\nAgglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, the\nfirst model to integrate multiple foundation models-such as CLIP, DINOv2, and\nStable Diffusion-into 3D scene understanding. We further introduce a\ndeterministic uncertainty estimation to adaptively distill and harmonize the\nheterogeneous 2D feature embeddings from these models. Our method addresses two\nkey challenges: (1) incorporating semantic priors from VLMs alongside the\ngeometric knowledge of spatially-aware vision foundation models, and (2) using\na novel deterministic uncertainty estimation to capture model-specific\nuncertainties across diverse semantic and geometric sensitivities, helping to\nreconcile heterogeneous representations during training. Extensive experiments\non ScanNetV2 and Matterport3D demonstrate that our method not only advances\nopen-vocabulary segmentation but also achieves robust cross-domain alignment\nand competitive spatial perception capabilities. The code will be available at\n\\href{https://github.com/TyroneLi/CUA_O3D}{CUA_O3D}."}
{"id": "2503.16554", "pdf": "https://arxiv.org/pdf/2503.16554", "abs": "https://arxiv.org/abs/2503.16554", "authors": ["Brian Keith", "Fausto German", "Eric Krokos", "Sarah Joseph", "Chris North"], "title": "Explainable AI Components for Narrative Map Extraction", "categories": ["cs.CL", "cs.HC"], "comment": "Text2Story Workshop 2025 at ECIR 2025", "summary": "As narrative extraction systems grow in complexity, establishing user trust\nthrough interpretable and explainable outputs becomes increasingly critical.\nThis paper presents an evaluation of an Explainable Artificial Intelligence\n(XAI) system for narrative map extraction that provides meaningful explanations\nacross multiple levels of abstraction. Our system integrates explanations based\non topical clusters for low-level document relationships, connection\nexplanations for event relationships, and high-level structure explanations for\noverall narrative patterns. In particular, we evaluate the XAI system through a\nuser study involving 10 participants that examined narratives from the 2021\nCuban protests. The analysis of results demonstrates that participants using\nthe explanations made the users trust in the system's decisions, with\nconnection explanations and important event detection proving particularly\neffective at building user confidence. Survey responses indicate that the\nmulti-level explanation approach helped users develop appropriate trust in the\nsystem's narrative extraction capabilities. This work advances the\nstate-of-the-art in explainable narrative extraction while providing practical\ninsights for developing reliable narrative extraction systems that support\neffective human-AI collaboration."}
{"id": "2503.16709", "pdf": "https://arxiv.org/pdf/2503.16709", "abs": "https://arxiv.org/abs/2503.16709", "authors": ["Xuan Shen", "Weize Ma", "Jing Liu", "Changdi Yang", "Rui Ding", "Quanyi Wang", "Henghui Ding", "Wei Niu", "Yanzhi Wang", "Pu Zhao", "Jun Lin", "Jiuxiang Gu"], "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer\nvision, supporting numerous real-world applications. However, deploying\naccurate depth estimation models on resource-limited edge devices, especially\nApplication-Specific Integrated Circuits (ASICs), is challenging due to the\nhigh computational and memory demands. Recent advancements in foundational\ndepth estimation deliver impressive results but further amplify the difficulty\nof deployment on ASICs. To address this, we propose QuartDepth which adopts\npost-training quantization to quantize MDE models with hardware accelerations\nfor ASICs. Our approach involves quantizing both weights and activations to\n4-bit precision, reducing the model size and computation cost. To mitigate the\nperformance degradation, we introduce activation polishing and compensation\nalgorithm applied before and after activation quantization, as well as a weight\nreconstruction method for minimizing errors in weight quantization.\nFurthermore, we design a flexible and programmable hardware accelerator by\nsupporting kernel fusion and customized instruction programmability, enhancing\nthroughput and efficiency. Experimental results demonstrate that our framework\nachieves competitive accuracy while enabling fast inference and higher energy\nefficiency on ASICs, bridging the gap between high-performance depth estimation\nand practical edge-device applicability. Code:\nhttps://github.com/shawnricecake/quart-depth"}
{"id": "2503.16561", "pdf": "https://arxiv.org/pdf/2503.16561", "abs": "https://arxiv.org/abs/2503.16561", "authors": ["Ibrahim Al Azher", "Miftahul Jannat Mokarrama", "Zhishuai Guo", "Sagnik Ray Choudhury", "Hamed Alhoori"], "title": "FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article", "categories": ["cs.CL", "cs.LG"], "comment": "19 pages, 5 figures", "summary": "The future work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from key sections of a\nscientific article alongside related papers and analyze how the trends have\nevolved. We experimented with various Large Language Models (LLMs) and\nintegrated Retrieval-Augmented Generation (RAG) to enhance the generation\nprocess. We incorporate a LLM feedback mechanism to improve the quality of the\ngenerated content and propose an LLM-as-a-judge approach for evaluation. Our\nresults demonstrated that the RAG-based approach with LLM feedback outperforms\nother methods evaluated through qualitative and quantitative metrics. Moreover,\nwe conduct a human evaluation to assess the LLM as an extractor and judge. The\ncode and dataset for this project are here, code: HuggingFace"}
{"id": "2503.16710", "pdf": "https://arxiv.org/pdf/2503.16710", "abs": "https://arxiv.org/abs/2503.16710", "authors": ["Yanyan Li", "Youxu Fang", "Zunjie Zhu", "Kunyi Li", "Yong Ding", "Federico Tombari"], "title": "4D Gaussian Splatting SLAM", "categories": ["cs.CV"], "comment": null, "summary": "Simultaneously localizing camera poses and constructing Gaussian radiance\nfields in dynamic scenes establish a crucial bridge between 2D images and the\n4D real world. Instead of removing dynamic objects as distractors and\nreconstructing only static environments, this paper proposes an efficient\narchitecture that incrementally tracks camera poses and establishes the 4D\nGaussian radiance fields in unknown scenarios by using a sequence of RGB-D\nimages. First, by generating motion masks, we obtain static and dynamic priors\nfor each pixel. To eliminate the influence of static scenes and improve the\nefficiency on learning the motion of dynamic objects, we classify the Gaussian\nprimitives into static and dynamic Gaussian sets, while the sparse control\npoints along with an MLP is utilized to model the transformation fields of the\ndynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a\nnovel 2D optical flow map reconstruction algorithm is designed to render\noptical flows of dynamic objects between neighbor images, which are further\nused to supervise the 4D Gaussian radiance fields along with traditional\nphotometric and geometric constraints. In experiments, qualitative and\nquantitative evaluation results show that the proposed method achieves robust\ntracking and high-quality view synthesis performance in real-world\nenvironments."}
{"id": "2503.16575", "pdf": "https://arxiv.org/pdf/2503.16575", "abs": "https://arxiv.org/abs/2503.16575", "authors": ["Bo Hu", "Han Yuan", "Vlad Pandelea", "Wuqiong Luo", "Yingzhu Zhao", "Zheng Ma"], "title": "Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has sparked widespread\nadoption across diverse applications, making robust evaluation frameworks\ncrucial for assessing their performance. While conventional evaluation metrics\nremain applicable for shorter texts, their efficacy diminishes when evaluating\nthe quality of long-form answers. This limitation is particularly critical in\nreal-world scenarios involving extended questions, extensive context, and\nlong-form answers, such as financial analysis or regulatory compliance. In this\npaper, we use a practical financial use case to illustrate applications that\nhandle \"long question-context-answer triplets\". We construct a real-world\nfinancial dataset comprising long triplets and demonstrate the inadequacies of\ntraditional metrics. To address this, we propose an effective Extract, Match,\nand Score (EMS) evaluation approach tailored to the complexities of long-form\nLLMs' outputs, providing practitioners with a reliable methodology for\nassessing LLMs' performance in complex real-world scenarios."}
{"id": "2503.16726", "pdf": "https://arxiv.org/pdf/2503.16726", "abs": "https://arxiv.org/abs/2503.16726", "authors": ["Philipp Becker", "Abhinav Mehrotra", "Ruchika Chavhan", "Malcolm Chadwick", "Luca Morreale", "Mehdi Noroozi", "Alberto Gil Ramos", "Sourav Bhattacharya"], "title": "EDiT: Efficient Diffusion Transformers with Linear Compressed Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion Transformers (DiTs) have emerged as a leading architecture for\ntext-to-image synthesis, producing high-quality and photorealistic images.\nHowever, the quadratic scaling properties of the attention in DiTs hinder image\ngeneration with higher resolution or on devices with limited resources. This\nwork introduces an efficient diffusion transformer (EDiT) to alleviate these\nefficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs).\nFirst, we present a novel linear compressed attention method that uses a\nmulti-layer convolutional network to modulate queries with local information\nwhile keys and values are spatially aggregated. Second, we formulate a hybrid\nattention scheme for multi-modal inputs that combines linear attention for\nimage-to-image interactions and standard scaled dot-product attention for\ninteractions involving prompts. Merging these two approaches leads to an\nexpressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT).\nWe demonstrate the effectiveness of the EDiT and MM-EDiT architectures by\nintegrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion\n3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality\nafter distillation."}
{"id": "2503.16578", "pdf": "https://arxiv.org/pdf/2503.16578", "abs": "https://arxiv.org/abs/2503.16578", "authors": ["Yang Chen", "Hui Wang", "Shiyao Wang", "Junyang Chen", "Jiabei He", "Jiaming Zhou", "Xi Yang", "Yequan Wang", "Yonghua Lin", "Yong Qin"], "title": "SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "While voice technologies increasingly serve aging populations, current\nsystems exhibit significant performance gaps due to inadequate training data\ncapturing elderly-specific vocal characteristics like presbyphonia and\ndialectal variations. The limited data available on super-aged individuals in\nexisting elderly speech datasets, coupled with overly simple recording styles\nand annotation dimensions, exacerbates this issue. To address the critical\nscarcity of speech data from individuals aged 75 and above, we introduce\nSeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset\ncontains 55.53 hours of speech from 101 natural conversations involving 202\nparticipants, ensuring a strategic balance across gender, region, and age.\nThrough detailed annotation across multiple dimensions, it can support a wide\nrange of speech tasks. We perform extensive experiments on speaker\nverification, speaker diarization, speech recognition, and speech editing\ntasks, offering crucial insights for the development of speech technologies\ntargeting this age group."}
{"id": "2503.16742", "pdf": "https://arxiv.org/pdf/2503.16742", "abs": "https://arxiv.org/abs/2503.16742", "authors": ["Esther Y. H. Lin", "Yimin Ding", "Jogendra Kundu", "Yatong An", "Mohamed T. El-Haddad", "Alexander Fix"], "title": "Digitally Prototype Your Eye Tracker: Simulating Hardware Performance using 3D Synthetic Data", "categories": ["cs.CV"], "comment": "14 pages, 12 figures", "summary": "Eye tracking (ET) is a key enabler for Augmented and Virtual Reality (AR/VR).\nPrototyping new ET hardware requires assessing the impact of hardware choices\non eye tracking performance. This task is compounded by the high cost of\nobtaining data from sufficiently many variations of real hardware, especially\nfor machine learning, which requires large training datasets. We propose a\nmethod for end-to-end evaluation of how hardware changes impact machine\nlearning-based ET performance using only synthetic data. We utilize a dataset\nof real 3D eyes, reconstructed from light dome data using neural radiance\nfields (NeRF), to synthesize captured eyes from novel viewpoints and camera\nparameters. Using this framework, we demonstrate that we can predict the\nrelative performance across various hardware configurations, accounting for\nvariations in sensor noise, illumination brightness, and optical blur. We also\ncompare our simulator with the publicly available eye tracking dataset from the\nProject Aria glasses, demonstrating a strong correlation with real-world\nperformance. Finally, we present a first-of-its-kind analysis in which we vary\nET camera positions, evaluating ET performance ranging from on-axis direct\nviews of the eye to peripheral views on the frame. Such an analysis would have\npreviously required manufacturing physical devices to capture evaluation data.\nIn short, our method enables faster prototyping of ET hardware."}
{"id": "2503.16581", "pdf": "https://arxiv.org/pdf/2503.16581", "abs": "https://arxiv.org/abs/2503.16581", "authors": ["Zahra Khalila", "Arbi Haza Nasution", "Winda Monika", "Aytug Onan", "Yohei Murakami", "Yasir Bin Ismail Radi", "Noor Mohammad Osmani"], "title": "Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "11 pages, keywords: Large-language-models; retrieval-augmented\n  generation; question answering; Quranic studies; Islamic teachings", "summary": "Accurate and contextually faithful responses are critical when applying large\nlanguage models (LLMs) to sensitive and domain-specific tasks, such as\nanswering queries related to quranic studies. General-purpose LLMs often\nstruggle with hallucinations, where generated responses deviate from\nauthoritative sources, raising concerns about their reliability in religious\ncontexts. This challenge highlights the need for systems that can integrate\ndomain-specific knowledge while maintaining response accuracy, relevance, and\nfaithfulness. In this study, we investigate 13 open-source LLMs categorized\ninto large (e.g., Llama3:70b, Gemma2:27b, QwQ:32b), medium (e.g., Gemma2:9b,\nLlama3:8b), and small (e.g., Llama3.2:3b, Phi3:3.8b). A Retrieval-Augmented\nGeneration (RAG) is used to make up for the problems that come with using\nseparate models. This research utilizes a descriptive dataset of Quranic surahs\nincluding the meanings, historical context, and qualities of the 114 surahs,\nallowing the model to gather relevant knowledge before responding. The models\nare evaluated using three key metrics set by human evaluators: context\nrelevance, answer faithfulness, and answer relevance. The findings reveal that\nlarge models consistently outperform smaller models in capturing query\nsemantics and producing accurate, contextually grounded responses. The\nLlama3.2:3b model, even though it is considered small, does very well on\nfaithfulness (4.619) and relevance (4.857), showing the promise of smaller\narchitectures that have been well optimized. This article examines the\ntrade-offs between model size, computational efficiency, and response quality\nwhile using LLMs in domain-specific applications."}
{"id": "2503.16760", "pdf": "https://arxiv.org/pdf/2503.16760", "abs": "https://arxiv.org/abs/2503.16760", "authors": ["George Cazenavette", "Joel Julin", "Simon Lucey"], "title": "Rethinking the Role of Spatial Mixing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Until quite recently, the backbone of nearly every state-of-the-art computer\nvision model has been the 2D convolution. At its core, a 2D convolution\nsimultaneously mixes information across both the spatial and channel dimensions\nof a representation. Many recent computer vision architectures consist of\nsequences of isotropic blocks that disentangle the spatial and channel-mixing\ncomponents. This separation of the operations allows us to more closely\njuxtapose the effects of spatial and channel mixing in deep learning. In this\npaper, we take an initial step towards garnering a deeper understanding of the\nroles of these mixing operations. Through our experiments and analysis, we\ndiscover that on both classical (ResNet) and cutting-edge (ConvMixer) models,\nwe can reach nearly the same level of classification performance by and leaving\nthe spatial mixers at their random initializations. Furthermore, we show that\nmodels with random, fixed spatial mixing are naturally more robust to\nadversarial perturbations. Lastly, we show that this phenomenon extends past\nthe classification regime, as such models can also decode pixel-shuffled\nimages."}
{"id": "2503.16585", "pdf": "https://arxiv.org/pdf/2503.16585", "abs": "https://arxiv.org/abs/2503.16585", "authors": ["Hadi Amini", "Md Jueal Mia", "Yasaman Saadati", "Ahmed Imteaj", "Seyedsina Nabavirazavi", "Urmish Thakker", "Md Zarif Hossain", "Awal Ahmed Fime", "S. S. Iyengar"], "title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions", "categories": ["cs.CL", "cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs."}
{"id": "2503.16768", "pdf": "https://arxiv.org/pdf/2503.16768", "abs": "https://arxiv.org/abs/2503.16768", "authors": ["Meng Zhou", "Jiadong Xie", "Mingsheng Xu"], "title": "Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mainstream visual object tracking frameworks predominantly rely on template\nmatching paradigms. Their performance heavily depends on the quality of\ntemplate features, which becomes increasingly challenging to maintain in\ncomplex scenarios involving target deformation, occlusion, and background\nclutter. While existing spatiotemporal memory-based trackers emphasize memory\ncapacity expansion, they lack effective mechanisms for dynamic feature\nselection and adaptive fusion. To address this gap, we propose a Dynamic\nAttention Mechanism in Spatiotemporal Memory Network (DASTM) with two key\ninnovations: 1) A differentiable dynamic attention mechanism that adaptively\nadjusts channel-spatial attention weights by analyzing spatiotemporal\ncorrelations between the templates and memory features; 2) A lightweight gating\nnetwork that autonomously allocates computational resources based on target\nmotion states, prioritizing high-discriminability features in challenging\nscenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K\nbenchmarks demonstrate our DASTM's superiority, achieving state-of-the-art\nperformance in success rate, robustness, and real-time efficiency, thereby\noffering a novel solution for real-time tracking in complex environments."}
{"id": "2503.16614", "pdf": "https://arxiv.org/pdf/2503.16614", "abs": "https://arxiv.org/abs/2503.16614", "authors": ["Maria de Lourdes M. Silva", "André L. C. Mendonça", "Eduardo R. D. Neto", "Iago C. Chaves", "Felipe T. Brito", "Victor A. E. Farias", "Javam C. Machado"], "title": "Classification of User Reports for Detection of Faulty Computer Components using NLP Models: A Case Study", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 2 figures", "summary": "Computer manufacturers typically offer platforms for users to report faults.\nHowever, there remains a significant gap in these platforms' ability to\neffectively utilize textual reports, which impedes users from describing their\nissues in their own words. In this context, Natural Language Processing (NLP)\noffers a promising solution, by enabling the analysis of user-generated text.\nThis paper presents an innovative approach that employs NLP models to classify\nuser reports for detecting faulty computer components, such as CPU, memory,\nmotherboard, video card, and more. In this work, we build a dataset of 341 user\nreports obtained from many sources. Additionally, through extensive\nexperimental evaluation, our approach achieved an accuracy of 79% with our\ndataset."}
{"id": "2503.16775", "pdf": "https://arxiv.org/pdf/2503.16775", "abs": "https://arxiv.org/abs/2503.16775", "authors": ["Sreetama Sarkar", "Sumit Bam Shrestha", "Yue Che", "Leobardo Campos-Macias", "Gourav Datta", "Peter A. Beerel"], "title": "Region Masking to Accelerate Video Processing on Neuromorphic Hardware", "categories": ["cs.CV", "cs.NE", "eess.IV"], "comment": null, "summary": "The rapidly growing demand for on-chip edge intelligence on\nresource-constrained devices has motivated approaches to reduce energy and\nlatency of deep learning models. Spiking neural networks (SNNs) have gained\nparticular interest due to their promise to reduce energy consumption using\nevent-based processing. We assert that while sigma-delta encoding in SNNs can\ntake advantage of the temporal redundancy across video frames, they still\ninvolve a significant amount of redundant computations due to processing\ninsignificant events. In this paper, we propose a region masking strategy that\nidentifies regions of interest at the input of the SNN, thereby eliminating\ncomputation and data movement for events arising from unimportant regions. Our\napproach demonstrates that masking regions at the input not only significantly\nreduces the overall spiking activity of the network, but also provides\nsignificant improvement in throughput and latency. We apply region masking\nduring video object detection on Loihi 2, demonstrating that masking\napproximately 60% of input regions can reduce energy-delay product by 1.65x\nover a baseline sigma-delta network, with a degradation in mAP@0.5 by 1.09%."}
{"id": "2503.16622", "pdf": "https://arxiv.org/pdf/2503.16622", "abs": "https://arxiv.org/abs/2503.16622", "authors": ["Michele Fiori", "Gabriele Civitarese", "Priyankar Choudhary", "Claudio Bettini"], "title": "Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning\nof machine learning models. In IoT systems, XAI improves the transparency of\nmodels processing sensor data from multiple heterogeneous devices, ensuring\nend-users understand and trust their outputs. Among the many applications, XAI\nhas also been applied to sensor-based Activities of Daily Living (ADLs)\nrecognition in smart homes. Existing approaches highlight which sensor events\nare most important for each predicted activity, using simple rules to convert\nthese events into natural language explanations for non-expert users. However,\nthese methods produce rigid explanations lacking natural language flexibility\nand are not scalable. With the recent rise of Large Language Models (LLMs), it\nis worth exploring whether they can enhance explanation generation, considering\ntheir proven knowledge of human activities. This paper investigates potential\napproaches to combine XAI and LLMs for sensor-based ADL recognition. We\nevaluate if LLMs can be used: a) as explainable zero-shot ADL recognition\nmodels, avoiding costly labeled data collection, and b) to automate the\ngeneration of explanations for existing data-driven XAI approaches when\ntraining data is available and the goal is higher recognition rates. Our\ncritical evaluation provides insights into the benefits and challenges of using\nLLMs for explainable ADL recognition."}
{"id": "2503.16776", "pdf": "https://arxiv.org/pdf/2503.16776", "abs": "https://arxiv.org/abs/2503.16776", "authors": ["Valentin Bieri", "Marco Zamboni", "Nicolas S. Blumer", "Qingxuan Chen", "Francis Engelmann"], "title": "OpenCity3D: What do Vision-Language Models know about Urban Environments?", "categories": ["cs.CV"], "comment": "Published at WACV 2025", "summary": "Vision-language models (VLMs) show great promise for 3D scene understanding\nbut are mainly applied to indoor spaces or autonomous driving, focusing on\nlow-level tasks like segmentation. This work expands their use to urban-scale\nenvironments by leveraging 3D reconstructions from multi-view aerial imagery.\nWe propose OpenCity3D, an approach that addresses high-level tasks, such as\npopulation density estimation, building age classification, property price\nprediction, crime rate assessment, and noise pollution evaluation. Our findings\nhighlight OpenCity3D's impressive zero-shot and few-shot capabilities,\nshowcasing adaptability to new contexts. This research establishes a new\nparadigm for language-driven urban analytics, enabling applications in\nplanning, policy, and environmental monitoring. See our project page:\nopencity3d.github.io"}
{"id": "2503.16655", "pdf": "https://arxiv.org/pdf/2503.16655", "abs": "https://arxiv.org/abs/2503.16655", "authors": ["Maxime Delmas", "Magdalena Wysocka", "Danilo Gusicuma", "André Freitas"], "title": "Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs", "categories": ["cs.CL"], "comment": "11 pages, 9 figures, 3 tables", "summary": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available."}
{"id": "2503.16780", "pdf": "https://arxiv.org/pdf/2503.16780", "abs": "https://arxiv.org/abs/2503.16780", "authors": ["Uihyun Cho", "Namhun Kim"], "title": "A-IDE : Agent-Integrated Denoising Experts", "categories": ["cs.CV"], "comment": "10 pages, 11 figures", "summary": "Recent advances in deep-learning based denoising methods have improved\nLow-Dose CT image quality. However, due to distinct HU distributions and\ndiverse anatomical characteristics, a single model often struggles to\ngeneralize across multiple anatomies. To address this limitation, we introduce\n\\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates\nthree anatomical region-specialized RED-CNN models under the management of\ndecision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to\ndynamically route incoming LDCT scans to the most appropriate expert model. We\nhighlight three major advantages of our approach. A-IDE excels in\nheterogeneous, data-scarce environments. The framework automatically prevents\noverfitting by distributing tasks among multiple experts. Finally, our\nLLM-driven agentic pipeline eliminates the need for manual interventions.\nExperimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves\nsuperior performance in RMSE, PSNR, and SSIM compared to a single unified\ndenoiser."}
{"id": "2503.16674", "pdf": "https://arxiv.org/pdf/2503.16674", "abs": "https://arxiv.org/abs/2503.16674", "authors": ["Molly Kennedy", "Ayyoob Imani", "Timo Spinde", "Hinrich Schütze"], "title": "Through the LLM Looking Glass: A Socratic Self-Assessment of Donkeys, Elephants, and Markets", "categories": ["cs.CL"], "comment": null, "summary": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\neight widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via self-assessment. By using self-assessment, the\nstudy aims to directly measure the models' biases rather than relying on\nexternal interpretations, thereby minimizing subjective judgments about media\nbias. Our results reveal a consistent preference of Democratic over Republican\npositions across all models. Conversely, in economic topics, biases vary among\nWestern LLMs, while those developed in China lean more strongly toward\nsocialism."}
{"id": "2503.16782", "pdf": "https://arxiv.org/pdf/2503.16782", "abs": "https://arxiv.org/abs/2503.16782", "authors": ["Enguang Wang", "Zhimao Peng", "Zhengyuan Xie", "Haori Lu", "Fei Yang", "Xialei Liu"], "title": "Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data\ncontaining both seen and novel categories. Although existing methods perform\nwell on generic datasets, they struggle in fine-grained scenarios. We attribute\nthis difficulty to their reliance on contrastive learning over global image\nfeatures to automatically capture discriminative cues, which fails to capture\nthe subtle local differences essential for distinguishing fine-grained\ncategories. Therefore, in this paper, we propose incorporating part knowledge\nto address fine-grained GCD, which introduces two key challenges: the absence\nof annotations for novel classes complicates the extraction of the part\nfeatures, and global contrastive learning prioritizes holistic feature\ninvariance, inadvertently suppressing discriminative local part patterns. To\naddress these challenges, we propose PartGCD, including 1) Adaptive Part\nDecomposition, which automatically extracts class-specific semantic parts via\nGaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing\nexplicit separation between part features to amplify fine-grained local part\ndistinctions.\n  Experiments demonstrate state-of-the-art performance across multiple\nfine-grained benchmarks while maintaining competitiveness on generic datasets,\nvalidating the effectiveness and robustness of our approach."}
{"id": "2503.16728", "pdf": "https://arxiv.org/pdf/2503.16728", "abs": "https://arxiv.org/abs/2503.16728", "authors": ["Emiel van Miltenburg", "Chenghua Lin"], "title": "Natural Language Generation", "categories": ["cs.CL"], "comment": "3 pages + references. Submitted for publication in the Encyclopedia\n  of Language & Linguistics", "summary": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text."}
{"id": "2503.16793", "pdf": "https://arxiv.org/pdf/2503.16793", "abs": "https://arxiv.org/abs/2503.16793", "authors": ["Haori Lu", "Xusheng Cao", "Linlan Huang", "Enguang Wang", "Fei Yang", "Xialei Liu"], "title": "Restoring Forgotten Knowledge in Non-Exemplar Class Incremental Learning through Test-Time Semantic Evolution", "categories": ["cs.CV"], "comment": null, "summary": "Continual learning aims to accumulate knowledge over a data stream while\nmitigating catastrophic forgetting. In Non-exemplar Class Incremental Learning\n(NECIL), forgetting arises during incremental optimization because old classes\nare inaccessible, hindering the retention of prior knowledge. To solve this,\nprevious methods struggle in achieving the stability-plasticity balance in the\ntraining stages. However, we note that the testing stage is rarely considered\namong them, but is promising to be a solution to forgetting. Therefore, we\npropose RoSE, which is a simple yet effective method that\n\\textbf{R}est\\textbf{o}res forgotten knowledge through test-time\n\\textbf{S}emantic \\textbf{E}volution. Specifically designed for minimizing\nforgetting, RoSE is a test-time semantic drift compensation framework that\nenables more accurate drift estimation in a self-supervised manner. Moreover,\nto avoid incomplete optimization during online testing, we derive an analytical\nsolution as an alternative to gradient descent. We evaluate RoSE on CIFAR-100,\nTinyImageNet, and ImageNet100 datasets, under both cold-start and warm-start\nsettings. Our method consistently outperforms most state-of-the-art (SOTA)\nmethods across various scenarios, validating the potential and feasibility of\ntest-time evolution in NECIL."}
{"id": "2503.16745", "pdf": "https://arxiv.org/pdf/2503.16745", "abs": "https://arxiv.org/abs/2503.16745", "authors": ["Shiva Upadhye", "Jiaxuan Li", "Richard Futrell"], "title": "SPACER: A Parallel Dataset of Speech Production And Comprehension of Error Repairs", "categories": ["cs.CL"], "comment": "11 pages, 11 figures", "summary": "Speech errors are a natural part of communication, yet they rarely lead to\ncomplete communicative failure because both speakers and comprehenders can\ndetect and correct errors. Although prior research has examined error\nmonitoring and correction in production and comprehension separately,\nintegrated investigation of both systems has been impeded by the scarcity of\nparallel data. In this study, we present SPACER, a parallel dataset that\ncaptures how naturalistic speech errors are corrected by both speakers and\ncomprehenders. We focus on single-word substitution errors extracted from the\nSwitchboard corpus, accompanied by speaker's self-repairs and comprehenders'\nresponses from an offline text-editing experiment. Our exploratory analysis\nsuggests asymmetries in error correction strategies: speakers are more likely\nto repair errors that introduce greater semantic and phonemic deviations,\nwhereas comprehenders tend to correct errors that are phonemically similar to\nmore plausible alternatives or do not fit into prior contexts. Our dataset\nenables future research on integrated approaches toward studying language\nproduction and comprehension."}
{"id": "2503.16795", "pdf": "https://arxiv.org/pdf/2503.16795", "abs": "https://arxiv.org/abs/2503.16795", "authors": ["Yihan Hu", "Jianing Peng", "Yiheng Lin", "Ting Liu", "Xiaochao Qu", "Luoqi Liu", "Yao Zhao", "Yunchao Wei"], "title": "DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel approach to improving text-guided image editing\nusing diffusion-based models. Text-guided image editing task poses key\nchallenge of precisly locate and edit the target semantic, and previous methods\nfall shorts in this aspect. Our method introduces a Precise Semantic\nLocalization strategy that leverages visual and textual self-attention to\nenhance the cross-attention map, which can serve as a regional cues to improve\nediting performance. Then we propose a Dual-Level Control mechanism for\nincorporating regional cues at both feature and latent levels, offering\nfine-grained control for more precise edits. To fully compare our methods with\nother DiT-based approaches, we construct the RW-800 benchmark, featuring high\nresolution images, long descriptive texts, real-world images, and a new text\nediting task. Experimental results on the popular PIE-Bench and RW-800\nbenchmarks demonstrate the superior performance of our approach in preserving\nbackground and providing accurate edits."}
{"id": "2503.16779", "pdf": "https://arxiv.org/pdf/2503.16779", "abs": "https://arxiv.org/abs/2503.16779", "authors": ["Mengsong Wu", "Tong Zhu", "Han Han", "Xiang Zhang", "Wenbiao Shao", "Wenliang Chen"], "title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https://github.com/fairyshine/Chain-of-Tools ."}
{"id": "2503.16811", "pdf": "https://arxiv.org/pdf/2503.16811", "abs": "https://arxiv.org/abs/2503.16811", "authors": ["Maoji Zheng", "Ziyu Xu", "Qiming Xia", "Hai Wu", "Chenglu Wen", "Cheng Wang"], "title": "Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "LiDAR-based 3D object detection and semantic segmentation are critical tasks\nin 3D scene understanding. Traditional detection and segmentation methods\nsupervise their models through bounding box labels and semantic mask labels.\nHowever, these two independent labels inherently contain significant\nredundancy. This paper aims to eliminate the redundancy by supervising 3D\nobject detection using only semantic labels. However, the challenge arises due\nto the incomplete geometry structure and boundary ambiguity of point-cloud\ninstances, leading to inaccurate pseudo labels and poor detection results. To\naddress these challenges, we propose a novel method, named Seg2Box. We first\nintroduce a Multi-Frame Multi-Scale Clustering (MFMS-C) module, which leverages\nthe spatio-temporal consistency of point clouds to generate accurate box-level\npseudo-labels. Additionally, the Semantic?Guiding Iterative-Mining\nSelf-Training (SGIM-ST) module is proposed to enhance the performance by\nprogressively refining the pseudo-labels and mining the instances without\ngenerating pseudo-labels. Experiments on the Waymo Open Dataset and nuScenes\nDataset show that our method significantly outperforms other competitive\nmethods by 23.7\\% and 10.3\\% in mAP, respectively. The results demonstrate the\ngreat label-efficient potential and advancement of our method."}
{"id": "2503.16789", "pdf": "https://arxiv.org/pdf/2503.16789", "abs": "https://arxiv.org/abs/2503.16789", "authors": ["Rupak Sarkar", "Bahareh Sarrafzadeh", "Nirupama Chandrasekaran", "Nagu Rangan", "Philip Resnik", "Longqi Yang", "Sujay Kumar Jauhar"], "title": "Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation", "categories": ["cs.CL"], "comment": "8 pages, ACL style", "summary": "Human-LLM conversations are increasingly becoming more pervasive in peoples'\nprofessional and personal lives, yet many users still struggle to elicit\nhelpful responses from LLM Chatbots. One of the reasons for this issue is\nusers' lack of understanding in crafting effective prompts that accurately\nconvey their information needs. Meanwhile, the existence of real-world\nconversational datasets on the one hand, and the text understanding faculties\nof LLMs on the other, present a unique opportunity to study this problem, and\nits potential solutions at scale. Thus, in this paper we present the first\nLLM-centric study of real human-AI chatbot conversations, focused on\ninvestigating aspects in which user queries fall short of expressing\ninformation needs, and the potential of using LLMs to rewrite suboptimal user\nprompts. Our findings demonstrate that rephrasing ineffective prompts can\nelicit better responses from a conversational system, while preserving the\nuser's original intent. Notably, the performance of rewrites improves in longer\nconversations, where contextual inferences about user needs can be made more\naccurately. Additionally, we observe that LLMs often need to -- and inherently\ndo -- make \\emph{plausible} assumptions about a user's intentions and goals\nwhen interpreting prompts. Our findings largely hold true across conversational\ndomains, user intents, and LLMs of varying sizes and families, indicating the\npromise of using prompt rewriting as a solution for better human-AI\ninteractions."}
{"id": "2503.16816", "pdf": "https://arxiv.org/pdf/2503.16816", "abs": "https://arxiv.org/abs/2503.16816", "authors": ["Yi Niu", "Jiashuai Liu", "Yingkang Zhan", "Jiangbo Shi", "Di Zhang", "Ines Machado", "Mireia Crispin-Ortuzar", "Chen Li", "Zeyu Gao"], "title": "ST-Prompt Guided Histological Hypergraph Learning for Spatial Gene Expression Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Spatial Transcriptomics (ST) reveals the spatial distribution of gene\nexpression in tissues, offering critical insights into biological processes and\ndisease mechanisms. However, predicting ST from H\\&E-stained histology images\nis challenging due to the heterogeneous relationship between histomorphology\nand gene expression, which arises from substantial variability across different\npatients and tissue sections. A more practical and valuable approach is to\nutilize ST data from a few local regions to predict the spatial transcriptomic\nlandscape across the remaining regions in H&E slides. In response, we propose\nPHG2ST, an ST-prompt guided histological hypergraph learning framework, which\nleverages sparse ST signals as prompts to guide histological hypergraph\nlearning for global spatial gene expression prediction. Our framework fuses\nhistological hypergraph representations at multiple scales through a masked\nST-prompt encoding mechanism, improving robustness and generalizability.\nBenchmark evaluations on two public ST datasets demonstrate that PHG2ST\noutperforms the existing state-of-the-art methods and closely aligns with the\nground truth. These results underscore the potential of leveraging sparse local\nST data for scalable and cost-effective spatial gene expression mapping in\nreal-world biomedical applications."}
{"id": "2503.16826", "pdf": "https://arxiv.org/pdf/2503.16826", "abs": "https://arxiv.org/abs/2503.16826", "authors": ["Jun Seong Kim", "Kyaw Ye Thu", "Javad Ismayilzada", "Junyeong Park", "Eunsu Kim", "Huzama Ahmad", "Na Min An", "James Thorne", "Alice Oh"], "title": "When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts", "categories": ["cs.CL"], "comment": "12 pages", "summary": "In a highly globalized world, it is important for multi-modal large language\nmodels (MLLMs) to recognize and respond correctly to mixed-cultural inputs. For\nexample, a model should correctly identify kimchi (Korean food) in an image\nboth when an Asian woman is eating it, as well as an African man is eating it.\nHowever, current MLLMs show an over-reliance on the visual features of the\nperson, leading to misclassification of the entities. To examine the robustness\nof MLLMs to different ethnicity, we introduce MixCuBe, a cross-cultural bias\nbenchmark, and study elements from five countries and four ethnicities. Our\nfindings reveal that MLLMs achieve both higher accuracy and lower sensitivity\nto such perturbation for high-resource cultures, but not for low-resource\ncultures. GPT-4o, the best-performing model overall, shows up to 58% difference\nin accuracy between the original and perturbed cultural settings in\nlow-resource cultures. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/kyawyethu/MixCuBe."}
{"id": "2503.16822", "pdf": "https://arxiv.org/pdf/2503.16822", "abs": "https://arxiv.org/abs/2503.16822", "authors": ["Yuxin Yao", "Zhi Deng", "Junhui Hou"], "title": "RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos", "categories": ["cs.CV"], "comment": null, "summary": "This paper considers the problem of modeling articulated objects captured in\n2D videos to enable novel view synthesis, while also being easily editable,\ndrivable, and re-posable. To tackle this challenging problem, we propose RigGS,\na new paradigm that leverages 3D Gaussian representation and skeleton-based\nmotion representation to model dynamic objects without utilizing additional\ntemplate priors. Specifically, we first propose skeleton-aware node-controlled\ndeformation, which deforms a canonical 3D Gaussian representation over time to\ninitialize the modeling process, producing candidate skeleton nodes that are\nfurther simplified into a sparse 3D skeleton according to their motion and\nsemantic information. Subsequently, based on the resulting skeleton, we design\nlearnable skin deformations and pose-dependent detailed deformations, thereby\neasily deforming the 3D Gaussian representation to generate new actions and\nrender further high-quality images from novel views. Extensive experiments\ndemonstrate that our method can generate realistic new actions easily for\nobjects and achieve high-quality rendering."}
{"id": "2503.16853", "pdf": "https://arxiv.org/pdf/2503.16853", "abs": "https://arxiv.org/abs/2503.16853", "authors": ["Suho Yoo", "Hyunjong Ok", "Jaeho Lee"], "title": "Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Preprint", "summary": "Language models pretrained on text-only corpora often struggle with tasks\nthat require auditory commonsense knowledge. Previous work addresses this\nproblem by augmenting the language model to retrieve knowledge from external\naudio databases. This approach has several limitations, such as the potential\nlack of relevant audio in databases and the high costs associated with\nconstructing and querying the databases. To address these issues, we propose\nImagine to Hear, a novel approach that dynamically generates auditory knowledge\nusing generative models. Our framework detects multiple audio-related textual\nspans from the given prompt and generates corresponding auditory knowledge. We\ndevelop several mechanisms to efficiently process multiple auditory knowledge,\nincluding a CLAP-based rejection sampler and a language-audio fusion module.\nOur experiments show that our method achieves state-of-the-art performance on\nAuditoryBench without relying on external databases, highlighting the\neffectiveness of our generation-based approach."}
{"id": "2503.16825", "pdf": "https://arxiv.org/pdf/2503.16825", "abs": "https://arxiv.org/abs/2503.16825", "authors": ["Xiyue Guo", "Jiarui Hu", "Junjie Hu", "Hujun Bao", "Guofeng Zhang"], "title": "SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recently, camera-based solutions have been extensively explored for scene\nsemantic completion (SSC). Despite their success in visible areas, existing\nmethods struggle to capture complete scene semantics due to frequent visual\nocclusions. To address this limitation, this paper presents the first\nsatellite-ground cooperative SSC framework, i.e., SGFormer, exploring the\npotential of satellite-ground image pairs in the SSC task. Specifically, we\npropose a dual-branch architecture that encodes orthogonal satellite and ground\nviews in parallel, unifying them into a common domain. Additionally, we design\na ground-view guidance strategy that corrects satellite image biases during\nfeature encoding, addressing misalignment between satellite and ground views.\nMoreover, we develop an adaptive weighting strategy that balances contributions\nfrom satellite and ground views. Experiments demonstrate that SGFormer\noutperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360\ndatasets. Our code is available on https://github.com/gxytcrc/SGFormer."}
{"id": "2503.16856", "pdf": "https://arxiv.org/pdf/2503.16856", "abs": "https://arxiv.org/abs/2503.16856", "authors": ["Yang Tian", "Zheng Lu", "Mingqi Gao", "Zheng Liu", "Bo Zhao"], "title": "MMCR: Benchmarking Cross-Source Reasoning in Scientific Papers", "categories": ["cs.CL"], "comment": null, "summary": "Fully comprehending scientific papers by machines reflects a high level of\nArtificial General Intelligence, requiring the ability to reason across\nfragmented and heterogeneous sources of information, presenting a complex and\npractically significant challenge. While Vision-Language Models (VLMs) have\nmade remarkable strides in various tasks, particularly those involving\nreasoning with evidence source from single image or text page, their ability to\nuse cross-source information for reasoning remains an open problem. This work\npresents MMCR, a high-difficulty benchmark designed to evaluate VLMs' capacity\nfor reasoning with cross-source information from scientific papers. The\nbenchmark comprises 276 high-quality questions, meticulously annotated by\nhumans across 7 subjects and 10 task types. Experiments with 18 VLMs\ndemonstrate that cross-source reasoning presents a substantial challenge for\nexisting models. Notably, even the top-performing model, GPT-4o, achieved only\n48.55% overall accuracy, with only 20% accuracy in multi-table comprehension\ntasks, while the second-best model, Qwen2.5-VL-72B, reached 39.86% overall\naccuracy. Furthermore, we investigated the impact of the Chain-of-Thought (CoT)\ntechnique on cross-source reasoning and observed a detrimental effect on small\nmodels, whereas larger models demonstrated substantially enhanced performance.\nThese results highlight the pressing need to develop VLMs capable of\neffectively utilizing cross-source information for reasoning."}
{"id": "2503.16832", "pdf": "https://arxiv.org/pdf/2503.16832", "abs": "https://arxiv.org/abs/2503.16832", "authors": ["Ali Shah Ali", "Syed Ahmed Mahmood", "Mubin Saeed", "Andrey Konin", "M. Zeeshan Zia", "Quoc-Huy Tran"], "title": "Joint Self-Supervised Video Alignment and Action Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel approach for simultaneous self-supervised video\nalignment and action segmentation based on a unified optimal transport\nframework. In particular, we first tackle self-supervised video alignment by\ndeveloping a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior, which trains efficiently on GPUs and needs only a few\niterations for solving the optimal transport problem. Our single-task method\nachieves the state-of-the-art performance on multiple video alignment\nbenchmarks and outperforms VAVA, which relies on a traditional Kantorovich\noptimal transport formulation with an optimality prior. Furthermore, we extend\nour approach by proposing a unified optimal transport framework for joint\nself-supervised video alignment and action segmentation, which requires\ntraining and storing a single model and saves both time and memory consumption\nas compared to two different single-task models. Extensive evaluations on\nseveral video alignment and action segmentation datasets demonstrate that our\nmulti-task method achieves comparable video alignment yet superior action\nsegmentation results over previous methods in video alignment and action\nsegmentation respectively. Finally, to the best of our knowledge, this is the\nfirst work to unify video alignment and action segmentation into a single\nmodel."}
{"id": "2503.16858", "pdf": "https://arxiv.org/pdf/2503.16858", "abs": "https://arxiv.org/abs/2503.16858", "authors": ["Jialin Chen", "Aosong Feng", "Ziyu Zhao", "Juan Garza", "Gaukhar Nurbek", "Cheng Qin", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information."}
{"id": "2503.16835", "pdf": "https://arxiv.org/pdf/2503.16835", "abs": "https://arxiv.org/abs/2503.16835", "authors": ["Huiqiang Chen", "Tianqing Zhu", "Linlin Wang", "Xin Yu", "Longxiang Gao", "Wanlei Zhou"], "title": "Safe and Reliable Diffusion Models via Subspace Projection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large-scale text-to-image (T2I) diffusion models have revolutionized image\ngeneration, enabling the synthesis of highly detailed visuals from textual\ndescriptions. However, these models may inadvertently generate inappropriate\ncontent, such as copyrighted works or offensive images. While existing methods\nattempt to eliminate specific unwanted concepts, they often fail to ensure\ncomplete removal, allowing the concept to reappear in subtle forms. For\ninstance, a model may successfully avoid generating images in Van Gogh's style\nwhen explicitly prompted with 'Van Gogh', yet still reproduce his signature\nartwork when given the prompt 'Starry Night'. In this paper, we propose SAFER,\na novel and efficient approach for thoroughly removing target concepts from\ndiffusion models. At a high level, SAFER is inspired by the observed\nlow-dimensional structure of the text embedding space. The method first\nidentifies a concept-specific subspace $S_c$ associated with the target concept\nc. It then projects the prompt embeddings onto the complementary subspace of\n$S_c$, effectively erasing the concept from the generated images. Since\nconcepts can be abstract and difficult to fully capture using natural language\nalone, we employ textual inversion to learn an optimized embedding of the\ntarget concept from a reference image. This enables more precise subspace\nestimation and enhances removal performance. Furthermore, we introduce a\nsubspace expansion strategy to ensure comprehensive and robust concept erasure.\nExtensive experiments demonstrate that SAFER consistently and effectively\nerases unwanted concepts from diffusion models while preserving generation\nquality."}
{"id": "2503.16868", "pdf": "https://arxiv.org/pdf/2503.16868", "abs": "https://arxiv.org/abs/2503.16868", "authors": ["Mengsay Loem", "Taiju Hosaka"], "title": "Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visual question answering (VQA) has emerged as a flexible approach for\nextracting specific pieces of information from document images. However,\nexisting work typically queries each field in isolation, overlooking potential\ndependencies across multiple items. This paper investigates the merits of\nextracting multiple fields jointly versus separately. Through experiments on\nmultiple large vision language models and datasets, we show that jointly\nextracting fields often improves accuracy, especially when the fields share\nstrong numeric or contextual dependencies. We further analyze how performance\nscales with the number of requested items and use a regression based metric to\nquantify inter field relationships. Our results suggest that multi field\nprompts can mitigate confusion arising from similar surface forms and related\nnumeric values, providing practical methods for designing robust VQA systems in\ndocument information extraction tasks."}
{"id": "2503.16843", "pdf": "https://arxiv.org/pdf/2503.16843", "abs": "https://arxiv.org/abs/2503.16843", "authors": ["Jian Liang", "Wenke Huang", "Guancheng Wan", "Qu Yang", "Mang Ye"], "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "While Multimodal Large Language Models (MLLMs) excel at generalizing across\nmodalities and tasks, effectively adapting them to specific downstream tasks\nwhile simultaneously retaining both general and specialized knowledge remains\nchallenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently\nacquire specialized knowledge in MLLMs, it introduces substantial harmful\nredundancy during visual instruction tuning, which exacerbates the forgetting\nof general knowledge and degrades downstream task performance. To address this\nissue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby\nharmonizing general and specialized knowledge. Specifically, under theoretical\nguarantees, we introduce sparse updates into LoRA to discard redundant\nparameters effectively. Furthermore, we propose a Conflict Mitigation\nRegularizer to refine the update trajectory of LoRA, mitigating knowledge\nconflicts with the pretrained weights. Extensive experimental results\ndemonstrate that even at very high degree of sparsity ($\\le$ 5%), our method\nsimultaneously enhances generalization and downstream task performance. This\nconfirms that our approach effectively mitigates the catastrophic forgetting\nissue and further promotes knowledge harmonization in MLLMs."}
{"id": "2503.16883", "pdf": "https://arxiv.org/pdf/2503.16883", "abs": "https://arxiv.org/abs/2503.16883", "authors": ["Deniss Ruder", "Andero Uusberg", "Kairit Sirts"], "title": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion Appraisal Ratings", "categories": ["cs.CL"], "comment": null, "summary": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals."}
{"id": "2503.16852", "pdf": "https://arxiv.org/pdf/2503.16852", "abs": "https://arxiv.org/abs/2503.16852", "authors": ["Jiaxi Li", "Di Lin", "Hao Chen", "Hongying Liu", "Liang Wan", "Wei Feng"], "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization", "categories": ["cs.CV", "cs.AI"], "comment": "under review", "summary": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios."}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965", "abs": "https://arxiv.org/abs/2503.16965", "authors": ["Zhe Hu", "Jing Li", "Yu Yin"], "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms."}
{"id": "2503.16854", "pdf": "https://arxiv.org/pdf/2503.16854", "abs": "https://arxiv.org/abs/2503.16854", "authors": ["Zhibo Yang", "Wei Hua", "Sibo Song", "Cong Yao", "Yingying Zhu", "Wenqing Cheng", "Xiang Bai"], "title": "Generative Compositor for Few-Shot Visual Information Extraction", "categories": ["cs.CV"], "comment": null, "summary": "Visual Information Extraction (VIE), aiming at extracting structured\ninformation from visually rich document images, plays a pivotal role in\ndocument processing. Considering various layouts, semantic scopes, and\nlanguages, VIE encompasses an extensive range of types, potentially numbering\nin the thousands. However, many of these types suffer from a lack of training\ndata, which poses significant challenges. In this paper, we propose a novel\ngenerative model, named Generative Compositor, to address the challenge of\nfew-shot VIE. The Generative Compositor is a hybrid pointer-generator network\nthat emulates the operations of a compositor by retrieving words from the\nsource text and assembling them based on the provided prompts. Furthermore,\nthree pre-training strategies are employed to enhance the model's perception of\nspatial context information. Besides, a prompt-aware resampler is specially\ndesigned to enable efficient matching by leveraging the entity-semantic prior\ncontained in prompts. The introduction of the prompt-based retrieval mechanism\nand the pre-training strategies enable the model to acquire more effective\nspatial and semantic clues with limited training samples. Experiments\ndemonstrate that the proposed method achieves highly competitive results in the\nfull-sample training, while notably outperforms the baseline in the 1-shot,\n5-shot, and 10-shot settings."}
{"id": "2503.17003", "pdf": "https://arxiv.org/pdf/2503.17003", "abs": "https://arxiv.org/abs/2503.17003", "authors": ["Jian Guan", "Junfei Wu", "Jia-Nan Li", "Chuanqi Cheng", "Wei Wu"], "title": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."}
{"id": "2503.16855", "pdf": "https://arxiv.org/pdf/2503.16855", "abs": "https://arxiv.org/abs/2503.16855", "authors": ["Koki Hirooka", "Abu Saleh Musa Miah", "Tatsuya Murakami", "Yuto Akiba", "Yong Seok Hwang", "Jungpil Shin"], "title": "Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Multi-Culture Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Hand gesture-based Sign Language Recognition (SLR) serves as a crucial\ncommunication bridge between deaf and non-deaf individuals. Existing SLR\nsystems perform well for their cultural SL but may struggle with multi-cultural\nsign languages (McSL). To address these challenges, this paper proposes a Stack\nSpatial-Temporal Transformer Network that leverages multi-head attention\nmechanisms to capture both spatial and temporal dependencies with hierarchical\nfeatures using the Stack Transfer concept. In the proceed, firstly, we applied\na fully connected layer to make a embedding vector which has high expressive\npower from the original dataset, then fed them a stack newly proposed\ntransformer to achieve hierarchical features with short-range and long-range\ndependency. The network architecture is composed of several stages that process\nspatial and temporal relationships sequentially, ensuring effective feature\nextraction. After making the fully connected layer, the embedding vector is\nprocessed by the Spatial Multi-Head Attention Transformer, which captures\nspatial dependencies between joints. In the next stage, the Temporal Multi-Head\nAttention Transformer captures long-range temporal dependencies, and again, the\nfeatures are concatenated with the output using another skip connection. The\nprocessed features are then passed to the Feed-Forward Network (FFN), which\nrefines the feature representations further. After the FFN, additional skip\nconnections are applied to combine the output with earlier layers, followed by\na final normalization layer to produce the final output feature tensor. This\nprocess is repeated for 10 transformer blocks. The extensive experiment shows\nthat the JSL, KSL and ASL datasets achieved good performance accuracy. Our\napproach demonstrates improved performance in McSL, and it will be consider as\na novel work in this domain."}
{"id": "2503.17039", "pdf": "https://arxiv.org/pdf/2503.17039", "abs": "https://arxiv.org/abs/2503.17039", "authors": ["Jeremy Barnes", "Naiara Perez", "Alba Bonet-Jover", "Begoña Altuna"], "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads."}
{"id": "2503.16867", "pdf": "https://arxiv.org/pdf/2503.16867", "abs": "https://arxiv.org/abs/2503.16867", "authors": ["Kaisi Guan", "Zhengfeng Lai", "Yuchong Sun", "Peng Zhang", "Wei Liu", "Kieran Liu", "Meng Cao", "Ruihua Song"], "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering", "categories": ["cs.CV"], "comment": null, "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation."}
{"id": "2503.17073", "pdf": "https://arxiv.org/pdf/2503.17073", "abs": "https://arxiv.org/abs/2503.17073", "authors": ["Jonas Wallat", "Abdelrahman Abdallah", "Adam Jatowt", "Avishek Anand"], "title": "A Study into Investigating Temporal Robustness of LLMs", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "comment": "8 pages", "summary": "Large Language Models (LLMs) encapsulate a surprising amount of factual world\nknowledge. However, their performance on temporal questions and historical\nknowledge is limited because they often cannot understand temporal scope and\norientation or neglect the temporal aspect altogether. In this study, we aim to\nmeasure precisely how robust LLMs are for question answering based on their\nability to process temporal information and perform tasks requiring temporal\nreasoning and temporal factual knowledge. Specifically, we design eight\ntime-sensitive robustness tests for factual information to check the\nsensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs\nlacking temporal robustness, especially to temporal reformulations and the use\nof different granularities of temporal references. We show how a selection of\nthese eight tests can be used automatically to judge a model's temporal\nrobustness for user questions on the fly. Finally, we apply the findings of\nthis study to improve the temporal QA performance by up to 55 percent."}
{"id": "2503.16873", "pdf": "https://arxiv.org/pdf/2503.16873", "abs": "https://arxiv.org/abs/2503.16873", "authors": ["Dongseob Kim", "Hyunjung Shim"], "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025 Accepted", "summary": "Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https://github.com/k0u-id/CCD."}
{"id": "2503.17126", "pdf": "https://arxiv.org/pdf/2503.17126", "abs": "https://arxiv.org/abs/2503.17126", "authors": ["John Joon Young Chung", "Vishakh Padmakumar", "Melissa Roemmele", "Yuqian Sun", "Max Kreminski"], "title": "Modifying Large Language Model Post-Training for Diverse Creative Writing", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO."}
{"id": "2503.16910", "pdf": "https://arxiv.org/pdf/2503.16910", "abs": "https://arxiv.org/abs/2503.16910", "authors": ["Yu Qiu", "Yuhang Sun", "Jie Mei", "Lin Xiao", "Jing Xu"], "title": "Salient Object Detection in Traffic Scene through the TSOD10K Dataset", "categories": ["cs.CV"], "comment": "12 pages, 12 figures", "summary": "Traffic Salient Object Detection (TSOD) aims to segment the objects critical\nto driving safety by combining semantic (e.g., collision risks) and visual\nsaliency. Unlike SOD in natural scene images (NSI-SOD), which prioritizes\nvisually distinctive regions, TSOD emphasizes the objects that demand immediate\ndriver attention due to their semantic impact, even with low visual contrast.\nThis dual criterion, i.e., bridging perception and contextual risk, re-defines\nsaliency for autonomous and assisted driving systems. To address the lack of\ntask-specific benchmarks, we collect the first large-scale TSOD dataset with\npixel-wise saliency annotations, named TSOD10K. TSOD10K covers the diverse\nobject categories in various real-world traffic scenes under various\nchallenging weather/illumination variations (e.g., fog, snowstorms,\nlow-contrast, and low-light). Methodologically, we propose a Mamba-based TSOD\nmodel, termed Tramba. Considering the challenge of distinguishing inconspicuous\nvisual information from complex traffic backgrounds, Tramba introduces a novel\nDual-Frequency Visual State Space module equipped with shifted window\npartitioning and dilated scanning to enhance the perception of fine details and\nglobal structure by hierarchically decomposing high/low-frequency components.\nTo emphasize critical regions in traffic scenes, we propose a traffic-oriented\nHelix 2D-Selective-Scan (Helix-SS2D) mechanism that injects driving attention\npriors while effectively capturing global multi-direction spatial dependencies.\nWe establish a comprehensive benchmark by evaluating Tramba and 22 existing\nNSI-SOD models on TSOD10K, demonstrating Tramba's superiority. Our research\nestablishes the first foundation for safety-aware saliency analysis in\nintelligent transportation systems."}
{"id": "2503.17136", "pdf": "https://arxiv.org/pdf/2503.17136", "abs": "https://arxiv.org/abs/2503.17136", "authors": ["Brihi Joshi", "Sriram Venkatapathy", "Mohit Bansal", "Nanyun Peng", "Haw-Shiuan Chang"], "title": "CoKe: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating creative text such as human-written stories using language models\nhas always been a challenging task -- owing to the subjectivity of\nmulti-annotator ratings. To mimic the thinking process of humans, chain of\nthought (CoT) generates free-text explanations that help guide a model's\npredictions and Self-Consistency (SC) marginalizes predictions over multiple\ngenerated explanations. In this study, we discover that the widely-used\nself-consistency reasoning methods cause suboptimal results due to an objective\nmismatch between generating 'fluent-looking' explanations vs. actually leading\nto a good rating prediction for an aspect of a story. To overcome this\nchallenge, we propose $\\textbf{C}$hain-$\\textbf{o}$f-$\\textbf{Ke}$ywords\n(CoKe), that generates a sequence of keywords $\\textit{before}$ generating a\nfree-text rationale, that guide the rating prediction of our evaluation\nlanguage model. Then, we generate a diverse set of such keywords, and aggregate\nthe scores corresponding to these generations. On the StoryER dataset, CoKe\nbased on our small fine-tuned evaluation models not only reach human-level\nperformance and significantly outperform GPT-4 with a 2x boost in correlation\nwith human annotators, but also requires drastically less number of parameters."}
{"id": "2503.16916", "pdf": "https://arxiv.org/pdf/2503.16916", "abs": "https://arxiv.org/abs/2503.16916", "authors": ["Xiaoyong Chen", "Yong Guo", "Jiaming Liang", "Sitong Zhuang", "Runhao Zeng", "Xiping Hu"], "title": "Temporal Action Detection Model Compression by Progressive Block Drop", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Temporal action detection (TAD) aims to identify and localize action\ninstances in untrimmed videos, which is essential for various video\nunderstanding tasks. However, recent improvements in model performance, driven\nby larger feature extractors and datasets, have led to increased computational\ndemands. This presents a challenge for applications like autonomous driving and\nrobotics, which rely on limited computational resources. While existing channel\npruning methods can compress these models, reducing the number of channels\noften hinders the parallelization efficiency of GPU, due to the inefficient\nmultiplication between small matrices. Instead of pruning channels, we propose\na Progressive Block Drop method that reduces model depth while retaining layer\nwidth. In this way, we still use large matrices for computation but reduce the\nnumber of multiplications. Our approach iteratively removes redundant blocks in\ntwo steps: first, we drop blocks with minimal impact on model performance; and\nsecond, we employ a parameter-efficient cross-depth alignment technique,\nfine-tuning the pruned model to restore model accuracy. Our method achieves a\n25% reduction in computational overhead on two TAD benchmarks (THUMOS14 and\nActivityNet-1.3) to achieve lossless compression. More critically, we\nempirically show that our method is orthogonal to channel pruning methods and\ncan be combined with it to yield further efficiency gains."}
{"id": "2503.17211", "pdf": "https://arxiv.org/pdf/2503.17211", "abs": "https://arxiv.org/abs/2503.17211", "authors": ["Zilin Dai", "Lehong Wang", "Fangzhou Lin", "Yidong Wang", "Zhigang Li", "Kazunori D Yamada", "Ziming Zhang", "Wang Lu"], "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels."}
{"id": "2503.16921", "pdf": "https://arxiv.org/pdf/2503.16921", "abs": "https://arxiv.org/abs/2503.16921", "authors": ["Lingfan Zhang", "Chen Liu", "Chengming Xu", "Kai Hu", "Donghao Luo", "Chengjie Wang", "Yanwei Fu", "Yuan Yao"], "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks."}
{"id": "2503.17222", "pdf": "https://arxiv.org/pdf/2503.17222", "abs": "https://arxiv.org/abs/2503.17222", "authors": ["Sonish Sivarajkumar", "Kimia Ameri", "Chuqin Li", "Yanshan Wang", "Min Jiang"], "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies."}
{"id": "2503.16924", "pdf": "https://arxiv.org/pdf/2503.16924", "abs": "https://arxiv.org/abs/2503.16924", "authors": ["Joo Chan Lee", "Jong Hwan Ko", "Eunbyung Park"], "title": "Optimized Minimal 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Project page: https://maincold2.github.io/omg/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/."}
{"id": "2503.17239", "pdf": "https://arxiv.org/pdf/2503.17239", "abs": "https://arxiv.org/abs/2503.17239", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Syed Zawad", "Holger Boche"], "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses."}
{"id": "2503.16929", "pdf": "https://arxiv.org/pdf/2503.16929", "abs": "https://arxiv.org/abs/2503.16929", "authors": ["Shicheng Li", "Lei Li", "Kun Ouyang", "Shuhuai Ren", "Yuanxin Liu", "Yuanxing Zhang", "Fuzheng Zhang", "Lingpeng Kong", "Qi Liu", "Xu Sun"], "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs."}
{"id": "2503.17247", "pdf": "https://arxiv.org/pdf/2503.17247", "abs": "https://arxiv.org/abs/2503.17247", "authors": ["Michael J Bommarito", "Daniel Martin Katz", "Jillian Bommarito"], "title": "KL3M Tokenizers: A Family of Domain-Specific and Character-Level Tokenizers for Legal, Financial, and Preprocessing Applications", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 7 tables, 3 figures; Source code available at\n  https://github.com/alea-institute/kl3m-tokenizer-paper", "summary": "We present the KL3M tokenizers, a family of specialized tokenizers for legal,\nfinancial, and governmental text. Despite established work on tokenization,\nspecialized tokenizers for professional domains remain understudied. Our paper\noffers two main contributions to this area.\n  First, we introduce domain-specific BPE tokenizers for legal, financial, and\ngovernmental text. Our kl3m-004-128k-cased tokenizer uses 9-17% fewer tokens\nthan GPT-4o and Llama3 for domain-specific documents, despite having a smaller\nvocabulary. For specialized terminology, our cased tokenizer is even more\nefficient, using up to 83% fewer tokens for legal terms and 39% fewer tokens\nfor financial terms.\n  Second, we develop character-level BPE tokenizers (4K, 8K, and 16K vocabulary\nsizes) for text correction tasks like OCR post-processing. These tokenizers\nkeep consistent token boundaries between error-containing and correct text,\nmaking it easier for models to learn correction patterns.\n  These tokenizers help professional applications by fitting more text in\ncontext windows, reducing computational needs, and preserving the meaning of\ndomain-specific terms. Our analysis shows these efficiency gains directly\nbenefit the processing of long legal and financial documents. We release all\ntokenizers and code through GitHub and Hugging Face to support further research\nin specialized tokenization."}
{"id": "2503.16930", "pdf": "https://arxiv.org/pdf/2503.16930", "abs": "https://arxiv.org/abs/2503.16930", "authors": ["Haijin Zeng", "Xiangming Wang", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Dynamic image degradations, including noise, blur and lighting\ninconsistencies, pose significant challenges in image restoration, often due to\nsensor limitations or adverse environmental conditions. Existing Deep Unfolding\nNetworks (DUNs) offer stable restoration performance but require manual\nselection of degradation matrices for each degradation type, limiting their\nadaptability across diverse scenarios. To address this issue, we propose the\nVision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for\nhandling multiple degradation types simultaneously. VLU-Net leverages a\nVision-Language Model (VLM) refined on degraded image-text pairs to align image\nfeatures with degradation descriptions, selecting the appropriate transform for\ntarget degradation. By integrating an automatic VLM-based gradient estimation\nstrategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net\neffectively tackles complex multi-degradation restoration tasks while\nmaintaining interpretability. Furthermore, we design a hierarchical feature\nunfolding structure to enhance VLU-Net framework, efficiently synthesizing\ndegradation patterns across various levels. VLU-Net is the first all-in-one DUN\nframework and outperforms current leading one-by-one and all-in-one end-to-end\nmethods by 3.74 dB on the SOTS dehazing dataset and 1.70 dB on the Rain100L\nderaining dataset."}
{"id": "2503.17279", "pdf": "https://arxiv.org/pdf/2503.17279", "abs": "https://arxiv.org/abs/2503.17279", "authors": ["Gaifan Zhang", "Yi Zhou", "Danushka Bollegala"], "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement", "categories": ["cs.CL"], "comment": null, "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance."}
{"id": "2503.16942", "pdf": "https://arxiv.org/pdf/2503.16942", "abs": "https://arxiv.org/abs/2503.16942", "authors": ["Yingying Fan", "Quanwei Yang", "Kaisiyuan Wang", "Hang Zhou", "Yingying Li", "Haocheng Feng", "Yu Wu", "Jingdong Wang"], "title": "Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Current digital human studies focusing on lip-syncing and body movement are\nno longer sufficient to meet the growing industrial demand, while human video\ngeneration techniques that support interacting with real-world environments\n(e.g., objects) have not been well investigated. Despite human hand synthesis\nalready being an intricate problem, generating objects in contact with hands\nand their interactions presents an even more challenging task, especially when\nthe objects exhibit obvious variations in size and shape. To cope with these\nissues, we present a novel video Reenactment framework focusing on Human-Object\nInteraction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).\nOur key insight is to employ specialized layout representation for hands and\nobjects, respectively. Such representations enable effective disentanglement of\nhand modeling and object adaptation to diverse motion sequences. To further\nimprove the generation quality of HOI, we have designed an interactive textural\nenhancement module for both hands and objects by introducing two independent\nmemory banks. We also propose a layout-adjusting strategy for the cross-object\nreenactment scenario to adaptively adjust unreasonable layouts caused by\ndiverse object sizes during inference. Comprehensive qualitative and\nquantitative evaluations demonstrate that our proposed framework significantly\noutperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD."}
{"id": "2503.17287", "pdf": "https://arxiv.org/pdf/2503.17287", "abs": "https://arxiv.org/abs/2503.17287", "authors": ["Mingyang Song", "Mao Zheng", "Zheng Li", "Wenjie Yang", "Xuan Luo", "Yue Pan", "Feng Zhang"], "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose \\textbf{\\textsc{FastCuRL}}, a simple yet efficient\n\\textbf{Cu}rriculum \\textbf{R}einforcement \\textbf{L}earning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textbf{\\textsc{FastCuRL}} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textbf{\\textsc{FastCuRL}}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs."}
{"id": "2503.16944", "pdf": "https://arxiv.org/pdf/2503.16944", "abs": "https://arxiv.org/abs/2503.16944", "authors": ["Mengtian Li", "Jinshu Chen", "Wanquan Feng", "Bingchuan Li", "Fei Dai", "Songtao Zhao", "Qian He"], "title": "HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Personalized portrait synthesis, essential in domains like social\nentertainment, has recently made significant progress. Person-wise fine-tuning\nbased methods, such as LoRA and DreamBooth, can produce photorealistic outputs\nbut need training on individual samples, consuming time and resources and\nposing an unstable risk. Adapter based techniques such as IP-Adapter freeze the\nfoundational model parameters and employ a plug-in architecture to enable\nzero-shot inference, but they often exhibit a lack of naturalness and\nauthenticity, which are not to be overlooked in portrait synthesis tasks. In\nthis paper, we introduce a parameter-efficient adaptive generation method,\nnamely HyperLoRA, that uses an adaptive plug-in network to generate LoRA\nweights, merging the superior performance of LoRA with the zero-shot capability\nof adapter scheme. Through our carefully designed network structure and\ntraining strategy, we achieve zero-shot personalized portrait generation\n(supporting both single and multiple image inputs) with high photorealism,\nfidelity, and editability."}
{"id": "2503.17336", "pdf": "https://arxiv.org/pdf/2503.17336", "abs": "https://arxiv.org/abs/2503.17336", "authors": ["Reem Gody", "Mohamed Abdelghaffar", "Mohammed Jabreel", "Ahmed Tawfik"], "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments."}
{"id": "2503.16945", "pdf": "https://arxiv.org/pdf/2503.16945", "abs": "https://arxiv.org/abs/2503.16945", "authors": ["Ibtissam Saadi", "Abdenour Hadid", "Douglas W. Cunningham", "Abdelmalik Taleb-Ahmed", "Yassin El Hillali"], "title": "PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic\nFacial Expression Recognition (DFER) but face challenges such as inefficient\nfull fine-tuning, high complexity, and poor alignment between textual and\nvisual representations. Additionally, existing methods struggle with\nineffective temporal modeling. To address these issues, we propose PE-CLIP, a\nparameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER\nwhile significantly reducing trainable parameters while maintaining high\naccuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic\nAdapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with\ndynamic scaling that captures sequential dependencies while emphasizing\ninformative temporal features and suppressing irrelevant variations. The ShA is\na lightweight adapter that refines representations within both textual and\nvisual encoders, ensuring consistency and efficiency. Additionally, we\nintegrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts\nfor visual and action unit-based textual inputs, enhancing semantic alignment\nbetween modalities and enabling efficient CLIP adaptation for dynamic tasks. We\nevaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving\ncompetitive performance compared to state-of-the-art methods while requiring\nfewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets\na new benchmark in resource-efficient DFER. The source code of the proposed\nPE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP ."}
{"id": "2503.17363", "pdf": "https://arxiv.org/pdf/2503.17363", "abs": "https://arxiv.org/abs/2503.17363", "authors": ["Yansi Li", "Jiahao Xu", "Tian Liang", "Xingyu Chen", "Zhiwei He", "Qiuzhi Liu", "Rui Wang", "Zhuosheng Zhang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field."}
{"id": "2503.16948", "pdf": "https://arxiv.org/pdf/2503.16948", "abs": "https://arxiv.org/abs/2503.16948", "authors": ["Yinhan Zhang", "Yue Ma", "Bingyuan Wang", "Qifeng Chen", "Zeyu Wang"], "title": "MagicColor: Multi-Instance Sketch Colorization", "categories": ["cs.CV"], "comment": null, "summary": "We present \\textit{MagicColor}, a diffusion-based framework for\nmulti-instance sketch colorization. The production of multi-instance 2D line\nart colorization adheres to an industry-standard workflow, which consists of\nthree crucial stages: the design of line art characters, the coloring of\nindividual objects, and the refinement process. The artists are required to\nrepeat the process of coloring each instance one by one, which is inaccurate\nand inefficient. Meanwhile, current generative methods fail to solve this task\ndue to the challenge of multi-instance pair data collection. To tackle these\nchallenges, we incorporate three technical designs to ensure precise character\ndetail transcription and achieve multi-instance sketch colorization in a single\nforward. Specifically, we first propose the self-play training strategy to\nsolve the lack of training data. Then we introduce an instance guider to feed\nthe color of the instance. To achieve accurate color matching, we present\nfine-grained color matching with edge loss to enhance visual quality. Equipped\nwith the proposed modules, MagicColor enables automatically transforming\nsketches into vividly-colored images with accurate consistency and\nmulti-instance control. Experiments on our collected datasets show that our\nmodel outperforms existing methods regarding chromatic precision. Specifically,\nour model critically automates the colorization process with zero manual\nadjustments, so novice users can produce stylistically consistent artwork by\nproviding reference instances and the original line art. Our code and\nadditional details are available at https://yinhan-zhang.github.io/color"}
{"id": "2503.16432", "pdf": "https://arxiv.org/pdf/2503.16432", "abs": "https://arxiv.org/abs/2503.16432", "authors": ["Young-Ho Bae", "Casey C. Bennett"], "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "36 pages", "summary": "This study investigates multimodal turn-taking prediction within human-agent\ninteractions (HAI), particularly focusing on cooperative gaming environments.\nIt comprises both model development and subsequent user study, aiming to refine\nour understanding and improve conversational dynamics in spoken dialogue\nsystems (SDSs). For the modeling phase, we introduce a novel transformer-based\ndeep learning (DL) model that simultaneously integrates multiple modalities -\ntext, vision, audio, and contextual in-game data to predict turn-taking events\nin real-time. Our model employs a Crossmodal Transformer architecture to\neffectively fuse information from these diverse modalities, enabling more\ncomprehensive turn-taking predictions. The model demonstrates superior\nperformance compared to baseline models, achieving 87.3% accuracy and 83.0%\nmacro F1 score. A human user study was then conducted to empirically evaluate\nthe turn-taking DL model in an interactive scenario with a virtual avatar while\nplaying the game \"Dont Starve Together\", comparing a control condition without\nturn-taking prediction (n=20) to an experimental condition with our model\ndeployed (n=40). Both conditions included a mix of English and Korean speakers,\nsince turn-taking cues are known to vary by culture. We then analyzed the\ninteraction quality, examining aspects such as utterance counts, interruption\nfrequency, and participant perceptions of the avatar. Results from the user\nstudy suggest that our multimodal turn-taking model not only enhances the\nfluidity and naturalness of human-agent conversations, but also maintains a\nbalanced conversational dynamic without significantly altering dialogue\nfrequency. The study provides in-depth insights into the influence of\nturn-taking abilities on user perceptions and interaction quality, underscoring\nthe potential for more contextually adaptive and responsive conversational\nagents."}
{"id": "2503.16963", "pdf": "https://arxiv.org/pdf/2503.16963", "abs": "https://arxiv.org/abs/2503.16963", "authors": ["Wei Zhang", "Mengting Ma", "Yizhen Jiang", "Rongrong Lian", "Zhenkai Wu", "Kangning Cui", "Xiaowen Ma"], "title": "Center-guided Classifier for Semantic Segmentation of Remote Sensing Images", "categories": ["cs.CV"], "comment": null, "summary": "Compared with natural images, remote sensing images (RSIs) have the unique\ncharacteristic. i.e., larger intraclass variance, which makes semantic\nsegmentation for remote sensing images more challenging. Moreover, existing\nsemantic segmentation models for remote sensing images usually employ a vanilla\nsoftmax classifier, which has three drawbacks: (1) non-direct supervision for\nthe pixel representations during training; (2) inadequate modeling ability of\nparametric softmax classifiers under large intraclass variance; and (3) opaque\nprocess of classification decision. In this paper, we propose a novel\nclassifier (called CenterSeg) customized for RSI semantic segmentation, which\nsolves the abovementioned problems with multiple prototypes, direct supervision\nunder Grassmann manifold, and interpretability strategy. Specifically, for each\nclass, our CenterSeg obtains local class centers by aggregating corresponding\npixel features based on ground-truth masks, and generates multiple prototypes\nthrough hard attention assignment and momentum updating. In addition, we\nintroduce the Grassmann manifold and constrain the joint embedding space of\npixel features and prototypes based on two additional regularization terms.\nEspecially, during the inference, CenterSeg can further provide\ninterpretability to the model by restricting the prototype as a sample of the\ntraining set. Experimental results on three remote sensing segmentation\ndatasets validate the effectiveness of the model. Besides the superior\nperformance, CenterSeg has the advantages of simplicity, lightweight,\ncompatibility, and interpretability. Code is available at\nhttps://github.com/xwmaxwma/rssegmentation."}
{"id": "2503.16433", "pdf": "https://arxiv.org/pdf/2503.16433", "abs": "https://arxiv.org/abs/2503.16433", "authors": ["Andrew Cho", "Jason M. Woo", "Brian Shi", "Aishwaryaa Udeshi", "Jonathan S. H. Woo"], "title": "The Application of MATEC (Multi-AI Agent Team Care) Framework in Sepsis Care", "categories": ["cs.HC", "cs.CL", "cs.MA"], "comment": "15 pages", "summary": "Under-resourced or rural hospitals have limited access to medical specialists\nand healthcare professionals, which can negatively impact patient outcomes in\nsepsis. To address this gap, we developed the MATEC (Multi-AI Agent Team Care)\nframework, which integrates a team of specialized AI agents for sepsis care.\nThe sepsis AI agent team includes five doctor agents, four health professional\nagents, and a risk prediction model agent, with an additional 33 doctor agents\navailable for consultations. Ten attending physicians at a teaching hospital\nevaluated this framework, spending approximately 40 minutes on the web-based\nMATEC application and participating in the 5-point Likert scale survey (rated\nfrom 1-unfavorable to 5-favorable). The physicians found the MATEC framework\nvery useful (Median=4, P=0.01), and very accurate (Median=4, P<0.01). This\npilot study demonstrates that a Multi-AI Agent Team Care framework (MATEC) can\npotentially be useful in assisting medical professionals, particularly in\nunder-resourced hospital settings."}
{"id": "2503.16964", "pdf": "https://arxiv.org/pdf/2503.16964", "abs": "https://arxiv.org/abs/2503.16964", "authors": ["Jiadong Tang", "Yu Gao", "Dianyi Yang", "Liqi Yan", "Yufeng Yue", "Yi Yang"], "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery."}
{"id": "2503.16457", "pdf": "https://arxiv.org/pdf/2503.16457", "abs": "https://arxiv.org/abs/2503.16457", "authors": ["Iago Alves Brito", "Julia Soares Dollis", "Fernanda Bufon Färber", "Pedro Schindler Freire Brasil Ribeiro", "Rafael Teixeira Sousa", "Arlindo Rodrigues Galvão Filho"], "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "The integration of large language models (LLMs) into virtual reality (VR)\nenvironments has opened new pathways for creating more immersive and\ninteractive digital humans. By leveraging the generative capabilities of LLMs\nalongside multimodal outputs such as facial expressions and gestures, virtual\nagents can simulate human-like personalities and emotions, fostering richer and\nmore engaging user experiences. This paper provides a comprehensive review of\nmethods for enabling digital humans to adopt nuanced personality traits,\nexploring approaches such as zero-shot, few-shot, and fine-tuning.\nAdditionally, it highlights the challenges of integrating LLM-driven\npersonality traits into VR, including computational demands, latency issues,\nand the lack of standardized evaluation frameworks for multimodal interactions.\nBy addressing these gaps, this work lays a foundation for advancing\napplications in education, therapy, and gaming, while fostering\ninterdisciplinary collaboration to redefine human-computer interaction in VR."}
{"id": "2503.16970", "pdf": "https://arxiv.org/pdf/2503.16970", "abs": "https://arxiv.org/abs/2503.16970", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Distilling Monocular Foundation Model for Fine-grained Depth Completion", "categories": ["cs.CV"], "comment": null, "summary": "Depth completion involves predicting dense depth maps from sparse LiDAR\ninputs. However, sparse depth annotations from sensors limit the availability\nof dense supervision, which is necessary for learning detailed geometric\nfeatures. In this paper, we propose a two-stage knowledge distillation\nframework that leverages powerful monocular foundation models to provide dense\nsupervision for depth completion. In the first stage, we introduce a\npre-training strategy that generates diverse training data from natural images,\nwhich distills geometric knowledge to depth completion. Specifically, we\nsimulate LiDAR scans by utilizing monocular depth and mesh reconstruction,\nthereby creating training data without requiring ground-truth depth. Besides,\nmonocular depth estimation suffers from inherent scale ambiguity in real-world\nsettings. To address this, in the second stage, we employ a scale- and\nshift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on\nreal-world datasets. Our two-stage distillation framework enables depth\ncompletion models to harness the strengths of monocular foundation models.\nExperimental results demonstrate that models trained with our two-stage\ndistillation framework achieve state-of-the-art performance, ranking\n\\textbf{first place} on the KITTI benchmark. Code is available at\nhttps://github.com/Sharpiless/DMD3C"}
{"id": "2503.16463", "pdf": "https://arxiv.org/pdf/2503.16463", "abs": "https://arxiv.org/abs/2503.16463", "authors": ["Zhoujian Sun", "Ziyi Liu", "Cheng Luo", "Jiebin Chu", "Zhengxing Huang"], "title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "30 pages", "summary": "Recent advances in large language models (LLMs) have shown promising results\nin medical diagnosis, with some studies indicating superior performance\ncompared to human physicians in specific scenarios. However, the diagnostic\ncapabilities of LLMs are often overestimated, as their performance\nsignificantly deteriorates in interactive diagnostic settings that require\nactive information gathering. This study investigates the underlying mechanisms\nbehind the performance degradation phenomenon and proposes a solution. We\nidentified that the primary deficiency of LLMs lies in the initial diagnosis\nphase, particularly in information-gathering efficiency and initial diagnosis\nformation, rather than in the subsequent differential diagnosis phase. To\naddress this limitation, we developed a plug-and-play method enhanced (PPME)\nLLM agent, leveraging over 3.5 million electronic medical records from Chinese\nand American healthcare facilities. Our approach integrates specialized models\nfor initial disease diagnosis and inquiry into the history of the present\nillness, trained through supervised and reinforcement learning techniques. The\nexperimental results indicate that the PPME LLM achieved over 30% improvement\ncompared to baselines. The final diagnostic accuracy of the PPME LLM in\ninteractive diagnostic scenarios approached levels comparable to those achieved\nusing complete clinical data. These findings suggest a promising potential for\ndeveloping autonomous diagnostic systems, although further validation studies\nare needed."}
{"id": "2503.16973", "pdf": "https://arxiv.org/pdf/2503.16973", "abs": "https://arxiv.org/abs/2503.16973", "authors": ["Wentao Jiang", "Jingya Wang", "Haotao Lu", "Kaiyang Ji", "Baoxiong Jia", "Siyuan Huang", "Ye Shi"], "title": "ARFlow: Human Action-Reaction Flow Matching with Physical Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Computer Vision and Pattern Recognition (cs.CV); Artificial\n  Intelligence (cs.AI)", "summary": "Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics."}
{"id": "2503.16464", "pdf": "https://arxiv.org/pdf/2503.16464", "abs": "https://arxiv.org/abs/2503.16464", "authors": ["Shinnosuke Sawano", "Satoshi Kodera"], "title": "Human-Centered AI in Multidisciplinary Medical Discussions: Evaluating the Feasibility of a Chat-Based Approach to Case Assessment", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "11 pages, 2 figures, 3 tables, 2 supplemental figures", "summary": "In this study, we investigate the feasibility of using a human-centered\nartificial intelligence (AI) chat platform where medical specialists\ncollaboratively assess complex cases. As the target population for this\nplatform, we focus on patients with cardiovascular diseases who are in a state\nof multimorbidity, that is, suffering from multiple chronic conditions. We\nevaluate simulated cases with multiple diseases using a chat application by\ncollaborating with physicians to assess feasibility, efficiency gains through\nAI utilization, and the quantification of discussion content. We constructed\nsimulated cases based on past case reports, medical errors reports and complex\ncases of cardiovascular diseases experienced by the physicians. The analysis of\ndiscussions across five simulated cases demonstrated a significant reduction in\nthe time required for summarization using AI, with an average reduction of\n79.98\\%. Additionally, we examined hallucination rates in AI-generated\nsummaries used in multidisciplinary medical discussions. The overall\nhallucination rate ranged from 1.01\\% to 5.73\\%, with an average of 3.62\\%,\nwhereas the harmful hallucination rate varied from 0.00\\% to 2.09\\%, with an\naverage of 0.49\\%. Furthermore, morphological analysis demonstrated that\nmultidisciplinary assessments enabled a more complex and detailed\nrepresentation of medical knowledge compared with single physician assessments.\nWe examined structural differences between multidisciplinary and single\nphysician assessments using centrality metrics derived from the knowledge\ngraph. In this study, we demonstrated that AI-assisted summarization\nsignificantly reduced the time required for medical discussions while\nmaintaining structured knowledge representation. These findings can support the\nfeasibility of AI-assisted chat-based discussions as a human-centered approach\nto multidisciplinary medical decision-making."}
{"id": "2503.16975", "pdf": "https://arxiv.org/pdf/2503.16975", "abs": "https://arxiv.org/abs/2503.16975", "authors": ["Xiaofeng Mao", "Yuefeng Chen", "Rong Zhang", "Hui Xue", "Zhao Li", "Hang Su"], "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust."}
{"id": "2503.16480", "pdf": "https://arxiv.org/pdf/2503.16480", "abs": "https://arxiv.org/abs/2503.16480", "authors": ["Yara Kyrychenko", "Jon Roozenbeek", "Brandon Davidson", "Sander van der Linden", "Ramit Debnath"], "title": "Human Preferences for Constructive Interactions in Language Model Alignment", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "1 Figure, 1 Table, 11 pages", "summary": "As large language models (LLMs) enter the mainstream, aligning them to foster\nconstructive dialogue rather than exacerbate societal divisions is critical.\nUsing an individualized and multicultural alignment dataset of over 7,500\nconversations of individuals from 74 countries engaging with 21 LLMs, we\nexamined how linguistic attributes linked to constructive interactions are\nreflected in human preference data used for training AI. We found that users\nconsistently preferred well-reasoned and nuanced responses while rejecting\nthose high in personal storytelling. However, users who believed that AI should\nreflect their values tended to place less preference on reasoning in LLM\nresponses and more on curiosity. Encouragingly, we observed that users could\nset the tone for how constructive their conversation would be, as LLMs mirrored\nlinguistic attributes, including toxicity, in user queries."}
{"id": "2503.16976", "pdf": "https://arxiv.org/pdf/2503.16976", "abs": "https://arxiv.org/abs/2503.16976", "authors": ["Weihao Yu", "Xiaoqing Guo", "Chenxin Li", "Yifan Liu", "Yixuan Yuan"], "title": "GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "IPMI2025", "summary": "Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data."}
{"id": "2503.16498", "pdf": "https://arxiv.org/pdf/2503.16498", "abs": "https://arxiv.org/abs/2503.16498", "authors": ["Enzo Sinacola", "Arnault Pachot", "Thierry Petit"], "title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Accepted, proceedings of the 17th International Conference on Machine\n  Learning and Computing", "summary": "Large Language Models (LLMs) offer a promising alternative to traditional\nsurvey methods, potentially enhancing efficiency and reducing costs. In this\nstudy, we use LLMs to create virtual populations that answer survey questions,\nenabling us to predict outcomes comparable to human responses. We evaluate\nseveral LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the\nLlama and Mistral models-comparing their performance to that of a traditional\nRandom Forests algorithm using demographic data from the World Values Survey\n(WVS). LLMs demonstrate competitive performance overall, with the significant\nadvantage of requiring no additional training data. However, they exhibit\nbiases when predicting responses for certain religious and population groups,\nunderperforming in these areas. On the other hand, Random Forests demonstrate\nstronger performance than LLMs when trained with sufficient data. We observe\nthat removing censorship mechanisms from LLMs significantly improves predictive\naccuracy, particularly for underrepresented demographic segments where censored\nmodels struggle. These findings highlight the importance of addressing biases\nand reconsidering censorship approaches in LLMs to enhance their reliability\nand fairness in public opinion research."}
{"id": "2503.16979", "pdf": "https://arxiv.org/pdf/2503.16979", "abs": "https://arxiv.org/abs/2503.16979", "authors": ["Jinbo Yan", "Rui Peng", "Zhiyan Wang", "Luyang Tang", "Jiayu Yang", "Jie Liang", "Jiahao Wu", "Ronggang Wang"], "title": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Building Free-Viewpoint Videos in a streaming manner offers the advantage of\nrapid responsiveness compared to offline training methods, greatly enhancing\nuser experience. However, current streaming approaches face challenges of high\nper-frame reconstruction time (10s+) and error accumulation, limiting their\nbroader application. In this paper, we propose Instant Gaussian Stream (IGS), a\nfast and generalizable streaming framework, to address these issues. First, we\nintroduce a generalized Anchor-driven Gaussian Motion Network, which projects\nmulti-view 2D motion features into 3D space, using anchor points to drive the\nmotion of all Gaussians. This generalized Network generates the motion of\nGaussians for each target frame in the time required for a single inference.\nSecond, we propose a Key-frame-guided Streaming Strategy that refines each key\nframe, enabling accurate reconstruction of temporally complex scenes while\nmitigating error accumulation. We conducted extensive in-domain and\ncross-domain evaluations, demonstrating that our approach can achieve streaming\nwith a average per-frame reconstruction time of 2s+, alongside a enhancement in\nview synthesis quality."}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505", "abs": "https://arxiv.org/abs/2503.16505", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "title": "Scalable Evaluation of Online Moderation Strategies via Synthetic Simulations", "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "25 pages, 6 tables, 9 figures", "summary": "Despite the ever-growing importance of online moderation, there has been no\nlarge-scale study evaluating the effectiveness of alternative moderation\nstrategies. This is largely due to the lack of appropriate datasets, and the\ndifficulty of getting human discussants, moderators, and evaluators involved in\nmultiple experiments. In this paper, we propose a methodology for leveraging\nsynthetic experiments performed exclusively by Large Language Models (LLMs) to\ninitially bypass the need for human participation in experiments involving\nonline moderation. We evaluate six LLM moderation configurations; two currently\nused real-life moderation strategies (guidelines issued for human moderators\nfor online moderation and real-life facilitation), two baseline strategies\n(guidelines elicited for LLM alignment work, and LLM moderation with minimal\nprompting) a baseline with no moderator at all, as well as our own proposed\nstrategy inspired by a Reinforcement Learning (RL) formulation of the problem.\nWe find that our own moderation strategy significantly outperforms established\nmoderation guidelines, as well as out-of-the-box LLM moderation. We also find\nthat smaller LLMs, with less intensive instruction-tuning, can create more\nvaried discussions than larger models. In order to run these experiments, we\ncreate and release an efficient, purpose-built, open-source Python framework,\ndubbed \"SynDisco\" to easily simulate hundreds of discussions using LLM\nuser-agents and moderators. Additionally, we release the Virtual Moderation\nDataset (VMD), a large dataset of LLM-generated and LLM-annotated discussions,\ngenerated by three families of open-source LLMs accompanied by an exploratory\nanalysis of the dataset."}
{"id": "2503.16980", "pdf": "https://arxiv.org/pdf/2503.16980", "abs": "https://arxiv.org/abs/2503.16980", "authors": ["Haichao Zhang", "Zhuowei Li", "Dimitris Metaxas", "Yun Fu"], "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs."}
{"id": "2503.16509", "pdf": "https://arxiv.org/pdf/2503.16509", "abs": "https://arxiv.org/abs/2503.16509", "authors": ["Deep Patel", "Panthadeep Bhattacharjee", "Amit Reza", "Priodyuti Pradhan"], "title": "Earthquake Response Analysis with AI", "categories": ["cs.SI", "cs.CL", "cs.CY", "nlin.AO"], "comment": "10 pages, 5 figures", "summary": "A timely and effective response is crucial to minimize damage and save lives\nduring natural disasters like earthquakes. Microblogging platforms,\nparticularly Twitter, have emerged as valuable real-time information sources\nfor such events. This work explores the potential of leveraging Twitter data\nfor earthquake response analysis. We develop a machine learning (ML) framework\nby incorporating natural language processing (NLP) techniques to extract and\nanalyze relevant information from tweets posted during earthquake events. The\napproach primarily focuses on extracting location data from tweets to identify\naffected areas, generating severity maps, and utilizing WebGIS to display\nvaluable information. The insights gained from this analysis can aid emergency\nresponders, government agencies, humanitarian organizations, and NGOs in\nenhancing their disaster response strategies and facilitating more efficient\nresource allocation during earthquake events."}
{"id": "2503.16983", "pdf": "https://arxiv.org/pdf/2503.16983", "abs": "https://arxiv.org/abs/2503.16983", "authors": ["Xu Zhang", "Hao Zhou", "Haoming Qin", "Xiaobin Lu", "Jiaxing Yan", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "title": "Enabling Versatile Controls for Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Codes and Supplementary Material:\n  http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl", "summary": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl."}
{"id": "2503.16545", "pdf": "https://arxiv.org/pdf/2503.16545", "abs": "https://arxiv.org/abs/2503.16545", "authors": ["Xinyan Chen", "Jiaxin Ge", "Hongming Dai", "Qiang Zhou", "Qiuxuan Feng", "Jingtong Hu", "Yizhou Wang", "Jiaming Liu", "Shanghang Zhang"], "title": "EmpathyAgent: Can Embodied Agents Conduct Empathetic Actions?", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Empathy is fundamental to human interactions, yet it remains unclear whether\nembodied agents can provide human-like empathetic support. Existing works have\nstudied agents' tasks solving and social interactions abilities, but whether\nagents can understand empathetic needs and conduct empathetic behaviors remains\noverlooked. To address this, we introduce EmpathyAgent, the first benchmark to\nevaluate and enhance agents' empathetic actions across diverse scenarios.\nEmpathyAgent contains 10,000 multimodal samples with corresponding empathetic\ntask plans and three different challenges. To systematically evaluate the\nagents' empathetic actions, we propose an empathy-specific evaluation suite\nthat evaluates the agents' empathy process. We benchmark current models and\nfound that exhibiting empathetic actions remains a significant challenge.\nMeanwhile, we train Llama3-8B using EmpathyAgent and find it can potentially\nenhance empathetic behavior. By establishing a standard benchmark for\nevaluating empathetic actions, we hope to advance research in empathetic\nembodied agents. Our code and data are publicly available at\nhttps://github.com/xinyan-cxy/EmpathyAgent."}
{"id": "2503.16997", "pdf": "https://arxiv.org/pdf/2503.16997", "abs": "https://arxiv.org/abs/2503.16997", "authors": ["Qinghe Ma", "Jian Zhang", "Zekun Li", "Lei Qi", "Qian Yu", "Yinghuan Shi"], "title": "Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Large pretrained visual foundation models exhibit impressive general\ncapabilities. However, the extensive prior knowledge inherent in these models\ncan sometimes be a double-edged sword when adapting them to downstream tasks in\nspecific domains. In the context of semi-supervised medical image segmentation\nwith domain shift, foundation models like MedSAM tend to make overconfident\npredictions, some of which are incorrect. The error accumulation hinders the\neffective utilization of unlabeled data and limits further improvements. In\nthis paper, we introduce a Synergistic training framework for Foundation and\nConventional models (SynFoC) to address the issue. We observe that a\nconventional model trained from scratch has the ability to correct the\nhigh-confidence mispredictions of the foundation model, while the foundation\nmodel can supervise it with high-quality pseudo-labels in the early training\nstages. Furthermore, to enhance the collaborative training effectiveness of\nboth models and promote reliable convergence towards optimization, the\nconsensus-divergence consistency regularization is proposed. We demonstrate the\nsuperiority of our method across four public multi-domain datasets. In\nparticular, our method improves the Dice score by 10.31\\% on the Prostate\ndataset. Our code is available at https://github.com/MQinghe/SynFoC ."}
{"id": "2503.16563", "pdf": "https://arxiv.org/pdf/2503.16563", "abs": "https://arxiv.org/abs/2503.16563", "authors": ["Aahan Singh", "Engin Tekin", "Maryam Nadeem", "Nancy A. ElNaker", "Mohammad Amaan Sayeed", "Natalia Vassilieva", "Boulbaba Ben Amor"], "title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Revolutionizing drug discovery demands more than just understanding molecular\ninteractions - it requires generative models that can design novel ligands\ntailored to specific biological targets. While chemical Language Models (cLMs)\nhave made strides in learning molecular properties, most fail to incorporate\ntarget-specific insights, restricting their ability to drive de-novo ligand\ngeneration. Chem42, a cutting-edge family of generative chemical Language\nModels, is designed to bridge this gap. By integrating atomic-level\ninteractions with multimodal inputs from Prot42, a complementary protein\nLanguage Model, Chem42 achieves a sophisticated cross-modal representation of\nmolecular structures, interactions, and binding patterns. This innovative\nframework enables the creation of structurally valid, synthetically accessible\nligands with enhanced target specificity. Evaluations across diverse protein\ntargets confirm that Chem42 surpasses existing approaches in chemical validity,\ntarget-aware design, and predicted binding affinity. By reducing the search\nspace of viable drug candidates, Chem42 could accelerate the drug discovery\npipeline, offering a powerful generative AI tool for precision medicine. Our\nChem42 models set a new benchmark in molecule property prediction, conditional\nmolecule generation, and target-aware ligand design. The models are publicly\navailable at huggingface.co/inceptionai."}
{"id": "2503.17027", "pdf": "https://arxiv.org/pdf/2503.17027", "abs": "https://arxiv.org/abs/2503.17027", "authors": ["Ziteng Cui", "Jianfei Yang", "Tatsuya Harada"], "title": "RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark", "categories": ["cs.CV"], "comment": "23 pages, 17 figures, extension of ECCV 2024 work: arXiv:2408.14802", "summary": "In the computer vision community, the preference for pre-training visual\nmodels has largely shifted toward sRGB images due to their ease of acquisition\nand compact storage. However, camera RAW images preserve abundant physical\ndetails across diverse real-world scenarios. Despite this, most existing visual\nperception methods that utilize RAW data directly integrate image signal\nprocessing (ISP) stages with subsequent network modules, often overlooking\npotential synergies at the model level. Building on recent advances in\nadapter-based methodologies in both NLP and computer vision, we propose\nRAW-Adapter, a novel framework that incorporates learnable ISP modules as\ninput-level adapters to adjust RAW inputs. At the same time, it employs\nmodel-level adapters to seamlessly bridge ISP processing with high-level\ndownstream architectures. Moreover, RAW-Adapter serves as a general framework\napplicable to various computer vision frameworks.\n  Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based\ncommon corruptions, including lightness degradations, weather effects,\nblurriness, camera imaging degradations, and variations in camera color\nresponse. Using this benchmark, we systematically compare the performance of\nRAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based\nhigh-level vision algorithms. Additionally, we propose a RAW-based data\naugmentation strategy to further enhance RAW-Adapter's performance and improve\nits out-of-domain (OOD) generalization ability. Extensive experiments\nsubstantiate the effectiveness and efficiency of RAW-Adapter, highlighting its\nrobust performance across diverse scenarios."}
{"id": "2503.16565", "pdf": "https://arxiv.org/pdf/2503.16565", "abs": "https://arxiv.org/abs/2503.16565", "authors": ["Kirill Vishniakov", "Boulbaba Ben Amor", "Engin Tekin", "Nancy A. ElNaker", "Karthik Viswanathan", "Aleksandr Medvedev", "Aahan Singh", "Maryam Nadeem", "Mohammad Amaan Sayeed", "Praveenkumar Kanithi", "Tiago Magalhaes", "Natalia Vassilieva", "Dwarikanath Mahapatra", "Marco Pimentel", "and Shadab Khan"], "title": "Gene42: Long-Range Genomic Foundation Model With Dense Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Gene42, a novel family of Genomic Foundation Models (GFMs)\ndesigned to manage context lengths of up to 192,000 base pairs (bp) at a\nsingle-nucleotide resolution. Gene42 models utilize a decoder-only\n(LLaMA-style) architecture with a dense self-attention mechanism. Initially\ntrained on fixed-length sequences of 4,096 bp, our models underwent continuous\npretraining to extend the context length to 192,000 bp. This iterative\nextension allowed for the comprehensive processing of large-scale genomic data\nand the capture of intricate patterns and dependencies within the human genome.\nGene42 is the first dense attention model capable of handling such extensive\nlong context lengths in genomics, challenging state-space models that often\nrely on convolutional operators among other mechanisms. Our pretrained models\nexhibit notably low perplexity values and high reconstruction accuracy,\nhighlighting their strong ability to model genomic data. Extensive experiments\non various genomic benchmarks have demonstrated state-of-the-art performance\nacross multiple tasks, including biotype classification, regulatory region\nidentification, chromatin profiling prediction, variant pathogenicity\nprediction, and species classification. The models are publicly available at\nhuggingface.co/inceptionai."}
{"id": "2503.17029", "pdf": "https://arxiv.org/pdf/2503.17029", "abs": "https://arxiv.org/abs/2503.17029", "authors": ["Junjie Hu", "Shuyong Gao", "Qianyu Guo", "Yan Wang", "Qishan Wang", "Yuang Feng", "Wenqiang Zhang"], "title": "AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing Painting Process", "categories": ["cs.CV"], "comment": null, "summary": "Humans can intuitively decompose an image into a sequence of strokes to\ncreate a painting, yet existing methods for generating drawing processes are\nlimited to specific data types and often rely on expensive human-annotated\ndatasets. We propose a novel self-supervised framework for generating drawing\nprocesses from any type of image, treating the task as a video generation\nproblem. Our approach reverses the drawing process by progressively removing\nstrokes from a reference image, simulating a human-like creation sequence.\nCrucially, our method does not require costly datasets of real human drawing\nprocesses; instead, we leverage depth estimation and stroke rendering to\nconstruct a self-supervised dataset. We model human drawings as \"refinement\"\nand \"layering\" processes and introduce depth fusion layers to enable video\ngeneration models to learn and replicate human drawing behavior. Extensive\nexperiments validate the effectiveness of our approach, demonstrating its\nability to generate realistic drawings without the need for real drawing\nprocess data."}
{"id": "2503.16689", "pdf": "https://arxiv.org/pdf/2503.16689", "abs": "https://arxiv.org/abs/2503.16689", "authors": ["Tianze Luo", "Xingchen Miao", "Wenbo Duan"], "title": "WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to the main conference of NAACL 2025. The codes are\n  available at https://github.com/luotianze666/WaveFM", "summary": "Flow matching offers a robust and stable approach to training diffusion\nmodels. However, directly applying flow matching to neural vocoders can result\nin subpar audio quality. In this work, we present WaveFM, a reparameterized\nflow matching model for mel-spectrogram conditioned speech synthesis, designed\nto enhance both sample quality and generation speed for diffusion vocoders.\nSince mel-spectrograms represent the energy distribution of waveforms, WaveFM\nadopts a mel-conditioned prior distribution instead of a standard Gaussian\nprior to minimize unnecessary transportation costs during synthesis. Moreover,\nwhile most diffusion vocoders rely on a single loss function, we argue that\nincorporating auxiliary losses, including a refined multi-resolution STFT loss,\ncan further improve audio quality. To speed up inference without degrading\nsample quality significantly, we introduce a tailored consistency distillation\nmethod for WaveFM. Experiment results demonstrate that our model achieves\nsuperior performance in both quality and efficiency compared to previous\ndiffusion vocoders, while enabling waveform generation in a single inference\nstep."}
{"id": "2503.17032", "pdf": "https://arxiv.org/pdf/2503.17032", "abs": "https://arxiv.org/abs/2503.17032", "authors": ["Jianchuan Chen", "Jingchuan Hu", "Gaige Wang", "Zhonghua Jiang", "Tiansong Zhou", "Zhiwen Chen", "Chengfei Lv"], "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025, project page:\n  https://PixelAI-Team.github.io/TaoAvatar", "summary": "Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro."}
{"id": "2503.16718", "pdf": "https://arxiv.org/pdf/2503.16718", "abs": "https://arxiv.org/abs/2503.16718", "authors": ["Massa Baali", "Xiang Li", "Hao Chen", "Rita Singh", "Bhiksha Raj"], "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization", "categories": ["cs.SD", "cs.CL", "cs.LG"], "comment": null, "summary": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released."}
{"id": "2503.17034", "pdf": "https://arxiv.org/pdf/2503.17034", "abs": "https://arxiv.org/abs/2503.17034", "authors": ["Stephen Lloyd-Brown", "Susan Francis", "Caroline Hoad", "Penny Gowland", "Karen Mullinger", "Andrew French", "Xin Chen"], "title": "An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ISBI 2025", "summary": "An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario."}
{"id": "2503.16731", "pdf": "https://arxiv.org/pdf/2503.16731", "abs": "https://arxiv.org/abs/2503.16731", "authors": ["Zhaoqin \"Richie\" Li", "Sicheng Chen"], "title": "Design and Implementation of an FPGA-Based Tiled Matrix Multiplication Accelerator for Transformer Self-Attention on the Xilinx KV260 SoM", "categories": ["cs.AR", "cs.CL", "cs.LG", "B.7.1; C.1.4"], "comment": "7 pages, 4 figures, 2 tables. Prepared in ACM conference style.\n  Preprint under review", "summary": "Transformer-based LLMs spend most of their compute in large matrix\nmultiplications for attention and feed-forward layers. Recognizing that the Q,\nK, and V linear projections within the Multi-Head Self-Attention (MHA) module\nrepresent a critical computational bottleneck, we strategically focused our\nefforts on accelerating these operations. We present a tiled matrix\nmultiplication accelerator optimized for such workloads on a Xilinx KV260\non-board FPGA. Key innovations include persistent on-chip storage for one\nmatrix operand, two-level tiling for data reuse, and a systolic-like unrolled\ncompute engine. Implemented via high-level synthesis (HLS) and integrated with\nDistilBERT for Q, K, V projections, our accelerator achieves significant\nspeedup and energy efficiency gains over CPU baselines. Standalone GEMM\nbenchmarks show up to a 7x speedup over an ARM CPU (PyTorch) and ~200x over\nnaive numpy, with a throughput of up to 3.1 GFLOPs on 768x3072 matrices.\nAlthough the overall end-to-end DistilBERT acceleration is more modest, our\nresults validate the potential of FPGA-based acceleration for critical\ncomponents of Transformer models."}
{"id": "2503.17044", "pdf": "https://arxiv.org/pdf/2503.17044", "abs": "https://arxiv.org/abs/2503.17044", "authors": ["Chandan Yeshwanth", "David Rozenberszki", "Angela Dai"], "title": "ExCap3D: Expressive 3D Scene Understanding via Object Captioning with Varying Detail", "categories": ["cs.CV"], "comment": "Project page: https://cy94.github.io/excap3d/, Video:\n  https://www.youtube.com/watch?v=SQRV1l_0oY0", "summary": "Generating text descriptions of objects in 3D indoor scenes is an important\nbuilding block of embodied understanding. Existing methods do this by\ndescribing objects at a single level of detail, which often does not capture\nfine-grained details such as varying textures, materials, and shapes of the\nparts of objects. We propose the task of expressive 3D captioning: given an\ninput 3D scene, describe objects at multiple levels of detail: a high-level\nobject description, and a low-level description of the properties of its parts.\nTo produce such captions, we present ExCap3D, an expressive 3D captioning model\nwhich takes as input a 3D scan, and for each detected object in the scan,\ngenerates a fine-grained collective description of the parts of the object,\nalong with an object-level description conditioned on the part-level\ndescription. We design ExCap3D to encourage semantic consistency between the\ngenerated text descriptions, as well as textual similarity in the latent space,\nto further increase the quality of the generated captions. To enable this task,\nwe generated the ExCap3D Dataset by leveraging a visual-language model (VLM)\nfor multi-view captioning. The ExCap3D Dataset contains captions on the\nScanNet++ dataset with varying levels of detail, comprising 190k text\ndescriptions of 34k 3D objects in 947 indoor scenes. Our experiments show that\nthe object- and part-level of detail captions generated by ExCap3D are of\nhigher quality than those produced by state-of-the-art methods, with a Cider\nscore improvement of 17% and 124% for object- and part-level details\nrespectively. Our code, dataset and models will be made publicly available."}
{"id": "2503.16833", "pdf": "https://arxiv.org/pdf/2503.16833", "abs": "https://arxiv.org/abs/2503.16833", "authors": ["Luxi He", "Xiangyu Qi", "Michel Liao", "Inyoung Cheong", "Prateek Mittal", "Danqi Chen", "Peter Henderson"], "title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CY", "eess.AS"], "comment": null, "summary": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs."}
{"id": "2503.17050", "pdf": "https://arxiv.org/pdf/2503.17050", "abs": "https://arxiv.org/abs/2503.17050", "authors": ["Yuang Feng", "Shuyong Gao", "Fuzhen Yan", "Yicheng Song", "Lingyi Hong", "Junjie Hu", "Wenqiang Zhang"], "title": "Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos", "categories": ["cs.CV"], "comment": null, "summary": "Video Camouflaged Object Detection (VCOD) aims to segment objects whose\nappearances closely resemble their surroundings, posing a challenging and\nemerging task. Existing vision models often struggle in such scenarios due to\nthe indistinguishable appearance of camouflaged objects and the insufficient\nexploitation of dynamic information in videos. To address these challenges, we\npropose an end-to-end VCOD framework inspired by human memory-recognition,\nwhich leverages historical video information by integrating memory reference\nframes for camouflaged sequence processing. Specifically, we design a\ndual-purpose decoder that simultaneously generates predicted masks and scores,\nenabling reference frame selection based on scores while introducing auxiliary\nsupervision to enhance feature extraction.Furthermore, this study introduces a\nnovel reference-guided multilevel asymmetric attention mechanism, effectively\nintegrating long-term reference information with short-term motion cues for\ncomprehensive feature extraction. By combining these modules, we develop the\nScoring, Remember, and Reference (SRR) framework, which efficiently extracts\ninformation to locate targets and employs memory guidance to improve subsequent\nprocessing. With its optimized module design and effective utilization of video\ndata, our model achieves significant performance improvements, surpassing\nexisting approaches by 10% on benchmark datasets while requiring fewer\nparameters (54M) and only a single pass through the video. The code will be\nmade publicly available."}
{"id": "2503.16851", "pdf": "https://arxiv.org/pdf/2503.16851", "abs": "https://arxiv.org/abs/2503.16851", "authors": ["Zeqing He", "Zhibo Wang", "Huiyu Xu", "Kui Ren"], "title": "Towards LLM Guardrails via Sparse Representation Steering", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nnatural language generation tasks, yet their uncontrolled outputs pose\nsignificant ethical and safety risks. Recently, representation engineering\nmethods have shown promising results in steering model behavior by modifying\nthe rich semantic information encoded in activation vectors. However, due to\nthe difficulty of precisely disentangling semantic directions within\nhigh-dimensional representation space, existing approaches suffer from three\nmajor limitations: lack of fine-grained control, quality degradation of\ngenerated content, and poor interpretability. To address these challenges, we\npropose a sparse encoding-based representation engineering method, named SRE,\nwhich decomposes polysemantic activations into a structured, monosemantic\nfeature space. By leveraging sparse autoencoding, our approach isolates and\nadjusts only task-specific sparse feature dimensions, enabling precise and\ninterpretable steering of model behavior while preserving content quality. We\nvalidate our method on three critical domains, i.e., safety, fairness, and\ntruthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show\nthat SRE achieves superior controllability while maintaining the overall\nquality of generated content (i.e., controllability and quality), demonstrating\nits effectiveness as a fine-grained and interpretable activation steering\nframework."}
{"id": "2503.17069", "pdf": "https://arxiv.org/pdf/2503.17069", "abs": "https://arxiv.org/abs/2503.17069", "authors": ["Yufei Shi", "Weilong Yan", "Gang Xu", "Yumeng Li", "Yuchen Li", "Zhenxi Li", "Fei Richard Yu", "Ming Li", "Si Yong Yeo"], "title": "PVChat: Personalized Video Chat with One-Shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs."}
{"id": "2503.16870", "pdf": "https://arxiv.org/pdf/2503.16870", "abs": "https://arxiv.org/abs/2503.16870", "authors": ["Anshumann", "Mohd Abbas Zaidi", "Akhil Kedia", "Jinwoo Ahn", "Taehwak Kwon", "Kangwook Lee", "Haejun Lee", "Joohyung Lee"], "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution", "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."}
{"id": "2503.17071", "pdf": "https://arxiv.org/pdf/2503.17071", "abs": "https://arxiv.org/abs/2503.17071", "authors": ["Pablo Garcia-Fernandez", "Lorenzo Vaquero", "Mingxuan Liu", "Feng Xue", "Daniel Cores", "Nicu Sebe", "Manuel Mucientes", "Elisa Ricci"], "title": "Superpowering Open-Vocabulary Object Detectors for X-ray Vision", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO."}
{"id": "2503.16875", "pdf": "https://arxiv.org/pdf/2503.16875", "abs": "https://arxiv.org/abs/2503.16875", "authors": ["Jiangcheng Qin", "Xueyuan Zhang", "Baisong Liu", "Jiangbo Qian", "Yangyang Wang"], "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation", "categories": ["cs.IR", "cs.CL", "cs.DC"], "comment": null, "summary": "Accurately predicting click-through rates (CTR) under stringent privacy\nconstraints poses profound challenges, particularly when user-item interactions\nare sparse and fragmented across domains. Conventional cross-domain CTR (CCTR)\nmethods frequently assume homogeneous feature spaces and rely on centralized\ndata sharing, neglecting complex inter-domain discrepancies and the subtle\ntrade-offs imposed by privacy-preserving protocols. Here, we present Federated\nCross-Domain CTR Prediction with Large Language Model Augmentation\n(FedCCTR-LM), a federated framework engineered to address these limitations by\nsynchronizing data augmentation, representation disentanglement, and adaptive\nprivacy protection. Our approach integrates three core innovations. First, the\nPrivacy-Preserving Augmentation Network (PrivAugNet) employs large language\nmodels to enrich user and item representations and expand interaction\nsequences, mitigating data sparsity and feature incompleteness. Second, the\nIndependent Domain-Specific Transformer with Contrastive Learning (IDST-CL)\nmodule disentangles domain-specific and shared user preferences, employing\nintra-domain representation alignment (IDRA) and crossdomain representation\ndisentanglement (CDRD) to refine the learned embeddings and enhance knowledge\ntransfer across domains. Finally, the Adaptive Local Differential Privacy\n(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal\nbalance between rigorous privacy guarantees and predictive accuracy. Empirical\nevaluations on four real-world datasets demonstrate that FedCCTR-LM\nsubstantially outperforms existing baselines, offering robust,\nprivacy-preserving, and generalizable cross-domain CTR prediction in\nheterogeneous, federated environments."}
{"id": "2503.17074", "pdf": "https://arxiv.org/pdf/2503.17074", "abs": "https://arxiv.org/abs/2503.17074", "authors": ["Vittorio Pippi", "Fabio Quattrini", "Silvia Cascianelli", "Alessio Tonioni", "Rita Cucchiara"], "title": "Zero-Shot Styled Text Image Generation, but Make It Autoregressive", "categories": ["cs.CV"], "comment": "Accepted at CVPR2025", "summary": "Styled Handwritten Text Generation (HTG) has recently received attention from\nthe computer vision and document analysis communities, which have developed\nseveral solutions, either GAN- or diffusion-based, that achieved promising\nresults. Nonetheless, these strategies fail to generalize to novel styles and\nhave technical constraints, particularly in terms of maximum output length and\ntraining efficiency. To overcome these limitations, in this work, we propose a\nnovel framework for text image generation, dubbed Emuru. Our approach leverages\na powerful text image representation model (a variational autoencoder) combined\nwith an autoregressive Transformer. Our approach enables the generation of\nstyled text images conditioned on textual content and style examples, such as\nspecific fonts or handwriting styles. We train our model solely on a diverse,\nsynthetic dataset of English text rendered in over 100,000 typewritten and\ncalligraphy fonts, which gives it the capability to reproduce unseen styles\n(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,\nEmuru is the first autoregressive model for HTG, and the first designed\nspecifically for generalization to novel styles. Moreover, our model generates\nimages without background artifacts, which are easier to use for downstream\napplications. Extensive evaluation on both typewritten and handwritten,\nany-length text image generation scenarios demonstrates the effectiveness of\nour approach."}
{"id": "2503.16974", "pdf": "https://arxiv.org/pdf/2503.16974", "abs": "https://arxiv.org/abs/2503.16974", "authors": ["Julian Junyan Wang", "Victor Xiaoqi Wang"], "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks", "categories": ["q-fin.GN", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": "96 pages", "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks."}
{"id": "2503.17076", "pdf": "https://arxiv.org/pdf/2503.17076", "abs": "https://arxiv.org/abs/2503.17076", "authors": ["Victor Besnier", "Mickael Chen", "David Hurych", "Eduardo Valle", "Matthieu Cord"], "title": "Halton Scheduler For Masked Generative Image Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and\nefficient image generation framework, able to deliver high-quality visuals with\nlow inference costs. However, MaskGIT's token unmasking scheduler, an essential\ncomponent of the framework, has not received the attention it deserves. We\nanalyze the sampling objective in MaskGIT, based on the mutual information\nbetween tokens, and elucidate its shortcomings. We then propose a new sampling\nstrategy based on our Halton scheduler instead of the original Confidence\nscheduler. More precisely, our method selects the token's position according to\na quasi-random, low-discrepancy Halton sequence. Intuitively, that method\nspreads the tokens spatially, progressively covering the image uniformly at\neach step. Our analysis shows that it allows reducing non-recoverable sampling\nerrors, leading to simpler hyper-parameters tuning and better quality images.\nOur scheduler does not require retraining or noise injection and may serve as a\nsimple drop-in replacement for the original sampling strategy. Evaluation of\nboth class-to-image synthesis on ImageNet and text-to-image generation on the\nCOCO dataset demonstrates that the Halton scheduler outperforms the Confidence\nscheduler quantitatively by reducing the FID and qualitatively by generating\nmore diverse and more detailed images. Our code is at\nhttps://github.com/valeoai/Halton-MaskGIT."}
{"id": "2503.16980", "pdf": "https://arxiv.org/pdf/2503.16980", "abs": "https://arxiv.org/abs/2503.16980", "authors": ["Haichao Zhang", "Zhuowei Li", "Dimitris Metaxas", "Yun Fu"], "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs."}
{"id": "2503.17080", "pdf": "https://arxiv.org/pdf/2503.17080", "abs": "https://arxiv.org/abs/2503.17080", "authors": ["Gensheng Pei", "Tao Chen", "Yujia Wang", "Xinhao Cai", "Xiangbo Shu", "Tianfei Zhou", "Yazhou Yao"], "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "The CLIP model has demonstrated significant advancements in aligning visual\nand language modalities through large-scale pre-training on image-text pairs,\nenabling strong zero-shot classification and retrieval capabilities on various\ndomains. However, CLIP's training remains computationally intensive, with high\ndemands on both data processing and memory. To address these challenges, recent\nmasking strategies have emerged, focusing on the selective removal of image\npatches to improve training efficiency. Although effective, these methods often\ncompromise key semantic information, resulting in suboptimal alignment between\nvisual features and text descriptions. In this work, we present a concise yet\neffective approach called Patch Generation-to-Selection to enhance CLIP's\ntraining efficiency while preserving critical semantic content. Our method\nintroduces a gradual masking process in which a small set of candidate patches\nis first pre-selected as potential mask regions. Then, we apply Sobel edge\ndetection across the entire image to generate an edge mask that prioritizes the\nretention of the primary object areas. Finally, similarity scores between the\ncandidate mask patches and their neighboring patches are computed, with optimal\ntransport normalization refining the selection process to ensure a balanced\nsimilarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in\nzero-shot classification and retrieval tasks, achieving superior performance in\nrobustness evaluation and language compositionality benchmarks."}
{"id": "2503.17004", "pdf": "https://arxiv.org/pdf/2503.17004", "abs": "https://arxiv.org/abs/2503.17004", "authors": ["Sophia Rupprecht", "Yassine Hounat", "Monisha Kumar", "Giacomo Lastrucci", "Artur M. Schweidtmann"], "title": "Text2Model: Generating dynamic chemical reactor models using large language models (LLMs)", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "As large language models have shown remarkable capabilities in conversing via\nnatural language, the question arises as to how LLMs could potentially assist\nchemical engineers in research and industry with domain-specific tasks. We\ngenerate dynamic chemical reactor models in Modelica code format from textual\ndescriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically\ngenerated Modelica code for different reactor scenarios. We compare the\nperformance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model\nand GPT4o. We manually assess the models' predictions regarding the syntactic\nand semantic accuracy of the generated dynamic models. We find that\nconsiderable improvements are achieved by the fine-tuned model with respect to\nboth the semantic and the syntactic accuracy of the Modelica models. However,\nthe fine-tuned model lacks a satisfactory ability to generalize to unseen\nscenarios compared to GPT4o."}
{"id": "2503.17093", "pdf": "https://arxiv.org/pdf/2503.17093", "abs": "https://arxiv.org/abs/2503.17093", "authors": ["Johan Edstedt", "André Mateus", "Alberto Jaenal"], "title": "ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Structure-from-Motion (SfM) is the task of estimating 3D structure and camera\nposes from images. We define Collaborative SfM (ColabSfM) as sharing\ndistributed SfM reconstructions. Sharing maps requires estimating a joint\nreference frame, which is typically referred to as registration. However, there\nis a lack of scalable methods and training datasets for registering SfM\nreconstructions. In this paper, we tackle this challenge by proposing the\nscalable task of point cloud registration for SfM reconstructions. We find that\ncurrent registration methods cannot register SfM point clouds when trained on\nexisting datasets. To this end, we propose a SfM registration dataset\ngeneration pipeline, leveraging partial reconstructions from synthetically\ngenerated camera trajectories for each scene. Finally, we propose a simple but\nimpactful neural refiner on top of the SotA registration method RoITr that\nyields significant improvements, which we call RefineRoITr. Our extensive\nexperimental evaluation shows that our proposed pipeline and model enables\nColabSfM. Code is available at https://github.com/EricssonResearch/ColabSfM"}
{"id": "2503.17229", "pdf": "https://arxiv.org/pdf/2503.17229", "abs": "https://arxiv.org/abs/2503.17229", "authors": ["Albert Sawczyn", "Jakub Binkowski", "Denis Janiak", "Bogdan Gabrys", "Tomasz Kajdanowicz"], "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content."}
{"id": "2503.17096", "pdf": "https://arxiv.org/pdf/2503.17096", "abs": "https://arxiv.org/abs/2503.17096", "authors": ["Ruiyang Ha", "Songyi Jiang", "Bin Li", "Bikang Pan", "Yihang Zhu", "Junjie Zhang", "Xiatian Zhu", "Shaogang Gong", "Jingya Wang"], "title": "Multi-modal Multi-platform Person Re-Identification: Benchmark and Method", "categories": ["cs.CV"], "comment": null, "summary": "Conventional person re-identification (ReID) research is often limited to\nsingle-modality sensor data from static cameras, which fails to address the\ncomplexities of real-world scenarios where multi-modal signals are increasingly\nprevalent. For instance, consider an urban ReID system integrating stationary\nRGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic\ntracking capabilities. Such systems face significant challenges due to\nvariations in camera perspectives, lighting conditions, and sensor modalities,\nhindering effective person ReID. To address these challenges, we introduce the\nMP-ReID benchmark, a novel dataset designed specifically for multi-modality and\nmulti-platform ReID. This benchmark uniquely compiles data from 1,930\nidentities across diverse modalities, including RGB, infrared, and thermal\nimaging, captured by both UAVs and ground-based cameras in indoor and outdoor\nenvironments. Building on this benchmark, we introduce Uni-Prompt ReID, a\nframework with specific-designed prompts, tailored for cross-modality and\ncross-platform scenarios. Our method consistently outperforms state-of-the-art\napproaches, establishing a robust foundation for future research in complex and\ndynamic ReID environments. Our dataset are available\nat:https://mp-reid.github.io/."}
{"id": "2503.17285", "pdf": "https://arxiv.org/pdf/2503.17285", "abs": "https://arxiv.org/abs/2503.17285", "authors": ["Louis Y. Kim", "Michelle Karker", "Victoria Valledor", "Seiyoung C. Lee", "Karl F. Brzoska", "Margaret Duff", "Anthony Palladino"], "title": "An Iterative Feedback Mechanism for Improving Natural Language Class Descriptions in Open-Vocabulary Object Detection", "categories": ["cs.CV", "cs.CL"], "comment": "To appear in the Proceedings of SPIE 13463 Automatic Target\n  Recognition XXXV, Orlando, FL, 2025", "summary": "Recent advances in open-vocabulary object detection models will enable\nAutomatic Target Recognition systems to be sustainable and repurposed by\nnon-technical end-users for a variety of applications or missions. New, and\npotentially nuanced, classes can be defined with natural language text\ndescriptions in the field, immediately before runtime, without needing to\nretrain the model. We present an approach for improving non-technical users'\nnatural language text descriptions of their desired targets of interest, using\na combination of analysis techniques on the text embeddings, and proper\ncombinations of embeddings for contrastive examples. We quantify the\nimprovement that our feedback mechanism provides by demonstrating performance\nwith multiple publicly-available open-vocabulary object detection models."}
{"id": "2503.17097", "pdf": "https://arxiv.org/pdf/2503.17097", "abs": "https://arxiv.org/abs/2503.17097", "authors": ["Boyuan Zheng", "Shouyi Lu", "Renbo Huang", "Minqing Huang", "Fan Lu", "Wei Tian", "Guirong Zhuo", "Lu Xiong"], "title": "R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "We introduce R2LDM, an innovative approach for generating dense and accurate\n4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of\nutilizing range images or bird's eye view (BEV) images, we represent both LiDAR\nand 4D radar point clouds using voxel features, which more effectively capture\n3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model\n(LVDM), which performs the diffusion process in the latent space. Additionally,\na novel Latent Point Cloud Reconstruction (LPCR) module is utilized to\nreconstruct point clouds from high-dimensional latent voxel features. As a\nresult, R2LDM effectively generates LiDAR-like point clouds from paired raw\nradar data. We evaluate our approach on two different datasets, and the\nexperimental results demonstrate that our model achieves 6- to 10-fold\ndensification of radar point clouds, outperforming state-of-the-art baselines\nin 4D radar point cloud super-resolution. Furthermore, the enhanced radar point\nclouds generated by our method significantly improve downstream tasks,\nachieving up to 31.7% improvement in point cloud registration recall rate and\n24.9% improvement in object detection accuracy."}
{"id": "2503.17352", "pdf": "https://arxiv.org/pdf/2503.17352", "abs": "https://arxiv.org/abs/2503.17352", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "categories": ["cs.CV", "cs.CL"], "comment": "23 pages, 11 figures, 8 tables", "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker."}
{"id": "2503.17106", "pdf": "https://arxiv.org/pdf/2503.17106", "abs": "https://arxiv.org/abs/2503.17106", "authors": ["Yizhe Liu", "Tong Jia", "Da Cai", "Hao Wang", "Dongyue Chen"], "title": "GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and Specular Objects", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Transparent and specular objects are frequently encountered in daily life,\nfactories, and laboratories. However, due to the unique optical properties, the\ndepth information on these objects is usually incomplete and inaccurate, which\nposes significant challenges for downstream robotics tasks. Therefore, it is\ncrucial to accurately restore the depth information of transparent and specular\nobjects. Previous depth completion methods for these objects usually use RGB\ninformation as an additional channel of the depth image to perform depth\nprediction. Due to the poor-texture characteristics of transparent and specular\nobjects, these methods that rely heavily on color information tend to generate\nstructure-less depth predictions. Moreover, these 2D methods cannot effectively\nexplore the 3D structure hidden in the depth channel, resulting in depth\nambiguity. To this end, we propose a geometry-aware assisted depth completion\nmethod for transparent and specular objects, which focuses on exploring the 3D\nstructural cues of the scene. Specifically, besides extracting 2D features from\nRGB-D input, we back-project the input depth to a point cloud and build the 3D\nbranch to extract hierarchical scene-level 3D structural features. To exploit\n3D geometric information, we design several gated cross-modal fusion modules to\neffectively propagate multi-level 3D geometric features to the image branch. In\naddition, we propose an adaptive correlation aggregation strategy to\nappropriately assign 3D features to the corresponding 2D features. Extensive\nexperiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method\noutperforms other state-of-the-art methods. We further demonstrate that our\nmethod significantly enhances the performance of downstream robotic grasping\ntasks."}
{"id": "2503.17109", "pdf": "https://arxiv.org/pdf/2503.17109", "abs": "https://arxiv.org/abs/2503.17109", "authors": ["Yuanmin Tang", "Jing Yu", "Keke Gai", "Jiamin Zhuang", "Gang Xiong", "Gaopeng Gou", "Qi Wu"], "title": "Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": "This work has been accepted to CVPR 2025", "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a\nbroad range of visual content manipulation intent across domain, scene, object,\nand attribute. The key challenge for ZS-CIR tasks is to modify a reference\nimage according to manipulation text to accurately retrieve a target image,\nespecially when the reference image is missing essential target content. In\nthis paper, we propose a novel prediction-based mapping network, named\nPrediCIR, to adaptively predict the missing target visual content in reference\nimages in the latent space before mapping for accurate ZS-CIR. Specifically, a\nworld view generation module first constructs a source view by omitting certain\nvisual content of a target view, coupled with an action that includes the\nmanipulation intent derived from existing image-caption pairs. Then, a target\ncontent prediction module trains a world model as a predictor to adaptively\npredict the missing visual information guided by user intention in manipulating\ntext at the latent space. The two modules map an image with the predicted\nrelevant information to a pseudo-word token without extra supervision. Our\nmodel shows strong generalization ability on six ZS-CIR tasks. It obtains\nconsistent and significant performance boosts ranging from 1.73% to 4.45% over\nthe best methods and achieves new state-of-the-art results on ZS-CIR. Our code\nis available at https://github.com/Pter61/predicir."}
{"id": "2503.17110", "pdf": "https://arxiv.org/pdf/2503.17110", "abs": "https://arxiv.org/abs/2503.17110", "authors": ["Robin Hesse", "Doğukan Bağcı", "Bernt Schiele", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Beyond Accuracy: What Matters in Designing Well-Behaved Models?", "categories": ["cs.CV", "cs.LG"], "comment": "Code: https://github.com/visinf/beyond-accuracy", "summary": "Deep learning has become an essential part of computer vision, with deep\nneural networks (DNNs) excelling in predictive performance. However, they often\nfall short in other critical quality dimensions, such as robustness,\ncalibration, or fairness. While existing studies have focused on a subset of\nthese quality dimensions, none have explored a more general form of\n\"well-behavedness\" of DNNs. With this work, we address this gap by\nsimultaneously studying nine different quality dimensions for image\nclassification. Through a large-scale study, we provide a bird's-eye view by\nanalyzing 326 backbone models and how different training paradigms and model\narchitectures affect the quality dimensions. We reveal various new insights\nsuch that (i) vision-language models exhibit high fairness on ImageNet-1k\nclassification and strong robustness against domain changes; (ii)\nself-supervised learning is an effective training paradigm to improve almost\nall considered quality dimensions; and (iii) the training dataset size is a\nmajor driver for most of the quality dimensions. We conclude our study by\nintroducing the QUBA score (Quality Understanding Beyond Accuracy), a novel\nmetric that ranks models across multiple dimensions of quality, enabling\ntailored recommendations based on specific user needs."}
{"id": "2503.17122", "pdf": "https://arxiv.org/pdf/2503.17122", "abs": "https://arxiv.org/abs/2503.17122", "authors": ["Jonas Mirlach", "Lei Wan", "Andreas Wiedholz", "Hannan Ejaz Keen", "Andreas Eich"], "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception", "categories": ["cs.CV"], "comment": "10 pages, 7 figures, submitted to ICCV2025", "summary": "In autonomous driving, the integration of roadside perception systems is\nessential for overcoming occlusion challenges and enhancing the safety of\nVulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly\nused, thermal imaging remains underrepresented in datasets, despite its\nacknowledged advantages for VRU detection in extreme lighting conditions. In\nthis paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and\nthermal imaging from a roadside perspective, with a strong focus on VRUs.\nR-LiViT captures three intersections during both day and night, ensuring a\ndiverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and\nspatially aligned RGB and thermal images across over 150 traffic scenarios,\nwith 6 and 8 annotated classes respectively, providing a comprehensive resource\nfor tasks such as object detection and tracking. The dataset1 and the code for\nreproducing our evaluation results2 are made publicly available."}
{"id": "2503.17132", "pdf": "https://arxiv.org/pdf/2503.17132", "abs": "https://arxiv.org/abs/2503.17132", "authors": ["Siyuan Yang", "Shilin Lu", "Shizheng Wang", "Meng Hwa Er", "Zengwei Zheng", "Alex C. Kot"], "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.NE"], "comment": null, "summary": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR."}
{"id": "2503.17142", "pdf": "https://arxiv.org/pdf/2503.17142", "abs": "https://arxiv.org/abs/2503.17142", "authors": ["Davide Berasi", "Matteo Farina", "Massimiliano Mancini", "Elisa Ricci", "Nicola Strisciuglio"], "title": "Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "Camera-ready version for CVPR 2025 (with Supp.Mat.)", "summary": "Vision-Language Models (VLMs) learn a shared feature space for text and\nimages, enabling the comparison of inputs of different modalities. While prior\nworks demonstrated that VLMs organize natural language representations into\nregular structures encoding composite meanings, it remains unclear if\ncompositional patterns also emerge in the visual embedding space. In this work,\nwe investigate compositionality in the image domain, where the analysis of\ncompositional properties is challenged by noise and sparsity of visual data. We\naddress these problems and propose a framework, called Geodesically\nDecomposable Embeddings (GDE), that approximates image representations with\ngeometry-aware compositional structures in the latent space. We demonstrate\nthat visual embeddings of pre-trained VLMs exhibit a compositional arrangement,\nand evaluate the effectiveness of this property in the tasks of compositional\nclassification and group robustness. GDE achieves stronger performance in\ncompositional classification compared to its counterpart method that assumes\nlinear geometry of the latent space. Notably, it is particularly effective for\ngroup robustness, where we achieve higher results than task-specific solutions.\nOur results indicate that VLMs can automatically develop a human-like form of\ncompositional reasoning in the visual domain, making their underlying processes\nmore interpretable. Code is available at\nhttps://github.com/BerasiDavide/vlm_image_compositionality."}
{"id": "2503.17153", "pdf": "https://arxiv.org/pdf/2503.17153", "abs": "https://arxiv.org/abs/2503.17153", "authors": ["Fouad Makiyeh", "Huy-Dung Nguyen", "Patrick Chareyre", "Ramin Hasani", "Marc Blanchon", "Daniela Rus"], "title": "Enhancing Steering Estimation with Semantic-Aware GNNs", "categories": ["cs.CV"], "comment": "Submitted to ICCV 2025", "summary": "Steering estimation is a critical task in autonomous driving, traditionally\nrelying on 2D image-based models. In this work, we explore the advantages of\nincorporating 3D spatial information through hybrid architectures that combine\n3D neural network models with recurrent neural networks (RNNs) for temporal\nmodeling, using LiDAR-based point clouds as input. We systematically evaluate\nfour hybrid 3D models, all of which outperform the 2D-only baseline, with the\nGraph Neural Network (GNN) - RNN model yielding the best results.\n  To reduce reliance on LiDAR, we leverage a pretrained unified model to\nestimate depth from monocular images, reconstructing pseudo-3D point clouds. We\nthen adapt the GNN-RNN model, originally designed for LiDAR-based point clouds,\nto work with these pseudo-3D representations, achieving comparable or even\nsuperior performance compared to the LiDAR-based model. Additionally, the\nunified model provides semantic labels for each point, enabling a more\nstructured scene representation. To further optimize graph construction, we\nintroduce an efficient connectivity strategy where connections are\npredominantly formed between points of the same semantic class, with only 20\\%\nof inter-class connections retained. This targeted approach reduces graph\ncomplexity and computational cost while preserving critical spatial\nrelationships.\n  Finally, we validate our approach on the KITTI dataset, achieving a 71%\nimprovement over 2D-only models. Our findings highlight the advantages of 3D\nspatial information and efficient graph construction for steering estimation,\nwhile maintaining the cost-effectiveness of monocular images and avoiding the\nexpense of LiDAR-based systems."}
{"id": "2503.17155", "pdf": "https://arxiv.org/pdf/2503.17155", "abs": "https://arxiv.org/abs/2503.17155", "authors": ["Panpan Wang", "Liqiang Niu", "Fandong Meng", "Jinan Xu", "Yufeng Chen", "Jie Zhou"], "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens", "categories": ["cs.CV"], "comment": null, "summary": "In the domain of image generation, latent-based generative models occupy a\ndominant status; however, these models rely heavily on image tokenizer. To meet\nmodeling requirements, autoregressive models possessing the characteristics of\nscalability and flexibility embrace a discrete-valued tokenizer, but face the\nchallenge of poor image generation quality. In contrast, diffusion models take\nadvantage of the continuous-valued tokenizer to achieve better generation\nquality but are subject to low efficiency and complexity. The existing hybrid\nmodels are mainly to compensate for information loss and simplify the diffusion\nlearning process. The potential of merging discrete-valued and\ncontinuous-valued tokens in the field of image generation has not yet been\nexplored. In this paper, we propose D2C, a novel two-stage method to enhance\nmodel generation capacity. In the first stage, the discrete-valued tokens\nrepresenting coarse-grained image features are sampled by employing a small\ndiscrete-valued generator. Then in the second stage, the continuous-valued\ntokens representing fine-grained image features are learned conditioned on the\ndiscrete token sequence. In addition, we design two kinds of fusion modules for\nseamless interaction. On the ImageNet-256 benchmark, extensive experiment\nresults validate that our model achieves superior performance compared with\nseveral continuous-valued and discrete-valued generative models on the\nclass-conditional image generation tasks."}
{"id": "2503.17162", "pdf": "https://arxiv.org/pdf/2503.17162", "abs": "https://arxiv.org/abs/2503.17162", "authors": ["Tonmoy Hossain ana Miaomiao Zhang"], "title": "CoRLD: Contrastive Representation Learning Of Deformable Shapes In Images", "categories": ["cs.CV"], "comment": null, "summary": "Deformable shape representations, parameterized by deformations relative to a\ngiven template, have proven effective for improved image analysis tasks.\nHowever, their broader applicability is hindered by two major challenges.\nFirst, existing methods mainly rely on a known template during testing, which\nis impractical and limits flexibility. Second, they often struggle to capture\nfine-grained, voxel-level distinctions between similar shapes (e.g., anatomical\nvariations among healthy individuals, those with mild cognitive impairment, and\ndiseased states). To address these limitations, we propose a novel framework -\nContrastive Representation Learning of Deformable shapes (CoRLD) in learned\ndeformation spaces and demonstrate its effectiveness in the context of image\nclassification. Our CoRLD leverages a class-aware contrastive supervised\nlearning objective in latent deformation spaces, promoting proximity among\nrepresentations of similar classes while ensuring separation of dissimilar\ngroups. In contrast to previous deep learning networks that require a reference\nimage as input to predict deformation changes, our approach eliminates this\ndependency. Instead, template images are utilized solely as ground truth in the\nloss function during the training process, making our model more flexible and\ngeneralizable to a wide range of medical applications. We validate CoRLD on\ndiverse datasets, including real brain magnetic resonance imaging (MRIs) and\nadrenal shapes derived from computed tomography (CT) scans. Experimental\nresults show that our model effectively extracts deformable shape features,\nwhich can be easily integrated with existing classifiers to substantially boost\nthe classification accuracy. Our code is available at GitHub."}
{"id": "2503.17168", "pdf": "https://arxiv.org/pdf/2503.17168", "abs": "https://arxiv.org/abs/2503.17168", "authors": ["Alexandra Arzberger", "Ramin Tavakoli Kolagari"], "title": "Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults."}
{"id": "2503.17175", "pdf": "https://arxiv.org/pdf/2503.17175", "abs": "https://arxiv.org/abs/2503.17175", "authors": ["Duanrui Yu", "Jing You", "Xin Pei", "Anqi Qu", "Dingyu Wang", "Shaocheng Jia"], "title": "Which2comm: An Efficient Collaborative Perception Framework for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative perception allows real-time inter-agent information exchange\nand thus offers invaluable opportunities to enhance the perception capabilities\nof individual agents. However, limited communication bandwidth in practical\nscenarios restricts the inter-agent data transmission volume, consequently\nresulting in performance declines in collaborative perception systems. This\nimplies a trade-off between perception performance and communication cost. To\naddress this issue, we propose Which2comm, a novel multi-agent 3D object\ndetection framework leveraging object-level sparse features. By integrating\nsemantic information of objects into 3D object detection boxes, we introduce\nsemantic detection boxes (SemDBs). Innovatively transmitting these\ninformation-rich object-level sparse features among agents not only\nsignificantly reduces the demanding communication volume, but also improves 3D\nobject detection performance. Specifically, a fully sparse network is\nconstructed to extract SemDBs from individual agents; a temporal fusion\napproach with a relative temporal encoding mechanism is utilized to obtain the\ncomprehensive spatiotemporal features. Extensive experiments on the V2XSet and\nOPV2V datasets demonstrate that Which2comm consistently outperforms other\nstate-of-the-art methods on both perception performance and communication cost,\nexhibiting better robustness to real-world latency. These results present that\nfor multi-agent collaborative 3D object detection, transmitting only\nobject-level sparse features is sufficient to achieve high-precision and robust\nperformance."}
{"id": "2503.17182", "pdf": "https://arxiv.org/pdf/2503.17182", "abs": "https://arxiv.org/abs/2503.17182", "authors": ["Patrick Rim", "Hyoungseob Park", "Vadim Ezhov", "Jeffrey Moon", "Alex Wong"], "title": "Radar-Guided Polynomial Fitting for Metric Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We propose PolyRad, a novel radar-guided depth estimation method that\nintroduces polynomial fitting to transform scaleless depth predictions from\npretrained monocular depth estimation (MDE) models into metric depth maps.\nUnlike existing approaches that rely on complex architectures or expensive\nsensors, our method is grounded in a simple yet fundamental insight: using\npolynomial coefficients predicted from cheap, ubiquitous radar data to\nadaptively adjust depth predictions non-uniformly across depth ranges. Although\nMDE models often infer reasonably accurate local depth structure within each\nobject or local region, they may misalign these regions relative to one\nanother, making a linear scale-and-shift transformation insufficient given\nthree or more of these regions. In contrast, PolyRad generalizes beyond linear\ntransformations and is able to correct such misalignments by introducing\ninflection points. Importantly, our polynomial fitting framework preserves\nstructural consistency through a novel training objective that enforces\nmonotonicity via first-derivative regularization. PolyRad achieves\nstate-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft\ndatasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE."}
{"id": "2503.17184", "pdf": "https://arxiv.org/pdf/2503.17184", "abs": "https://arxiv.org/abs/2503.17184", "authors": ["Xueqi Qiu", "Xingyu Miao", "Fan Wan", "Haoran Duan", "Tejal Shah", "Varun Ojhab", "Yang Longa", "Rajiv Ranjan"], "title": "D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life."}
{"id": "2503.17193", "pdf": "https://arxiv.org/pdf/2503.17193", "abs": "https://arxiv.org/abs/2503.17193", "authors": ["Xiaojin Lu", "Taoran yue", "Jiaxi cai", "Shibing Chu"], "title": "MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Detecting infrared small targets in complex backgrounds remains a challenging\ntask because of the low contrast and high noise levels inherent in infrared\nimages. These factors often lead to the loss of crucial details during feature\nextraction. Moreover, existing detection methods have limitations in adequately\nintegrating global and local information, which constrains the efficiency and\naccuracy of infrared small target detection. To address these challenges, this\npaper proposes a novel network architecture named MSCA-Net, which integrates\nthree key components: Multi-Scale Enhanced Detection Attention\nmechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and\nChannel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale\nfeature fusion attention mechanism to adaptively aggregate information across\ndifferent scales, enriching feature representation. PCBAM captures the\ncorrelation between global and local features through a correlation\nmatrix-based strategy, enabling deep feature interaction. Moreover, CAB\nredistributes input feature channels, facilitating the efficient transmission\nof beneficial features and further enhancing the model detection capability in\ncomplex backgrounds. The experimental results demonstrate that MSCA-Net\nachieves outstanding small target detection performance in complex backgrounds.\nSpecifically, it attains mIoU scores of 78.43\\%, 94.56\\%, and 67.08\\% on the\nNUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its\neffectiveness and strong potential for real-world applications."}
{"id": "2503.17197", "pdf": "https://arxiv.org/pdf/2503.17197", "abs": "https://arxiv.org/abs/2503.17197", "authors": ["Xingchao Yang", "Takafumi Taketomi", "Yuki Endo", "Yoshihiro Kanamori"], "title": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy", "categories": ["cs.CV"], "comment": "CVPR 2025. Project: https://yangxingchao.github.io/FreeUV-page/", "summary": "Recovering high-quality 3D facial textures from single-view 2D images is a\nchallenging task, especially under constraints of limited data and complex\nfacial details such as makeup, wrinkles, and occlusions. In this paper, we\nintroduce FreeUV, a novel ground-truth-free UV texture recovery framework that\neliminates the need for annotated or synthetic UV data. FreeUV leverages\npre-trained stable diffusion model alongside a Cross-Assembly inference\nstrategy to fulfill this objective. In FreeUV, separate networks are trained\nindependently to focus on realistic appearance and structural consistency, and\nthese networks are combined during inference to generate coherent textures. Our\napproach accurately captures intricate facial features and demonstrates robust\nperformance across diverse poses and occlusions. Extensive experiments validate\nFreeUV's effectiveness, with results surpassing state-of-the-art methods in\nboth quantitative and qualitative metrics. Additionally, FreeUV enables new\napplications, including local editing, facial feature interpolation, and\nmulti-view texture recovery. By reducing data requirements, FreeUV offers a\nscalable solution for generating high-fidelity 3D facial textures suitable for\nreal-world scenarios."}
{"id": "2503.17212", "pdf": "https://arxiv.org/pdf/2503.17212", "abs": "https://arxiv.org/abs/2503.17212", "authors": ["Matthew Kenely", "Dylan Seychell", "Carl James Debono", "Chris Porter"], "title": "A Deep Learning Framework for Visual Attention Prediction and Analysis of News Interfaces", "categories": ["cs.CV", "cs.HC"], "comment": "This is a preprint submitted to the 2025 IEEE Conference on\n  Artificial Intelligence (CAI)", "summary": "News outlets' competition for attention in news interfaces has highlighted\nthe need for demographically-aware saliency prediction models. Despite recent\nadvancements in saliency detection applied to user interfaces (UI), existing\ndatasets are limited in size and demographic representation. We present a deep\nlearning framework that enhances the SaRa (Saliency Ranking) model with\nDeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our\nframework optimizes three key components: saliency map generation, grid segment\nscoring, and map normalization. Through a two-fold experiment using\neye-tracking (30 participants) and mouse-tracking (375 participants aged\n13--70), we analyze attention patterns across demographic groups. Statistical\nanalysis reveals significant age-based variations (p < 0.05, {\\epsilon^2} =\n0.042), with older users (36--70) engaging more with textual content and\nyounger users (13--35) interacting more with images. Mouse-tracking data\nclosely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI\nelements that immediately stand out, validating its use in large-scale studies.\nWe conclude that saliency studies should prioritize gathering data from a\nlarger, demographically representative sample and report exact demographic\ndistributions."}
{"id": "2503.17213", "pdf": "https://arxiv.org/pdf/2503.17213", "abs": "https://arxiv.org/abs/2503.17213", "authors": ["Ting Sun", "Cheng Cui", "Yuning Du", "Yi Liu"], "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction", "categories": ["cs.CV", "cs.AI"], "comment": "Github Repo: https://github.com/PaddlePaddle/PaddleX", "summary": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX ."}
{"id": "2503.17221", "pdf": "https://arxiv.org/pdf/2503.17221", "abs": "https://arxiv.org/abs/2503.17221", "authors": ["Fanghua Yu", "Jinjin Gu", "Jinfan Hu", "Zheyuan Li", "Chao Dong"], "title": "UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models", "categories": ["cs.CV"], "comment": "This work has been accepted for publication at the International\n  Conference on Learning Representations (ICLR) 2025", "summary": "We introduce UniCon, a novel architecture designed to enhance control and\nefficiency in training adapters for large-scale diffusion models. Unlike\nexisting methods that rely on bidirectional interaction between the diffusion\nmodel and control adapter, UniCon implements a unidirectional flow from the\ndiffusion network to the adapter, allowing the adapter alone to generate the\nfinal output. UniCon reduces computational demands by eliminating the need for\nthe diffusion model to compute and store gradients during adapter training. Our\nresults indicate that UniCon reduces GPU memory usage by one-third and\nincreases training speed by 2.3 times, while maintaining the same adapter\nparameter size. Additionally, without requiring extra computational resources,\nUniCon enables the training of adapters with double the parameter volume of\nexisting ControlNets. In a series of image conditional generation tasks, UniCon\nhas demonstrated precise responsiveness to control inputs and exceptional\ngeneration capabilities."}
{"id": "2503.17224", "pdf": "https://arxiv.org/pdf/2503.17224", "abs": "https://arxiv.org/abs/2503.17224", "authors": ["Giacomo Savazzi", "Eugenio Lomurno", "Cristian Sbrolli", "Agnese Chiatti", "Matteo Matteucci"], "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks."}
{"id": "2503.17226", "pdf": "https://arxiv.org/pdf/2503.17226", "abs": "https://arxiv.org/abs/2503.17226", "authors": ["Aryan Yazdan Parast", "Basim Azam", "Naveed Akhtar"], "title": "Leveraging Text-to-Image Generation for Handling Spurious Correlation", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks trained with Empirical Risk Minimization (ERM) perform\nwell when both training and test data come from the same domain, but they often\nfail to generalize to out-of-distribution samples. In image classification,\nthese models may rely on spurious correlations that often exist between labels\nand irrelevant features of images, making predictions unreliable when those\nfeatures do not exist. We propose a technique to generate training samples with\ntext-to-image (T2I) diffusion models for addressing the spurious correlation\nproblem. First, we compute the best describing token for the visual features\npertaining to the causal components of samples by a textual inversion\nmechanism. Then, leveraging a language segmentation method and a diffusion\nmodel, we generate new samples by combining the causal component with the\nelements from other classes. We also meticulously prune the generated samples\nbased on the prediction probabilities and attribution scores of the ERM model\nto ensure their correct composition for our objective. Finally, we retrain the\nERM model on our augmented dataset. This process reduces the model's reliance\non spurious correlations by learning from carefully crafted samples for in\nwhich this correlation does not exist. Our experiments show that across\ndifferent benchmarks, our technique achieves better worst-group accuracy than\nthe existing state-of-the-art methods."}
{"id": "2503.17237", "pdf": "https://arxiv.org/pdf/2503.17237", "abs": "https://arxiv.org/abs/2503.17237", "authors": ["Yu-Hsi Chen"], "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures, 5 tables", "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID ."}
{"id": "2503.17238", "pdf": "https://arxiv.org/pdf/2503.17238", "abs": "https://arxiv.org/abs/2503.17238", "authors": ["Devavrat Tomar", "Guillaume Vray", "Dwarikanath Mahapatra", "Sudipta Roy", "Jean-Philippe Thiran", "Behzad Bozorgtabar"], "title": "Slide-Level Prompt Learning with Vision Language Models for Few-Shot Multiple Instance Learning in Histopathology", "categories": ["cs.CV"], "comment": "Accepted to ISBI 2025", "summary": "In this paper, we address the challenge of few-shot classification in\nhistopathology whole slide images (WSIs) by utilizing foundational\nvision-language models (VLMs) and slide-level prompt learning. Given the\ngigapixel scale of WSIs, conventional multiple instance learning (MIL) methods\nrely on aggregation functions to derive slide-level (bag-level) predictions\nfrom patch representations, which require extensive bag-level labels for\ntraining. In contrast, VLM-based approaches excel at aligning visual embeddings\nof patches with candidate class text prompts but lack essential pathological\nprior knowledge. Our method distinguishes itself by utilizing pathological\nprior knowledge from language models to identify crucial local tissue types\n(patches) for WSI classification, integrating this within a VLM-based MIL\nframework. Our approach effectively aligns patch images with tissue types, and\nwe fine-tune our model via prompt learning using only a few labeled WSIs per\ncategory. Experimentation on real-world pathological WSI datasets and ablation\nstudies highlight our method's superior performance over existing MIL- and\nVLM-based methods in few-shot WSI classification tasks. Our code is publicly\navailable at https://github.com/LTS5/SLIP."}
{"id": "2503.17262", "pdf": "https://arxiv.org/pdf/2503.17262", "abs": "https://arxiv.org/abs/2503.17262", "authors": ["Shuang Guo", "Friedhelm Hamann", "Guillermo Gallego"], "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "14 page, 8 figures, 9 tables. Project page:\n  https://github.com/tub-rip/e2fai", "summary": "Event cameras rely on motion to obtain information about scene appearance. In\nother words, for event cameras, motion and appearance are seen both or neither,\nwhich are encoded in the output event stream. Previous works consider\nrecovering these two visual quantities as separate tasks, which does not fit\nwith the nature of event cameras and neglects the inherent relations between\nboth tasks. In this paper, we propose an unsupervised learning framework that\njointly estimates optical flow (motion) and image intensity (appearance), with\na single network. Starting from the event generation model, we newly derive the\nevent-based photometric error as a function of optical flow and image\nintensity, which is further combined with the contrast maximization framework,\nyielding a comprehensive loss function that provides proper constraints for\nboth flow and intensity estimation. Exhaustive experiments show that our model\nachieves state-of-the-art performance for both optical flow (achieves 20% and\n25% improvement in EPE and AE respectively in the unsupervised learning\ncategory) and intensity estimation (produces competitive results with other\nbaselines, particularly in high dynamic range scenarios). Last but not least,\nour model achieves shorter inference time than all the other optical flow\nmodels and many of the image reconstruction models, while they output only one\nquantity. Project page: https://github.com/tub-rip/e2fai"}
{"id": "2503.17267", "pdf": "https://arxiv.org/pdf/2503.17267", "abs": "https://arxiv.org/abs/2503.17267", "authors": ["Hiromu Taketsugu", "Takeru Oba", "Takahiro Maeda", "Shohei Nobuhara", "Norimichi Ukita"], "title": "Physical Plausibility-aware Trajectory Prediction via Locomotion Embodiment", "categories": ["cs.CV"], "comment": "CVPR2025. Project page: https://iminthemiddle.github.io/EmLoco-Page/", "summary": "Humans can predict future human trajectories even from momentary observations\nby using human pose-related cues. However, previous Human Trajectory Prediction\n(HTP) methods leverage the pose cues implicitly, resulting in implausible\npredictions. To address this, we propose Locomotion Embodiment, a framework\nthat explicitly evaluates the physical plausibility of the predicted trajectory\nby locomotion generation under the laws of physics. While the plausibility of\nlocomotion is learned with an indifferentiable physics simulator, it is\nreplaced by our differentiable Locomotion Value function to train an HTP\nnetwork in a data-driven manner. In particular, our proposed Embodied\nLocomotion loss is beneficial for efficiently training a stochastic HTP network\nusing multiple heads. Furthermore, the Locomotion Value filter is proposed to\nfilter out implausible trajectories at inference. Experiments demonstrate that\nour method enhances even the state-of-the-art HTP methods across diverse\ndatasets and problem settings. Our code is available at:\nhttps://github.com/ImIntheMiddle/EmLoco."}
{"id": "2503.17269", "pdf": "https://arxiv.org/pdf/2503.17269", "abs": "https://arxiv.org/abs/2503.17269", "authors": ["Vineet R Shenoy", "Suhas Lohit", "Hassan Mansour", "Rama Chellappa", "Tim K. Marks"], "title": "Recovering Pulse Waves from Video Using Deep Unrolling and Deep Equilibrium Models", "categories": ["cs.CV", "eess.IV"], "comment": "13 pages, 9 figures", "summary": "Camera-based monitoring of vital signs, also known as imaging\nphotoplethysmography (iPPG), has seen applications in driver-monitoring,\nperfusion assessment in surgical settings, affective computing, and more. iPPG\ninvolves sensing the underlying cardiac pulse from video of the skin and\nestimating vital signs such as the heart rate or a full pulse waveform. Some\nprevious iPPG methods impose model-based sparse priors on the pulse signals and\nuse iterative optimization for pulse wave recovery, while others use end-to-end\nblack-box deep learning methods. In contrast, we introduce methods that combine\nsignal processing and deep learning methods in an inverse problem framework.\nOur methods estimate the underlying pulse signal and heart rate from facial\nvideo by learning deep-network-based denoising operators that leverage deep\nalgorithm unfolding and deep equilibrium models. Experiments show that our\nmethods can denoise an acquired signal from the face and infer the correct\nunderlying pulse rate, achieving state-of-the-art heart rate estimation\nperformance on well-known benchmarks, all with less than one-fifth the number\nof learnable parameters as the closest competing method."}
{"id": "2503.17276", "pdf": "https://arxiv.org/pdf/2503.17276", "abs": "https://arxiv.org/abs/2503.17276", "authors": ["Maria Pilligua", "Danna Xue", "Javier Vazquez-Corral"], "title": "HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks", "categories": ["cs.CV"], "comment": "CVPR 2025, project page: https://hypernvd.github.io/", "summary": "Decomposing a video into a layer-based representation is crucial for easy\nvideo editing for the creative industries, as it enables independent editing of\nspecific layers. Existing video-layer decomposition models rely on implicit\nneural representations (INRs) trained independently for each video, making the\nprocess time-consuming when applied to new videos. Noticing this limitation, we\npropose a meta-learning strategy to learn a generic video decomposition model\nto speed up the training on new videos. Our model is based on a hypernetwork\narchitecture which, given a video-encoder embedding, generates the parameters\nfor a compact INR-based neural video decomposition model. Our strategy\nmitigates the problem of single-video overfitting and, importantly, shortens\nthe convergence of video decomposition on new, unseen videos. Our code is\navailable at: https://hypernvd.github.io/"}
{"id": "2503.17285", "pdf": "https://arxiv.org/pdf/2503.17285", "abs": "https://arxiv.org/abs/2503.17285", "authors": ["Louis Y. Kim", "Michelle Karker", "Victoria Valledor", "Seiyoung C. Lee", "Karl F. Brzoska", "Margaret Duff", "Anthony Palladino"], "title": "An Iterative Feedback Mechanism for Improving Natural Language Class Descriptions in Open-Vocabulary Object Detection", "categories": ["cs.CV", "cs.CL"], "comment": "To appear in the Proceedings of SPIE 13463 Automatic Target\n  Recognition XXXV, Orlando, FL, 2025", "summary": "Recent advances in open-vocabulary object detection models will enable\nAutomatic Target Recognition systems to be sustainable and repurposed by\nnon-technical end-users for a variety of applications or missions. New, and\npotentially nuanced, classes can be defined with natural language text\ndescriptions in the field, immediately before runtime, without needing to\nretrain the model. We present an approach for improving non-technical users'\nnatural language text descriptions of their desired targets of interest, using\na combination of analysis techniques on the text embeddings, and proper\ncombinations of embeddings for contrastive examples. We quantify the\nimprovement that our feedback mechanism provides by demonstrating performance\nwith multiple publicly-available open-vocabulary object detection models."}
{"id": "2503.17288", "pdf": "https://arxiv.org/pdf/2503.17288", "abs": "https://arxiv.org/abs/2503.17288", "authors": ["Xianghan Meng", "Zhiyuan Huang", "Wei He", "Xianbiao Qi", "Rong Xiao", "Chun-Guang Li"], "title": "Exploring a Principled Framework for Deep Subspace Clustering", "categories": ["cs.CV", "cs.LG"], "comment": "The paper is accepted by ICLR 2025. The first two authors are equally\n  contributed", "summary": "Subspace clustering is a classical unsupervised learning task, built on a\nbasic assumption that high-dimensional data can be approximated by a union of\nsubspaces (UoS). Nevertheless, the real-world data are often deviating from the\nUoS assumption. To address this challenge, state-of-the-art deep subspace\nclustering algorithms attempt to jointly learn UoS representations and\nself-expressive coefficients. However, the general framework of the existing\nalgorithms suffers from a catastrophic feature collapse and lacks a theoretical\nguarantee to learn desired UoS representation. In this paper, we present a\nPrincipled fRamewOrk for Deep Subspace Clustering (PRO-DSC), which is designed\nto learn structured representations and self-expressive coefficients in a\nunified manner. Specifically, in PRO-DSC, we incorporate an effective\nregularization on the learned representations into the self-expressive model,\nprove that the regularized self-expressive model is able to prevent feature\nspace collapse, and demonstrate that the learned optimal representations under\ncertain condition lie on a union of orthogonal subspaces. Moreover, we provide\na scalable and efficient approach to implement our PRO-DSC and conduct\nextensive experiments to verify our theoretical findings and demonstrate the\nsuperior performance of our proposed deep subspace clustering approach. The\ncode is available at https://github.com/mengxianghan123/PRO-DSC."}
{"id": "2503.17316", "pdf": "https://arxiv.org/pdf/2503.17316", "abs": "https://arxiv.org/abs/2503.17316", "authors": ["Wonbong Jang", "Philippe Weinzaepfel", "Vincent Leroy", "Lourdes Agapito", "Jerome Revaud"], "title": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We present Pow3r, a novel large 3D vision regression model that is highly\nversatile in the input modalities it accepts. Unlike previous feed-forward\nmodels that lack any mechanism to exploit known camera or scene priors at test\ntime, Pow3r incorporates any combination of auxiliary information such as\nintrinsics, relative pose, dense or sparse depth, alongside input images,\nwithin a single network. Building upon the recent DUSt3R paradigm, a\ntransformer-based architecture that leverages powerful pre-training, our\nlightweight and versatile conditioning acts as additional guidance for the\nnetwork to predict more accurate estimates when auxiliary information is\navailable. During training we feed the model with random subsets of modalities\nat each iteration, which enables the model to operate under different levels of\nknown priors at test time. This in turn opens up new capabilities, such as\nperforming inference in native image resolution, or point-cloud completion. Our\nexperiments on 3D reconstruction, depth completion, multi-view depth\nprediction, multi-view stereo, and multi-view pose estimation tasks yield\nstate-of-the-art results and confirm the effectiveness of Pow3r at exploiting\nall available information. The project webpage is\nhttps://europe.naverlabs.com/pow3r."}
{"id": "2503.17347", "pdf": "https://arxiv.org/pdf/2503.17347", "abs": "https://arxiv.org/abs/2503.17347", "authors": ["Jichen Hu", "Chen Yang", "Zanwei Zhou", "Jiemin Fang", "Xiaokang Yang", "Qi Tian", "Wei Shen"], "title": "Dereflection Any Image with Diffusion Priors and Diversified Data", "categories": ["cs.CV"], "comment": null, "summary": "Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes."}
{"id": "2503.17349", "pdf": "https://arxiv.org/pdf/2503.17349", "abs": "https://arxiv.org/abs/2503.17349", "authors": ["Jianing Qi", "Jiawei Liu", "Hao Tang", "Zhigang Zhu"], "title": "Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) excel at identifying and describing objects but\nstruggle with spatial reasoning such as accurately understanding the relative\npositions of objects. Inspired by the dual-pathway (ventral-dorsal) model of\nhuman vision, we investigate why VLMs fail spatial tasks despite strong object\nrecognition capabilities. Our interpretability-driven analysis reveals a\ncritical underlying cause: vision embeddings in VLMs are treated primarily as\nsemantic ``bag-of-tokens,\" overshadowing subtle yet crucial positional cues due\nto their disproportionately large embedding norms. We validate this insight\nthrough extensive diagnostic experiments, demonstrating minimal performance\nimpact when token orders or fine-grained spatial details are removed. Guided by\nthese findings, we propose simple, interpretable interventions, including\nnormalizing vision embedding norms and extracting mid-layer spatially rich\nfeatures, to restore spatial awareness. Empirical results on both our synthetic\ndata and standard benchmarks demonstrate improved spatial reasoning\ncapabilities, highlighting the value of interpretability-informed design\nchoices. Our study not only uncovers fundamental limitations in current VLM\narchitectures but also provides actionable insights for enhancing structured\nperception of visual scenes."}
{"id": "2503.17350", "pdf": "https://arxiv.org/pdf/2503.17350", "abs": "https://arxiv.org/abs/2503.17350", "authors": ["Qingyu Shi", "Jianzong Wu", "Jinbin Bai", "Jiangning Zhang", "Lu Qi", "Xiangtai Li", "Yunhai Tong"], "title": "Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer", "categories": ["cs.CV"], "comment": null, "summary": "The motion transfer task involves transferring motion from a source video to\nnewly generated videos, requiring the model to decouple motion from appearance.\nPrevious diffusion-based methods primarily rely on separate spatial and\ntemporal attention mechanisms within 3D U-Net. In contrast, state-of-the-art\nvideo Diffusion Transformers (DiT) models use 3D full attention, which does not\nexplicitly separate temporal and spatial information. Thus, the interaction\nbetween spatial and temporal dimensions makes decoupling motion and appearance\nmore challenging for DiT models. In this paper, we propose DeT, a method that\nadapts DiT models to improve motion transfer ability. Our approach introduces a\nsimple yet effective temporal kernel to smooth DiT features along the temporal\ndimension, facilitating the decoupling of foreground motion from background\nappearance. Meanwhile, the temporal kernel effectively captures temporal\nvariations in DiT features, which are closely related to motion. Moreover, we\nintroduce explicit supervision along dense trajectories in the latent feature\nspace to further enhance motion consistency. Additionally, we present MTBench,\na general and challenging benchmark for motion transfer. We also introduce a\nhybrid motion fidelity metric that considers both the global and local motion\nsimilarity. Therefore, our work provides a more comprehensive evaluation than\nprevious works. Extensive experiments on MTBench demonstrate that DeT achieves\nthe best trade-off between motion fidelity and edit fidelity."}
{"id": "2503.17351", "pdf": "https://arxiv.org/pdf/2503.17351", "abs": "https://arxiv.org/abs/2503.17351", "authors": ["Vineet R. Shenoy", "Shaoju Wu", "Armand Comas", "Tim K. Marks", "Suhas Lohit", "Hassan Mansour"], "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography", "categories": ["cs.CV"], "comment": "14 Pages, 8 figures", "summary": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets."}
{"id": "2503.17352", "pdf": "https://arxiv.org/pdf/2503.17352", "abs": "https://arxiv.org/abs/2503.17352", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "categories": ["cs.CV", "cs.CL"], "comment": "23 pages, 11 figures, 8 tables", "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker."}
{"id": "2503.17358", "pdf": "https://arxiv.org/pdf/2503.17358", "abs": "https://arxiv.org/abs/2503.17358", "authors": ["Jerred Chen", "Ronald Clark"], "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image", "categories": ["cs.CV"], "comment": "Project page: https://jerredchen.github.io/image-as-imu/", "summary": "In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP."}
{"id": "2503.17359", "pdf": "https://arxiv.org/pdf/2503.17359", "abs": "https://arxiv.org/abs/2503.17359", "authors": ["Jiwen Yu", "Yiran Qin", "Haoxuan Che", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Xihui Liu"], "title": "Position: Interactive Generative Video as Next-Generation Game Engine", "categories": ["cs.CV"], "comment": null, "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced."}
{"id": "2503.16482", "pdf": "https://arxiv.org/pdf/2503.16482", "abs": "https://arxiv.org/abs/2503.16482", "authors": ["Mahmoud Hamash", "Md Raqib Khan", "Peter Tiernan"], "title": "Inclusive STEAM Education: A Framework for Teaching Cod-2 ing and Robotics to Students with Visually Impairment Using 3 Advanced Computer Vision", "categories": ["cs.HC", "cs.CV", "cs.RO"], "comment": "14 pages, 2 figures", "summary": "STEAM education integrates Science, Technology, Engineering, Arts, and\nMathematics to foster creativity and problem-solving. However, students with\nvisual impairments (VI) encounter significant challenges in programming and\nrobotics, particularly in tracking robot movements and developing spatial\nawareness. This paper presents a framework that leverages pre-constructed\nrobots and algorithms, such as maze-solving techniques, within an accessible\nlearning environment. The proposed system employs Contrastive Language-Image\nPre-training (CLIP) to process global camera-captured maze layouts, converting\nvisual data into textual descriptions that generate spatial audio prompts in an\nAudio Virtual Reality (AVR) system. Students issue verbal commands, which are\nrefined through CLIP, while robot-mounted stereo cameras provide real-time data\nprocessed via Simultaneous Localization and Mapping (SLAM) for continuous\nfeedback. By integrating these technologies, the framework empowers VI students\nto develop coding skills and engage in complex problem-solving tasks. Beyond\nmaze-solving applications, this approach demonstrates the broader potential of\ncomputer vision in special education, contributing to improved accessibility\nand learning experiences in STEAM disciplines."}
{"id": "2503.16488", "pdf": "https://arxiv.org/pdf/2503.16488", "abs": "https://arxiv.org/abs/2503.16488", "authors": ["Kunal Chavan", "Keertan Balaji", "Spoorti Barigidad", "Samba Raju Chiluveru"], "title": "VocalEyes: Enhancing Environmental Perception for the Visually Impaired through Vision-Language Models and Distance-Aware Object Detection", "categories": ["cs.HC", "cs.CV", "cs.LG"], "comment": null, "summary": "With an increasing demand for assistive technologies that promote the\nindependence and mobility of visually impaired people, this study suggests an\ninnovative real-time system that gives audio descriptions of a user's\nsurroundings to improve situational awareness. The system acquires live video\ninput and processes it with a quantized and fine-tuned Florence-2 big model,\nadjusted to 4-bit accuracy for efficient operation on low-power edge devices\nsuch as the NVIDIA Jetson Orin Nano. By transforming the video signal into\nframes with a 5-frame latency, the model provides rapid and contextually\npertinent descriptions of objects, pedestrians, and barriers, together with\ntheir estimated distances. The system employs Parler TTS Mini, a lightweight\nand adaptable Text-to-Speech (TTS) solution, for efficient audio feedback. It\naccommodates 34 distinct speaker types and enables customization of speech\ntone, pace, and style to suit user requirements. This study examines the\nquantization and fine-tuning techniques utilized to modify the Florence-2 model\nfor this application, illustrating how the integration of a compact model\narchitecture with a versatile TTS component improves real-time performance and\nuser experience. The proposed system is assessed based on its accuracy,\nefficiency, and usefulness, providing a viable option to aid vision-impaired\nusers in navigating their surroundings securely and successfully."}
{"id": "2503.16537", "pdf": "https://arxiv.org/pdf/2503.16537", "abs": "https://arxiv.org/abs/2503.16537", "authors": ["Grigorii Khvatskii", "Yong Suk Lee", "Corey Angst", "Maria Gibbs", "Robert Landers", "Nitesh V. Chawla"], "title": "Do Multimodal Large Language Models Understand Welding?", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages", "summary": "This paper examines the performance of Multimodal LLMs (MLLMs) in skilled\nproduction work, with a focus on welding. Using a novel data set of real-world\nand online weld images, annotated by a domain expert, we evaluate the\nperformance of two state-of-the-art MLLMs in assessing weld acceptability\nacross three contexts: RV \\& Marine, Aeronautical, and Farming. While both\nmodels perform better on online images, likely due to prior exposure or\nmemorization, they also perform relatively well on unseen, real-world weld\nimages. Additionally, we introduce WeldPrompt, a prompting strategy that\ncombines Chain-of-Thought generation with in-context learning to mitigate\nhallucinations and improve reasoning. WeldPrompt improves model recall in\ncertain contexts but exhibits inconsistent performance across others. These\nresults underscore the limitations and potentials of MLLMs in high-stakes\ntechnical domains and highlight the importance of fine-tuning, domain-specific\ndata, and more sophisticated prompting strategies to improve model reliability.\nThe study opens avenues for further research into multimodal learning in\nindustry applications."}
{"id": "2503.16543", "pdf": "https://arxiv.org/pdf/2503.16543", "abs": "https://arxiv.org/abs/2503.16543", "authors": ["Hanae Elmekki", "Saidul Islam", "Ahmed Alagha", "Hani Sami", "Amanda Spilkin", "Ehsan Zakeri", "Antonela Mariel Zanuttini", "Jamal Bentahar", "Lyes Kadem", "Wen-Fang Xie", "Philippe Pibarot", "Rabeb Mizouni", "Hadi Otrok", "Shakti Singh", "Azzam Mourad"], "title": "Comprehensive Review of Reinforcement Learning for Medical Ultrasound Imaging", "categories": ["eess.IV", "cs.CV"], "comment": "89 pages, 23 figures", "summary": "Medical Ultrasound (US) imaging has seen increasing demands over the past\nyears, becoming one of the most preferred imaging modalities in clinical\npractice due to its affordability, portability, and real-time capabilities.\nHowever, it faces several challenges that limit its applicability, such as\noperator dependency, variability in interpretation, and limited resolution,\nwhich are amplified by the low availability of trained experts. This calls for\nthe need of autonomous systems that are capable of reducing the dependency on\nhumans for increased efficiency and throughput. Reinforcement Learning (RL)\ncomes as a rapidly advancing field under Artificial Intelligence (AI) that\nallows the development of autonomous and intelligent agents that are capable of\nexecuting complex tasks through rewarded interactions with their environments.\nExisting surveys on advancements in the US scanning domain predominantly focus\non partially autonomous solutions leveraging AI for scanning guidance, organ\nidentification, plane recognition, and diagnosis. However, none of these\nsurveys explore the intersection between the stages of the US process and the\nrecent advancements in RL solutions. To bridge this gap, this review proposes a\ncomprehensive taxonomy that integrates the stages of the US process with the RL\ndevelopment pipeline. This taxonomy not only highlights recent RL advancements\nin the US domain but also identifies unresolved challenges crucial for\nachieving fully autonomous US systems. This work aims to offer a thorough\nreview of current research efforts, highlighting the potential of RL in\nbuilding autonomous US solutions while identifying limitations and\nopportunities for further advancements in this field."}
{"id": "2503.16556", "pdf": "https://arxiv.org/pdf/2503.16556", "abs": "https://arxiv.org/abs/2503.16556", "authors": ["Sabeen Ahmed", "Nathan Parker", "Margaret Park", "Daniel Jeong", "Lauren Peres", "Evan W. Davis", "Jennifer B. Permuth", "Erin Siegel", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "title": "Reliable Radiologic Skeletal Muscle Area Assessment -- A Biomarker for Cancer Cachexia Diagnosis", "categories": ["eess.IV", "cs.AI", "cs.CE", "cs.CV"], "comment": "47 pages, 19 figures, 9 Tables", "summary": "Cancer cachexia is a common metabolic disorder characterized by severe muscle\natrophy which is associated with poor prognosis and quality of life. Monitoring\nskeletal muscle area (SMA) longitudinally through computed tomography (CT)\nscans, an imaging modality routinely acquired in cancer care, is an effective\nway to identify and track this condition. However, existing tools often lack\nfull automation and exhibit inconsistent accuracy, limiting their potential for\nintegration into clinical workflows. To address these challenges, we developed\nSMAART-AI (Skeletal Muscle Assessment-Automated and Reliable Tool-based on AI),\nan end-to-end automated pipeline powered by deep learning models (nnU-Net 2D)\ntrained on mid-third lumbar level CT images with 5-fold cross-validation,\nensuring generalizability and robustness. SMAART-AI incorporates an\nuncertainty-based mechanism to flag high-error SMA predictions for expert\nreview, enhancing reliability. We combined the SMA, skeletal muscle index, BMI,\nand clinical data to train a multi-layer perceptron (MLP) model designed to\npredict cachexia at the time of cancer diagnosis. Tested on the\ngastroesophageal cancer dataset, SMAART-AI achieved a Dice score of 97.80% +/-\n0.93%, with SMA estimated across all four datasets in this study at a median\nabsolute error of 2.48% compared to manual annotations with SliceOmatic.\nUncertainty metrics-variance, entropy, and coefficient of variation-strongly\ncorrelated with SMA prediction errors (0.83, 0.76, and 0.73 respectively). The\nMLP model predicts cachexia with 79% precision, providing clinicians with a\nreliable tool for early diagnosis and intervention. By combining automation,\naccuracy, and uncertainty awareness, SMAART-AI bridges the gap between research\nand clinical application, offering a transformative approach to managing cancer\ncachexia."}
{"id": "2503.16585", "pdf": "https://arxiv.org/pdf/2503.16585", "abs": "https://arxiv.org/abs/2503.16585", "authors": ["Hadi Amini", "Md Jueal Mia", "Yasaman Saadati", "Ahmed Imteaj", "Seyedsina Nabavirazavi", "Urmish Thakker", "Md Zarif Hossain", "Awal Ahmed Fime", "S. S. Iyengar"], "title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions", "categories": ["cs.CL", "cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs."}
{"id": "2503.16629", "pdf": "https://arxiv.org/pdf/2503.16629", "abs": "https://arxiv.org/abs/2503.16629", "authors": ["Julian Ziegler", "Patrick Frenzel", "Mirco Fuchs"], "title": "Utilizing Reinforcement Learning for Bottom-Up part-wise Reconstruction of 2D Wire-Frame Projections", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted to RLDM 2025", "summary": "This work concerns itself with the task of reconstructing all edges of an\narbitrary 3D wire-frame model projected to an image plane. We explore a\nbottom-up part-wise procedure undertaken by an RL agent to segment and\nreconstruct these 2D multipart objects. The environment's state is represented\nas a four-colour image, where different colours correspond to background, a\ntarget edge, a reconstruction line, and the overlap of both. At each step, the\nagent can transform the reconstruction line within a four-dimensional action\nspace or terminate the episode using a specific termination action. To\ninvestigate the impact of reward function formulations, we tested episodic and\nincremental rewards, as well as combined approaches. Empirical results\ndemonstrated that the latter yielded the most effective training performance.\nTo further enhance efficiency and stability, we introduce curriculum learning\nstrategies. First, an action-based curriculum was implemented, where the agent\nwas initially restricted to a reduced action space, being able to only perform\nthree of the five possible actions, before progressing to the full action\nspace. Second, we test a task-based curriculum, where the agent first solves a\nsimplified version of the problem before being presented with the full, more\ncomplex task. This second approach produced promising results, as the agent not\nonly successfully transitioned from learning the simplified task to mastering\nthe full task, but in doing so gained significant performance. This study\ndemonstrates the potential of an iterative RL wire-frame reconstruction in two\ndimensions. By combining optimized reward function formulations with curriculum\nlearning strategies, we achieved significant improvements in training success.\nThe proposed methodology provides an effective framework for solving similar\ntasks and represents a promising direction for future research in the field."}
{"id": "2503.16630", "pdf": "https://arxiv.org/pdf/2503.16630", "abs": "https://arxiv.org/abs/2503.16630", "authors": ["Dana Cohen-Bar", "Daniel Cohen-Or", "Gal Chechik", "Yoni Kasten"], "title": "TriTex: Learning Texture from a Single Mesh via Triplane Semantic Features", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://danacohen95.github.io/TriTex/", "summary": "As 3D content creation continues to grow, transferring semantic textures\nbetween 3D meshes remains a significant challenge in computer graphics. While\nrecent methods leverage text-to-image diffusion models for texturing, they\noften struggle to preserve the appearance of the source texture during texture\ntransfer. We present \\ourmethod, a novel approach that learns a volumetric\ntexture field from a single textured mesh by mapping semantic features to\nsurface colors. Using an efficient triplane-based architecture, our method\nenables semantic-aware texture transfer to a novel target mesh. Despite\ntraining on just one example, it generalizes effectively to diverse shapes\nwithin the same category. Extensive evaluation on our newly created benchmark\ndataset shows that \\ourmethod{} achieves superior texture transfer quality and\nfast inference times compared to existing methods. Our approach advances\nsingle-example texture transfer, providing a practical solution for maintaining\nvisual coherence across related 3D models in applications like game development\nand simulation."}
{"id": "2503.16635", "pdf": "https://arxiv.org/pdf/2503.16635", "abs": "https://arxiv.org/abs/2503.16635", "authors": ["Yinchi Zhou", "Huidong Xie", "Menghua Xia", "Qiong Liu", "Bo Zhou", "Tianqi Chen", "Jun Hou", "Liang Guo", "Xinyuan Zheng", "Hanzhong Wang", "Biao Li", "Axel Rominger", "Kuangyu Shi", "Nicha C. Dvorneka", "Chi Liu"], "title": "Fed-NDIF: A Noise-Embedded Federated Diffusion Model For Low-Count Whole-Body PET Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Low-count positron emission tomography (LCPET) imaging can reduce patients'\nexposure to radiation but often suffers from increased image noise and reduced\nlesion detectability, necessitating effective denoising techniques. Diffusion\nmodels have shown promise in LCPET denoising for recovering degraded image\nquality. However, training such models requires large and diverse datasets,\nwhich are challenging to obtain in the medical domain. To address data scarcity\nand privacy concerns, we combine diffusion models with federated learning -- a\ndecentralized training approach where models are trained individually at\ndifferent sites, and their parameters are aggregated on a central server over\nmultiple iterations. The variation in scanner types and image noise levels\nwithin and across institutions poses additional challenges for federated\nlearning in LCPET denoising. In this study, we propose a novel noise-embedded\nfederated learning diffusion model (Fed-NDIF) to address these challenges,\nleveraging a multicenter dataset and varying count levels. Our approach\nincorporates liver normalized standard deviation (NSTD) noise embedding into a\n2.5D diffusion model and utilizes the Federated Averaging (FedAvg) algorithm to\naggregate locally trained models into a global model, which is subsequently\nfine-tuned on local datasets to optimize performance and obtain personalized\nmodels. Extensive validation on datasets from the University of Bern, Ruijin\nHospital in Shanghai, and Yale-New Haven Hospital demonstrates the superior\nperformance of our method in enhancing image quality and improving lesion\nquantification. The Fed-NDIF model shows significant improvements in PSNR,\nSSIM, and NMSE of the entire 3D volume, as well as enhanced lesion\ndetectability and quantification, compared to local diffusion models and\nfederated UNet-based models."}
{"id": "2503.16711", "pdf": "https://arxiv.org/pdf/2503.16711", "abs": "https://arxiv.org/abs/2503.16711", "authors": ["Mihaela-Larisa Clement", "Mónika Farsang", "Felix Resch", "Radu Grosu"], "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Submitted to IROS 2025", "summary": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task."}
{"id": "2503.16747", "pdf": "https://arxiv.org/pdf/2503.16747", "abs": "https://arxiv.org/abs/2503.16747", "authors": ["Chiara Schiavo", "Elena Camuffo", "Leonardo Badia", "Simone Milani"], "title": "SAGE: Semantic-Driven Adaptive Gaussian Splatting in Extended Reality", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has significantly improved the efficiency and\nrealism of three-dimensional scene visualization in several applications,\nranging from robotics to eXtended Reality (XR). This work presents SAGE\n(Semantic-Driven Adaptive Gaussian Splatting in Extended Reality), a novel\nframework designed to enhance the user experience by dynamically adapting the\nLevel of Detail (LOD) of different 3DGS objects identified via a semantic\nsegmentation. Experimental results demonstrate how SAGE effectively reduces\nmemory and computational overhead while keeping a desired target visual\nquality, thus providing a powerful optimization for interactive XR\napplications."}
{"id": "2503.16759", "pdf": "https://arxiv.org/pdf/2503.16759", "abs": "https://arxiv.org/abs/2503.16759", "authors": ["Yancheng Cai", "Ali Bozorgian", "Maliha Ashraf", "Robert Wanat", "Rafał K. Mantiuk"], "title": "elaTCSF: A Temporal Contrast Sensitivity Function for Flicker Detection and Modeling Variable Refresh Rate Flicker", "categories": ["cs.GR", "cs.CV"], "comment": "Published at SIGGRAPH Asia 2024", "summary": "The perception of flicker has been a prominent concern in illumination and\nelectronic display fields for over a century. Traditional approaches often rely\non Critical Flicker Frequency (CFF), primarily suited for high-contrast\n(full-on, full-off) flicker. To tackle varying contrast flicker, the\nInternational Committee for Display Metrology (ICDM) introduced a Temporal\nContrast Sensitivity Function TCSF$_{IDMS}$ within the Information Display\nMeasurements Standard (IDMS). Nevertheless, this standard overlooks crucial\nparameters: luminance, eccentricity, and area. Existing models incorporating\nthese parameters are inadequate for flicker detection, especially at low\nspatial frequencies. To address these limitations, we extend the TCSF$_{IDMS}$\nand combine it with a new spatial probability summation model to incorporate\nthe effects of luminance, eccentricity, and area (elaTCSF). We train the\nelaTCSF on various flicker detection datasets and establish the first variable\nrefresh rate flicker detection dataset for further verification. Additionally,\nwe contribute to resolving a longstanding debate on whether the flicker is more\nvisible in peripheral vision. We demonstrate how elaTCSF can be used to predict\nflicker due to low-persistence in VR headsets, identify flicker-free VRR\noperational ranges, and determine flicker sensitivity in lighting design."}
{"id": "2503.16801", "pdf": "https://arxiv.org/pdf/2503.16801", "abs": "https://arxiv.org/abs/2503.16801", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Saeed Mian"], "title": "Auto-Regressive Diffusion for Generating 3D Human-Object Interactions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging\nfield with applications in animation, video games, virtual reality, and\nrobotics. A key challenge in HOI generation is maintaining interaction\nconsistency in long sequences. Existing Text-to-Motion-based approaches, such\nas discrete motion tokenization, cannot be directly applied to HOI generation\ndue to limited data in this domain and the complexity of the modality. To\naddress the problem of interaction consistency in long sequences, we propose an\nautoregressive diffusion model (ARDHOI) that predicts the next continuous\ntoken. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)\nto learn a physically plausible space of continuous HOI tokens, thereby\nensuring that generated human-object motions are realistic and natural. For\ngenerating sequences autoregressively, we develop a Mamba-based context encoder\nto capture and maintain consistent sequential actions. Additionally, we\nimplement an MLP-based denoiser to generate the subsequent token conditioned on\nthe encoded context. Our model has been evaluated on the OMOMO and BEHAVE\ndatasets, where it outperforms existing state-of-the-art methods in terms of\nboth performance and inference speed. This makes ARDHOI a robust and efficient\nsolution for text-driven HOI tasks"}
{"id": "2503.16818", "pdf": "https://arxiv.org/pdf/2503.16818", "abs": "https://arxiv.org/abs/2503.16818", "authors": ["Shunki Tatsumi", "Ryo Hayakawa", "Youji Iiguni"], "title": "Depth-Aided Color Image Inpainting in Quaternion Domain", "categories": ["eess.IV", "cs.CV"], "comment": "accepted to IEEE Signal Processing Letters", "summary": "In this paper, we propose a depth-aided color image inpainting method in the\nquaternion domain, called depth-aided low-rank quaternion matrix completion\n(D-LRQMC). In conventional quaternion-based inpainting techniques, the color\nimage is expressed as a quaternion matrix by using the three imaginary parts as\nthe color channels, whereas the real part is set to zero and has no\ninformation. Our approach incorporates depth information as the real part of\nthe quaternion representations, leveraging the correlation between color and\ndepth to improve the result of inpainting. In the proposed method, we first\nrestore the observed image with the conventional LRQMC and estimate the depth\nof the restored result. We then incorporate the estimated depth into the real\npart of the observed image and perform LRQMC again. Simulation results\ndemonstrate that the proposed D-LRQMC can improve restoration accuracy and\nvisual quality for various images compared to the conventional LRQMC. These\nresults suggest the effectiveness of the depth information for color image\nprocessing in quaternion domain."}
{"id": "2503.16842", "pdf": "https://arxiv.org/pdf/2503.16842", "abs": "https://arxiv.org/abs/2503.16842", "authors": ["Basar Demir", "Soumitri Chattopadhyay", "Thomas Hastings Greer", "Boqi Chen", "Marc Niethammer"], "title": "Downstream Analysis of Foundational Medical Vision Models for Disease Progression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical vision foundational models are used for a wide variety of tasks,\nincluding medical image segmentation and registration. This work evaluates the\nability of these models to predict disease progression using a simple linear\nprobe. We hypothesize that intermediate layer features of segmentation models\ncapture structural information, while those of registration models encode\nknowledge of change over time. Beyond demonstrating that these features are\nuseful for disease progression prediction, we also show that registration model\nfeatures do not require spatially aligned input images. However, for\nsegmentation models, spatial alignment is essential for optimal performance.\nOur findings highlight the importance of spatial alignment and the utility of\nfoundation model features for image registration."}
{"id": "2503.16848", "pdf": "https://arxiv.org/pdf/2503.16848", "abs": "https://arxiv.org/abs/2503.16848", "authors": ["Hou In Derek Pun", "Hou In Ivan Tam", "Austin T. Wang", "Xiaoliang Huo", "Angel X. Chang", "Manolis Savva"], "title": "HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation", "categories": ["cs.GR", "cs.CV"], "comment": "23 pages, 7 figures", "summary": "Despite advances in indoor 3D scene layout generation, synthesizing scenes\nwith dense object arrangements remains challenging. Existing methods primarily\nfocus on large furniture while neglecting smaller objects, resulting in\nunrealistically empty scenes. Those that place small objects typically do not\nhonor arrangement specifications, resulting in largely random placement not\nfollowing the text description. We present HSM, a hierarchical framework for\nindoor scene generation with dense object arrangements across spatial scales.\nIndoor scenes are inherently hierarchical, with surfaces supporting objects at\ndifferent scales, from large furniture on floors to smaller objects on tables\nand shelves. HSM embraces this hierarchy and exploits recurring cross-scale\nspatial patterns to generate complex and realistic indoor scenes in a unified\nmanner. Our experiments show that HSM outperforms existing methods by\ngenerating scenes that are more realistic and better conform to user input\nacross room types and spatial configurations."}
{"id": "2503.16862", "pdf": "https://arxiv.org/pdf/2503.16862", "abs": "https://arxiv.org/abs/2503.16862", "authors": ["Yiqiang Cai", "Yizhou Tan", "Peihong Zhang", "Yuxuan Liu", "Shengchen Li", "Xi Shao", "Mark D. Plumbley"], "title": "City2Scene: Improving Acoustic Scene Classification with City Features", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Acoustic scene recordings are often collected from a diverse range of cities.\nMost existing acoustic scene classification (ASC) approaches focus on\nidentifying common acoustic scene patterns across cities to enhance\ngeneralization. In contrast, we hypothesize that city-specific environmental\nand cultural differences in acoustic features are beneficial for the ASC task.\nIn this paper, we introduce City2Scene, a novel framework that leverages city\nfeatures to improve ASC. City2Scene transfers the city-specific knowledge from\ncity classification models to a scene classification model using knowledge\ndistillation. We evaluated City2Scene on the DCASE Challenge Task 1 datasets,\nwhere each audio clip is annotated with both scene and city labels.\nExperimental results demonstrate that city features provide valuable\ninformation for classifying scenes. By distilling the city-specific knowledge,\nCity2Scene effectively improves accuracy for various state-of-the-art ASC\nbackbone models, including both CNNs and Transformers."}
{"id": "2503.16868", "pdf": "https://arxiv.org/pdf/2503.16868", "abs": "https://arxiv.org/abs/2503.16868", "authors": ["Mengsay Loem", "Taiju Hosaka"], "title": "Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visual question answering (VQA) has emerged as a flexible approach for\nextracting specific pieces of information from document images. However,\nexisting work typically queries each field in isolation, overlooking potential\ndependencies across multiple items. This paper investigates the merits of\nextracting multiple fields jointly versus separately. Through experiments on\nmultiple large vision language models and datasets, we show that jointly\nextracting fields often improves accuracy, especially when the fields share\nstrong numeric or contextual dependencies. We further analyze how performance\nscales with the number of requested items and use a regression based metric to\nquantify inter field relationships. Our results suggest that multi field\nprompts can mitigate confusion arising from similar surface forms and related\nnumeric values, providing practical methods for designing robust VQA systems in\ndocument information extraction tasks."}
{"id": "2503.16872", "pdf": "https://arxiv.org/pdf/2503.16872", "abs": "https://arxiv.org/abs/2503.16872", "authors": ["Xuan Wang", "Siyuan Liang", "Dongping Liao", "Han Fang", "Aishan Liu", "Xiaochun Cao", "Yu-liang Lu", "Ee-Chien Chang", "Xitong Gao"], "title": "Lie Detector: Unified Backdoor Detection via Cross-Examination Framework", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Institutions with limited data and computing resources often outsource model\ntraining to third-party providers in a semi-honest setting, assuming adherence\nto prescribed training protocols with pre-defined learning paradigm (e.g.,\nsupervised or semi-supervised learning). However, this practice can introduce\nsevere security risks, as adversaries may poison the training data to embed\nbackdoors into the resulting model. Existing detection approaches predominantly\nrely on statistical analyses, which often fail to maintain universally accurate\ndetection accuracy across different learning paradigms. To address this\nchallenge, we propose a unified backdoor detection framework in the semi-honest\nsetting that exploits cross-examination of model inconsistencies between two\nindependent service providers. Specifically, we integrate central kernel\nalignment to enable robust feature similarity measurements across different\nmodel architectures and learning paradigms, thereby facilitating precise\nrecovery and identification of backdoor triggers. We further introduce backdoor\nfine-tuning sensitivity analysis to distinguish backdoor triggers from\nadversarial perturbations, substantially reducing false positives. Extensive\nexperiments demonstrate that our method achieves superior detection\nperformance, improving accuracy by 5.4%, 1.6%, and 11.9% over SoTA baselines\nacross supervised, semi-supervised, and autoregressive learning tasks,\nrespectively. Notably, it is the first to effectively detect backdoors in\nmultimodal large language models, further highlighting its broad applicability\nand advancing secure deep learning."}
{"id": "2503.16956", "pdf": "https://arxiv.org/pdf/2503.16956", "abs": "https://arxiv.org/abs/2503.16956", "authors": ["Ji-Hoon Kim", "Jeongsoo Choi", "Jaehun Kim", "Chaeyoung Jung", "Joon Son Chung"], "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.SD"], "comment": "CVPR 2025, demo page: https://mm.kaist.ac.kr/projects/faces2voices/", "summary": "The objective of this study is to generate high-quality speech from silent\ntalking face videos, a task also known as video-to-speech synthesis. A\nsignificant challenge in video-to-speech synthesis lies in the substantial\nmodality gap between silent video and multi-faceted speech. In this paper, we\npropose a novel video-to-speech system that effectively bridges this modality\ngap, significantly enhancing the quality of synthesized speech. This is\nachieved by learning of hierarchical representations from video to speech.\nSpecifically, we gradually transform silent video into acoustic feature spaces\nthrough three sequential stages -- content, timbre, and prosody modeling. In\neach stage, we align visual factors -- lip movements, face identity, and facial\nexpressions -- with corresponding acoustic counterparts to ensure the seamless\ntransformation. Additionally, to generate realistic and coherent speech from\nthe visual representations, we employ a flow matching model that estimates\ndirect trajectories from a simple prior distribution to the target speech\ndistribution. Extensive experiments demonstrate that our method achieves\nexceptional generation quality comparable to real utterances, outperforming\nexisting methods by a significant margin."}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965", "abs": "https://arxiv.org/abs/2503.16965", "authors": ["Zhe Hu", "Jing Li", "Yu Yin"], "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms."}
{"id": "2503.16988", "pdf": "https://arxiv.org/pdf/2503.16988", "abs": "https://arxiv.org/abs/2503.16988", "authors": ["Ying Ming", "Shaoze Luo", "Longfei Zhao", "Qiqi Xu", "Wei Song"], "title": "High Accuracy Pulmonary Vessel Segmentation for Contrast and Non-contrast CT Images and Its Clinical Evaluation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of pulmonary vessels plays a very critical role in\ndiagnosing and assessing various lung diseases. In clinical practice, diagnosis\nis typically carried out using CTPA images. However, there is a lack of\nhigh-precision pulmonary vessel segmentation algorithms for CTPA, and pulmonary\nvessel segmentation for NCCT poses an even greater challenge. In this study, we\npropose a 3D image segmentation algorithm for automated pulmonary vessel\nsegmentation from both contrast and non-contrast CT images. In the network, we\ndesigned a Vessel Lumen Structure Optimization Module (VLSOM), which extracts\nthe centerline of vessels and adjusts the weights based on the positional\ninformation and adds a Cl-Dice-Loss to supervise the stability of the vessels\nstructure. In addition, we designed a method for generating vessel GT from CTPA\nto NCCT for training models that support both CTPA and NCCT. In this work, we\nused 427 sets of high-precision annotated CT data from multiple vendors and\ncountries. Finally, our experimental model achieved Cl-Recall, Cl-DICE and\nRecall values of 0.879, 0.909, 0.934 (CTPA) and 0.928, 0.936, 0.955 (NCCT)\nrespectively. This shows that our model has achieved good performance in both\naccuracy and completeness of pulmonary vessel segmentation. In clinical visual\nevaluation, our model also had good segmentation performance on various disease\ntypes and can assist doctors in medical diagnosis, verifying the great\npotential of this method in clinical application."}
{"id": "2503.17017", "pdf": "https://arxiv.org/pdf/2503.17017", "abs": "https://arxiv.org/abs/2503.17017", "authors": ["Aoting Zhang", "Dongbao Yang", "Chang Liu", "Xiaopeng Hong", "Yu Zhou"], "title": "Specifying What You Know or Not for Multi-Label Class-Incremental Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Existing class incremental learning is mainly designed for single-label\nclassification task, which is ill-equipped for multi-label scenarios due to the\ninherent contradiction of learning objectives for samples with incomplete\nlabels. We argue that the main challenge to overcome this contradiction in\nmulti-label class-incremental learning (MLCIL) lies in the model's inability to\nclearly distinguish between known and unknown knowledge. This ambiguity hinders\nthe model's ability to retain historical knowledge, master current classes, and\nprepare for future learning simultaneously. In this paper, we target at\nspecifying what is known or not to accommodate Historical, Current, and\nProspective knowledge for MLCIL and propose a novel framework termed as HCP.\nSpecifically, (i) we clarify the known classes by dynamic feature purification\nand recall enhancement with distribution prior, enhancing the precision and\nretention of known information. (ii) We design prospective knowledge mining to\nprobe the unknown, preparing the model for future learning. Extensive\nexperiments validate that our method effectively alleviates catastrophic\nforgetting in MLCIL, surpassing the previous state-of-the-art by 3.3% on\naverage accuracy for MS-COCO B0-C10 setting without replay buffers."}
{"id": "2503.17024", "pdf": "https://arxiv.org/pdf/2503.17024", "abs": "https://arxiv.org/abs/2503.17024", "authors": ["David Mildenberger", "Paul Hager", "Daniel Rueckert", "Martin J Menten"], "title": "A Tale of Two Classes: Adapting Supervised Contrastive Learning to Binary Imbalanced Datasets", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Supervised contrastive learning (SupCon) has proven to be a powerful\nalternative to the standard cross-entropy loss for classification of\nmulti-class balanced datasets. However, it struggles to learn well-conditioned\nrepresentations of datasets with long-tailed class distributions. This problem\nis potentially exacerbated for binary imbalanced distributions, which are\ncommonly encountered during many real-world problems such as medical diagnosis.\nIn experiments on seven binary datasets of natural and medical images, we show\nthat the performance of SupCon decreases with increasing class imbalance. To\nsubstantiate these findings, we introduce two novel metrics that evaluate the\nquality of the learned representation space. By measuring the class\ndistribution in local neighborhoods, we are able to uncover structural\ndeficiencies of the representation space that classical metrics cannot detect.\nInformed by these insights, we propose two new supervised contrastive learning\nstrategies tailored to binary imbalanced datasets that improve the structure of\nthe representation space and increase downstream classification accuracy over\nstandard SupCon by up to 35%. We make our code available."}
{"id": "2503.17030", "pdf": "https://arxiv.org/pdf/2503.17030", "abs": "https://arxiv.org/abs/2503.17030", "authors": ["Snigdha Paul", "Sambit Mallick", "Anindya Sen"], "title": "Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes."}
{"id": "2503.17046", "pdf": "https://arxiv.org/pdf/2503.17046", "abs": "https://arxiv.org/abs/2503.17046", "authors": ["Dongsheng Yang", "Qianying Liu", "Wataru Sato", "Takashi Minato", "Chaoran Liu", "Shin'ya Nishida"], "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses."}
{"id": "2503.17057", "pdf": "https://arxiv.org/pdf/2503.17057", "abs": "https://arxiv.org/abs/2503.17057", "authors": ["Fangyijie Wang", "Kathleen M. Curran", "Guénolé Silvestre"], "title": "Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted for an oral presentation at ISBI 2025 Fetal Ultrasound Grand\n  Challenge: Semi-Supervised Cervical Segmentation", "summary": "Accurate segmentation of ultrasound (US) images of the cervical muscles is\ncrucial for precision healthcare. The demand for automatic computer-assisted\nmethods is high. However, the scarcity of labeled data hinders the development\nof these methods. Advanced semi-supervised learning approaches have displayed\npromise in overcoming this challenge by utilizing labeled and unlabeled data.\nThis study introduces a novel semi-supervised learning (SSL) framework that\nintegrates dual neural networks. This SSL framework utilizes both networks to\ngenerate pseudo-labels and cross-supervise each other at the pixel level.\nAdditionally, a self-supervised contrastive learning strategy is introduced,\nwhich employs a pair of deep representations to enhance feature learning\ncapabilities, particularly on unlabeled data. Our framework demonstrates\ncompetitive performance in cervical segmentation tasks. Our codes are publicly\navailable on https://github.com/13204942/SSL\\_Cervical\\_Segmentation."}
{"id": "2503.17059", "pdf": "https://arxiv.org/pdf/2503.17059", "abs": "https://arxiv.org/abs/2503.17059", "authors": ["Yongkang Cheng", "Shaoli Huang", "Xuelin Chen", "Jifeng Ning", "Mingming Gong"], "title": "DIDiffGes: Decoupled Semi-Implicit Diffusion Models for Real-time Gesture Generation from Speech", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted by AAAI 2025", "summary": "Diffusion models have demonstrated remarkable synthesis quality and diversity\nin generating co-speech gestures. However, the computationally intensive\nsampling steps associated with diffusion models hinder their practicality in\nreal-world applications. Hence, we present DIDiffGes, for a Decoupled\nSemi-Implicit Diffusion model-based framework, that can synthesize\nhigh-quality, expressive gestures from speech using only a few sampling steps.\nOur approach leverages Generative Adversarial Networks (GANs) to enable\nlarge-step sampling for diffusion model. We decouple gesture data into body and\nhands distributions and further decompose them into marginal and conditional\ndistributions. GANs model the marginal distribution implicitly, while L2\nreconstruction loss learns the conditional distributions exciplictly. This\nstrategy enhances GAN training stability and ensures expressiveness of\ngenerated full-body gestures. Our framework also learns to denoise root noise\nconditioned on local body representation, guaranteeing stability and realism.\nDIDiffGes can generate gestures from speech with just 10 sampling steps,\nwithout compromising quality and expressiveness, reducing the number of\nsampling steps by a factor of 100 compared to existing methods. Our user study\nreveals that our method outperforms state-of-the-art approaches in human\nlikeness, appropriateness, and style correctness. Project is\nhttps://cyk990422.github.io/DIDiffGes."}
{"id": "2503.17089", "pdf": "https://arxiv.org/pdf/2503.17089", "abs": "https://arxiv.org/abs/2503.17089", "authors": ["Tiarna Lee", "Esther Puyol-Antón", "Bram Ruijsink", "Miaojing Shi", "Andrew P. King"], "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further."}
{"id": "2503.17095", "pdf": "https://arxiv.org/pdf/2503.17095", "abs": "https://arxiv.org/abs/2503.17095", "authors": ["Kwan Yun", "Chaelin Kim", "Hangyeul Shin", "Junyong Noh"], "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.3; I.3.8"], "comment": "CVPR2025, 11 pages, 14 figures", "summary": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}."}
{"id": "2503.17105", "pdf": "https://arxiv.org/pdf/2503.17105", "abs": "https://arxiv.org/abs/2503.17105", "authors": ["Marco Usai", "Andrea Loddo", "Alessandra Perniciano", "Maurizio Atzori", "Cecilia Di Ruberto"], "title": "A Comparative Analysis of Image Descriptors for Histopathological Classification of Gastric Cancer", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Gastric cancer ranks as the fifth most common and fourth most lethal cancer\nglobally, with a dismal 5-year survival rate of approximately 20%. Despite\nextensive research on its pathobiology, the prognostic predictability remains\ninadequate, compounded by pathologists' high workload and potential diagnostic\nerrors. Thus, automated, accurate histopathological diagnosis tools are\ncrucial. This study employs Machine Learning and Deep Learning techniques to\nclassify histopathological images into healthy and cancerous categories. Using\nhandcrafted and deep features with shallow learning classifiers on the\nGasHisSDB dataset, we offer a comparative analysis and insights into the most\nrobust and high-performing combinations of features and classifiers for\ndistinguishing between normal and abnormal histopathological images without\nfine-tuning strategies. With the RF classifier, our approach can reach F1 of\n93.4%, demonstrating its validity."}
{"id": "2503.17107", "pdf": "https://arxiv.org/pdf/2503.17107", "abs": "https://arxiv.org/abs/2503.17107", "authors": ["Davide Antonio Mura", "Michela Pinna", "Lorenzo Putzu", "Andrea Loddo", "Alessandra Perniciano", "Olga Mulas", "Cecilia Di Ruberto"], "title": "Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study of Leukocytes and Schistocytes", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The detection of blood disorders often hinges upon the quantification of\nspecific blood cell types. Variations in cell counts may indicate the presence\nof pathological conditions. Thus, the significance of developing precise\nautomatic systems for blood cell enumeration is underscored. The investigation\nfocuses on a novel approach termed DE-ViT. This methodology is employed in a\nFew-Shot paradigm, wherein training relies on a limited number of images. Two\ndistinct datasets are utilised for experimental purposes: the Raabin-WBC\ndataset for Leukocyte detection and a local dataset for Schistocyte\nidentification. In addition to the DE-ViT model, two baseline models, Faster\nR-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being\ncompared against those of the proposed model. While DE-ViT has demonstrated\nstate-of-the-art performance on the COCO and LVIS datasets, both baseline\nmodels surpassed its performance on the Raabin-WBC dataset. Moreover, only\nFaster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed\ndisparities in performance may possibly be attributed to domain shift\nphenomena."}
{"id": "2503.17116", "pdf": "https://arxiv.org/pdf/2503.17116", "abs": "https://arxiv.org/abs/2503.17116", "authors": ["Luca Rossetto", "Werner Bailer", "Duc-Tien Dang-Nguyen", "Graham Healy", "Björn Þór Jónsson", "Onanong Kongmeesub", "Hoang-Bao Le", "Stevan Rudinac", "Klaus Schöffmann", "Florian Spiess", "Allie Tran", "Minh-Triet Tran", "Quang-Linh Tran", "Cathal Gurrin"], "title": "The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.IR"], "comment": "7 pages, 6 figures, dataset available via\n  https://castle-dataset.github.io/", "summary": "Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/."}
{"id": "2503.17117", "pdf": "https://arxiv.org/pdf/2503.17117", "abs": "https://arxiv.org/abs/2503.17117", "authors": ["Théo Bodrito", "Olivier Flasseur", "Julien Mairal", "Jean Ponce", "Maud Langlois", "Anne-Marie Lagrange"], "title": "A New Statistical Model of Star Speckles for Learning to Detect and Characterize Exoplanets in Direct Imaging Observations", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.CV", "cs.LG", "stat.AP"], "comment": "Accepted to CVPR 2025", "summary": "The search for exoplanets is an active field in astronomy, with direct\nimaging as one of the most challenging methods due to faint exoplanet signals\nburied within stronger residual starlight. Successful detection requires\nadvanced image processing to separate the exoplanet signal from this nuisance\ncomponent. This paper presents a novel statistical model that captures nuisance\nfluctuations using a multi-scale approach, leveraging problem symmetries and a\njoint spectral channel representation grounded in physical principles. Our\nmodel integrates into an interpretable, end-to-end learnable framework for\nsimultaneous exoplanet detection and flux estimation. The proposed algorithm is\nevaluated against the state of the art using datasets from the SPHERE\ninstrument operating at the Very Large Telescope (VLT). It significantly\nimproves the precision-recall trade-off, notably on challenging datasets that\nare otherwise unusable by astronomers. The proposed approach is computationally\nefficient, robust to varying data quality, and well suited for large-scale\nobservational surveys."}
{"id": "2503.17198", "pdf": "https://arxiv.org/pdf/2503.17198", "abs": "https://arxiv.org/abs/2503.17198", "authors": ["Yongli Xiang", "Ziming Hong", "Lina Yao", "Dadong Wang", "Tongliang Liu"], "title": "Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "Code is released at https://github.com/tmllab/2025_CVPR_JailNTL", "summary": "Non-transferable learning (NTL) has been proposed to protect model\nintellectual property (IP) by creating a \"non-transferable barrier\" to restrict\ngeneralization from authorized to unauthorized domains. Recently, well-designed\nattack, which restores the unauthorized-domain performance by fine-tuning NTL\nmodels on few authorized samples, highlights the security risks of NTL-based\napplications. However, such attack requires modifying model weights, thus being\ninvalid in the black-box scenario. This raises a critical question: can we\ntrust the security of NTL models deployed as black-box systems? In this work,\nwe reveal the first loophole of black-box NTL models by proposing a novel\nattack method (dubbed as JailNTL) to jailbreak the non-transferable barrier\nthrough test-time data disguising. The main idea of JailNTL is to disguise\nunauthorized data so it can be identified as authorized by the NTL model,\nthereby bypassing the non-transferable barrier without modifying the NTL model\nweights. Specifically, JailNTL encourages unauthorized-domain disguising in two\nlevels, including: (i) data-intrinsic disguising (DID) for eliminating domain\ndiscrepancy and preserving class-related content at the input-level, and (ii)\nmodel-guided disguising (MGD) for mitigating output-level statistics difference\nof the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL\nmodels in the black-box scenario, JailNTL achieves an accuracy increase of up\nto 55.7% in the unauthorized domain by using only 1% authorized samples,\nlargely exceeding existing SOTA white-box attacks."}
{"id": "2503.17211", "pdf": "https://arxiv.org/pdf/2503.17211", "abs": "https://arxiv.org/abs/2503.17211", "authors": ["Zilin Dai", "Lehong Wang", "Fangzhou Lin", "Yidong Wang", "Zhigang Li", "Kazunori D Yamada", "Ziming Zhang", "Wang Lu"], "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels."}
{"id": "2503.17244", "pdf": "https://arxiv.org/pdf/2503.17244", "abs": "https://arxiv.org/abs/2503.17244", "authors": ["Jyothi Rikhab Chand", "Mathews Jacob"], "title": "Deep End-to-End Posterior ENergy (DEEPEN) for image recovery", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings."}
{"id": "2503.17261", "pdf": "https://arxiv.org/pdf/2503.17261", "abs": "https://arxiv.org/abs/2503.17261", "authors": ["Jie Mei", "Chenyu Lin", "Yu Qiu", "Yaonan Wang", "Hui Zhang", "Ziyang Wang", "Dong Dai"], "title": "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor Segmentation in PET-CT Images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is\ncrucial for imaging lung tumors, providing essential metabolic and anatomical\ninformation, while it faces challenges such as poor image quality, motion\nartifacts, and complex tumor morphology. Deep learning-based models are\nexpected to address these problems, however, existing small-scale and private\ndatasets limit significant performance improvements for these methods. Hence,\nwe introduce a large-scale PET-CT lung tumor segmentation dataset, termed\nPCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.\nFurthermore, we propose a cross-modal interactive perception network with Mamba\n(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a\nchannel-wise rectification module (CRM) that implements a channel state space\nblock across multi-modal features to learn correlated representations and helps\nfilter out modality-specific noise. A dynamic cross-modality interaction module\n(DCIM) is designed to effectively integrate position and context information,\nwhich employs PET images to learn regional position information and serves as a\nbridge to assist in modeling the relationships between local features of CT\nimages. Extensive experiments on a comprehensive benchmark demonstrate the\neffectiveness of our CIPA compared to the current state-of-the-art segmentation\nmethods. We hope our research can provide more exploration opportunities for\nmedical image segmentation. The dataset and code are available at\nhttps://github.com/mj129/CIPA."}
{"id": "2503.17275", "pdf": "https://arxiv.org/pdf/2503.17275", "abs": "https://arxiv.org/abs/2503.17275", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Jazib", "Zeeshan Alam", "Muhmmad Farhan Khan", "Muhammad Saad", "Muhammad Ali Jamshed"], "title": "Vision Transformer Based Semantic Communications for Next Generation Wireless Networks", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "Accepted @ ICC 2025", "summary": "In the evolving landscape of 6G networks, semantic communications are poised\nto revolutionize data transmission by prioritizing the transmission of semantic\nmeaning over raw data accuracy. This paper presents a Vision Transformer\n(ViT)-based semantic communication framework that has been deliberately\ndesigned to achieve high semantic similarity during image transmission while\nsimultaneously minimizing the demand for bandwidth. By equipping ViT as the\nencoder-decoder framework, the proposed architecture can proficiently encode\nimages into a high semantic content at the transmitter and precisely\nreconstruct the images, considering real-world fading and noise consideration\nat the receiver. Building on the attention mechanisms inherent to ViTs, our\nmodel outperforms Convolution Neural Network (CNNs) and Generative Adversarial\nNetworks (GANs) tailored for generating such images. The architecture based on\nthe proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38\ndB, which is higher than other Deep Learning (DL) approaches in maintaining\nsemantic similarity across different communication environments. These findings\nestablish our ViT-based approach as a significant breakthrough in semantic\ncommunications."}
{"id": "2503.17331", "pdf": "https://arxiv.org/pdf/2503.17331", "abs": "https://arxiv.org/abs/2503.17331", "authors": ["Francisco Tellez", "Enrique Torres-Giese"], "title": "A Topological Data Analysis Framework for Quantifying Necrosis in Glioblastomas", "categories": ["math.AT", "cs.CV"], "comment": null, "summary": "In this paper, we introduce a shape descriptor that we call \"interior\nfunction\". This is a Topological Data Analysis (TDA) based descriptor that\nrefines previous descriptors for image analysis. Using this concept, we define\nsubcomplex lacunarity, a new index that quantifies geometric characteristics of\nnecrosis in tumors such as conglomeration. Building on this framework, we\npropose a set of indices to analyze necrotic morphology and construct a diagram\nthat captures the distinct structural and geometric properties of necrotic\nregions in tumors. We present an application of this framework in the study of\nMRIs of Glioblastomas (GB). Using cluster analysis, we identify four distinct\nsubtypes of Glioblastomas that reflect geometric properties of necrotic\nregions."}
{"id": "2503.17340", "pdf": "https://arxiv.org/pdf/2503.17340", "abs": "https://arxiv.org/abs/2503.17340", "authors": ["Congyi Fan", "Jian Guan", "Xuanjia Zhao", "Dongli Xu", "Youtian Lin", "Tong Ye", "Pengming Feng", "Haiwei Pan"], "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": "10 pages, 6 figures", "summary": "Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ ."}
