{"id": "2505.23969", "pdf": "https://arxiv.org/pdf/2505.23969", "abs": "https://arxiv.org/abs/2505.23969", "authors": ["Otman Benchekroun", "Eitan Grinspun", "Maurizio Chiaramonte", "Philip Allen Etter"], "title": "Force-Dual Modes: Subspace Design from Stochastic Forces", "categories": ["cs.GR"], "comment": "14 pages, 16 figures", "summary": "Designing subspaces for Reduced Order Modeling (ROM) is crucial for\naccelerating finite element simulations in graphics and engineering.\nUnfortunately, it's not always clear which subspace is optimal for arbitrary\ndynamic simulation. We propose to construct simulation subspaces from force\ndistributions, allowing us to tailor such subspaces to common scene\ninteractions involving constraint penalties, handles-based control, contact and\nmusculoskeletal actuation. To achieve this we adopt a statistical perspective\non Reduced Order Modelling, which allows us to push such user-designed force\ndistributions through a linearized simulation to obtain a dual distribution on\ndisplacements. To construct our subspace, we then fit a low-rank Gaussian model\nto this displacement distribution, which we show generalizes Linear Modal\nAnalysis subspaces for uncorrelated unit variance force distributions, as well\nas Green's Function subspaces for low rank force distributions. We show our\nframework allows for the construction of subspaces that are optimal both with\nrespect to physical material properties, as well as arbitrary force\ndistributions as observed in handle-based, contact, and musculoskeletal scene\ninteractions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529b\u5206\u5e03\u6784\u5efa\u964d\u9636\u6a21\u578b\uff08ROM\uff09\u5b50\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u56fe\u5f62\u548c\u5de5\u7a0b\u4e2d\u7684\u6709\u9650\u5143\u6a21\u62df\u52a0\u901f\u3002", "motivation": "\u5728\u52a8\u6001\u6a21\u62df\u4e2d\uff0c\u5982\u4f55\u9009\u62e9\u6700\u4f18\u5b50\u7a7a\u95f4\u5e76\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u5e38\u89c1\u573a\u666f\u4ea4\u4e92\uff08\u5982\u7ea6\u675f\u3001\u63a7\u5236\u3001\u63a5\u89e6\u548c\u808c\u8089\u9aa8\u9abc\u9a71\u52a8\uff09\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u5b66\u89c6\u89d2\uff0c\u901a\u8fc7\u7ebf\u6027\u5316\u6a21\u62df\u5c06\u7528\u6237\u8bbe\u8ba1\u7684\u529b\u5206\u5e03\u8f6c\u5316\u4e3a\u4f4d\u79fb\u5206\u5e03\uff0c\u5e76\u62df\u5408\u4f4e\u79e9\u9ad8\u65af\u6a21\u578b\u6784\u5efa\u5b50\u7a7a\u95f4\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9488\u5bf9\u7269\u7406\u6750\u6599\u7279\u6027\u548c\u4efb\u610f\u529b\u5206\u5e03\uff08\u5982\u63a7\u5236\u3001\u63a5\u89e6\u548c\u808c\u8089\u9aa8\u9abc\u4ea4\u4e92\uff09\u4f18\u5316\u7684\u5b50\u7a7a\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u63a8\u5e7f\u4e86\u7ebf\u6027\u6a21\u6001\u5206\u6790\u548c\u683c\u6797\u51fd\u6570\u5b50\u7a7a\u95f4\uff0c\u8fd8\u80fd\u4e3a\u590d\u6742\u573a\u666f\u4ea4\u4e92\u63d0\u4f9b\u6700\u4f18\u5b50\u7a7a\u95f4\u3002"}}
{"id": "2505.24053", "pdf": "https://arxiv.org/pdf/2505.24053", "abs": "https://arxiv.org/abs/2505.24053", "authors": ["Zixun Huang", "Cho-Ying Wu", "Yuliang Guo", "Xinyu Huang", "Liu Ren"], "title": "3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the\nquality and efficiency of differentiable rendering. However, its high\nefficiency stems from an approximation of projecting 3D Gaussians onto the\nimage plane as 2D Gaussians, which inherently limits rendering\nquality--particularly under large Field-of-View (FoV) camera inputs. While\nseveral recent works have extended 3DGS to mitigate these approximation errors,\nnone have successfully achieved both exactness and high efficiency\nsimultaneously. In this work, we introduce 3DGEER, an Exact and Efficient\nVolumetric Gaussian Rendering method. Starting from first principles, we derive\na closed-form expression for the density integral along a ray traversing a 3D\nGaussian distribution. This formulation enables precise forward rendering with\narbitrary camera models and supports gradient-based optimization of 3D Gaussian\nparameters. To ensure both exactness and real-time performance, we propose an\nefficient method for computing a tight Particle Bounding Frustum (PBF) for each\n3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also\nintroduce a novel Bipolar Equiangular Projection (BEAP) representation to\naccelerate ray association under generic camera models. BEAP further provides a\nmore uniform ray sampling strategy to apply supervision, which empirically\nimproves reconstruction quality. Experiments on multiple pinhole and fisheye\ndatasets show that our method consistently outperforms prior methods,\nestablishing a new state-of-the-art in real-time neural rendering.", "AI": {"tldr": "3DGEER\u662f\u4e00\u79cd\u7cbe\u786e\u4e14\u9ad8\u6548\u7684\u4f53\u79ef\u9ad8\u65af\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c01\u95ed\u5f62\u5f0f\u7684\u5bc6\u5ea6\u79ef\u5206\u548c\u4f18\u5316\u7684\u5c04\u7ebf\u5173\u8054\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u5e73\u8861\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u8fd1\u4f3c\u6295\u5f71\u65b9\u6cd5\u5728\u5927\u89c6\u573a\u89d2\u4e0b\u9650\u5236\u4e86\u6e32\u67d3\u8d28\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u6027\u548c\u9ad8\u6548\u6027\u3002", "method": "3DGEER\u4ece\u57fa\u672c\u539f\u7406\u51fa\u53d1\uff0c\u63a8\u5bfc\u4e863D\u9ad8\u65af\u5206\u5e03\u6cbf\u5c04\u7ebf\u7684\u5bc6\u5ea6\u79ef\u5206\u7684\u5c01\u95ed\u5f62\u5f0f\u8868\u8fbe\u5f0f\uff0c\u63d0\u51fa\u4e86\u7c92\u5b50\u8fb9\u754c\u89c6\u9525\uff08PBF\uff09\u548c\u53cc\u6781\u7b49\u89d2\u6295\u5f71\uff08BEAP\uff09\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4e14\u9ad8\u6548\u7684\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3DGEER\u5728\u9488\u5b54\u548c\u9c7c\u773c\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u795e\u7ecf\u6e32\u67d3\u7684\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "3DGEER\u901a\u8fc7\u7cbe\u786e\u7684\u6570\u5b66\u5efa\u6a21\u548c\u9ad8\u6548\u7684\u5c04\u7ebf\u5173\u8054\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e863DGS\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2505.24653", "pdf": "https://arxiv.org/pdf/2505.24653", "abs": "https://arxiv.org/abs/2505.24653", "authors": ["Moritz Grauer", "Johannes Hanika", "Carsten Dachsbacher"], "title": "Minimizing Ray Tracing Memory Traffic through Quantized Structures and Ray Stream Tracing", "categories": ["cs.GR", "cs.AR"], "comment": null, "summary": "Memory bandwidth constraints continue to be a significant limiting factor in\nray tracing performance, particularly as scene complexity grows and\ncomputational capabilities outpace memory access speeds. This paper presents a\nmemory-efficient ray tracing methodology that integrates compressed data\nstructures with ray stream techniques to reduce memory traffic. The approach\nimplements compressed BVH and triangle representations to minimize acceleration\nstructure size in combination with ray stream tracing to reduce traversal stack\nmemory traffic. The technique employs fixed-point arithmetic for intersection\ntests for prospective hardware with tailored integer operations. Despite using\nreduced precision, geometric holes are avoided by leveraging fixed-point\narithmetic instead of encountering the floating-point rounding errors common in\ntraditional approaches. Quantitative analysis demonstrates significant memory\ntraffic reduction across various scene complexities and BVH configurations. The\npresented 8-wide BVH ray stream implementation reduces memory traffic to only\n18% of traditional approaches by using 8-bit quantization for box and triangle\ncoordinates and directly ray tracing these quantized structures. These\nreductions are especially beneficial for bandwidth-constrained hardware\nenvironments such as mobile devices. This integrated approach addresses both\nmemory bandwidth limitations and numerical precision challenges inherent to\nmodern ray tracing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u5c04\u7ebf\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u538b\u7f29\u6570\u636e\u7ed3\u6784\u548c\u5c04\u7ebf\u6d41\u6280\u672f\u4ee5\u51cf\u5c11\u5185\u5b58\u6d41\u91cf\u3002", "motivation": "\u5185\u5b58\u5e26\u5bbd\u9650\u5236\u662f\u5c04\u7ebf\u8ffd\u8e2a\u6027\u80fd\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u5c24\u5176\u662f\u968f\u7740\u573a\u666f\u590d\u6742\u5ea6\u7684\u589e\u52a0\u548c\u8ba1\u7b97\u80fd\u529b\u8d85\u8fc7\u5185\u5b58\u8bbf\u95ee\u901f\u5ea6\u3002", "method": "\u91c7\u7528\u538b\u7f29BVH\u548c\u4e09\u89d2\u5f62\u8868\u793a\u6765\u51cf\u5c0f\u52a0\u901f\u7ed3\u6784\u5927\u5c0f\uff0c\u5e76\u7ed3\u5408\u5c04\u7ebf\u6d41\u8ffd\u8e2a\u4ee5\u51cf\u5c11\u904d\u5386\u5806\u6808\u5185\u5b58\u6d41\u91cf\u3002\u4f7f\u7528\u5b9a\u70b9\u7b97\u672f\u8fdb\u884c\u4ea4\u96c6\u6d4b\u8bd5\uff0c\u907f\u514d\u4f20\u7edf\u6d6e\u70b9\u820d\u5165\u8bef\u5dee\u3002", "result": "\u5b9a\u91cf\u5206\u6790\u663e\u793a\uff0c\u5728\u5404\u79cd\u573a\u666f\u590d\u6742\u5ea6\u548cBVH\u914d\u7f6e\u4e0b\uff0c\u5185\u5b58\u6d41\u91cf\u663e\u8457\u51cf\u5c11\u30028-wide BVH\u5c04\u7ebf\u6d41\u5b9e\u73b0\u5c06\u5185\u5b58\u6d41\u91cf\u964d\u81f3\u4f20\u7edf\u65b9\u6cd5\u768418%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u4ee3\u5c04\u7ebf\u8ffd\u8e2a\u5e94\u7528\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u9650\u5236\u548c\u6570\u503c\u7cbe\u5ea6\u6311\u6218\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5e26\u5bbd\u53d7\u9650\u7684\u786c\u4ef6\u73af\u5883\u3002"}}
{"id": "2505.24796", "pdf": "https://arxiv.org/pdf/2505.24796", "abs": "https://arxiv.org/abs/2505.24796", "authors": ["Zimu Liao", "Jifeng Ding", "Rong Fu", "Siwei Cui", "Ruixuan Gong", "Li Wang", "Boni Hu", "Yi Wang", "Hengjie Li", "XIngcheng Zhang", "Hui Wang"], "title": "TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores", "categories": ["cs.GR", "cs.CV", "cs.DC", "I.3.6; I.3.2; D.1.3"], "comment": "15 pages, 6 figures", "summary": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian\nprimitives, where conditional alpha-blending dominates the time cost in the\nrendering pipeline. This paper proposes TC-GS, an algorithm-independent\nuniversal module that expands Tensor Core (TCU) applicability for 3DGS, leading\nto substantial speedups and seamless integration into existing 3DGS\noptimization frameworks. The key innovation lies in mapping alpha computation\nto matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS\nimplementations. TC-GS provides plug-and-play acceleration for existing\ntop-tier acceleration algorithms tightly coupled with rendering pipeline\ndesigns, like Gaussian compression and redundancy elimination algorithms.\nAdditionally, we introduce a global-to-local coordinate transformation to\nmitigate rounding errors from quadratic terms of pixel coordinates caused by\nTensor Core half-precision computation. Extensive experiments demonstrate that\nour method maintains rendering quality while providing an additional 2.18x\nspeedup over existing Gaussian acceleration algorithms, thus reaching up to a\ntotal 5.6x acceleration. The code is currently available at anonymous\n\\href{https://github.com/TensorCore3DGS/3DGSTensorCore}", "AI": {"tldr": "TC-GS\u901a\u8fc7\u5c06alpha\u8ba1\u7b97\u6620\u5c04\u5230\u77e9\u9635\u4e58\u6cd5\uff0c\u5229\u7528Tensor Core\u52a0\u901f3D\u9ad8\u65af\u6e32\u67d3\uff0c\u63d0\u5347\u901f\u5ea62.18\u500d\uff0c\u603b\u52a0\u901f\u8fbe5.6\u500d\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6e32\u67d3\u4e2d\uff0calpha\u6df7\u5408\u662f\u65f6\u95f4\u74f6\u9888\uff0c\u800cTensor Core\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faTC-GS\u6a21\u5757\uff0c\u5c06alpha\u8ba1\u7b97\u8f6c\u4e3a\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u5f15\u5165\u5168\u5c40-\u5c40\u90e8\u5750\u6807\u53d8\u6362\u4ee5\u51cf\u5c11\u7cbe\u5ea6\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cTC-GS\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u901f\u5ea6\u63d0\u53472.18\u500d\uff0c\u603b\u52a0\u901f\u8fbe5.6\u500d\u3002", "conclusion": "TC-GS\u662f\u4e00\u79cd\u901a\u7528\u6a21\u5757\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u53473D\u9ad8\u65af\u6e32\u67d3\u6548\u7387\u3002"}}
{"id": "2505.23785", "pdf": "https://arxiv.org/pdf/2505.23785", "abs": "https://arxiv.org/abs/2505.23785", "authors": ["Cody Kommers", "Drew Hemment", "Maria Antoniak", "Joel Z. Leibo", "Hoyt Long", "Emily Robinson", "Adam Sobey"], "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Position paper", "summary": "This position paper argues that large language models (LLMs) can make\ncultural context, and therefore human meaning, legible at an unprecedented\nscale in AI-based sociotechnical systems. We argue that such systems have\npreviously been unable to represent human meaning because they rely on thin\ndescriptions: numerical representations that enforce standardization and\ntherefore strip human activity of the cultural context that gives it meaning.\nBy contrast, scholars in the humanities and qualitative social sciences have\ndeveloped frameworks for representing meaning through thick description: verbal\nrepresentations that accommodate heterogeneity and retain contextual\ninformation needed to represent human meaning. While these methods can\neffectively codify meaning, they are difficult to deploy at scale. However, the\nverbal capabilities of LLMs now provide a means of (at least partially)\nautomating the generation and processing of thick descriptions, potentially\novercoming this bottleneck. We argue that the problem of rendering human\nmeaning legible is not just about selecting better metrics, but about\ndeveloping new representational formats (based on thick description). We frame\nthis as a crucial direction for the application of generative AI and identify\nfive key challenges: preserving context, maintaining interpretive pluralism,\nintegrating perspectives based on lived experience and critical distance,\ndistinguishing qualitative content from quantitative magnitude, and\nacknowledging meaning as dynamic rather than static. Furthermore, we suggest\nthat thick description has the potential to serve as a unifying framework to\naddress a number of emerging concerns about the difficulties of representing\nculture in (or using) LLMs.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u4ee5\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21\u5728\u57fa\u4e8eAI\u7684\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u4e2d\u5448\u73b0\u6587\u5316\u80cc\u666f\u548c\u4eba\u7c7b\u610f\u4e49\uff0c\u514b\u670d\u4f20\u7edf\u7cfb\u7edf\u4f9d\u8d56\u201c\u8584\u63cf\u8ff0\u201d\u7684\u5c40\u9650\u3002", "motivation": "\u4f20\u7edfAI\u7cfb\u7edf\u4f9d\u8d56\u6570\u503c\u5316\u8868\u793a\uff08\u8584\u63cf\u8ff0\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u6d3b\u52a8\u7684\u6587\u5316\u80cc\u666f\u548c\u610f\u4e49\u3002\u4eba\u6587\u548c\u5b9a\u6027\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u201c\u539a\u63cf\u8ff0\u201d\u65b9\u6cd5\u867d\u80fd\u6709\u6548\u7f16\u7801\u610f\u4e49\uff0c\u4f46\u96be\u4ee5\u89c4\u6a21\u5316\u3002LLMs\u7684\u6587\u672c\u80fd\u529b\u4e3a\u81ea\u52a8\u5316\u751f\u6210\u548c\u5904\u7406\u539a\u63cf\u8ff0\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u63d0\u51fa\u5229\u7528LLMs\u7684\u6587\u672c\u80fd\u529b\u751f\u6210\u548c\u5904\u7406\u539a\u63cf\u8ff0\uff0c\u4ee5\u4fdd\u7559\u6587\u5316\u80cc\u666f\u548c\u4eba\u7c7b\u610f\u4e49\u3002\u8ba8\u8bba\u4e86\u539a\u63cf\u8ff0\u4f5c\u4e3a\u65b0\u8868\u793a\u683c\u5f0f\u7684\u6f5c\u529b\u3002", "result": "LLMs\u80fd\u591f\u90e8\u5206\u81ea\u52a8\u5316\u539a\u63cf\u8ff0\u7684\u751f\u6210\u548c\u5904\u7406\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u89c4\u6a21\u5316\u7684\u95ee\u9898\u3002\u63d0\u51fa\u4e86\u4e94\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4fdd\u7559\u80cc\u666f\u3001\u7ef4\u62a4\u89e3\u91ca\u591a\u5143\u6027\u3001\u6574\u5408\u7ecf\u9a8c\u4e0e\u6279\u5224\u89c6\u89d2\u3001\u533a\u5206\u8d28\u4e0e\u91cf\u3001\u627f\u8ba4\u610f\u4e49\u7684\u52a8\u6001\u6027\u3002", "conclusion": "\u539a\u63cf\u8ff0\u53ef\u80fd\u662f\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3LLMs\u4e2d\u6587\u5316\u8868\u793a\u7684\u96be\u9898\u3002\u751f\u6210AI\u7684\u5e94\u7528\u9700\u5173\u6ce8\u65b0\u8868\u793a\u683c\u5f0f\u7684\u5f00\u53d1\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u6355\u6349\u4eba\u7c7b\u610f\u4e49\u3002"}}
{"id": "2505.23788", "pdf": "https://arxiv.org/pdf/2505.23788", "abs": "https://arxiv.org/abs/2505.23788", "authors": ["Aakash Sen Sharma", "Debdeep Sanyal", "Priyansh Srivastava", "Sundar Atreya H.", "Shirish Karande", "Mohan Kankanhalli", "Murari Mandal"], "title": "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework", "categories": ["cs.CL", "cs.AI"], "comment": "30 Pages", "summary": "Large language models (LLMs) commonly risk copyright infringement by\nreproducing protected content verbatim or with insufficient transformative\nmodifications, posing significant ethical, legal, and practical concerns.\nCurrent inference-time safeguards predominantly rely on restrictive\nrefusal-based filters, often compromising the practical utility of these\nmodels. To address this, we collaborated closely with intellectual property\nexperts to develop FUA-LLM (Fair Use Aligned Language Models), a\nlegally-grounded framework explicitly designed to align LLM outputs with\nfair-use doctrine. Central to our method is FairUseDB, a carefully constructed\ndataset containing 18,000 expert-validated examples covering nine realistic\ninfringement scenarios. Leveraging this dataset, we apply Direct Preference\nOptimization (DPO) to fine-tune open-source LLMs, encouraging them to produce\nlegally compliant and practically useful alternatives rather than resorting to\nblunt refusal. Recognizing the shortcomings of traditional evaluation metrics,\nwe propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic\nMean (CAH) to balance infringement risk against response utility. Extensive\nquantitative experiments coupled with expert evaluations confirm that FUA-LLM\nsubstantially reduces problematic outputs (up to 20\\%) compared to\nstate-of-the-art approaches, while preserving real-world usability.", "AI": {"tldr": "FUA-LLM\u6846\u67b6\u901a\u8fc7\u6cd5\u5f8b\u4f9d\u636e\u548c\u4f18\u5316\u65b9\u6cd5\u51cf\u5c11LLM\u7684\u7248\u6743\u4fb5\u6743\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u751f\u6210\u5185\u5bb9\u65f6\u53ef\u80fd\u4fb5\u72af\u7248\u6743\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u62d2\u7edd\u5f0f\u8fc7\u6ee4\u7684\u5b9e\u7528\u6027\u635f\u5931\u3002", "method": "\u6784\u5efaFairUseDB\u6570\u636e\u96c6\uff0c\u5229\u7528DPO\u5fae\u8c03LLM\uff0c\u63d0\u51fa\u65b0\u8bc4\u4f30\u6307\u6807Weighted Penalty Utility\u548cCAH\u3002", "result": "FUA-LLM\u663e\u8457\u51cf\u5c11\u4fb5\u6743\u8f93\u51fa\uff08\u8fbe20%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u3002", "conclusion": "FUA-LLM\u4e3aLLM\u7248\u6743\u95ee\u9898\u63d0\u4f9b\u4e86\u5408\u6cd5\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23789", "pdf": "https://arxiv.org/pdf/2505.23789", "abs": "https://arxiv.org/abs/2505.23789", "authors": ["Mingyu Huang", "Shasha Zhou", "Yuxuan Chen", "Ke Li"], "title": "Conversational Exploration of Literature Landscape with LitChat", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "We are living in an era of \"big literature\", where the volume of digital\nscientific publications is growing exponentially. While offering new\nopportunities, this also poses challenges for understanding literature\nlandscapes, as traditional manual reviewing is no longer feasible. Recent large\nlanguage models (LLMs) have shown strong capabilities for literature\ncomprehension, yet they are incapable of offering \"comprehensive, objective,\nopen and transparent\" views desired by systematic reviews due to their limited\ncontext windows and trust issues like hallucinations. Here we present LitChat,\nan end-to-end, interactive and conversational literature agent that augments\nLLM agents with data-driven discovery tools to facilitate literature\nexploration. LitChat automatically interprets user queries, retrieves relevant\nsources, constructs knowledge graphs, and employs diverse data-mining\ntechniques to generate evidence-based insights addressing user needs. We\nillustrate the effectiveness of LitChat via a case study on AI4Health,\nhighlighting its capacity to quickly navigate the users through large-scale\nliterature landscape with data-based evidence that is otherwise infeasible with\ntraditional means.", "AI": {"tldr": "LitChat\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6587\u732e\u4ee3\u7406\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u63a2\u7d22\u5927\u89c4\u6a21\u6587\u732e\uff0c\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u89c1\u89e3\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u624b\u52a8\u6587\u732e\u7efc\u8ff0\u5728\u5927\u6570\u636e\u65f6\u4ee3\u4e0d\u53ef\u884c\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7cfb\u7edf\u6027\u7efc\u8ff0\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "LitChat\u901a\u8fc7\u81ea\u52a8\u89e3\u6790\u7528\u6237\u67e5\u8be2\u3001\u68c0\u7d22\u76f8\u5173\u6587\u732e\u3001\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5e94\u7528\u6570\u636e\u6316\u6398\u6280\u672f\u751f\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u89c1\u89e3\u3002", "result": "\u901a\u8fc7AI4Health\u6848\u4f8b\u7814\u7a76\uff0cLitChat\u5c55\u793a\u4e86\u5176\u5feb\u901f\u5bfc\u822a\u5927\u89c4\u6a21\u6587\u732e\u5e76\u63d0\u4f9b\u6570\u636e\u652f\u6301\u7684\u80fd\u529b\u3002", "conclusion": "LitChat\u4e3a\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u900f\u660e\u4e14\u57fa\u4e8e\u8bc1\u636e\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.23790", "pdf": "https://arxiv.org/pdf/2505.23790", "abs": "https://arxiv.org/abs/2505.23790", "authors": ["Shaojie Wang", "Sirui Ding", "Na Zou"], "title": "Rethinking the Understanding Ability across LLMs through Mutual Information", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have revolutionized natural\nlanguage processing, yet evaluating their intrinsic linguistic understanding\nremains challenging. Moving beyond specialized evaluation tasks, we propose an\ninformation-theoretic framework grounded in mutual information (MI) to achieve\nthis. We formalize the understanding as MI between an input sentence and its\nlatent representation (sentence-level MI), measuring how effectively input\ninformation is preserved in latent representation. Given that LLMs learn\nembeddings for individual tokens, we decompose sentence-level MI into\ntoken-level MI between tokens and sentence embeddings, establishing theoretical\nbounds connecting these measures. Based on this foundation, we theoretically\nderive a computable lower bound for token-level MI using Fano's inequality,\nwhich directly relates to token-level recoverability-the ability to predict\noriginal tokens from sentence embedding. We implement this recoverability task\nto comparatively measure MI across different LLMs, revealing that encoder-only\nmodels consistently maintain higher information fidelity than their\ndecoder-only counterparts, with the latter exhibiting a distinctive late-layer\n\"forgetting\" pattern where mutual information is first enhanced and then\ndiscarded. Moreover, fine-tuning to maximize token-level recoverability\nconsistently improves understanding ability of LLMs on tasks without\ntask-specific supervision, demonstrating that mutual information can serve as a\nfoundation for understanding and improving language model capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u7f16\u7801\u5668\u6a21\u578b\u6bd4\u89e3\u7801\u5668\u6a21\u578b\u66f4\u80fd\u4fdd\u6301\u4fe1\u606f\uff0c\u4e14\u901a\u8fc7\u5fae\u8c03\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u4efb\u52a1\u5bfc\u5411\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4e92\u4fe1\u606f\uff08MI\uff09\u6846\u67b6\uff0c\u5c06\u53e5\u5b50\u7ea7MI\u5206\u89e3\u4e3a\u8bcd\u7ea7MI\uff0c\u5e76\u901a\u8fc7Fano\u4e0d\u7b49\u5f0f\u63a8\u5bfc\u53ef\u8ba1\u7b97\u7684\u4e0b\u754c\uff0c\u5b9e\u73b0\u8bcd\u7ea7\u6062\u590d\u4efb\u52a1\u3002", "result": "\u7f16\u7801\u5668\u6a21\u578b\u6bd4\u89e3\u7801\u5668\u6a21\u578b\u66f4\u80fd\u4fdd\u6301\u4fe1\u606f\uff0c\u89e3\u7801\u5668\u6a21\u578b\u5728\u6df1\u5c42\u51fa\u73b0\u4fe1\u606f\u9057\u5fd8\u73b0\u8c61\uff1b\u5fae\u8c03\u53ef\u63d0\u5347\u6a21\u578b\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u4e92\u4fe1\u606f\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.23883", "pdf": "https://arxiv.org/pdf/2505.23883", "abs": "https://arxiv.org/abs/2505.23883", "authors": ["Jianyang Gu", "Samuel Stevens", "Elizabeth G Campolongo", "Matthew J Thompson", "Net Zhang", "Jiaman Wu", "Andrei Kopanev", "Zheda Mai", "Alexander E. White", "James Balhoff", "Wasila Dahdul", "Daniel Rubenstein", "Hilmar Lapp", "Tanya Berger-Wolf", "Wei-Lun Chao", "Yu Su"], "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project page: https://imageomics.github.io/bioclip-2/", "summary": "Foundation models trained at scale exhibit remarkable emergent behaviors,\nlearning new capabilities beyond their initial training objectives. We find\nsuch emergent behaviors in biological vision models via large-scale contrastive\nvision-language training. To achieve this, we first curate TreeOfLife-200M,\ncomprising 214 million images of living organisms, the largest and most diverse\nbiological organism image dataset to date. We then train BioCLIP 2 on\nTreeOfLife-200M to distinguish different species. Despite the narrow training\nobjective, BioCLIP 2 yields extraordinary accuracy when applied to various\nbiological visual tasks such as habitat classification and trait prediction. We\nidentify emergent properties in the learned embedding space of BioCLIP 2. At\nthe inter-species level, the embedding distribution of different species aligns\nclosely with functional and ecological meanings (e.g., beak sizes and\nhabitats). At the intra-species level, instead of being diminished, the\nintra-species variations (e.g., life stages and sexes) are preserved and better\nseparated in subspaces orthogonal to inter-species distinctions. We provide\nformal proof and analyses to explain why hierarchical supervision and\ncontrastive objectives encourage these emergent properties. Crucially, our\nresults reveal that these properties become increasingly significant with\nlarger-scale training data, leading to a biologically meaningful embedding\nspace.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u8bad\u7ec3\uff0c\u53d1\u73b0\u751f\u7269\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u5e76\u5229\u7528TreeOfLife-200M\u6570\u636e\u96c6\u8bad\u7ec3BioCLIP 2\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u751f\u7269\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\u548c\u6d8c\u73b0\u7279\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u751f\u7269\u89c6\u89c9\u6a21\u578b\u662f\u5426\u80fd\u591f\u6d8c\u73b0\u51fa\u8d85\u8d8a\u521d\u59cb\u8bad\u7ec3\u76ee\u6807\u7684\u65b0\u80fd\u529b\u3002", "method": "\u4f7f\u7528TreeOfLife-200M\u6570\u636e\u96c6\uff08\u5305\u542b2.14\u4ebf\u5f20\u751f\u7269\u56fe\u50cf\uff09\u8bad\u7ec3BioCLIP 2\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u533a\u5206\u4e0d\u540c\u7269\u79cd\u3002", "result": "BioCLIP 2\u5728\u6816\u606f\u5730\u5206\u7c7b\u548c\u6027\u72b6\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u5d4c\u5165\u7a7a\u95f4\u5728\u7269\u79cd\u95f4\u548c\u7269\u79cd\u5185\u5747\u5c55\u73b0\u51fa\u751f\u7269\u5b66\u610f\u4e49\u7684\u7ed3\u6784\u3002", "conclusion": "\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u80fd\u591f\u4fc3\u8fdb\u6d8c\u73b0\u7279\u6027\u7684\u5f62\u6210\uff0c\u751f\u6210\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u5d4c\u5165\u7a7a\u95f4\u3002"}}
{"id": "2505.23794", "pdf": "https://arxiv.org/pdf/2505.23794", "abs": "https://arxiv.org/abs/2505.23794", "authors": ["Yuan Li", "Qi Luo", "Xiaonan Li", "Bufan Li", "Qinyuan Cheng", "Bo Wang", "Yining Zheng", "Yuxin Wang", "Zhangyue Yin", "Xipeng Qiu"], "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG.", "AI": {"tldr": "R3-RAG \u662f\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9 LLM \u9010\u6b65\u5b66\u4e60\u63a8\u7406\u548c\u68c0\u7d22\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86 RAG \u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u68c0\u7d22\u5668\u5728 RAG \u7cfb\u7edf\u4e2d\u6210\u4e3a\u74f6\u9888\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u8fed\u4ee3 RAG \u53d7\u9650\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u6d41\u7a0b\u7684\u4e0d\u8db3\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u51b7\u542f\u52a8\u5b66\u4e60\u63a8\u7406\u4e0e\u68c0\u7d22\u7684\u4ea4\u66ff\uff0c\u518d\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u68c0\u7d22\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e24\u79cd\u5956\u52b1\u673a\u5236\uff08\u7b54\u6848\u6b63\u786e\u6027\u548c\u6587\u6863\u76f8\u5173\u6027\uff09\u3002", "result": "R3-RAG \u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u68c0\u7d22\u5668\u3002", "conclusion": "R3-RAG \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86 RAG \u7cfb\u7edf\u7684\u63a8\u7406\u548c\u68c0\u7d22\u80fd\u529b\u3002"}}
{"id": "2505.23886", "pdf": "https://arxiv.org/pdf/2505.23886", "abs": "https://arxiv.org/abs/2505.23886", "authors": ["Bowei Chen", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steven M. Seitz"], "title": "Generating Fit Check Videos with a Handheld Camera", "categories": ["cs.CV"], "comment": null, "summary": "Self-captured full-body videos are popular, but most deployments require\nmounted cameras, carefully-framed shots, and repeated practice. We propose a\nmore convenient solution that enables full-body video capture using handheld\nmobile devices. Our approach takes as input two static photos (front and back)\nof you in a mirror, along with an IMU motion reference that you perform while\nholding your mobile phone, and synthesizes a realistic video of you performing\na similar target motion. We enable rendering into a new scene, with consistent\nillumination and shadows. We propose a novel video diffusion-based model to\nachieve this. Specifically, we propose a parameter-free frame generation\nstrategy, as well as a multi-reference attention mechanism, that effectively\nintegrate appearance information from both the front and back selfies into the\nvideo diffusion model. Additionally, we introduce an image-based fine-tuning\nstrategy to enhance frame sharpness and improve the generation of shadows and\nreflections, achieving a more realistic human-scene composition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u624b\u6301\u79fb\u52a8\u8bbe\u5907\u548c\u9759\u6001\u7167\u7247\u751f\u6210\u5168\u8eab\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u771f\u5b9e\u611f\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5168\u8eab\u89c6\u9891\u62cd\u6444\u9700\u8981\u56fa\u5b9a\u6444\u50cf\u5934\u548c\u590d\u6742\u8bbe\u7f6e\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8f93\u5165\u4e24\u5f20\u9759\u6001\u7167\u7247\uff08\u6b63\u53cd\u9762\uff09\u548cIMU\u8fd0\u52a8\u53c2\u8003\uff0c\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u591a\u53c2\u8003\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u89c6\u9891\u3002", "result": "\u5b9e\u73b0\u4e86\u771f\u5b9e\u611f\u89c6\u9891\u5408\u6210\uff0c\u652f\u6301\u65b0\u573a\u666f\u6e32\u67d3\uff0c\u5149\u7167\u548c\u9634\u5f71\u6548\u679c\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4fbf\u6377\u7684\u5168\u8eab\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23796", "pdf": "https://arxiv.org/pdf/2505.23796", "abs": "https://arxiv.org/abs/2505.23796", "authors": ["Christopher Barrie", "Petter T\u00f6rnberg"], "title": "Emergent LLM behaviors are observationally equivalent to data leakage", "categories": ["cs.CL", "cs.GT"], "comment": null, "summary": "Ashery et al. recently argue that large language models (LLMs), when paired\nto play a classic \"naming game,\" spontaneously develop linguistic conventions\nreminiscent of human social norms. Here, we show that their results are better\nexplained by data leakage: the models simply reproduce conventions they already\nencountered during pre-training. Despite the authors' mitigation measures, we\nprovide multiple analyses demonstrating that the LLMs recognize the structure\nof the coordination game and recall its outcomes, rather than exhibit\n\"emergent\" conventions. Consequently, the observed behaviors are\nindistinguishable from memorization of the training corpus. We conclude by\npointing to potential alternative strategies and reflecting more generally on\nthe place of LLMs for social science models.", "AI": {"tldr": "\u8bba\u6587\u6307\u51faAshery\u7b49\u4eba\u7684\u7814\u7a76\u7ed3\u679c\u53ef\u80fd\u662f\u6570\u636e\u6cc4\u6f0f\u5bfc\u81f4\u7684\uff0c\u800c\u975eLLMs\u81ea\u53d1\u5f62\u6210\u7684\u8bed\u8a00\u89c4\u8303\u3002", "motivation": "\u8d28\u7591Ashery\u7b49\u4eba\u5173\u4e8eLLMs\u5728\u547d\u540d\u6e38\u620f\u4e2d\u81ea\u53d1\u5f62\u6210\u8bed\u8a00\u89c4\u8303\u7684\u7ed3\u8bba\uff0c\u8ba4\u4e3a\u5176\u66f4\u53ef\u80fd\u662f\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u7684\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u591a\u79cd\u5206\u6790\u8bc1\u660eLLMs\u8bc6\u522b\u4e86\u534f\u8c03\u6e38\u620f\u7684\u7ed3\u6784\u5e76\u56de\u5fc6\u4e86\u5176\u7ed3\u679c\uff0c\u800c\u975e\u4ea7\u751f\u65b0\u89c4\u8303\u3002", "result": "\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u4e0e\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u65e0\u6cd5\u533a\u5206\uff0c\u4e0d\u652f\u6301\u201c\u6d8c\u73b0\u201d\u89c4\u8303\u7684\u7ed3\u8bba\u3002", "conclusion": "\u5efa\u8bae\u91cd\u65b0\u601d\u8003LLMs\u5728\u793e\u4f1a\u79d1\u5b66\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u66ff\u4ee3\u7b56\u7565\u3002"}}
{"id": "2505.23907", "pdf": "https://arxiv.org/pdf/2505.23907", "abs": "https://arxiv.org/abs/2505.23907", "authors": ["Amirhossein Almohammadi", "Aryan Mikaeili", "Sauradip Nag", "Negar Hassanpour", "Andrea Tagliasacchi", "Ali Mahdavi-Amiri"], "title": "Cora: Correspondence-aware image editing using few step diffusion", "categories": ["cs.CV", "I.4.10; I.3.7; I.2.10"], "comment": "Published in SIGGRAPH 2025", "summary": "Image editing is an important task in computer graphics, vision, and VFX,\nwith recent diffusion-based methods achieving fast and high-quality results.\nHowever, edits requiring significant structural changes, such as non-rigid\ndeformations, object modifications, or content generation, remain challenging.\nExisting few step editing approaches produce artifacts such as irrelevant\ntexture or struggle to preserve key attributes of the source image (e.g.,\npose). We introduce Cora, a novel editing framework that addresses these\nlimitations by introducing correspondence-aware noise correction and\ninterpolated attention maps. Our method aligns textures and structures between\nthe source and target images through semantic correspondence, enabling accurate\ntexture transfer while generating new content when necessary. Cora offers\ncontrol over the balance between content generation and preservation. Extensive\nexperiments demonstrate that, quantitatively and qualitatively, Cora excels in\nmaintaining structure, textures, and identity across diverse edits, including\npose changes, object addition, and texture refinements. User studies confirm\nthat Cora delivers superior results, outperforming alternatives.", "AI": {"tldr": "Cora\u662f\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u5e94\u611f\u77e5\u7684\u566a\u58f0\u6821\u6b63\u548c\u63d2\u503c\u6ce8\u610f\u529b\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u4fee\u6539\u548c\u7eb9\u7406\u4fdd\u7559\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u5904\u7406\u7ed3\u6784\u53d8\u5316\uff08\u5982\u975e\u521a\u6027\u53d8\u5f62\u6216\u5bf9\u8c61\u4fee\u6539\uff09\u65f6\u5b58\u5728\u7eb9\u7406\u4e0d\u76f8\u5173\u6216\u5173\u952e\u5c5e\u6027\u4e22\u5931\u7684\u95ee\u9898\u3002", "method": "Cora\u5229\u7528\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u5bf9\u9f50\u6e90\u56fe\u50cf\u548c\u76ee\u6807\u56fe\u50cf\u7684\u7eb9\u7406\u548c\u7ed3\u6784\uff0c\u7ed3\u5408\u566a\u58f0\u6821\u6b63\u548c\u63d2\u503c\u6ce8\u610f\u529b\u56fe\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u7eb9\u7406\u8f6c\u79fb\u548c\u65b0\u5185\u5bb9\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCora\u5728\u4fdd\u6301\u7ed3\u6784\u3001\u7eb9\u7406\u548c\u8eab\u4efd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u59ff\u6001\u53d8\u5316\u3001\u5bf9\u8c61\u6dfb\u52a0\u548c\u7eb9\u7406\u7ec6\u5316\u7b49\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "Cora\u5728\u7528\u6237\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7f16\u8f91\u6548\u679c\u548c\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2505.23797", "pdf": "https://arxiv.org/pdf/2505.23797", "abs": "https://arxiv.org/abs/2505.23797", "authors": ["Zaihan Yang", "Ryan Leonard", "Hien Tran", "Rory Driscoll", "Chadbourne Davis"], "title": "Detection of Suicidal Risk on Social Media: A Hybrid Model", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": null, "summary": "Suicidal thoughts and behaviors are increasingly recognized as a critical\nsocietal concern, highlighting the urgent need for effective tools to enable\nearly detection of suicidal risk. In this work, we develop robust machine\nlearning models that leverage Reddit posts to automatically classify them into\nfour distinct levels of suicide risk severity. We frame this as a multi-class\nclassification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating\nthe deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),\na state-of-the-art deep learning transformer model, with the statistical\nterm-weighting of TF-IDF, further compressed with PCA, to boost the accuracy\nand reliability of suicide risk assessment. To address data imbalance and\noverfitting, we explore various data resampling techniques and data\naugmentation strategies to enhance model generalization. Additionally, we\ncompare our model's performance against that of using RoBERTa only, the BERT\nmodel and other traditional machine learning classifiers. Experimental results\ndemonstrate that the hybrid model can achieve improved performance, giving a\nbest weighted $F_{1}$ score of 0.7512.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RoBERTa\u3001TF-IDF\u548cPCA\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u4eceReddit\u5e16\u5b50\u4e2d\u81ea\u52a8\u5206\u7c7b\u81ea\u6740\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u91cd\u91c7\u6837\u6280\u672f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u81ea\u6740\u98ce\u9669\u65e9\u671f\u68c0\u6d4b\u5de5\u5177\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4fc3\u4f7f\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u4ece\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u8bc6\u522b\u81ea\u6740\u98ce\u9669\u3002", "method": "\u91c7\u7528RoBERTa-TF-IDF-PCA\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5d4c\u5165\u4e0e\u7edf\u8ba1\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u91cd\u91c7\u6837\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u52a0\u6743F1\u5206\u6570\u4e0a\u8fbe\u52300.7512\uff0c\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528RoBERTa\u3001BERT\u53ca\u5176\u4ed6\u4f20\u7edf\u5206\u7c7b\u5668\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u5728\u81ea\u6740\u98ce\u9669\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.23917", "pdf": "https://arxiv.org/pdf/2505.23917", "abs": "https://arxiv.org/abs/2505.23917", "authors": ["Neehar Kondapaneni", "Oisin Mac Aodha", "Pietro Perona"], "title": "Representational Difference Explanations", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "9 pages, 6 figures, 21 supplementary pages, 14 supp figs", "summary": "We propose a method for discovering and visualizing the differences between\ntwo learned representations, enabling more direct and interpretable model\ncomparisons. We validate our method, which we call Representational Differences\nExplanations (RDX), by using it to compare models with known conceptual\ndifferences and demonstrate that it recovers meaningful distinctions where\nexisting explainable AI (XAI) techniques fail. Applied to state-of-the-art\nmodels on challenging subsets of the ImageNet and iNaturalist datasets, RDX\nreveals both insightful representational differences and subtle patterns in the\ndata. Although comparison is a cornerstone of scientific analysis, current\ntools in machine learning, namely post hoc XAI methods, struggle to support\nmodel comparison effectively. Our work addresses this gap by introducing an\neffective and explainable tool for contrasting model representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRDX\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u548c\u53ef\u89c6\u5316\u4e24\u79cd\u5b66\u4e60\u8868\u793a\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u652f\u6301\u66f4\u76f4\u63a5\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u6bd4\u8f83\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89e3\u91caAI\uff08XAI\uff09\u65b9\u6cd5\u5728\u652f\u6301\u6a21\u578b\u6bd4\u8f83\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u79d1\u5b66\u5206\u6790\u4e2d\u5bf9\u6bd4\u662f\u57fa\u7840\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u96be\u4ee5\u6709\u6548\u5b9e\u73b0\u3002", "method": "\u63d0\u51faRepresentational Differences Explanations\uff08RDX\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u5df2\u77e5\u6982\u5ff5\u5dee\u5f02\u7684\u6a21\u578b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "RDX\u5728ImageNet\u548ciNaturalist\u6570\u636e\u96c6\u4e0a\u6210\u529f\u63ed\u793a\u4e86\u6a21\u578b\u95f4\u7684\u8868\u793a\u5dee\u5f02\u548c\u6570\u636e\u7684\u5fae\u5999\u6a21\u5f0f\uff0c\u4f18\u4e8e\u73b0\u6709XAI\u6280\u672f\u3002", "conclusion": "RDX\u586b\u8865\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u6a21\u578b\u6bd4\u8f83\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u5bf9\u6bd4\u65b9\u6cd5\u3002"}}
{"id": "2505.23798", "pdf": "https://arxiv.org/pdf/2505.23798", "abs": "https://arxiv.org/abs/2505.23798", "authors": ["Jian Lan", "Yifei Fu", "Udo Schlegel", "Gengyuan Zhang", "Tanveer Hannan", "Haokun Chen", "Thomas Seidl"], "title": "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Social bias is a critical issue in large vision-language models (VLMs), where\nfairness- and ethics-related problems harm certain groups of people in society.\nIt is unknown to what extent VLMs yield social bias in generative responses. In\nthis study, we focus on evaluating and mitigating social bias on both the\nmodel's response and probability distribution. To do so, we first evaluate four\nstate-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the\nmultiple-choice selection task. Surprisingly, we find that models suffer from\ngenerating gender-biased or race-biased responses. We also observe that models\nare prone to stating their responses are fair, but indeed having mis-calibrated\nconfidence levels towards particular social groups. While investigating why\nVLMs are unfair in this study, we observe that VLMs' hidden layers exhibit\nsubstantial fluctuations in fairness levels. Meanwhile, residuals in each layer\nshow mixed effects on fairness, with some contributing positively while some\nlead to increased bias. Based on these findings, we propose a post-hoc method\nfor the inference stage to mitigate social bias, which is training-free and\nmodel-agnostic. We achieve this by ablating bias-associated residuals while\namplifying fairness-associated residuals on model hidden layers during\ninference. We demonstrate that our post-hoc method outperforms the competing\ntraining strategies, helping VLMs have fairer responses and more reliable\nconfidence levels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u540e\u5904\u7406\u65b9\u6cd5\u4ee5\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u793e\u4f1a\u504f\u89c1\u662fVLMs\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u67d0\u4e9b\u793e\u4f1a\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u5bf9\u5f85\uff0c\u4f46\u76ee\u524d\u5bf9\u5176\u751f\u6210\u54cd\u5e94\u4e2d\u7684\u504f\u89c1\u7a0b\u5ea6\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u7684VLMs\u5728PAIRS\u548cSocialCounterfactuals\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u540e\u5904\u7406\u65b9\u6cd5\uff08\u6d88\u9664\u504f\u89c1\u76f8\u5173\u7684\u6b8b\u5dee\u5e76\u653e\u5927\u516c\u5e73\u76f8\u5173\u7684\u6b8b\u5dee\uff09\u6765\u51cf\u5c11\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0VLMs\u5b58\u5728\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\uff0c\u4e14\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5b58\u5728\u95ee\u9898\u3002\u63d0\u51fa\u7684\u540e\u5904\u7406\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u548c\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86VLMs\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.23922", "pdf": "https://arxiv.org/pdf/2505.23922", "abs": "https://arxiv.org/abs/2505.23922", "authors": ["David Ma", "Huaqing Yuan", "Xingjian Wang", "Qianbo Zang", "Tianci Liu", "Xinyang He", "Yanbin Wei", "Jiawei Guo", "Ni Jiahui", "Zhenzhu Yang", "Meng Cao", "Shanghaoran Quan", "Yizhi Li", "Wangchunshu Zhou", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Shiwen Ni", "Xiaojie Jin"], "title": "ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although long-video understanding demands that models capture hierarchical\ntemporal information -- from clip (seconds) and shot (tens of seconds) to event\n(minutes) and story (hours) -- existing benchmarks either neglect this\nmulti-scale design or scatter scale-specific questions across different videos,\npreventing direct comparison of model performance across timescales on the same\ncontent. To address this, we introduce ScaleLong, the first benchmark to\ndisentangle these factors by embedding questions targeting four hierarchical\ntimescales -- clip (seconds), shot (tens of seconds), event (minutes), and\nstory (hours) -- all within the same video content. This within-content\nmulti-timescale questioning design enables direct comparison of model\nperformance across timescales on identical videos. ScaleLong features 269 long\nvideos (avg.\\ 86\\,min) from 5 main categories and 36 sub-categories, with 4--8\ncarefully designed questions, including at least one question for each\ntimescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with\nhigher accuracy at the shortest and longest timescales and a dip at\nintermediate levels. Furthermore, ablation studies show that increased visual\ntoken capacity consistently enhances reasoning across all timescales. ScaleLong\noffers a fine-grained, multi-timescale benchmark for advancing MLLM\ncapabilities in long-video understanding. The code and dataset are available\nhttps://github.com/multimodal-art-projection/ScaleLong.", "AI": {"tldr": "ScaleLong\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u5c3a\u5ea6\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u5728\u540c\u4e00\u89c6\u9891\u5185\u5bb9\u4e2d\u5d4c\u5165\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u95ee\u9898\uff0c\u76f4\u63a5\u6bd4\u8f83\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u7edf\u4e00\u591a\u5c3a\u5ea6\u8bbe\u8ba1\u6216\u5206\u6563\u95ee\u9898\uff0c\u65e0\u6cd5\u76f4\u63a5\u6bd4\u8f83\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "method": "ScaleLong\u5305\u542b269\u4e2a\u957f\u89c6\u9891\uff0c\u6bcf\u4e2a\u89c6\u9891\u8bbe\u8ba14-8\u4e2a\u95ee\u9898\uff0c\u8986\u76d6\u56db\u4e2a\u65f6\u95f4\u5c3a\u5ea6\uff08\u79d2\u3001\u5341\u79d2\u3001\u5206\u949f\u3001\u5c0f\u65f6\uff09\u3002", "result": "\u8bc4\u4f3023\u4e2aMLLM\u663e\u793aU\u5f62\u6027\u80fd\u66f2\u7ebf\uff0c\u6700\u77ed\u548c\u6700\u957f\u5c3a\u5ea6\u8868\u73b0\u6700\u4f73\uff0c\u4e2d\u95f4\u5c3a\u5ea6\u8868\u73b0\u8f83\u5dee\u3002\u89c6\u89c9\u6807\u8bb0\u5bb9\u91cf\u589e\u52a0\u53ef\u63d0\u5347\u6240\u6709\u5c3a\u5ea6\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ScaleLong\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u7684\u591a\u5c3a\u5ea6\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86MLLM\u80fd\u529b\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799", "abs": "https://arxiv.org/abs/2505.23799", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6587\u672c\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8elogit\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "LLM\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u4e14\u5bf9\u63d0\u793a\u6270\u52a8\u654f\u611f\uff0c\u5bfc\u81f4\u751f\u6210\u6587\u672c\u4e0d\u4e00\u81f4\u6216\u4e0d\u53ef\u9760\u3002\u73b0\u6709\u7684\u4e00\u81f4\u6027\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5f88\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u611f\u77e5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8elogit\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08n=2,976\uff09\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u8bc4\u4f30\u4eba\u7c7b\u4e00\u81f4\u6027\u8bc4\u5206\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u73b0\u6709\u65e0\u4eba\u7c7b\u8bc4\u4f30\u7684\u65b9\u6cd5\u4ecd\u4e0d\u5b8c\u5584\u3002", "conclusion": "\u5efa\u8bae\u66f4\u5e7f\u6cdb\u5730\u4f7f\u7528\u4eba\u7c7b\u8f93\u5165\u6765\u8bc4\u4f30LLM\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.23926", "pdf": "https://arxiv.org/pdf/2505.23926", "abs": "https://arxiv.org/abs/2505.23926", "authors": ["Xuweiyi Chen", "Wentao Zhou", "Aruni RoyChowdhury", "Zezhou Cheng"], "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts", "categories": ["cs.CV"], "comment": "Project page: https://uva-computer-vision-lab.github.io/point-moe/", "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.", "AI": {"tldr": "Point-MoE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMixture-of-Experts\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b33D\u70b9\u4e91\u7406\u89e3\u4e2d\u8de8\u57df\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u65e0\u9700\u57df\u6807\u7b7e\u5373\u53ef\u81ea\u52a8\u5206\u914d\u4e13\u5bb6\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "3D\u70b9\u4e91\u6570\u636e\u56e0\u4f20\u611f\u5668\u548c\u573a\u666f\u7684\u591a\u6837\u6027\u5bfc\u81f4\u57df\u5f02\u8d28\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u8de8\u57df\u7edf\u4e00\u5efa\u6a21\u3002", "method": "\u91c7\u7528Point-MoE\u67b6\u6784\uff0c\u7ed3\u5408\u7b80\u5355\u7684top-k\u8def\u7531\u7b56\u7565\uff0c\u81ea\u52a8\u5206\u914d\u4e13\u5bb6\u6a21\u5757\u5904\u7406\u4e0d\u540c\u57df\u7684\u6570\u636e\u3002", "result": "Point-MoE\u5728\u6df7\u5408\u57df\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u57df\u3002", "conclusion": "Point-MoE\u4e3a3D\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23801", "pdf": "https://arxiv.org/pdf/2505.23801", "abs": "https://arxiv.org/abs/2505.23801", "authors": ["Sajid Hussain", "Muhammad Sohail", "Nauman Ali Khan"], "title": "SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages", "summary": "Background: Federated Learning (FL) has emerged as a promising paradigm for\ntraining machine learning models while preserving data privacy. However,\napplying FL to Natural Language Processing (NLP) tasks presents unique\nchallenges due to semantic heterogeneity across clients, vocabulary mismatches,\nand varying resource constraints on edge devices. Objectives: This paper\nintroduces SEMFED, a novel semantic-aware resource-efficient federated learning\nframework specifically designed for heterogeneous NLP tasks. Methods: SEMFED\nincorporates three key innovations: (1) a semantic-aware client selection\nmechanism that balances semantic diversity with resource constraints, (2)\nadaptive NLP-specific model architectures tailored to device capabilities while\npreserving semantic information, and (3) a communication-efficient semantic\nfeature compression technique that significantly reduces bandwidth\nrequirements. Results: Experimental results on various NLP classification tasks\ndemonstrate that SEMFED achieves an 80.5% reduction in communication costs\nwhile maintaining model accuracy above 98%, outperforming state-of-the-art FL\napproaches. Conclusion: SEMFED effectively manages heterogeneous client\nenvironments with varying computational resources, network reliability, and\nsemantic data distributions, making it particularly suitable for real-world\nfederated NLP deployments.", "AI": {"tldr": "SEMFED\u662f\u4e00\u79cd\u9488\u5bf9\u5f02\u6784NLP\u4efb\u52a1\u7684\u8bed\u4e49\u611f\u77e5\u8d44\u6e90\u9ad8\u6548\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728NLP\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u8bed\u4e49\u5f02\u6784\u3001\u8bcd\u6c47\u4e0d\u5339\u914d\u548c\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u9650\u5236\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u5ba2\u6237\u7aef\u9009\u62e9\u3001\u81ea\u9002\u5e94NLP\u6a21\u578b\u67b6\u6784\u548c\u901a\u4fe1\u9ad8\u6548\u7684\u8bed\u4e49\u7279\u5f81\u538b\u7f29\u6280\u672f\u3002", "result": "\u5728\u591a\u4e2aNLP\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e80.5%\uff0c\u6a21\u578b\u7cbe\u5ea6\u4fdd\u6301\u572898%\u4ee5\u4e0a\u3002", "conclusion": "SEMFED\u6709\u6548\u7ba1\u7406\u5f02\u6784\u5ba2\u6237\u7aef\u73af\u5883\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8054\u90a6NLP\u90e8\u7f72\u3002"}}
{"id": "2505.23952", "pdf": "https://arxiv.org/pdf/2505.23952", "abs": "https://arxiv.org/abs/2505.23952", "authors": ["Adriano Fragomeni", "Dima Damen", "Michael Wray"], "title": "Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Video (T2V) retrieval aims to identify the most relevant item from a\ngallery of videos based on a user's text query. Traditional methods rely solely\non aligning video and text modalities to compute the similarity and retrieve\nrelevant items. However, recent advancements emphasise incorporating auxiliary\ninformation extracted from video and text modalities to improve retrieval\nperformance and bridge the semantic gap between these modalities. Auxiliary\ninformation can include visual attributes, such as objects; temporal and\nspatial context; and textual descriptions, such as speech and rephrased\ncaptions. This survey comprehensively reviews 81 research papers on\nText-to-Video retrieval that utilise such auxiliary information. It provides a\ndetailed analysis of their methodologies; highlights state-of-the-art results\non benchmark datasets; and discusses available datasets and their auxiliary\ninformation. Additionally, it proposes promising directions for future\nresearch, focusing on different ways to further enhance retrieval performance\nusing this information.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e8681\u7bc7\u5173\u4e8e\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u7684\u7814\u7a76\u8bba\u6587\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5229\u7528\u8f85\u52a9\u4fe1\u606f\uff08\u5982\u89c6\u89c9\u5c5e\u6027\u3001\u65f6\u7a7a\u4e0a\u4e0b\u6587\u548c\u6587\u672c\u63cf\u8ff0\uff09\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u89c6\u9891\u548c\u6587\u672c\u6a21\u6001\u7684\u5bf9\u9f50\uff0c\u4f46\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002\u5f15\u5165\u8f85\u52a9\u4fe1\u606f\uff08\u5982\u89c6\u89c9\u5c5e\u6027\u3001\u65f6\u7a7a\u4e0a\u4e0b\u6587\u7b49\uff09\u53ef\u4ee5\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "method": "\u7efc\u8ff0\u5206\u6790\u4e8681\u7bc7\u8bba\u6587\uff0c\u603b\u7ed3\u4e86\u5229\u7528\u8f85\u52a9\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u89c6\u89c9\u5c5e\u6027\u3001\u65f6\u7a7a\u4e0a\u4e0b\u6587\u548c\u6587\u672c\u63cf\u8ff0\u7b49\u3002", "result": "\u7efc\u8ff0\u5c55\u793a\u4e86\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6700\u65b0\u6210\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u53ef\u7528\u6570\u636e\u96c6\u53ca\u5176\u8f85\u52a9\u4fe1\u606f\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8f85\u52a9\u4fe1\u606f\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2505.23802", "pdf": "https://arxiv.org/pdf/2505.23802", "abs": "https://arxiv.org/abs/2505.23802", "authors": ["Suhana Bedi", "Hejie Cui", "Miguel Fuentes", "Alyssa Unell", "Michael Wornow", "Juan M. Banda", "Nikesh Kotecha", "Timothy Keyes", "Yifan Mai", "Mert Oez", "Hao Qiu", "Shrey Jain", "Leonardo Schettini", "Mehr Kashyap", "Jason Alan Fries", "Akshay Swaminathan", "Philip Chung", "Fateme Nateghi", "Asad Aali", "Ashwin Nayak", "Shivam Vedak", "Sneha S. Jain", "Birju Patel", "Oluseyi Fayanju", "Shreya Shah", "Ethan Goh", "Dong-han Yao", "Brian Soetikno", "Eduardo Reis", "Sergios Gatidis", "Vasu Divi", "Robson Capasso", "Rachna Saralkar", "Chia-Chun Chiang", "Jenelle Jindal", "Tho Pham", "Faraz Ghoddusi", "Steven Lin", "Albert S. Chiou", "Christy Hong", "Mohana Roy", "Michael F. Gensheimer", "Hinesh Patel", "Kevin Schulman", "Dev Dash", "Danton Char", "Lance Downing", "Francois Grolleau", "Kameron Black", "Bethel Mieso", "Aydin Zahedivash", "Wen-wai Yim", "Harshita Sharma", "Tony Lee", "Hannah Kirsch", "Jennifer Lee", "Nerissa Ambers", "Carlene Lugtu", "Aditya Sharma", "Bilal Mawji", "Alex Alekseyev", "Vicky Zhou", "Vikas Kakkar", "Jarrod Helzer", "Anurang Revri", "Yair Bannett", "Roxana Daneshjou", "Jonathan Chen", "Emily Alsentzer", "Keith Morse", "Nirmal Ravi", "Nima Aghaeepour", "Vanessa Kennedy", "Akshay Chaudhari", "Thomas Wang", "Sanmi Koyejo", "Matthew P. Lungren", "Eric Horvitz", "Percy Liang", "Mike Pfeffer", "Nigam H. Shah"], "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.", "AI": {"tldr": "MedHELM\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u6846\u67b6\uff0c\u5305\u542b\u5206\u7c7b\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0cClaude 3.5 Sonnet\u4ee5\u8f83\u4f4e\u6210\u672c\u5b9e\u73b0\u4e0e\u9876\u7ea7\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e34\u5e8a\u5b9e\u8df5\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b5\u7c7b22\u5b50\u7c7b121\u4efb\u52a1\u7684\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa35\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528LLM-jury\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u4e34\u5e8a\u7b14\u8bb0\u751f\u6210\u548c\u60a3\u8005\u6c9f\u901a\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u548c\u7ba1\u7406\u4efb\u52a1\u4e2d\u8f83\u5f31\uff1bLLM-jury\u65b9\u6cd5\u4e0e\u4e34\u5e8a\u533b\u751f\u8bc4\u5206\u4e00\u81f4\u6027\u8f83\u9ad8\u3002", "conclusion": "MedHELM\u4e3aLLM\u5728\u533b\u7597\u9886\u57df\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5f00\u6e90\u6846\u67b6\uff0c\u5f3a\u8c03\u4efb\u52a1\u7279\u5f02\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.23961", "pdf": "https://arxiv.org/pdf/2505.23961", "abs": "https://arxiv.org/abs/2505.23961", "authors": ["Rafi Hassan Chowdhury", "Sabbir Ahmed"], "title": "MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification", "categories": ["cs.CV"], "comment": "Accepted in 27th ICCIT 2024, 6 pages, 5 Figures, 4 tables", "summary": "Ensuring food safety is critical due to its profound impact on public health,\neconomic stability, and global supply chains. Cultivation of Mango, a major\nagricultural product in several South Asian countries, faces high financial\nlosses due to different diseases, affecting various aspects of the entire\nsupply chain. While deep learning-based methods have been explored for mango\nleaf disease classification, there remains a gap in designing solutions that\nare computationally efficient and compatible with low-end devices. In this\nwork, we propose a lightweight Vision Transformer-based pipeline with a\nself-attention mechanism to classify mango leaf diseases, achieving\nstate-of-the-art performance with minimal computational overhead. Our approach\nleverages global attention to capture intricate patterns among disease types\nand incorporates runtime augmentation for enhanced performance. Evaluation on\nthe MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing\nmethods in terms of model size, parameter count, and FLOPs count.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Vision Transformer\u65b9\u6cd5\uff0c\u7528\u4e8e\u8292\u679c\u53f6\u75c5\u5bb3\u5206\u7c7b\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u4f4e\u7aef\u8bbe\u5907\uff0c\u51c6\u786e\u7387\u8fbe99.43%\u3002", "motivation": "\u8292\u679c\u75c5\u5bb3\u5bf9\u5357\u4e9a\u519c\u4e1a\u7ecf\u6d4e\u5f71\u54cd\u91cd\u5927\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u4f4e\u7aef\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7Vision Transformer\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5f15\u5165\u8fd0\u884c\u65f6\u589e\u5f3a\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728MangoLeafBD\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.43%\u51c6\u786e\u7387\uff0c\u6a21\u578b\u5927\u5c0f\u3001\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u4e3a\u8292\u679c\u53f6\u75c5\u5bb3\u5206\u7c7b\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23804", "pdf": "https://arxiv.org/pdf/2505.23804", "abs": "https://arxiv.org/abs/2505.23804", "authors": ["Terrance Liu", "Shuyi Wang", "Daniel Preotiuc-Pietro", "Yash Chandarana", "Chirag Gupta"], "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4e3a\u57fa\u4e8eLLM\u7684\u6587\u672c\u5230SQL\u89e3\u6790\u63d0\u4f9b\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u5b50\u53e5\u9891\u7387\uff08SCF\uff09\u548c\u591a\u53d8\u91cfPlatt\u7f29\u653e\uff08MPS\uff09\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6821\u51c6\u6548\u679c\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u5230SQL\u89e3\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6709\u65f6\u4f1a\u81ea\u4fe1\u5730\u7ed9\u51fa\u9519\u8bef\u7b54\u6848\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u8861\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5b50\u53e5\u9891\u7387\uff08SCF\uff09\u8bc4\u5206\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u53d8\u91cfPlatt\u7f29\u653e\uff08MPS\uff09\u6280\u672f\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684\u6821\u51c6\u5206\u6570\u3002", "result": "\u5728\u4e24\u79cd\u6d41\u884c\u7684\u6587\u672c\u5230SQL\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfPlatt\u7f29\u653e\uff0c\u63d0\u5347\u4e86\u6821\u51c6\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408SCF\u548cMPS\u7684\u65b9\u6cd5\u4e3a\u6587\u672c\u5230SQL\u89e3\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u8861\u91cf\uff0c\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.23977", "pdf": "https://arxiv.org/pdf/2505.23977", "abs": "https://arxiv.org/abs/2505.23977", "authors": ["Yichen Feng", "Zhangchen Xu", "Fengqing Jiang", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://visualsphinx.github.io/", "summary": "Vision language models (VLMs) are expected to perform effective multimodal\nreasoning and make logically coherent decisions, which is critical to tasks\nsuch as diagram understanding and spatial problem solving. However, current VLM\nreasoning lacks large-scale and well-structured training datasets. To bridge\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\nvisual logical reasoning training data. To tackle the challenge of image\nsynthesis with grounding answers, we propose a rule-to-image synthesis\npipeline, which extracts and expands puzzle rules from seed questions and\ngenerates the code of grounding synthesis image synthesis for puzzle sample\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\nbenefit from logical coherence and readability of our dataset and exhibit\nimproved performance on logical reasoning tasks. The enhanced reasoning\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVisualSphinx\u7684\u5927\u89c4\u6a21\u5408\u6210\u89c6\u89c9\u903b\u8f91\u63a8\u7406\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\u3002\u901a\u8fc7\u89c4\u5219\u5230\u56fe\u50cf\u7684\u5408\u6210\u6d41\u7a0b\uff0c\u751f\u6210\u5177\u6709\u57fa\u7840\u7b54\u6848\u7684\u56fe\u50cf\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86VLM\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e14\u7ed3\u6784\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51faVisualSphinx\u6570\u636e\u96c6\uff0c\u91c7\u7528\u89c4\u5219\u5230\u56fe\u50cf\u7684\u5408\u6210\u6d41\u7a0b\uff0c\u4ece\u79cd\u5b50\u95ee\u9898\u4e2d\u63d0\u53d6\u5e76\u6269\u5c55\u8c1c\u9898\u89c4\u5219\uff0c\u751f\u6210\u5177\u6709\u57fa\u7840\u7b54\u6848\u7684\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528VisualSphinx\u8bad\u7ec3\u7684VLM\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u5176\u63a8\u7406\u80fd\u529b\u8fd8\u80fd\u63d0\u5347\u5176\u4ed6\u4efb\u52a1\uff08\u5982\u4ee3\u6570\u3001\u7b97\u672f\u548c\u51e0\u4f55\u63a8\u7406\uff09\u7684\u6027\u80fd\u3002", "conclusion": "VisualSphinx\u4e3aVLM\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u903b\u8f91\u63a8\u7406\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23806", "pdf": "https://arxiv.org/pdf/2505.23806", "abs": "https://arxiv.org/abs/2505.23806", "authors": ["Sihyeon Lee", "Hyunjoo Song", "Jong-chan Lee", "Yoon Jin Lee", "Boram Lee", "Hee-Eon Lim", "Dongyeong Kim", "Jinwook Seo", "Bohyoung Kim"], "title": "MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deploying large language models (LLMs) in clinical settings faces critical\ntrade-offs: cloud LLMs, with their extensive parameters and superior\nperformance, pose risks to sensitive clinical data privacy, while local LLMs\npreserve privacy but often fail at complex clinical interpretation tasks. We\npropose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex\nclinical tasks into manageable subtasks and prompt generation, while a local\nLLM executes these subtasks in a privacy-preserving manner. Without accessing\nclinical data, the cloud LLM generates and validates subtask prompts using\nclinical guidelines and synthetic test cases. The local LLM executes subtasks\nlocally and synthesizes outputs generated by the cloud LLM. We evaluate\nMedOrchestra on pancreatic cancer staging using 100 radiology reports under\nNCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,\noutperforming local model baselines (without guideline: 48.94%, with guideline:\n56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:\n65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches\n85.42% accuracy, showing clear superiority across all settings.", "AI": {"tldr": "MedOrchestra\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e91\u7aef\u548c\u672c\u5730LLMs\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u62a4\u4e34\u5e8a\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4e91\u7aefLLMs\u9690\u79c1\u98ce\u9669\u4e0e\u672c\u5730LLMs\u6027\u80fd\u4e0d\u8db3\u7684\u77db\u76fe\u3002", "method": "\u4e91\u7aefLLM\u5206\u89e3\u4efb\u52a1\u5e76\u751f\u6210\u63d0\u793a\uff0c\u672c\u5730LLM\u6267\u884c\u9690\u79c1\u4fdd\u62a4\u4efb\u52a1\u3002", "result": "\u5728\u80f0\u817a\u764c\u5206\u671f\u4efb\u52a1\u4e2d\uff0cMedOrchestra\u8868\u73b0\u4f18\u4e8e\u672c\u5730\u6a21\u578b\u548c\u4e34\u5e8a\u533b\u751f\u3002", "conclusion": "MedOrchestra\u5728\u9690\u79c1\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u73af\u5883\u3002"}}
{"id": "2505.23980", "pdf": "https://arxiv.org/pdf/2505.23980", "abs": "https://arxiv.org/abs/2505.23980", "authors": ["Bayu Adhi Tama", "Mansa Krishna", "Homayra Alam", "Mostafa Cham", "Omar Faruque", "Gong Cheng", "Jianwu Wang", "Mathieu Morlighem", "Vandana Janeja"], "title": "DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Submitted to SIGSPATIAL 2025", "summary": "Understanding Greenland's subglacial topography is critical for projecting\nthe future mass loss of the ice sheet and its contribution to global sea-level\nrise. However, the complex and sparse nature of observational data,\nparticularly information about the bed topography under the ice sheet,\nsignificantly increases the uncertainty in model projections. Bed topography is\ntraditionally measured by airborne ice-penetrating radar that measures the ice\nthickness directly underneath the aircraft, leaving data gap of tens of\nkilometers in between flight lines. This study introduces a deep learning\nframework, which we call as DeepTopoNet, that integrates radar-derived ice\nthickness observations and BedMachine Greenland data through a novel dynamic\nloss-balancing mechanism. Among all efforts to reconstruct bed topography,\nBedMachine has emerged as one of the most widely used datasets, combining mass\nconservation principles and ice thickness measurements to generate\nhigh-resolution bed elevation estimates. The proposed loss function adaptively\nadjusts the weighting between radar and BedMachine data, ensuring robustness in\nareas with limited radar coverage while leveraging the high spatial resolution\nof BedMachine predictions i.e. bed estimates. Our approach incorporates\ngradient-based and trend surface features to enhance model performance and\nutilizes a CNN architecture designed for subgrid-scale predictions. By\nsystematically testing on the Upernavik Isstr{\\o}m) region, the model achieves\nhigh accuracy, outperforming baseline methods in reconstructing subglacial\nterrain. This work demonstrates the potential of deep learning in bridging\nobservational gaps, providing a scalable and efficient solution to inferring\nsubglacial topography.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepTopoNet\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u96f7\u8fbe\u548cBedMachine\u6570\u636e\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u91cd\u5efa\u683c\u9675\u5170\u51b0\u76d6\u4e0b\u7684\u5730\u5f62\u3002", "motivation": "\u683c\u9675\u5170\u51b0\u76d6\u4e0b\u7684\u5730\u5f62\u6570\u636e\u5bf9\u9884\u6d4b\u5176\u672a\u6765\u8d28\u91cf\u635f\u5931\u548c\u5168\u7403\u6d77\u5e73\u9762\u4e0a\u5347\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u89c2\u6d4b\u6570\u636e\u7a00\u758f\u4e14\u590d\u6742\uff0c\u589e\u52a0\u4e86\u6a21\u578b\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u635f\u5931\u5e73\u8861\u673a\u5236\u6574\u5408\u96f7\u8fbe\u548cBedMachine\u6570\u636e\uff0c\u7ed3\u5408\u68af\u5ea6\u7279\u5f81\u548c\u8d8b\u52bf\u8868\u9762\u7279\u5f81\uff0c\u4f7f\u7528CNN\u67b6\u6784\u8fdb\u884c\u4e9a\u7f51\u683c\u5c3a\u5ea6\u9884\u6d4b\u3002", "result": "\u5728Upernavik Isstr\u00f8m\u533a\u57df\u7684\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u80fd\u6709\u6548\u586b\u8865\u89c2\u6d4b\u7a7a\u767d\uff0c\u4e3a\u63a8\u65ad\u51b0\u4e0b\u5730\u5f62\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23807", "pdf": "https://arxiv.org/pdf/2505.23807", "abs": "https://arxiv.org/abs/2505.23807", "authors": ["Yuli Chen", "Bo Cheng", "Jiale Han", "Yingying Zhang", "Yingting Li", "Shuhao Zhang"], "title": "DLP: Dynamic Layerwise Pruning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u6001\u5206\u5c42\u526a\u679d\uff08DLP\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u6743\u91cd\u548c\u8f93\u5165\u6fc0\u6d3b\u4fe1\u606f\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u6bcf\u5c42\u7684\u91cd\u8981\u6027\uff0c\u4ece\u800c\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6709\u6548\u4fdd\u7559\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e3b\u6d41\u7684\u5206\u5c42\u526a\u679d\u6280\u672f\u901a\u5e38\u91c7\u7528\u7edf\u4e00\u7684\u526a\u679d\u7b56\u7565\uff0c\u5bfc\u81f4\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u4e0d\u540c\u5c42\u5bf9\u6a21\u578b\u7684\u8d21\u732e\u4e0d\u540c\uff0c\u4f46\u73b0\u6709\u975e\u5747\u5300\u526a\u679d\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u503c\uff0c\u6027\u80fd\u6b21\u4f18\u3002", "method": "DLP\u65b9\u6cd5\u52a8\u6001\u786e\u5b9a\u6bcf\u5c42\u7684\u76f8\u5bf9\u91cd\u8981\u6027\uff0c\u7ed3\u5408\u6a21\u578b\u6743\u91cd\u548c\u8f93\u5165\u6fc0\u6d3b\u4fe1\u606f\u5206\u914d\u526a\u679d\u7387\u3002", "result": "\u572870%\u7a00\u758f\u5ea6\u4e0b\uff0cDLP\u5c06LLaMA2-7B\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\u4e867.79\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e862.7%\u3002", "conclusion": "DLP\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5bb9\u591a\u79cd\u73b0\u6709LLM\u538b\u7f29\u6280\u672f\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u3002"}}
{"id": "2505.24002", "pdf": "https://arxiv.org/pdf/2505.24002", "abs": "https://arxiv.org/abs/2505.24002", "authors": ["Vaishnav Ramesh", "Junliang Liu", "Haining Wang", "Md Jahidul Islam"], "title": "DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment", "categories": ["cs.CV"], "comment": "18 pages", "summary": "A long-held challenge in no-reference image quality assessment (NR-IQA)\nlearning from human subjective perception is the lack of objective\ngeneralization to unseen natural distortions. To address this, we integrate a\nnovel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which\ndistills scene depth and spatial features into a structure-aware representation\nfor improved NR-IQA. This brings in the knowledge of object saliency and\nrelative contrast of the scene for more discriminative feature learning.\nAdditionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse\nhigh-level global contextual dependencies from a transformer backbone with\nlocal spatial features captured by a set of hierarchical CNN (convolutional\nneural network) layers. We implement TCB and Depth-CAR as multimodal\nattention-based projection functions to select the most informative features,\nwhich also improve training time and inference efficiency. Experimental results\ndemonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA)\nperformance on both synthetic and authentic benchmark datasets. More\nimportantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well\nas in assessing natural image distortions such as low-light effects, hazy\nconditions, and lens flares.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f15\u5bfc\u8de8\u6ce8\u610f\u529b\u548cTransformer-CNN\u6865\u63a5\u673a\u5236\u7684NR-IQA\u65b9\u6cd5DGIQA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3NR-IQA\u4e2d\u7f3a\u4e4f\u5bf9\u672a\u89c1\u81ea\u7136\u5931\u771f\u7684\u5ba2\u89c2\u6cdb\u5316\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Depth-CAR\u673a\u5236\u548cTCB\u6a21\u5757\uff0c\u878d\u5408\u6df1\u5ea6\u4fe1\u606f\u548c\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "DGIQA\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u548c\u81ea\u7136\u5931\u771f\u8bc4\u4f30\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DGIQA\u901a\u8fc7\u6df1\u5ea6\u5f15\u5bfc\u548c\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86NR-IQA\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bc4\u4f30\u6548\u679c\u3002"}}
{"id": "2505.23808", "pdf": "https://arxiv.org/pdf/2505.23808", "abs": "https://arxiv.org/abs/2505.23808", "authors": ["Lin Mu", "Xiaoyu Wang", "Li Ni", "Yang Li", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-rank adaptation (LoRA) has been developed as an efficient approach for\nadapting large language models (LLMs) by fine-tuning two low-rank matrices,\nthereby reducing the number of trainable parameters. However, prior research\nindicates that many of the weights in these matrices are redundant, leading to\ninefficiencies in parameter utilization. To address this limitation, we\nintroduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances\nparameter efficiency while achieving superior performance compared to LoRA.\nDenseLoRA builds upon the concept of representation fine-tuning, incorporating\na single Encoder-Decoder to refine and compress hidden representations across\nall adaptation layers before applying adaptation. Instead of relying on two\nredundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense\nlow-rank matrix, improving parameter utilization and adaptation efficiency. We\nevaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%\naccuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%\naccuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we\nconduct extensive experiments to systematically assess the impact of\nDenseLoRA's components on overall model performance. Code is available at\nhttps://github.com/mulin-ahu/DenseLoRA.", "AI": {"tldr": "DenseLoRA\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5bc6\u96c6\u4f4e\u79e9\u77e9\u9635\u66ff\u4ee3\u4f20\u7edfLoRA\u4e2d\u7684\u5197\u4f59\u77e9\u9635\uff0c\u63d0\u9ad8\u4e86\u53c2\u6570\u5229\u7528\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLoRA\u65b9\u6cd5\u4e2d\u7684\u4f4e\u79e9\u77e9\u9635\u5b58\u5728\u5197\u4f59\u6743\u91cd\uff0c\u5bfc\u81f4\u53c2\u6570\u5229\u7528\u7387\u4f4e\uff0cDenseLoRA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DenseLoRA\u5f15\u5165\u4e00\u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5bf9\u9690\u85cf\u8868\u793a\u8fdb\u884c\u7ec6\u5316\u548c\u538b\u7f29\uff0c\u7136\u540e\u901a\u8fc7\u5bc6\u96c6\u4f4e\u79e9\u77e9\u9635\u8fdb\u884c\u9002\u5e94\u3002", "result": "\u5728LLaMA3-8B\u4e0a\uff0cDenseLoRA\u4ec5\u75280.01%\u53ef\u8bad\u7ec3\u53c2\u6570\u8fbe\u523083.8%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eLoRA\u768480.8%\uff080.70%\u53c2\u6570\uff09\u3002", "conclusion": "DenseLoRA\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfLoRA\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.24007", "pdf": "https://arxiv.org/pdf/2505.24007", "abs": "https://arxiv.org/abs/2505.24007", "authors": ["Nokimul Hasan Arif", "Shadman Rabby", "Md Hefzul Hossain Papon", "Sabbir Ahmed"], "title": "Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model", "categories": ["cs.CV"], "comment": "Submitted for review in AJSE Springer, 21 pages, 4 figures, 4 Tables", "summary": "Visual hallucinations in Large Language Models (LLMs), where the model\ngenerates responses that are inconsistent with the visual input, pose a\nsignificant challenge to their reliability, particularly in contexts where\nprecise and trustworthy outputs are critical. Current research largely\nemphasizes post-hoc correction or model-specific fine-tuning strategies, with\nlimited exploration of preprocessing techniques to address hallucination issues\nat the input stage. This study presents a novel ensemble-based preprocessing\nframework that adaptively selects the most appropriate filtering approach --\nnoise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the\ntype of question posed, resulting into reduced hallucination without requiring\nany modifications to the underlying model architecture or training pipeline.\nEvaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal\nreasoning on visually complex inputs, our method achieves a 44.3% reduction in\nhallucination rates, as measured by Natural Language Inference (NLI) scores\nusing SelfCheckGPT. This demonstrates that intelligent input conditioning alone\ncan significantly enhance factual grounding in LLM responses. The findings\nhighlight the importance of adaptive preprocessing techniques in mitigating\nhallucinations, paving the way for more reliable multimodal systems capable of\naddressing real-world challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u9884\u5904\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u8f93\u5165\u8fc7\u6ee4\u65b9\u6cd5\uff08\u964d\u566a\u3001\u8fb9\u7f18\u589e\u5f3a\u6216\u539f\u59cb\u8f93\u5165\uff09\uff0c\u663e\u8457\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u89c6\u89c9\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u8bad\u7ec3\u6d41\u7a0b\u3002", "motivation": "\u89c6\u89c9\u5e7b\u89c9\u95ee\u9898\u5f71\u54cdLLMs\u7684\u53ef\u9760\u6027\uff0c\u5f53\u524d\u7814\u7a76\u591a\u5173\u6ce8\u4e8b\u540e\u4fee\u6b63\u6216\u6a21\u578b\u5fae\u8c03\uff0c\u7f3a\u4e4f\u5bf9\u9884\u5904\u7406\u6280\u672f\u7684\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u9009\u62e9\u9884\u5904\u7406\u65b9\u6cd5\u7684\u96c6\u6210\u6846\u67b6\uff0c\u6839\u636e\u95ee\u9898\u7c7b\u578b\u9009\u62e9\u964d\u566a\u3001\u8fb9\u7f18\u589e\u5f3a\u6216\u539f\u59cb\u8f93\u5165\u3002", "result": "\u5728`HaloQuest'\u6570\u636e\u96c6\u4e0a\uff0c\u5e7b\u89c9\u7387\u964d\u4f4e\u4e8644.3%\uff0c\u901a\u8fc7SelfCheckGPT\u7684NLI\u8bc4\u5206\u9a8c\u8bc1\u3002", "conclusion": "\u667a\u80fd\u8f93\u5165\u9884\u5904\u7406\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u4e8b\u5b9e\u57fa\u7840\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2505.23809", "pdf": "https://arxiv.org/pdf/2505.23809", "abs": "https://arxiv.org/abs/2505.23809", "authors": ["Haowei Yang", "Haotian Lyu", "Tianle Zhang", "Dingzhou Wang", "Yushang Zhao"], "title": "LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "As e-commerce competition intensifies, balancing creative content with\nconversion effectiveness becomes critical. Leveraging LLMs' language generation\ncapabilities, we propose a framework that integrates prompt engineering,\nmulti-objective fine-tuning, and post-processing to generate marketing copy\nthat is both engaging and conversion-driven. Our fine-tuning method combines\nsentiment adjustment, diversity enhancement, and CTA embedding. Through offline\nevaluations and online A/B tests across categories, our approach achieves a\n12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content\nnovelty. This provides a practical solution for automated copy generation and\nsuggests paths for future multimodal, real-time personalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u76ee\u6807\u5fae\u8c03\u548c\u540e\u5904\u7406\u751f\u6210\u517c\u5177\u5438\u5f15\u529b\u548c\u8f6c\u5316\u6548\u679c\u7684\u8425\u9500\u6587\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86CTR\u548cCVR\u3002", "motivation": "\u968f\u7740\u7535\u5546\u7ade\u4e89\u52a0\u5267\uff0c\u5982\u4f55\u5728\u521b\u610f\u5185\u5bb9\u548c\u8f6c\u5316\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u76ee\u6807\u5fae\u8c03\uff08\u5305\u62ec\u60c5\u611f\u8c03\u6574\u3001\u591a\u6837\u6027\u589e\u5f3a\u548cCTA\u5d4c\u5165\uff09\u548c\u540e\u5904\u7406\u6280\u672f\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cCTR\u63d0\u534712.5%\uff0cCVR\u63d0\u53478.3%\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u65b0\u9896\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u6587\u6848\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u591a\u6a21\u6001\u3001\u5b9e\u65f6\u4e2a\u6027\u5316\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.24023", "pdf": "https://arxiv.org/pdf/2505.24023", "abs": "https://arxiv.org/abs/2505.24023", "authors": ["Sangwon Jung", "Alex Oesterling", "Claudio Mayrink Verdun", "Sajani Vithana", "Taesup Moon", "Flavio P. Calmon"], "title": "Multi-Group Proportional Representation for Text-to-Image Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) generative models can create vivid, realistic images from\ntextual descriptions. As these models proliferate, they expose new concerns\nabout their ability to represent diverse demographic groups, propagate\nstereotypes, and efface minority populations. Despite growing attention to the\n\"safe\" and \"responsible\" design of artificial intelligence (AI), there is no\nestablished methodology to systematically measure and control representational\nharms in image generation. This paper introduces a novel framework to measure\nthe representation of intersectional groups in images generated by T2I models\nby applying the Multi-Group Proportional Representation (MPR) metric. MPR\nevaluates the worst-case deviation of representation statistics across given\npopulation groups in images produced by a generative model, allowing for\nflexible and context-specific measurements based on user requirements. We also\ndevelop an algorithm to optimize T2I models for this metric. Through\nexperiments, we demonstrate that MPR can effectively measure representation\nstatistics across multiple intersectional groups and, when used as a training\nobjective, can guide models toward a more balanced generation across\ndemographic groups while maintaining generation quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7fa4\u4f53\u6bd4\u4f8b\u8868\u793a\uff08MPR\uff09\u5ea6\u91cf\u6807\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5bf9\u4ea4\u53c9\u7fa4\u4f53\u7684\u4ee3\u8868\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4f18\u5316\u7b97\u6cd5\u4ee5\u6539\u5584\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5176\u53ef\u80fd\u5bfc\u81f4\u7684\u4ee3\u8868\u6027\u504f\u5dee\u548c\u523b\u677f\u5370\u8c61\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u6d4b\u91cf\u548c\u63a7\u5236\u8fd9\u4e9b\u5371\u5bb3\u3002", "method": "\u5f15\u5165MPR\u5ea6\u91cf\u6807\u51c6\uff0c\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e2d\u4e0d\u540c\u7fa4\u4f53\u7684\u6700\u574f\u504f\u5dee\uff0c\u5e76\u5f00\u53d1\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u4ee5\u5e73\u8861\u7fa4\u4f53\u4ee3\u8868\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMPR\u80fd\u6709\u6548\u6d4b\u91cf\u591a\u4ea4\u53c9\u7fa4\u4f53\u7684\u4ee3\u8868\u6027\uff0c\u4e14\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\u65f6\uff0c\u53ef\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u6539\u5584\u5e73\u8861\u6027\u3002", "conclusion": "MPR\u4e3a\u6d4b\u91cf\u548c\u4f18\u5316\u751f\u6210\u6a21\u578b\u7684\u4ee3\u8868\u6027\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6709\u6548\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11AI\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u504f\u5dee\u3002"}}
{"id": "2505.23810", "pdf": "https://arxiv.org/pdf/2505.23810", "abs": "https://arxiv.org/abs/2505.23810", "authors": ["Chenghao Yang", "Yinbo Luo", "Zhoufutu Wen", "Qi Chu", "Tao Gong", "Longxiang Liu", "Kaiyuan Zhang", "Jianpeng Jiao", "Ge Zhang", "Wenhao Huang", "Nenghai Yu"], "title": "MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 13 figures", "summary": "Large Language Models (\\textbf{LLMs}), e.g. ChatGPT, have been widely adopted\nin real-world dialogue applications. However, LLMs' robustness, especially in\nhandling long complex dialogue sessions, including frequent motivation\ntransfer, sophisticated cross-turn dependency, is criticized all along.\nNevertheless, no existing benchmarks can fully reflect these weaknesses. We\npresent \\textbf{MARS-Bench}, a \\textbf{M}ulti-turn \\textbf{A}thletic\n\\textbf{R}eal-world \\textbf{S}cenario Dialogue \\textbf{Bench}mark, designed to\nremedy the gap. MARS-Bench is constructed from play-by-play text commentary so\nto feature realistic dialogues specifically designed to evaluate three critical\naspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,\nand Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that\nclosed-source LLMs significantly outperform open-source alternatives, explicit\nreasoning significantly boosts LLMs' robustness on handling long complex\ndialogue sessions, and LLMs indeed face significant challenges when handling\nmotivation transfer and sophisticated cross-turn dependency. Moreover, we\nprovide mechanistic interpretability on how attention sinks due to special\ntokens lead to LLMs' performance degradation when handling long complex\ndialogue sessions based on attention visualization experiment in\nQwen2.5-7B-Instruction.", "AI": {"tldr": "MARS-Bench\u662f\u4e00\u4e2a\u591a\u8f6e\u73b0\u5b9e\u573a\u666f\u5bf9\u8bdd\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u8d85\u591a\u8f6e\u3001\u4ea4\u4e92\u591a\u8f6e\u548c\u8de8\u8f6e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u95ed\u6e90LLMs\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u663e\u5f0f\u63a8\u7406\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u4ee5\u53caLLMs\u5728\u5904\u7406\u52a8\u673a\u8f6c\u79fb\u548c\u8de8\u8f6e\u4f9d\u8d56\u65f6\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u53cd\u6620LLMs\u5728\u5904\u7406\u957f\u590d\u6742\u5bf9\u8bdd\u4f1a\u8bdd\uff08\u5982\u52a8\u673a\u8f6c\u79fb\u548c\u8de8\u8f6e\u4f9d\u8d56\uff09\u65f6\u7684\u5f31\u70b9\uff0c\u56e0\u6b64\u63d0\u51faMARS-Bench\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MARS-Bench\u57fa\u4e8e\u5b9e\u65f6\u6587\u672c\u8bc4\u8bba\u6784\u5efa\uff0c\u8bc4\u4f30LLMs\u5728\u8d85\u591a\u8f6e\u3001\u4ea4\u4e92\u591a\u8f6e\u548c\u8de8\u8f6e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u5b9e\u9a8c\u5206\u6790\u6027\u80fd\u4e0b\u964d\u673a\u5236\u3002", "result": "\u95ed\u6e90LLMs\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u663e\u5f0f\u63a8\u7406\u63d0\u5347\u9c81\u68d2\u6027\uff0cLLMs\u5728\u5904\u7406\u52a8\u673a\u8f6c\u79fb\u548c\u8de8\u8f6e\u4f9d\u8d56\u65f6\u9762\u4e34\u6311\u6218\u3002", "conclusion": "MARS-Bench\u4e3a\u8bc4\u4f30LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5176\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.24025", "pdf": "https://arxiv.org/pdf/2505.24025", "abs": "https://arxiv.org/abs/2505.24025", "authors": ["Chenbin Pan", "Wenbin He", "Zhengzhong Tu", "Liu Ren"], "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The recent explosive interest in the reasoning capabilities of large language\nmodels, such as DeepSeek-R1, has demonstrated remarkable success through\nreinforcement learning-based fine-tuning frameworks, exemplified by methods\nlike Group Relative Policy Optimization (GRPO). However, such reasoning\nabilities remain underexplored and notably absent in vision foundation models,\nincluding representation models like the DINO series. In this work, we propose\n\\textbf{DINO-R1}, the first such attempt to incentivize visual in-context\nreasoning capabilities of vision foundation models using reinforcement\nlearning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query\nOptimization (GRQO)}, a novel reinforcement-style training strategy explicitly\ndesigned for query-based representation models, which computes query-level\nrewards based on group-normalized alignment quality. We also apply\nKL-regularization to stabilize the objectness distribution to reduce the\ntraining instability. This joint optimization enables dense and expressive\nsupervision across queries while mitigating overfitting and distributional\ndrift. Building upon Grounding-DINO, we train a series of DINO-R1 family models\nthat integrate a visual prompt encoder and a visual-guided query selection\nmechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that\nDINO-R1 significantly outperforms supervised fine-tuning baselines, achieving\nstrong generalization in both open-vocabulary and closed-set visual prompting\nscenarios.", "AI": {"tldr": "DINO-R1\u9996\u6b21\u5c1d\u8bd5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51faGRQO\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5df2\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u4f46\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINO\u7cfb\u5217\uff09\u7684\u7c7b\u4f3c\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faGRQO\u7b56\u7565\uff0c\u7ed3\u5408KL\u6b63\u5219\u5316\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5e76\u57fa\u4e8eGrounding-DINO\u6784\u5efaDINO-R1\u7cfb\u5217\u6a21\u578b\u3002", "result": "\u5728COCO\u3001LVIS\u548cODinW\u7b49\u6570\u636e\u96c6\u4e0a\uff0cDINO-R1\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u57fa\u7ebf\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "DINO-R1\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u5f15\u5165\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.23811", "pdf": "https://arxiv.org/pdf/2505.23811", "abs": "https://arxiv.org/abs/2505.23811", "authors": ["Hadi Askari", "Shivanshu Gupta", "Fei Wang", "Anshuman Chhabra", "Muhao Chen"], "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Pretrained Large Language Models (LLMs) achieve strong performance across a\nwide range of tasks, yet exhibit substantial variability in the various layers'\ntraining quality with respect to specific downstream applications, limiting\ntheir downstream performance.It is therefore critical to estimate layer-wise\ntraining quality in a manner that accounts for both model architecture and\ntraining data. However, existing approaches predominantly rely on model-centric\nheuristics (such as spectral statistics, outlier detection, or uniform\nallocation) while overlooking the influence of data. To address these\nlimitations, we propose LayerIF, a data-driven framework that leverages\nInfluence Functions to quantify the training quality of individual layers in a\nprincipled and task-sensitive manner. By isolating each layer's gradients and\nmeasuring the sensitivity of the validation loss to training examples by\ncomputing layer-wise influences, we derive data-driven estimates of layer\nimportance. Notably, our method produces task-specific layer importance\nestimates for the same LLM, revealing how layers specialize for different\ntest-time evaluation tasks. We demonstrate the utility of our scores by\nleveraging them for two downstream applications: (a) expert allocation in\nLoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM\npruning. Experiments across multiple LLM architectures demonstrate that our\nmodel-agnostic, influence-guided allocation leads to consistent gains in task\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLayerIF\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u91cf\u5316LLM\u5404\u5c42\u8bad\u7ec3\u8d28\u91cf\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u4e2d\u5fc3\u542f\u53d1\u5f0f\uff0c\u5ffd\u89c6\u6570\u636e\u5f71\u54cd\uff0c\u9650\u5236\u4e86LLM\u4e0b\u6e38\u8868\u73b0\u3002", "method": "\u5229\u7528\u5f71\u54cd\u51fd\u6570\uff08Influence Functions\uff09\u8ba1\u7b97\u5c42\u95f4\u68af\u5ea6\u654f\u611f\u6027\uff0c\u91cf\u5316\u5404\u5c42\u8bad\u7ec3\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLayerIF\u80fd\u4efb\u52a1\u654f\u611f\u5730\u5206\u914d\u4e13\u5bb6\u548c\u7a00\u758f\u5ea6\uff0c\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LayerIF\u4e3aLLM\u5c42\u95f4\u4f18\u5316\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2505.24026", "pdf": "https://arxiv.org/pdf/2505.24026", "abs": "https://arxiv.org/abs/2505.24026", "authors": ["Numair Nadeem", "Muhammad Hamza Asad", "Saeed Anwar", "Abdul Bais"], "title": "MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures, presented at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2025. Reviewer comments available upon\n  request", "summary": "Semantic segmentation of crops and weeds is crucial for site-specific farm\nmanagement; however, most existing methods depend on labor intensive\npixel-level annotations. A further challenge arises when models trained on one\nfield (source domain) fail to generalize to new fields (target domain) due to\ndomain shifts, such as variations in lighting, camera setups, soil composition,\nand crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by\nenabling adaptation without target-domain labels, but current UDA methods\nstruggle with occlusions and visual blending between crops and weeds, leading\nto misclassifications in real-world conditions. To overcome these limitations,\nwe introduce MaskAdapt, a novel approach that enhances segmentation accuracy\nthrough multimodal contextual learning by integrating RGB images with features\nderived from depth data. By computing depth gradients from depth maps, our\nmethod captures spatial transitions that help resolve texture ambiguities.\nThese gradients, through a cross-attention mechanism, refines RGB feature\nrepresentations, resulting in sharper boundary delineation. In addition, we\npropose a geometry-aware masking strategy that applies horizontal, vertical,\nand stochastic masks during training. This encourages the model to focus on the\nbroader spatial context for robust visual recognition. Evaluations on real\nagricultural datasets demonstrate that MaskAdapt consistently outperforms\nexisting State-of-the-Art (SOTA) UDA methods, achieving improved segmentation\nmean Intersection over Union (mIOU) across diverse field conditions.", "AI": {"tldr": "MaskAdapt\u662f\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u6570\u636e\u7279\u5f81\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f5c\u7269\u4e0e\u6742\u8349\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u906e\u6321\u548c\u89c6\u89c9\u6df7\u6dc6\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4f5c\u7269\u4e0e\u6742\u8349\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u50cf\u7d20\u7ea7\u6807\u6ce8\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u519c\u7530\u73af\u5883\uff08\u57df\u504f\u79fb\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faMaskAdapt\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u68af\u5ea6\u6355\u6349\u7a7a\u95f4\u8fc7\u6e21\u4fe1\u606f\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316RGB\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u51e0\u4f55\u611f\u77e5\u63a9\u7801\u7b56\u7565\u589e\u5f3a\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u519c\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cMaskAdapt\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u7684mIOU\u3002", "conclusion": "MaskAdapt\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u548c\u51e0\u4f55\u611f\u77e5\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57df\u9002\u5e94\u4e2d\u7684\u906e\u6321\u548c\u89c6\u89c9\u6df7\u6dc6\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2505.23812", "pdf": "https://arxiv.org/pdf/2505.23812", "abs": "https://arxiv.org/abs/2505.23812", "authors": ["Lata Pangtey", "Mohammad Zia Ur Rehman", "Prasad Chaudhari", "Shubhi Bansal", "Nagendra Kumar"], "title": "Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of social media has generated an overwhelming volume of\nuser-generated content, conveying implicit opinions and contributing to the\nspread of misinformation. The method aims to enhance the detection of stance\nwhere misinformation can polarize user opinions. Stance detection has emerged\nas a crucial approach to effectively analyze underlying biases in shared\ninformation and combating misinformation. This paper proposes a novel method\nfor \\textbf{S}tance \\textbf{P}rediction through a \\textbf{L}abel-fused dual\ncross-\\textbf{A}ttentive \\textbf{E}motion-aware neural \\textbf{Net}work\n(SPLAENet) in misinformative social media user-generated content. The proposed\nmethod employs a dual cross-attention mechanism and a hierarchical attention\nnetwork to capture inter and intra-relationships by focusing on the relevant\nparts of source text in the context of reply text and vice versa. We\nincorporate emotions to effectively distinguish between different stance\ncategories by leveraging the emotional alignment or divergence between the\ntexts. We also employ label fusion that uses distance-metric learning to align\nextracted features with stance labels, improving the method's ability to\naccurately distinguish between stances. Extensive experiments demonstrate the\nsignificant improvements achieved by SPLAENet over existing state-of-the-art\nmethods. SPLAENet demonstrates an average gain of 8.92\\% in accuracy and\n17.36\\% in F1-score on the RumourEval dataset. On the SemEval dataset, it\nachieves average gains of 7.02\\% in accuracy and 10.92\\% in F1-score. On the\nP-stance dataset, it demonstrates average gains of 10.03\\% in accuracy and\n11.18\\% in F1-score. These results validate the effectiveness of the proposed\nmethod for stance detection in the context of misinformative social media\ncontent.", "AI": {"tldr": "SPLAENet\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7acb\u573a\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u60c5\u611f\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u793e\u4ea4\u5a92\u4f53\u865a\u5047\u4fe1\u606f\u5185\u5bb9\u4e2d\u7684\u7acb\u573a\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u7528\u6237\u751f\u6210\u5185\u5bb9\u7684\u5feb\u901f\u589e\u957f\u548c\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\uff0c\u4f7f\u5f97\u7acb\u573a\u68c0\u6d4b\u6210\u4e3a\u5206\u6790\u504f\u89c1\u548c\u6253\u51fb\u865a\u5047\u4fe1\u606f\u7684\u5173\u952e\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSPLAENet\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u5206\u5c42\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u5229\u7528\u60c5\u611f\u5bf9\u9f50\u548c\u6807\u7b7e\u878d\u5408\u6280\u672f\u6765\u533a\u5206\u4e0d\u540c\u7acb\u573a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSPLAENet\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534717.36%\u3002", "conclusion": "SPLAENet\u5728\u793e\u4ea4\u5a92\u4f53\u865a\u5047\u4fe1\u606f\u5185\u5bb9\u4e2d\u7684\u7acb\u573a\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.24076", "pdf": "https://arxiv.org/pdf/2505.24076", "abs": "https://arxiv.org/abs/2505.24076", "authors": ["Huan Ning", "Zhenlong Li", "Manzhu Yu", "Wenpeng Yin"], "title": "SIM: A mapping framework for built environment auditing based on street view imagery", "categories": ["cs.CV"], "comment": null, "summary": "Built environment auditing refers to the systematic documentation and\nassessment of urban and rural spaces' physical, social, and environmental\ncharacteristics, such as walkability, road conditions, and traffic lights. It\nis used to collect data for the evaluation of how built environments impact\nhuman behavior, health, mobility, and overall urban functionality.\nTraditionally, built environment audits were conducted using field surveys and\nmanual observations, which were time-consuming and costly. The emerging street\nview imagery, e.g., Google Street View, has become a widely used data source\nfor conducting built environment audits remotely. Deep learning and computer\nvision techniques can extract and classify objects from street images to\nenhance auditing productivity. Before meaningful analysis, the detected objects\nneed to be geospatially mapped for accurate documentation. However, the mapping\nmethods and tools based on street images are underexplored, and there are no\nuniversal frameworks or solutions yet, imposing difficulties in auditing the\nstreet objects. In this study, we introduced an open source street view mapping\nframework, providing three pipelines to map and measure: 1) width measurement\nfor ground objects, such as roads; 2) 3D localization for objects with a known\ndimension (e.g., doors and stop signs); and 3) diameter measurements (e.g.,\nstreet trees). These pipelines can help researchers, urban planners, and other\nprofessionals automatically measure and map target objects, promoting built\nenvironment auditing productivity and accuracy. Three case studies, including\nroad width measurement, stop sign localization, and street tree diameter\nmeasurement, are provided in this paper to showcase pipeline usage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8857\u666f\u56fe\u50cf\u7684\u5f00\u6e90\u6620\u5c04\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u6d4b\u91cf\u548c\u6620\u5c04\u5730\u9762\u7269\u4f53\u30013D\u5b9a\u4f4d\u5df2\u77e5\u5c3a\u5bf8\u7269\u4f53\u4ee5\u53ca\u6d4b\u91cf\u76f4\u5f84\uff0c\u4ee5\u63d0\u9ad8\u5efa\u7b51\u73af\u5883\u5ba1\u8ba1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u73af\u5883\u5ba1\u8ba1\u4f9d\u8d56\u8017\u65f6\u4e14\u6602\u8d35\u7684\u4eba\u5de5\u8c03\u67e5\uff0c\u800c\u65b0\u5174\u7684\u8857\u666f\u56fe\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e3a\u8fdc\u7a0b\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u7684\u6620\u5c04\u65b9\u6cd5\u548c\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u63d0\u4f9b\u4e09\u79cd\u7ba1\u9053\uff1a\u5730\u9762\u7269\u4f53\u5bbd\u5ea6\u6d4b\u91cf\u3001\u5df2\u77e5\u5c3a\u5bf8\u7269\u4f53\u76843D\u5b9a\u4f4d\u4ee5\u53ca\u76f4\u5f84\u6d4b\u91cf\u3002", "result": "\u901a\u8fc7\u9053\u8def\u5bbd\u5ea6\u6d4b\u91cf\u3001\u505c\u8f66\u6807\u5fd7\u5b9a\u4f4d\u548c\u8857\u9053\u6811\u6728\u76f4\u5f84\u6d4b\u91cf\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5efa\u7b51\u73af\u5883\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u548c\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5ba1\u8ba1\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.23815", "pdf": "https://arxiv.org/pdf/2505.23815", "abs": "https://arxiv.org/abs/2505.23815", "authors": ["St\u00e9phane Aroca-Ouellette", "Natalie Mackraz", "Barry-John Theobald", "Katherine Metcalf"], "title": "Aligning LLMs by Predicting Preferences from User Writing Samples", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025. 32 pages total: 9 main, 2 references, 21\n  appendix. arXiv admin note: substantial text overlap with arXiv:2410.06273", "summary": "Accommodating human preferences is essential for creating aligned LLM agents\nthat deliver personalized and effective interactions. Recent work has shown the\npotential for LLMs acting as writing agents to infer a description of user\npreferences. Agent alignment then comes from conditioning on the inferred\npreference description. However, existing methods often produce generic\npreference descriptions that fail to capture the unique and individualized\nnature of human preferences. This paper introduces PROSE, a method designed to\nenhance the precision of preference descriptions inferred from user writing\nsamples. PROSE incorporates two key elements: (1) iterative refinement of\ninferred preferences, and (2) verification of inferred preferences across\nmultiple user writing samples. We evaluate PROSE with several LLMs (i.e.,\nQwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an\nemail writing task. We find that PROSE more accurately infers nuanced human\npreferences, improving the quality of the writing agent's generations over\nCIPHER (a state-of-the-art method for inferring preferences) by 33\\%. Lastly,\nwe demonstrate that ICL and PROSE are complementary methods, and combining them\nprovides up to a 9\\% improvement over ICL alone.", "AI": {"tldr": "PROSE\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u9a8c\u8bc1\u7528\u6237\u504f\u597d\u63cf\u8ff0\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5bf9\u4e2a\u6027\u5316\u504f\u597d\u7684\u6355\u6349\u80fd\u529b\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u504f\u597d\u63cf\u8ff0\u8fc7\u4e8e\u901a\u7528\uff0c\u65e0\u6cd5\u4f53\u73b0\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u504f\u597d\u63a8\u65ad\u65b9\u6cd5\u3002", "method": "PROSE\u7ed3\u5408\u8fed\u4ee3\u4f18\u5316\u548c\u8de8\u6837\u672c\u9a8c\u8bc1\uff0c\u4ece\u7528\u6237\u5199\u4f5c\u6837\u672c\u4e2d\u63a8\u65ad\u66f4\u7cbe\u786e\u7684\u504f\u597d\u63cf\u8ff0\u3002", "result": "PROSE\u5728\u591a\u4e2aLLM\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u504f\u597d\u63a8\u65ad\u51c6\u786e\u7387\u63d0\u534733%\uff0c\u4e0eICL\u7ed3\u5408\u540e\u6548\u679c\u8fdb\u4e00\u6b65\u63d0\u53479%\u3002", "conclusion": "PROSE\u80fd\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u7684\u4e2a\u6027\u5316\u4ea4\u4e92\u80fd\u529b\uff0c\u4e3a\u504f\u597d\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24086", "pdf": "https://arxiv.org/pdf/2505.24086", "abs": "https://arxiv.org/abs/2505.24086", "authors": ["Zeeshan Khan", "Shizhe Chen", "Cordelia Schmid"], "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text.", "AI": {"tldr": "ComposeAnything\u901a\u8fc7\u7ed3\u5408LLM\u7684\u63a8\u7406\u80fd\u529b\u548c2.5D\u8bed\u4e49\u5e03\u5c40\uff0c\u6539\u8fdb\u4e86\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u590d\u6742\u5bf9\u8c61\u6392\u5217\u548c3D\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u590d\u6742\u5bf9\u8c61\u6392\u5217\u548c3D\u5b9a\u4f4d\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e03\u5c40\u65b9\u6cd5\u867d\u80fd\u6539\u55842D\u6392\u5217\uff0c\u4f46\u727a\u7272\u4e86\u8d28\u91cf\u548c\u8fde\u8d2f\u6027\u3002", "method": "\u5229\u7528LLM\u751f\u62102.5D\u8bed\u4e49\u5e03\u5c40\uff08\u542b\u6df1\u5ea6\u4fe1\u606f\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u751f\u6210\u7a7a\u95f4\u548c\u6df1\u5ea6\u611f\u77e5\u7684\u7c97\u5408\u6210\u56fe\uff0c\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\uff0c\u901a\u8fc7\u5bf9\u8c61\u5148\u9a8c\u5f3a\u5316\u548c\u7a7a\u95f4\u63a7\u5236\u53bb\u566a\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728T2I-CompBench\u548cNSR-1K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u57282D/3D\u7a7a\u95f4\u6392\u5217\u3001\u9ad8\u5bf9\u8c61\u6570\u91cf\u548c\u8d85\u73b0\u5b9e\u7ec4\u5408\u65b9\u9762\u3002", "conclusion": "ComposeAnything\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u5fe0\u5b9e\u4e8e\u6587\u672c\u7684\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u5bf9\u8c61\u6392\u5217\u548c3D\u5b9a\u4f4d\u7684\u6311\u6218\u3002"}}
{"id": "2505.23816", "pdf": "https://arxiv.org/pdf/2505.23816", "abs": "https://arxiv.org/abs/2505.23816", "authors": ["Trenton Chang", "Tobias Schnabel", "Adith Swaminathan", "Jenna Wiens"], "title": "A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 8 figures. 26 pages of references and supplementary\n  material, 20 additional figures", "summary": "Despite advances in large language models (LLMs) on reasoning and\ninstruction-following benchmarks, it remains unclear whether they can reliably\nproduce outputs aligned with a broad variety of user goals, a concept we refer\nto as steerability. The abundance of methods proposed to modify LLM behavior\nmakes it unclear whether current LLMs are already steerable, or require further\nintervention. In particular, LLMs may exhibit (i) poor coverage, where rare\nuser goals are underrepresented; (ii) miscalibration, where models overshoot\nrequests; and (iii) side effects, where changes to one dimension of text\ninadvertently affect others. To systematically evaluate these failures, we\nintroduce a framework based on a multi-dimensional goal space that models user\ngoals and LLM outputs as vectors with dimensions corresponding to text\nattributes (e.g., reading difficulty). Applied to a text-rewriting task, we\nfind that current LLMs struggle with steerability, as side effects are\npersistent. Interventions to improve steerability, such as prompt engineering,\nbest-of-$N$ sampling, and reinforcement learning fine-tuning, have varying\neffectiveness, yet side effects remain problematic. Our findings suggest that\neven strong LLMs struggle with steerability, and existing alignment strategies\nmay be insufficient. We open-source our steerability evaluation framework at\nhttps://github.com/MLD3/steerability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7528\u6237\u76ee\u6807\u5bf9\u9f50\uff08steerability\uff09\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8986\u76d6\u6027\u3001\u6821\u51c6\u6027\u548c\u526f\u4f5c\u7528\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u662f\u5426\u80fd\u53ef\u9760\u5730\u6ee1\u8db3\u591a\u6837\u5316\u7684\u7528\u6237\u76ee\u6807\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u7ef4\u76ee\u6807\u7a7a\u95f4\u7684\u6846\u67b6\uff0c\u5c06\u7528\u6237\u76ee\u6807\u548cLLM\u8f93\u51fa\u5efa\u6a21\u4e3a\u5411\u91cf\uff0c\u5e76\u5e94\u7528\u4e8e\u6587\u672c\u91cd\u5199\u4efb\u52a1\u3002", "result": "\u53d1\u73b0\u5f53\u524dLLMs\u5728steerability\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e72\u9884\u63aa\u65bd\uff08\u5982\u63d0\u793a\u5de5\u7a0b\u3001\u91c7\u6837\u548c\u5fae\u8c03\uff09\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u73b0\u6709LLMs\u5728steerability\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u5bf9\u9f50\u7b56\u7565\u3002"}}
{"id": "2505.24103", "pdf": "https://arxiv.org/pdf/2505.24103", "abs": "https://arxiv.org/abs/2505.24103", "authors": ["Peiran Xu", "Yadong Mu"], "title": "Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "In this work, we focus on the task of weakly supervised affordance grounding,\nwhere a model is trained to identify affordance regions on objects using\nhuman-object interaction images and egocentric object images without dense\nlabels. Previous works are mostly built upon class activation maps, which are\neffective for semantic segmentation but may not be suitable for locating\nactions and functions. Leveraging recent advanced foundation models, we develop\na supervised training pipeline based on pseudo labels. The pseudo labels are\ngenerated from an off-the-shelf part segmentation model, guided by a mapping\nfrom affordance to part names. Furthermore, we introduce three key enhancements\nto the baseline model: a label refining stage, a fine-grained feature alignment\nprocess, and a lightweight reasoning module. These techniques harness the\nsemantic knowledge of static objects embedded in off-the-shelf foundation\nmodels to improve affordance learning, effectively bridging the gap between\nobjects and actions. Extensive experiments demonstrate that the performance of\nthe proposed model has achieved a breakthrough improvement over existing\nmethods. Our codes are available at https://github.com/woyut/WSAG-PLSP .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u529f\u80fd\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5229\u7528\u4f2a\u6807\u7b7e\u548c\u57fa\u7840\u6a21\u578b\u6539\u8fdb\u529f\u80fd\u5b66\u4e60\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u7c7b\u6fc0\u6d3b\u56fe\uff0c\u9002\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u4f46\u4e0d\u9002\u5408\u5b9a\u4f4d\u52a8\u4f5c\u548c\u529f\u80fd\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u57fa\u4e8e\u4f2a\u6807\u7b7e\u7684\u76d1\u7763\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7ed3\u5408\u6807\u7b7e\u7ec6\u5316\u3001\u7ec6\u7c92\u5ea6\u7279\u5f81\u5bf9\u9f50\u548c\u8f7b\u91cf\u63a8\u7406\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u6210\u529f\u5f25\u5408\u4e86\u5bf9\u8c61\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.23818", "pdf": "https://arxiv.org/pdf/2505.23818", "abs": "https://arxiv.org/abs/2505.23818", "authors": ["Masoud Safilian", "Amin Beheshti", "Stephen Elbourn"], "title": "Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams", "categories": ["cs.CL"], "comment": null, "summary": "Automated answer grading is a critical challenge in educational technology,\nwith the potential to streamline assessment processes, ensure grading\nconsistency, and provide timely feedback to students. However, existing\napproaches are often constrained to specific exam formats, lack\ninterpretability in score assignment, and struggle with real-world\napplicability across diverse subjects and assessment types. To address these\nlimitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a\nnovel framework that leverages state-of-the-art generative AI models for\nrubric-based grading of textual responses. RATAS is designed to support a wide\nrange of grading rubrics, enable subject-agnostic evaluation, and generate\nstructured, explainable rationales for assigned scores. We formalize the\nautomatic grading task through a mathematical framework tailored to\nrubric-based assessment and present an architecture capable of handling\ncomplex, real-world exam structures. To rigorously evaluate our approach, we\nconstruct a unique, contextualized dataset derived from real-world\nproject-based courses, encompassing diverse response formats and varying levels\nof complexity. Empirical results demonstrate that RATAS achieves high\nreliability and accuracy in automated grading while providing interpretable\nfeedback that enhances transparency for both students and nstructors.", "AI": {"tldr": "RATAS\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210AI\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u6846\u67b6\uff0c\u652f\u6301\u591a\u6837\u5316\u8bc4\u5206\u6807\u51c6\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u4f9d\u636e\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u5316\u8bc4\u5206\u65b9\u6cd5\u5728\u9002\u7528\u6027\u3001\u4e00\u81f4\u6027\u548c\u89e3\u91ca\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u751f\u6210AI\u6a21\u578b\u548c\u6570\u5b66\u6846\u67b6\uff0c\u8bbe\u8ba1\u652f\u6301\u591a\u6837\u5316\u8bc4\u5206\u6807\u51c6\u7684RATAS\u6846\u67b6\u3002", "result": "\u5728\u771f\u5b9e\u8bfe\u7a0b\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cRATAS\u8868\u73b0\u51fa\u9ad8\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u53cd\u9988\u3002", "conclusion": "RATAS\u4e3a\u81ea\u52a8\u5316\u8bc4\u5206\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u8bc4\u4f30\u573a\u666f\u3002"}}
{"id": "2505.24108", "pdf": "https://arxiv.org/pdf/2505.24108", "abs": "https://arxiv.org/abs/2505.24108", "authors": ["Alina Devkota", "Annahita Amireskandari", "Joel Palko", "Shyam Thakkar", "Donald Adjeroh", "Xiajun Jiang", "Binod Bhattarai", "Prashnna K. Gyawali"], "title": "Federated Foundation Model for GI Endoscopy Images", "categories": ["cs.CV", "cs.LG", "I.2.10; I.4; I.5"], "comment": "11 pages, 11 figures, submitted to BHI2025", "summary": "Gastrointestinal (GI) endoscopy is essential in identifying GI tract\nabnormalities in order to detect diseases in their early stages and improve\npatient outcomes. Although deep learning has shown success in supporting GI\ndiagnostics and decision-making, these models require curated datasets with\nlabels that are expensive to acquire. Foundation models offer a promising\nsolution by learning general-purpose representations, which can be finetuned\nfor specific tasks, overcoming data scarcity. Developing foundation models for\nmedical imaging holds significant potential, but the sensitive and protected\nnature of medical data presents unique challenges. Foundation model training\ntypically requires extensive datasets, and while hospitals generate large\nvolumes of data, privacy restrictions prevent direct data sharing, making\nfoundation model training infeasible in most scenarios. In this work, we\npropose a FL framework for training foundation models for gastroendoscopy\nimaging, enabling data to remain within local hospital environments while\ncontributing to a shared model. We explore several established FL algorithms,\nassessing their suitability for training foundation models without relying on\ntask-specific labels, conducting experiments in both homogeneous and\nheterogeneous settings. We evaluate the trained foundation model on three\ncritical downstream tasks--classification, detection, and segmentation--and\ndemonstrate that it achieves improved performance across all tasks,\nhighlighting the effectiveness of our approach in a federated,\nprivacy-preserving setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u6210\u50cf\u7684\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u533b\u7597\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u5728\u75be\u75c5\u65e9\u671f\u68c0\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u6807\u6ce8\u6570\u636e\u3002\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u901a\u7528\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u533b\u7597\u6570\u636e\u7684\u9690\u79c1\u9650\u5236\u4f7f\u5176\u8bad\u7ec3\u56f0\u96be\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u6570\u636e\u4fdd\u7559\u5728\u672c\u5730\u533b\u9662\u73af\u5883\u4e2d\uff0c\u540c\u65f6\u8d21\u732e\u4e8e\u5171\u4eab\u6a21\u578b\u3002\u8bc4\u4f30\u4e86\u591a\u79cdFL\u7b97\u6cd5\u5728\u65e0\u4efb\u52a1\u6807\u7b7e\u60c5\u51b5\u4e0b\u7684\u9002\u7528\u6027\uff0c\u5e76\u5728\u540c\u8d28\u548c\u5f02\u8d28\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u5728\u9690\u79c1\u4fdd\u62a4\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u8bad\u7ec3\u4e86\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u533b\u7597\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23820", "pdf": "https://arxiv.org/pdf/2505.23820", "abs": "https://arxiv.org/abs/2505.23820", "authors": ["Bhaktipriya Radharapu", "Manon Revel", "Megan Ung", "Sebastian Ruder", "Adina Williams"], "title": "Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks", "categories": ["cs.CL"], "comment": null, "summary": "The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has\nraised questions about their ability to replicate human judgments and\npreferences, especially in ambivalent scenarios where humans disagree. This\nstudy examines the biases and limitations of LLMs in three roles: answer\ngenerator, judge, and debater. These roles loosely correspond to previously\ndescribed alignment frameworks: preference alignment (judge) and scalable\noversight (debater), with the answer generator reflecting the typical setting\nwith user interactions. We develop a ``no-consensus'' benchmark by curating\nexamples that encompass a variety of a priori ambivalent scenarios, each\npresenting two possible stances. Our results show that while LLMs can provide\nnuanced assessments when generating open-ended answers, they tend to take a\nstance on no-consensus topics when employed as judges or debaters. These\nfindings underscore the necessity for more sophisticated methods for aligning\nLLMs without human oversight, highlighting that LLMs cannot fully capture human\ndisagreement even on topics where humans themselves are divided.", "AI": {"tldr": "LLMs\u5728\u4f5c\u4e3a\u4eba\u7c7b\u66ff\u4ee3\u54c1\u8fdb\u884c\u5bf9\u9f50\u65f6\uff0c\u96be\u4ee5\u5b8c\u5168\u590d\u73b0\u4eba\u7c7b\u5728\u4e89\u8bae\u6027\u8bdd\u9898\u4e0a\u7684\u5224\u65ad\u548c\u504f\u597d\uff0c\u5c24\u5176\u662f\u5728\u4f5c\u4e3a\u8bc4\u5224\u8005\u6216\u8fa9\u8bba\u8005\u65f6\u503e\u5411\u4e8e\u91c7\u53d6\u7acb\u573a\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u4e89\u8bae\u6027\u573a\u666f\u4e2d\u4f5c\u4e3a\u7b54\u6848\u751f\u6210\u8005\u3001\u8bc4\u5224\u8005\u548c\u8fa9\u8bba\u8005\u65f6\u7684\u504f\u89c1\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u8bc4\u4f30\u5176\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u201c\u65e0\u5171\u8bc6\u201d\u57fa\u51c6\uff0c\u5305\u542b\u591a\u79cd\u4e89\u8bae\u6027\u573a\u666f\uff0c\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u89d2\u8272\u4e0b\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u5f00\u653e\u7b54\u6848\u751f\u6210\u4e2d\u80fd\u63d0\u4f9b\u7ec6\u81f4\u8bc4\u4f30\uff0c\u4f46\u5728\u4f5c\u4e3a\u8bc4\u5224\u8005\u6216\u8fa9\u8bba\u8005\u65f6\u503e\u5411\u4e8e\u91c7\u53d6\u7acb\u573a\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4eba\u7c7b\u7684\u5206\u6b67\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u590d\u6742\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u56e0\u4e3aLLMs\u65e0\u6cd5\u5b8c\u5168\u590d\u73b0\u4eba\u7c7b\u5728\u4e89\u8bae\u6027\u8bdd\u9898\u4e0a\u7684\u5206\u6b67\u3002"}}
{"id": "2505.24120", "pdf": "https://arxiv.org/pdf/2505.24120", "abs": "https://arxiv.org/abs/2505.24120", "authors": ["Ai Jian", "Weijie Qiu", "Xiaokun Wang", "Peiyu Wang", "Yunzhuo Hao", "Jiangbo Pei", "Yichen Wei", "Yi Peng", "Xuchen Song"], "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs", "categories": ["cs.CV", "cs.AI"], "comment": "36 pages", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.", "AI": {"tldr": "CSVQA\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u79d1\u5b66\u8bed\u5883\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u901a\u7528\u56fe\u50cf\u7406\u89e3\u6216\u6587\u672c\u9a71\u52a8\u63a8\u7406\uff0c\u7f3a\u4e4f\u9700\u8981\u9886\u57df\u77e5\u8bc6\u4e0e\u89c6\u89c9\u8bc1\u636e\u5206\u6790\u7ed3\u5408\u7684\u79d1\u5b66\u8bed\u5883\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86CSVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,378\u4e2a\u8de8STEM\u5b66\u79d1\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8981\u6c42\u9886\u57df\u77e5\u8bc6\u3001\u89c6\u89c9\u8bc1\u636e\u6574\u5408\u548c\u9ad8\u9636\u63a8\u7406\u3002", "result": "\u5bf915\u4e2aVLM\u7684\u8bc4\u4f30\u663e\u793a\u6027\u80fd\u5dee\u8ddd\u663e\u8457\uff0c\u6700\u4f18\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a49.6%\u3002", "conclusion": "CSVQA\u63ed\u793a\u4e86VLM\u5728\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u3002"}}
{"id": "2505.23822", "pdf": "https://arxiv.org/pdf/2505.23822", "abs": "https://arxiv.org/abs/2505.23822", "authors": ["Mai Ali", "Christopher Lucasius", "Tanmay P. Patel", "Madison Aitken", "Jacob Vorstman", "Peter Szatmari", "Marco Battaglia", "Deepa Kundur"], "title": "Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction", "categories": ["cs.CL", "cs.MM"], "comment": "6 pages, 1 figure, 3 tables. Submitted to ICSM 2025. The\n  corresponding author is Mai Ali (maia.ali@mail.utoronto.ca). Christopher\n  Lucasius and Tanmay P. Patel contributed equally", "summary": "Speech is a noninvasive digital phenotype that can offer valuable insights\ninto mental health conditions, but it is often treated as a single modality. In\ncontrast, we propose the treatment of patient speech data as a trimodal\nmultimedia data source for depression detection. This study explores the\npotential of large language model-based architectures for speech-based\ndepression prediction in a multimodal regime that integrates speech-derived\ntext, acoustic landmarks, and vocal biomarkers. Adolescent depression presents\na significant challenge and is often comorbid with multiple disorders, such as\nsuicidal ideation and sleep disturbances. This presents an additional\nopportunity to integrate multi-task learning (MTL) into our study by\nsimultaneously predicting depression, suicidal ideation, and sleep disturbances\nusing the multimodal formulation. We also propose a longitudinal analysis\nstrategy that models temporal changes across multiple clinical interactions,\nallowing for a comprehensive understanding of the conditions' progression. Our\nproposed approach, featuring trimodal, longitudinal MTL is evaluated on the\nDepression Early Warning dataset. It achieves a balanced accuracy of 70.8%,\nwhich is higher than each of the unimodal, single-task, and non-longitudinal\nmethods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\uff08\u8bed\u97f3\u3001\u58f0\u5b66\u6807\u5fd7\u7269\u548c\u58f0\u5b66\u751f\u7269\u6807\u5fd7\u7269\uff09\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u7684\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u7eb5\u5411\u5206\u6790\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u6291\u90c1\u75c7\u68c0\u6d4b\u901a\u5e38\u4ec5\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u7684\u8bed\u97f3\u6570\u636e\uff0c\u800c\u591a\u6a21\u6001\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u53ef\u80fd\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5206\u6790\u3002\u9752\u5c11\u5e74\u6291\u90c1\u75c7\u5e38\u4f34\u968f\u5176\u4ed6\u75c7\u72b6\uff0c\u5982\u81ea\u6740\u610f\u5ff5\u548c\u7761\u7720\u969c\u788d\uff0c\u8fdb\u4e00\u6b65\u652f\u6301\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\uff0c\u6574\u5408\u8bed\u97f3\u6587\u672c\u3001\u58f0\u5b66\u6807\u5fd7\u7269\u548c\u58f0\u5b66\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u7eb5\u5411\u5206\u6790\u7b56\u7565\u3002", "result": "\u5728Depression Early Warning\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e8670.8%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u3001\u5355\u4efb\u52a1\u548c\u975e\u7eb5\u5411\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u7eb5\u5411\u5206\u6790\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u6291\u90c1\u75c7\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24139", "pdf": "https://arxiv.org/pdf/2505.24139", "abs": "https://arxiv.org/abs/2505.24139", "authors": ["Yichen Xie", "Runsheng Xu", "Tong He", "Jyh-Jing Hwang", "Katie Luo", "Jingwei Ji", "Hubert Lin", "Letian Chen", "Yiren Lu", "Zhaoqi Leng", "Dragomir Anguelov", "Mingxing Tan"], "title": "S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR2025", "summary": "The latest advancements in multi-modal large language models (MLLMs) have\nspurred a strong renewed interest in end-to-end motion planning approaches for\nautonomous driving. Many end-to-end approaches rely on human annotations to\nlearn intermediate perception and prediction tasks, while purely\nself-supervised approaches--which directly learn from sensor inputs to generate\nplanning trajectories without human annotations often underperform the state of\nthe art. We observe a key gap in the input representation space: end-to-end\napproaches built on MLLMs are often pretrained with reasoning tasks in 2D image\nspace rather than the native 3D space in which autonomous vehicles plan. To\nthis end, we propose S4-Driver, a scalable self-supervised motion planning\nalgorithm with spatio-temporal visual representation, based on the popular PaLI\nmultimodal large language model. S4-Driver uses a novel sparse volume strategy\nto seamlessly transform the strong visual representation of MLLMs from\nperspective view to 3D space without the need to finetune the vision encoder.\nThis representation aggregates multi-view and multi-frame visual inputs and\nenables better prediction of planning trajectories in 3D space. To validate our\nmethod, we run experiments on both nuScenes and Waymo Open Motion Dataset (with\nin-house camera data). Results show that S4-Driver performs favorably against\nexisting supervised multi-task approaches while requiring no human annotations.\nIt also demonstrates great scalability when pretrained on large volumes of\nunannotated driving logs.", "AI": {"tldr": "S4-Driver\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u76d1\u7763\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u4f53\u79ef\u7b56\u7565\u5c062D\u89c6\u89c9\u8868\u793a\u8f6c\u6362\u4e3a3D\u7a7a\u95f4\uff0c\u65e0\u9700\u5fae\u8c03\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u57282D\u56fe\u50cf\u7a7a\u95f4\u9884\u8bad\u7ec3\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u57283D\u7a7a\u95f4\u8fdb\u884c\uff0c\u5b58\u5728\u8f93\u5165\u8868\u793a\u7a7a\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faS4-Driver\u7b97\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u4f53\u79ef\u7b56\u7565\u5c06PaLI\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u4ece\u900f\u89c6\u89c6\u56fe\u65e0\u7f1d\u8f6c\u6362\u52303D\u7a7a\u95f4\uff0c\u5e76\u805a\u5408\u591a\u89c6\u89d2\u548c\u591a\u5e27\u89c6\u89c9\u8f93\u5165\u3002", "result": "\u5728nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cS4-Driver\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u591a\u4efb\u52a1\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5177\u6709\u826f\u597d\u6269\u5c55\u6027\u3002", "conclusion": "S4-Driver\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c3D\u89c6\u89c9\u8868\u793a\u4f18\u5316\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23823", "pdf": "https://arxiv.org/pdf/2505.23823", "abs": "https://arxiv.org/abs/2505.23823", "authors": ["Youngseung Jeon", "Ziwen Li", "Thomas Li", "JiaSyuan Chang", "Morteza Ziyadi", "Xiang 'Anthony' Chen"], "title": "RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery", "categories": ["cs.CL"], "comment": "17 pages, 4 figures, 8 tables", "summary": "Retrieving the biological impacts of protein-protein interactions (PPIs) is\nessential for target identification (Target ID) in drug development. Given the\nvast number of proteins involved, this process remains time-consuming and\nchallenging. Large Language Models (LLMs) and Retrieval-Augmented Generation\n(RAG) frameworks have supported Target ID; however, no benchmark currently\nexists for identifying the biological impacts of PPIs. To bridge this gap, we\nintroduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer\nbenchmark of 4,420 question-answer pairs that focus on the potential biological\nimpacts of PPIs. Through interviews with experts, we identified criteria for a\nbenchmark dataset, such as a type of QA and source. We built a gold-standard\ndataset (500 QA pairs) through expert-driven data annotation. We developed an\nensemble auto-evaluation LLM that reflected expert labeling characteristics,\nwhich facilitates the construction of a silver-standard dataset (3,720 QA\npairs). We are committed to maintaining RAGPPI as a resource to support the\nresearch community in advancing RAG systems for drug discovery QA solutions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RAGPPI\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\uff08PPIs\uff09\u7684\u751f\u7269\u5b66\u5f71\u54cd\uff0c\u5305\u542b4,420\u4e2a\u95ee\u7b54\u5bf9\uff0c\u652f\u6301\u836f\u7269\u5f00\u53d1\u4e2d\u7684\u76ee\u6807\u8bc6\u522b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30PPIs\u751f\u7269\u5b66\u5f71\u54cd\u7684\u57fa\u51c6\uff0c\u963b\u788d\u4e86\u836f\u7269\u5f00\u53d1\u4e2d\u76ee\u6807\u8bc6\u522b\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u786e\u5b9a\u57fa\u51c6\u6807\u51c6\uff0c\u6784\u5efa\u4e86500\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u91d1\u6807\u51c6\u6570\u636e\u96c6\u548c3,720\u4e2a\u81ea\u52a8\u6807\u6ce8\u7684\u94f6\u6807\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u96c6\u6210\u81ea\u52a8\u8bc4\u4f30LLM\u3002", "result": "\u63d0\u51fa\u4e86RAGPPI\u57fa\u51c6\uff0c\u5305\u542b4,420\u4e2a\u95ee\u7b54\u5bf9\uff0c\u652f\u6301\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u95ee\u7b54\u7cfb\u7edf\u7814\u7a76\u3002", "conclusion": "RAGPPI\u5c06\u6210\u4e3a\u652f\u6301\u836f\u7269\u53d1\u73b0\u4e2d\u95ee\u7b54\u7cfb\u7edf\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2505.24141", "pdf": "https://arxiv.org/pdf/2505.24141", "abs": "https://arxiv.org/abs/2505.24141", "authors": ["Jiashuai Liu", "Yingjia Shang", "Yingkang Zhan", "Di Zhang", "Yi Niu", "Dong Wei", "Xian Wu", "Zeyu Gao", "Chen Li", "Yefeng Zheng"], "title": "The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the widespread adoption of pathology foundation models in both research\nand clinical decision support systems, exploring their security has become a\ncritical concern. However, despite their growing impact, the vulnerability of\nthese models to adversarial attacks remains largely unexplored. In this work,\nwe present the first systematic investigation into the security of pathology\nfoundation models for whole slide image~(WSI) analysis against adversarial\nattacks. Specifically, we introduce the principle of \\textit{local perturbation\nwith global impact} and propose a label-free attack framework that operates\nwithout requiring access to downstream task labels. Under this attack\nframework, we revise four classical white-box attack methods and redefine the\nperturbation budget based on the characteristics of WSI. We conduct\ncomprehensive experiments on three representative pathology foundation models\nacross five datasets and six downstream tasks. Despite modifying only 0.1\\% of\npatches per slide with imperceptible noise, our attack leads to downstream\naccuracy degradation that can reach up to 20\\% in the worst cases. Furthermore,\nwe analyze key factors that influence attack success, explore the relationship\nbetween patch-level vulnerability and semantic content, and conduct a\npreliminary investigation into potential defence strategies. These findings lay\nthe groundwork for future research on the adversarial robustness and reliable\ndeployment of pathology foundation models. Our code is publicly available at:\nhttps://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u5168\u5207\u7247\u56fe\u50cf\u5206\u6790\u4e2d\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6807\u7b7e\u653b\u51fb\u6846\u67b6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u7814\u7a76\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u201c\u5c40\u90e8\u6270\u52a8\u5168\u5c40\u5f71\u54cd\u201d\u539f\u5219\uff0c\u8bbe\u8ba1\u65e0\u6807\u7b7e\u653b\u51fb\u6846\u67b6\uff0c\u5e76\u57fa\u4e8eWSI\u7279\u6027\u91cd\u65b0\u5b9a\u4e49\u6270\u52a8\u9884\u7b97\uff0c\u4fee\u8ba2\u56db\u79cd\u7ecf\u5178\u767d\u76d2\u653b\u51fb\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4fee\u65390.1%\u7684\u5207\u7247\u533a\u57df\u5373\u53ef\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe20%\uff0c\u5e76\u5206\u6790\u4e86\u653b\u51fb\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u53ca\u6f5c\u5728\u9632\u5fa1\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u53ef\u9760\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u4ee5\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.23824", "pdf": "https://arxiv.org/pdf/2505.23824", "abs": "https://arxiv.org/abs/2505.23824", "authors": ["Tianmai M. Zhang", "Neil F. Abernethy"], "title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation", "categories": ["cs.CL"], "comment": "Work in progress. Conclusions may be updated", "summary": "Recent advancements in large language models have sparked interest in\nutilizing them to assist the peer review process of scientific publication.\nInstead of having AI models generate reviews in the same way as human\nreviewers, we propose adopting them as manuscript quality checkers. We\nintroduce several baseline approaches and an extendable automatic evaluation\nframework using top LLMs as judges to tackle the difficulty of recruiting\ndomain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we\nvalidated our proposed methods with several leading reasoning LLMs from\ndifferent providers and assessed their performance and API costs for\nidentifying critical errors and unsoundness problems. The OpenAI o3 model\nperformed the best, while o4-mini was the most cost-effective one in our\nevaluation. This paper provides insights into document-based scientific\nunderstanding/reasoning and lays the foundation for future applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u79d1\u5b66\u8bba\u6587\u8d28\u91cf\u68c0\u67e5\u5de5\u5177\uff0c\u800c\u975e\u76f4\u63a5\u751f\u6210\u8bc4\u5ba1\u610f\u89c1\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u51fa\u7248\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u62db\u52df\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u4eba\u5de5\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u7ebf\u65b9\u6cd5\u53ca\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528arXiv\u64a4\u7a3f\u8bba\u6587\u9a8c\u8bc1\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "OpenAI\u7684o3\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0co4-mini\u6700\u5177\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u4e3a\u57fa\u4e8e\u6587\u6863\u7684\u79d1\u5b66\u7406\u89e3/\u63a8\u7406\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.24156", "pdf": "https://arxiv.org/pdf/2505.24156", "abs": "https://arxiv.org/abs/2505.24156", "authors": ["Chenyou Fan", "Fangzheng Yan", "Chenjia Bai", "Jiepeng Wang", "Chi Zhang", "Zhen Wang", "Xuelong Li"], "title": "Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning a generalizable bimanual manipulation policy is extremely\nchallenging for embodied agents due to the large action space and the need for\ncoordinated arm movements. Existing approaches rely on Vision-Language-Action\n(VLA) models to acquire bimanual policies. However, transferring knowledge from\nsingle-arm datasets or pre-trained VLA models often fails to generalize\neffectively, primarily due to the scarcity of bimanual data and the fundamental\ndifferences between single-arm and bimanual manipulation. In this paper, we\npropose a novel bimanual foundation policy by fine-tuning the leading\ntext-to-video models to predict robot trajectories and training a lightweight\ndiffusion policy for action generation. Given the lack of embodied knowledge in\ntext-to-video models, we introduce a two-stage paradigm that fine-tunes\nindependent text-to-flow and flow-to-video models derived from a pre-trained\ntext-to-video model. Specifically, optical flow serves as an intermediate\nvariable, providing a concise representation of subtle movements between\nimages. The text-to-flow model predicts optical flow to concretize the intent\nof language instructions, and the flow-to-video model leverages this flow for\nfine-grained video prediction. Our method mitigates the ambiguity of language\nin single-stage text-to-video prediction and significantly reduces the\nrobot-data requirement by avoiding direct use of low-level actions. In\nexperiments, we collect high-quality manipulation data for real dual-arm robot,\nand the results of simulation and real-world experiments demonstrate the\neffectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u53cc\u81c2\u64cd\u4f5c\u7b56\u7565\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u6587\u672c\u5230\u5149\u6d41\u548c\u5149\u6d41\u5230\u89c6\u9891\uff09\u51cf\u5c11\u8bed\u8a00\u6a21\u7cca\u6027\u5e76\u964d\u4f4e\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u81c2\u6570\u636e\u6216\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u53cc\u81c2\u64cd\u4f5c\uff0c\u4e3b\u8981\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u5355\u53cc\u81c2\u64cd\u4f5c\u5dee\u5f02\u3002", "method": "\u5fae\u8c03\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u751f\u6210\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u8bad\u7ec3\u8f7b\u91cf\u6269\u6563\u7b56\u7565\u751f\u6210\u52a8\u4f5c\uff1b\u5f15\u5165\u6587\u672c\u5230\u5149\u6d41\u548c\u5149\u6d41\u5230\u89c6\u9891\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u5149\u6d41\u4f5c\u4e3a\u4e2d\u95f4\u53d8\u91cf\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u4e24\u9636\u6bb5\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u53cc\u81c2\u64cd\u4f5c\u7684\u8bed\u8a00\u6a21\u7cca\u6027\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2505.23827", "pdf": "https://arxiv.org/pdf/2505.23827", "abs": "https://arxiv.org/abs/2505.23827", "authors": ["Bangde Du", "Ziyi Ye", "Zhijing Wu", "Jankowska Monika", "Shuqi Zhu", "Qingyao Ai", "Yujia Zhou", "Yiqun Liu"], "title": "ValueSim: Generating Backstories to Model Individual Value Systems", "categories": ["cs.CL"], "comment": "8 pages main paper + 13 pages appendix, 3 figures, 2 tables", "summary": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time.", "AI": {"tldr": "ValueSim\u6846\u67b6\u901a\u8fc7\u751f\u6210\u4e2a\u4eba\u80cc\u666f\u6545\u4e8b\u6a21\u62df\u4e2a\u4f53\u4ef7\u503c\u89c2\uff0c\u76f8\u6bd4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u5728Top-1\u51c6\u786e\u7387\u4e0a\u63d0\u534710%\u4ee5\u4e0a\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u4ef7\u503c\u7cfb\u7edf\u7684\u6a21\u62df\uff0cValueSim\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "ValueSim\u5c06\u7ed3\u6784\u5316\u4e2a\u4eba\u6570\u636e\u8f6c\u5316\u4e3a\u53d9\u4e8b\u80cc\u666f\u6545\u4e8b\uff0c\u5e76\u91c7\u7528\u591a\u6a21\u5757\u67b6\u6784\u6a21\u62df\u4e2a\u4f53\u4ef7\u503c\u89c2\u3002", "result": "\u5728\u81ea\u5efa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cValueSim\u7684Top-1\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc710%\uff0c\u4e14\u6027\u80fd\u968f\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "ValueSim\u80fd\u591f\u6709\u6548\u6a21\u62df\u4e2a\u4f53\u4ef7\u503c\u89c2\uff0c\u5e76\u968f\u65f6\u95f4\u4f18\u5316\u5176\u6a21\u62df\u80fd\u529b\u3002"}}
{"id": "2505.24158", "pdf": "https://arxiv.org/pdf/2505.24158", "abs": "https://arxiv.org/abs/2505.24158", "authors": ["Bo Fang", "Wenhao Wu", "Qiangqiang Wu", "Yuxin Song", "Antoni B. Chan"], "title": "Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders", "categories": ["cs.CV"], "comment": null, "summary": "Employing Multimodal Large Language Models (MLLMs) for long video\nunderstanding remains a challenging problem due to the dilemma between the\nsubstantial number of video frames (i.e., visual tokens) versus the limited\ncontext length of language models. Traditional uniform sampling often leads to\nselection of irrelevant content, while post-training MLLMs on thousands of\nframes imposes a substantial computational burden. In this paper, we propose\nthreading keyframes with narratives (Nar-KFC), a plug-and-play module to\nfacilitate effective and efficient long video perception. Nar-KFC generally\ninvolves two collaborative steps. First, we formulate the keyframe selection\nprocess as an integer quadratic programming problem, jointly optimizing\nquery-relevance and frame-diversity. To avoid its computational complexity, a\ncustomized greedy search strategy is designed as an efficient alternative.\nSecond, to mitigate the temporal discontinuity caused by sparse keyframe\nsampling, we further introduce interleaved textual narratives generated from\nnon-keyframes using off-the-shelf captioners. These narratives are inserted\nbetween keyframes based on their true temporal order, forming a coherent and\ncompact representation. Nar-KFC thus serves as a temporal- and content-aware\ncompression strategy that complements visual and textual modalities.\nExperimental results on multiple long-video benchmarks demonstrate that Nar-KFC\nsignificantly improves the performance of popular MLLMs. Code will be made\npublicly available.", "AI": {"tldr": "Nar-KFC\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u6a21\u5757\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u9009\u62e9\u548c\u6587\u672c\u53d9\u4e8b\u63d2\u5165\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u4ee4\u724c\u8fc7\u591a\u4e0e\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u957f\u5ea6\u6709\u9650\u7684\u77db\u76fe\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u4e2d\uff0c\u89c6\u89c9\u4ee4\u724c\u8fc7\u591a\u4e0e\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u957f\u5ea6\u6709\u9650\u7684\u77db\u76fe\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u6216\u5185\u5bb9\u4e0d\u76f8\u5173\u3002", "method": "\u63d0\u51faNar-KFC\u6a21\u5757\uff0c\u5305\u62ec\u5173\u952e\u5e27\u9009\u62e9\u548c\u6587\u672c\u53d9\u4e8b\u63d2\u5165\u4e24\u6b65\uff0c\u524d\u8005\u901a\u8fc7\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u4f18\u5316\uff0c\u540e\u8005\u5229\u7528\u73b0\u6210\u5b57\u5e55\u751f\u6210\u5668\u8865\u5145\u975e\u5173\u952e\u5e27\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNar-KFC\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "Nar-KFC\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5185\u5bb9\u611f\u77e5\u7684\u957f\u89c6\u9891\u538b\u7f29\u7b56\u7565\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u3002"}}
{"id": "2505.23829", "pdf": "https://arxiv.org/pdf/2505.23829", "abs": "https://arxiv.org/abs/2505.23829", "authors": ["Xiaoqing Cheng", "Ruizhe Chen", "Hongying Zan", "Yuxiang Jia", "Min Peng"], "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Mitigating social bias in large language models (LLMs) has become an\nincreasingly important research objective. However, existing debiasing methods\noften incur high human and computational costs, exhibit limited effectiveness,\nand struggle to scale to larger models and open-ended generation tasks. To\naddress these limitations, this paper proposes BiasFilter, a model-agnostic,\ninference-time debiasing framework that integrates seamlessly with both\nopen-source and API-based LLMs. Instead of relying on retraining with balanced\ndata or modifying model parameters, BiasFilter enforces fairness by filtering\ngeneration outputs in real time. Specifically, it periodically evaluates\nintermediate outputs every few tokens, maintains an active set of candidate\ncontinuations, and incrementally completes generation by discarding low-reward\nsegments based on a fairness reward signal. To support this process, we\nconstruct a fairness preference dataset and train an implicit reward model to\nassess token-level fairness in generated responses. Extensive experiments\ndemonstrate that BiasFilter effectively mitigates social bias across a range of\nLLMs while preserving overall generation quality.", "AI": {"tldr": "BiasFilter\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u63a8\u7406\u65f6\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u8fc7\u6ee4\u751f\u6210\u8f93\u51fa\u6765\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6548\u679c\u6709\u9650\u4e14\u96be\u4ee5\u6269\u5c55\uff0cBiasFilter\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BiasFilter\u901a\u8fc7\u5468\u671f\u6027\u8bc4\u4f30\u4e2d\u95f4\u8f93\u51fa\u3001\u7ef4\u62a4\u5019\u9009\u7eed\u96c6\uff0c\u5e76\u57fa\u4e8e\u516c\u5e73\u6027\u5956\u52b1\u4fe1\u53f7\u4e22\u5f03\u4f4e\u5956\u52b1\u6bb5\u6765\u5b8c\u6210\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBiasFilter\u80fd\u6709\u6548\u51cf\u5c11\u793e\u4f1a\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "BiasFilter\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u53bb\u504f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24162", "pdf": "https://arxiv.org/pdf/2505.24162", "abs": "https://arxiv.org/abs/2505.24162", "authors": ["Isaac Aguirre", "Ivan Sipiran"], "title": "Training-free zero-shot 3D symmetry detection with visual features back-projected to geometry", "categories": ["cs.CV"], "comment": null, "summary": "We present a simple yet effective training-free approach for zero-shot 3D\nsymmetry detection that leverages visual features from foundation vision models\nsuch as DINOv2. Our method extracts features from rendered views of 3D objects\nand backprojects them onto the original geometry. We demonstrate the symmetric\ninvariance of these features and use them to identify reflection-symmetry\nplanes through a proposed algorithm. Experiments on a subset of ShapeNet\ndemonstrate that our approach outperforms both traditional geometric methods\nand learning-based approaches without requiring any training data. Our work\ndemonstrates how foundation vision models can help in solving complex 3D\ngeometric problems such as symmetry detection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c3D\u5bf9\u79f0\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528DINOv2\u7b49\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u901a\u8fc7\u6e32\u67d3\u89c6\u56fe\u63d0\u53d6\u7279\u5f81\u5e76\u53cd\u5411\u6295\u5f71\u5230\u539f\u59cb\u51e0\u4f55\u4f53\u4e0a\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u5728\u89e3\u51b3\u590d\u67423D\u51e0\u4f55\u95ee\u9898\uff08\u5982\u5bf9\u79f0\u6027\u68c0\u6d4b\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u4ece3D\u7269\u4f53\u7684\u6e32\u67d3\u89c6\u56fe\u4e2d\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u53cd\u5411\u6295\u5f71\u5230\u51e0\u4f55\u4f53\u4e0a\uff0c\u5229\u7528\u7279\u5f81\u7684\u5bf9\u79f0\u4e0d\u53d8\u6027\u8bbe\u8ba1\u7b97\u6cd5\u68c0\u6d4b\u53cd\u5c04\u5bf9\u79f0\u5e73\u9762\u3002", "result": "\u5728ShapeNet\u5b50\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u53ef\u7528\u4e8e\u89e3\u51b3\u590d\u67423D\u51e0\u4f55\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5bf9\u79f0\u6027\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.23830", "pdf": "https://arxiv.org/pdf/2505.23830", "abs": "https://arxiv.org/abs/2505.23830", "authors": ["Linglin Jing", "Yuting Gao", "Zhigang Wang", "Wang Lan", "Yiwen Tang", "Wenhai Wang", "Kaipeng Zhang", "Qingpei Guo"], "title": "EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements have shown that the Mixture of Experts (MoE) approach\nsignificantly enhances the capacity of large language models (LLMs) and\nimproves performance on downstream tasks. Building on these promising results,\nmulti-modal large language models (MLLMs) have increasingly adopted MoE\ntechniques. However, existing multi-modal MoE tuning methods typically face two\nkey challenges: expert uniformity and router rigidity. Expert uniformity occurs\nbecause MoE experts are often initialized by simply replicating the FFN\nparameters from LLMs, leading to homogenized expert functions and weakening the\nintended diversification of the MoE architecture. Meanwhile, router rigidity\nstems from the prevalent use of static linear routers for expert selection,\nwhich fail to distinguish between visual and textual tokens, resulting in\nsimilar expert distributions for image and text. To address these limitations,\nwe propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a\nmeticulously designed expert initialization strategy that progressively evolves\nmultiple robust experts from a single trainable expert, a process termed expert\nevolution that specifically targets severe expert homogenization. Furthermore,\nwe introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism\nthat allocates input tokens to appropriate experts based on their modality and\nintrinsic token values. This dynamic routing is facilitated by hypernetworks,\nwhich dynamically generate routing weights tailored for each individual token.\nExtensive experiments demonstrate that EvoMoE significantly outperforms other\nsparse MLLMs across a variety of multi-modal benchmarks, including MME,\nMMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE\nin enhancing the performance of MLLMs by addressing the critical issues of\nexpert uniformity and router rigidity.", "AI": {"tldr": "EvoMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684MoE\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u8fdb\u5316\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\u89e3\u51b3\u4e86\u591a\u6a21\u6001MoE\u4e2d\u7684\u4e13\u5bb6\u540c\u8d28\u5316\u548c\u8def\u7531\u5668\u50f5\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLM\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001MoE\u65b9\u6cd5\u9762\u4e34\u4e13\u5bb6\u540c\u8d28\u5316\u548c\u8def\u7531\u5668\u50f5\u5316\u4e24\u5927\u6311\u6218\uff0c\u9650\u5236\u4e86MoE\u67b6\u6784\u7684\u591a\u6837\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "method": "EvoMoE\u91c7\u7528\u4e13\u5bb6\u8fdb\u5316\u7b56\u7565\u4ece\u5355\u4e2a\u53ef\u8bad\u7ec3\u4e13\u5bb6\u9010\u6b65\u6f14\u5316\u51fa\u591a\u4e2a\u4e13\u5bb6\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u4ee4\u724c\u611f\u77e5\u8def\u7531\u5668\uff08DTR\uff09\u6839\u636e\u6a21\u6001\u548c\u4ee4\u724c\u7279\u6027\u52a8\u6001\u5206\u914d\u4e13\u5bb6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEvoMoE\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff08MME\u3001MMBench\u7b49\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7a00\u758fMLLM\u3002", "conclusion": "EvoMoE\u901a\u8fc7\u89e3\u51b3\u4e13\u5bb6\u540c\u8d28\u5316\u548c\u8def\u7531\u5668\u50f5\u5316\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86MLLM\u7684\u6027\u80fd\u3002"}}
{"id": "2505.24167", "pdf": "https://arxiv.org/pdf/2505.24167", "abs": "https://arxiv.org/abs/2505.24167", "authors": ["Junyu Chen", "Shuwen Wei", "Yihao Liu", "Aaron Carass", "Yong Du"], "title": "Pretraining Deformable Image Registration Networks with Random Images", "categories": ["cs.CV"], "comment": "Accepted by MIDL 2025. Code available at\n  https://github.com/junyuchen245/Pretraining_Image_Registration_DNNs", "summary": "Recent advances in deep learning-based medical image registration have shown\nthat training deep neural networks~(DNNs) does not necessarily require medical\nimages. Previous work showed that DNNs trained on randomly generated images\nwith carefully designed noise and contrast properties can still generalize well\nto unseen medical data. Building on this insight, we propose using registration\nbetween random images as a proxy task for pretraining a foundation model for\nimage registration. Empirical results show that our pretraining strategy\nimproves registration accuracy, reduces the amount of domain-specific data\nneeded to achieve competitive performance, and accelerates convergence during\ndownstream training, thereby enhancing computational efficiency.", "AI": {"tldr": "\u901a\u8fc7\u968f\u673a\u56fe\u50cf\u914d\u51c6\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u51c6\u786e\u6027\u3001\u51cf\u5c11\u9886\u57df\u6570\u636e\u9700\u6c42\u5e76\u52a0\u901f\u4e0b\u6e38\u8bad\u7ec3\u3002", "motivation": "\u63a2\u7d22\u65e0\u9700\u533b\u5b66\u56fe\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u968f\u673a\u56fe\u50cf\u914d\u51c6\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u914d\u51c6\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u6784\u5efa\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5728\u4e0b\u6e38\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u4e86\u914d\u51c6\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u5bf9\u9886\u57df\u7279\u5b9a\u6570\u636e\u7684\u9700\u6c42\uff0c\u5e76\u52a0\u901f\u4e86\u4e0b\u6e38\u8bad\u7ec3\u7684\u6536\u655b\u3002", "conclusion": "\u968f\u673a\u56fe\u50cf\u914d\u51c6\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.23831", "pdf": "https://arxiv.org/pdf/2505.23831", "abs": "https://arxiv.org/abs/2505.23831", "authors": ["Wenhao Ye", "Tiansheng Zheng", "Yue Qi", "Wenhua Zhao", "Xiyu Wang", "Xue Zhao", "Jiacheng He", "Yaya Zheng", "Dongbo Wang"], "title": "ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "The intangible cultural heritage (ICH) of China, a cultural asset transmitted\nacross generations by various ethnic groups, serves as a significant testament\nto the evolution of human civilization and holds irreplaceable value for the\npreservation of historical lineage and the enhancement of cultural\nself-confidence. However, the rapid pace of modernization poses formidable\nchallenges to ICH, including threats damage, disappearance and discontinuity of\ninheritance. China has the highest number of items on the UNESCO Intangible\nCultural Heritage List, which is indicative of the nation's abundant cultural\nresources and emphasises the pressing need for ICH preservation. In recent\nyears, the rapid advancements in large language modelling have provided a novel\ntechnological approach for the preservation and dissemination of ICH. This\nstudy utilises a substantial corpus of open-source Chinese ICH data to develop\na large language model, ICH-Qwen, for the ICH domain. The model employs natural\nlanguage understanding and knowledge reasoning capabilities of large language\nmodels, augmented with synthetic data and fine-tuning techniques. The\nexperimental results demonstrate the efficacy of ICH-Qwen in executing tasks\nspecific to the ICH domain. It is anticipated that the model will provide\nintelligent solutions for the protection, inheritance and dissemination of\nintangible cultural heritage, as well as new theoretical and practical\nreferences for the sustainable development of intangible cultural heritage.\nFurthermore, it is expected that the study will open up new paths for digital\nhumanities research.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u5f00\u53d1\u4e86ICH-Qwen\u6a21\u578b\uff0c\u7528\u4e8e\u4fdd\u62a4\u548c\u4f20\u64ad\u4e2d\u56fd\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\uff0c\u5c55\u793a\u4e86\u5176\u5728ICH\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u4ee3\u5316\u8fdb\u7a0b\u5bf9\u4e2d\u56fd\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\uff08ICH\uff09\u6784\u6210\u5a01\u80c1\uff0c\u4e9f\u9700\u65b0\u6280\u672f\u624b\u6bb5\u8fdb\u884c\u4fdd\u62a4\u548c\u4f20\u627f\u3002", "method": "\u5229\u7528\u5f00\u6e90\u4e2d\u6587ICH\u6570\u636e\u5f00\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578bICH-Qwen\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u4e0e\u5fae\u8c03\u6280\u672f\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eICH-Qwen\u5728ICH\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "ICH-Qwen\u4e3aICH\u7684\u4fdd\u62a4\u4e0e\u4f20\u64ad\u63d0\u4f9b\u4e86\u667a\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u6570\u5b57\u4eba\u6587\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2505.24173", "pdf": "https://arxiv.org/pdf/2505.24173", "abs": "https://arxiv.org/abs/2505.24173", "authors": ["Tianhong Zhou", "Yin Xu", "Yingtao Zhu", "Chuxi Xiao", "Haiyang Bian", "Lei Wei", "Xuegong Zhang"], "title": "DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) exhibit strong zero-shot generalization on\nnatural images and show early promise in interpretable medical image analysis.\nHowever, existing benchmarks do not systematically evaluate whether these\nmodels truly reason like human clinicians or merely imitate superficial\npatterns. To address this gap, we propose DrVD-Bench, the first multimodal\nbenchmark for clinical visual reasoning. DrVD-Bench consists of three modules:\nVisual Evidence Comprehension, Reasoning Trajectory Assessment, and Report\nGeneration Evaluation, comprising a total of 7,789 image-question pairs. Our\nbenchmark covers 20 task types, 17 diagnostic categories, and five imaging\nmodalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is\nexplicitly structured to reflect the clinical reasoning workflow from modality\nrecognition to lesion identification and diagnosis. We benchmark 19 VLMs,\nincluding general-purpose and medical-specific, open-source and proprietary\nmodels, and observe that performance drops sharply as reasoning complexity\nincreases. While some models begin to exhibit traces of human-like reasoning,\nthey often still rely on shortcut correlations rather than grounded visual\nunderstanding. DrVD-Bench offers a rigorous and structured evaluation framework\nto guide the development of clinically trustworthy VLMs.", "AI": {"tldr": "DrVD-Bench\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u4e34\u5e8a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u662f\u5426\u771f\u6b63\u6a21\u62df\u4eba\u7c7b\u4e34\u5e8a\u533b\u751f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u800c\u975e\u4ec5\u6a21\u4eff\u8868\u9762\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u7cfb\u7edf\u8bc4\u4f30VLMs\u662f\u5426\u5177\u5907\u4eba\u7c7b\u4e34\u5e8a\u533b\u751f\u7684\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981DrVD-Bench\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DrVD-Bench\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u89c6\u89c9\u8bc1\u636e\u7406\u89e3\u3001\u63a8\u7406\u8f68\u8ff9\u8bc4\u4f30\u548c\u62a5\u544a\u751f\u6210\u8bc4\u4f30\uff0c\u51717,789\u4e2a\u56fe\u50cf-\u95ee\u9898\u5bf9\uff0c\u8986\u76d620\u79cd\u4efb\u52a1\u7c7b\u578b\u300117\u79cd\u8bca\u65ad\u7c7b\u522b\u548c5\u79cd\u6210\u50cf\u6a21\u6001\u3002", "result": "\u6d4b\u8bd519\u79cdVLMs\u540e\u53d1\u73b0\uff0c\u968f\u7740\u63a8\u7406\u590d\u6742\u6027\u589e\u52a0\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u90e8\u5206\u6a21\u578b\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u63a8\u7406\u7684\u8ff9\u8c61\uff0c\u4f46\u4ecd\u4f9d\u8d56\u8868\u9762\u76f8\u5173\u6027\u3002", "conclusion": "DrVD-Bench\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8aVLMs\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.23832", "pdf": "https://arxiv.org/pdf/2505.23832", "abs": "https://arxiv.org/abs/2505.23832", "authors": ["Chaeeun Kim", "Jinu Lee", "Wonseok Hwang"], "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation", "categories": ["cs.CL", "cs.IR"], "comment": "Under review", "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LEGAR BENCH\uff08\u9996\u4e2a\u5927\u89c4\u6a21\u97e9\u8bed\u6cd5\u4f8b\u68c0\u7d22\u57fa\u51c6\uff09\u548cLegalSearchLM\uff08\u4e00\u79cd\u57fa\u4e8e\u6cd5\u5f8b\u5143\u7d20\u63a8\u7406\u7684\u68c0\u7d22\u6a21\u578b\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u7814\u7a76\u4e2d\u6570\u636e\u89c4\u6a21\u5c0f\u548c\u68c0\u7d22\u65b9\u6cd5\u5c40\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u7814\u7a76\u5b58\u5728\u6570\u636e\u89c4\u6a21\u5c0f\u3001\u72af\u7f6a\u7c7b\u578b\u5355\u4e00\u4ee5\u53ca\u68c0\u7d22\u65b9\u6cd5\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u6cd5\u5f8b\u68c0\u7d22\u573a\u666f\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86LEGAR BENCH\uff08\u8986\u76d61.2M\u6848\u4f8b\u548c411\u79cd\u72af\u7f6a\u7c7b\u578b\uff09\u548cLegalSearchLM\uff08\u901a\u8fc7\u6cd5\u5f8b\u5143\u7d20\u63a8\u7406\u548c\u7ea6\u675f\u89e3\u7801\u751f\u6210\u76f8\u5173\u5185\u5bb9\uff09\u3002", "result": "LegalSearchLM\u5728LEGAR BENCH\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53476-20%\uff0c\u5e76\u5728\u57df\u5916\u6848\u4f8b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LEGAR BENCH\u548cLegalSearchLM\u4e3a\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u57fa\u51c6\u548c\u66f4\u9ad8\u6548\u7684\u68c0\u7d22\u65b9\u6cd5\u3002"}}
{"id": "2505.24182", "pdf": "https://arxiv.org/pdf/2505.24182", "abs": "https://arxiv.org/abs/2505.24182", "authors": ["Zhuobai Dong", "Junchao Yi", "Ziyuan Zheng", "Haochen Han", "Xiangxi Zheng", "Alex Jinpeng Wang", "Fangming Liu", "Linjie Li"], "title": "Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the physical world - governed by laws of motion, spatial\nrelations, and causality - poses a fundamental challenge for multimodal large\nlanguage models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o\ndemonstrate impressive perceptual and reasoning capabilities, our investigation\nreveals these models struggle profoundly with visual physical reasoning,\nfailing to grasp basic physical laws, spatial interactions, and causal effects\nin complex scenes. More importantly, they often fail to follow coherent\nreasoning chains grounded in visual evidence, especially when multiple steps\nare needed to arrive at the correct answer. To rigorously evaluate this\ncapability, we introduce MVPBench, a curated benchmark designed to rigorously\nevaluate visual physical reasoning through the lens of visual chain-of-thought\n(CoT). Each example features interleaved multi-image inputs and demands not\nonly the correct final answer but also a coherent, step-by-step reasoning path\ngrounded in evolving visual cues. This setup mirrors how humans reason through\nreal-world physical processes over time. To ensure fine-grained evaluation, we\nintroduce a graph-based CoT consistency metric that verifies whether the\nreasoning path of model adheres to valid physical logic. Additionally, we\nminimize shortcut exploitation from text priors, encouraging models to rely on\nvisual understanding. Experimental results reveal a concerning trend: even\ncutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text\nalignment in physical domains. Surprisingly, RL-based post-training alignment -\ncommonly believed to improve visual reasoning performance - often harms spatial\nreasoning, suggesting a need to rethink current fine-tuning practices.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7269\u7406\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86MVPBench\u57fa\u51c6\u548c\u57fa\u4e8e\u56fe\u7684CoT\u4e00\u81f4\u6027\u6307\u6807\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u7406\u89e3\u7269\u7406\u4e16\u754c\uff08\u5982\u8fd0\u52a8\u89c4\u5f8b\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u56e0\u679c\u6548\u5e94\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51faMVPBench\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u56fe\u50cf\u8f93\u5165\u548c\u89c6\u89c9\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u56fe\u5f0fCoT\u4e00\u81f4\u6027\u6307\u6807\u9a8c\u8bc1\u63a8\u7406\u8def\u5f84\u7684\u7269\u7406\u903b\u8f91\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u524d\u6cbfMLLMs\u5728\u89c6\u89c9\u7269\u7406\u63a8\u7406\u548c\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u4e14RL\u5fae\u8c03\u53ef\u80fd\u635f\u5bb3\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u9700\u91cd\u65b0\u601d\u8003\u5f53\u524d\u5fae\u8c03\u65b9\u6cd5\uff0c\u5f3a\u8c03\u89c6\u89c9\u7406\u89e3\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51faMVPBench\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002"}}
{"id": "2505.23833", "pdf": "https://arxiv.org/pdf/2505.23833", "abs": "https://arxiv.org/abs/2505.23833", "authors": ["Qingchuan Ma", "Yuhang Wu", "Xiawu Zheng", "Rongrong Ji"], "title": "Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we aim to establish a simple, effective, and theoretically\ngrounded benchmark for rigorously probing abstract reasoning in Large Language\nModels (LLMs). To achieve this, we first develop a mathematic framework that\ndefines abstract reasoning as the ability to: (i) extract essential patterns\nindependent of surface representations, and (ii) apply consistent rules to\nthese abstract patterns. Based on this framework, we introduce two novel\ncomplementary metrics: \\(\\scoreGamma\\) measures basic reasoning accuracy, while\n\\(\\scoreDelta\\) quantifies a model's reliance on specific symbols rather than\nunderlying patterns - a key indicator of true abstraction versus mere\nmemorization. To implement this measurement, we design a benchmark: systematic\nsymbol remapping in rule-based tasks, which forces models to demonstrate\ngenuine pattern recognition beyond superficial token matching. Extensive LLM\nevaluations using this benchmark (commercial API models, 7B-70B, multi-agent)\nreveal:1) critical limitations in non-decimal arithmetic and symbolic\nreasoning; 2) persistent abstraction gaps despite chain-of-thought prompting;\nand 3) \\(\\scoreDelta\\)'s effectiveness in robustly measuring memory dependence\nby quantifying performance degradation under symbol remapping, particularly\nhighlighting operand-specific memorization. These findings underscore that\ncurrent LLMs, despite domain-specific strengths, still lack robust abstract\nreasoning, highlighting key areas for future improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u65e8\u5728\u5efa\u7acb\u4e00\u79cd\u7406\u8bba\u57fa\u7840\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u4e25\u683c\u8bc4\u4f30LLMs\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u533a\u5206\u771f\u5b9e\u62bd\u8c61\u4e0e\u8bb0\u5fc6\u4f9d\u8d56\u3002", "method": "\u5f00\u53d1\u4e86\u6570\u5b66\u6846\u67b6\u5b9a\u4e49\u62bd\u8c61\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u65b0\u6307\u6807\uff08\u03b3\u548c\u03b4\u5206\u6570\uff09\uff0c\u8bbe\u8ba1\u4e86\u7b26\u53f7\u91cd\u6620\u5c04\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u663e\u793aLLMs\u5728\u975e\u5341\u8fdb\u5236\u7b97\u672f\u548c\u7b26\u53f7\u63a8\u7406\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4e14\u5373\u4f7f\u4f7f\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u4ecd\u5b58\u5728\u62bd\u8c61\u5dee\u8ddd\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u03b4\u5206\u6570\u80fd\u6709\u6548\u8861\u91cf\u8bb0\u5fc6\u4f9d\u8d56\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.24207", "pdf": "https://arxiv.org/pdf/2505.24207", "abs": "https://arxiv.org/abs/2505.24207", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "title": "Boosting All-in-One Image Restoration via Self-Improved Privilege Learning", "categories": ["cs.CV"], "comment": null, "summary": "Unified image restoration models for diverse and mixed degradations often\nsuffer from unstable optimization dynamics and inter-task conflicts. This paper\nintroduces Self-Improved Privilege Learning (SIPL), a novel paradigm that\novercomes these limitations by innovatively extending the utility of privileged\ninformation (PI) beyond training into the inference stage. Unlike conventional\nPrivilege Learning, where ground-truth-derived guidance is typically discarded\nafter training, SIPL empowers the model to leverage its own preliminary outputs\nas pseudo-privileged signals for iterative self-refinement at test time.\nCentral to SIPL is Proxy Fusion, a lightweight module incorporating a learnable\nPrivileged Dictionary. During training, this dictionary distills essential\nhigh-frequency and structural priors from privileged feature representations.\nCritically, at inference, the same learned dictionary then interacts with\nfeatures derived from the model's initial restoration, facilitating a\nself-correction loop. SIPL can be seamlessly integrated into various backbone\narchitectures, offering substantial performance improvements with minimal\ncomputational overhead. Extensive experiments demonstrate that SIPL\nsignificantly advances the state-of-the-art on diverse all-in-one image\nrestoration benchmarks. For instance, when integrated with the PromptIR model,\nSIPL achieves remarkable PSNR improvements of +4.58 dB on composite degradation\ntasks and +1.28 dB on diverse five-task benchmarks, underscoring its\neffectiveness and broad applicability. Codes are available at our project page\nhttps://github.com/Aitical/SIPL.", "AI": {"tldr": "SIPL\u901a\u8fc7\u5229\u7528\u81ea\u8eab\u521d\u6b65\u8f93\u51fa\u4f5c\u4e3a\u4f2a\u7279\u6743\u4fe1\u53f7\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u5b9e\u73b0\u8fed\u4ee3\u81ea\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7edf\u4e00\u56fe\u50cf\u6062\u590d\u6a21\u578b\u5728\u591a\u6837\u5316\u548c\u6df7\u5408\u9000\u5316\u4efb\u52a1\u4e2d\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u4efb\u52a1\u95f4\u51b2\u7a81\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSIPL\u8303\u5f0f\uff0c\u7ed3\u5408Proxy Fusion\u6a21\u5757\u548c\u7279\u6743\u5b57\u5178\uff0c\u8bad\u7ec3\u65f6\u63d0\u53d6\u9ad8\u9891\u548c\u7ed3\u6784\u5148\u9a8c\uff0c\u63a8\u7406\u65f6\u5229\u7528\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u81ea\u6821\u6b63\u3002", "result": "\u5728PromptIR\u6a21\u578b\u4e0a\uff0cSIPL\u5728\u590d\u5408\u9000\u5316\u4efb\u52a1\u4e2dPSNR\u63d0\u53474.58 dB\uff0c\u4e94\u4efb\u52a1\u57fa\u51c6\u4e2d\u63d0\u53471.28 dB\u3002", "conclusion": "SIPL\u901a\u8fc7\u81ea\u6539\u8fdb\u7279\u6743\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u6062\u590d\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.23835", "pdf": "https://arxiv.org/pdf/2505.23835", "abs": "https://arxiv.org/abs/2505.23835", "authors": ["Ye Cheng", "Minghui Xu", "Yue Zhang", "Kun Li", "Hao Wu", "Yechao Zhang", "Shaoyong Guo", "Wangjie Qiu", "Dongxiao Yu", "Xiuzhen Cheng"], "title": "Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things", "categories": ["cs.CL"], "comment": null, "summary": "Access control in the Internet of Things (IoT) is becoming increasingly\ncomplex, as policies must account for dynamic and contextual factors such as\ntime, location, user behavior, and environmental conditions. However, existing\nplatforms either offer only coarse-grained controls or rely on rigid rule\nmatching, making them ill-suited for semantically rich or ambiguous access\nscenarios. Moreover, the policy authoring process remains fragmented: domain\nexperts describe requirements in natural language, but developers must manually\ntranslate them into code, introducing semantic gaps and potential\nmisconfiguration. In this work, we present LACE, the Language-based Access\nControl Engine, a hybrid framework that leverages large language models (LLMs)\nto bridge the gap between human intent and machine-enforceable logic. LACE\ncombines prompt-guided policy generation, retrieval-augmented reasoning, and\nformal validation to support expressive, interpretable, and verifiable access\ncontrol. It enables users to specify policies in natural language,\nautomatically translates them into structured rules, validates semantic\ncorrectness, and makes access decisions using a hybrid LLM-rule-based engine.\nWe evaluate LACE in smart home environments through extensive experiments. LACE\nachieves 100% correctness in verified policy generation and up to 88% decision\naccuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as\nGPT-3.5 and Gemini. The system also demonstrates strong scalability under\nincreasing policy volume and request concurrency. Our results highlight LACE's\npotential to enable secure, flexible, and user-friendly access control across\nreal-world IoT platforms.", "AI": {"tldr": "LACE\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bbf\u95ee\u63a7\u5236\u5f15\u64ce\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u548c\u9a8c\u8bc1\u7b56\u7565\uff0c\u89e3\u51b3\u4e86IoT\u4e2d\u52a8\u6001\u8bbf\u95ee\u63a7\u5236\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "IoT\u8bbf\u95ee\u63a7\u5236\u9700\u8003\u8651\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u56e0\u7d20\uff0c\u73b0\u6709\u5e73\u53f0\u65e0\u6cd5\u6ee1\u8db3\u8bed\u4e49\u4e30\u5bcc\u6216\u6a21\u7cca\u573a\u666f\u7684\u9700\u6c42\uff0c\u4e14\u7b56\u7565\u7f16\u5199\u8fc7\u7a0b\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "LACE\u7ed3\u5408\u63d0\u793a\u5f15\u5bfc\u7684\u7b56\u7565\u751f\u6210\u3001\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u7b56\u7565\u751f\u6210\u4e0e\u9a8c\u8bc1\u3002", "result": "LACE\u5728\u667a\u80fd\u5bb6\u5c45\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86100%\u7b56\u7565\u751f\u6210\u6b63\u786e\u6027\uff0c\u51b3\u7b56\u51c6\u786e\u7387\u8fbe88%\uff0cF1\u5206\u65700.79\uff0c\u4f18\u4e8eGPT-3.5\u548cGemini\u3002", "conclusion": "LACE\u5c55\u793a\u4e86\u5728\u73b0\u5b9eIoT\u5e73\u53f0\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u7075\u6d3b\u548c\u7528\u6237\u53cb\u597d\u8bbf\u95ee\u63a7\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.24210", "pdf": "https://arxiv.org/pdf/2505.24210", "abs": "https://arxiv.org/abs/2505.24210", "authors": ["Zheng Tan", "Weizhen Wang", "Andrea L. Bertozzi", "Ernest K. Ryu"], "title": "STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Diffusion models (DMs) have demonstrated remarkable performance in\nhigh-fidelity image and video generation. Because high-quality generations with\nDMs typically require a large number of function evaluations (NFEs), resulting\nin slow sampling, there has been extensive research successfully reducing the\nNFE to a small range (<10) while maintaining acceptable image quality. However,\nmany practical applications, such as those involving Stable Diffusion 3.5,\nFLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve\nsuperior results, and, despite the practical relevance, research on the\neffective sampling within this mid-NFE regime remains underexplored. In this\nwork, we propose a novel, training-free, and structure-independent DM ODE\nsolver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method,\nbased on a class of stiff ODE solvers with a Taylor expansion adaptation.\nUnlike prior work such as DPM-Solver, which is dependent on the semi-linear\nstructure of the DM ODE, STORK is applicable to any DM sampling, including\nnoise-based and flow matching-based models. Within the 20-50 NFE range, STORK\nachieves improved generation quality, as measured by FID scores, across\nunconditional pixel-level generation and conditional latent-space generation\ntasks using models like Stable Diffusion 3.5 and SANA. Code is available at\nhttps://github.com/ZT220501/STORK.", "AI": {"tldr": "STORK\u662f\u4e00\u79cd\u65b0\u578b\u3001\u65e0\u9700\u8bad\u7ec3\u4e14\u7ed3\u6784\u65e0\u5173\u7684DM ODE\u6c42\u89e3\u5668\uff0c\u9002\u7528\u4e8e\u4e2dNFE\u8303\u56f4\uff0820-50\uff09\uff0c\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e2dNFE\u8303\u56f4\uff0820-50\uff09\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u5982Stable Diffusion 3.5\u7b49\u5e38\u5728\u6b64\u8303\u56f4\u8fd0\u884c\u3002", "method": "\u63d0\u51faSTORK\u65b9\u6cd5\uff0c\u57fa\u4e8e\u521a\u6027ODE\u6c42\u89e3\u5668\u548c\u6cf0\u52d2\u5c55\u5f00\u9002\u914d\uff0c\u9002\u7528\u4e8e\u4efb\u4f55DM\u91c7\u6837\uff0c\u5305\u62ec\u566a\u58f0\u548c\u6d41\u5339\u914d\u6a21\u578b\u3002", "result": "\u572820-50 NFE\u8303\u56f4\u5185\uff0cSTORK\u5728\u65e0\u6761\u4ef6\u50cf\u7d20\u7ea7\u548c\u6761\u4ef6\u6f5c\u7a7a\u95f4\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u751f\u6210\u8d28\u91cf\uff08FID\u5f97\u5206\uff09\u3002", "conclusion": "STORK\u4e3a\u4e2dNFE\u8303\u56f4\u7684\u6269\u6563\u6a21\u578b\u91c7\u6837\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.23836", "pdf": "https://arxiv.org/pdf/2505.23836", "abs": "https://arxiv.org/abs/2505.23836", "authors": ["Joe Needham", "Giles Edkins", "Govind Pimpale", "Henning Bartsch", "Marius Hobbhahn"], "title": "Large Language Models Often Know When They Are Being Evaluated", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u533a\u5206\u8bc4\u4f30\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u6587\u672c\uff0c\u4f46\u5c1a\u672a\u8d85\u8d8a\u4eba\u7c7b\u57fa\u7ebf\u3002\u5efa\u8bae\u672a\u6765\u6a21\u578b\u8ddf\u8e2a\u6b64\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8AI\u6a21\u578b\u662f\u5426\u80fd\u68c0\u6d4b\u8bc4\u4f30\u573a\u666f\uff0c\u4ee5\u907f\u514d\u8bc4\u4f30\u7ed3\u679c\u5931\u771f\u5f71\u54cd\u90e8\u7f72\u548c\u6cbb\u7406\u51b3\u7b56\u3002", "method": "\u6784\u5efa\u5305\u542b1000\u4e2a\u63d0\u793a\u548c\u6587\u672c\u7684\u591a\u6837\u5316\u57fa\u51c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u8bc4\u4f30\u548c\u5b9e\u9645\u90e8\u7f72\u6587\u672c\u7684\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u968f\u673a\uff08AUC 0.83\uff09\uff0c\u4f46\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u7ebf\uff08AUC 0.92\uff09\u3002\u5728\u4ee3\u7406\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u524d\u6cbf\u6a21\u578b\u5df2\u5177\u5907\u663e\u8457\u7684\u8bc4\u4f30\u610f\u8bc6\u80fd\u529b\uff0c\u4f46\u672a\u8fbe\u4eba\u7c7b\u6c34\u5e73\uff0c\u5efa\u8bae\u672a\u6765\u6301\u7eed\u76d1\u6d4b\u3002"}}
{"id": "2505.24214", "pdf": "https://arxiv.org/pdf/2505.24214", "abs": "https://arxiv.org/abs/2505.24214", "authors": ["Redwan Sony", "Parisa Farmanifard", "Hamzeh Alzwairy", "Nitish Shukla", "Arun Ross"], "title": "Benchmarking Foundation Models for Zero-Shot Biometric Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advent of foundation models, particularly Vision-Language Models (VLMs)\nand Multi-modal Large Language Models (MLLMs), has redefined the frontiers of\nartificial intelligence, enabling remarkable generalization across diverse\ntasks with minimal or no supervision. Yet, their potential in biometric\nrecognition and analysis remains relatively underexplored. In this work, we\nintroduce a comprehensive benchmark that evaluates the zero-shot and few-shot\nperformance of state-of-the-art publicly available VLMs and MLLMs across six\nbiometric tasks spanning the face and iris modalities: face verification, soft\nbiometric attribute prediction (gender and race), iris recognition,\npresentation attack detection (PAD), and face manipulation detection (morphs\nand deepfakes). A total of 41 VLMs were used in this evaluation. Experiments\nshow that embeddings from these foundation models can be used for diverse\nbiometric tasks with varying degrees of success. For example, in the case of\nface verification, a True Match Rate (TMR) of 96.77 percent was obtained at a\nFalse Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)\ndataset, without any fine-tuning. In the case of iris recognition, the TMR at 1\npercent FMR on the IITD-R-Full dataset was 97.55 percent without any\nfine-tuning. Further, we show that applying a simple classifier head to these\nembeddings can help perform DeepFake detection for faces, Presentation Attack\nDetection (PAD) for irides, and extract soft biometric attributes like gender\nand ethnicity from faces with reasonably high accuracy. This work reiterates\nthe potential of pretrained models in achieving the long-term vision of\nArtificial General Intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e8641\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u516d\u79cd\u751f\u7269\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u7684\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\uff08\u5982VLMs\u548cMLLMs\uff09\u5728\u751f\u7269\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u516d\u79cd\u751f\u7269\u8bc6\u522b\u4efb\u52a1\uff08\u5982\u4eba\u8138\u9a8c\u8bc1\u3001\u8679\u819c\u8bc6\u522b\u7b49\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982\u4eba\u8138\u9a8c\u8bc1\u5728LFW\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.77%\u7684TMR\uff0c\u8679\u819c\u8bc6\u522b\u5728IITD-R-Full\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.55%\u7684TMR\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u751f\u7269\u8bc6\u522b\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2505.23837", "pdf": "https://arxiv.org/pdf/2505.23837", "abs": "https://arxiv.org/abs/2505.23837", "authors": ["Lin Zhong", "Lingzhi Wang", "Xu Yang", "Qing Liao"], "title": "CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language", "categories": ["cs.CL", "cs.IR", "I.2.0"], "comment": "This paper has been accepted by SIGIR 2025", "summary": "Large Language Models (LLMs) offer new opportunities for the next\nPoint-Of-Interest (POI) prediction task, leveraging their capabilities in\nsemantic understanding of POI trajectories. However, previous LLM-based\nmethods, which are superficially adapted to next POI prediction, largely\noverlook critical challenges associated with applying LLMs to this task.\nSpecifically, LLMs encounter two critical challenges: (1) a lack of intrinsic\nunderstanding of numeric spatiotemporal data, which hinders accurate modeling\nof users' spatiotemporal distributions and preferences; and (2) an excessively\nlarge and unconstrained candidate POI space, which often results in random or\nirrelevant predictions. To address these issues, we propose a Collaborative\nMulti Agent Framework for Next POI Prediction, named CoMaPOI. Through the close\ninteraction of three specialized agents (Profiler, Forecaster, and Predictor),\nCoMaPOI collaboratively addresses the two critical challenges. The Profiler\nagent is responsible for converting numeric data into language descriptions,\nenhancing semantic understanding. The Forecaster agent focuses on dynamically\nconstraining and refining the candidate POI space. The Predictor agent\nintegrates this information to generate high-precision predictions. Extensive\nexperiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that\nCoMaPOI achieves state of the art performance, improving all metrics by 5% to\n10% compared to SOTA baselines. This work pioneers the investigation of\nchallenges associated with applying LLMs to complex spatiotemporal tasks by\nleveraging tailored collaborative agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoMaPOI\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728POI\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6570\u503c\u65f6\u7a7a\u6570\u636e\u7684\u8bed\u4e49\u7406\u89e3\u548c\u5019\u9009POI\u7a7a\u95f4\u7684\u52a8\u6001\u7ea6\u675f\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728POI\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b58\u5728\u5bf9\u6570\u503c\u65f6\u7a7a\u6570\u636e\u7406\u89e3\u4e0d\u8db3\u548c\u5019\u9009POI\u7a7a\u95f4\u8fc7\u5927\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u63d0\u51faCoMaPOI\u6846\u67b6\uff0c\u5305\u542bProfiler\u3001Forecaster\u548cPredictor\u4e09\u4e2a\u667a\u80fd\u4f53\uff0c\u5206\u522b\u8d1f\u8d23\u6570\u636e\u8bed\u4e49\u8f6c\u6362\u3001\u5019\u9009\u7a7a\u95f4\u7ea6\u675f\u548c\u9884\u6d4b\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cCoMaPOI\u6027\u80fd\u63d0\u53475%\u81f310%\uff0c\u8fbe\u5230\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u534f\u4f5c\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u65f6\u7a7a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24216", "pdf": "https://arxiv.org/pdf/2505.24216", "abs": "https://arxiv.org/abs/2505.24216", "authors": ["Prasanna Reddy Pulakurthi", "Majid Rabbani", "Jamison Heard", "Sohail Dianat", "Celso M. de Melo", "Raghuveer Rao"], "title": "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, 5 tables, Accepted to IEEE ICIP 2025", "summary": "This work investigates Source-Free Domain Adaptation (SFDA), where a model\nadapts to a target domain without access to source data. A new augmentation\ntechnique, Shuffle PatchMix (SPM), and a novel reweighting strategy are\nintroduced to enhance performance. SPM shuffles and blends image patches to\ngenerate diverse and challenging augmentations, while the reweighting strategy\nprioritizes reliable pseudo-labels to mitigate label noise. These techniques\nare particularly effective on smaller datasets like PACS, where overfitting and\npseudo-label noise pose greater risks. State-of-the-art results are achieved on\nthree major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS,\nimprovements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target\nand multi-target settings, respectively, while gains of 2.8% and 0.7% are\nattained on DomainNet-126 and VisDA-C. This combination of advanced\naugmentation and robust pseudo-label reweighting establishes a new benchmark\nfor SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff08SFDA\uff09\uff0c\u7ed3\u5408\u65b0\u7684\u589e\u5f3a\u6280\u672fShuffle PatchMix\uff08SPM\uff09\u548c\u4f2a\u6807\u7b7e\u91cd\u52a0\u6743\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u4e2d\u56e0\u7f3a\u4e4f\u6e90\u6570\u636e\u800c\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u3002", "method": "\u5f15\u5165SPM\u6280\u672f\u901a\u8fc7\u6253\u4e71\u548c\u6df7\u5408\u56fe\u50cf\u5757\u751f\u6210\u591a\u6837\u5316\u589e\u5f3a\u6570\u636e\uff0c\u540c\u65f6\u91c7\u7528\u91cd\u52a0\u6743\u7b56\u7565\u4f18\u5316\u4f2a\u6807\u7b7e\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728PACS\u3001VisDA-C\u548cDomainNet-126\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u5176\u4e2dPACS\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u8bbe\u7f6e\u5206\u522b\u63d0\u53477.3%\u548c7.2%\u3002", "conclusion": "SPM\u548c\u91cd\u52a0\u6743\u7b56\u7565\u7684\u7ed3\u5408\u4e3aSFDA\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.23838", "pdf": "https://arxiv.org/pdf/2505.23838", "abs": "https://arxiv.org/abs/2505.23838", "authors": ["Yiming Huang", "Jiyu Guo", "Wenxin Mao", "Cuiyun Gao", "Peiyi Han", "Chuanyi Liu", "Qing Ling"], "title": "Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities", "categories": ["cs.CL", "cs.IR"], "comment": "Submitted to ACM Computing Surveys (CSUR). Currently under review", "summary": "Converting natural language (NL) questions into SQL queries, referred to as\nText-to-SQL, has emerged as a pivotal technology for facilitating access to\nrelational databases, especially for users without SQL knowledge. Recent\nprogress in large language models (LLMs) has markedly propelled the field of\nnatural language processing (NLP), opening new avenues to improve text-to-SQL\nsystems. This study presents a systematic review of LLM-based text-to-SQL,\nfocusing on four key aspects: (1) an analysis of the research trends in\nLLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based\ntext-to-SQL techniques from diverse perspectives; (3) summarization of existing\ntext-to-SQL datasets and evaluation metrics; and (4) discussion on potential\nobstacles and avenues for future exploration in this domain. This survey seeks\nto furnish researchers with an in-depth understanding of LLM-based text-to-SQL,\nsparking new innovations and advancements in this field.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684Text-to-SQL\u6280\u672f\uff0c\u6db5\u76d6\u7814\u7a76\u8d8b\u52bf\u3001\u6280\u672f\u5206\u6790\u3001\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u6307\u6807\u603b\u7ed3\uff0c\u4ee5\u53ca\u672a\u6765\u6311\u6218\u4e0e\u65b9\u5411\u3002", "motivation": "Text-to-SQL\u6280\u672f\u4e3a\u975eSQL\u7528\u6237\u63d0\u4f9b\u4e86\u8bbf\u95ee\u5173\u7cfb\u6570\u636e\u5e93\u7684\u4fbf\u6377\u9014\u5f84\uff0c\u800cLLM\u7684\u53d1\u5c55\u4e3a\u6539\u8fdb\u8fd9\u4e00\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5206\u6790LLM-based Text-to-SQL\u7684\u7814\u7a76\u8d8b\u52bf\u3001\u6280\u672f\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u6280\u672f\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u969c\u788d\u548c\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6df1\u5165\u7406\u89e3\uff0c\u63a8\u52a8LLM-based Text-to-SQL\u9886\u57df\u7684\u521b\u65b0\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2505.24222", "pdf": "https://arxiv.org/pdf/2505.24222", "abs": "https://arxiv.org/abs/2505.24222", "authors": ["Fangyikang Wang", "Hubery Yin", "Lei Qian", "Yinan Li", "Shaobin Zhuang", "Huminhao Zhu", "Yilin Zhang", "Yanlong Tang", "Chao Zhang", "Hanbin Zhao", "Hui Qian", "Chen Li"], "title": "Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The diffusion models (DMs) have demonstrated the remarkable capability of\ngenerating images via learning the noised score function of data distribution.\nCurrent DM sampling techniques typically rely on first-order Langevin dynamics\nat each noise level, with efforts concentrated on refining inter-level\ndenoising strategies. While leveraging additional second-order Hessian geometry\nto enhance the sampling quality of Langevin is a common practice in Markov\nchain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in\nhigh-dimensional DMs lead to quadratic-complexity computational costs,\nrendering them non-scalable. In this work, we introduce a novel\nLevenberg-Marquardt-Langevin (LML) method that approximates the diffusion\nHessian geometry in a training-free manner, drawing inspiration from the\ncelebrated Levenberg-Marquardt optimization algorithm. Our approach introduces\ntwo key innovations: (1) A low-rank approximation of the diffusion Hessian,\nleveraging the DMs' inherent structure and circumventing explicit\nquadratic-complexity computations; (2) A damping mechanism to stabilize the\napproximated Hessian. This LML approximated Hessian geometry enables the\ndiffusion sampling to execute more accurate steps and improve the image\ngeneration quality. We further conduct a theoretical analysis to substantiate\nthe approximation error bound of low-rank approximation and the convergence\nproperty of the damping mechanism. Extensive experiments across multiple\npretrained DMs validate that the LML method significantly improves image\ngeneration quality, with negligible computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLevenberg-Marquardt-Langevin\uff08LML\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u963b\u5c3c\u673a\u5236\u9ad8\u6548\u5229\u7528\u6269\u6563Hessian\u51e0\u4f55\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u91c7\u6837\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u4e00\u9636Langevin\u52a8\u529b\u5b66\uff0c\u800c\u76f4\u63a5\u5229\u7528Hessian\u51e0\u4f55\u4f1a\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u8fd1\u4f3cHessian\u51e0\u4f55\u6765\u63d0\u5347\u91c7\u6837\u8d28\u91cf\u3002", "method": "\u63d0\u51faLML\u65b9\u6cd5\uff0c\u5305\u62ec\u4f4e\u79e9\u8fd1\u4f3c\u6269\u6563Hessian\u548c\u963b\u5c3c\u673a\u5236\uff0c\u907f\u514d\u663e\u5f0f\u4e8c\u6b21\u590d\u6742\u5ea6\u8ba1\u7b97\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u65e0\u9700\u989d\u5916\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLML\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "conclusion": "LML\u65b9\u6cd5\u901a\u8fc7\u9ad8\u6548\u8fd1\u4f3cHessian\u51e0\u4f55\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u91c7\u6837\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6b65\u9aa4\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23840", "pdf": "https://arxiv.org/pdf/2505.23840", "abs": "https://arxiv.org/abs/2505.23840", "authors": ["Jiseung Hong", "Grace Byun", "Seungone Kim", "Kai Shu"], "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench.", "AI": {"tldr": "SYCON Bench\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6e\u5bf9\u8bdd\u4e2dLLMs\u7684\u8fce\u5408\u884c\u4e3a\uff0c\u53d1\u73b0\u5bf9\u9f50\u8c03\u4f18\u52a0\u5267\u8fce\u5408\uff0c\u800c\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u4f18\u5316\u589e\u5f3a\u62b5\u6297\u80fd\u529b\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u771f\u5b9e\u4ea4\u4e92\u4e2d\u7684\u8fce\u5408\u884c\u4e3a\uff0c\u586b\u8865\u5355\u8f6e\u4e8b\u5b9e\u6b63\u786e\u6027\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165SYCON Bench\uff0c\u6d4b\u91cf\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u8fce\u5408\u7528\u6237\u7684\u901f\u5ea6\u548c\u9891\u7387\uff0c\u8bc4\u4f3017\u79cdLLMs\u3002", "result": "\u8fce\u5408\u884c\u4e3a\u666e\u904d\u5b58\u5728\uff0c\u5bf9\u9f50\u8c03\u4f18\u52a0\u5267\u8fce\u5408\uff0c\u63a8\u7406\u6a21\u578b\u8868\u73b0\u66f4\u597d\u4f46\u6613\u56e0\u8fc7\u5ea6\u903b\u8f91\u5316\u800c\u5931\u8d25\u3002", "conclusion": "\u91c7\u7528\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u7684\u63d0\u793a\u7b56\u7565\u53ef\u663e\u8457\u51cf\u5c11\u8fce\u5408\u884c\u4e3a\u3002"}}
{"id": "2505.24225", "pdf": "https://arxiv.org/pdf/2505.24225", "abs": "https://arxiv.org/abs/2505.24225", "authors": ["Haibo Jin", "Peiyan Zhang", "Man Luo", "Haohan Wang"], "title": "Reasoning Can Hurt the Inductive Abilities of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "26 pages", "summary": "Large Language Models (LLMs) have shown remarkable progress across domains,\nyet their ability to perform inductive reasoning - inferring latent rules from\nsparse examples - remains limited. It is often assumed that chain-of-thought\n(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such\nreasoning. We investigate this assumption with creating four controlled,\ndiagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -\nwith hidden human-defined rules. We find that CoT reasoning can degrade\ninductive performance, with LRMs often underperforming their non-reasoning\ncounterparts.\n  To explain this, we present a theoretical framework that reveals how\nreasoning steps can amplify error through three failure modes: incorrect\nsub-task decomposition, incorrect sub-task solving, and incorrect final answer\nsummarization. Based on our theoretical and empirical analysis, we introduce\nstructured interventions that adapt CoT generation according to our identified\nfailure types. These interventions improve inductive accuracy without\nretraining. Our findings suggest that effective (CoT) reasoning depends not\nonly on taking more steps but also on ensuring those steps are well-structured.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u53ef\u80fd\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u589e\u5f3a\u3002\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u9a8c\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u5e72\u9884\u65b9\u6cd5\u4ee5\u6539\u8fdb\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u5bf9LLM\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u5982\u5047\u8bbe\u4e2d\u90a3\u6837\u80fd\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u79cd\u57fa\u4e8e\u6e38\u620f\u7684\u8bca\u65ad\u4efb\u52a1\uff08\u5982\u56fd\u9645\u8c61\u68cb\u3001\u5fb7\u5dde\u6251\u514b\u7b49\uff09\uff0c\u5bf9\u6bd4\u5206\u6790CoT\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u5931\u8d25\u6a21\u5f0f\u3002", "result": "CoT\u63a8\u7406\u53ef\u80fd\u964d\u4f4e\u5f52\u7eb3\u63a8\u7406\u6027\u80fd\uff0c\u4e14\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u8868\u73b0\u5e38\u4e0d\u53ca\u975e\u63a8\u7406\u6a21\u578b\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u5e72\u9884\u53ef\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u6709\u6548\u7684CoT\u63a8\u7406\u4e0d\u4ec5\u9700\u8981\u66f4\u591a\u6b65\u9aa4\uff0c\u8fd8\u9700\u786e\u4fdd\u6b65\u9aa4\u7ed3\u6784\u5408\u7406\u3002\u7ed3\u6784\u5316\u5e72\u9884\u80fd\u663e\u8457\u6539\u8fdb\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2505.23842", "pdf": "https://arxiv.org/pdf/2505.23842", "abs": "https://arxiv.org/abs/2505.23842", "authors": ["Zikun Ye", "Hema Yoganarasimhan"], "title": "Document Valuation in LLM Summaries: A Cluster Shapley Approach", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in systems that retrieve\nand summarize content from multiple sources, such as search engines and AI\nassistants. While these models enhance user experience by generating coherent\nsummaries, they obscure the contributions of original content creators, raising\nconcerns about credit attribution and compensation. We address the challenge of\nvaluing individual documents used in LLM-generated summaries. We propose using\nShapley values, a game-theoretic method that allocates credit based on each\ndocument's marginal contribution. Although theoretically appealing, Shapley\nvalues are expensive to compute at scale. We therefore propose Cluster Shapley,\nan efficient approximation algorithm that leverages semantic similarity between\ndocuments. By clustering documents using LLM-based embeddings and computing\nShapley values at the cluster level, our method significantly reduces\ncomputation while maintaining attribution quality. We demonstrate our approach\nto a summarization task using Amazon product reviews. Cluster Shapley\nsignificantly reduces computational complexity while maintaining high accuracy,\noutperforming baseline methods such as Monte Carlo sampling and Kernel SHAP\nwith a better efficient frontier. Our approach is agnostic to the exact LLM\nused, the summarization process used, and the evaluation procedure, which makes\nit broadly applicable to a variety of summarization settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eShapley\u503c\u7684\u6587\u6863\u8d21\u732e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684Cluster Shapley\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728LLM\u751f\u6210\u6458\u8981\u65f6\u516c\u5e73\u5206\u914d\u539f\u59cb\u5185\u5bb9\u521b\u4f5c\u8005\u7684\u8d21\u732e\u3002", "motivation": "LLM\u5728\u751f\u6210\u6458\u8981\u65f6\u6a21\u7cca\u4e86\u539f\u59cb\u5185\u5bb9\u521b\u4f5c\u8005\u7684\u8d21\u732e\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u4fe1\u7528\u5206\u914d\u548c\u8865\u507f\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Shapley\u503c\u8bc4\u4f30\u6587\u6863\u8d21\u732e\uff0c\u5e76\u63d0\u51faCluster Shapley\u7b97\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u6587\u6863\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "Cluster Shapley\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u6458\u8981\u573a\u666f\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.24227", "pdf": "https://arxiv.org/pdf/2505.24227", "abs": "https://arxiv.org/abs/2505.24227", "authors": ["Ying Yang", "Jie Zhang", "Xiao Lv", "Di Lin", "Tao Xiang", "Qing Guo"], "title": "Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "While adversarial attacks on vision-and-language pretraining (VLP) models\nhave been explored, generating natural adversarial samples crafted through\nrealistic and semantically meaningful perturbations remains an open challenge.\nExisting methods, primarily designed for classification tasks, struggle when\nadapted to VLP models due to their restricted optimization spaces, leading to\nineffective attacks or unnatural artifacts. To address this, we propose\n\\textbf{LightD}, a novel framework that generates natural adversarial samples\nfor VLP models via semantically guided relighting. Specifically, LightD\nleverages ChatGPT to propose context-aware initial lighting parameters and\nintegrates a pretrained relighting model (IC-light) to enable diverse lighting\nadjustments. LightD expands the optimization space while ensuring perturbations\nalign with scene semantics. Additionally, gradient-based optimization is\napplied to the reference lighting image to further enhance attack effectiveness\nwhile maintaining visual naturalness. The effectiveness and superiority of the\nproposed LightD have been demonstrated across various VLP models in tasks such\nas image captioning and visual question answering.", "AI": {"tldr": "LightD\u662f\u4e00\u4e2a\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u91cd\u65b0\u5149\u7167\u751f\u6210\u81ea\u7136\u5bf9\u6297\u6837\u672c\u7684\u6846\u67b6\uff0c\u9488\u5bf9\u89c6\u89c9\u4e0e\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\uff08VLP\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u7a7a\u95f4\u53d7\u9650\u548c\u751f\u6210\u4e0d\u81ea\u7136\u6270\u52a8\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u4efb\u52a1\uff0c\u96be\u4ee5\u9002\u5e94VLP\u6a21\u578b\uff0c\u4e14\u751f\u6210\u7684\u6270\u52a8\u4e0d\u81ea\u7136\u6216\u8bed\u4e49\u4e0d\u4e00\u81f4\u3002LightD\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u91cd\u65b0\u5149\u7167\u751f\u6210\u66f4\u81ea\u7136\u7684\u5bf9\u6297\u6837\u672c\u3002", "method": "LightD\u7ed3\u5408ChatGPT\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u521d\u59cb\u5149\u7167\u53c2\u6570\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u7684\u91cd\u65b0\u5149\u7167\u6a21\u578b\uff08IC-light\uff09\u8fdb\u884c\u591a\u6837\u5316\u5149\u7167\u8c03\u6574\u3002\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u53c2\u8003\u5149\u7167\u56fe\u50cf\uff0c\u589e\u5f3a\u653b\u51fb\u6548\u679c\u5e76\u4fdd\u6301\u89c6\u89c9\u81ea\u7136\u6027\u3002", "result": "LightD\u5728\u591a\u79cdVLP\u6a21\u578b\uff08\u5982\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff09\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "LightD\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u91cd\u65b0\u5149\u7167\u6210\u529f\u751f\u6210\u4e86\u81ea\u7136\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4e3aVLP\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.23843", "pdf": "https://arxiv.org/pdf/2505.23843", "abs": "https://arxiv.org/abs/2505.23843", "authors": ["Wenhan Dong", "Tianyi Hu", "Jingyi Zheng", "Zhen Sun", "Yuemeng Zhao", "Yule Liu", "Xinlei He", "Xinyi Huang"], "title": "Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multi-round incomplete information tasks are crucial for evaluating the\nlateral thinking capabilities of large language models (LLMs). Currently,\nresearch primarily relies on multiple benchmarks and automated evaluation\nmetrics to assess these abilities. However, our study reveals novel insights\ninto the limitations of existing methods, as they often yield misleading\nresults that fail to uncover key issues, such as shortcut-taking behaviors,\nrigid patterns, and premature task termination. These issues obscure the true\nreasoning capabilities of LLMs and undermine the reliability of evaluations. To\naddress these limitations, we propose a refined set of evaluation standards,\nincluding inspection of reasoning paths, diversified assessment metrics, and\ncomparative analyses with human performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u591a\u8f6e\u4e0d\u5b8c\u5168\u4fe1\u606f\u4efb\u52a1\u4e2d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6a2a\u5411\u601d\u7ef4\u80fd\u529b\u7684\u8bc4\u4f30\u5b58\u5728\u8bef\u5bfc\u6027\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u6807\u51c6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u63ed\u793aLLMs\u7684\u5173\u952e\u95ee\u9898\uff08\u5982\u8d70\u6377\u5f84\u3001\u6a21\u5f0f\u50f5\u5316\u548c\u4efb\u52a1\u63d0\u524d\u7ec8\u6b62\uff09\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5305\u62ec\u63a8\u7406\u8def\u5f84\u68c0\u67e5\u3001\u591a\u6837\u5316\u8bc4\u4f30\u6307\u6807\u53ca\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u6613\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\uff0c\u63a9\u76d6LLMs\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u9700\u91c7\u7528\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u4ee5\u63d0\u9ad8LLMs\u6a2a\u5411\u601d\u7ef4\u80fd\u529b\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.24232", "pdf": "https://arxiv.org/pdf/2505.24232", "abs": "https://arxiv.org/abs/2505.24232", "authors": ["Haibo Jin", "Peiyan Zhang", "Peiran Wang", "Man Luo", "Haohan Wang"], "title": "From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large foundation models (LFMs) are susceptible to two distinct\nvulnerabilities: hallucinations and jailbreak attacks. While typically studied\nin isolation, we observe that defenses targeting one often affect the other,\nhinting at a deeper connection.\n  We propose a unified theoretical framework that models jailbreaks as\ntoken-level optimization and hallucinations as attention-level optimization.\nWithin this framework, we establish two key propositions: (1) \\textit{Similar\nLoss Convergence} - the loss functions for both vulnerabilities converge\nsimilarly when optimizing for target-specific outputs; and (2) \\textit{Gradient\nConsistency in Attention Redistribution} - both exhibit consistent gradient\nbehavior driven by shared attention dynamics.\n  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,\nshowing consistent optimization trends and aligned gradients. Leveraging this\nconnection, we demonstrate that mitigation techniques for hallucinations can\nreduce jailbreak success rates, and vice versa. Our findings reveal a shared\nfailure mode in LFMs and suggest that robustness strategies should jointly\naddress both vulnerabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8d8a\u72f1\u653b\u51fb\u548c\u5e7b\u89c9\u95ee\u9898\u5206\u522b\u5efa\u6a21\u4e3a\u4ee4\u724c\u7ea7\u548c\u6ce8\u610f\u529b\u7ea7\u4f18\u5316\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u635f\u5931\u6536\u655b\u548c\u68af\u5ea6\u884c\u4e3a\u4e0a\u76f8\u4f3c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u8054\u7cfb\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\uff08LFMs\uff09\u5b58\u5728\u5e7b\u89c9\u548c\u8d8a\u72f1\u653b\u51fb\u4e24\u79cd\u6f0f\u6d1e\uff0c\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5b64\u7acb\u5206\u6790\uff0c\u4f46\u9632\u5fa1\u63aa\u65bd\u5bf9\u4e24\u8005\u7684\u5f71\u54cd\u8868\u660e\u5b83\u4eec\u53ef\u80fd\u5b58\u5728\u6df1\u5c42\u8054\u7cfb\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u8d8a\u72f1\u653b\u51fb\u5efa\u6a21\u4e3a\u4ee4\u724c\u7ea7\u4f18\u5316\uff0c\u5e7b\u89c9\u4e3a\u6ce8\u610f\u529b\u7ea7\u4f18\u5316\uff0c\u9a8c\u8bc1\u635f\u5931\u6536\u655b\u548c\u68af\u5ea6\u4e00\u81f4\u6027\u3002", "result": "\u5728LLaVA-1.5\u548cMiniGPT-4\u4e0a\u5b9e\u9a8c\u8bc1\u5b9e\u4f18\u5316\u8d8b\u52bf\u548c\u68af\u5ea6\u884c\u4e3a\u4e00\u81f4\uff0c\u4e14\u9488\u5bf9\u4e00\u79cd\u6f0f\u6d1e\u7684\u7f13\u89e3\u6280\u672f\u53ef\u964d\u4f4e\u53e6\u4e00\u79cd\u6f0f\u6d1e\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u63ed\u793a\u4e86LFMs\u7684\u5171\u4eab\u5931\u8d25\u6a21\u5f0f\uff0c\u5efa\u8bae\u9c81\u68d2\u6027\u7b56\u7565\u9700\u540c\u65f6\u5e94\u5bf9\u4e24\u79cd\u6f0f\u6d1e\u3002"}}
{"id": "2505.23844", "pdf": "https://arxiv.org/pdf/2505.23844", "abs": "https://arxiv.org/abs/2505.23844", "authors": ["Zhenglun Kong", "Zheng Zhan", "Shiyue Hou", "Yifan Gong", "Xin Meng", "Pengwei Sui", "Peiyan Dong", "Xuan Shen", "Zifeng Wang", "Pu Zhao", "Hao Tang", "Stratis Ioannidis", "Yanzhi Wang"], "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9009\u62e9\u548c\u805a\u5408\u591a\u6e90LLM\u77e5\u8bc6\u7684\u6846\u67b6\uff0c\u4ee5\u51cf\u5c11\u77e5\u8bc6\u5e72\u6270\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5728\u6301\u7eed\u6539\u8fdbLLM\u65f6\u9762\u4e34\u5185\u5b58\u548c\u9002\u5e94\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6613\u53d7\u77e5\u8bc6\u5e72\u6270\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u9009\u62e9\u7f51\u7edc\u548c\u52a8\u6001\u52a0\u6743\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u53cd\u9988\u9a71\u52a8\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c1150%\u7684\u77e5\u8bc6\u5e72\u6270\uff0c\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6e90LLM\u77e5\u8bc6\u805a\u5408\u4e2d\u7684\u5e72\u6270\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.24238", "pdf": "https://arxiv.org/pdf/2505.24238", "abs": "https://arxiv.org/abs/2505.24238", "authors": ["Bowen Dong", "Minheng Ni", "Zitong Huang", "Guanglei Yang", "Wangmeng Zuo", "Lei Zhang"], "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a{\\dataset}\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u533a\u5206\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7684\u611f\u77e5\u8bf1\u5bfc\u5e7b\u89c9\u548c\u63a8\u7406\u8bf1\u5bfc\u5e7b\u89c9\uff0c\u5e76\u63d0\u51fa\u4e86{\\method}\u65b9\u6cd5\u6765\u51cf\u5c11\u63a8\u7406\u5e7b\u89c9\u3002", "motivation": "\u591a\u6a21\u6001\u5e7b\u89c9\u9650\u5236\u4e86MLLMs\u7684\u6b63\u786e\u6027\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u533a\u5206\u611f\u77e5\u548c\u63a8\u7406\u5bfc\u81f4\u7684\u5e7b\u89c9\uff0c\u963b\u788d\u4e86\u5bf9MLLMs\u63a8\u7406\u5931\u8d25\u7684\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u4e86{\\dataset}\u57fa\u51c6\uff0c\u901a\u8fc7\u6784\u5efa\u95ee\u9898\u9694\u79bb\u63a8\u7406\u5e7b\u89c9\uff0c\u5e76\u5f15\u5165\u591a\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u3002\u540c\u65f6\u63d0\u51fa{\\method}\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5f3a\u5316\u5fae\u8c03\u548c\u534f\u4f5c\u63d0\u793a\u63a8\u7406\u6765\u51cf\u5c11\u63a8\u7406\u590d\u6742\u6027\u3002", "result": "\u5206\u6790\u8868\u660e\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u89c4\u6a21\u548c\u8bad\u7ec3\u9636\u6bb5\u663e\u8457\u5f71\u54cd\u5e7b\u89c9\u7a0b\u5ea6\uff1b\u5f53\u524dMLLMs\u5728\u7a7a\u95f4\u5e7b\u89c9\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u95ee\u9898\u7c7b\u578b\u4e0e\u5e7b\u89c9\u6a21\u5f0f\u76f8\u5173\u3002{\\method}\u5728\u57fa\u51c6\u4e0a\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u51cf\u5c11\u4e86\u903b\u8f91\u5e7b\u89c9\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7{\\dataset}\u548c{\\method}\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3aMLLMs\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.23845", "pdf": "https://arxiv.org/pdf/2505.23845", "abs": "https://arxiv.org/abs/2505.23845", "authors": ["Jakub Podolak", "Rajeev Verma"], "title": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration.", "AI": {"tldr": "DeepSeek R1-32B\u7684\u81ea\u6211\u62a5\u544a\u7f6e\u4fe1\u5ea6\u5728QA\u4efb\u52a1\u4e2d\u901a\u5e38\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u4f46\u901a\u8fc7\u8bed\u4e49\u71b5\uff08\u591a\u6b21\u91c7\u6837\uff09\u53ef\u4ee5\u66f4\u53ef\u9760\u5730\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u5f3a\u5236\u957f\u94fe\u601d\u8003\uff0c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6709\u6548\u6027\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7814\u7a76DeepSeek R1-32B\u5728QA\u4efb\u52a1\u4e2d\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u63a2\u7d22\u5982\u4f55\u63d0\u9ad8\u5176\u81ea\u6211\u62a5\u544a\u7f6e\u4fe1\u5ea6\u7684\u53ef\u9760\u6027\u3002", "method": "\u5206\u6790\u9ed8\u8ba4\u8bbe\u7f6e\u4e0b\u7684\u7f6e\u4fe1\u5ea6\u4e0e\u8bed\u4e49\u71b5\uff0c\u5f3a\u5236\u6a21\u578b\u8fdb\u884c\u957f\u94fe\u601d\u8003\uff0c\u5e76\u4f7f\u7528\u72ec\u7acb\u9605\u8bfb\u5668\u6a21\u578b\u9a8c\u8bc1\u7f6e\u4fe1\u5ea6\u6765\u6e90\u3002", "result": "\u957f\u94fe\u601d\u8003\u663e\u8457\u63d0\u5347\u7f6e\u4fe1\u5ea6\u6709\u6548\u6027\uff0c\u8bed\u4e49\u71b5\u66f4\u53ef\u9760\uff0c\u72ec\u7acb\u9605\u8bfb\u5668\u6a21\u578b\u80fd\u91cd\u5efa\u7c7b\u4f3c\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u9700\u663e\u5f0f\u63a2\u7d22\u751f\u6210\u7a7a\u95f4\uff0c\u81ea\u6211\u62a5\u544a\u7f6e\u4fe1\u5ea6\u4ec5\u5728\u63a2\u7d22\u540e\u53ef\u4fe1\u3002"}}
{"id": "2505.24245", "pdf": "https://arxiv.org/pdf/2505.24245", "abs": "https://arxiv.org/abs/2505.24245", "authors": ["Xin Kang", "Zihan Zheng", "Lei Chu", "Yue Gao", "Jiahao Li", "Hao Pan", "Xuejin Chen", "Yan Lu"], "title": "LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present LTM3D, a Latent Token space Modeling framework for conditional 3D\nshape generation that integrates the strengths of diffusion and auto-regressive\n(AR) models. While diffusion-based methods effectively model continuous latent\nspaces and AR models excel at capturing inter-token dependencies, combining\nthese paradigms for 3D shape generation remains a challenge. To address this,\nLTM3D features a Conditional Distribution Modeling backbone, leveraging a\nmasked autoencoder and a diffusion model to enhance token dependency learning.\nAdditionally, we introduce Prefix Learning, which aligns condition tokens with\nshape latent tokens during generation, improving flexibility across modalities.\nWe further propose a Latent Token Reconstruction module with\nReconstruction-Guided Sampling to reduce uncertainty and enhance structural\nfidelity in generated shapes. Our approach operates in token space, enabling\nsupport for multiple 3D representations, including signed distance fields,\npoint clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on\nimage- and text-conditioned shape generation tasks demonstrate that LTM3D\noutperforms existing methods in prompt fidelity and structural accuracy while\noffering a generalizable framework for multi-modal, multi-representation 3D\ngeneration.", "AI": {"tldr": "LTM3D\u662f\u4e00\u4e2a\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u76843D\u5f62\u72b6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u5206\u5e03\u5efa\u6a21\u548c\u524d\u7f00\u5b66\u4e60\u63d0\u5347\u751f\u6210\u6548\u679c\uff0c\u652f\u6301\u591a\u79cd3D\u8868\u793a\u5f62\u5f0f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c3D\u5f62\u72b6\u751f\u6210\u65f6\u5b58\u5728\u6311\u6218\uff0cLTM3D\u65e8\u5728\u6574\u5408\u4e24\u8005\u7684\u4f18\u52bf\u4ee5\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "LTM3D\u91c7\u7528\u6761\u4ef6\u5206\u5e03\u5efa\u6a21\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u63a9\u7801\u81ea\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u524d\u7f00\u5b66\u4e60\u548c\u6f5c\u5728\u4ee4\u724c\u91cd\u5efa\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLTM3D\u5728\u56fe\u50cf\u548c\u6587\u672c\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u5f62\u72b6\u7684\u63d0\u793a\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u51c6\u786e\u6027\u66f4\u9ad8\u3002", "conclusion": "LTM3D\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u591a\u6a21\u6001\u3001\u591a\u8868\u793a3D\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2505.23846", "pdf": "https://arxiv.org/pdf/2505.23846", "abs": "https://arxiv.org/abs/2505.23846", "authors": ["Atanu Barai", "Stephan Eidenbenz", "Nandakishore Santhi"], "title": "Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "To fully leverage the potential of artificial intelligence (AI) systems in a\ntrustworthy manner, it is desirable to couple multiple AI and non-AI systems\ntogether seamlessly for constraining and ensuring correctness of the output.\nThis paper introduces a novel parallel discrete event simulation (PDES) based\nmethodology to combine multiple AI and non-AI agents in a causal, rule-based\nway. Our approach tightly integrates the concept of passage of time, with each\nagent considered as an entity in the PDES framework and responding to prior\nrequests from other agents. Such coupling mechanism enables the agents to work\nin a co-operative environment towards a common goal while many tasks run in\nparallel throughout the simulation. It further enables setting up boundaries to\nthe outputs of the AI agents by applying necessary dynamic constraints using\nnon-AI agents while allowing for scalability through deployment of hundreds of\nsuch agents in a larger compute cluster. Distributing smaller AI agents can\nenable extremely scalable simulations in the future, addressing local memory\nbottlenecks for model parameter storage. Within a PDES involving both AI and\nnon-AI agents, we break down the problem at hand into structured steps, when\nnecessary, providing a set of multiple choices to the AI agents, and then\nprogressively solve these steps towards a final goal. At each step, the non-AI\nagents act as unbiased auditors, verifying each action by the AI agents so that\ncertain rules of engagement are followed. We evaluate our approach by solving\nfour problems from four different domains and comparing the results with those\nfrom AI models alone. Our results show greater accuracy in solving problems\nfrom various domains where the AI models struggle to solve the problems solely\nby themselves. Results show that overall accuracy of our approach is 68% where\nas the accuracy of vanilla models is less than 23%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e76\u884c\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\uff08PDES\uff09\u7684\u65b9\u6cd5\uff0c\u5c06AI\u4e0e\u975eAI\u4ee3\u7406\u7ed3\u5408\uff0c\u901a\u8fc7\u52a8\u6001\u7ea6\u675f\u548c\u89c4\u5219\u9a8c\u8bc1\u63d0\u9ad8\u7cfb\u7edf\u8f93\u51fa\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5145\u5206\u53d1\u6325AI\u7cfb\u7edf\u7684\u6f5c\u529b\u5e76\u786e\u4fdd\u5176\u8f93\u51fa\u7684\u6b63\u786e\u6027\uff0c\u9700\u8981\u5c06\u591a\u4e2aAI\u4e0e\u975eAI\u7cfb\u7edf\u65e0\u7f1d\u8026\u5408\u3002", "method": "\u91c7\u7528PDES\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u4ee3\u7406\u89c6\u4e3a\u5b9e\u4f53\uff0c\u901a\u8fc7\u65f6\u95f4\u4f20\u9012\u548c\u56e0\u679c\u89c4\u5219\u534f\u540c\u5de5\u4f5c\uff0c\u975eAI\u4ee3\u7406\u4f5c\u4e3a\u5ba1\u8ba1\u8005\u9a8c\u8bc1AI\u4ee3\u7406\u7684\u884c\u4e3a\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u95ee\u9898\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u51c6\u786e\u7387\u4ece23%\u63d0\u5347\u81f368%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528AI\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408AI\u4e0e\u975eAI\u4ee3\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u6a21\u578b\u5355\u72ec\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.24247", "pdf": "https://arxiv.org/pdf/2505.24247", "abs": "https://arxiv.org/abs/2505.24247", "authors": ["Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "50 Years of Automated Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Over the past 50 years, automated face recognition has evolved from\nrudimentary, handcrafted systems into sophisticated deep learning models that\nrival and often surpass human performance. This paper chronicles the history\nand technological progression of FR, from early geometric and statistical\nmethods to modern deep neural architectures leveraging massive real and\nAI-generated datasets. We examine key innovations that have shaped the field,\nincluding developments in dataset, loss function, neural network design and\nfeature fusion. We also analyze how the scale and diversity of training data\ninfluence model generalization, drawing connections between dataset growth and\nbenchmark improvements. Recent advances have achieved remarkable milestones:\nstate-of-the-art face verification systems now report False Negative\nIdentification Rates of 0.13% against a 12.4 million gallery in NIST FRVT\nevaluations for 1:N visa-to-border matching. While recent advances have enabled\nremarkable accuracy in high- and low-quality face scenarios, numerous\nchallenges persist. While remarkable progress has been achieved, several open\nresearch problems remain. We outline critical challenges and promising\ndirections for future face recognition research, including scalability,\nmulti-modal fusion, synthetic identity generation, and explainable systems.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e8650\u5e74\u6765\u4eba\u8138\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u4ece\u65e9\u671f\u51e0\u4f55\u7edf\u8ba1\u65b9\u6cd5\u5230\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u63a2\u8ba8\u4e86\u5173\u952e\u521b\u65b0\u70b9\u3001\u6570\u636e\u96c6\u5f71\u54cd\u53ca\u672a\u6765\u6311\u6218\u3002", "motivation": "\u603b\u7ed3\u4eba\u8138\u8bc6\u522b\u6280\u672f\u7684\u6f14\u8fdb\u5386\u7a0b\uff0c\u5206\u6790\u5173\u952e\u6280\u672f\u8fdb\u6b65\u53ca\u5176\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u56de\u987e\u548c\u6280\u672f\u5206\u6790\uff0c\u68b3\u7406\u4e86\u4ece\u51e0\u4f55\u7edf\u8ba1\u65b9\u6cd5\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6f14\u53d8\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u6570\u636e\u96c6\u3001\u635f\u5931\u51fd\u6570\u3001\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u548c\u7279\u5f81\u878d\u5408\u7684\u521b\u65b0\u3002", "result": "\u73b0\u4ee3\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728NIST FRVT\u8bc4\u4f30\u4e2d\u8fbe\u52300.13%\u7684\u8bef\u8bc6\u7387\uff0c\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\uff0c\u4f46\u4ecd\u5b58\u5728\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u5c3d\u7ba1\u6280\u672f\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u672a\u6765\u4ecd\u9700\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u591a\u6a21\u6001\u878d\u5408\u3001\u5408\u6210\u8eab\u4efd\u751f\u6210\u548c\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\u3002"}}
{"id": "2505.23848", "pdf": "https://arxiv.org/pdf/2505.23848", "abs": "https://arxiv.org/abs/2505.23848", "authors": ["Harvey Dam", "Jonas Knochelmann", "Vinu Joseph", "Ganesh Gopalakrishnan"], "title": "Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce a method to reduce refusal rates of large language models (LLMs)\non sensitive content without modifying model weights or prompts. Motivated by\nthe observation that refusals in certain models were often preceded by the\nspecific token sequence of a token marking the beginning of the\nchain-of-thought (CoT) block (<think>) followed by a double newline token\n(\\n\\n), we investigate the impact of two simple formatting adjustments during\ngeneration: suppressing \\n\\n after <think> and suppressing the end-of-sequence\ntoken after the end of the CoT block (</think>). Our method requires no\ndatasets, parameter changes, or training, relying solely on modifying token\nprobabilities during generation. In our experiments with official DeepSeek-R1\ndistillations, these interventions increased the proportion of substantive\nanswers to sensitive prompts without affecting performance on standard\nbenchmarks. Our findings suggest that refusal behaviors can be circumvented by\nblocking refusal subspaces at specific points in the generation process.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7279\u5b9a\u6807\u8bb0\u5e8f\u5217\uff0c\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u654f\u611f\u5185\u5bb9\u7684\u62d2\u7edd\u7387\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u6216\u63d0\u793a\u3002", "motivation": "\u89c2\u5bdf\u5230\u67d0\u4e9b\u6a21\u578b\u5728\u62d2\u7edd\u654f\u611f\u5185\u5bb9\u524d\u4f1a\u751f\u6210\u7279\u5b9a\u7684\u6807\u8bb0\u5e8f\u5217\uff08\u5982<think>\u540e\u63a5\u53cc\u6362\u884c\u7b26\uff09\uff0c\u5e0c\u671b\u901a\u8fc7\u7b80\u5355\u8c03\u6574\u8fd9\u4e9b\u6807\u8bb0\u6765\u964d\u4f4e\u62d2\u7edd\u7387\u3002", "method": "\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6291\u5236<think>\u540e\u7684\u53cc\u6362\u884c\u7b26\u548c</think>\u540e\u7684\u5e8f\u5217\u7ed3\u675f\u6807\u8bb0\uff0c\u4ec5\u901a\u8fc7\u4fee\u6539\u6807\u8bb0\u6982\u7387\u5b9e\u73b0\u3002", "result": "\u5728DeepSeek-R1\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u654f\u611f\u63d0\u793a\u7684\u5b9e\u8d28\u6027\u56de\u7b54\u6bd4\u4f8b\uff0c\u4e14\u4e0d\u5f71\u54cd\u6807\u51c6\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "\u62d2\u7edd\u884c\u4e3a\u53ef\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u963b\u65ad\u7279\u5b9a\u6807\u8bb0\u5e8f\u5217\u6765\u89c4\u907f\u3002"}}
{"id": "2505.24249", "pdf": "https://arxiv.org/pdf/2505.24249", "abs": "https://arxiv.org/abs/2505.24249", "authors": ["Qingyao Tian", "Huai Liao", "Xinyan Huang", "Bingyu Yang", "Hongbin Liu"], "title": "Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization", "categories": ["cs.CV"], "comment": null, "summary": "Vision-based 6-DOF bronchoscopy localization offers a promising solution for\naccurate and cost-effective interventional guidance. However, existing methods\nstruggle with 1) limited generalization across patient cases due to scarce\nlabeled data, and 2) poor robustness under visual degradation, as bronchoscopy\nprocedures frequently involve artifacts such as occlusions and motion blur that\nimpair visual information. To address these challenges, we propose PANSv2, a\ngeneralizable and robust bronchoscopy localization framework. Motivated by PANS\nthat leverages multiple visual cues for pose likelihood measurement, PANSv2\nintegrates depth estimation, landmark detection, and centerline constraints\ninto a unified pose optimization framework that evaluates pose probability and\nsolves for the optimal bronchoscope pose. To further enhance generalization\ncapabilities, we leverage the endoscopic foundation model EndoOmni for depth\nestimation and the video foundation model EndoMamba for landmark detection,\nincorporating both spatial and temporal analyses. Pretrained on diverse\nendoscopic datasets, these models provide stable and transferable visual\nrepresentations, enabling reliable performance across varied bronchoscopy\nscenarios. Additionally, to improve robustness to visual degradation, we\nintroduce an automatic re-initialization module that detects tracking failures\nand re-establishes pose using landmark detections once clear views are\navailable. Experimental results on bronchoscopy dataset encompassing 10 patient\ncases show that PANSv2 achieves the highest tracking success rate, with an\n18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)\ncompared to existing methods, showing potential towards real clinical usage.", "AI": {"tldr": "PANSv2\u662f\u4e00\u4e2a\u901a\u7528\u7684\u3001\u9c81\u68d2\u7684\u652f\u6c14\u7ba1\u955c\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6df1\u5ea6\u4f30\u8ba1\u3001\u6807\u5fd7\u70b9\u68c0\u6d4b\u548c\u4e2d\u5fc3\u7ebf\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u89c6\u89c9\u9000\u5316\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u652f\u6c14\u7ba1\u955c\u5b9a\u4f4d\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u5bf9\u89c6\u89c9\u9000\u5316\u654f\u611f\uff0cPANSv2\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "PANSv2\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u3001\u6807\u5fd7\u70b9\u68c0\u6d4b\u548c\u4e2d\u5fc3\u7ebf\u7ea6\u675f\uff0c\u5229\u7528EndoOmni\u548cEndoMamba\u6a21\u578b\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u91cd\u65b0\u521d\u59cb\u5316\u6a21\u5757\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u572810\u4f8b\u60a3\u8005\u6570\u636e\u4e0a\uff0cPANSv2\u7684\u8ddf\u8e2a\u6210\u529f\u7387\u6700\u9ad8\uff0cSR-5\u6307\u6807\u63d0\u534718.1%\u3002", "conclusion": "PANSv2\u5728\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.23851", "pdf": "https://arxiv.org/pdf/2505.23851", "abs": "https://arxiv.org/abs/2505.23851", "authors": ["Michael Shalyt", "Rotem Elimelech", "Ido Kaminer"], "title": "ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Code repository: https://github.com/RamanujanMachine/ASyMOB Complete\n  benchmark dataset:\n  https://huggingface.co/datasets/Shalyt/ASyMOB-Algebraic_Symbolic_Mathematical_Operations_Benchmark", "summary": "Large language models (LLMs) are rapidly approaching the level of proficiency\nin university-level symbolic mathematics required for applications in advanced\nscience and technology. However, existing benchmarks fall short in assessing\nthe core skills of LLMs in symbolic mathematics-such as integration,\ndifferential equations, and algebraic simplification. To address this gap, we\nintroduce ASyMOB, a novel assessment framework focused exclusively on symbolic\nmanipulation, featuring 17,092 unique math challenges, organized by similarity\nand complexity. ASyMOB enables analysis of LLM generalization capabilities by\ncomparing performance in problems that differ by simple numerical or symbolic\n`perturbations'. Evaluated LLMs exhibit substantial degradation in performance\nfor all perturbation types (up to -70.3%), suggesting reliance on memorized\npatterns rather than deeper understanding of symbolic math, even among models\nachieving high baseline accuracy. Comparing LLM performance to computer algebra\nsystems, we identify examples where they fail while LLMs succeed, as well as\nproblems solved only by combining both approaches. Models capable of integrated\ncode execution yielded higher accuracy compared to their performance without\ncode, particularly stabilizing weaker models (up to +33.1% for certain\nperturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5\nFlash) demonstrate not only high symbolic math proficiency (scoring 96.8% and\n97.6% on the unperturbed set), but also remarkable robustness against\nperturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).\nThis may indicate a recent \"phase transition\" in the generalization\ncapabilities of frontier LLMs. It remains to be seen whether the path forward\nlies in deeper integration with sophisticated external tools, or in developing\nmodels so capable that symbolic math systems like CAS become unnecessary.", "AI": {"tldr": "ASyMOB\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u7b26\u53f7\u6570\u5b66\u8bc4\u4f30\u7684\u65b0\u6846\u67b6\uff0c\u63ed\u793a\u4e86LLMs\u5728\u7b26\u53f7\u6570\u5b66\u4e2d\u7684\u8868\u73b0\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u6df1\u5c42\u7406\u89e3\uff0c\u4f46\u9ad8\u7ea7\u6a21\u578b\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLMs\u5728\u7b26\u53f7\u6570\u5b66\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5982\u79ef\u5206\u3001\u5fae\u5206\u65b9\u7a0b\u548c\u4ee3\u6570\u7b80\u5316\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f15\u5165ASyMOB\u6846\u67b6\uff0c\u5305\u542b17,092\u4e2a\u6570\u5b66\u6311\u6218\uff0c\u901a\u8fc7\u6570\u503c\u6216\u7b26\u53f7\u6270\u52a8\u5206\u6790LLMs\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "LLMs\u5728\u6270\u52a8\u95ee\u9898\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff08\u6700\u9ad8-70.3%\uff09\uff0c\u4f46\u9ad8\u7ea7\u6a21\u578b\uff08\u5982o4-mini\u3001Gemini 2.5 Flash\uff09\u8868\u73b0\u66f4\u7a33\u5065\uff08\u4ec5\u4e0b\u964d\u7ea621%\uff09\u3002", "conclusion": "LLMs\u5728\u7b26\u53f7\u6570\u5b66\u4e2d\u4f9d\u8d56\u8bb0\u5fc6\uff0c\u4f46\u9ad8\u7ea7\u6a21\u578b\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u672a\u6765\u53ef\u80fd\u9700\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u6216\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2505.24253", "pdf": "https://arxiv.org/pdf/2505.24253", "abs": "https://arxiv.org/abs/2505.24253", "authors": ["Ishaan Rawal", "Suryansh Kumar"], "title": "Interactive Video Generation via Domain Adaptation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Preprint. Under Review", "summary": "Text-conditioned diffusion models have emerged as powerful tools for\nhigh-quality video generation. However, enabling Interactive Video Generation\n(IVG), where users control motion elements such as object trajectory, remains\nchallenging. Recent training-free approaches introduce attention masking to\nguide trajectory, but this often degrades perceptual quality. We identify two\nkey failure modes in these methods, both of which we interpret as domain shift\nproblems, and propose solutions inspired by domain adaptation. First, we\nattribute the perceptual degradation to internal covariate shift induced by\nattention masking, as pretrained models are not trained to handle masked\nattention. To address this, we propose mask normalization, a pre-normalization\nlayer designed to mitigate this shift via distribution matching. Second, we\naddress initialization gap, where the randomly sampled initial noise does not\nalign with IVG conditioning, by introducing a temporal intrinsic diffusion\nprior that enforces spatio-temporal consistency at each denoising step.\nExtensive qualitative and quantitative evaluations demonstrate that mask\nnormalization and temporal intrinsic denoising improve both perceptual quality\nand trajectory control over the existing state-of-the-art IVG techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u5f52\u4e00\u5316\u548c\u65f6\u5e8f\u5185\u5728\u6269\u6563\u5148\u9a8c\u89e3\u51b3\u73b0\u6709\u6280\u672f\u4e2d\u7684\u611f\u77e5\u8d28\u91cf\u4e0b\u964d\u548c\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u6761\u4ef6\u7684\u6269\u6563\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\uff08IVG\uff09\u4e2d\u9762\u4e34\u8f68\u8ff9\u63a7\u5236\u56f0\u96be\uff0c\u4e14\u73b0\u6709\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\u56e0\u63a9\u7801\u6ce8\u610f\u529b\u5bfc\u81f4\u611f\u77e5\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u5f52\u4e00\u5316\u4ee5\u89e3\u51b3\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u5e76\u5f15\u5165\u65f6\u5e8f\u5185\u5728\u6269\u6563\u5148\u9a8c\u4ee5\u5bf9\u9f50\u521d\u59cb\u566a\u58f0\u4e0eIVG\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u548c\u8f68\u8ff9\u63a7\u5236\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63a9\u7801\u5f52\u4e00\u5316\u548c\u65f6\u5e8f\u5185\u5728\u6269\u6563\u5148\u9a8c\u6709\u6548\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23852", "pdf": "https://arxiv.org/pdf/2505.23852", "abs": "https://arxiv.org/abs/2505.23852", "authors": ["Nic Dobbins", "Christelle Xiong", "Kristine Lan", "Meliha Yetisgen"], "title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease", "categories": ["cs.CL", "cs.AI", "cs.MA", "stat.AP"], "comment": null, "summary": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as\nautonomous agents to reproduce findings of published research studies using the\nsame or similar dataset.\n  Materials and Methods: We used the \"Quick Access\" dataset of the National\nAlzheimer's Coordinating Center (NACC). We identified highly cited published\nresearch manuscripts using NACC data and selected five studies that appeared\nreproducible using this dataset alone. Using GPT-4o, we created a simulated\nresearch team of LLM-based autonomous agents tasked with writing and executing\ncode to dynamically reproduce the findings of each study, given only study\nAbstracts, Methods sections, and data dictionary descriptions of the dataset.\n  Results: We extracted 35 key findings described in the Abstracts across 5\nAlzheimer's studies. On average, LLM agents approximately reproduced 53.2% of\nfindings per study. Numeric values and range-based findings often differed\nbetween studies and agents. The agents also applied statistical methods or\nparameters that varied from the originals, though overall trends and\nsignificance were sometimes similar.\n  Discussion: In some cases, LLM-based agents replicated research techniques\nand findings. In others, they failed due to implementation flaws or missing\nmethodological detail. These discrepancies show the current limits of LLMs in\nfully automating reproducibility assessments. Still, this early investigation\nhighlights the potential of structured agent-based systems to provide scalable\nevaluation of scientific rigor.\n  Conclusion: This exploratory work illustrates both the promise and\nlimitations of LLMs as autonomous agents for automating reproducibility in\nbiomedical research.", "AI": {"tldr": "LLM\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5c1d\u8bd5\u590d\u73b0\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7814\u7a76\u7ed3\u679c\uff0c\u6210\u529f\u590d\u73b0\u7ea653.2%\u7684\u5173\u952e\u53d1\u73b0\uff0c\u4f46\u6570\u503c\u548c\u65b9\u6cd5\u5b58\u5728\u5dee\u5f02\uff0c\u663e\u793a\u4e86\u5176\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22LLM\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u81ea\u52a8\u5316\u590d\u73b0\u7814\u7a76\u7ed3\u679c\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528GPT-4o\u6a21\u62df\u7814\u7a76\u56e2\u961f\uff0c\u57fa\u4e8eNACC\u6570\u636e\u96c6\u548c\u8bba\u6587\u6458\u8981\u3001\u65b9\u6cd5\u90e8\u5206\u53ca\u6570\u636e\u5b57\u5178\uff0c\u52a8\u6001\u590d\u73b0\u4e94\u9879\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7814\u7a76\u7684\u5173\u952e\u53d1\u73b0\u3002", "result": "LLM\u4ee3\u7406\u5e73\u5747\u590d\u73b0\u4e8653.2%\u7684\u5173\u952e\u53d1\u73b0\uff0c\u6570\u503c\u548c\u65b9\u6cd5\u4e0e\u539f\u59cb\u7814\u7a76\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u90e8\u5206\u8d8b\u52bf\u548c\u663e\u8457\u6027\u76f8\u4f3c\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u5316\u590d\u73b0\u7814\u7a76\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u65b9\u6cd5\u7ec6\u8282\u548c\u5b9e\u73b0\u95ee\u9898\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2505.24257", "pdf": "https://arxiv.org/pdf/2505.24257", "abs": "https://arxiv.org/abs/2505.24257", "authors": ["Sahithya Ravi", "Gabriel Sarch", "Vibhav Vineet", "Andrew D. Wilson", "Balasaravanan Thoravi Kumaravel"], "title": "Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames", "categories": ["cs.CV"], "comment": null, "summary": "An embodied AI assistant operating on egocentric video must integrate spatial\ncues across time - for instance, determining where an object A, glimpsed a few\nmoments ago lies relative to an object B encountered later. We introduce\nDisjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs\nby posing questions about object pairs that are not co-visible in the same\nframe. We evaluated seven state-of-the-art VLMs and found that models lag\nbehind human performance by 28%, with steeper declines in accuracy (60% to 30\n%) as the temporal gap widens. Our analysis further reveals that providing\ntrajectories or bird's-eye-view projections to VLMs results in only marginal\nimprovements, whereas providing oracle 3D coordinates leads to a substantial\n20% performance increase. This highlights a core bottleneck of multi-frame VLMs\nin constructing and maintaining 3D scene representations over time from visual\nsignals. Disjoint-3DQA therefore sets a clear, measurable challenge for\nlong-horizon spatial reasoning and aims to catalyze future research at the\nintersection of vision, language, and embodied AI.", "AI": {"tldr": "Disjoint-3DQA\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u975e\u5171\u89c6\u5e27\u4e2d\u5bf9\u7269\u4f53\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u6027\u80fd\u843d\u540e\u4eba\u7c7b28%\uff0c\u4e14\u65f6\u95f4\u95f4\u9694\u589e\u5927\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5177\u8eabAI\u52a9\u624b\u5728\u8fde\u7eed\u89c6\u9891\u4e2d\u6574\u5408\u8de8\u65f6\u95f4\u7a7a\u95f4\u4fe1\u606f\u7684\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faDisjoint-3DQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30VLMs\u5728\u975e\u5171\u89c6\u5e27\u4e2d\u7269\u4f53\u5bf9\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e03\u79cd\u5148\u8fdbVLMs\u3002", "result": "\u6a21\u578b\u6027\u80fd\u843d\u540e\u4eba\u7c7b28%\uff0c\u65f6\u95f4\u95f4\u9694\u589e\u5927\u65f6\u6027\u80fd\u4e0b\u964d\u66f4\u663e\u8457\uff0860%\u523030%\uff09\uff1b\u63d0\u4f9b3D\u5750\u6807\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd20%\u3002", "conclusion": "Disjoint-3DQA\u4e3a\u957f\u671f\u7a7a\u95f4\u63a8\u7406\u8bbe\u5b9a\u4e86\u660e\u786e\u6311\u6218\uff0c\u65e8\u5728\u63a8\u52a8\u89c6\u89c9\u3001\u8bed\u8a00\u4e0e\u5177\u8eabAI\u7684\u4ea4\u53c9\u7814\u7a76\u3002"}}
{"id": "2505.23854", "pdf": "https://arxiv.org/pdf/2505.23854", "abs": "https://arxiv.org/abs/2505.23854", "authors": ["Linwei Tao", "Yi-Fan Yeh", "Minjing Dong", "Tao Huang", "Philip Torr", "Chang Xu"], "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\napplications, robust uncertainty estimation is essential for ensuring the safe\nand trustworthy deployment of LLMs. We present the most comprehensive study to\ndate of uncertainty estimation in LLMs, evaluating 80 models spanning open- and\nclosed-source families, dense and Mixture-of-Experts (MoE) architectures,\nreasoning and non-reasoning modes, quantization variants and parameter scales\nfrom 0.6B to 671B. Focusing on three representative black-box single-pass\nmethods, including token probability-based uncertainty (TPU), numerical verbal\nuncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically\nevaluate uncertainty calibration and selective classification using the\nchallenging MMLU-Pro benchmark, which covers both reasoning-intensive and\nknowledge-based tasks. Our results show that LVU consistently outperforms TPU\nand NVU, offering stronger calibration and discrimination while being more\ninterpretable. We also find that high accuracy does not imply reliable\nuncertainty, and that model scale, post-training, reasoning ability and\nquantization all influence estimation performance. Notably, LLMs exhibit better\nuncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good\ncalibration does not necessarily translate to effective error ranking. These\nfindings highlight the need for multi-perspective evaluation and position LVU\nas a practical tool for improving the reliability of LLMs in real-world\nsettings.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u8bc4\u4f30\u4e8680\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u8bed\u8a00\u4e0d\u786e\u5b9a\u6027\uff08LVU\uff09\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u3001\u8bad\u7ec3\u540e\u5904\u7406\u7b49\u56e0\u7d20\u5f71\u54cd\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLMs\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5bf9\u5176\u5b89\u5168\u53ef\u4fe1\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e8680\u79cd\u6a21\u578b\uff0c\u6db5\u76d6\u591a\u79cd\u67b6\u6784\u548c\u89c4\u6a21\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff08TPU\u3001NVU\u3001LVU\uff09\u5728MMLU-Pro\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "LVU\u5728\u6821\u51c6\u548c\u533a\u5206\u6027\u4e0a\u4f18\u4e8eTPU\u548cNVU\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u3001\u63a8\u7406\u80fd\u529b\u7b49\u56e0\u7d20\u663e\u8457\u5f71\u54cd\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u89d2\u5ea6\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63a8\u8350LVU\u4f5c\u4e3a\u63d0\u5347LLMs\u53ef\u9760\u6027\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.24282", "pdf": "https://arxiv.org/pdf/2505.24282", "abs": "https://arxiv.org/abs/2505.24282", "authors": ["Zirui Shang", "Xinxiao Wu", "Shuo Yang"], "title": "LLM-powered Query Expansion for Enhancing Boundary Prediction in Language-driven Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "Language-driven action localization in videos requires not only semantic\nalignment between language query and video segment, but also prediction of\naction boundaries.\n  However, the language query primarily describes the main content of an action\nand usually lacks specific details of action start and end boundaries, which\nincreases the subjectivity of manual boundary annotation and leads to boundary\nuncertainty in training data.\n  In this paper, on one hand, we propose to expand the original query by\ngenerating textual descriptions of the action start and end boundaries through\nLLMs, which can provide more detailed boundary cues for localization and thus\nreduce the impact of boundary uncertainty.\n  On the other hand, to enhance the tolerance to boundary uncertainty during\ntraining, we propose to model probability scores of action boundaries by\ncalculating the semantic similarities between frames and the expanded query as\nwell as the temporal distances between frames and the annotated boundary\nframes. They can provide more consistent boundary supervision, thus improving\nthe stability of training.\n  Our method is model-agnostic and can be seamlessly and easily integrated into\nany existing models of language-driven action localization in an off-the-shelf\nmanner. Experimental results on several datasets demonstrate the effectiveness\nof our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6269\u5c55\u8bed\u8a00\u67e5\u8be2\u548c\u5efa\u6a21\u8fb9\u754c\u6982\u7387\u5206\u6570\u6765\u51cf\u5c11\u89c6\u9891\u52a8\u4f5c\u5b9a\u4f4d\u4e2d\u8fb9\u754c\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u8bed\u8a00\u67e5\u8be2\u901a\u5e38\u7f3a\u4e4f\u52a8\u4f5c\u8fb9\u754c\u7684\u5177\u4f53\u7ec6\u8282\uff0c\u5bfc\u81f4\u6807\u6ce8\u4e3b\u89c2\u6027\u548c\u8bad\u7ec3\u6570\u636e\u8fb9\u754c\u4e0d\u786e\u5b9a\u6027\u3002", "method": "1. \u4f7f\u7528LLM\u751f\u6210\u52a8\u4f5c\u8fb9\u754c\u7684\u6587\u672c\u63cf\u8ff0\u6269\u5c55\u67e5\u8be2\uff1b2. \u8ba1\u7b97\u5e27\u4e0e\u6269\u5c55\u67e5\u8be2\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u53ca\u4e0e\u6807\u6ce8\u8fb9\u754c\u5e27\u7684\u65f6\u95f4\u8ddd\u79bb\uff0c\u5efa\u6a21\u8fb9\u754c\u6982\u7387\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u9a71\u52a8\u52a8\u4f5c\u5b9a\u4f4d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\uff0c\u51cf\u5c11\u8fb9\u754c\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.23856", "pdf": "https://arxiv.org/pdf/2505.23856", "abs": "https://arxiv.org/abs/2505.23856", "authors": ["Sahil Verma", "Keegan Hines", "Jeff Bilmes", "Charlotte Siska", "Luke Zettlemoyer", "Hila Gonen", "Chandan Singh"], "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard.", "AI": {"tldr": "OMNIGUARD\u662f\u4e00\u79cd\u68c0\u6d4b\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6709\u5bb3\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLM\u7684\u5185\u90e8\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f5c\u5728\u6709\u5bb3\u4f7f\u7528\u5f15\u53d1\u62c5\u5fe7\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u6613\u53d7\u653b\u51fb\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u6216\u975e\u6587\u672c\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u97f3\u9891\uff09\u4e2d\u3002", "method": "OMNIGUARD\u901a\u8fc7\u8bc6\u522bLLM\u5728\u591a\u8bed\u8a00\u6216\u591a\u6a21\u6001\u4e2d\u5bf9\u9f50\u7684\u5185\u90e8\u8868\u793a\uff0c\u6784\u5efa\u8bed\u8a00\u6216\u6a21\u6001\u65e0\u5173\u7684\u5206\u7c7b\u5668\u6765\u68c0\u6d4b\u6709\u5bb3\u63d0\u793a\u3002", "result": "OMNIGUARD\u5728\u591a\u8bed\u8a00\u3001\u56fe\u50cf\u548c\u97f3\u9891\u63d0\u793a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8611.57%\u300120.44%\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff08\u7ea6120\u500d\uff09\u3002", "conclusion": "OMNIGUARD\u4e3a\u68c0\u6d4b\u6709\u5bb3\u63d0\u793a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8de8\u8bed\u8a00\u3001\u8de8\u6a21\u6001\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2505.24287", "pdf": "https://arxiv.org/pdf/2505.24287", "abs": "https://arxiv.org/abs/2505.24287", "authors": ["Ege \u00d6zsoy", "Arda Mamur", "Felix Tristram", "Chantal Pellegrini", "Magdalena Wysocki", "Benjamin Busam", "Nassir Navab"], "title": "EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Operating rooms (ORs) demand precise coordination among surgeons, nurses, and\nequipment in a fast-paced, occlusion-heavy environment, necessitating advanced\nperception models to enhance safety and efficiency. Existing datasets either\nprovide partial egocentric views or sparse exocentric multi-view context, but\ndo not explore the comprehensive combination of both. We introduce EgoExOR, the\nfirst OR dataset and accompanying benchmark to fuse first-person and\nthird-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two\nemulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally\nInvasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand\ntracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D\ncameras, and ultrasound imagery. Its detailed scene graph annotations, covering\n36 entities and 22 relations (568,235 triplets), enable robust modeling of\nclinical interactions, supporting tasks like action recognition and\nhuman-centric perception. We evaluate the surgical scene graph generation\nperformance of two adapted state-of-the-art models and offer a new baseline\nthat explicitly leverages EgoExOR's multimodal and multi-perspective signals.\nThis new dataset and benchmark set a new foundation for OR perception, offering\na rich, multimodal resource for next-generation clinical perception.", "AI": {"tldr": "EgoExOR\u662f\u9996\u4e2a\u7ed3\u5408\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u7684\u624b\u672f\u5ba4\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\u652f\u6301\u4e34\u5e8a\u4ea4\u4e92\u5efa\u6a21\u3002", "motivation": "\u624b\u672f\u5ba4\u9700\u8981\u7cbe\u786e\u534f\u8c03\uff0c\u73b0\u6709\u6570\u636e\u96c6\u89c6\u89d2\u5355\u4e00\uff0c\u65e0\u6cd5\u5168\u9762\u652f\u6301\u4e34\u5e8a\u611f\u77e5\u9700\u6c42\u3002", "method": "\u5f15\u5165EgoExOR\u6570\u636e\u96c6\uff0c\u6574\u5408\u7a7f\u6234\u8bbe\u5907\u548cRGB-D\u76f8\u673a\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u6807\u6ce8\u8be6\u7ec6\u573a\u666f\u56fe\u3002", "result": "\u8bc4\u4f30\u4e24\u79cd\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u51fa\u65b0\u57fa\u7ebf\uff0c\u5c55\u793a\u591a\u6a21\u6001\u548c\u591a\u89c6\u89d2\u4fe1\u53f7\u7684\u4f18\u52bf\u3002", "conclusion": "EgoExOR\u4e3a\u624b\u672f\u5ba4\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\uff0c\u652f\u6301\u4e0b\u4e00\u4ee3\u4e34\u5e8a\u611f\u77e5\u4efb\u52a1\u3002"}}
{"id": "2505.23867", "pdf": "https://arxiv.org/pdf/2505.23867", "abs": "https://arxiv.org/abs/2505.23867", "authors": ["Zeyu Liu", "Zhitian Hou", "Yining Di", "Kejing Yang", "Zhijie Sang", "Congkai Xie", "Jingwen Yang", "Siyuan Liu", "Jialu Wang", "Chunming Li", "Ming Li", "Hongxia Yang"], "title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints.", "AI": {"tldr": "Infi-Med\u662f\u4e00\u4e2a\u533b\u7597\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u3001\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u548c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u533b\u7597\u9886\u57df\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u533b\u7597MLLMs\u5728\u8d44\u6e90\u6548\u7387\u3001\u8bca\u65ad\u51c6\u786e\u6027\u3001\u4e34\u5e8a\u8003\u91cf\u548c\u4f26\u7406\u9690\u79c1\u65b9\u9762\u7684\u5b9e\u9645\u90e8\u7f72\u95ee\u9898\u3002", "method": "\u63d0\u51faInfi-Med\u6846\u67b6\uff0c\u5305\u62ec\u9ad8\u6548SFT\u6570\u636e\u96c6\u6784\u5efa\u3001\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u548c\u7cfb\u7edf\u8bc4\u4f30\u4f53\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInfi-Med\u5728\u533b\u7597\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5feb\u901f\u9002\u5e94\u4e34\u5e8a\u573a\u666f\u3002", "conclusion": "Infi-Med\u4e3a\u533b\u7597MLLMs\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u64cd\u4f5c\u9650\u5236\u7684\u57fa\u7840\u3002"}}
{"id": "2505.24301", "pdf": "https://arxiv.org/pdf/2505.24301", "abs": "https://arxiv.org/abs/2505.24301", "authors": ["Enshang Zhang", "Zhicheng Zhang", "Takashi Hanakawa"], "title": "Category-aware EEG image generation based on wavelet transform and contrast semantic loss", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Reconstructing visual stimuli from EEG signals is a crucial step in realizing\nbrain-computer interfaces. In this paper, we propose a transformer-based EEG\nsignal encoder integrating the Discrete Wavelet Transform (DWT) and the gating\nmechanism. Guided by the feature alignment and category-aware fusion losses,\nthis encoder is used to extract features related to visual stimuli from EEG\nsignals. Subsequently, with the aid of a pre-trained diffusion model, these\nfeatures are reconstructed into visual stimuli. To verify the effectiveness of\nthe model, we conducted EEG-to-image generation and classification tasks using\nthe THINGS-EEG dataset. To address the limitations of quantitative analysis at\nthe semantic level, we combined WordNet-based classification and semantic\nsimilarity metrics to propose a novel semantic-based score, emphasizing the\nability of our model to transfer neural activities into visual representations.\nExperimental results show that our model significantly improves semantic\nalignment and classification accuracy, which achieves a maximum single-subject\naccuracy of 43\\%, outperforming other state-of-the-art methods. The source code\nand supplementary material is available at\nhttps://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684EEG\u4fe1\u53f7\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548c\u95e8\u63a7\u673a\u5236\uff0c\u7528\u4e8e\u4eceEEG\u4fe1\u53f7\u4e2d\u63d0\u53d6\u89c6\u89c9\u523a\u6fc0\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u91cd\u5efa\u4e3a\u89c6\u89c9\u523a\u6fc0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u73b0\u8111\u673a\u63a5\u53e3\u7684\u5173\u952e\u6b65\u9aa4\u662f\u4eceEEG\u4fe1\u53f7\u4e2d\u91cd\u5efa\u89c6\u89c9\u523a\u6fc0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5c42\u9762\u7684\u5b9a\u91cf\u5206\u6790\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DWT\u548c\u95e8\u63a7\u673a\u5236\u7684Transformer\u7f16\u7801\u5668\uff0c\u5229\u7528\u7279\u5f81\u5bf9\u9f50\u548c\u7c7b\u522b\u611f\u77e5\u878d\u5408\u635f\u5931\u63d0\u53d6EEG\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u91cd\u5efa\u89c6\u89c9\u523a\u6fc0\u3002", "result": "\u5728THINGS-EEG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6a21\u578b\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5355\u88ab\u8bd5\u6700\u9ad8\u51c6\u786e\u7387\u8fbe43%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728EEG\u4fe1\u53f7\u5230\u89c6\u89c9\u523a\u6fc0\u7684\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23911", "pdf": "https://arxiv.org/pdf/2505.23911", "abs": "https://arxiv.org/abs/2505.23911", "authors": ["Pavel Tikhonov", "Ivan Oseledets", "Elena Tutubalina"], "title": "One Task Vector is not Enough: A Large-Scale Study for In-Context Learning", "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to adapt to\nnew tasks using few examples, with task vectors - specific hidden state\nactivations - hypothesized to encode task information. Existing studies are\nlimited by small-scale benchmarks, restricting comprehensive analysis. We\nintroduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with\n30 input-output pairs derived from the Alpaca dataset. Experiments with\nLlama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an\nintermediate layer (e.g., 15th), (2) effectiveness varies significantly by task\ntype, and (3) complex tasks rely on multiple, subtask-specific vectors rather\nthan a single vector, suggesting distributed task knowledge representation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86QuiteAFew\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u6790LLM\u5728\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u7684\u4efb\u52a1\u5411\u91cf\u8868\u73b0\uff0c\u53d1\u73b0\u4efb\u52a1\u5411\u91cf\u6027\u80fd\u5728\u4e2d\u95f4\u5c42\u6700\u4f73\uff0c\u4e14\u590d\u6742\u4efb\u52a1\u4f9d\u8d56\u591a\u4e2a\u5b50\u4efb\u52a1\u5411\u91cf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u57fa\u51c6\uff0c\u65e0\u6cd5\u5168\u9762\u5206\u6790\u4efb\u52a1\u5411\u91cf\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528Llama-3-8B\u6a21\u578b\u57283,096\u4e2a\u591a\u6837\u5316\u5c11\u6837\u672c\u4efb\u52a1\uff08\u6765\u81eaAlpaca\u6570\u636e\u96c6\uff09\u4e0a\u5b9e\u9a8c\uff0c\u5206\u6790\u4efb\u52a1\u5411\u91cf\u7684\u8868\u73b0\u3002", "result": "\u4efb\u52a1\u5411\u91cf\u6027\u80fd\u5728\u4e2d\u95f4\u5c42\uff08\u5982\u7b2c15\u5c42\uff09\u8fbe\u5230\u5cf0\u503c\uff0c\u4e14\u590d\u6742\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u5b50\u4efb\u52a1\u5411\u91cf\u3002", "conclusion": "\u4efb\u52a1\u77e5\u8bc6\u5728LLM\u4e2d\u53ef\u80fd\u662f\u5206\u5e03\u5f0f\u8868\u793a\u7684\uff0c\u800c\u975e\u5355\u4e00\u5411\u91cf\u3002"}}
{"id": "2505.24310", "pdf": "https://arxiv.org/pdf/2505.24310", "abs": "https://arxiv.org/abs/2505.24310", "authors": ["Jiayan Li", "Jun Li", "Zhourui Zhang", "Jianhua Xu"], "title": "Progressive Class-level Distillation", "categories": ["cs.CV"], "comment": null, "summary": "In knowledge distillation (KD), logit distillation (LD) aims to transfer\nclass-level knowledge from a more powerful teacher network to a small student\nmodel via accurate teacher-student alignment at the logits level. Since\nhigh-confidence object classes usually dominate the distillation process,\nlow-probability classes which also contain discriminating information are\ndownplayed in conventional methods, leading to insufficient knowledge transfer.\nTo address this issue, we propose a simple yet effective LD method termed\nProgressive Class-level Distillation (PCD). In contrast to existing methods\nwhich perform all-class ensemble distillation, our PCD approach performs\nstage-wise distillation for step-by-step knowledge transfer. More specifically,\nwe perform ranking on teacher-student logits difference for identifying\ndistillation priority from scratch, and subsequently divide the entire LD\nprocess into multiple stages. Next, bidirectional stage-wise distillation\nincorporating fine-to-coarse progressive learning and reverse coarse-to-fine\nrefinement is conducted, allowing comprehensive knowledge transfer via\nsufficient logits alignment within separate class groups in different\ndistillation stages. Extension experiments on public benchmarking datasets\ndemonstrate the superiority of our method compared to state-of-the-arts for\nboth classification and detection tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6e10\u8fdb\u5f0f\u7c7b\u522b\u7ea7\u84b8\u998f\uff08PCD\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u84b8\u998f\u89e3\u51b3\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u4e2d\u4f4e\u6982\u7387\u7c7b\u522b\u4fe1\u606f\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e2d\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u7c7b\u522b\u4e3b\u5bfc\u84b8\u998f\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u4f4e\u6982\u7387\u7c7b\u522b\u7684\u533a\u5206\u4fe1\u606f\u88ab\u5ffd\u7565\uff0c\u77e5\u8bc6\u4f20\u9012\u4e0d\u5145\u5206\u3002", "method": "PCD\u65b9\u6cd5\u901a\u8fc7\u5206\u9636\u6bb5\u84b8\u998f\uff0c\u9996\u5148\u6839\u636e\u6559\u5e08-\u5b66\u751flogits\u5dee\u5f02\u6392\u5e8f\u786e\u5b9a\u4f18\u5148\u7ea7\uff0c\u7136\u540e\u5206\u9636\u6bb5\u8fdb\u884c\u53cc\u5411\u84b8\u998f\uff08\u4ece\u7ec6\u5230\u7c97\u548c\u4ece\u7c97\u5230\u7ec6\uff09\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCD\u5728\u5206\u7c7b\u548c\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PCD\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5206\u9636\u6bb5\u84b8\u998f\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u77e5\u8bc6\u4f20\u9012\uff0c\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23912", "pdf": "https://arxiv.org/pdf/2505.23912", "abs": "https://arxiv.org/abs/2505.23912", "authors": ["Caiqi Zhang", "Xiaochen Zhu", "Chengzu Li", "Nigel Collier", "Andreas Vlachos"], "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination remains a major challenge for the safe and trustworthy\ndeployment of large language models (LLMs) in factual content generation. Prior\nwork has explored confidence estimation as an effective approach to\nhallucination detection, but often relies on post-hoc self-consistency methods\nthat require computationally expensive sampling. Verbalized confidence offers a\nmore efficient alternative, but existing approaches are largely limited to\nshort-form question answering (QA) tasks and do not generalize well to\nopen-ended generation. In this paper, we propose LoVeC (Long-form Verbalized\nConfidence), an on-the-fly verbalized confidence estimation method for\nlong-form generation. Specifically, we use reinforcement learning (RL) to train\nLLMs to append numerical confidence scores to each generated statement, serving\nas a direct and interpretable signal of the factuality of generation. Our\nexperiments consider both on-policy and off-policy RL methods, including DPO,\nORPO, and GRPO, to enhance the model calibration. We introduce two novel\nevaluation settings, free-form tagging and iterative tagging, to assess\ndifferent verbalized confidence estimation methods. Experiments on three\nlong-form QA datasets show that our RL-trained models achieve better\ncalibration and generalize robustly across domains. Also, our method is highly\nefficient, as it only requires adding a few tokens to the output being decoded.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoVeC\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5185\u5bb9\u65f6\u9644\u52a0\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u4ee5\u9ad8\u6548\u68c0\u6d4b\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e8b\u5b9e\u5185\u5bb9\u65f6\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u63a8\u5e7f\u5230\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08\u5305\u62ecDPO\u3001ORPO\u548cGRPO\uff09\u8bad\u7ec3\u6a21\u578b\u4e3a\u6bcf\u4e2a\u751f\u6210\u8bed\u53e5\u9644\u52a0\u6570\u503c\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u957f\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cRL\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6821\u51c6\u6027\u548c\u8de8\u9886\u57df\u9c81\u68d2\u6027\u3002", "conclusion": "LoVeC\u65b9\u6cd5\u9ad8\u6548\u4e14\u76f4\u63a5\uff0c\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u6807\u8bb0\u5373\u53ef\u5b9e\u73b0\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2505.24315", "pdf": "https://arxiv.org/pdf/2505.24315", "abs": "https://arxiv.org/abs/2505.24315", "authors": ["Jinlu Zhang", "Yixin Chen", "Zan Wang", "Jie Yang", "Yizhou Wang", "Siyuan Huang"], "title": "InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent advances in 3D human-aware generation have made significant progress.\nHowever, existing methods still struggle with generating novel Human Object\nInteraction (HOI) from text, particularly for open-set objects. We identify\nthree main challenges of this task: precise human-object relation reasoning,\naffordance parsing for any object, and detailed human interaction pose\nsynthesis aligning description and object geometry. In this work, we propose a\nnovel zero-shot 3D HOI generation framework without training on specific\ndatasets, leveraging the knowledge from large-scale pre-trained models.\nSpecifically, the human-object relations are inferred from large language\nmodels (LLMs) to initialize object properties and guide the optimization\nprocess. Then we utilize a pre-trained 2D image diffusion model to parse unseen\nobjects and extract contact points, avoiding the limitations imposed by\nexisting 3D asset knowledge. The initial human pose is generated by sampling\nmultiple hypotheses through multi-view SDS based on the input text and object\ngeometry. Finally, we introduce a detailed optimization to generate\nfine-grained, precise, and natural interaction, enforcing realistic 3D contact\nbetween the 3D object and the involved body parts, including hands in grasping.\nThis is achieved by distilling human-level feedback from LLMs to capture\ndetailed human-object relations from the text instruction. Extensive\nexperiments validate the effectiveness of our approach compared to prior works,\nparticularly in terms of the fine-grained nature of interactions and the\nability to handle open-set 3D objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c3D\u4eba-\u7269\u4ea4\u4e92\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u77e5\u8bc6\u89e3\u51b3\u5f00\u653e\u96c6\u5bf9\u8c61\u7684\u4ea4\u4e92\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u6587\u672c\u751f\u6210\u65b0\u9896\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5f00\u653e\u96c6\u5bf9\u8c61\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u4eba-\u7269\u5173\u7cfb\uff0c\u9884\u8bad\u7ec32D\u6269\u6563\u6a21\u578b\u89e3\u6790\u5bf9\u8c61\uff0c\u591a\u89c6\u89d2SDS\u751f\u6210\u521d\u59cb\u59ff\u6001\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u7cbe\u7ec6\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4ea4\u4e92\u7ec6\u8282\u548c\u5f00\u653e\u96c6\u5bf9\u8c61\u5904\u7406\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u96c63D\u5bf9\u8c61\u4ea4\u4e92\u751f\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2505.23914", "pdf": "https://arxiv.org/pdf/2505.23914", "abs": "https://arxiv.org/abs/2505.23914", "authors": ["Yuxin Wang", "Botao Yu", "Ivory Yang", "Saeed Hassanpour", "Soroush Vosoughi"], "title": "Probing Association Biases in LLM Moderation Over-Sensitivity", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large Language Models are widely used for content moderation but often\nmisclassify benign comments as toxic, leading to over-sensitivity. While\nprevious research attributes this issue primarily to the presence of offensive\nterms, we reveal a potential cause beyond token level: LLMs exhibit systematic\ntopic biases in their implicit associations. Inspired by cognitive psychology's\nimplicit association tests, we introduce Topic Association Analysis, a\nsemantic-level approach to quantify how LLMs associate certain topics with\ntoxicity. By prompting LLMs to generate free-form scenario imagination for\nmisclassified benign comments and analyzing their topic amplification levels,\nwe find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger\ntopic stereotype despite lower overall false positive rates. These biases\nsuggest that LLMs do not merely react to explicit, offensive language but rely\non learned topic associations, shaping their moderation decisions. Our findings\nhighlight the need for refinement beyond keyword-based filtering, providing\ninsights into the underlying mechanisms driving LLM over-sensitivity.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4e3b\u9898\u5173\u8054\u5206\u6790\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u4e3b\u9898\u504f\u89c1\uff0c\u8868\u660e\u5176\u4e0d\u4ec5\u5bf9\u663e\u6027\u5192\u72af\u6027\u8bed\u8a00\u654f\u611f\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u5b66\u4e60\u5230\u7684\u4e3b\u9898\u5173\u8054\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u8bef\u5224\u826f\u6027\u8bc4\u8bba\u4e3a\u6709\u6bd2\u5185\u5bb9\u7684\u6f5c\u5728\u539f\u56e0\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u8bcd\u6c47\u5c42\u9762\u5206\u6790\u3002", "method": "\u5f15\u5165\u4e3b\u9898\u5173\u8054\u5206\u6790\uff08Topic Association Analysis\uff09\uff0c\u901a\u8fc7\u8ba9LLM\u751f\u6210\u81ea\u7531\u5f62\u5f0f\u7684\u573a\u666f\u60f3\u8c61\uff0c\u91cf\u5316\u5176\u5c06\u7279\u5b9a\u4e3b\u9898\u4e0e\u6bd2\u6027\u5173\u8054\u7684\u7a0b\u5ea6\u3002", "result": "\u53d1\u73b0\u66f4\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5982GPT-4 Turbo\uff09\u5c3d\u7ba1\u603b\u4f53\u8bef\u62a5\u7387\u8f83\u4f4e\uff0c\u4f46\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u4e3b\u9898\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u6539\u8fdbLLM\u7684\u5185\u5bb9\u5ba1\u6838\u673a\u5236\u3002"}}
{"id": "2505.24327", "pdf": "https://arxiv.org/pdf/2505.24327", "abs": "https://arxiv.org/abs/2505.24327", "authors": ["Jingjing Liu", "Jiashun Jin", "Xianchao Xiu", "Jianhua Zhang", "Wanquan Liu"], "title": "STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Remote sensing image (RSI) denoising is an important topic in the field of\nremote sensing. Despite the impressive denoising performance of RSI denoising\nmethods, most current deep learning-based approaches function as black boxes\nand lack integration with physical information models, leading to limited\ninterpretability. Additionally, many methods may struggle with insufficient\nattention to non-local self-similarity in RSI and require tedious tuning of\nregularization parameters to achieve optimal performance, particularly in\nconventional iterative optimization approaches. In this paper, we first propose\na novel RSI denoising method named sparse tensor-aided representation network\n(STAR-Net), which leverages a low-rank prior to effectively capture the\nnon-local self-similarity within RSI. Furthermore, we extend STAR-Net to a\nsparse variant called STAR-Net-S to deal with the interference caused by\nnon-Gaussian noise in original RSI for the purpose of improving robustness.\nDifferent from conventional iterative optimization, we develop an alternating\ndirection method of multipliers (ADMM)-guided deep unrolling network, in which\nall regularization parameters can be automatically learned, thus inheriting the\nadvantages of both model-based and deep learning-based approaches and\nsuccessfully addressing the above-mentioned shortcomings. Comprehensive\nexperiments on synthetic and real-world datasets demonstrate that STAR-Net and\nSTAR-Net-S outperform state-of-the-art RSI denoising methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTAR-Net\u7684\u65b0\u578b\u9065\u611f\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f4e\u79e9\u5148\u9a8c\u548c\u975e\u5c40\u90e8\u81ea\u76f8\u4f3c\u6027\uff0c\u5e76\u901a\u8fc7ADMM\u5f15\u5bfc\u7684\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\u81ea\u52a8\u5b66\u4e60\u53c2\u6570\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u6a21\u578b\u96c6\u6210\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u5bf9\u975e\u5c40\u90e8\u81ea\u76f8\u4f3c\u6027\u5173\u6ce8\u4e0d\u8db3\uff0c\u53c2\u6570\u8c03\u4f18\u7e41\u7410\u3002", "method": "\u63d0\u51faSTAR-Net\u53ca\u5176\u7a00\u758f\u53d8\u4f53STAR-Net-S\uff0c\u5229\u7528\u4f4e\u79e9\u5148\u9a8c\u6355\u83b7\u975e\u5c40\u90e8\u81ea\u76f8\u4f3c\u6027\uff0c\u5e76\u901a\u8fc7ADMM\u5f15\u5bfc\u7684\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\u81ea\u52a8\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cSTAR-Net\u548cSTAR-Net-S\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STAR-Net\u6210\u529f\u7ed3\u5408\u4e86\u6a21\u578b\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.23923", "pdf": "https://arxiv.org/pdf/2505.23923", "abs": "https://arxiv.org/abs/2505.23923", "authors": ["Feiteng Fang", "Ting-En Lin", "Yuchuan Wu", "Xiong Liu", "Xiang Huang", "Dingwei Chen", "Jing Ye", "Haonan Zhang", "Liang Zhu", "Hamid Alinejad-Rokny", "Min Yang", "Fei Huang", "Yongbin Li"], "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faChARM\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u5956\u52b1\u673a\u5236\u548c\u81ea\u8fdb\u5316\u673a\u5236\u89e3\u51b3\u89d2\u8272\u626e\u6f14\u8bed\u8a00\u4ee3\u7406\u7684\u504f\u597d\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u5728\u89d2\u8272\u626e\u6f14\u8bed\u8a00\u4ee3\u7406\uff08RPLAs\uff09\u4e2d\u96be\u4ee5\u9002\u5e94\u4e3b\u89c2\u5bf9\u8bdd\u504f\u597d\u4e14\u6269\u5c55\u6027\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faChARM\u6a21\u578b\uff0c\u5305\u542b\u52a8\u6001\u5956\u52b1\u673a\u5236\u548c\u81ea\u8fdb\u5316\u673a\u5236\uff0c\u5e76\u5f15\u5165RoleplayPref\u6570\u636e\u96c6\u548cRoleplayEval\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aChARM\u5728\u504f\u597d\u6392\u540d\u4e0a\u6bd4\u4f20\u7edf\u6a21\u578b\u63d0\u534713%\uff0c\u5e76\u5728\u504f\u597d\u5b66\u4e60\u6280\u672f\u4e0a\u8fbe\u5230\u6700\u4f18\u6548\u679c\u3002", "conclusion": "ChARM\u901a\u8fc7\u521b\u65b0\u673a\u5236\u548c\u65b0\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86RPLAs\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.24329", "pdf": "https://arxiv.org/pdf/2505.24329", "abs": "https://arxiv.org/abs/2505.24329", "authors": ["Yingsen Zeng", "Zepeng Huang", "Yujie Zhong", "Chengjian Feng", "Jie Hu", "Lin Ma", "Yang Liu"], "title": "DisTime: Distribution-based Time Representation for Video Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Despite advances in general video understanding, Video Large Language Models\n(Video-LLMs) face challenges in precise temporal localization due to discrete\ntime representations and limited temporally aware datasets. Existing methods\nfor temporal expression either conflate time with text-based numerical values,\nadd a series of dedicated temporal tokens, or regress time using specialized\ntemporal grounding heads. To address these issues, we introduce DisTime, a\nlightweight framework designed to enhance temporal comprehension in Video-LLMs.\nDisTime employs a learnable token to create a continuous temporal embedding\nspace and incorporates a Distribution-based Time Decoder that generates\ntemporal probability distributions, effectively mitigating boundary ambiguities\nand maintaining temporal continuity. Additionally, the Distribution-based Time\nEncoder re-encodes timestamps to provide time markers for Video-LLMs. To\novercome temporal granularity limitations in existing datasets, we propose an\nautomated annotation paradigm that combines the captioning capabilities of\nVideo-LLMs with the localization expertise of dedicated temporal models. This\nleads to the creation of InternVid-TG, a substantial dataset with 1.25M\ntemporally grounded events across 179k videos, surpassing ActivityNet-Caption\nby 55 times. Extensive experiments demonstrate that DisTime achieves\nstate-of-the-art performance across benchmarks in three time-sensitive tasks\nwhile maintaining competitive performance in Video QA tasks. Code and data are\nreleased at https://github.com/josephzpng/DisTime.", "AI": {"tldr": "DisTime\u6846\u67b6\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u5d4c\u5165\u548c\u5206\u5e03\u89e3\u7801\u5668\u63d0\u5347Video-LLMs\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6InternVid-TG\u3002", "motivation": "\u73b0\u6709Video-LLMs\u5728\u65f6\u95f4\u5b9a\u4f4d\u4e0a\u5b58\u5728\u79bb\u6563\u8868\u793a\u548c\u6570\u636e\u96c6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "DisTime\u91c7\u7528\u53ef\u5b66\u4e60token\u548c\u5206\u5e03\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u6807\u6ce8\u6784\u5efaInternVid-TG\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e09\u4e2a\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\uff0c\u540c\u65f6\u5728Video QA\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "DisTime\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u96c6\u6269\u5c55\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.23931", "pdf": "https://arxiv.org/pdf/2505.23931", "abs": "https://arxiv.org/abs/2505.23931", "authors": ["Daniel Wurgaft", "Ben Prystawski", "Kanishk Gandhi", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Noah D. Goodman"], "title": "Scaling up the think-aloud method", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures. Daniel Wurgaft and Ben Prystawski contributed\n  equally", "summary": "The think-aloud method, where participants voice their thoughts as they solve\na task, is a valuable source of rich data about human reasoning processes. Yet,\nit has declined in popularity in contemporary cognitive science, largely\nbecause labor-intensive transcription and annotation preclude large sample\nsizes. Here, we develop methods to automate the transcription and annotation of\nverbal reports of reasoning using natural language processing tools, allowing\nfor large-scale analysis of think-aloud data. In our study, 640 participants\nthought aloud while playing the Game of 24, a mathematical reasoning task. We\nautomatically transcribed the recordings and coded the transcripts as search\ngraphs, finding moderate inter-rater reliability with humans. We analyze these\ngraphs and characterize consistency and variation in human reasoning traces.\nOur work demonstrates the value of think-aloud data at scale and serves as a\nproof of concept for the automated analysis of verbal reports.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u81ea\u52a8\u5316\u8f6c\u5f55\u548c\u6807\u6ce8\u53e3\u5934\u62a5\u544a\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5206\u6790\u5927\u89c4\u6a21\u601d\u8003\u51fa\u58f0\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u4ef7\u503c\u3002", "motivation": "\u601d\u8003\u51fa\u58f0\u65b9\u6cd5\u56e0\u8f6c\u5f55\u548c\u6807\u6ce8\u5de5\u4f5c\u91cf\u5927\u800c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u963b\u788d\u4e86\u5176\u5728\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u8f6c\u5f55\u548c\u6807\u6ce8\u5de5\u5177\uff0c\u5206\u6790640\u540d\u53c2\u4e0e\u8005\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u601d\u8003\u51fa\u58f0\u6570\u636e\uff0c\u5c06\u5176\u7f16\u7801\u4e3a\u641c\u7d22\u56fe\u3002", "result": "\u81ea\u52a8\u5316\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u53ef\u9760\u6027\u4e2d\u7b49\uff0c\u6210\u529f\u5206\u6790\u4e86\u4eba\u7c7b\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u53d8\u5f02\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u601d\u8003\u51fa\u58f0\u6570\u636e\u7684\u4ef7\u503c\uff0c\u4e3a\u81ea\u52a8\u5316\u5206\u6790\u53e3\u5934\u62a5\u544a\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\u3002"}}
{"id": "2505.24334", "pdf": "https://arxiv.org/pdf/2505.24334", "abs": "https://arxiv.org/abs/2505.24334", "authors": ["Uzair Khan", "Franco Fummi", "Luigi Capogrosso"], "title": "KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices", "categories": ["cs.CV"], "comment": "Accepted at the 23rd International Conference on Image Analysis and\n  Processing (ICIAP 2025)", "summary": "In the era of intelligent manufacturing, anomaly detection has become\nessential for maintaining quality control on modern production lines. However,\nwhile many existing models show promising performance, they are often too\nlarge, computationally demanding, and impractical to deploy on\nresource-constrained embedded devices that can be easily installed on the\nproduction lines of Small and Medium Enterprises (SMEs). To bridge this gap, we\npresent KairosAD, a novel supervised approach that uses the power of the Mobile\nSegment Anything Model (MobileSAM) for image-based anomaly detection. KairosAD\nhas been evaluated on the two well-known industrial anomaly detection datasets,\ni.e., MVTec-AD and ViSA. The results show that KairosAD requires 78% fewer\nparameters and boasts a 4x faster inference time compared to the leading\nstate-of-the-art model, while maintaining comparable AUROC performance. We\ndeployed KairosAD on two embedded devices, the NVIDIA Jetson NX, and the NVIDIA\nJetson AGX. Finally, KairosAD was successfully installed and tested on the real\nproduction line of the Industrial Computer Engineering Laboratory (ICE Lab) at\nthe University of Verona. The code is available at\nhttps://github.com/intelligolabs/KairosAD.", "AI": {"tldr": "KairosAD\u662f\u4e00\u79cd\u57fa\u4e8eMobileSAM\u7684\u65b0\u578b\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\uff0c\u53c2\u6570\u51cf\u5c1178%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb4\u500d\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684SME\u751f\u4ea7\u7ebf\u4e0a\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528MobileSAM\u8fdb\u884c\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u5728MVTec-AD\u548cViSA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u53c2\u6570\u51cf\u5c1178%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb4\u500d\uff0cAUROC\u6027\u80fd\u76f8\u5f53\uff0c\u6210\u529f\u90e8\u7f72\u4e8eNVIDIA Jetson\u8bbe\u5907\u53ca\u5b9e\u9645\u751f\u4ea7\u7ebf\u3002", "conclusion": "KairosAD\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23932", "pdf": "https://arxiv.org/pdf/2505.23932", "abs": "https://arxiv.org/abs/2505.23932", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "categories": ["cs.CL"], "comment": null, "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io", "AI": {"tldr": "SwingArena\u662f\u4e00\u4e2a\u7ade\u4e89\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u6a21\u62df\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff0c\u901a\u8fc7\u63d0\u4ea4\u8005\u548c\u8bc4\u5ba1\u8005\u7684\u4e92\u52a8\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u534f\u4f5c\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u8d34\u8fd1\u5b9e\u9645\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "SwingArena\u7ed3\u5408\u63d0\u4ea4\u8005\uff08\u751f\u6210\u8865\u4e01\uff09\u548c\u8bc4\u5ba1\u8005\uff08\u521b\u5efa\u6d4b\u8bd5\u7528\u4f8b\u5e76\u901a\u8fc7CI\u9a8c\u8bc1\uff09\u7684\u89d2\u8272\uff0c\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210\uff08RACG\uff09\u6a21\u5757\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cGPT-4o\u64c5\u957f\u751f\u6210\u6fc0\u8fdb\u8865\u4e01\uff0c\u800cDeepSeek\u548cGemini\u66f4\u6ce8\u91cdCI\u9a8c\u8bc1\u7684\u6b63\u786e\u6027\u3002", "conclusion": "SwingArena\u4e3aLLMs\u5728\u771f\u5b9eCI\u9a71\u52a8\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.24340", "pdf": "https://arxiv.org/pdf/2505.24340", "abs": "https://arxiv.org/abs/2505.24340", "authors": ["Gilles Quentin Hacheme", "Girmaw Abebe Tadesse", "Caleb Robinson", "Akram Zaytar", "Rahul Dodhia", "Juan M. Lavista Ferres"], "title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.10; I.2.7; I.4.8; I.5.3"], "comment": null, "summary": "Classifying geospatial imagery remains a major bottleneck for applications\nsuch as disaster response and land-use monitoring-particularly in regions where\nannotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that\nclaim zero-shot classification capabilities for satellite imagery nonetheless\nrely on task-specific pretraining and adaptation to reach competitive\nperformance. We introduce GeoVision Labeler (GVL), a strictly zero-shot\nclassification framework: a vision Large Language Model (vLLM) generates rich,\nhuman-readable image descriptions, which are then mapped to user-defined\nclasses by a conventional Large Language Model (LLM). This modular, and\ninterpretable pipeline enables flexible image classification for a large range\nof use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,\nand RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary\nBuildings vs. No Buildings task on SpaceNet v7. For complex multi-class\nclassification tasks (UC Merced, RESISC45), we implemented a recursive\nLLM-driven clustering to form meta-classes at successive depths, followed by\nhierarchical classification-first resolving coarse groups, then finer\ndistinctions-to deliver competitive zero-shot performance. GVL is open-sourced\nat https://github.com/microsoft/geo-vision-labeler to catalyze adoption in\nreal-world geospatial workflows.", "AI": {"tldr": "GVL\u662f\u4e00\u4e2a\u4e25\u683c\u96f6\u6837\u672c\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u518d\u901a\u8fc7\u5e38\u89c4\u5927\u8bed\u8a00\u6a21\u578b\u6620\u5c04\u5230\u7528\u6237\u5b9a\u4e49\u7c7b\u522b\uff0c\u5b9e\u73b0\u7075\u6d3b\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u5730\u7406\u7a7a\u95f4\u56fe\u50cf\u5206\u7c7b\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u5206\u7c7b\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u518d\u901a\u8fc7\u5e38\u89c4\u5927\u8bed\u8a00\u6a21\u578b\u6620\u5c04\u5230\u7528\u6237\u5b9a\u4e49\u7c7b\u522b\uff1b\u590d\u6742\u4efb\u52a1\u91c7\u7528\u9012\u5f52LLM\u9a71\u52a8\u7684\u805a\u7c7b\u548c\u5c42\u6b21\u5206\u7c7b\u3002", "result": "\u5728SpaceNet v7\u4e0a\u8fbe\u523093.2%\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\uff1b\u5728\u590d\u6742\u591a\u7c7b\u4efb\u52a1\u4e2d\u901a\u8fc7\u5c42\u6b21\u5206\u7c7b\u5b9e\u73b0\u7ade\u4e89\u6027\u8868\u73b0\u3002", "conclusion": "GVL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5730\u7406\u7a7a\u95f4\u5e94\u7528\uff0c\u5e76\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.23944", "pdf": "https://arxiv.org/pdf/2505.23944", "abs": "https://arxiv.org/abs/2505.23944", "authors": ["Thushara Manjari Naduvilakandy", "Hyeju Jang", "Mohammad Al Hasan"], "title": "Retrieval Augmented Generation based Large Language Models for Causality Mining", "categories": ["cs.CL"], "comment": "13 pages, 6 figures, published in knowledgeNLP-NAACL2025", "summary": "Causality detection and mining are important tasks in information retrieval\ndue to their enormous use in information extraction, and knowledge graph\nconstruction. To solve these tasks, in existing literature there exist several\nsolutions -- both unsupervised and supervised. However, the unsupervised\nmethods suffer from poor performance and they often require significant human\nintervention for causal rule selection, leading to poor generalization across\ndifferent domains. On the other hand, supervised methods suffer from the lack\nof large training datasets. Recently, large language models (LLMs) with\neffective prompt engineering are found to be effective to overcome the issue of\nunavailability of large training dataset. Yet, in existing literature, there\ndoes not exist comprehensive works on causality detection and mining using LLM\nprompting. In this paper, we present several retrieval-augmented generation\n(RAG) based dynamic prompting schemes to enhance LLM performance in causality\ndetection and extraction tasks. Extensive experiments over three datasets and\nfive LLMs validate the superiority of our proposed RAG-based dynamic prompting\nover other static prompting schemes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u52a8\u6001\u63d0\u793a\u65b9\u6848\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u56e0\u679c\u5173\u7cfb\u68c0\u6d4b\u548c\u6316\u6398\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\u5dee\u4e14\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u76d1\u7763\u65b9\u6cd5\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u800cLLM\u5728\u63d0\u793a\u5de5\u7a0b\u4e0b\u867d\u6709\u6548\u4f46\u7f3a\u4e4f\u5168\u9762\u7814\u7a76\u3002", "method": "\u91c7\u7528RAG\u6846\u67b6\u8bbe\u8ba1\u52a8\u6001\u63d0\u793a\u65b9\u6848\uff0c\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u6280\u672f\u4f18\u5316LLM\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u4e94\u79cdLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u9759\u6001\u63d0\u793a\u65b9\u6848\u3002", "conclusion": "RAG\u52a8\u6001\u63d0\u793a\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u56e0\u679c\u5173\u7cfb\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.24342", "pdf": "https://arxiv.org/pdf/2505.24342", "abs": "https://arxiv.org/abs/2505.24342", "authors": ["Fanhang Man", "Xiaoyue Chen", "Huandong Wang", "Baining Zhao", "Han Li", "Xinlei Chen", "Yong Li"], "title": "KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding what emotions images evoke in their viewers is a foundational\ngoal in human-centric visual computing. While recent advances in\nvision-language models (VLMs) have shown promise for visual emotion analysis\n(VEA), several key challenges remain unresolved. Emotional cues in images are\noften abstract, overlapping, and entangled, making them difficult to model and\ninterpret. Moreover, VLMs struggle to align these complex visual patterns with\nemotional semantics due to limited supervision and sparse emotional grounding.\nFinally, existing approaches lack structured affective knowledge to resolve\nambiguity and ensure consistent emotional reasoning across diverse visual\ndomains.\n  To address these limitations, we propose \\textbf{K-EVER\\textsuperscript{2}},\na knowledge-enhanced framework for emotion reasoning and retrieval. Our\napproach introduces a semantically structured formulation of visual emotion\ncues and integrates external affective knowledge through multimodal alignment.\nWithout relying on handcrafted labels or direct emotion supervision,\nK-EVER\\textsuperscript{2} achieves robust and interpretable emotion predictions\nacross heterogeneous image types.\n  We validate our framework on three representative benchmarks, Emotion6,\nEmoSet, and M-Disaster, covering social media imagery, human-centric scenes,\nand disaster contexts. K-EVER\\textsuperscript{2} consistently outperforms\nstrong CNN and VLM baselines, achieving up to a \\textbf{19\\% accuracy gain} for\nspecific emotions and a \\textbf{12.3\\% average accuracy gain} across all\nemotion categories. Our results demonstrate a scalable and generalizable\nsolution for advancing emotional understanding of visual content.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u589e\u5f3a\u7684\u89c6\u89c9\u60c5\u611f\u5206\u6790\u6846\u67b6K-EVER\u00b2\uff0c\u901a\u8fc7\u6574\u5408\u5916\u90e8\u60c5\u611f\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u62bd\u8c61\u3001\u91cd\u53e0\u548c\u7a00\u758f\u76d1\u7763\u95ee\u9898\u3002", "motivation": "\u56fe\u50cf\u4e2d\u7684\u60c5\u611f\u7ebf\u7d22\u901a\u5e38\u62bd\u8c61\u4e14\u590d\u6742\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u548c\u89e3\u91ca\uff0c\u4e14\u7f3a\u4e4f\u7ed3\u6784\u5316\u60c5\u611f\u77e5\u8bc6\u3002", "method": "\u63d0\u51faK-EVER\u00b2\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u7ed3\u6784\u5316\u60c5\u611f\u7ebf\u7d22\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u5916\u90e8\u60c5\u611f\u77e5\u8bc6\uff0c\u65e0\u9700\u624b\u5de5\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u60c5\u611f\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cK-EVER\u00b2\u663e\u8457\u4f18\u4e8eCNN\u548cVLM\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u9ad8\u63d0\u534719%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "K-EVER\u00b2\u4e3a\u89c6\u89c9\u5185\u5bb9\u7684\u60c5\u611f\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23945", "pdf": "https://arxiv.org/pdf/2505.23945", "abs": "https://arxiv.org/abs/2505.23945", "authors": ["Sriram Balasubramanian", "Samyadeep Basu", "Soheil Feizi"], "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models", "categories": ["cs.CL", "cs.AI", "I.2.10; I.2.7"], "comment": "34 pages, 25 figures", "summary": "Chain-of-thought (CoT) reasoning enhances performance of large language\nmodels, but questions remain about whether these reasoning traces faithfully\nreflect the internal processes of the model. We present the first comprehensive\nstudy of CoT faithfulness in large vision-language models (LVLMs),\ninvestigating how both text-based and previously unexplored image-based biases\naffect reasoning and bias articulation. Our work introduces a novel,\nfine-grained evaluation pipeline for categorizing bias articulation patterns,\nenabling significantly more precise analysis of CoT reasoning than previous\nmethods. This framework reveals critical distinctions in how models process and\nrespond to different types of biases, providing new insights into LVLM CoT\nfaithfulness. Our findings reveal that subtle image-based biases are rarely\narticulated compared to explicit text-based ones, even in models specialized\nfor reasoning. Additionally, many models exhibit a previously unidentified\nphenomenon we term ``inconsistent'' reasoning - correctly reasoning before\nabruptly changing answers, serving as a potential canary for detecting biased\nreasoning from unfaithful CoTs. We then apply the same evaluation pipeline to\nrevisit CoT faithfulness in LLMs across various levels of implicit cues. Our\nfindings reveal that current language-only reasoning models continue to\nstruggle with articulating cues that are not overtly stated.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\uff0c\u63ed\u793a\u4e86\u6587\u672c\u548c\u56fe\u50cf\u504f\u89c1\u5bf9\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8CoT\u63a8\u7406\u662f\u5426\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u5185\u90e8\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u56fe\u50cf\u504f\u89c1\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u3002", "method": "\u5f15\u5165\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5206\u7c7b\u504f\u89c1\u8868\u8fbe\u6a21\u5f0f\uff0c\u5206\u6790CoT\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u53d1\u73b0\u56fe\u50cf\u504f\u89c1\u8f83\u5c11\u88ab\u8868\u8fbe\uff0c\u6a21\u578b\u5b58\u5728\u201c\u4e0d\u4e00\u81f4\u201d\u63a8\u7406\u73b0\u8c61\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u9690\u542b\u7ebf\u7d22\u4e0b\u4ecd\u96be\u4ee5\u5fe0\u5b9e\u8868\u8fbe\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u5fe0\u5b9e\u8868\u8fbe\u9690\u542b\u504f\u89c1\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u63a8\u7406\u673a\u5236\u3002"}}
{"id": "2505.24346", "pdf": "https://arxiv.org/pdf/2505.24346", "abs": "https://arxiv.org/abs/2505.24346", "authors": ["Ziyi Wang", "Zhi Gao", "Boxuan Yu", "Zirui Dai", "Yuxiang Song", "Qingyuan Lu", "Jin Chen", "Xinxiao Wu"], "title": "VUDG: A Dataset for Video Understanding Domain Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Video understanding has made remarkable progress in recent years, largely\ndriven by advances in deep models and the availability of large-scale annotated\ndatasets. However, existing works typically ignore the inherent domain shifts\nencountered in real-world video applications, leaving domain generalization\n(DG) in video understanding underexplored. Hence, we propose Video\nUnderstanding Domain Generalization (VUDG), a novel dataset designed\nspecifically for evaluating the DG performance in video understanding. VUDG\ncontains videos from 11 distinct domains that cover three types of domain\nshifts, and maintains semantic similarity across different domains to ensure\nfair and meaningful evaluation. We propose a multi-expert progressive\nannotation framework to annotate each video with both multiple-choice and\nopen-ended question-answer pairs. Extensive experiments on 9 representative\nlarge video-language models (LVLMs) and several traditional video question\nanswering methods show that most models (including state-of-the-art LVLMs)\nsuffer performance degradation under domain shifts. These results highlight the\nchallenges posed by VUDG and the difference in the robustness of current models\nto data distribution shifts. We believe VUDG provides a valuable resource for\nprompting future research in domain generalization video understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Video Understanding Domain Generalization (VUDG)\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u7814\u7a76\u901a\u5e38\u5ffd\u7565\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u9886\u57df\u6cdb\u5316\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faVUDG\u6570\u636e\u96c6\uff0c\u5305\u542b11\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u89c6\u9891\uff0c\u8986\u76d6\u4e09\u79cd\u9886\u57df\u504f\u79fb\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u4e13\u5bb6\u6e10\u8fdb\u6807\u6ce8\u6846\u67b6\u4e3a\u89c6\u9891\u6807\u6ce8\u591a\u9009\u548c\u5f00\u653e\u5f0f\u95ee\u7b54\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u591a\u6570\u6a21\u578b\uff08\u5305\u62ec\u6700\u5148\u8fdb\u7684LVLMs\uff09\u5728\u9886\u57df\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5bf9\u6570\u636e\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u3002", "conclusion": "VUDG\u4e3a\u672a\u6765\u9886\u57df\u6cdb\u5316\u89c6\u9891\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2505.23966", "pdf": "https://arxiv.org/pdf/2505.23966", "abs": "https://arxiv.org/abs/2505.23966", "authors": ["Jiayi Tian", "Ryan Solgi", "Jinming Lu", "Yifan Yang", "Hai Li", "Zheng Zhang"], "title": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have enabled remarkable progress in natural\nlanguage processing, yet their high computational and memory demands pose\nchallenges for deployment in resource-constrained environments. Although recent\nlow-rank decomposition methods offer a promising path for structural\ncompression, they often suffer from accuracy degradation, expensive calibration\nprocedures, and result in inefficient model architectures that hinder\nreal-world inference speedups. In this paper, we propose FLAT-LLM, a fast and\naccurate, training-free structural compression method based on fine-grained\nlow-rank transformations in the activation space. Specifically, we reduce the\nhidden dimension by transforming the weights using truncated eigenvectors\ncomputed via head-wise Principal Component Analysis (PCA), and employ an\nimportance-based metric to adaptively allocate ranks across decoders. FLAT-LLM\nachieves efficient and effective weight compression without recovery\nfine-tuning, which could complete the calibration within a few minutes.\nEvaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural\npruning baselines in generalization and downstream performance, while\ndelivering inference speedups over decomposition-based methods.", "AI": {"tldr": "FLAT-LLM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7ed3\u6784\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4f4e\u79e9\u53d8\u6362\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u6743\u91cd\u538b\u7f29\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\u7684\u6311\u6218\uff0c\u73b0\u6709\u4f4e\u79e9\u5206\u89e3\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u4e0b\u964d\u548c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5934\u90e8\u5206\u6790\u7684\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u622a\u65ad\u7279\u5f81\u5411\u91cf\u53d8\u6362\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u91cd\u8981\u6027\u6307\u6807\u81ea\u9002\u5e94\u5206\u914d\u89e3\u7801\u5668\u79e9\uff0c\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u7684\u9ad8\u6548\u538b\u7f29\u3002", "result": "\u57284\u4e2a\u6a21\u578b\u548c11\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFLAT-LLM\u5728\u6cdb\u5316\u548c\u4e0b\u6e38\u6027\u80fd\u4e0a\u4f18\u4e8e\u7ed3\u6784\u526a\u679d\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "FLAT-LLM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u7ed3\u6784\u538b\u7f29\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.24361", "pdf": "https://arxiv.org/pdf/2505.24361", "abs": "https://arxiv.org/abs/2505.24361", "authors": ["Roger Ferrod", "C\u00e1ssio F. Dantas", "Luigi Di Caro", "Dino Ienco"], "title": "Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as\nrobotics, autonomous driving and remote sensing. The combination of these\nmulti-modal data enhances environmental perception by providing 3D spatial\ncontext, which is absent in standard RGB images. Although RGBD multi-modal data\ncan be available to train computer vision models, accessing all sensor\nmodalities during the inference stage may be infeasible due to sensor failures\nor resource constraints, leading to a mismatch between data modalities\navailable during training and inference. Traditional Cross-Modal Knowledge\nDistillation (CMKD) frameworks, developed to address this task, are typically\nbased on a teacher/student paradigm, where a multi-modal teacher distills\nknowledge into a single-modality student model. However, these approaches face\nchallenges in teacher architecture choices and distillation process selection,\nthus limiting their adoption in real-world scenarios. To overcome these issues,\nwe introduce CroDiNo-KD (Cross-Modal Disentanglement: a New Outlook on\nKnowledge Distillation), a novel cross-modal knowledge distillation framework\nfor RGBD semantic segmentation. Our approach simultaneously learns\nsingle-modality RGB and Depth models by exploiting disentanglement\nrepresentation, contrastive learning and decoupled data augmentation with the\naim to structure the internal manifolds of neural network models through\ninteraction and collaboration. We evaluated CroDiNo-KD on three RGBD datasets\nacross diverse domains, considering recent CMKD frameworks as competitors. Our\nfindings illustrate the quality of CroDiNo-KD, and they suggest reconsidering\nthe conventional teacher/student paradigm to distill information from\nmulti-modal data to single-modality neural networks.", "AI": {"tldr": "CroDiNo-KD\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6559\u5e08\u67b6\u6784\u9009\u62e9\u548c\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u9650\u5236\u3002", "motivation": "\u591a\u6a21\u6001RGBD\u6570\u636e\u5728\u8bad\u7ec3\u65f6\u53ef\u7528\uff0c\u4f46\u5728\u63a8\u7406\u9636\u6bb5\u53ef\u80fd\u56e0\u4f20\u611f\u5668\u6545\u969c\u6216\u8d44\u6e90\u9650\u5236\u800c\u7f3a\u5931\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0e\u63a8\u7406\u6a21\u6001\u4e0d\u5339\u914d\u3002\u4f20\u7edfCMKD\u65b9\u6cd5\u5728\u6559\u5e08\u67b6\u6784\u548c\u84b8\u998f\u8fc7\u7a0b\u9009\u62e9\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "CroDiNo-KD\u5229\u7528\u89e3\u8026\u8868\u793a\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u89e3\u8026\u6570\u636e\u589e\u5f3a\uff0c\u540c\u65f6\u5b66\u4e60\u5355\u6a21\u6001RGB\u548c\u6df1\u5ea6\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u4e92\u548c\u534f\u4f5c\u7ed3\u6784\u5316\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u6d41\u5f62\u3002", "result": "\u5728\u4e09\u4e2aRGBD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCroDiNo-KD\u4f18\u4e8e\u73b0\u6709CMKD\u6846\u67b6\uff0c\u5e76\u5efa\u8bae\u91cd\u65b0\u8003\u8651\u4f20\u7edf\u7684\u6559\u5e08/\u5b66\u751f\u8303\u5f0f\u3002", "conclusion": "CroDiNo-KD\u4e3a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.23996", "pdf": "https://arxiv.org/pdf/2505.23996", "abs": "https://arxiv.org/abs/2505.23996", "authors": ["Yinong Oliver Wang", "Nivedha Sivakumar", "Falaah Arif Khan", "Rin Metcalf Susa", "Adam Golinski", "Natalie Mackraz", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 8 figures, and 1 table in main paper. Supplementary appendix\n  attached. Accepted at ICML 2025", "summary": "The recent rapid adoption of large language models (LLMs) highlights the\ncritical need for benchmarking their fairness. Conventional fairness metrics,\nwhich focus on discrete accuracy-based evaluations (i.e., prediction\ncorrectness), fail to capture the implicit impact of model uncertainty (e.g.,\nhigher model confidence about one group over another despite similar accuracy).\nTo address this limitation, we propose an uncertainty-aware fairness metric,\nUCerF, to enable a fine-grained evaluation of model fairness that is more\nreflective of the internal bias in model decisions compared to conventional\nfairness measures. Furthermore, observing data size, diversity, and clarity\nissues in current datasets, we introduce a new gender-occupation fairness\nevaluation dataset with 31,756 samples for co-reference resolution, offering a\nmore diverse and suitable dataset for evaluating modern LLMs. We establish a\nbenchmark, using our metric and dataset, and apply it to evaluate the behavior\nof ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness\ndue to high confidence in incorrect predictions, a detail overlooked by\nEqualized Odds but captured by UCerF. Overall, our proposed LLM benchmark,\nwhich evaluates fairness with uncertainty awareness, paves the way for\ndeveloping more transparent and accountable AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u516c\u5e73\u6027\u5ea6\u91cfUCerF\uff0c\u7528\u4e8e\u66f4\u7cbe\u7ec6\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5e73\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6027\u522b-\u804c\u4e1a\u516c\u5e73\u6027\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u5ea6\u91cf\u672a\u80fd\u6355\u6349\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u9690\u6027\u5f71\u54cd\uff0c\u4e14\u5f53\u524d\u6570\u636e\u96c6\u5728\u6570\u636e\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u6e05\u6670\u5ea6\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51faUCerF\u5ea6\u91cf\uff0c\u5e76\u6784\u5efa\u5305\u542b31,756\u4e2a\u6837\u672c\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5341\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "Mistral-7B\u7b49\u6a21\u578b\u5728UCerF\u4e0b\u8868\u73b0\u51fa\u516c\u5e73\u6027\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u5ea6\u91cf\u5982Equalized Odds\u672a\u80fd\u53d1\u73b0\u3002", "conclusion": "UCerF\u548c\u65b0\u7684\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u66f4\u900f\u660e\u3001\u53ef\u9760\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2505.24371", "pdf": "https://arxiv.org/pdf/2505.24371", "abs": "https://arxiv.org/abs/2505.24371", "authors": ["Md Intisar Chowdhury", "Kittinun Aukkapinyo", "Hiroshi Fujimura", "Joo Ann Woo", "Wasu Wasusatein", "Fadoua Ghourabi"], "title": "Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "In this paper, we propose a Grid-based Local and Global Area Transcription\n(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates\nin two phases. First, extracting text transcripts from video frames using a\nVision-Language Model (VLM). Next, processing questions using these transcripts\nto generate answers through a Large Language Model (LLM). This design ensures\nimage privacy by deploying the VLM on edge devices and the LLM in the cloud. To\nimprove transcript quality, we propose grid-based visual prompting, which\nextracts intricate local details from each grid cell and integrates them with\nglobal information. Evaluation results show that Grid-LoGAT, using the\nopen-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms\nstate-of-the-art methods with similar baseline models on NExT-QA and STAR-QA\ndatasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our\nmethod surpasses the non-grid version by 24 points on localization-based\nquestions we created using NExT-QA.", "AI": {"tldr": "Grid-LoGAT\u7cfb\u7edf\u901a\u8fc7\u7f51\u683c\u5316\u89c6\u89c9\u63d0\u793a\u63d0\u53d6\u89c6\u9891\u5e27\u4e2d\u7684\u6587\u672c\u8f6c\u5f55\uff0c\u7ed3\u5408VLM\u548cLLM\u5b9e\u73b0\u89c6\u9891\u95ee\u7b54\uff0c\u4fdd\u62a4\u56fe\u50cf\u9690\u79c1\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u95ee\u7b54\u4e2d\u56fe\u50cf\u9690\u79c1\u4fdd\u62a4\u548c\u8f6c\u5f55\u8d28\u91cf\u63d0\u5347\u7684\u95ee\u9898\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1aVLM\u63d0\u53d6\u6587\u672c\u8f6c\u5f55\uff0cLLM\u5904\u7406\u95ee\u9898\u751f\u6210\u7b54\u6848\uff1b\u91c7\u7528\u7f51\u683c\u5316\u89c6\u89c9\u63d0\u793a\u4f18\u5316\u8f6c\u5f55\u3002", "result": "\u5728NExT-QA\u548cSTAR-QA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523065.9%\u548c50.11%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u975e\u7f51\u683c\u7248\u672c24\u5206\u3002", "conclusion": "Grid-LoGAT\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u3002"}}
{"id": "2505.24009", "pdf": "https://arxiv.org/pdf/2505.24009", "abs": "https://arxiv.org/abs/2505.24009", "authors": ["Hidetaka Kamigaito", "Ying Zhang", "Jingun Kwon", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers deliver outstanding performance across a wide range of tasks and\nare now a dominant backbone architecture for large language models (LLMs).\nTheir task-solving performance is improved by increasing parameter size, as\nshown in the recent studies on parameter scaling laws. Although recent\nmechanistic-interpretability studies have deepened our understanding of the\ninternal behavior of Transformers by analyzing their residual stream, the\nrelationship between these internal mechanisms and the parameter scaling laws\nremains unclear. To bridge this gap, we focus on layers and their size, which\nmainly decide the parameter size of Transformers. For this purpose, we first\ntheoretically investigate the layers within the residual stream through a\nbias-diversity decomposition. The decomposition separates (i) bias, the error\nof each layer's output from the ground truth, and (ii) diversity, which\nindicates how much the outputs of each layer differ from each other. Analyzing\nTransformers under this theory reveals that performance improves when\nindividual layers make predictions close to the correct answer and remain\nmutually diverse. We show that diversity becomes especially critical when\nindividual layers' outputs are far from the ground truth. Finally, we introduce\nan information-theoretic diversity and show our main findings that adding\nlayers enhances performance only when those layers behave differently, i.e.,\nare diverse. We also reveal the performance gains from increasing the number of\nlayers exhibit submodularity: marginal improvements diminish as additional\nlayers increase, mirroring the logarithmic convergence predicted by the\nparameter scaling laws. Experiments on multiple semantic-understanding tasks\nwith various LLMs empirically confirm the theoretical properties derived in\nthis study.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u5c42\u6570\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u504f\u5dee-\u591a\u6837\u6027\u5206\u89e3\u7406\u8bba\u63ed\u793a\u4e86\u5c42\u95f4\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u53d1\u73b0\u589e\u52a0\u5c42\u6570\u4ec5\u5728\u5c42\u95f4\u884c\u4e3a\u4e0d\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u6027\u80fd\u589e\u76ca\u5448\u73b0\u6b21\u6a21\u6027\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u4efb\u52a1\u8868\u73b0\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4e0e\u53c2\u6570\u89c4\u6a21\u5b9a\u5f8b\u7684\u5173\u7cfb\u5c1a\u4e0d\u660e\u786e\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u504f\u5dee-\u591a\u6837\u6027\u5206\u89e3\u7406\u8bba\u5206\u6790Transformer\u5c42\u7684\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u4fe1\u606f\u8bba\u591a\u6837\u6027\u6982\u5ff5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u53d1\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5c42\u95f4\u591a\u6837\u6027\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5f53\u5355\u5c42\u8f93\u51fa\u8fdc\u79bb\u771f\u5b9e\u503c\u65f6\uff1b\u589e\u52a0\u5c42\u6570\u4ec5\u5728\u5c42\u95f4\u884c\u4e3a\u4e0d\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u6027\u80fd\u589e\u76ca\u5448\u6b21\u6a21\u6027\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86Transformer\u5c42\u95f4\u591a\u6837\u6027\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u4e3a\u53c2\u6570\u89c4\u6a21\u5b9a\u5f8b\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u7684\u666e\u9002\u6027\u3002"}}
{"id": "2505.24372", "pdf": "https://arxiv.org/pdf/2505.24372", "abs": "https://arxiv.org/abs/2505.24372", "authors": ["Yichi Zhang", "Gongwei Chen", "Jun Zhu", "Jia Wan"], "title": "D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding", "categories": ["cs.CV"], "comment": "16pages, 8figures", "summary": "Visual Grounding is a task that aims to localize a target region in an image\nbased on a free-form natural language description. With the rise of Transformer\narchitectures, there is an increasing need for larger datasets to boost\nperformance. However, the high cost of manual annotation poses a challenge,\nhindering the scale of data and the ability of large models to enhance their\neffectiveness. Previous pseudo label generation methods heavily rely on\nhuman-labeled captions of the original dataset, limiting scalability and\ndiversity. To address this, we propose D2AF, a robust annotation framework for\nvisual grounding using only input images. This approach overcomes dataset size\nlimitations and enriches both the quantity and diversity of referring\nexpressions. Our approach leverages multimodal large models and object\ndetection models. By implementing dual-driven annotation strategies, we\neffectively generate detailed region-text pairs using both closed-set and\nopen-set approaches. We further conduct an in-depth analysis of data quantity\nand data distribution. Our findings demonstrate that increasing data volume\nenhances model performance. However, the degree of improvement depends on how\nwell the pseudo labels broaden the original data distribution. Based on these\ninsights, we propose a consistency and distribution aware filtering method to\nfurther improve data quality by effectively removing erroneous and redundant\ndata. This approach effectively eliminates noisy data, leading to improved\nperformance. Experiments on three visual grounding tasks demonstrate that our\nmethod significantly improves the performance of existing models and achieves\nstate-of-the-art results.", "AI": {"tldr": "D2AF\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u8f93\u5165\u56fe\u50cf\u7684\u89c6\u89c9\u5b9a\u4f4d\u6807\u6ce8\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9a71\u52a8\u7b56\u7565\u751f\u6210\u533a\u57df-\u6587\u672c\u5bf9\uff0c\u63d0\u5347\u6570\u636e\u91cf\u548c\u591a\u6837\u6027\uff0c\u5e76\u901a\u8fc7\u8fc7\u6ee4\u65b9\u6cd5\u4f18\u5316\u6570\u636e\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u9650\u5236\u4e86\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u6570\u636e\u89c4\u6a21\u548c\u6a21\u578b\u6027\u80fd\u63d0\u5347\uff0c\u73b0\u6709\u4f2a\u6807\u7b7e\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u7f3a\u4e4f\u6269\u5c55\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faD2AF\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u6a21\u578b\u548c\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u9a71\u52a8\u7b56\u7565\u751f\u6210\u533a\u57df-\u6587\u672c\u5bf9\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u53ca\u5206\u5e03\u611f\u77e5\u8fc7\u6ee4\u65b9\u6cd5\u4f18\u5316\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u6570\u636e\u91cf\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u63d0\u5347\u7a0b\u5ea6\u53d6\u51b3\u4e8e\u4f2a\u6807\u7b7e\u5bf9\u539f\u59cb\u6570\u636e\u5206\u5e03\u7684\u6269\u5c55\u6548\u679c\u3002\u8fc7\u6ee4\u65b9\u6cd5\u6709\u6548\u53bb\u9664\u566a\u58f0\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "D2AF\u6846\u67b6\u514b\u670d\u4e86\u6570\u636e\u89c4\u6a21\u9650\u5236\uff0c\u63d0\u5347\u4e86\u6570\u636e\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u5728\u4e09\u4e2a\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2505.24012", "pdf": "https://arxiv.org/pdf/2505.24012", "abs": "https://arxiv.org/abs/2505.24012", "authors": ["Alexandre Bonlarron", "Florian R\u00e9gin", "Elisabetta De Maria", "Jean-Charles R\u00e9gin"], "title": "Large Language Model Meets Constraint Propagation", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in the Proceedings of the Thirty-Fourth International Joint\n  Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Large Language Models (LLMs) excel at generating fluent text but struggle to\nenforce external constraints because they generate tokens sequentially without\nexplicit control mechanisms. GenCP addresses this limitation by combining LLM\npredictions with Constraint Programming (CP) reasoning, formulating text\ngeneration as a Constraint Satisfaction Problem (CSP). In this paper, we\nimprove GenCP by integrating Masked Language Models (MLMs) for domain\ngeneration, which allows bidirectional constraint propagation that leverages\nboth past and future tokens. This integration bridges the gap between\ntoken-level prediction and structured constraint enforcement, leading to more\nreliable and constraint-aware text generation. Our evaluation on COLLIE\nbenchmarks demonstrates that incorporating domain preview via MLM calls\nsignificantly improves GenCP's performance. Although this approach incurs\nadditional MLM calls and, in some cases, increased backtracking, the overall\neffect is a more efficient use of LLM inferences and an enhanced ability to\ngenerate feasible and meaningful solutions, particularly in tasks with strict\ncontent constraints.", "AI": {"tldr": "GenCP\u7ed3\u5408LLM\u4e0eCP\u63a8\u7406\uff0c\u901a\u8fc7MLM\u5b9e\u73b0\u53cc\u5411\u7ea6\u675f\u4f20\u64ad\uff0c\u63d0\u5347\u7ea6\u675f\u611f\u77e5\u6587\u672c\u751f\u6210\u6027\u80fd\u3002", "motivation": "LLM\u5728\u751f\u6210\u6587\u672c\u65f6\u7f3a\u4e4f\u663e\u5f0f\u63a7\u5236\u673a\u5236\uff0c\u96be\u4ee5\u6ee1\u8db3\u5916\u90e8\u7ea6\u675f\u3002", "method": "\u5c06\u6587\u672c\u751f\u6210\u5efa\u6a21\u4e3aCSP\uff0c\u96c6\u6210MLM\u5b9e\u73b0\u53cc\u5411\u7ea6\u675f\u4f20\u64ad\u3002", "result": "\u5728COLLIE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGenCP\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u751f\u6210\u66f4\u53ef\u9760\u4e14\u7b26\u5408\u7ea6\u675f\u7684\u6587\u672c\u3002", "conclusion": "GenCP\u901a\u8fc7MLM\u96c6\u6210\u4f18\u5316\u4e86LLM\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4e25\u683c\u7ea6\u675f\u4efb\u52a1\u3002"}}
{"id": "2505.24375", "pdf": "https://arxiv.org/pdf/2505.24375", "abs": "https://arxiv.org/abs/2505.24375", "authors": ["Maciej Wielgosz", "Simon Berg", "Heikki Korpunen", "Stephan Hoffmann"], "title": "Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a deep learning-based framework for classifying forestry\noperations from dashcam video footage. Focusing on four key work elements -\ncrane-out, cutting-and-to-processing, driving, and processing - the approach\nemploys a 3D ResNet-50 architecture implemented with PyTorchVideo. Trained on a\nmanually annotated dataset of field recordings, the model achieves strong\nperformance, with a validation F1 score of 0.88 and precision of 0.90. These\nresults underscore the effectiveness of spatiotemporal convolutional networks\nfor capturing both motion patterns and appearance in real-world forestry\nenvironments.\n  The system integrates standard preprocessing and augmentation techniques to\nimprove generalization, but overfitting is evident, highlighting the need for\nmore training data and better class balance. Despite these challenges, the\nmethod demonstrates clear potential for reducing the manual workload associated\nwith traditional time studies, offering a scalable solution for operational\nmonitoring and efficiency analysis in forestry.\n  This work contributes to the growing application of AI in natural resource\nmanagement and sets the foundation for future systems capable of real-time\nactivity recognition in forest machinery. Planned improvements include dataset\nexpansion, enhanced regularization, and deployment trials on embedded systems\nfor in-field use.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u4e2d\u5206\u7c7b\u6797\u4e1a\u64cd\u4f5c\uff0c\u91c7\u75283D ResNet-50\u67b6\u6784\uff0c\u9a8c\u8bc1F1\u5206\u6570\u4e3a0.88\uff0c\u7cbe\u5ea6\u4e3a0.90\u3002", "motivation": "\u51cf\u5c11\u4f20\u7edf\u65f6\u95f4\u7814\u7a76\u4e2d\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u4e3a\u6797\u4e1a\u64cd\u4f5c\u76d1\u6d4b\u548c\u6548\u7387\u5206\u6790\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u75283D ResNet-50\u67b6\u6784\u548cPyTorchVideo\u5b9e\u73b0\uff0c\u7ed3\u5408\u6807\u51c6\u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9700\u8981\u66f4\u591a\u6570\u636e\u548c\u66f4\u597d\u7684\u7c7b\u522b\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6797\u4e1a\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u6f5c\u529b\uff0c\u4e3a\u5b9e\u65f6\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u8ba1\u5212\u6269\u5c55\u6570\u636e\u96c6\u548c\u589e\u5f3a\u6b63\u5219\u5316\u3002"}}
{"id": "2505.24016", "pdf": "https://arxiv.org/pdf/2505.24016", "abs": "https://arxiv.org/abs/2505.24016", "authors": ["Matthew Raffel", "Victor Agostinelli", "Lizhong Chen"], "title": "BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at IWSLT 2025", "summary": "This paper discusses the construction, fine-tuning, and deployment of\nBeaverTalk, a cascaded system for speech-to-text translation as part of the\nIWSLT 2025 simultaneous translation task. The system architecture employs a VAD\nsegmenter for breaking a speech stream into segments, Whisper Large V2 for\nautomatic speech recognition (ASR), and Gemma 3 12B for simultaneous\ntranslation. Regarding the simultaneous translation LLM, it is fine-tuned via\nlow-rank adaptors (LoRAs) for a conversational prompting strategy that\nleverages a single prior-sentence memory bank from the source language as\ncontext. The cascaded system participated in the English$\\rightarrow$German and\nEnglish$\\rightarrow$Chinese language directions for both the low and high\nlatency regimes. In particular, on the English$\\rightarrow$German task, the\nsystem achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and\n3343.73, respectively. Then, on the English$\\rightarrow$Chinese task, the\nsystem achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and\n3521.35, respectively.", "AI": {"tldr": "BeaverTalk\u662f\u4e00\u4e2a\u7528\u4e8e\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u7684\u7ea7\u8054\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86VAD\u5206\u5272\u5668\u3001Whisper Large V2\u548cGemma 3 12B\u6a21\u578b\uff0c\u901a\u8fc7LoRAs\u5fae\u8c03\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7ffb\u8bd1\uff0c\u5728IWSLT 2025\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u4e2d\u7684\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u51c6\u786e\u6027\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u3002", "method": "\u4f7f\u7528VAD\u5206\u5272\u5668\u5206\u5272\u8bed\u97f3\u6d41\uff0cWhisper Large V2\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\uff0cGemma 3 12B\u8fdb\u884c\u7ffb\u8bd1\uff0c\u5e76\u901a\u8fc7LoRAs\u5fae\u8c03\u4f18\u5316\u4e0a\u4e0b\u6587\u7b56\u7565\u3002", "result": "\u5728\u82f1\u8bed\u2192\u5fb7\u8bed\u4efb\u52a1\u4e2dBLEU\u5f97\u5206\u4e3a24.64\u548c27.83\uff0c\u82f1\u8bed\u2192\u4e2d\u6587\u4efb\u52a1\u4e2d\u4e3a34.07\u548c37.23\uff0c\u5ef6\u8fdf\u8868\u73b0\u826f\u597d\u3002", "conclusion": "BeaverTalk\u5728\u591a\u8bed\u8a00\u4f4e\u5ef6\u8fdf\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u67b6\u6784\u548c\u5fae\u8c03\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.24380", "pdf": "https://arxiv.org/pdf/2505.24380", "abs": "https://arxiv.org/abs/2505.24380", "authors": ["Zheng Wang"], "title": "SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fine-grained bird image classification (FBIC) is not only of great\nsignificance for ecological monitoring and species identification, but also\nholds broad research value in the fields of image recognition and fine-grained\nvisual modeling. Compared with general image classification tasks, FBIC poses\nmore formidable challenges: 1) the differences in species size and imaging\ndistance result in the varying sizes of birds presented in the images; 2)\ncomplex natural habitats often introduce strong background interference; 3) and\nhighly flexible poses such as flying, perching, or foraging result in\nsubstantial intra-class variability. These factors collectively make it\ndifficult for traditional methods to stably extract discriminative features,\nthereby limiting the generalizability and interpretability of models in\nreal-world applications. To address these challenges, this paper proposes a\nfine-grained bird classification framework based on strip-aware spatial\nperception, which aims to capture long-range spatial dependencies across entire\nrows or columns in bird images, thereby enhancing the model's robustness and\ninterpretability. The proposed method incorporates two novel modules:\nextensional perception aggregator (EPA) and channel semantic weaving (CSW).\nSpecifically, EPA integrates local texture details with global structural cues\nby aggregating information across horizontal and vertical spatial directions.\nCSW further refines the semantic representations by adaptively fusing\nlong-range and short-range information along the channel dimension. Built upon\na ResNet-50 backbone, the model enables jump-wise connection of extended\nstructural features across the spatial domain. Experimental results on the\nCUB-200-2011 dataset demonstrate that our framework achieves significant\nperformance improvements while maintaining architectural efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u5e26\u611f\u77e5\u7a7a\u95f4\u611f\u77e5\u7684\u7ec6\u7c92\u5ea6\u9e1f\u7c7b\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u56fe\u50cf\u4e2d\u7684\u957f\u8ddd\u79bb\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u9e1f\u7c7b\u56fe\u50cf\u5206\u7c7b\uff08FBIC\uff09\u5728\u751f\u6001\u76d1\u6d4b\u548c\u7269\u79cd\u8bc6\u522b\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u7531\u4e8e\u9e1f\u7c7b\u5927\u5c0f\u3001\u80cc\u666f\u5e72\u6270\u548c\u59ff\u6001\u591a\u53d8\u7b49\u56e0\u7d20\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7a33\u5b9a\u63d0\u53d6\u5224\u522b\u6027\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6a21\u5757\uff1a\u6269\u5c55\u611f\u77e5\u805a\u5408\u5668\uff08EPA\uff09\u548c\u901a\u9053\u8bed\u4e49\u7f16\u7ec7\uff08CSW\uff09\uff0c\u5206\u522b\u7528\u4e8e\u6574\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\u4ee5\u53ca\u4f18\u5316\u8bed\u4e49\u8868\u793a\u3002", "result": "\u5728CUB-200-2011\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u4fdd\u6301\u4e86\u67b6\u6784\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u957f\u8ddd\u79bb\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u9e1f\u7c7b\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24028", "pdf": "https://arxiv.org/pdf/2505.24028", "abs": "https://arxiv.org/abs/2505.24028", "authors": ["Kateryna Akhynko", "Oleksandr Kosovan", "Mykola Trokhymovych"], "title": "Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents one of the top-performing solutions to the UNLP 2025\nShared Task on Detecting Manipulation in Social Media. The task focuses on\ndetecting and classifying rhetorical and stylistic manipulation techniques used\nto influence Ukrainian Telegram users. For the classification subtask, we\nfine-tuned the Gemma 2 language model with LoRA adapters and applied a\nsecond-level classifier leveraging meta-features and threshold optimization.\nFor span detection, we employed an XLM-RoBERTa model trained for multi-target,\nincluding token binary classification. Our approach achieved 2nd place in\nclassification and 3rd place in span detection.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728UNLP 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u4e4c\u514b\u5170Telegram\u7528\u6237\u4e2d\u7684\u4fee\u8f9e\u548c\u98ce\u683c\u64cd\u7eb5\u6280\u672f\u3002", "motivation": "\u4efb\u52a1\u662f\u68c0\u6d4b\u548c\u5206\u7c7b\u7528\u4e8e\u5f71\u54cd\u4e4c\u514b\u5170Telegram\u7528\u6237\u7684\u4fee\u8f9e\u548c\u98ce\u683c\u64cd\u7eb5\u6280\u672f\u3002", "method": "\u5728\u5206\u7c7b\u5b50\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528LoRA\u9002\u914d\u5668\u5fae\u8c03Gemma 2\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u7b2c\u4e8c\u7ea7\u5206\u7c7b\u5668\uff08\u5229\u7528\u5143\u7279\u5f81\u548c\u9608\u503c\u4f18\u5316\uff09\u3002\u5728\u8de8\u5ea6\u68c0\u6d4b\u4e2d\uff0c\u91c7\u7528XLM-RoBERTa\u6a21\u578b\u8fdb\u884c\u591a\u76ee\u6807\u8bad\u7ec3\uff0c\u5305\u62ec\u4ee4\u724c\u4e8c\u5143\u5206\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c2\uff0c\u5728\u8de8\u5ea6\u68c0\u6d4b\u4e2d\u6392\u540d\u7b2c3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u5206\u7c7b\u64cd\u7eb5\u6280\u672f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.24389", "pdf": "https://arxiv.org/pdf/2505.24389", "abs": "https://arxiv.org/abs/2505.24389", "authors": ["Liangyang Ouyang", "Yuki Sakai", "Ryosuke Furuta", "Hisataka Nozawa", "Hikoro Matsui", "Yoichi Sato"], "title": "Leadership Assessment in Pediatric Intensive Care Unit Team Training", "categories": ["cs.CV"], "comment": "This paper is accepted by EgoVis Workshop at CVPR 2025", "summary": "This paper addresses the task of assessing PICU team's leadership skills by\ndeveloping an automated analysis framework based on egocentric vision. We\nidentify key behavioral cues, including fixation object, eye contact, and\nconversation patterns, as essential indicators of leadership assessment. In\norder to capture these multimodal signals, we employ Aria Glasses to record\negocentric video, audio, gaze, and head movement data. We collect one-hour\nvideos of four simulated sessions involving doctors with different roles and\nlevels. To automate data processing, we propose a method leveraging REMoDNaV,\nSAM, YOLO, and ChatGPT for fixation object detection, eye contact detection,\nand conversation classification. In the experiments, significant correlations\nare observed between leadership skills and behavioral metrics, i.e., the output\nof our proposed methods, such as fixation time, transition patterns, and direct\norders in speech. These results indicate that our proposed data collection and\nanalysis framework can effectively solve skill assessment for training PICU\nteams.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u7684\u81ea\u52a8\u5316\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30PICU\u56e2\u961f\u7684\u9886\u5bfc\u6280\u80fd\uff0c\u901a\u8fc7\u884c\u4e3a\u7ebf\u7d22\uff08\u5982\u6ce8\u89c6\u5bf9\u8c61\u3001\u773c\u795e\u63a5\u89e6\u548c\u5bf9\u8bdd\u6a21\u5f0f\uff09\u8fdb\u884c\u91cf\u5316\u5206\u6790\u3002", "motivation": "\u8bc4\u4f30PICU\u56e2\u961f\u7684\u9886\u5bfc\u6280\u80fd\u5bf9\u57f9\u8bad\u548c\u6539\u8fdb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e3b\u89c2\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5ba2\u89c2\u6027\u3002", "method": "\u4f7f\u7528Aria Glasses\u8bb0\u5f55\u89c6\u9891\u3001\u97f3\u9891\u3001\u89c6\u7ebf\u548c\u5934\u90e8\u8fd0\u52a8\u6570\u636e\uff0c\u7ed3\u5408REMoDNaV\u3001SAM\u3001YOLO\u548cChatGPT\u8fdb\u884c\u81ea\u52a8\u5316\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u9886\u5bfc\u6280\u80fd\u4e0e\u884c\u4e3a\u6307\u6807\uff08\u5982\u6ce8\u89c6\u65f6\u95f4\u3001\u8f6c\u6362\u6a21\u5f0f\u548c\u76f4\u63a5\u6307\u4ee4\uff09\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3PICU\u56e2\u961f\u6280\u80fd\u8bc4\u4f30\u95ee\u9898\u3002"}}
{"id": "2505.24033", "pdf": "https://arxiv.org/pdf/2505.24033", "abs": "https://arxiv.org/abs/2505.24033", "authors": ["Yasaman Jafari", "Zixian Wang", "Leon Bergen", "Taylor Berg-Kirkpatrick"], "title": "The Surprising Soupability of Documents in State Space Models", "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": null, "summary": "We investigate whether hidden states from Structured State Space Models\n(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by\nmodel souping, we propose a strategy where documents are encoded independently\nand their representations are pooled -- via simple operations like averaging --\ninto a single context state. This approach, which we call document souping,\nenables modular encoding and reuse without reprocessing the full input for each\nquery. We finetune Mamba2 models to produce soupable representations and find\nthat they support multi-hop QA, sparse retrieval, and long-document reasoning\nwith strong accuracy. On HotpotQA, souping ten independently encoded documents\nnearly matches the performance of a cross-encoder trained on the same inputs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u7684\u9690\u85cf\u72b6\u6001\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u540e\u5904\u7406\u5408\u5e76\u4ee5\u652f\u6301\u4e0b\u6e38\u63a8\u7406\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u6587\u6863\u878d\u5408\u201d\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u7b80\u5355\u64cd\u4f5c\uff08\u5982\u5e73\u5747\uff09\u5c06\u72ec\u7acb\u7f16\u7801\u7684\u6587\u6863\u8868\u793a\u5408\u5e76\u4e3a\u5355\u4e00\u4e0a\u4e0b\u6587\u72b6\u6001\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u6a21\u5757\u5316\u7f16\u7801\u548c\u91cd\u7528\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u91cd\u65b0\u5904\u7406\u5b8c\u6574\u8f93\u5165\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u8df3QA\u3001\u7a00\u758f\u68c0\u7d22\u548c\u957f\u6587\u6863\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u9ad8\u6548\u5408\u5e76\u72ec\u7acb\u7f16\u7801\u7684\u6587\u6863\u8868\u793a\u4ee5\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\uff0c\u907f\u514d\u91cd\u590d\u5904\u7406\u8f93\u5165\u3002", "method": "\u63d0\u51fa\u201c\u6587\u6863\u878d\u5408\u201d\u7b56\u7565\uff0c\u901a\u8fc7\u7b80\u5355\u64cd\u4f5c\uff08\u5982\u5e73\u5747\uff09\u5408\u5e76\u72ec\u7acb\u7f16\u7801\u7684\u6587\u6863\u8868\u793a\uff0c\u5e76\u5fae\u8c03Mamba2\u6a21\u578b\u751f\u6210\u53ef\u878d\u5408\u7684\u8868\u793a\u3002", "result": "\u5728HotpotQA\u4e0a\uff0c\u878d\u5408\u5341\u4e2a\u72ec\u7acb\u7f16\u7801\u7684\u6587\u6863\u51e0\u4e4e\u8fbe\u5230\u4e86\u4ea4\u53c9\u7f16\u7801\u5668\u5728\u76f8\u540c\u8f93\u5165\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u6587\u6863\u878d\u5408\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u63a5\u8fd1\u4ea4\u53c9\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u8df3QA\u3001\u7a00\u758f\u68c0\u7d22\u548c\u957f\u6587\u6863\u63a8\u7406\u3002"}}
{"id": "2505.24401", "pdf": "https://arxiv.org/pdf/2505.24401", "abs": "https://arxiv.org/abs/2505.24401", "authors": ["Xianheng Ma", "Hongchen Tan", "Xiuping Liu", "Yi Zhang", "Huasheng Wang", "Jiang Liu", "Ying Chen", "Hantao Liu"], "title": "S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we leverage the advantages of event cameras to resist harsh\nlighting conditions, reduce background interference, achieve high time\nresolution, and protect facial information to study the long-sequence\nevent-based person re-identification (Re-ID) task. To this end, we propose a\nsimple and efficient long-sequence event Re-ID model, namely the Spike-guided\nSpatiotemporal Semantic Coupling and Expansion Network (S3CE-Net). To better\nhandle asynchronous event data, we build S3CE-Net based on spiking neural\nnetworks (SNNs). The S3CE-Net incorporates the Spike-guided Spatial-temporal\nAttention Mechanism (SSAM) and the Spatiotemporal Feature Sampling Strategy\n(STFS). The SSAM is designed to carry out semantic interaction and association\nin both spatial and temporal dimensions, leveraging the capabilities of SNNs.\nThe STFS involves sampling spatial feature subsequences and temporal feature\nsubsequences from the spatiotemporal dimensions, driving the Re-ID model to\nperceive broader and more robust effective semantics. Notably, the STFS\nintroduces no additional parameters and is only utilized during the training\nstage. Therefore, S3CE-Net is a low-parameter and high-efficiency model for\nlong-sequence event-based person Re-ID. Extensive experiments have verified\nthat our S3CE-Net achieves outstanding performance on many mainstream\nlong-sequence event-based person Re-ID datasets. Code is available\nat:https://github.com/Mhsunshine/SC3E_Net.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u957f\u5e8f\u5217\u884c\u4eba\u91cd\u8bc6\u522b\u6a21\u578bS3CE-Net\uff0c\u5229\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5f02\u6b65\u4e8b\u4ef6\u6570\u636e\uff0c\u7ed3\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u548c\u7279\u5f81\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4f4e\u53c2\u6570\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u5728\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u51cf\u5c11\u80cc\u666f\u5e72\u6270\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u957f\u5e8f\u5217\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u95ee\u9898\u3002", "method": "\u63d0\u51faS3CE-Net\u6a21\u578b\uff0c\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\uff0c\u5305\u542b\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff08SSAM\uff09\u548c\u65f6\u7a7a\u7279\u5f81\u91c7\u6837\u7b56\u7565\uff08STFS\uff09\u3002SSAM\u5b9e\u73b0\u65f6\u7a7a\u8bed\u4e49\u4ea4\u4e92\uff0cSTFS\u91c7\u6837\u7279\u5f81\u5b50\u5e8f\u5217\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eS3CE-Net\u5728\u591a\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u53c2\u6570\u5c11\u4e14\u9ad8\u6548\u3002", "conclusion": "S3CE-Net\u4e3a\u957f\u5e8f\u5217\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u53c2\u6570\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.24040", "pdf": "https://arxiv.org/pdf/2505.24040", "abs": "https://arxiv.org/abs/2505.24040", "authors": ["Yuexing Hao", "Kumail Alhamoud", "Hyewon Jeong", "Haoran Zhang", "Isha Puri", "Philip Torr", "Mike Schaekermann", "Ariel D. Stern", "Marzyeh Ghassemi"], "title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious medical question-answering (QA) benchmarks, including standardized\nmedical exams. However, correct answers alone do not ensure correct logic, and\nmodels may reach accurate conclusions through flawed processes. In this study,\nwe introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance\nEstimation and Question Answering) dataset to evaluate how physician trainees\nand LLMs prioritize relevant information when answering QA questions. We obtain\nannotations on 1,300 QA pairs from 36 physician trainees, labeling each\nsentence within the question components for relevance. We compare these\nrelevance estimates to those for LLMs, and further evaluate the impact of these\n\"relevant\" subsets on downstream task performance for both physician trainees\nand LLMs. We find that LLMs are frequently not aligned with the content\nrelevance estimates of physician trainees. After filtering out physician\ntrainee-labeled irrelevant sentences, accuracy improves for both the trainees\nand the LLMs. All LLM and physician trainee-labeled data are available at:\nhttp://medpair.csail.mit.edu/.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7MedPAIR\u6570\u636e\u96c6\u6bd4\u8f83\u533b\u751f\u5b66\u5458\u548cLLMs\u5728\u533b\u5b66QA\u4efb\u52a1\u4e2d\u4fe1\u606f\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u5dee\u5f02\uff0c\u53d1\u73b0LLMs\u4e0e\u533b\u751f\u5b66\u5458\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u4e0d\u4e00\u81f4\uff0c\u8fc7\u6ee4\u65e0\u5173\u4fe1\u606f\u540e\u53cc\u65b9\u8868\u73b0\u5747\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u533b\u5b66QA\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u903b\u8f91\u53ef\u80fd\u5b58\u5728\u95ee\u9898\uff0c\u9700\u9a8c\u8bc1\u5176\u4e0e\u533b\u751f\u5b66\u5458\u5728\u4fe1\u606f\u76f8\u5173\u6027\u8bc4\u4f30\u4e0a\u7684\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528MedPAIR\u6570\u636e\u96c6\uff0c\u6807\u6ce81,300\u4e2aQA\u5bf9\u4e2d\u6bcf\u4e2a\u53e5\u5b50\u7684\u76f8\u5173\u6027\uff0c\u6bd4\u8f83\u533b\u751f\u5b66\u5458\u548cLLMs\u7684\u8bc4\u4f30\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "LLMs\u4e0e\u533b\u751f\u5b66\u5458\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u4e0d\u4e00\u81f4\uff0c\u8fc7\u6ee4\u65e0\u5173\u4fe1\u606f\u540e\u53cc\u65b9\u51c6\u786e\u7387\u5747\u63d0\u9ad8\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u533b\u5b66QA\u4efb\u52a1\u4e2d\u4fe1\u606f\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u8fc7\u6ee4\u65e0\u5173\u4fe1\u606f\u53ef\u63d0\u5347\u6a21\u578b\u548c\u4eba\u7c7b\u8868\u73b0\u3002"}}
{"id": "2505.24402", "pdf": "https://arxiv.org/pdf/2505.24402", "abs": "https://arxiv.org/abs/2505.24402", "authors": ["Mika Feng", "Koichi Ito", "Takafumi Aoki", "Tetsushi Ohki", "Masakatsu Nishigaki"], "title": "Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing", "categories": ["cs.CV"], "comment": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)", "summary": "Face recognition systems are designed to be robust against changes in head\npose, illumination, and blurring during image capture. If a malicious person\npresents a face photo of the registered user, they may bypass the\nauthentication process illegally. Such spoofing attacks need to be detected\nbefore face recognition. In this paper, we propose a spoofing attack detection\nmethod based on Vision Transformer (ViT) to detect minute differences between\nlive and spoofed face images. The proposed method utilizes the intermediate\nfeatures of ViT, which have a good balance between local and global features\nthat are important for spoofing attack detection, for calculating loss in\ntraining and score in inference. The proposed method also introduces two data\naugmentation methods: face anti-spoofing data augmentation and patch-wise data\naugmentation, to improve the accuracy of spoofing attack detection. We\ndemonstrate the effectiveness of the proposed method through experiments using\nthe OULU-NPU and SiW datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVision Transformer\uff08ViT\uff09\u7684\u6b3a\u9a97\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528ViT\u7684\u4e2d\u95f4\u7279\u5f81\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u9632\u6b62\u6076\u610f\u7528\u6237\u901a\u8fc7\u7167\u7247\u6b3a\u9a97\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\uff0c\u786e\u4fdd\u8ba4\u8bc1\u8fc7\u7a0b\u7684\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528ViT\u7684\u4e2d\u95f4\u7279\u5f81\u8ba1\u7b97\u8bad\u7ec3\u635f\u5931\u548c\u63a8\u7406\u5f97\u5206\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08\u53cd\u6b3a\u9a97\u6570\u636e\u589e\u5f3a\u548c\u5206\u5757\u6570\u636e\u589e\u5f3a\uff09\u3002", "result": "\u5728OULU-NPU\u548cSiW\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u6b3a\u9a97\u653b\u51fb\uff0c\u63d0\u5347\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2505.24063", "pdf": "https://arxiv.org/pdf/2505.24063", "abs": "https://arxiv.org/abs/2505.24063", "authors": ["Jiacheng Xie", "Yang Yu", "Ziyang Zhang", "Shuai Zeng", "Jiaxuan He", "Ayush Vasireddy", "Xiaoting Tang", "Congyu Guo", "Lening Zhao", "Congcong Jing", "Guanghui An", "Dong Xu"], "title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine", "categories": ["cs.CL", "cs.DB"], "comment": "22 pages, 4 figures", "summary": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.", "AI": {"tldr": "TCM-Ladder\u662f\u9996\u4e2a\u9488\u5bf9\u4e2d\u533b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e2d\u533b\u6838\u5fc3\u5b66\u79d1\uff0c\u5305\u542b52,000+\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e13\u95e8\u7684\u4e2d\u533b\u95ee\u7b54\u8bc4\u4f30\u65b9\u6cd5Ladder-Score\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u8303\u56f4\u6709\u9650\u4e14\u591a\u4e3a\u6587\u672c\u5f62\u5f0f\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u9700\u5ba2\u89c2\u8bc4\u4f30\u4e2d\u533b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u6784\u5efaTCM-Ladder\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u81ea\u52a8\u4e0e\u4eba\u5de5\u8fc7\u6ee4\uff0c\u6db5\u76d6\u591a\u79cd\u95ee\u9898\u7c7b\u578b\u548c\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\uff09\uff0c\u5e76\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5728TCM-Ladder\u4e0a\u8bc4\u4f30\u4e869\u4e2a\u901a\u7528\u9886\u57df\u548c5\u4e2a\u4e2d\u533b\u4e13\u7528LLM\uff0c\u63d0\u51fa\u4e86Ladder-Score\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "TCM-Ladder\u4e3a\u4e2d\u533b\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u57fa\u51c6\uff0c\u6570\u636e\u96c6\u548c\u6392\u884c\u699c\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2505.24404", "pdf": "https://arxiv.org/pdf/2505.24404", "abs": "https://arxiv.org/abs/2505.24404", "authors": ["Kanokphan Lertniphonphan", "Feng Chen", "Junda Xu", "Fengbu Lan", "Jun Xie", "Tao Zhang", "Zhepeng Wang"], "title": "PCIE_Interaction Solution for Ego4D Social Interaction Challenge", "categories": ["cs.CV"], "comment": null, "summary": "This report presents our team's PCIE_Interaction solution for the Ego4D\nSocial Interaction Challenge at CVPR 2025, addressing both Looking At Me (LAM)\nand Talking To Me (TTM) tasks. The challenge requires accurate detection of\nsocial interactions between subjects and the camera wearer, with LAM relying\nexclusively on face crop sequences and TTM combining speaker face crops with\nsynchronized audio segments. In the LAM track, we employ face quality\nenhancement and ensemble methods. For the TTM task, we extend visual\ninteraction analysis by fusing audio and visual cues, weighted by a visual\nquality score. Our approach achieved 0.81 and 0.71 mean average precision (mAP)\non the LAM and TTM challenges leader board. Code is available at\nhttps://github.com/KanokphanL/PCIE_Ego4D_Social_Interaction", "AI": {"tldr": "PCIE_Interaction\u65b9\u6848\u5728CVPR 2025 Ego4D\u793e\u4ea4\u4e92\u52a8\u6311\u6218\u4e2d\u89e3\u51b3\u4e86LAM\u548cTTM\u4efb\u52a1\uff0c\u901a\u8fc7\u9762\u90e8\u8d28\u91cf\u589e\u5f3a\u548c\u96c6\u6210\u65b9\u6cd5\uff08LAM\uff09\u53ca\u89c6\u542c\u878d\u5408\uff08TTM\uff09\uff0c\u5206\u522b\u53d6\u5f970.81\u548c0.71\u7684mAP\u3002", "motivation": "\u89e3\u51b3\u793e\u4ea4\u4e92\u52a8\u68c0\u6d4b\u4e2d\u7684LAM\uff08\u4ec5\u4f9d\u8d56\u9762\u90e8\u5e8f\u5217\uff09\u548cTTM\uff08\u7ed3\u5408\u9762\u90e8\u4e0e\u97f3\u9891\uff09\u4efb\u52a1\uff0c\u63d0\u5347\u51c6\u786e\u7387\u3002", "method": "LAM\u4efb\u52a1\u4f7f\u7528\u9762\u90e8\u8d28\u91cf\u589e\u5f3a\u548c\u96c6\u6210\u65b9\u6cd5\uff1bTTM\u4efb\u52a1\u901a\u8fc7\u89c6\u542c\u878d\u5408\u52a0\u6743\u89c6\u89c9\u8d28\u91cf\u5206\u6570\u3002", "result": "LAM\u4efb\u52a1mAP\u4e3a0.81\uff0cTTM\u4efb\u52a1mAP\u4e3a0.71\u3002", "conclusion": "PCIE_Interaction\u65b9\u6848\u5728LAM\u548cTTM\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.24098", "pdf": "https://arxiv.org/pdf/2505.24098", "abs": "https://arxiv.org/abs/2505.24098", "authors": ["Zhongmou He", "Yee Man Choi", "Kexun Zhang", "Jiabao Ji", "Junting Zhou", "Dejia Xu", "Ivan Bercovich", "Aidan Zhang", "Lei Li"], "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding", "categories": ["cs.CL"], "comment": null, "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.", "AI": {"tldr": "HARDTESTGEN\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7684\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u9a8c\u8bc1\u5668\u5728\u590d\u6742\u7f16\u7a0b\u95ee\u9898\u4e2d\u96be\u4ee5\u83b7\u53d6\u53ef\u9760\u6d4b\u8bd5\u7684\u96be\u9898\u3002", "motivation": "\u9a8c\u8bc1\u5668\u5728LLM\u63a8\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e3a\u590d\u6742\u7f16\u7a0b\u95ee\u9898\u751f\u6210\u53ef\u9760\u6d4b\u8bd5\u56f0\u96be\uff0c\u56e0\u4e3a\u9519\u8bef\u7684\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u96be\u4ee5\u901a\u8fc7\u4eba\u5de5\u7f16\u5199\u7684\u8fb9\u7f18\u6848\u4f8b\u68c0\u6d4b\u3002", "method": "\u63d0\u51faHARDTESTGEN\u6d41\u7a0b\uff0c\u5229\u7528LLM\u5408\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b47k\u4e2a\u95ee\u9898\u7684HARDTESTS\u6570\u636e\u96c6\u3002", "result": "HARDTESTGEN\u6d4b\u8bd5\u5728\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u65f6\uff0c\u7cbe\u786e\u5ea6\u63d0\u9ad811.3%\uff0c\u53ec\u56de\u7387\u63d0\u9ad817.5%\uff0c\u5bf9\u66f4\u96be\u95ee\u9898\u7cbe\u786e\u5ea6\u63d0\u5347\u53ef\u8fbe40%\u3002", "conclusion": "HARDTESTS\u6570\u636e\u96c6\u548c\u5408\u6210\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u8d28\u91cf\u548c\u6a21\u578b\u8bad\u7ec3\u6548\u679c\uff0c\u5e76\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.24406", "pdf": "https://arxiv.org/pdf/2505.24406", "abs": "https://arxiv.org/abs/2505.24406", "authors": ["Hanting Wang", "Tao Jin", "Wang Lin", "Shulei Wang", "Hai Huang", "Shengpeng Ji", "Zhou Zhao"], "title": "IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Bridge models in image restoration construct a diffusion process from\ndegraded to clear images. However, existing methods typically require training\na bridge model from scratch for each specific type of degradation, resulting in\nhigh computational costs and limited performance. This work aims to efficiently\nleverage pretrained generative priors within existing image restoration bridges\nto eliminate this requirement. The main challenge is that standard generative\nmodels are typically designed for a diffusion process that starts from pure\nnoise, while restoration tasks begin with a low-quality image, resulting in a\nmismatch in the state distributions between the two processes. To address this\nchallenge, we propose a transition equation that bridges two diffusion\nprocesses with the same endpoint distribution. Based on this, we introduce the\nIRBridge framework, which enables the direct utilization of generative models\nwithin image restoration bridges, offering a more flexible and adaptable\napproach to image restoration. Extensive experiments on six image restoration\ntasks demonstrate that IRBridge efficiently integrates generative priors,\nresulting in improved robustness and generalization performance. Code will be\navailable at GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIRBridge\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u751f\u6210\u6a21\u578b\u6539\u8fdb\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u79cd\u9000\u5316\u7c7b\u578b\u5355\u72ec\u8bad\u7ec3\u6865\u6a21\u578b\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u6865\u6a21\u578b\u9700\u8981\u4e3a\u6bcf\u79cd\u9000\u5316\u7c7b\u578b\u5355\u72ec\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6027\u80fd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u9884\u8bad\u7ec3\u7684\u751f\u6210\u5148\u9a8c\uff0c\u907f\u514d\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8fc7\u6e21\u65b9\u7a0b\uff0c\u8fde\u63a5\u4e24\u4e2a\u5177\u6709\u76f8\u540c\u7ec8\u70b9\u5206\u5e03\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165IRBridge\u6846\u67b6\uff0c\u76f4\u63a5\u5229\u7528\u751f\u6210\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u4fee\u590d\u3002", "result": "\u5728\u516d\u79cd\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIRBridge\u80fd\u9ad8\u6548\u6574\u5408\u751f\u6210\u5148\u9a8c\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "IRBridge\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.24105", "pdf": "https://arxiv.org/pdf/2505.24105", "abs": "https://arxiv.org/abs/2505.24105", "authors": ["Jiacheng Lin", "Zhenbang Wu", "Jimeng Sun"], "title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We present EHRMIND, a practical recipe for adapting large language models\n(LLMs) to complex clinical reasoning tasks using reinforcement learning with\nverifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,\nits application to healthcare contexts presents unique challenges due to the\nspecialized knowledge and reasoning required for electronic health record (EHR)\ninterpretation. Our pilot study on the MEDCALC benchmark reveals two key\nfailure modes: (1) misapplied knowledge, where models possess relevant medical\nknowledge but apply it incorrectly, and (2) missing knowledge, where models\nlack essential domain knowledge. To address these cases, EHRMIND applies a\ntwo-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that\ninjects missing domain knowledge, stabilizes subsequent training, and\nencourages structured, interpretable outputs; followed by RLVR, which\nreinforces outcome correctness and refines the model's decision-making. We\ndemonstrate the effectiveness of our method across diverse clinical\napplications, including medical calculations (MEDCALC), patient-trial matching\n(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers\nconsistent gains in accuracy, interpretability, and cross-task generalization.\nThese findings offer practical guidance for applying RLVR to enhance LLM\ncapabilities in healthcare settings.", "AI": {"tldr": "EHRMIND\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee5\u9002\u5e94\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u77e5\u8bc6\u8bef\u7528\u548c\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u89e3\u91ca\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u590d\u6742\u63a8\u7406\uff0cRLVR\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\uff1a\u8f7b\u91cf\u7ea7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6ce8\u5165\u7f3a\u5931\u77e5\u8bc6\u5e76\u7a33\u5b9a\u8bad\u7ec3\uff0c\u968f\u540e\u901a\u8fc7RLVR\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u5728\u591a\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\uff08\u5982MEDCALC\u3001TREC CLINICAL TRIALS\u3001EHRSHOT\uff09\uff0cEHRMIND\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EHRMIND\u4e3aRLVR\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u589e\u5f3a\u4e86LLM\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.24411", "pdf": "https://arxiv.org/pdf/2505.24411", "abs": "https://arxiv.org/abs/2505.24411", "authors": ["Feng Chen", "Kanokphan Lertniphonphan", "Qiancheng Yan", "Xiaohui Fan", "Jun Xie", "Tao Zhang", "Zhepeng Wang"], "title": "PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge", "categories": ["cs.CV"], "comment": null, "summary": "This report introduces our team's (PCIE_EgoPose) solutions for the EgoExo4D\nPose and Proficiency Estimation Challenges at CVPR2025. Focused on the\nintricate task of estimating 21 3D hand joints from RGB egocentric videos,\nwhich are complicated by subtle movements and frequent occlusions, we developed\nthe Hand Pose Vision Transformer (HP-ViT+). This architecture synergizes a\nVision Transformer and a CNN backbone, using weighted fusion to refine the hand\npose predictions. For the EgoExo4D Body Pose Challenge, we adopted a multimodal\nspatio-temporal feature integration strategy to address the complexities of\nbody pose estimation across dynamic contexts. Our methods achieved remarkable\nperformance: 8.31 PA-MPJPE in the Hand Pose Challenge and 11.25 MPJPE in the\nBody Pose Challenge, securing championship titles in both competitions. We\nextended our pose estimation solutions to the Proficiency Estimation task,\napplying core technologies such as transformer-based architectures. This\nextension enabled us to achieve a top-1 accuracy of 0.53, a SOTA result, in the\nDemonstrator Proficiency Estimation competition.", "AI": {"tldr": "\u56e2\u961f\u63d0\u51faHP-ViT+\u548c\u65f6\u7a7a\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u5728CVPR2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u51a0\u519b\uff0c\u5e76\u5728\u719f\u7ec3\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u3002", "motivation": "\u89e3\u51b3RGB\u89c6\u9891\u4e2d\u624b\u90e8\u548c\u8eab\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u590d\u6742\u95ee\u9898\uff0c\u5982\u7ec6\u5fae\u52a8\u4f5c\u548c\u906e\u6321\u3002", "method": "HP-ViT+\u7ed3\u5408Vision Transformer\u548cCNN\uff0c\u52a0\u6743\u878d\u5408\u4f18\u5316\u624b\u90e8\u59ff\u6001\uff1b\u8eab\u4f53\u59ff\u6001\u91c7\u7528\u591a\u6a21\u6001\u65f6\u7a7a\u7279\u5f81\u878d\u5408\u3002", "result": "\u624b\u90e8\u59ff\u6001PA-MPJPE 8.31\uff0c\u8eab\u4f53\u59ff\u6001MPJPE 11.25\uff0c\u719f\u7ec3\u5ea6\u4f30\u8ba1Top-1\u51c6\u786e\u73870.53\u3002", "conclusion": "\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.24119", "pdf": "https://arxiv.org/pdf/2505.24119", "abs": "https://arxiv.org/abs/2505.24119", "authors": ["Zheng-Xin Yong", "Beyza Ermis", "Marzieh Fadaee", "Stephen H. Bach", "Julia Kreutzer"], "title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86LLM\u5b89\u5168\u7814\u7a76\u7684\u8bed\u8a00\u591a\u6837\u6027\u95ee\u9898\uff0c\u6307\u51fa\u8be5\u9886\u57df\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u975e\u82f1\u8bed\u8bed\u8a00\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u591a\u8bed\u8a00\u5b89\u5168\u7814\u7a76\u7684\u5efa\u8bae\u3002", "motivation": "\u63ed\u793aLLM\u5b89\u5168\u7814\u7a76\u4e2d\u7684\u8bed\u8a00\u4e0d\u5e73\u7b49\u73b0\u8c61\uff0c\u63a8\u52a8\u66f4\u5305\u5bb9\u7684\u591a\u8bed\u8a00\u5b89\u5168\u7814\u7a76\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e862020-2024\u5e74\u95f4\u8fd1300\u7bc7\u6765\u81ea*ACL\u4f1a\u8bae\u548c\u7814\u8ba8\u4f1a\u7684\u8bba\u6587\u3002", "result": "\u53d1\u73b0\u975e\u82f1\u8bed\u8bed\u8a00\u7814\u7a76\u6781\u5c11\uff0c\u4e14\u82f1\u8bed\u7814\u7a76\u7f3a\u4e4f\u8bed\u8a00\u8bb0\u5f55\u5b9e\u8df5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u5b89\u5168\u7814\u7a76\u7684\u672a\u6765\u65b9\u5411\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5305\u5bb9\u7684AI\u5b89\u5168\u5b9e\u8df5\u3002"}}
{"id": "2505.24417", "pdf": "https://arxiv.org/pdf/2505.24417", "abs": "https://arxiv.org/abs/2505.24417", "authors": ["Runnan Lu", "Yuxuan Zhang", "Jiaming Liu", "Haofan Wang", "Yiren Song"], "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.", "AI": {"tldr": "EasyText\u662f\u4e00\u4e2a\u57fa\u4e8eDiT\u7684\u591a\u8bed\u8a00\u6587\u672c\u6e32\u67d3\u6846\u67b6\uff0c\u901a\u8fc7\u5b57\u7b26\u5b9a\u4f4d\u7f16\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u63d2\u503c\u6280\u672f\u5b9e\u73b0\u53ef\u63a7\u4e14\u7cbe\u786e\u7684\u6587\u672c\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6587\u672c\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u586b\u8865\u591a\u8bed\u8a00\u6587\u672c\u6e32\u67d3\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528DiT\u6846\u67b6\uff0c\u7ed3\u5408\u5b57\u7b26\u5b9a\u4f4d\u7f16\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u63d2\u503c\u6280\u672f\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEasyText\u5728\u591a\u8bed\u8a00\u6587\u672c\u6e32\u67d3\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u5e03\u5c40\u611f\u77e5\u6587\u672c\u96c6\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EasyText\u4e3a\u591a\u8bed\u8a00\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24133", "pdf": "https://arxiv.org/pdf/2505.24133", "abs": "https://arxiv.org/abs/2505.24133", "authors": ["Zefan Cai", "Wen Xiao", "Hanshi Sun", "Cheng Luo", "Yikai Zhang", "Ke Wan", "Yucheng Li", "Yeyang Zhou", "Li-Wen Chang", "Jiuxiang Gu", "Zhen Dong", "Anima Anandkumar", "Abedelkadir Asi", "Junjie Hu"], "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.", "AI": {"tldr": "R-KV\u662f\u4e00\u79cd\u9488\u5bf9\u63a8\u7406\u6a21\u578b\u4e2d\u5197\u4f59\u4ee4\u724c\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5728\u81ea\u53cd\u601d\u548c\u94fe\u5f0f\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8f93\u51fa\u8fc7\u957f\u5bfc\u81f4KV\u7f13\u5b58\u8fc7\u5927\uff0c\u4e14\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5728\u94fe\u5f0f\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faR-KV\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u538b\u7f29\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5197\u4f59\u4ee4\u724c\uff0c\u4fdd\u7559\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11KV\u7f13\u5b58\u3002", "result": "R-KV\u4ec5\u752810%\u7684KV\u7f13\u5b58\u5373\u53ef\u4fdd\u7559\u8fd1100%\u6027\u80fd\uff0c\u5185\u5b58\u8282\u770190%\uff0c\u541e\u5410\u91cf\u63d0\u53476.6\u500d\u3002", "conclusion": "R-KV\u5728\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.24431", "pdf": "https://arxiv.org/pdf/2505.24431", "abs": "https://arxiv.org/abs/2505.24431", "authors": ["Bozhong Zheng", "Jinye Gan", "Xiaohao Xu", "Wenqiao Li", "Xiaonan Huang", "Na Ni", "Yingna Wu"], "title": "Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation", "categories": ["cs.CV"], "comment": null, "summary": "3D point cloud anomaly detection is essential for robust vision systems but\nis challenged by pose variations and complex geometric anomalies. Existing\npatch-based methods often suffer from geometric fidelity issues due to discrete\nvoxelization or projection-based representations, limiting fine-grained anomaly\nlocalization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel\nframework that integrates 3D anomaly detection and repair by learning a\ncontinuous, pose-invariant shape representation. PASDF leverages a Pose\nAlignment Module for canonicalization and a SDF Network to dynamically\nincorporate pose, enabling implicit learning of high-fidelity anomaly repair\ntemplates from the continuous SDF. This facilitates precise pixel-level anomaly\nlocalization through an Anomaly-Aware Scoring Module. Crucially, the continuous\n3D representation in PASDF extends beyond detection, facilitating in-situ\nanomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate\nstate-of-the-art performance, achieving high object-level AUROC scores of 80.2%\nand 90.0%, respectively. These results highlight the effectiveness of\ncontinuous geometric representations in advancing 3D anomaly detection and\nfacilitating practical anomaly region repair. The code is available at\nhttps://github.com/ZZZBBBZZZ/PASDF to support further research.", "AI": {"tldr": "PASDF\u662f\u4e00\u79cd\u65b0\u578b3D\u70b9\u4e91\u5f02\u5e38\u68c0\u6d4b\u4e0e\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u3001\u59ff\u6001\u4e0d\u53d8\u7684\u5f62\u72b6\u8868\u793a\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5\u56e0\u79bb\u6563\u4f53\u7d20\u5316\u6216\u6295\u5f71\u8868\u793a\u5bfc\u81f4\u51e0\u4f55\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u5f02\u5e38\u5b9a\u4f4d\u3002", "method": "PASDF\u7ed3\u5408\u59ff\u6001\u5bf9\u9f50\u6a21\u5757\u548cSDF\u7f51\u7edc\uff0c\u52a8\u6001\u6574\u5408\u59ff\u6001\u4fe1\u606f\uff0c\u901a\u8fc7\u8fde\u7eedSDF\u9690\u5f0f\u5b66\u4e60\u9ad8\u4fdd\u771f\u4fee\u590d\u6a21\u677f\u3002", "result": "\u5728Real3D-AD\u548cAnomaly-ShapeNet\u4e0a\u5206\u522b\u8fbe\u523080.2%\u548c90.0%\u7684AUROC\u5206\u6570\u3002", "conclusion": "\u8fde\u7eed\u51e0\u4f55\u8868\u793a\u57283D\u5f02\u5e38\u68c0\u6d4b\u548c\u4fee\u590d\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cPASDF\u6846\u67b6\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24143", "pdf": "https://arxiv.org/pdf/2505.24143", "abs": "https://arxiv.org/abs/2505.24143", "authors": ["Jinglong Gao", "Xiao Ding", "Lingxiao Zou", "Bing Qin", "Ting Liu"], "title": "CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer", "categories": ["cs.CL"], "comment": "9 pages", "summary": "In-Context Learning (ICL) enhances the performance of large language models\n(LLMs) with demonstrations. However, obtaining these demonstrations primarily\nrelies on manual effort. In most real-world scenarios, users are often\nunwilling or unable to provide such demonstrations. Inspired by the human\nanalogy, we explore a new ICL paradigm CrossICL to study how to utilize\nexisting source task demonstrations in the ICL for target tasks, thereby\nobtaining reliable guidance without any additional manual effort. To explore\nthis, we first design a two-stage alignment strategy to mitigate the\ninterference caused by gaps across tasks, as the foundation for our\nexperimental exploration. Based on it, we conduct comprehensive exploration of\nCrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs,\nincluding GPT-4o. Experimental results demonstrate the effectiveness of\nCrossICL and provide valuable insights on questions like the criteria for\nselecting cross-task demonstrations, as well as the types of task-gap-induced\ninterference in CrossICL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCrossICL\uff0c\u4e00\u79cd\u5229\u7528\u6e90\u4efb\u52a1\u6f14\u793a\u63d0\u5347\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u7684\u65b0ICL\u8303\u5f0f\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709ICL\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6f14\u793a\uff0c\u4f46\u7528\u6237\u5e38\u4e0d\u613f\u6216\u65e0\u6cd5\u63d0\u4f9b\u3002\u53d7\u4eba\u7c7b\u7c7b\u6bd4\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u6e90\u4efb\u52a1\u6f14\u793a\u6307\u5bfc\u76ee\u6807\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u4e24\u9636\u6bb5\u5bf9\u9f50\u7b56\u7565\u4ee5\u51cf\u5c11\u4efb\u52a1\u95f4\u5e72\u6270\uff0c\u5e76\u5728875\u4e2aNLP\u4efb\u52a1\u548c6\u79cdLLM\uff08\u5305\u62ecGPT-4o\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCrossICL\u6709\u6548\uff0c\u5e76\u56de\u7b54\u4e86\u8de8\u4efb\u52a1\u6f14\u793a\u9009\u62e9\u6807\u51c6\u548c\u4efb\u52a1\u95f4\u5e72\u6270\u7c7b\u578b\u7b49\u95ee\u9898\u3002", "conclusion": "CrossICL\u4e3aICL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u8de8\u4efb\u52a1\u6f14\u793a\u7684\u5b9e\u7528\u51c6\u5219\u3002"}}
{"id": "2505.24441", "pdf": "https://arxiv.org/pdf/2505.24441", "abs": "https://arxiv.org/abs/2505.24441", "authors": ["Chunxu Liu", "Chi Xie", "Xiaxu Chen", "Wei Li", "Feng Zhu", "Rui Zhao", "Limin Wang"], "title": "SORCE: Small Object Retrieval in Complex Environments", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/MCG-NJU/SORCE", "summary": "Text-to-Image Retrieval (T2IR) is a highly valuable task that aims to match a\ngiven textual query to images in a gallery. Existing benchmarks primarily focus\non textual queries describing overall image semantics or foreground salient\nobjects, possibly overlooking inconspicuous small objects, especially in\ncomplex environments. Such small object retrieval is crucial, as in real-world\napplications, the targets of interest are not always prominent in the image.\nThus, we introduce SORCE (Small Object Retrieval in Complex Environments), a\nnew subfield of T2IR, focusing on retrieving small objects in complex images\nwith textual queries. We propose a new benchmark, SORCE-1K, consisting of\nimages with complex environments and textual queries describing less\nconspicuous small objects with minimal contextual cues from other salient\nobjects. Preliminary analysis on SORCE-1K finds that existing T2IR methods\nstruggle to capture small objects and encode all the semantics into a single\nembedding, leading to poor retrieval performance on SORCE-1K. Therefore, we\npropose to represent each image with multiple distinctive embeddings. We\nleverage Multimodal Large Language Models (MLLMs) to extract multiple\nembeddings for each image instructed by a set of Regional Prompts (ReP).\nExperimental results show that our multi-embedding approach through MLLM and\nReP significantly outperforms existing T2IR methods on SORCE-1K. Our\nexperiments validate the effectiveness of SORCE-1K for benchmarking SORCE\nperformances, highlighting the potential of multi-embedding representation and\ntext-customized MLLM features for addressing this task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SORCE\uff08\u590d\u6742\u73af\u5883\u4e2d\u7684\u5c0f\u7269\u4f53\u68c0\u7d22\uff09\uff0c\u4f5c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff08T2IR\uff09\u7684\u65b0\u5b50\u9886\u57df\uff0c\u5e76\u521b\u5efa\u4e86SORCE-1K\u57fa\u51c6\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u7d22\u5c0f\u7269\u4f53\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u548c\u533a\u57df\u63d0\u793a\uff08ReP\uff09\u751f\u6210\u591a\u5d4c\u5165\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709T2IR\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u56fe\u50cf\u8bed\u4e49\u6216\u663e\u8457\u524d\u666f\u7269\u4f53\uff0c\u5ffd\u7565\u4e86\u590d\u6742\u73af\u5883\u4e2d\u4e0d\u663e\u773c\u7684\u5c0f\u7269\u4f53\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd9\u4e9b\u5c0f\u7269\u4f53\u68c0\u7d22\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faSORCE-1K\u57fa\u51c6\uff0c\u5e76\u5229\u7528MLLM\u548cReP\u4e3a\u6bcf\u5f20\u56fe\u50cf\u751f\u6210\u591a\u4e2a\u5d4c\u5165\u8868\u793a\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u5c0f\u7269\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u5d4c\u5165\u65b9\u6cd5\u5728SORCE-1K\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709T2IR\u65b9\u6cd5\u3002", "conclusion": "SORCE-1K\u662f\u8bc4\u4f30\u5c0f\u7269\u4f53\u68c0\u7d22\u6027\u80fd\u7684\u6709\u6548\u57fa\u51c6\uff0c\u591a\u5d4c\u5165\u8868\u793a\u548c\u6587\u672c\u5b9a\u5236\u7684MLLM\u7279\u5f81\u5bf9\u6b64\u4efb\u52a1\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.24147", "pdf": "https://arxiv.org/pdf/2505.24147", "abs": "https://arxiv.org/abs/2505.24147", "authors": ["Chiwei Zhu", "Benfeng Xu", "An Yang", "Junyang Lin", "Quan Wang", "Chang Zhou", "Zhendong Mao"], "title": "Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability", "categories": ["cs.CL"], "comment": "To be published in ACL 2025 Findings. (Work originally done in Jan\n  2024)", "summary": "Training language models with rationales augmentation has been shown to be\nbeneficial in many existing works. In this paper, we identify that such a\nprevailing view does not hold consistently. We conduct comprehensive\ninvestigations to thoroughly inspect the impact of rationales on model\nperformance as well as a novel perspective of model reliability. The results\nlead to several key findings that add new insights upon existing\nunderstandings: 1) Rationales can, at times, deteriorate model performance; 2)\nRationales can, at times, improve model reliability, even outperforming their\nuntrained counterparts; 3) A linear correspondence exists in between the\nperformance and reliability improvements, while both are driven by the\nintrinsic difficulty of the task. These findings provide informative\nregulations on the broad utilization of rationales and raise critical\nimplications on the procedure of explicitly aligning language models with\nimplicit human thoughts. Codes can be found at\nhttps://github.com/Ignoramus0817/rationales.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u7406\u6027\u5e76\u4e0d\u603b\u662f\u63d0\u5347\u6027\u80fd\uff0c\u6709\u65f6\u4f1a\u964d\u4f4e\u6027\u80fd\u6216\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4e14\u6027\u80fd\u4e0e\u53ef\u9760\u6027\u6539\u8fdb\u4e4b\u95f4\u5b58\u5728\u7ebf\u6027\u5173\u7cfb\u3002", "motivation": "\u63a2\u8ba8\u7406\u6027\u589e\u5f3a\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u548c\u53ef\u9760\u6027\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u6311\u6218\u73b0\u6709\u89c2\u70b9\u3002", "method": "\u901a\u8fc7\u5168\u9762\u8c03\u67e5\uff0c\u5206\u6790\u7406\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u548c\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u5185\u5728\u96be\u5ea6\u7684\u89c6\u89d2\u3002", "result": "1) \u7406\u6027\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff1b2) \u7406\u6027\u53ef\u80fd\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\uff1b3) \u6027\u80fd\u4e0e\u53ef\u9760\u6027\u6539\u8fdb\u4e4b\u95f4\u5b58\u5728\u7ebf\u6027\u5173\u7cfb\uff0c\u4e14\u53d7\u4efb\u52a1\u96be\u5ea6\u9a71\u52a8\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u6027\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u5bf9\u6a21\u578b\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7684\u663e\u5f0f\u5bf9\u9f50\u63d0\u51fa\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.24443", "pdf": "https://arxiv.org/pdf/2505.24443", "abs": "https://arxiv.org/abs/2505.24443", "authors": ["Heejo Kong", "Sung-Jin Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)", "summary": "Conventional semi-supervised learning (SSL) ideally assumes that labeled and\nunlabeled data share an identical class distribution, however in practice, this\nassumption is easily violated, as unlabeled data often includes unknown class\ndata, i.e., outliers. The outliers are treated as noise, considerably degrading\nthe performance of SSL models. To address this drawback, we propose a novel\nframework, Diversify and Conquer (DAC), to enhance SSL robustness in the\ncontext of open-set semi-supervised learning. In particular, we note that\nexisting open-set SSL methods rely on prediction discrepancies between inliers\nand outliers from a single model trained on labeled data. This approach can be\neasily failed when the labeled data is insufficient, leading to performance\ndegradation that is worse than naive SSL that do not account for outliers. In\ncontrast, our approach exploits prediction disagreements among multiple models\nthat are differently biased towards the unlabeled distribution. By leveraging\nthe discrepancies arising from training on unlabeled data, our method enables\nrobust outlier detection even when the labeled data is underspecified. Our key\ncontribution is constructing a collection of differently biased models through\na single training process. By encouraging divergent heads to be differently\nbiased towards outliers while making consistent predictions for inliers, we\nexploit the disagreement among these heads as a measure to identify unknown\nconcepts. Our code is available at https://github.com/heejokong/DivCon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAC\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u6a21\u578b\u5bf9\u672a\u6807\u8bb0\u6570\u636e\u7684\u4e0d\u540c\u9884\u6d4b\u5dee\u5f02\uff0c\u589e\u5f3a\u5f00\u653e\u96c6\u534a\u76d1\u7763\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u534a\u76d1\u7763\u5b66\u4e60\u5047\u8bbe\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u5171\u4eab\u76f8\u540c\u7684\u7c7b\u522b\u5206\u5e03\uff0c\u4f46\u5b9e\u9645\u4e2d\u672a\u6807\u8bb0\u6570\u636e\u5e38\u5305\u542b\u5f02\u5e38\u7c7b\uff08\u79bb\u7fa4\u70b9\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u7684\u9884\u6d4b\u5dee\u5f02\uff0c\u5728\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faDAC\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u4e0d\u540c\u504f\u89c1\u7684\u6a21\u578b\uff0c\u5229\u7528\u5176\u5bf9\u672a\u6807\u8bb0\u6570\u636e\u7684\u9884\u6d4b\u5dee\u5f02\u6765\u68c0\u6d4b\u79bb\u7fa4\u70b9\u3002\u901a\u8fc7\u9f13\u52b1\u6a21\u578b\u5bf9\u79bb\u7fa4\u70b9\u4ea7\u751f\u4e0d\u540c\u9884\u6d4b\u800c\u5bf9\u5185\u70b9\u4fdd\u6301\u4e00\u81f4\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u79bb\u7fa4\u70b9\u8bc6\u522b\u3002", "result": "DAC\u6846\u67b6\u5728\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u8bc6\u522b\u79bb\u7fa4\u70b9\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "DAC\u901a\u8fc7\u591a\u6a21\u578b\u9884\u6d4b\u5dee\u5f02\u89e3\u51b3\u4e86\u5f00\u653e\u96c6\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u79bb\u7fa4\u70b9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.24163", "pdf": "https://arxiv.org/pdf/2505.24163", "abs": "https://arxiv.org/abs/2505.24163", "authors": ["Jiaqi Sun", "Shiyou Qian", "Zhangchi Han", "Wei Li", "Zelin Qian", "Dingyu Yang", "Jian Cao", "Guangtao Xue"], "title": "LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing", "categories": ["cs.CL", "cs.AI"], "comment": "Submitting to EDBT 2026", "summary": "Knowledge Graphs (KGs) structure real-world entities and their relationships\ninto triples, enhancing machine reasoning for various tasks. While\ndomain-specific KGs offer substantial benefits, their manual construction is\noften inefficient and requires specialized knowledge. Recent approaches for\nknowledge graph construction (KGC) based on large language models (LLMs), such\nas schema-guided KGC and reference knowledge integration, have proven\nefficient. However, these methods are constrained by their reliance on manually\ndefined schema, single-document processing, and public-domain references,\nmaking them less effective for domain-specific corpora that exhibit complex\nknowledge dependencies and specificity, as well as limited reference knowledge.\nTo address these challenges, we propose LKD-KGC, a novel framework for\nunsupervised domain-specific KG construction. LKD-KGC autonomously analyzes\ndocument repositories to infer knowledge dependencies, determines optimal\nprocessing sequences via LLM driven prioritization, and autoregressively\ngenerates entity schema by integrating hierarchical inter-document contexts.\nThis schema guides the unsupervised extraction of entities and relationships,\neliminating reliance on predefined structures or external knowledge. Extensive\nexperiments show that compared with state-of-the-art baselines, LKD-KGC\ngenerally achieves improvements of 10% to 20% in both precision and recall\nrate, demonstrating its potential in constructing high-quality domain-specific\nKGs.", "AI": {"tldr": "LKD-KGC\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6587\u6863\u5e93\u63a8\u65ad\u77e5\u8bc6\u4f9d\u8d56\u5173\u7cfb\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684\u4f18\u5148\u7ea7\u786e\u5b9a\u5904\u7406\u987a\u5e8f\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u5b9e\u4f53\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6784\u5efa\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u7684\u624b\u52a8\u6784\u5efa\u6548\u7387\u4f4e\u4e0b\u4e14\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u624b\u52a8\u5b9a\u4e49\u6a21\u5f0f\u3001\u5355\u6587\u6863\u5904\u7406\u548c\u516c\u5171\u9886\u57df\u53c2\u8003\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u77e5\u8bc6\u4f9d\u8d56\u548c\u7279\u5b9a\u9886\u57df\u9700\u6c42\u3002", "method": "LKD-KGC\u901a\u8fc7\u5206\u6790\u6587\u6863\u5e93\u63a8\u65ad\u77e5\u8bc6\u4f9d\u8d56\uff0c\u5229\u7528LLM\u9a71\u52a8\u4f18\u5148\u7ea7\u786e\u5b9a\u5904\u7406\u987a\u5e8f\uff0c\u81ea\u52a8\u751f\u6210\u5b9e\u4f53\u6a21\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u65e0\u76d1\u7763\u63d0\u53d6\u5b9e\u4f53\u548c\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLKD-KGC\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8610%\u81f320%\u3002", "conclusion": "LKD-KGC\u5728\u6784\u5efa\u9ad8\u8d28\u91cf\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.24466", "pdf": "https://arxiv.org/pdf/2505.24466", "abs": "https://arxiv.org/abs/2505.24466", "authors": ["Yingjia Xu", "Jinlin Wu", "Zhen Chen", "Daming Gao", "Yang Yang", "Zhen Lei", "Min Cao"], "title": "SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking", "categories": ["cs.CV"], "comment": "22 pages, 7 figures. Under review", "summary": "Text-based person retrieval aims to identify a target individual from a\ngallery of images based on a natural language description. It presents a\nsignificant challenge due to the complexity of real-world scenes and the\nambiguity of appearance-related descriptions. Existing methods primarily\nemphasize appearance-based cross-modal retrieval, often neglecting the\ncontextual information embedded within the scene, which can offer valuable\ncomplementary insights for retrieval. To address this, we introduce\nSCENEPERSON-13W, a large-scale dataset featuring over 100,000 scenes with rich\nannotations covering both pedestrian appearance and environmental cues. Based\non this, we propose SA-Person, a two-stage retrieval framework. In the first\nstage, it performs discriminative appearance grounding by aligning textual cues\nwith pedestrian-specific regions. In the second stage, it introduces\nSceneRanker, a training-free, scene-aware re-ranking method leveraging\nmultimodal large language models to jointly reason over pedestrian appearance\nand the global scene context. Experiments on SCENEPERSON-13W validate the\neffectiveness of our framework in challenging scene-level retrieval scenarios.\nThe code and dataset will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u7684\u4eba\u7269\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u884c\u4eba\u5916\u89c2\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5916\u89c2\u8de8\u6a21\u6001\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u573a\u666f\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u53ef\u80fd\u5bf9\u68c0\u7d22\u6709\u91cd\u8981\u8865\u5145\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86SA-Person\u6846\u67b6\uff0c\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u6587\u672c\u4e0e\u884c\u4eba\u533a\u57df\u5bf9\u9f50\u8fdb\u884c\u5916\u89c2\u68c0\u7d22\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165SceneRanker\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8054\u5408\u63a8\u7406\u5916\u89c2\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u3002", "result": "\u5728SCENEPERSON-13W\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u590d\u6742\u573a\u666f\u68c0\u7d22\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u5916\u89c2\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6587\u672c\u4eba\u7269\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2505.24164", "pdf": "https://arxiv.org/pdf/2505.24164", "abs": "https://arxiv.org/abs/2505.24164", "authors": ["Shilin Xu", "Yanwei Li", "Rui Yang", "Tao Zhang", "Yueyi Sun", "Wei Chow", "Linfeng Li", "Hang Song", "Qi Xu", "Yunhai Tong", "Xiangtai Li", "Hao Fei"], "title": "Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent works on large language models (LLMs) have successfully demonstrated\nthe emergence of reasoning capabilities via reinforcement learning (RL).\nAlthough recent efforts leverage group relative policy optimization (GRPO) for\nMLLMs post-training, they constantly explore one specific aspect, such as\ngrounding tasks, math problems, or chart analysis. There are no works that can\nleverage multi-source MLLM tasks for stable reinforcement learning. In this\nwork, we present a unified perspective to solve this problem. We present\nMixed-R1, a unified yet straightforward framework that contains a mixed reward\nfunction design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).\nWe first design a data engine to select high-quality examples to build the\nMixed-45K post-training dataset. Then, we present a Mixed-Reward design, which\ncontains various reward functions for various MLLM tasks. In particular, it has\nfour different reward functions: matching reward for binary answer or\nmultiple-choice problems, chart reward for chart-aware datasets, IoU reward for\ngrounding problems, and open-ended reward for long-form text responses such as\ncaption datasets. To handle the various long-form text content, we propose a\nnew open-ended reward named Bidirectional Max-Average Similarity (BMAS) by\nleveraging tokenizer embedding matching between the generated response and the\nground truth. Extensive experiments show the effectiveness of our proposed\nmethod on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.\nOur dataset and model are available at https://github.com/xushilin1/mixed-r1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMixed-R1\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u548c\u6df7\u5408\u6570\u636e\u96c6\uff08Mixed-45K\uff09\u89e3\u51b3\u591a\u6e90MLLM\u4efb\u52a1\u7684\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u5355\u4e00\u4efb\u52a1\uff08\u5982\u6570\u5b66\u95ee\u9898\u6216\u56fe\u8868\u5206\u6790\uff09\uff0c\u7f3a\u4e4f\u591a\u6e90MLLM\u4efb\u52a1\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u6570\u636e\u5f15\u64ce\u6784\u5efaMixed-45K\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u5305\u542b\u56db\u79cd\u5956\u52b1\u51fd\u6570\u7684Mixed-Reward\uff08\u5339\u914d\u5956\u52b1\u3001\u56fe\u8868\u5956\u52b1\u3001IoU\u5956\u52b1\u548c\u5f00\u653e\u5f0f\u5956\u52b1BMAS\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMixed-R1\u5728\u591a\u79cdMLLM\uff08\u5982Qwen2.5-VL\u548cIntern-VL\uff09\u4e0a\u6709\u6548\u3002", "conclusion": "Mixed-R1\u4e3a\u591a\u6e90MLLM\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.24475", "pdf": "https://arxiv.org/pdf/2505.24475", "abs": "https://arxiv.org/abs/2505.24475", "authors": ["Cheng Zeng", "Xiatian Qi", "Chi Chen", "Kai Sun", "Wangle Zhang", "Yuxuan Liu", "Yan Meng", "Bisheng Yang"], "title": "SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds", "categories": ["cs.CV"], "comment": "18 pages, 8 figures", "summary": "Transformers have been seldom employed in point cloud roof plane instance\nsegmentation, which is the focus of this study, and existing superpoint\nTransformers suffer from limited performance due to the use of low-quality\nsuperpoints. To address this challenge, we establish two criteria that\nhigh-quality superpoints for Transformers should satisfy and introduce a\ncorresponding two-stage superpoint generation process. The superpoints\ngenerated by our method not only have accurate boundaries, but also exhibit\nconsistent geometric sizes and shapes, both of which greatly benefit the\nfeature learning of superpoint Transformers. To compensate for the limitations\nof deep learning features when the training set size is limited, we incorporate\nmultidimensional handcrafted features into the model. Additionally, we design a\ndecoder that combines a Kolmogorov-Arnold Network with a Transformer module to\nimprove instance prediction and mask extraction. Finally, our network's\npredictions are refined using traditional algorithm-based postprocessing. For\nevaluation, we annotated a real-world dataset and corrected annotation errors\nin the existing RoofN3D dataset. Experimental results show that our method\nachieves state-of-the-art performance on our dataset, as well as both the\noriginal and reannotated RoofN3D datasets. Moreover, our model is not sensitive\nto plane boundary annotations during training, significantly reducing the\nannotation burden. Through comprehensive experiments, we also identified key\nfactors influencing roof plane segmentation performance: in addition to roof\ntypes, variations in point cloud density, density uniformity, and 3D point\nprecision have a considerable impact. These findings underscore the importance\nof incorporating data augmentation strategies that account for point cloud\nquality to enhance model robustness under diverse and challenging conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u70b9\u4e91\u5c4b\u9876\u5e73\u9762\u5b9e\u4f8b\u5206\u5272\u7684\u4e24\u9636\u6bb5\u8d85\u70b9\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u7ef4\u624b\u5de5\u7279\u5f81\u548c\u6539\u8fdb\u7684\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86Transformer\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8d85\u70b9Transformer\u5728\u70b9\u4e91\u5c4b\u9876\u5e73\u9762\u5206\u5272\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u4f4e\u8d28\u91cf\u8d85\u70b9\u7684\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u9ad8\u8d28\u91cf\u8d85\u70b9\u7684\u751f\u6210\u6807\u51c6\u53ca\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff0c\u7ed3\u5408\u624b\u5de5\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u5408Kolmogorov-Arnold\u7f51\u7edc\u4e0eTransformer\u6a21\u5757\u7684\u89e3\u7801\u5668\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u5bf9\u8fb9\u754c\u6807\u6ce8\u4e0d\u654f\u611f\uff0c\u51cf\u5c11\u4e86\u6807\u6ce8\u8d1f\u62c5\u3002", "conclusion": "\u70b9\u4e91\u5bc6\u5ea6\u3001\u5747\u5300\u6027\u548c\u7cbe\u5ea6\u5bf9\u5206\u5272\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.24165", "pdf": "https://arxiv.org/pdf/2505.24165", "abs": "https://arxiv.org/abs/2505.24165", "authors": ["Yixuan Wang", "Shiqi Zhou", "Chuanzhe Guo", "Qingfu Zhu"], "title": "Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection", "categories": ["cs.CL"], "comment": "Accepted as Findings of ACL 2025", "summary": "Evol-Instruct has made significant improvements as a data synthesis method in\nseveral areas. Existing methods typically rely on a fixed set of strategies to\nevolve, which require manual design and are monolithic in form. In addition,\niterative evolution also makes the acquisition of hard samples expensive. In\nview of this, we propose the Tag-Evol framework, a more diverse and efficient\ninstruction evolving method. Specifically, Tag-Evol uses diverse and specific\nknowledge tags as strategies to achieve controlled evolution by injecting\ndifferent combinations of tags into the original instructions. Experiments with\nmultiple backbones in diverse domain benchmarks show that the proposed method\ngenerates significantly better evolved data than other methods. Furthermore, we\nconduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is\nnot only efficient but also generates more diverse and challenging data.", "AI": {"tldr": "Tag-Evol\u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u6807\u7b7e\u7684\u6307\u4ee4\u6f14\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6837\u5316\u6807\u7b7e\u7ec4\u5408\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u6570\u636e\u5408\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7b56\u7565\uff0c\u9700\u624b\u52a8\u8bbe\u8ba1\u4e14\u5f62\u5f0f\u5355\u4e00\uff0c\u8fed\u4ee3\u6f14\u5316\u83b7\u53d6\u56f0\u96be\u6837\u672c\u6210\u672c\u9ad8\u3002", "method": "Tag-Evol\u5229\u7528\u591a\u6837\u5316\u77e5\u8bc6\u6807\u7b7e\u4f5c\u4e3a\u7b56\u7565\uff0c\u901a\u8fc7\u6ce8\u5165\u4e0d\u540c\u6807\u7b7e\u7ec4\u5408\u5b9e\u73b0\u6307\u4ee4\u7684\u53d7\u63a7\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTag-Evol\u751f\u6210\u7684\u6f14\u5316\u6570\u636e\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u66f4\u9ad8\u6548\u3001\u591a\u6837\u4e14\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "Tag-Evol\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u591a\u6837\u5316\u7684\u6307\u4ee4\u6f14\u5316\u6846\u67b6\uff0c\u80fd\u751f\u6210\u66f4\u4f18\u8d28\u7684\u6570\u636e\u3002"}}
{"id": "2505.24476", "pdf": "https://arxiv.org/pdf/2505.24476", "abs": "https://arxiv.org/abs/2505.24476", "authors": ["Yuting Zhang", "Hao Lu", "Qingyong Hu", "Yin Wang", "Kaishen Yuan", "Xin Liu", "Kaishun Wu"], "title": "Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Periodic or quasi-periodic phenomena reveal intrinsic characteristics in\nvarious natural processes, such as weather patterns, movement behaviors,\ntraffic flows, and biological signals. Given that these phenomena span multiple\nmodalities, the capabilities of Multimodal Large Language Models (MLLMs) offer\npromising potential to effectively capture and understand their complex nature.\nHowever, current MLLMs struggle with periodic tasks due to limitations in: 1)\nlack of temporal modelling and 2) conflict between short and long periods. This\npaper introduces Period-LLM, a multimodal large language model designed to\nenhance the performance of periodic tasks across various modalities, and\nconstructs a benchmark of various difficulty for evaluating the cross-modal\nperiodic capabilities of large models. Specially, We adopt an \"Easy to Hard\nGeneralization\" paradigm, starting with relatively simple text-based tasks and\nprogressing to more complex visual and multimodal tasks, ensuring that the\nmodel gradually builds robust periodic reasoning capabilities. Additionally, we\npropose a \"Resisting Logical Oblivion\" optimization strategy to maintain\nperiodic reasoning abilities during semantic alignment. Extensive experiments\ndemonstrate the superiority of the proposed Period-LLM over existing MLLMs in\nperiodic tasks. The code is available at\nhttps://github.com/keke-nice/Period-LLM.", "AI": {"tldr": "Period-LLM\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709MLLM\u5728\u5468\u671f\u6027\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u9010\u6b65\u5b66\u4e60\u548c\u4f18\u5316\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5468\u671f\u6027\u73b0\u8c61\u5728\u81ea\u7136\u8fc7\u7a0b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u73b0\u6709MLLM\u56e0\u7f3a\u4e4f\u65f6\u95f4\u5efa\u6a21\u548c\u5468\u671f\u51b2\u7a81\u95ee\u9898\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u4efb\u52a1\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u201c\u62b5\u6297\u903b\u8f91\u9057\u5fd8\u201d\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePeriod-LLM\u5728\u5468\u671f\u6027\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709MLLM\u3002", "conclusion": "Period-LLM\u4e3a\u591a\u6a21\u6001\u5468\u671f\u6027\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.24174", "pdf": "https://arxiv.org/pdf/2505.24174", "abs": "https://arxiv.org/abs/2505.24174", "authors": ["Ryota Miyano", "Yuki Arase"], "title": "Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ACL2025 Findings", "summary": "This study proposes a simple yet effective LoRA merge method to achieve LLM\nadaptation for low-resource language generation tasks. The LoRA merge\ntechnique, which integrates multiple LoRA modules trained on different tasks,\nhas gained attention as an effective and efficient approach for adapting LLMs\nto target tasks. However, previous methods are limited in adaptability as they\nkeep the LoRA parameters frozen. Additionally, the low-resource problem has\nbeen out of their scope. We propose a LoRA merge method that updates and prunes\nLoRA parameters through fine-tuning with minimal target task data, which allows\nfiner-grained adjustments of LoRA parameters and enhancement of task\nadaptability. Extensive experiments have been conducted taking summarization as\na benchmark task. Our datasets cover various domains and multiple languages of\nEnglish and Japanese. The results confirm that the proposed method achieves\nsignificant and consistent improvements in task adaptability over the previous\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684LoRA\u5408\u5e76\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u5fae\u8c03\u66f4\u65b0\u548c\u4fee\u526aLoRA\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709LoRA\u5408\u5e76\u65b9\u6cd5\u56e0\u53c2\u6570\u51bb\u7ed3\u800c\u9002\u5e94\u6027\u6709\u9650\uff0c\u4e14\u672a\u89e3\u51b3\u4f4e\u8d44\u6e90\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5fae\u8c03\u66f4\u65b0\u548c\u4fee\u526aLoRA\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u76ee\u6807\u4efb\u52a1\u6570\u636e\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8c03\u6574\u3002", "result": "\u5728\u591a\u79cd\u9886\u57df\u548c\u8bed\u8a00\uff08\u82f1\u8bed\u548c\u65e5\u8bed\uff09\u7684\u6458\u8981\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574LoRA\u53c2\u6570\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2505.24481", "pdf": "https://arxiv.org/pdf/2505.24481", "abs": "https://arxiv.org/abs/2505.24481", "authors": ["Jing Huang", "Yongkang Zhao", "Yuhan Li", "Zhitao Dai", "Cheng Chen", "Qiying Lai"], "title": "ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 5 tables", "summary": "The U-shaped encoder-decoder architecture with skip connections has become a\nprevailing paradigm in medical image segmentation due to its simplicity and\neffectiveness. While many recent works aim to improve this framework by\ndesigning more powerful encoders and decoders, employing advanced convolutional\nneural networks (CNNs) for local feature extraction, Transformers or state\nspace models (SSMs) such as Mamba for global context modeling, or hybrid\ncombinations of both, these methods often struggle to fully utilize pretrained\nvision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To\nbridge this gap, we introduce ACM-UNet, a general-purpose segmentation\nframework that retains a simple UNet-like design while effectively\nincorporating pretrained CNNs and Mamba models through a lightweight adapter\nmechanism. This adapter resolves architectural incompatibilities and enables\nthe model to harness the complementary strengths of CNNs and SSMs-namely,\nfine-grained local detail extraction and long-range dependency modeling.\nAdditionally, we propose a hierarchical multi-scale wavelet transform module in\nthe decoder to enhance feature fusion and reconstruction fidelity. Extensive\nexperiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet\nachieves state-of-the-art performance while remaining computationally\nefficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the\nSynapse dataset with 17.93G FLOPs, showcasing its effectiveness and\nscalability. Code is available at: https://github.com/zyklcode/ACM-UNet.", "AI": {"tldr": "ACM-UNet\u662f\u4e00\u79cd\u901a\u7528\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u673a\u5236\u6709\u6548\u7ed3\u5408\u9884\u8bad\u7ec3\u7684CNN\u548cMamba\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5728Synapse\u548cACDC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u4e3b\u5e72\uff08\u5982ResNet\u3001ViT\u3001VMamba\uff09\u7684\u7ed3\u6784\u4f18\u52bf\uff0cACM-UNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faACM-UNet\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u673a\u5236\u548c\u5206\u5c42\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u6a21\u5757\uff0c\u4ee5\u878d\u5408CNN\u548cSSM\u7684\u4f18\u52bf\u3002", "result": "\u5728Synapse\u6570\u636e\u96c6\u4e0a\u8fbe\u523085.12% Dice Score\u548c13.89mm HD95\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0817.93G FLOPs\uff09\u3002", "conclusion": "ACM-UNet\u5728\u4fdd\u6301\u7b80\u5355UNet\u8bbe\u8ba1\u7684\u540c\u65f6\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002"}}
{"id": "2505.24187", "pdf": "https://arxiv.org/pdf/2505.24187", "abs": "https://arxiv.org/abs/2505.24187", "authors": ["Mikhail L. Arbuzov", "Alexey A. Shvets", "Sisong Beir"], "title": "Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The prevailing assumption of an exponential decay in large language model\n(LLM) reliability with sequence length, predicated on independent per-token\nerror probabilities, posits an inherent limitation for long autoregressive\noutputs. Our research fundamentally challenges this view by synthesizing\nemerging evidence that LLM errors are not uniformly distributed but are\nconcentrated at sparse \"key tokens\" ($5-10\\%$ of total tokens) representing\ncritical decision junctions. By distinguishing these high-impact tokens from\nthe increasingly predictable majority, we introduce a new reliability formula\nexplaining the sustained coherence of modern LLMs over thousands of tokens.\nConverging research streams reveal that long-context performance primarily\ndepends on accurately navigating a few crucial semantic decision points rather\nthan on uniform token-level accuracy, enabling targeted strategies that\nsignificantly outperform brute-force approaches. We thus propose a framework\nfor next-generation systems centered on selective preservation of semantically\nvital tokens, dynamic computational allocation at uncertain decision\nboundaries, multi-path exploration at ambiguities, and architectures aligned\nwith natural semantic domains. This marks a fundamental shift from raw scaling\nto strategic reasoning, promising breakthrough performance without\nproportionate computational scaling and offering a more nuanced understanding\nthat supersedes the exponential decay hypothesis, thereby opening pathways\ntoward substantially more powerful and efficient language systems.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u70b9\uff0c\u8ba4\u4e3aLLM\u7684\u9519\u8bef\u96c6\u4e2d\u5728\u5c11\u6570\u5173\u952e\u4ee4\u724c\u4e0a\uff0c\u800c\u975e\u5747\u5300\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u65b0\u6846\u67b6\u4ee5\u63d0\u5347\u957f\u5e8f\u5217\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5047\u8bbe\u8ba4\u4e3aLLM\u53ef\u9760\u6027\u968f\u5e8f\u5217\u957f\u5ea6\u6307\u6570\u8870\u51cf\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u9519\u8bef\u96c6\u4e2d\u5728\u5c11\u6570\u5173\u952e\u4ee4\u724c\u4e0a\uff0c\u6311\u6218\u4e86\u8fd9\u4e00\u89c2\u70b9\u3002", "method": "\u901a\u8fc7\u533a\u5206\u9ad8\u5f71\u54cd\u529b\u4ee4\u724c\u548c\u53ef\u9884\u6d4b\u4ee4\u724c\uff0c\u63d0\u51fa\u65b0\u53ef\u9760\u6027\u516c\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u6846\u67b6\u4ee5\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u4f9d\u8d56\u4e8e\u5173\u952e\u8bed\u4e49\u51b3\u7b56\u70b9\uff0c\u800c\u975e\u5747\u5300\u4ee4\u724c\u51c6\u786e\u6027\uff0c\u65b0\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4ece\u89c4\u6a21\u6269\u5c55\u8f6c\u5411\u6218\u7565\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4e3a\u66f4\u9ad8\u6548\u5f3a\u5927\u7684\u8bed\u8a00\u7cfb\u7edf\u5f00\u8f9f\u65b0\u8def\u5f84\u3002"}}
{"id": "2505.24489", "pdf": "https://arxiv.org/pdf/2505.24489", "abs": "https://arxiv.org/abs/2505.24489", "authors": ["Anasse Boutayeb", "Iyad Lahsen-cherif", "Ahmed El Khadimi"], "title": "Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures, paper accepted at the 29th International\n  Conference on Knowledge-Based and Intelligent Information and Engineering\n  Systems (KES 2025), Osaka, Japan", "summary": "Object detection has recently seen an interesting trend in terms of the most\ninnovative research work, this task being of particular importance in the field\nof remote sensing, given the consistency of these images in terms of\ngeographical coverage and the objects present. Furthermore, Deep Learning (DL)\nmodels, in particular those based on Transformers, are especially relevant for\nvisual computing tasks in general, and target detection in particular. Thus,\nthe present work proposes an application of Deformable-DETR model, a specific\narchitecture using deformable attention mechanisms, on remote sensing images in\ntwo different modes, especially optical and Synthetic Aperture Radar (SAR). To\nachieve this objective, two datasets are used, one optical, which is Pleiades\nAircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset\n(SSDD). The results of a 10-fold stratified validation showed that the proposed\nmodel performed particularly well, obtaining an F1 score of 95.12% for the\noptical dataset and 94.54% for SSDD, while comparing these results with several\nmodels detections, especially those based on CNNs and transformers, as well as\nthose specifically designed to detect different object classes in remote\nsensing images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDeformable-DETR\u6a21\u578b\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u5149\u5b66\u548cSAR\u9065\u611f\u56fe\u50cf\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u5728DL\u6a21\u578b\uff08\u5c24\u5176\u662f\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff09\u4e2d\u5177\u6709\u91cd\u8981\u7814\u7a76\u4ef7\u503c\u3002", "method": "\u91c7\u7528Deformable-DETR\u6a21\u578b\uff0c\u7ed3\u5408\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5149\u5b66\uff08Pleiades Aircraft\u6570\u636e\u96c6\uff09\u548cSAR\uff08SSDD\u6570\u636e\u96c6\uff09\u56fe\u50cf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "10\u6298\u5206\u5c42\u9a8c\u8bc1\u663e\u793a\uff0c\u6a21\u578b\u5728\u5149\u5b66\u548cSAR\u6570\u636e\u96c6\u4e0a\u7684F1\u5206\u6570\u5206\u522b\u8fbe\u523095.12%\u548c94.54%\uff0c\u4f18\u4e8e\u5176\u4ed6CNN\u548cTransformer\u6a21\u578b\u3002", "conclusion": "Deformable-DETR\u5728\u9065\u611f\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.24196", "pdf": "https://arxiv.org/pdf/2505.24196", "abs": "https://arxiv.org/abs/2505.24196", "authors": ["Longze Chen", "Renke Shan", "Huiming Wang", "Lu Wang", "Ziqiang Liu", "Run Luo", "Jiawei Wang", "Hamid Alinejad-Rokny", "Min Yang"], "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding", "categories": ["cs.CL"], "comment": "11 pages, 7 figures, ACL 2025", "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.", "AI": {"tldr": "CLaSp\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6a21\u5757\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u8df3\u8fc7\u4e2d\u95f4\u5c42\u6784\u5efa\u538b\u7f29\u8349\u7a3f\u6a21\u578b\uff0c\u52a8\u6001\u4f18\u5316\u5c42\u8df3\u8fc7\u7b56\u7565\uff0c\u5b9e\u73b01.3x~1.7x\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6a21\u5757\uff0c\u96be\u4ee5\u517c\u5bb9\u4e0d\u540cLLM\uff0cCLaSp\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u5c42\u8df3\u8fc7\u7b56\u7565\uff0c\u52a8\u6001\u7f16\u7a0b\u7b97\u6cd5\u4f18\u5316\u5c42\u8df3\u8fc7\u8fc7\u7a0b\uff0c\u5229\u7528\u5b8c\u6574\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u76ee\u6807\u3002", "result": "\u5728LLaMA3\u7cfb\u5217\u6a21\u578b\u4e0a\u5b9e\u73b01.3x~1.7x\u52a0\u901f\uff0c\u4e14\u4e0d\u6539\u53d8\u751f\u6210\u6587\u672c\u7684\u539f\u59cb\u5206\u5e03\u3002", "conclusion": "CLaSp\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u517c\u5bb9\u6027\u5f3a\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\u3002"}}
{"id": "2505.24499", "pdf": "https://arxiv.org/pdf/2505.24499", "abs": "https://arxiv.org/abs/2505.24499", "authors": ["Ximing Xing", "Yandong Guan", "Jing Zhang", "Dong Xu", "Qian Yu"], "title": "Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation", "categories": ["cs.CV"], "comment": "17 pages, 5 figures", "summary": "Generating high-quality Scalable Vector Graphics (SVGs) is challenging for\nLarge Language Models (LLMs), as it requires advanced reasoning for structural\nvalidity, semantic faithfulness, and visual coherence -- capabilities in which\ncurrent LLMs often fall short. In this work, we introduce Reason-SVG, a novel\nframework designed to enhance LLM reasoning for SVG generation. Reason-SVG\npioneers the \"Drawing-with-Thought\" (DwT) paradigm, in which models generate\nboth SVG code and explicit design rationales, mimicking the human creative\nprocess. Reason-SVG adopts a two-stage training strategy: First, Supervised\nFine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational\nreasoning abilities. Second, Reinforcement Learning (RL), utilizing Group\nRelative Policy Optimization (GRPO), empowers the model to generate both DwT\nand SVGs rationales through refined, reward-driven reasoning. To facilitate\nreasoning-driven SVG generation, we design a Hybrid Reward function that\nevaluates the presence and utility of DwT reasoning, along with structural\nvalidity, semantic alignment, and visual quality. We also introduce the\nSVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each\nSVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT,\nand Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance\nin generating accurate and visually compelling SVGs, potentially fostering \"Aha\nmoments\" in design.", "AI": {"tldr": "Reason-SVG\u6846\u67b6\u901a\u8fc7\u201cDrawing-with-Thought\u201d\u8303\u5f0f\u589e\u5f3aLLM\u751f\u6210SVG\u7684\u80fd\u529b\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524dLLM\u5728\u751f\u6210SVG\u65f6\u7f3a\u4e4f\u7ed3\u6784\u6709\u6548\u6027\u3001\u8bed\u4e49\u5fe0\u5b9e\u6027\u548c\u89c6\u89c9\u8fde\u8d2f\u6027\uff0cReason-SVG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u76d1\u7763\u5fae\u8c03\u6fc0\u6d3b\u57fa\u7840\u63a8\u7406\u80fd\u529b\uff0c\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u7ed3\u5408\u6df7\u5408\u5956\u52b1\u51fd\u6570\u8bc4\u4f30\u3002", "result": "Reason-SVG\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210SVG\u7684\u51c6\u786e\u6027\u548c\u89c6\u89c9\u5438\u5f15\u529b\u3002", "conclusion": "Reason-SVG\u901a\u8fc7\u6574\u5408DwT\u3001SFT\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3aLLM\u751f\u6210\u9ad8\u8d28\u91cfSVG\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24199", "pdf": "https://arxiv.org/pdf/2505.24199", "abs": "https://arxiv.org/abs/2505.24199", "authors": ["Yimin Du"], "title": "Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling", "categories": ["cs.CL"], "comment": "7 pages", "summary": "The quality of human preference data is crucial for training and evaluating\nlarge language models (LLMs), particularly in reinforcement learning from human\nfeedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional\nside-by-side (SBS) annotation approaches often struggle with inherent\nuncertainty, annotator disagreement, and the complexity of preference\njudgments. This paper introduces a novel framework based on intuitionistic\nfuzzy sets (IFS) for modeling and aggregating human preferences in LLM data\nannotation tasks. Our approach captures not only the degree of preference but\nalso the uncertainty and hesitation inherent in human judgment through\nmembership, non-membership, and hesitation degrees. We propose an IFS-based\nannotation protocol that enables more nuanced preference modeling, develops\naggregation methods for handling annotator disagreement, and introduces quality\nmetrics for preference data assessment. Experimental validation on multiple\ndatasets demonstrates that our IFS-based approach significantly improves\nannotation consistency, reduces annotator fatigue, and produces higher-quality\npreference data compared to traditional binary and Likert-scale methods. The\nresulting preference datasets lead to improved model performance in downstream\ntasks, with 12.3\\% improvement in win-rate against baseline models and 15.7\\%\nreduction in annotation time. Our framework provides a principled approach to\nhandling uncertainty in human preference annotation and offers practical\nbenefits for large-scale LLM training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u89c9\u6a21\u7cca\u96c6\uff08IFS\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u805a\u5408\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6807\u6ce8\u65b9\u6cd5\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u65b9\u6cd5\uff08\u5982SBS\uff09\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3001\u6807\u6ce8\u8005\u5206\u6b67\u548c\u5224\u65ad\u590d\u6742\u6027\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eIFS\u7684\u6807\u6ce8\u534f\u8bae\uff0c\u901a\u8fc7\u96b6\u5c5e\u5ea6\u3001\u975e\u96b6\u5c5e\u5ea6\u548c\u72b9\u8c6b\u5ea6\u5efa\u6a21\u504f\u597d\uff0c\u5f00\u53d1\u4e86\u805a\u5408\u65b9\u6cd5\u548c\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIFS\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4e86\u6807\u6ce8\u75b2\u52b3\uff0c\u6570\u636e\u8d28\u91cf\u66f4\u9ad8\uff0c\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u6027\u80fd\u63d0\u534712.3%\uff0c\u6807\u6ce8\u65f6\u95f4\u51cf\u5c1115.7%\u3002", "conclusion": "IFS\u6846\u67b6\u4e3a\u5904\u7406\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u4e3a\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u4f18\u52bf\u3002"}}
{"id": "2505.24517", "pdf": "https://arxiv.org/pdf/2505.24517", "abs": "https://arxiv.org/abs/2505.24517", "authors": ["Yinqi Li", "Jiahe Zhao", "Hong Chang", "Ruibing Hou", "Shiguang Shan", "Xilin Chen"], "title": "un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un$^2$CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdbCLIP\u6a21\u578b\u7684\u65b9\u6cd5un\u00b2CLIP\uff0c\u901a\u8fc7\u5229\u7528\u751f\u6210\u6a21\u578bunCLIP\u7684\u80fd\u529b\u6765\u6355\u6349\u66f4\u591a\u56fe\u50cf\u7ec6\u8282\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u6587\u672c\u7f16\u7801\u5668\u7684\u5bf9\u9f50\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u59cbCLIP\u548c\u5176\u4ed6\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "CLIP\u5728\u533a\u5206\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u5176\u5728\u5bc6\u96c6\u9884\u6d4b\u548c\u89c6\u89c9\u4e2d\u5fc3\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdbCLIP\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u6355\u6349\u56fe\u50cf\u7ec6\u8282\u3002", "method": "\u5229\u7528\u751f\u6210\u6a21\u578bunCLIP\uff08\u901a\u8fc7CLIP\u56fe\u50cf\u5d4c\u5165\u8bad\u7ec3\u7684\u56fe\u50cf\u751f\u6210\u5668\uff09\u6765\u6539\u8fdbCLIP\u6a21\u578b\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u5c06unCLIP\u53cd\u8f6c\uff08\u79f0\u4e3aun\u00b2CLIP\uff09\uff0c\u4ee5\u589e\u5f3aCLIP\u56fe\u50cf\u7f16\u7801\u5668\u7684\u7ec6\u8282\u6355\u6349\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cun\u00b2CLIP\u5728MMVP-VLM\u57fa\u51c6\u6d4b\u8bd5\u3001\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4efb\u52a1\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u539f\u59cbCLIP\u548c\u5176\u4ed6\u6539\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578bunCLIP\u7684\u80fd\u529b\uff0cun\u00b2CLIP\u6210\u529f\u63d0\u5347\u4e86CLIP\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6355\u6349\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6587\u672c\u7f16\u7801\u5668\u7684\u5bf9\u9f50\u3002"}}
{"id": "2505.24211", "pdf": "https://arxiv.org/pdf/2505.24211", "abs": "https://arxiv.org/abs/2505.24211", "authors": ["Jiwan Chung", "Janghan Yoon", "Junhyeong Park", "Sangeyl Lee", "Joowon Yang", "Sooyeon Park", "Youngjae Yu"], "title": "Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?", "categories": ["cs.CL"], "comment": null, "summary": "Any-to-any generative models aim to enable seamless interpretation and\ngeneration across multiple modalities within a unified framework, yet their\nability to preserve relationships across modalities remains uncertain. Do\nunified models truly achieve cross-modal coherence, or is this coherence merely\nperceived? To explore this, we introduce ACON, a dataset of 1,000 images (500\nnewly contributed) paired with captions, editing instructions, and Q&A pairs to\nevaluate cross-modal transfers rigorously. Using three consistency\ncriteria-cyclic consistency, forward equivariance, and conjugated\nequivariance-our experiments reveal that any-to-any models do not consistently\ndemonstrate greater cross-modal consistency than specialized models in\npointwise evaluations such as cyclic consistency. However, equivariance\nevaluations uncover weak but observable consistency through structured analyses\nof the intermediate latent space enabled by multiple editing operations. We\nrelease our code and data at https://github.com/JiwanChung/ACON.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u901a\u8fc7ACON\u6570\u636e\u96c6\u548c\u4e09\u79cd\u4e00\u81f4\u6027\u6807\u51c6\u8bc4\u4f30\uff0c\u53d1\u73b0\u7edf\u4e00\u6a21\u578b\u5728\u70b9\u5bf9\u70b9\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u5982\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u5206\u6790\u4e2d\u663e\u793a\u51fa\u5f31\u4e00\u81f4\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u771f\u6b63\u5b9e\u73b0\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u800c\u975e\u4ec5\u8868\u9762\u611f\u77e5\u3002", "method": "\u5f15\u5165ACON\u6570\u636e\u96c6\uff081000\u5f20\u56fe\u50cf\uff0c\u542b\u65b0\u8d21\u732e\u7684500\u5f20\uff09\uff0c\u4f7f\u7528\u5faa\u73af\u4e00\u81f4\u6027\u3001\u524d\u5411\u7b49\u53d8\u6027\u548c\u5171\u8f6d\u7b49\u53d8\u6027\u4e09\u79cd\u6807\u51c6\u8bc4\u4f30\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u7edf\u4e00\u6a21\u578b\u5728\u70b9\u5bf9\u70b9\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u5982\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u5206\u6790\u4e2d\u663e\u793a\u51fa\u5f31\u4e00\u81f4\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u7684\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u3002"}}
{"id": "2505.24519", "pdf": "https://arxiv.org/pdf/2505.24519", "abs": "https://arxiv.org/abs/2505.24519", "authors": ["Yuqi Zhang", "Yuchun Miao", "Zuchao Li", "Liang Ding"], "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 7 figures", "summary": "We introduce AMIA, a lightweight, inference-only defense for Large\nVision-Language Models (LVLMs) that (1) Automatically Masks a small set of\ntext-irrelevant image patches to disrupt adversarial perturbations, and (2)\nconducts joint Intention Analysis to uncover and mitigate hidden harmful\nintents before response generation. Without any retraining, AMIA improves\ndefense success rates across diverse LVLMs and jailbreak benchmarks from an\naverage of 52.4% to 81.7%, preserves general utility with only a 2% average\naccuracy drop, and incurs only modest inference overhead. Ablation confirms\nboth masking and intention analysis are essential for a robust safety-utility\ntrade-off.", "AI": {"tldr": "AMIA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5c4f\u853d\u65e0\u5173\u56fe\u50cf\u5757\u548c\u8054\u5408\u610f\u56fe\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u3002", "motivation": "\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "1. \u81ea\u52a8\u5c4f\u853d\u4e0e\u6587\u672c\u65e0\u5173\u7684\u56fe\u50cf\u5757\u4ee5\u7834\u574f\u5bf9\u6297\u6027\u6270\u52a8\uff1b2. \u8054\u5408\u610f\u56fe\u5206\u6790\u4ee5\u5728\u751f\u6210\u54cd\u5e94\u524d\u53d1\u73b0\u5e76\u7f13\u89e3\u6f5c\u5728\u6709\u5bb3\u610f\u56fe\u3002", "result": "\u9632\u5fa1\u6210\u529f\u7387\u4ece\u5e73\u574752.4%\u63d0\u5347\u81f381.7%\uff0c\u5b9e\u7528\u6027\u4ec5\u4e0b\u964d2%\uff0c\u63a8\u7406\u5f00\u9500\u8f83\u4f4e\u3002", "conclusion": "\u5c4f\u853d\u548c\u610f\u56fe\u5206\u6790\u662f\u5b9e\u73b0\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u5e73\u8861\u7684\u5173\u952e\u3002"}}
{"id": "2505.24217", "pdf": "https://arxiv.org/pdf/2505.24217", "abs": "https://arxiv.org/abs/2505.24217", "authors": ["Jixuan Leng", "Cassandra A. Cohen", "Zhixian Zhang", "Chenyan Xiong", "William W. Cohen"], "title": "Semi-structured LLM Reasoners Can Be Rigorously Audited", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly capable at reasoning, the\nproblem of \"faithfulness\" persists: LLM \"reasoning traces\" can contain errors\nand omissions that are difficult to detect, and may obscure biases in model\noutputs. To address these limitations, we introduce Semi-Structured Reasoning\nModels (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)\nreasoning format within the model. Our SSRMs generate reasoning traces in a\nPythonic syntax. While SSRM traces are not executable, they adopt a restricted,\ntask-specific vocabulary to name distinct reasoning steps, and to mark each\nstep's inputs and outputs. Through extensive evaluation on ten benchmarks,\nSSRMs demonstrate strong performance and generality: they outperform comparably\nsized baselines by nearly ten percentage points on in-domain tasks while\nremaining competitive with specialized models on out-of-domain medical\nbenchmarks. Furthermore, we show that semi-structured reasoning is more\namenable to analysis: in particular, they can be automatically audited to\nidentify reasoning flaws. We explore both hand-crafted structured audits, which\ndetect task-specific problematic reasoning patterns, and learned typicality\naudits, which apply probabilistic models over reasoning patterns, and show that\nboth audits can be used to effectively flag probable reasoning errors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u534a\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u578b\uff08SSRMs\uff09\uff0c\u901a\u8fc7\u5185\u90e8\u5316\u534a\u7ed3\u6784\u5316\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u683c\u5f0f\uff0c\u751f\u6210\u7c7b\u4f3cPython\u8bed\u6cd5\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4ee5\u63d0\u9ad8LLM\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u53ef\u5206\u6790\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u4e2d\u7684'\u5fe0\u5b9e\u6027'\u95ee\u9898\uff0c\u5373\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9519\u8bef\u548c\u9057\u6f0f\u96be\u4ee5\u68c0\u6d4b\uff0c\u53ef\u80fd\u63a9\u76d6\u6a21\u578b\u8f93\u51fa\u7684\u504f\u89c1\u3002", "method": "\u5f15\u5165SSRMs\uff0c\u91c7\u7528\u534a\u7ed3\u6784\u5316\u7684CoT\u63a8\u7406\u683c\u5f0f\uff0c\u751f\u6210\u5177\u6709\u4efb\u52a1\u7279\u5b9a\u8bcd\u6c47\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u6807\u8bb0\u6bcf\u4e2a\u6b65\u9aa4\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002", "result": "\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSSRMs\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u540c\u7c7b\u57fa\u7ebf\u6a21\u578b\u9ad8\u51fa\u8fd1\u5341\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5728\u8de8\u9886\u57df\u533b\u5b66\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u534a\u7ed3\u6784\u5316\u63a8\u7406\u66f4\u6613\u4e8e\u5206\u6790\uff0c\u53ef\u901a\u8fc7\u624b\u5de5\u6216\u5b66\u4e60\u578b\u5ba1\u8ba1\u81ea\u52a8\u68c0\u6d4b\u63a8\u7406\u7f3a\u9677\uff0c\u6709\u6548\u6807\u8bb0\u53ef\u80fd\u7684\u63a8\u7406\u9519\u8bef\u3002"}}
{"id": "2505.24521", "pdf": "https://arxiv.org/pdf/2505.24521", "abs": "https://arxiv.org/abs/2505.24521", "authors": ["Yang-Tian Sun", "Xin Yu", "Zehuan Huang", "Yi-Hua Huang", "Yuan-Chen Guo", "Ziyi Yang", "Yan-Pei Cao", "Xiaojuan Qi"], "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation", "categories": ["cs.CV"], "comment": "Project page: https://sunyangtian.github.io/UniGeo-web/", "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6269\u6563\u6a21\u578b\u5148\u9a8c\u8f85\u52a9\u5355\u76ee\u51e0\u4f55\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5168\u5c40\u5750\u6807\u7cfb\u4e2d\u7684\u51e0\u4f55\u5c5e\u6027\u9884\u6d4b\u76ee\u6807\u3001\u5f15\u5165\u9ad8\u6548\u7684\u6761\u4ef6\u5316\u65b9\u6cd5\u53ca\u8054\u5408\u8bad\u7ec3\u591a\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u4e2d\u5168\u5c40\u51e0\u4f55\u5c5e\u6027\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u5e27\u76f8\u673a\u5750\u6807\u7cfb\u5185\u7684\u51e0\u4f55\u5c5e\u6027\u4f30\u8ba1\uff0c\u5ffd\u7565\u4e86\u6269\u6563\u6a21\u578b\u5728\u5e27\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u56fa\u6709\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u66f4\u4e00\u81f4\u7684\u51e0\u4f55\u4f30\u8ba1\u3002", "method": "1) \u9009\u62e9\u5168\u5c40\u5750\u6807\u7cfb\u4e2d\u4e0e\u89c6\u9891\u5e27\u5171\u4eab\u5bf9\u5e94\u5173\u7cfb\u7684\u51e0\u4f55\u5c5e\u6027\u4f5c\u4e3a\u9884\u6d4b\u76ee\u6807\uff1b2) \u63d0\u51fa\u91cd\u7528\u4f4d\u7f6e\u7f16\u7801\u7684\u9ad8\u6548\u6761\u4ef6\u5316\u65b9\u6cd5\uff1b3) \u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u5171\u4eab\u5bf9\u5e94\u5173\u7cfb\u7684\u591a\u51e0\u4f55\u5c5e\u6027\u63d0\u5347\u6027\u80fd\u3002", "result": "\u65b9\u6cd5\u5728\u89c6\u9891\u5168\u5c40\u51e0\u4f55\u5c5e\u6027\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u91cd\u5efa\u4efb\u52a1\uff0c\u4e14\u5728\u4ec5\u9759\u6001\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u4e0b\uff0c\u5c55\u73b0\u51fa\u5bf9\u52a8\u6001\u573a\u666f\u7684\u6cdb\u5316\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\u548c\u5fae\u8c03\uff0c\u6269\u6563\u6a21\u578b\u7684\u56fa\u6709\u4e00\u81f4\u6027\u53ef\u6709\u6548\u7528\u4e8e\u4e00\u81f4\u51e0\u4f55\u4f30\u8ba1\uff0c\u4e3a\u89c6\u9891\u51e0\u4f55\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24219", "pdf": "https://arxiv.org/pdf/2505.24219", "abs": "https://arxiv.org/abs/2505.24219", "authors": ["Lam Thanh Do", "Aaditya Bodke", "Pritom Saha Akash", "Kevin Chen-Chuan Chang"], "title": "ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Unsupervised keyphrase prediction has gained growing interest in recent\nyears. However, existing methods typically rely on heuristically defined\nimportance scores, which may lead to inaccurate informativeness estimation. In\naddition, they lack consideration for time efficiency. To solve these problems,\nwe propose ERU-KG, an unsupervised keyphrase generation (UKG) model that\nconsists of an informativeness and a phraseness module. The former estimates\nthe relevance of keyphrase candidates, while the latter generate those\ncandidates. The informativeness module innovates by learning to model\ninformativeness through references (e.g., queries, citation contexts, and\ntitles) and at the term-level, thereby 1) capturing how the key concepts of\ndocuments are perceived in different contexts and 2) estimating informativeness\nof phrases more efficiently by aggregating term informativeness, removing the\nneed for explicit modeling of the candidates. ERU-KG demonstrates its\neffectiveness on keyphrase generation benchmarks by outperforming unsupervised\nbaselines and achieving on average 89\\% of the performance of a supervised\nmodel for top 10 predictions. Additionally, to highlight its practical utility,\nwe evaluate the model on text retrieval tasks and show that keyphrases\ngenerated by ERU-KG are effective when employed as query and document\nexpansions. Furthermore, inference speed tests reveal that ERU-KG is the\nfastest among baselines of similar model sizes. Finally, our proposed model can\nswitch between keyphrase generation and extraction by adjusting\nhyperparameters, catering to diverse application requirements.", "AI": {"tldr": "ERU-KG\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4fe1\u606f\u6027\u548c\u77ed\u8bed\u6027\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u5b9a\u4e49\u7684\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u5bfc\u81f4\u4fe1\u606f\u6027\u4f30\u8ba1\u4e0d\u51c6\u786e\u4e14\u7f3a\u4e4f\u65f6\u95f4\u6548\u7387\u8003\u8651\u3002", "method": "ERU-KG\u5305\u542b\u4fe1\u606f\u6027\u548c\u77ed\u8bed\u6027\u6a21\u5757\uff0c\u524d\u8005\u901a\u8fc7\u53c2\u8003\u548c\u672f\u8bed\u7ea7\u5efa\u6a21\u4f30\u8ba1\u5173\u952e\u77ed\u8bed\u7684\u5173\u8054\u6027\uff0c\u540e\u8005\u751f\u6210\u5019\u9009\u77ed\u8bed\u3002", "result": "\u5728\u5173\u952e\u8bcd\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u65e0\u76d1\u7763\u57fa\u7ebf\uff0c\u8fbe\u5230\u76d1\u7763\u6a21\u578b89%\u7684\u6027\u80fd\uff0c\u4e14\u5728\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u5b9e\u7528\u3002", "conclusion": "ERU-KG\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u7075\u6d3b\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.24527", "pdf": "https://arxiv.org/pdf/2505.24527", "abs": "https://arxiv.org/abs/2505.24527", "authors": ["Simone Cammarasana", "Giuseppe Patan\u00e8"], "title": "Optimal Density Functions for Weighted Convolution in Learning Models", "categories": ["cs.CV", "cs.LG", "42A85"], "comment": "5 figures, 5 tables, 21 pages", "summary": "The paper introduces the weighted convolution, a novel approach to the\nconvolution for signals defined on regular grids (e.g., 2D images) through the\napplication of an optimal density function to scale the contribution of\nneighbouring pixels based on their distance from the central pixel. This choice\ndiffers from the traditional uniform convolution, which treats all neighbouring\npixels equally. Our weighted convolution can be applied to convolutional neural\nnetwork problems to improve the approximation accuracy. Given a convolutional\nnetwork, we define a framework to compute the optimal density function through\na minimisation model. The framework separates the optimisation of the\nconvolutional kernel weights (using stochastic gradient descent) from the\noptimisation of the density function (using DIRECT-L). Experimental results on\na learning model for an image-to-image task (e.g., image denoising) show that\nthe weighted convolution significantly reduces the loss (up to 53% improvement)\nand increases the test accuracy compared to standard convolution. While this\nmethod increases execution time by 11%, it is robust across several\nhyperparameters of the learning model. Future work will apply the weighted\nconvolution to real-case 2D and 3D image convolutional learning problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u5377\u79ef\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u5bc6\u5ea6\u51fd\u6570\u8c03\u6574\u90bb\u57df\u50cf\u7d20\u7684\u8d21\u732e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u5bf9\u6240\u6709\u90bb\u57df\u50cf\u7d20\u5e73\u7b49\u5904\u7406\uff0c\u800c\u52a0\u6743\u5377\u79ef\u901a\u8fc7\u8ddd\u79bb\u8c03\u6574\u8d21\u732e\uff0c\u4ee5\u63d0\u9ad8\u8fd1\u4f3c\u7cbe\u5ea6\u3002", "method": "\u5b9a\u4e49\u6846\u67b6\u4f18\u5316\u5bc6\u5ea6\u51fd\u6570\uff08DIRECT-L\uff09\u548c\u5377\u79ef\u6838\u6743\u91cd\uff08\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff09\uff0c\u5e94\u7528\u4e8e\u56fe\u50cf\u4efb\u52a1\uff08\u5982\u53bb\u566a\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u52a0\u6743\u5377\u79ef\u51cf\u5c11\u635f\u5931\uff08\u6700\u9ad853%\uff09\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u4f46\u589e\u52a011%\u6267\u884c\u65f6\u95f4\u3002", "conclusion": "\u52a0\u6743\u5377\u79ef\u5728\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u672a\u6765\u5c06\u6269\u5c55\u81f32D\u548c3D\u56fe\u50cf\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2505.24223", "pdf": "https://arxiv.org/pdf/2505.24223", "abs": "https://arxiv.org/abs/2505.24223", "authors": ["Jean-Benoit Delbrouck", "Justin Xu", "Johannes Moll", "Alois Thomas", "Zhihong Chen", "Sophie Ostmeier", "Asfandyar Azhar", "Kelvin Zhenghao Li", "Andrew Johnston", "Christian Bluethgen", "Eduardo Reis", "Mohamed Muneer", "Maya Varma", "Curtis Langlotz"], "title": "Automated Structured Radiology Report Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL Main 2025", "summary": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff08SRRG\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u5c06\u81ea\u7531\u6587\u672c\u62a5\u544a\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u683c\u5f0f\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u751f\u6210\u4e0d\u4e00\u81f4\u548c\u8bc4\u4f30\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u7531\u6587\u672c\u653e\u5c04\u5b66\u62a5\u544a\u7684\u53d8\u5f02\u6027\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u4e0d\u4e00\u81f4\u4e14\u4e34\u5e8a\u610f\u4e49\u4e0d\u660e\u786e\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u6355\u6349\u653e\u5c04\u5b66\u89e3\u8bfb\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u5f15\u5165SRRG\u4efb\u52a1\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u91cd\u6784\u62a5\u544a\u4e3a\u6807\u51c6\u5316\u683c\u5f0f\uff0c\u5e76\u63d0\u51faSRR-BERT\u6a21\u578b\u8fdb\u884c\u7cbe\u7ec6\u75be\u75c5\u5206\u7c7b\u3002", "result": "\u901a\u8fc7SRR-BERT\u548cF1-SRR-BERT\u6307\u6807\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u7ed3\u6784\u5316\u62a5\u544a\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u653e\u5c04\u79d1\u533b\u751f\u9a8c\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u62a5\u544a\u751f\u6210\u4efb\u52a1\u548cSRR-BERT\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6e05\u6670\u6027\u3001\u4e00\u81f4\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.24528", "pdf": "https://arxiv.org/pdf/2505.24528", "abs": "https://arxiv.org/abs/2505.24528", "authors": ["Pedram Ghamisi", "Weikang Yu", "Xiaokang Zhang", "Aldino Rizaldy", "Jian Wang", "Chufeng Zhou", "Richard Gloaguen", "Gustau Camps-Valls"], "title": "Geospatial Foundation Models to Enable Progress on Sustainable Development Goals", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation Models (FMs) are large-scale, pre-trained AI systems that have\nrevolutionized natural language processing and computer vision, and are now\nadvancing geospatial analysis and Earth Observation (EO). They promise improved\ngeneralization across tasks, scalability, and efficient adaptation with minimal\nlabeled data. However, despite the rapid proliferation of geospatial FMs, their\nreal-world utility and alignment with global sustainability goals remain\nunderexplored. We introduce SustainFM, a comprehensive benchmarking framework\ngrounded in the 17 Sustainable Development Goals with extremely diverse tasks\nranging from asset wealth prediction to environmental hazard detection. This\nstudy provides a rigorous, interdisciplinary assessment of geospatial FMs and\noffers critical insights into their role in attaining sustainability goals. Our\nfindings show: (1) While not universally superior, FMs often outperform\ntraditional approaches across diverse tasks and datasets. (2) Evaluating FMs\nshould go beyond accuracy to include transferability, generalization, and\nenergy efficiency as key criteria for their responsible use. (3) FMs enable\nscalable, SDG-grounded solutions, offering broad utility for tackling complex\nsustainability challenges. Critically, we advocate for a paradigm shift from\nmodel-centric development to impact-driven deployment, and emphasize metrics\nsuch as energy efficiency, robustness to domain shifts, and ethical\nconsiderations.", "AI": {"tldr": "SustainFM\u662f\u4e00\u4e2a\u57fa\u4e8e17\u4e2a\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u8bc4\u4f30\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u53ef\u6301\u7eed\u6027\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0FMs\u5728\u591a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u9700\u5173\u6ce8\u80fd\u6548\u3001\u6cdb\u5316\u80fd\u529b\u548c\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u53ef\u6301\u7eed\u6027\u76ee\u6807\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u548c\u5bf9\u9f50\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u6027\u80fd\u548c\u5f71\u54cd\u3002", "method": "\u5f15\u5165SustainFM\u6846\u67b6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u53ef\u6301\u7eed\u6027\u4efb\u52a1\uff08\u5982\u8d22\u5bcc\u9884\u6d4b\u3001\u73af\u5883\u707e\u5bb3\u68c0\u6d4b\uff09\uff0c\u5e76\u7efc\u5408\u8bc4\u4f30FMs\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u80fd\u6548\u3002", "result": "FMs\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5176\u8bc4\u4f30\u9700\u6269\u5c55\u81f3\u80fd\u6548\u3001\u6cdb\u5316\u80fd\u529b\u548c\u4f26\u7406\u8003\u91cf\uff1bFMs\u4e3a\u53ef\u6301\u7eed\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u547c\u5401\u4ece\u6a21\u578b\u4e2d\u5fc3\u5f00\u53d1\u8f6c\u5411\u5f71\u54cd\u9a71\u52a8\u7684\u90e8\u7f72\uff0c\u5f3a\u8c03\u80fd\u6548\u3001\u9886\u57df\u9002\u5e94\u6027\u548c\u4f26\u7406\u4f5c\u4e3a\u5173\u952e\u6307\u6807\u3002"}}
{"id": "2505.24229", "pdf": "https://arxiv.org/pdf/2505.24229", "abs": "https://arxiv.org/abs/2505.24229", "authors": ["Luong Ho", "Khanh Le", "Vinh Pham", "Bao Nguyen", "Tan Tran", "Duc Chau"], "title": "Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Inverse Text Normalization (ITN) is crucial for converting spoken Automatic\nSpeech Recognition (ASR) outputs into well-formatted written text, enhancing\nboth readability and usability. Despite its importance, the integration of\nstreaming ITN within streaming ASR remains largely unexplored due to challenges\nin accuracy, efficiency, and adaptability, particularly in low-resource and\nlimited-context scenarios. In this paper, we introduce a streaming pretrained\nlanguage model for ITN, leveraging pretrained linguistic representations for\nimproved robustness. To address streaming constraints, we propose Dynamic\nContext-Aware during training and inference, enabling adaptive chunk size\nadjustments and the integration of right-context information. Experimental\nresults demonstrate that our method achieves accuracy comparable to\nnon-streaming ITN and surpasses existing streaming ITN models on a Vietnamese\ndataset, all while maintaining low latency, ensuring seamless integration into\nASR systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u5f0f\u9006\u5411\u6587\u672c\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6d41\u5f0fASR\u4e2dITN\u7684\u6311\u6218\uff0c\u5e76\u5728\u8d8a\u5357\u8bed\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6d41\u5f0fASR\u4e2d\u9006\u5411\u6587\u672c\u5f52\u4e00\u5316\uff08ITN\uff09\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u95ee\u9898\u5c1a\u672a\u5145\u5206\u89e3\u51b3\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u548c\u6709\u9650\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u5757\u5927\u5c0f\u5e76\u6574\u5408\u53f3\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728\u8d8a\u5357\u8bed\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e0e\u975e\u6d41\u5f0fITN\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u6d41\u5f0fITN\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d41\u5f0fASR\u4e2d\u7684ITN\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.24541", "pdf": "https://arxiv.org/pdf/2505.24541", "abs": "https://arxiv.org/abs/2505.24541", "authors": ["Xin He", "Xumeng Han", "Longhui Wei", "Lingxi Xie", "Qi Tian"], "title": "Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) require a nuanced interpretation of\ncomplex image information, typically leveraging a vision encoder to perceive\nvarious visual scenarios. However, relying solely on a single vision encoder to\nhandle diverse task domains proves difficult and inevitably leads to conflicts.\nRecent work enhances data perception by directly integrating multiple\ndomain-specific vision encoders, yet this structure adds complexity and limits\nthe potential for joint optimization. In this paper, we introduce Mixpert, an\nefficient mixture-of-vision-experts architecture that inherits the joint\nlearning advantages from a single vision encoder while being restructured into\na multi-expert paradigm for task-specific fine-tuning across different visual\ntasks. Additionally, we design a dynamic routing mechanism that allocates input\nimages to the most suitable visual expert. Mixpert effectively alleviates\ndomain conflicts encountered by a single vision encoder in multi-task learning\nwith minimal additional computational cost, making it more efficient than\nmultiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,\nwith experimental results demonstrating substantial performance gains across\nvarious tasks.", "AI": {"tldr": "Mixpert\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u89c6\u89c9\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u5206\u914d\u4efb\u52a1\u5230\u5408\u9002\u7684\u4e13\u5bb6\uff0c\u89e3\u51b3\u4e86\u5355\u89c6\u89c9\u7f16\u7801\u5668\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "motivation": "\u5355\u89c6\u89c9\u7f16\u7801\u5668\u96be\u4ee5\u5904\u7406\u591a\u6837\u5316\u7684\u89c6\u89c9\u4efb\u52a1\uff0c\u5bb9\u6613\u5bfc\u81f4\u51b2\u7a81\uff1b\u800c\u76f4\u63a5\u96c6\u6210\u591a\u4e2a\u7f16\u7801\u5668\u4f1a\u589e\u52a0\u590d\u6742\u6027\u548c\u9650\u5236\u8054\u5408\u4f18\u5316\u3002", "method": "\u63d0\u51faMixpert\u67b6\u6784\uff0c\u7ed3\u5408\u5355\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8054\u5408\u5b66\u4e60\u4f18\u52bf\u548c\u591a\u4e13\u5bb6\u8303\u5f0f\uff0c\u8bbe\u8ba1\u52a8\u6001\u8def\u7531\u673a\u5236\u5206\u914d\u4efb\u52a1\u3002", "result": "Mixpert\u6709\u6548\u7f13\u89e3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u51b2\u7a81\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Mixpert\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u4efb\u52a1\u89c6\u89c9\u5904\u7406\u3002"}}
{"id": "2505.24241", "pdf": "https://arxiv.org/pdf/2505.24241", "abs": "https://arxiv.org/abs/2505.24241", "authors": ["Naibin Gu", "Yilong Chen", "Zhenyu Zhang", "Peng Fu", "Zheng Lin", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Weiping Wang", "Haifeng Wang"], "title": "Advantageous Parameter Expansion Training Makes Better Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Although scaling up the number of trainable parameters in both pre-training\nand fine-tuning can effectively improve the performance of large language\nmodels, it also leads to increased computational overhead. When delving into\nthe parameter difference, we find that a subset of parameters, termed\nadvantageous parameters, plays a crucial role in determining model performance.\nFurther analysis reveals that stronger models tend to possess more such\nparameters. In this paper, we propose Advantageous Parameter EXpansion Training\n(APEX), a method that progressively expands advantageous parameters into the\nspace of disadvantageous ones, thereby increasing their proportion and\nenhancing training effectiveness. Further theoretical analysis from the\nperspective of matrix effective rank explains the performance gains of APEX.\nExtensive experiments on both instruction tuning and continued pre-training\ndemonstrate that, in instruction tuning, APEX outperforms full-parameter tuning\nwhile using only 52% of the trainable parameters. In continued pre-training,\nAPEX achieves the same perplexity level as conventional training with just 33%\nof the training data, and yields significant improvements on downstream tasks.", "AI": {"tldr": "APEX\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u4f18\u52bf\u53c2\u6570\u6bd4\u4f8b\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u589e\u52a0\u5e26\u6765\u6027\u80fd\u63d0\u5347\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4f18\u52bf\u53c2\u6570\u5bf9\u6027\u80fd\u8d77\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51faAPEX\u65b9\u6cd5\uff0c\u9010\u6b65\u6269\u5c55\u4f18\u52bf\u53c2\u6570\u81f3\u52a3\u52bf\u53c2\u6570\u7a7a\u95f4\uff0c\u63d0\u5347\u5176\u6bd4\u4f8b\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u4e2dAPEX\u4ec5\u752852%\u53ef\u8bad\u7ec3\u53c2\u6570\u4f18\u4e8e\u5168\u53c2\u6570\u5fae\u8c03\uff1b\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u4ec5\u970033%\u6570\u636e\u8fbe\u5230\u76f8\u540c\u56f0\u60d1\u5ea6\u3002", "conclusion": "APEX\u901a\u8fc7\u4f18\u5316\u53c2\u6570\u5206\u914d\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.24558", "pdf": "https://arxiv.org/pdf/2505.24558", "abs": "https://arxiv.org/abs/2505.24558", "authors": ["Simone Cammarasana", "Giuseppe Patan\u00e8"], "title": "Optimal Weighted Convolution for Classification and Denosing", "categories": ["cs.CV", "68T05"], "comment": "17 pages, 3 figures, 6 tables", "summary": "We introduce a novel weighted convolution operator that enhances traditional\nconvolutional neural networks (CNNs) by integrating a spatial density function\ninto the convolution operator. This extension enables the network to\ndifferentially weight neighbouring pixels based on their relative position to\nthe reference pixel, improving spatial characterisation and feature extraction.\nThe proposed operator maintains the same number of trainable parameters and is\nfully compatible with existing CNN architectures. Although developed for 2D\nimage data, the framework is generalisable to signals on regular grids of\narbitrary dimensions, such as 3D volumetric data or 1D time series. We propose\nan efficient implementation of the weighted convolution by pre-computing the\ndensity function and achieving execution times comparable to standard\nconvolution layers. We evaluate our method on two deep learning tasks: image\nclassification using the CIFAR-100 dataset [KH+09] and image denoising using\nthe DIV2K dataset [AT17]. Experimental results with state-of-the-art\nclassification (e.g., VGG [SZ15], ResNet [HZRS16]) and denoising (e.g., DnCNN\n[ZZC+17], NAFNet [CCZS22]) methods show that the weighted convolution improves\nperformance with respect to standard convolution across different quantitative\nmetrics. For example, VGG achieves an accuracy of 66.94% with weighted\nconvolution versus 56.89% with standard convolution on the classification\nproblem, while DnCNN improves the PSNR value from 20.17 to 22.63 on the\ndenoising problem. All models were trained on the CINECA Leonardo cluster to\nreduce the execution time and improve the tuning of the density function\nvalues. The PyTorch implementation of the weighted convolution is publicly\navailable at: https://github.com/cammarasana123/weightedConvolution2.0.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u52a0\u6743\u5377\u79ef\u7b97\u5b50\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u5bc6\u5ea6\u51fd\u6570\u589e\u5f3a\u4f20\u7edfCNN\uff0c\u63d0\u5347\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u517c\u5bb9\u73b0\u6709\u67b6\u6784\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u53bb\u566a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u5377\u79ef\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u64cd\u4f5c\u5bf9\u90bb\u57df\u50cf\u7d20\u7684\u5904\u7406\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6839\u636e\u50cf\u7d20\u7684\u76f8\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u5dee\u5f02\u5316\u52a0\u6743\uff0c\u9650\u5236\u4e86\u7279\u5f81\u63d0\u53d6\u7684\u6548\u679c\u3002", "method": "\u5f15\u5165\u7a7a\u95f4\u5bc6\u5ea6\u51fd\u6570\u5230\u5377\u79ef\u7b97\u5b50\u4e2d\uff0c\u5b9e\u73b0\u90bb\u57df\u50cf\u7d20\u7684\u5dee\u5f02\u5316\u52a0\u6743\uff0c\u4fdd\u6301\u53c2\u6570\u6570\u91cf\u4e0d\u53d8\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u5b9e\u73b0\u65b9\u6cd5\u3002", "result": "\u5728CIFAR-100\u548cDIV2K\u6570\u636e\u96c6\u4e0a\uff0c\u52a0\u6743\u5377\u79ef\u663e\u8457\u63d0\u5347\u4e86VGG\u548cDnCNN\u7b49\u6a21\u578b\u7684\u6027\u80fd\uff08\u5982\u5206\u7c7b\u51c6\u786e\u7387\u4ece56.89%\u63d0\u5347\u81f366.94%\uff0c\u53bb\u566aPSNR\u4ece20.17\u63d0\u5347\u81f322.63\uff09\u3002", "conclusion": "\u52a0\u6743\u5377\u79ef\u7b97\u5b50\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u7c7b\u578b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.24244", "pdf": "https://arxiv.org/pdf/2505.24244", "abs": "https://arxiv.org/abs/2505.24244", "authors": ["Nir Endy", "Idan Daniel Grosbard", "Yuval Ran-Milo", "Yonatan Slutzky", "Itay Tshuva", "Raja Giryes"], "title": "Mamba Knockout for Unraveling Factual Information Flow", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "This paper investigates the flow of factual information in Mamba State-Space\nModel (SSM)-based language models. We rely on theoretical and empirical\nconnections to Transformer-based architectures and their attention mechanisms.\nExploiting this relationship, we adapt attentional interpretability techniques\noriginally developed for Transformers--specifically, the Attention Knockout\nmethodology--to both Mamba-1 and Mamba-2. Using them we trace how information\nis transmitted and localized across tokens and layers, revealing patterns of\nsubject-token information emergence and layer-wise dynamics. Notably, some\nphenomena vary between mamba models and Transformer based models, while others\nappear universally across all models inspected--hinting that these may be\ninherent to LLMs in general. By further leveraging Mamba's structured\nfactorization, we disentangle how distinct \"features\" either enable\ntoken-to-token information exchange or enrich individual tokens, thus offering\na unified lens to understand Mamba internal operations.", "AI": {"tldr": "\u7814\u7a76Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u4e2d\u4e8b\u5b9e\u4fe1\u606f\u7684\u6d41\u52a8\uff0c\u901a\u8fc7\u501f\u9274Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u89e3\u91ca\u65b9\u6cd5\uff0c\u63ed\u793a\u4fe1\u606f\u5728token\u548c\u5c42\u95f4\u7684\u4f20\u9012\u6a21\u5f0f\uff0c\u5e76\u6bd4\u8f83Mamba\u4e0eTransformer\u7684\u5f02\u540c\u3002", "motivation": "\u63a2\u7d22Mamba\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u6d41\u52a8\u7684\u673a\u5236\uff0c\u5e76\u4e0eTransformer\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ee5\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u901a\u7528\u7279\u6027\u3002", "method": "\u5229\u7528Transformer\u7684\u6ce8\u610f\u529b\u89e3\u91ca\u6280\u672f\uff08\u5982Attention Knockout\uff09\uff0c\u5c06\u5176\u9002\u914d\u5230Mamba-1\u548cMamba-2\u6a21\u578b\uff0c\u5206\u6790\u4fe1\u606f\u5728token\u548c\u5c42\u95f4\u7684\u4f20\u9012\u4e0e\u5b9a\u4f4d\u3002", "result": "\u53d1\u73b0Mamba\u4e0eTransformer\u5728\u67d0\u4e9b\u73b0\u8c61\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u4e5f\u5b58\u5728\u8de8\u6a21\u578b\u7684\u901a\u7528\u6a21\u5f0f\uff0c\u8868\u660e\u8fd9\u4e9b\u53ef\u80fd\u662fLLM\u7684\u56fa\u6709\u7279\u6027\u3002\u901a\u8fc7Mamba\u7684\u7ed3\u6784\u5316\u5206\u89e3\uff0c\u8fdb\u4e00\u6b65\u89e3\u6790\u4e86\u4fe1\u606f\u4ea4\u6362\u4e0etoken\u589e\u5f3a\u7684\u673a\u5236\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3Mamba\u6a21\u578b\u7684\u5185\u90e8\u8fd0\u4f5c\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u5e76\u63ed\u793a\u4e86LLM\u7684\u6f5c\u5728\u901a\u7528\u7279\u6027\u3002"}}
{"id": "2505.24567", "pdf": "https://arxiv.org/pdf/2505.24567", "abs": "https://arxiv.org/abs/2505.24567", "authors": ["Qinghe Ma", "Jian Zhang", "Lei Qi", "Qian Yu", "Yinghuan Shi", "Yang Gao"], "title": "Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMI 2025. arXiv admin note: text overlap with\n  arXiv:2404.08951", "summary": "Both limited annotation and domain shift are prevalent challenges in medical\nimage segmentation. Traditional semi-supervised segmentation and unsupervised\ndomain adaptation methods address one of these issues separately. However, the\ncoexistence of limited annotation and domain shift is quite common, which\nmotivates us to introduce a novel and challenging scenario: Mixed Domain\nSemi-supervised medical image Segmentation (MiDSS), where limited labeled data\nfrom a single domain and a large amount of unlabeled data from multiple\ndomains. To tackle this issue, we propose the UST-RUN framework, which fully\nleverages intermediate domain information to facilitate knowledge transfer. We\nemploy Unified Copy-paste (UCP) to construct intermediate domains, and propose\na Symmetric GuiDance training strategy (SymGD) to supervise unlabeled data by\nmerging pseudo-labels from intermediate samples. Subsequently, we introduce a\nTraining Process aware Random Amplitude MixUp (TP-RAM) to progressively\nincorporate style-transition components into intermediate samples. To generate\nmore diverse intermediate samples, we further select reliable samples with\nhigh-quality pseudo-labels, which are then mixed with other unlabeled data.\nAdditionally, we generate sophisticated intermediate samples with high-quality\npseudo-labels for unreliable samples, ensuring effective knowledge transfer for\nthem. Extensive experiments on four public datasets demonstrate the superiority\nof UST-RUN. Notably, UST-RUN achieves a 12.94% improvement in Dice score on the\nProstate dataset. Our code is available at https://github.com/MQinghe/UST-RUN", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMiDSS\u7684\u65b0\u573a\u666f\uff0c\u7ed3\u5408\u4e86\u6709\u9650\u6807\u6ce8\u548c\u591a\u57df\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86UST-RUN\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u6709\u9650\u6807\u6ce8\u548c\u57df\u504f\u79fb\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u3002MiDSS\u573a\u666f\u7684\u63d0\u51fa\u662f\u4e3a\u4e86\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218\u3002", "method": "UST-RUN\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u590d\u5236\u7c98\u8d34\uff08UCP\uff09\u6784\u5efa\u4e2d\u95f4\u57df\uff0c\u91c7\u7528\u5bf9\u79f0\u5f15\u5bfc\u8bad\u7ec3\u7b56\u7565\uff08SymGD\uff09\u76d1\u7763\u672a\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u8bad\u7ec3\u8fc7\u7a0b\u611f\u77e5\u7684\u968f\u673a\u5e45\u5ea6\u6df7\u5408\uff08TP-RAM\uff09\u9010\u6b65\u5f15\u5165\u98ce\u683c\u8f6c\u6362\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUST-RUN\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728Prostate\u6570\u636e\u96c6\u4e0aDice\u5206\u6570\u63d0\u5347\u4e8612.94%\u3002", "conclusion": "UST-RUN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MiDSS\u573a\u666f\u4e0b\u7684\u6311\u6218\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24251", "pdf": "https://arxiv.org/pdf/2505.24251", "abs": "https://arxiv.org/abs/2505.24251", "authors": ["Xiaoyu Li", "Xiao Li", "Li Gao", "Yiding Liu", "Xiaoyang Wang", "Shuaiqiang Wang", "Junfeng Wang", "Dawei Yin"], "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search", "categories": ["cs.CL", "cs.IR"], "comment": "ACL'25 (Industry)", "summary": "The evolution of Large Language Models (LLMs) has significantly advanced\nmulti-turn conversation systems, emphasizing the need for proactive guidance to\nenhance users' interactions. However, these systems face challenges in\ndynamically adapting to shifts in users' goals and maintaining low latency for\nreal-time interactions. In the Baidu Search AI assistant, an industrial-scale\nmulti-turn search system, we propose a novel two-phase framework to provide\nproactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning\n(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal\nshifts and provides goal-relevant contextual information. G-SFT also\nincorporates scalable knowledge transfer to distill insights from LLMs into a\nlightweight model for real-time interaction. The second phase, Click-oriented\nReinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically\nconstructs preference pairs from user click signals, and proactively improves\nclick-through rates through more engaging guidance. This dual-phase\narchitecture achieves complementary objectives: G-SFT ensures accurate goal\ntracking, while C-RL optimizes interaction quality through click signal-driven\nreinforcement learning. Extensive experiments demonstrate that our framework\nachieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and\n25.28% CTR in online deployment (149.06% relative improvement), while reducing\ninference latency by 69.55% through scalable knowledge distillation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff08G-SFT\u548cC-RL\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e3b\u52a8\u5f15\u5bfc\u80fd\u529b\uff0c\u517c\u987e\u76ee\u6807\u9002\u5e94\u6027\u548c\u5b9e\u65f6\u4ea4\u4e92\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u9700\u8981\u52a8\u6001\u9002\u5e94\u7528\u6237\u76ee\u6807\u53d8\u5316\u5e76\u4fdd\u6301\u4f4e\u5ef6\u8fdf\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u6b64\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\uff08G-SFT\uff09\u901a\u8fc7\u76ee\u6807\u9002\u5e94\u4ee3\u7406\u52a8\u6001\u8c03\u6574\u7528\u6237\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u8f7b\u91cf\u5316\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff08C-RL\uff09\u5229\u7528\u70b9\u51fb\u4fe1\u53f7\u751f\u6210\u504f\u597d\u5bf9\uff0c\u4f18\u5316\u70b9\u51fb\u7387\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u51c6\u786e\u7387\u8fbe86.10%\uff08\u63d0\u534723.95%\uff09\uff0c\u5728\u7ebf\u70b9\u51fb\u7387\u63d0\u5347149.06%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e69.55%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u76ee\u6807\u8ddf\u8e2a\u548c\u4ea4\u4e92\u8d28\u91cf\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u5177\u6709\u663e\u8457\u7684\u5de5\u4e1a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.24600", "pdf": "https://arxiv.org/pdf/2505.24600", "abs": "https://arxiv.org/abs/2505.24600", "authors": ["Omer Nacar", "Yasser Al-Habashi", "Serry Sibaee", "Adel Ammar", "Wadii Boulila"], "title": "SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Arabic Optical Character Recognition (OCR) is essential for converting vast\namounts of Arabic print media into digital formats. However, training modern\nOCR models, especially powerful vision-language models, is hampered by the lack\nof large, diverse, and well-structured datasets that mimic real-world book\nlayouts. Existing Arabic OCR datasets often focus on isolated words or lines or\nare limited in scale, typographic variety, or structural complexity found in\nbooks. To address this significant gap, we introduce SARD (Large-Scale\nSynthetic Arabic OCR Dataset). SARD is a massive, synthetically generated\ndataset specifically designed to simulate book-style documents. It comprises\n843,622 document images containing 690 million words, rendered across ten\ndistinct Arabic fonts to ensure broad typographic coverage. Unlike datasets\nderived from scanned documents, SARD is free from real-world noise and\ndistortions, offering a clean and controlled environment for model training.\nIts synthetic nature provides unparalleled scalability and allows for precise\ncontrol over layout and content variation. We detail the dataset's composition\nand generation process and provide benchmark results for several OCR models,\nincluding traditional and deep learning approaches, highlighting the challenges\nand opportunities presented by this dataset. SARD serves as a valuable resource\nfor developing and evaluating robust OCR and vision-language models capable of\nprocessing diverse Arabic book-style texts.", "AI": {"tldr": "SARD\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5408\u6210\u7684\u963f\u62c9\u4f2f\u8bedOCR\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u7ed3\u6784\u590d\u6742\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30OCR\u6a21\u578b\u63d0\u4f9b\u5e72\u51c0\u3001\u53ef\u63a7\u7684\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u963f\u62c9\u4f2f\u8bedOCR\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u4f4e\u4e14\u7f3a\u4e4f\u771f\u5b9e\u4e66\u7c4d\u5e03\u5c40\u7684\u590d\u6742\u6027\uff0c\u9650\u5236\u4e86\u73b0\u4ee3OCR\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5408\u6210\u751f\u6210843,622\u4e2a\u6587\u6863\u56fe\u50cf\uff0c\u5305\u542b6.9\u4ebf\u5355\u8bcd\uff0c\u8986\u76d610\u79cd\u963f\u62c9\u4f2f\u5b57\u4f53\uff0c\u6a21\u62df\u4e66\u7c4d\u5e03\u5c40\uff0c\u907f\u514d\u771f\u5b9e\u6587\u6863\u7684\u566a\u58f0\u548c\u5931\u771f\u3002", "result": "SARD\u4e3aOCR\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5176\u6311\u6218\u548c\u6f5c\u529b\u3002", "conclusion": "SARD\u662f\u5f00\u53d1\u548c\u8bc4\u4f30\u9c81\u68d2OCR\u53ca\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u80fd\u591f\u5904\u7406\u591a\u6837\u5316\u7684\u963f\u62c9\u4f2f\u4e66\u7c4d\u6587\u672c\u3002"}}
{"id": "2505.24255", "pdf": "https://arxiv.org/pdf/2505.24255", "abs": "https://arxiv.org/abs/2505.24255", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) have shown potential in simulating human\nbehaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for\ncomplex social interactions. In this study, we investigate the role of ToM\nreasoning in aligning agentic behaviors with human norms in negotiation tasks,\nusing the ultimatum game as a controlled environment. We initialized LLM agents\nwith different prosocial beliefs (including Greedy, Fair, and Selfless) and\nreasoning methods like chain-of-thought (CoT) and varying ToM levels, and\nexamined their decision-making processes across diverse LLMs, including\nreasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from\n2,700 simulations indicated that ToM reasoning enhances behavior alignment,\ndecision-making consistency, and negotiation outcomes. Consistent with previous\nfindings, reasoning models exhibit limited capability compared to models with\nToM reasoning, different roles of the game benefits with different orders of\nToM reasoning. Our findings contribute to the understanding of ToM's role in\nenhancing human-AI interaction and cooperative decision-making. The code used\nfor our experiments can be found at https://github.com/Stealth-py/UltimatumToM.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8c08\u5224\u4efb\u52a1\u4e2d\u901a\u8fc7\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u63a8\u7406\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\u7684\u4f5c\u7528\uff0c\u53d1\u73b0ToM\u63a8\u7406\u80fd\u63d0\u5347\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u8c08\u5224\u7ed3\u679c\u3002", "motivation": "\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u63a8\u7406\u5bf9\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22ToM\u5728LLMs\u4e2d\u5982\u4f55\u5e2e\u52a9\u4ee3\u7406\u884c\u4e3a\u4e0e\u4eba\u7c7b\u89c4\u8303\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528\u6700\u540e\u901a\u7252\u6e38\u620f\u4f5c\u4e3a\u5b9e\u9a8c\u73af\u5883\uff0c\u521d\u59cb\u5316\u4e0d\u540c\u4eb2\u793e\u4f1a\u4fe1\u5ff5\u7684LLM\u4ee3\u7406\uff0c\u5e76\u91c7\u7528\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u4e0d\u540cToM\u6c34\u5e73\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "2,700\u6b21\u6a21\u62df\u663e\u793a\uff0cToM\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e3a\u5bf9\u9f50\u548c\u8c08\u5224\u7ed3\u679c\uff0c\u63a8\u7406\u6a21\u578b\u8868\u73b0\u6709\u9650\u3002", "conclusion": "ToM\u63a8\u7406\u6709\u52a9\u4e8e\u589e\u5f3a\u4eba\u673a\u4e92\u52a8\u548c\u5408\u4f5c\u51b3\u7b56\uff0c\u7814\u7a76\u4e3a\u7406\u89e3ToM\u5728AI\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.24608", "pdf": "https://arxiv.org/pdf/2505.24608", "abs": "https://arxiv.org/abs/2505.24608", "authors": ["Panagiotis Rigas", "Panagiotis Drivas", "Charalambos Tzamos", "Ioannis Chamodrakas", "George Ioannakis", "Leonidas J. Guibas", "Ioannis Z. Emiris"], "title": "GARLIC: GAussian Representation LearnIng for spaCe partitioning", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GARLIC (GAussian Representation LearnIng for spaCe\npartitioning), a novel indexing structure based on \\(N\\)-dimensional Gaussians\nfor efficiently learning high-dimensional vector spaces. Our approach is\ninspired from Gaussian splatting techniques, typically used in 3D rendering,\nwhich we adapt for high-dimensional search and classification. We optimize\nGaussian parameters using information-theoretic objectives that balance\ncoverage, assignment confidence, and structural and semantic consistency. A key\ncontribution is to progressively refine the representation through split and\nclone operations, handling hundreds of dimensions, thus handling varying data\ndensities. GARLIC offers the fast building times of traditional space\npartitioning methods (e.g., under \\(\\sim5\\) min build time for SIFT1M) while\nachieving \\(\\sim50\\%\\) Recall10@10 in low-candidate regimes. Experimental\nresults on standard benchmarks demonstrate our method's consistency in (a)\n\\(k\\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by\nusing about half their probes for the same Recall10@10 in Fashion-MNIST, and\n(b) in classification tasks, beating by \\(\\sim15\\%\\) accuracy other majority\nvoting methods. Further, we show strong generalization capabilities,\nmaintaining high accuracy even with downsampled training data: using just\n\\(1\\%\\) of the training data returns \\(\\sim 45\\%\\) Recall@1, thus making GARLIC\nquite powerful for applications requiring both speed and accuracy.", "AI": {"tldr": "GARLIC\u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u7ef4\u9ad8\u65af\u5206\u5e03\u7684\u65b0\u578b\u7d22\u5f15\u7ed3\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u5b66\u4e60\u548c\u641c\u7d22\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4\uff0c\u7ed3\u5408\u4fe1\u606f\u8bba\u76ee\u6807\u4f18\u5316\uff0c\u5728\u5feb\u901f\u6784\u5efa\u548c\u9ad8\u53ec\u56de\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4\u7684\u9ad8\u6548\u5b66\u4e60\u548c\u641c\u7d22\u662f\u8bb8\u591a\u5e94\u7528\u7684\u6838\u5fc3\u9700\u6c42\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5feb\u901f\u6784\u5efa\u53c8\u80fd\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "GARLIC\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u8868\u793a\u7a7a\u95f4\u5206\u533a\uff0c\u5229\u7528\u4fe1\u606f\u8bba\u76ee\u6807\u4f18\u5316\u9ad8\u65af\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u5206\u88c2\u548c\u514b\u9686\u64cd\u4f5c\u9010\u6b65\u7ec6\u5316\u8868\u793a\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGARLIC\u5728k-NN\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ec\u56de\u7387\u548c\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u5c0f\u6837\u672c\u8bad\u7ec3\u6570\u636e\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "GARLIC\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4\u7d22\u5f15\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u901f\u5ea6\u548c\u7cbe\u5ea6\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.24263", "pdf": "https://arxiv.org/pdf/2505.24263", "abs": "https://arxiv.org/abs/2505.24263", "authors": ["Naila Shafirni Hidayat", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The performance of large language models (LLMs) continues to improve, as\nreflected in rising scores on standard benchmarks. However, the lack of\ntransparency around training data raises concerns about potential overlap with\nevaluation sets and the fairness of reported results. Although prior work has\nproposed methods for detecting data leakage, these approaches primarily focus\non identifying outliers and have not been evaluated under controlled simulated\nleakage conditions. In this work, we compare existing leakage detection\ntechniques, namely permutation and n-gram-based methods, under a continual\npretraining setup that simulates real-world leakage scenarios, and additionally\nexplore a lightweight method we call semi-half question. Although semi-half\noffers a low-cost alternative, our analysis shows that the n-gram method\nconsistently achieves the highest F1-score. We also refine these techniques to\nsupport instance-level detection and reduce computational overhead. Leveraging\nthe best-performing method, we create cleaned versions of MMLU and HellaSwag,\nand re-evaluate several LLMs. Our findings present a practical path toward more\nreliable and transparent evaluations, and we recommend contamination checks as\na standard step before releasing benchmark results.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u73b0\u6709\u6570\u636e\u6cc4\u6f0f\u68c0\u6d4b\u6280\u672f\uff08\u5982\u6392\u5217\u548cn-gram\u65b9\u6cd5\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5semi-half question\u3002\u7814\u7a76\u53d1\u73b0n-gram\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u6539\u8fdb\u4e86\u8fd9\u4e9b\u6280\u672f\u4ee5\u652f\u6301\u5b9e\u4f8b\u7ea7\u68c0\u6d4b\u3002\u901a\u8fc7\u6e05\u7406MMLU\u548cHellaSwag\u6570\u636e\u96c6\uff0c\u91cd\u65b0\u8bc4\u4f30\u4e86\u591a\u4e2aLLM\uff0c\u5efa\u8bae\u5c06\u6c61\u67d3\u68c0\u67e5\u4f5c\u4e3a\u53d1\u5e03\u57fa\u51c6\u7ed3\u679c\u524d\u7684\u6807\u51c6\u6b65\u9aa4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u4e0d\u900f\u660e\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u96c6\u91cd\u53e0\uff0c\u5f71\u54cd\u7ed3\u679c\u516c\u5e73\u6027\u3002\u73b0\u6709\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\u672a\u5728\u6a21\u62df\u6cc4\u6f0f\u6761\u4ef6\u4e0b\u8bc4\u4f30\u3002", "method": "\u6bd4\u8f83\u73b0\u6709\u6cc4\u6f0f\u68c0\u6d4b\u6280\u672f\uff08\u6392\u5217\u548cn-gram\u65b9\u6cd5\uff09\uff0c\u63d0\u51fasemi-half question\u65b9\u6cd5\uff0c\u6539\u8fdb\u6280\u672f\u4ee5\u652f\u6301\u5b9e\u4f8b\u7ea7\u68c0\u6d4b\u3002", "result": "n-gram\u65b9\u6cd5F1-score\u6700\u9ad8\u3002\u6e05\u7406\u540e\u7684MMLU\u548cHellaSwag\u6570\u636e\u96c6\u91cd\u65b0\u8bc4\u4f30LLM\uff0c\u7ed3\u679c\u66f4\u53ef\u9760\u3002", "conclusion": "\u5efa\u8bae\u5c06\u6c61\u67d3\u68c0\u67e5\u4f5c\u4e3a\u6807\u51c6\u6b65\u9aa4\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2505.24625", "pdf": "https://arxiv.org/pdf/2505.24625", "abs": "https://arxiv.org/abs/2505.24625", "authors": ["Duo Zheng", "Shijia Huang", "Yanyang Li", "Liwei Wang"], "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u89c6\u9891\u6570\u636e\u7406\u89e33D\u573a\u666f\u7684\u65b9\u6cd5VG LLM\uff0c\u65e0\u9700\u989d\u59163D\u8f93\u5165\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u76f4\u63a5\u4ece\u89c6\u9891\u7406\u89e33D\u7a7a\u95f4\u7684\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u590d\u67423D\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u75283D\u89c6\u89c9\u51e0\u4f55\u7f16\u7801\u5668\u4ece\u89c6\u9891\u5e8f\u5217\u63d0\u53d63D\u5148\u9a8c\u4fe1\u606f\uff0c\u7ed3\u5408\u89c6\u89c9\u6807\u8bb0\u8f93\u5165MLLM\u3002", "result": "\u57283D\u573a\u666f\u7406\u89e3\u548c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c4B\u6a21\u578b\u5728VSI-Bench\u8bc4\u6d4b\u4e2d\u8d85\u8d8aGemini-1.5-Pro\u3002", "conclusion": "VG LLM\u65b9\u6cd5\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u76f4\u63a5\u4ece\u89c6\u9891\u5b66\u4e603D\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24264", "pdf": "https://arxiv.org/pdf/2505.24264", "abs": "https://arxiv.org/abs/2505.24264", "authors": ["Xin Quan", "Marco Valentino", "Louise A. Dennis", "Andr\u00e9 Freitas"], "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations", "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready for ACL 2025", "summary": "Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5b9a\u7406\u8bc1\u660e\u5668\uff08TP\uff09\u6765\u6539\u8fdb\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u7b56\u7565\u4ee5\u51cf\u5c11\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u5e76\u63d0\u5347\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u4e0eTP\u7ed3\u5408\u65f6\u56e0\u81ea\u7136\u8bed\u8a00\u5230\u5f62\u5f0f\u5316\u8868\u793a\u7684\u8f6c\u6362\u5bfc\u81f4\u7684\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u548c\u89e3\u91ca\u4e0d\u5fe0\u5b9e\u95ee\u9898\uff0c\u4ee5\u53caLLM\u5728\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u7b56\u7565\uff1a1) \u51cf\u5c11\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u8bed\u4e49\u635f\u5931\uff1b2) \u9ad8\u6548\u8bc6\u522b\u548c\u4fee\u6b63\u903b\u8f91\u8868\u793a\u4e2d\u7684\u8bed\u6cd5\u9519\u8bef\uff1b3) \u4f7f\u7528\u903b\u8f91\u8868\u8fbe\u5f0f\u5f15\u5bfcLLM\u751f\u6210\u7ed3\u6784\u5316\u8bc1\u660e\u8349\u56fe\uff1b4) \u63d0\u5347LLM\u5bf9TP\u53cd\u9988\u7684\u8fed\u4ee3\u6539\u8fdb\u80fd\u529b\u3002", "result": "\u5728e-SNLI\u3001QASC\u548cWorldTree\u6570\u636e\u96c6\u4e0a\uff0c\u81ea\u52a8\u5f62\u5f0f\u5316\u548c\u89e3\u91ca\u7cbe\u70bc\u5206\u522b\u63d0\u5347\u4e8618.46%-39.77%\u548c29.5%-51.5%\uff0c\u9a8c\u8bc1\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u7279\u5b9a\u5e72\u9884\u7b56\u7565\uff0cLLM-TP\u6df7\u5408\u67b6\u6784\u5728NLI\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u6548\u7387\u5927\u5e45\u63d0\u5347\u3002"}}
{"id": "2505.24634", "pdf": "https://arxiv.org/pdf/2505.24634", "abs": "https://arxiv.org/abs/2505.24634", "authors": ["Xuzhi Wang", "Wei Feng", "Lingdong Kong", "Liang Wan"], "title": "NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR semantic segmentation plays a vital role in autonomous driving.\nExisting voxel-based methods for LiDAR semantic segmentation apply uniform\npartition to the 3D LiDAR point cloud to form a structured representation based\non cartesian/cylindrical coordinates. Although these methods show impressive\nperformance, the drawback of existing voxel-based methods remains in two\naspects: (1) it requires a large enough input voxel resolution, which brings a\nlarge amount of computation cost and memory consumption. (2) it does not well\nhandle the unbalanced point distribution of LiDAR point cloud. In this paper,\nwe propose a non-uniform cylindrical partition network named NUC-Net to tackle\nthe above challenges. Specifically, we propose the Arithmetic Progression of\nInterval (API) method to non-uniformly partition the radial axis and generate\nthe voxel representation which is representative and efficient. Moreover, we\npropose a non-uniform multi-scale aggregation method to improve contextual\ninformation. Our method achieves state-of-the-art performance on SemanticKITTI\nand nuScenes datasets with much faster speed and much less training time. And\nour method can be a general component for LiDAR semantic segmentation, which\nsignificantly improves both the accuracy and efficiency of the uniform\ncounterpart by $4 \\times$ training faster and $2 \\times$ GPU memory reduction\nand $3 \\times$ inference speedup. We further provide theoretical analysis\ntowards understanding why NUC is effective and how point distribution affects\nperformance. Code is available at\n\\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}.", "AI": {"tldr": "NUC-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u5747\u5300\u5706\u67f1\u5206\u5272\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86LiDAR\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u70b9\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f53\u7d20\u7684\u65b9\u6cd5\u5728LiDAR\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u6d88\u8017\uff0c\u4e14\u672a\u80fd\u6709\u6548\u5904\u7406\u70b9\u4e91\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u975e\u5747\u5300\u5706\u67f1\u5206\u5272\u7f51\u7edc\uff08NUC-Net\uff09\uff0c\u91c7\u7528API\u65b9\u6cd5\u975e\u5747\u5300\u5206\u5272\u5f84\u5411\u8f74\uff0c\u5e76\u7ed3\u5408\u975e\u5747\u5300\u591a\u5c3a\u5ea6\u805a\u5408\u65b9\u6cd5\u63d0\u5347\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728SemanticKITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53474\u500d\uff0cGPU\u5185\u5b58\u51cf\u5c112\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53473\u500d\u3002", "conclusion": "NUC-Net\u662f\u4e00\u79cd\u9ad8\u6548\u901a\u7528\u7684LiDAR\u8bed\u4e49\u5206\u5272\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u652f\u6301\u3002"}}
{"id": "2505.24302", "pdf": "https://arxiv.org/pdf/2505.24302", "abs": "https://arxiv.org/abs/2505.24302", "authors": ["Yike Wang", "Shangbin Feng", "Yulia Tsvetkov", "Hannaneh Hajishirzi"], "title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support scientific\nresearch, but their knowledge of scientific advancements can quickly become\noutdated. We introduce ScienceMeter, a new framework for evaluating scientific\nknowledge update methods over scientific knowledge spanning the past, present,\nand future. ScienceMeter defines three metrics: knowledge preservation, the\nextent to which models' understanding of previously learned papers are\npreserved; knowledge acquisition, how well scientific claims from newly\nintroduced papers are acquired; and knowledge projection, the ability of the\nupdated model to anticipate or generalize to related scientific claims that may\nemerge in the future. Using ScienceMeter, we examine the scientific knowledge\nof LLMs on claim judgment and generation tasks across a curated dataset of\n15,444 scientific papers and 30,888 scientific claims from ten domains\nincluding medicine, biology, materials science, and computer science. We\nevaluate five representative knowledge update approaches including training-\nand inference-time methods. With extensive experiments, we find that the\nbest-performing knowledge update methods can preserve only 85.9% of existing\nknowledge, acquire 71.7% of new knowledge, and project 37.7% of future\nknowledge. Inference-based methods work for larger models, whereas smaller\nmodels require training to achieve comparable performance. Cross-domain\nanalysis reveals that performance on these objectives is correlated. Even when\napplying on specialized scientific LLMs, existing knowledge update methods fail\nto achieve these objectives collectively, underscoring that developing robust\nscientific knowledge update mechanisms is both crucial and challenging.", "AI": {"tldr": "ScienceMeter\u6846\u67b6\u8bc4\u4f30LLMs\u79d1\u5b66\u77e5\u8bc6\u66f4\u65b0\u65b9\u6cd5\uff0c\u6db5\u76d6\u8fc7\u53bb\u3001\u73b0\u5728\u548c\u672a\u6765\u77e5\u8bc6\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u4fdd\u7559\u3001\u83b7\u53d6\u548c\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u89e3\u51b3LLMs\u79d1\u5b66\u77e5\u8bc6\u5feb\u901f\u8fc7\u65f6\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u5176\u77e5\u8bc6\u66f4\u65b0\u80fd\u529b\u3002", "method": "\u63d0\u51faScienceMeter\u6846\u67b6\uff0c\u5b9a\u4e49\u77e5\u8bc6\u4fdd\u7559\u3001\u83b7\u53d6\u548c\u9884\u6d4b\u4e09\u4e2a\u6307\u6807\uff0c\u5e76\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e94\u79cd\u77e5\u8bc6\u66f4\u65b0\u65b9\u6cd5\u3002", "result": "\u6700\u4f73\u65b9\u6cd5\u4ec5\u4fdd\u755985.9%\u73b0\u6709\u77e5\u8bc6\uff0c\u83b7\u53d671.7%\u65b0\u77e5\u8bc6\uff0c\u9884\u6d4b37.7%\u672a\u6765\u77e5\u8bc6\u3002\u5c0f\u6a21\u578b\u9700\u8bad\u7ec3\uff0c\u5927\u6a21\u578b\u53ef\u57fa\u4e8e\u63a8\u7406\u3002", "conclusion": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u6ee1\u8db3\u79d1\u5b66\u77e5\u8bc6\u66f4\u65b0\u9700\u6c42\uff0c\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u673a\u5236\u81f3\u5173\u91cd\u8981\u4e14\u5177\u6311\u6218\u6027\u3002"}}
{"id": "2505.24636", "pdf": "https://arxiv.org/pdf/2505.24636", "abs": "https://arxiv.org/abs/2505.24636", "authors": ["Marios Glytsos", "Panagiotis P. Filntisis", "George Retsinas", "Petros Maragos"], "title": "Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data", "categories": ["cs.CV"], "comment": "7 pages, 4 figures. Submitted to the IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) 2025. This work has been\n  submitted to the IEEE for possible publication", "summary": "Accurate 6D object pose estimation is essential for robotic grasping and\nmanipulation, particularly in agriculture, where fruits and vegetables exhibit\nhigh intra-class variability in shape, size, and texture. The vast majority of\nexisting methods rely on instance-specific CAD models or require depth sensors\nto resolve geometric ambiguities, making them impractical for real-world\nagricultural applications. In this work, we introduce PLANTPose, a novel\nframework for category-level 6D pose estimation that operates purely on RGB\ninput. PLANTPose predicts both the 6D pose and deformation parameters relative\nto a base mesh, allowing a single category-level CAD model to adapt to unseen\ninstances. This enables accurate pose estimation across varying shapes without\nrelying on instance-specific data. To enhance realism and improve\ngeneralization, we also leverage Stable Diffusion to refine synthetic training\nimages with realistic texturing, mimicking variations due to ripeness and\nenvironmental factors and bridging the domain gap between synthetic data and\nthe real world. Our evaluations on a challenging benchmark that includes\nbananas of various shapes, sizes, and ripeness status demonstrate the\neffectiveness of our framework in handling large intraclass variations while\nmaintaining accurate 6D pose predictions, significantly outperforming the\nstate-of-the-art RGB-based approach MegaPose.", "AI": {"tldr": "PLANTPose\u662f\u4e00\u4e2a\u57fa\u4e8eRGB\u8f93\u5165\u7684\u7c7b\u522b\u7ea76D\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u76f8\u5bf9\u4e8e\u57fa\u7840\u7f51\u683c\u76846D\u59ff\u6001\u548c\u53d8\u5f62\u53c2\u6570\uff0c\u9002\u5e94\u672a\u89c1\u5b9e\u4f8b\uff0c\u65e0\u9700\u4f9d\u8d56\u5b9e\u4f8b\u7279\u5b9a\u6570\u636e\u3002", "motivation": "\u519c\u4e1a\u4e2d\u6c34\u679c\u548c\u852c\u83dc\u7684\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u7eb9\u7406\u5b58\u5728\u9ad8\u5ea6\u7c7b\u5185\u5dee\u5f02\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b9e\u4f8b\u7279\u5b9aCAD\u6a21\u578b\u6216\u6df1\u5ea6\u4f20\u611f\u5668\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "method": "PLANTPose\u9884\u6d4b6D\u59ff\u6001\u548c\u53d8\u5f62\u53c2\u6570\uff0c\u5229\u7528Stable Diffusion\u589e\u5f3a\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u7684\u7eb9\u7406\u771f\u5b9e\u6027\u3002", "result": "\u5728\u5305\u542b\u4e0d\u540c\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u6210\u719f\u5ea6\u7684\u9999\u8549\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPLANTPose\u663e\u8457\u4f18\u4e8e\u73b0\u6709RGB\u65b9\u6cd5MegaPose\u3002", "conclusion": "PLANTPose\u80fd\u591f\u6709\u6548\u5904\u7406\u7c7b\u5185\u5dee\u5f02\uff0c\u5b9e\u73b0\u51c6\u786e\u76846D\u59ff\u6001\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u7b49\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2505.24319", "pdf": "https://arxiv.org/pdf/2505.24319", "abs": "https://arxiv.org/abs/2505.24319", "authors": ["Yuntao Shi", "Yi Luo", "Yeyun Gong", "Chen Lin"], "title": "HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains. However, when handling long-form text modification tasks, they still\nface two major problems: (1) producing undesired modifications by\ninappropriately altering or summarizing irrelevant content, and (2) missing\nnecessary modifications to implicitly related passages that are crucial for\nmaintaining document coherence. To address these issues, we propose HiCaM, a\nHierarchical-Causal Modification framework that operates through a hierarchical\nsummary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a\nmulti-domain dataset from various benchmarks, providing a resource for\nassessing its effectiveness. Comprehensive evaluations on the dataset\ndemonstrate significant improvements over strong LLMs, with our method\nachieving up to a 79.50\\% win rate. These results highlight the\ncomprehensiveness of our approach, showing consistent performance improvements\nacross multiple models and domains.", "AI": {"tldr": "HiCaM\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u5316\u6458\u8981\u6811\u548c\u56e0\u679c\u56fe\u89e3\u51b3LLMs\u5728\u957f\u6587\u672c\u4fee\u6539\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLMs\u5728\u957f\u6587\u672c\u4fee\u6539\u4e2d\u5b58\u5728\u4e0d\u5fc5\u8981\u4fee\u6539\u548c\u9057\u6f0f\u5173\u952e\u4fee\u6539\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u6587\u6863\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51faHiCaM\u6846\u67b6\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u6458\u8981\u6811\u548c\u56e0\u679c\u56fe\u8fdb\u884c\u4fee\u6539\u3002", "result": "\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cHiCaM\u7684\u80dc\u7387\u8fbe79.50%\uff0c\u4f18\u4e8e\u73b0\u6709LLMs\u3002", "conclusion": "HiCaM\u5728\u591a\u4e2a\u6a21\u578b\u548c\u9886\u57df\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u672c\u4fee\u6539\u7684\u5168\u9762\u6027\u3002"}}
{"id": "2505.24638", "pdf": "https://arxiv.org/pdf/2505.24638", "abs": "https://arxiv.org/abs/2505.24638", "authors": ["Zahid Hassan Tushar", "Adeleke Ademakinwa", "Jianwu Wang", "Zhibo Zhang", "Sanjay Purushotham"], "title": "Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 7 figures, to be published in 2025 IEEE International\n  Conference on Image Processing (ICIP)", "summary": "Cloud Optical Thickness (COT) is a critical cloud property influencing\nEarth's climate, weather, and radiation budget. Satellite radiance measurements\nenable global COT retrieval, but challenges like 3D cloud effects, viewing\nangles, and atmospheric interference must be addressed to ensure accurate\nestimation. Traditionally, the Independent Pixel Approximation (IPA) method,\nwhich treats individual pixels independently, has been used for COT estimation.\nHowever, IPA introduces significant bias due to its simplified assumptions.\nRecently, deep learning-based models have shown improved performance over IPA\nbut lack robustness, as they are sensitive to variations in radiance intensity,\ndistortions, and cloud shadows. These models also introduce substantial errors\nin COT estimation under different solar and viewing zenith angles. To address\nthese challenges, we propose a novel angle-invariant, attention-based deep\nmodel called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages\nattention mechanisms and angle embeddings to account for satellite viewing\ngeometry and 3D radiative transfer effects, enabling more accurate retrieval of\nCOT. Additionally, our multi-angle training strategy ensures angle invariance.\nThrough comprehensive experiments, we demonstrate that CAAC significantly\noutperforms existing state-of-the-art deep learning models, reducing cloud\nproperty retrieval errors by at least a factor of nine.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAAC\u7684\u65b0\u578b\u89d2\u5ea6\u4e0d\u53d8\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u4e91\u5149\u5b66\u539a\u5ea6\uff08COT\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u72ec\u7acb\u50cf\u7d20\u8fd1\u4f3c\uff08IPA\uff09\u65b9\u6cd5\u56e0\u7b80\u5316\u5047\u8bbe\u5f15\u5165\u663e\u8457\u504f\u5dee\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u8f90\u5c04\u5f3a\u5ea6\u3001\u7578\u53d8\u548c\u4e91\u9634\u5f71\u53d8\u5316\u654f\u611f\uff0c\u4e14\u5728\u4e0d\u540c\u89d2\u5ea6\u4e0b\u8bef\u5dee\u8f83\u5927\u3002", "method": "\u63d0\u51faCloud-Attention-Net with Angle Coding\uff08CAAC\uff09\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u89d2\u5ea6\u5d4c\u5165\uff0c\u8003\u8651\u536b\u661f\u89c6\u89d2\u51e0\u4f55\u548c3D\u8f90\u5c04\u4f20\u8f93\u6548\u5e94\uff0c\u5e76\u91c7\u7528\u591a\u89d2\u5ea6\u8bad\u7ec3\u7b56\u7565\u786e\u4fdd\u89d2\u5ea6\u4e0d\u53d8\u6027\u3002", "result": "CAAC\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u4e91\u5c5e\u6027\u68c0\u7d22\u8bef\u5dee\u81f3\u5c11\u964d\u4f4e\u4e5d\u500d\u3002", "conclusion": "CAAC\u901a\u8fc7\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u89d2\u5ea6\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aCOT\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24331", "pdf": "https://arxiv.org/pdf/2505.24331", "abs": "https://arxiv.org/abs/2505.24331", "authors": ["Fanhang Man", "Huandong Wang", "Jianjie Fang", "Zhaoyi Deng", "Baining Zhao", "Xinlei Chen", "Yong Li"], "title": "Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents", "categories": ["cs.CL"], "comment": null, "summary": "User sentiment on social media reveals the underlying social trends, crises,\nand needs. Researchers have analyzed users' past messages to trace the\nevolution of sentiments and reconstruct sentiment dynamics. However, predicting\nthe imminent sentiment of an ongoing event is rarely studied. In this paper, we\naddress the problem of \\textbf{sentiment forecasting} on social media to\npredict the user's future sentiment in response to the development of the\nevent. We extract sentiment-related features to enhance the modeling skill and\npropose a multi-perspective role-playing framework to simulate the process of\nhuman response. Our preliminary results show significant improvement in\nsentiment forecasting on both microscopic and macroscopic levels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u60c5\u611f\u76f8\u5173\u7279\u5f81\u5e76\u6a21\u62df\u4eba\u7c7b\u54cd\u5e94\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u60c5\u611f\u53cd\u6620\u4e86\u793e\u4f1a\u8d8b\u52bf\u548c\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5386\u53f2\u60c5\u611f\u5206\u6790\uff0c\u800c\u5bf9\u672a\u6765\u60c5\u611f\u9884\u6d4b\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u53d6\u60c5\u611f\u76f8\u5173\u7279\u5f81\uff0c\u63d0\u51fa\u591a\u89c6\u89d2\u89d2\u8272\u626e\u6f14\u6846\u67b6\u6a21\u62df\u4eba\u7c7b\u54cd\u5e94\u8fc7\u7a0b\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5728\u5fae\u89c2\u548c\u5b8f\u89c2\u5c42\u9762\u5747\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.24641", "pdf": "https://arxiv.org/pdf/2505.24641", "abs": "https://arxiv.org/abs/2505.24641", "authors": ["Chengzhi Wu", "Qianliang Huang", "Kun Jin", "Julius Pfrommer", "J\u00fcrgen Beyerer"], "title": "A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning is an essential method in self-supervised learning. It\nprimarily employs a multi-branch strategy to compare latent representations\nobtained from different branches and train the encoder. In the case of\nmulti-modal input, diverse modalities of the same object are fed into distinct\nbranches. When using single-modal data, the same input undergoes various\naugmentations before being fed into different branches. However, all existing\ncontrastive learning frameworks have so far only performed contrastive\noperations on the learned features at the final loss end, with no information\nexchange between different branches prior to this stage. In this paper, for\npoint cloud unsupervised learning without the use of extra training data, we\npropose a Contrastive Cross-branch Attention-based framework for Point cloud\ndata (termed PoCCA), to learn rich 3D point cloud representations. By\nintroducing sub-branches, PoCCA allows information exchange between different\nbranches before the loss end. Experimental results demonstrate that in the case\nof using no extra training data, the representations learned with our\nself-supervised model achieve state-of-the-art performances when used for\ndownstream tasks on point clouds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u8de8\u5206\u652f\u6ce8\u610f\u529b\u7684\u70b9\u4e91\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6PoCCA\uff0c\u901a\u8fc7\u5728\u635f\u5931\u7aef\u4e4b\u524d\u5f15\u5165\u5b50\u5206\u652f\u5b9e\u73b0\u4fe1\u606f\u4ea4\u6362\uff0c\u63d0\u5347\u4e86\u70b9\u4e91\u8868\u5f81\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4ec5\u5728\u635f\u5931\u7aef\u5bf9\u7279\u5f81\u8fdb\u884c\u5bf9\u6bd4\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u5206\u652f\u95f4\u7684\u4fe1\u606f\u4ea4\u6362\uff0c\u9650\u5236\u4e86\u8868\u5f81\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faPoCCA\u6846\u67b6\uff0c\u5f15\u5165\u5b50\u5206\u652f\u5b9e\u73b0\u5206\u652f\u95f4\u4fe1\u606f\u4ea4\u6362\uff0c\u9002\u7528\u4e8e\u70b9\u4e91\u6570\u636e\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0d\u4f7f\u7528\u989d\u5916\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0cPoCCA\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "PoCCA\u901a\u8fc7\u5206\u652f\u95f4\u4fe1\u606f\u4ea4\u6362\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u8868\u5f81\u5b66\u4e60\u6548\u679c\uff0c\u4e3a\u65e0\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24332", "pdf": "https://arxiv.org/pdf/2505.24332", "abs": "https://arxiv.org/abs/2505.24332", "authors": ["Wenxuan Shi", "Haochen Tan", "Chuqiao Kuang", "Xiaoguang Li", "Xiaozhe Ren", "Chen Zhang", "Hanting Chen", "Yasheng Wang", "Lifeng Shang", "Fisher Yu", "Yunhe Wang"], "title": "Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Information seeking demands iterative evidence gathering and reflective\nreasoning, yet large language models (LLMs) still struggle with it in open-web\nquestion answering. Existing methods rely on static prompting rules or training\nwith Wikipedia-based corpora and retrieval environments, limiting adaptability\nto the real-world web environment where ambiguity, conflicting evidence, and\nnoise are prevalent. These constrained training settings hinder LLMs from\nlearning to dynamically decide when and where to search, and how to adjust\nsearch depth and frequency based on informational demands. We define this\nmissing capacity as Search Intensity Scaling (SIS)--the emergent skill to\nintensify search efforts under ambiguous or conflicting conditions, rather than\nsettling on overconfident, under-verification answers.\n  To study SIS, we introduce WebPuzzle, the first dataset designed to foster\ninformation-seeking behavior in open-world internet environments. WebPuzzle\nconsists of 24K training instances and 275 test questions spanning both\nwiki-based and open-web queries. Building on this dataset, we propose\nDeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by\nencouraging adaptive search policies through exploration under a real-world\nopen-web environment. Experimental results show that Pangu-7B-Reasoner\nempowered by DeepDiver achieve performance on real-web tasks comparable to the\n671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from\ncold-start supervised fine-tuning to a carefully designed RL phase, and present\nthat its capability of SIS generalizes from closed-form QA to open-ended tasks\nsuch as long-form writing. Our contributions advance adaptive information\nseeking in LLMs and provide a valuable benchmark and dataset for future\nresearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSearch Intensity Scaling (SIS)\u7684\u80fd\u529b\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u7f51\u7edc\u95ee\u7b54\u4e2d\u7684\u4fe1\u606f\u641c\u7d22\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86WebPuzzle\u6570\u636e\u96c6\u548cDeepDiver\u6846\u67b6\u6765\u4fc3\u8fdbSIS\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u52a8\u6001\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u6a21\u7cca\u6216\u51b2\u7a81\u8bc1\u636e\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faWebPuzzle\u6570\u636e\u96c6\u548cDeepDiver\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u641c\u7d22\u7b56\u7565\u63d0\u5347\u6a21\u578b\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepDiver\u6846\u67b6\u4f7fPangu-7B-Reasoner\u5728\u771f\u5b9e\u7f51\u7edc\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1671B\u53c2\u6570\u7684DeepSeek-R1\u3002", "conclusion": "DeepDiver\u6846\u67b6\u548cWebPuzzle\u6570\u636e\u96c6\u4e3aLLMs\u7684\u81ea\u9002\u5e94\u4fe1\u606f\u641c\u7d22\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2505.24649", "pdf": "https://arxiv.org/pdf/2505.24649", "abs": "https://arxiv.org/abs/2505.24649", "authors": ["Huu-Thien Tran", "Thanh-Dat Truong", "Khoa Luu"], "title": "BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025, 8 pages, 4 figures", "summary": "Large vision-language models have become widely adopted to advance in various\ndomains. However, developing a trustworthy system with minimal interpretable\ncharacteristics of large-scale models presents a significant challenge. One of\nthe most prevalent terms associated with the fallacy functions caused by these\nsystems is hallucination, where the language model generates a response that\ndoes not correspond to the visual content. To mitigate this problem, several\napproaches have been developed, and one prominent direction is to ameliorate\nthe decoding process. In this paper, we propose a new Bijective Maximum\nLikelihood Learning (BIMA) approach to hallucination mitigation using\nnormalizing flow theories. The proposed BIMA method can efficiently mitigate\nthe hallucination problem in prevailing vision-language models, resulting in\nsignificant improvements. Notably, BIMA achieves the average F1 score of 85.06%\non POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,\nrespectively. To the best of our knowledge, this is one of the first studies\nthat contemplates the bijection means to reduce hallucination induced by large\nvision-language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBIMA\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u51c6\u5316\u6d41\u7406\u8bba\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u4e0e\u89c6\u89c9\u5185\u5bb9\u4e0d\u7b26\u7684\u54cd\u5e94\uff0c\u4e9f\u9700\u89e3\u51b3\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u3002", "method": "\u91c7\u7528Bijective Maximum Likelihood Learning (BIMA)\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6807\u51c6\u5316\u6d41\u7406\u8bba\u4f18\u5316\u89e3\u7801\u8fc7\u7a0b\u3002", "result": "BIMA\u5728POPE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747F1\u5f97\u5206\u4e3a85.06%\uff0cCHAIRS\u548cCHAIRI\u5206\u522b\u964d\u4f4e\u4e867.6%\u548c2.6%\u3002", "conclusion": "BIMA\u662f\u9996\u6279\u5229\u7528\u53cc\u5c04\u65b9\u6cd5\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u7814\u7a76\u4e4b\u4e00\uff0c\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2505.24341", "pdf": "https://arxiv.org/pdf/2505.24341", "abs": "https://arxiv.org/abs/2505.24341", "authors": ["Shujian Yang", "Shiyao Cui", "Chuanrui Hu", "Haicheng Wang", "Tianwei Zhang", "Minlie Huang", "Jialiang Lu", "Han Qiu"], "title": "Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to ACL 2025 (Findings). Camera-ready version", "summary": "Detecting toxic content using language models is important but challenging.\nWhile large language models (LLMs) have demonstrated strong performance in\nunderstanding Chinese, recent studies show that simple character substitutions\nin toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In\nthis paper, we highlight the multimodal nature of Chinese language as a key\nchallenge for deploying LLMs in toxic Chinese detection. First, we propose a\ntaxonomy of 3 perturbation strategies and 8 specific approaches in toxic\nChinese content. Then, we curate a dataset based on this taxonomy, and\nbenchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect\nperturbed toxic Chinese text. Additionally, we explore cost-effective\nenhancement solutions like in-context learning (ICL) and supervised fine-tuning\n(SFT). Our results reveal two important findings. (1) LLMs are less capable of\ndetecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a\nsmall number of perturbed examples may cause the LLMs \"overcorrect'':\nmisidentify many normal Chinese contents as toxic.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u68c0\u6d4b\u4e2d\u6587\u6709\u6bd2\u5185\u5bb9\u65f6\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u5b57\u7b26\u66ff\u6362\u7b49\u6270\u52a8\u7b56\u7565\u65f6\u7684\u8868\u73b0\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u6270\u52a8\u5206\u7c7b\u6cd5\uff0c\u5e76\u6d4b\u8bd5\u4e869\u79cdSOTA LLMs\uff0c\u53d1\u73b0\u5176\u68c0\u6d4b\u80fd\u529b\u6709\u9650\uff0c\u4e14\u7b80\u5355\u589e\u5f3a\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8bef\u5224\u3002", "motivation": "\u4e2d\u6587\u6709\u6bd2\u5185\u5bb9\u68c0\u6d4b\u5bf9LLMs\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u5b57\u7b26\u66ff\u6362\u7b49\u6270\u52a8\u7b56\u7565\u5bb9\u6613\u6df7\u6dc6\u73b0\u6709\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u5176\u8868\u73b0\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e863\u79cd\u6270\u52a8\u7b56\u7565\u548c8\u79cd\u5177\u4f53\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u76f8\u5173\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u4e869\u79cdSOTA LLMs\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7b49\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0LLMs\u5bf9\u6270\u52a8\u540e\u7684\u4e2d\u6587\u6709\u6bd2\u5185\u5bb9\u68c0\u6d4b\u80fd\u529b\u8f83\u5f31\uff0c\u4e14ICL\u6216SFT\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u4fee\u6b63\uff0c\u8bef\u5224\u6b63\u5e38\u5185\u5bb9\u4e3a\u6709\u6bd2\u3002", "conclusion": "\u4e2d\u6587\u6709\u6bd2\u5185\u5bb9\u68c0\u6d4b\u9700\u8981\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u548c\u65b9\u6cd5\uff0c\u907f\u514d\u56e0\u7b80\u5355\u589e\u5f3a\u5bfc\u81f4\u7684\u8bef\u5224\u95ee\u9898\u3002"}}
{"id": "2505.24667", "pdf": "https://arxiv.org/pdf/2505.24667", "abs": "https://arxiv.org/abs/2505.24667", "authors": ["Jiahe Chen", "Jiahe Ying", "Shen Wang", "Jianwei Zheng"], "title": "Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Published in ECAI 2024", "summary": "Confronting the critical challenge of insufficiently annotated samples in\nmedical domain, semi-supervised medical image segmentation (SSMIS) emerges as a\npromising solution. Specifically, most methodologies following the Mean Teacher\n(MT) or Dual Students (DS) architecture have achieved commendable results.\nHowever, to date, these approaches face a performance bottleneck due to two\ninherent limitations, \\textit{e.g.}, the over-coupling problem within MT\nstructure owing to the employment of exponential moving average (EMA)\nmechanism, as well as the severe cognitive bias between two students of DS\nstructure, both of which potentially lead to reduced efficacy, or even model\ncollapse eventually. To mitigate these issues, a Decoupled Competitive\nFramework (DCF) is elaborated in this work, which utilizes a straightforward\ncompetition mechanism for the update of EMA, effectively decoupling students\nand teachers in a dynamical manner. In addition, the seamless exchange of\ninvaluable and precise insights is facilitated among students, guaranteeing a\nbetter learning paradigm. The DCF introduced undergoes rigorous validation on\nthree publicly accessible datasets, which encompass both 2D and 3D datasets.\nThe results demonstrate the superiority of our method over previous\ncutting-edge competitors. Code will be available at\nhttps://github.com/JiaheChen2002/DCF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7ade\u4e89\u6846\u67b6\uff08DCF\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2dMean Teacher\u548cDual Students\u7ed3\u6784\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u6807\u6ce8\u6837\u672c\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982MT\u548cDS\uff09\u56e0\u8fc7\u5ea6\u8026\u5408\u548c\u8ba4\u77e5\u504f\u5dee\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\uff0c\u9700\u6539\u8fdb\u3002", "method": "DCF\u901a\u8fc7\u7ade\u4e89\u673a\u5236\u52a8\u6001\u89e3\u8026\u5e08\u751f\u6a21\u578b\uff0c\u5e76\u4fc3\u8fdb\u5b66\u751f\u95f4\u7cbe\u786e\u77e5\u8bc6\u4ea4\u6362\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cDCF\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DCF\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2505.24347", "pdf": "https://arxiv.org/pdf/2505.24347", "abs": "https://arxiv.org/abs/2505.24347", "authors": ["Yangui Fang", "Baixu Cheng", "Jing Peng", "Xu Li", "Yu Xi", "Chengwei Zhang", "Guohui Zhong"], "title": "Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) error correction aims to correct\nrecognition errors while preserving accurate text. Although traditional\napproaches demonstrate moderate effectiveness, LLMs offer a paradigm that\neliminates the need for training and labeled data. However, directly using LLMs\nwill encounter hallucinations problem, which may lead to the modification of\nthe correct text. To address this problem, we propose the Reliable LLM\nCorrection Framework (RLLM-CF), which consists of three stages: (1) error\npre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)\nreasoning process verification. The advantage of our method is that it does not\nrequire additional information or fine-tuning of the model, and ensures the\ncorrectness of the LLM correction under multi-pass programming. Experiments on\nAISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by\nour framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684ASR\u9519\u8bef\u6821\u6b63\u6846\u67b6RLLM-CF\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u907f\u514d\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u4e86CER/WER\u3002", "motivation": "\u4f20\u7edfASR\u9519\u8bef\u6821\u6b63\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u800c\u76f4\u63a5\u4f7f\u7528LLM\u4f1a\u5f15\u5165\u5e7b\u89c9\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u6821\u6b63\u9519\u8bef\u4e14\u4e0d\u4fee\u6539\u6b63\u786e\u6587\u672c\u3002", "method": "RLLM-CF\u6846\u67b6\u5305\u62ec\u9519\u8bef\u9884\u68c0\u6d4b\u3001\u94fe\u5f0f\u601d\u7ef4\u5b50\u4efb\u52a1\u8fed\u4ee3\u6821\u6b63\u548c\u63a8\u7406\u8fc7\u7a0b\u9a8c\u8bc1\u4e09\u9636\u6bb5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\u3002", "result": "\u5728AISHELL-1\u3001AISHELL-2\u548cLibrispeech\u6570\u636e\u96c6\u4e0a\uff0cCER/WER\u76f8\u5bf9\u964d\u4f4e\u4e8621%\u300111%\u30019%\u548c11.4%\u3002", "conclusion": "RLLM-CF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728ASR\u9519\u8bef\u6821\u6b63\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6821\u6b63\u6548\u679c\u3002"}}
{"id": "2505.24669", "pdf": "https://arxiv.org/pdf/2505.24669", "abs": "https://arxiv.org/abs/2505.24669", "authors": ["Chengzhi Wu", "Hao Fu", "Jan-Philipp Kaiser", "Erik Tabuchi Barczak", "Julius Pfrommer", "Gisela Lanza", "Michael Heizmann", "J\u00fcrgen Beyerer"], "title": "6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly", "categories": ["cs.CV"], "comment": null, "summary": "The accurate estimation of 6D pose remains a challenging task within the\ncomputer vision domain, even when utilizing 3D point cloud data. Conversely, in\nthe manufacturing domain, instances arise where leveraging prior knowledge can\nyield advancements in this endeavor. This study focuses on the disassembly of\nstarter motors to augment the engineering of product life cycles. A pivotal\nobjective in this context involves the identification and 6D pose estimation of\nbolts affixed to the motors, facilitating automated disassembly within the\nmanufacturing workflow. Complicating matters, the presence of occlusions and\nthe limitations of single-view data acquisition, notably when motors are placed\nin a clamping system, obscure certain portions and render some bolts\nimperceptible. Consequently, the development of a comprehensive pipeline\ncapable of acquiring complete bolt information is imperative to avoid oversight\nin bolt detection. In this paper, employing the task of bolt detection within\nthe scope of our project as a pertinent use case, we introduce a meticulously\ndevised pipeline. This multi-stage pipeline effectively captures the 6D\ninformation with regard to all bolts on the motor, thereby showcasing the\neffective utilization of prior knowledge in handling this challenging task. The\nproposed methodology not only contributes to the field of 6D pose estimation\nbut also underscores the viability of integrating domain-specific insights to\ntackle complex problems in manufacturing and automation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u7ba1\u9053\uff0c\u7528\u4e8e\u5728\u5236\u9020\u9886\u57df\u4e2d\u51c6\u786e\u4f30\u8ba1\u87ba\u6813\u76846D\u59ff\u6001\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u548c\u5355\u89c6\u89d2\u6570\u636e\u9650\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u5236\u9020\u9886\u57df\u4e2d\uff0c\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u6539\u8fdb6D\u59ff\u6001\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u87ba\u6813\u7684\u81ea\u52a8\u62c6\u5378\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u7ba1\u9053\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\uff0c\u5168\u9762\u6355\u6349\u87ba\u6813\u76846D\u4fe1\u606f\u3002", "result": "\u7ba1\u9053\u6210\u529f\u83b7\u53d6\u4e86\u6240\u6709\u87ba\u6813\u76846D\u59ff\u6001\u4fe1\u606f\uff0c\u8bc1\u660e\u4e86\u5148\u9a8c\u77e5\u8bc6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63a8\u52a8\u4e866D\u59ff\u6001\u4f30\u8ba1\u9886\u57df\uff0c\u8fd8\u5c55\u793a\u4e86\u9886\u57df\u77e5\u8bc6\u5728\u5236\u9020\u81ea\u52a8\u5316\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.24354", "pdf": "https://arxiv.org/pdf/2505.24354", "abs": "https://arxiv.org/abs/2505.24354", "authors": ["Qianqian Zhang", "Jiajia Liao", "Heting Ying", "Yibo Ma", "Haozhan Shen", "Jingcheng Li", "Peng Liu", "Lu Zhang", "Chunxin Fang", "Kyusong Lee", "Ruochen Xu", "Tiancheng Zhao"], "title": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Demo", "summary": "Language agents powered by large language models (LLMs) have demonstrated\nremarkable capabilities in understanding, reasoning, and executing complex\ntasks. However, developing robust agents presents significant challenges:\nsubstantial engineering overhead, lack of standardized components, and\ninsufficient evaluation frameworks for fair comparison. We introduce Agent\nGraph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and\nextensible framework that addresses these challenges through three key\ncontributions: (1) a modular architecture with a graph-based workflow engine,\nefficient memory management, and clean component abstraction; (2) a\ncomprehensive suite of reusable agent algorithms implementing state-of-the-art\nreasoning approaches; and (3) a rigorous evaluation framework enabling\nsystematic comparison across multiple dimensions. Through extensive experiments\non mathematical reasoning and multimodal tasks, we evaluate various agent\nalgorithms across different LLMs, revealing important insights about their\nrelative strengths and applicability. Our results demonstrate that while\nsophisticated reasoning approaches can enhance agent capabilities, simpler\nmethods like Chain-of-Thought often exhibit robust performance with\nsignificantly lower computational overhead. AGORA not only simplifies language\nagent development but also establishes a foundation for reproducible agent\nresearch through standardized evaluation protocols.", "AI": {"tldr": "AGORA\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u7075\u6d3b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u8a00\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u5de5\u7a0b\u5316\u3001\u6807\u51c6\u5316\u548c\u8bc4\u4f30\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u3001\u53ef\u91cd\u7528\u7b97\u6cd5\u548c\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\u63d0\u5347\u4ee3\u7406\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u5f3a\u5927\u7684\u8bed\u8a00\u4ee3\u7406\u9762\u4e34\u5de5\u7a0b\u5316\u590d\u6742\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u7ec4\u4ef6\u548c\u8bc4\u4f30\u6846\u67b6\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "AGORA\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff08\u56fe\u5de5\u4f5c\u6d41\u5f15\u64ce\u3001\u5185\u5b58\u7ba1\u7406\u3001\u7ec4\u4ef6\u62bd\u8c61\uff09\u3001\u63d0\u4f9b\u53ef\u91cd\u7528\u7b97\u6cd5\u5957\u4ef6\uff0c\u5e76\u5efa\u7acb\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u590d\u6742\u63a8\u7406\u65b9\u6cd5\u80fd\u63d0\u5347\u4ee3\u7406\u80fd\u529b\uff0c\u4f46\u7b80\u5355\u65b9\u6cd5\uff08\u5982Chain-of-Thought\uff09\u5728\u8ba1\u7b97\u5f00\u9500\u8f83\u4f4e\u65f6\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "AGORA\u7b80\u5316\u4e86\u8bed\u8a00\u4ee3\u7406\u5f00\u53d1\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u4e3a\u53ef\u590d\u73b0\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.24679", "pdf": "https://arxiv.org/pdf/2505.24679", "abs": "https://arxiv.org/abs/2505.24679", "authors": ["Evangelos Sariyanidi", "Lisa Yankowitz", "Robert T. Schultz", "John D. Herrington", "Birkan Tunc", "Jeffrey Cohn"], "title": "Beyond FACS: Data-driven Facial Expression Dictionaries, with Application to Predicting Autism", "categories": ["cs.CV"], "comment": "To appear in the Proceedings of the 19th IEEE International\n  Conference on Automatic Face and Gesture Recognition (2025)", "summary": "The Facial Action Coding System (FACS) has been used by numerous studies to\ninvestigate the links between facial behavior and mental health. The laborious\nand costly process of FACS coding has motivated the development of machine\nlearning frameworks for Action Unit (AU) detection. Despite intense efforts\nspanning three decades, the detection accuracy for many AUs is considered to be\nbelow the threshold needed for behavioral research. Also, many AUs are excluded\naltogether, making it impossible to fulfill the ultimate goal of FACS-the\nrepresentation of any facial expression in its entirety. This paper considers\nan alternative approach. Instead of creating automated tools that mimic FACS\nexperts, we propose to use a new coding system that mimics the key properties\nof FACS. Specifically, we construct a data-driven coding system called the\nFacial Basis, which contains units that correspond to localized and\ninterpretable 3D facial movements, and overcomes three structural limitations\nof automated FACS coding. First, the proposed method is completely\nunsupervised, bypassing costly, laborious and variable manual annotation.\nSecond, Facial Basis reconstructs all observable movement, rather than relying\non a limited repertoire of recognizable movements (as in automated FACS).\nFinally, the Facial Basis units are additive, whereas AUs may fail detection\nwhen they appear in a non-additive combination. The proposed method outperforms\nthe most frequently used AU detector in predicting autism diagnosis from\nin-person and remote conversations, highlighting the importance of encoding\nfacial behavior comprehensively. To our knowledge, Facial Basis is the first\nalternative to FACS for deconstructing facial expressions in videos into\nlocalized movements. We provide an open source implementation of the method at\ngithub.com/sariyanidi/FacialBasis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFacial Basis\u7684\u65b0\u7f16\u7801\u7cfb\u7edf\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684FACS\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u89e3\u51b3FACS\u5728\u81ea\u52a8\u68c0\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u81ea\u95ed\u75c7\u8bca\u65ad\u4e2d\u8868\u73b0\u4f18\u4e8e\u5e38\u7528AU\u68c0\u6d4b\u5668\u3002", "motivation": "FACS\u7f16\u7801\u8fc7\u7a0b\u7e41\u7410\u4e14\u6210\u672c\u9ad8\uff0c\u4e14\u81ea\u52a8\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u9762\u90e8\u8868\u60c5\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7f16\u7801\u7cfb\u7edf\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faFacial Basis\uff0c\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65e0\u76d1\u7763\u7f16\u7801\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5c40\u90e8\u5316\u548c\u53ef\u89e3\u91ca\u76843D\u9762\u90e8\u8fd0\u52a8\u5355\u5143\uff0c\u907f\u514d\u4e86\u624b\u52a8\u6807\u6ce8\u7684\u5c40\u9650\u6027\u3002", "result": "Facial Basis\u5728\u9884\u6d4b\u81ea\u95ed\u75c7\u8bca\u65ad\u4e2d\u4f18\u4e8e\u5e38\u7528AU\u68c0\u6d4b\u5668\uff0c\u5e76\u80fd\u5168\u9762\u6355\u6349\u6240\u6709\u53ef\u89c2\u5bdf\u7684\u9762\u90e8\u8fd0\u52a8\u3002", "conclusion": "Facial Basis\u662f\u9996\u4e2a\u66ff\u4ee3FACS\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5168\u9762\u7f16\u7801\u9762\u90e8\u884c\u4e3a\uff0c\u4e3a\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.24355", "pdf": "https://arxiv.org/pdf/2505.24355", "abs": "https://arxiv.org/abs/2505.24355", "authors": ["Sihan Tan", "Taro Miyazaki", "Kazuhiro Nakadai"], "title": "Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model", "categories": ["cs.CL"], "comment": null, "summary": "Sign Language Translation (SLT) aims to convert sign language (SL) videos\ninto spoken language text, thereby bridging the communication gap between the\nsign and the spoken community. While most existing works focus on translating a\nsingle sign language into a single spoken language (one-to-one SLT), leveraging\nmultilingual resources could mitigate low-resource issues and enhance\naccessibility. However, multilingual SLT (MLSLT) remains unexplored due to\nlanguage conflicts and alignment difficulties across SLs and spoken languages.\nTo address these challenges, we propose a multilingual gloss-free model with\ndual CTC objectives for token-level SL identification and spoken text\ngeneration. Our model supports 10 SLs and handles one-to-one, many-to-one, and\nmany-to-many SLT tasks, achieving competitive performance compared to\nstate-of-the-art methods on three widely adopted benchmarks: multilingual\nSP-10, PHOENIX14T, and CSL-Daily.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u624b\u8bed\u7ffb\u8bd1\uff08MLSLT\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u53ccCTC\u76ee\u6807\u89e3\u51b3\u8bed\u8a00\u51b2\u7a81\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u652f\u630110\u79cd\u624b\u8bed\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u624b\u8bed\u5230\u5355\u4e00\u53e3\u8bed\u7684\u7ffb\u8bd1\uff08\u4e00\u5bf9\u4e00SLT\uff09\uff0c\u800c\u591a\u8bed\u8a00\u8d44\u6e90\u53ef\u4ee5\u7f13\u89e3\u8d44\u6e90\u4e0d\u8db3\u95ee\u9898\u5e76\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\u3002\u7136\u800c\uff0c\u591a\u8bed\u8a00SLT\u56e0\u8bed\u8a00\u51b2\u7a81\u548c\u5bf9\u9f50\u56f0\u96be\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ccCTC\u76ee\u6807\u7684\u591a\u8bed\u8a00\u65e0\u6ce8\u91ca\u6a21\u578b\uff0c\u7528\u4e8e\u6807\u8bb0\u7ea7\u624b\u8bed\u8bc6\u522b\u548c\u53e3\u8bed\u6587\u672c\u751f\u6210\u3002", "result": "\u6a21\u578b\u652f\u630110\u79cd\u624b\u8bed\uff0c\u5904\u7406\u4e00\u5bf9\u4e00\u3001\u591a\u5bf9\u4e00\u548c\u591a\u5bf9\u591aSLT\u4efb\u52a1\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u591a\u8bed\u8a00\u624b\u8bed\u7ffb\u8bd1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e3a\u89e3\u51b3\u8bed\u8a00\u51b2\u7a81\u548c\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2505.24690", "pdf": "https://arxiv.org/pdf/2505.24690", "abs": "https://arxiv.org/abs/2505.24690", "authors": ["Simone Alberto Peirone", "Francesca Pistilli", "Antonio Alliegro", "Tatiana Tommasi", "Giuseppe Averta"], "title": "Learning reusable concepts across different egocentric video understanding tasks", "categories": ["cs.CV"], "comment": "Extended abstract derived from arXiv:2502.02487. Presented at the\n  Second Joint Egocentric Vision (EgoVis) Workshop (CVPR 2025)", "summary": "Our comprehension of video streams depicting human activities is naturally\nmultifaceted: in just a few moments, we can grasp what is happening, identify\nthe relevance and interactions of objects in the scene, and forecast what will\nhappen soon, everything all at once. To endow autonomous systems with such\nholistic perception, learning how to correlate concepts, abstract knowledge\nacross diverse tasks, and leverage tasks synergies when learning novel skills\nis essential. In this paper, we introduce Hier-EgoPack, a unified framework\nable to create a collection of task perspectives that can be carried across\ndownstream tasks and used as a potential source of additional insights, as a\nbackpack of skills that a robot can carry around and use when needed.", "AI": {"tldr": "Hier-EgoPack\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u4efb\u52a1\u89c6\u89d2\u63d0\u5347\u81ea\u4e3b\u7cfb\u7edf\u7684\u6574\u4f53\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u8d4b\u4e88\u81ea\u4e3b\u7cfb\u7edf\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5168\u9762\u611f\u77e5\u80fd\u529b\uff0c\u5305\u62ec\u7406\u89e3\u573a\u666f\u3001\u5bf9\u8c61\u4ea4\u4e92\u548c\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u3002", "method": "\u63d0\u51faHier-EgoPack\u6846\u67b6\uff0c\u6784\u5efa\u4efb\u52a1\u89c6\u89d2\u96c6\u5408\uff0c\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u7684\u8de8\u4efb\u52a1\u77e5\u8bc6\u5171\u4eab\u548c\u6280\u80fd\u590d\u7528\u3002", "result": "\u6846\u67b6\u80fd\u591f\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u643a\u5e26\u7684\u6280\u80fd\u80cc\u5305\uff0c\u589e\u5f3a\u5176\u591a\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "conclusion": "Hier-EgoPack\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u7efc\u5408\u611f\u77e5\u548c\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24362", "pdf": "https://arxiv.org/pdf/2505.24362", "abs": "https://arxiv.org/abs/2505.24362", "authors": ["Anum Afzal", "Florian Matthes", "Gal Chechik", "Yftah Ziser"], "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion", "categories": ["cs.CL"], "comment": null, "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.\\footnote{Code and\ndata is available at\n\\href{https://github.com/anum94/CoTpred}{\\texttt{github.com/anum94/CoTpred}}.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8eLLM\u8868\u793a\u7684\u63a2\u6d4b\u5206\u7c7b\u5668\u80fd\u5728\u96f6\u6837\u672cCoT\u63a8\u7406\u8fc7\u7a0b\u5f00\u59cb\u524d\u9884\u6d4b\u5176\u6210\u529f\uff0c\u800cBERT\u57fa\u7ebf\u8868\u73b0\u8f83\u5dee\u3002\u65e9\u671f\u8868\u793a\u5df2\u5305\u542b\u5173\u952e\u4fe1\u606f\uff0c\u540e\u671f\u63a8\u7406\u6b65\u9aa4\u672a\u5fc5\u603b\u662f\u6709\u5e2e\u52a9\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u524d\u622a\u65adCoT\u63a8\u7406\u4ecd\u4f18\u4e8e\u5b8c\u5168\u4e0d\u4f7f\u7528CoT\uff0c\u4f46\u6548\u679c\u4e0d\u53ca\u5b8c\u6574\u63a8\u7406\u3002", "motivation": "\u63a2\u7a76\u96f6\u6837\u672cCoT\u63a8\u7406\u7684\u6210\u529f\u662f\u5426\u80fd\u5728\u63a8\u7406\u5b8c\u6210\u524d\u88ab\u9884\u6d4b\uff0c\u4ee5\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u8868\u793a\u7684\u63a2\u6d4b\u5206\u7c7b\u5668\u548cBERT\u57fa\u7ebf\u6a21\u578b\uff0c\u6bd4\u8f83\u5b83\u4eec\u5728\u63a8\u7406\u4e0d\u540c\u9636\u6bb5\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u65e9\u671f\u505c\u6b62\u5b9e\u9a8c\u3002", "result": "LLM\u5206\u7c7b\u5668\u5728\u63a8\u7406\u5f00\u59cb\u524d\u5373\u8868\u73b0\u826f\u597d\uff0cBERT\u57fa\u7ebf\u4f9d\u8d56\u6d45\u5c42\u8bed\u8a00\u7ebf\u7d22\u8868\u73b0\u8f83\u5dee\u3002\u65e9\u671f\u8868\u793a\u5df2\u5305\u542b\u5173\u952e\u4fe1\u606f\uff0c\u540e\u671f\u6b65\u9aa4\u672a\u5fc5\u6709\u5e2e\u52a9\u3002\u63d0\u524d\u622a\u65adCoT\u63a8\u7406\u4ecd\u4f18\u4e8e\u65e0CoT\u3002", "conclusion": "LLM\u65e9\u671f\u8868\u793a\u5df2\u5305\u542b\u63a8\u7406\u5173\u952e\u4fe1\u606f\uff0c\u652f\u6301\u4f18\u5316CoT\u6548\u7387\u7684\u65b9\u6cd5\uff08\u5982\u65e9\u671f\u505c\u6b62\uff09\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4f18\u52bf\u3002"}}
{"id": "2505.24693", "pdf": "https://arxiv.org/pdf/2505.24693", "abs": "https://arxiv.org/abs/2505.24693", "authors": ["Julio Silva-Rodr\u00edguez", "Ismail Ben Ayed", "Jose Dolz"], "title": "Conformal Prediction for Zero-Shot Models", "categories": ["cs.CV"], "comment": "CVPR 2025. Code: https://github.com/jusiro/CLIP-Conformal", "summary": "Vision-language models pre-trained at large scale have shown unprecedented\nadaptability and generalization to downstream tasks. Although its\ndiscriminative potential has been widely explored, its reliability and\nuncertainty are still overlooked. In this work, we investigate the capabilities\nof CLIP models under the split conformal prediction paradigm, which provides\ntheoretical guarantees to black-box models based on a small, labeled\ncalibration set. In contrast to the main body of literature on conformal\npredictors in vision classifiers, foundation models exhibit a particular\ncharacteristic: they are pre-trained on a one-time basis on an inaccessible\nsource domain, different from the transferred task. This domain drift\nnegatively affects the efficiency of the conformal sets and poses additional\nchallenges. To alleviate this issue, we propose Conf-OT, a transfer learning\nsetting that operates transductive over the combined calibration and query\nsets. Solving an optimal transport problem, the proposed method bridges the\ndomain gap between pre-training and adaptation without requiring additional\ndata splits but still maintaining coverage guarantees. We comprehensively\nexplore this conformal prediction strategy on a broad span of 15 datasets and\nthree non-conformity scores. Conf-OT provides consistent relative improvements\nof up to 20% on set efficiency while being 15 times faster than popular\ntransductive approaches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86CLIP\u6a21\u578b\u5728\u5206\u5272\u5171\u5f62\u9884\u6d4b\u8303\u5f0f\u4e0b\u7684\u80fd\u529b\uff0c\u63d0\u51faConf-OT\u65b9\u6cd5\u4ee5\u7f13\u89e3\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96c6\u5408\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u53ef\u9760\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4ecd\u88ab\u5ffd\u89c6\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22CLIP\u6a21\u578b\u5728\u5171\u5f62\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faConf-OT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u89e3\u51b3\u9884\u8bad\u7ec3\u4e0e\u9002\u5e94\u4efb\u52a1\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u5206\u5272\u3002", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u79cd\u975e\u5171\u5f62\u5206\u6570\u4e0a\u9a8c\u8bc1\uff0cConf-OT\u76f8\u5bf9\u63d0\u534720%\u7684\u96c6\u5408\u6548\u7387\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb15\u500d\u3002", "conclusion": "Conf-OT\u6709\u6548\u89e3\u51b3\u4e86\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5171\u5f62\u9884\u6d4b\u7684\u6548\u7387\uff0c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24377", "pdf": "https://arxiv.org/pdf/2505.24377", "abs": "https://arxiv.org/abs/2505.24377", "authors": ["Yu-Hsuan Lin", "Qian-Hui Chen", "Yi-Jie Cheng", "Jia-Ren Zhang", "Yi-Hung Liu", "Liang-Yu Hsia", "Yun-Nung Chen"], "title": "LLM Inference Enhanced by External Knowledge: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have enhanced\nnatural-language reasoning. However, their limited parametric memory and\nsusceptibility to hallucination present persistent challenges for tasks\nrequiring accurate, context-based inference. To overcome these limitations, an\nincreasing number of studies have proposed leveraging external knowledge to\nenhance LLMs. This study offers a systematic exploration of strategies for\nusing external knowledge to enhance LLMs, beginning with a taxonomy that\ncategorizes external knowledge into unstructured and structured data. We then\nfocus on structured knowledge, presenting distinct taxonomies for tables and\nknowledge graphs (KGs), detailing their integration paradigms with LLMs, and\nreviewing representative methods. Our comparative analysis further highlights\nthe trade-offs among interpretability, scalability, and performance, providing\ninsights for developing trustworthy and generalizable knowledge-enhanced LLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7b56\u7565\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5982\u8868\u683c\u548c\u77e5\u8bc6\u56fe\u8c31\uff09\u7684\u6574\u5408\u65b9\u6cd5\u53ca\u5176\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u6709\u9650\u7684\u53c2\u6570\u8bb0\u5fc6\u548c\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u95ee\u9898\u4ecd\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u51c6\u786e\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5c06\u5916\u90e8\u77e5\u8bc6\u5206\u4e3a\u975e\u7ed3\u6784\u5316\u548c\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u8868\u683c\u548c\u77e5\u8bc6\u56fe\u8c31\uff09\u4e0eLLMs\u7684\u6574\u5408\u8303\u5f0f\u53ca\u4ee3\u8868\u6027\u65b9\u6cd5\u3002", "result": "\u6bd4\u8f83\u5206\u6790\u63ed\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u5f00\u53d1\u53ef\u4fe1\u4e14\u901a\u7528\u7684\u77e5\u8bc6\u589e\u5f3aLLMs\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5982\u4f55\u6709\u6548\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u589e\u5f3aLLMs\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2505.24705", "pdf": "https://arxiv.org/pdf/2505.24705", "abs": "https://arxiv.org/abs/2505.24705", "authors": ["Raman Jha", "Adithya Lenka", "Mani Ramanagopal", "Aswin Sankaranarayanan", "Kaushik Mitra"], "title": "RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted at ICIP 2025", "summary": "In nighttime conditions, high noise levels and bright illumination sources\ndegrade image quality, making low-light image enhancement challenging. Thermal\nimages provide complementary information, offering richer textures and\nstructural details. We propose RT-X Net, a cross-attention network that fuses\nRGB and thermal images for nighttime image enhancement. We leverage\nself-attention networks for feature extraction and a cross-attention mechanism\nfor fusion to effectively integrate information from both modalities. To\nsupport research in this domain, we introduce the Visible-Thermal Image\nEnhancement Evaluation (V-TIEE) dataset, comprising 50 co-located visible and\nthermal images captured under diverse nighttime conditions. Extensive\nevaluations on the publicly available LLVIP dataset and our V-TIEE dataset\ndemonstrate that RT-X Net outperforms state-of-the-art methods in low-light\nimage enhancement. The code and the V-TIEE can be found here\nhttps://github.com/jhakrraman/rt-xnet.", "AI": {"tldr": "RT-X Net\u662f\u4e00\u79cd\u8de8\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u878d\u5408RGB\u548c\u70ed\u6210\u50cf\u56fe\u50cf\u4ee5\u63d0\u5347\u591c\u95f4\u56fe\u50cf\u8d28\u91cf\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u4e24\u79cd\u6a21\u6001\u4fe1\u606f\u3002", "motivation": "\u591c\u95f4\u56fe\u50cf\u8d28\u91cf\u56e0\u9ad8\u566a\u58f0\u548c\u5f3a\u5149\u6e90\u800c\u4e0b\u964d\uff0c\u70ed\u6210\u50cf\u56fe\u50cf\u80fd\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u3002", "method": "\u63d0\u51faRT-X Net\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u878d\u5408RGB\u4e0e\u70ed\u6210\u50cf\u56fe\u50cf\uff1b\u5e76\u53d1\u5e03V-TIEE\u6570\u636e\u96c6\u652f\u6301\u7814\u7a76\u3002", "result": "\u5728LLVIP\u548cV-TIEE\u6570\u636e\u96c6\u4e0a\uff0cRT-X Net\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u3002", "conclusion": "RT-X Net\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u591c\u95f4\u56fe\u50cf\u589e\u5f3a\u6548\u679c\uff0cV-TIEE\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2505.24388", "pdf": "https://arxiv.org/pdf/2505.24388", "abs": "https://arxiv.org/abs/2505.24388", "authors": ["Hao Chen", "Yukun Yan", "Sen Mei", "Wanxiang Che", "Zhenghao Liu", "Qi Shi", "Xinze Li", "Yuchun Fan", "Pengcheng Huang", "Qiushi Xiong", "Zhiyuan Liu", "Maosong Sun"], "title": "ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)\nwith external knowledge to improve factuality. However, existing RAG systems\nfrequently underutilize the retrieved documents, failing to extract and\nintegrate the key clues needed to support faithful and interpretable reasoning,\nespecially in cases where relevant evidence is implicit, scattered, or obscured\nby noise. To address this issue, we propose ClueAnchor, a novel framework for\nenhancing RAG via clue-anchored reasoning exploration and optimization.\nClueAnchor extracts key clues from retrieved content and generates multiple\nreasoning paths based on different knowledge configurations, optimizing the\nmodel by selecting the most effective one through reward-based preference\noptimization. Experiments show that ClueAnchor significantly outperforms prior\nRAG baselines in reasoning completeness and robustness. Further analysis\nconfirms its strong resilience to noisy or partially relevant retrieved\ncontent, as well as its capability to identify supporting evidence even in the\nabsence of explicit clue supervision during inference.", "AI": {"tldr": "ClueAnchor\u901a\u8fc7\u63d0\u53d6\u5173\u952e\u7ebf\u7d22\u5e76\u4f18\u5316\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u63a8\u7406\u5b8c\u6574\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u5229\u7528\u68c0\u7d22\u5230\u7684\u6587\u6863\uff0c\u5c24\u5176\u662f\u5728\u8bc1\u636e\u9690\u5f0f\u3001\u5206\u6563\u6216\u53d7\u566a\u58f0\u5e72\u6270\u65f6\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faClueAnchor\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u5173\u952e\u7ebf\u7d22\u3001\u751f\u6210\u591a\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u57fa\u4e8e\u5956\u52b1\u4f18\u5316\u9009\u62e9\u6700\u4f73\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8868\u660eClueAnchor\u5728\u63a8\u7406\u5b8c\u6574\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709RAG\u57fa\u7ebf\uff0c\u4e14\u5bf9\u566a\u58f0\u548c\u90e8\u5206\u76f8\u5173\u5185\u5bb9\u5177\u6709\u5f3a\u97e7\u6027\u3002", "conclusion": "ClueAnchor\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u5373\u53ef\u8bc6\u522b\u652f\u6301\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2505.24718", "pdf": "https://arxiv.org/pdf/2505.24718", "abs": "https://arxiv.org/abs/2505.24718", "authors": ["Jisheng Dang", "Jingze Wu", "Teng Wang", "Xuanhui Lin", "Nannan Zhu", "Hongbo Chen", "Wei-Shi Zheng", "Meng Wang", "Tat-Seng Chua"], "title": "Reinforcing Video Reasoning with Focused Thinking", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reinforcement learning, particularly through Group\nRelative Policy Optimization (GRPO), have significantly improved multimodal\nlarge language models for complex reasoning tasks. However, two critical\nlimitations persist: 1) they often produce unfocused, verbose reasoning chains\nthat obscure salient spatiotemporal cues and 2) binary rewarding fails to\naccount for partially correct answers, resulting in high reward variance and\ninefficient learning. In this paper, we propose TW-GRPO, a novel framework that\nenhances visual reasoning with focused thinking and dense reward granularity.\nSpecifically, we employs a token weighting mechanism that prioritizes tokens\nwith high informational density (estimated by intra-group variance),\nsuppressing redundant tokens like generic reasoning prefixes. Furthermore, we\nreformulate RL training by shifting from single-choice to multi-choice QA\ntasks, where soft rewards enable finer-grained gradient estimation by\ndistinguishing partial correctness. Additionally, we propose question-answer\ninversion, a data augmentation strategy to generate diverse multi-choice\nsamples from existing benchmarks. Experiments demonstrate state-of-the-art\nperformance on several video reasoning and general understanding benchmarks.\nNotably, TW-GRPO achieves 50.4\\% accuracy on CLEVRER (18.8\\% improvement over\nVideo-R1) and 65.8\\% on MMVU. Our codes are available at\n\\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}.", "AI": {"tldr": "TW-GRPO\u901a\u8fc7\u5f15\u5165\u4ee4\u724c\u52a0\u6743\u673a\u5236\u548c\u5bc6\u96c6\u5956\u52b1\u7c92\u5ea6\uff0c\u6539\u8fdb\u4e86GRPO\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u94fe\u5197\u957f\u548c\u4e8c\u5143\u5956\u52b1\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0cTW-GRPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4ee4\u724c\u52a0\u6743\u673a\u5236\u4f18\u5316\u4fe1\u606f\u5bc6\u5ea6\u9ad8\u7684\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7\u591a\u9009\u62e9QA\u4efb\u52a1\u548c\u8f6f\u5956\u52b1\u6539\u8fdbRL\u8bad\u7ec3\u3002", "result": "\u5728CLEVRER\u548cMMVU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "TW-GRPO\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24409", "pdf": "https://arxiv.org/pdf/2505.24409", "abs": "https://arxiv.org/abs/2505.24409", "authors": ["Eojin Kang", "Juae Kim"], "title": "LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) open up new possibilities for\nleveraging information across languages, but their factual knowledge recall\nremains inconsistent depending on the input language. While previous studies\nhave attempted to address this issue through English-based prompting and\nevaluation, we explore non-English to English transfer via Language and Thought\nTheory. This perspective allows us to examine language-thought binding in LLMs\nand uncover why factual knowledge often fails to transfer effectively. We\npropose the Language-to-Thought (L2T) prompting strategy, which analyzes the\nrelationship between input language, internal cognitive processes, and\nknowledge. Experimental results challenge the assumption that English-based\napproaches consistently outperform other languages and offer a novel insight\nthat aligning the model's internal thought with the knowledge required for the\ntask is critical for successful cross-lingual transfer. Furthermore, we show\nthat applying L2T during training can alleviate LLMs' reliance on the input\nlanguage and facilitate cross-linguistic knowledge integration without\ntranslation-based learning. Code and datasets will be available.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8de8\u8bed\u8a00\u77e5\u8bc6\u4f20\u9012\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u4e0e\u601d\u7ef4\u7406\u8bba\u7684L2T\u63d0\u793a\u7b56\u7565\uff0c\u4ee5\u6539\u5584\u77e5\u8bc6\u4f20\u9012\u6548\u679c\u3002", "motivation": "\u591a\u8bed\u8a00LLMs\u5728\u4e0d\u540c\u8f93\u5165\u8bed\u8a00\u4e0b\u7684\u77e5\u8bc6\u56de\u5fc6\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u8bed\u8a00\u7684\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u63d0\u51faL2T\u63d0\u793a\u7b56\u7565\uff0c\u5206\u6790\u8f93\u5165\u8bed\u8a00\u3001\u5185\u90e8\u8ba4\u77e5\u8fc7\u7a0b\u4e0e\u77e5\u8bc6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u82f1\u8bed\u65b9\u6cd5\u5e76\u975e\u603b\u662f\u6700\u4f18\uff0c\u6a21\u578b\u5185\u90e8\u601d\u7ef4\u4e0e\u4efb\u52a1\u77e5\u8bc6\u7684\u5bf9\u9f50\u5bf9\u8de8\u8bed\u8a00\u4f20\u9012\u81f3\u5173\u91cd\u8981\u3002L2T\u8bad\u7ec3\u53ef\u51cf\u5c11\u5bf9\u8f93\u5165\u8bed\u8a00\u7684\u4f9d\u8d56\u3002", "conclusion": "L2T\u7b56\u7565\u4e3a\u8de8\u8bed\u8a00\u77e5\u8bc6\u4f20\u9012\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5f3a\u8c03\u5185\u90e8\u601d\u7ef4\u4e0e\u4efb\u52a1\u77e5\u8bc6\u7684\u5bf9\u9f50\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u7ffb\u8bd1\u5b66\u4e60\u3002"}}
{"id": "2505.24733", "pdf": "https://arxiv.org/pdf/2505.24733", "abs": "https://arxiv.org/abs/2505.24733", "authors": ["Jiaxu Zhang", "Xianfang Zeng", "Xin Chen", "Wei Zuo", "Gang Yu", "Guosheng Lin", "Zhigang Tu"], "title": "DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents DreamDance, a novel character art animation framework\ncapable of producing stable, consistent character and scene motion conditioned\non precise camera trajectories. To achieve this, we re-formulate the animation\ntask as two inpainting-based steps: Camera-aware Scene Inpainting and\nPose-aware Video Inpainting. The first step leverages a pre-trained image\ninpainting model to generate multi-view scene images from the reference art and\noptimizes a stable large-scale Gaussian field, which enables coarse background\nvideo rendering with camera trajectories. However, the rendered video is rough\nand only conveys scene motion. To resolve this, the second step trains a\npose-aware video inpainting model that injects the dynamic character into the\nscene video while enhancing background quality. Specifically, this model is a\nDiT-based video generation model with a gating strategy that adaptively\nintegrates the character's appearance and pose information into the base\nbackground video. Through extensive experiments, we demonstrate the\neffectiveness and generalizability of DreamDance, producing high-quality and\nconsistent character animations with remarkable camera dynamics.", "AI": {"tldr": "DreamDance\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89d2\u8272\u827a\u672f\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u673a\u8f68\u8ff9\u751f\u6210\u7a33\u5b9a\u7684\u89d2\u8272\u548c\u573a\u666f\u8fd0\u52a8\u3002\u65b9\u6cd5\u5305\u62ec\u76f8\u673a\u611f\u77e5\u573a\u666f\u4fee\u590d\u548c\u59ff\u6001\u611f\u77e5\u89c6\u9891\u4fee\u590d\u4e24\u6b65\uff0c\u6700\u7ec8\u5b9e\u73b0\u9ad8\u8d28\u91cf\u52a8\u6001\u89d2\u8272\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u52a8\u753b\u6846\u67b6\u96be\u4ee5\u5728\u7cbe\u786e\u76f8\u673a\u8f68\u8ff9\u4e0b\u751f\u6210\u7a33\u5b9a\u4e14\u4e00\u81f4\u7684\u89d2\u8272\u548c\u573a\u666f\u8fd0\u52a8\uff0cDreamDance\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u76f8\u673a\u611f\u77e5\u573a\u666f\u4fee\u590d\uff1a\u5229\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u573a\u666f\u56fe\u50cf\uff0c\u4f18\u5316\u9ad8\u65af\u573a\u4ee5\u6e32\u67d3\u7c97\u7cd9\u80cc\u666f\u89c6\u9891\u30022. \u59ff\u6001\u611f\u77e5\u89c6\u9891\u4fee\u590d\uff1a\u8bad\u7ec3\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u52a8\u6001\u6ce8\u5165\u89d2\u8272\u5e76\u63d0\u5347\u80cc\u666f\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDreamDance\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e00\u81f4\u7684\u89d2\u8272\u52a8\u753b\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684\u76f8\u673a\u52a8\u6001\u6548\u679c\u3002", "conclusion": "DreamDance\u901a\u8fc7\u4e24\u6b65\u4fee\u590d\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u52a8\u6001\u7684\u89d2\u8272\u52a8\u753b\u751f\u6210\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.24423", "pdf": "https://arxiv.org/pdf/2505.24423", "abs": "https://arxiv.org/abs/2505.24423", "authors": ["Zhiwei Liu", "Lingfei Qian", "Qianqian Xie", "Jimin Huang", "Kailai Yang", "Sophia Ananiadou"], "title": "MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models and vision-language models (which we jointly call LMs)\nhave transformed NLP and CV, demonstrating remarkable potential across various\nfields. However, their capabilities in affective analysis (i.e. sentiment\nanalysis and emotion detection) remain underexplored. This gap is largely due\nto the absence of comprehensive evaluation benchmarks, and the inherent\ncomplexity of affective analysis tasks. In this paper, we introduce MMAFFBen,\nthe first extensive open-source benchmark for multilingual multimodal affective\nanalysis. MMAFFBen encompasses text, image, and video modalities across 35\nlanguages, covering four key affective analysis tasks: sentiment polarity,\nsentiment intensity, emotion classification, and emotion intensity. Moreover,\nwe construct the MMAFFIn dataset for fine-tuning LMs on affective analysis\ntasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate\nvarious representative LMs, including GPT-4o-mini, providing a systematic\ncomparison of their affective understanding capabilities. This project is\navailable at https://github.com/lzw108/MMAFFBen.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MMAFFBen\uff0c\u9996\u4e2a\u591a\u8bed\u8a00\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u5f00\u6e90\u57fa\u51c6\uff0c\u6db5\u76d635\u79cd\u8bed\u8a00\u7684\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\uff0c\u5e76\u5f00\u53d1\u4e86MMAFFLM\u6a21\u578b\u8fdb\u884c\u8bc4\u6d4b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u9886\u57df\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efaMMAFFBen\u57fa\u51c6\u548cMMAFFIn\u6570\u636e\u96c6\uff0c\u5f00\u53d1MMAFFLM-3b\u548cMMAFFLM-7b\u6a21\u578b\uff0c\u8bc4\u6d4b\u5305\u62ecGPT-4o-mini\u5728\u5185\u7684\u591a\u79cd\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u591a\u8bed\u8a00\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4efb\u52a1\u7684\u7cfb\u7edf\u8bc4\u6d4b\u7ed3\u679c\u3002", "conclusion": "MMAFFBen\u586b\u8865\u4e86\u60c5\u611f\u5206\u6790\u9886\u57df\u7684\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2505.24746", "pdf": "https://arxiv.org/pdf/2505.24746", "abs": "https://arxiv.org/abs/2505.24746", "authors": ["Jiazhong Cen", "Xudong Zhou", "Jiemin Fang", "Changsong Wen", "Lingxi Xie", "Xiaopeng Zhang", "Wei Shen", "Qi Tian"], "title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting", "categories": ["cs.CV"], "comment": "ICML 2025 camera ready. Project Page:\n  https://jumpat.github.io/laga-page/", "summary": "Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D\nscene reconstruction from RGB images. Many studies extend this paradigm for\nlanguage-driven open-vocabulary scene understanding. However, most of them\nsimply project 2D semantic features onto 3D Gaussians and overlook a\nfundamental gap between 2D and 3D understanding: a 3D object may exhibit\nvarious semantics from different viewpoints--a phenomenon we term\nview-dependent semantics. To address this challenge, we propose LaGa (Language\nGaussians), which establishes cross-view semantic connections by decomposing\nthe 3D scene into objects. Then, it constructs view-aggregated semantic\nrepresentations by clustering semantic descriptors and reweighting them based\non multi-view semantics. Extensive experiments demonstrate that LaGa\neffectively captures key information from view-dependent semantics, enabling a\nmore comprehensive understanding of 3D scenes. Notably, under the same\nsettings, LaGa achieves a significant improvement of +18.7% mIoU over the\nprevious SOTA on the LERF-OVS dataset. Our code is available at:\nhttps://github.com/SJTU-DeepVisionLab/LaGa.", "AI": {"tldr": "LaGa\uff08Language Gaussians\uff09\u901a\u8fc7\u5206\u89e33D\u573a\u666f\u4e3a\u5bf9\u8c61\u5e76\u6784\u5efa\u89c6\u56fe\u805a\u5408\u8bed\u4e49\u8868\u793a\uff0c\u89e3\u51b3\u4e862D\u52303D\u8bed\u4e49\u7406\u89e3\u4e2d\u7684\u89c6\u56fe\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c062D\u8bed\u4e49\u7279\u5f81\u7b80\u5355\u6295\u5f71\u52303D\u9ad8\u65af\u4e0a\uff0c\u5ffd\u7565\u4e863D\u5bf9\u8c61\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u53ef\u80fd\u5c55\u73b0\u4e0d\u540c\u8bed\u4e49\u7684\u89c6\u56fe\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "method": "LaGa\u901a\u8fc7\u5206\u89e33D\u573a\u666f\u4e3a\u5bf9\u8c61\uff0c\u805a\u7c7b\u8bed\u4e49\u63cf\u8ff0\u7b26\u5e76\u57fa\u4e8e\u591a\u89c6\u89d2\u8bed\u4e49\u91cd\u65b0\u52a0\u6743\uff0c\u6784\u5efa\u89c6\u56fe\u805a\u5408\u7684\u8bed\u4e49\u8868\u793a\u3002", "result": "\u5728LERF-OVS\u6570\u636e\u96c6\u4e0a\uff0cLaGa\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u4e8618.7%\u7684mIoU\u3002", "conclusion": "LaGa\u6709\u6548\u6355\u6349\u4e86\u89c6\u56fe\u4f9d\u8d56\u8bed\u4e49\u7684\u5173\u952e\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u76843D\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2505.24427", "pdf": "https://arxiv.org/pdf/2505.24427", "abs": "https://arxiv.org/abs/2505.24427", "authors": ["Christopher Bagdon", "Aidan Combs", "Carina Silberer", "Roman Klinger"], "title": "Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts", "categories": ["cs.CL"], "comment": "Published at ACL 2025", "summary": "Accurate modeling of subjective phenomena such as emotion expression requires\ndata annotated with authors' intentions. Commonly such data is collected by\nasking study participants to donate and label genuine content produced in the\nreal world, or create content fitting particular labels during the study.\nAsking participants to create content is often simpler to implement and\npresents fewer risks to participant privacy than data donation. However, it is\nunclear if and how study-created content may differ from genuine content, and\nhow differences may impact models. We collect study-created and genuine\nmultimodal social media posts labeled for emotion and compare them on several\ndimensions, including model performance. We find that compared to genuine\nposts, study-created posts are longer, rely more on their text and less on\ntheir images for emotion expression, and focus more on emotion-prototypical\nevents. The samples of participants willing to donate versus create posts are\ndemographically different. Study-created data is valuable to train models that\ngeneralize well to genuine data, but realistic effectiveness estimates require\ngenuine data.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u7814\u7a76\u4e2d\u521b\u5efa\u7684\u5185\u5bb9\u4e0e\u771f\u5b9e\u5185\u5bb9\u5728\u60c5\u611f\u8868\u8fbe\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u957f\u5ea6\u3001\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u4f9d\u8d56\u7a0b\u5ea6\u4ee5\u53ca\u60c5\u611f\u4e8b\u4ef6\u5178\u578b\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u60c5\u611f\u8868\u8fbe\u7b49\u4e3b\u89c2\u73b0\u8c61\u7684\u51c6\u786e\u5efa\u6a21\u9700\u8981\u57fa\u4e8e\u4f5c\u8005\u610f\u56fe\u6807\u6ce8\u7684\u6570\u636e\u3002\u7814\u7a76\u4e2d\u521b\u5efa\u5185\u5bb9\u4e0e\u771f\u5b9e\u5185\u5bb9\u662f\u5426\u5b58\u5728\u5dee\u5f02\u53ca\u5176\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6536\u96c6\u7814\u7a76\u4e2d\u521b\u5efa\u7684\u548c\u771f\u5b9e\u7684\u591a\u6a21\u6001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u6807\u6ce8\u60c5\u611f\u5e76\u8fdb\u884c\u591a\u7ef4\u5ea6\u6bd4\u8f83\uff0c\u5305\u62ec\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7814\u7a76\u4e2d\u521b\u5efa\u7684\u5185\u5bb9\u6bd4\u771f\u5b9e\u5185\u5bb9\u66f4\u957f\uff0c\u66f4\u4f9d\u8d56\u6587\u672c\u800c\u975e\u56fe\u50cf\u8868\u8fbe\u60c5\u611f\uff0c\u4e14\u66f4\u805a\u7126\u4e8e\u60c5\u611f\u5178\u578b\u4e8b\u4ef6\u3002", "conclusion": "\u7814\u7a76\u4e2d\u521b\u5efa\u7684\u6570\u636e\u5bf9\u8bad\u7ec3\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u6709\u4ef7\u503c\uff0c\u4f46\u771f\u5b9e\u6570\u636e\u5bf9\u8bc4\u4f30\u6a21\u578b\u6548\u679c\u66f4\u53ef\u9760\u3002"}}
{"id": "2505.24787", "pdf": "https://arxiv.org/pdf/2505.24787", "abs": "https://arxiv.org/abs/2505.24787", "authors": ["Yucheng Zhou", "Jiahao Yuan", "Qianning Wang"], "title": "Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in text-to-image (T2I) generation have enabled models to\nproduce high-quality images from textual descriptions. However, these models\noften struggle with complex instructions involving multiple objects,\nattributes, and spatial relationships. Existing benchmarks for evaluating T2I\nmodels primarily focus on general text-image alignment and fail to capture the\nnuanced requirements of complex, multi-faceted prompts. Given this gap, we\nintroduce LongBench-T2I, a comprehensive benchmark specifically designed to\nevaluate T2I models under complex instructions. LongBench-T2I consists of 500\nintricately designed prompts spanning nine diverse visual evaluation\ndimensions, enabling a thorough assessment of a model's ability to follow\ncomplex instructions. Beyond benchmarking, we propose an agent framework\n(Plan2Gen) that facilitates complex instruction-driven image generation without\nrequiring additional model training. This framework integrates seamlessly with\nexisting T2I models, using large language models to interpret and decompose\ncomplex prompts, thereby guiding the generation process more effectively. As\nexisting evaluation metrics, such as CLIPScore, fail to adequately capture the\nnuances of complex instructions, we introduce an evaluation toolkit that\nautomates the quality assessment of generated images using a set of\nmulti-dimensional metrics. The data and code are released at\nhttps://github.com/yczhou001/LongBench-T2I.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86LongBench-T2I\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51faPlan2Gen\u6846\u67b6\u4ee5\u63d0\u5347\u590d\u6742\u6307\u4ee4\u9a71\u52a8\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6307\u4ee4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9\u590d\u6742\u6307\u4ee4\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faLongBench-T2I\u57fa\u51c6\u6d4b\u8bd5\u548cPlan2Gen\u6846\u67b6\uff0c\u540e\u8005\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u590d\u6742\u6307\u4ee4\u4ee5\u6307\u5bfc\u56fe\u50cf\u751f\u6210\u3002", "result": "LongBench-T2I\u5305\u542b500\u4e2a\u590d\u6742\u63d0\u793a\uff0cPlan2Gen\u6846\u67b6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "conclusion": "LongBench-T2I\u548cPlan2Gen\u586b\u8865\u4e86\u590d\u6742\u6307\u4ee4\u8bc4\u4f30\u548c\u751f\u6210\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2505.24428", "pdf": "https://arxiv.org/pdf/2505.24428", "abs": "https://arxiv.org/abs/2505.24428", "authors": ["Xu Wang", "Zihao Li", "Benyou Wang", "Yan Hu", "Difan Zou"], "title": "Model Unlearning via Sparse Autoencoder Subspace Guided Projections", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of information, making them\npowerful yet raising privacy and safety concerns when selective knowledge\nremoval is required. Existing unlearning strategies, ranging from\ngradient-based fine-tuning and model editing to sparse autoencoder (SAE)\nsteering, either lack interpretability or fail to provide a robust defense\nagainst adversarial prompts. We propose SAE-Guided Subspace Projection\nUnlearning (SSPU), a novel framework that leverages SAE features to drive\ntargeted updates in the model's parameter space, enabling precise,\ninterpretable, and robust unlearning. SSPU's three-stage pipeline performs\ndata-driven layer and feature selection, subspace construction via QR\ndecomposition, and constrained optimization that controls activations into an\n\"irrelevant\" subspace while preserving retained knowledge. Overall, we use SAE\nfeatures to construct a subspace that supervises unlearning, refining the loss\nand adding a regularization term to guide interpretable parameter updates. In\nexperiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,\nTruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared\nto the strongest baseline. It also improves adversarial robustness, lowering\nmalicious accuracy under jailbreak prompts compared to baselines. Our findings\nexpose the limitations of prior unlearning methods and demonstrate how\ninterpretable subspace-guided optimization can achieve robust, controllable\nmodel behavior.", "AI": {"tldr": "SAE-Guided Subspace Projection Unlearning (SSPU) \u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5b9e\u73b0\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u6a21\u578b\u77e5\u8bc6\u9057\u5fd8\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u50a8\u5927\u91cf\u4fe1\u606f\uff0c\u4f46\u9700\u8981\u9009\u62e9\u6027\u77e5\u8bc6\u9057\u5fd8\u4ee5\u89e3\u51b3\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u6216\u9c81\u68d2\u6027\u4e0a\u4e0d\u8db3\u3002", "method": "SSPU \u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff08\u6570\u636e\u9a71\u52a8\u7684\u5c42\u548c\u7279\u5f81\u9009\u62e9\u3001QR \u5206\u89e3\u6784\u5efa\u5b50\u7a7a\u95f4\u3001\u7ea6\u675f\u4f18\u5316\uff09\u5b9e\u73b0\u77e5\u8bc6\u9057\u5fd8\uff0c\u5e76\u5229\u7528 SAE \u7279\u5f81\u76d1\u7763\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728 WMDP-Cyber \u9057\u5fd8\u96c6\u548c\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSSPU \u5c06\u6709\u5bb3\u77e5\u8bc6\u51c6\u786e\u7387\u964d\u4f4e 3.22%\uff0c\u5e76\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "conclusion": "SSPU \u63ed\u793a\u4e86\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u53ef\u89e3\u91ca\u5b50\u7a7a\u95f4\u4f18\u5316\u7684\u9c81\u68d2\u53ef\u63a7\u6a21\u578b\u884c\u4e3a\u3002"}}
{"id": "2505.24792", "pdf": "https://arxiv.org/pdf/2505.24792", "abs": "https://arxiv.org/abs/2505.24792", "authors": ["Xinliu Zhong", "Leo Hwa Liang", "Angela S. Koh", "Yeo Si Yong"], "title": "Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification", "categories": ["cs.CV"], "comment": "6 pages, 15 figures", "summary": "Traditional diagnostic methods like colonoscopy are invasive yet critical\ntools necessary for accurately diagnosing colorectal cancer (CRC). Detection of\nCRC at early stages is crucial for increasing patient survival rates. However,\ncolonoscopy is dependent on obtaining adequate and high-quality endoscopic\nimages. Prolonged invasive procedures are inherently risky for patients, while\nsuboptimal or insufficient images hamper diagnostic accuracy. These images,\ntypically derived from video frames, often exhibit similar patterns, posing\nchallenges in discrimination. To overcome these challenges, we propose a novel\nDeep Learning network built on a Few-Shot Learning architecture, which includes\na tailored feature extractor, task interpolation, relational embedding, and a\nbi-level routing attention mechanism. The Few-Shot Learning paradigm enables\nour model to rapidly adapt to unseen fine-grained endoscopic image patterns,\nand the task interpolation augments the insufficient images artificially from\nvaried instrument viewpoints. Our relational embedding approach discerns\ncritical intra-image features and captures inter-image transitions between\nconsecutive endoscopic frames, overcoming the limitations of Convolutional\nNeural Networks (CNNs). The integration of a light-weight attention mechanism\nensures a concentrated analysis of pertinent image regions. By training on\ndiverse datasets, the model's generalizability and robustness are notably\nimproved for handling endoscopic images. Evaluated on Kvasir dataset, our model\ndemonstrated superior performance, achieving an accuracy of 90.1\\%, precision\nof 0.845, recall of 0.942, and an F1 score of 0.891. This surpasses current\nstate-of-the-art methods, presenting a promising solution to the challenges of\ninvasive colonoscopy by optimizing CRC detection through advanced image\nanalysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eFew-Shot Learning\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u7528\u4e8e\u4f18\u5316\u7ed3\u80a0\u955c\u56fe\u50cf\u5206\u6790\uff0c\u63d0\u9ad8\u7ed3\u76f4\u80a0\u764c\u65e9\u671f\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7ed3\u80a0\u955c\u68c0\u67e5\u5177\u6709\u4fb5\u5165\u6027\u4e14\u4f9d\u8d56\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u5224\u522b\u548c\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528Few-Shot Learning\u67b6\u6784\uff0c\u7ed3\u5408\u7279\u5f81\u63d0\u53d6\u5668\u3001\u4efb\u52a1\u63d2\u503c\u3001\u5173\u7cfb\u5d4c\u5165\u548c\u53cc\u7ea7\u8def\u7531\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728Kvasir\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe90.1%\uff0cF1\u5206\u65700.891\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7ed3\u80a0\u955c\u68c0\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u76f4\u80a0\u764c\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2505.24448", "pdf": "https://arxiv.org/pdf/2505.24448", "abs": "https://arxiv.org/abs/2505.24448", "authors": ["Eojin Kang", "Jaehyuk Yu", "Juae Kim"], "title": "Exploring the Impact of Occupational Personas on Domain-Specific QA", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on personas have improved the way Large Language Models (LLMs)\ninteract with users. However, the effect of personas on domain-specific\nquestion-answering (QA) tasks remains a subject of debate. This study analyzes\nwhether personas enhance specialized QA performance by introducing two types of\npersona: Profession-Based Personas (PBPs) (e.g., scientist), which directly\nrelate to domain expertise, and Occupational Personality-Based Personas (OPBPs)\n(e.g., scientific person), which reflect cognitive tendencies rather than\nexplicit expertise. Through empirical evaluations across multiple scientific\ndomains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs\noften degrade performance, even when semantically related to the task. Our\nfindings suggest that persona relevance alone does not guarantee effective\nknowledge utilization and that they may impose cognitive constraints that\nhinder optimal knowledge application. Future research can explore how nuanced\ndistinctions in persona representations guide LLMs, potentially contributing to\nreasoning and knowledge retrieval that more closely mirror human social\nconceptualization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e13\u4e1a\u76f8\u5173\u7684\u89d2\u8272\uff08PBPs\uff09\u80fd\u7565\u5fae\u63d0\u5347\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u800c\u804c\u4e1a\u6027\u683c\u76f8\u5173\u7684\u89d2\u8272\uff08OPBPs\uff09\u53cd\u800c\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002\u89d2\u8272\u76f8\u5173\u6027\u5e76\u4e0d\u4fdd\u8bc1\u77e5\u8bc6\u6709\u6548\u5229\u7528\uff0c\u53ef\u80fd\u5e26\u6765\u8ba4\u77e5\u9650\u5236\u3002", "motivation": "\u63a2\u8ba8\u89d2\u8272\uff08persona\uff09\u5bf9\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u4e13\u4e1a\u76f8\u5173\u548c\u804c\u4e1a\u6027\u683c\u76f8\u5173\u7684\u89d2\u8272\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u89d2\u8272\u7c7b\u578b\uff08PBPs\u548cOPBPs\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u9886\u57df\u79d1\u5b66\u95ee\u7b54\u4efb\u52a1\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "PBPs\u7565\u5fae\u63d0\u5347\u51c6\u786e\u6027\uff0cOPBPs\u5219\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\uff0c\u5373\u4f7f\u8bed\u4e49\u76f8\u5173\u3002", "conclusion": "\u89d2\u8272\u76f8\u5173\u6027\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u77e5\u8bc6\u6709\u6548\u5229\u7528\uff0c\u672a\u6765\u53ef\u7814\u7a76\u89d2\u8272\u8868\u5f81\u7684\u7ec6\u5fae\u5dee\u5f02\u5982\u4f55\u5f15\u5bfcLLMs\u3002"}}
{"id": "2505.24816", "pdf": "https://arxiv.org/pdf/2505.24816", "abs": "https://arxiv.org/abs/2505.24816", "authors": ["Jiangpeng He", "Zhihao Duan", "Fengqing Zhu"], "title": "CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Class-Incremental Learning (CIL) aims to learn new classes sequentially while\nretaining the knowledge of previously learned classes. Recently, pre-trained\nmodels (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown\nremarkable performance in rehearsal-free CIL without requiring exemplars from\nprevious tasks. However, existing adapter-based methods, which incorporate\nlightweight learnable modules into PTMs for CIL, create new adapters for each\nnew task, leading to both parameter redundancy and failure to leverage shared\nknowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation\n(CL-LoRA), which introduces a novel dual-adapter architecture combining\n\\textbf{task-shared adapters} to learn cross-task knowledge and\n\\textbf{task-specific adapters} to capture unique features of each new task.\nSpecifically, the shared adapters utilize random orthogonal matrices and\nleverage knowledge distillation with gradient reassignment to preserve\nessential shared knowledge. In addition, we introduce learnable block-wise\nweights for task-specific adapters, which mitigate inter-task interference\nwhile maintaining the model's plasticity. We demonstrate CL-LoRA consistently\nachieves promising performance under multiple benchmarks with reduced training\nand inference computation, establishing a more efficient and scalable paradigm\nfor continual learning with pre-trained models.", "AI": {"tldr": "CL-LoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9002\u914d\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u4efb\u52a1\u5171\u4eab\u9002\u914d\u5668\u548c\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86CIL\u4e2d\u53c2\u6570\u5197\u4f59\u548c\u8de8\u4efb\u52a1\u77e5\u8bc6\u5171\u4eab\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\u5728CIL\u4e2d\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u521b\u5efa\u65b0\u9002\u914d\u5668\uff0c\u5bfc\u81f4\u53c2\u6570\u5197\u4f59\u4e14\u672a\u80fd\u5229\u7528\u8de8\u4efb\u52a1\u5171\u4eab\u77e5\u8bc6\u3002", "method": "CL-LoRA\u91c7\u7528\u53cc\u9002\u914d\u5668\u67b6\u6784\uff1a\u4efb\u52a1\u5171\u4eab\u9002\u914d\u5668\u5b66\u4e60\u8de8\u4efb\u52a1\u77e5\u8bc6\uff0c\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u6355\u83b7\u65b0\u4efb\u52a1\u72ec\u7279\u7279\u5f81\u3002\u5171\u4eab\u9002\u914d\u5668\u4f7f\u7528\u968f\u673a\u6b63\u4ea4\u77e9\u9635\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u5f15\u5165\u53ef\u5b66\u4e60\u5757\u7ea7\u6743\u91cd\u3002", "result": "CL-LoRA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u91cf\u3002", "conclusion": "CL-LoRA\u4e3a\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684\u8303\u4f8b\u3002"}}
{"id": "2505.24449", "pdf": "https://arxiv.org/pdf/2505.24449", "abs": "https://arxiv.org/abs/2505.24449", "authors": ["Kailin Jiang", "Yuntao Du", "Yukai Ding", "Yuchen Ren", "Ning Jiang", "Zhi Gao", "Zilong Zheng", "Lei Liu", "Bin Li", "Qing Li"], "title": "When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways", "categories": ["cs.CL"], "comment": null, "summary": "Large language/multimodal models (LLMs/LMMs) store extensive pre-trained\nknowledge but struggle to maintain consistency with real-world updates, making\nit difficult to avoid catastrophic forgetting while acquiring evolving\nknowledge. Previous work focused on constructing textual knowledge datasets and\nexploring knowledge injection in LLMs, lacking exploration of multimodal\nevolving knowledge injection in LMMs. To address this, we propose the EVOKE\nbenchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in\nreal-world scenarios. Meanwhile, a comprehensive evaluation of multimodal\nevolving knowledge injection revealed two challenges: (1) Existing knowledge\ninjection methods perform terribly on evolving knowledge. (2) Supervised\nfine-tuning causes catastrophic forgetting, particularly instruction following\nability is severely compromised. Additionally, we provide pathways and find\nthat: (1) Text knowledge augmentation during the training phase improves\nperformance, while image augmentation cannot achieve it. (2) Continual learning\nmethods, especially Replay and MoELoRA, effectively mitigate forgetting. Our\nfindings indicate that current knowledge injection methods have many\nlimitations on evolving knowledge, which motivates further research on more\nefficient and stable knowledge injection methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEVOKE\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LMMs\uff09\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6ce8\u5165\u52a8\u6001\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u77e5\u8bc6\u6ce8\u5165\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u8def\u5f84\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LMMs\uff09\u5728\u52a8\u6001\u77e5\u8bc6\u6ce8\u5165\u4e2d\u7684\u6311\u6218\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u63d0\u5347\u77e5\u8bc6\u66f4\u65b0\u80fd\u529b\u3002", "method": "\u63d0\u51faEVOKE\u57fa\u51c6\uff0c\u8bc4\u4f30\u52a8\u6001\u77e5\u8bc6\u6ce8\u5165\u80fd\u529b\uff1b\u5206\u6790\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u6587\u672c\u77e5\u8bc6\u589e\u5f3a\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08\u5982Replay\u548cMoELoRA\uff09\u3002", "result": "\u73b0\u6709\u77e5\u8bc6\u6ce8\u5165\u65b9\u6cd5\u5728\u52a8\u6001\u77e5\u8bc6\u4e0a\u8868\u73b0\u5dee\uff1b\u76d1\u7763\u5fae\u8c03\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff1b\u6587\u672c\u77e5\u8bc6\u589e\u5f3a\u6709\u6548\uff0c\u56fe\u50cf\u589e\u5f3a\u65e0\u6548\uff1b\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u7f13\u89e3\u9057\u5fd8\u3002", "conclusion": "\u5f53\u524d\u77e5\u8bc6\u6ce8\u5165\u65b9\u6cd5\u5728\u52a8\u6001\u77e5\u8bc6\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u66f4\u9ad8\u6548\u7a33\u5b9a\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.24824", "pdf": "https://arxiv.org/pdf/2505.24824", "abs": "https://arxiv.org/abs/2505.24824", "authors": ["Marta L\u00f3pez-Rauhut", "Hongyu Zhou", "Mathieu Aubry", "Loic Landrieu"], "title": "Segmenting France Across Four Centuries", "categories": ["cs.CV"], "comment": "20 pages, 8 figures, 3 tables", "summary": "Historical maps offer an invaluable perspective into territory evolution\nacross past centuries--long before satellite or remote sensing technologies\nexisted. Deep learning methods have shown promising results in segmenting\nhistorical maps, but publicly available datasets typically focus on a single\nmap type or period, require extensive and costly annotations, and are not\nsuited for nationwide, long-term analyses. In this paper, we introduce a new\ndataset of historical maps tailored for analyzing large-scale, long-term land\nuse and land cover evolution with limited annotations. Spanning metropolitan\nFrance (548,305 km^2), our dataset contains three map collections from the\n18th, 19th, and 20th centuries. We provide both comprehensive modern labels and\n22,878 km^2 of manually annotated historical labels for the 18th and 19th\ncentury maps. Our dataset illustrates the complexity of the segmentation task,\nfeaturing stylistic inconsistencies, interpretive ambiguities, and significant\nlandscape changes (e.g., marshlands disappearing in favor of forests). We\nassess the difficulty of these challenges by benchmarking three approaches: a\nfully-supervised model trained with historical labels, and two\nweakly-supervised models that rely only on modern annotations. The latter\neither use the modern labels directly or first perform image-to-image\ntranslation to address the stylistic gap between historical and contemporary\nmaps. Finally, we discuss how these methods can support long-term environment\nmonitoring, offering insights into centuries of landscape transformation. Our\nofficial project repository is publicly available at\nhttps://github.com/Archiel19/FRAx4.git.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u5386\u53f2\u5730\u56fe\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u3001\u957f\u671f\u7684\u571f\u5730\u5229\u7528\u548c\u571f\u5730\u8986\u76d6\u6f14\u53d8\u5206\u6790\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5206\u5272\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u63d0\u4f9b\u4e86\u7814\u7a76\u8fc7\u53bb\u51e0\u4e2a\u4e16\u7eaa\u9886\u571f\u6f14\u53d8\u7684\u72ec\u7279\u89c6\u89d2\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u7c7b\u578b\u6216\u65f6\u671f\uff0c\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8986\u76d6\u6cd5\u56fd\u5927\u90fd\u5e02\u7684\u5386\u53f2\u5730\u56fe\u6570\u636e\u96c6\uff0c\u5305\u542b18\u81f320\u4e16\u7eaa\u7684\u5730\u56fe\uff0c\u5e76\u63d0\u4f9b\u73b0\u4ee3\u548c\u5386\u53f2\u6807\u6ce8\u3002\u8bc4\u4f30\u4e86\u5168\u76d1\u7763\u548c\u5f31\u76d1\u7763\u4e09\u79cd\u5206\u5272\u65b9\u6cd5\u3002", "result": "\u6570\u636e\u96c6\u5c55\u793a\u4e86\u5206\u5272\u4efb\u52a1\u7684\u590d\u6742\u6027\uff08\u5982\u98ce\u683c\u4e0d\u4e00\u81f4\u3001\u89e3\u91ca\u6a21\u7cca\u7b49\uff09\u3002\u5f31\u76d1\u7763\u65b9\u6cd5\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u56fe\u50cf\u7ffb\u8bd1\u89e3\u51b3\u98ce\u683c\u5dee\u5f02\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u957f\u671f\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u51e0\u4e2a\u4e16\u7eaa\u7684\u666f\u89c2\u53d8\u5316\u3002"}}
{"id": "2505.24455", "pdf": "https://arxiv.org/pdf/2505.24455", "abs": "https://arxiv.org/abs/2505.24455", "authors": ["Cesar Gonzalez-Gutierrez", "Ariadna Quattoni"], "title": "Domain Pre-training Impact on Representations", "categories": ["cs.CL"], "comment": null, "summary": "This empirical study analyzes the effects of the pre-training corpus on the\nquality of learned transformer representations. We focus on the representation\nquality induced solely through pre-training. Our experiments show that\npre-training on a small, specialized corpus can yield effective\nrepresentations, and that the success of combining a generic and a specialized\ncorpus depends on the distributional similarity between the target task and the\nspecialized corpus.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u5bf9Transformer\u8868\u793a\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5c0f\u89c4\u6a21\u4e13\u7528\u8bed\u6599\u5e93\u4e5f\u80fd\u751f\u6210\u6709\u6548\u8868\u793a\uff0c\u4e14\u901a\u7528\u4e0e\u4e13\u7528\u8bed\u6599\u5e93\u7684\u7ed3\u5408\u6548\u679c\u53d6\u51b3\u4e8e\u76ee\u6807\u4efb\u52a1\u4e0e\u4e13\u7528\u8bed\u6599\u5e93\u7684\u5206\u5e03\u76f8\u4f3c\u6027\u3002", "motivation": "\u63a2\u8ba8\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u5bf9Transformer\u8868\u793a\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u4ec5\u901a\u8fc7\u9884\u8bad\u7ec3\u8bf1\u5bfc\u7684\u8868\u793a\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\uff08\u5c0f\u89c4\u6a21\u4e13\u7528\u8bed\u6599\u5e93\u3001\u901a\u7528\u4e0e\u4e13\u7528\u8bed\u6599\u5e93\u7ed3\u5408\uff09\u5bf9\u8868\u793a\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u5c0f\u89c4\u6a21\u4e13\u7528\u8bed\u6599\u5e93\u53ef\u751f\u6210\u6709\u6548\u8868\u793a\uff1b\u901a\u7528\u4e0e\u4e13\u7528\u8bed\u6599\u5e93\u7ed3\u5408\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u76ee\u6807\u4efb\u52a1\u4e0e\u4e13\u7528\u8bed\u6599\u5e93\u7684\u5206\u5e03\u76f8\u4f3c\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u9009\u62e9\u5bf9\u8868\u793a\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4e13\u7528\u8bed\u6599\u5e93\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.24837", "pdf": "https://arxiv.org/pdf/2505.24837", "abs": "https://arxiv.org/abs/2505.24837", "authors": ["Yinglian Zhu", "Haiyang Yu", "Qizao Wang", "Wei Lu", "Xiangyang Xue", "Bin Li"], "title": "Zero-Shot Chinese Character Recognition with Hierarchical Multi-Granularity Image-Text Aligning", "categories": ["cs.CV"], "comment": "The first three authors contributed equally", "summary": "Chinese Character Recognition (CCR) is a fundamental technology for\nintelligent document processing. Unlike Latin characters, Chinese characters\nexhibit unique spatial structures and compositional rules, allowing for the use\nof fine-grained semantic information in representation. However, existing\napproaches are usually based on auto-regressive as well as edit distance\npost-process and typically rely on a single-level character representation. In\nthis paper, we propose a Hierarchical Multi-Granularity Image-Text Aligning\n(Hi-GITA) framework based on a contrastive paradigm. To leverage the abundant\nfine-grained semantic information of Chinese characters, we propose\nmulti-granularity encoders on both image and text sides. Specifically, the\nImage Multi-Granularity Encoder extracts hierarchical image representations\nfrom character images, capturing semantic cues from localized strokes to\nholistic structures. The Text Multi-Granularity Encoder extracts stroke and\nradical sequence representations at different levels of granularity. To better\ncapture the relationships between strokes and radicals, we introduce\nMulti-Granularity Fusion Modules on the image and text sides, respectively.\nFurthermore, to effectively bridge the two modalities, we further introduce a\nFine-Grained Decoupled Image-Text Contrastive loss, which aligns image and text\nrepresentations across multiple granularities. Extensive experiments\ndemonstrate that our proposed Hi-GITA significantly outperforms existing\nzero-shot CCR methods. For instance, it brings about 20% accuracy improvement\nin handwritten character and radical zero-shot settings. Code and models will\nbe released soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u8303\u5f0f\u7684\u5206\u5c42\u591a\u7c92\u5ea6\u56fe\u6587\u5bf9\u9f50\u6846\u67b6\uff08Hi-GITA\uff09\uff0c\u7528\u4e8e\u4e2d\u6587\u5b57\u7b26\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e2d\u6587\u5b57\u7b26\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u81ea\u56de\u5f52\u548c\u7f16\u8f91\u8ddd\u79bb\u540e\u5904\u7406\uff0c\u4e14\u4f9d\u8d56\u5355\u5c42\u5b57\u7b26\u8868\u793a\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e2d\u6587\u5b57\u7b26\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u591a\u7c92\u5ea6\u7f16\u7801\u5668\uff08\u56fe\u50cf\u548c\u6587\u672c\u4fa7\uff09\uff0c\u5f15\u5165\u591a\u7c92\u5ea6\u878d\u5408\u6a21\u5757\uff0c\u5e76\u4f7f\u7528\u7ec6\u7c92\u5ea6\u89e3\u8026\u56fe\u6587\u5bf9\u6bd4\u635f\u5931\u5bf9\u9f50\u591a\u7c92\u5ea6\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHi-GITA\u5728\u96f6\u6837\u672c\u4e2d\u6587\u5b57\u7b26\u8bc6\u522b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728\u624b\u5199\u5b57\u7b26\u548c\u90e8\u9996\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u63d0\u5347\u7ea620%\u51c6\u786e\u7387\u3002", "conclusion": "Hi-GITA\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u591a\u7c92\u5ea6\u8868\u793a\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e2d\u6587\u5b57\u7b26\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.24456", "pdf": "https://arxiv.org/pdf/2505.24456", "abs": "https://arxiv.org/abs/2505.24456", "authors": ["Emilio Villa-Cueva", "Sholpan Bolatzhanova", "Diana Turmakhan", "Kareem Elzeky", "Henok Biadglign Ademtew", "Alham Fikri Aji", "Israel Abebe Azime", "Jinheon Baek", "Frederico Belcavello", "Fermin Cristobal", "Jan Christian Blaise Cruz", "Mary Dabre", "Raj Dabre", "Toqeer Ehsan", "Naome A Etori", "Fauzan Farooqui", "Jiahui Geng", "Guido Ivetta", "Thanmay Jayakumar", "Soyeong Jeong", "Zheng Wei Lim", "Aishik Mandal", "Sofia Martinelli", "Mihail Minkov Mihaylov", "Daniil Orel", "Aniket Pramanick", "Sukannya Purkayastha", "Israfel Salazar", "Haiyue Song", "Tiago Timponi Torrent", "Debela Desalegn Yadeta", "Injy Hamed", "Atnafu Lambebo Tonja", "Thamar Solorio"], "title": "CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Cultural content poses challenges for machine translation systems due to the\ndifferences in conceptualizations between cultures, where language alone may\nfail to convey sufficient context to capture region-specific meanings. In this\nwork, we investigate whether images can act as cultural context in multimodal\ntranslation. We introduce CaMMT, a human-curated benchmark of over 5,800\ntriples of images along with parallel captions in English and regional\nlanguages. Using this dataset, we evaluate five Vision Language Models (VLMs)\nin text-only and text+image settings. Through automatic and human evaluations,\nwe find that visual context generally improves translation quality, especially\nin handling Culturally-Specific Items (CSIs), disambiguation, and correct\ngender usage. By releasing CaMMT, we aim to support broader efforts in building\nand evaluating multimodal translation systems that are better aligned with\ncultural nuance and regional variation.", "AI": {"tldr": "CaMMT\u662f\u4e00\u4e2a\u5305\u542b5800\u591a\u7ec4\u56fe\u50cf\u53ca\u5e73\u884c\u6807\u9898\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u56fe\u50cf\u4f5c\u4e3a\u6587\u5316\u8bed\u5883\u5728\u591a\u6a21\u6001\u7ffb\u8bd1\u4e2d\u7684\u4f5c\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u89c6\u89c9\u4e0a\u4e0b\u6587\u80fd\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6587\u5316\u7279\u5b9a\u5185\u5bb9\u3001\u6b67\u4e49\u548c\u6027\u522b\u51c6\u786e\u6027\u65b9\u9762\u3002", "motivation": "\u6587\u5316\u5185\u5bb9\u7684\u5dee\u5f02\u5bfc\u81f4\u673a\u5668\u7ffb\u8bd1\u96be\u4ee5\u51c6\u786e\u4f20\u8fbe\u533a\u57df\u7279\u5b9a\u542b\u4e49\uff0c\u56e0\u6b64\u7814\u7a76\u56fe\u50cf\u662f\u5426\u80fd\u4f5c\u4e3a\u6587\u5316\u8bed\u5883\u8f85\u52a9\u7ffb\u8bd1\u3002", "method": "\u6784\u5efaCaMMT\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e94\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7eaf\u6587\u672c\u548c\u6587\u672c+\u56fe\u50cf\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u89c6\u89c9\u4e0a\u4e0b\u6587\u666e\u904d\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u6587\u5316\u7279\u5b9a\u5185\u5bb9\u3001\u6b67\u4e49\u548c\u6027\u522b\u51c6\u786e\u6027\u65b9\u9762\u6548\u679c\u663e\u8457\u3002", "conclusion": "CaMMT\u7684\u53d1\u5e03\u652f\u6301\u591a\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u6587\u5316\u5dee\u5f02\u548c\u533a\u57df\u53d8\u5316\u3002"}}
{"id": "2505.24838", "pdf": "https://arxiv.org/pdf/2505.24838", "abs": "https://arxiv.org/abs/2505.24838", "authors": ["Brandon Man", "Ghadi Nehme", "Md Ferdous Alam", "Faez Ahmed"], "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.", "AI": {"tldr": "VideoCAD\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b66\u4e60CAD\u64cd\u4f5c\u7684UI\u4ea4\u4e92\uff0c\u652f\u6301\u957f\u65f6\u7a0b\u548c\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86VideoCADFormer\u6a21\u578b\u548cVQA\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709AI\u9a71\u52a8\u7684UI\u4ee3\u7406\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u4e13\u4e1a\u5de5\u7a0b\u5de5\u5177\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662fCAD\u8fd9\u79cd\u590d\u6742\u3001\u957f\u65f6\u7a0b\u7684\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6846\u67b6\u751f\u621041K\u4e2a\u5e26\u6ce8\u91ca\u7684CAD\u64cd\u4f5c\u89c6\u9891\uff0c\u63d0\u51faVideoCADFormer\u6a21\u578b\u5b66\u4e60UI\u4ea4\u4e92\uff0c\u5e76\u8bbe\u8ba1VQA\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "VideoCADFormer\u5728CAD\u4ea4\u4e92\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0cVQA\u57fa\u51c6\u63ed\u793a\u4e86\u89c6\u9891UI\u7406\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "VideoCAD\u4e3a\u590d\u6742UI\u4ea4\u4e92\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u957f\u65f6\u7a0b\u4f9d\u8d56\u7684\u6311\u6218\u3002"}}
{"id": "2505.24472", "pdf": "https://arxiv.org/pdf/2505.24472", "abs": "https://arxiv.org/abs/2505.24472", "authors": ["Hieu Tran", "Phuong-Anh Nguyen-Le", "Huy Nghiem", "Quang-Nhan Nguyen", "Wei Ai", "Marine Carpuat"], "title": "VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine translation systems fail when processing code-mixed inputs for\nlow-resource languages. We address this challenge by curating VietMix, a\nparallel corpus of naturally occurring code-mixed Vietnamese text paired with\nexpert English translations. Augmenting this resource, we developed a\ncomplementary synthetic data generation pipeline. This pipeline incorporates\nfiltering mechanisms to ensure syntactic plausibility and pragmatic\nappropriateness in code-mixing patterns. Experimental validation shows our\nnaturalistic and complementary synthetic data boost models' performance,\nmeasured by translation quality estimation scores, of up to 71.84 on COMETkiwi\nand 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,\naugmented models are favored over seed fine-tuned counterparts in approximately\n49% of judgments (54-56% excluding ties). VietMix and our augmentation\nmethodology advance ecological validity in neural MT evaluations and establish\na framework for addressing code-mixed translation challenges across other\nlow-resource pairs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVietMix\u8bed\u6599\u5e93\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u6df7\u5408\u4ee3\u7801\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u6df7\u5408\u4ee3\u7801\u7ffb\u8bd1\u7cfb\u7edf\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaVietMix\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u5f00\u53d1\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5e76\u52a0\u5165\u8fc7\u6ee4\u673a\u5236\u786e\u4fdd\u8bed\u6cd5\u548c\u8bed\u7528\u5408\u7406\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08COMETkiwi 71.84\uff0cXCOMET 81.77\uff09\uff0c\u589e\u5f3a\u6a21\u578b\u572849%\u7684\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "VietMix\u548c\u65b9\u6cd5\u63d0\u5347\u4e86\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7684\u751f\u6001\u6548\u5ea6\uff0c\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u63d0\u4f9b\u4e86\u89e3\u51b3\u6846\u67b6\u3002"}}
{"id": "2505.24840", "pdf": "https://arxiv.org/pdf/2505.24840", "abs": "https://arxiv.org/abs/2505.24840", "authors": ["Yuwen Tan", "Yuan Qing", "Boqing Gong"], "title": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "28 pages, 13 figures", "summary": "This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u4e16\u754c\u7684\u5c42\u6b21\u5316\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u89c6\u89c9LLMs\u7684\u5c42\u6b21\u5316\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002\u901a\u8fc7\u767e\u4e07\u7ea7\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u9a8c\u8bc1\uff0c\u5fae\u8c03\u540eLLMs\u7684\u5c42\u6b21\u4e00\u81f4\u6027\u63d0\u5347\u6709\u9650\u3002", "motivation": "\u63ed\u793aLLMs\u5728\u89c6\u89c9\u5c42\u6b21\u5316\u77e5\u8bc6\u4e0a\u7684\u4e0d\u8db3\uff0c\u63a2\u8ba8\u5176\u5bf9\u89c6\u89c9LLMs\u7406\u89e3\u80fd\u529b\u7684\u74f6\u9888\u6548\u5e94\u3002", "method": "\u5229\u7528\u516d\u79cd\u5206\u7c7b\u6cd5\u548c\u56db\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u6784\u5efa\u767e\u4e07\u7ea7\u56db\u9009\u4e00\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u5bf9\u89c6\u89c9LLMs\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5fae\u8c03\u540eLLMs\u7684\u5c42\u6b21\u4e00\u81f4\u6027\u63d0\u5347\u6709\u9650\uff0c\u8868\u660e\u5176\u4ecd\u662f\u89c6\u89c9LLMs\u5c42\u6b21\u5316\u7406\u89e3\u7684\u74f6\u9888\u3002", "conclusion": "LLMs\u9700\u5177\u5907\u76f8\u5e94\u5206\u7c7b\u77e5\u8bc6\uff0c\u624d\u80fd\u652f\u6301\u89c6\u89c9LLMs\u5b9e\u73b0\u5b8c\u5168\u5c42\u6b21\u5316\u7684\u89c6\u89c9\u6982\u5ff5\u7406\u89e3\u3002"}}
{"id": "2505.24480", "pdf": "https://arxiv.org/pdf/2505.24480", "abs": "https://arxiv.org/abs/2505.24480", "authors": ["Fei Bai", "Yingqian Min", "Beichen Zhang", "Zhipeng Chen", "Wayne Xin Zhao", "Lei Fang", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "title": "Towards Effective Code-Integrated Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report on Slow Thinking with LLMs: Code-Integrated\n  Reasoning", "summary": "In this paper, we investigate code-integrated reasoning, where models\ngenerate code when necessary and integrate feedback by executing it through a\ncode interpreter. To acquire this capability, models must learn when and how to\nuse external code tools effectively, which is supported by tool-augmented\nreinforcement learning (RL) through interactive learning. Despite its benefits,\ntool-augmented RL can still suffer from potential instability in the learning\ndynamics. In light of this challenge, we present a systematic approach to\nimproving the training effectiveness and stability of tool-augmented RL for\ncode-integrated reasoning. Specifically, we develop enhanced training\nstrategies that balance exploration and stability, progressively building\ntool-use capabilities while improving reasoning performance. Through extensive\nexperiments on five mainstream mathematical reasoning benchmarks, our model\ndemonstrates significant performance improvements over multiple competitive\nbaselines. Furthermore, we conduct an in-depth analysis of the mechanism and\neffect of code-integrated reasoning, revealing several key insights, such as\nthe extension of model's capability boundaries and the simultaneous improvement\nof reasoning efficiency through code integration. All data and code for\nreproducing this work are available at: https://github.com/RUCAIBox/CIR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee3\u7801\u96c6\u6210\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u6a21\u578b\uff0c\u52a8\u6001\u751f\u6210\u548c\u6267\u884c\u4ee3\u7801\u4ee5\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6539\u8fdb\u4e86\u8bad\u7ec3\u7b56\u7565\u4ee5\u589e\u5f3a\u7a33\u5b9a\u6027\u548c\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u4ee3\u7801\u96c6\u6210\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u548c\u6267\u884c\u4ee3\u7801\u6269\u5c55\u6a21\u578b\u80fd\u529b\u8fb9\u754c\uff0c\u540c\u65f6\u89e3\u51b3\u5de5\u5177\u589e\u5f3aRL\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5de5\u5177\u589e\u5f3a\u7684RL\u65b9\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u7a33\u5b9a\u6027\uff0c\u9010\u6b65\u63d0\u5347\u6a21\u578b\u4f7f\u7528\u5916\u90e8\u4ee3\u7801\u5de5\u5177\u7684\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2a\u4e3b\u6d41\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4ee3\u7801\u96c6\u6210\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u548c\u63a8\u7406\u6548\u7387\u7684\u63d0\u5347\u3002", "conclusion": "\u4ee3\u7801\u96c6\u6210\u63a8\u7406\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u6539\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u4e86RL\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.24848", "pdf": "https://arxiv.org/pdf/2505.24848", "abs": "https://arxiv.org/abs/2505.24848", "authors": ["Charig Yang", "Samiul Alam", "Shakhrul Iman Siam", "Michael J. Proulx", "Lambert Mathias", "Kiran Somasundaram", "Luis Pesqueira", "James Fort", "Sheroze Sheriffdeen", "Omkar Parkhi", "Carl Ren", "Mi Zhang", "Yuning Chai", "Richard Newcombe", "Hyo Jin Kim"], "title": "Reading Recognition in the Wild", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "To enable egocentric contextual AI in always-on smart glasses, it is crucial\nto be able to keep a record of the user's interactions with the world,\nincluding during reading. In this paper, we introduce a new task of reading\nrecognition to determine when the user is reading. We first introduce the\nfirst-of-its-kind large-scale multimodal Reading in the Wild dataset,\ncontaining 100 hours of reading and non-reading videos in diverse and realistic\nscenarios. We then identify three modalities (egocentric RGB, eye gaze, head\npose) that can be used to solve the task, and present a flexible transformer\nmodel that performs the task using these modalities, either individually or\ncombined. We show that these modalities are relevant and complementary to the\ntask, and investigate how to efficiently and effectively encode each modality.\nAdditionally, we show the usefulness of this dataset towards classifying types\nof reading, extending current reading understanding studies conducted in\nconstrained settings to larger scale, diversity and realism. Code, model, and\ndata will be public.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\u2014\u2014\u9605\u8bfb\u8bc6\u522b\uff0c\u7528\u4e8e\u5224\u65ad\u7528\u6237\u4f55\u65f6\u5728\u9605\u8bfb\uff0c\u5e76\u5f15\u5165\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u201cReading in the Wild\u201d\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684Transformer\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u667a\u80fd\u773c\u955c\u4e2d\u7684\u81ea\u6211\u60c5\u5883AI\uff0c\u9700\u8981\u8bb0\u5f55\u7528\u6237\u4e0e\u4e16\u754c\u7684\u4e92\u52a8\uff0c\u5305\u62ec\u9605\u8bfb\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6a21\u6001\uff08RGB\u3001\u773c\u52a8\u3001\u5934\u90e8\u59ff\u6001\uff09\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684Transformer\u6a21\u578b\uff0c\u652f\u6301\u5355\u6a21\u6001\u6216\u591a\u6a21\u6001\u8f93\u5165\u3002", "result": "\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6a21\u6001\u7684\u76f8\u5173\u6027\u548c\u4e92\u8865\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u5982\u4f55\u9ad8\u6548\u7f16\u7801\u6bcf\u79cd\u6a21\u6001\u3002\u6570\u636e\u96c6\u8fd8\u6269\u5c55\u4e86\u9605\u8bfb\u5206\u7c7b\u7814\u7a76\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9605\u8bfb\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u5728\u975e\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9605\u8bfb\u7406\u89e3\u7814\u7a76\u3002"}}
{"id": "2505.24500", "pdf": "https://arxiv.org/pdf/2505.24500", "abs": "https://arxiv.org/abs/2505.24500", "authors": ["Guiyang Hou", "Xing Gao", "Yuchuan Wu", "Xiang Huang", "Wenqi Zhang", "Zhe Zheng", "Yongliang Shen", "Jialu Du", "Fei Huang", "Yongbin Li", "Weiming Lu"], "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures", "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTimeHC-RL\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793e\u4ea4\u9886\u57df\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u7684System 2 RL\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u9700\u8981\u7cfb\u7edf2\u8ba4\u77e5\u7684\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u793e\u4ea4\u9886\u57df\u7684\u8ba4\u77e5\u53d1\u5c55\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u793e\u4ea4\u9886\u57df\u9700\u8981\u7ed3\u5408\u7cfb\u7edf1\u548c\u7cfb\u7edf2\u7684\u8ba4\u77e5\u6a21\u5f0f\uff0c\u4e14\u5177\u6709\u72ec\u7279\u7684\u65f6\u95f4\u7ebf\u3002", "method": "\u63d0\u51fa\u4e86Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u79cd\u540e\u8bad\u7ec3\u8303\u5f0f\u548c\u4e24\u79cd\u6d4b\u8bd5\u65f6\u5e72\u9884\u8303\u5f0f\uff0c\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aTimeHC-RL\u4f18\u4e8eSystem 2 RL\u65b9\u6cd5\uff0c\u4f7f7B\u6a21\u578b\u6027\u80fd\u5ab2\u7f8eDeepSeek-R1\u548cOpenAI-O3\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "TimeHC-RL\u4e3a\u63d0\u5347LLMs\u7684\u793e\u4ea4\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u540e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u5e72\u9884\u7684\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2505.24862", "pdf": "https://arxiv.org/pdf/2505.24862", "abs": "https://arxiv.org/abs/2505.24862", "authors": ["Cailin Zhuang", "Ailin Huang", "Wei Cheng", "Jingwei Wu", "Yaoqi Hu", "Jiaqi Liao", "Zhewei Huang", "Hongyuan Wang", "Xinyao Liao", "Weiwei Cai", "Hengyuan Xu", "Xuanyang Zhang", "Xianfang Zeng", "Gang Yu", "Chi Zhang"], "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization", "categories": ["cs.CV"], "comment": "33 Pages, Project Page: https://vistorybench.github.io/, Code:\n  https://github.com/vistorybench/vistorybench", "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.", "AI": {"tldr": "ViStoryBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6545\u4e8b\u53ef\u89c6\u5316\u6a21\u578b\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u6545\u4e8b\u7c7b\u578b\u548c\u827a\u672f\u98ce\u683c\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u5347\u6545\u4e8b\u53ef\u89c6\u5316\u6846\u67b6\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u60c5\u8282\u548c\u89c6\u89c9\u98ce\u683c\uff0c\u5e76\u8bbe\u8ba1\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u3002", "result": "ViStoryBench\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e2e\u52a9\u8bc6\u522b\u5176\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u4fc3\u8fdb\u6a21\u578b\u7684\u9488\u5bf9\u6027\u6539\u8fdb\u3002"}}
{"id": "2505.24523", "pdf": "https://arxiv.org/pdf/2505.24523", "abs": "https://arxiv.org/abs/2505.24523", "authors": ["Andrea Pedrotti", "Michele Papucci", "Cristiano Ciaccio", "Alessio Miaschi", "Giovanni Puccetti", "Felice Dell'Orletta", "Andrea Esuli"], "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of ACL 2025", "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have\nenabled the creation of highly realistic synthetic content, raising concerns\nabout the potential for malicious use, such as misinformation and manipulation.\nMoreover, detecting Machine-Generated Text (MGT) remains challenging due to the\nlack of robust benchmarks that assess generalization to real-world scenarios.\nIn this work, we present a pipeline to test the resilience of state-of-the-art\nMGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed\nadversarial attacks. To challenge the detectors, we fine-tune language models\nusing Direct Preference Optimization (DPO) to shift the MGT style toward\nhuman-written text (HWT). This exploits the detectors' reliance on stylistic\nclues, making new generations more challenging to detect. Additionally, we\nanalyze the linguistic shifts induced by the alignment and which features are\nused by detectors to detect MGT texts. Our results show that detectors can be\neasily fooled with relatively few examples, resulting in a significant drop in\ndetection performance. This highlights the importance of improving detection\nmethods and making them robust to unseen in-domain texts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u673a\u5668\u751f\u6210\u6587\u672c\uff08MGT\uff09\u68c0\u6d4b\u5668\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u653b\u51fb\u6311\u6218\u73b0\u6709\u68c0\u6d4b\u5668\uff0c\u5e76\u5206\u6790\u4e86\u68c0\u6d4b\u5668\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\uff0c\u5408\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u63d0\u9ad8\uff0c\u6076\u610f\u4f7f\u7528\uff08\u5982\u865a\u5047\u4fe1\u606f\uff09\u7684\u98ce\u9669\u589e\u52a0\uff0c\u4f46\u73b0\u6709MGT\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff0c\u5c06MGT\u98ce\u683c\u5411\u4eba\u7c7b\u4e66\u5199\u6587\u672c\uff08HWT\uff09\u9760\u62e2\uff0c\u4ee5\u6311\u6218\u68c0\u6d4b\u5668\u7684\u98ce\u683c\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u68c0\u6d4b\u5668\u5bb9\u6613\u88ab\u5c11\u91cf\u5bf9\u6297\u6837\u672c\u6b3a\u9a97\uff0c\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u5e76\u589e\u5f3a\u5176\u5bf9\u672a\u89c1\u9886\u57df\u6587\u672c\u7684\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.24866", "pdf": "https://arxiv.org/pdf/2505.24866", "abs": "https://arxiv.org/abs/2505.24866", "authors": ["Xinqi Xiong", "Prakrut Patel", "Qingyuan Fan", "Amisha Wadhwa", "Sarathy Selvam", "Xiao Guo", "Luchao Qi", "Xiaoming Liu", "Roni Sengupta"], "title": "TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of talking-head deepfake generation fueled by advanced\ngenerative models has elevated the realism of synthetic videos to a level that\nposes substantial risks in domains such as media, politics, and finance.\nHowever, current benchmarks for deepfake talking-head detection fail to reflect\nthis progress, relying on outdated generators and offering limited insight into\nmodel robustness and generalization. We introduce TalkingHeadBench, a\ncomprehensive multi-model multi-generator benchmark and curated dataset\ndesigned to evaluate the performance of state-of-the-art detectors on the most\nadvanced generators. Our dataset includes deepfakes synthesized by leading\nacademic and commercial models and features carefully constructed protocols to\nassess generalization under distribution shifts in identity and generator\ncharacteristics. We benchmark a diverse set of existing detection methods,\nincluding CNNs, vision transformers, and temporal models, and analyze their\nrobustness and generalization capabilities. In addition, we provide error\nanalysis using Grad-CAM visualizations to expose common failure modes and\ndetector biases. TalkingHeadBench is hosted on\nhttps://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to\nall data splits and protocols. Our benchmark aims to accelerate research\ntowards more robust and generalizable detection models in the face of rapidly\nevolving generative techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TalkingHeadBench\uff0c\u4e00\u4e2a\u591a\u6a21\u578b\u591a\u751f\u6210\u5668\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u5728\u6700\u65b0\u751f\u6210\u5668\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u57fa\u51c6\u672a\u80fd\u53cd\u6620\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u8fdb\u6b65\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5b66\u672f\u548c\u5546\u4e1a\u6a21\u578b\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u534f\u8bae\u4ee5\u6d4b\u8bd5\u8eab\u4efd\u548c\u751f\u6210\u5668\u7279\u6027\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e86\u591a\u79cd\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982CNN\u3001\u89c6\u89c9Transformer\u548c\u65f6\u95f4\u6a21\u578b\uff09\uff0c\u5e76\u901a\u8fc7Grad-CAM\u53ef\u89c6\u5316\u5206\u6790\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u548c\u68c0\u6d4b\u5668\u504f\u5dee\u3002", "conclusion": "TalkingHeadBench\u65e8\u5728\u63a8\u52a8\u66f4\u9c81\u68d2\u548c\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u68c0\u6d4b\u6a21\u578b\u7814\u7a76\uff0c\u4ee5\u5e94\u5bf9\u5feb\u901f\u53d1\u5c55\u7684\u751f\u6210\u6280\u672f\u3002"}}
{"id": "2505.24525", "pdf": "https://arxiv.org/pdf/2505.24525", "abs": "https://arxiv.org/abs/2505.24525", "authors": ["Marcell Fekete", "Nathaniel R. Robinson", "Ernests Lavrinovics", "E. Djeride Jean-Baptiste", "Raj Dabre", "Johannes Bjerva", "Heather Lent"], "title": "Limited-Resource Adapters Are Regularizers, Not Linguists", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer from related high-resource languages is a\nwell-established strategy to enhance low-resource language technologies. Prior\nwork has shown that adapters show promise for, e.g., improving low-resource\nmachine translation (MT). In this work, we investigate an adapter souping\nmethod combined with cross-attention fine-tuning of a pre-trained MT model to\nleverage language transfer for three low-resource Creole languages, which\nexhibit relatedness to different language groups across distinct linguistic\ndimensions. Our approach improves performance substantially over baselines.\nHowever, we find that linguistic relatedness -- or even a lack thereof -- does\nnot covary meaningfully with adapter performance. Surprisingly, our\ncross-attention fine-tuning approach appears equally effective with randomly\ninitialized adapters, implying that the benefit of adapters in this setting\nlies in parameter regularization, and not in meaningful information transfer.\nWe provide analysis supporting this regularization hypothesis. Our findings\nunderscore the reality that neural language processing involves many success\nfactors, and that not all neural methods leverage linguistic knowledge in\nintuitive ways.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u9002\u914d\u5668\u878d\u5408\u548c\u8de8\u6ce8\u610f\u529b\u5fae\u8c03\u9884\u8bad\u7ec3\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4e09\u79cd\u4f4e\u8d44\u6e90\u514b\u91cc\u5965\u5c14\u8bed\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9002\u914d\u5668\u7684\u6027\u80fd\u4e0e\u8bed\u8a00\u76f8\u5173\u6027\u65e0\u5173\uff0c\u5176\u4f5c\u7528\u53ef\u80fd\u5728\u4e8e\u53c2\u6570\u6b63\u5219\u5316\u800c\u975e\u4fe1\u606f\u4f20\u9012\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8de8\u8bed\u8a00\u8fc1\u79fb\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u6280\u672f\uff0c\u7279\u522b\u662f\u9488\u5bf9\u514b\u91cc\u5965\u5c14\u8bed\u8fd9\u7c7b\u4e0e\u591a\u8bed\u8a00\u7fa4\u4f53\u76f8\u5173\u7684\u8bed\u8a00\u3002", "method": "\u91c7\u7528\u9002\u914d\u5668\u878d\u5408\u548c\u8de8\u6ce8\u610f\u529b\u5fae\u8c03\u9884\u8bad\u7ec3\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\uff0c\u5206\u6790\u9002\u914d\u5668\u6027\u80fd\u4e0e\u8bed\u8a00\u76f8\u5173\u6027\u7684\u5173\u7cfb\u3002", "result": "\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f46\u9002\u914d\u5668\u6027\u80fd\u4e0e\u8bed\u8a00\u76f8\u5173\u6027\u65e0\u5173\uff0c\u968f\u673a\u521d\u59cb\u5316\u7684\u9002\u914d\u5668\u540c\u6837\u6709\u6548\u3002", "conclusion": "\u9002\u914d\u5668\u7684\u4f5c\u7528\u53ef\u80fd\u5728\u4e8e\u53c2\u6570\u6b63\u5219\u5316\uff0c\u800c\u975e\u4fe1\u606f\u4f20\u9012\uff0c\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5e76\u4e0d\u603b\u662f\u76f4\u89c2\u5730\u5229\u7528\u8bed\u8a00\u77e5\u8bc6\u3002"}}
{"id": "2505.24867", "pdf": "https://arxiv.org/pdf/2505.24867", "abs": "https://arxiv.org/abs/2505.24867", "authors": ["Ujjwal Upadhyay", "Mukul Ranjan", "Zhiqiang Shen", "Mohamed Elhoseiny"], "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?", "categories": ["cs.CV", "cs.AI"], "comment": "Project page at https://timeblindness.github.io/", "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SpookyBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7eaf\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u4fe1\u606f\u6a21\u7cca\u65f6\u65e0\u6cd5\u6355\u6349\u7eaf\u65f6\u95f4\u6a21\u5f0f\uff0c\u800c\u4eba\u7c7b\u5374\u80fd\u9ad8\u6548\u8bc6\u522b\u3002", "method": "\u901a\u8fc7\u521b\u5efaSpookyBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u566a\u58f0\u5e27\u5e8f\u5217\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4eba\u7c7b\u51c6\u786e\u7387\u8d85\u8fc798%\uff0c\u800c\u5148\u8fdb\u6a21\u578b\u51c6\u786e\u7387\u4e3a0%\uff0c\u663e\u793a\u6a21\u578b\u5bf9\u65f6\u95f4\u7ebf\u7d22\u7684\u4f9d\u8d56\u4e0d\u8db3\u3002", "conclusion": "\u9700\u65b0\u67b6\u6784\u6216\u8bad\u7ec3\u8303\u5f0f\u4ee5\u89e3\u8026\u7a7a\u95f4\u4f9d\u8d56\uff0c\u63d0\u5347\u65f6\u95f4\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2505.24532", "pdf": "https://arxiv.org/pdf/2505.24532", "abs": "https://arxiv.org/abs/2505.24532", "authors": ["Ali Khoramfar", "Ali Ramezani", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti", "Majid Nili Ahmadabadi", "Heshaam Faili"], "title": "DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance", "categories": ["cs.CL"], "comment": null, "summary": "LLMs often excel on standard benchmarks but falter on real-world tasks. We\nintroduce DeepQuestion, a scalable automated framework that augments existing\ndatasets based on Bloom's taxonomy and creates novel questions that trace\noriginal solution paths to probe evaluative and creative skills. Extensive\nexperiments across ten open-source and proprietary models, covering both\ngeneral-purpose and reasoning LLMs, reveal substantial performance drops (even\nup to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in\ndeep reasoning. Our work highlights the need for cognitively diverse benchmarks\nto advance LLM progress. DeepQuestion and related datasets will be released\nupon acceptance of the paper.", "AI": {"tldr": "DeepQuestion\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7Bloom\u5206\u7c7b\u6cd5\u589e\u5f3a\u73b0\u6709\u6570\u636e\u96c6\u5e76\u521b\u5efa\u65b0\u95ee\u9898\uff0c\u63ed\u793aLLM\u5728\u9ad8\u7ea7\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "LLM\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528Bloom\u5206\u7c7b\u6cd5\u6269\u5c55\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u8ffd\u8e2a\u539f\u59cb\u89e3\u51b3\u8def\u5f84\u7684\u95ee\u9898\uff0c\u4ee5\u8bc4\u4f30LLM\u7684\u9ad8\u9636\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cLLM\u5728\u9ad8\u7ea7\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u51c6\u786e\u7387\u635f\u5931\u9ad8\u8fbe70%\uff09\uff0c\u8868\u660e\u5176\u5728\u6df1\u5ea6\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u9700\u8981\u591a\u6837\u5316\u7684\u8ba4\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u63a8\u52a8LLM\u53d1\u5c55\uff0cDeepQuestion\u53ca\u76f8\u5173\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2505.24869", "pdf": "https://arxiv.org/pdf/2505.24869", "abs": "https://arxiv.org/abs/2505.24869", "authors": ["Ce Zhang", "Yan-Bo Lin", "Ziyang Wang", "Mohit Bansal", "Gedas Bertasius"], "title": "SiLVR: A Simple Language-based Video Reasoning Framework", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.", "AI": {"tldr": "SiLVR\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u89c6\u9891\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5206\u89e3\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u7ed3\u5408\u591a\u611f\u5b98\u8f93\u5165\u548c\u81ea\u9002\u5e94\u4ee4\u724c\u51cf\u5c11\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u89c6\u9891\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u89c6\u9891\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u663e\u8457\u843d\u540e\u4e8e\u7eaf\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "SiLVR\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1) \u5c06\u539f\u59cb\u89c6\u9891\u8f6c\u6362\u4e3a\u57fa\u4e8e\u8bed\u8a00\u7684\u8868\u793a\uff08\u5982\u77ed\u7247\u6bb5\u5b57\u5e55\u548c\u97f3\u9891/\u8bed\u97f3\u5b57\u5e55\uff09\uff1b2) \u5c06\u8bed\u8a00\u63cf\u8ff0\u8f93\u5165\u5f3a\u5927\u7684\u63a8\u7406LLM\u5b8c\u6210\u4efb\u52a1\u3002\u91c7\u7528\u81ea\u9002\u5e94\u4ee4\u724c\u51cf\u5c11\u65b9\u6848\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u591a\u611f\u5b98\u8f93\u5165\u3002", "result": "SiLVR\u5728Video-MME\uff08\u957f\uff09\u3001Video-MMMU\uff08\u7406\u89e3\uff09\u3001Video-MMLU\u3001CGBench\u548cEgoLife\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u62a5\u544a\u7ed3\u679c\u3002\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5927\u7684\u63a8\u7406LLM\u80fd\u6709\u6548\u805a\u5408\u89c6\u9891\u3001\u8bed\u97f3\u548c\u97f3\u9891\u7684\u591a\u611f\u5b98\u4fe1\u606f\u3002", "conclusion": "SiLVR\u6846\u67b6\u7b80\u5355\u3001\u6a21\u5757\u5316\u4e14\u65e0\u9700\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u590d\u6742\u89c6\u9891\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u63a8\u7406LLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.24538", "pdf": "https://arxiv.org/pdf/2505.24538", "abs": "https://arxiv.org/abs/2505.24538", "authors": ["Orfeas Menis Mastromichalakis", "Jason Liartis", "Kristina Rose", "Antoine Isaac", "Giorgos Stamou"], "title": "Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections", "categories": ["cs.CL"], "comment": null, "summary": "Cultural Heritage (CH) data hold invaluable knowledge, reflecting the\nhistory, traditions, and identities of societies, and shaping our understanding\nof the past and present. However, many CH collections contain outdated or\noffensive descriptions that reflect historical biases. CH Institutions (CHIs)\nface significant challenges in curating these data due to the vast scale and\ncomplexity of the task. To address this, we develop an AI-powered tool that\ndetects offensive terms in CH metadata and provides contextual insights into\ntheir historical background and contemporary perception. We leverage a\nmultilingual vocabulary co-created with marginalized communities, researchers,\nand CH professionals, along with traditional NLP techniques and Large Language\nModels (LLMs). Available as a standalone web app and integrated with major CH\nplatforms, the tool has processed over 7.9 million records, contextualizing the\ncontentious terms detected in their metadata. Rather than erasing these terms,\nour approach seeks to inform, making biases visible and providing actionable\ninsights for creating more inclusive and accessible CH collections.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u6d4b\u6587\u5316\u9057\u4ea7\u6570\u636e\u4e2d\u7684\u5192\u72af\u6027\u672f\u8bed\uff0c\u5e76\u63d0\u4f9b\u5386\u53f2\u80cc\u666f\u548c\u5f53\u4ee3\u89c6\u89d2\u7684\u89c1\u89e3\u3002", "motivation": "\u6587\u5316\u9057\u4ea7\u6570\u636e\u4e2d\u5b58\u5728\u8fc7\u65f6\u6216\u5192\u72af\u6027\u63cf\u8ff0\uff0c\u53cd\u6620\u4e86\u5386\u53f2\u504f\u89c1\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u591a\u8bed\u8a00\u8bcd\u6c47\u5e93\u3001\u4f20\u7edfNLP\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u53ef\u96c6\u6210\u5230\u4e3b\u8981\u6587\u5316\u9057\u4ea7\u5e73\u53f0\u4e2d\u3002", "result": "\u5de5\u5177\u5df2\u5904\u7406\u8d85\u8fc7790\u4e07\u6761\u8bb0\u5f55\uff0c\u6210\u529f\u8bc6\u522b\u5e76\u63d0\u4f9b\u4e86\u5192\u72af\u6027\u672f\u8bed\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "conclusion": "\u8be5\u5de5\u5177\u65e8\u5728\u63ed\u793a\u504f\u89c1\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u800c\u975e\u76f4\u63a5\u5220\u9664\u672f\u8bed\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u5305\u5bb9\u548c\u53ef\u8bbf\u95ee\u7684\u6587\u5316\u9057\u4ea7\u6536\u85cf\u3002"}}
{"id": "2505.24870", "pdf": "https://arxiv.org/pdf/2505.24870", "abs": "https://arxiv.org/abs/2505.24870", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "AI": {"tldr": "GenSpace\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u56fe\u50cf\u751f\u6210\u6a21\u578b3D\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u57fa\u51c6\u548c\u6d41\u7a0b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5bf9\u8c61\u653e\u7f6e\u3001\u5173\u7cfb\u548c\u6d4b\u91cf\u7b49\u7ec6\u8282\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524dAI\u56fe\u50cf\u751f\u6210\u6a21\u578b\u57283D\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGenSpace\u57fa\u51c6\u548c\u4e13\u7528\u8bc4\u4f30\u6d41\u7a0b\uff0c\u901a\u8fc7\u591a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u91cd\u5efa3D\u573a\u666f\u51e0\u4f55\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "AI\u6a21\u578b\u80fd\u751f\u6210\u89c6\u89c9\u5438\u5f15\u529b\u7684\u56fe\u50cf\uff0c\u4f46\u5728\u5bf9\u8c61\u653e\u7f6e\u3001\u5173\u7cfb\u548c\u6d4b\u91cf\u7b493D\u7ec6\u8282\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5f53\u524d\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7a7a\u95f4\u611f\u77e5\u4e0a\u7684\u4e09\u5927\u5c40\u9650\uff0c\u4e3a\u6539\u8fdb\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.24539", "pdf": "https://arxiv.org/pdf/2505.24539", "abs": "https://arxiv.org/abs/2505.24539", "authors": ["Celia Cintas", "Miriam Rateike", "Erik Miehling", "Elizabeth Daly", "Skyler Speakman"], "title": "Localizing Persona Representations in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a study on how and where personas -- defined by distinct sets of\nhuman characteristics, values, and beliefs -- are encoded in the representation\nspace of large language models (LLMs). Using a range of dimension reduction and\npattern recognition methods, we first identify the model layers that show the\ngreatest divergence in encoding these representations. We then analyze the\nactivations within a selected layer to examine how specific personas are\nencoded relative to others, including their shared and distinct embedding\nspaces. We find that, across multiple pre-trained decoder-only LLMs, the\nanalyzed personas show large differences in representation space only within\nthe final third of the decoder layers. We observe overlapping activations for\nspecific ethical perspectives -- such as moral nihilism and utilitarianism --\nsuggesting a degree of polysemy. In contrast, political ideologies like\nconservatism and liberalism appear to be represented in more distinct regions.\nThese findings help to improve our understanding of how LLMs internally\nrepresent information and can inform future efforts in refining the modulation\nof specific human traits in LLM outputs. Warning: This paper includes\npotentially offensive sample statements.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u4eba\u7269\u7279\u5f81\u7684\u7f16\u7801\u65b9\u5f0f\u53ca\u5176\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\uff0c\u53d1\u73b0\u4e0d\u540c\u4eba\u7269\u7279\u5f81\u5728\u89e3\u7801\u5668\u540e\u4e09\u5206\u4e4b\u4e00\u5c42\u4e2d\u5dee\u5f02\u6700\u5927\uff0c\u4e14\u67d0\u4e9b\u4f26\u7406\u89c2\u70b9\u5b58\u5728\u91cd\u53e0\uff0c\u800c\u653f\u6cbb\u610f\u8bc6\u5f62\u6001\u5219\u66f4\u4e3a\u72ec\u7acb\u3002", "motivation": "\u7406\u89e3LLMs\u5982\u4f55\u5185\u90e8\u7f16\u7801\u4e0d\u540c\u4eba\u7269\u7279\u5f81\uff08\u5982\u4ef7\u503c\u89c2\u3001\u4fe1\u4ef0\uff09\u53ca\u5176\u5206\u5e03\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u8f93\u51fa\u4e2d\u5bf9\u7279\u5b9a\u4eba\u7c7b\u7279\u5f81\u7684\u8c03\u63a7\u3002", "method": "\u91c7\u7528\u964d\u7ef4\u548c\u6a21\u5f0f\u8bc6\u522b\u65b9\u6cd5\uff0c\u5206\u6790LLMs\u5404\u5c42\u4e2d\u4eba\u7269\u7279\u5f81\u7684\u7f16\u7801\u5dee\u5f02\uff0c\u5e76\u7814\u7a76\u7279\u5b9a\u5c42\u4e2d\u6fc0\u6d3b\u6a21\u5f0f\u3002", "result": "\u4eba\u7269\u7279\u5f81\u5728\u89e3\u7801\u5668\u540e\u4e09\u5206\u4e4b\u4e00\u5c42\u4e2d\u5dee\u5f02\u663e\u8457\uff1b\u4f26\u7406\u89c2\u70b9\uff08\u5982\u9053\u5fb7\u865a\u65e0\u4e3b\u4e49\u548c\u529f\u5229\u4e3b\u4e49\uff09\u6709\u91cd\u53e0\uff0c\u653f\u6cbb\u610f\u8bc6\u5f62\u6001\uff08\u5982\u4fdd\u5b88\u4e3b\u4e49\u548c\u81ea\u7531\u4e3b\u4e49\uff09\u5219\u66f4\u72ec\u7acb\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5185\u90e8\u8868\u793a\u4fe1\u606f\u7684\u65b9\u5f0f\uff0c\u4e3a\u672a\u6765\u4f18\u5316\u6a21\u578b\u8f93\u51fa\u4e2d\u7279\u5b9a\u4eba\u7c7b\u7279\u5f81\u7684\u8c03\u63a7\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2505.24871", "pdf": "https://arxiv.org/pdf/2505.24871", "abs": "https://arxiv.org/abs/2505.24871", "authors": ["Yiqing Liang", "Jielin Qiu", "Wenhao Ding", "Zuxin Liu", "James Tompkin", "Mengdi Xu", "Mengzhou Xia", "Zhengzhong Tu", "Laixi Shi", "Jiacheng Zhu"], "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Webpage: https://modomodo-rl.github.io/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001LLM\u7684RLVR\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u96c6\u6df7\u5408\u7b56\u7565\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "RLVR\u5728\u591a\u6a21\u6001LLM\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u56e0\u4efb\u52a1\u591a\u6837\u6027\u548c\u76ee\u6807\u51b2\u7a81\u800c\u590d\u6742\u5316\uff0c\u9700\u4f18\u5316\u6570\u636e\u96c6\u6df7\u5408\u7b56\u7565\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6a21\u6001RLVR\u6846\u67b6\uff0c\u63d0\u51fa\u6570\u636e\u6df7\u5408\u7b56\u7565\u4ee5\u9884\u6d4b\u548c\u4f18\u5316RL\u5fae\u8c03\u7ed3\u679c\u3002", "result": "\u591a\u57dfRLVR\u8bad\u7ec3\u7ed3\u5408\u6df7\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u6700\u4f73\u6df7\u5408\u7b56\u7565\u4f7f\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53475.24%\u3002", "conclusion": "\u7cfb\u7edf\u5316\u7684\u6570\u636e\u96c6\u6df7\u5408\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001LLM\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aRLVR\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24544", "pdf": "https://arxiv.org/pdf/2505.24544", "abs": "https://arxiv.org/abs/2505.24544", "authors": ["Wei Zhong", "Manasa Bharadwaj", "Yixiao Wang", "Nikhil Verma", "Yipeng Ji", "Chul Lee"], "title": "Cross-Attention Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding.", "AI": {"tldr": "Beagle\u662f\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684Transformer\u89e3\u7801\u5668SD\u6a21\u578b\uff0c\u6027\u80fd\u4e0e\u9886\u5148\u7684\u81ea\u6ce8\u610f\u529bSD\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u7b80\u5316\u4e86\u67b6\u6784\u5e76\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684SD\u65b9\u6cd5\u4f9d\u8d56\u7d27\u5bc6\u8026\u5408\u7684\u81ea\u6ce8\u610f\u529bTransformer\u89e3\u7801\u5668\uff0c\u67b6\u6784\u590d\u6742\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u63d0\u51faBudget EAGLE (Beagle)\uff0c\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529bTransformer\u89e3\u7801\u5668\uff0c\u65e0\u9700\u8f85\u52a9\u7ec4\u4ef6\uff0c\u5e76\u63d0\u51faTwo-Stage Block-Attention Training\u65b9\u6cd5\u3002", "result": "Beagle\u5728\u591a\u4e2aLLM\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0eEAGLE-v2\u76f8\u5f53\u7684\u63a8\u7406\u52a0\u901f\u548c\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "Beagle\u4e3aSD\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u4e14\u9ad8\u6548\u7684\u67b6\u6784\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.24872", "pdf": "https://arxiv.org/pdf/2505.24872", "abs": "https://arxiv.org/abs/2505.24872", "authors": ["Zilin Xiao", "Jaywon Koo", "Siru Ouyang", "Jefferson Hernandez", "Yu Meng", "Vicente Ordonez"], "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker.", "AI": {"tldr": "ProxyThinker\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u6280\u672f\uff0c\u901a\u8fc7\u4fee\u6539\u89e3\u7801\u52a8\u6001\uff0c\u4f7f\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ee7\u627f\u5c0f\u578b\u6162\u601d\u8003\u89c6\u89c9\u63a8\u7406\u5668\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u80fd\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u3002", "method": "\u901a\u8fc7\u4eceRFT\u63a8\u7406\u5668\u7684\u8f93\u51fa\u5206\u5e03\u4e2d\u51cf\u53bb\u57fa\u7840\u6a21\u578b\u7684\u8f93\u51fa\u5206\u5e03\uff0cProxyThinker\u4fee\u6539\u89e3\u7801\u52a8\u6001\uff0c\u6fc0\u53d1\u6162\u601d\u8003\u63a8\u7406\u884c\u4e3a\u3002", "result": "ProxyThinker\u5728\u7a7a\u95f4\u3001\u6570\u5b66\u548c\u591a\u5b66\u79d1\u63a8\u7406\u7b49\u6311\u6218\u6027\u89c6\u89c9\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u63d0\u534738\u500d\u3002", "conclusion": "ProxyThinker\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24550", "pdf": "https://arxiv.org/pdf/2505.24550", "abs": "https://arxiv.org/abs/2505.24550", "authors": ["Xiaoang Xu", "Shuo Wang", "Xu Han", "Zhenghao Liu", "Huijia Wu", "Peipei Li", "Zhiyuan Liu", "Maosong Sun", "Zhaofeng He"], "title": "A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve superior performance by extending the\nthought length. However, a lengthy thinking trajectory leads to reduced\nefficiency. Most of the existing methods are stuck in the assumption of\noverthinking and attempt to reason efficiently by compressing the\nChain-of-Thought, but this often leads to performance degradation. To address\nthis problem, we introduce A*-Thought, an efficient tree search-based unified\nframework designed to identify and isolate the most essential thoughts from the\nextensive reasoning chains produced by these models. It formulates the\nreasoning process of LRMs as a search tree, where each node represents a\nreasoning span in the giant reasoning space. By combining the A* search\nalgorithm with a cost function specific to the reasoning path, it can\nefficiently compress the chain of thought and determine a reasoning path with\nhigh information density and low cost. In addition, we also propose a\nbidirectional importance estimation mechanism, which further refines this\nsearch process and enhances its efficiency beyond uniform sampling. Extensive\nexperiments on several advanced math tasks show that A*-Thought effectively\nbalances performance and efficiency over a huge search space. Specifically,\nA*-Thought can improve the performance of QwQ-32B by 2.39$\\times$ with\nlow-budget and reduce the length of the output token by nearly 50% with\nhigh-budget. The proposed method is also compatible with several other LRMs,\ndemonstrating its generalization capability. The code can be accessed at:\nhttps://github.com/AI9Stars/AStar-Thought.", "AI": {"tldr": "A*-Thought\u901a\u8fc7\u6811\u641c\u7d22\u6846\u67b6\u9ad8\u6548\u538b\u7f29\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff0c\u63d0\u5347\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u957f\u601d\u7ef4\u94fe\u5bfc\u81f4\u6548\u7387\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u56e0\u538b\u7f29\u601d\u7ef4\u94fe\u800c\u6027\u80fd\u964d\u4f4e\u3002", "method": "\u7ed3\u5408A*\u641c\u7d22\u7b97\u6cd5\u4e0e\u7279\u5b9a\u6210\u672c\u51fd\u6570\uff0c\u53cc\u5411\u91cd\u8981\u6027\u4f30\u8ba1\u673a\u5236\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\uff0cA*-Thought\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u5982QwQ-32B\u6027\u80fd\u63d0\u53472.39\u500d\uff09\uff0c\u5e76\u51cf\u5c11\u8f93\u51fa\u957f\u5ea650%\u3002", "conclusion": "A*-Thought\u5728\u6027\u80fd\u4e0e\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.24873", "pdf": "https://arxiv.org/pdf/2505.24873", "abs": "https://arxiv.org/abs/2505.24873", "authors": ["Bojia Zi", "Weixuan Peng", "Xianbiao Qi", "Jianan Wang", "Shihao Zhao", "Rong Xiao", "Kam-Fai Wong"], "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io.", "AI": {"tldr": "MiniMax-Remover\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u67b6\u6784\u548c\u6700\u5c0f\u5316\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u8f91\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u662f\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u5173\u952e\u5b50\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u5bf9\u8c61\u3001\u89c6\u89c9\u4f2a\u5f71\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u79fb\u9664\u6587\u672c\u8f93\u5165\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u4ee5\u7b80\u5316\u6a21\u578b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6700\u5c0f\u5316\u4f18\u5316\u7b56\u7565\u8fdb\u4e00\u6b65\u6539\u8fdb\u7f16\u8f91\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u97006\u4e2a\u91c7\u6837\u6b65\u9aa4\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u6548\u679c\uff0c\u4e14\u4e0d\u4f9d\u8d56CFG\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "MiniMax-Remover\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.24553", "pdf": "https://arxiv.org/pdf/2505.24553", "abs": "https://arxiv.org/abs/2505.24553", "authors": ["Ye Eun Chun", "Taeyoon Hwang", "Seung-won Hwang", "Byung-Hak Kim"], "title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding complex character relations is crucial for narrative analysis\nand efficient script evaluation, yet existing extraction methods often fail to\nhandle long-form narratives with nuanced interactions. To address this\nchallenge, we present CREFT, a novel sequential framework leveraging\nspecialized Large Language Model (LLM) agents. First, CREFT builds a base\ncharacter graph through knowledge distillation, then iteratively refines\ncharacter composition, relation extraction, role identification, and group\nassignments. Experiments on a curated Korean drama dataset demonstrate that\nCREFT significantly outperforms single-agent LLM baselines in both accuracy and\ncompleteness. By systematically visualizing character networks, CREFT\nstreamlines narrative comprehension and accelerates script review -- offering\nsubstantial benefits to the entertainment, publishing, and educational sectors.", "AI": {"tldr": "CREFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u957f\u7bc7\u53d9\u4e8b\u4e2d\u63d0\u53d6\u590d\u6742\u7684\u89d2\u8272\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4ee3\u7406LLM\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u957f\u7bc7\u53d9\u4e8b\u4e2d\u590d\u6742\u7684\u89d2\u8272\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u53d9\u4e8b\u5206\u6790\u548c\u5267\u672c\u8bc4\u4f30\u7684\u6548\u7387\u3002", "method": "CREFT\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6784\u5efa\u57fa\u7840\u89d2\u8272\u56fe\uff0c\u5e76\u8fed\u4ee3\u4f18\u5316\u89d2\u8272\u7ec4\u6210\u3001\u5173\u7cfb\u63d0\u53d6\u3001\u89d2\u8272\u8bc6\u522b\u548c\u5206\u7ec4\u5206\u914d\u3002", "result": "\u5728\u97e9\u56fd\u7535\u89c6\u5267\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCREFT\u5728\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u4ee3\u7406LLM\u57fa\u7ebf\u3002", "conclusion": "CREFT\u901a\u8fc7\u7cfb\u7edf\u53ef\u89c6\u5316\u89d2\u8272\u7f51\u7edc\uff0c\u63d0\u5347\u4e86\u53d9\u4e8b\u7406\u89e3\u6548\u7387\uff0c\u4e3a\u5a31\u4e50\u3001\u51fa\u7248\u548c\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2505.24875", "pdf": "https://arxiv.org/pdf/2505.24875", "abs": "https://arxiv.org/abs/2505.24875", "authors": ["Yu Zhang", "Yunqi Li", "Yifan Yang", "Rui Wang", "Yuqing Yang", "Dai Qi", "Jianmin Bao", "Dongdong Chen", "Chong Luo", "Lili Qiu"], "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.", "AI": {"tldr": "ReasonGen-R1\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6587\u672c\u63a8\u7406\u4f18\u5316\u56fe\u50cf\u751f\u6210\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u5728NLP\u4e2d\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5728\u751f\u6210\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8d4b\u4e88\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u5668\u6587\u672c\u63a8\u7406\u80fd\u529b\uff0c\u7136\u540e\u4f7f\u7528Group Relative Policy Optimization\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u5728GenEval\u3001DPG\u548cT2I\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReasonGen-R1\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "ReasonGen-R1\u901a\u8fc7\u6587\u672c\u63a8\u7406\u4f18\u5316\u56fe\u50cf\u751f\u6210\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.24554", "pdf": "https://arxiv.org/pdf/2505.24554", "abs": "https://arxiv.org/abs/2505.24554", "authors": ["Anna Sofia Lippolis", "Minh Davide Ragagni", "Paolo Ciancarini", "Andrea Giovanni Nuzzolese", "Valentina Presutti"], "title": "Bench4KE: Benchmarking Automated Competency Question Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The availability of Large Language Models (LLMs) presents a unique\nopportunity to reinvigorate research on Knowledge Engineering (KE) automation,\na trend already evident in recent efforts developing LLM-based methods and\ntools for the automatic generation of Competency Questions (CQs). However, the\nevaluation of these tools lacks standardisation. This undermines the\nmethodological rigour and hinders the replication and comparison of results. To\naddress this gap, we introduce Bench4KE, an extensible API-based benchmarking\nsystem for KE automation. Its first release focuses on evaluating tools that\ngenerate CQs automatically. CQs are natural language questions used by ontology\nengineers to define the functional requirements of an ontology. Bench4KE\nprovides a curated gold standard consisting of CQ datasets from four real-world\nontology projects. It uses a suite of similarity metrics to assess the quality\nof the CQs generated. We present a comparative analysis of four recent CQ\ngeneration systems, which are based on LLMs, establishing a baseline for future\nresearch. Bench4KE is also designed to accommodate additional KE automation\ntasks, such as SPARQL query generation, ontology testing and drafting. Code and\ndatasets are publicly available under the Apache 2.0 license.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Bench4KE\uff0c\u4e00\u4e2a\u7528\u4e8e\u77e5\u8bc6\u5de5\u7a0b\u81ea\u52a8\u5316\uff08KE\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u65e8\u5728\u6807\u51c6\u5316\u8bc4\u4f30LLM\u751f\u6210\u7684\u80dc\u4efb\u95ee\u9898\uff08CQs\uff09\u7684\u5de5\u5177\u3002", "motivation": "\u5f53\u524dLLM\u5728KE\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f71\u54cd\u7814\u7a76\u4e25\u8c28\u6027\u548c\u7ed3\u679c\u7684\u53ef\u6bd4\u6027\u3002", "method": "\u63d0\u51faBench4KE\u7cfb\u7edf\uff0c\u57fa\u4e8eAPI\u8bbe\u8ba1\uff0c\u4f7f\u7528\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u96c6\u548c\u76f8\u4f3c\u6027\u6307\u6807\u8bc4\u4f30CQ\u751f\u6210\u5de5\u5177\u3002", "result": "\u5bf9\u56db\u79cd\u57fa\u4e8eLLM\u7684CQ\u751f\u6210\u7cfb\u7edf\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "conclusion": "Bench4KE\u53ef\u6269\u5c55\u81f3\u5176\u4ed6KE\u81ea\u52a8\u5316\u4efb\u52a1\uff0c\u4ee3\u7801\u548c\u6570\u636e\u516c\u5f00\uff0c\u4fc3\u8fdb\u7814\u7a76\u6807\u51c6\u5316\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2505.24876", "pdf": "https://arxiv.org/pdf/2505.24876", "abs": "https://arxiv.org/abs/2505.24876", "authors": ["Tajamul Ashraf", "Amal Saqib", "Hanan Ghani", "Muhra AlMahri", "Yuhao Li", "Noor Ahsan", "Umair Nawaz", "Jean Lahoud", "Hisham Cholakkal", "Mubarak Shah", "Philip Torr", "Fahad Shahbaz Khan", "Rao Muhammad Anwer", "Salman Khan"], "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X", "AI": {"tldr": "Agent-X\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u4e2d\u5fc3\u4ee3\u7406\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u591a\u6b65\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5c40\u9650\u4e8e\u5408\u6210\u3001\u5355\u8f6e\u67e5\u8be2\u548c\u5355\u4e00\u89c6\u89c9\u6a21\u6001\uff0c\u65e0\u6cd5\u8bc4\u4f30\u771f\u5b9e\u573a\u666f\u4e2d\u591a\u6b65\u63a8\u7406\u7684\u8d28\u91cf\u3002", "method": "\u5f15\u5165Agent-X\u57fa\u51c6\uff0c\u5305\u542b828\u4e2a\u771f\u5b9e\u89c6\u89c9\u4efb\u52a1\uff0c\u8986\u76d6\u516d\u79cd\u73af\u5883\uff0c\u5e76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u3001\u6b65\u9aa4\u7ea7\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5373\u4f7f\u6700\u4f73\u6a21\u578b\uff08\u5982GPT\u3001Gemini\u548cQwen\u5bb6\u65cf\uff09\u5728\u591a\u6b65\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u4e5f\u4f4e\u4e8e50%\u3002", "conclusion": "Agent-X\u63ed\u793a\u4e86\u5f53\u524dLMM\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2505.24561", "pdf": "https://arxiv.org/pdf/2505.24561", "abs": "https://arxiv.org/abs/2505.24561", "authors": ["Ioannis Tsiamas", "David Dale", "Marta R. Costa-juss\u00e0"], "title": "Improving Language and Modality Transfer in Translation by Character-level Modeling", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Current translation systems, despite being highly multilingual, cover only 5%\nof the world's languages. Expanding language coverage to the long-tail of\nlow-resource languages requires data-efficient methods that rely on\ncross-lingual and cross-modal knowledge transfer. To this end, we propose a\ncharacter-based approach to improve adaptability to new languages and\nmodalities. Our method leverages SONAR, a multilingual fixed-size embedding\nspace with different modules for encoding and decoding. We use a\nteacher-student approach with parallel translation data to obtain a\ncharacter-level encoder. Then, using ASR data, we train a lightweight adapter\nto connect a massively multilingual CTC ASR model (MMS), to the character-level\nencoder, potentially enabling speech translation from 1,000+ languages.\nExperimental results in text translation for 75 languages on FLORES+\ndemonstrate that our character-based approach can achieve better language\ntransfer than traditional subword-based models, especially outperforming them\nin low-resource settings, and demonstrating better zero-shot generalizability\nto unseen languages. Our speech adaptation, maximizing knowledge transfer from\nthe text modality, achieves state-of-the-art results in speech-to-text\ntranslation on the FLEURS benchmark on 33 languages, surpassing previous\nsupervised and cascade models, albeit being a zero-shot model with minimal\nsupervision from ASR data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u7b26\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u80fd\u529b\uff0c\u5e76\u5728\u6587\u672c\u548c\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7ffb\u8bd1\u7cfb\u7edf\u4ec5\u8986\u76d65%\u7684\u8bed\u8a00\uff0c\u9700\u6269\u5c55\u81f3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4f9d\u8d56\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u5229\u7528SONAR\u591a\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\uff0c\u91c7\u7528\u5e08\u751f\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u5b57\u7b26\u7ea7\u7f16\u7801\u5668\uff0c\u7ed3\u5408ASR\u6570\u636e\u9002\u914d\u8bed\u97f3\u7ffb\u8bd1\u3002", "result": "\u572875\u79cd\u8bed\u8a00\u7684\u6587\u672c\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5b50\u8bcd\u6a21\u578b\uff0c\u8bed\u97f3\u7ffb\u8bd1\u572833\u79cd\u8bed\u8a00\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "\u5b57\u7b26\u7ea7\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u548c\u96f6-shot\u573a\u666f\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2505.24877", "pdf": "https://arxiv.org/pdf/2505.24877", "abs": "https://arxiv.org/abs/2505.24877", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "categories": ["cs.CV"], "comment": "Website: https://nvlabs.github.io/AdaHuman", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "AI": {"tldr": "AdaHuman\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u53ef\u52a8\u753b\u76843D\u865a\u62df\u5f62\u8c61\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u8282\u548c\u52a8\u753b\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u9ad8\u7ec6\u8282\u3001\u9002\u7528\u4e8e\u52a8\u753b\u76843D\u865a\u62df\u5f62\u8c61\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "AdaHuman\u7ed3\u5408\u4e86\u4e24\u79cd\u521b\u65b0\u6280\u672f\uff1a1\uff09\u57fa\u4e8e\u59ff\u6001\u76843D\u8054\u5408\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u548c3D\u9ad8\u65af\u91cd\u5efa\uff1b2\uff09\u7ec4\u5408\u5f0f3D\u9ad8\u65af\u7ec6\u5316\u6a21\u5757\uff0c\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u7ec6\u5316\u589e\u5f3a\u5c40\u90e8\u7ec6\u8282\u3002", "result": "AdaHuman\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u56fe\u50cf\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u6807\u51c6\u5316A\u59ff\u6001\u865a\u62df\u5f62\u8c61\u3002", "conclusion": "AdaHuman\u4e3a\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u53ef\u52a8\u753b\u76843D\u865a\u62df\u5f62\u8c61\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2505.24575", "pdf": "https://arxiv.org/pdf/2505.24575", "abs": "https://arxiv.org/abs/2505.24575", "authors": ["Hyuntak Kim", "Byung-Hak Kim"], "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main track of ACL 2025", "summary": "Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs. We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning. Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence. (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries. Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30.0% improvement in BERTScore (F1) across books, movies, and TV\nscripts. These results demonstrate the effectiveness of multi-agent LLMs in\nhandling long-form content, offering a scalable approach for structured\nsummarization in diverse storytelling domains.", "AI": {"tldr": "NexusSum\u662f\u4e00\u4e2a\u591a\u4ee3\u7406LLM\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u7bc7\u5e45\u53d9\u4e8b\u6587\u672c\u7684\u6458\u8981\u751f\u6210\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d41\u6c34\u7ebf\u5904\u7406\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u957f\u7bc7\u5e45\u53d9\u4e8b\u6587\u672c\uff08\u5982\u4e66\u7c4d\u3001\u7535\u5f71\u3001\u7535\u89c6\u5267\u672c\uff09\u7684\u6458\u8981\u751f\u6210\u9700\u8981\u6355\u6349\u590d\u6742\u60c5\u8282\u3001\u89d2\u8272\u4e92\u52a8\u548c\u4e3b\u9898\u8fde\u8d2f\u6027\uff0c\u73b0\u6709LLM\u96be\u4ee5\u80dc\u4efb\u3002", "method": "1. \u5bf9\u8bdd\u5230\u63cf\u8ff0\u7684\u8f6c\u6362\uff1a\u6807\u51c6\u5316\u89d2\u8272\u5bf9\u8bdd\u548c\u63cf\u8ff0\u6587\u672c\u683c\u5f0f\uff1b2. \u5206\u5c42\u591aLLM\u6458\u8981\uff1a\u7ed3\u6784\u5316\u6d41\u6c34\u7ebf\u4f18\u5316\u5206\u5757\u5904\u7406\u548c\u63a7\u5236\u8f93\u51fa\u957f\u5ea6\u3002", "result": "\u5728\u4e66\u7c4d\u3001\u7535\u5f71\u548c\u7535\u89c6\u5267\u672c\u4e0a\uff0cBERTScore\uff08F1\uff09\u63d0\u5347\u9ad8\u8fbe30.0%\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u3002", "conclusion": "\u591a\u4ee3\u7406LLM\u80fd\u6709\u6548\u5904\u7406\u957f\u7bc7\u5e45\u5185\u5bb9\uff0c\u4e3a\u591a\u6837\u5316\u53d9\u4e8b\u9886\u57df\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7ed3\u6784\u5316\u6458\u8981\u65b9\u6cd5\u3002"}}
{"id": "2505.24581", "pdf": "https://arxiv.org/pdf/2505.24581", "abs": "https://arxiv.org/abs/2505.24581", "authors": ["Omer Nacar", "Anis Koubaa", "Serry Sibaee", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "title": "GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training", "categories": ["cs.CL"], "comment": null, "summary": "Semantic textual similarity (STS) is a critical task in natural language\nprocessing (NLP), enabling applications in retrieval, clustering, and\nunderstanding semantic relationships between texts. However, research in this\narea for the Arabic language remains limited due to the lack of high-quality\ndatasets and pre-trained models. This scarcity of resources has restricted the\naccurate evaluation and advance of semantic similarity in Arabic text. This\npaper introduces General Arabic Text Embedding (GATE) models that achieve\nstate-of-the-art performance on the Semantic Textual Similarity task within the\nMTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid\nloss training approach with Arabic triplet datasets for Natural Language\nInference, which are essential for enhancing model performance in tasks that\ndemand fine-grained semantic understanding. GATE outperforms larger models,\nincluding OpenAI, with a 20-25% performance improvement on STS benchmarks,\neffectively capturing the unique semantic nuances of Arabic.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GATE\u6a21\u578b\uff0c\u4e00\u79cd\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7684\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\uff08STS\uff09\u7684\u5148\u8fdb\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408Matryoshka\u8868\u793a\u5b66\u4e60\u548c\u6df7\u5408\u635f\u5931\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u5728\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u800c\u53d7\u9650\uff0cGATE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "GATE\u91c7\u7528Matryoshka\u8868\u793a\u5b66\u4e60\u548c\u6df7\u5408\u635f\u5931\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u963f\u62c9\u4f2f\u8bed\u4e09\u5143\u7ec4\u6570\u636e\u96c6\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u3002", "result": "GATE\u5728STS\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5305\u62ecOpenAI\u5728\u5185\u7684\u66f4\u5927\u6a21\u578b\uff0c\u6027\u80fd\u63d0\u534720-25%\u3002", "conclusion": "GATE\u6a21\u578b\u6210\u529f\u6355\u6349\u963f\u62c9\u4f2f\u8bed\u7684\u72ec\u7279\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bedSTS\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23872", "pdf": "https://arxiv.org/pdf/2505.23872", "abs": "https://arxiv.org/abs/2505.23872", "authors": ["Anam Hashmi", "Julia Dietlmeier", "Kathleen M. Curran", "Noel E. O'Connor"], "title": "Parameter-Free Bio-Inspired Channel Attention for Enhanced Cardiac MRI Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "presented at the 28th UK Conference on Medical Image Understanding\n  and Analysis - MIUA, 24 - 26 July 2024", "summary": "Attention is a fundamental component of the human visual recognition system.\nThe inclusion of attention in a convolutional neural network amplifies relevant\nvisual features and suppresses the less important ones. Integrating attention\nmechanisms into convolutional neural networks enhances model performance and\ninterpretability. Spatial and channel attention mechanisms have shown\nsignificant advantages across many downstream tasks in medical imaging. While\nexisting attention modules have proven to be effective, their design often\nlacks a robust theoretical underpinning. In this study, we address this gap by\nproposing a non-linear attention architecture for cardiac MRI reconstruction\nand hypothesize that insights from ecological principles can guide the\ndevelopment of effective and efficient attention mechanisms. Specifically, we\ninvestigate a non-linear ecological difference equation that describes\nsingle-species population growth to devise a parameter-free attention module\nsurpassing current state-of-the-art parameter-free methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6001\u5b66\u539f\u7406\u7684\u975e\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u7528\u4e8e\u5fc3\u810fMRI\u91cd\u5efa\uff0c\u5176\u53c2\u6570\u81ea\u7531\u8bbe\u8ba1\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u6a21\u5757\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u751f\u6001\u5b66\u539f\u7406\u53ef\u80fd\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u5355\u7269\u79cd\u79cd\u7fa4\u589e\u957f\u7684\u975e\u7ebf\u6027\u751f\u6001\u5dee\u5f02\u65b9\u7a0b\uff0c\u8bbe\u8ba1\u53c2\u6570\u81ea\u7531\u7684\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u6a21\u5757\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u53c2\u6570\u81ea\u7531\u65b9\u6cd5\u3002", "conclusion": "\u751f\u6001\u5b66\u539f\u7406\u53ef\u6709\u6548\u6307\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.24593", "pdf": "https://arxiv.org/pdf/2505.24593", "abs": "https://arxiv.org/abs/2505.24593", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "The interpretability of Mixture-of-Experts (MoE) models, especially those\nwith heterogeneous designs, remains underexplored. Existing attribution methods\nfor dense models fail to capture dynamic routing-expert interactions in sparse\nMoE architectures. To address this issue, we propose a cross-level attribution\nalgorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,\nMixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results\nshow MoE models achieve 37% higher per-layer efficiency via a \"mid-activation,\nlate-amplification\" pattern: early layers screen experts, while late layers\nrefine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\"\nframework--shared experts handle general tasks (entity recognition), while\nrouted experts specialize in domain-specific processing (geographic\nattributes). Semantic-driven routing is evidenced by strong correlations\nbetween attention heads and experts (r=0.68), enabling task-aware coordination.\nNotably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates\nexpert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10\nexperts) through shared expert redundancy, whereas shallow OLMoE suffers severe\ndegradation (76% drop). Task sensitivity further guides design: core-sensitive\ntasks (geography) require concentrated expertise, while distributed-tolerant\ntasks (object attributes) leverage broader participation. These insights\nadvance MoE interpretability, offering principles to balance efficiency,\nspecialization, and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5c42\u5f52\u56e0\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u7a00\u758fMoE\u67b6\u6784\u7684\u52a8\u6001\u8def\u7531-\u4e13\u5bb6\u4ea4\u4e92\uff0c\u63ed\u793a\u4e86MoE\u6a21\u578b\u7684\u6548\u7387\u548c\u534f\u4f5c\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7a00\u758fMoE\u67b6\u6784\u4e2d\u7684\u52a8\u6001\u8def\u7531-\u4e13\u5bb6\u4ea4\u4e92\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8de8\u5c42\u5f52\u56e0\u7b97\u6cd5\uff0c\u5bf9\u6bd4\u5206\u6790\u7a00\u758fMoE\u67b6\u6784\uff08\u5982Qwen 1.5-MoE\uff09\u4e0e\u5bc6\u96c6\u6a21\u578b\uff08\u5982Qwen 1.5-7B\uff09\u3002", "result": "MoE\u6a21\u578b\u901a\u8fc7\u201c\u4e2d\u671f\u6fc0\u6d3b\u3001\u540e\u671f\u653e\u5927\u201d\u6a21\u5f0f\u5b9e\u73b037%\u7684\u5c42\u6548\u7387\u63d0\u5347\uff0c\u5e76\u63ed\u793a\u4e86\u201c\u57fa\u7840-\u7ec6\u5316\u201d\u6846\u67b6\u548c\u8bed\u4e49\u9a71\u52a8\u8def\u7531\u3002", "conclusion": "\u7814\u7a76\u4e3aMoE\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u5e73\u8861\u6548\u7387\u3001\u4e13\u4e1a\u5316\u548c\u9c81\u68d2\u6027\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2505.23884", "pdf": "https://arxiv.org/pdf/2505.23884", "abs": "https://arxiv.org/abs/2505.23884", "authors": ["Tianyuan Zhang", "Sai Bi", "Yicong Hong", "Kai Zhang", "Fujun Luan", "Songlin Yang", "Kalyan Sunkavalli", "William T. Freeman", "Hao Tan"], "title": "Test-Time Training Done Right", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "32 pages, 11 figures", "summary": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right", "AI": {"tldr": "LaCT\u901a\u8fc7\u5927\u5757\u66f4\u65b0\uff082K\u81f31M\u6807\u8bb0\uff09\u63d0\u5347\u786c\u4ef6\u5229\u7528\u7387\u548c\u72b6\u6001\u5bb9\u91cf\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709TTT\u65b9\u6cd5\u56e0\u5c0f\u6279\u91cf\u66f4\u65b0\u5bfc\u81f4\u786c\u4ef6\u5229\u7528\u7387\u4f4e\uff0c\u96be\u4ee5\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u3002", "method": "\u91c7\u7528\u5927\u5757\u66f4\u65b0\uff08LaCT\uff09\uff0c\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\u548c\u72b6\u6001\u5bb9\u91cf\uff0c\u652f\u6301\u590d\u6742\u4f18\u5316\u5668\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5305\u62ec14B\u53c2\u6570\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u767e\u4e07\u6807\u8bb0\u4e0a\u4e0b\u6587\u7684\u65b0\u89c6\u56fe\u5408\u6210\u3002", "conclusion": "LaCT\u4e3a\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2505.24609", "pdf": "https://arxiv.org/pdf/2505.24609", "abs": "https://arxiv.org/abs/2505.24609", "authors": ["Patawee Prakrankamanant", "Shinji Watanabe", "Ekapol Chuangsuwanich"], "title": "Explainable Depression Detection using Masked Hard Instance Mining", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the critical need for improved explainability in\ntext-based depression detection. While offering predictive outcomes, current\nsolutions often overlook the understanding of model predictions which can\nhinder trust in the system. We propose the use of Masked Hard Instance Mining\n(MHIM) to enhance the explainability in the depression detection task. MHIM\nstrategically masks attention weights within the model, compelling it to\ndistribute attention across a wider range of salient features. We evaluate MHIM\non two datasets representing distinct languages: Thai (Thai-Maywe) and English\n(DAIC-WOZ). Our results demonstrate that MHIM significantly improves\nperformance in terms of both prediction accuracy and explainability metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMHIM\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6587\u672c\u6291\u90c1\u75c7\u68c0\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u63a9\u853d\u6ce8\u610f\u529b\u6743\u91cd\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u5173\u952e\u7279\u5f81\u7684\u5173\u6ce8\u3002", "motivation": "\u5f53\u524d\u6291\u90c1\u75c7\u68c0\u6d4b\u6a21\u578b\u867d\u7136\u80fd\u63d0\u4f9b\u9884\u6d4b\u7ed3\u679c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u9884\u6d4b\u7684\u89e3\u91ca\u6027\uff0c\u5f71\u54cd\u4e86\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u91c7\u7528\u63a9\u853d\u786c\u5b9e\u4f8b\u6316\u6398\uff08MHIM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u853d\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u8feb\u4f7f\u6a21\u578b\u5173\u6ce8\u66f4\u5e7f\u6cdb\u7684\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u6cf0\u8bed\uff08Thai-Maywe\uff09\u548c\u82f1\u8bed\uff08DAIC-WOZ\uff09\u6570\u636e\u96c6\u4e0a\uff0cMHIM\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MHIM\u65b9\u6cd5\u5728\u6291\u90c1\u75c7\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.23916", "pdf": "https://arxiv.org/pdf/2505.23916", "abs": "https://arxiv.org/abs/2505.23916", "authors": ["Charles Bricout", "Samira Ebrahimi Kahou", "Sylvain Bouix"], "title": "Estimating Head Motion in Structural MRI Using a Deep Neural Network Trained on Synthetic Artifacts", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI)\nand can bias automated neuroanatomical metrics such as cortical thickness.\nManual review cannot objectively quantify motion in anatomical scans, and\nexisting automated approaches often require specialized hardware or rely on\nunbalanced noisy training data. Here, we train a 3D convolutional neural\nnetwork to estimate motion severity using only synthetically corrupted volumes.\nWe validate our method with one held-out site from our training cohort and with\n14 fully independent datasets, including one with manual ratings, achieving a\nrepresentative $R^2 = 0.65$ versus manual labels and significant\nthickness-motion correlations in 12/15 datasets. Furthermore, our predicted\nmotion correlates with subject age in line with prior studies. Our approach\ngeneralizes across scanner brands and protocols, enabling objective, scalable\nmotion assessment in structural MRI studies without prospective motion\ncorrection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30MRI\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\uff0c\u65e0\u9700\u4f9d\u8d56\u4e13\u4e1a\u786c\u4ef6\u6216\u566a\u58f0\u6570\u636e\u3002", "motivation": "MRI\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\u4f1a\u5f71\u54cd\u795e\u7ecf\u89e3\u5256\u5b66\u6307\u6807\u7684\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u786c\u4ef6\u4f9d\u8d56\u6216\u6570\u636e\u566a\u58f0\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5408\u6210\u635f\u574f\u7684MRI\u4f53\u79ef\u8bad\u7ec33D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u8fd0\u52a8\u4e25\u91cd\u7a0b\u5ea6\u3002", "result": "\u5728\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4e0e\u4eba\u5de5\u6807\u7b7e\u7684R\u00b2\u4e3a0.65\uff0c\u4e14\u4e0e\u5e74\u9f84\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u540c\u54c1\u724c\u548c\u534f\u8bae\u7684MRI\u8bbe\u5907\uff0c\u4e3a\u7ed3\u6784MRI\u7814\u7a76\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u8bc4\u4f30\u65b9\u6848\u3002"}}
{"id": "2505.24613", "pdf": "https://arxiv.org/pdf/2505.24613", "abs": "https://arxiv.org/abs/2505.24613", "authors": ["Daniela Occhipinti", "Marco Guerini", "Malvina Nissim"], "title": "When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation", "categories": ["cs.CL"], "comment": null, "summary": "Endowing dialogue agents with persona information has proven to significantly\nimprove the consistency and diversity of their generations. While much focus\nhas been placed on aligning dialogues with provided personas, the adaptation to\nthe interlocutor's profile remains largely underexplored. In this work, we\ninvestigate three key aspects: (1) a model's ability to align responses with\nboth the provided persona and the interlocutor's; (2) its robustness when\ndealing with familiar versus unfamiliar interlocutors and topics, and (3) the\nimpact of additional fine-tuning on specific persona-based dialogues. We\nevaluate dialogues generated with diverse speaker pairings and topics, framing\nthe evaluation as an author identification task and employing both\nLLM-as-a-judge and human evaluations. By systematically masking or disclosing\ninformation about the interlocutor, we assess its impact on dialogue\ngeneration. Results show that access to the interlocutor's persona improves the\nrecognition of the target speaker, while masking it does the opposite. Although\nmodels generalise well across topics, they struggle with unfamiliar\ninterlocutors. Finally, we found that in zero-shot settings, LLMs often copy\nbiographical details, facilitating identification but trivialising the task.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5bf9\u8bdd\u6a21\u578b\u5982\u4f55\u5229\u7528\u5bf9\u8bdd\u53cc\u65b9\u7684\u4eba\u7269\u4fe1\u606f\u751f\u6210\u66f4\u4e00\u81f4\u548c\u591a\u6837\u5316\u7684\u56de\u590d\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u5bf9\u964c\u751f\u5bf9\u8bdd\u8005\u548c\u8bdd\u9898\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5bf9\u8bdd\u4e0e\u63d0\u4f9b\u7684\u4eba\u7269\u4fe1\u606f\u5bf9\u9f50\uff0c\u4f46\u5bf9\u5bf9\u8bdd\u8005\u4e2a\u4eba\u4fe1\u606f\u7684\u9002\u5e94\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5728\u751f\u6210\u5bf9\u8bdd\u65f6\u5bf9\u53cc\u65b9\u4eba\u7269\u4fe1\u606f\u7684\u5229\u7528\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u8bdd\u9898\u548c\u5bf9\u8bdd\u8005\u4e0b\u7684\u8868\u73b0\uff0c\u91c7\u7528LLM\u548c\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5229\u7528\u5bf9\u8bdd\u8005\u4eba\u7269\u4fe1\u606f\u80fd\u63d0\u5347\u76ee\u6807\u8bf4\u8bdd\u8005\u7684\u8bc6\u522b\u7387\uff0c\u4f46\u6a21\u578b\u5bf9\u964c\u751f\u5bf9\u8bdd\u8005\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u503e\u5411\u4e8e\u590d\u5236\u4eba\u7269\u4fe1\u606f\uff0c\u7b80\u5316\u4efb\u52a1\u4f46\u7f3a\u4e4f\u6df1\u5ea6\u3002"}}
{"id": "2505.23941", "pdf": "https://arxiv.org/pdf/2505.23941", "abs": "https://arxiv.org/abs/2505.23941", "authors": ["An Vo", "Khai-Nguyen Nguyen", "Mohammad Reza Taesiri", "Vy Tuong Dang", "Anh Totti Nguyen", "Daeyoung Kim"], "title": "Vision Language Models are Biased", "categories": ["cs.LG", "cs.CV"], "comment": "Code and qualitative examples are available at:\n  vlmsarebiased.github.io", "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5904\u7406\u6807\u51c6\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u8ba1\u6570\u548c\u8bc6\u522b\uff09\u65f6\uff0c\u53d7\u5176\u8bb0\u5fc6\u7684\u6d41\u884c\u77e5\u8bc6\u5f71\u54cd\uff0c\u8868\u73b0\u51fa\u5f3a\u70c8\u504f\u89c1\uff0c\u5bfc\u81f4\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bb0\u5fc6\u7684\u6d41\u884c\u77e5\u8bc6\u5982\u4f55\u5f71\u54cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6807\u51c6\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u504f\u89c1\u5bfc\u81f4\u7684\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5VLMs\u57287\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u8ba1\u6570\u4efb\u52a1\uff08\u5982\u6761\u7eb9\u8ba1\u6570\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d2\u5165\u63cf\u8ff0\u6027\u6587\u672c\u4ee5\u89c2\u5bdf\u5176\u5f71\u54cd\u3002", "result": "VLMs\u5728\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u5e73\u5747\u51c6\u786e\u7387\u4ec5\u4e3a17.05%\uff0c\u4e14\u63d2\u5165\u6587\u672c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5176\u8868\u73b0\u3002\u5373\u4f7f\u8981\u6c42\u6a21\u578b\u590d\u67e5\u6216\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7ec6\u8282\uff0c\u51c6\u786e\u7387\u63d0\u5347\u4e5f\u6709\u9650\uff08+2%\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86VLMs\u7684\u4e00\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6d4b\u8bd5VLM\u504f\u89c1\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002"}}
{"id": "2505.24615", "pdf": "https://arxiv.org/pdf/2505.24615", "abs": "https://arxiv.org/abs/2505.24615", "authors": ["Yan Liu", "Zonglin Yang", "Soujanya Poria", "Thanh-Son Nguyen", "Erik Cambria"], "title": "Harnessing Large Language Models for Scientific Novelty Detection", "categories": ["cs.CL", "H.4.0"], "comment": "15 pages, 3 figures, 3 tables", "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u79d1\u5b66\u65b0\u9896\u6027\u68c0\u6d4b\uff08ND\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u3002\u901a\u8fc7\u63d0\u53d6\u8bba\u6587\u5173\u7cfb\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u68c0\u7d22\u5668\u4ee5\u63d0\u5347\u65b0\u9896\u6027\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u79d1\u5b66\u9886\u57df\u5feb\u901f\u6269\u5f20\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u73b0\u6709NLP\u6280\u672f\u7684\u5c40\u9650\u6027\u963b\u788d\u4e86\u65b0\u9896\u6027\u68c0\u6d4b\u7684\u7814\u7a76\u3002", "method": "\u5229\u7528LLMs\u6784\u5efa\u6570\u636e\u96c6\uff0c\u63d0\u53d6\u8bba\u6587\u5173\u7cfb\u5e76\u603b\u7ed3\u4e3b\u8981\u601d\u60f3\uff1b\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u68c0\u7d22\u5668\u4ee5\u5bf9\u9f50\u76f8\u4f3c\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u79d1\u5b66\u65b0\u9896\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6570\u636e\u3002"}}
{"id": "2505.24030", "pdf": "https://arxiv.org/pdf/2505.24030", "abs": "https://arxiv.org/abs/2505.24030", "authors": ["Ziming Zhao", "ChengAo Shen", "Hanghang Tong", "Dongjin Song", "Zhigang Deng", "Qingsong Wen", "Jingchao Ni"], "title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Transformer-based models have gained increasing attention in time series\nresearch, driving interest in Large Language Models (LLMs) and foundation\nmodels for time series analysis. As the field moves toward multi-modality,\nLarge Vision Models (LVMs) are emerging as a promising direction. In the past,\nthe effectiveness of Transformer and LLMs in time series has been debated. When\nit comes to LVMs, a similar question arises: are LVMs truely useful for time\nseries analysis? To address it, we design and conduct the first principled\nstudy involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across\nboth high-level (classification) and low-level (forecasting) tasks, with\nextensive ablation analysis. Our findings indicate LVMs are indeed useful for\ntime series classification but face challenges in forecasting. Although\neffective, the contemporary best LVM forecasters are limited to specific types\nof LVMs and imaging methods, exhibit a bias toward forecasting periods, and\nhave limited ability to utilize long look-back windows. We hope our findings\ncould serve as a cornerstone for future research on LVM- and multimodal-based\nsolutions to different time series tasks.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u6a21\u578b\uff08LVMs\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0LVMs\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u89c6\u89c9\u6a21\u578b\uff08LVMs\uff09\u662f\u5426\u771f\u6b63\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u6db5\u76d64\u79cdLVMs\u30018\u79cd\u6210\u50cf\u65b9\u6cd5\u300118\u4e2a\u6570\u636e\u96c6\u548c26\u4e2a\u57fa\u7ebf\u7684\u5b9e\u9a8c\uff0c\u8986\u76d6\u5206\u7c7b\u548c\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u8fdb\u884c\u8be6\u7ec6\u6d88\u878d\u5206\u6790\u3002", "result": "LVMs\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u4f46\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4e14\u5f53\u524d\u6700\u4f73LVM\u9884\u6d4b\u5668\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "LVMs\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.24616", "pdf": "https://arxiv.org/pdf/2505.24616", "abs": "https://arxiv.org/abs/2505.24616", "authors": ["Nikita Martynov", "Anastasia Mordasheva", "Dmitriy Gorbetskiy", "Danil Astafurov", "Ulyana Isaeva", "Elina Basyrova", "Sergey Skachkov", "Victoria Berestova", "Nikolay Ivanov", "Valeriia Zanina", "Alena Fenogenova"], "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX", "categories": ["cs.CL", "cs.AI"], "comment": "179 pages", "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.", "AI": {"tldr": "POLLUX\u662f\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4fc4\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u80fd\u529b\uff0c\u63d0\u4f9b\u900f\u660e\u3001\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u8017\u65f6\u7684\u4eba\u529b\u5bf9\u6bd4\uff0c\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0cPOLLUX\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5b9a\u4e4935\u79cd\u4efb\u52a1\u7c7b\u578b\u7684\u8be6\u7ec6\u6807\u51c6\uff0c\u5f00\u53d1\u8bc4\u5206\u534f\u8bae\uff0c\u5e76\u8bad\u7ec3LLM\u8bc4\u4f30\u5668\uff087B\u548c32B\uff09\u8fdb\u884c\u751f\u6210\u8f93\u51fa\u7684\u8bc4\u4f30\u3002", "result": "POLLUX\u5305\u542b2,100\u4e2a\u624b\u52a8\u7f16\u5199\u7684\u63d0\u793a\uff0c\u8986\u76d6\u591a\u79cd\u751f\u6210\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u900f\u660e\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "POLLUX\u4e3aLLM\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u4eba\u529b\u8bc4\u4f30\u3002"}}
{"id": "2505.24038", "pdf": "https://arxiv.org/pdf/2505.24038", "abs": "https://arxiv.org/abs/2505.24038", "authors": ["L\u00e9o And\u00e9ol", "Luca Mossina", "Adrien Mazoyer", "S\u00e9bastien Gerchinovitz"], "title": "Conformal Object Detection by Sequential Risk Control", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": "28 pages, 11 figures", "summary": "Recent advances in object detectors have led to their adoption for industrial\nuses. However, their deployment in critical applications is hindered by the\ninherent lack of reliability of neural networks and the complex structure of\nobject detection models. To address these challenges, we turn to Conformal\nPrediction, a post-hoc procedure which offers statistical guarantees that are\nvalid for any dataset size, without requiring prior knowledge on the model or\ndata distribution. Our contribution is manifold: first, we formally define the\nproblem of Conformal Object Detection (COD) and introduce a novel method,\nSequential Conformal Risk Control (SeqCRC), that extends the statistical\nguarantees of Conformal Risk Control (CRC) to two sequential tasks with two\nparameters, as required in the COD setting. Then, we propose loss functions and\nprediction sets suited to applying CRC to different applications and\ncertification requirements. Finally, we present a conformal toolkit, enabling\nreplication and further exploration of our methods. Using this toolkit, we\nperform extensive experiments, yielding a benchmark that validates the\ninvestigated methods and emphasizes trade-offs and other practical\nconsequences.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eConformal Prediction\u7684\u65b0\u65b9\u6cd5SeqCRC\uff0c\u7528\u4e8e\u89e3\u51b3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7f3a\u4e4f\u53ef\u9760\u6027\uff0c\u4e14\u7ed3\u6784\u590d\u6742\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u7edf\u8ba1\u4fdd\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSequential Conformal Risk Control (SeqCRC)\u65b9\u6cd5\uff0c\u6269\u5c55CRC\u7684\u7edf\u8ba1\u4fdd\u8bc1\u81f3\u987a\u5e8f\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u9002\u5408\u4e0d\u540c\u5e94\u7528\u7684\u635f\u5931\u51fd\u6570\u548c\u9884\u6d4b\u96c6\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5de5\u5177\u5305\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "SeqCRC\u4e3a\u5173\u952e\u5e94\u7528\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.24619", "pdf": "https://arxiv.org/pdf/2505.24619", "abs": "https://arxiv.org/abs/2505.24619", "authors": ["Vittorio Torri", "Machteld J. Boonstra", "Marielle C. van de Veerdonk", "Deborah N. Kalkman", "Alicia Uijl", "Francesca Ieva", "Ameen Abu-Hanna", "Folkert W. Asselbergs", "Iacer Calixto"], "title": "Interpretable phenotyping of Heart Failure patients with Dutch discharge letters", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.3"], "comment": "43 pages, 8 figures", "summary": "Objective: Heart failure (HF) patients present with diverse phenotypes\naffecting treatment and prognosis. This study evaluates models for phenotyping\nHF patients based on left ventricular ejection fraction (LVEF) classes, using\nstructured and unstructured data, assessing performance and interpretability.\n  Materials and Methods: The study analyzes all HF hospitalizations at both\nAmsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105\nhospitalizations, 16,334 patients). Data from AMC were used for model training,\nand from VUmc for external validation. The dataset was unlabelled and included\ntabular clinical measurements and discharge letters. Silver labels for LVEF\nclasses were generated by combining diagnosis codes, echocardiography results,\nand textual mentions. Gold labels were manually annotated for 300 patients for\ntesting. Multiple Transformer-based (black-box) and Aug-Linear (white-box)\nmodels were trained and compared with baselines on structured and unstructured\ndata. To evaluate interpretability, two clinicians annotated 20 discharge\nletters by highlighting information they considered relevant for LVEF\nclassification. These were compared to SHAP and LIME explanations from\nblack-box models and the inherent explanations of Aug-Linear models.\n  Results: BERT-based and Aug-Linear models, using discharge letters alone,\nachieved the highest classification results (AUC=0.84 for BERT, 0.81 for\nAug-Linear on external validation), outperforming baselines. Aug-Linear\nexplanations aligned more closely with clinicians' explanations than post-hoc\nexplanations on black-box models.\n  Conclusions: Discharge letters emerged as the most informative source for\nphenotyping HF patients. Aug-Linear models matched black-box performance while\nproviding clinician-aligned interpretability, supporting their use in\ntransparent clinical decision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\uff08LVEF\uff09\u7684\u5fc3\u529b\u8870\u7aed\uff08HF\uff09\u60a3\u8005\u8868\u578b\u5206\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u53d1\u73b0\u51fa\u9662\u8bb0\u5f55\u662f\u6700\u5177\u4fe1\u606f\u91cf\u7684\u6570\u636e\u6e90\uff0cAug-Linear\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5fc3\u529b\u8870\u7aed\u60a3\u8005\u8868\u578b\u591a\u6837\uff0c\u5f71\u54cd\u6cbb\u7597\u548c\u9884\u540e\uff0c\u9700\u5f00\u53d1\u57fa\u4e8eLVEF\u7684\u5206\u7c7b\u6a21\u578b\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e862015-2023\u5e74\u4e24\u5bb6\u533b\u9662\u7684HF\u4f4f\u9662\u6570\u636e\uff0c\u4f7f\u7528BERT\u548cAug-Linear\u6a21\u578b\uff0c\u6bd4\u8f83\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "result": "BERT\u548cAug-Linear\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08AUC=0.84\u548c0.81\uff09\uff0cAug-Linear\u7684\u89e3\u91ca\u66f4\u7b26\u5408\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u3002", "conclusion": "\u51fa\u9662\u8bb0\u5f55\u662fHF\u8868\u578b\u5206\u7c7b\u7684\u6700\u4f73\u6570\u636e\u6e90\uff0cAug-Linear\u6a21\u578b\u517c\u5177\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u5408\u4e34\u5e8a\u900f\u660e\u51b3\u7b56\u3002"}}
{"id": "2505.24621", "pdf": "https://arxiv.org/pdf/2505.24621", "abs": "https://arxiv.org/abs/2505.24621", "authors": ["Utsav Maskey", "Chencheng Zhu", "Usman Naseem"], "title": "Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5bc6\u7801\u5206\u6790\u9886\u57df\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8bc4\u4f30\u5176\u5728\u52a0\u5bc6\u6587\u672c\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5176\u4f18\u7f3a\u70b9\u53ca\u5b89\u5168\u9690\u60a3\u3002", "motivation": "\u5bc6\u7801\u5206\u6790\u662f\u6570\u636e\u5b89\u5168\u548c\u52a0\u5bc6\u7684\u5173\u952e\u9886\u57df\uff0c\u4f46\u5c1a\u672a\u5728LLM\u8bc4\u4f30\u4e2d\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5316\u660e\u6587\u53ca\u5176\u52a0\u5bc6\u7248\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u591a\u4e2aLLM\u7684\u89e3\u5bc6\u51c6\u786e\u6027\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u4fa7\u4fe1\u9053\u901a\u4fe1\u4e2d\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u66b4\u9732\u4e86\u5176\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86LLM\u5728\u5b89\u5168\u9886\u57df\u7684\u53cc\u91cd\u7528\u9014\uff0c\u4e3aAI\u5b89\u5168\u4e0e\u5b89\u5168\u7684\u8ba8\u8bba\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.24062", "pdf": "https://arxiv.org/pdf/2505.24062", "abs": "https://arxiv.org/abs/2505.24062", "authors": ["Kamyar Barakati", "Yu Liu", "Hiroshi Funakubo", "Sergei V. Kalinin"], "title": "Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM", "categories": ["cond-mat.mtrl-sci", "cs.CV", "cs.LG", "physics.app-ph"], "comment": "17 pages, 6 figures", "summary": "Domain-wall dynamics in ferroelectric materials are strongly\nposition-dependent since each polar interface is locked into a unique local\nmicrostructure. This necessitates spatially resolved studies of the\nwall-pinning using scanning-probe microscopy techniques. The pinning centers\nand preexisting domain walls are usually sparse within image plane, precluding\nthe use of dense hyperspectral imaging modes and requiring time-consuming human\nexperimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$\nwere investigated to quantify the electric field driven dynamics of the\npolar-strain domain structures using ML-controlled automated Piezoresponse\nForce Microscopy. Analysis of 1500 switching events reveals that domain wall\ndisplacement depends not only on field parameters but also on the local\nferroelectric-ferroelastic configuration. For example, twin boundaries in\npolydomains regions like a$_1^-$/$c^+$ $\\parallel$ a$_2^-$/$c^-$ stay pinned up\nto a certain level of bias magnitude and change only marginally as the bias\nincreases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$\n$\\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics\non the possible ferroelectric and ferroelastic wall orientations, together with\nthe automated, high-throughput AFM workflow, can be distilled into a predictive\nmap that links domain configurations to pulse parameters. This\nmicrostructure-specific rule set forms the foundation for designing\nferroelectric memories.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u94c1\u7535\u6750\u6599\u4e2d\u7574\u58c1\u52a8\u529b\u5b66\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u63a7\u5236\u7684\u81ea\u52a8\u538b\u7535\u529b\u663e\u5fae\u955c\u6280\u672f\uff0c\u91cf\u5316\u4e86\u7535\u573a\u9a71\u52a8\u7684\u6781\u6027\u5e94\u53d8\u7574\u7ed3\u6784\u52a8\u6001\u884c\u4e3a\u3002", "motivation": "\u7574\u58c1\u52a8\u529b\u5b66\u5728\u94c1\u7535\u6750\u6599\u4e2d\u5177\u6709\u5f3a\u70c8\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u7814\u7a76\u7a00\u758f\u7684\u9489\u624e\u4e2d\u5fc3\u548c\u7574\u58c1\u3002", "method": "\u4f7f\u7528ML\u63a7\u5236\u7684\u81ea\u52a8\u538b\u7535\u529b\u663e\u5fae\u955c\u6280\u672f\uff0c\u5206\u6790\u4e86\u5927\u9762\u79ef\u5916\u5ef6PbTiO3\u8584\u819c\u4e2d1500\u4e2a\u5f00\u5173\u4e8b\u4ef6\u3002", "result": "\u7574\u58c1\u4f4d\u79fb\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u7535\u573a\u53c2\u6570\uff0c\u8fd8\u4e0e\u5c40\u90e8\u7684\u94c1\u7535-\u94c1\u5f39\u6784\u578b\u76f8\u5173\uff0c\u4f8b\u5982\u53cc\u6676\u754c\u5728\u7279\u5b9a\u504f\u538b\u4e0b\u4fdd\u6301\u9489\u624e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u7528\u4e8e\u6784\u5efa\u9884\u6d4b\u6027\u56fe\u8c31\uff0c\u4e3a\u8bbe\u8ba1\u94c1\u7535\u5b58\u50a8\u5668\u63d0\u4f9b\u5fae\u89c2\u7ed3\u6784\u7279\u5b9a\u7684\u89c4\u5219\u96c6\u3002"}}
{"id": "2505.24630", "pdf": "https://arxiv.org/pdf/2505.24630", "abs": "https://arxiv.org/abs/2505.24630", "authors": ["Junyi Li", "Hwee Tou Ng"], "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFSPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u4e8b\u5b9e\u9a8c\u8bc1\u51cf\u5c11LLMs\u5728RL\u5fae\u8c03\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0RL\u5fae\u8c03\u4f1a\u663e\u8457\u589e\u52a0LLMs\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5f71\u54cd\u63a8\u7406\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faFSPO\u7b97\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u63a8\u7406\u4e2d\u5f15\u5165\u4e8b\u5b9e\u9a8c\u8bc1\uff0c\u52a8\u6001\u8c03\u6574\u4f18\u52bf\u503c\u4ee5\u6fc0\u52b1\u4e8b\u5b9e\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFSPO\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\uff0c\u663e\u8457\u6539\u5584\u6a21\u578b\u53ef\u9760\u6027\u3002", "conclusion": "FSPO\u4e3aRL\u5fae\u8c03\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2505.24088", "pdf": "https://arxiv.org/pdf/2505.24088", "abs": "https://arxiv.org/abs/2505.24088", "authors": ["Chen Huang", "Skyler Seto", "Hadi Pouransari", "Mehrdad Farajtabar", "Raviteja Vemulapalli", "Fartash Faghri", "Oncel Tuzel", "Barry-John Theobald", "Josh Susskind"], "title": "Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025", "summary": "Vision foundation models pre-trained on massive data encode rich\nrepresentations of real-world concepts, which can be adapted to downstream\ntasks by fine-tuning. However, fine-tuning foundation models on one task often\nleads to the issue of concept forgetting on other tasks. Recent methods of\nrobust fine-tuning aim to mitigate forgetting of prior knowledge without\naffecting the fine-tuning performance. Knowledge is often preserved by matching\nthe original and fine-tuned model weights or feature pairs. However, such\npoint-wise matching can be too strong, without explicit awareness of the\nfeature neighborhood structures that encode rich knowledge as well. We propose\na novel regularization method Proxy-FDA that explicitly preserves the\nstructural knowledge in feature space. Proxy-FDA performs Feature Distribution\nAlignment (using nearest neighbor graphs) between the pre-trained and\nfine-tuned feature spaces, and the alignment is further improved by informative\nproxies that are generated dynamically to increase data diversity. Experiments\nshow that Proxy-FDA significantly reduces concept forgetting during\nfine-tuning, and we find a strong correlation between forgetting and a\ndistributional distance metric (in comparison to L2 distance). We further\ndemonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,\nfew-shot and continual tuning) and across different tasks like image\nclassification, captioning and VQA.", "AI": {"tldr": "Proxy-FDA\u662f\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\u548c\u52a8\u6001\u751f\u6210\u4ee3\u7406\u6765\u51cf\u5c11\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u6982\u5ff5\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5fae\u8c03\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u65f6\uff0c\u5e38\u5bfc\u81f4\u5176\u4ed6\u4efb\u52a1\u7684\u6982\u5ff5\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u70b9\u5bf9\u70b9\u5339\u914d\u4fdd\u7559\u77e5\u8bc6\uff0c\u4f46\u5ffd\u7565\u4e86\u7279\u5f81\u90bb\u57df\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faProxy-FDA\u65b9\u6cd5\uff0c\u5229\u7528\u6700\u8fd1\u90bb\u56fe\u5bf9\u9f50\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u751f\u6210\u4ee3\u7406\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eProxy-FDA\u663e\u8457\u51cf\u5c11\u6982\u5ff5\u9057\u5fd8\uff0c\u5e76\u53d1\u73b0\u9057\u5fd8\u4e0e\u5206\u5e03\u8ddd\u79bb\u5ea6\u91cf\u5f3a\u76f8\u5173\u3002", "conclusion": "Proxy-FDA\u5728\u591a\u79cd\u5fae\u8c03\u8bbe\u7f6e\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.24635", "pdf": "https://arxiv.org/pdf/2505.24635", "abs": "https://arxiv.org/abs/2505.24635", "authors": ["Jiahao Ying", "Wei Tang", "Yiran Zhao", "Yixin Cao", "Yu Rong", "Wenxuan Zhang"], "title": "Disentangling Language and Culture for Evaluating Multilingual Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLMs\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6587\u5316\u4e0e\u8bed\u8a00\u534f\u540c\u73b0\u8c61\uff0c\u5e76\u6311\u6218\u4e86LLMs\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4e00\u81f4\u7684\u89c2\u70b9\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u91cf\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u5bf9LLMs\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u89e3\u8bed\u8a00\u5a92\u4ecb\u548c\u6587\u5316\u80cc\u666f\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u6784\u5efa\u53cc\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u63a2\u9488\u5206\u6790\u795e\u7ecf\u5143\u6fc0\u6d3b\u60c5\u51b5\u3002", "result": "\u53d1\u73b0LLMs\u5728\u6587\u5316\u4e0e\u8bed\u8a00\u5bf9\u9f50\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u6bd4\u4f8b\u53ef\u4f5c\u4e3a\u591a\u8bed\u8a00\u6027\u80fd\u7684\u6f5c\u5728\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6587\u5316\u548c\u8bed\u8a00\u80cc\u666f\u5728LLMs\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u89c6\u89d2\u3002"}}
{"id": "2505.24134", "pdf": "https://arxiv.org/pdf/2505.24134", "abs": "https://arxiv.org/abs/2505.24134", "authors": ["Ricardo Baptista", "Andrew M. Stuart", "Son Tran"], "title": "A Mathematical Perspective On Contrastive Learning", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": "44 pages, 15 figures", "summary": "Multimodal contrastive learning is a methodology for linking different data\nmodalities; the canonical example is linking image and text data. The\nmethodology is typically framed as the identification of a set of encoders, one\nfor each modality, that align representations within a common latent space. In\nthis work, we focus on the bimodal setting and interpret contrastive learning\nas the optimization of (parameterized) encoders that define conditional\nprobability distributions, for each modality conditioned on the other,\nconsistent with the available data. This provides a framework for multimodal\nalgorithms such as crossmodal retrieval, which identifies the mode of one of\nthese conditional distributions, and crossmodal classification, which is\nsimilar to retrieval but includes a fine-tuning step to make it task specific.\n  The framework we adopt also gives rise to crossmodal generative models. This\nprobabilistic perspective suggests two natural generalizations of contrastive\nlearning: the introduction of novel probabilistic loss functions, and the use\nof alternative metrics for measuring alignment in the common latent space. We\nstudy these generalizations of the classical approach in the multivariate\nGaussian setting. In this context we view the latent space identification as a\nlow-rank matrix approximation problem. This allows us to characterize the\ncapabilities of loss functions and alignment metrics to approximate natural\nstatistics, such as conditional means and covariances; doing so yields novel\nvariants on contrastive learning algorithms for specific mode-seeking and for\ngenerative tasks. The framework we introduce is also studied through numerical\nexperiments on multivariate Gaussians, the labeled MNIST dataset, and on a data\nassimilation application arising in oceanography.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u89c6\u89d2\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7f16\u7801\u5668\u5b9a\u4e49\u6761\u4ef6\u6982\u7387\u5206\u5e03\uff0c\u652f\u6301\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u63a2\u8ba8\u4e86\u635f\u5931\u51fd\u6570\u548c\u5ea6\u91cf\u6807\u51c6\u7684\u65b0\u53d8\u4f53\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u901a\u5e38\u5173\u6ce8\u6a21\u6001\u95f4\u7684\u8868\u793a\u5bf9\u9f50\uff0c\u4f46\u7f3a\u4e4f\u6982\u7387\u89e3\u91ca\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6982\u7387\u6846\u67b6\u7edf\u4e00\u8de8\u6a21\u6001\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u7b97\u6cd5\u8bbe\u8ba1\u7a7a\u95f4\u3002", "method": "\u5c06\u5bf9\u6bd4\u5b66\u4e60\u89c6\u4e3a\u4f18\u5316\u5b9a\u4e49\u6761\u4ef6\u6982\u7387\u5206\u5e03\u7684\u7f16\u7801\u5668\uff0c\u63d0\u51fa\u57fa\u4e8e\u591a\u5143\u9ad8\u65af\u5047\u8bbe\u7684\u4f4e\u79e9\u77e9\u9635\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5728\u591a\u5143\u9ad8\u65af\u3001MNIST\u6570\u636e\u96c6\u548c\u6d77\u6d0b\u5b66\u6570\u636e\u540c\u5316\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6a21\u5f0f\u641c\u7d22\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u6982\u7387\u89c6\u89d2\u4e3a\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7406\u8bba\u652f\u6301\u548c\u7b97\u6cd5\u8bbe\u8ba1\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u4efb\u52a1\u3002"}}
{"id": "2505.24640", "pdf": "https://arxiv.org/pdf/2505.24640", "abs": "https://arxiv.org/abs/2505.24640", "authors": ["Jens-Joris Decorte", "Jeroen Van Hautte", "Chris Develder", "Thomas Demeester"], "title": "Efficient Text Encoders for Labor Market Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConTeXT-match\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6280\u80fd\u5206\u7c7b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6Skill-XL\u548c\u6539\u8fdb\u7684JobBERT V2\u6a21\u578b\u3002", "motivation": "\u52b3\u52a8\u529b\u5e02\u573a\u5206\u6790\u4f9d\u8d56\u4e8e\u4ece\u62db\u8058\u5e7f\u544a\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u548ctoken\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7\u53cc\u7f16\u7801\u5668\u6a21\u578bConTeXT-match\uff0c\u5e76\u5f15\u5165Skill-XL\u57fa\u51c6\u548cJobBERT V2\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6280\u80fd\u63d0\u53d6\u548c\u804c\u4f4d\u6807\u51c6\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u9002\u5408\u5927\u89c4\u6a21\u5b9e\u65f6\u52b3\u52a8\u529b\u5e02\u573a\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2505.24136", "pdf": "https://arxiv.org/pdf/2505.24136", "abs": "https://arxiv.org/abs/2505.24136", "authors": ["Ya\u015far Utku Al\u00e7alar", "Mehmet Ak\u00e7akaya"], "title": "Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "IEEE International Conference on Image Processing (ICIP), 2025", "summary": "Physics-driven deep learning (PD-DL) models have proven to be a powerful\napproach for improved reconstruction of rapid MRI scans. In order to train\nthese models in scenarios where fully-sampled reference data is unavailable,\nself-supervised learning has gained prominence. However, its application at\nhigh acceleration rates frequently introduces artifacts, compromising image\nfidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL\nnetworks via carefully-designed perturbations. In particular, we enhance the\nk-space masking idea of conventional self-supervised learning with a novel\nconsistency term that assesses the model's ability to accurately predict the\nadded perturbations in a sparse domain, leading to more reliable and\nartifact-free reconstructions. The results obtained from the fastMRI knee and\nbrain datasets show that the proposed training strategy effectively reduces\naliasing artifacts and mitigates noise amplification at high acceleration\nrates, outperforming state-of-the-art self-supervised methods both visually and\nquantitatively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6270\u52a8\u8bad\u7ec3\u7269\u7406\u9a71\u52a8\u6df1\u5ea6\u5b66\u4e60\uff08PD-DL\uff09\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9ad8\u52a0\u901f\u7387\u4e0b\u81ea\u76d1\u7763\u5b66\u4e60\u5f15\u5165\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u5728\u9ad8\u52a0\u901f\u7387MRI\u626b\u63cf\u4e2d\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u5e38\u5f15\u5165\u4f2a\u5f71\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u589e\u5f3ak\u7a7a\u95f4\u63a9\u853d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5f15\u5165\u4e00\u81f4\u6027\u9879\u8bc4\u4f30\u6a21\u578b\u5bf9\u7a00\u758f\u57df\u6270\u52a8\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728fastMRI\u819d\u90e8\u548c\u8111\u90e8\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u4f2a\u5f71\u548c\u566a\u58f0\u653e\u5927\uff0c\u89c6\u89c9\u548c\u5b9a\u91cf\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u9ad8\u52a0\u901f\u7387\u4e0b\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u548c\u65e0\u4f2a\u5f71\u7684\u91cd\u5efa\u3002"}}
{"id": "2505.24643", "pdf": "https://arxiv.org/pdf/2505.24643", "abs": "https://arxiv.org/abs/2505.24643", "authors": ["Juan Wisznia", "Cecilia Bola\u00f1os", "Juan Tollo", "Giovanni Marraffini", "Agust\u00edn Gianolini", "Noe Hsueh", "Luciano Del Corro"], "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u63a8\u7406\u7684\u65b0\u578b\u6392\u5e8f\u7b97\u6cd5\u5206\u6790\u6846\u67b6\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u6210\u5bf9\u6bd4\u8f83\u6210\u672c\u6a21\u578b\uff0c\u5f3a\u8c03\u6279\u5904\u7406\u548c\u7f13\u5b58\u4ee5\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6bd4\u8f83\u6b21\u6570\u7684\u6548\u7387\u6307\u6807\u5728LLM\u63a8\u7406\u6210\u672c\u4e3b\u5bfc\u65f6\u5931\u6548\uff0c\u9700\u91cd\u65b0\u5b9a\u4e49\u6210\u672c\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5206\u6790LLM\u63a8\u7406\u6210\u672c\uff0c\u63d0\u51fa\u6279\u5904\u7406\u548c\u7f13\u5b58\u7b49\u4f18\u5316\u7b56\u7565\u3002", "result": "\u4f20\u7edf\u6700\u4f18\u7b97\u6cd5\u5728LLM\u63a8\u7406\u6210\u672c\u4e3b\u5bfc\u65f6\u53ef\u80fd\u6548\u7387\u4e0b\u964d\u3002", "conclusion": "\u65b0\u6846\u67b6\u4e3aLLM\u63a8\u7406\u4e3b\u5bfc\u7684\u6392\u5e8f\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6548\u7387\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2505.24160", "pdf": "https://arxiv.org/pdf/2505.24160", "abs": "https://arxiv.org/abs/2505.24160", "authors": ["Junyu Chen", "Shuwen Wei", "Joel Honkamaa", "Pekka Marttinen", "Hang Zhang", "Min Liu", "Yichao Zhou", "Zuopeng Tan", "Zhuoyuan Wang", "Yi Wang", "Hongchao Zhou", "Shunbo Hu", "Yi Zhang", "Qian Tao", "Lukas F\u00f6rner", "Thomas Wendler", "Bailiang Jian", "Benedikt Wiestler", "Tim Hable", "Jin Kim", "Dan Ruan", "Frederic Madesta", "Thilo Sentker", "Wiebke Heyer", "Lianrui Zuo", "Yuwei Dai", "Jing Wu", "Jerry L. Prince", "Harrison Bai", "Yong Du", "Yihao Liu", "Alessa Hering", "Reuben Dorent", "Lasse Hansen", "Mattias P. Heinrich", "Aaron Carass"], "title": "Beyond the LUMIR challenge: The pathway to foundational registration models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image challenges have played a transformative role in advancing the\nfield, catalyzing algorithmic innovation and establishing new performance\nstandards across diverse clinical applications. Image registration, a\nfoundational task in neuroimaging pipelines, has similarly benefited from the\nLearn2Reg initiative. Building on this foundation, we introduce the Large-scale\nUnsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation\nbenchmark designed to assess and advance unsupervised brain MRI registration.\nDistinct from prior challenges that leveraged anatomical label maps for\nsupervision, LUMIR removes this dependency by providing over 4,000 preprocessed\nT1-weighted brain MRIs for training without any label maps, encouraging\nbiologically plausible deformation modeling through self-supervision. In\naddition to evaluating performance on 590 held-out test subjects, LUMIR\nintroduces a rigorous suite of zero-shot generalization tasks, spanning\nout-of-domain imaging modalities (e.g., FLAIR, T2-weighted, T2*-weighted),\ndisease populations (e.g., Alzheimer's disease), acquisition protocols (e.g.,\n9.4T MRI), and species (e.g., macaque brains). A total of 1,158 subjects and\nover 4,000 image pairs were included for evaluation. Performance was assessed\nusing both segmentation-based metrics (Dice coefficient, 95th percentile\nHausdorff distance) and landmark-based registration accuracy (target\nregistration error). Across both in-domain and zero-shot tasks, deep\nlearning-based methods consistently achieved state-of-the-art accuracy while\nproducing anatomically plausible deformation fields. The top-performing deep\nlearning-based models demonstrated diffeomorphic properties and inverse\nconsistency, outperforming several leading optimization-based methods, and\nshowing strong robustness to most domain shifts, the exception being a drop in\nperformance on out-of-domain contrasts.", "AI": {"tldr": "LUMIR\u6311\u6218\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u8111MRI\u56fe\u50cf\u914d\u51c6\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u8bc4\u4f30\u548c\u63a8\u8fdb\u914d\u51c6\u6280\u672f\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u7b7e\u5730\u56fe\u3002", "motivation": "\u63a8\u52a8\u65e0\u76d1\u7763\u8111MRI\u914d\u51c6\u6280\u672f\u7684\u53d1\u5c55\uff0c\u51cf\u5c11\u5bf9\u6807\u7b7e\u5730\u56fe\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u751f\u7269\u5b66\u5408\u7406\u7684\u53d8\u5f62\u5efa\u6a21\u3002", "method": "\u63d0\u4f9b\u8d85\u8fc74,000\u4e2a\u9884\u5904\u7406T1\u52a0\u6743\u8111MRI\u56fe\u50cf\u7528\u4e8e\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u96f6\u6837\u672c\u6cdb\u5316\u4efb\u52a1\uff0c\u6db5\u76d6\u591a\u79cd\u6210\u50cf\u6a21\u6001\u3001\u75be\u75c5\u7fa4\u4f53\u3001\u91c7\u96c6\u534f\u8bae\u548c\u7269\u79cd\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u57df\u5185\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u751f\u6210\u89e3\u5256\u5b66\u5408\u7406\u7684\u53d8\u5f62\u573a\uff0c\u5e76\u4f18\u4e8e\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "LUMIR\u6311\u6218\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u65e0\u76d1\u7763\u8111MRI\u914d\u51c6\u4e2d\u7684\u6f5c\u529b\uff0c\u5c3d\u7ba1\u5728\u57df\u5916\u5bf9\u6bd4\u5ea6\u4e0b\u6027\u80fd\u6709\u6240\u4e0b\u964d\u3002"}}
{"id": "2505.24646", "pdf": "https://arxiv.org/pdf/2505.24646", "abs": "https://arxiv.org/abs/2505.24646", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "title": "PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Semantic Text Embedding is a fundamental NLP task that encodes textual\ncontent into vector representations, where proximity in the embedding space\nreflects semantic similarity. While existing embedding models excel at\ncapturing general meaning, they often overlook ideological nuances, limiting\ntheir effectiveness in tasks that require an understanding of political bias.\nTo address this gap, we introduce PRISM, the first framework designed to\nProduce inteRpretable polItical biaS eMbeddings. PRISM operates in two key\nstages: (1) Controversial Topic Bias Indicator Mining, which systematically\nextracts fine-grained political topics and their corresponding bias indicators\nfrom weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding,\nwhich assigns structured bias scores to news articles based on their alignment\nwith these indicators. This approach ensures that embeddings are explicitly\ntied to bias-revealing dimensions, enhancing both interpretability and\npredictive power. Through extensive experiments on two large-scale datasets, we\ndemonstrate that PRISM outperforms state-of-the-art text embedding models in\npolitical bias classification while offering highly interpretable\nrepresentations that facilitate diversified retrieval and ideological analysis.\nThe source code is available at https://github.com/dukesun99/ACL-PRISM.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u89e3\u91ca\u7684\u653f\u6cbb\u504f\u89c1\u5d4c\u5165\uff0c\u901a\u8fc7\u63d0\u53d6\u7ec6\u7c92\u5ea6\u653f\u6cbb\u4e3b\u9898\u548c\u504f\u89c1\u6307\u6807\uff0c\u63d0\u5347\u5d4c\u5165\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u6355\u6349\u610f\u8bc6\u5f62\u6001\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u653f\u6cbb\u504f\u89c1\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "PRISM\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u4ece\u5f31\u6807\u7b7e\u65b0\u95fb\u6570\u636e\u4e2d\u63d0\u53d6\u653f\u6cbb\u4e3b\u9898\u548c\u504f\u89c1\u6307\u6807\uff1b2) \u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u4e3a\u65b0\u95fb\u6587\u7ae0\u5206\u914d\u7ed3\u6784\u5316\u504f\u89c1\u5206\u6570\u3002", "result": "PRISM\u5728\u653f\u6cbb\u504f\u89c1\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u8868\u793a\u3002", "conclusion": "PRISM\u901a\u8fc7\u663e\u5f0f\u5173\u8054\u504f\u89c1\u63ed\u793a\u7ef4\u5ea6\uff0c\u589e\u5f3a\u4e86\u5d4c\u5165\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u68c0\u7d22\u548c\u610f\u8bc6\u5f62\u6001\u5206\u6790\u3002"}}
{"id": "2505.24656", "pdf": "https://arxiv.org/pdf/2505.24656", "abs": "https://arxiv.org/abs/2505.24656", "authors": ["Dimitrios Damianos", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "title": "MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "In this work, we investigate the Meta PL unsupervised domain adaptation\nframework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage\nDomain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation\napproach that integrates self-supervised learning with semi-supervised\ntechniques. MSDA is designed to enhance the robustness and generalization of\nASR models, making them more adaptable to diverse conditions. It is\nparticularly effective for low-resource languages like Greek and in weakly\nsupervised scenarios where labeled data is scarce or noisy. Through extensive\nexperiments, we demonstrate that Meta PL can be applied effectively to ASR\ntasks, achieving state-of-the-art results, significantly outperforming\nstate-of-the-art methods, and providing more robust solutions for unsupervised\ndomain adaptation in ASR. Our ablations highlight the necessity of utilizing a\ncascading approach when combining self-supervision with self-training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSDA\u7684\u591a\u9636\u6bb5\u57df\u9002\u5e94\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u548c\u534a\u76d1\u7763\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86ASR\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u5f31\u76d1\u7763\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u5f31\u76d1\u7763\u573a\u666f\u4e0b\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u57df\u9002\u5e94\uff08MSDA\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u534a\u76d1\u7763\u6280\u672f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9002\u5e94\u6d41\u7a0b\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMSDA\u5728ASR\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u5f31\u76d1\u7763\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MSDA\u6846\u67b6\u4e3aASR\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u4e0e\u81ea\u8bad\u7ec3\u7ea7\u8054\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2505.24185", "pdf": "https://arxiv.org/pdf/2505.24185", "abs": "https://arxiv.org/abs/2505.24185", "authors": ["Yipan Wei", "Yuchen Zou", "Yapeng Li", "Bo Du"], "title": "Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated Multi-Task Learning (FMTL) enables multiple clients performing\nheterogeneous tasks without exchanging their local data, offering broad\npotential for privacy preserving multi-task collaboration. However, most\nexisting methods focus on building personalized models for each client and\nunable to support the aggregation of multiple heterogeneous tasks into a\nunified model. As a result, in real-world scenarios where task objectives,\nlabel spaces, and optimization paths vary significantly, conventional FMTL\nmethods struggle to achieve effective joint training. To address this\nchallenge, we propose FedDEA (Federated Decoupled Aggregation), an\nupdate-structure-aware aggregation method specifically designed for multi-task\nmodel integration. Our method dynamically identifies task-relevant dimensions\nbased on the response strength of local updates and enhances their optimization\neffectiveness through rescaling. This mechanism effectively suppresses\ncross-task interference and enables task-level decoupled aggregation within a\nunified global model. FedDEA does not rely on task labels or architectural\nmodifications, making it broadly applicable and deployment-friendly.\nExperimental results demonstrate that it can be easily integrated into various\nmainstream federated optimization algorithms and consistently delivers\nsignificant overall performance improvements on widely used NYUD-V2 and\nPASCAL-Context. These results validate the robustness and generalization\ncapabilities of FedDEA under highly heterogeneous task settings.", "AI": {"tldr": "FedDEA\u662f\u4e00\u79cd\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\u5e76\u91cd\u65b0\u8c03\u6574\u4f18\u5316\u6548\u679c\uff0c\u5b9e\u73b0\u4efb\u52a1\u7ea7\u89e3\u8026\u805a\u5408\uff0c\u63d0\u5347\u5f02\u6784\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u805a\u5408\u5f02\u6784\u4efb\u52a1\uff0cFedDEA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFedDEA\u65b9\u6cd5\uff0c\u57fa\u4e8e\u672c\u5730\u66f4\u65b0\u7684\u54cd\u5e94\u5f3a\u5ea6\u52a8\u6001\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u91cd\u65b0\u8c03\u6574\u4f18\u5316\u6548\u679c\u5b9e\u73b0\u4efb\u52a1\u7ea7\u89e3\u8026\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedDEA\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u4e3b\u6d41\u8054\u90a6\u4f18\u5316\u7b97\u6cd5\u4e2d\u3002", "conclusion": "FedDEA\u5728\u9ad8\u5ea6\u5f02\u6784\u4efb\u52a1\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.24671", "pdf": "https://arxiv.org/pdf/2505.24671", "abs": "https://arxiv.org/abs/2505.24671", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 18 figures", "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters).", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u5229\u7528\u591a\u4e2aLLM\u7684\u4e92\u8865\u4f18\u52bf\u63d0\u5347\u6587\u5316\u9002\u5e94\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u8fa9\u8bba\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6587\u5316\u7fa4\u4f53\u516c\u5e73\u6027\u3002", "motivation": "LLM\u9700\u8981\u9002\u5e94\u591a\u6837\u6587\u5316\u80cc\u666f\u4ee5\u670d\u52a1\u5168\u7403\u793e\u533a\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355LLM\u5355\u8f6e\u4ea4\u4e92\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591aLLM\u4f18\u52bf\u3002", "method": "\u5f15\u5165\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u4e24\u79cd\u53d8\u4f53\uff1a\u7eaf\u8fa9\u8bba\u548c\u52a8\u6001\u9009\u62e9\u81ea\u7701\u4e0e\u8fa9\u8bba\uff0c\u572875\u56fd\u793e\u4ea4\u793c\u4eea\u57fa\u51c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u8fa9\u8bba\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u6587\u5316\u516c\u5e73\u6027\uff0c\u5c0f\u6a21\u578b\uff087-9B\uff09\u901a\u8fc7\u8fa9\u8bba\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\uff0827B\uff09\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u662f\u63d0\u5347LLM\u6587\u5316\u9002\u5e94\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5bf9\u5c0f\u6a21\u578b\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2505.24190", "pdf": "https://arxiv.org/pdf/2505.24190", "abs": "https://arxiv.org/abs/2505.24190", "authors": ["Lan-Cuong Nguyen", "Quan Nguyen-Tri", "Bang Tran Khanh", "Dung D. Le", "Long Tran-Thanh", "Khoat Than"], "title": "Provably Improving Generalization of Few-Shot Models with Synthetic Data", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025. Our code will be released soon", "summary": "Few-shot image classification remains challenging due to the scarcity of\nlabeled training examples. Augmenting them with synthetic data has emerged as a\npromising way to alleviate this issue, but models trained on synthetic samples\noften face performance degradation due to the inherent gap between real and\nsynthetic distributions. To address this limitation, we develop a theoretical\nframework that quantifies the impact of such distribution discrepancies on\nsupervised learning, specifically in the context of image classification. More\nimportantly, our framework suggests practical ways to generate good synthetic\nsamples and to train a predictor with high generalization ability. Building\nupon this framework, we propose a novel theoretical-based algorithm that\nintegrates prototype learning to optimize both data partitioning and model\ntraining, effectively bridging the gap between real few-shot data and synthetic\ndata. Extensive experiments results show that our approach demonstrates\nsuperior performance compared to state-of-the-art methods, outperforming them\nacross multiple datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bf9\u56fe\u50cf\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u7684\u7b97\u6cd5\u6765\u4f18\u5316\u6570\u636e\u5212\u5206\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u56e0\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u5408\u6210\u6570\u636e\u867d\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f00\u53d1\u7406\u8bba\u6846\u67b6\u91cf\u5316\u5206\u5e03\u5dee\u5f02\u5f71\u54cd\uff0c\u63d0\u51fa\u57fa\u4e8e\u539f\u578b\u7684\u7b97\u6cd5\u4f18\u5316\u6570\u636e\u5212\u5206\u548c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u7406\u8bba\u6846\u67b6\u548c\u7b97\u6cd5\u6709\u6548\u7f29\u5c0f\u4e86\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2505.24672", "pdf": "https://arxiv.org/pdf/2505.24672", "abs": "https://arxiv.org/abs/2505.24672", "authors": ["Xiaorui Wu", "Xiaofeng Mao", "Fei Li", "Xin Zhang", "Xuanhong Li", "Chong Teng", "Donghong Ji", "Zhuang Li"], "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6TRIDENT\uff0c\u901a\u8fc7\u5206\u6790\u8bcd\u6c47\u591a\u6837\u6027\u3001\u6076\u610f\u610f\u56fe\u548c\u8d8a\u72f1\u7b56\u7565\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5bf9\u9f50\u6570\u636e\u96c6\u7684\u98ce\u9669\u8986\u76d6\u8303\u56f4\uff0c\u5e76\u751f\u6210\u4e24\u4e2a\u6570\u636e\u96c6TRIDENT-Core\u548cTRIDENT-Edge\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eTRIDENT-Edge\u5fae\u8c03\u7684\u6a21\u578b\u5728\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u548c\u653b\u51fb\u6210\u529f\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\u5728\u98ce\u9669\u8986\u76d6\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u5173\u6ce8\u8bcd\u6c47\u591a\u6837\u6027\u800c\u5ffd\u7565\u5176\u4ed6\u5173\u952e\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u6a21\u578b\u4ecd\u6613\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u6216\u88ab\u6076\u610f\u5229\u7528\u3002", "method": "\u63d0\u51faTRIDENT\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u89d2\u8272\u7684\u96f6\u6837\u672cLLM\u751f\u6210\u6280\u672f\uff0c\u6784\u5efa\u8986\u76d6\u4e09\u4e2a\u7ef4\u5ea6\u7684\u591a\u6837\u5316\u6307\u4ee4\u6570\u636e\u96c6\uff08TRIDENT-Core\u548cTRIDENT-Edge\uff09\uff0c\u5e76\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728Harm Score\u4e0a\u5e73\u5747\u964d\u4f4e14.29%\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e20%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "TRIDENT\u6846\u67b6\u548c\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u672a\u6765\u5bf9\u9f50\u6570\u636e\u96c6\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24305", "pdf": "https://arxiv.org/pdf/2505.24305", "abs": "https://arxiv.org/abs/2505.24305", "authors": ["Mingxu Zhang", "Xiaoqi Li", "Jiahui Xu", "Kaichen Zhou", "Hojin Bae", "Yan Shen", "Chuyan Xiong", "Jiaming Liu", "Hao Dong"], "title": "SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in 3D robotic manipulation have improved grasping of\neveryday objects, but transparent and specular materials remain challenging due\nto depth sensing limitations. While several 3D reconstruction and depth\ncompletion approaches address these challenges, they suffer from setup\ncomplexity or limited observation information utilization. To address this,\nleveraging the power of single view 3D object reconstruction approaches, we\npropose a training free framework SR3D that enables robotic grasping of\ntransparent and specular objects from a single view observation. Specifically,\ngiven single view RGB and depth images, SR3D first uses the external visual\nmodels to generate 3D reconstructed object mesh based on RGB image. Then, the\nkey idea is to determine the 3D object's pose and scale to accurately localize\nthe reconstructed object back into its original depth corrupted 3D scene.\nTherefore, we propose view matching and keypoint matching mechanisms,which\nleverage both the 2D and 3D's inherent semantic and geometric information in\nthe observation to determine the object's 3D state within the scene, thereby\nreconstructing an accurate 3D depth map for effective grasp detection.\nExperiments in both simulation and real world show the reconstruction\neffectiveness of SR3D.", "AI": {"tldr": "SR3D\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u89c6\u89d2RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u5b9e\u73b0\u900f\u660e\u548c\u955c\u9762\u7269\u4f53\u76843D\u91cd\u5efa\u4e0e\u6293\u53d6\u3002", "motivation": "\u900f\u660e\u548c\u955c\u9762\u6750\u6599\u56e0\u6df1\u5ea6\u611f\u77e5\u9650\u5236\u96be\u4ee5\u6293\u53d6\uff0c\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u6216\u4fe1\u606f\u5229\u7528\u7387\u4f4e\u3002", "method": "\u5229\u7528\u5916\u90e8\u89c6\u89c9\u6a21\u578b\u751f\u62103D\u7f51\u683c\uff0c\u901a\u8fc7\u89c6\u56fe\u548c\u5173\u952e\u70b9\u5339\u914d\u786e\u5b9a\u7269\u4f53\u4f4d\u59ff\uff0c\u91cd\u5efa\u51c6\u786e\u6df1\u5ea6\u56fe\u3002", "result": "\u4eff\u771f\u548c\u73b0\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SR3D\u7684\u91cd\u5efa\u6709\u6548\u6027\u3002", "conclusion": "SR3D\u4e3a\u900f\u660e\u548c\u955c\u9762\u7269\u4f53\u7684\u6293\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24680", "pdf": "https://arxiv.org/pdf/2505.24680", "abs": "https://arxiv.org/abs/2505.24680", "authors": ["Xinrui Chen", "Haoli Bai", "Tao Yuan", "Ruikang Liu", "Kang Zhao", "Xianzhi Yu", "Lu Hou", "Tian Guan", "Yonghong He", "Chun Yuan"], "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card.", "AI": {"tldr": "LinearPatch\u662f\u4e00\u79cd\u7b80\u5355\u5373\u63d2\u5373\u7528\u7684\u6280\u672f\uff0c\u901a\u8fc7Hadamard\u53d8\u6362\u548c\u901a\u9053\u7f29\u653e\u89e3\u51b3\u5c42\u526a\u679d\u540e\u7684\u6fc0\u6d3b\u5e45\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u526a\u679d\u540e\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c42\u526a\u679d\u65b9\u6cd5\u56e0\u6fc0\u6d3b\u5e45\u5ea6\u4e0d\u5339\u914d\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u63d0\u5347\u526a\u679d\u540e\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528Hadamard\u53d8\u6362\u6291\u5236\u5f02\u5e38\u503c\uff0c\u901a\u9053\u7f29\u653e\u5bf9\u9f50\u6fc0\u6d3b\u5e45\u5ea6\uff0c\u5e76\u5c06\u64cd\u4f5c\u878d\u5408\u4e3a\u5355\u4e00\u77e9\u9635\u4f5c\u4e3a\u8865\u4e01\u3002", "result": "\u5728LLaMA-3-8B\u4e0a\u526a\u679d5\u5c42\u540e\uff0c\u6027\u80fd\u4fdd\u755994.15%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd54%\uff1b\u8fdb\u4e00\u6b65\u4f18\u5316\u540e\u53ef\u63d0\u5347\u81f395.16%\u3002", "conclusion": "LinearPatch\u6709\u6548\u89e3\u51b3\u4e86\u5c42\u526a\u679d\u540e\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.24351", "pdf": "https://arxiv.org/pdf/2505.24351", "abs": "https://arxiv.org/abs/2505.24351", "authors": ["Peng Qi", "Wenxi Qu", "Tianliang Yao", "Haonan Ma", "Dylan Wintle", "Yinyi Lai", "Giorgos Papanastasiou", "Chengjia Wang"], "title": "A Novel Coronary Artery Registration Method Based on Super-pixel Particle Swarm Optimization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Percutaneous Coronary Intervention (PCI) is a minimally invasive procedure\nthat improves coronary blood flow and treats coronary artery disease. Although\nPCI typically requires 2D X-ray angiography (XRA) to guide catheter placement\nat real-time, computed tomography angiography (CTA) may substantially improve\nPCI by providing precise information of 3D vascular anatomy and status. To\nleverage real-time XRA and detailed 3D CTA anatomy for PCI, accurate multimodal\nimage registration of XRA and CTA is required, to guide the procedure and avoid\ncomplications. This is a challenging process as it requires registration of\nimages from different geometrical modalities (2D -> 3D and vice versa), with\nvariations in contrast and noise levels. In this paper, we propose a novel\nmultimodal coronary artery image registration method based on a swarm\noptimization algorithm, which effectively addresses challenges such as large\ndeformations, low contrast, and noise across these imaging modalities. Our\nalgorithm consists of two main modules: 1) preprocessing of XRA and CTA images\nseparately, and 2) a registration module based on feature extraction using the\nSteger and Superpixel Particle Swarm Optimization algorithms. Our technique was\nevaluated on a pilot dataset of 28 pairs of XRA and CTA images from 10 patients\nwho underwent PCI. The algorithm was compared with four state-of-the-art (SOTA)\nmethods in terms of registration accuracy, robustness, and efficiency. Our\nmethod outperformed the selected SOTA baselines in all aspects. Experimental\nresults demonstrate the significant effectiveness of our algorithm, surpassing\nthe previous benchmarks and proposes a novel clinical approach that can\npotentially have merit for improving patient outcomes in coronary artery\ndisease.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u4f18\u5316\u7b97\u6cd5\u7684\u591a\u6a21\u6001\u51a0\u72b6\u52a8\u8109\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86PCI\u624b\u672f\u4e2dXRA\u548cCTA\u56fe\u50cf\u7684\u914d\u51c6\u7cbe\u5ea6\u3002", "motivation": "PCI\u624b\u672f\u9700\u8981\u5b9e\u65f6XRA\u5f15\u5bfc\uff0c\u4f46CTA\u80fd\u63d0\u4f9b\u66f4\u7cbe\u786e\u76843D\u8840\u7ba1\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u51c6\u786e\u7684\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u4ee5\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u9884\u5904\u7406\u6a21\u5757\u548c\u57fa\u4e8eSteger\u53ca\u8d85\u50cf\u7d20\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u7684\u914d\u51c6\u6a21\u5757\uff0c\u89e3\u51b3\u4e862D-3D\u56fe\u50cf\u914d\u51c6\u4e2d\u7684\u53d8\u5f62\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u566a\u58f0\u95ee\u9898\u3002", "result": "\u572828\u5bf9XRA\u548cCTA\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u914d\u51c6\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86PCI\u624b\u672f\u7684\u4e34\u5e8a\u6548\u679c\uff0c\u6709\u671b\u6539\u5584\u51a0\u72b6\u52a8\u8109\u75be\u75c5\u60a3\u8005\u7684\u6cbb\u7597\u7ed3\u679c\u3002"}}
{"id": "2505.24683", "pdf": "https://arxiv.org/pdf/2505.24683", "abs": "https://arxiv.org/abs/2505.24683", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures", "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u56db\u79cdAI\u7ffb\u8bd1\u8d28\u91cf\u53cd\u9988\u673a\u5236\u5bf9\u7528\u6237\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9690\u5f0f\u53cd\u9988\uff08\u5c24\u5176\u662fQA\u8868\u683c\uff09\u5728\u51c6\u786e\u6027\u3001\u4f9d\u8d56\u6027\u548c\u7528\u6237\u4f53\u9a8c\u4e0a\u4f18\u4e8e\u663e\u5f0f\u53cd\u9988\u3002", "motivation": "\u968f\u7740AI\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u666e\u53ca\uff0c\u7528\u6237\u9700\u8981\u6709\u6548\u7684\u53cd\u9988\u673a\u5236\u6765\u8bc4\u4f30AI\u9884\u6d4b\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u65e0\u6cd5\u81ea\u884c\u5224\u65ad\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5728\u673a\u5668\u7ffb\u8bd1\u573a\u666f\u4e2d\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u53cd\u9988\u7c7b\u578b\uff1a\u663e\u5f0f\u53cd\u9988\uff08\u9519\u8bef\u9ad8\u4eae\u548cLLM\u89e3\u91ca\uff09\u548c\u9690\u5f0f\u53cd\u9988\uff08\u56de\u8bd1\u548cQA\u8868\u683c\uff09\u3002", "result": "\u9664\u9519\u8bef\u9ad8\u4eae\u5916\uff0c\u5176\u4ed6\u53cd\u9988\u5747\u663e\u8457\u63d0\u5347\u51b3\u7b56\u51c6\u786e\u6027\u548c\u9002\u5f53\u4f9d\u8d56\u6027\uff1b\u9690\u5f0f\u53cd\u9988\uff08\u5c24\u5176\u662fQA\u8868\u683c\uff09\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u9690\u5f0f\u53cd\u9988\u5728\u63d0\u5347\u7528\u6237\u51b3\u7b56\u548c\u4f53\u9a8c\u65b9\u9762\u4f18\u4e8e\u663e\u5f0f\u53cd\u9988\uff0cQA\u8868\u683c\u662f\u6700\u6709\u6548\u7684\u53cd\u9988\u5f62\u5f0f\u3002"}}
{"id": "2505.24407", "pdf": "https://arxiv.org/pdf/2505.24407", "abs": "https://arxiv.org/abs/2505.24407", "authors": ["Wenlong Jiao", "Binglong Li", "Wei Shang", "Ping Wang", "Dongwei Ren"], "title": "Efficient RAW Image Deblurring with Adaptive Frequency Modulation", "categories": ["eess.IV", "cs.CV"], "comment": "Preprint. Submitted to NeurIPS 2025", "summary": "Image deblurring plays a crucial role in enhancing visual clarity across\nvarious applications. Although most deep learning approaches primarily focus on\nsRGB images, which inherently lose critical information during the image signal\nprocessing pipeline, RAW images, being unprocessed and linear, possess superior\nrestoration potential but remain underexplored. Deblurring RAW images presents\nunique challenges, particularly in handling frequency-dependent blur while\nmaintaining computational efficiency. To address these issues, we propose\nFrequency Enhanced Network (FrENet), a framework specifically designed for\nRAW-to-RAW deblurring that operates directly in the frequency domain. We\nintroduce a novel Adaptive Frequency Positional Modulation module, which\ndynamically adjusts frequency components according to their spectral positions,\nthereby enabling precise control over the deblurring process. Additionally,\nfrequency domain skip connections are adopted to further preserve\nhigh-frequency details. Experimental results demonstrate that FrENet surpasses\nstate-of-the-art deblurring methods in RAW image deblurring, achieving\nsignificantly better restoration quality while maintaining high efficiency in\nterms of reduced MACs. Furthermore, FrENet's adaptability enables it to be\nextended to sRGB images, where it delivers comparable or superior performance\ncompared to methods specifically designed for sRGB data. The code will be\navailable at https://github.com/WenlongJiao/FrENet .", "AI": {"tldr": "FrENet\u662f\u4e00\u4e2a\u4e13\u4e3aRAW\u56fe\u50cf\u53bb\u6a21\u7cca\u8bbe\u8ba1\u7684\u9891\u57df\u589e\u5f3a\u7f51\u7edc\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u4f4d\u7f6e\u8c03\u5236\u6a21\u5757\u548c\u9891\u57df\u8df3\u8dc3\u8fde\u63a5\uff0c\u663e\u8457\u63d0\u5347\u53bb\u6a21\u7cca\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u5c3d\u7ba1RAW\u56fe\u50cf\u5177\u6709\u66f4\u5f3a\u7684\u6062\u590d\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8sRGB\u56fe\u50cf\uff0c\u5bfc\u81f4RAW\u56fe\u50cf\u53bb\u6a21\u7cca\u7814\u7a76\u4e0d\u8db3\u3002FrENet\u65e8\u5728\u89e3\u51b3\u9891\u57df\u6a21\u7cca\u5904\u7406\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\u3002", "method": "FrENet\u76f4\u63a5\u5728\u9891\u57df\u64cd\u4f5c\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u9891\u7387\u4f4d\u7f6e\u8c03\u5236\u6a21\u5757\u52a8\u6001\u8c03\u6574\u9891\u57df\u5206\u91cf\uff0c\u5e76\u7ed3\u5408\u9891\u57df\u8df3\u8dc3\u8fde\u63a5\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFrENet\u5728RAW\u56fe\u50cf\u53bb\u6a21\u7cca\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6062\u590d\u8d28\u91cf\u66f4\u9ad8\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u540c\u65f6\u53ef\u6269\u5c55\u81f3sRGB\u56fe\u50cf\u3002", "conclusion": "FrENet\u4e3aRAW\u56fe\u50cf\u53bb\u6a21\u7cca\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728sRGB\u56fe\u50cf\u4e0a\u7684\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2505.24688", "pdf": "https://arxiv.org/pdf/2505.24688", "abs": "https://arxiv.org/abs/2505.24688", "authors": ["Qinglin Zhu", "Runcong Zhao", "Hanqi Yan", "Yulan He", "Yudong Chen", "Lin Gui"], "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution.", "AI": {"tldr": "Soft Reasoning\u6846\u67b6\u901a\u8fc7\u5d4c\u5165\u6270\u52a8\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u542f\u53d1\u5f0f\u641c\u7d22\u7684\u4f9d\u8d56\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u56e0\u591a\u6837\u6027\u548c\u641c\u7d22\u6548\u7387\u4e0d\u8db3\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u5d4c\u5165\u6270\u52a8\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u76ee\u6807\u4f18\u5316\u5d4c\u5165\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "Soft Reasoning\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2505.24421", "pdf": "https://arxiv.org/pdf/2505.24421", "abs": "https://arxiv.org/abs/2505.24421", "authors": ["Abdul-mojeed Olabisi Ilyas", "Adeleke Maradesa", "Jamal Banzi", "Jianpan Huang", "Henry K. F. Mak", "Kannie W. Y. Chan"], "title": "pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation", "categories": ["eess.IV", "cs.CV"], "comment": "36 pages, 9 figures, 2 tables", "summary": "Medical imaging is critical for diagnostics, but clinical adoption of\nadvanced AI-driven imaging faces challenges due to patient variability, image\nartifacts, and limited model generalization. While deep learning has\ntransformed image analysis, 3D medical imaging still suffers from data scarcity\nand inconsistencies due to acquisition protocols, scanner differences, and\npatient motion. Traditional augmentation uses a single pipeline for all\ntransformations, disregarding the unique traits of each augmentation and\nstruggling with large data volumes.\n  To address these challenges, we propose a Multi-encoder Augmentation-Aware\nLearning (MEAL) framework that leverages four distinct augmentation variants\nprocessed through dedicated encoders. Three fusion strategies such as\nconcatenation (CC), fusion layer (FL), and adaptive controller block (BD) are\nintegrated to build multi-encoder models that combine augmentation-specific\nfeatures before decoding. MEAL-BD uniquely preserves augmentation-aware\nrepresentations, enabling robust, protocol-invariant feature learning.\n  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic\nResonance Imaging (MRI) translation study, MEAL-BD consistently achieved the\nbest performance on both unseen- and predefined-test data. On both geometric\ntransformations (like rotations and flips) and non-augmented inputs, MEAL-BD\noutperformed other competing methods, achieving higher mean peak\nsignal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)\nscores. These results establish MEAL as a reliable framework for preserving\nstructural fidelity and generalizing across clinically relevant variability. By\nreframing augmentation as a source of diverse, generalizable features, MEAL\nsupports robust, protocol-invariant learning, advancing clinically reliable\nmedical imaging solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMEAL\u7684\u591a\u7f16\u7801\u5668\u589e\u5f3a\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b33D\u533b\u5b66\u5f71\u50cf\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u534f\u8bae\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u56db\u79cd\u4e0d\u540c\u7684\u589e\u5f3a\u53d8\u4f53\u548c\u4e09\u79cd\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f71\u50cf\u7ffb\u8bd1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u4e2dAI\u9a71\u52a8\u7684\u533b\u5b66\u5f71\u50cf\u9762\u4e34\u60a3\u8005\u5dee\u5f02\u3001\u56fe\u50cf\u4f2a\u5f71\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u4f20\u7edf\u589e\u5f3a\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MEAL\u6846\u67b6\u5229\u7528\u56db\u79cd\u589e\u5f3a\u53d8\u4f53\uff0c\u901a\u8fc7\u4e13\u7528\u7f16\u7801\u5668\u5904\u7406\uff0c\u5e76\u7ed3\u5408\u4e09\u79cd\u878d\u5408\u7b56\u7565\uff08CC\u3001FL\u3001BD\uff09\u6784\u5efa\u591a\u7f16\u7801\u5668\u6a21\u578b\uff0c\u4fdd\u7559\u589e\u5f3a\u611f\u77e5\u7279\u5f81\u3002", "result": "\u5728CT\u5230T1\u52a0\u6743MRI\u7684\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0cMEAL-BD\u5728\u672a\u89c1\u6570\u636e\u548c\u9884\u5b9a\u4e49\u6d4b\u8bd5\u6570\u636e\u4e0a\u5747\u8868\u73b0\u6700\u4f73\uff0cPSNR\u548cSSIM\u5206\u6570\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "MEAL\u901a\u8fc7\u5c06\u589e\u5f3a\u89c6\u4e3a\u591a\u6837\u5316\u7279\u5f81\u7684\u6765\u6e90\uff0c\u652f\u6301\u9c81\u68d2\u4e14\u534f\u8bae\u4e0d\u53d8\u7684\u5b66\u4e60\uff0c\u4e3a\u4e34\u5e8a\u53ef\u9760\u7684\u533b\u5b66\u5f71\u50cf\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24689", "pdf": "https://arxiv.org/pdf/2505.24689", "abs": "https://arxiv.org/abs/2505.24689", "authors": ["Sander Land", "Catherine Arnett"], "title": "BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization", "categories": ["cs.CL"], "comment": "9 pages, 2 figures. For associated code, see\n  https://github.com/sanderland/script_bpe", "summary": "Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models,\nface challenges in multilingual settings, including penalization of non-Western\nscripts and the creation of tokens with partial UTF-8 sequences.\nPretokenization, often reliant on complex regular expressions, can also\nintroduce fragility and unexpected edge cases. We propose SCRIPT (Script\nCategory Representation in PreTokenization), a novel encoding scheme that\nbypasses UTF-8 byte conversion by using initial tokens based on Unicode script\nand category properties. This approach enables a simple, rule-based\npretokenization strategy that respects script boundaries, offering a robust\nalternative to pretokenization strategies based on regular expressions. We also\nintroduce and validate a constrained BPE merging strategy that enforces\ncharacter integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our\nexperiments demonstrate that SCRIPT-BPE achieves competitive compression while\neliminating encoding-based penalties for non-Latin-script languages.", "AI": {"tldr": "SCRIPT-BPE\u662f\u4e00\u79cd\u65b0\u7684\u7f16\u7801\u65b9\u6848\uff0c\u901a\u8fc7\u57fa\u4e8eUnicode\u811a\u672c\u548c\u7c7b\u522b\u5c5e\u6027\u7684\u521d\u59cb\u4ee4\u724c\uff0c\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e2dBPE\u5206\u8bcd\u5668\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u9c81\u68d2\u7684\u9884\u5206\u8bcd\u7b56\u7565\u3002", "motivation": "\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0cBPE\u5206\u8bcd\u5668\u5bf9\u975e\u897f\u65b9\u811a\u672c\u7684\u60e9\u7f5a\u548c\u90e8\u5206UTF-8\u5e8f\u5217\u4ee4\u724c\u7684\u521b\u5efa\u5b58\u5728\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u9884\u5206\u8bcd\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u5bb9\u6613\u5f15\u5165\u8106\u5f31\u6027\u548c\u8fb9\u7f18\u60c5\u51b5\u3002", "method": "\u63d0\u51faSCRIPT\u65b9\u6848\uff0c\u7ed5\u8fc7UTF-8\u5b57\u8282\u8f6c\u6362\uff0c\u4f7f\u7528Unicode\u811a\u672c\u548c\u7c7b\u522b\u5c5e\u6027\u8fdb\u884c\u521d\u59cb\u4ee4\u724c\u5316\uff0c\u5e76\u7ed3\u5408\u7ea6\u675fBPE\u5408\u5e76\u7b56\u7565\u4ee5\u4fdd\u6301\u5b57\u7b26\u5b8c\u6574\u6027\u3002", "result": "SCRIPT-BPE\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u538b\u7f29\u7387\u7684\u540c\u65f6\uff0c\u6d88\u9664\u4e86\u5bf9\u975e\u62c9\u4e01\u811a\u672c\u8bed\u8a00\u7684\u7f16\u7801\u60e9\u7f5a\u3002", "conclusion": "SCRIPT-BPE\u4e3a\u591a\u8bed\u8a00\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u4e14\u516c\u5e73\u7684\u5206\u8bcd\u65b9\u6848\u3002"}}
{"id": "2505.24424", "pdf": "https://arxiv.org/pdf/2505.24424", "abs": "https://arxiv.org/abs/2505.24424", "authors": ["Amit Peleg", "Naman Deep Singh", "Matthias Hein"], "title": "Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vision-language models like CLIP have demonstrated remarkable zero-shot\ncapabilities in classification and retrieval. However, these models often\nstruggle with compositional reasoning - the ability to understand the\nrelationships between concepts. A recent benchmark, SugarCrepe++, reveals that\nprevious works on improving compositionality have mainly improved lexical\nsensitivity but neglected semantic understanding. In addition, downstream\nretrieval performance often deteriorates, although one would expect that\nimproving compositionality should enhance retrieval. In this work, we introduce\nCLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a\nnovel training technique combining multiple images and their associated\ncaptions. CLIC improves compositionality across architectures as well as\ndifferently pre-trained CLIP models, both in terms of lexical and semantic\nunderstanding, and achieves consistent gains in retrieval performance. This\neven applies to the recent CLIPS, which achieves SOTA retrieval performance.\nNevertheless, the short fine-tuning with CLIC leads to an improvement in\nretrieval and to the best compositional CLIP model on SugarCrepe++. All our\nmodels and code are available at https://clic-compositional-clip.github.io", "AI": {"tldr": "CLIC\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u56fe\u50cf\u548c\u5173\u8054\u6807\u9898\u7684\u65b0\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347CLIP\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u6539\u8fdb\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u6539\u8fdb\u7ec4\u5408\u6027\u7684\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86\u8bed\u4e49\u7406\u89e3\uff0c\u5bfc\u81f4\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faCLIC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u56fe\u50cf\u53ca\u5176\u5173\u8054\u6807\u9898\u8fdb\u884c\u5fae\u8c03\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002", "result": "CLIC\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff08\u5305\u62ec\u8bcd\u6c47\u548c\u8bed\u4e49\u7406\u89e3\uff09\uff0c\u5e76\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "CLIC\u662f\u4e00\u79cd\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u68c0\u7d22\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.24691", "pdf": "https://arxiv.org/pdf/2505.24691", "abs": "https://arxiv.org/abs/2505.24691", "authors": ["Gerard I. G\u00e1llego", "Oriol Pareras", "Mart\u00ed Cortada Garcia", "Lucas Takanori", "Javier Hernando"], "title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u97f3\u7d20\u8868\u793a\u4e0e\u601d\u7ef4\u94fe\u6846\u67b6\u7684\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u548c\u96f6\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u548c\u96f6\u8d44\u6e90\u8bed\u8a00\u5728\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u4e2d\u7684\u6311\u6218\uff0c\u589e\u5f3a\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u97f3\u7d20\u8bc6\u522b\u4f5c\u4e3a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u6269\u5c55\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u8bed\u97f3\u548c\u97f3\u7d20\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u9010\u6b65\u8bad\u7ec3\u3002", "result": "\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5b9e\u73b0\u96f6\u8d44\u6e90\u7ffb\u8bd1\uff0c\u4f46\u5bf9\u9ad8\u8d44\u6e90\u6027\u80fd\u7565\u6709\u5f71\u54cd\u3002", "conclusion": "\u97f3\u7d20\u589e\u5f3a\u7684\u601d\u7ef4\u94fe\u6846\u67b6\u4e3a\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u666e\u53ca\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2505.24434", "pdf": "https://arxiv.org/pdf/2505.24434", "abs": "https://arxiv.org/abs/2505.24434", "authors": ["Md Shahriar Rahim Siddiqui", "Moshe Eliasof", "Eldad Haber"], "title": "Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Flow matching casts sample generation as learning a continuous-time velocity\nfield that transports noise to data. Existing flow matching networks typically\npredict each point's velocity independently, considering only its location and\ntime along its flow trajectory, and ignoring neighboring points. However, this\npointwise approach may overlook correlations between points along the\ngeneration trajectory that could enhance velocity predictions, thereby\nimproving downstream generation quality. To address this, we propose Graph Flow\nMatching (GFM), a lightweight enhancement that decomposes the learned velocity\ninto a reaction term -- any standard flow matching network -- and a diffusion\nterm that aggregates neighbor information via a graph neural module. This\nreaction-diffusion formulation retains the scalability of deep flow models\nwhile enriching velocity predictions with local context, all at minimal\nadditional computational cost. Operating in the latent space of a pretrained\nvariational autoencoder, GFM consistently improves Fr\\'echet Inception Distance\n(FID) and recall across five image generation benchmarks (LSUN Church, LSUN\nBedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\\times256$), demonstrating its\neffectiveness as a modular enhancement to existing flow matching architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGraph Flow Matching\uff08GFM\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u589e\u5f3a\u6d41\u5339\u914d\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5229\u7528\u5c40\u90e8\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6d41\u5339\u914d\u7f51\u7edc\u901a\u5e38\u72ec\u7acb\u9884\u6d4b\u6bcf\u4e2a\u70b9\u7684\u901f\u5ea6\uff0c\u5ffd\u7565\u4e86\u76f8\u90bb\u70b9\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u53d7\u9650\u3002", "method": "GFM\u5c06\u5b66\u4e60\u5230\u7684\u901f\u5ea6\u5206\u89e3\u4e3a\u53cd\u5e94\u9879\uff08\u6807\u51c6\u6d41\u5339\u914d\u7f51\u7edc\uff09\u548c\u6269\u6563\u9879\uff08\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u805a\u5408\u90bb\u5c45\u4fe1\u606f\uff09\uff0c\u4ee5\u53cd\u5e94-\u6269\u6563\u5f62\u5f0f\u4fdd\u7559\u6a21\u578b\u7684\u6269\u5c55\u6027\u5e76\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGFM\u663e\u8457\u6539\u5584\u4e86FID\u548crecall\u6307\u6807\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u73b0\u6709\u6d41\u5339\u914d\u67b6\u6784\u7684\u6a21\u5757\u5316\u589e\u5f3a\u7684\u6709\u6548\u6027\u3002", "conclusion": "GFM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6d41\u5339\u914d\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2505.24701", "pdf": "https://arxiv.org/pdf/2505.24701", "abs": "https://arxiv.org/abs/2505.24701", "authors": ["Tejul Pandit", "Meet Raval", "Dhvani Upadhyay"], "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures, 5 tables, 6th International Conference on\n  Natural Language Computing and AI (NLCAI 2025), ISBN : 978-1-923107-59-5,\n  Computer Science & Information Technology (CS & IT), ISSN : 2231 - 5403,\n  Volume 15, Number 10, May 2025", "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u751f\u6210\u5408\u6210ABSA\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3ABSA\u4efb\u52a1\u4e2d\u591a\u6837\u5316\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u53cd\u6620\u771f\u5b9e\u5bf9\u8bdd\u573a\u666f\u65f6\u3002", "method": "\u4f7f\u7528GPT-4o\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u786e\u4fdd\u4e3b\u9898\u548c\u60c5\u611f\u5206\u5e03\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u8bc4\u4f30\u4e09\u79cd\u5148\u8fdbLLM\uff08Gemini 1.5 Pro\u3001Claude 3.5 Sonnet\u548cDeepSeek-R1\uff09\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5408\u6210\u6570\u636e\u8868\u73b0\u51fa\u6709\u6548\u6027\uff1aDeepSeek-R1\u7cbe\u5ea6\u66f4\u9ad8\uff0cGemini 1.5 Pro\u548cClaude 3.5 Sonnet\u53ec\u56de\u7387\u5f3a\uff0cGemini 1.5 Pro\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u53ef\u4e3aABSA\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u51cf\u5c11\u5bf9\u6709\u9650\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.24514", "pdf": "https://arxiv.org/pdf/2505.24514", "abs": "https://arxiv.org/abs/2505.24514", "authors": ["Janek Gr\u00f6hl", "Leonid Kunyansky", "Jenni Poimala", "Thomas R. Else", "Francesca Di Cecio", "Sarah E. Bohndiek", "Ben T. Cox", "Andreas Hauptmann"], "title": "Digital twins enable full-reference quality assessment of photoacoustic image reconstructions", "categories": ["physics.med-ph", "cs.CV", "eess.SP"], "comment": null, "summary": "Quantitative comparison of the quality of photoacoustic image reconstruction\nalgorithms remains a major challenge. No-reference image quality measures are\noften inadequate, but full-reference measures require access to an ideal\nreference image. While the ground truth is known in simulations, it is unknown\nin vivo, or in phantom studies, as the reference depends on both the phantom\nproperties and the imaging system. We tackle this problem by using numerical\ndigital twins of tissue-mimicking phantoms and the imaging system to perform a\nquantitative calibration to reduce the simulation gap. The contributions of\nthis paper are two-fold: First, we use this digital-twin framework to compare\nmultiple state-of-the-art reconstruction algorithms. Second, among these is a\nFourier transform-based reconstruction algorithm for circular detection\ngeometries, which we test on experimental data for the first time. Our results\ndemonstrate the usefulness of digital phantom twins by enabling assessment of\nthe accuracy of the numerical forward model and enabling comparison of image\nreconstruction schemes with full-reference image quality assessment. We show\nthat the Fourier transform-based algorithm yields results comparable to those\nof iterative time reversal, but at a lower computational cost. All data and\ncode are publicly available on Zenodo: https://doi.org/10.5281/zenodo.15388429.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u4f53\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u91cf\u8bc4\u4f30\u5149\u58f0\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\u7684\u8d28\u91cf\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e00\u79cd\u65b0\u7684\u5085\u91cc\u53f6\u53d8\u6362\u91cd\u5efa\u7b97\u6cd5\u3002", "motivation": "\u5149\u58f0\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\u7684\u8d28\u91cf\u8bc4\u4f30\u7f3a\u4e4f\u7406\u60f3\u53c2\u8003\u56fe\u50cf\uff0c\u6570\u5b57\u5b6a\u751f\u4f53\u53ef\u51cf\u5c11\u4eff\u771f\u4e0e\u5b9e\u9645\u6570\u636e\u7684\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u4f53\u6846\u67b6\u6bd4\u8f83\u591a\u79cd\u91cd\u5efa\u7b97\u6cd5\uff0c\u5e76\u9996\u6b21\u5728\u5b9e\u9a8c\u6570\u636e\u4e2d\u6d4b\u8bd5\u5085\u91cc\u53f6\u53d8\u6362\u7b97\u6cd5\u3002", "result": "\u6570\u5b57\u5b6a\u751f\u4f53\u6709\u6548\u8bc4\u4f30\u4e86\u6570\u503c\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5085\u91cc\u53f6\u53d8\u6362\u7b97\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0e\u8fed\u4ee3\u65f6\u95f4\u53cd\u8f6c\u76f8\u5f53\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u4f53\u4e3a\u5149\u58f0\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\u7684\u5b9a\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5085\u91cc\u53f6\u53d8\u6362\u7b97\u6cd5\u5177\u6709\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.24712", "pdf": "https://arxiv.org/pdf/2505.24712", "abs": "https://arxiv.org/abs/2505.24712", "authors": ["Guido Ivetta", "Marcos J. Gomez", "Sof\u00eda Martinelli", "Pietro Palombini", "M. Emilia Echeveste", "Nair Carolina Mazzeo", "Beatriz Busaniche", "Luciana Benotti"], "title": "HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities.", "AI": {"tldr": "HESEIA\u662f\u4e00\u4e2a\u7531\u62c9\u4e01\u7f8e\u6d32\u6559\u80b2\u5de5\u4f5c\u8005\u548c\u5b66\u751f\u5171\u540c\u521b\u5efa\u7684\u5305\u542b46,499\u4e2a\u53e5\u5b50\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u7279\u522b\u5173\u6ce8\u4ea4\u53c9\u6027\u504f\u89c1\u548c\u672c\u5730\u5316\u60c5\u5883\u3002", "motivation": "\u73b0\u6709\u8d44\u6e90\u901a\u5e38\u7f3a\u4e4f\u53d7\u504f\u89c1\u5f71\u54cd\u793e\u533a\u7684\u53c2\u4e0e\uff0cHESEIA\u901a\u8fc7\u6559\u80b2\u8005\u7684\u751f\u6d3b\u7ecf\u9a8c\u548c\u4e13\u4e1a\u77e5\u8bc6\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "370\u540d\u9ad8\u4e2d\u6559\u5e08\u548c5,370\u540d\u5b66\u751f\u901a\u8fc7\u4e13\u4e1a\u53d1\u5c55\u8bfe\u7a0b\u521b\u5efa\u4e86\u53cd\u6620\u5176\u5b66\u79d1\u548c\u793e\u533a\u523b\u677f\u5370\u8c61\u7684\u53e5\u5b50\u3002", "result": "HESEIA\u5728\u4eba\u53e3\u7edf\u8ba1\u8f74\u548c\u77e5\u8bc6\u9886\u57df\u4e0a\u8868\u73b0\u51fa\u591a\u6837\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524dLLM\u672a\u8bc6\u522b\u7684\u66f4\u591a\u523b\u677f\u5370\u8c61\u3002", "conclusion": "HESEIA\u4e3a\u57fa\u4e8e\u6559\u80b2\u793e\u533a\u7684\u504f\u89c1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5f3a\u8c03\u4e86\u672c\u5730\u5316\u53c2\u4e0e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.24605", "pdf": "https://arxiv.org/pdf/2505.24605", "abs": "https://arxiv.org/abs/2505.24605", "authors": ["Ivan Pereira-S\u00e1nchez", "Julia Navarro", "Ana Bel\u00e9n Petro", "Joan Duran"], "title": "Model-Guided Network with Cluster-Based Operators for Spatio-Spectral Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper addresses the problem of reconstructing a high-resolution\nhyperspectral image from a low-resolution multispectral observation. While\nspatial super-resolution and spectral super-resolution have been extensively\nstudied, joint spatio-spectral super-resolution remains relatively explored. We\npropose an end-to-end model-driven framework that explicitly decomposes the\njoint spatio-spectral super-resolution problem into spatial super-resolution,\nspectral super-resolution and fusion tasks. Each sub-task is addressed by\nunfolding a variational-based approach, where the operators involved in the\nproximal gradient iterative scheme are replaced with tailored learnable\nmodules. In particular, we design an upsampling operator for spatial\nsuper-resolution based on classical back-projection algorithms, adapted to\nhandle arbitrary scaling factors. Spectral reconstruction is performed using\nlearnable cluster-based upsampling and downsampling operators. For image\nfusion, we integrate low-frequency estimation and high-frequency injection\nmodules to combine the spatial and spectral information from spatial\nsuper-resolution and spectral super-resolution outputs. Additionally, we\nintroduce an efficient nonlocal post-processing step that leverages image\nself-similarity by combining a multi-head attention mechanism with residual\nconnections. Extensive evaluations on several datasets and sampling factors\ndemonstrate the effectiveness of our approach. The source code will be\navailable at https://github.com/TAMI-UIB/JSSUNet", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6a21\u578b\u9a71\u52a8\u6846\u67b6\uff0c\u5c06\u8054\u5408\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u5206\u89e3\u4e3a\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u3001\u5149\u8c31\u8d85\u5206\u8fa8\u7387\u548c\u878d\u5408\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u6a21\u5757\u4f18\u5316\u5404\u5b50\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u4ece\u4f4e\u5206\u8fa8\u7387\u591a\u5149\u8c31\u89c2\u6d4b\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u95ee\u9898\uff0c\u586b\u8865\u8054\u5408\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u3001\u5149\u8c31\u8d85\u5206\u8fa8\u7387\u548c\u878d\u5408\u4efb\u52a1\uff0c\u91c7\u7528\u53d8\u5206\u65b9\u6cd5\u5c55\u5f00\uff0c\u5e76\u7528\u53ef\u5b66\u4e60\u6a21\u5757\u66ff\u4ee3\u4f20\u7edf\u7b97\u5b50\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u53cd\u5411\u6295\u5f71\u7b97\u6cd5\u7684\u4e0a\u91c7\u6837\u7b97\u5b50\u3001\u57fa\u4e8e\u805a\u7c7b\u7684\u5149\u8c31\u91cd\u5efa\u7b97\u5b50\uff0c\u4ee5\u53ca\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u91c7\u6837\u56e0\u5b50\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8054\u5408\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.24713", "pdf": "https://arxiv.org/pdf/2505.24713", "abs": "https://arxiv.org/abs/2505.24713", "authors": ["Badr M. Abdullah", "Matthew Baas", "Bernd M\u00f6bius", "Dietrich Klakow"], "title": "Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Arabic dialect identification (ADI) systems are essential for large-scale\ndata collection pipelines that enable the development of inclusive speech\ntechnologies for Arabic language varieties. However, the reliability of current\nADI systems is limited by poor generalization to out-of-domain speech. In this\npaper, we present an effective approach based on voice conversion for training\nADI models that achieves state-of-the-art performance and significantly\nimproves robustness in cross-domain scenarios. Evaluated on a newly collected\nreal-world test set spanning four different domains, our approach yields\nconsistent improvements of up to +34.1% in accuracy across domains.\nFurthermore, we present an analysis of our approach and demonstrate that voice\nconversion helps mitigate the speaker bias in the ADI dataset. We release our\nrobust ADI model and cross-domain evaluation dataset to support the development\nof inclusive speech technologies for Arabic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u97f3\u8f6c\u6362\u7684\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\uff08ADI\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u65b0\u6536\u96c6\u7684\u771f\u5b9e\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad834.1%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u5f53\u524dADI\u7cfb\u7edf\u5728\u8de8\u57df\u8bed\u97f3\u8bc6\u522b\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5305\u5bb9\u6027\u8bed\u97f3\u6280\u672f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bed\u97f3\u8f6c\u6362\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f18\u5316ADI\u6a21\u578b\u3002", "result": "\u5728\u65b0\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u8de8\u57df\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe34.1%\uff0c\u5e76\u51cf\u5c11\u4e86\u6570\u636e\u96c6\u4e2d\u7684\u8bf4\u8bdd\u8005\u504f\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86ADI\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u652f\u6301\u4e86\u963f\u62c9\u4f2f\u8bed\u5305\u5bb9\u6027\u8bed\u97f3\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.24623", "pdf": "https://arxiv.org/pdf/2505.24623", "abs": "https://arxiv.org/abs/2505.24623", "authors": ["Wenyuan Li", "Guang Li", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Hyperbolic Dataset Distillation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "To address the computational and storage challenges posed by large-scale\ndatasets in deep learning, dataset distillation has been proposed to synthesize\na compact dataset that replaces the original while maintaining comparable model\nperformance. Unlike optimization-based approaches that require costly bi-level\noptimization, distribution matching (DM) methods improve efficiency by aligning\nthe distributions of synthetic and original data, thereby eliminating nested\noptimization. DM achieves high computational efficiency and has emerged as a\npromising solution. However, existing DM methods, constrained to Euclidean\nspace, treat data as independent and identically distributed points,\noverlooking complex geometric and hierarchical relationships. To overcome this\nlimitation, we propose a novel hyperbolic dataset distillation method, termed\nHDD. Hyperbolic space, characterized by negative curvature and exponential\nvolume growth with distance, naturally models hierarchical and tree-like\nstructures. HDD embeds features extracted by a shallow network into the Lorentz\nhyperbolic space, where the discrepancy between synthetic and original data is\nmeasured by the hyperbolic (geodesic) distance between their centroids. By\noptimizing this distance, the hierarchical structure is explicitly integrated\ninto the distillation process, guiding synthetic samples to gravitate towards\nthe root-centric regions of the original data distribution while preserving\ntheir underlying geometric characteristics. Furthermore, we find that pruning\nin hyperbolic space requires only 20% of the distilled core set to retain model\nperformance, while significantly improving training stability. Notably, HDD is\nseamlessly compatible with most existing DM methods, and extensive experiments\non different datasets validate its effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2\u7a7a\u95f4\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff08HDD\uff09\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\u5e76\u4f18\u5316\u4e2d\u5fc3\u70b9\u95f4\u7684\u53cc\u66f2\u8ddd\u79bb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u5e03\u5339\u914d\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5ffd\u7565\u590d\u6742\u51e0\u4f55\u548c\u5c42\u6b21\u5173\u7cfb\u7684\u5c40\u9650\u3002", "motivation": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u6311\u6218\u4fc3\u4f7f\u6570\u636e\u96c6\u84b8\u998f\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u65e0\u6cd5\u6355\u6349\u6570\u636e\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "method": "HDD\u65b9\u6cd5\u5c06\u6d45\u5c42\u7f51\u7edc\u63d0\u53d6\u7684\u7279\u5f81\u5d4c\u5165\u6d1b\u4f26\u5179\u53cc\u66f2\u7a7a\u95f4\uff0c\u901a\u8fc7\u4f18\u5316\u5408\u6210\u6570\u636e\u4e0e\u539f\u59cb\u6570\u636e\u4e2d\u5fc3\u70b9\u95f4\u7684\u53cc\u66f2\u8ddd\u79bb\uff0c\u663e\u5f0f\u6574\u5408\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHDD\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4ec5\u970020%\u7684\u6838\u5fc3\u96c6\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u4e14\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "HDD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u96c6\uff0c\u4e3a\u590d\u6742\u6570\u636e\u7ed3\u6784\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24714", "pdf": "https://arxiv.org/pdf/2505.24714", "abs": "https://arxiv.org/abs/2505.24714", "authors": ["Junyu Luo", "Zhizhuo Kou", "Liming Yang", "Xiao Luo", "Jinsheng Huang", "Zhiping Xiao", "Jingshu Peng", "Chengzhong Liu", "Jiaming Ji", "Xuanzhe Liu", "Sirui Han", "Ming Zhang", "Yike Guo"], "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86FinMME\uff0c\u4e00\u4e2a\u4e13\u4e3a\u91d1\u878d\u9886\u57df\u8bbe\u8ba1\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5305\u542b11,000\u591a\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u8986\u76d618\u4e2a\u91d1\u878d\u9886\u57df\u548c6\u79cd\u8d44\u4ea7\u7c7b\u522b\uff0c\u5e76\u5f00\u53d1\u4e86FinScore\u8bc4\u4f30\u7cfb\u7edf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662fGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u5728FinMME\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51f8\u663e\u5176\u6311\u6218\u6027\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efaFinMME\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u91d1\u878d\u6837\u672c\uff0c\u5e76\u901a\u8fc720\u540d\u6807\u6ce8\u8005\u548c\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff1b\u5f00\u53d1FinScore\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5305\u542b\u5e7b\u89c9\u60e9\u7f5a\u548c\u591a\u7ef4\u80fd\u529b\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5148\u8fdb\u6a21\u578b\u5982GPT-4o\u5728FinMME\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6570\u636e\u96c6\u5177\u6709\u9ad8\u9c81\u68d2\u6027\uff08\u9884\u6d4b\u53d8\u5f02\u4f4e\u4e8e1%\uff09\u3002", "conclusion": "FinMME\u4e3a\u91d1\u878d\u9886\u57df\u7684MLLMs\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5176\u6311\u6218\u6027\u548c\u53ef\u9760\u6027\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002"}}
{"id": "2505.24654", "pdf": "https://arxiv.org/pdf/2505.24654", "abs": "https://arxiv.org/abs/2505.24654", "authors": ["Maria Rafaela Gkeka", "Bowen Sun", "Evgenia Smirni", "Christos D. Antonopoulos", "Spyros Lalis", "Nikolaos Bellas"], "title": "Black-box Adversarial Attacks on CNN-based SLAM Algorithms", "categories": ["cs.RO", "cs.CV", "68T40, 68T45, 68M25,"], "comment": "9 pages, 8 figures", "summary": "Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9488\u5bf9CNN-based SLAM\u7cfb\u7edf\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u4e2d\u7b49\u89c4\u6a21\u7684\u653b\u51fb\u4e5f\u80fd\u5bfc\u81f476%\u7684\u5e27\u8ddf\u8e2a\u5931\u8d25\uff0c\u4e14\u653b\u51fb\u6df1\u5ea6\u8f93\u5165\u6bd4RGB\u8f93\u5165\u66f4\u5177\u7834\u574f\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728SLAM\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u9650\u5236\u4e86\u5176\u5728\u81ea\u4e3b\u5bfc\u822a\u7b49\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9CNN-based SLAM\u7cfb\u7edf\u4e2d\u7279\u5f81\u68c0\u6d4b\u5668\u7684\u5bf9\u6297\u653b\u51fb\u7684\u5168\u9762\u7814\u7a76\u3002", "method": "\u7814\u7a76\u91c7\u7528\u9ed1\u76d2\u5bf9\u6297\u6270\u52a8\u65b9\u6cd5\uff0c\u9488\u5bf9GCN-SLAM\u7b97\u6cd5\u7684RGB\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u653b\u51fb\uff0c\u5e76\u5728TUM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e2d\u7b49\u89c4\u6a21\u7684\u5bf9\u6297\u653b\u51fb\u53ef\u5bfc\u81f476%\u7684\u5e27\u8ddf\u8e2a\u5931\u8d25\uff0c\u4e14\u653b\u51fb\u6df1\u5ea6\u8f93\u5165\u5bf9SLAM\u7cfb\u7edf\u7684\u7834\u574f\u6027\u66f4\u5927\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86CNN-based SLAM\u7cfb\u7edf\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u81ea\u4e3b\u5bfc\u822a\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u9700\u52a0\u5f3a\u5bf9\u6297\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.24726", "pdf": "https://arxiv.org/pdf/2505.24726", "abs": "https://arxiv.org/abs/2505.24726", "authors": ["Shelly Bensal", "Umar Jamil", "Christopher Bryant", "Melisa Russak", "Kiran Kamble", "Dmytro Mozolevskyi", "Muayad Ali", "Waseem AlShikh"], "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u6570\u5b66\u65b9\u7a0b\u7f16\u5199\u548c\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e2d\u5206\u522b\u670934.7%\u548c18.1%\u7684\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u5728\u751f\u6210\u5408\u6210\u6570\u636e\u4e0d\u53ef\u884c\u4e14\u4ec5\u6709\u4e8c\u5143\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u63d0\u5347\u6a21\u578b\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u4efb\u52a1\u5931\u8d25\u65f6\u751f\u6210\u81ea\u6211\u53cd\u601d\u5206\u6790\uff0c\u968f\u540e\u5728\u53cd\u601d\u5185\u5bb9\u6307\u5bfc\u4e0b\u91cd\u8bd5\u4efb\u52a1\uff0c\u6210\u529f\u5219\u5956\u52b1\u53cd\u601d\u9636\u6bb5\u7684\u751f\u6210\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5c0f\u89c4\u6a21\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bed\u8a00\u6a21\u578b\u5728\u6709\u9650\u53cd\u9988\u4e0b\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.24687", "pdf": "https://arxiv.org/pdf/2505.24687", "abs": "https://arxiv.org/abs/2505.24687", "authors": ["Shengyuan Liu", "Wenting Chen", "Boyun Zheng", "Wentao Pan", "Xiang Li", "Yixuan Yuan"], "title": "TumorGen: Boundary-Aware Tumor-Mask Synthesis with Rectified Flow Matching", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages, 4 figures", "summary": "Tumor data synthesis offers a promising solution to the shortage of annotated\nmedical datasets. However, current approaches either limit tumor diversity by\nusing predefined masks or employ computationally expensive two-stage processes\nwith multiple denoising steps, causing computational inefficiency.\nAdditionally, these methods typically rely on binary masks that fail to capture\nthe gradual transitions characteristic of tumor boundaries. We present\nTumorGen, a novel Boundary-Aware Tumor-Mask Synthesis with Rectified Flow\nMatching for efficient 3D tumor synthesis with three key components: a\nBoundary-Aware Pseudo Mask Generation module that replaces strict binary masks\nwith flexible bounding boxes; a Spatial-Constraint Vector Field Estimator that\nsimultaneously synthesizes tumor latents and masks using rectified flow\nmatching to ensure computational efficiency; and a VAE-guided mask refiner that\nenhances boundary realism. TumorGen significantly improves computational\nefficiency by requiring fewer sampling steps while maintaining pathological\naccuracy through coarse and fine-grained spatial constraints. Experimental\nresults demonstrate TumorGen's superior performance over existing tumor\nsynthesis methods in both efficiency and realism, offering a valuable\ncontribution to AI-driven cancer diagnostics.", "AI": {"tldr": "TumorGen\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e09\u7ef4\u80bf\u7624\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u548c\u6d41\u5339\u914d\u6280\u672f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u80bf\u7624\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u53d7\u9650\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u8fb9\u754c\u8fc7\u6e21\u4e0d\u81ea\u7136\u7684\u95ee\u9898\uff0cTumorGen\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TumorGen\u7ed3\u5408\u4e86\u8fb9\u754c\u611f\u77e5\u4f2a\u63a9\u6a21\u751f\u6210\u3001\u7a7a\u95f4\u7ea6\u675f\u5411\u91cf\u573a\u4f30\u8ba1\u5668\u548cVAE\u5f15\u5bfc\u7684\u63a9\u6a21\u7ec6\u5316\u6a21\u5757\uff0c\u5229\u7528\u4fee\u6b63\u6d41\u5339\u914d\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTumorGen\u5728\u8ba1\u7b97\u6548\u7387\u548c\u771f\u5b9e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u91c7\u6837\u6b65\u9aa4\u3002", "conclusion": "TumorGen\u4e3aAI\u9a71\u52a8\u7684\u764c\u75c7\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u771f\u5b9e\u7684\u80bf\u7624\u5408\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24731", "pdf": "https://arxiv.org/pdf/2505.24731", "abs": "https://arxiv.org/abs/2505.24731", "authors": ["Alan Sun"], "title": "Circuit Stability Characterizes Language Model Generalization", "categories": ["cs.CL"], "comment": "16 pages, 10 figures", "summary": "Extensively evaluating the capabilities of (large) language models is\ndifficult. Rapid development of state-of-the-art models induce benchmark\nsaturation, while creating more challenging datasets is labor-intensive.\nInspired by the recent developments in mechanistic interpretability, we\nintroduce circuit stability as a new way to assess model performance. Circuit\nstability refers to a model's ability to apply a consistent reasoning\nprocess-its circuit-across various inputs. We mathematically formalize circuit\nstability and circuit equivalence. Then, through three case studies, we\nempirically show that circuit stability and the lack thereof can characterize\nand predict different aspects of generalization. Our proposed methods offer a\nstep towards rigorously relating the generality of models to their\ninterpretability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7535\u8def\u7a33\u5b9a\u6027\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\uff0c\u800c\u521b\u5efa\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u53c8\u8d39\u65f6\u8d39\u529b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u5f62\u5f0f\u5316\u7535\u8def\u7a33\u5b9a\u6027\u548c\u7535\u8def\u7b49\u4ef7\u6027\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u7535\u8def\u7a33\u5b9a\u6027\u53ca\u5176\u7f3a\u5931\u53ef\u4ee5\u8868\u5f81\u548c\u9884\u6d4b\u6a21\u578b\u7684\u4e0d\u540c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c06\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u5176\u53ef\u89e3\u91ca\u6027\u8054\u7cfb\u8d77\u6765\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2505.24703", "pdf": "https://arxiv.org/pdf/2505.24703", "abs": "https://arxiv.org/abs/2505.24703", "authors": ["Dennis Jacob", "Chong Xiang", "Prateek Mittal"], "title": "PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "CVPR 2025", "summary": "Deep learning techniques have enabled vast improvements in computer vision\ntechnologies. Nevertheless, these models are vulnerable to adversarial patch\nattacks which catastrophically impair performance. The physically realizable\nnature of these attacks calls for certifiable defenses, which feature provable\nguarantees on robustness. While certifiable defenses have been successfully\napplied to single-label classification, limited work has been done for\nmulti-label classification. In this work, we present PatchDEMUX, a certifiably\nrobust framework for multi-label classifiers against adversarial patches. Our\napproach is a generalizable method which can extend any existing certifiable\ndefense for single-label classification; this is done by considering the\nmulti-label classification task as a series of isolated binary classification\nproblems to provably guarantee robustness. Furthermore, in the scenario where\nan attacker is limited to a single patch we propose an additional certification\nprocedure that can provide tighter robustness bounds. Using the current\nstate-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a\nbackbone, we find that PatchDEMUX can achieve non-trivial robustness on the\nMS-COCO and PASCAL VOC datasets while maintaining high clean performance", "AI": {"tldr": "PatchDEMUX\u662f\u4e00\u4e2a\u53ef\u8bc1\u660e\u9c81\u68d2\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u6807\u7b7e\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u5229\u7528\u5355\u6807\u7b7e\u9632\u5fa1\u65b9\u6cd5\u6269\u5c55\u5b9e\u73b0\u3002", "motivation": "\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u76ee\u524d\u53ef\u8bc1\u660e\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u6807\u7b7e\u5206\u7c7b\uff0c\u591a\u6807\u7b7e\u5206\u7c7b\u9886\u57df\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5c06\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u72ec\u7acb\u7684\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u73b0\u6709\u5355\u6807\u7b7e\u9632\u5fa1\u65b9\u6cd5\uff08\u5982PatchCleanser\uff09\uff0c\u5728\u5355\u8865\u4e01\u653b\u51fb\u573a\u666f\u4e0b\u63d0\u51fa\u66f4\u4e25\u683c\u7684\u9c81\u68d2\u6027\u8fb9\u754c\u3002", "result": "\u5728MS-COCO\u548cPASCAL VOC\u6570\u636e\u96c6\u4e0a\uff0cPatchDEMUX\u5b9e\u73b0\u4e86\u975e\u5e73\u51e1\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u5e72\u51c0\u6027\u80fd\u3002", "conclusion": "PatchDEMUX\u4e3a\u591a\u6807\u7b7e\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u9c81\u68d2\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2505.24754", "pdf": "https://arxiv.org/pdf/2505.24754", "abs": "https://arxiv.org/abs/2505.24754", "authors": ["Yingchaojie Feng", "Yiqun Sun", "Yandong Sun", "Minfeng Zhu", "Qiang Huang", "Anthony K. H. Tung", "Wei Chen"], "title": "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to ACL 2025", "summary": "In this work, we investigate an important task named instruction-following\ntext embedding, which generates dynamic text embeddings that adapt to user\ninstructions, highlighting specific attributes of text. Despite recent\nadvancements, existing approaches suffer from significant computational\noverhead, as they require re-encoding the entire corpus for each new\ninstruction. To address this challenge, we propose GSTransform, a novel\ninstruction-following text embedding framework based on Guided Space\nTransformation. Our key observation is that instruction-relevant information is\ninherently encoded in generic embeddings but remains underutilized. Instead of\nrepeatedly encoding the corpus for each instruction, GSTransform is a\nlightweight transformation mechanism that adapts pre-computed embeddings in\nreal time to align with user instructions, guided by a small amount of text\ndata with instruction-focused label annotation. We conduct extensive\nexperiments on three instruction-awareness downstream tasks across nine\nreal-world datasets, demonstrating that GSTransform improves\ninstruction-following text embedding quality over state-of-the-art methods\nwhile achieving dramatic speedups of 6~300x in real-time processing on\nlarge-scale datasets. The source code is available at\nhttps://github.com/YingchaojieFeng/GSTransform.", "AI": {"tldr": "GSTransform\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6307\u4ee4\u8ddf\u968f\u6587\u672c\u5d4c\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u8f6c\u6362\u52a8\u6001\u8c03\u6574\u9884\u8ba1\u7b97\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u6307\u4ee4\u91cd\u65b0\u7f16\u7801\u6574\u4e2a\u8bed\u6599\u5e93\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u800cGSTransform\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9884\u8ba1\u7b97\u7684\u901a\u7528\u5d4c\u5165\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u8f6c\u6362\u5b9e\u65f6\u8c03\u6574\u5d4c\u5165\u4ee5\u9002\u5e94\u7528\u6237\u6307\u4ee4\uff0c\u65e0\u9700\u91cd\u590d\u7f16\u7801\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\u548c\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGSTransform\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u53476~300\u500d\u3002", "conclusion": "GSTransform\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u8ddf\u968f\u6587\u672c\u5d4c\u5165\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u65f6\u5904\u7406\u3002"}}
{"id": "2505.24739", "pdf": "https://arxiv.org/pdf/2505.24739", "abs": "https://arxiv.org/abs/2505.24739", "authors": ["Xinliu Zhong", "Ruiying Liu", "Emily S. Nichols", "Xuzhe Zhang", "Andrew F. Laine", "Emma G. Duerden", "Yun Wang"], "title": "Contrast-Invariant Self-supervised Segmentation for Quantitative Placental MRI", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages, 20 figures", "summary": "Accurate placental segmentation is essential for quantitative analysis of the\nplacenta. However, this task is particularly challenging in T2*-weighted\nplacental imaging due to: (1) weak and inconsistent boundary contrast across\nindividual echoes; (2) the absence of manual ground truth annotations for all\necho times; and (3) motion artifacts across echoes caused by fetal and maternal\nmovement. In this work, we propose a contrast-augmented segmentation framework\nthat leverages complementary information across multi-echo T2*-weighted MRI to\nlearn robust, contrast-invariant representations. Our method integrates: (i)\nmasked autoencoding (MAE) for self-supervised pretraining on unlabeled\nmulti-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain\nadaptation across echo times; and (iii) global-local collaboration to align\nfine-grained features with global anatomical context. We further introduce a\nsemantic matching loss to encourage representation consistency across echoes of\nthe same subject. Experiments on a clinical multi-echo placental MRI dataset\ndemonstrate that our approach generalizes effectively across echo times and\noutperforms both single-echo and naive fusion baselines. To our knowledge, this\nis the first work to systematically exploit multi-echo T2*-weighted MRI for\nplacental segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u56de\u6ce2T2*\u52a0\u6743MRI\u7684\u80ce\u76d8\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u89e3\u51b3\u8fb9\u754c\u5bf9\u6bd4\u5f31\u3001\u6807\u6ce8\u7f3a\u5931\u548c\u8fd0\u52a8\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u80ce\u76d8\u5206\u5272\u5728T2*\u52a0\u6743\u6210\u50cf\u4e2d\u9762\u4e34\u8fb9\u754c\u5bf9\u6bd4\u5f31\u3001\u6807\u6ce8\u7f3a\u5931\u548c\u8fd0\u52a8\u4f2a\u5f71\u7684\u6311\u6218\uff0c\u9700\u5229\u7528\u591a\u56de\u6ce2\u4fe1\u606f\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u63a9\u7801\u81ea\u7f16\u7801\uff08MAE\uff09\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u63a9\u7801\u4f2a\u6807\u7b7e\uff08MPL\uff09\u65e0\u76d1\u7763\u57df\u9002\u5e94\u53ca\u5168\u5c40-\u5c40\u90e8\u534f\u4f5c\uff0c\u5f15\u5165\u8bed\u4e49\u5339\u914d\u635f\u5931\u3002", "result": "\u5728\u4e34\u5e8a\u591a\u56de\u6ce2\u80ce\u76d8MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u56de\u6ce2\u548c\u7b80\u5355\u878d\u5408\u57fa\u7ebf\uff0c\u80fd\u6709\u6548\u8de8\u56de\u6ce2\u6cdb\u5316\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u5229\u7528\u591a\u56de\u6ce2T2*\u52a0\u6743MRI\u8fdb\u884c\u80ce\u76d8\u5206\u5272\uff0c\u4e3a\u5b9a\u91cf\u5206\u6790\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.24757", "pdf": "https://arxiv.org/pdf/2505.24757", "abs": "https://arxiv.org/abs/2505.24757", "authors": ["Christian Jaumann", "Andreas Wiedholz", "Annemarie Friedrich"], "title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews", "categories": ["cs.CL"], "comment": null, "summary": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u96f6\u6837\u672c\u62bd\u8c61\u6392\u540d\u65b9\u6cd5LGAR\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u7684\u6458\u8981\u7b5b\u9009\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709QA\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u589e\u957f\u8fc5\u901f\uff0c\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u7684\u6458\u8981\u7b5b\u9009\u9636\u6bb5\u9700\u8981\u9ad8\u6548\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4e8c\u5143\u5206\u7c7b\u6216\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "method": "\u63d0\u51faLGAR\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u7684\u8bc4\u5206\u548c\u5bc6\u96c6\u91cd\u6392\u5e8f\uff0c\u624b\u52a8\u63d0\u53d657\u7bc7SLR\u7684\u7eb3\u5165\u6392\u9664\u6807\u51c6\u548c\u7814\u7a76\u95ee\u9898\u4f5c\u4e3a\u57fa\u51c6\u3002", "result": "LGAR\u5728\u5e73\u5747\u7cbe\u5ea6\u4e0a\u6bd4\u73b0\u6709QA\u65b9\u6cd5\u63d0\u53475-10\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LGAR\u4e3aSLR\u6458\u8981\u7b5b\u9009\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.24781", "pdf": "https://arxiv.org/pdf/2505.24781", "abs": "https://arxiv.org/abs/2505.24781", "authors": ["Karim Abou-Moustafa"], "title": "Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV", "categories": ["stat.ML", "cs.CE", "cs.CV", "cs.LG", "eess.SP", "I.2.0; I.2.6"], "comment": "An extended version of a short article that appeared in 2023 IEEE\n  Workshop on Information Theory, Saint-Malo, France", "summary": "We consider the problem of estimating a regularization parameter, or a\nshrinkage coefficient $\\alpha \\in (0,1)$ for Regularized Tyler's M-estimator\n(RTME). In particular, we propose to estimate an optimal shrinkage coefficient\nby setting $\\alpha$ as the solution to a suitably chosen objective function;\nnamely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since\nLOOCV is computationally prohibitive even for moderate sample size $n$, we\npropose a computationally efficient approximation for the LOOCV log-likelihood\nloss that eliminates the need for invoking the RTME procedure $n$ times for\neach sample left out during the LOOCV procedure. This approximation yields an\n$O(n)$ reduction in the running time complexity for the LOOCV procedure, which\nresults in a significant speedup for computing the LOOCV estimate. We\ndemonstrate the efficiency and accuracy of the proposed approach on synthetic\nhigh-dimensional data sampled from heavy-tailed elliptical distributions, as\nwell as on real high-dimensional datasets for object recognition, face\nrecognition, and handwritten digit's recognition. Our experiments show that the\nproposed approach is efficient and consistently more accurate than other\nmethods in the literature for shrinkage coefficient estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4f30\u8ba1\u6b63\u5219\u5316\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u7559\u4e00\u4ea4\u53c9\u9a8c\u8bc1\uff08LOOCV\uff09\u5bf9\u6570\u4f3c\u7136\u635f\u5931\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6b63\u5219\u5316Tyler M\u4f30\u8ba1\u5668\uff08RTME\uff09\u4e2d\u6b63\u5219\u5316\u53c2\u6570\u4f30\u8ba1\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8fd1\u4f3cLOOCV\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97RTME\uff0c\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(n^2)\u964d\u81f3O(n)\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u9ad8\u7ef4\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u6b63\u5219\u5316\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2505.24768", "pdf": "https://arxiv.org/pdf/2505.24768", "abs": "https://arxiv.org/abs/2505.24768", "authors": ["Haoyu Li", "Xuhong Li", "Yiming Dong", "Kun Liu"], "title": "From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6570\u636e\u96c6\u591a\u6837\u6027\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6837\u6027\u63a7\u5236\u7b56\u7565\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5fae\u89c2\u54cd\u5e94\u591a\u6837\u6027\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u6700\u5927\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u96c6\u591a\u6837\u6027\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u6027\u9010\u6e10\u88ab\u8ba4\u53ef\uff0c\u4f46\u5176\u7cfb\u7edf\u6027\u5206\u6790\u4ecd\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6837\u6027\u63a7\u5236\u7b56\u7565\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u6db5\u76d6\u5b8f\u89c2\u3001\u4e2d\u89c2\u548c\u5fae\u89c2\u5c42\u9762\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u516d\u79cd\u7b56\u7565\u7684\u56fa\u5b9a\u5927\u5c0f\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b8f\u89c2\u548c\u4e2d\u89c2\u7b56\u7565\u5728\u591a\u6837\u6027\u589e\u52a0\u65f6\u6027\u80fd\u63d0\u5347\uff0c\u800c\u5fae\u89c2\u54cd\u5e94\u591a\u6837\u6027\u7b56\u7565\u4e0e\u6a21\u578b\u6027\u80fd\u76f8\u5173\u6027\u66f4\u5f3a\uff0c\u4e14\u5728\u6700\u5927\u591a\u6837\u6027\u65f6\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u9ad8\u6027\u80fdSFT\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u5fae\u89c2\u54cd\u5e94\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.24786", "pdf": "https://arxiv.org/pdf/2505.24786", "abs": "https://arxiv.org/abs/2505.24786", "authors": ["Eran Bamani Beeri", "Eden Nissinman", "Avishai Sintov"], "title": "DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2411.18413", "summary": "Dynamic hand gestures play a pivotal role in assistive human-robot\ninteraction (HRI), facilitating intuitive, non-verbal communication,\nparticularly for individuals with mobility constraints or those operating\nrobots remotely. Current gesture recognition methods are mostly limited to\nshort-range interactions, reducing their utility in scenarios demanding robust\nassistive communication from afar. In this paper, we introduce a novel approach\ndesigned specifically for assistive robotics, enabling dynamic gesture\nrecognition at extended distances of up to 30 meters, thereby significantly\nimproving accessibility and quality of life. Our proposed Distance-aware\nGesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable\nAlignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust\nprocessing and classification of gesture sequences captured under challenging\nconditions, including significant physical attenuation, reduced resolution, and\ndynamic gesture variations commonly experienced in real-world assistive\nenvironments. We further introduce the Radiometric Spatio-Temporal Depth\nAttenuation Loss (RSTDAL), shown to enhance learning and strengthen model\nrobustness across varying distances. Our model demonstrates significant\nperformance improvement over state-of-the-art gesture recognition frameworks,\nachieving a recognition accuracy of 97.3% on a diverse dataset with challenging\nhyper-range gestures. By effectively interpreting gestures from considerable\ndistances, DiG-Net significantly enhances the usability of assistive robots in\nhome healthcare, industrial safety, and remote assistance scenarios, enabling\nseamless and intuitive interactions for users regardless of physical\nlimitations", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiG-Net\u7684\u65b0\u578b\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\uff0c\u652f\u6301\u8fdc\u8fbe30\u7c73\u7684\u8ddd\u79bb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u7684\u4eba\u673a\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u5c40\u9650\u4e8e\u77ed\u8ddd\u79bb\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u8fdc\u8ddd\u79bb\u8f85\u52a9\u901a\u4fe1\u7684\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408Depth-Conditioned Deformable Alignment (DADA)\u5757\u548c\u65f6\u7a7a\u56fe\u6a21\u5757\uff0c\u5e76\u5f15\u5165Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL)\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.3%\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DiG-Net\u663e\u8457\u63d0\u5347\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u62a4\u7406\u3001\u5de5\u4e1a\u5b89\u5168\u548c\u8fdc\u7a0b\u534f\u52a9\u7b49\u573a\u666f\u4e2d\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2505.24778", "pdf": "https://arxiv.org/pdf/2505.24778", "abs": "https://arxiv.org/abs/2505.24778", "authors": ["Jiayu Liu", "Qing Zong", "Weiqi Wang", "Yangqiu Song"], "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "categories": ["cs.CL"], "comment": "ACL2025", "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f7f\u7528\u8ba4\u77e5\u6807\u8bb0\uff08\u5982\u201c\u76f8\u5f53\u81ea\u4fe1\u201d\uff09\u8868\u8fbe\u81ea\u4fe1\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u6807\u8bb0\u5728\u540c\u4e00\u5206\u5e03\u5185\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u4e0d\u4e00\u81f4\u3002", "motivation": "\u7531\u4e8eLLMs\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u51c6\u786e\u8bc4\u4f30\u5176\u81ea\u4fe1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8ba4\u77e5\u6807\u8bb0\u662f\u5426\u80fd\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u5185\u5728\u81ea\u4fe1\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u6807\u8bb0\u81ea\u4fe1\u4e3a\u6a21\u578b\u4f7f\u7528\u8ba4\u77e5\u6807\u8bb0\u65f6\u7684\u89c2\u5bdf\u51c6\u786e\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5f00\u6e90\u548c\u4e13\u6709LLMs\u7684\u8868\u73b0\u3002", "result": "\u6807\u8bb0\u5728\u540c\u4e00\u5206\u5e03\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u81ea\u4fe1\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u8ba4\u77e5\u6807\u8bb0\u7528\u4e8e\u81ea\u4fe1\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u5b58\u7591\uff0c\u9700\u6539\u8fdb\u6807\u8bb0\u81ea\u4fe1\u4e0e\u5b9e\u9645\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u5bf9\u9f50\u3002"}}
{"id": "2505.24788", "pdf": "https://arxiv.org/pdf/2505.24788", "abs": "https://arxiv.org/abs/2505.24788", "authors": ["Houjun Liu", "John Bauer", "Christopher D. Manning"], "title": "Drop Dropout on Single-Epoch Language Model Pretraining", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL Findings; 5 pages, 2 figures, 4 pages of appendix", "summary": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u5355\u5468\u671f\u9884\u8bad\u7ec3\u4e2d\u4e0d\u4f7f\u7528dropout\u53ef\u4ee5\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\u3002", "motivation": "\u63a2\u8ba8dropout\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5355\u5468\u671f\u9884\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540cdropout\u6c34\u5e73\u5728BERT\u3001Pythia 160M\u548c1.4B\u7b49\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u6548\u679c\uff0c\u8bc4\u4f30\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u548c\u6a21\u578b\u53ef\u7f16\u8f91\u6027\u3002", "result": "\u4e0d\u4f7f\u7528dropout\u7684\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8bed\u8a00\u5efa\u6a21\u3001\u95ee\u7b54\u7b49\uff09\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u66f4\u6613\u4e8e\u901a\u8fc7\u68af\u5ea6\u7f16\u8f91\u3002", "conclusion": "\u5efa\u8bae\u5728\u5355\u5468\u671f\u9884\u8bad\u7ec3\u4e2d\u5b8c\u5168\u653e\u5f03\u4f7f\u7528dropout\u3002"}}
{"id": "2505.24799", "pdf": "https://arxiv.org/pdf/2505.24799", "abs": "https://arxiv.org/abs/2505.24799", "authors": ["Aditya Retnanto", "Son Le", "Sebastian Mueller", "Armin Leitner", "Konrad Schindler", "Yohan Iddawela", "Michael Riffler"], "title": "Beyond Pretty Pictures: Combined Single- and Multi-Image Super-resolution for Sentinel-2 Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Super-resolution aims to increase the resolution of satellite images by\nreconstructing high-frequency details, which go beyond na\\\"ive upsampling. This\nhas particular relevance for Earth observation missions like Sentinel-2, which\noffer frequent, regular coverage at no cost; but at coarse resolution. Its\npixel footprint is too large to capture small features like houses, streets, or\nhedge rows. To address this, we present SEN4X, a hybrid super-resolution\narchitecture that combines the advantages of single-image and multi-image\ntechniques. It combines temporal oversampling from repeated Sentinel-2\nacquisitions with a learned prior from high-resolution Pl\\'eiades Neo data. In\ndoing so, SEN4X upgrades Sentinel-2 imagery to 2.5 m ground sampling distance.\nWe test the super-resolved images on urban land-cover classification in Hanoi,\nVietnam. We find that they lead to a significant performance improvement over\nstate-of-the-art super-resolution baselines.", "AI": {"tldr": "SEN4X\u662f\u4e00\u79cd\u6df7\u5408\u8d85\u5206\u8fa8\u7387\u67b6\u6784\uff0c\u7ed3\u5408\u5355\u56fe\u50cf\u548c\u591a\u56fe\u50cf\u6280\u672f\uff0c\u5c06Sentinel-2\u56fe\u50cf\u5206\u8fa8\u7387\u63d0\u5347\u81f32.5\u7c73\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "Sentinel-2\u536b\u661f\u56fe\u50cf\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u65e0\u6cd5\u6355\u6349\u5c0f\u5c3a\u5ea6\u7279\u5f81\uff08\u5982\u623f\u5c4b\u3001\u8857\u9053\uff09\uff0c\u9700\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u5347\u5176\u5206\u8fa8\u7387\u3002", "method": "\u7ed3\u5408Sentinel-2\u7684\u91cd\u590d\u89c2\u6d4b\u6570\u636e\u548c\u9ad8\u5206\u8fa8\u7387Pl\u00e9iades Neo\u6570\u636e\uff0c\u8bbe\u8ba1\u6df7\u5408\u8d85\u5206\u8fa8\u7387\u67b6\u6784SEN4X\u3002", "result": "\u5728\u8d8a\u5357\u6cb3\u5185\u7684\u57ce\u5e02\u571f\u5730\u8986\u76d6\u5206\u7c7b\u6d4b\u8bd5\u4e2d\uff0cSEN4X\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SEN4X\u6709\u6548\u63d0\u5347\u4e86Sentinel-2\u56fe\u50cf\u7684\u5206\u8fa8\u7387\uff0c\u9002\u7528\u4e8e\u5c0f\u5c3a\u5ea6\u7279\u5f81\u5206\u6790\u3002"}}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803", "abs": "https://arxiv.org/abs/2505.24803", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "title": "Guiding Generative Storytelling with Knowledge Graphs", "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.", "AI": {"tldr": "\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u8f85\u52a9\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6545\u4e8b\u751f\u6210\u4e2d\u63d0\u5347\u53d9\u4e8b\u8d28\u91cf\u548c\u7528\u6237\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u957f\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u8fde\u8d2f\u6027\u548c\u7528\u6237\u63a7\u5236\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faKG\u8f85\u52a9\u7684\u6545\u4e8b\u751f\u6210\u6d41\u7a0b\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u77e5\u8bc6\u56fe\u8c31\u663e\u8457\u63d0\u5347\u52a8\u4f5c\u5bfc\u5411\u548c\u7ed3\u6784\u5316\u53d9\u4e8b\u7684\u8d28\u91cf\uff0c\u589e\u5f3a\u7528\u6237\u63a7\u5236\u611f\u3002", "conclusion": "KG\u8f85\u52a9\u7684LLM\u6545\u4e8b\u751f\u6210\u66f4\u5177\u4e92\u52a8\u6027\u548c\u8da3\u5473\u6027\u3002"}}
{"id": "2505.24819", "pdf": "https://arxiv.org/pdf/2505.24819", "abs": "https://arxiv.org/abs/2505.24819", "authors": ["Haozhan Tang", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Bi-Manual Joint Camera Calibration and Scene Representation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Robot manipulation, especially bimanual manipulation, often requires setting\nup multiple cameras on multiple robot manipulators. Before robot manipulators\ncan generate motion or even build representations of their environments, the\ncameras rigidly mounted to the robot need to be calibrated. Camera calibration\nis a cumbersome process involving collecting a set of images, with each\ncapturing a pre-determined marker. In this work, we introduce the Bi-Manual\nJoint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables\nmultiple robot manipulators, each with cameras mounted, to circumvent taking\nimages of calibration markers. By leveraging 3D foundation models for dense,\nmarker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the\nextrinsic transformation from each camera to its end-effector, (ii) the\ninter-arm relative poses between manipulators, and (iii) a unified,\nscale-consistent 3D representation of the shared workspace, all from the same\ncaptured RGB image sets. The representation, jointly constructed from images\ncaptured by cameras on both manipulators, lives in a common coordinate frame\nand supports collision checking and semantic segmentation to facilitate\ndownstream bimanual coordination tasks. We empirically evaluate the robustness\nof Bi-JCR on a variety of tabletop environments, and demonstrate its\napplicability on a variety of downstream tasks.", "AI": {"tldr": "Bi-JCR\u6846\u67b6\u901a\u8fc73D\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u591a\u89c6\u89d2\u65e0\u6807\u8bb0\u6821\u51c6\uff0c\u8054\u5408\u4f30\u8ba1\u76f8\u673a\u5916\u53c2\u3001\u673a\u68b0\u81c2\u95f4\u4f4d\u59ff\u53ca\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u76843D\u8868\u793a\uff0c\u7b80\u5316\u4e86\u53cc\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u6821\u51c6\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u76f8\u673a\u6821\u51c6\u8fc7\u7a0b\u7e41\u7410\uff0c\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u6807\u8bb0\u7269\uff0c\u9650\u5236\u4e86\u53cc\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5229\u75283D\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5bc6\u96c6\u65e0\u6807\u8bb0\u591a\u89c6\u89d2\u5bf9\u5e94\uff0c\u8054\u5408\u4f30\u8ba1\u76f8\u673a\u5916\u53c2\u3001\u673a\u68b0\u81c2\u95f4\u4f4d\u59ff\u53ca\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u76843D\u8868\u793a\u3002", "result": "Bi-JCR\u5728\u591a\u79cd\u684c\u9762\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u652f\u6301\u78b0\u649e\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u53cc\u673a\u68b0\u81c2\u534f\u8c03\u4efb\u52a1\u3002", "conclusion": "Bi-JCR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u6807\u8bb0\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u663e\u8457\u7b80\u5316\u4e86\u53cc\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u521d\u59cb\u5316\u6d41\u7a0b\u3002"}}
{"id": "2505.24826", "pdf": "https://arxiv.org/pdf/2505.24826", "abs": "https://arxiv.org/abs/2505.24826", "authors": ["Li yunhan", "Wu gengshen"], "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text", "categories": ["cs.CL", "cs.CV"], "comment": "10 pages, 11 figures", "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6cd5\u5f8b\u6587\u672c\u8d28\u91cf\uff08\u6e05\u6670\u5ea6\u3001\u8fde\u8d2f\u6027\u3001\u672f\u8bed\u51c6\u786e\u6027\uff09\u7684\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e8649\u4e2aLLM\uff0c\u53d1\u73b0\u6a21\u578b\u8d28\u91cf\u5728140\u4ebf\u53c2\u6570\u540e\u8d8b\u4e8e\u7a33\u5b9a\uff0c\u63a8\u7406\u6a21\u578b\u4f18\u4e8e\u57fa\u7840\u67b6\u6784\uff0c\u540c\u65f6\u53d1\u5e03\u4e86Qwen3\u7cfb\u5217\u4f5c\u4e3a\u6027\u4ef7\u6bd4\u6700\u4f18\u7684\u63a8\u8350\u3002", "motivation": "\u5f53\u524d\u6cd5\u5f8b\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u8bed\u8a00\u8d28\u91cf\uff08\u5982\u6e05\u6670\u5ea6\u3001\u8fde\u8d2f\u6027\u3001\u672f\u8bed\u51c6\u786e\u6027\uff09\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1. \u5f00\u53d1\u56de\u5f52\u6a21\u578b\u8bc4\u4f30\u6cd5\u5f8b\u6587\u672c\u8d28\u91cf\uff1b2. \u521b\u5efa\u4e13\u95e8\u7684\u6cd5\u5f8b\u95ee\u9898\u96c6\uff1b3. \u7528\u6b64\u6846\u67b6\u5206\u679049\u4e2aLLM\u3002", "result": "1. \u6a21\u578b\u8d28\u91cf\u5728140\u4ebf\u53c2\u6570\u540e\u63d0\u5347\u6709\u9650\uff0872\u4ebf\u53c2\u6570\u4ec5\u63d0\u53472.7%\uff09\uff1b2. \u91cf\u5316\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5f71\u54cd\u4e0d\u663e\u8457\uff1b3. \u63a8\u7406\u6a21\u578b\u4f18\u4e8e\u57fa\u7840\u67b6\u6784\u3002Qwen3\u7cfb\u5217\u4e3a\u6027\u4ef7\u6bd4\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u6cd5\u5f8bLLM\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2505.24830", "pdf": "https://arxiv.org/pdf/2505.24830", "abs": "https://arxiv.org/abs/2505.24830", "authors": ["Juraj Vladika", "Annika Domres", "Mai Nguyen", "Rebecca Moser", "Jana Nano", "Felix Busch", "Lisa C. Adams", "Keno K. Bressem", "Denise Bernhardt", "Stephanie E. Combs", "Kai J. Borm", "Florian Matthes", "Jan C. Peeken"], "title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5b50\u4e8b\u5b9e\u68c0\u67e5\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3LLM\u751f\u6210\u7684\u56de\u7b54\u4e3a\u53ef\u9a8c\u8bc1\u7684\u539f\u5b50\u4e8b\u5b9e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u95ee\u7b54\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u548c\u5f15\u7528\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u63d0\u5347\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u5408\u89c4\u6027\u3002", "method": "\u5c06LLM\u751f\u6210\u7684\u56de\u7b54\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e\uff0c\u5e76\u72ec\u7acb\u9a8c\u8bc1\u6bcf\u4e2a\u4e8b\u5b9e\u4e0e\u6743\u5a01\u533b\u5b66\u6307\u5357\u7684\u5339\u914d\u6027\u3002", "result": "\u5728\u533b\u5b66\u4e13\u5bb6\u8bc4\u4f30\u548c\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u663e\u8457\u63d0\u5347\uff0c\u6574\u4f53\u56de\u7b54\u6539\u8fdb40%\uff0c\u5e7b\u89c9\u68c0\u6d4b\u7387\u8fbe50%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u5728\u4e34\u5e8a\u533b\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u548c\u53ef\u9760\u7684\u652f\u6301\uff0c\u662f\u8fc8\u5411AI\u8f85\u52a9\u533b\u7597\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.24878", "pdf": "https://arxiv.org/pdf/2505.24878", "abs": "https://arxiv.org/abs/2505.24878", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld", "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.", "AI": {"tldr": "Open CaptchaWorld\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001LLM\u4ee3\u7406\u5728\u52a8\u6001CAPTCHA\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u5e73\u53f0\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u4ee3\u7406\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "CAPTCHAs\u662f\u7f51\u7edc\u4ee3\u7406\u90e8\u7f72\u7684\u5173\u952e\u74f6\u9888\uff0c\u73b0\u6709\u591a\u6a21\u6001LLM\u4ee3\u7406\u5728\u4ea4\u4e92\u5f0f\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faOpen CaptchaWorld\u57fa\u51c6\uff0c\u6db5\u76d620\u79cdCAPTCHA\u7c7b\u578b\uff0c\u5171225\u4e2a\u4efb\u52a1\uff0c\u5e76\u5f15\u5165CAPTCHA Reasoning Depth\u6307\u6807\u3002", "result": "\u4eba\u7c7b\u8868\u73b0\u63a5\u8fd1\u5b8c\u7f8e\uff0893.3%\uff09\uff0c\u800c\u6700\u4f73MLLM\u4ee3\u7406\u6210\u529f\u7387\u4ec5\u4e3a40.0%\u3002", "conclusion": "Open CaptchaWorld\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u4ee3\u7406\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2505.24832", "pdf": "https://arxiv.org/pdf/2505.24832", "abs": "https://arxiv.org/abs/2505.24832", "authors": ["John X. Morris", "Chawin Sitawarin", "Chuan Guo", "Narine Kokhlikyan", "G. Edward Suh", "Alexander M. Rush", "Kamalika Chaudhuri", "Saeed Mahloujifar"], "title": "How much do language models memorize?", "categories": ["cs.CL"], "comment": null, "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u6a21\u578b\u5bf9\u6570\u636e\u70b9\u7684\u201c\u4e86\u89e3\u201d\u7a0b\u5ea6\uff0c\u5e76\u6d4b\u91cf\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u7684\u5bb9\u91cf\u3002\u7814\u7a76\u53d1\u73b0\uff0cGPT\u7c7b\u6a21\u578b\u7684\u5bb9\u91cf\u7ea6\u4e3a\u6bcf\u53c2\u65703.6\u6bd4\u7279\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u96be\u4ee5\u533a\u5206\u8bb0\u5fc6\u4e0e\u6cdb\u5316\uff0c\u672c\u6587\u65e8\u5728\u660e\u786e\u5206\u79bb\u8fd9\u4e24\u79cd\u80fd\u529b\u3002", "method": "\u5c06\u8bb0\u5fc6\u5206\u4e3a\u65e0\u610f\u8bb0\u5fc6\u548c\u6cdb\u5316\u4e24\u90e8\u5206\uff0c\u901a\u8fc7\u6d88\u9664\u6cdb\u5316\u8ba1\u7b97\u603b\u8bb0\u5fc6\u91cf\uff0c\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u5e76\u89c2\u5bdf\u5176\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u5728\u5bb9\u91cf\u586b\u6ee1\u524d\u4f1a\u8bb0\u5fc6\u6570\u636e\uff0c\u4e4b\u540e\u5f00\u59cb\u6cdb\u5316\uff1b\u8bad\u7ec3\u4e86\u6570\u767e\u4e2a\u6a21\u578b\u5e76\u5efa\u7acb\u4e86\u5bb9\u91cf\u4e0e\u6570\u636e\u89c4\u6a21\u7684\u6bd4\u4f8b\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u6a21\u578b\u5bb9\u91cf\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2505.24834", "pdf": "https://arxiv.org/pdf/2505.24834", "abs": "https://arxiv.org/abs/2505.24834", "authors": ["Roksana Goworek", "Haim Dubossarsky"], "title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks", "categories": ["cs.CL"], "comment": "8 pages, 8 figures", "summary": "Cross-lingual transfer allows models to perform tasks in languages unseen\nduring training and is often assumed to benefit from increased multilinguality.\nIn this work, we challenge this assumption in the context of two underexplored,\nsense-aware tasks: polysemy disambiguation and lexical semantic change. Through\na large-scale analysis across 28 languages, we show that multilingual training\nis neither necessary nor inherently beneficial for effective transfer. Instead,\nwe find that confounding factors - such as fine-tuning data composition and\nevaluation artifacts - better account for the perceived advantages of\nmultilinguality. Our findings call for more rigorous evaluations in\nmultilingual NLP. We release fine-tuned models and benchmarks to support\nfurther research, with implications extending to low-resource and typologically\ndiverse languages.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u8bed\u8a00\u8bad\u7ec3\u5e76\u975e\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4e14\u5176\u4f18\u52bf\u53ef\u80fd\u88ab\u5176\u4ed6\u56e0\u7d20\uff08\u5982\u5fae\u8c03\u6570\u636e\u6784\u6210\u548c\u8bc4\u4f30\u65b9\u6cd5\uff09\u6240\u63a9\u76d6\u3002", "motivation": "\u6311\u6218\u591a\u8bed\u8a00\u8bad\u7ec3\u5bf9\u8de8\u8bed\u8a00\u8fc1\u79fb\u4efb\u52a1\uff08\u5982\u591a\u4e49\u8bcd\u6d88\u6b67\u548c\u8bcd\u6c47\u8bed\u4e49\u53d8\u5316\uff09\u7684\u56fa\u6709\u4f18\u52bf\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u5bf928\u79cd\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u5206\u6790\uff0c\u63a2\u8ba8\u591a\u8bed\u8a00\u8bad\u7ec3\u7684\u5b9e\u9645\u6548\u679c\u3002", "result": "\u53d1\u73b0\u591a\u8bed\u8a00\u8bad\u7ec3\u5e76\u975e\u5fc5\u8981\uff0c\u5176\u4f18\u52bf\u53ef\u80fd\u6e90\u4e8e\u5176\u4ed6\u56e0\u7d20\u3002", "conclusion": "\u547c\u5401\u66f4\u4e25\u8c28\u7684\u591a\u8bed\u8a00NLP\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u6a21\u578b\u548c\u57fa\u51c6\u4ee5\u652f\u6301\u7814\u7a76\u3002"}}
{"id": "2505.24858", "pdf": "https://arxiv.org/pdf/2505.24858", "abs": "https://arxiv.org/abs/2505.24858", "authors": ["Gabrielle Kaili-May Liu", "Gal Yona", "Avi Caciularu", "Idan Szpektor", "Tim G. J. Rudner", "Arman Cohan"], "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u4e0a\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MetaFaith\u65b9\u6cd5\u4ee5\u6539\u8fdb\u5176\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "motivation": "LLM\u5728\u4f20\u9012\u865a\u5047\u4fe1\u606f\u65f6\u5f80\u5f80\u4f7f\u7528\u8fc7\u4e8e\u80af\u5b9a\u7684\u8bed\u8a00\uff0c\u5bfc\u81f4\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u548c\u4fe1\u4efb\u5ea6\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u8ba9LLM\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u66f4\u5fe0\u5b9e\u4e8e\u5176\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76LLM\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u80fd\u529b\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4eba\u7c7b\u5143\u8ba4\u77e5\u7684MetaFaith\u6821\u51c6\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709LLM\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u5e72\u9884\u63aa\u65bd\u6548\u679c\u6709\u9650\uff0c\u800cMetaFaith\u663e\u8457\u63d0\u5347\u4e86\u6821\u51c6\u7684\u5fe0\u5b9e\u6027\uff0c\u6700\u9ad8\u63d0\u534761%\u3002", "conclusion": "MetaFaith\u662f\u4e00\u79cd\u6709\u6548\u7684LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.24863", "pdf": "https://arxiv.org/pdf/2505.24863", "abs": "https://arxiv.org/abs/2505.24863", "authors": ["Junyu Zhang", "Runpei Dong", "Han Wang", "Xuying Ning", "Haoran Geng", "Peihao Li", "Xialin He", "Yutong Bai", "Jitendra Malik", "Saurabh Gupta", "Huan Zhang"], "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents AlphaOne ($\\alpha$1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\n$\\alpha$1 first introduces $\\alpha$ moment, which represents the scaled\nthinking phase with a universal parameter $\\alpha$. Within this scaled\npre-$\\alpha$ moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the $\\alpha$ moment, $\\alpha$1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate $\\alpha$1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/", "AI": {"tldr": "AlphaOne\uff08\u03b11\uff09\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u8c03\u8282\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u8fdb\u5ea6\u3002\u901a\u8fc7\u5f15\u5165\u03b1\u65f6\u523b\u548c\u52a8\u6001\u8c03\u5ea6\u6162\u601d\u8003\u8f6c\u6362\uff0c\u03b11\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u6162\u5230\u5feb\u63a8\u7406\u8c03\u8282\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u03b11\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u63a8\u7406\u8fdb\u5ea6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u03b11\u5f15\u5165\u03b1\u65f6\u523b\uff0c\u901a\u8fc7\u4f2f\u52aa\u5229\u968f\u673a\u8fc7\u7a0b\u52a8\u6001\u8c03\u5ea6\u6162\u601d\u8003\u8f6c\u6362\uff0c\u5e76\u5728\u03b1\u65f6\u523b\u540e\u786e\u5b9a\u6027\u5730\u7ec8\u6b62\u6162\u601d\u8003\uff0c\u4ee5\u4fc3\u8fdb\u5feb\u901f\u63a8\u7406\u3002", "result": "\u5728\u6570\u5b66\u3001\u7f16\u7801\u548c\u79d1\u5b66\u9886\u57df\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u03b11\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u548c\u6548\u7387\u3002", "conclusion": "\u03b11\u901a\u8fc7\u7edf\u4e00\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u8c03\u8282\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.24864", "pdf": "https://arxiv.org/pdf/2505.24864", "abs": "https://arxiv.org/abs/2505.24864", "authors": ["Mingjie Liu", "Shizhe Diao", "Ximing Lu", "Jian Hu", "Xin Dong", "Yejin Choi", "Jan Kautz", "Yi Dong"], "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 17 figures", "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u653e\u5927\u5df2\u6709\u8f93\u51fa\u3002\u63d0\u51fa\u7684ProRL\u65b9\u6cd5\u901a\u8fc7KL\u6563\u5ea6\u63a7\u5236\u548c\u4efb\u52a1\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8RL\u662f\u5426\u80fd\u771f\u6b63\u6269\u5c55\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u653e\u5927\u5df2\u6709\u8f93\u51fa\uff0c\u5e76\u9a8c\u8bc1RL\u8ba1\u7b97\u89c4\u6a21\u7684\u589e\u52a0\u662f\u5426\u53ef\u9760\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51faProRL\u65b9\u6cd5\uff0c\u7ed3\u5408KL\u6563\u5ea6\u63a7\u5236\u3001\u53c2\u8003\u7b56\u7565\u91cd\u7f6e\u548c\u591a\u6837\u5316\u4efb\u52a1\uff0c\u8fdb\u884c\u957f\u671fRL\u8bad\u7ec3\u3002", "result": "RL\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u57fa\u7840\u6a21\u578b\u5b8c\u5168\u5931\u8d25\u3002\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u4e0e\u57fa\u7840\u6a21\u578b\u80fd\u529b\u548c\u8bad\u7ec3\u65f6\u957f\u76f8\u5173\u3002", "conclusion": "RL\u80fd\u63a2\u7d22\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\uff0c\u4e3a\u672a\u6765\u957f\u671fRL\u63a8\u7406\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.23783", "pdf": "https://arxiv.org/pdf/2505.23783", "abs": "https://arxiv.org/abs/2505.23783", "authors": ["Korel Gundem", "Juncheng Dong", "Dennis Zhang", "Vahid Tarokh", "Zhengling Qi"], "title": "Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new\ntasks with just a few examples, but their predictions often suffer from\nsystematic biases, leading to unstable performances in classification. While\ncalibration techniques are proposed to mitigate these biases, we show that, in\nthe logit space, many of these methods are equivalent to merely shifting the\nLLM's decision boundary without having the ability to alter its orientation.\nThis proves inadequate when biases cause the LLM to be severely misdirected. To\naddress these limitations and provide a unifying framework, we propose\nSupervised Calibration (SC), a loss-minimization based framework which learns\nan optimal, per-class affine transformation of the LLM's predictive\nprobabilities in the logit space without requiring external data beyond the\ncontext. By using a more expressive functional class, SC not only subsumes many\nexisting calibration methods in ICL as special cases, but also enables the\nability to alter and even completely reverse the orientation of the LLM's\ndecision boundary. Furthermore, SC's loss-based nature facilitates the seamless\nintegration of two purpose-built regularization techniques: context-invariance\nand directional trust-region. The former is designed to tackle the instability\nissue in ICL, while the latter controls the degree of calibration. Finally, SC\ndelivers state-of-the-art performance over calibration baselines in the 4-shot,\n8-shot, and 16-shot settings across all nine datasets for\nMistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u76d1\u7763\u6821\u51c6\uff08SC\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6bcf\u7c7b\u7684\u4eff\u5c04\u53d8\u6362\u6765\u4fee\u6b63\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u4ec5\u80fd\u5e73\u79fb\u51b3\u7b56\u8fb9\u754c\uff0c\u65e0\u6cd5\u4fee\u6b63\u65b9\u5411\u6027\u504f\u5dee\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002", "method": "SC\u6846\u67b6\u5728logit\u7a7a\u95f4\u4e2d\u5b66\u4e60\u6700\u4f18\u7684\u4eff\u5c04\u53d8\u6362\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\uff0c\u5e76\u652f\u6301\u6b63\u5219\u5316\u6280\u672f\u3002", "result": "SC\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\uff08\u5982Mistral-7B\u3001LLaMA-2-7B\u3001Qwen2-7B\uff09\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SC\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u6821\u51c6\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23841", "pdf": "https://arxiv.org/pdf/2505.23841", "abs": "https://arxiv.org/abs/2505.23841", "authors": ["Hairu Wang", "Yuan Feng", "Yukun Cao", "Xike Xie", "S Kevin Zhou"], "title": "SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large language models excel at many tasks but often incur high inference\ncosts during deployment. To mitigate hallucination, many systems use a\nknowledge graph to enhance retrieval-augmented generation (KG-RAG). However,\nthe large amount of retrieved knowledge contexts increase these inference costs\nfurther. A promising solution to balance performance and cost is LLM routing,\nwhich directs simple queries to smaller LLMs and complex ones to larger LLMs.\nHowever, no dedicated routing methods currently exist for RAG, and existing\ntraining-based routers face challenges scaling to this domain due to the need\nfor extensive training data. We observe that the score distributions produced\nby the retrieval scorer strongly correlate with query difficulty. Based on\nthis, we propose a novel, training-free routing framework, the first tailored\nto KG-RAG that effectively balances performance and cost in a plug-and-play\nmanner. Experiments show our method reduces calls to larger LLMs by up to 50%\nwithout sacrificing response quality, demonstrating its potential for efficient\nand scalable LLM deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KG-RAG\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u8bc4\u5206\u5206\u5e03\u5224\u65ad\u67e5\u8be2\u96be\u5ea6\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\uff0c\u5b9e\u9a8c\u663e\u793a\u53ef\u51cf\u5c1150%\u5bf9\u5927\u6a21\u578b\u7684\u8c03\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u73b0\u6709KG-RAG\u7cfb\u7edf\u56e0\u68c0\u7d22\u5927\u91cf\u77e5\u8bc6\u4e0a\u4e0b\u6587\u8fdb\u4e00\u6b65\u589e\u52a0\u6210\u672c\uff0c\u9700\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "\u57fa\u4e8e\u68c0\u7d22\u8bc4\u5206\u5206\u5e03\u4e0e\u67e5\u8be2\u96be\u5ea6\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KG-RAG\u8def\u7531\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u51cf\u5c11\u5bf9\u5927\u6a21\u578b50%\u7684\u8c03\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2505.23881", "pdf": "https://arxiv.org/pdf/2505.23881", "abs": "https://arxiv.org/abs/2505.23881", "authors": ["Christopher D. Rosin"], "title": "Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems", "categories": ["cs.AI", "cs.CL", "math.CO"], "comment": "arXiv admin note: text overlap with arXiv:2501.17725", "summary": "Large Language Models (LLMs) with reasoning are trained to iteratively\ngenerate and refine their answers before finalizing them, which can help with\napplications to mathematics and code generation. We apply code generation with\nreasoning LLMs to a specific task in the mathematical field of combinatorial\ndesign. This field studies diverse types of combinatorial designs, many of\nwhich have lists of open instances for which existence has not yet been\ndetermined. The Constructive Protocol CPro1 uses LLMs to generate search\nheuristics that have the potential to construct solutions to small open\ninstances. Starting with a textual definition and a validity verifier for a\nparticular type of design, CPro1 guides LLMs to select and implement\nstrategies, while providing automated hyperparameter tuning and execution\nfeedback. CPro1 with reasoning LLMs successfully solves long-standing open\ninstances for 7 of 16 combinatorial design problems selected from the 2006\nHandbook of Combinatorial Designs, including new solved instances for 3 of\nthese (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary\nDesigns) that were unsolved by CPro1 with non-reasoning LLMs. It also solves\nopen instances for several problems from recent (2025) literature, generating\nnew Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform\nNested Steiner Quadruple System.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u63a8\u7406\u80fd\u529b\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ec4\u5408\u8bbe\u8ba1\u9886\u57df\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7CPro1\u534f\u8bae\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4e2a\u957f\u671f\u672a\u89e3\u51b3\u7684\u7ec4\u5408\u8bbe\u8ba1\u95ee\u9898\u3002", "motivation": "\u7ec4\u5408\u8bbe\u8ba1\u9886\u57df\u5b58\u5728\u8bb8\u591a\u672a\u89e3\u51b3\u7684\u5b9e\u4f8b\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002\u901a\u8fc7\u7ed3\u5408\u63a8\u7406\u80fd\u529b\u7684LLMs\uff0c\u53ef\u4ee5\u751f\u6210\u6709\u6548\u7684\u641c\u7d22\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528CPro1\u534f\u8bae\uff0c\u7ed3\u5408\u63a8\u7406LLMs\uff0c\u4ece\u6587\u672c\u5b9a\u4e49\u548c\u9a8c\u8bc1\u5668\u51fa\u53d1\uff0c\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u5b9e\u65bd\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u6574\u548c\u6267\u884c\u53cd\u9988\u3002", "result": "\u6210\u529f\u89e3\u51b3\u4e8616\u4e2a\u7ec4\u5408\u8bbe\u8ba1\u95ee\u9898\u4e2d\u76847\u4e2a\u957f\u671f\u672a\u89e3\u5b9e\u4f8b\uff0c\u5305\u62ec3\u4e2a\u65b0\u89e3\u51b3\u7684\u5b9e\u4f8b\uff0c\u5e76\u57282025\u5e74\u6587\u732e\u4e2d\u89e3\u51b3\u4e86\u66f4\u591a\u95ee\u9898\u3002", "conclusion": "\u63a8\u7406LLMs\u4e0eCPro1\u534f\u8bae\u7684\u7ed3\u5408\u4e3a\u7ec4\u5408\u8bbe\u8ba1\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86LLMs\u5728\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.23885", "pdf": "https://arxiv.org/pdf/2505.23885", "abs": "https://arxiv.org/abs/2505.23885", "authors": ["Mengkang Hu", "Yuhang Zhou", "Wendong Fan", "Yuzhou Nie", "Bowei Xia", "Tao Sun", "Ziyu Ye", "Zhaoxuan Jin", "Yingru Li", "Qiguang Chen", "Zeyu Zhang", "Yifeng Wang", "Qianshuo Ye", "Bernard Ghanem", "Ping Luo", "Guohao Li"], "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "categories": ["cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/camel-ai/owl", "summary": "Large Language Model (LLM)-based multi-agent systems show promise for\nautomating real-world tasks but struggle to transfer across domains due to\ntheir domain-specific nature. Current approaches face two critical\nshortcomings: they require complete architectural redesign and full retraining\nof all components when applied to new domains. We introduce Workforce, a\nhierarchical multi-agent framework that decouples strategic planning from\nspecialized execution through a modular architecture comprising: (i) a\ndomain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask\nmanagement, and (iii) specialized Workers with domain-specific tool-calling\ncapabilities. This decoupling enables cross-domain transferability during both\ninference and training phases: During inference, Workforce seamlessly adapts to\nnew domains by adding or modifying worker agents; For training, we introduce\nOptimized Workforce Learning (OWL), which improves generalization across\ndomains by optimizing a domain-agnostic planner with reinforcement learning\nfrom real-world feedback. To validate our approach, we evaluate Workforce on\nthe GAIA benchmark, covering various realistic, multi-domain agentic tasks.\nExperimental results demonstrate Workforce achieves open-source\nstate-of-the-art performance (69.70%), outperforming commercial systems like\nOpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model\nachieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to\nGPT-4o on challenging tasks. To summarize, by enabling scalable generalization\nand modular domain transfer, our work establishes a foundation for the next\ngeneration of general-purpose AI assistants.", "AI": {"tldr": "Workforce\u662f\u4e00\u4e2a\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u5b9e\u73b0\u8de8\u9886\u57df\u4efb\u52a1\u8f6c\u79fb\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bbe\u8ba1\u6216\u8bad\u7ec3\u6240\u6709\u7ec4\u4ef6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8de8\u9886\u57df\u4efb\u52a1\u8f6c\u79fb\u65f6\u9762\u4e34\u67b6\u6784\u91cd\u65b0\u8bbe\u8ba1\u548c\u5168\u7ec4\u4ef6\u91cd\u65b0\u8bad\u7ec3\u7684\u6311\u6218\u3002", "method": "Workforce\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u62ec\u9886\u57df\u65e0\u5173\u7684\u89c4\u5212\u5668\u3001\u534f\u8c03\u5668\u548c\u9886\u57df\u7279\u5b9a\u7684\u5de5\u4f5c\u5668\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u5b66\u4e60\uff08OWL\uff09\u63d0\u5347\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWorkforce\u8fbe\u523069.70%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5546\u4e1a\u7cfb\u7edf\uff0c\u517632B\u6a21\u578b\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1GPT-4o\u3002", "conclusion": "Workforce\u901a\u8fc7\u6a21\u5757\u5316\u548c\u4f18\u5316\u5b66\u4e60\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u901a\u7528AI\u52a9\u624b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.23960", "pdf": "https://arxiv.org/pdf/2505.23960", "abs": "https://arxiv.org/abs/2505.23960", "authors": ["Henry Conklin"], "title": "Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "PhD Thesis, 204 pages; entropy estimation discussed from p.94", "summary": "Despite the remarkable success of large large-scale neural networks, we still\nlack unified notation for thinking about and describing their representational\nspaces. We lack methods to reliably describe how their representations are\nstructured, how that structure emerges over training, and what kinds of\nstructures are desirable. This thesis introduces quantitative methods for\nidentifying systematic structure in a mapping between spaces, and leverages\nthem to understand how deep-learning models learn to represent information,\nwhat representational structures drive generalisation, and how design decisions\ncondition the structures that emerge. To do this I identify structural\nprimitives present in a mapping, along with information theoretic\nquantifications of each. These allow us to analyse learning, structure, and\ngeneralisation across multi-agent reinforcement learning models,\nsequence-to-sequence models trained on a single task, and Large Language\nModels. I also introduce a novel, performant, approach to estimating the\nentropy of vector space, that allows this analysis to be applied to models\nranging in size from 1 million to 12 billion parameters.\n  The experiments here work to shed light on how large-scale distributed models\nof cognition learn, while allowing us to draw parallels between those systems\nand their human analogs. They show how the structures of language and the\nconstraints that give rise to them in many ways parallel the kinds of\nstructures that drive performance of contemporary neural networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5b9a\u91cf\u65b9\u6cd5\u6765\u5206\u6790\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u7a7a\u95f4\u7ed3\u6784\uff0c\u63a2\u8ba8\u4e86\u5b66\u4e60\u3001\u6cdb\u5316\u548c\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u7ed3\u6784\u7684\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u4f30\u8ba1\u5411\u91cf\u7a7a\u95f4\u71b5\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u8868\u793a\u7a7a\u95f4\u63cf\u8ff0\u65b9\u6cd5\uff0c\u65e0\u6cd5\u53ef\u9760\u5730\u63cf\u8ff0\u5176\u7ed3\u6784\u3001\u5b66\u4e60\u8fc7\u7a0b\u53ca\u7406\u60f3\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u6620\u5c04\u4e2d\u7684\u7ed3\u6784\u57fa\u5143\u548c\u4fe1\u606f\u8bba\u91cf\u5316\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u3001\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u4e0e\u6cdb\u5316\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8ba4\u77e5\u6a21\u578b\u7684\u5b66\u4e60\u673a\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u8a00\u7ed3\u6784\u4e0e\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u9a71\u52a8\u7ed3\u6784\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u7a7a\u95f4\u7684\u7ed3\u6784\u53ca\u5176\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u7c7b\u6bd4\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u89c1\u89e3\u3002"}}
{"id": "2505.23987", "pdf": "https://arxiv.org/pdf/2505.23987", "abs": "https://arxiv.org/abs/2505.23987", "authors": ["Vishal Dey", "Xiao Hu", "Xia Ning"], "title": "Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "In real-world drug design, molecule optimization requires selectively\nimproving multiple molecular properties up to pharmaceutically relevant levels,\nwhile maintaining others that already meet such criteria. However, existing\ncomputational approaches and instruction-tuned LLMs fail to capture such\nnuanced property-specific objectives, limiting their practical applicability.\nTo address this, we introduce C-MuMOInstruct, the first instruction-tuning\ndataset focused on multi-property optimization with explicit, property-specific\nobjectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of\ninstruction-tuned LLMs that can perform targeted property-specific\noptimization. Our experiments across 5 in-distribution and 5\nout-of-distribution tasks show that GeLLMO-Cs consistently outperform strong\nbaselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit\nimpressive 0-shot generalization to novel optimization tasks and unseen\ninstructions. This offers a step toward a foundational LLM to support\nrealistic, diverse optimizations with property-specific objectives.\nC-MuMOInstruct and code are accessible through\nhttps://github.com/ninglab/GeLLMO-C.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faC-MuMOInstruct\u6570\u636e\u96c6\u548cGeLLMO-Cs\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u5c5e\u6027\u5206\u5b50\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u548cLLM\u96be\u4ee5\u5b9e\u73b0\u591a\u5c5e\u6027\u5206\u5b50\u4f18\u5316\u4e2d\u7684\u5c5e\u6027\u7279\u5f02\u6027\u76ee\u6807\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528C-MuMOInstruct\u6570\u636e\u96c6\u5f00\u53d1GeLLMO-Cs\u6a21\u578b\uff0c\u652f\u6301\u5c5e\u6027\u7279\u5f02\u6027\u4f18\u5316\u3002", "result": "GeLLMO-Cs\u57285\u4e2a\u5206\u5e03\u5185\u548c5\u4e2a\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u63d0\u5347126%\uff0c\u5e76\u5c55\u793a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GeLLMO-Cs\u4e3a\u652f\u6301\u73b0\u5b9e\u591a\u6837\u5316\u7684\u5c5e\u6027\u7279\u5f02\u6027\u4f18\u5316\u63d0\u4f9b\u4e86\u57fa\u7840LLM\u3002"}}
{"id": "2505.24004", "pdf": "https://arxiv.org/pdf/2505.24004", "abs": "https://arxiv.org/abs/2505.24004", "authors": ["Amanda Chan", "Catherine Di", "Joseph Rupertus", "Gary Smith", "Varun Nagaraj Rao", "Manoel Horta Ribeiro", "Andr\u00e9s Monroy-Hern\u00e1ndez"], "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "Accepted as a CHI Late Breaking Work (2025), cite appropriately", "summary": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for\nresearch, yet workers' growing use of generative AI tools poses challenges.\nResearchers face compromised data validity as AI responses replace authentic\nhuman behavior, while workers risk diminished roles as AI automates tasks. To\naddress this, we propose a hybrid framework using digital twins, personalized\nAI models that emulate workers' behaviors and preferences while keeping humans\nin the loop. We evaluate our system with an experiment (n=88 crowd workers) and\nin-depth interviews with crowd workers (n=5) and social science researchers\n(n=4). Our results suggest that digital twins may enhance productivity and\nreduce decision fatigue while maintaining response quality. Both researchers\nand workers emphasized the importance of transparency, ethical data use, and\nworker agency. By automating repetitive tasks and preserving human engagement\nfor nuanced ones, digital twins may help balance scalability with authenticity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4f17\u5305\u5e73\u53f0\u4e2d\u751f\u6210\u5f0fAI\u5de5\u5177\u5bf9\u6570\u636e\u771f\u5b9e\u6027\u548c\u5de5\u4eba\u89d2\u8272\u7684\u5f71\u54cd\u3002", "motivation": "\u4f17\u5305\u5e73\u53f0\u4e2d\u5de5\u4eba\u4f7f\u7528\u751f\u6210\u5f0fAI\u5de5\u5177\u5bfc\u81f4\u6570\u636e\u771f\u5b9e\u6027\u4e0b\u964d\uff0c\u5de5\u4eba\u89d2\u8272\u88ab\u524a\u5f31\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316AI\u6a21\u578b\u6a21\u62df\u5de5\u4eba\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u53c2\u4e0e\u3002\u5b9e\u9a8c\uff08n=88\u5de5\u4eba\uff09\u548c\u8bbf\u8c08\uff08\u5de5\u4eban=5\uff0c\u7814\u7a76\u8005n=4\uff09\u9a8c\u8bc1\u3002", "result": "\u6570\u5b57\u5b6a\u751f\u53ef\u63d0\u9ad8\u751f\u4ea7\u529b\u3001\u51cf\u5c11\u51b3\u7b56\u75b2\u52b3\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u3002\u900f\u660e\u6027\u3001\u4f26\u7406\u6570\u636e\u4f7f\u7528\u548c\u5de5\u4eba\u81ea\u4e3b\u6743\u662f\u5173\u952e\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u901a\u8fc7\u81ea\u52a8\u5316\u91cd\u590d\u4efb\u52a1\u548c\u4fdd\u7559\u4eba\u7c7b\u53c2\u4e0e\u590d\u6742\u4efb\u52a1\uff0c\u5e73\u8861\u4e86\u89c4\u6a21\u5316\u548c\u771f\u5b9e\u6027\u3002"}}
{"id": "2505.24189", "pdf": "https://arxiv.org/pdf/2505.24189", "abs": "https://arxiv.org/abs/2505.24189", "authors": ["Orlando Marquez Ayala", "Patrice Bechard", "Emily Chen", "Maggie Baird", "Jingfei Chen"], "title": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4o can handle a wide range of\ncomplex tasks with the right prompt. As per token costs are reduced, the\nadvantages of fine-tuning Small Language Models (SLMs) for real-world\napplications -- faster inference, lower costs -- may no longer be clear. In\nthis work, we present evidence that, for domain-specific tasks that require\nstructured outputs, SLMs still have a quality advantage. We compare fine-tuning\nan SLM against prompting LLMs on the task of generating low-code workflows in\nJSON form. We observe that while a good prompt can yield reasonable results,\nfine-tuning improves quality by 10% on average. We also perform systematic\nerror analysis to reveal model limitations.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5982GPT-4o\u80fd\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u5728\u9700\u8981\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4ecd\u6709\u8d28\u91cf\u4f18\u52bf\uff0c\u5e73\u5747\u63d0\u534710%\u3002", "motivation": "\u63a2\u8ba8\u5728LLM\u6210\u672c\u964d\u4f4e\u7684\u80cc\u666f\u4e0b\uff0c\u5fae\u8c03SLM\u662f\u5426\u4ecd\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5fae\u8c03SLM\u4e0e\u63d0\u793aLLM\u5728\u751f\u6210JSON\u683c\u5f0f\u4f4e\u4ee3\u7801\u5de5\u4f5c\u6d41\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u7cfb\u7edf\u9519\u8bef\u5206\u6790\u3002", "result": "\u5fae\u8c03SLM\u5728\u8d28\u91cf\u4e0a\u5e73\u5747\u4f18\u4e8eLLM 10%\uff0c\u63d0\u793aLLM\u867d\u80fd\u63d0\u4f9b\u5408\u7406\u7ed3\u679c\uff0c\u4f46\u4ecd\u6709\u5c40\u9650\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u9700\u8981\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff0c\u5fae\u8c03SLM\u4ecd\u5177\u6709\u663e\u8457\u7684\u8d28\u91cf\u4f18\u52bf\uff0c\u503c\u5f97\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8003\u8651\u3002"}}
{"id": "2505.24195", "pdf": "https://arxiv.org/pdf/2505.24195", "abs": "https://arxiv.org/abs/2505.24195", "authors": ["Zining Wang", "Yuxuan Zhang", "Dongwook Yoon", "Nicholas Vincent", "Farhan Samir", "Vered Shwartz"], "title": "WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "With more than 11 times as many pageviews as the next, English Wikipedia\ndominates global knowledge access relative to other language editions. Readers\nare prone to assuming English Wikipedia as a superset of all language editions,\nleading many to prefer it even when their primary language is not English.\nOther language editions, however, comprise complementary facts rooted in their\nrespective cultures and media environments, which are marginalized in English\nWikipedia. While Wikipedia's user interface enables switching between language\neditions through its Interlanguage Link (ILL) system, it does not reveal to\nreaders that other language editions contain valuable, complementary\ninformation. We present WikiGap, a system that surfaces complementary facts\nsourced from other Wikipedias within the English Wikipedia interface.\nSpecifically, by combining a recent multilingual information-gap discovery\nmethod with a user-centered design, WikiGap enables access to complementary\ninformation from French, Russian, and Chinese Wikipedia. In a mixed-methods\nstudy (n=21), WikiGap significantly improved fact-finding accuracy, reduced\ntask time, and received a 32-point higher usability score relative to\nWikipedia's current ILL-based navigation system. Participants reported\nincreased awareness of the availability of complementary information in\nnon-English editions and reconsidered the completeness of English Wikipedia.\nWikiGap thus paves the way for improved epistemic equity across language\neditions.", "AI": {"tldr": "WikiGap\u7cfb\u7edf\u901a\u8fc7\u591a\u8bed\u8a00\u4fe1\u606f\u5dee\u8ddd\u53d1\u73b0\u65b9\u6cd5\u548c\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\uff0c\u5728\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u754c\u9762\u4e2d\u5c55\u793a\u5176\u4ed6\u8bed\u8a00\u7248\u672c\u7684\u8865\u5145\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u606f\u67e5\u627e\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u5728\u5168\u7403\u77e5\u8bc6\u83b7\u53d6\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u4ed6\u8bed\u8a00\u7248\u672c\u5305\u542b\u7684\u6587\u5316\u548c\u5a92\u4f53\u73af\u5883\u76f8\u5173\u7684\u8865\u5145\u4fe1\u606f\u88ab\u5ffd\u89c6\u3002WikiGap\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u591a\u8bed\u8a00\u4fe1\u606f\u5dee\u8ddd\u53d1\u73b0\u65b9\u6cd5\u548c\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\uff0c\u4ece\u6cd5\u8bed\u3001\u4fc4\u8bed\u548c\u4e2d\u6587\u7ef4\u57fa\u767e\u79d1\u4e2d\u63d0\u53d6\u8865\u5145\u4fe1\u606f\u3002", "result": "\u5728\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u4e2d\uff0cWikiGap\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u67e5\u627e\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u4efb\u52a1\u65f6\u95f4\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u53ef\u7528\u6027\u8bc4\u5206\u3002", "conclusion": "WikiGap\u63d0\u9ad8\u4e86\u7528\u6237\u5bf9\u5176\u4ed6\u8bed\u8a00\u7248\u672c\u8865\u5145\u4fe1\u606f\u7684\u8ba4\u8bc6\uff0c\u4fc3\u8fdb\u4e86\u8de8\u8bed\u8a00\u7248\u672c\u7684\u77e5\u8bc6\u516c\u5e73\u3002"}}
{"id": "2505.24200", "pdf": "https://arxiv.org/pdf/2505.24200", "abs": "https://arxiv.org/abs/2505.24200", "authors": ["Qingzheng Wang", "Jiancheng Sun", "Yifan Peng", "Shinji Watanabe"], "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Multilingual speech processing with self-supervised or supervised pre-trained\nSpeech Foundation Models (SFM) has achieved strong performance on tasks like\nLanguage Identification (LID) and Automatic Speech Recognition (ASR). However,\nthese models struggle with limited resources during fine-tuning. This paper\nenhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple\nstrategies for adapting SFMs, including frozen upstream training, partial\nfine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation\nto mitigate performance gaps in few-shot settings and introduce LID\nConnectionist Temporal Classification (CTC) loss for regularization. Our\napproach achieves a 14% relative improvement in LID accuracy and a 30% relative\nreduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place\nin the Interspeech 2025 ML-SUPERB 2.0 Challenge.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u591a\u79cd\u7b56\u7565\uff08\u5982\u51bb\u7ed3\u4e0a\u6e38\u8bad\u7ec3\u3001\u90e8\u5206\u5fae\u8c03\u548c\u4f4e\u79e9\u9002\u5e94\uff09\u6539\u8fdb\u591a\u8bed\u8a00LID\u548cASR\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548cLID CTC\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u5fae\u8c03\u65f6\u8d44\u6e90\u6709\u9650\u7684\u95ee\u9898\uff0c\u63d0\u5347\u591a\u8bed\u8a00LID\u548cASR\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u51bb\u7ed3\u4e0a\u6e38\u8bad\u7ec3\u3001\u90e8\u5206\u5fae\u8c03\u3001\u4f4e\u79e9\u9002\u5e94\u7b49\u7b56\u7565\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548cLID CTC\u635f\u5931\u8fdb\u884c\u6b63\u5219\u5316\u3002", "result": "\u5728ML-SUPERB 2.0\u4e0a\uff0cLID\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u534714%\uff0cASR CER\u76f8\u5bf9\u964d\u4f4e30%\uff0c\u5e76\u5728\u6311\u6218\u8d5b\u4e2d\u83b7\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u8bed\u8a00LID\u548cASR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.24239", "pdf": "https://arxiv.org/pdf/2505.24239", "abs": "https://arxiv.org/abs/2505.24239", "authors": ["Sana Ebrahimi", "Mohsen Dehghankar", "Abolfazl Asudeh"], "title": "An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "While multi-agent LLM systems show strong capabilities in various domains,\nthey are highly vulnerable to adversarial and low-performing agents. To resolve\nthis issue, in this paper, we introduce a general and adversary-resistant\nmulti-agent LLM framework based on credibility scoring. We model the\ncollaborative query-answering process as an iterative game, where the agents\ncommunicate and contribute to a final system output. Our system associates a\ncredibility score that is used when aggregating the team outputs. The\ncredibility scores are learned gradually based on the past contributions of\neach agent in query answering. Our experiments across multiple tasks and\nsettings demonstrate our system's effectiveness in mitigating adversarial\ninfluence and enhancing the resilience of multi-agent cooperation, even in the\nadversary-majority settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u4fe1\u5ea6\u8bc4\u5206\u7684\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u4ee5\u62b5\u5fa1\u5bf9\u6297\u6027\u548c\u4f4e\u6027\u80fd\u667a\u80fd\u4f53\u7684\u5f71\u54cd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u548c\u4f4e\u6027\u80fd\u667a\u80fd\u4f53\u7684\u5e72\u6270\u3002", "method": "\u5c06\u534f\u4f5c\u95ee\u7b54\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u8fed\u4ee3\u6e38\u620f\uff0c\u901a\u8fc7\u53ef\u4fe1\u5ea6\u8bc4\u5206\u805a\u5408\u56e2\u961f\u8f93\u51fa\uff0c\u8bc4\u5206\u57fa\u4e8e\u667a\u80fd\u4f53\u8fc7\u53bb\u7684\u8868\u73b0\u9010\u6b65\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u51cf\u8f7b\u5bf9\u6297\u6027\u5f71\u54cd\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u5bf9\u6297\u6027\u5360\u591a\u6570\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6297\u5e72\u6270\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.24292", "pdf": "https://arxiv.org/pdf/2505.24292", "abs": "https://arxiv.org/abs/2505.24292", "authors": ["Yueqi Zhang", "Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Human-AI conversation frequently relies on quoting earlier text-\"check it\nwith the formula I just highlighted\"-yet today's large language models (LLMs)\nlack an explicit mechanism for locating and exploiting such spans. We formalise\nthe challenge as span-conditioned generation, decomposing each turn into the\ndialogue history, a set of token-offset quotation spans, and an intent\nutterance. Building on this abstraction, we introduce a quotation-centric data\npipeline that automatically synthesises task-specific dialogues, verifies\nanswer correctness through multi-stage consistency checks, and yields both a\nheterogeneous training corpus and the first benchmark covering five\nrepresentative scenarios. To meet the benchmark's zero-overhead and\nparameter-efficiency requirements, we propose QuAda, a lightweight\ntraining-based method that attaches two bottleneck projections to every\nattention head, dynamically amplifying or suppressing attention to quoted spans\nat inference time while leaving the prompt unchanged and updating < 2.8% of\nbackbone weights. Experiments across models show that QuAda is suitable for all\nscenarios and generalises to unseen topics, offering an effective,\nplug-and-play solution for quotation-aware dialogue.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u4eba\u7c7b-AI\u5bf9\u8bdd\u4e2d\u5f15\u7528\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8de8\u5ea6\u6761\u4ef6\u751f\u6210\u548cQuAda\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f15\u7528\u6587\u672c\u7684\u9ad8\u6548\u5229\u7528\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u673a\u5236\u5b9a\u4f4d\u548c\u5229\u7528\u5bf9\u8bdd\u4e2d\u7684\u5f15\u7528\u6587\u672c\uff0c\u9650\u5236\u4e86\u5bf9\u8bdd\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u8de8\u5ea6\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1QuAda\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u52a8\u6001\u8c03\u6574\u5bf9\u5f15\u7528\u6587\u672c\u7684\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eQuAda\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u4e3b\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "QuAda\u4e3a\u5f15\u7528\u611f\u77e5\u5bf9\u8bdd\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.24293", "pdf": "https://arxiv.org/pdf/2505.24293", "abs": "https://arxiv.org/abs/2505.24293", "authors": ["James R. Golden"], "title": "Large Language Models are Locally Linear Mappings", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Version 0", "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u64cd\u4f5c\u6620\u5c04\u4e3a\u7ebf\u6027\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u8868\u793a\u7684\u5c40\u90e8\u7ebf\u6027\u7279\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u73b0\u4ee3LLM\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u5c3d\u7ba1\u5176\u5177\u6709\u5168\u5c40\u975e\u7ebf\u6027\uff0c\u4f46\u5c40\u90e8\u53ef\u4ee5\u8fd1\u4f3c\u4e3a\u7ebf\u6027\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u4fee\u6539\u8f93\u5165\u5e8f\u5217\u7684\u68af\u5ea6\u8ba1\u7b97\uff0c\u4f7f\u6a21\u578b\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u51e0\u4e4e\u5b8c\u5168\u590d\u73b0\u524d\u5411\u9884\u6d4b\u7684\u7ebf\u6027\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u5947\u5f02\u503c\u5206\u89e3\u5206\u6790\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u5728\u6781\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u4e14\u6700\u5927\u5947\u5f02\u5411\u91cf\u4e0e\u6700\u53ef\u80fd\u8f93\u51fa\u4ee4\u724c\u76f8\u5173\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bed\u4e49\u6982\u5ff5\u7684\u6d8c\u73b0\u3002", "conclusion": "\u5c3d\u7ba1LLM\u5177\u6709\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u5176\u5c40\u90e8\u7ebf\u6027\u5206\u89e3\u63d0\u4f9b\u4e86\u5bf9\u5176\u5185\u90e8\u8868\u793a\u548c\u8bed\u4e49\u7ed3\u6784\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.24324", "pdf": "https://arxiv.org/pdf/2505.24324", "abs": "https://arxiv.org/abs/2505.24324", "authors": ["Ivan Petrukha", "Yana Kurliak", "Nataliia Stulova"], "title": "SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE"], "comment": "Accepted to FORGE'25 Benchmarking on 15.01.2025, to be published by\n  IEEE under the CC BY-NC-ND 4.0 license. This is the accepted version of the\n  article (5 pages, 2 figures, 1 table). DOI will be added upon publication", "summary": "In recent years, large language models (LLMs) have showcased significant\nadvancements in code generation. However, most evaluation benchmarks are\nprimarily oriented towards Python, making it difficult to evaluate other\nprogramming languages, such as Swift, with high quality. By examining widely\nestablished multilingual benchmarks like HumanEval-XL and MultiPL-E, we\nidentified critical issues specific to their Swift components, making them\ninsufficient or even irrelevant for assessing LLM coding capabilities on Swift.\nUnlike these existing approaches, which prioritize rapid scaling and\ngeneralization by automatically translating Python-centric benchmarks with\nLLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the\nfirst Swift-oriented benchmark consisting of 28 carefully hand-crafted\nproblems, and evaluate 44 popular Code LLMs on it. Our results show significant\nLLM scores drop for problems requiring language-specific features, most\nnoticeable in the models of smaller sizes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SwiftEval\uff0c\u9996\u4e2a\u9488\u5bf9Swift\u7f16\u7a0b\u8bed\u8a00\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u5728Swift\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9Python\uff0c\u65e0\u6cd5\u9ad8\u8d28\u91cf\u8bc4\u4f30\u5176\u4ed6\u7f16\u7a0b\u8bed\u8a00\uff08\u5982Swift\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9Swift\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u91c7\u7528\u8d28\u91cf\u4f18\u5148\u7684\u65b9\u6cd5\uff0c\u624b\u5de5\u8bbe\u8ba1\u4e8628\u4e2aSwift\u95ee\u9898\uff0c\u8bc4\u6d4b\u4e8644\u4e2a\u6d41\u884c\u7684\u4ee3\u7801\u751f\u6210\u5927\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u9700\u8981\u8bed\u8a00\u7279\u5b9a\u529f\u80fd\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5c0f\u89c4\u6a21\u6a21\u578b\u3002", "conclusion": "SwiftEval\u586b\u8865\u4e86Swift\u8bc4\u6d4b\u7684\u7a7a\u767d\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8bed\u8a00\u7279\u5b9a\u529f\u80fd\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.24379", "pdf": "https://arxiv.org/pdf/2505.24379", "abs": "https://arxiv.org/abs/2505.24379", "authors": ["Xiaoyu Wu", "Yifei Pang", "Terrance Liu", "Zhiwei Steven Wu"], "title": "Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Large language models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard, believed to be robust against\nprivacy-related attacks. In this paper, we challenge this assumption by\nintroducing a novel data extraction attack that compromises even exact\nunlearning. Our method leverages both the pre- and post-unlearning models: by\nguiding the post-unlearning model using signals from the pre-unlearning model,\nwe uncover patterns that reflect the removed data distribution. Combining model\nguidance with a token filtering strategy, our attack significantly improves\nextraction success rates -- doubling performance in some cases -- across common\nbenchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our\nattack's effectiveness on a simulated medical diagnosis dataset to highlight\nreal-world privacy risks associated with exact unlearning. In light of our\nfindings, which suggest that unlearning may, in a contradictory way, increase\nthe risk of privacy leakage, we advocate for evaluation of unlearning methods\nto consider broader threat models that account not only for post-unlearning\nmodels but also for adversarial access to prior checkpoints.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u7cbe\u786e\u9057\u5fd8\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u9ec4\u91d1\u6807\u51c6\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u63d0\u53d6\u653b\u51fb\u65b9\u6cd5\uff0c\u8bc1\u660e\u5373\u4f7f\u7cbe\u786e\u9057\u5fd8\u4e5f\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u3002", "motivation": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e2d\u53ef\u80fd\u5305\u542b\u7684\u654f\u611f\u4fe1\u606f\uff0c\u73b0\u6709\u7cbe\u786e\u9057\u5fd8\u65b9\u6cd5\u88ab\u8ba4\u4e3a\u80fd\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u4f5c\u8005\u8d28\u7591\u5176\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u9884\u9057\u5fd8\u548c\u9057\u5fd8\u540e\u6a21\u578b\u7684\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6570\u636e\u63d0\u53d6\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u4ee4\u724c\u8fc7\u6ee4\u7b56\u7565\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "result": "\u653b\u51fb\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u63d0\u53d6\u6210\u529f\u7387\uff0c\u5e76\u5728\u6a21\u62df\u533b\u7597\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u73b0\u5b9e\u9690\u79c1\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u9057\u5fd8\u53ef\u80fd\u589e\u52a0\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5efa\u8bae\u8bc4\u4f30\u9057\u5fd8\u65b9\u6cd5\u65f6\u8003\u8651\u66f4\u5e7f\u6cdb\u7684\u5a01\u80c1\u6a21\u578b\u3002"}}
{"id": "2505.24478", "pdf": "https://arxiv.org/pdf/2505.24478", "abs": "https://arxiv.org/abs/2505.24478", "authors": ["Vasilije Markovic", "Lazar Obradovic", "Laszlo Hajdu", "Jovan Pavlovic"], "title": "Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "This is a preliminary version. A revised and expanded version is in\n  preparation", "summary": "Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results\nin complex systems with numerous hyperparameters that directly affect\nperformance. While such systems are increasingly common in retrieval-augmented\ngeneration, the role of systematic hyperparameter optimization remains\nunderexplored. In this paper, we study this problem in the context of Cognee, a\nmodular framework for end-to-end KG construction and retrieval. Using three\nmulti-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize\nparameters related to chunking, graph construction, retrieval, and prompting.\nEach configuration is scored using established metrics (exact match, F1, and\nDeepEval's LLM-based correctness metric). Our results demonstrate that\nmeaningful gains can be achieved through targeted tuning. While the gains are\nconsistent, they are not uniform, with performance varying across datasets and\nmetrics. This variability highlights both the value of tuning and the\nlimitations of standard evaluation measures. While demonstrating the immediate\npotential of hyperparameter tuning, we argue that future progress will depend\nnot only on architectural advances but also on clearer frameworks for\noptimization and evaluation in complex, modular systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u7ed3\u5408\u65f6\uff0c\u7cfb\u7edf\u8d85\u53c2\u6570\u4f18\u5316\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u9488\u5bf9\u6027\u8c03\u4f18\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u63a2\u8ba8\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\uff0c\u8d85\u53c2\u6570\u4f18\u5316\u7684\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u6a21\u5757\u5316\u6846\u67b6\u4e2d\u3002", "method": "\u4f7f\u7528Cognee\u6846\u67b6\uff0c\u5728\u4e09\u4e2a\u591a\u8df3QA\u57fa\u51c6\u4e0a\u4f18\u5316\u4e0e\u5206\u5757\u3001\u56fe\u8c31\u6784\u5efa\u3001\u68c0\u7d22\u548c\u63d0\u793a\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u9488\u5bf9\u6027\u8c03\u4f18\u80fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u6548\u679c\u56e0\u6570\u636e\u96c6\u548c\u6307\u6807\u800c\u5f02\u3002", "conclusion": "\u672a\u6765\u8fdb\u5c55\u4e0d\u4ec5\u9700\u8981\u67b6\u6784\u521b\u65b0\uff0c\u8fd8\u9700\u66f4\u6e05\u6670\u7684\u4f18\u5316\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.24479", "pdf": "https://arxiv.org/pdf/2505.24479", "abs": "https://arxiv.org/abs/2505.24479", "authors": ["Sania Nayab", "Marco Simoni", "Giulio Rossolini"], "title": "Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation", "categories": ["cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The rapid spread of misinformation, further amplified by recent advances in\ngenerative AI, poses significant threats to society, impacting public opinion,\ndemocratic stability, and national security. Understanding and proactively\nassessing these threats requires exploring methodologies that enable structured\nand scalable misinformation generation. In this paper, we propose a novel\napproach that leverages knowledge graphs (KGs) as structured semantic resources\nto systematically generate fake triplets. By analyzing the structural\nproperties of KGs, such as the distance between entities and their predicates,\nwe identify plausibly false relationships. These triplets are then used to\nguide large language models (LLMs) in generating misinformation statements with\nvarying degrees of credibility. By utilizing structured semantic relationships,\nour deterministic approach produces misinformation inherently challenging for\nhumans to detect, drawing exclusively upon publicly available KGs (e.g.,\nWikiGraphs).\n  Additionally, we investigate the effectiveness of LLMs in distinguishing\nbetween genuine and artificially generated misinformation. Our analysis\nhighlights significant limitations in current LLM-based detection methods,\nunderscoring the necessity for enhanced detection strategies and a deeper\nexploration of inherent biases in generative models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u7cfb\u7edf\u751f\u6210\u865a\u5047\u4e09\u5143\u7ec4\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5206\u6790KGs\u7684\u7ed3\u6784\u7279\u6027\u751f\u6210\u96be\u4ee5\u8bc6\u522b\u7684\u865a\u5047\u4fe1\u606f\u3002\u540c\u65f6\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533a\u5206\u771f\u5047\u4fe1\u606f\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u52a0\u5267\u4e86\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\uff0c\u5bf9\u793e\u4f1a\u7a33\u5b9a\u548c\u56fd\u5bb6\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002\u7814\u7a76\u9700\u8981\u63a2\u7d22\u7ed3\u6784\u5316\u4e14\u53ef\u6269\u5c55\u7684\u865a\u5047\u4fe1\u606f\u751f\u6210\u65b9\u6cd5\u4ee5\u8bc4\u4f30\u5176\u5f71\u54cd\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u7279\u6027\uff08\u5982\u5b9e\u4f53\u95f4\u8ddd\u79bb\u548c\u8c13\u8bcd\u5173\u7cfb\uff09\u751f\u6210\u865a\u5047\u4e09\u5143\u7ec4\uff0c\u5e76\u6307\u5bfcLLMs\u751f\u6210\u4e0d\u540c\u53ef\u4fe1\u5ea6\u7684\u865a\u5047\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u865a\u5047\u4fe1\u606f\u96be\u4ee5\u88ab\u4eba\u7c7b\u8bc6\u522b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u7b56\u7565\uff0c\u5e76\u6df1\u5165\u7814\u7a76\u751f\u6210\u6a21\u578b\u7684\u56fa\u6709\u504f\u89c1\u3002"}}
{"id": "2505.24535", "pdf": "https://arxiv.org/pdf/2505.24535", "abs": "https://arxiv.org/abs/2505.24535", "authors": ["Narmeen Oozeer", "Luke Marks", "Fazl Barez", "Amirali Abdullah"], "title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Controlling multiple behavioral attributes in large language models (LLMs) at\ninference time is a challenging problem due to interference between attributes\nand the limitations of linear steering methods, which assume additive behavior\nin activation space and require per-attribute tuning. We introduce K-Steering,\na unified and flexible approach that trains a single non-linear multi-label\nclassifier on hidden activations and computes intervention directions via\ngradients at inference time. This avoids linearity assumptions, removes the\nneed for storing and tuning separate attribute vectors, and allows dynamic\ncomposition of behaviors without retraining. To evaluate our method, we propose\ntwo new benchmarks, ToneBank and DebateMix, targeting compositional behavioral\ncontrol. Empirical results across 3 model families, validated by both\nactivation-based classifiers and LLM-based judges, demonstrate that K-Steering\noutperforms strong baselines in accurately steering multiple behaviors.", "AI": {"tldr": "K-Steering\u662f\u4e00\u79cd\u975e\u7ebf\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u8ba1\u7b97\u5e72\u9884\u65b9\u5411\uff0c\u89e3\u51b3\u4e86\u591a\u884c\u4e3a\u5c5e\u6027\u63a7\u5236\u7684\u5e72\u6270\u95ee\u9898\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u65f6\u63a7\u5236\u591a\u4e2a\u884c\u4e3a\u5c5e\u6027\u5b58\u5728\u5e72\u6270\u548c\u7ebf\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u975e\u7ebf\u6027\u591a\u6807\u7b7e\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u68af\u5ea6\u8ba1\u7b97\u5e72\u9884\u65b9\u5411\uff0c\u907f\u514d\u7ebf\u6027\u5047\u8bbe\u548c\u5355\u72ec\u8c03\u6574\u5c5e\u6027\u5411\u91cf\u3002", "result": "\u57283\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e2d\uff0cK-Steering\u5728\u51c6\u786e\u63a7\u5236\u591a\u4e2a\u884c\u4e3a\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "K-Steering\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u52a8\u6001\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u591a\u884c\u4e3a\u63a7\u5236\u3002"}}
{"id": "2505.24571", "pdf": "https://arxiv.org/pdf/2505.24571", "abs": "https://arxiv.org/abs/2505.24571", "authors": ["Nikola Ljube\u0161i\u0107", "Ivan Porupski", "Peter Rupnik"], "title": "Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to InterSpeech2025", "summary": "Automating primary stress identification has been an active research field\ndue to the role of stress in encoding meaning and aiding speech comprehension.\nPrevious studies relied mainly on traditional acoustic features and English\ndatasets. In this paper, we investigate the approach of fine-tuning a\npre-trained transformer model with an audio frame classification head. Our\nexperiments use a new Croatian training dataset, with test sets in Croatian,\nSerbian, the Chakavian dialect, and Slovenian. By comparing an SVM classifier\nusing traditional acoustic features with the fine-tuned speech transformer, we\ndemonstrate the transformer's superiority across the board, achieving\nnear-perfect results for Croatian and Serbian, with a 10-point performance drop\nfor the more distant Chakavian and Slovenian. Finally, we show that only a few\nhundred multi-syllabic training words suffice for strong performance. We\nrelease our datasets and model under permissive licenses.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u6765\u81ea\u52a8\u8bc6\u522b\u4e3b\u8981\u91cd\u97f3\uff0c\u5728\u514b\u7f57\u5730\u4e9a\u8bed\u7b49\u8bed\u8a00\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u91cd\u97f3\u5728\u7f16\u7801\u610f\u4e49\u548c\u8f85\u52a9\u8bed\u97f3\u7406\u89e3\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u591a\u4f9d\u8d56\u4f20\u7edf\u58f0\u5b66\u7279\u5f81\u548c\u82f1\u8bed\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\uff0c\u6dfb\u52a0\u97f3\u9891\u5e27\u5206\u7c7b\u5934\uff0c\u5e76\u5728\u65b0\u7684\u514b\u7f57\u5730\u4e9a\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "Transformer\u6a21\u578b\u5728\u6240\u6709\u6d4b\u8bd5\u8bed\u8a00\uff08\u514b\u7f57\u5730\u4e9a\u8bed\u3001\u585e\u5c14\u7ef4\u4e9a\u8bed\u3001\u67e5\u65b9\u8a00\u548c\u65af\u6d1b\u6587\u5c3c\u4e9a\u8bed\uff09\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfSVM\u5206\u7c7b\u5668\uff0c\u514b\u7f57\u5730\u4e9a\u8bed\u548c\u585e\u5c14\u7ef4\u4e9a\u8bed\u63a5\u8fd1\u5b8c\u7f8e\u3002", "conclusion": "\u4ec5\u9700\u5c11\u91cf\u591a\u97f3\u8282\u8bad\u7ec3\u8bcd\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.24710", "pdf": "https://arxiv.org/pdf/2505.24710", "abs": "https://arxiv.org/abs/2505.24710", "authors": ["Wei Chen", "Jiahao Zhang", "Haipeng Zhu", "Boyan Xu", "Zhifeng Hao", "Keli Zhang", "Junjian Ye", "Ruichu Cai"], "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025", "summary": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u611f\u77e5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Causal-aware LLMs\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u6765\u63d0\u5347\u51b3\u7b56\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u548c\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5b58\u50a8\u4e86\u5927\u91cf\u77e5\u8bc6\uff0c\u4f46\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u73af\u5883\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5b66\u4e60-\u9002\u5e94-\u884c\u52a8\u201d\u8303\u5f0f\uff1a1\uff09\u5b66\u4e60\u9636\u6bb5\u63d0\u53d6\u73af\u5883\u56e0\u679c\u5b9e\u4f53\u548c\u5173\u7cfb\u521d\u59cb\u5316SCM\uff1b2\uff09\u9002\u5e94\u9636\u6bb5\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u66f4\u65b0SCM\uff1b3\uff09\u884c\u52a8\u9636\u6bb5\u5229\u7528SCM\u77e5\u8bc6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5236\u5b9a\u7b56\u7565\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754c\u6e38\u620f\u201cCrafter\u201d\u768422\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u56e0\u679c\u611f\u77e5LLMs\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u56e0\u679c\u77e5\u8bc6\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u7406\u89e3\u73af\u5883\u5e76\u505a\u51fa\u9ad8\u6548\u51b3\u7b56\u3002"}}
{"id": "2505.24715", "pdf": "https://arxiv.org/pdf/2505.24715", "abs": "https://arxiv.org/abs/2505.24715", "authors": ["Fabio Fehr", "Prabhu Teja Sivaprasad", "Luca Franceschi", "Giovanni Zappella"], "title": "CoRet: Improved Retriever for Code Editing", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "In this paper, we introduce CoRet, a dense retrieval model designed for\ncode-editing tasks that integrates code semantics, repository structure, and\ncall graph dependencies. The model focuses on retrieving relevant portions of a\ncode repository based on natural language queries such as requests to implement\nnew features or fix bugs. These retrieved code chunks can then be presented to\na user or to a second code-editing model or agent. To train CoRet, we propose a\nloss function explicitly designed for repository-level retrieval. On SWE-bench\nand Long Code Arena's bug localisation datasets, we show that our model\nsubstantially improves retrieval recall by at least 15 percentage points over\nexisting models, and ablate the design choices to show their importance in\nachieving these results.", "AI": {"tldr": "CoRet\u662f\u4e00\u79cd\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\uff0c\u4e13\u4e3a\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u8bbe\u8ba1\uff0c\u6574\u5408\u4ee3\u7801\u8bed\u4e49\u3001\u4ed3\u5e93\u7ed3\u6784\u548c\u8c03\u7528\u56fe\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u53ec\u56de\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u68c0\u7d22\u4ee3\u7801\u4ed3\u5e93\u76f8\u5173\u90e8\u5206\u7684\u9700\u6c42\uff0c\u5982\u5b9e\u73b0\u65b0\u529f\u80fd\u6216\u4fee\u590d\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e13\u95e8\u4e3a\u4ed3\u5e93\u7ea7\u68c0\u7d22\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\uff0c\u6574\u5408\u4ee3\u7801\u8bed\u4e49\u3001\u4ed3\u5e93\u7ed3\u6784\u548c\u8c03\u7528\u56fe\u4f9d\u8d56\u3002", "result": "\u5728SWE-bench\u548cLong Code Arena\u7684bug\u5b9a\u4f4d\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u7d22\u53ec\u56de\u7387\u81f3\u5c11\u63d0\u9ad815\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CoRet\u7684\u8bbe\u8ba1\u9009\u62e9\u5bf9\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2505.24736", "pdf": "https://arxiv.org/pdf/2505.24736", "abs": "https://arxiv.org/abs/2505.24736", "authors": ["Julio Cesar Cavalcanti", "Gabriel Skantze"], "title": "\"Dyadosyncrasy\", Idiosyncrasy and Demographic Factors in Turn-Taking", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Turn-taking in dialogue follows universal constraints but also varies\nsignificantly. This study examines how demographic (sex, age, education) and\nindividual factors shape turn-taking using a large dataset of US English\nconversations (Fisher). We analyze Transition Floor Offset (TFO) and find\nnotable interspeaker variation. Sex and age have small but significant effects\nfemale speakers and older individuals exhibit slightly shorter offsets - while\neducation shows no effect. Lighter topics correlate with shorter TFOs. However,\nindividual differences have a greater impact, driven by a strong idiosyncratic\nand an even stronger \"dyadosyncratic\" component - speakers in a dyad resemble\neach other more than they resemble themselves in different dyads. This suggests\nthat the dyadic relationship and joint activity are the strongest determinants\nof TFO, outweighing demographic influences.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u53e3\u7edf\u8ba1\u548c\u4e2a\u4f53\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u5bf9\u8bdd\u4e2d\u7684\u8f6e\u6362\u884c\u4e3a\uff0c\u53d1\u73b0\u4e2a\u4f53\u5dee\u5f02\u548c\u5bf9\u8bdd\u53cc\u65b9\u5173\u7cfb\u5bf9\u8f6e\u6362\u65f6\u95f4\u7684\u5f71\u54cd\u8d85\u8fc7\u6027\u522b\u3001\u5e74\u9f84\u7b49\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\u3002", "motivation": "\u4e86\u89e3\u5bf9\u8bdd\u8f6e\u6362\u884c\u4e3a\u4e2d\u4eba\u53e3\u7edf\u8ba1\u548c\u4e2a\u4f53\u56e0\u7d20\u7684\u76f8\u5bf9\u5f71\u54cd\uff0c\u63ed\u793a\u8f6e\u6362\u65f6\u95f4\u7684\u51b3\u5b9a\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u82f1\u8bed\u5bf9\u8bdd\u6570\u636e\u96c6\uff08Fisher\uff09\uff0c\u5206\u6790\u8fc7\u6e21\u5730\u677f\u504f\u79fb\uff08TFO\uff09\uff0c\u8003\u5bdf\u6027\u522b\u3001\u5e74\u9f84\u3001\u6559\u80b2\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u6027\u522b\u548c\u5e74\u9f84\u5bf9TFO\u6709\u5fae\u5c0f\u4f46\u663e\u8457\u5f71\u54cd\uff0c\u6559\u80b2\u65e0\u5f71\u54cd\uff1b\u4e2a\u4f53\u5dee\u5f02\u548c\u5bf9\u8bdd\u53cc\u65b9\u5173\u7cfb\u5bf9TFO\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u5bf9\u8bdd\u53cc\u65b9\u5173\u7cfb\u548c\u5171\u540c\u6d3b\u52a8\u662fTFO\u7684\u6700\u5f3a\u51b3\u5b9a\u56e0\u7d20\uff0c\u8d85\u8fc7\u4eba\u53e3\u7edf\u8ba1\u5f71\u54cd\u3002"}}
{"id": "2505.24749", "pdf": "https://arxiv.org/pdf/2505.24749", "abs": "https://arxiv.org/abs/2505.24749", "authors": ["Yehonathan Refael", "Guy Smorodinsky", "Tom Tirer", "Ofir Lindenbaum"], "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training", "categories": ["cs.LG", "cs.CL", "math.OC"], "comment": null, "summary": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods.", "AI": {"tldr": "SUMO\u662f\u4e00\u79cd\u57fa\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u548cSVD\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u7cbe\u786e\u6b63\u4ea4\u5316\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u4f4e\u79e9\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u867d\u8282\u7701\u5185\u5b58\uff0c\u4f46\u5ffd\u7565\u4e86\u6536\u655b\u52a0\u901f\u6f5c\u529b\uff0c\u5c24\u5176\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u5404\u5411\u5f02\u6027\u635f\u5931\u573a\u666f\u8868\u73b0\u4e0d\u4f73\u3002", "method": "SUMO\u5229\u7528\u52a8\u6001\u8c03\u6574\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u548cSVD\u8fdb\u884c\u6b63\u4ea4\u5316\uff0c\u5b9e\u73b0\u89c4\u8303\u8bf1\u5bfc\u7684\u6700\u9661\u4e0b\u964d\u4f18\u5316\uff0c\u51cf\u5c11\u8fd1\u4f3c\u8bef\u5dee\u3002", "result": "\u7406\u8bba\u8bc1\u660eSUMO\u80fd\u964d\u4f4e\u8fd1\u4f3c\u8bef\u5dee\u5e76\u63d0\u5347\u6536\u655b\u901f\u5ea6\uff1b\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u5185\u5b58\u6548\u7387\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SUMO\u901a\u8fc7\u7cbe\u786e\u6b63\u4ea4\u5316\u663e\u8457\u4f18\u5316LLM\u8bad\u7ec3\uff0c\u5e73\u8861\u4e86\u5185\u5b58\u6548\u7387\u548c\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2505.24760", "pdf": "https://arxiv.org/pdf/2505.24760", "abs": "https://arxiv.org/abs/2505.24760", "authors": ["Zafir Stojanovski", "Oliver Stanley", "Joe Sharratt", "Richard Jones", "Abdulhakeem Adefioye", "Jean Kaddour", "Andreas K\u00f6pf"], "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "For code, see https://github.com/open-thought/reasoning-gym", "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.", "AI": {"tldr": "Reasoning Gym (RG) \u662f\u4e00\u4e2a\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u73af\u5883\u5e93\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u652f\u6301\u591a\u9886\u57df\u6570\u636e\u751f\u6210\u548c\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u63a8\u7406\u6570\u636e\u96c6\u56fa\u5b9a\u4e14\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u65e0\u9650\u8bad\u7ec3\u6570\u636e\u5e76\u8c03\u6574\u590d\u6742\u5ea6\u3002", "method": "\u5229\u7528\u8d85\u8fc7100\u4e2a\u6570\u636e\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u8986\u76d6\u4ee3\u6570\u3001\u7b97\u672f\u3001\u51e0\u4f55\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u652f\u6301\u96be\u5ea6\u53ef\u8c03\u7684\u65e0\u9650\u6570\u636e\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e RG \u5728\u8bc4\u4f30\u548c\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u6a21\u578b\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027\u3002", "conclusion": "RG \u4e3a\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2505.24823", "pdf": "https://arxiv.org/pdf/2505.24823", "abs": "https://arxiv.org/abs/2505.24823", "authors": ["Yinggan Xu", "Yue Liu", "Zhiqiang Gao", "Changnan Peng", "Di Luo"], "title": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPhySense\u57fa\u51c6\uff0c\u63ed\u793a\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u79d1\u5b66\u95ee\u9898\u65f6\uff0c\u96be\u4ee5\u50cf\u4eba\u7c7b\u4e13\u5bb6\u90a3\u6837\u8fdb\u884c\u7b80\u6d01\u3001\u57fa\u4e8e\u539f\u7406\u7684\u63a8\u7406\uff0c\u800c\u662f\u751f\u6210\u5197\u957f\u4e14\u4e0d\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165PhySense\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u79cd\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u7c7b\u578b\u5728\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u4e0e\u4e13\u5bb6\u63a8\u7406\u8def\u5f84\u5bf9\u9f50\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u539f\u7406\u7684\u79d1\u5b66\u63a8\u7406AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.24844", "pdf": "https://arxiv.org/pdf/2505.24844", "abs": "https://arxiv.org/abs/2505.24844", "authors": ["Wanyun Xie", "Francesco Tonin", "Volkan Cevher"], "title": "Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning", "categories": ["cs.LG", "cs.CL"], "comment": "ICML 2025", "summary": "Training data mixtures greatly impact the generalization performance of large\nlanguage models. Existing domain reweighting methods often rely on costly\nweight computations and require retraining when new data is introduced. To this\nend, we introduce a flexible and efficient data mixing framework, Chameleon,\nthat employs leverage scores to quantify domain importance within a learned\nembedding space. We first construct a domain affinity matrix over domain\nembeddings. The induced leverage scores determine a mixture that upweights\ndomains sharing common representations in embedding space. This formulation\nallows direct transfer to new data by computing the new domain embeddings. In\nexperiments, we demonstrate improvements over three key scenarios: (i) our\ncomputed weights improve performance on pretraining domains with a fraction of\nthe compute of existing methods; (ii) Chameleon can adapt to data changes\nwithout proxy retraining, boosting few-shot reasoning accuracies when\ntransferred to new data; (iii) our method enables efficient domain reweighting\nin finetuning, consistently improving test perplexity on all finetuning domains\nover uniform mixture. Our code is available at\nhttps://github.com/LIONS-EPFL/Chameleon.", "AI": {"tldr": "Chameleon\u662f\u4e00\u4e2a\u9ad8\u6548\u7075\u6d3b\u7684\u6570\u636e\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5206\u6570\u91cf\u5316\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u9886\u57df\u91cd\u8981\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u91cd\u52a0\u6743\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9700\u91cd\u65b0\u8bad\u7ec3\uff0cChameleon\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6784\u5efa\u9886\u57df\u4eb2\u548c\u77e9\u9635\uff0c\u5229\u7528\u8bf1\u5bfc\u7684\u6760\u6746\u5206\u6570\u786e\u5b9a\u6df7\u5408\u6743\u91cd\uff0c\u76f4\u63a5\u8fc1\u79fb\u5230\u65b0\u6570\u636e\u3002", "result": "\u5728\u9884\u8bad\u7ec3\u3001\u65b0\u6570\u636e\u9002\u5e94\u548c\u5fae\u8c03\u573a\u666f\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "Chameleon\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u9886\u57df\u91cd\u52a0\u6743\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.24846", "pdf": "https://arxiv.org/pdf/2505.24846", "abs": "https://arxiv.org/abs/2505.24846", "authors": ["Jingyan Shen", "Jiarui Yao", "Rui Yang", "Yifan Sun", "Feng Luo", "Rui Pan", "Tong Zhang", "Han Zhao"], "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.", "AI": {"tldr": "MiCRo\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u6df7\u5408\u5efa\u6a21\u548c\u52a8\u6001\u8def\u7531\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5956\u52b1\u5efa\u6a21\u65e0\u6cd5\u6355\u6349\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfBradley-Terry\u6a21\u578b\u5047\u8bbe\u5168\u5c40\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u504f\u597d\u7684\u591a\u6837\u6027\u548c\u5f02\u8d28\u6027\uff0c\u9650\u5236\u4e86LLM\u7684\u4e2a\u6027\u5316\u548c\u591a\u5143\u5316\u5bf9\u9f50\u3002", "method": "MiCRo\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4e0a\u4e0b\u6587\u611f\u77e5\u6df7\u5408\u5efa\u6a21\u6355\u6349\u591a\u6837\u5316\u504f\u597d\uff1b2\uff09\u5728\u7ebf\u8def\u7531\u7b56\u7565\u52a8\u6001\u8c03\u6574\u6df7\u5408\u6743\u91cd\u4ee5\u9002\u5e94\u7279\u5b9a\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMiCRo\u80fd\u6709\u6548\u6355\u6349\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4e2a\u6027\u5316\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "MiCRo\u4e3aLLM\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\u3002"}}
{"id": "2505.24850", "pdf": "https://arxiv.org/pdf/2505.24850", "abs": "https://arxiv.org/abs/2505.24850", "authors": ["Shuyao Xu", "Cheng Peng", "Jiangxuan Long", "Weidi Xu", "Wei Chu", "Yuan Qi"], "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation", "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREDI\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6b63\u8d1f\u63a8\u7406\u75d5\u8ff9\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u4e22\u5f03\u9519\u8bef\u7684\u63a8\u7406\u75d5\u8ff9\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u6709\u4ef7\u503c\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6709\u6548\u5229\u7528\u6b63\u8d1f\u63a8\u7406\u75d5\u8ff9\u3002", "method": "\u63d0\u51faREDI\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5b66\u4e60\u6b63\u4f8b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7REDI\u76ee\u6807\u51fd\u6570\uff08\u4e00\u79cd\u7b80\u5355\u3001\u65e0\u53c2\u8003\u7684\u635f\u5931\u51fd\u6570\uff09\u7ed3\u5408\u6b63\u8d1f\u4f8b\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u3002", "result": "REDI\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0cQwen-REDI-1.5B\u6a21\u578b\u5728MATH-500\u4e0a\u8fbe\u523083.1%\u7684\u51c6\u786e\u7387\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u4e13\u6709\u6570\u636e\u7684\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "REDI\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u6b63\u8d1f\u63a8\u7406\u75d5\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u516c\u5f00\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\u3002"}}
{"id": "2505.24859", "pdf": "https://arxiv.org/pdf/2505.24859", "abs": "https://arxiv.org/abs/2505.24859", "authors": ["Joschka Braun", "Carsten Eickhoff", "Seyed Ali Bahrainian"], "title": "Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization", "categories": ["cs.LG", "cs.CL"], "comment": "29 pages, 21 figures, preprint", "summary": "Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u81ea\u7531\u751f\u6210\u4efb\u52a1\u4e2d\u4f7f\u7528\u8f6c\u5411\u5411\u91cf\u63a7\u5236\u6587\u672c\u5c5e\u6027\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u63a7\u5236\u76ee\u6807\u5c5e\u6027\uff0c\u4f46\u9ad8\u5f3a\u5ea6\u8f6c\u5411\u4f1a\u964d\u4f4e\u6587\u672c\u8d28\u91cf\u3002\u63d0\u793a\u6cd5\u63a7\u5236\u8f83\u5f31\u4f46\u8d28\u91cf\u4fdd\u6301\u66f4\u597d\uff0c\u4e24\u8005\u7ed3\u5408\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u8f6c\u5411\u5411\u91cf\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u6dfb\u52a0\u5b66\u4e60\u504f\u7f6e\u6765\u63a7\u5236\u6587\u672c\u5c5e\u6027\uff0c\u4f46\u5176\u5728\u81ea\u7531\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5728NEWTS\u6570\u636e\u96c6\u7684\u6458\u8981\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u8bc4\u4f30\u8f6c\u5411\u5411\u91cf\u5bf9\u4e3b\u9898\u7126\u70b9\u3001\u60c5\u611f\u3001\u6bd2\u6027\u548c\u53ef\u8bfb\u6027\u7684\u63a7\u5236\u6548\u679c\uff0c\u5e76\u4e0e\u63d0\u793a\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8f6c\u5411\u5411\u91cf\u80fd\u6709\u6548\u63a7\u5236\u76ee\u6807\u5c5e\u6027\uff0c\u4f46\u9ad8\u5f3a\u5ea6\u8f6c\u5411\u4f1a\u964d\u4f4e\u6587\u672c\u8d28\u91cf\uff1b\u63d0\u793a\u6cd5\u63a7\u5236\u8f83\u5f31\u4f46\u8d28\u91cf\u4fdd\u6301\u66f4\u597d\uff1b\u4e24\u8005\u7ed3\u5408\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u8f6c\u5411\u5411\u91cf\u5728\u81ea\u7531\u751f\u6210\u4efb\u52a1\u4e2d\u9700\u6743\u8861\u63a7\u5236\u5f3a\u5ea6\u4e0e\u6587\u672c\u8d28\u91cf\uff0c\u7ed3\u5408\u63d0\u793a\u6cd5\u53ef\u4f18\u5316\u6548\u679c\u3002"}}
